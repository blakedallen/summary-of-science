{"id": "http://arxiv.org/abs/1012.4769v1", "guidislink": true, "updated": "2010-12-21T19:18:49Z", "updated_parsed": [2010, 12, 21, 19, 18, 49, 1, 355, 0], "published": "2010-12-21T19:18:49Z", "published_parsed": [2010, 12, 21, 19, 18, 49, 1, 355, 0], "title": "Scalable Inference of Customer Similarities from Interactions Data using\n  Dirichlet Processes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.2161%2C1012.5209%2C1012.1640%2C1012.0885%2C1012.4234%2C1012.3378%2C1012.3916%2C1012.2457%2C1012.1307%2C1012.5986%2C1012.5036%2C1012.3064%2C1012.1693%2C1012.3223%2C1012.2829%2C1012.5001%2C1012.4532%2C1012.4804%2C1012.0538%2C1012.1647%2C1012.5517%2C1012.5959%2C1012.0686%2C1012.0846%2C1012.4311%2C1012.1281%2C1012.4787%2C1012.0587%2C1012.2037%2C1012.4355%2C1012.0887%2C1012.2101%2C1012.0975%2C1012.1608%2C1012.3931%2C1012.4243%2C1012.0520%2C1012.0791%2C1012.2808%2C1012.1231%2C1012.4769%2C1012.2868%2C1012.4158%2C1012.4679%2C1012.3723%2C1012.4924%2C1012.3169%2C1012.3782%2C1012.2046%2C1012.3079%2C1012.5163%2C1012.2424%2C1012.5907%2C1012.2167%2C1012.3450%2C1012.3546%2C1012.2840%2C1012.1766%2C1012.1137%2C1012.5269%2C1012.2708%2C1012.3415%2C1012.3650%2C1012.5420%2C1012.0343%2C1012.2773%2C1012.1478%2C1012.1863%2C1012.1461%2C1012.2765%2C1012.5595%2C1012.4214%2C1012.0563%2C1012.3952%2C1012.4919%2C1012.4593%2C1012.0788%2C1012.0517%2C1012.3201%2C1012.2026%2C1012.3733%2C1012.3126%2C1012.4589%2C1012.5286%2C1012.1420%2C1012.4293%2C1012.2678%2C1012.2619%2C1012.5927%2C1012.0522%2C1012.5974%2C1012.0996%2C1012.0436%2C1012.1132%2C1012.6007%2C1012.4504%2C1012.5591%2C1012.2376%2C1012.0825%2C1012.5103%2C1012.0990&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Scalable Inference of Customer Similarities from Interactions Data using\n  Dirichlet Processes"}, "summary": "Under the sociological theory of homophily, people who are similar to one\nanother are more likely to interact with one another. Marketers often have\naccess to data on interactions among customers from which, with homophily as a\nguiding principle, inferences could be made about the underlying similarities.\nHowever, larger networks face a quadratic explosion in the number of potential\ninteractions that need to be modeled. This scalability problem renders\nprobability models of social interactions computationally infeasible for all\nbut the smallest networks. In this paper we develop a probabilistic framework\nfor modeling customer interactions that is both grounded in the theory of\nhomophily, and is flexible enough to account for random variation in who\ninteracts with whom. In particular, we present a novel Bayesian nonparametric\napproach, using Dirichlet processes, to moderate the scalability problems that\nmarketing researchers encounter when working with networked data. We find that\nthis framework is a powerful way to draw insights into latent similarities of\ncustomers, and we discuss how marketers can apply these insights to\nsegmentation and targeting activities.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.2161%2C1012.5209%2C1012.1640%2C1012.0885%2C1012.4234%2C1012.3378%2C1012.3916%2C1012.2457%2C1012.1307%2C1012.5986%2C1012.5036%2C1012.3064%2C1012.1693%2C1012.3223%2C1012.2829%2C1012.5001%2C1012.4532%2C1012.4804%2C1012.0538%2C1012.1647%2C1012.5517%2C1012.5959%2C1012.0686%2C1012.0846%2C1012.4311%2C1012.1281%2C1012.4787%2C1012.0587%2C1012.2037%2C1012.4355%2C1012.0887%2C1012.2101%2C1012.0975%2C1012.1608%2C1012.3931%2C1012.4243%2C1012.0520%2C1012.0791%2C1012.2808%2C1012.1231%2C1012.4769%2C1012.2868%2C1012.4158%2C1012.4679%2C1012.3723%2C1012.4924%2C1012.3169%2C1012.3782%2C1012.2046%2C1012.3079%2C1012.5163%2C1012.2424%2C1012.5907%2C1012.2167%2C1012.3450%2C1012.3546%2C1012.2840%2C1012.1766%2C1012.1137%2C1012.5269%2C1012.2708%2C1012.3415%2C1012.3650%2C1012.5420%2C1012.0343%2C1012.2773%2C1012.1478%2C1012.1863%2C1012.1461%2C1012.2765%2C1012.5595%2C1012.4214%2C1012.0563%2C1012.3952%2C1012.4919%2C1012.4593%2C1012.0788%2C1012.0517%2C1012.3201%2C1012.2026%2C1012.3733%2C1012.3126%2C1012.4589%2C1012.5286%2C1012.1420%2C1012.4293%2C1012.2678%2C1012.2619%2C1012.5927%2C1012.0522%2C1012.5974%2C1012.0996%2C1012.0436%2C1012.1132%2C1012.6007%2C1012.4504%2C1012.5591%2C1012.2376%2C1012.0825%2C1012.5103%2C1012.0990&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Under the sociological theory of homophily, people who are similar to one\nanother are more likely to interact with one another. Marketers often have\naccess to data on interactions among customers from which, with homophily as a\nguiding principle, inferences could be made about the underlying similarities.\nHowever, larger networks face a quadratic explosion in the number of potential\ninteractions that need to be modeled. This scalability problem renders\nprobability models of social interactions computationally infeasible for all\nbut the smallest networks. In this paper we develop a probabilistic framework\nfor modeling customer interactions that is both grounded in the theory of\nhomophily, and is flexible enough to account for random variation in who\ninteracts with whom. In particular, we present a novel Bayesian nonparametric\napproach, using Dirichlet processes, to moderate the scalability problems that\nmarketing researchers encounter when working with networked data. We find that\nthis framework is a powerful way to draw insights into latent similarities of\ncustomers, and we discuss how marketers can apply these insights to\nsegmentation and targeting activities."}, "authors": ["Michael Braun", "Andr\u00e9 Bonfrer"], "author_detail": {"name": "Andr\u00e9 Bonfrer"}, "author": "Andr\u00e9 Bonfrer", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1287/mksc.1110.0640", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1012.4769v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1012.4769v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1012.4769v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1012.4769v1", "arxiv_comment": null, "journal_reference": "Marketing Science 30:3 513-531 2011", "doi": "10.1287/mksc.1110.0640", "fulltext": "arXiv:1012.4769v1 [stat.AP] 21 Dec 2010\n\nScalable Inference of Customer Similarities from Interactions\nData using Dirichlet Processes\nMichael Braun\nMIT Sloan School of Management\nMassachusetts Institute of Technology\nCambridge, MA 02139\nbraunm@mit.edu\n\nAndr\u00e9 Bonfrer\nCollege of Business and Economics\nAustralian National University\nCanberra ACT 0200\nandre.bonfrer@anu.edu.au \u2217\n\nDecember 21, 2010\n\nAbstract\nUnder the sociological theory of homophily, people who are similar to one another are more\nlikely to interact with one another. Marketers often have access to data on interactions among\ncustomers from which, with homophily as a guiding principle, inferences could be made about\nthe underlying similarities. However, larger networks face a quadratic explosion in the number\nof potential interactions that need to be modeled. This scalability problem renders probability models of social interactions computationally infeasible for all but the smallest networks.\nIn this paper we develop a probabilistic framework for modeling customer interactions that\nis both grounded in the theory of homophily, and is flexible enough to account for random\nvariation in who interacts with whom. In particular, we present a novel Bayesian nonparametric approach, using Dirichlet processes, to moderate the scalability problems that marketing\nresearchers encounter when working with networked data. We find that this framework is a\npowerful way to draw insights into latent similarities of customers, and we discuss how marketers can apply these insights to segmentation and targeting activities.\n\n\u2217 The\n\nauthors thank David Dahl, Daria Dzyabura , Pete Fader, Jacob Goldenberg, John Hauser, Barak Libai, Jon\nMcAuliffe, Carl Mela, Adrian Raftery, David Schweidel, and Romain Thibaux for useful suggestions and helpful comments on previous versions of this paper, as well as Rico Bumbaca and Alex Riegler for research assistance, and Jeongwen Chiang and China Mobile for providing the dataset.\n\n\fIntroduction\nMarketers have long been interested in the notion that interactions among customers will affect\nbehavior. For example, knowledge of how customers relate to one another improves our understanding on how preferences are formed (Reingen et al. 1984), how preferences are correlated\nwithin groups (Witt and Bruce 1972; Park and Lessig 1977; Ford and Ellis 1980; Bearden and Etzel 1982), or how useful referrals are for marketers when developing new markets (Reingen and\nKernan 1986). Connections among customers are opportunities for preference influence (e.g. contagion in diffusion, Bass 1969). Marketers can leverage \"word of mouth\" to amplify the efficacy\nof their communication campaigns (Goldenberg et al. 2001; Nam et al. 2007; Iyengar et al. 2008;\nGodes and Mayzlin 2009). Incorporating network information into marketing models has also\nbeen shown to improve forecasts of both new product adoption (Hill et al. 2006) and customer\nchurn (Dasgupta et al. 2008).\nSimilar customers are more likely to interact with one another, so given the need for marketers\nto find efficient ways to attract and cultivate customers, there exists vast opportunity in leveraging interactions data to infer similarity and connect this to marketing behavior (e.g., Yang and\nAllenby 2003; Bell and Song 2007; Nam et al. 2007). This link between similarity and interactions\nis the sociological theory of homophily (Akerlof 1997; Blau 1977; Lazarsfeld and Merton 1954) and\nis the basis for many marketing studies that examine or accommodate interactions among customers (e.g. Gatignon and Robertson 1985; Brown and Reingen 1987; Choi et al. 2010). Put simply,\nhomophily implies that customers who are similar to one another are more likely to interact with\none another, and share information and influence, than customers who are not. There is a substantial volume of literature that links similarities to interactions (see McPherson et al. 2001, for\na review), but interactions and similarities are not the same thing. We consider interactions to be\nthe \"data\" that records some observable action between two individuals, while similarities form a\nlatent, unobserved construct (though possibly correlated with other observed measurements) that\ndetermines which individuals are more likely to interact with others. In this paper, we present an\nillustrative yet parsimonious model, grounded in the theory of homophily, that allows marketers\nto infer latent similarities from observed interactions. The idea is to develop a probability model\nthat uses interactions data to infer latent similarities, and generates output that can help marketers\n\n2\n\n\fbetter understand why customers interact with whom they do, or why they behave the way they\ndo, in terms that are useful to marketers.\nWe build on a class of probability models known as latent space models (Hoff et al. 2002;\nHandcock et al. 2007). The fundamental idea behind latent space models is that each individual\nis characterized as occupying some unobserved point on a multi-dimensional space. When estimated on relationship data (e.g. a list of self-reported friendships, as in Reingen et al. 1984; Brown\nand Reingen 1987, or working relationships, as in Iyengar et al. 2008), the distance among points in\nlatent space determines the probabilities for the incidence of these relationships.1 What is becoming increasingly available to marketers, however, are clean, observational data on the interactions\namong customers, such as phone call records or online social networking transactions, but with\nno observed information about the content of the interaction (who these people are and what they\ntalk about), or the nature of the relationship between these individuals (what it is about these two\nparticular people that generates an interaction between them).\nWhen latent space models are estimated on interactions data, we can interpret the distance\namong points as relative similarity. Homophily gives us the theoretical foundation on which we\ncan make this claim. The managerial usefulness of estimating latent space models on interactions\ndata comes from identifying and inferring these similarities. Sometimes, such as our application in\ntelecommunication services, interactions generate revenue directly. There are many examples, like\nthose mentioned in the first paragraph, where marketers deliberately target customers who will\ncontact, and hopefully influence, others. But in other cases, the marketing activities themselves\nmight have nothing at all to do with \"following network links,\" or generating \"word of mouth.\"\nKnowing how similar customers are to one another is of direct relevance to marketing practitioners because it forms the basis of segmentation and targeting across a heterogeneous population.\nOnce we have inferences about relative similarities of customers in hand (through posterior distributions of latent distances), we can segment and target customers accordingly. Ordinarily, this\nsegmentation is done based on observed characteristics of individuals. Very little attention has\nbeen paid to how marketers might be able exploit the information contained in interactions data\nfor traditional, non-networked marketing tactics, like deciding in which publications (online or\n1 Latent space methods are, of course, not limited to examining social network data, and could be used to model similarities between units in two distinct groups (Bradlow and Schmittlein 2000), or to model the difference in knowledge\nby individuals (van Alstyne and Brynjolfsson 2005). Further applications are discussed in Toivonen et al. (2009).\n\n3\n\n\fotherwise) to advertise. Indeed, the company that uses interactions data for segmentation and\ntargeting (e.g. an online retailer) does not necessarily have to be the same company that collects it\n(e.g., the cell phone provider).\nOne reason modelers have not been able to apply latent space models to marketing data in\na general sense is that it can be a daunting computational challenge. One of the key tenets of\nprobability modeling is that we need to take all data into account, including pairs of individuals for whom we do not observe any interactions at all (the \"zeros\" in the data offer valuable\ninformation about relative similarities). Thus, there has been a formidable obstacle to using probability models for larger observational network datasets. A dataset with N individuals involves\n\n( N2 ) dyads (the binomial coefficient ( Nx ) is defined as\n\nN!\n).\nx!( N \u2212 x )!\n\nFor the exemplar dataset that we\n\nuse in this paper, there are 11,426,590 sets of dyad-specific parameters that we need to consider,\nand this is for a dataset of only 4,781 individuals. Unless we want to break the interdependencies\namong dyads, ignore unobserved heterogeneity, or make other assumptions that are similarly restrictive, we need to compute all of these ( N2 ) dyad-specific likelihoods, and the same number of\ndyad-specific parameters, at each iteration of our estimation algorithm. The problem with scale\nmakes probability models of social interactions computationally intractable for all but the smallest\ndatasets.\nThe modeling challenge is therefore to reveal similarities in heterogeneous characteristics from\ncustomers' interaction data, in a scalable and interpretable way. We accomplish this by applying\na Bayesian nonparametric prior, the Dirichlet process (DP), as the distribution of locations on the\nlatent space. The DP is essentially a distribution over distributions (as opposed to over scalars or\nvectors), and for our purposes, its most salient characteristic is that each realized distribution is\ndiscrete. Consequently, individuals in the network are clustered on common locations on the latent\nspace. So if this discrete distribution has k mass points, there are only (2k ) + 1 distinct distances on\nthe latent space (the +1 comes from the zero distance between two individuals at the same latent\ncoordinate). Since k must be smaller than N, there are substantially fewer distinct likelihoods to\ncompute and parameters to estimate.\nIn this research, we show how marketers could use latent space models to segment customers\nbased on posterior inferences of latent similarities, using this more efficient Bayesian nonparametric approach. An output of our algorithm is a posterior estimate of the latent space that is inferred\n4\n\n\ffrom the interactions data. Our probabilistic approach to modeling these data allows for the fact\nthat similar individuals may not interact, even though they may have similar characteristics and\ntravel in the same social circles. Also, we recognize that while interactions typically occur among\nsimilar customers, there is also the possibility that dissimilar customers (who may have different\npurchase patterns and preferences) may interact at some time. To demonstrate the power and utility of this approach to modeling interactions data, we apply it to a dataset of observed interactions\nfrom a cellular communication network. We propose a probability specification for this particular\ndataset, in which the incidence and rates of interactions are functions of distances in latent space.\nWe validate the approach in two ways: by showing that adding the latent space structure to the\nprobability model improves the fit of the model, with respect to several metrics commonly used\nin the social networking literature; and by showing that the latent space model can distinguish\namong pairs of individuals for whom the observed number of interactions are all identically zero\nduring a calibration period, in terms of how well the model predicts which of those pairs will\neventually interact in a future holdout period. These tests demonstrate that failing to account for\nthe unobserved heterogeneous interdependencies among individuals leads to a model that simply\ndoes not represent the observed patterns in interactions.\nWe then assess the computational improvements and scalability issues surrounding our Bayesian\nnonparametric approach, and the managerial insights that one can get from estimates of the latent\nspace itself. By using a graphical representation of the latent space, we show how marketers can\naugment network based practices that follow observed interaction paths, with tactics that segment\nand target customers according to inferred latent similarities. The data that are available to us do\nnot let us offer hard evidence of a correlation between similarities and purchase preferences but,\ngiven the findings in the marketing literature that show the importance of similarities and interactions in customer behavior, it is reasonable to expect that marketing mix efforts benefit from being\nable to distinguish interactions among similar customers from interactions among dissimilar customers. The computational improvements from using a DP prior for the latent space make these\ninferences attainable for the datasets that marketers typically encounter.\n\n5\n\n\f1\n1.1\n\nGeneral model formulation\nIntuitive description\n\nIn a probability model of network data, each dyad in the network generates some vector of data,\nwhich can represent a wide variety of behavior. Examples include binary indicators of relationships, counts of transactions (among customers), times between interactions, or combinations\nthereof. However simple or complicated the data are, they should be treated as some output\nof a stochastic process that is governed by dyad-specific parameters (and possibly some additional population-level parameters). Data that is generated from a network of customers differs\nfrom individually-generated data, such as household purchase data, in that we can no longer assume that the data-generating processes are independent across dyads. For example, if we were\nto observe telephone calls between members of a dyad, the rate at which A calls B, and B calls\nC, can provide information about how often A calls C. However, we do assume that the dyadlevel processes are conditionally independent, so the only correlation among dyads is what occurs\nbecause of similarities in parameters. This means that even though frequencies of phone calls\nmight be dependent across dyads, the specific times at which those calls ultimately take place are\nindependent, conditional on the rate of interactions.\nWe determine dyad-level parameters so that similar individuals will have higher incidence of\ninteraction than dissimilar individuals. The characteristics upon which this similarity is based are\nlikely unobservable by the researcher. Therefore, we represent unobserved, exogenous characteristics of the individual (and thus, the individual himself), as a D-dimensional vector on some\nlatent space (Hoff et al. 2002; Handcock et al. 2007; Bradlow and Schmittlein 2000; van Alstyne and\nBrynjolfsson 2005). Similarity between two individuals is measured by the distance between their\nlatent coordinates across this latent space, and we can express the rates or probabilities of interaction between two people as a decreasing function of the latent distance between them. Note that\nthese distances and locations do not directly represent physical or geographic locations in any way\n(although they may, of course, be incidentally correlated with them). Instead, they are individuallevel parameters to be estimated, based on observed patterns of interaction. For the purposes of\nthis article, we treat the location of each latent coordinate as persistent and stationary. So even\nthough interactions among people may appear and disappear periodically (a non-stationary ob6\n\n\fserved phenomenon that Kossinets and Watts (2006) describe as evolving), the underlying rates\nand probabilities of these incidences remain the same. Thus, our stationary model can still capture non-stationary behavior in the observed data. Also, we want to emphasize that a latent space\nmodel is an abstraction of reality, and we caution researchers not to place too much concrete meaning on any one dimension. It is the relative distances among individual latent coordinates, and\nnot the absolute positioning in the latent space, that matter.\n\n1.2\n\nFormal model\n\nA more formal definition of the general model is as follows. Let yij be a vector of observed data\n\u0001\nthat is attributable to the dyad of person i and person j, and let f yij |\u03b8ij be the likelihood of\nobserving yij , given the dyad-specific parameter vector \u03b8ij . Next, let \u03b8ij be heterogeneous across\n\u0001\ndyads, with each \u03b8ij drawn randomly from a dyad-specific prior distribution g \u03b8ij |\u03c6ij . A model\nin which \u03c6ij is common across all dyads, or itself distributed independently (drawn from its own\nmixing distribution), would imply cross-dyad independence of \u03b8ij , which may not make sense\nin a network setting. To incorporate some network-based dependence in the distribution of \u03b8ij ,\nwe instill a pattern of heterogeneity of \u03c6ij that allows for a useful, intuitive interpretation of the\nsimilarities. Thus, there are two sources of heterogeneity that generate \u03b8ij : independent dyad-level\n\u0001\nvariation from g \u03b8ij |\u03c6ij , and network-induced interdependence in the distribution of \u03c6ij .\nBefore explaining how we model heterogeneity in \u03c6ij , let us shift our focus from the level of\nthe dyad to the level of the individual. Each dyad is made up of two individuals, each of whom\nhas its own, mostly unobserved traits and characteristics. Let zi be a D \u2212dimensional vector that\nis associated with person i, and let z be the collection of all N of these vectors. Since each zi\nis unobserved, we call it a \"latent coordinate,\" and the D-dimensional space on which it lies a\n\"latent space,\" as in Hoff et al. (2002) and Handcock et al. (2007). Even if the N vectors in z are\ndistributed independently on the latent space, the distances between every pair of zi (the \"latent\ndistances\") are not. By expressing \u03c6ij as a monotonic function of the distance between zi and\nz j , we induce dependency among all the \u03c6ij and, in turn, all the \u03b8ij . As an example, suppose\nthat \u03b8ij represents a rate of contact between i and j, and the distribution of \u03b8ij depends positively\n\u0001\non \u03c6ij (for example, the mean of g \u03b8ij |\u03c6ij increases with \u03c6ij ). We determine \u03c6ij by evaluating a\n\n7\n\n\fmonotonically decreasing function of the latent distance, so as i and j are less similar (the distance\nbetween zi and z j goes up), the rate of interaction between i and j goes down. But we never need\nto estimate \u03b8ij or \u03c6ij directly. We need only to estimate the locations of zi for all N people to get the\nvalues of \u03c6ij for all ( N2 ) dyads.\n\n1.3\n\nMixtures of Dirichlet processes: what they are, and how to use them to model the\nlatent space\n\nEven if we model \u03c6ij as a function of the latent distance among the zi 's, we still have the issue\nthat there are many zi 's, and thus a large number of latent distances, to model. This means that at\neach iteration of our estimation algorithm, we need to compute ( N2 ) values of \u03c6ij , and ( N2 ) corresponding data likelihoods. When N is small, scalability becomes less of a problem, and one could\nuse the original parametric formulation of the latent space model. But as N becomes even moderately large, estimating the latent coordinates becomes computationally infeasible. We reduce\nthe number of distinct values of zi by using a discrete distribution, H (zi |*) for the distribution of\nzi on the latent space. If this discrete distribution has k mass points, then there are only (2k ) + 1\ndistinct latent distances. For a given network size, a larger difference between k and N leads to a\ngreater computational savings by having fewer distinct values of \u03c6ij to consider. To avoid having\n\u0001\n\u0001\nto estimate each \u03b8ij directly, we choose f yij |\u03b8ij and g \u03b8ij |\u03c6ij such that we can integrate over \u03b8ij\n\u0001\nanalytically, and express the marginal distribution f yij |\u03c6ij in closed form. However, we do not\nwant to prespecify the functional form of H, because we don't know for certain what it is, nor do\nwe want to prespecify k, since we do not know what the \"correct\" number of mass points for H is.\nOur approach is to use a mixture of Dirichlet processes as a Bayesian nonparametric prior distribution for the points on the latent space. Although the properties of Dirichlet processes (DP)\nhave been known for a while (back to Ferguson 1973), they are still relatively new to marketing.\nThe few examples include Ansari and Mela (2003) (as a Bayesian alternative to collaborative filtering), Kim et al. (2004) (identifying clusters of customers in discrete choice models), Wedel and\nZhang (2004) (analyzing brand competition across subcategories) and Braun et al. (2006) (estimating thresholds of claiming behavior for home owners' insurance). In our context, a Dirichlet\nprocess is a probability distribution over distributions (as opposed to a distribution over a scalar\n\n8\n\n\for vector). Accordingly, a single draw from a Dirichlet process is itself a random distribution,\nfrom which we can draw samples of a variable of interest. An important feature for our context\nis that each realization of a DP is a discrete distribution, with its support having a finite number of\nmass points (k in the previous paragraph), so the DP can be thought of as a prior distribution on\ndiscrete distributions.2\nThere are, of course, many ways to model discrete points on a space; a traditional latent class\nmodel with a prespecified number of locations is an extreme example. What makes the DP more\nuseful in this context is that it has a parsimonious representation, with straightforward sampling\nproperties, and does not require a prespecification of the number of mass points. In our latent\nspace framework, we let H be a realization from DP( H0 , \u03b1), and then have each zi be a draw from\nH. The first parameter, H0 , is itself a probability distribution, and is the \"mean\" of the distributions that the DP generates. A scalar \u03b1 controls the variance of the realizations of the DP around\nH0 . This variance is low when \u03b1 is high, so for high \u03b1, realizations from DP( H0 , \u03b1) will look a lot\nlike the distribution function of H0 . This concentration of the DP towards H0 results from a DP\nthat generates a discrete distribution with a lot of mass points (a high k). When \u03b1 is low, realizations from DP( H0 , \u03b1) look much less like H0 (high variance), because this DP generates discrete\ndistributions with fewer mass points. So \u03b1 plays an important role in determining just how discrete (i.e. value of k), or clustered, a DP-generated distribution really is. Reasonable choices for H0\nare those distributions for zi that one might use in a purely parametric model (note that H0 could\nhave parameters of its own, with their own priors, that need to be estimated.) Depending on the\napplication, one can either put a prior on \u03b1 or set it directly.\nGiven H0 and \u03b1 we need to know how to simulate H from DP( H0 , \u03b1) and then each zi from H.\nSince H is nonparametric, even though we know it was generated by the DP( H0 , \u03b1), the posterior\ndistribution of any new zi depends on all the other z\u2212i . Consequently, there is no obvious way to\ndraw a zi from H directly. The \"trick\" is to integrate out H analytically, and treat zi as if it were\ndrawn from this marginal distribution, a mixture of Dirichlet processes (MDP, see Antoniak 1974).\n2 The\n\nformal definition of what makes a stochastic distribution a DP is a more technical issue. Essentially, the probabilities of certain events occurring must follow a Dirichlet distribution with parameters that depend on H0 and \u03b1).\nThere is an accessible and readable explanation in O'Hagan and Forster (2004, ch. 13).\n\n9\n\n\fThe probability of any one zi , given the empirical distribution ED (*) of all the other z\u2212i , is\nProb(zi |z\u2212i , H0 , \u03b1) =\n\n\u03b1H0 + ED (z\u2212i )\n\u03b1+N\u22121\n\n(1)\n\n(Blackwell and MacQueen 1973; Escobar 1994). So in the estimation algorithm, \u03b1 determines how\nlikely it is that any new draw of zi comes from one of the existing, distinct values already possessed\nby another individual in the dataset (if this is likely, then there are few mass points, with lots of\nclustering), or from the baseline distribution H0 , as a new value.\nTo illustrate how this works, Figure 1 shows simulations from an MDP when H0 is a univariate\nstandard normal distribution, for different values of \u03b1. In the figure, the heavy black line is the\nstandard normal cdf, and each colored line is a single realization from the MDP. We see that when\n\u03b1 is low, there are fewer mass points in each realization, and when \u03b1 is high, the higher number\nof mass points allows the realizations to approximate the normal cdf. In Figure 1b, for each \u03b1\nwe present histograms from draws of a single realization of the MDP (so these are draws from a\ndistribution that the MDP generated). Again, we see fewer distinct clusters (low k) when \u03b1 is low,\nmore clusters when \u03b1 is high. In our network model, we are dealing with more dimensions and a\nricher specification of H0 , but the basic idea remains the same.\nHow we select H0 and \u03b1, and the priors we place on them, is described in more detail in\nAppendix B. Selection of an appropriate distribution for H0 requires that we introduce some identifying restrictions on the location vectors (zi ). The concern is that we cannot simultaneously and\nuniquely identify both the scale of the latent space, and the parameters of the distance function\ndetermining \u03c6ij . To handle this problem, we constrain the prior distribution of zi so the mean\ndistance of any zi from the origin is one. However, we need to do this without introducing too\nmuch \"incorrect\" prior information. For example, a simple choice for H0 could be a standard multivariate normal distribution; setting the mean at the origin and the variance at one addresses the\ntranslation and scale identification issues. The problem with defining H0 as a multivariate normal\nis that it implies that our prior on the distribution of zi has a mode at the origin. This prior turns out\nto be informative, as it generates artifactual clusters of individuals around the origin in the posterior. An alternative specification for H0 could be a bounded uniform distribution (so the mean\ndistance from the origin remains one), but that would constrain all zi to be inside a hypersphere,\n\n10\n\n\f3\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\n\n1\n\n2\n\n0.15\n0.10\n0.05\n0.00\n\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\n0.8\n\n\u22122\n\n0\n\n2\n\n\u22122\n\n0\n\n2\n\n4\n\nalpha : 100\n\n0.6\n0.4\n\n\u22124\n\nalpha : 20\n\n0.020\n\nalpha : 100\n\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\nalpha : 5\n\n0.010\n\nalpha : 20\n\nalpha : 0.4\n\n0.2\n0.000\n\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\n\n2\n\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\np\n\n0.2\n\n1\n\nmass\n\n0.4\n\n0\n\nalpha : 5\n\n0.10\n\n0.6\n\n\u22121\n\n0.05\n\n0.8\n\n\u22122\n\n0.00\n\n\u22123\n\nalpha : 0.4\n\n3\n\n\u22124\n\nx\n\n\u22122\n\n0\n\n2\n\n\u22122\n\n0\n\n2\n\nx\n\n(a) Sample realizations\n\n(b) Histograms of draws from a single realization\n\nFigure 1: Illustration of realizations from a mixture of Dirichlet process with H0 being a standard\nnormal. At left, the black line is the cdf of the H0 , and each colored line represents a single realization of a MDP. Each panel corresponds to a different value of \u03b1. At right, each panel is a histogram\nof draws from a single realization.\neffectively placing an upper bound on the latent distance between customers. This, too, seems\nlike an unreasonable expression of prior information. Our solution involves using spherical coordinates for zi , consisting of two components: a radius representing the distance from the origin,\nand the location on the surface of a hypersphere that has that radius. We show in Appendix B that\nH0 can be factored into priors for these two components from which it is straightforward to draw\nsamples.\nThis prior on zi , combined with the data likelihood, leads to conditional posterior distributions\nthat are easily incorporated into Gibbs samplers. Escobar (1994) and Escobar and West (1998)\ndescribe some of the theory and derivations behind this, while Neal (2000) details step-by-step\ninstructions on how to add MDPs to Gibbs samplers for both conjugate and nonconjugate models.3 Thus, MDPs allow marketing modelers to relax many of their distributional assumptions by\nadding only one additional step to the parametric Gibbs sampling algorithm. We give details of\nour estimation algorithm in Appendix C. Our exploitation of the discreteness property of Dirichlet\n3 This way of expressing the MDP (and the approach we took in our estimation is known as the \"Polya urn\" representation. There is another, equally useful approach known as the \"stick-breaking\" representation (Sethuraman 1994)\nthat one can also use to build conditional posterior distributions for Gibbs samplers Ishwaran and James (2001).\n\n11\n\n\fprocesses also lets us reduce the computational burden substantially, as we demonstrate in Section\n3.\n\n2\n\nExample: telephone calls\n\nWe now turn to a specific application of our model, using a dataset provided by Chongqing Mobile, a subsidiary of China Mobile, the largest cellular phone operator in China. Cellular phone\nnetworks have been reported to be highly representative of self-reported friendships (Eagle et al.\n2009), making such data ideal for studies of network based interdependencies among customers.\nThe data consist of contact record information (for phone calls and SMS messages) for a panel\nof 4, 781 residents of Chongqing who are members of the \"silver tier\", \"gold tier\" or \"diamond\ntier\" of the company's preferred customer program. Each record contains the identifiers for both\nparties in the contact, and the date of when the contact takes place. For the purposes of this example, we ignore contacts with people outside this N-person network.4 The observed geodesic\ndistance is finite for all dyads (i..e, all customers are connected to every other customer in a finite\nnumber of steps). We divide the observation period into a six-month calibration period and a\nsix-month holdout period. Descriptive statistics for this dataset are summarized in Table 1. Of\nthe 18, 078 nonempty dyads in the dataset, only 7, 559 appear in both the calibration and holdout\nsamples. 5, 058 dyads are nonempty in calibration, but empty in holdout, and 5, 461 are empty in\ncalibration, but nonempty in holdout.\nOne way to describe the structure of the observed network is to compare it to the \"small world\"\nnetworks described in Watts and Strogatz (1998) and Watts (1999). Generally speaking, a small\nworld network is one in which everyone in the network is connected to everyone else through\na relatively small number of intermediaries (i.e., a low mean geodesic distance), and a relatively\nlarge number of common friends who are connected among themselves (i.e., a high clustering\ncoefficient). We can assess the extent to which a network is \"small world\" comparing the mean\ngeodesic distances and clustering coefficients to those that we would expect to see from a network\n4 Our\n\nintent in using this dataset is to demonstrate the effectiveness of our estimation method, and to illustrate\nsome of the issues that arise when modeling dyadic data. Therefore, we treat our dataset as an entire population of\nindividuals, and not as a random sample; our interest is only in contacts made among individuals in this population. If\nwe were to generalize parameter estimates and predictions to a greater population, ignoring out-of-network calls could\ninfluence specific parameter estimates. There are an additional 209 silver, gold or diamond customers in the panel for\nwhom there were no observed calls to other silver, gold or diamond customers during the observation period.\n\n12\n\n\fWeeks\nCustomers\nNon-empty dyads\nProportion of empty dyads\nClustering coefficient\nMean (s.d.) degree distribution\nMean (s.d.) geodesic distance\nMean (s.d.) calls per non-empty dyad\nMean (s.d.) shared friends in non-empty dyad\n\nCalibration\n26\n4,781\n12,617\n0.9989\n0.128\n5.3 (4.9)\n5.5 (1.4)\n7.5 (18.7)\n3.0 (2.7)\n\nHoldout\n26\n4,781\n13,020\n0.9989\n0.127\n5.4 (5.3)\n5.3 (1.3)\n7.6 (18.7)\n3.3 (2.8)\n\nFull\n52\n4,781\n18,078\n0.9984\n0.127\n7.6 (6.7)\n4.7 (1.1)\n15.1 (36.0)\n5.7 (2.8)\n\nTable 1: Descriptive statistics of China Mobile dataset.\nin which connections are determined at random for the same number of people (4,781) and average number of \"friends\" per person (7.6). Using the asymptotic approximations in Watts (1999),\nthe mean geodesic distance we would expect from a random graph of this size is about 4.2, and\nthe expected clustering coefficient is about 0.002. In the observed Chongqing Mobile network\nwe observe quite a bit more clustering than we expect to see from a random graph, while the\nmean geodesic path is slightly longer what we would expect. One possible reason that our mean\ngeodesic distance is not smaller is that we could have a large number of small clusters, and not\nall small clusters are connected to each other. In fact, our estimates of k (illustrated in Figure 4)\nwill bear this out. We also note that our network would not qualify as a \"scale free\" network, in\nthat the degree distribution clearly does not follow a power law-type distribution (we show the\nobserved degree distribution in Figure 2).\n\n2.1\n\nModel specifics\n\nUsing the notation introduced in Section 1, yij is the vector of intercontact times, ending with\nthe survival time (the duration between the last observed contact and the end of the observation\nperiod). If there are no observed contacts in the dyad, yij is the length of the observation period,\nand we call that dyad \"empty.\" If there are observed calls, the dyad is \"non-empty.\" The definition\n\u0001\nof f yij |\u03b8ij follows the logic of the \"exponential never-triers\" model in (Fader et al. 2003), which\nin turn draws from the \"hard-core never-buyers\" model in Morrison and Schmittlein (1981) and\nMorrison and Schmittlein (1988). First, there is a probability pij that a dyad will remain forever\nempty, no matter how long we wait. We call dyads like this \"closed\". Next, for dyads that are\n\n13\n\n\f\"open,\" (with probability 1 \u2212 pij ), intercontact times follow an exponential distribution with rate\n\u03bbij . To link these specifics with our general model, \u03b8ij = { pij , \u03bbij }. Note that there are two ways\nwe could observe an empty dyad. The dyad is either closed, or it is open, but with a contact rate\nthat is sufficiently low that we just happened to not observe any contacts during the observation\nperiod.\nWhether the exponential distribution is appropriate for this dataset is ultimately an empirical\nquestion, but we choose it for four reasons. First, we do not need to make special provisions for\nleft-censoring because of the memorylessness property. Second, the number of contacts is a sufficient statistic for the individual elements in yij . We were able to exploit these two features of\nthe exponential distribution to gain computational savings without compromising the fundamental purpose of the research. Third, we did run the model on a much smaller dataset where f (*)\nis governed by a \"Weibull never-triers\" model, in order to allow for duration dependence, and\nfound that since the shape parameter of the Weibull was close to 1, it reduced to the exponential\ndistribution anyway. Finally, we chose the exponential distribution because it forms a conjugate\n\u0001\npair with our choice of g \u03b8ij |\u03c6ij , a gamma distribution for \u03bbij with dyad-specific mean \u03bcij and\ncommon variance v, and a degenerate distribution over pij , so that at this level of the hierarchy, pij\nis homogeneous for all dyads (we will add heterogeneity to pij later through the latent space). The\nexponential-gamma pair lets us integrate over \u03b8ij analytically, further easing computational effort.\nThe vector \u03c6ij therefore contains three elements, pij , \u03bcij and v (pij is contained in both \u03b8ij and \u03c6ij .)\nTo evaluate whether latent space is worth adding to a model of interactions data, we estimated\nthe model with three different definitions of the elements of \u03c6ij . For a \"Baseline\" model, we let\n\u03c6ij = \u03c6, a common value for all dyads (note that we still maintain dyad-level heterogeneity in\n\u03b8, but it does not appear explicitly in the data likelihood). For a second model, HMCR (for \"Homogeneous Mean Contact Rate\"), \u03bcij and v remain homogeneous across dyads, but pij is now\ndetermined by the distribution on the latent space. Specifically, we define\n\u03b2\n\nlogit pij = \u03b2 1p \u2212 \u03b2 2p dij3p\n\n(2)\n\nwhere the \u03b2's are coefficients to be estimated, and dij is the latent distance between zi and z j . For a\nthird model, named \"Full,\" pij retains the same definition as in Equation 2, except that \u03bcij is now\n\n14\n\n\fheterogeneous across dyads, defined as\n\u03b2\n\nlog \u03bcij = \u03b2 1\u03bc \u2212 \u03b2 2\u03bc dij3\u03bc\n\n(3)\n\nEquations (2) and (3) allow the respective relationships to latent distance to be concave, linear\nor convex. The parameters \u03b2 2p , \u03b2 3p , \u03b2 2\u03bc , and \u03b2 3\u03bc are constrained to be nonnegative, because as\nlatent distance increases, the probability of contact, and the rate of contact, should decrease. We\nselected Euclidean distance as our distance measure, after experimenting with others that did not\nperform as well (van Alstyne and Brynjolfsson 2005).5 Another candidate for this distance metric is the Mahabalonis distance (as used in Bradlow and Schmittlein 2000), which weights some\ndimensions more than others in the computation of the distance among individuals. However,\nthe non-parametric nature of the estimated latent space means the dimensions are already differentially scaled. Also, the Euclidean distance is computationally more efficient. As with the\nparametric specification, the functions in Equations 2 and 3, and the distance measure, are subject\nto empirical testing and may not be appropriate in all contexts.\n\n2.2\n\nAssessing contribution of the latent space\n\nSo far, we have assumed that parameter interdependence is an important characteristic of a model\nof customer interactions. However, one could falsify this claim by showing that models in which\ndyad-level parameters are independent fit no worse than models that incorporate a latent space.\nWe ran our algorithm with latent spaces of different dimensionality and, based on estimates of log\nmarginal likelihoods, we decided that the parsimonious choice of D = 2 is most appropriate (see\nAppendix A). As evidence that the latent space models do better than independent models, we\nevaluate the contribution of latent space based on both posterior predictive checks (PPCs) and on\nforecasting interactions in empty dyads.\nPosterior predictive checks allow us to evaluate how well our model represents the datagenerating process (Rubin 1984; Gelman et al. 1996). Three of our PPC test statistics are the same as\nthose used by Hunter et al. (2008) to assess goodness-of-fit for social networking data: the degree\n5 Here,\n\nwe are talking about distance between two individuals' coordinates on the latent space. This concept of\ndistance is different from when we talk about geodesic distance, which is the smallest number of observed connections\nalong the shortest path between two individuals.\n\n15\n\n\fdistribution, the dyad-wise shared partner distribution, and the distribution of geodesic distances.\nWe also examine the histogram of the number of calls made within non-empty dyads, the density\nof the network, and the clustering coefficient for the network. All of our PPCs in this article are\nwith respect to the 26-week holdout sample. Figure 2 shows the results for the distributional PPCs,\nand Figure 3 shows the PPCs for the density and clustering coefficients. The x-axis in each panel\nis the count of individuals or dyads, and the y-axis is the log proportion of those individuals\nwith each count. The dark dots represent the log probabilities generated from the actual dataset,\nand the box-and-whisker plot represents the distribution of log probabilities across the simulated\ndatasets. Figure 3 shows the PPCs for the network density and clustering coefficients; the vertical\nline is the observed value.\nAt first glance, it might appear that all of the models replicate the actual datasets rather well.\nThe reason that even the Baseline model does as well as it does is that most of the value from\nposterior prediction comes from inferring whether a dyad is open or closed. Simply looking at\nwhether a dyad is empty or non-empty provides a lot of information about the likelihood of future\nemptiness, because non-empty dyads must be open. However, closer examination reveals that the\nBaseline model is not well calibrated at all. The \"actual\" dots lie far outside the whiskers for the\npredictive distributions for many of the counts. The two models that involve some kind of latent\nspace structure fit better on these test statistics. However, we do not see much difference between\nthe HMCR and Full models. This suggests that the value of the latent space is more in predicting\nthe potential existence of an interaction (whether the dyad is open or closed), than in predicting\nthe contact rate.\nIn addition to assessing model fit in aggregate, we also care about how well the model performs at the dyad level. Our approach here is to predict which of the dyads that are empty during\nthe calibration period become nonempty in the holdout period. Empty dyads all have the same\nobserved data pattern, so there is no obvious way to differentiate among them. We can, however,\nuse the latent space structure and a straightforward application of Bayes's Theorem to compute\nposterior distributions of unobserved parameters, and then use those probabilities to rank dyads\nin terms of those most likely to generate interactions during some future period of any duration\nwe want. To assess the predictive ability of a model, we first identify, individual by individual,\nthe top Q% lift (or the top q lift, where q = Q/100) most likely, previously uncontacted, individ16\n\n\fproportion of individuals \u2212 log scale\nproportion of dyads \u2212 log scale\n\nBaseline\n\u25cf\n\nHMCR\n\u25cf\n\n\u25cf\n\nFull\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n0.1\n\n\u25cf\n\u25cf\n\n\u25cf\n\n0.1\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n0.05\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n0.05\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n0.025\n\n0.025\n\u25cf\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n\u25cf\n\n12 0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n\u25cf\n\n12 0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\ndegree count\n\n(a) Degree Distribution\n\nBaseline\n\nHMCR\n\n\u25cf\n\nFull\n\n\u25cf\n\n\u25cf\n\n0.5\n\n0.5\n\n0.05\n\n0.05\n\n\u25cf\n\n\u25cf\n\n0.005\n\n5e\u221204\n\n\u25cf\n\u25cf\n\n5e\u221204\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\n5e\u221205\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n6\n\n8\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n5e\u221206\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n10\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\n4\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\n2\n\n5e\u221205\n\n\u25cf\n\n\u25cf\n\n5e\u221206\n\n0\n\n0.005\n\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\n12 0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12 0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\nnumber of shared partners\n\nproportion of dyads \u2212 log scale\n\n(b) Dyadwise Shared Partners Distribution\n\nBaseline\n0.5\n\n\u25cf\n\u25cf\n\nHMCR\n\u25cf\n\n\u25cf\n\n\u25cf\n\n0.5\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n0.05\n\nFull\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n0.05\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n0.005\n\n0.005\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n5e\u221204\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n5e\u221204\n\n\u25cf\n\n\u25cf\n\n5e\u221205\n\n5e\u221205\n\u25cf\n\n\u25cf\n\n\u25cf\n\n5e\u221206\n\n5e\u221206\n\u25cf\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n\u25cf\n\n12 0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n\u25cf\n\n12 0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\nnumber of dyads \u2212 log scale\n\nlength of geodesic path\n\n(c) Geodesic distance distribution\n\u25cf\n\n\u25cf\n\nBaseline\n\u25cf\n\n1000\n\n\u25cf\n\nHMCR\n\u25cf\n\n\u25cf\n\nFull\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf \u25cf \u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf \u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf \u25cf \u25cf\n\n\u25cf \u25cf \u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n15\n\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf \u25cf \u25cf\n\n\u25cf \u25cf \u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf \u25cf\n\n25\n\n5\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf \u25cf\n\n15\n\n\u25cf\n\n25\n\n10\n\n\u25cf\n\n\u25cf\n\nnumber of calls\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf \u25cf\n\u25cf \u25cf\n\n\u25cf\n\u25cf \u25cf\n\n5\n\n\u25cf\n\u25cf \u25cf \u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf \u25cf\n\u25cf\n\n25\n\n\u25cf \u25cf \u25cf\n\n\u25cf\n\u25cf \u25cf \u25cf\n\n\u25cf\n\n100\n\n\u25cf\n\u25cf \u25cf\n\n\u25cf \u25cf\n\n\u25cf\n\n\u25cf \u25cf \u25cf\n\n\u25cf\n\n15\n\n\u25cf\n\n\u25cf\n\u25cf \u25cf\n\n\u25cf \u25cf\n\n5\n\n\u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\n\u25cf\n\n\u25cf \u25cf\n\n10\n\n\u25cf\n\n\u25cf\n\u25cf\n\n100\n\n1000\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf \u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n(d) Number of calls\n\nFigure 2: Posterior predictive checks for holdout sample. For each sub-figure, observed data are\nrepresented using dots, and the posterior predictive distributions are represented by the \"boxand-whisker\" symbols.\n\n17\n\n\fFull\n\n0.00112 0.00114\n\n0.00112\n\n0.00114\n\n0.115\n\n0.120\n\n0.125\n\n0.00112 0.00114\n\n0.120\n\nFull\n\nClustering Coef\n\nHMCR\n\nClustering Coef\n\nBaseline\n\nClustering Coef\n\nNet Density\n\nHMCR\n\nNet Density\n\nNet Density\n\nBaseline\n\n0.125\n\n0.130\n\n0.120\n\n0.125\n\n0.130\n\nFigure 3: PPCs for the density and clustering coefficients.\nuals observed to contact during the holdout period6 . For a completely random or naive model,\nthe percentage of the top Q% most likely empty dyads to become non-empty should be equal to\nq. For any other model, if the value for the top Q% metric is greater than q, then the model provides some \"better-than-chance\" predictive value. The use of the Q% lift metric ensures that the\nmaximum of this value is always equal to one.\nTable 2 presents these lift metrics for different models and values of Q%, and different calibration/holdout samples. Results are presented for all three model variants, with D = 2 for the latent\nspace models. In addition, we present results for a \"Condition on Observed\" prediction rule, under which empty dyads are to remain empty in holdout, and non-empty dyads remain non-empty\nin holdout. The \"Geodesic Distance\" model ranks dyads according to their geodesic distances, as\nin Kossinets and Watts (2006) (we break ties in two different ways: randomly, or based on the\ntotal number of observed interactions along the path). The lift metrics suggest that both the Baseline model and the \"Condition on Observed\" rule do exactly as well as one would expect from\nrandom selection. This is because they both assumes that there is no network structure among\nindividuals in the dataset, and thus all empty dyads are considered to be identical. In contrast,\n6 For\n\nany individual i, the top q lift requires rank ordering all potential customers j 6= i based on the predicted\nprobability of interaction. For a holdout sample, the top q lift of this rank ordered list is equal to the proportion of\nthe top q customers for whom we observe interactions with i, divided by the proportion of total interactions made by\ncustomer i.\n\n18\n\n\fQ%=\nBaseline\nHMCR\nFull\nCondition on Observed\nGeodesic - random tiebreak\nGeodesic - # calls tiebreak\n\nDuration of calibration (holdout) period\n13 (39) weeks\n26 (26) weeks\n39 (13) weeks\n0.1\n0.2\n1.0\n0.1\n0.2\n1.0\n0.1\n0.2\n1.0\n.001 .002 .010 .001 .002 .010 .001 .002 .010\n.100 .112 .141 .133 .138 .178 .153 .154 .177\n.100 .109 .146 .132 .135 .157 .154 .159 .197\n.001 .002 .010 .001 .002 .010 .001 .002 .010\n.050 .088 .166 .036 .067 .149 .025 .051 .198\n.064 .110 .186 .056 .098 .190 .046 .083 .206\n\nTable 2: Percentage of the top Q% of the empty dyads (in calibration period) that are most likely to\nbecome nonempty during holdout period, that actually did become nonempty during the holdout\nperiod. Reported values are posterior means; credible intervals are removed for space and clarity.\nin the two latent space models, some dyads are more likely to contact each other than others. By\nsorting the empty dyads according to their posterior latent distances, we no longer assume that\nall empty dyads are the same. Thus, we can improve on dyad-level prediction dramatically. We\ndo not, however, see any substantive differences between the HMCR and Full models, suggesting\nthat, in this application, all of the action is on the open/closed probability and not on the contact\nrates. Nevertheless, our results indicate that the use of the latent space structure for networked\ndata is a better model than assuming independence across dyads.7\n\n3\n\nScalability and computation\n\nHaving demonstrated the contribution of latent space models, we now turn to the issue of scalability and computation. The amount of computational improvement one can expect from using\nDP priors on the latent space depends on how well we can cluster dyads into groups that have\nthe same data and parameters. In networks in which every dyad generates a different observed\noutcome (e.g., if the network is dense and the observed value is continuous), the likelihood for\neach dyad will have to be computed separately, and a discrete representation of the latent space\n7 There\n\nare of course many different ways to predict link formation (known in the machine learning community as\n\"link mining\"), such as the Katz Score (Katz 1953) and the SimRank algorithm (Jeh and Widom 2003). Getoor and\nDiehl (2005) provides a detailed review of link mining methods, and Liben-Nowell and Kleinberg (2007) compare the\nperformance of some of them. We compared the predictive ability of our latent space approach against some of these\nmethods, and found that while our model did best when tests were more discriminating (low Q), the other models\n\"caught up\" when Q was increased. However, our model offers behavioral intuition (see Section 4) that machine\nlearning algorithms cannot provide, and we are willing trade off some predictive power for managerial interpretability.\nNevertheless, our objective in predicting future link formation is only to demonstrate the value of accounting for latent\nnetwork structure when modeling interactions data.\n\n19\n\n\fwill have little effect. However, the density of many (if not most) social networks tends to be very\nlow. Even if non-empty dyads generate data on a continuous domain (as in our China Mobile\nexample), there are so many empty dyads, all with the same data, that the number of distinct\nlikelihoods to compute is much lower than the total number of dyads in the network. If the observed data is discrete, then even more aggregation is possible. Of course, aggregation according\nto observed data is standard practice when a model is homogeneous, or marginal likelihoods are\navailable in closed form. The DP prior lets us group observations with similar latent parameters\nas well.\nThe number of likelihood evaluations at each MCMC iteration depends on two factors: i) the\nnumber of groups with distinct data patterns (which in turn depends on the size and density of\nthe network); and ii) the number of mass points for each realization from the Dirichlet process.\nDyads with the same zi , z j pair, and the same value of yij , must have the same likelihood, since\nthey have the same data and same parameters. As long as we keep track of the number of dyads\nwith each zi , z j pair, we can compute the log likelihood for that pair once for each y, and multiply\nby the number of dyads with that pair and that y. Among all the data zeros, there are only (2k ) + 1\npossible likelihood values. If k is less than N, there is a computational saving, even if all of the\nnon-zero values of y are different (as happens when y is continuous). If y is discrete (so Cy is\n\u0010\n\u0011\nthe number of distinct values of y), there are at most (2k ) + 1 Cy possible likelihoods. For a\ncontinuous y, but with a large number of zeros, the number of possible likelihoods is (2k ) + 1, plus\nthe number of nonzero y's. Clearly, the more distinct observed data patterns there are, the less one\ncan take advantage of the discretization of the latent space that is generated by the DP. But in the\nsocial networking applications that are common in marketing, networks are often very sparse, so\nwe have at least one very large group of dyads with the same data.\nTo assess just how much computational savings there is, consider the Full model in the telephone call example. The calibration dataset has 12, 617 non-empty dyads; likelihoods for each of\nthese dyads must be computed individually. The mean of k is 530, so there are 140, 186 distinct\ndistances between mass points on the latent space. Instead of computing 11, 413, 973 separate likelihoods for each of the empty dyads, we only need to compute 140, 186 of them. Thus, the number\nof likelihoods to compute at each MCMC iteration is 152, 803. This represents a 98.7% reduction\nin computational requirements.\n20\n\n\falpha = 0.5\nalpha = 20\nalpha = 300\nTotal Likelihood Evaluations\n\n150000\n100000\n\n400\n0\n\n0\n\n50000\n\n200\n\nvalue\n\n600\n\n200000\n\nNumber of Mass Points\n\n1000\n\n2000\n\n3000\n\n4000\n\n1000\n\n2000\n\n3000\n\n4000\n\nSize of Network\n\nFigure 4: Posterior means for number of mass points, and total likelihood computations, for subsampled networks.\nThe extent to which our method can scale for datasets with many more individuals (large N)\ndepends on how both the network density and k change as N increases. Ultimately, these are\nboth empirical questions, the second of which we cannot know up front because not only is k\nunobserved, but it can be influenced by the choice of H0 and \u03b1. However, the expected number of\n\u0001\nmass points can be asymptotically approximated as E(k ) \u2248 \u03b1 log \u03b1+\u03b1 N (Antoniak 1974; Escobar\n1994). Thus, if \u03b1 is small, the expected number of mass points is also expected to be small, but it\nwill grow for larger datasets. If \u03b1 is large, the number of mass points for smaller datasets might\nbe larger, but this number will not grow as quickly for larger datasets. To test how well this\napproximation works in practice, we estimated the full model using successively larger subsets of\nour original network. We then fixed \u03b1 at three different values: 0.5, 20 and 300 (instead of placing\na weakly informative prior on \u03b1, as we did in the main analysis). We also computed the total\nnumber of likelihood computations for each sweep of the Gibbs sampler, which is just (2k ), plus\nthe number of non-empty dyads in the dataset.\nFigure 4 plots the posterior mean of k (the number of mass points) and the total number of\nlikelihood evaluations, against the size of the network. For the number of mass points, we observe\n\n21\n\n\fthe expected pattern. For small \u03b1, the number of mass points is small, but the incremental number\nof mass points grows with network size. For large \u03b1, the number of mass points is large, but the\nincremental change goes down with network size. The asymptotic approximation suggests that\nincremental computational effort would decrease even more for even larger values of N, even\nthough the number of total dyads continues to grow quadratically. In terms of total computation,\nfor low \u03b1, the number of computations increases more rapidly with N than for higher values of\n\u03b1, but when \u03b1 is high, the relationship becomes more linear. Even though the number of dyads\ngrows quadratically with k, larger networks will tend to have larger number of nonempty dyads.\nFor low \u03b1, computation grows faster than linear, but the number of latent dyads is low to begin\nwith, because of the increased clustering. Collectively, our results suggest that, if using a DP\nprior is not computationally feasible for a particular dataset, the incremental effort likely comes\nfrom the inability to aggregate the observed data, and not from an inability to aggregate the latent\nparameters. Of course, this is no different from scalability problems faced by Bayesian hierarchical\nmodelers who use MCMC to update model parameter estimates from non-networked data.\n\n4\n\nInterpretation and usefulness of the latent space\n\nIn Figure 5(a), we plot a single draw from the joint posterior distribution of the latent coordinates\nfrom the Full model with D = 2 (we chose the draw with the largest conditional likelihood).\nEach person in the network occupies a position in the latent space, and we define a cluster as all\nindividuals who share the same coordinates in the latent space (this is akin to two observations\nhaving the same mass point in a realization from a mixture of Dirichlet processes, and we counted\n593 such clusters in this realization). But since multiple individuals are located at the same coordinates, to aid visualization we \"jitter\" the individual locations of customers by adding a small\namount of random noise (drawn from a uniform(-0.03,0.03) distribution) to each coordinate. The\nscale labels on the axes are included to help reference certain parts of the space, and do not have\na concrete interpretation themselves. In Figure 5(a), we see that there is considerable clustering,\nwith distinct \"super-clusters\" (clusters of clusters, or clusters closer to other clusters), of individuals on the latent space. Also, there are some clusters that contain only a few customers, and which\nare quite separate from the rest of the network of customers, such as the one at { x, y} coordinate\n\n22\n\n\f(b) Segmentation and Targeting\n\n2\n\n\u22120.2\n\n(a) Latent space\n\nD\n!\n\nB\n!\n\n\u22120.6\n\n0\n\ny\u2212coordinate\n\n1\n\n\u22120.4\n\nE\n!\n\nC\n!\n\n\u22121\n\nA\n\n\u22122\n\n\u22120.8\n\n!\n\n\u22122\n\n\u22121\n\n0\n\n1\n\n2\n\n\u22120.6\n\nx\u2212coordinate\n\n\u22120.4\n\n\u22120.2\n\n0.0\n\nx\u2212coordinate\n\nFigure 5: Illustration of the full latent space, and connections among some selected customers. In\nboth panels the black dots represent jittered locations for individual customer on { x, y} coordinates. Lines represent observed connections among customers. In panel (b), we labeled several\ncustomers who have hypothetically adopted a new product. Circles around customers are used to\nidentify other customers who may be similar to the targeted customers and therefore have similar\nadoption likelihoods.\n(-0.9,-2.2). Note that the latent space is a random variable, so this figure represents just one possible configuration of the individuals into clusters. Figure5(b) \"zooms in\" on a small partition of\nthe latent space. Having provided and discussed a graphical depiction of latent space, the next\nquestion becomes: what use is this to marketers? We examine this in the context of segmentation\nand targeting.\n\nSegmentation and targeting using interactions data Segmentation and targeting is central to\nthe development of effective marketing strategy. The fundamental idea behind segmentation is to\nfind people who are similar to one another, with the assumption that they will respond in similar ways, and therefore can be targeted using similar methods (e.g. the same price discount, the\nsame promotion, or same advertising copy). In our study we reveal two ways marketers can use\n\n23\n\n\fnetwork based data (our interactions observations) in practice. As is well documented by authors\nsuch as Novak et al. (1992) and Hill et al. (2006), following observed interactions to or from customers who have already adopted a product or service can help identify other potential customers.\nTheir results show improvements in response rates, compared with methods using observed, traditional segmentation and targeting bases. While these network-based methods are powerful\ntools for eliciting new customers, our graphical representation of the latent space highlights that\nthere are sometimes interactions among customers who are quite different from one another. We\nsee this in Figure 5(b). In this figure, we identify five individuals for whom a marketer might have\nsome specific information (e.g., an existing customer, or a respondent to a promotion). The lines\nradiating from these individuals represent observed links in the dataset. We also placed circles of\ncommon radius around these focal individuals.\nNetwork marketing tactics that \"follow the links,\" would use the lines to determine the next\npotential customers to target. While this is useful in reaching new clusters, there are many marketing tactics that have nothing to do with following links or word of mouth, and instead depend more on understanding which customers can be grouped into more homogeneous segments.\nGiven the interpretation of the latent space as representing similarity, anyone in close proximity\n(within the circle) to a focal customer should also be a target. Although most observed contacts\nalso occur within a cluster, there are certainly interactions among dissimilar individuals as well.\nAs an example, consider a marketer of trendy casual clothing, targeting a college student who\ninteracts with two people: a classmate at the same college, who shares similar demographic traits\nsuch as age, education, gender, values; and an older relative with whom there is a closer personal\nrelationship, but nothing else in common in terms of purchase patterns. While the college student\nmight have identical observed interaction patterns with his classmate and with the relative, he\nshares many more common friends with his classmate than with his relative. Our model places\nthe student closer on the latent space to his classmate than to his relative, and the relative is closer\nto her own friends and others in her social circle. This is useful for marketers to be aware of, because the classmate and the relative represent quite different marketing prospects. If the marketer\nwere to identify prospects based on the observed interactions alone, however, he could be targeting the relative, and her friends, who are unlikely to behave in the same way as the focal customer\n(the student). Targeting these prospects incurs additional costs with little expected return. In ad24\n\n\fdition, there are many individuals within the circle who never talk to our focal customer, but still\n\"travel in the same social circles,\" or who might otherwise be exposed to, or susceptible to, similar\nmarketing activities.\nWe can compare the use of targeting based on proximity in latent space, with targeting based\non geodesic distance8 . In fact, many more customers can be identified for targeting than if one\nwere to use a geodesic-type distance metric represented by observed interactions. We calculate\nthat if one were to follow the first degree geodesic distance, on average the marketer would expect\nto reach eight customers (rounded up from 7.56) . If this data were available and the marketer also\nincluded in the target set the \"second degree,\" or friends of friends, the marketer then expects\nto reach on average 97 customers. Drawing a circle of radius equal to 0.1 around the customer,\nthe marketer may expect to reach, on average, 232 customers9 . Since homophily implies that\nsimilar individuals are more likely to interact, then targeting based on latent space means that the\ncustomers identified for targeting are more likely to be similar to the focal customer. The geodesic\ndistance in some cases could be connections which span much of the space and therefore may lead\nto leads that are substantially different than the original customer. For example, in Figure 5(b), the\ncustomer labelled \"C\" has seven interactions, but two of these interactions are to customers who\nare at substantially different locations in the latent space. Given the desire for marketers to find\ncustomers similar to the focal customer, we assert that it is better to target those customers close\nto the labeled customers in the network. The latent space model presents an opportunity to refine\nthese targeting methods, and using the Dirichlet process to model the latent space makes the\napproach computationally feasible for marketing data.\n\nExtracting information from limited data Another advantage of using our probability modeling approach is that the interpretation of the posterior latent space is only loosely dependent on\nthe kind of data that one uses to estimate it. Of course, larger, richer datasets, collected over\nlonger periods of time, might lead to better posterior distributions, but the data itself could really\nbe anything, as long as the underlying data-generating process is dyad-specific, and depends on\n8A\n\nthird perspective is that perhaps one should first follow connections within the clusters, then the similar customers, and then follow the geodesic links outside of some cluster\n9 Of course, this depends on the size of the circle, but since customers in the same cluster occupy identical coordinates, even a circle of minimal radius gives us the result that more customers are identified for targeting, on average,\nthan the first-degree geodesic.\n\n25\n\n\fsimilarities in a monotonic way (events are more likely if latent distances are small). The data\nthat is available could be limited in terms of time (a censored dataset), or by the fact that observed cell phone interactions are only one of many possible means of communication. Interactions occur among customers at heterogeneous rates, and since it is not practical for marketers\nto wait extended amounts of time to see if interactions will occur among customers, it may be\nthat some interactions that exist among customers occur at such a rate that they may not be observed in a small observation window. One might be tempted to treat the addition or subtraction\nof observed interactions as evidence of nonstationarity. The Bayesian approach to data analysis\nmakes it straightforward to update our estimates of the latent space as new data becomes available, even when the space itself is stationary. Therefore, what Kossinets and Watts (2006) refer\nto as an \"evolving social network\" may, in effect, be an artifact of the censoring of the data. All\nthe observed data does is provide some clues from which the true underlying similarities must be\ninferred.\nRelated to censoring, one of the concerns about using data like phone call records is that they\ndo not necessarily represent the universe of interactions among the population. Unless customers\nreveal all of the people with whom they ever interact with, observed data cannot be an authoritative document of the underlying social network. There may be other modes of communication,\nsuch as email or face-to-face contact. For example, Ansari et al. (2008) consider the case of a Swiss\nmusic sharing website, where the connection between users could be described alternatively as\n\"friendship,\" \"communication,\" or \"download.\" So what value does using only a network of cell\nphone calls have, if it is an incomplete representation of all interactions? We can think of any\n\"true\" observed interaction network as a population of subnetworks, and each observed network\n(e.g., the cell phone data), as a single draw from that population (Gelman 2007). We then treat\nthat observed network as a single data point, and use it to update our beliefs about the structure\nof the latent space. If we had observed another mode of communication first, we might get a different posterior latent space but, in any event, the posterior of the latent space after observing one\nnetwork becomes the prior before observing the next.\n\nFocus on Diffusion and WOM We see two key contributions of the latent space useful to marketers managing the diffusion of innovation of information through customer networks. The first\n26\n\n\finvolves the concept of \"Word of Mouth\" (e.g. Arndt 1967). While contagion could occur via\nnon-explicit advocacy (e.g. fashion can be seen by people that one does not interact with), explicit\ncommunication is well regarded as an important source of information for customers. WOM is at\nthe heart of models of information diffusion in networks (e.g. Goldenberg et al. 2001). As testimony to the importance of consumer reviews, there are many services and organizations focused\non collecting and presenting such information on just about every product or service. Of key\ninterest in the WOM literature is how the network structure affects diffusion patterns. The contribution of our work is in considering that WOM may work via some geodesic distance versus\ndistance measured in latent space. From relations data, only the geodesic distance can be studied,\nbut there is likely considerable value to considering distance in latent space as a channel for WOM.\nThe second area where the latent space model could be useful in practice is in identifying\ninfluential customers. The concept of a market maven, or opinion leader (Katz and Lazarsfeld\n1955; Feick and Price 1987; Iyengar et al. 2008; Kratzer and Lettl 2009) has frequently been studied in the context of diffusion research. However, recent research challenges the notion that such\ninfluentials are the primary reason for \"global cascading influence\" (Watts and Dodds 2007), i.e.\nthe contagious diffusion of innovation or information throughout an entire network. These insights suggests that it is vital for marketers to understand how influence can occur among all\ncustomers, and that there is more to WOM marketing than focusing attention on just influentials.\nObservations from practice certainly seem to support this, from the popularity of services such\nas BzzAgent, and Procter & Gamble's Vocalpoint, which are not selective about recruiting only\n\"opinion leaders,\" but rather would prefer more people in their network.\n\nFurther research opportunities The sociological theory of homophily, coupled with the latent\nspace framework, yields a stochastic representation of the relative latent characteristics underlying\ninteractions data. The latent characteristics are represented on a latent space, and we propose a\nBayesian nonparametric for the latent space using Dirichlet processes. Latent spaces are well\nknown in social network analysis, and Dirichlet processes and probability models are known in\nmarketing. However, due to the computational obstacle involved in estimating larger networks,\nthe concepts have not yet fully integrated across disciplines. Our research lowers this obstacle,\nand makes probability modeling more accessible to marketing researchers who possess data on\n27\n\n\fcustomer interactions. This approach maintains the properties of interdependence, heterogeneity\nand interpretability, a goal that is harder to accomplish with extant classical or machine learning\napproaches.\nWe readily admit that our interpretation of the latent space, and our suggestions on how to\nuse it, depend on an acceptance of two premises. First, we need to believe that similarities drive\ninteractions, and so one can use interactions to infer similarities. We use the volumes of research\nof homophily to support this contention, but we have not tried to test this directly. Second, our\nrecommendation that managers consider using latent distance, rather than observed geodesic distance, to segment and target customers assumes that similar individuals have correlated purchase\npreferences or behavior. This is a premise that could be tested, and we hope that both researchers\nand practitioners will undertake that challenge. Unfortunately, the data that we have at our disposal does not allow us to follow that path, but we would like to describe briefly how we think\nthis might work.\nThe output of the latent space model is a posterior distribution of configurations on a latent\nspace. Figure 5 is one such configuration. Although the distances among individuals in a single configuration do not have a physical interpretation, we can still treat them as distances in a\nstatistical sense. So we could draw upon the methods of hierarchical spatial modeling to infer a\ncorrelation structure among individuals, similar to those described in Banerjee et al. (2004). An example of this kind of treating a non-physical distance as a physical one is Yang and Allenby (2003),\nwho computed a demographic distance between people based on profiles of personal characteristics. So just as they modeled correlations in preferences as functions of observed geographic and\ndemographic distance, we propose modeling these correlations as functions of latent distances.\nWe hypothesize that since observed interactions represent only one possible path for the sharing\nof information, it is latent distance, rather than geodesic, demographic or geographic distance,\nthat would best predict these correlations. In order to conduct a test like this, one would need two\ntypes of data for the same set of people: dyad-level interactions data to infer the latent space, and\nindividual-level purchase data to see if latent distance explains correlations in purchase behavior.\nAs more and more business is conducted through mobile communications devices, we anticipate\nthat data like this will become more available. A corollary to this research stream would be to\nincorporate individual-level demographics or covariates into the latent space model, and to better\n28\n\n\funderstand how that information might complement interactions data in understanding purchase\nbehavior.\n\nAppendices\nA\n\nChoosing dimensionality of the latent space\n\nThere are a number of approaches one could take to selecting D, and finding a general method\nfor choosing among different specifications of Bayesian hierarchical models, especially those that\nincorporate nonparametric priors, remains an area of active research among statisticians. We believe that because of the abstract nature of the latent space, there is no \"correct\" value of D that one\nneeds to infer from the data. Hoff (2005) notes that one should choose the smallest value of D that\noffers a reasonable model fit, erring on the side of parsimony. Adding more dimensions improves\nmodel flexibility, but can also lead to over-fitting. As far as objective measures go, he suggests examining the log marginal likelihoods (LML) (we use the holdout LML for the Full model), as well\nas the posterior predictive checks (PPC) for test statistics that capture important characteristics of\nthe data (we discuss PPCs in Section 2.2).\nIn Table 3 we present relative estimated LML of the HMCR and Full models, for different values of D, from both the calibration and holdout datasets. Our estimates were generated using\ncumulant approximations, and adjusting for the discrepancy between posterior and prior support, using the methods proposed in Lenk (2009). Results are normalized such that the reported\nestimate for the HMCR model, with D = 2 is 0, for each of the calibration and holdout datasets,\nand then scaled by 1,000 for readability.\nD\n2\n3\n4\n5\n6\n\ncalibration\nHMCR\nFull\n0\n-40\n221\n-310\n-396 -1,609\n-1,550 -1,130\n-4,729\n-829\n\nholdout\nHMCR\nFull\n0\n-337\n-378\n-690\n-190 -1,792\n-3,183 -1,560\n-5,069 -1,573\n\nTable 3: Estimates of log marginal likelihoods (LML) of models, by dimensionality of latent space\nand model variant. Estimates are normalized with respect to the D = 2 HCMR model for each\ndataset.\n29\n\n\fIn three of the four cases, D = 2 is preferred, and in the remaining one, D = 3 is preferred.\nAlso, we found essentially no difference among values of D in posterior predictive checks. So,\nfollowing Hoff's advice, we use the D = 2 models for subsequent analysis.\n\nB\n\nModel specification for China Mobile example\n\nIn this appendix, we derive the data likelihood and hyperprior specifications for our Chongqing\nMobile application. Let pij be the probability that a dyad between i and j is open, and let \u03bbij\nbe the rate of contacts between i and j (assuming exponentially-distributed intercontact times), if\nthe dyad is open. We also define an auxilliary latent Bernoulli variable sij that indicates whether a\ndyad is open (sij = 1) or closed (sij = 0). To remain consistent with our general model specification\nin the text, we use yij to denote the vector of observed intercontact times, and yij\u2217 to denote the\ncount of observed contacts. Also, let T be the duration of the observation period.\nThe data likelihood is similar to an \"exponential never triers\" model (Fader et al. 2003). There\nare two ways a dyad could be empty (i.e., y\u2217 = 0): the dyad could be closed (sij = 0), or it\ncould be open, but the contact rate \u03bbij is sufficiently low that there just happened to be no contacts\nduring the observation period. If we observe any contacts at all, we know the dyad must be open.\nTherefore, the data likelihood is\ni\n\u0001\n\u0001 h\n\u0001\ny\u2217\nf yij |\u03bbij , pij = 1 \u2212 pij I yij\u2217 = 0 + pij \u03bbijij exp \u2212\u03bbij T\n\n(4)\n\nWe incorporate dyad-wise unobserved heterogeneity in \u03bbij by using a gamma distribution with\ndyad-specific shape parameter rij and dyad-specific scale parameter aij\nr\n\n\u0001\n\nf \u03bbij |rij , aij =\n\naijij\n\u0393 rij\n\nr \u22121\n\n\u0001 \u03bbijij\n\nexp \u2212 aij \u03bbij\n\n\u0001\n\n(5)\n\nAfter integrating over \u03bbij ,\n\nf yij | pij , rij , aij\n\n\u0001\n\n\u0010\n\u0011\ni\n\u0393 rij + yij\u2217 \u0012 aij \u0013rij \u0012 1 \u0013yij\u2217\n\u0001 h \u2217\n\u0001\n= 1 \u2212 pij I yij = 0 + pij\naij + T\naij + T\n\u0393 rij\n\n(6)\n\nWe will also use the reparameterizations rij = \u03bc2ij /v and aij = \u03bcij /v, where \u03bcij and v are the dyad30\n\n\fspecific mean and common variance of the gamma distribution, respectively. Linking back to our\n\u0002\n\u0003\ngeneral model formulation in Section 1.1, \u03c6ij = pij , \u03bcij , v . The definitions of these parameters are\ndescribed in Section 2.1.\n\nHyperprior specifics\nFor the choice of H0 , we decompose each latent coordinate into two components: the distance\nfrom the origin (a \"radius\") and the location on the surface of a hypersphere that has that radius.\nWe then choose a H0 that factors into a prior on these two components. In other words, we think\nof the elements of zi in terms of their spherical, rather than Cartesian, coordinates. If the Cartesian\ncoordinates of zi (herein suppressing the i subscript) are z1 , z2 , ..., z D , then its polar coordinates are\n\n(\u03b81 , ..., \u03b8D\u22121 , \u03c1), where \u03c1 is a distance from the origin and the \u03b80 s are angles, expressed in radians,\nsuch that 0 < \u03b81 < 2\u03c0, and 0 < \u03b8 j < \u03c0 for 2 \u2264 j \u2264 D \u2212 1. We can then factor H0 as\ng0 (z) = f (\u03b81 , ..., \u03b8D\u22121 , \u03c1) = f (\u03b81 , ..., \u03b8D\u22121 |\u03c1) f (\u03c1)\n\n(7)\n\nConditioning on \u03c1, we want to place a distribution on \u03b8 = (\u03b81 , ..., \u03b8D\u22121 ), such that there is a\nuniform probability of being at any location on a D \u2212dimensional hypersphere with radius \u03c1. This\nis achieved by letting f (\u03b8 |\u03c1) be a multivariate power sine distribution (Johnson 1987; Nachtsheim\nand Johnson 1988), where\nf (\u03b8 ) \u221d\n\nD \u22121\n\n\u220f sinj\u22121 \u03b8j\n\n(8)\n\nj =1\n\nThus, \u03b81 has a uniform distribution, f (\u03b82 ) \u221d sin (\u03b82 ) , f (\u03b83 ) \u221d sin2 (\u03b83 ), and so forth. Johnson\n(1987, ch 7) proposes some algorithms for simulating from a multivariate power sine distribution.\nFor f (\u03c1), recall that \u03c1 is defined on the positive real line, with E (\u03c1) = 1. We also need the\nability to trade off tail weight (probability of draws of z being far away from the origin) against\nkurtosis (likelihood of draws of z being clustered around the origin). Beginning with the generalized Laplace distribution (Kotz et al. 2001, sec. 4.4.2), we center, and then fold, at zero, to\nget\n\u0014\n\n\u0012\n\n1\nf (\u03c1) = \u03ba \u03c3\u0393 1 +\n\u03ba\n1\n\u03ba\n\n\u0013\u0015\u22121\n\n31\n\nh \u0010 \u03c1 \u0011\u03ba i\nexp \u2212\n,\u03c1>0\n\u03ba\u03c3\n\n(9)\n\n\fwhere\n1\n\u03ba\n\nE (\u03c1) = \u03ba \u03c3\n\n\u0393\n\n2\n\u03ba\n\u0001\n1\n\u03ba\n\n\u0001\n\n\u0393\n\n(10)\n\nSetting E (\u03c1) = 1,\n\u03c3=\n\n\u0393\n\u0393\n\n1\n\u03ba\n2\n\u03ba\n\n\u0001\n(11)\n\n1\n\n\u0001\n\n\u03ba\u03ba\n\nso the density of \u03c1, constrained so E (\u03c1) = 1, is\n\nf ( \u03c1 |\u03ba ) =\n\n\u03ba\u0393\n\n2\n\u03ba\n\n\u0393\n\n\u0001\n1 2\n\u03ba\n\n\"\n\n\u0001\n\n\u03c1\u0393\n\nexp \u2212\n\n\u0393\n\n\u0001 !\u03ba #\n\n2\n\u03ba\n\u0001\n1\n\u03ba\n\n(12)\n\nThe parameter \u03ba controls the tradeoff between tail weight and kurtosis. If \u03ba = 1, f (\u03c1|\u03ba ) reduces\nto an exponential distribution, and if \u03ba = 2, f (\u03c1|\u03ba ) is a half-normal distribution. As \u03ba becomes\nlarge, the mode of \u03c1 becomes less and less peaked, and f (\u03c1|\u03ba ) converges to a uniform(0, 2) distribution. The \"correct\" value for \u03ba is inferred through the estimation process, letting the data drive\nthe tradeoff between placing a mode on \u03c1 and bounding the locations such that \u03c1 \u2264 2. Note that\nthis prior using the multivariate power sine distribution and our restricted half-Laplace distribution adds only one additional parameter to the model, compared to an independent multivariate\nnormal hyperprior, which adds many more.\nIt turns out that f (\u03c1|\u03ba ) is a special case of a power gamma distribution. To see this, perform a\n1\n\n1\n\nchange of variables so v = \u03c1\u03ba , \u03c1 = v k and d\u03c1 = \u03ba1 v \u03ba \u22121 . Then,\n\nf ( v |\u03ba ) =\n\n\u0393\n\u0393\n\n2\n1\n\u03ba\n\u0001 v \u03ba \u22121 exp\n1\n\u03ba\n\n\u0001\n\n\"\n\nwhich is a gamma distribution with shape parameter\n\n\u2212\n\n1\n\u03ba\n\n\u0393\n\u0393\n\n\u0001 !\u03ba\n\n2\n\u03ba\n\u0001\n1\n\u03ba\n\n#\n(13)\n\nv\n\n\u0012\nand rate parameter\n\n\u0393( \u03ba2 )\n\n\u0013\u03ba\n\n\u0393( \u03ba1 )\n\n. This result\n\nmakes it easy to simulate values of \u03c1; just draw v from this gamma distribution, and transform\n1\n\n\u03c1 = v \u03ba . After simulating values of \u03b8 and \u03c1, it is often convenient to convert zij back to its Cartesian\ncoordinates. The elements of z can be expressed as:\nj \u22121\n\nz1 = \u03c1 cos \u03b81\n\nz j = \u03c1 cos \u03b8 j \u220f sin \u03b8l , for 2 \u2264 j < D\nl =1\n\n32\n\nD \u22121\n\nzD = \u03c1\n\n\u220f sin \u03b8l\nl =1\n\n(14)\n\n\f(Johnson 1987, ch 7).\nWe selected the other hyperpriors to balance weak information content against numerical stability. Following Escobar and West (1995), we place a weakly informative gamma prior on \u03b1, with\nmean of 4 and variance of 80. Since \u03b2 and v are all population-level parameters that appear only in\nthe definitions of \u03c6ij , we can combine them all into a single parameter vector \u039e (log-transforming\nparameters when necessary), with a multivariate normal prior \u039e0 , centered at the origin, with\ncovariance matrix A = 10I. Note that if \u03ba = 2, then H0 is a multivariate normal distribution.\nSince we were concerned about a mode of H0 introducing too much prior information, we used a\ngamma prior with a mean of 3 and a variance of about 5. Experimenting with alternative values\nled to no substantive effect.\n\nC\n\nEstimation algorithm\n\nIn this section we present the complete MCMC sampling algorithm for the general latent space\nmodel. The parameters to be estimated are \u03b1, \u03ba, \u03b2, and zi , i = 1...N. Recall that since H is discrete,\nat each iteration there are only k possible values that any zi can take.\n\nC.1\n\nSimulate \u03b1|*\n\nLet r\u03b1 and a\u03b1 be the parameters of the gamma hyperprior on \u03b1. Using the algorithm proposed by\nEscobar and West (1995), do the following.\n1. Starting with the current value of \u03b1, draw a temporary variable \u03b7 from a Beta (\u03b1 + 1, N )\ndistribution.\n2. Draw \u03c4 from a Bernoulli trial with probability\n\nr \u03b1 + k \u22121\nN ( a\u2212log(\u03b7 ))+r\u03b1 +k\u22121\n\n3. If \u03c4 = 0, draw \u03b1 from a gamma (r\u03b1 + k \u2212 1, a\u03b1 \u2212 log (\u03b7 )) distribution. If \u03c4 = 1, draw \u03b1 from\na gamma (r\u03b1 + 1, a\u03b1 \u2212 log (\u03b7 )) distribution.\n\nC.2\n\nSimulate \u03b2, v|*\n\nTo simplify notation, we combine \u03b2 and v into a single parameter vector \u039e. The conditional posterior distribution of \u039e depends on the data likelihood and the prior. The data likelihood we care\n33\n\n\fabout here is the marginal likelihood in Section 1.2, after integrating over \u03b8i , multiplied across all\ndyads (using our assumption of conditional independence across dyads). Note that \u03c6ij is a function\nof \u039e, zi and z j . The prior on \u039e is a multivariate normal with mean \u039e0 and covariance A. Thus, the\nlog conditional posterior (without normalizing constant) for \u039e is\n\nlog f (\u039e|*) =\n\nN\n\nN\n\n\u2211 \u2211\n\nlog f (yij |\u039e, zi , z j ) \u2212\n\ni =1 j = i +1\n\n1\n( \u039e \u2212 \u039e 0 ) 0 A \u22121 ( \u039e \u2212 \u039e 0 )\n2\n\n(15)\n\nWe simulate \u039e using a random walk Metropolis sampler (Rossi et al. 2005, ch. 3).\n\nC.3\n\nSimulate \u03ba |*\n\nWe place a gamma (r\u03ba , a\u03ba ) prior on \u03ba. Let \u03c11:k be the radii of the k distinct values of z. Combining\nthe likelihood of the radii in 12 with the prior, the conditional posterior distribution for \u03ba is\n\"\nf (\u03ba |*) \u221d\n\n\u0001 #k\n\n\u03ba\u0393\n\n2\n\u03ba\n\n\u0393\n\n\u0001\n1 2\n\u03ba\n\n\"\n\nk\n\nexp \u2212 \u2211\n\n\u03c1j \u0393\n\nj =1\n\n\u0393\n\n2\n\u03ba\n1\n\u03ba\n\n\u0001 !\u03ba\n\n\u0001\n\n#\n\n\u2212 a \u03ba \u03ba \u03ba r\u03ba \u22121\n\n(16)\n\nThere are many different ways to simulate from this univariate density. We chose to use samplingimportance resampling (SIR) (Smith and Gelfand 1992), but one might choose Metropolis, gridbased inverse CDF, or slice sampling methods instead.\n\nC.4\n\nSimulate z|*\n\nThis step, in which we draw each of the zi vectors from the mixture of Dirichlet processes (MDP),\nis an adaptation Algorithm 8 in Neal (2000). We direct the reader there for an explanation of\nhow and why the algorithm works, but we present a summary here, using our terminology and\nnotation. The distribution of the zi 's is discrete, so at each iteration of the estimation algorithm,\n\u0001\nthere are only k possible values that zi can take. Let z\u2217 = z1\u2217 . . . z\u2217k define these k distinct latent\ncoordinates, let z\u2217\u2212i be the distinct mass points when not including person i, and let k \u2212i be the\nnumber of distinct mass points in z\u2217\u2212i when not including person i (z\u2217 and z\u2217\u2212i , and k and k \u2212i ,will\ndiffer only if i is a \"singleton\" who is the only person located at zi ). At the current state of the\nsampler, each person is \"assigned\" to one of the z\u2217j , in the sense that there is exactly one j for\nwhich zi = z\u2217j . Let Nj be the number of people assigned to z\u2217j , and let N\u2212i,j be the number of\n34\n\n\fpeople assigned to z\u2217j , when not counting person i.\nThe algorithm involves choosing some number of proposal values (determined by a prespecified control parameter m) for each zi . Define f i (yi |zi , z\u2212i ) as the likelihood contribution for all\ndyads that involve person i, given the current values of zi assigned to i, and of z\u2212i assigned to\neveryone else. There are two cases that we need to consider. The first is if there is some other\nperson i0 for which zi = zi0 (i.e., i is not a singleton). In this case, draw m proposal draws from H0 ,\n\u0001\ncall them z\u2217k+1 , ..., z\u2217k+m , and let z\u0303 = z1\u2217 , ..., z\u2217k , z\u2217k+1 , ..., z\u2217k+m . Intuitively, z\u0303 is the union of the set of\nall latent vectors that are already assigned to someone in the population, with the set of m new\n\u0001\nproposal vectors. Next, compute f i yi |z\u0303 j , z\u2212i , for all j = 1 . . . (k + m). These are the likelihood\ncontributions for all dyads involving i, if zi were set to each of the values in z\u0303. These \"proposal\nlikelihoods\" form a set of weights that we use to draw a new zi for each i. Thus, draw a new value\nfor zi from z\u0303 using the following probabilities:\n\u001a\n\n\u0001\n\nPr zi = z\u0303 j =\n\n\u0001\nn\n\u03c9 N \u2212\u22121i,j+\u03b1 Fi yi |z\u0303 j , z\u2212i for 1 \u2264 j \u2264 k\n\u0001\n(\u03b1/m)\n\u03c9 N \u22121+\u03b1 Fi yi |z\u0303 j , z\u2212i for k + 1 \u2264 j \u2264 k + m\n\nwhere \u03c9 is a normalizing constant (and does not need to be known for the purposes of random\nsampling). Thus, zi can take on the value of any of the existing elements of z\u2217 , or one of the m new\ncandidate values. Which value is selected depends on three values: the likelihood of the data for\neach z\u0303 j (values of z\u0303 j that yield a high likelihood are more likely to be chosen), the number of other\npeople who also are assigned to z\u0303 j (coordinates where the prior distribution has more mass are\nmore likely to be chosen), and the DP control parameter \u03b1, which governs how close the DP prior\non z is to H0 . If i is a singleton, then there are only k \u2212i = k \u2212 1 elements in z\u2217 . In this case, draw\n\b\n\u0001\nm + 1 candidate values from H0 , re-index them so z\u0303 = z\u2217\u2212i , z\u2217k , ..., z\u2217k+m , and select according\nto the probabilities\n\n\u0001\n\nPr zi = z\u0303 j =\n\n\u001a\n\n\u0001\nN\n\u03c9 N \u2212\u22121i,j+\u03b1 Fi yi |z\u0303 j , z\u2212i for 1 \u2264 j \u2264 k \u2212 1\n\u0001\n(\u03b1/m)\n\u03c9 N \u22121+\u03b1 Fi yi |z\u0303 j , z\u2212i for k \u2264 j \u2264 k + m\n\nReferences\nGeorge A. Akerlof. Social distance and social decisions. Econometrica, 65(5):1005\u20131027, Sept. 1997.\n35\n\n\fAsim Ansari and Carl F. Mela. E-customization. Journal of Marketing Research, 40:131\u2013145, May\n2003.\nAsim Ansari, Oded Koenigsberg, and Florian Stahl. Modeling multiple relationships in social\nnetworks. working paper, Dec. 2008.\nC.E. Antoniak. Mixtures of Dirichlet processes with applications to nonparametric problems. Annals of Statistics, 2(4):1152\u20131174, 1974.\nJohan Arndt. Role of product-related conversations in the diffusion of a new product. Journal of\nMarketing Research, 4(3):291\u2013295, August 1967.\nSudipto Banerjee, Bradley P. Carlin, and Alan E. Gelfand. Hierarchical Modeling and Analysis for\nSpatial Data. CRC Press, 2004.\nFrank M. Bass. A new product growth model for consumer durables. Management Science, 15(5):\n215\u2013227, Jan. 1969.\nWilliam O. Bearden and Michael J. Etzel. Reference group influence on product and brand purchase decisions. Journal of Consumer Research, 9, Sept. 1982.\nDavid R. Bell and Sangyoung Song. Neighborhood effects and trial on the Internet: Evidence from\nonline grocery retailing. Quantitative Marketing and Economics, 5:361\u2013400, 2007.\nDavid Blackwell and James B. MacQueen. Ferguson distributions via polya urn schemes. The\nAnnals of Statistics, 1(2):353\u2013355, Mar. 1973.\nPeter M. Blau. Inequality and Heterogeneity: A Primitive Theory of Social Structure. Free Press, New\nYork, 1977.\nEric T. Bradlow and David C. Schmittlein. The little engines that could: Modeling the performance\nof world wide web search engines. Marketing Science, 19(1):43\u201362, Winter 2000.\nMichael Braun, Peter S. Fader, Eric T. Bradlow, and Howard Kunreuther. Modeling the \"Pseudodeductible\" in homeowners' insurance. Management Science, 52(8):1258\u20131272, August 2006.\nJacqueline Johnson Brown and Peter H. Reingen. Social ties and word-of-mouth referral behavior.\nJournal of Consumer Research, 14(3):350\u2013362, Dec. 1987.\nJeonghye Choi, Sam K. Hui, and David R. Bell. Spatiotemporal analysis of imitation behavior\nacross new buyers at an online grocery retailer. Journal of Marketing Research, 47(1):75\u201389, Feb.\n2010.\nKoustuv Dasgupta, Rahul Singh, Balaji Viswanathan, Dipanjan Chakraborty, Sougata Mukherjea,\nAmit A. Nanavati, and Anupam Joshi. Social ties and their relevance to churn in mobile telecom\nnetworks. In Proceedings of the 11th International Conference on Extending Database Technology:\nAdvances in Database Technology, volume 261 of ACM International Conference Proceeding Series,\npages 668\u2013677, New York, 2008. ACM.\nNathan Eagle, Alex Pentland, and David Lazer. Inferring friendship network structure by using\nmobile phone data. Proceedings of the National Academy of Sciences, 106(36):15274\u201315278, September 2009.\n\n36\n\n\fMichael D. Escobar. Estimating normal means with a dirichlet process prior. Journal of the American\nStatistical Association, 89(425):268\u2013277, March 1994.\nMichael D. Escobar and Michael West. Computing nonparametric hierarchical models. In D. Day,\nP. M\u00fcller, and D. Sinha, editors, Practical Nonparametric and Semiparametric Bayesian Statistics,\nchapter 1, pages 1\u201322. Springer-Verlag, 1998.\nMichael D. Escobar and Mike West. Bayesian density estimation and inference using mixtures.\nJournal of the American Statistical Association, 90(430):577\u2013588, 1995.\nPeter S. Fader, Bruce G. S. Hardie, and Robert Zeithammer. Forecasting new product trial in a\ncontrolled test market environment. Journal of Forecasting, 22:391\u2013410, 2003.\nLawrence F. Feick and Linda L. Price. The market maven: A diffuser of marketplace information.\nJournal of Marketing, 51(1):83\u201397, Jan. 1987.\nThomas S. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of Statistics,\n1(2):209\u2013230, 1973.\nJeffrey D. Ford and Elwood A. Ellis. A reexamination of group influence on member brand preference. Journal of Marketing Research, 17(1):125\u2013132, Feb. 1980.\nHubert Gatignon and Thomas S. Robertson. A propositional inventory for new diffusion research.\nJournal of Consumer Research, 11(4):849\u2013867, Mar. 1985.\nAndrew Gelman. Comment on Handcock, et. al., \"Model-based clustering for social networks\".\nJournal of the Royal Statistical Society, Series B, 170(2):337, 2007.\nAndrew Gelman, Xiao-Li Meng, and Hal Stern. Posterior predictive assessment of model fitness\nvia realized discrepancies. Statistica Sinica, 6(4):733\u2013807, 1996.\nLisa Getoor and Christopher P. Diehl. Link mining: A survey. ACM SIGKDD Explorations Newsletter, 7(2):3\u201312, 2005.\nDavid Godes and Dina Mayzlin. Firm-created word-of-mouth communication: Evidence from a\nfield test. Marketing Science, 28(4):721\u2013739, Jul.-Aug. 2009.\nJacob Goldenberg, Barak Libai, and Eitan Muller. Talk of the network: A complex systems look at\nthe underlying process of word-of-mouth. Marketing Letters, 12(3):211\u2013223, 2001.\nMark S. Handcock, Adrian E. Raftery, and Jeremy M. Tantrum. Model-based clustering for social\nnetworks. Journal of the Royal Statistical Society. Series A, 170(2):301\u2013354, 2007.\nShawndra Hill, Foster Provost, and Chris Volinsky. Network-based marketing: Identifying likely\nadopters via consumer networks. Statistical Science, 21(2):256\u2013276, 2006.\nPeter D. Hoff. Bilinear mixed-effects models for dyadic data. Journal of the American Statistical\nAssociation, 100(469):286\u2013295, March 2005.\nPeter D. Hoff, Adrian E. Raftery, and Mark S. Handcock. Latent space approaches to social network analysis. Journal of the American Statistical Association, 97(460):1090\u20131098, Dec. 2002.\nDavid R. Hunter, Steven M. Goodreau, and Mark S. Handcock. Goodness of fit of social network\nmodels. Journal of the American Statistical Association, 103(481):248\u2013258, March 2008.\n37\n\n\fHemant Ishwaran and Lancelot F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association, 96(453):161\u2013173, Mar. 2001.\nRaghuram Iyengar, Christophe Van den Bulte, and Thomas W. Valente. Opinion leadership and\nsocial contagion in new product diffusion. Technical Report 08-120, Marketing Science Institute,\n2008. working paper, University of Pennsylvania.\nGlen Jeh and Jennifer Widom. Simrank: A measure of structural-context similarity. In Proceedings\nof the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 271\u2013\n279, New York, 2003. ACM Press.\nMark E. Johnson. Multivariate Statistical Simulation. Wiley Series in Probability and Mathematical\nStatistics. Wiley, 1987.\nElihu Katz and Paul F. Lazarsfeld. Personal Influence. Free Press, New York, 1955.\nLeo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39\u201343, March\n1953.\nJin Gyo Kim, Ulrich Menzefricke, and Fred M. Feinberg. Assessing heterogeneity in discrete choice\nmodels using a dirichlet process prior. Review of Marketing Science, 2(1):1\u201339, 2004.\nGueorgi Kossinets and Duncan J. Watts. Empirical analysis of an evolving social network. Science,\n311:88\u201390, Jan. 2006.\nSamuel Kotz, Tomasz J. Kozubowski, and Krzysztof Podgorski. The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering and Finance.\nBirkhauser, Boston, 2001.\nJan Kratzer and Christopher Lettl. Distinctive roles of lead users and opinion leaders in the social\nnetworks of schoolchildren. Journal of Consumer Research, 36(4):646\u2013659, Dec. 2009.\nPaul Lazarsfeld and Robert K. Merton. Friendship as a social process: a substantitive and methodological analysis. In M. Berger, editor, Freedom and Control in Modern Society, chapter 2, pages\n18\u201366. Van Nostrand, New York, 1954.\nPeter Lenk. Simulation pseudo-bias correction to the harmonic mean estimator of integrated likelihoods. Journal of Computational and Graphical Statistics, 18(4):941\u2013960, 2009.\nDavid Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal\nof the American Society for Information Science and Technology, 58(7):1019\u20131031, 2007.\nMiller McPherson, Lynn Smith-Lovin, and James M. Cook. Birds of a feather: Homophily in social\nnetworks. Annual Review of Sociology, 27:415\u2013444, 2001.\nDonald G. Morrison and David C. Schmittlein. Predicting future random events based on past\nperformance. Management Science, 27(9):1006\u20131023, 1981.\nDonald G. Morrison and David C. Schmittlein. Generalizing the NBD Model for Customer Purchases: What are the Implications and is it Worth the Effort? Journal of Business and Economic\nStatistics, 6(2):145\u2013159, 1988.\n\n38\n\n\fChristopher J. Nachtsheim and Mark E. Johnson. A new family of multivariate distributions with\napplications to Monte Carlo studies. Journal of the American Statistical Association, 83(404):984\u2013\n989, Dec. 1988.\nSungjoon Nam, Puneet Manchanda, and Pradeep K. Chintagunta. The effects of service quality\nand word of mouth on customer acquisition, retention and usage. working paper, March 2007.\nRadford M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal\nof Computational and Graphical Statistics, 9(2):249\u2013265, June 2000.\nThomas P. Novak, Jan de Leeuw, and Bruce MacEvoy. Richness curves for evaluating market\nsegmentation. Journal of Marketing Research, 29(2):254\u2013267, May 1992.\nAnthony O'Hagan and Jonathan Forster. Kendall's Advanced Theory of Statistics, volume 2B:\nBayesian Inference. Arnold, London, 2nd edition, 2004.\nC. Whan Park and V. Parker Lessig. Students and housewives: Differences in susceptibility to\nreference group infuence. Journal of Consumer Research, 4(2):102\u2013110, Sept. 1977.\nPeter H. Reingen and Jerome B. Kernan. Referral networks in marketing: Methods and illustration.\nJournal of Marketing Research, 23(4):370\u2013378, Nov. 1986.\nPeter H. Reingen, Brian L. Foster, Jacqueline Johnson Brown, and Stephen B. Seidman. Brand\ncongruence in interpersonal relations: A social network analysis. Journal of Consumer Research,\n11(3):771\u2013783, Dec. 1984.\nPeter E. Rossi, Greg M. Allenby, and Robert McCulloch. Bayesian Statistics and Marketing. Wiley\nSeries in Probability and Statistics. John Wiley and Sons, Chichester, UK, 2005.\nDonald B. Rubin. Bayesianly justifiable and relevant frequency calculations for the applied statistician. Annals of Statistics, 12(4):1151\u20131172, 1984.\nJ. Sethuraman. A constructive definition of dirichlet priors. Statistica Sinica, 4(2):639\u2013650, 1994.\nA.F.M. Smith and A.E. Gelfand. Bayesian statistics without tears: A sampling-resampling perspective. The American Statistician, 46(2):84\u201388, 1992.\nRiitta Toivonen, Lauri Kovanen, Mikko Kivel\u00e4, Jukka-Pekka Onnela, Jari Saram\u00e4ki, and Kimmo\nKaski. A comparative study of social network models: Network evolution models and nodal\nattribute models. Social Networks, 31:240\u2013254, 2009.\nMarshall van Alstyne and Erik Brynjolfsson. Global village or cyber-balkans? modeling and\nmeasuring the integration of electronic communities. Management Science, 51(6):851\u2013868, June\n2005.\nDuncan J. Watts. Networks, dynamics and the small-world phenomenon. American Journal of\nSociology, 105(2):493\u2013527, Sept. 1999.\nDuncan J. Watts and Peter Sheridan Dodds. Influentials, networks and public opinion formation.\nJournal of Consumer Research, 34:441\u2013458, December 2007.\nDuncan J. Watts and Steven H. Strogatz. Collective dynamics of \"small-world\" networks. Nature,\n393:440\u201342, June 1998.\n39\n\n\fMichel Wedel and Jie Zhang. Analyzing brand competition across subcategories. Journal of Marketing Research, 41(4):448\u2013456, Nov. 2004.\nRobert E. Witt and Grady D. Bruce. Group influence and brand choice congruence. Journal of\nMarketing Research, 9(4):440\u2013443, Nov. 1972.\nSha Yang and Greg M. Allenby. Modeling interdependent consumer preferences. Journal of Marketing Research, 40(3):282\u2013294, 2003.\n\n40\n\n\f"}