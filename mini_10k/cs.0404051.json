{"id": "http://arxiv.org/abs/cs/0404051v1", "guidislink": true, "updated": "2004-04-24T14:16:04Z", "updated_parsed": [2004, 4, 24, 14, 16, 4, 5, 115, 0], "published": "2004-04-24T14:16:04Z", "published_parsed": [2004, 4, 24, 14, 16, 4, 5, 115, 0], "title": "Knowledge And The Action Description Language A", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0404055%2Ccs%2F0404019%2Ccs%2F0404036%2Ccs%2F0404039%2Ccs%2F0404014%2Ccs%2F0404046%2Ccs%2F0404034%2Ccs%2F0404051%2Ccs%2F0404017%2Ccs%2F0404008%2Ccs%2F0404016%2Ccs%2F0404022%2Ccs%2F0404026%2Ccs%2F0404013%2Ccs%2F0404027%2Ccs%2F0404056%2Ccs%2F0404018%2Ccs%2F0404020%2Ccs%2F0404002%2Ccs%2F0404023%2Ccs%2F0404033%2Ccs%2F0404021%2Ccs%2F0404028%2Ccs%2F0404057%2Ccs%2F0404003%2Ccs%2F0404015%2Ccs%2F0404052%2Ccs%2F0404038%2Ccs%2F0404044%2Ccs%2F0404007%2Ccs%2F0404049%2Ccs%2F0404009%2Chep-ex%2F0303038%2Chep-ex%2F0303043%2Chep-ex%2F0303016%2Chep-ex%2F0303032%2Chep-ex%2F0303017%2Chep-ex%2F0303033%2Chep-ex%2F0303027%2Chep-ex%2F0303041%2Chep-ex%2F0303037%2Chep-ex%2F0303006%2Chep-ex%2F0303012%2Chep-ex%2F0303007%2Chep-ex%2F0303031%2Chep-ex%2F0303046%2Chep-ex%2F0303001%2Chep-ex%2F0303018%2Chep-ex%2F0303034%2Chep-ex%2F0303004%2Chep-ex%2F0303002%2Chep-ex%2F0303003%2Chep-ex%2F0303014%2Chep-ex%2F0303013%2Chep-ex%2F0303025%2Chep-ex%2F0303042%2Chep-ex%2F0303035%2Chep-ex%2F0303019%2Chep-ex%2F0303028%2Chep-ex%2F0303011%2Chep-ex%2F0303026%2Chep-ex%2F0303039%2Chep-ex%2F0303040%2Chep-ex%2F0303020%2Chep-ex%2F0303005%2Chep-ex%2F0303022%2Chep-ex%2F0303029%2Chep-ex%2F0303009%2Chep-ex%2F0303015%2Chep-ex%2F0303030%2Chep-ex%2F0303036%2Chep-ex%2F0303023%2Chep-ex%2F0303024%2Chep-ex%2F0303010%2Chep-ex%2F0303008%2Chep-ex%2F0303021%2Chep-ex%2F0303045%2Chep-ex%2F0303044%2Chep-ex%2F0201007%2Chep-ex%2F0201032%2Chep-ex%2F0201008%2Chep-ex%2F0201017%2Chep-ex%2F0201034%2Chep-ex%2F0201004%2Chep-ex%2F0201031%2Chep-ex%2F0201002%2Chep-ex%2F0201044%2Chep-ex%2F0201014%2Chep-ex%2F0201027%2Chep-ex%2F0201013%2Chep-ex%2F0201011%2Chep-ex%2F0201024%2Chep-ex%2F0201025%2Chep-ex%2F0201010%2Chep-ex%2F0201023%2Chep-ex%2F0201021%2Chep-ex%2F0201012%2Chep-ex%2F0201042%2Chep-ex%2F0201022%2Chep-ex%2F0201036%2Chep-ex%2F0201043&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Knowledge And The Action Description Language A"}, "summary": "We introduce Ak, an extension of the action description language A (Gelfond\nand Lifschitz, 1993) to handle actions which affect knowledge. We use sensing\nactions to increase an agent's knowledge of the world and non-deterministic\nactions to remove knowledge. We include complex plans involving conditionals\nand loops in our query language for hypothetical reasoning. We also present a\ntranslation of Ak domain descriptions into epistemic logic programs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0404055%2Ccs%2F0404019%2Ccs%2F0404036%2Ccs%2F0404039%2Ccs%2F0404014%2Ccs%2F0404046%2Ccs%2F0404034%2Ccs%2F0404051%2Ccs%2F0404017%2Ccs%2F0404008%2Ccs%2F0404016%2Ccs%2F0404022%2Ccs%2F0404026%2Ccs%2F0404013%2Ccs%2F0404027%2Ccs%2F0404056%2Ccs%2F0404018%2Ccs%2F0404020%2Ccs%2F0404002%2Ccs%2F0404023%2Ccs%2F0404033%2Ccs%2F0404021%2Ccs%2F0404028%2Ccs%2F0404057%2Ccs%2F0404003%2Ccs%2F0404015%2Ccs%2F0404052%2Ccs%2F0404038%2Ccs%2F0404044%2Ccs%2F0404007%2Ccs%2F0404049%2Ccs%2F0404009%2Chep-ex%2F0303038%2Chep-ex%2F0303043%2Chep-ex%2F0303016%2Chep-ex%2F0303032%2Chep-ex%2F0303017%2Chep-ex%2F0303033%2Chep-ex%2F0303027%2Chep-ex%2F0303041%2Chep-ex%2F0303037%2Chep-ex%2F0303006%2Chep-ex%2F0303012%2Chep-ex%2F0303007%2Chep-ex%2F0303031%2Chep-ex%2F0303046%2Chep-ex%2F0303001%2Chep-ex%2F0303018%2Chep-ex%2F0303034%2Chep-ex%2F0303004%2Chep-ex%2F0303002%2Chep-ex%2F0303003%2Chep-ex%2F0303014%2Chep-ex%2F0303013%2Chep-ex%2F0303025%2Chep-ex%2F0303042%2Chep-ex%2F0303035%2Chep-ex%2F0303019%2Chep-ex%2F0303028%2Chep-ex%2F0303011%2Chep-ex%2F0303026%2Chep-ex%2F0303039%2Chep-ex%2F0303040%2Chep-ex%2F0303020%2Chep-ex%2F0303005%2Chep-ex%2F0303022%2Chep-ex%2F0303029%2Chep-ex%2F0303009%2Chep-ex%2F0303015%2Chep-ex%2F0303030%2Chep-ex%2F0303036%2Chep-ex%2F0303023%2Chep-ex%2F0303024%2Chep-ex%2F0303010%2Chep-ex%2F0303008%2Chep-ex%2F0303021%2Chep-ex%2F0303045%2Chep-ex%2F0303044%2Chep-ex%2F0201007%2Chep-ex%2F0201032%2Chep-ex%2F0201008%2Chep-ex%2F0201017%2Chep-ex%2F0201034%2Chep-ex%2F0201004%2Chep-ex%2F0201031%2Chep-ex%2F0201002%2Chep-ex%2F0201044%2Chep-ex%2F0201014%2Chep-ex%2F0201027%2Chep-ex%2F0201013%2Chep-ex%2F0201011%2Chep-ex%2F0201024%2Chep-ex%2F0201025%2Chep-ex%2F0201010%2Chep-ex%2F0201023%2Chep-ex%2F0201021%2Chep-ex%2F0201012%2Chep-ex%2F0201042%2Chep-ex%2F0201022%2Chep-ex%2F0201036%2Chep-ex%2F0201043&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We introduce Ak, an extension of the action description language A (Gelfond\nand Lifschitz, 1993) to handle actions which affect knowledge. We use sensing\nactions to increase an agent's knowledge of the world and non-deterministic\nactions to remove knowledge. We include complex plans involving conditionals\nand loops in our query language for hypothetical reasoning. We also present a\ntranslation of Ak domain descriptions into epistemic logic programs."}, "authors": ["Jorge Lobo", "Gisela Mendez", "Stuart R. Taylor"], "author_detail": {"name": "Stuart R. Taylor"}, "author": "Stuart R. Taylor", "arxiv_comment": "Appeared in Theory and Practice of Logic Programming, vol. 1, no. 2,\n  2001", "links": [{"href": "http://arxiv.org/abs/cs/0404051v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0404051v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "D.1.6; D.3.2", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0404051v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0404051v1", "journal_reference": "Theory and Practice of Logic Programming, vol. 1, no. 2, 2001", "doi": null, "fulltext": "arXiv:cs/0404051v1 [cs.AI] 24 Apr 2004\n\nUnder consideration for publication in Theory and Practice of Logic Programming\n\n1\n\nKnowledge and the Action Description Language\nA\nJORGE LOBO\u2217\nNetwork Computing Research Department, Bell Laboratories, Murray Hill, NJ 07974, USA\n(e-mail: jlobo@research.bell-labs.com)\n\nGISELA MENDEZ\u2020\nDepartamento de Matem\u00e1ticas, Universidad Central de Venezuela, Caracas, Venezuela\n(e-mail: gmendez@ciens.ucv.ve)\n\nSTUART R. TAYLOR\nRaytheon Systems Company, Expeditionary Warfare & Industrial Automotive,\n13532 N. Central Exp., MS 37, Dallas, TX, 75243, USA\n(e-mail: s-taylor5@ti.com)\n\nAbstract\nWe introduce Ak , an extension of the action description language A (Gelfond & Lifschitz, 1993)\nto handle actions which affect knowledge. We use sensing actions to increase an agent's\nknowledge of the world and non-deterministic actions to remove knowledge. We include\ncomplex plans involving conditionals and loops in our query language for hypothetical\nreasoning. We also present a translation of Ak domain descriptions into epistemic logic\nprograms.1\n\n1 Introduction\nSince its introduction, the action description language A has served as a platform\nto study several aspects that arise when we try to formalize theories of actions\nin logic (Gelfond & Lifschitz, 1993). A was designed as a minimal core of a high\nlevel language to represent and reason about actions and their effects. Domain\ndescriptions written in this language have direct translations into extended logic\nprograms. Extensions to A have been developed to study and reason about the concurrent execution of actions (Baral & Gelfond, 1997), the non-deterministic effects\nof some actions (Thielscher, 1994) and to study many instances of the qualification\nand ramification problems (Kartha & Lifschitz, 1994), (Kartha & Lifschitz, 1997),\n(McCain & Turner., 1997).\nIn this paper we propose a new action description language called Ak . Ak is a\n\u2217 Partially funded by Argonne National Laboratory under Contract No. 963042401. The research\nwas partially conducted at the EECS department of the University of Illinois at Chicago.\n\u2020 Work done while visiting the University of Illinois at Chicago and Bell Labs.\n1 This paper extends the results of the work first presented in (Lobo et al., 1997).\n\n\f2\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nminimal extension of A to handle sensing actions. A sensing action is an action that\ndoes not have any effect in the world. The effect is only in the perception of the\nreasoning agent about the world. The execution of a sensing action will increase\nthe agent's knowledge about the current state of the world. Take for example a\ndeactivated agent placed inside a room. The agent has duties to carry out and will\nbe activated by a timer. Let us assume the agent is always placed facing the door.\nThe agent, once activated, may become damaged if it attempts to leave the room\nand the door is closed. Before the agent tries to leave the room it needs to perform\nsome act of sensing in order to determine whether the door is opened or not. The\nagent has incomplete knowledge with respect to the door. A sensing action such as\nlooking at the door would provide information to the agent concerning the status\nof the door.\nIn our simple model there will be two sources of knowledge available to an agent:\ninitial knowledge, i.e., knowledge provided to the agent at initialization time, and\nknowledge gained from sensing actions. We will assume that the agent is acting\nin isolation. Thus, once an agent has gained knowledge about its world, only its\nactions or limitations of its reasoning mechanism (such as limited memory) could\nmake the agent lose knowledge. We will assume an ideal agent and expect that only\nactions can remove knowledge. An action can cause the loss of knowledge if its effect\nis non-deterministic. Take for example the action of tossing a coin. We know it will\nland with either heads showing or with tails showing, but exactly which cannot be\npredicted. Non-deterministic actions and sensing actions have opposite effects on\nan agent's knowledge.\nThe main contributions of this paper are:\n\n\u2022 The language Ak , which incorporates sensing and non-deterministic actions.\n\u2022 A query sub-language with complex plans that allow hypothetical reasoning in the presence of incomplete information. These complex plans include\nconditionals (if-then-else) and routines (while-do).\n\u2022 A sound and complete translation of domain descriptions written in Ak into\nepistemic logic programs.\n\nThe rest of this paper is organized as follows. In Section 2, we start with the\nsyntax and semantics of domains with deterministic and sensing actions only. Section 3 presents the query sub-language of Ak with conditional plans. In Section 4,\nthe language is extended to include non-deterministic actions and Section 5 adds\nloops to the query language. Section 6 gives an outline of epistemic logic programs\nas they pertain to Ak . In Section 7, we present the translation of domains in Ak into\nepistemic logic programs. In Section 8, we discuss how our work relates to other\nwork in the field. Section 9 presents a few directions for future work and concluding\nremarks.\n\n\fKnowledge and the Action Description Language A\n\n3\n\n2 Ak : Domain Language\n2.1 Syntax of Ak\nThe language of Ak consists of two non-empty disjoint sets of symbols F , A. They\nare called fluents, and actions. As in A, fluents are statements or observations about\nthe world. The set A consists of two disjoint sets of actions, sensing actions and\nnon-sensing actions. Actions will be generically denoted by a, possibly indexed. A\nfluent literal is a fluent or a fluent preceded by a \u00ac sign. A fluent literal is negative\nwhen preceded by \u00ac and is positive otherwise. Fluent literals will be denoted by f ,\np and q possibly indexed.\nThere are three kinds of propositions in Ak , object effect propositions, value\npropositions and non-deterministic effect propositions. We discuss non-deterministic\neffect propositions in Section 4.\nObject effect propositions are expressions of the form\na causes f if p1 , . . . , pn\n\n(1)\n\nwhere a is a non-sensing action, and f and p1 , . . . , pn , with n \u2264 0, are fluent literals.\nThis expression intuitively means that in a situation where p1 , . . . , pn are true, the\nexecution of a causes f to become true.\nWhen n = 0 in the preconditions of (1) we will write the proposition as\na causes f\n\n(2)\n\nA value proposition is an expression of the form\ninitially f\n\n(3)\n\nwhere f denotes a fluent literal. Value propositions describe the initial knowledge\nthe agent has about the world.\nThere are also knowledge laws. Knowledge laws are expressions of the form\nas causes to know f if p1 , . . . , pn\n\n(4)\n\nwhere as is a sensing action, f is a fluent and p1 , . . . , pn are preconditions as in\n(1). Intuitively this expression says that in a situation where p1 , . . . , pn are true the\nexecution of as causes the agent to realize the current value of f in the world. We\ndo not allow sensing actions to occur in any effect proposition.\nIf n = 0 in (4), we will write the knowledge law as\nas causes to know f\n\n(5)\n\nAt this point we should remark that we are assuming the agent may have incomplete but always correct knowledge about the world. Propositions and laws in\nAk describe how the knowledge of the agent changes, but if these changes are the\nresult of propositions like (1) we assume that the effects in the world would be the\nsame as if the world were in a state where p1 , . . . , pn are true, that is, there are not\nexternal entities that modify the world and the specification of the laws are correct\nand deterministic.\nDefinition 2.1\n\n\f4\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nA collection of the above propositions and laws is called a domain description. A\ndomain description D is simple if for any sensing action as and any fluent f there\nexists at most one knowledge law in D of type (4).\nThe following example illustrates how knowledge laws can be used to reason about\nactions.\nExample 2.2\nA robot is instructed to replace the bulb of a halogen lamp. If the lamp is on when\nthe bulb is screwed in, the robot's circuits will get burned out from the heat of the\nhalogen bulb, and it will not be able to complete the task. The robot will have to\nfind a sequence of actions that will allow it to complete the task without burning\nout. We assume that the robot is already at the lamp. This is represented by the\nfollowing domain description,\n\uf8f1\n\uf8f4\nr : initially \u00acburnOut\n\uf8f4\n\uf8f4 1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4 r2 : initially \u00acbulbF ixed\n\uf8f2\nr3 : changeBulb causes burnOut if switchOn\nD1\n\uf8f4\nr\n4 : changeBulb causes bulbF ixed if \u00acswitchOn\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nr\n5 : turnSwitch causes switchOn if \u00acswitchOn\n\uf8f4\n\uf8f4\n\uf8f3\nr6 : turnSwitch causes \u00acswitchOn if switchOn\n\nIt follows from D1 that in the initial state the robot does not know the state of the\nswitch in the lamp. Hence, there does not exist a way to determine before hand\nwhat will be the result of the action changeBulb. When the robot goes to carry\nout the action changeBulb, it could end up in a resulting state in which either\nbulbF ixed is true or in a state where it will be burned out and unable to complete\nthe task. Without knowing whether the switch is on or off, the robot will not be\nable to find a plan to accomplish its task. The robot must first check the state of the\nswitch. After realizing whether the switch is on or off, it will take the appropriate\nactions to complete the task. The robot will need a knowledge law such as:\nr7 : checkSwitch causes to know switchOn if \u00acburnOut\nAfter checking the switch the robot will know whether the switch is on or off.\nSensing gives the robot that extra knowledge it would need to accomplish the task\nwithout burning out and provides a branching point in its hypothetical reasoning.\nIf the switch is on it will turn the switch and replace the bulb. If the switch is off\nit will directly replace the bulb. This conditional reasoning will enable the robot to\nshow that there is a sequence of actions to accomplish the task.\n2.2 Semantics of Ak\nThe semantics of Ak must describe how an agent's knowledge changes according to\nthe effects of actions defined by a domain description. We begin by presenting the\nstructure of an agent's knowledge. We will represent the knowledge of an agent by\na set of possibly incomplete worlds in which the agent believes it can be. We call\nthese worlds situations and a collection of worlds an epistemic state. A situation,\nsince it could be an incomplete description of the world, will be represented by a\n\n\fKnowledge and the Action Description Language A\n\nwet\nrain\n\nwet\nrain\n\n5\n\nstate\nsituation\n\nwet\n\n(a) Agent A\n\nwet\n\nepistemic state\n\n(b) Agent B\n\nFig. 1. Epistemic states for Agent A and Agent B.\ncollection of sets of fluents. A set of fluents will be called a state. If a formula is\ntrue in an epistemic state of an agent (to be defined later), by our assumption it\nmeans that the agent knows that the formula is true in the real world. Epistemic\nstates will also allow us to distinguish when the agent knows that the disjunction\nf1 \u2228 f2 is true from when it either knows f1 or knows f2 .2\nWe will say that a fluent f is true or holds in a state \u03c3 (denoted by \u03c3 |= f ) iff\nf \u2208 \u03c3. A fluent f does not hold in a state \u03c3 (denoted by \u03c3 6|= f ) iff f 6\u2208 \u03c3. \u03c3 |= \u00acf\niff \u03c3 6|= f . For more complex formulas, their truth value can be recursively defined\nas usual. A formula \u03c6 made of fluents is true in (or modeled by) a situation \u03a3\n(denoted by \u03a3 |= \u03c6) if the formula is true in every state in \u03a3; it is false if \u00ac\u03c6 is true\nin every state \u03a3. A formula is true in an epistemic state if is true in every situation\nin the epistemic state; it is false if its negation is true.\nA situation is consistent if it is non-empty; otherwise it is inconsistent . A situation\nis complete if it contains a single state; otherwise it is incomplete. An epistemic\nstate is inconsistent if it is empty or contains an inconsistent situation; otherwise\nit is consistent. An epistemic state is complete if it contains only one complete\nsituation. Figure 1 shows two consistent epistemic states in which the fact \"Ollie is\nwet \" (represented by wet) is known by Agent A and Agent B. In the epistemic state\n(a), containing an incomplete situation, Agent A does not have knowledge about\nthe weather. In the other epistemic state (b), containing two complete situations,\nAgent B either knows it is raining or knows that it is not raining outside. Recall\nthat epistemic states will be used in the context of plans for hypothetical reasoning.\nThat is, predicting properties if the plan were executed. Thus, if an agent plans to\nexecute a series of actions that takes it to the epistemic state (a), it will not know\nhow to dress if it needs to go outside and does not want to get wet. In the epistemic\nstate (b), the agent will know how to proceed.\nInterpretations for Ak are transition functions that map pairs of actions and\nsituations into situations. To define when an interpretation models a domain de2\n\nNote the similarity with a collection of belief sets in (Gelfond & Przymusinska, 1991).\n\n\f6\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nscription, we will define an auxiliary function that interprets the effect of actions at\nthe state level. We call this function a 0-interpretation. 0-interpretations are functions that map actions and states into states3 . A 0-interpretation \u03a60 is a 0-model\nof a domain description D iff for every state \u03c3\n1. For a fluent f of any effect proposition of the form \"a causes f if p1 , . . . , pn \"\nin D, the fluent f holds in \u03a60 (a, \u03c3) if its preconditions p1 , . . . , pn holds in \u03c3,\n2. For a fluent literal \u00acf of any effect proposition of the form \"a causes \u00acf if\np1 , . . . , pn \" in D, the fluent f does not hold in \u03a60 (a, \u03c3) if its preconditions\np1 , . . . , pn holds in \u03c3,\n3. For a fluent f , if there are no effect propositions of the above types, then\nf \u2208 \u03a60 (a, \u03c3) if and only if f \u2208 \u03c3.\nBefore we define when an interpretation \u03a6 is a model of a domain description D, we\nneed the following definition that will let us interpret knowledge laws. The interest\nof the defintion will become clear after we explore the scenarion in Example 2.4.\nDefinition 2.3\nLet \u03a3 be a consistent situation, f a fluent and \u03c6 a disjunction of conjunctions of\nfluent literals (preconditions). A consistent situation \u03a3\u2032 is \"f, \u03c6-compatible\" with\n\u03a3 iff \u03a3\u2032 = \u03a3 whenever f is either true or false in \u03a3. Otherwise \u03a3\u2032 must satisfy one\nof the following conditions:\n1. \u03a3\u2032 = {\u03c3 \u2208 \u03a3 | \u03c6 is not true in \u03c3}\n2. \u03a3\u2032 = {\u03c3 \u2208 \u03a3 | \u03c6 is true in \u03c3, f 6\u2208 \u03c3}\n3. \u03a3\u2032 = {\u03c3 \u2208 \u03a3 | \u03c6 is true in \u03c3, f \u2208 \u03c3}\nExample 2.4\nLet us return to the agent scenario from the introduction. Imagine that currently\nthe agent is deactivated in the room. The agent will be automatically activated\nby an internal clock. Then it needs to find the door, leave the room, and perform\nsome duties. When the agent is initially activated it will know nothing about its\nsurroundings and will remain ignorant of its surroundings until it performs a sensing\naction. We will show how the conditions presented in Definition 2.3 are enough to\nrepresent the result of sensing. Its only action is to look. We assume the action\nconsist of opening its \"eyes\" and looking. This domain is represented below with\nonly one knowledge law,\n\b\nD2 r1 : look causes to know doorOpened if f acingDoor\n\nThis initial situation of complete ignorance is represented by the situation\n{{}, {doorOpened}, {f acingDoor}, {doorOpened, f acingDoor}}.\nIf the action look is executed in the real world the agent may find that it is not facing\nthe door and will not know whether the door is opened or not, this is represented\nby the situation\n{ { }, {doorOpened} }.\n3\n\n0-interpretations and 0-models are similar to interpretations and models for domains in A.\n\n\fKnowledge and the Action Description Language A\n\n7\n\nAnother possibility could be that the agent was facing the door and after it is\nactivated, it will know that it is facing the door and will also know that the door\nis not opened\n{ {f acingDoor} }.\nStill another possibility could be that the agent was facing the door and after being\nactivated, it will learn that it is facing the door and that the door is opened\n{ {doorOpened, f acingDoor} }.\nSince the agent will be doing hypothetical reasoning (i.e. planning) it will have no\nway of knowing which situation it will be in until the action is actually executed.\nThus, the agent can only assume that it will be in one of the three situations,\nso when the agent analyzes what would be the consequences of executing look it\nconcludes that the result will take it to the epistemic state that consists of the\nfollowing three situations.\n1. { { }, {doorOpened} }\n2. { {f acingDoor} }\n3. { {doorOpened, f acingDoor} }\nEach situation is doorOpened, f acingDoor \u2212 compatible. The first situation corresponds to the first case of Definition 2.3. The agent knows it is not facing the door\nsince f acingDoor is false in all states contained in the situation. The same cannot\nbe said for doorOpened since in one state it is false and the other state it is true.\nThis is to be expected since in this situation the agent is not facing the door, and\nit cannot know if the door is opened or closed.\nThe second situation corresponds to the second case of Definition 2.3. This situation\ncontains all the states in which the precondition f acingDoor is true and the fluent\ndoorOpen is false. The agent not only knows it is facing the door but also knows\nthe door is not opened.\nThe last situation is from the last case of Definition 2.3. In this situation the agent\nknows it is facing the door and also knows that the door is opened.\nObserve that a result of sensing is that the preconditions of the sensing action\nwill become known to the agent if the value of the fluent being sensed is initially\nunknown. This occurs even if the effect of the action remains unknown after executing the action, which hapens in the situation coming from the states where the\npreconditions of the execution of the sensing action in a knowledge laws are not\ntrue.\nDefinition 2.5\nA state \u03c3 is called an initial state of a domain description D iff for every value\nproposition of the form \"initially \u03c6\" in D, \u03c6 is true in \u03c3. The initial situation \u03a30\nof D is the set of all the initial states of D.\nDefinition 2.6\nA fluent f is a potential sensing effect of a sensing action as in a domain D if there\nis a knowledge law of the form\nas causes to know f if \u03c6\n\n\f8\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nin D. We will also say that f is the potential sensing effect of the knowledge law.\nThe knowledge precondition of a fluent f with respect to a sensing action as in a\ndomain D is the disjunction \u03c61 \u2228 . . . \u2228 \u03c6n if and only if\nas causes to know f if \u03c61\n..\n.\nas causes to know f if \u03c6n\nare all the knowledge laws in which as occurs and f is a potential sensing effect.\nNote that if the domain is simple (Definition 2.1) then the knowledge precondition\nof any fluent in the domain with respect to any sensing actions is either empty or\nit has only one disjoint.\nDefinition 2.7\nGiven an interpretation \u03a6 of Ak , \u03a6 is a model of a domain description D, if and\nonly if for any consistent situation \u03a3:\n1. There exists[a 0-model \u03a60 of D, such that for any non-sensing action a,\n\u03a6(a, \u03a3) =\n{\u03a60 (a, \u03c3)}.\n\u03c3\u2208\u03a3\n\n2. For each sensing action as , let f1 , . . . , fn be the potential sensing effects\nof as and \u03c6i the knowledge precondition of fi with respect to as . Then,\n\u03a6(as , \u03a3) must be consistent and if n = 0, \u03a6(as , \u03a3) = \u03a3 otherwise \u03a6(as , \u03a3) =\nT\ni\u2208[1..n] \u03a3i , such that each \u03a3i is a situation fi , \u03c6i \u2212 compatible with \u03a3.\n\u03a6(a, \u03a3) = \u2205 for any action a if \u03a3 = \u2205.\nExample 2.8\nThe third floor agent of a building has the job of making sure the white-board in a\nroom on that floor is clean. The agent will approach the room, look into the room,\nclean the white-board if it is not clean, and then leave the room. We focus here on\n\"looking into the room\". When the agent looks into the room it will know whether\nthe white-board in that room is clean. Also if the curtains are open the agent will\nlearn whether it is raining outside. Sensing actions can not appear in object effect\npropositions, but there is no restriction on the number of knowledge laws associated\nwith a sensing action. Thus, the action could affect the truth value of several fluents\nsimultaneously. In this example the sensing action lookInRoom will appear in two\nknowledge laws. We will see how the resulting situations are f, \u03c6-compatible with\nthe initial situation and briefly discuss the models of this domain description. The\nfollowing simple domain description illustrates the scenario,\n\uf8f1\nr1\n\uf8f4\n\uf8f4\n\uf8f2\nr2\nD3\n\uf8f4 r3\n\uf8f4\n\uf8f3\nr4\n\n: initially curtainOpen\n: initially lightOn\n: lookInRoom causes to know rainOutside if curtainOpen\n: lookInRoom causes to know boardClean if lightOn\n\n\fKnowledge and the Action Description Language A\n\n9\n\nThe initial situation \u03a30 of D3 has four states.4\n\u03a30 = { {curtainOpen, lightOn},\n{rainOutside, curtainOpen, lightOn},\n{boardClean, curtainOpen, lightOn},\n{rainOutside, boardClean, curtainOpen, lightOn}}\nThere is only one action in D3 , and any model of D3 applied to the initial situation\n\u03a30 may behave in one of the following forms:\n\u03a61 (lookInRoom, \u03a30 ) = {{curtainOpen, lightOn}}\n\u03a62 (lookInRoom, \u03a30 ) = {{rainOutside, curtainOpen, lightOn}}\n\u03a63 (lookInRoom, \u03a30 ) = {{boardClean, curtainOpen, lightOn}}\n\u03a64 (lookInRoom, \u03a30 ) = {{rainOutside, boardClean, curtainOpen, lightOn}}\nModels may differ in how they behave when they are applied to other situations\ndifferent to \u03a30 , but for \u03a30 they must be equal to one of the \u03a6i above. Unlike\ndomains in A in which given an initial situation there is only one model for the\ndomain, our language allows for several models.\nObserve too that since lookInRoom is a sensing action, its occurrence does not\nchange any fluent's value. If we start from \u03a30 , and then reach one of the four\nsituations, any new execution of lookInRoom will result in the same situation.\nTo verify that each of the \u03a6i can be a partial description of a model of r3 and\nr4 , let\n\u03a31 = { {curtainOpen, lightOn}, {boardClean, curtainOpen, lightOn}}\n\u03a32 = { {rainOutside, curtainOpen, lightOn},\n{rainOutside, boardClean, curtainOpen, lightOn}}\n\u03a33 = { {curtainOpen, lightOn}, {rainOutside, curtainOpen, lightOn}}\n\u03a34 = { {boardClean, curtainOpen, lightOn},\n{rainOutside, boardClean, curtainOpen, lightOn}}\nNote that \u03a31 and \u03a32 are rainOutside, curtainOpen-compatible with \u03a30 , and that\n\u03a33 and \u03a34 are boardClean, lightOn-compatible with \u03a30 , and\n\u03a61 (lookInRoom, \u03a30 ) = \u03a31 \u2229 \u03a33\n\u03a62 (lookInRoom, \u03a30 ) = \u03a32 \u2229 \u03a33\n\u03a63 (lookInRoom, \u03a30 ) = \u03a31 \u2229 \u03a34\n\u03a64 (lookInRoom, \u03a30 ) = \u03a32 \u2229 \u03a34\nNote also that none of the situations are f, \u03c6-compatible with \u03a30 by part (1) of\nDefinition 2.3 because there is no knowledge precondition \u03c6 of either rainOutside\nor boardClean with respect to lookInRoom in the domain description D3 that is\nfalse in any of the states in the initial situation \u03a30 .\n\n4\n\nObserve that the initial epistemic state of the robot has always a single situation. To be able to\nspecify more complex initial epistemic states the language must be changed.\n\n\f10\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n3 Ak : Query language\u2013part I\n\nGiven a domain description, an agent would like to ask how the world would be\nafter the execution of a sequence of actions starting from the initial situation. Using\nactions as in A, queries in Ak can be of the form\n\u03c6 after [a1 , . . . , an ]\n\n(6)\n\nwhere \u03c6 is a conjunction of fluent literals. The answer to this query will be yes (or\ntrue) in a domain D if for every model \u03a6 of D the test condition \u03c6 is true in the\nsituation\n\u03a6(an , \u03a6(an\u22121 , . . . \u03a6(a1 , \u03a30 ) * * *))\ni.e. the situation that results after the execution of a1 , . . . , an from the initial situation \u03a30 of D. The answer will be no (or false) if for every model \u03a6 of D \u03c6 is\nfalse in \u03a6(an , \u03a6(an\u22121 , . . . \u03a6(a1 , \u03a30 ) * * *)). Otherwise the answer will be unknown.\nWith this notion we can define an entailment relation between domain descriptions\nand queries. We say that a domain D entails a query Q, denoted by D |= Q, if the\nanswer for Q in D is yes. For example, if we add initially switchOn to D1 , it can\nbe easily shown that\nD1 |= bulbF ixed after [turnSwitch, changeBulb]\nHowever, from the original D1 (even including r7 ) there does not exist a sequence\nof actions \u03b1 such that D1 |= bulbF ixed after \u03b1. The inferences from D1 are conditioned to the output of the sensing action: if the switch is on then the sequence\n[turnSwitch, changeBulb] will cause the light to be fixed, else the single action\n[changeBulb] will fix it. Reasoning in the presence of sensing actions requires the\nprojections to be over plans more complex than a simple sequence of actions.\nWe recursively define a plan as follows,5\n1. an empty sequence denoted by [] is a plan.\n2. If a is an action and \u03b1 is a plan then the concatenation of a with \u03b1 denoted\nby [a|\u03b1] is also a plan.\n3. If \u03c6 is a conjunction of fluent literals and \u03b1, \u03b11 and \u03b12 are plans then\n[ if \u03c6then \u03b11 |\u03b1] and [ if \u03c6then \u03b11 else \u03b12 |\u03b1] are (conditional) plans.\n4. Nothing else is a plan.\nNow we redefine a query to be a sentence of the form\n\u03c6 after \u03b1\nWhere \u03c6 is a test condition (a conjunction of fluent literals) and \u03b1 is a plan.\nExample 3.1\n5\n\nWe will use the list notation of Prolog to denote sequences.\n\n(7)\n\n\fKnowledge and the Action Description Language A\n\n11\n\n(Conditionals) Here we add the knowledge law to D1 and rename it D1\u2032 .\n\uf8f1\nr1 : initially \u00acburnOut\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nr2 : initially \u00acbulbF ixed\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 r3 : changeBulb causes burnOut if switchOn\nD1\u2032\nr4 : changeBulb causes bulbF ixed if \u00acswitchOn\n\uf8f4\n\uf8f4\n\uf8f4\nr\n5 : turnSwitch causes switchOn if \u00acswitchOn\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nr\n\uf8f4\n6 : turnSwitch causes \u00acswitchOn if switchOn\n\uf8f4\n\uf8f3\nr7 : checkSwitch causes to know switchOn if \u00acburnOut\n\nWe can define a conditional plan to fix the bulb:\nbulbF ixed after\n\n[checkSwitch,\nif \u00acswitchOn\n\nthen [changeBulb]\nelse [turnSwitch, changeBulb]].\n\nThe above query provides two alternatives for reasoning. The else clause is followed\nif the test condition is false. A conditional can be expanded to a case statement in\ngeneral when reasoning needs to be done along several different sequences of plans.\nNote that if the conditional plan was attempted before or without the sensing action\ncheckSwitch, the query may not succeed because the test condition could evaluate\nto neither true nor false but rather unknown. Sensing actions need to be executed\nbefore the conditionals to ensure the test conditions will evaluate to either true or\nfalse.\n3.1 Plan Evaluation Function and Query Entailment\nTo formally define entailment we need to define first the evaluation of a plan in\nterms of interpretations. In other words, we define how the plan will change an\ninitial situation based on an interpretation.\nDefinition 3.2\nThe plan evaluation function \u0393\u03a6 of an interpretation \u03a6 is a function such that for\nany situation \u03a3\n1. \u0393\u03a6 ([], \u03a3) = \u03a3.\n2. \u0393\u03a6 ([a|\u03b1], \u03a3) = \u0393\u03a6 (\u03b1, \u03a6(a, \u03a3)) for any action a.\n3. \u0393\u03a6 ([ if \u03c6then \u03b11 |\u03b1], \u03a3) = \u0393\u03a6 (\u03b1, \u03a3\u2032 ), where\n\uf8f1\n\uf8f2 \u0393\u03a6 (\u03b11 , \u03a3) if \u03c6 is true in \u03a3\n\u03a3\u2032 =\n\u03a3\nif \u03c6 is false in \u03a3\n\uf8f3\n\u2205\notherwise\n4. \u0393\u03a6 ([ if \u03c6then \u03b11 else \u03b12 |\u03b1], \u03a3) = \u0393\u03a6 (\u03b1, \u03a3\u2032 ), where\n\uf8f1\n\uf8f2 \u0393\u03a6 (\u03b11 , \u03a3) if \u03c6 is true in \u03a3\n\u03a3\u2032 =\n\u0393 (\u03b1 , \u03a3) if \u03c6 is false in \u03a3\n\uf8f3 \u03a6 2\n\u2205\notherwise\n\nDefinition 3.3\n\n\f12\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nA query \u03c6 after \u03b1 is entailed by a domain description D (D |= \u03c6 after \u03b1) iff for\nevery model \u03a6 of D, \u03c6 is true in \u0393\u03a6 (\u03b1, \u03a3).\nIt is easy to check that\nD1\u2032 |= bulbF ixed after\n\n[checkSwitch,\nif \u00acswitchOn\n\nthen [changeBulb]\nelse [turnSwitch, changeBulb]].\n\nIt is easy to see the task will be completed regardless of what model we are in. This\nis due in part to the combination of the sensing action and the conditional plan.\n\n4 Actions with non-deterministic effects\nThere are several different reasons why knowledge may be removed from the set of\nfacts known by the agent. There may be decay of the knowledge, difficulty accessing\nthe knowledge, or it may execute an action that makes a particular knowledge no\nlonger valid. In our description we assume an ideal agent; an agent whose knowledge persists and is not subject to any type of failure or obstacles preventing the\nquick access of its knowledge. Given this assumption, the first two possibilities for\nthe removal of knowledge are impossible. However, non-deterministic actions may\nremove knowledge. A non-deterministic action is an action in which the outcome\ncannot be predicted beforehand. An example of such an action with an unpredictable outcome is the toss of a coin. A coin on a table will show either heads or\ntails. Looking at the coin, one can gain knowledge of which side of the coin shows.\nOnce the action of tossing the coin takes place we are no longer certain of which\nside will show. The coin will land and will show either heads or tails. We will not\nknow which side shows until we do the sensing action of looking. We describe the\nremoval of knowledge as no longer knowing the truth value of a fluent.\nA non-deterministic effect proposition is an expression of the form\na may affect f if p1 , . . . , pn\n\n(8)\n\nwhere a is a non-sensing action and f is a fluent. The preconditions p1 , . . . , pn are\ndefined as in equation (1). Intuitively the proposition states that the truth value of\nf may change if a is executed in a situation where p1 , . . . , pn is true.\nWhen n = 0, equation (10) becomes\na may affect f\n\n(9)\n\nWe now re-define 0-interpretations to take into account non-deterministic actions.\nA 0-interpretation \u03a60 is a 0-model of a domain description D iff for every state \u03c3,\n\u03a60 (a, \u03c3) is such that\n1. For a fluent f of any effect proposition of the form \"a causes f if p1 , . . . , pn \"\nin D, f \u2208 \u03a60 (a, \u03c3) if p1 , . . . , pn holds in \u03c3,\n2. For a fluent literal \u00acf of any effect proposition of the form \"a causes \u00acf if\np1 , . . . , pn \" in D, the f 6\u2208 \u03a60 (a, \u03c3) if p1 , . . . , pn holds in \u03c3,\n\n\fKnowledge and the Action Description Language A\n\n13\n\n3. For a fluent f such that there are no effect propositions of the above types,\nf \u2208 \u03a60 (a, \u03c3) if and only if f \u2208 \u03c3 unless there is a non-deterministic effect\nproposition of the form \"a may affect f if p1 , . . . , pn \" for which p1 , . . . , pn\nholds in \u03c3.\nExample 4.1\nOur agent is ordered at this time to put ice from a bag into cups. The ice in the\nbag is solid. The agent needs to break the ice into pieces that are able to fit in the\ncups. The agent decides to drop the bag of ice as a means to complete the task.\n\uf8f1\n\uf8f4\nt1 : initially inHandIceBag\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nt2 : initially solidIce\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nt3 : initially noDrops\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nt4 : pickU p causes inHandIceBag if \u00acinHandIceBag\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nt5 : drop causes \u00acinHandIceBag if inHandIceBag\n\uf8f4\n\uf8f4\n\uf8f2\nt6 : drop may affect solidIce if noDrops\nD5\n\uf8f4\nt7\n: drop may affect solidIce if f ewDrops\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nt8\n:\ndrop causes f ewDrops if noDrops\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nt9\n:\ndrop causes enoughDrops if f ewDrops\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nt10 : drop causes \u00acsolidIce if enoughDrops\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nt11\n: checkIce causes to know solidIce\n\uf8f4\n\uf8f4\n\uf8f3\nt12 : putIceInCups causes iceInCups if \u00acsolidIce\n\nThis example combines many of the ideas previously presented. Let us examine this\ndomain description to see how this all fits together.\n\n\u2022 Rules t1 - t3 establish what is initially known in the world. The values of all\nother fluents are unknown at this time.\n\u2022 Rules t4 and t5 describe the effect that theactions Drop and pickU p have on\ninHandIceBag.\n\u2022 Rules t6 and t7 describe the non-deterministic effect of the action drop on\nthe ice.\n\u2022 Rules t8 - t10 are object effect propositions which ensure that the ice will\nbreak after no more than three drops (i.e. the execution of the action drop\nthree times). In the example noDrops is equated with 0 drops, f ewDrops\nwith 1 drop, and enoughDrops with 2 drops.\n\u2022 Rule t11 is the sensing action which allows the agent to know whether the\nice is broken or not after the execution of the non-deterministic action drop.\nRule t12 is the goal of the task the agent is to perform.\nThe non-determinism appears in the action of dropping the bag of ice. Before\nthe action is carried out, the agent knows that the ice is solid. After the nondeterministic action, the agent is no longer certain if the ice is still solid or in\npieces. The knowledge of knowing the ice is solid has been removed. The agent can\nonly regain that knowledge by performing a sensing action.\nIf the robot wants to fill the cup with ice it will iterate the process of dropping\nthe ice until it breaks. A plan to accomplish this goal will look like:\nwhile \u00acsolidIce do [drop, pickup, checkIce], putIceInCups]\n\n\f14\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nAdding loops to plans is the topic of the next section.\n\n5 Ak : Query language\u2013part II\nIf we allow while-loops in our plan we could verify that D5 entails the following\nquery,\niceInCups after [while \u00acsolidIce do [drop, pickU p, checkIce], putIceInCups]\nSimilar to conditional plans, a sensing action is placed before checking the exit\ncondition of the loop. We extend the definition of plans to include loops as follows.\n1. An empty sequence denoted by [] is a plan.\n2. If a is an action and \u03b1 is a plan then the concatenation of a with \u03b1 denoted\nby [a|\u03b1] is also a plan.\n3. If \u03c6 is a conjunction of fluent literals and \u03b1, \u03b11 and \u03b12 are plans then\n[ if \u03c6 then \u03b11 |\u03b1] and [ if \u03c6 then \u03b11 else \u03b12 |\u03b1] are (conditional) plans.\n4. If \u03c6 is a conjunction of fluent literals and \u03b1 and \u03b11 are plans then\n[ while \u03c6 do \u03b11 |\u03b1] is also a (routine) plan.\n5. Nothing else is a plan.\n5.1 Plan Evaluation Function and Query Entailment\nTo extend the definition of entailment to plans with while loops we need to extend\nthe definition of the plan evaluation function \u0393\u03a6 . We will define this function using very elementary tools from denotational semantics for programming languages\n(as in Chapter 4 of (Davey & Priestley, 1990)). The intuitive idea of the denotational semantics is to associate the execution of a plan (or a program) of the form\n\" while \u03c6 do \u03b1\" with one of the while-free plans:6\nif \u03c6 then \u2205\nif \u03c6 then [\u03b1, if \u03c6 then \u2205]\nif \u03c6 then [\u03b1, if \u03c6 then [\u03b1, if \u03c6 then \u2205]]\n..\n.\nIf the while-plan terminates then there exists a n such that the nth plan in this\ninfinite sequence computes exactly the same function that the while-plan computes.\nMoreover, for each m < n, the mth plan is an approximation of the computation\nof the while-plan. If the while-plan does not terminate, any plan in the sequence\nis an approximation of the while-plan but none is equivalent since the while-plan\ncomputation is infinite. Thus, to define this sequence we start by defining a partial\norder over the set of functions that map situations into situations. The order will\narrange the functions as in the sequence of plans above.\nDefinition 5.1\n6\n\nRecall that the situation \u2205 represents inconsistency.\n\n\fKnowledge and the Action Description Language A\n\n15\n\nLet E be the set of all situations and P the set of all total functions f mapping\nsituations into situations, P = {f | f : E \u2192 E }. We say that for any pair of\nfunctions f1 , f2 \u2208 P, f1 \u2264 f2 if and only if for any \u03a3 \u2208 E if f1 (\u03a3) 6= \u2205, then\nf1 (\u03a3) = f2 (\u03a3).\nThen, we associate a (continuous) transformation inside this order to each plan \u03b1.\nInformally speaking, the transformation starts with the first plan in the sequence\nand in each application returns the next element in the sequence. Finally, we will\ndefine the meaning of the plan based on the least fix-points of these transformations.\nLet f\u2205 denote the function that maps any situation into the empty situation \u2205.\nDefinition 5.2\nLet \u03b1 be a plan and \u0393 a function that maps plans and situations into situations.\n\u0393\nLet \u03c6 be a conjunction of fluent literals. Then, we define the function F\u03b1,\u03c6\n:P \u2192P\nsuch that for any\uf8f1function f \u2208 P,\nif \u03c6 is false in \u03a3\n\uf8f2 \u03a3\n\u0393\nF\u03b1,\u03c6\n(f )(\u03a3) =\nf (\u0393(\u03b1, \u03a3)) if \u03c6 is true in \u03a3\n\uf8f3\n\u2205\notherwise\n\u0393\nWe can define the powers of F\u03b1,\u03c6\nas follows:\n\n\u0393\n1. F\u03b1,\u03c6\n\u2191 0 = f\u2205 .\n\u0393\n\u0393\n\u0393\n\u2191 n).\n(F\u03b1,\u03c6\n2. F\u03b1,\u03c6 \u2191 n + 1 = F\u03b1,\u03c6\n\u0393\n\u0393\n\u0393\n\u0393\n\u0393\n\u2191 0)) . . .) . . ., i.e. the infinite compo(F\u03b1,\u03c6\n(F\u03b1,\u03c6\n3. F\u03b1,\u03c6 \u2191 \u03c9 = . . . F\u03b1,\u03c6 . . . (F\u03b1,\u03c6\n\u0393\nsition of F\u03b1,\u03c6 applied to f\u2205 .\n\nIt can be shown that this power is correctly defined. Proof and a formal definition\nof powers can be found in Appendix A.\nWe now extend the definition of the evaluation function \u0393\u03a6 to apply to plans\nwith routines by adding item\n\u0393\n\n5. \u0393\u03a6 ([ while \u03c6 do \u03b11 |\u03b1], \u03a3) = \u0393\u03a6 (\u03b1, \u03a3\u2032 ), where \u03a3\u2032 = Fif \u03a6\u03c6then\u03b11 ,\u03c6 \u2191 \u03c9\nto Definition 3.2. The definition of entailment remains unchanged. That is, D |=\n\u03c6 after \u03b1 iff for every model \u03a6 of D, \u03c6 is true in \u0393\u03a6 (\u03b1, \u03a30 ).\nExample 5.3\nD5 |= iceInCups after [\n\nwhile \u00acsolidIce do [drop, pickup, checkIce],\nputIceInCups]\n\n5.2 Plan Termination\nNotice that the query above with the while loop could have been written using\nthree nested conditionals. A more natural example will replace rules t6 \u2212 10 with\nthe single rule\ndrop may affect solidIce\nHowever, in this domain we are not be able to prove termination. The verification\nof termination is a difficult task, especially for planning. How do we really know\n\n\f16\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nthat the ice will eventually break? Or how do we know that the cup is filling up?\nWith time the ice will either melt or break, and if we do not place infinitesimally\nsmall amounts of ice in the cup the cup will eventually fill up or we will run out\nof ice. We have simplified the problem in our example by adding t6 \u2212 t10. These\npropositions state that the ice will break with no more than three \"drops\".\nWe are faced with a similar situation in the following example.\nExample 5.4\nConsider the following situation. On the floor of a room there are cans. An agent is\ngiven an empty bag and instructed to fill the bag with cans. We assume that there\nare more than enough cans on the floor to fill the bag. The domain description for\nthis task is\n\uf8f1\nr1 : initially \u00acbagF ull\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 r2 : drop causes \u00accanInHand\nD4\nr3 : drop causes canInBag if canInHand\n\uf8f4\n\uf8f4\n\uf8f4 r4 : lookInBag causes to know bagF ull\n\uf8f4\n\uf8f3\nr5 : pickU p causes canInHand if \u00accanInHand\nThis task of picking up cans and dropping them into the bag involves the repetition\nof a small sequence of actions. There is a degree of uncertainty inherent in this\ntask because it is unknown how many cans are needed to fill the bag. Therefore a\nloop that executes the sequence of actions repeatedly until the task is completed is\nneeded. If the number of cans needed to fill the bag is known beforehand, then the\nset of actions would be repeated sequentially for those number of times.\nA query that we would like to prove is\nbagF ull after [lookInBag, while \u00acbagF ull do [ pickU pCan,\ndropCanInBag,\nlookInBag]]\nIdeally, we would like to use a routine which could solve any type of task that\ninvolves uncertainty of its end. However, each task has its own conditions for termination. For example, filling the volume of a bag differs from finding an unfamiliar\nstore in an unfamiliar area based on the vague directions of a stranger. Do we really\nknow the bag will become full? Or how useful are vague directions such as, \"Just\nwalk down Lincoln Avenue, you can't miss it\" when generating a plan. A hole may\ntear in the bag, or suppose that the stranger who had all the best intentions was\nmistaken about the location of the store. To ensure termination (either with success\nor failure) we need to add to our domain descriptions general axioms or constraints.\nWe do not have constraints in Ak but we may be able to add them by using other\nextensions of A such as the one in (Baral et al., 1997). To address this problem\nwe should first look at the standard techniques of problem verifications such as\nthe ones founded in (Aho & Ullman, 1995) or (Cousot, 1990). These classical ideas\nhave been used by Manna and Waldinger to prove termination of plans with loops\nbut without sensing actions (Manna & Waldinger, 1987). For sensing, it might also\nbe useful to consider the techniques described in (Geffner & Bonet, 1998) to detect\n\n\fKnowledge and the Action Description Language A\n\n17\n\nloop-termination using probability approaches. However, analysis of the termination of plans is outside the scope of this paper. We will discuss in Section 9 how\nsome of the problems of termination may be addressed in simple situations.\n6 Epistemic Logic Programs\nIn the past, domain descriptions of dialects of A have been translated into extended\nlogic programs (Baral & Gelfond, 1997; Gelfond & Lifschitz, 1993). Extended logic\nprograms use two types of negation to represent incomplete information. There\nis strong or classical negation \u00ac and negation as failure not. The semantics of\nextended logic programs is defined by a collection of sets of literals called answer\nsets (Gelfond & Lifschitz, 1991). However, we are required to represent incomplete\ninformation that crosses over multiple sets of answer sets. This will be the case in\nour translation of domain descriptions into logic programs where situations will be\nclosely related to sets of answer sets, and domain descriptions act over epistemic\nstates which are sets of situations. In this case, extended logic programs will no\nlonger be sufficient to codify domain descriptions.\nGelfond has extended disjunctive logic programs to work with sets of sets of\nanswer sets (Gelfond & Lifschitz, 1991). He calls his new programs epistemic logic\nprograms. In epistemic logic programs, the language of extended logic programs is\nexpanded with two modal operators K and M . KF is read as \"F is known to be\ntrue\" and M F is read as \"F may be believed to be true.\"\nUniversal and existential quantifiers are also allowed as well as the epistemic\ndisjunctive \"or\" which the semantics is based on the minimal model semantics\nassociated with disjunctive logic programs (Lobo et al., 1992). As an example, when\nF or G is defined as a logic program, its models are exactly F and G. Note that the\nclassical F \u2228 G cannot be defined as a logic program, because it has models which\nare not minimal.\nIn the rest of this section we will review the syntax and the semantics of the\nsubclass of epistemic logic programs that will be required to represent our domain\ndescriptions. Readers interested in more details about epistemic logic programs are\nreferred to (Gelfond, 1994).\nThe semantics of an epistemic logic program is defined by pairs hA, W i. A is a\ncollection of sets of ground literals called the set of possible beliefs. Each set in A\ncan be indexed as A = {A1 . . . An }. W is a set in A called the working set of beliefs.\nTo define the semantics, we restrict our formulas to be: ground literals, a ground\nliteral preceded by a modal operator, a ground literal preceded by a modal operator\nand \u00ac, or a conjunction of such formulas. The truth of a formula F in hA, W i is\ndenoted by hA, W i |= F and the falsity by hA, W i =|F , and are defined as follows.\nhA, W i |= F iff F \u2208 W , when F is a ground atom.\nhA, W i |= KF iff hA, Ai i |= F, \u2200Ai \u2208 A.\nhA, W i |= F \u2227 G iff hA, W i |= F and hA, W i |= G.\nhA, W i |= \u00acF iff hA, W i =| F .\n\n\f18\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nhA, W i =| F iff \u00acF \u2208 W , when F is a ground atom.\nhA, W i =| KF iff hA, W i 6|= KF\nhA, W i =| F \u2227 G iff hA, W i =|F or hA, W i =|G.\nhA, W i =| \u00acF iff hA, W i |= F .\nhA, W i |= F or G iff hA, W i |= \u00ac(\u00acF \u2227 \u00acG)\nNote that when a formula G is of the form KF , or \u00acKF , its evaluation in hA, W i\ndoes not depend on W . Thus, we will write A |= G or A = | G. Moreover, the\nevaluation of object formulas does not depends on A. If G is objective we sometimes\nwrite W |= G or W =| G.\nAn epistemic logic program is a collection of rules of the form\nF1 or . . . or Fk \u2190 G1 , . . . , Gm , not Fm+1 , . . . , not Fn\n\n(10)\n\nwhere F1 . . . Fk and Fm+1 . . . Fn are (not necessarily ground) objective literals\n(without K or M ) and G1 . . . Gm are (not necessarily ground) subjective (with\nK or M ) or objective literals.\nLet \u03a0 be an epistemic logic program without variables, not, or modal operators. A\nset W of ground literals is a belief set of \u03a0 if it is a minimal set of ground literals,\nsatisfying the following properties:\n1. W |= F for every rule F \u2190 G1 . . . Gm in \u03a0 which W |= G1 \u2227 . . . \u2227 Gm .\n2. If there is a pair of complementary literals, i.e F and \u00acF , in W then W is\nthe set of all literals.\nLet \u03a0 be an epistemic logic program with not and variables but does not contain\nany modal operator. Let Ground(\u03a0) be the epistemic logic program that is obtained\nfrom \u03a0 by replacing each rule in \u03a0 with all its ground instances. Let W be a set of\nground literals ( literals in W and \u03a0 are from the same language). \u03a0W is obtained\nfrom \u03a0 by removing from Ground(\u03a0)\n1. All the rules which contain formulas of the form not G such that W |= G.\n2. All occurrences of formulas of the form not G from the remaining rules.\nW is a belief set of \u03a0 if and only if W is a belief set of \u03a0W .\nLet \u03a0 be any epistemic logic program, and A a collection of sets of literals. [\u03a0]A\nis the epistemic logic program obtained by removing from Ground(\u03a0)\n1. All rules with formulas of the form G such that G contains M or K, and\nA 6|= G,\n2. All occurrences of formulas containing M or K from the remaining rules.\nA set A is a world view of \u03a0 if A is the collection of all belief sets of [\u03a0]A . A world\nview of \u03a0 is consistent if it does not contain the belief set of all literals. An epistemic\nlogic program is consistent if it has at least one consistent non-empty world view.\nIn epistemic logic programs the only working sets of beliefs that are considered are\n\n\fKnowledge and the Action Description Language A\n\n19\n\nworld views and the possible belief is always a member of the working set under\nconsideration (i.e. a belief set).\nLet \u03a0 be an epistemic logic program and A be a world view of \u03a0. A literal L is\ntrue in A iff for every ground instance F of L, hA, Ai i |= F for all Ai in A. F is\ntrue in \u03a0, denoted by \u03a0 |= F , iff A |= F for every world view A of \u03a0.\nExample 6.1\nThe epistemic program\n1. q(a) \u2190 \u00acKp(a).\n2. p(a) \u2190 \u00acKq(a).\nhas two world views\n{{p(a)}}\n{{q(a)}}\nIn the first world view Kp(a) is true and Kq(a) is true in the second.\nThe epistemic program\n1. q(a) or q(b).\n2. p(a) \u2190 \u00acKq(a).\nhas one world view\n{{p(a), q(a)}, {p(a), q(b)}}\nNote that Kq(a) is not true in this world view because q(a) is not member of the\nsecond belief set. The main intuition to have when reading a formula of the form\nKF is that it will be true iff F is true in every belief set of the program.\n7 Translation to Epistemic Logic Programs\nIn this section we start with a sound and complete translation of simple domain\ndescriptions into epistemic logic programs. This will let us explain the logic program\nrules under the simple scenario and will make clear the rules for the general case.\nOur epistemic logic programs will use variables of three sorts: situation variables\ndenoted by S or S \u2032 possibly indexed, fluent variables denoted by F or F \u2032 possibly\nindexed, action variables denoted by A or A\u2032 possibly indexed, and the special situation constant s0 that represents the initial situation. We will also have a constant\nsymbol for each fluent symbol f in the language and we add the constant symbol\nf \u0304 to represent \u00acf . For simplicity we will denote the fluent literal constants by the\nfluent literal they represent. We will also add the special constant symbol true to\nthe set of fluent literal constants.\n7.1 The Domain Independent Translation\nWe start by first giving the rules for inertia. These rules encode that a fluent remains\nunchanged if no actions that affect the fluent is executed. Whenever a fluent literal\nappears as an argument in a predicate, it is representing a corresponding constant\nin the program. For any fluent literal l, if l = \u00acf , l\u0304 will denote f in the program.\n\n\f20\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nFor every fluent literal f there is an inertia rule of the form:\nholds(f, res(A, S)) \u2190 holds(f, S), not ab(f \u0304, A, S).\nFor every fluent symbol f there is an or-classicalization rule of the form\nholds(f, s0 ) or holds(f \u0304, s0 )\nThe above rule states that our belief sets are complete in the sense that either\nholds(f, s0 ) or holds(f \u0304, s0 ) must be true since f \u2228 \u00acf is a tautology in every state.\nNote that because of the minimal model semantics interpretation of the \"or\" we\nwill not have both holds(f, s0 ) or holds(f \u0304, s0 ) holding simultaneously.\nWe will also have two more domain independent rules that we will call rules of\nsuppression.\nholds(true, res(A, S)) \u2190 holds(true, S)\nholds(F, S) \u2190 holds(true, S)\nThese rules will be used to implement compatibility. For example, if a situation\n\u03a3 = {\u03c31 , \u03c32 } with two states is split into two situations \u03a31 = {\u03c31 } and \u03a32 = {\u03c32 },\nfor compatibility after the execution of a sensing action, \u03a31 will be generated by\nsuppressing \u03c32 from \u03a3 using these rules. How this is accomplished will become\napparent when we introduce the domain dependent rules produced by the knowledge\nlaws.\n\n7.2 The Domain Dependent Translation\nValue propositions of the form \"initially f \" are translated into\nholds(f, s0 )\nThe translation of effect propositions of the form \"a causes f if p1 , . . . , pn \" is the\nstandard translation for effect propositions introduced by Gelfond and Lifschitz in\n(Gelfond & Lifschitz, 1993) for A. The translation produces two rules. The first one\nis:\nholds(f, res(a, S)) \u2190 holds(p1 , S), . . . , holds(pn , S)\nIt allows us to prove that f will hold after the result of the execution of a if\npreconditions are satisfied. The second rule is:\nab(f, a, S) \u2190 holds(p1 , S), . . . , holds(pn , S), not holds(true, res(a, S))\nwhere the predicate ab(f, a, S) disables the inertia rule in the cases where f can be\naffected by a.\nWe will introduce the domain dependent translation of knowledge laws using the\nfollowing domain description.\nExample 7.1\n\n\fKnowledge and the Action Description Language A\n\nD10\n\n\u001a\n\n21\n\nr1 : initially \u00acbulbF ixed\nr2 : checkSwitch causes to know switchOn if \u00acburnOut\n\nIn this example the initial situation is:\n\n\u03a3 = {{burnOut, \u00acbulbF ixed, switchOn},\n{burnOut, \u00acbulbF ixed, \u00acswitchOn},\n{\u00acburnOut, \u00acbulbF ixed, switchOn},\n{\u00acburnOut, \u00acbulbF ixed, \u00acswitchOn}}\n\nafter the robot executes the action checkSwitch we will have the following resulting\nsituations:\n\n\u03a61 (checkSwitch, \u03a3) = { {\u00acburnOut, \u00acbulbF ixed, switchOn}}\n\u03a62 (checkSwitch, \u03a3) = { {\u00acburnOut, \u00acbulbF ixed, \u00acswitchOn}}\n\u03a63 (checkSwitch, \u03a3) = { {burnOut, \u00acbulbF ixed, switchOn},\n{burnOut, \u00acbulbF ixed, \u00acswitchOn}}\n\nThese correspond to the three (switchOn, \u00acburnOut)-compatible sub-sets of \u03a3 (see\nDefinition 2.3). Note also that\n\n\u03a61 (checkSwitch, \u03a61 (checkSwitch, \u03a3)) = \u03a61 (checkSwitch, \u03a3)\n\u03a62 (checkSwitch, \u03a62 (checkSwitch, \u03a3)) = \u03a62 (checkSwitch, \u03a3)\n\u03a63 (checkSwitch, \u03a62 (checkSwitch, \u03a3)) = \u03a63 (checkSwitch, \u03a3)\n\nOur logic program translation of this domain will have three world views, one\ncorresponding to each of the transition functions \u03a61 \u03a62 , and \u03a63 . \u03a61 is depicted\non the left hand side of the figure below, \u03a62 on the right hand side and \u03a63 in the\nmiddle.\n\n\f22\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\nburnOut \u00acbulbF ixed\n\nburnOut \u00acbulbF ixed\n\nswitchOn\n\n\u00acswitchOn\n\n\u00acburnOut \u00acbulbF ixed\n\n\u00acburnOut \u00acbulbF ixed\n\nswitchOn\n\n\u00acswitchOn\n\n\u03a62\n\n\u03a61\n\u03a63\n\ncheckSwitch\n\n\u03a61\n\ncheckSwitch\ncheckSwitch\n\n\u00acburnOut \u00acbulbF ixed\n\n\u00acburnOut \u00acbulbF ixed\n\nswitchOn\n\n\u00acswitchOn\n\u03a62\n\ncheckSwitch\n\n\u03a63\n\ncheckSwitch\n\nburnOut \u00acbulbF ixed\n\nburnOut \u00acbulbF ixed\n\nswitchOn\n\n\u00acswitchOn\n\ncheckSwitch\n\nThe world view associated with \u03a61 on the left hand side of the figure will have four\nbelief sets. One will contain the union of the two sets\nWs10 = {holds(burnOut, s0 ), holds(bulbF ixed, s0 ), holds(switchOn, s0 )}\nand\n1\nWres(cs,s\n={\n0)\n\nholds(burnOut, res(checkSwitch, s0 )),\nholds(bulbF ixed, res(checkSwitch, s0 ),\nholds(switchOn, res(checkSwitch, s0 ))}\n\nThis union represents the fact that {\u00acburnOut, \u00acbulbF ixed, switchOn} is an initial\nstate (encoded Ws10 ) and that the same set is also a state in \u03a61 (checkSwitch, \u03a3)\n1\n1\n(encoded in Wres(cs,s\n). The rest of the literals in W 1 are the same as in Wres(cs,s\n0)\n0)\nexcept that the situation constant in each literal is replaced by situation constants of\nthe form res(checkSwitch, res(. . . , res(checkSwitch, s0 ) . . .)) representing that the\nstate remains the same after any number of applications of the action checkSwitch\nto the state (the loop arc on the left of the figure).\n\nThe second belief set will contain\nWs20 = {holds(burnOut, s0 ), holds(bulbF ixed, s0 ), holds(switchOn, s0 )}\n\n\fKnowledge and the Action Description Language A\n\n23\n\nrepresenting that {\u00acburnOut, \u00acbulbF ixed, \u00acswitchOn} is also an initial state. However, this state is not part of \u03a61 (checkSwitch, \u03a3). Then, we need to suppress this\nstate from the world view. We will do that by adding the set\n2\nWres(cs,s\n0)\n\n= { holds(burnOut, res(checkSwitch, s0 )),\nholds(bulbF ixed, res(checkSwitch, s0 ),\nholds(switchOn, res(checkSwitch, s0 ))\nS\n}\n{ holds(burnOut, res(checkSwitch, s0 )),\nholds(bulbF ixed, res(checkSwitch, s0),\nholds(switchOn, res(checkSwitch, s0 )),\nholds(true, res(checkSwitch, s0 )}\n\nto the belief set. Actually, we will have in the domain dependent translation a rule\nthat adds holds(true, res(checkSwithc, s0 )), and the second domain independent\nsuppression rule will add the rest. The rest of the literals in W 2 are the same as\n2\nin Wres(cs,s\nexcept that the situation constant is replaced by situation constants\n0)\nof the form res(checkSwitch, res(. . . , res(checkSwitch, s0 ) . . .)) representing that\nthe state remains suppressed in the result of applying the action checkSwitch to\nthe state. This is the effect of the first domain independent suppression rule.\nThe other two belief sets W 3 and W 4 are similar to W 2 .\nWs30 = {holds(burnOut, s0 ), holds(bulbF ixed, s0 ), holds(switchOn, s0 )}\nWs40 = {holds(burnOut, s0 ), holds(bulbF ixed, s0 ), holds(switchOn, s0 )}\nThe rest of W 3 and W 4 is exactly as in W 2 since the states they represent are also\nsuppressed from the result.\nNote that both holds(f , res(checkSwitch, s0 )) and holds(f, res(checkSwitch, s0 ))\nare members of the belief sets W 2 , W 3 and W 4 , for any fluent f . Therefore, for any\nfluent literal g, the proof of holds(g, res(checkSwitch, s0 ) in the world view is not\naffected by these belief sets. The consequence is that we are ignoring three states\nafter the execution of checkSwitch under the model \u03a61 .\nThere are two more world views that correspond to the transitions in the middle\nand on the right hand side of the figure. The definition is very similar to the first\nworld view. There are four belief sets in the middle, two of them suppressing initial\nstates, and four belief sets in the last world view, three of them suppressing initial\nstates.\nThus, the domain dependent translation of D10 will be:\nholds(bulbF ixed, s0 ) \u2190\nRule x1 is the translation of rule r1 . The rest of the rules correspond to the different\nsuppression cases since states that are not suppressed by the transition will be\nmoved to the next situation by the domain independent rule of inertia. Take for\nexample, \u03a61 .\n\u03a61 (checkSwitch, \u03a3) = {\u03c3 \u2208 \u03a3|\u03c3 |= \u00acburnOut, switchOn \u2208 \u03c3}\n\n\f24\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nHence, we would like to suppress two kinds of states. 1) States where \u00acswitchOn\nis true, and 2) States where burnOut is true. The rule for the first case is:\nholds(true, res(checkSwitch, S))\n\n\u2190\n\nKholds(switchOn, res(checkSwitch, S))\nKholds(burnOut, res(checkSwitch, S)),\nholds(switchOn, S)\n\n(11)\n\nThe first two literals in the body of the rule verify that we are in the case\nof \u03a61 , that is, both switchOn and \u00acburnOut are true in every state of the resulting situation (i.e. the two literals Kholds(switchOn, res(checkSwitch, S)) and\nKholds(burnOut, res(checkSwitch, S)) are true). The last predicate checks that\nwe are suppressing the state where \u00acswitchOn is true in the current situation (i.e.\nholds(switchOn, S)).\nThe rule for the second case is very similar. We only need to change the last\nliteral to indicate that we are suppressing the state where burnOut is true (i.e.\nholds(burnOut, S)):\nholds(true, res(checkSwitch, S))\n\n\u2190\n\nKholds(switchOn, res(checkSwitch, S))\nKholds(burnOut, res(checkSwitch, S)),\nholds(burnOut, S)\n\n(12)\n\nLet us look now at \u03a62 .\n\u03a62 (checkSwitch, \u03a3) = {\u03c3 \u2208 \u03a3|\u03c3 |= \u00acburnOut, switchOn 6\u2208 \u03c3}\nWe also suppress two kinds of states. 1) States where switchOn is true, and 2) States\nwhere burnOut is true. We need to check that \u00acswitchOn and \u00acburnOut are true in\nevery state of the resulting situation (i.e. Kholds(switchOn, res(checkSwitch, S))\nand Kholds(burnOut, res(checkSwitch, S)) are true) to verify that we are in the\ncase of \u03a62 . The rules for the cases are:\nholds(true, res(checkSwitch, S))\n\n\u2190\n\nKholds(switchOn, res(checkSwitch, S))\nKholds(burnOut, res(checkSwitch, S)),\nholds(switchOn, S)\n\nholds(true, res(checkSwitch, S))\n\n\u2190\n\nKholds(switchOn, res(checkSwitch, S))\nKholds(burnOut, res(checkSwitch, S)),\nholds(burnOut, S)\n\n(13)\n\nFor \u03a63 all the states where \u00acburnOut holds (i.e. holds(burnOut, S)) should be\nsuppressed since\n\u03a63 (checkSwitch, \u03a3) = {\u03c3 \u2208 \u03a3|\u03c3 6|= \u00acburnOut}\nTo verify that we are in the case of \u03a63 we need to check there is at least one state\nin the result where burnOut holds (i.e. \u00acKholds(burnOut, res(checkSwithc, S))).\nThe rule for this case is:\nholds(true, res(checkSwitch, S))\n\n\u2190\n\n\u00acKholds(burnOut, res(checkSwitch, S))\nholds(burnOut, S)\n\n(14)\n\n\fKnowledge and the Action Description Language A\n\n25\n\nThere is a condition that must be added to all the rules. The condition is that if the\nfluent switchOn is already known in the original situation (for example if we have\ninitially \u00acswitchOn) then none of the states is suppressed from the situations. In\nother words, the rules above applied only if switchOn is unknown. To check that this\nis the case we must add to the body of each rule the literals \u00acKholds(switchOn, S)\nand \u00acKholds(switchOn, S). These literals are not required in this particular example but it must be part of the general case.\nIn general, knowledge laws of the form \"a causes to know f if p1 , . . . , pn \" are\ntranslated into the rules\nholds(true, res(a, S))\n\n\u2190\n\n\u00acKholds(f, S), \u00acKholds(f \u0304, S),\n\u00acKholds(p1 , res(a, S)),\nholds(p1 , S), . . . , holds(pn , S)\n\n..\n.\nholds(true, res(a, S))\n\n\u2190\n\n\u00acKholds(f, S), \u00acKholds(f \u0304, S),\n\u00acKholds(pn , res(a, S)),\n\nholds(true, res(a, S))\n\n\u2190\n\nholds(p1 , S), . . . , holds(pn , S)\n(15)\n\u00acKholds(f, S), \u00acKholds(f \u0304, S), Kholds(f \u0304, res(a, S)),\nKholds(p1 , res(a, S)), . . . , Kholds(pn , res(a, S)),\n(16)\n \u0304\n \u0304\n\u00acKholds(f, S), \u00acKholds(f , S), Kholds(f , res(a, S)),\nholds(f, S)\n\nholds(true, res(a, S))\n\n\u2190\n\nKholds(p1 , res(a, S)), . . . , Kholds(pn , res(a, S)),\nholds(p \u03041 , S)\n..\n.\nholds(true, res(a, S))\n\n\u2190\n\n\u00acKholds(f, S), \u00acKholds(f \u0304, S), Kholds(f \u0304, res(a, S)),\nKholds(p1 , res(a, S)), . . . , Kholds(pn , res(a, S)),\nholds(p \u0304n , S)\n\nholds(true, res(a, S))\n\nholds(true, res(a, S))\n\n\u2190\n\n(17)\n \u0304\n\u00acKholds(f, S), \u00acKholds(f , S), Kholds(f, res(a, S)),\n\n\u2190\n\nKholds(p1 , res(a, S)), . . . , Kholds(pn , res(a, S)),\nholds(f \u0304, S)\n(18)\n\u00acKholds(f, S), \u00acKholds(f \u0304, S), Kholds(f, res(a, S)),\nKholds(p1 , res(a, S)), . . . , Kholds(pn , res(a, S)),\nholds(p \u03041 , S)\n\n..\n.\nholds(true, res(a, S))\n\n\u2190\n\n\u00acKholds(f, S), \u00acKholds(f \u0304, S), Kholds(f, res(a, S)),\nKholds(p1 , res(a, S)), . . . , Kholds(pn , res(a, S)),\nholds(p \u0304n , S)\n\n(19)\n\nWe have added to every rule the condition \u00acKholds(f, S), \u00acKholds(f \u0304, S). None\nof these rules apply if f is currently known. In this case, by inertia everything stays\nthe same after the execution of the sensing action a. Assume now that neither f\nnor f \u0304 holds in the \"situation\" S. Thus, according to the definition of compatibility\n\n\f26\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\n(Def. 2.3), we would have three types of world views. 1) One type for which we can\nfind a pi for each of the belief sets such holds(pi , res(a, S)) does not hold; 2) World\nviews in which Kholds(f \u0304, res(a, S)) and every Kholds(pi , res(a, S)) hold; 3) World\nviews in which Kholds(f, res(a, S)) and every Kholds(pi , res(a, S)) hold.\nIntuitively, to capture these three cases the logic programming rules will suppress\nthe belief set that breaks the rules. To suppress a belief set in the situation res(a, S)\nthe rules will add holds(true, res(a, S)) to the belief set, and this atom together\nwith the second suppression rule will add holds(l, res(a, S)), for every fluent literal\nl. Recall that the effect of having every literal hold for a particular situation in a\nbelief set is that the belief set can be ignored when checking if the literal holds\nin the world view. Case (1) is captured by the first set of rules (15). Case (2) is\ncaptured by rule (16) and the set of rules (17). Case (3) is captured by rule (18)\nand the set of rules (19).\nA non-deterministic effect proposition of the form \"a may affect f if p1 , . . . , pn \"\nis translated into\nnot holds(f \u0304, res(a, S)),\nholds(p1 , S), . . . , holds(pn , S)\nholds(f \u0304, res(a, S)) \u2190 not holds(f, res(a, S)),\nholds(p1 , S), . . . , holds(pn , S)\nholds(f, res(a, S)) \u2190\n\nnot holds(f \u0304, res(a, S)), holds(p1 , S), . . . , holds(pn , S),\nnot holds(true, S)\nab(f , a, S) \u2190 not holds(f, res(a, S)), holds(p1 , S), . . . , holds(pn , S),\nnot holds(true, S)\nab(f, a, S) \u2190\n\nTo illustrate this translation, we use Rule t6 from domain description D5 , and\nshow how it will work from the initial situation s0 . We also include the translation\nfor t8 , holds(noDrops, s0 ) and holds(solidIce, s0 ) along with the inertia rule to get\nthe following program. Notice that the suppression rules do not apply here since\nwe are not considering any knowledge laws.\nholds(solidIce, res(drop, s0 )) \u2190 not holds(solidIce, res(drop, s0 )),\nholds(noDrops, s0 ).\nholds(solidIce, res(drop, s0 )) \u2190 not holds(solidIce, res(drop, s0 )),\nholds(noDrops, s0 ).\nab(solidIce, drop, s0 ) \u2190 not holds(solidIce, res(drop, s0 )),\nholds(noDrops, s0 ), not holds(true, s0 ).\nab(solidIce, drop, s0 ) \u2190 not holds(solidIce, res(drop, s0 )),\nholds(noDrops, s0 ), not holds(true, s0 ).\nholds(noDrops, res(drop, s0 )) \u2190 holds(noDrops, s0 ).\nab(noDrops, drop, s0 ) \u2190 holds(noDrops, s0 ).\nholds(noDrops, s0 ) \u2190\nholds(solidIce, s0 ) \u2190\n\n\fKnowledge and the Action Description Language A\n\n27\n\nholds(noDrops, res(drop, s0 )) \u2190 holds(noDrops, s0 ),\nnot ab(noDrops, drop, s0 ).\nholds(noDrops, res(drop, s0 )) \u2190 holds(noDrops, s0 ),\nnot ab(noDrops, drop, s0 ).\nholds(solidIce, res(drop, s0 )) \u2190 holds(solidIce, s0 ),\nnot ab(solidIce, drop, s0 ).\nholds(solidIce, res(drop, s0 )) \u2190 holds(solidIce, s0 ),\nnot ab(solidIce, drop, s0 ).\nThe program only has objective formulas. Thus, its semantics is given by its world\nview which consists of belief sets (belief sets are the same as answer sets in extended\nlogic programs). The world view W of the above program is W = {B1 , B2 }\nB1 = { holds(solidIce, res(drop, s0 )), ab(solidIce, drop, s0 ),\nholds(solidIce, s0 ), holds(noDrops, res(drop, s0 )),\nab(noDrops, drop, s0 ), holds(noDrops, s0 )\n}\nB2 = { holds(solidIce, res(drop, s0 )), ab(solidIce, drop, s0 ),\nholds(solidIce, s0 ), holds(noDrops, res(drop, s0 )),\nab(noDrops, drop, s0 ), holds(noDrops, s0 )\n}\nNotice that the query holds(noDrops, res(drop, s0 )) evaluates to true for the\nabove belief sets. If we were to ask the queries holds(solidIce, res(drop, s0 )) or\nholds(solidIce, res(drop, s0 )), we see neither would be able to produce an answer\nof yes or no. Both queries' answer is unknown.\nThe recursion through negation provides the desired effect of two possible interpretations for the effect of a in f (note that the two first rules of the example have\nthe form c \u2190 not b and b \u2190 not c and this program has two answer sets, one is\n{c} and the other is {b}).\nThe translation of a domain D is defined as the union of the domain dependent\nand domain independent rules.\n7.3 General Domains\nThe assumption that we made for simple domains was that for any sensing action\nas and fluent f there is at most one knowledge law of the form\nas causes to know f if p1 , . . . , pn\n\n(20)\n\nSuppose now we have the following domain\n\u001a\nr1 : lookInRoom causes to know boardClean if curtainOpen\nD\nr2 : lookInRoom causes to know boardClean if lightOn\nand assume we start with the following situation\n\u03a3 = {{boardClean, curtainOpen}, {boardClean, lightOn}, {boardClean}, {}}\n\nThere is one model \u03a61 that will result in the states where the fluent boarClean\nis true and the knowledge precondition curtainOpen \u2228 lightOn of boardClean with\n\n\f28\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nrespect to lookInRoom is also true (this corresponds to the third case of compatibility).\n\u03a61 (lookInRoom, \u03a3) = {{boardClean, curtainOpen}, {boardClean, lightOn}}\n\nWe will need a suppression rule similar to the rules in group (19) of the translation\nof simple domains. The rule will be something like\nholds(true, res(lookInRoom, S))\n\u2190 \u00acKholds(boardClean, S), \u00acKholds(boardClean, S),\nKholds(boardClean, res(lookInRoom, S)),\n\"Kholds(curtainOpen \u2228 lightOn, res(lookInRoom, S))\",\nholds(curtainOpen, S), holds(lightOn, S),\nThe question is how to encode \"Kholds(curtainOpen \u2228 lightOn, . . .)\"? We will\ndo it by adding two rules to the program\nholds(plookInRoom\nboardClean , S) \u2190 holds(curtainOpen, S)\nholds(plookInRoom\nboardClean , S) \u2190 holds(lightOn, S)\nNow the disjunction can be replaced by \"Kholds(plookInRoom\nboardClean , res(lookInRoom, S))\".\nlookInRoom\nThe symbol pboardClean is a new constant symbol not appearing anywhere else in\nthe program. We complete the program with the rule\nlookInRoom\nholds(plookInRoom\nboardClean , S) \u2190 not holds(pboardClean , S)\n\nand the translation becomes\nholds(true, res(lookInRoom, S))\n\u2190 \u00acKholds(boardClean, S), \u00acKholds(boardClean, S),\nKholds(boardClean, res(lookInRoom, S)),\nKholds(plookInRoom\nboardClean , res(lookInRoom, S)),\nlookInRoom\nholds(pboardClean , S)\nIn general, if D is a domain description (not necessarily simple) then, for any sensing\naction a and any fluent f , if \u03c61 \u2228. . .\u2228\u03c6m with \u03c6i = pi1 \u2227. . .\u2227piki , i = 1, . . . , m, is the\nknowledge precondition of f with respect to a in the domain D, we will have a new\nconstant symbol paf in the language of the logic program. Then for the knowledge\nlaws:\na causes to know f if \u03c61\n..\n.\na causes to know f if \u03c6m\nthe domain dependent translation will have the rules:\nholds(paf , S)\n\n\u2190\n\nholds(p11 , S), . . . , holds(p1k1 , S)\n\n..\n.\nholds(paf , S)\n\n\u2190\n\nm\nholds(pm\n1 , S), . . . , holds(pkm , S)\n\nholds(paf , S)\n\nholds(paf , S)\n\n\u2190\n\nnot\n\nholds(true, res(a, S))\n\n\u2190\n\n\u00acKholds(f, S), \u00acKholds(f \u0304, S),\n\u00acKholds(paf , res(a, S)),\n\n(21)\n(22)\n\n\fKnowledge and the Action Description Language A\n\n29\n\nholds(paf , S)\n\nholds(true, res(a, S))\nholds(true, res(a, S))\n\n\u2190\n\n(23)\n \u0304\n \u0304\n\u00acKholds(f, S), \u00acKholds(f , S), Kholds(f , res(a, S)),\n\n\u2190\n\nKholds(paf , res(a, S)), holds(f, S)\n(24)\n \u0304\n \u0304\n\u00acKholds(f, S), \u00acKholds(f , S), Kholds(f , res(a, S)),\nKholds(paf , res(a, S)),\nholds(paf , S)\n\nholds(true, res(a, S))\n\n\u2190\n\n(25)\n \u0304\n\u00acKholds(f, S), \u00acKholds(f , S), Kholds(f, res(a, S)),\nKholds(paf , res(a, S)), holds(f \u0304, S)\n(26)\n\nholds(true, res(a, S))\n\n\u2190\n\n\u00acKholds(f, S), \u00acKholds(f \u0304, S), Kholds(f, res(a, S)),\nKholds(paf , res(a, S)),\nholds(paf , S)\n\n(27)\n\nThe set of rules (15) corresponds to rule (23). Rule (16) corresponds to rule (24).\nThe set of rules (17) correspond to rule (25). Rule (18) corresponds to rule (26)\nand rule (19) corresponds to rule (27).\n\n7.4 Query Translation\nTo answer queries in the epistemic logic program we need to include rules to implement the evaluation functions \u0393. The query \"f after \u03b1\" will be true in a consistent\ndomain D if and only if holds af ter plan(f, \u03b1) is true in the epistemic logic program obtained from D plus the rules\nholds af ter plan(F, P )\n\u2190\nf ind situation([], S, S)\n\u2190\nf ind situation([a|\u03b1], S, S1 )\n\u2190\nf ind situation([ if \u03c6 then \u03b11 |\u03b12 ], S, S1 )\n\u2190\nf ind situation([ if \u03c6 then \u03b11 |\u03b12 ], S, S1 )\n\u2190\nf ind situation([ if \u03c6 then \u03b11 else \u03b1\u20321 |\u03b12 ],\nS, S1 )\n\u2190\nf ind situation([ if \u03c6 then \u03b11 else \u03b1\u20321 |\u03b12 ],\nS, S1 )\n\u2190\nf ind situation([ while \u03c6 do \u03b11 |\u03b12 ], S, S1 )\n\u2190\n\nf ind situation(P, s0 , S), holds(F, S)\n\nf ind situation(\u03b1, res(a, S), S1 )\nKholds(\u03c6, S), f ind situation(\u03b12 , S, S1 )\nKholds(\u03c6, S), f ind situation(\u03b11 , S, S \u2032 ),\nf ind situation(\u03b12 , S \u2032 , S1 )\n\nKholds(\u03c6\u0304, S), f ind situation(\u03b1\u20321 , S, S \u2032 ),\nf ind situation(\u03b12 , S \u2032 , S1 )\n\nKholds(\u03c6, S), f ind situation(\u03b11 , S, S \u2032 ),\nf ind situation(\u03b12 , S \u2032 , S1 )\nKholds(\u03c6\u0304, S),\nf ind situation(\u03b12 , S, S1 )\n\n\f30\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nf ind situation([while \u03c6 do \u03b11 |\u03b12 ], S, S1 )\n\u2190\n\nKholds(\u03c6, S),\nf ind situation(\u03b11 , S, S \u2032 ),\nf ind situation([while \u03c6 do \u03b11 |\u03b12 ], S \u2032 , S1 )\n\nAs you may note from the rules, holds af ter plan(F, P ) works in two steps. First,\nit finds the situation s that results from applying P to the initial situation (using\nf ind situation(P, s0 , S)) and then shows that F holds in that situation. Since the\ntranslation of the domain may have several world views the program needs to find\na situation for each world view. The following example illustrates how the process\nworks.\nExample 7.2\n\n\uf8f1\nr1 : initially \u00acburnOut\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nr2 : initially \u00acbulbF ixed\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 r3 : changeBulb causes burnOut if switchOn\n1\nD1\nr4 : changeBulb causes bulbF ixed if \u00acswitchOn\n\uf8f4\n\uf8f4\n\uf8f4 r5 : turnSwitch causes switchOn if \u00acswitchOn\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nr6 : turnSwitch causes \u00acswitchOn if switchOn\n\uf8f4\n\uf8f4\n\uf8f3\nr7 : checkSwitch causes to know switchOn if \u00acburnOut\n\nAssume we would like to show that\nD11 |= bulbF ixed after [\n\ncheckSwitch,\nif switchOnthen [turnSwitch],\nchangeBulb]\n\nThe states in the initial situation of this example are:\n\u03a3={\n\n{\u00acburnOut, \u00acbulbF ixed, switchOn},\n{\u00acburnOut, \u00acbulbF ixed, \u00acswitchOn}}\n\nIt has two models \u03a61 and \u03a62 that for the sensing action checkSwitch behave\nvery much like in Example 7.1. Then, the logic program translation of this domain\nhas two world views. The world view W corresponding to \u03a61 has two belief sets\n1\nfrom Example 7.1 is a subset of W 1 and\nW 1 and W 2 , such that Ws10 \u222a Wres(cs,s\n0)\n2\n2\n1\n2\nWs0 \u222a Wres(cs,s0 ) is a subset of W . W also contains the sets\n1\nWres(ts,res(cs,s\n0 ))\n\n=\n{holds(burnOut, res(turnSwitch, res(checkSwitch, s0 ))),\nholds(bulbF ixed, res(turnSwitch, res(checkSwitch, s0 )),\nholds(switchOn, res(turnSwitch, res(checkSwitch, s0 )))}\n\nand\n1\nWres(cb,res(ts,res(cs,s\n=\n0 )))\n{holds(burnOut, res(changeBulb, res(turnSwitch, res(checkSwitch, s0)))),\nholds(bulbF ixed, res(changeBulb, res(turnSwitch, res(checkSwitch, s0)))),\nholds(switchOn, res(changeBulb, res(turnSwitch, res(checkSwitch, s0))))}\nand W 2 the set\n\n\fKnowledge and the Action Description Language A\n\n31\n\n2\nWres(ts,(res(cs,s\n= {holds(burnOut, res(turnSwitch, res(checkSwitch, s0 ))),\n0 )))\nholds(bulbF ixed, res(turnSwitch, res(checkSwitch, s0 ))),\nholds(switchOn, res(turnSwitch, res(checkSwitch, s0 )))}\nS\n\n{holds(burnOut, res(turnSwitch, res(checkSwitch, s0 ))),\nholds(bulbF ixed, res(turnSwitch, res(checkSwitch, s0))),\nholds(switchOn, res(turnSwitch, res(checkSwitch, s0 ))),\nholds(true, res(turnSwitch, res(checkSwitch, s0 )))}\n\n2\nThere is also a similar set Wres(cb,res(ts,(res(cs,s\n, with the same elements of\n0 ))))\n2\nWres(ts,(res(cs,s0 ))) , replacing the situation argument with\n\nres(changeBulb, res(turnSwitch, (res(checkSwitch, s0))))\nThis corresponds to the sequence\n\u03a3,\n\u03a61 (checkSwitch, \u03a3),\n\u03a61 (turnSwitch, \u03a61 (checkSwitch, \u03a3)),\n\u03a61 (changeBulb, \u03a61(turnSwitch, \u03a61 (checkSwitch, \u03a3)))\nThus, in this world view the predicate\nf ind situation([ checkSwitch,\nif switchOn then [turnSwitch],\nchangeBulb], s0, S)\nwill hold in W iff S = res(changeBulb, res(turnSwitch, res(checkSwitch, s0))).\nThe second step will check if\nholds(bulbF ixed, res(changeBulb, res(turnSwitch, res(checkSwitch, s0)))\nis in W. The answer is yes since the atom belongs to both W 1 and W 2 .\nThe world view associated with \u03a62 is defined in a similar manner, but in this\nworld view S = res(changeBulb, res(checkSwitch, s0)).\nLet \u03a0D be the epistemic logic program corresponding to the translation of a\ndomain description D, and denote by \u03a0Q\nD the union of \u03a0D and the rules to interpret\nqueries given above. Then we can show:\nTheorem 7.3\nGiven a consistent domain description D and a plan \u03b2. D |= F after \u03b2 iff \u03a0Q\nD |=\nholds af ter plan(F, \u03b2).\nProof: see appendix.\n8 Relation to Other Work\nIn (Levesque, 1996) there is also a programming language based on the situation calculus which uses sensing actions. This work is based on previous work\n\n\f32\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nfrom (Scherl & Levesque, 1993), in which knowledge is represented using two levels.\nThere is a representation of the actual situation (called s) in which the agent is in,\nand there are situations accessible from s (called s\u2032 ) which the agent thinks it might\nbe in. Something is known to the agent as being true (false) if it is true (false) in\nall situations s\u2032 which are accessible from the actual situation s and is unknown\notherwise. In other words (Scherl & Levesque, 1993) distinguishes between what is\nknown by the agent and what is true in world. We only represent what is know by\nthe agent, and assume that this knowledge might be incomplete but always correct.\nSomething is known in our representation if its value is the same throughout the\nstates in a situation and unknown otherwise.\nIn (Levesque, 1996; Scherl & Levesque, 1993) the authors use preconditions which\nare executability conditions for an action's execution. For example, a precondition\nto clean a white-board is one must be in front of the white-board. Our preconditions differ in that they are conditions on the effects. We can always execute an\naction but its effect varies according to its precondition in the effect propositions.\nExtending Ak to include executability conditions can be done as for extensions of\nA. The use of conditions on effects however allows us to represent a phenomenon of\nsensing in which the value of previously unknown preconditions are learned along\nwith the fluent we are trying to gain knowledge about. This is shown in examples\n2.4 and 2.8 where the robot will know whether or not it is facing the door after\nexecuting the action look.\nIn (Levesque, 1996) once knowledge is gained it is never lost. We, on the other\nhand, explore the use of non-deterministic actions as a mechanism to remove knowledge. Our use of non-deterministic actions is similar to (Thielscher, 1994) where the\neffect of a non-deterministic action is to make a fluent true or false, but exactly\nwhich is indeterminate. As might be expected, there are cases where the possible\noutcome is not intuitive. Take for example a deterministic action Shoot that causes\nOllie to be dead. Any observation, which depends on Ollie being alive, such as \"Ollie is walking\" can be made false using the same action Shoot. Shoot can be used\nas a restriction which causes Ollie not to walk. In the resulting situation, Ollie will\nnot be alive and therefore will not be walking around. This is not the case when\nShoot has the non-deterministic effect of making Ollie dead or leaving Ollie alive\n(suppose that the gun is not working well). With the same restriction, Ollie may\nbe dead and not walking in one situation and alive and not walking in the other.\nAssuming one can walk as long as one is alive, then the later situation makes no\nsense. The same holds true without the restriction but this time Ollie will be dead\nand walking in one situation. If shoot also has a non-deterministic effect on walking,\nwe are no better off.\nThese cases are prevented with integrity constraints as in (Kartha & Lifschitz, 1994).\nOur language could be extended to include constraints as in (Baral et al., 1997) but\nour interest in non-determinism is its effect on knowledge. We discuss the topic of\nintegrity constraints in Section 9.\nMost translations for dialects of A are to extended logic programs. Our translation is to epistemic logic programs because of its ability to represent knowledge\nand incomplete information. To the best of our knowledge this is the first use of\n\n\fKnowledge and the Action Description Language A\n\n33\n\nepistemic logic programs in a translation from action languages. The closest work\nrelated to our results is presented in (Baral & Son, 1997). In that paper A is also\nextended to handle sensing actions but the semantics is some what limited because they work with a three value semantics and only approximate knowledge.\nFurthermore, in their language sensing actions have no conditional effects. These\nrestrictions allow Baral and Son to write translations into extended logic programs.\nShowing whether is possible to find a translation into extended logic programs or\nfirst order logic of our domains is an open question.\n\n9 Future Work\nWe already mentioned the need to clarify the complexity of adding sensing actions\nto domain descriptions. Our translation suggests that it might be computationally\nmore complex to deal with conditional sensing actions than sensing with no conditions. Two other possible directions of research are: First, the ability of an agent to\nquery itself about what it knows (i.e introspection). This is useful when the cost of\nexecuting a series of plans is expensive maybe in terms of time. Allowing an agent\nto query whether it knows that it knows something may be a cheaper alternative\nand cost effective. The use of a modal operator as shown below may be sufficient\nto accomplish this.\nif \u00acknows(\u03c6) then [\u03b1]\n\n(28)\n\nwhere \u03c6 is a test condition (as defined in Section 5), \u03b1 a plan, and knows(\u03c6) would\nbe an introspective operator on the test condition.\nTake for example Agent A in Fig. 1 from Section 2. Agent A knows that Ollie is\nwet (denoted by wet), but does not know if it is raining outside (denoted by rain).\nAgent A would have to find a window and then look out that window to see if it\nis raining outside. Suppose the program or control module for finding a window in\na building is long and very costly as far as battery power, Agent A would have to\nfind a window and then check for rain. Agent B would benefit from the conditional\nbelow\nif \u00acknows(\u03c6) then [f indW indow, lookOutside]\nWithout an introspective operator, both Agent A and Agent B would have to find\na window and then look outside. Agent B can save on battery power if it has the\nability to query itself on what it knows.\nSecond, we could investigate expanding the initial epistemic state. At present,\ndomains only may start from a situation with only one initial epistemic state. For\nmore states or to represent multiple initial situations in a domain, the language to\ndescribe domains must be extended with modal operators.\nAs mentioned earlier in this paper, integrity constraints could be added. Integrity constraints define dependency relationships between fluents. Taking the\nexample from the previous section, walking depends on Ollie being alive. This\n\n\f34\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\ncould be represented following the approach outlined in (Kartha & Lifschitz, 1994;\nLifschitz, 1996)\nnever \u03c6 if \u03c8\n\n(29)\n\nwhere \u03c6 and \u03c8 are conjunctions of fluent literals. It states that \u03c6 can not be true\nwhen \u03c8 is true. Our example with Ollie would look like this\nnever walking if \u00acalive\nConditions of effect are used throughout this paper. An example of a executability\ncondition is the fact that one has to be at a light bulb to change the light bulb.\nExecutability conditions found in (Levesque, 1996) could be implemented using the\nmethods found in (Kartha & Lifschitz, 1994; Lifschitz, 1996).\nimpossible A if \u03c8\n\n(30)\n\nwhere A is an action and \u03c8 is a conjunction of fluent literals. The execution of\naction A cannot take place as long as \u03c8 is true.\nUsing (30) we could express the constraint that in order to change the bulb, one\nhas to be at the lamp as\nimpossible changeBulb if \u00acatLamp\nThis paper will conclude with three thoughts. One is relaxing the assumption\nthat an agent has incomplete but always correct knowledge of its world. One could\nimagine the agent not only reasoning on information that it knows is true, but also\nreasoning on what it believes is true. At present we have not explored this topic.\nThe other idea is given that Ak is a high level action description language that\ndeals with incomplete information across multiple possible worlds, it stands to reason that Ak could be translated to formalisms, such as Levesque's (Levesque, 1996;\nScherl & Levesque, 1993) or autoepistemic logic (Moore, 1985; Marek & Truszczynski, 1991),\nwhich also hold this property. The third refers to the termination of routines; there\nare certain tasks for which routines can be limited by a sensing action that determines the \"size\" of the problem. Take for example the number of pages in a\nbook or the number of doors on the second floor of an office building. The number\nof pages contained in a book will ensure the termination of a search for a word\nthrough that book. The same applies to the number of doors on the second floor\nwith respect to a security routine which checks that all the doors on the second\nfloor are locked. For this type of task, a counter is sufficient. To include counters, we\ndo not require constraints but variables in Ak . These loops correspond to for-loops\nin regular programming languages. Consider the situation described in 5.4. On the\nfloor of a room there are cans. An agent is given an empty bag and instructed to fill\nthe bag with cans. We assume that there are more than enough cans on the floor\nto fill the bag. We can model the space left in the bag by having initially true one\n\n\fKnowledge and the Action Description Language A\n\n35\n\n(and only one) of the following fluents.\nspaceLef t(0)\nspaceLef t(s(0))\n..\n.\nspaceLef t(sn(0))\n..\n.\nThe effect of drop can be now described by the effect proposition:\nr1 : drop causes spaceLef t(x) if spaceLef t(s(x))\nHowever, we need to restrict the world to only allow one spaceLef t fluent to be\ntrue at any moment. This can be described with a constraint of the form\nnever spaceLef t(x) \u2227 spaceLef t(y) if x 6= y\nNote that the constraint encodes a ramification of drop since not only the execution of the action drop makes spaceLef t(x) true, but also indirectly causes\nspaceLef t(s(x)) to become false.\nAn orthogonal problem to the issue of constraints, is that we still need in our\ndomain a value proposition that tells us how much space we initially have in the\nbag. Adding the initial value proposition is not a completely satisfactory solution\nsince the plan\nwhile nospaceLef t(0) do [dropCanInBag]\nfills the bag irrespectively of the initial situation and (in normal circumstances)\nthe plan will always terminate. Furthermore, in a realistic setting, plans need to\nconsider limitation of resources. Plans may need to limit the amount of time devoted\nto any task or limit the amount of energy that can be used. These bounds can be\napplied to all tasks, but still a counter is required. Further research in termination,\nspecially in a common-sense approach to proof of termination is necessary to deal\nwith loops in plans.\nAcknowledgments We would like to thank the anonymous referees for their invaluable suggestions. Section 7.3 is a direct result of one of their comments.\nReferences\nAho, A., & Ullman, J. (1995). Foundations of computer science, C edition. Computer\nScience Press.\nBaral, C., & Gelfond, M. (1997). Reasoning about effects of concurrent actions. Journal\nof logic programming, 31, 85\u2013118.\nBaral, C., & Son, T. (1997). Approximate reasoning about actions in presence of sensing\nand incomplete information. Pages 387\u2013401 of: Maluszynsky, Jan (ed), Proceedings of\ninternational logic programming symposium.\nBaral, C., Gelfond, M., & Provetti, A. (1997). Representing actions: Laws, observations\nand hypotheses. Journal of logic programming, 31(1\u20133), 201\u2013244.\n\n\f36\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nCousot, P. (1990). Methods and logics for proving programs. Pages 841\u2013993 of: van\nLeeuwen, J. (ed), Handbook of theoretical computer science, vol. B. MIT Press and\nElsevier Science Publisher B.V.\nDavey, B. A., & Priestley, H. A. (1990). Introduction to lattice theory. Cambridge Mathematical Text Books.\nGeffner, H., & Bonet, B. (1998). High-level planning and control with incomplete information using POMDP's. Working notes of the 1998 aaai fall symposium on cognitive\nrobotics.\nGelfond, M. (1994). Logic programming and reasoning with incomplete information. Annals of mathematics and artificial intelligence, 98\u2013116.\nGelfond, M., & Lifschitz, V. (1991). Classical negation in logic programs and disjunctive\ndatabases. New generation computing, 365\u2013387.\nGelfond, M., & Lifschitz, V. (1993). Representing actions and change by logic programs.\nJournal of logic programming, 17(2,3,4), 301\u2013323.\nGelfond, M., & Przymusinska, H. (1991). Definitions of epistemic specification. Pages\n249\u2013263 of: Nerode, A., Marek, V. W., & Subrahmanian, V. S. (eds), Proceedings of\nfirst international workshop on logic programming and non-monotonic reasoning.\nKartha, E. Giunchiglia G. N., & Lifschitz, V. (1997). Representing actions: Indeterminacy\nand ramification. Artificial intelligence, 95, 409\u2013443.\nKartha, G. N., & Lifschitz, V. (1994). Actions with indirect effects. Pages 341\u2013350 of:\nDoyle, J., Sandewall, E., & Torasso, P. (eds), Proceedings of the fourth international\nconference on principles of knowledge representation and reasoning.\nLevesque, H. (1996). What is planning in the presence of sensing? Pages 1139\u20131146 of:\nProceedings of thirteenth national conference on artificial intelligence, vol. 2.\nLifschitz, V. (1996). Two components of an action language. Working papers of the 3rd\nsymposium on logical formalizations of commonsense reasoning (common sense '96).\nLifschitz, V., & Turner, H. (1994). Splitting a logic program. Pages 23\u201337 of: van Hentenryck, P. (ed), Proceedings of the eleventh international conference of logic programming.\nLobo, J., Minker, J., & Rajasekar, A. (1992). Foundations of disjunctive logic programming.\nCambridge, Massachusetts: MIT press.\nLobo, J., Mendez, G., & Taylor, S. (1997). Adding knowledge to the action description\nlanguage A. Pages 454\u2013459 of: Proceedings of the fourteenth national conference on\nartificial intelligence. AAAI Press.\nManna, Z., & Waldinger, R. (1987). How to clear a block: A theory of plans. Journal of\nautomated reasoning, 3, 343\u2013377.\nMarek, W., & Truszczynski, M. (1991). Autoepistemic logic. Journal of ACM, 38(3),\n588\u2013619.\nMcCain, N., & Turner., H. (1997). Causal theories of action and change. Pages 460\u2013465\nof: Proceedings of the fourteenth national conference on artificial intelligence.\nMoore, R.C. (1985). Semantical considerations on non-monotonic reasoning. Artificial\nintelligence, 28, 75\u201394.\nScherl, R., & Levesque, H. (1993). The frame problem and knowledge-producing actions.\nPages 689\u2013695 of: Proceedings of the eleventh national conference on artificial intelligence.\nThielscher, M. (1994). Representing actions in equational logic programming. Pages 207\u2013\n224 of: van Hentenryck, P. (ed), Proceedings of the international conference on logic\nprogramming.\n\n\fKnowledge and the Action Description Language A\n\n37\n\nA While-evaluation Function\nDefinition A.1\nLet E be the set of all situations and P the set of all total functions f mapping\nsituations into situations, P = {f | f : E \u2192 E }. We say that for any pair of\nfunctions f1 , f2 \u2208 P, f1 \u2264 f2 if and only if for any \u03a3 \u2208 E if f1 (\u03a3) 6= \u2205, then\nf1 (\u03a3) = f2 (\u03a3).\nThe next proposition follows from the above definition.\nProposition A.2\nThe above relation \u2264 defines a partial order in P.\nMoreover, this partial order is a complete semi-lattice with the bottom element\nequal to the function that maps every situation to \u2205. We will denote the bottom\nelement by f\u2205 .\nDefinition A.3\nLet \u03b1 be a plan and \u0393 a function that maps plans and situations into situations.\n\u0393\nLet \u03c6 be a conjunction of fluent literals. Then, we define the function F\u03b1,\u03c6\n:P \u2192P\nsuch that for any\uf8f1function f \u2208 P,\nif \u03c6 is false in \u03a3\n\uf8f2 \u03a3\n\u0393\nf (\u0393(\u03b1, \u03a3)) if \u03c6 is true in \u03a3\nF\u03b1,\u03c6\n(f )(\u03a3) =\n\uf8f3\n\u2205\notherwise\n\n\u0393\nOur goal is to show that F\u03b1,\u03c6\nis continuous. For this, we will need to show that\nF\nfor any directed set D \u2286 P, the least upper bound of D, denoted by D exists,\nF\nF\n\u0393\n\u0393\n(d) | d \u2208 D}. A directed set is a set such that for\n( D) = {F\u03b1,\u03c6\nand that F\u03b1,\u03c6\nany finite subset of it, the least upper bound of that set exists, and belongs to the\nF\ndirected set. The existence of D follows from the following proposition.\n\nProposition A.4\n\u2032\nLet D be a directed subset of P, and let d \u2208 D. If d(\u03a3) = \u03a3 6= \u2205, for a situation\n\u2032\n\u2032\n\u2032\n\u2032\n\u03a3, then for any d \u2208 D either d (\u03a3) = \u2205 or d (\u03a3) = \u03a3 .\nIt follows from this proposition that,\n\u001a\nG\n\u2205\nif \u2200d \u2208 D, d(\u03a3) = \u2205\nD(\u03a3) =\n\u2032\n\u2032\n\u03a3 if \u2203d \u2208 D such that d(\u03a3) = \u03a3 and \u03a3 6= \u2205\nF \u0393\nA similar function is defined by {F\u03b1,\u03c6\n(d) | d \u2208 D}. This function will be used in\nthe proof of the following theorem.\nTheorem A.5\n\u0393\nFor any plan \u03b1 and any conjunction of fluent literals \u03c6, the function F\u03b1,\u03c6\nis continuous with respect to the order \u2264.\nF \u0393\nF \u0393\nProof: Let F\u03b1,\u03c6\n[D] denote the function {F\u03b1,\u03c6\n(d) | d \u2208 D}. To prove the theoF\nF \u0393\n\u0393\nrem, it suffices to show that, for any directed set D \u2286 P, F\u03b1,\u03c6\n( D) = F\u03b1,\u03c6\n[D].\nLet \u03a3 be a situation.\n\u0393\n(a) If \u03c6 is false in \u03a3 then for any f \u2208 P, F\u03b1,\u03c6\n(f )(\u03a3) = \u03a3. Hence,\nG\nG\n\u0393\n\u0393\nF\u03b1,\u03c6 ( D)(\u03a3) = \u03a3 =\nF\u03b1,\u03c6 [D](\u03a3).\n\n\f38\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nF\nF\nF \u0393\n\u2032\n\u0393\n(b) If \u03c6 is true in \u03a3, then F\u03b1,\u03c6\n( D)(\u03a3) = D(\u0393(\u03b1, \u03a3)). Let F\u03b1,\u03c6\n[D](\u03a3) = \u03a3 .\n\u2032\n\u0393\n\u0393\nBy Proposition A.4, \u03a3 = \u2205 iff F\u03b1,\u03c6\n(d)(\u03a3) = \u2205, for any d \u2208 D since F\u03b1,\u03c6\n(d)(\u03a3) =\nF\n\u2032\n\u0393\nd(\u0393(\u03b1, \u03a3)) and D is directed. Therefore, F\u03b1,\u03c6 ( D)(\u03a3) must be \u2205. If \u03a3 6= \u2205, then\n\u2032\n\u0393\n\u0393\nfor every d \u2208 D such that F\u03b1,\u03c6\n(d)(\u03a3) 6= \u2205, it must be the case that F\u03b1,\u03c6\n(d)(\u03a3) = \u03a3\nF\n\u2032\n\u0393\n\u0393\nsince F\u03b1,\u03c6\n(d)(\u03a3) = d(\u0393(\u03b1, \u03a3)) and D is directed. Then F\u03b1,\u03c6\n( D)(\u03a3) = \u03a3 .\n\n(c) When \u03c6 is neither true nor false in \u03a3, the proof is similar to part (a) since\n\u0393\nF\u03b1,\u03c6\n(f )(\u03a3) = \u2205 for any f \u2208 P.\n\u0393\nWe define the powers of F\u03b1,\u03c6\nas follows:\n\u0393\n1. F\u03b1,\u03c6\n\u2191 0 = f\u2205 .\n\u0393\n\u0393\n\u0393\n2. F\u03b1,\u03c6 \u2191 n + 1 = F\u03b1,\u03c6\n(F\u03b1,\u03c6\n\u2191 n).\nF \u0393\n\u0393\n3. F\u03b1,\u03c6 \u2191 \u03c9 = {F\u03b1,\u03c6 \u2191 n | n \u2264 \u03c9}.\n\n\u0393\nFrom the continuity of F\u03b1,\u03c6\nthe corollary below follows.\n\nCorollary A.6\n\u0393\n\u0393\nThe least fix-point of F\u03b1,\u03c6\nis F\u03b1,\u03c6\n\u2191 \u03c9.\nB Proofs\nIn this section we present the proof of Theorem 7.3 by givingn a detailed proof\nof the correctness of the translation for simple domains. The proof for the general\ncase is a direct extension. In our proofs we will use the splitting lemma of extending\nlogic programs (Lifschitz & Turner, 1994). For completeness we will include some\ndefinitions and the statement of the lemma below.\nConsider a nonempty set of symbols called atoms. A literal is an atom possibly\npreceded by the classical negation symbol \u00ac. A rule is determined by three finite\nset of literals - the set of head literals , the set of positive subgoals and the set of\nnegated subgoals. The rule with the head literals L1 , . . . , Lq , the positive subgoals\nLi+1 , . . . , Lm and the negated subgoals Lm+1 , . . . , Ln is written as\nL1 or . . . or Lq \u2190 Lq+1 , . . . , Lm , not Lm+1 , . . . , Ln\nThe three parts of a rule r are denoted by head(r), pos(r) and neg(r); lit(r)\nstands for head(r) \u222a pos(r) \u222a neg(r).\nDefinition B.1\n(Splitting set) (Lifschitz & Turner, 1994) A splitting set for a logic program \u03a0 is\nany set U of literals such that, for every rule r \u2208 \u03a0, if head(r) \u2229 U 6= \u2205 then\nlit(r) \u2286 U . If U is a splitting set for \u03a0, we also say that U splits \u03a0. The set of rules\nr \u2208 \u03a0 such that lit(r) \u2286 U is called the bottom of \u03a0 relative to the splitting set U\nand is denoted by bU (\u03a0). The subprogram \u03a0 \u2212 bU (\u03a0) is called the top of \u03a0 relative\nto U .\nDefinition B.2\n(Partial evaluation) (Lifschitz & Turner, 1994) The partial evaluation of a program\n\u03a0 with splitting set U w.r.t. a set of literals X is the program eU (\u03a0, X) defined as\nfollows. For each rule r \u2208 \u03a0 such that:\n(pos(r) \u2229 U ) \u2286 X \u2227 (neg(r) \u2229 U ) \u2229 X = \u2205\n\n\fKnowledge and the Action Description Language A\n\n39\n\nput in eu (\u03a0, X) the rule r\u2032 which satisfies the following property:\nhead(r\u2032 ) = head(r), pos(r\u2032 ) = pos(r) \u2212 U, neg(r\u2032 ) = neg(r) \u2212 U .\nDefinition B.3\n(Solution) (Lifschitz & Turner, 1994) Let U be a splitting set for a program \u03a0. A\nsolution to \u03a0 w.r.t. U is a pair (X, Y ) of sets of literals such that:\n\u2022 X is an answer set for for bU (\u03a0);\n\u2022 Y is an answer set for eU (\u03a0 \u2212 bU (\u03a0), X);\n\u2022 X \u222a Y is consistent.\nLemma B.4\n(Splitting lemma) (Lifschitz & Turner, 1994) Let U be a splitting set for a program\n\u03a0. A set A of literals is a consistent answer set of \u03a0 if and only if A = X \u222a Y for\nsome solution (X, Y ) to \u03a0 w.r.t. U.\nFrom now on we will refer to the simple domain description 20, as domain description to simplify the statements.\nThe proof of Theorem 7.3 (Theorem: Given a consistent domain description D\nand a plan \u03b2. D |= F after \u03b2 iff \u03a0Q\nD |= holds af ter plan(F, \u03b2).) is organized as\nfollows:\n1. First, we will prove that the epistemic logic program translation models correctly the execution of a single non-sensing action. Intuitively this can be done\nby looking at all the predicates of the form holds(f, res(a, s0 )), for any nonsensing action a. Furthermore we should be able to replace the initial constant\ns0 with any fixed situation constant s of the form res(a1 , . . . , res(ak , s0 ) . . .).\nIn the proof we will show that given any situation constant s, state \u03c3, and\n0-model of the domain \u03a60 , we can find a sub-set of the program ground(\u03a0D )\nin which assuming s to be the initial situation constant one of its belief sets\ncorresponds to \u03a60 . We will also prove the other direction. That is, for any belief set of the mentioned sub-set of ground(\u03a0D ), there exists a corresponding\nfunction \u03a60 , 0-model of D. This covers the general case of a single non-sensing\naction applied to a situation since, by the definition of 2.7, this reduces to the\napplication of 0-interpretations to each of the states in the situation.\n2. The second part of the proof extends the first part to cover the execution of\nsensing actions. In this case the sub-set of ground(\u03a0D ) includes rules with\nthe modal operator K. We show that each world view of the sub-program\ncorresponds to an interpretation \u03a6, model of D. We also show that for any\nmodel \u03a6 of D there is an associated world view of the sub-program.\n3. The next step extends step 2 from the application of a single action to the\napplication of any sequence of actions by induction.\n4. The final step extends the proof from sequence a of actions to complex plans.\nThe proof shows by structural induction on the complexity of the plans that\ngiven a fixed world view any plan (that terminates) can be reduced to the\nexecution of a sequence of actions.\n\n\f40\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nGiven a situation constant s, denote by \u03a01(D,s) the subprogram of Ground(\u03a0D )\nthat is restricted to those rules in ground(\u03a0D ), such that either the only situation\nconstant appearing in the heads is of the form res(a, s) for an action symbol a, or\nis of the form ab(f, a, s) for a fluent literal f and action symbol a.\nFor any possible action a, we will denote by \u03a01(D,a,s) the subprogram of \u03a01(D,s)\nthat is restricted to those rules in \u03a0D that only involve the action a in its predicates, besides other action symbols occurring in s. We call a domain description a\nuniversal domain if there are no value propositions in the domain. Given a universal domain description D, and state \u03c3, we denote by D\u03c3 the domain consisting of\nD \u222a {initially f : f \u2208 \u03c3} \u222a {initially \u00acf : f 6\u2208 \u03c3}.\nDefinition B.5\nFor a domain description D, let \u03c3 be a state, s a situation constant, and \u03a60 a\n0-interpretation, We define the set of literals A(\u03a60 ,\u03c3,s) as follows:\nFor any action a and any fluent f ,\n1.\n2.\n3.\n4.\n5.\n\nholds(f, s) \u2208 A(\u03a60 ,\u03c3,s) \u21d0\u21d2 f \u2208 \u03c3,\nholds(f \u0304, s) \u2208 A(\u03a60 ,\u03c3,s) \u21d0\u21d2 f 6\u2208 \u03c3,\nholds(f, res(a, s)) \u2208 A(\u03a60 ,\u03c3,s) \u21d0\u21d2 f \u2208 \u03a60 (a, \u03c3),\nholds(f \u0304, res(a, s)) \u2208 A(\u03a60 ,\u03c3,s) \u21d0\u21d2 f 6\u2208 \u03a60 (a, \u03c3).\nab(f, a, \u03c3) \u2208 A(\u03a60 ,\u03c3,s) if and only if there exists an object effect proposition\nof the form\na causes f if p1 . . . pm\nsuch that p1 . . . pm holds in \u03c3 or a non deterministic effect proposition of the\nform\na may affectf if p1 , . . . , pm\n\nin D such that p1 , . . . , pm holds in \u03c3 and f holds \u03a60 (a, \u03c3).\n6. ab(f \u0304, a, \u03c3) \u2208 A(\u03a60 ,\u03c3,s) if and only if there exists an object effect proposition\nof the form\na causes \u00acf if p1 . . . pm\nsuch that p1 . . . pm or a non deterministic effect proposition of the form\na may affectf if p1 , . . . , pm\nin D such that p1 , . . . , pm and f does not hold in \u03a60 (a, \u03c3).\nNothing else belongs to A(\u03a60 ,\u03c3,s) .\nDefinition B.6\nLet D be a domain description, \u03a60 a 0-interpretation, and \u03c3 a state. We will say\nthat the pair (\u03a60 , \u03c3) is a 0-specific model of the domain description D if \u03c3 is an\ninitial state of D and \u03a60 one of its 0-models.\nIn the next two theorems we will prove that the logic program models correctly\nthe execution of a single non-sensing action.\nTheorem B.7\n\n\fKnowledge and the Action Description Language A\n\n41\n\nLet D be a consistent universal domain description with no knowledge laws, and \u03c3\na state. If (\u03a60 , \u03c3) is a 0-specific model of D\u03c3 , then A(\u03a60 ,\u03c3,s) satisfies every rule in\n\u03a01D(\u03c3,s) , for any situation constant s.\nProof\nAssume that (\u03a60 , \u03c3) is a 0-specific model of D\u03c3 . Then any fact holds(f, s) or\nholds(f \u0304, s) in \u03a01(D\u03c3 ,s) is such that either holds(f, s) is in A(\u03a60 ,\u03c3,s) or holds(f \u0304, s)\nis in A(\u03a60 ,\u03c3,s) , which is obvious. Furthermore, for any literal of the form holds(f, s)\n(resp. holds(f \u0304, s)) in \u03a01(D\u03c3 ,s) obtained from the translation of a proposition of the\nform initially f (resp. initially \u00acf ), we will have by construction that holds(f, s) \u2208\nA(\u03a60 ,\u03c3,s) (resp. holds(f \u0304, s) \u2208 A(\u03a60 ,\u03c3,s) ). Now, let us take a pair of rules of the form\nholds(f, res(a, s)) \u2190 holds(p1 , s) . . . , holds(pm , s)\nab(f, a, s) \u2190 holds(p1 , s), . . . , holds(pm , s), not holds(true, s)\nobtained from the translation of a proposition of the form\na causes f if p1 , . . . , pm\n, and assume that holds(p1 , s), . . . , holds(pm , s) \u2208 A(\u03a60 ,\u03c3,s) . Then by construction,\np1 , . . . , pm holds in \u03c3 and holds(true, s) 6\u2208 A(\u03a60 ,\u03c3,s) . Therefore ab(f, a, s) \u2208 A(\u03a60 ,\u03c3,s)\nand f is in \u03a60 (a, \u03c3). Consequently, holds(f, res(a, s)) holds in A(\u03a60 ,\u03c3,s) .\nThe rules:\nholds(true, res(A, S)) \u2190 holds(true, S)\nholds(F, S) \u2190 holds(true, S)\nare trivially satisfied since there are no atoms of the form holds(true, s) in A(\u03a60 ,\u03c3,s) .\nNow we will make several considerations on A(\u03a60 ,\u03c3,s) to evaluate the other rules\n(ground instances of the inertia rule and rules obtained from the translation of\nnon-deterministic effect propositions):\n1. ab(f, a, s) holds in A(\u03a60 ,\u03c3,s) .\nAny rule of the form holds(f \u0304, res(a, s)) \u2190 holds(f, s), not ab(f, a, s) (instance of the inertia rule) is removed from \u03a01(D\u03c3 ,s) to verify that A(\u03a60 ,\u03c3,s) is\na belief set of \u03a01(D\u03c3 ,s) . Moreover, by the definition of A(\u03a60 ,\u03c3,s) , there must be\nan effect proposition with one of the following forms\na causes f if p1 , . . . , pm\nwith p1 , . . . , pm true in \u03c3 or\na may affectf if p1 , . . . , pm\nin D\u03c3 with p1 , . . . , pm true in \u03c3 and holds(f, res(a, s)) member of A(\u03a60 ,\u03c3,s)\nby case (3) above. So any pair of rules of the form\nnot holds(f \u0304, res(a, s)),\nholds(p1 , s)), . . . holds(pn , s)), not holds(true, s)\nholds(f, res(a, s)) \u2190 not holds(f \u0304, res(a, s)),\nholds(p1 , s), . . . holds(pn , s)\nab(f, a, s) \u2190\n\n\f42\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\ncoming from the translation of a non-deterministic effect proposition of the\nform\na may affectf if p1 , . . . , pm\nwill be trivially satisfied in A(\u03a60 ,\u03c3,s) . The other two rules obtained from the\nnon-deterministic effect propositions are of the form\nab(f \u0304, a, s) \u2190\n\nnot holds(f, res(a, s)),\nholds(p1 , s)), . . . , holds(pn , s), not holds(true, s)\nholds(f \u0304, res(a, s)) \u2190 not holds(f, res(a, s)),\nholds(p1 , s), . . . holds(pn , s)\n\nand they will also be removed from \u03a01(D\u03c3 ,s) to verify that A(\u03a60 ,\u03c3,s) is a belief\nset of \u03a01(D\u03c3 ,s) , since holds(f \u0304, res(a, s)) \u2208 A(\u03a60 ,\u03c3,s) , and this concludes the\nproof for this case.\n2. ab(f \u0304, a, s) holds in A(\u03a60 ,\u03c3,s) .\nSimilar to previous case.\n3. Neither ab(f, a, s) nor ab(f \u0304, a, s) are in A(\u03a60 ,\u03c3,s) .\nIn this case we will have no effect propositions of the form:\n\u2022 a causes f if p1 , . . . , pn\n\u2022 a causes f \u0304 if p1 , . . . , pn\n\u2022 a may affectf if p1 , . . . , pn\nin D\u03c3 with p1 , . . . , pn true in \u03c3. Therefore any rule r in \u03a01(D\u03c3 ,s) with predicates\ninvolving not , a and f , will be such that the body of r does not hold in\nA(\u03a60 ,\u03c3,s) , unless, possibly for those rules of the form\nholds(f, res(a, s)) \u2190 holds(f, s), not ab(f \u0304, a, s)\nholds(f \u0304, res(a, s)) \u2190 holds(f \u0304, s), not ab(f, a, s)\ninstances of the inertia rule. So we will verify these by cases,\n\u2022 holds(f, s) holds in A(\u03a6 ,\u03c3,s) , then holds(f \u0304, s) does not belong to A(\u03a6 ,\u03c3,s)\n0\n\n0\n\nand there is nothing to verify for the second rule. The first rule is transformed into\nholds(f, res(a, s)) \u2190 holds(f, s)\nand it is satisfied by A(\u03a60 ,\u03c3,s) because f is in \u03c3 and since there are no effect\npropositions of the above types, and f is true in \u03a60 (a, s), by definition of\nA(\u03a60 ,\u03c3,s) , we have holds(f, res(a, s)) \u2208 A(\u03a60 ,\u03c3,s) .\n\u2022 holds(f \u0304, s) holds in A(\u03a60 ,\u03c3,s)\nThe proof is similar to the previous case.\nTheorem B.8\nLet D be a consistent universal domain description with no knowledge laws, and\n\u03c3 be a state. If (\u03a60 , \u03c3) is a 0-specific model of D\u03c3 , then A(\u03a60 ,\u03c3,s) is a belief set of\n\u03a01D(\u03c3,s) , for any situation constant s.\nBy the above theorem we just need to prove that A(\u03a60 ,\u03c3,s) is minimal in the family\nof models of \u03a01D(\u03c3,s) . Let B be a proper subset of A(\u03a60 ,\u03c3,s) and Q some predicate\nin A(\u03a60 ,\u03c3,s) \\ B. Then Q could be a literal of one of the following five types:\n\n\fKnowledge and the Action Description Language A\n\n43\n\ni.- Q = holds(f, s), in this case there will be a fact in \u03a01(D\u03c3 ,s) not covered by B,\nso it would not be a belief set of \u03a01(D\u03c3 ,s) .\nii.- Q = holds(f, res(a, s)). f \u2208 \u03a60 (a, \u03c3) since Q is in A(\u03a60 ,\u03c3,s) ,7 therefore,\n- If there is a rule \"a causes f if p1 , . . . , pm \" in D\u03c3 with p1 , . . . , pm holding\nin \u03c3, there is a rule holds(f, res(a, s)) \u2190 holds(p1 , s), . . . , holds(pm , s)\nin \u03a01(D\u03c3 ,s) with holds(p1 , s) . . . , holds(pm , s) members of A(\u03a60 ,\u03c3,s) and by\n(i),\nholds(p1 , s) . . . , holds(pm , s) hold in B, therefore this rule will not be satisfied in B.\n- If there is a rule a may affectf if p1 , . . . , pm with p1 , . . . , pm in \u03c3, since\nholds(f \u0304, res(a, s)) can not be in B (otherwise A(\u03a60 ,\u03c3,s) would be inconsistent) and holds(f, res(a, s)) is not in B, we will have that B does not\nsatisfies the rule\nholds(f, res(a, s)) \u2190 not holds(f \u0304, res(a, s)), holds(p1 , s), . . . , holds(pm , s).\n- If there are no effect propositions in D\u03c3 involving a and f , then, we have\nthat f is in \u03c3, because in this case f \u2208 \u03a60 (a, \u03c3) if and only if f \u2208 \u03c3, and\nthe rule that will not be satisfied by B is the (ground instance of the)\ninertia rule holds(f, res(a, s)) \u2190 holds(f, s), not ab(f \u0304, a, s).\niii.- Q = holds(f \u0304, res(a, s))\nThe proof of this case is similar to the previous case.\niv.- Q = ab(f, a, s)\nIn this case we have that there is either an effect proposition of the form\na causes f if p1 , . . . , pm\nwith p1 , . . . , pm true in \u03c3 or\na may affectf if p1 , . . . , pm\nin D\u03c3 with p1 , . . . , pm true in \u03c3, and holds(f, res(a, s)) \u2208 A(\u03a60 ,\u03c3,s) .\nHence, one of the following two rules are not satisfied in B\nab(f, a, s) \u2190 holds(p1 , s) . . . holds(pm , s), not holds(true, s)\nab(f, a, s) \u2190 not holds(f \u0304, res(a, s)),\nholds(p1 , s), . . . , holds(pm , s), not holds(true, s).\nv.- Q = ab(f \u0304, a, s).\nSimilar to previous case.\nWe prove completeness in two steps. First, we show that if a belief set of \u03a01(D\u03c3 ,s) is\ndefined as in Defintion B.5 then (\u03a60 , \u03c3) is a 0-specific model of D\u03c3 . Then we show\nthat every belief set of \u03a01(D\u03c3 ,s) must be of this form.\nTheorem B.9\n7\n\nNote that by consistence of D\u03c3 (we are assuming that (\u03a60 , \u03c3) is a 0-specific model) there is no\nrule of the form \"a causes \u00acf if p1 , . . . , pm \" in D\u03c3 with p1 , . . . pm holding in \u03c3.\n\n\f44\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nLet D be a consistent universal domain description with no knowledge laws, \u03a60 a\n0-interpretation, and \u03c3 a state. If A(\u03a60 ,\u03c3,s) is a belief set of \u03a01(D\u03c3 ,s) , then (\u03a60 , \u03c3) is\na 0-specific model of D\u03c3 .\nProof\nLet A(\u03a60 ,\u03c3,s) be a belief set of \u03a01(D\u03c3 ,s) . Clearly, by construction, \u03c3 is an initial state\nof D\u03c3 . Now let f be a fluent such that there is an effect proposition of the form\n\"a causes f if p1 , . . . , pm \" in D\u03c3 , and assume that p1 , . . . , pm hold in \u03c3. Then, by\nconstruction, there is a rule of the form\nholds(f, res(a, s)) \u2190 holds(p1 , s), . . . , holds(pm , s)\nin \u03a01(D\u03c3 ,s) such that holds(p1 , s), . . . , holds(pm , s) \u2208 A(\u03a60 ,\u03c3,s) .\nTherefore holds(f, res(a, s)) \u2208 A(\u03a60 ,\u03c3,s) and hence f \u2208 \u03a60 (a, \u03c3). The proof is\nanalogous for effect propositions of the form a causes \u00acf if p1 , . . . , pm . If f is a\nfluent such that there are no effect propositions of the above two types we have two\npossible situations.\n(i) If there are no non-deterministic effect propositions of the form\na may affectf if p1 , . . . , pm\nwith p1 , . . . , pm holding in \u03c3, we will have that there are no rules in \u03a01(D\u03c3 ,s) such\nthat ab(f, a, s) appears in the head of the rule and whose body holds in A(\u03a60 ,\u03c3,s) .\nTherefore by the rules\nholds(f, res(a, s)) \u2190 holds(f, s), not ab(f, a, s) and\nholds(f \u0304, res(a, s)) \u2190 holds(f \u0304, s), not ab(f, a, s),\n(ground instances of the inertia rule), we will have that either holds(f, res(a, s)) is\nin A(\u03a60 ,\u03c3,s) or holds(f \u0304, res(a, s)) is in A(\u03a60 ,\u03c3,s) , since either f \u2208 \u03c3 or f 6\u2208 \u03c3, forcing\neither holds(f, s) or holds(f \u0304, s) to be in A(\u03a60 ,\u03c3,s) . Thus, holds(f, res(a, s)) (resp.\nholds(f \u0304, res(a, s))) is in A(\u03a60 ,\u03c3,s) if and only if holds(f, s) (resp. holds(f \u0304, s)) is in\nA(\u03a60 ,\u03c3,s) . Therefore f \u2208 \u03a60 (a, \u03c3) if and only if f is in \u03c3.\n(ii) On the other hand, if there is a proposition of the form\na may affectf if p1 , . . . , pm\nin D\u03c3 with p1 , . . . , pm holding in \u03c3, by construction, we will have in \u03a01(D\u03c3 ,s) the\nfollowing rules\nholds(f, res(a, s))\nholds(f \u0304, res(a, s))\nab(f, res(a, s))\nab(f \u0304, an , res(a, s))\n\n\u2190 not holds(f \u0304, res(a, s)),\nholds(p1 , s), . . . , holds(pm , s)\n\u2190 not holds(f, res(a, s)),\nholds(p1 , s), . . . , holds(pm , s)\n\u2190 not holds(f \u0304, res(a, s)), not holds(true, s),\nholds(p1 , s), . . . , holds(pm , s)\n\u2190 not holds(f, res(a, s)), not holds(true, s),\nholds(p1 , s), . . . , holds(pm , s)\n\nThus, since A(\u03a60 ,\u03c3,s) is a belief set of \u03a01(D\u03c3 ,s) and the holds(pi , s) are assumed to\n\n\fKnowledge and the Action Description Language A\n\n45\n\nbelong to A(\u03a60 ,\u03c3,s) for every i, then either holds(f, res(a, s)) or holds(f \u0304, res(a, s))\nmust be in A(\u03a60 ,\u03c3,s) , but not both. Therefore it does not matter if f is or is not\npart of \u03a60 (a, \u03c3). Hence (\u03a60 , \u03c3) is a 0-specific model of D\u03c3 .\nObserve that for any domain description D, any state \u03c3 and any initial situation\nconstant s, a set of predicates A will be a belief set of \u03a01(D,s) if and only if A\nS\nis the union of belief sets of \u03a0(D,s,a) , for each possible action a. A = {Aa :\na is a possible action} with each Aa a belief set of \u03a0(D,s,a) . This is because if a1\nand a2 are two different actions then none of the predicates in rules in \u03a0(D,s,a1 )\nappear in any predicate of any rule in \u03a0(D,s,a2 ) , so the computation of the belief\nsets for one of the programs does not affect the computation for the other one.\nTheorem B.10\nGiven a consistent domain description D, and a situation constant s. If A is a belief\nset for \u03a01(D,s) , then there exists a state \u03c3, and a 0-specific model \u03a60 of D\u03c3 such\nthat A = A(\u03a60 ,\u03c3,s) .\nProof\nBy definition of \u03a01D,s , A must be complete. That is, for any fluent f we have\nthat either holds(f, s) is in A or holds(f \u0304, s) is in A. Thus, if we let \u03c3 = {f :\nholds(f, s) \u2208 A} and \u03a60 be such that for any possible action a, f \u2208 \u03a60 (a, \u03c3) if\nand only if holds(f, res(a, s)) \u2208 A, we will have by completeness that f 6\u2208 \u03c3 if and\nonly if holds(f \u0304, s) is in A and f 6\u2208 \u03a60 (a, s) if and only if holds(f \u0304, res(a, s)) \u2208 A.\nMoreover, if some predicate ab(f, a, s) is in A then one of the following facts holds:\n\u2022 There is a rule in \u03a01(D,s) whose body is\nholds(p1 , s), . . . , holds(pm , s), not holds(true, s)\nand whose head is ab(f, a, s) such that holds(pi , s) \u2208 A for any i = 1, . . . , m,\nand holds(true, s) 6\u2208 A. Thus, there must be an effect proposition of the form\n\"a causes f if p1 , . . . , pm \" in D with p1 , . . . , pm true in \u03c3.\n\u2022 There is a rule in \u03a01(D,s) whose body is\nnot holds(f \u0304, res(a, s)), holds(p1 , s), . . . , holds(pm , s), not holds(true, s)\nwhose head is ab(f, a, s) such that holds(f, res(a, s)) and each holds(pi , s)\nare in A, for each i = 1, . . . , m, and holds(true, s) is not in A. Therefore,\nin this case there exists a non-deterministic effect proposition of the form\n\"a may affectf if p1 , . . . , pm \" in D with f, p1 , . . . pm true in \u03c3.\nIf for some fluent f , ab(f, a, s) is in A, we will have by similar reasons that there\nexists a proposition of the form\na causes \u00acf if p1 , . . . , pm or a may affectf if p1 , . . . , pm\nin D such that p1 , . . . , pm hold in \u03c3 and f 6\u2208 \u03a60 (a, \u03c3).\nSo we have proved that A = A(\u03a60 ,\u03c3,s) , and by Theorem B.10, (\u03a60 , \u03c3) is a 0-specific\nmodel.\nWe now extend the proof to handle sensing actions. Let M od0 (D, \u03a3) denote the\nset {(\u03a60 , \u03c3) : 0-specific model of D, \u03c3 \u2208 \u03a3}, where \u03a3 is the set of initial states\n\n\f46\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nof D. Note that the set can be empty if there is no state in \u03a3 that is an initial state\nof D.\nDefinition B.11\nGiven a consistent situation \u03a3, and an interpretation \u03a6. (\u03a6, \u03a3) will be a 1-specific\nmodel of a consistent domain description D if:\n1. \u03a3 is an initial situation of D.\nS\nS\n2. For any non-sensing action a, \u03a6(a, \u03a3) = \u03a60 \u2208Mod0 (D,\u03a3) \u03c3\u2208\u03a3 {\u03a60 (a, \u03c3)}.\n3. For each sensing action a, if\na causes to know f1 if \u03c61\n..\n.\na causes to know fs if \u03c6sa\nare all the knowledge laws in D where a occurs. Then, \u03a6(a, \u03a3) must be conT\nsistent and if sa = 0, \u03a6(a, \u03a3) = \u03a3; otherwise \u03a6(a, \u03a3) = l=1,...,s \u03a3l such\nthat each \u03a3l is a situation (fl , \u03c6l )-compatible with \u03a3. (Recall that since D is\nsimple, all the fi are different).\nDefinition B.12\nLet (\u03a6, \u03a3) be a 1-specific model of a domain description D. Denote by AssoD (\u03a6, \u03a3)\nthe set of 0-specific models of D such that for any non-sensing action a, \u03a6(a, \u03a3) =\nS\n\u03c3\u2208\u03a3 {\u03a60 (a, \u03c3)}.\nLet a be a sensing action. Define:\nAa(\u03c3,s) = {holds(f, res(a, s)) : \u03c3 |= f with f a fluent literal } if \u03c3 \u2208 \u03a6(a, \u03a3).\nOtherwise,\nAa(\u03c3,s) = {holds(true, res(a, s))} \u222a {holds(f, res(a, s)) : f fluent literal }.\nS\nLet A\u2032(\u03a60 ,\u03c3,s) = A(\u03a60 ,\u03c3,s) \u222a a\u2208Sensing Aa(\u03c3,s) .\nLet A(\u03a6,\u03a3,s) = {A\u2032(\u03a60 ,\u03c3,s) : (\u03a60 , \u03c3) \u2208 AssoD (\u03a6, \u03a3)}.\nAs a straightforward consequence of this definition we have that for any fluent f\nand any action a, \u03a6(a, \u03a3) |= f iff A(\u03a6,\u03a3,s) |= holds(f, res(a, s)).\nFor any set A of literals we will denote by \u03c3A the state \u03c3A = {f : holds(f, s) \u2208 A}\nWe will denote by Dn.s the set of value and effect propositions in D, and by Dsen\nthe set of knowledge laws in D.\nAs a corollary of the theorems [B.8, B.10] we will have the soundness and completeness of the logic program translation for the execution of a single action (sensing or not). The next corollary shows soundness and Corollary B.14 shows completeness.\nCorollary B.13\nLet D be a consistent domain description. If (\u03a6, \u03a3) is a 1-specific model of D then\nA(\u03a6,\u03a3,s) is a world view of \u03a01(D,s) .\n\n\fKnowledge and the Action Description Language A\n\n47\n\nProof\nLet us suppose that (\u03a6, \u03a3) is a 1-specific model of D. We will prove that A(\u03a6,\u03a3,s)\nis a world view of \u03a01(D,s) . In other words, we will show that A(\u03a6,\u03a3,s) is the collection of belief sets of [\u03a01(D,s) ]A(\u03a6,\u03a3,s) (see Section 6 for the definition of [\u03a0]A\n\u2032\nand [\u03a0]A\nA ). Given an A \u2208 A(\u03a6,\u03a3,s) , let A = A(\u03a60 ,\u03c3,s) , and denote by \u03a0 the pro1\n1\nA\ngram [\u03a0(D,s) \\ \u03a0(Dn.s ,s) ]A(\u03a6,\u03a3,s) which is equal to [\u03a01(Dsen ,s) ]A\nA(\u03a6,\u03a3,s) union all the\nrules of the form holds(true, res(a, s)) \u2190 holds(true, s) and holds(f, res(a, s)) \u2190\nholds(true, res(a, s)) where f is a fluent literal and a is a sensing action.\nThe set U = lit(\u03a01(Dn.s ,s) ) split [\u03a01(D,s) ]A(\u03a6,\u03a3,s) , and by theorem B.8 A(\u03a60 ,\u03c3,s) is\na belief set of \u03a01(Dn.s ,s) , moreover bU (\u03a01(D,s) ) = \u03a01(Dn.s ,s) and any answer set of\neU ([\u03a01(D,s) ]A(\u03a60 ,\u03c3,s) \\ \u03a01(Dn.s ,s) , A\u03a60 ,\u03c3,s) ) is a belief set of A\u03a60 ,\u03c3,s) ).\nHence by Splitting Lemma we only need to prove that A is a belief set of A(\u03a60 ,\u03c3,s) \u222a\n\u03a0. We first prove that all the rules in the program hold in A and then we show that\nA is minimal.\nObviously any fact in A(\u03a60 ,\u03c3,s) \u222a \u03a0 is in A, thus we will prove that any rule R in\n\u03a0 holds in A. Let, for a given sensing action a,\na causes to know f1 if p11 , . . . , p1n1\n..\n.\na causes to know fsa if ps1a , . . . , psnasa\nTsa\nbe all the knowledge laws in D, involving a, and \u03a6(a, \u03a3) = l=1\n\u03a3l where each \u03a3l\nl\nl\nis (fl , p1 , . . . , pnl )-compatible with \u03a3. The rules R in \u03a0 that mention a either in its\nbody or in its head will be evaluated as follows:\n1. If \u03a3 |= fl or \u03a3 |= \u00acfl , there are no rules in the program with the predicate\nholds(true, res(a, s)) in the head that are not ground instances of domain\nindependent rules ( because any rule in \u03a01(Dsen ,s) will be removed, to get\n[\u03a01(Dsen ,s) ]A\nA(\u03a6,\u03a3,s) after checking\n\u00acKholds(f1 , res(a, s)), \u00acKholds(f \u03041 , res(a, s))).\n2. If \u03a3 6|= fl and \u03a3 6|= \u00acfl , then either\n(a) \u03a6(a, \u03a3) |= pl1 , . . . , plnl , and \u03a6(a, \u03a3) |= fl . In this case, R must be one of\nthe following:\nholds(true, res(a, s)) \u2190 holds(p\u0304l1 , s)\n..\n.\nholds(true, res(a, s)) \u2190 holds(p\u0304lnl , s)\nholds(true, res(a, s)) \u2190 holds(f \u0304l , s)\nand each of these rules are verified in A, because; if holds(f \u0304l , s) is in A or\nfor some i holds(p\u0304li , s) \u2208 A then \u03c3A |= p\u0304li or \u03c3A |= f \u0304l and in both cases\n\u03c3A 6\u2208 \u03a6(a, \u03a3), and hence, holds(true, res(a, s)) \u2208 A,by definition of Aa(\u03c3,s) .\n(b) \u03a6(a, \u03a3) |= pl1 , . . . , plnl , and \u03a6(a, \u03a3) |= f \u0304l This case is similar to (a) changing\nthe last rule for\nholds(true, res(a, s)) \u2190 holds(fl , s).\n\n\f48\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n3. \u03a3 6|= fl and \u03a3 6|= \u00acfl , and there exists i = 1, . . . , nl , such that \u03a6(a, \u03a3) |6 = pli .\nIn this case, there is only one rule that remains in the program;\nholds(true, res(a, s)) \u2190 holds(pl1 , s), . . . , holds(plnl , s)\nNow, if for any i = 1, . . . , nl , holds(pli , s) \u2208 A, then \u03c3A |= pli , for every\ni = 1, . . . , nl , and \u03c3A 6\u2208 \u03a6(a, \u03a3). Thus, holds(true, res(a, s)) \u2208 A.\n4. From the domain independent rules, R could also be of the form\nholds(true, res(a, s)) \u2190 holds(true, s)\nBut, by construction of A, holds(true, s) 6\u2208 A, and thus, R is satisfied by A.\n5. The last rules to consider, also coming from the domain independent rules, are\nall the rules of the form holds(f, res(a, s)) \u2190 holds(true, res(a, s)), where f\nis a fluent literal. If holds(true, res(a, s)) belongs to A then, by construction,\n\u03c3A 6\u2208 \u03a6(a, \u03a3). Therefore, by construction too, holds(f, res(a, s)) also belongs\nto A, for every f , fluent literal.\n\nTo prove the minimality of A, let C be a proper subset of A and h a predicate in\nA \\ C.8 We will find a rule R in [\u03a01(D,s) ]A\nA(\u03a6,\u03a3,s) such that R does not hold in C.\nIf h is of the form ab(f, a, s), holds(f, s) or holds(f, res(a, s)), with a a nonsensing action, then R can be found in [\u03a01(Dn.s ,s) ]A\nA(\u03a6,\u03a3,s) by Theorem [B.8]. Thus,\nall these h must be in C. Therefore, it suffices to consider the case when h =\nholds(f, res(a, s)), with a a sensing action. If h = holds(f, res(a, s)), with f a\nfluent literal, and the rule holds(f, res(a, s)) \u2190 holds(true, res(a, s)) is satisfied\nin C, then holds(true, res(a, s)) 6\u2208 C, in which case, if holds(true, res(a, s)) is in\nA\u2032(\u03a60 ,\u03c3,s) = A then \u03c3 6\u2208 \u03a6(a, \u03a3), hence we will have that there exists an l = 1, . . . , sa ,\nsuch that \u03c3 6\u2208 \u03a3l ,then since \u03a6(a, \u03a3) is fl , pl1 , . . . , pnl -compatible with \u03a3 and the\nremark at the end of [B.12] we will have that one rule R of the following:\nholds(true, res(a, s))\nholds(true, res(a, s))\nholds(true, res(a, s))\n\n\u2190 holds(f \u0304l , s)\n\u2190 holds(fl , s)\n\u2190 holds(p\u0304l1 , s)\n..\n.\n\nholds(true, res(a, s))\nholds(true, res(a, s))\n\n\u2190 holds(p\u0304lnl , s)\n\u2190 holds(pl1 , s), . . . , holds(plnl , s)\n\nhas to be such that both i) R \u2208 [\u03a01(Dsens ,s) ]A\nA(\u03a6,\u03a3,s) and ii) the fluent literals appearing on the body of R will be in \u03c3A . Hence the body of R will be true in\nA and therefore in C, thus we can conclude that R is not satisfied by C. If\nholds(true, res(a, s)) is not in A, then \u03c3 \u2208 \u03a6(a, \u03a3) and holds(f, res(a, s)) is in\nA, but this happens if and only if f \u2208 \u03c3, which is true iff holds(f, s) \u2208 A, and the\ninertia rule will not be true in C. To complete the proof we need to show that any\nbelief set of [\u03a01(D,s) ]A(\u03a6,\u03a3,s) is of the form A\u2032(\u03a60 ,\u03c3,s) , for some (\u03a60 , \u03c3) \u2208 AssoD (\u03a6, \u03a3).\nTake now A, a belief set of [\u03a01(D,s) ]A(\u03a6,\u03a3,s) . By the splitting lemma, the set A0 =\nA \\ {holds(f, res(a, s)) : a sensing action }, is a belief set of \u03a01(Dn.s ,s) . Then by\n8\n\nRecall that A = A\u2032(\u03a6\n\n0 ,\u03a3,s)\n\n.\n\n\fKnowledge and the Action Description Language A\n\n49\n\n(B.10) there exists (\u03a60 , \u03c3) in AssoD (\u03a6, \u03a3) such that A0 = A(\u03a60 ,\u03c3,s) = A(\u03a60 ,\u03c3A ,s)\nS\ntaking \u03a6(a, \u03a3) = \u03c3\u2208\u03a3 {\u03a60 (a, \u03c3)}, it only remains to be shown that for any sensing\naction a, both of the following are satisfied: (i) If \u03c3A \u2208 \u03a6(a, \u03a3) then f \u2208 \u03c3A \u21d4\nholds(f, res(a, s)) \u2208 A, and (ii) \u03c3A 6\u2208 \u03a6(a, \u03a3) \u21d4 holds(true, res(a, s)) \u2208 A.\nFor case (i), let \u03c3A \u2208 \u03a6(a, \u03a3). Then, for any l = 1, . . . , sa , \u03c3A \u2208 \u03a3l . The rules with\nheads of the form holds(f, res(a, s)) and f a fluent literal are: holds(f, res(a, s)) \u2190\nholds(true, res(a, s)) and the one of the ground instances of the inertia rule. The\nbody of the first rule is false in A because any rule with holds(true, res(a, s)) in its\nhead must have its body false in A. Then, holds(f, res(a, s)) \u2208 A \u21d4 holds(f, s) \u2208\nA \u21d4 f \u2208 \u03c3A .\nFor (ii), holds(true, res(a, s)) \u2208 A if and only if there exists a rule R which body is\ntrue in A and its head holds(true, res(a, s)). Hence, R must be one of the following\nrules:\nholds(true, res(a, s))\nholds(true, res(a, s))\nholds(true, res(a, s))\n\n\u2190 holds(f \u0304l , s)\n\u2190 holds(fl , s)\n\u2190 holds(p\u0304l1 , s)\n..\n.\n\nholds(true, res(a, s))\nholds(true, res(a, s))\n\n\u2190 holds(p\u0304lnl , s)\n\u2190 holds(pl1 , s), . . . , holds(plnl , s)\n\nfor some l = 1, . . . , sa , and in any case, the rule R belongs to [\u03a01(D,s) ]A(\u03a6,\u03a3,s) and\nits body is true in A, if and only if \u03c3A 6\u2208 \u03a3l . Hence, \u03c3A 6\u2208 \u03a6(a, \u03a3).\nCorollary B.14\n(Completeness) Let D be a consistent domain description. If A is a world view of\n\u03a01(D,s) then there exists a 1-specific model (\u03a6, \u03a3) of D such that A = A(\u03a6,\u03a3,s) .\nProof\nLet A be a world view of \u03a01(D,s) . Let \u03a3 = {\u03c3A : A \u2208 A}. If A \u2208 A, \u03a6A\n0 will be a 0interpretation such that A(\u03a6A\n= A \\ {holds(f, res(a, s)) : a is a sensing action\n0 ,\u03c3A ,s)\nand f is true or a fluent literal}, which can be found making use of Theorem [B.10].\nWe define an interpretation \u03a6 such that AssoD (\u03a6, \u03a3) = {(\u03a6A\n0 , \u03c3A ) : A \u2208 A}, and\n\u03a6(a, \u03a3) = {\u03c3A : holds(true, res(a, s)) 6\u2208 A} for any sensing action a. Note that if\n(\u03a6, \u03a3) is a 1-specific model of D then A = A\u2032(\u03a6A ,\u03c3A ,s) for any A \u2208 A. Thus, we will\n0\nshow that (\u03a6, \u03a3) is a 1-specific model of D and we will have that A = A(\u03a6,\u03a3,s) .\nIt is clear that \u03a3 is the initial situation of D. Then, if a is a non-sensing action,\nS\nS\nby definition, \u03a6(a, \u03a3) = {{\u03a6A\n{{\u03a60 (a, \u03c3)} : (\u03a60 , \u03c3) \u2208\n0 (a, \u03c3A )} : A \u2208 A} =\nAssoD (\u03a6, \u03a3)}.\nIf a is a sensing action and, a causes to know fl if pl1 , . . . , plnl , l = 1, . . . , sa\nare exactly the knowledge laws where a appears, we need to show that for each l =\n1, . . . , sa , there exists a \u03a3l , (fl , pl1 , . . . , plnl )-compatible with \u03a3 such that \u03a6(a, \u03a3) =\nT\nl=1,...,sa \u03a3l .\n1. If A |= holds(fl , s) or A |= holds(f \u0304l , s) then let \u03a3l = \u03a3.\n\n\f50\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n2. If A 6|= holds(fl , s) and A 6|= holds(f \u0304l , s) and A |= holds(pli , res(a, s)), for\ni = 1, . . . , nl , then:\n\n(a) if A |= holds(fl , res(a, s)), then let \u03a3l = {\u03c3 \u2208 \u03a3 : \u03c3 |= pl1 , . . . , plnl , fl }.\n(b) if A |= holds(f \u0304l , res(a, s)), then let \u03a3l = {\u03c3 \u2208 \u03a3 : \u03c3 |= pl1 , . . . , plnl , f \u0304l }.\n3. If A 6|= holds(fl , s) and A 6|= holds(f \u0304l , s) and A 6|= holds(pli , res(a, s)), for\nsome i = 1, . . . , nl , we let \u03a3l = {\u03c3 \u2208 \u03a3 : \u2203k = 1, . . . , nl , \u03c3 6|= plk }.\nTsa\n\u03a3l . For that, we will prove that for\nWe need to show next that \u03a6(a, \u03a3) = l=1\nT\nany \u03c3A \u2208 \u03a3, \u03c3A 6\u2208 l=1,...,sa \u03a3l if and only if holds(true, res(a, s)) \u2208 A.\nT\nFirst we will show that if \u03c3A 6\u2208 l=1,...,sl \u03a3l , then holds(true, res(a, s)) \u2208 A. Let\nk be an l such that \u03c3A 6\u2208 \u03a3l . Thus, since \u03c3A \u2208 \u03a3 then \u03a3 6|= fk and \u03a3 6|= \u00acfk and\ncase (1) above does not occur, and one of the following cases must hold:\n2.a. A |= holds(pki , res(a, s)), for every i = 1, . . . , nk , and A |= holds(fk , res(a, s)).\nTherefore, either \u03c3A 6|= pkj for some j = 1, . . . , nk or \u03c3A 6|= fk . Hence,\nholds(p\u0304kj , s) \u2208 A or holds(f \u0304k , s) \u2208 A and the following rules will be part\nof [\u03a01(D,s) ]A\nA(\u03a6,\u03a3,s) :\nholds(true, res(a, s)) \u2190 holds(p\u0304kj , s)\nholds(true, res(a, s)) \u2190 holds(f \u0304k , s)\nThus, holds(true, res(a, s)) \u2208 A.\n2.b. A |= holds(pki , res(a, s)), for i = 1, . . . , nk , and A |= holds(f \u0304k , res(a, s)), is\nsimilar to (2.a).\n3. A 6|= holds(pki , res(a, s)), for some i = 1, . . . , nk . Therefore, for every j =\n1, . . . , nk , \u03c3A |= pkj . Hence A |= holds(pkj , s) and the following rule will be\npart of [\u03a01(D,s) ]A\nA(\u03a6,\u03a3,s) :\nholds(true, res(a, s)) \u2190 holds(pk1 , s), . . . , holds(pknk , s)\nThus, holds(true, res(a, s)) \u2208 A.\nFor the other direction, assume holds(true, res(a, s)) \u2208 A. Then, it must be the\ncase that there exists a rule in [\u03a01(D,s) ]A\nA(\u03a6,\u03a3,s) with holds(true, res(a, s)) in the\nhead and its body true in A. Note that this rule cannot be holds(true, res(a, s)) \u2190\nholds(true, s) by the construction of A. Thus, A 6|= holds(f, s) and A 6|= holds(f \u0304, s);\notherwise there will be no rule in [\u03a01(D,s) ]A\nA(\u03a6,\u03a3,s) with holds(true, res(a, S)) in its\nhead (these are ground instances of rules derived from knowledge laws). We will\ninspect the remaining rules with holds(true, res(a, s)) in the head and we will show\nthat there exists \u03a3k such that \u03c3A 6\u2208 \u03a3k .\n1. If the rules are of the form:\nholds(true, res(a, s)) \u2190 holds(p\u0304kj , s)\nholds(true, res(a, s)) \u2190 holds(f \u0304k , s)\nthen A |= holds(pki , res(a, s)), for i = 1, . . . , nk , and A |= holds(fk , res(a, s)).\nTherefore, since holds(p\u0304kj , s) or holds(f \u0304k , s) has to belong to A, \u03c3A 6\u2208 \u03a3k .\n2. If the rules are of the form:\nholds(true, res(a, s)) \u2190 holds(p\u0304kj , s)\nholds(true, res(a, s)) \u2190 holds(fk , s)\nthen, similar to 1, A |= holds(pki , res(a, s)), for i = 1, . . . , nk , and A |=\n\n\fKnowledge and the Action Description Language A\n\n51\n\nholds(f \u0304k , res(a, s)). Therefore, since holds(p\u0304kj , s) or holds(fk , s) has to belong\nto A, \u03c3A 6\u2208 \u03a3k .\n3. If the rule is of the form:\nholds(true, res(a, s)) \u2190 holds(pk1 , s), . . . , holds(pknk , s)\nthen A 6|= holds(pki , res(a, s)), for some i = 1, . . . , nk . Therefore, since for\nevery j = 1, . . . , nk holds(pkj , s) has to be in A, \u03c3A 6\u2208 \u03a3k .\nThe next step is to show soundness and completeness for sequences of actions.\nSequences of actions are the most simple plans. We then extend the proof to plans\nof all classes. The general proof will be by induction on the complexity of the plans.\nThus, we start by formally defining complexity and other definitions required for\nthe inductions.\nDefinition B.15\nWe will define the complexity of a plan \u03b2 (comp(\u03b2)) by: if the empty plan is [],\ncomp([]) = 0. For an action a comp(a) = 1. For complex plans, comp( if \u03c6 then \u03b1)\nand comp( while \u03c6 do \u03b1) is comp(\u03b1) + 1 and comp( if \u03c6 then \u03b11 else \u03b12 ) and\ncomp([\u03b11 , \u03b12 ] is comp(\u03b11 ) + comp(\u03b12 )\nWe will say that a plan \u03b1 is an n-plan if it has complexity n, it will be an \u2264 nplan if it has complexity less or equal than n. Pn will denote the set of n-plans, and\nP\u2264n the set of \u2264 n-plans.\nWe define the complexity of a situation constant s inductively as 0 if s = s0 ; or\n1 plus the complexity of s\u2032 if s = res(a, s\u2032 ), for any action a. A situation s will be\ncalled an n-situation if its complexity is n. The complexity of a predicate of the\nform holds(f, s) with f a fluent literal, or holds(true, s), will be the complexity of\ns, the complexity of predicates of the form ab(f, a, s), with f a fluent literal and a\nan action will be equal to the complexity of s plus one, the complexity of predicates\nof the form f ind situation(\u03b2, s1 , s) will be the complexity of \u03b2 plus the complexity\nof s1 , and the complexity of a predicate of the form holds af ter plan(F, \u03b2) will be\nthe complexity of the plan \u03b2. We will say that a predicate h is an \u2264 n-predicate, if\nh has complexity m and m \u2264 n. Given a plan \u03b1, [\u03b11 ] will denote the plan \u03b1 and\n[\u03b1n+1 ] will denote the plan [\u03b1|[\u03b1n ]]. Denote by \u03a0nD the subprogram of \u03a0D restricted\nto those rules in \u03a0D with \u2264 n-predicates. Note that in any k-predicate in \u03a0nD , the\nconstant situation is a sequence of k actions.\nGiven a domain description D denote by Dr the sub-domain of D obtained when\nwe remove from D any value proposition.\nDefinition B.16\nFor any n > 0, we will say that a pair (\u03a6, \u03a3) where \u03a6 is an interpretation and \u03a3 a\nsituation, is an n + 1-specific model of D if and only if it is an n-specific model of D\nand for any sequence of actions seqn = a1 , . . . , an , (\u03a6, \u0393\u03a6 ([seqn ], \u03a3)) is a 1-specific\nmodel of Dr (i.e. D minus the value propositions). (\u03a6, \u03a3) will be a specific model\nof D if it is an n-specific model of D for any n \u2265 1.\nDefinition B.17\n\n\f52\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\nGiven a sequence of actions seq = a1 , . . . , an and a situation constant s, res((seq), s)\ndenotes the situation constant res(an , . . . , res(a1 , s)), seq\u2205 denotes the empty sequence and res((seq\u2205 ), s) will be equal to s. Let Actn be the set of all the sequences\nof n actions. For any set of literals A we take \u03c3(A,(seq)) as the state such that\nf \u2208 \u03c3(A,(seq)) \u21d4 holds(f, res((seq), s0 )) \u2208 A.\nDefinition B.18\nGiven a pair (\u03a6, \u03a3) of interpretation and situation, n \u2265 0 and the situation constant\ns0 , we will denote by An+1\n(\u03a6,\u03a3,s0 ) the following family of sets:\n\u2022 if n = 1, A1(\u03a6,\u03a3,s0 ) = A(\u03a6,\u03a3,s0 )\n\u2022 If n \u2265 1, let for any set A of (\u2264 n + 1)-predicates, An and A1 denote the sets\nof (\u2264 n)-predicates in A and (n + 1)-predicates in A (resp.). Then, An+1\n(\u03a6,\u03a3,s0 )\nis defined as a family of sets A of (\u2264 n + 1)-predicates, such that the following\nis satisfied:\n1. An \u2208 An(\u03a6,\u03a3,s0 )\nS\n2. If holds(true, (seqn , s0 )) is in An then A1 = seqn \u2208Actn A(seqn ) with A(seqn )\nthe set of all the predicates of the form holds(true, res((seqn , a), s0 )) or\nholds(f, res((seqn , a), s0 ))\nS\n3. If holds(true, (seqn , s0 )) is not in An then A1 = An \u222a seqn \u2208Actn A(seqn )\nwhere A(seqn ) is A\u2032(\u03a60 ,\u03c3(A ,seq ) ,res((seqn ),s0 )) for some (\u03a60 , \u03c3(An ,(seqn )) ) in\nn\nn\nAssoDr (\u03a6, \u0393\u03a6 ((seqn ), \u03a3).\nLemma B.19\nGiven n > 0, a consistent domain description D, a pair (\u03a6, \u03a3), n-specific model of\nD, the initial situation constant s0 . We will have that for any fluent literal f and\nany sequence of actions seq = a1 , . . . , an , holds(f, res((seq), s0 ) holds in An(\u03a6,\u03a3,s0 )\nif and only if f \u2208 \u0393\u03a6 ([seq], \u03a3).\nProof\nThe proof is by induction on n and it is straightforward from the definition of\nAn+1\n(\u03a6,\u03a3,S)\nThe next corollary proves by induction on the length of the sequence of actions\nthat the logic program translation is sound and complete for the execution of a\nsequence of actions.\nCorollary B.20\nGiven a consistent domain description D and n > 0. A is a world view of \u03a0nD if\nand only if there exists a pair (\u03a6, \u03a3), n-specific model of D such that\nA = An(\u03a6,\u03a3,s0 )\nProof\n\u2022 The base case (n = 0) follows from B.13 and B.14.\n\u2022 Suppose the result is valid for any m \u2264 n.\n\n\fKnowledge and the Action Description Language A\n\n53\n\n\u2022 (\u21d2) Let A be a world view of \u03a0n+1\nD . As in Definition B.18, define for any\nA \u2208 A, An to be the subset of A restricted to those predicates in A involving just\n\u2264 n-predicates, so An = {An : A \u2208 A} will be a world view of \u03a0nD , and by inductive\nhypothesis there is a pair (\u03a61 , \u03a3), n-specific model of D such that An = An(\u03a61 ,\u03a3,s0 ) .\nWe first define an interpretation \u03a6 such that (\u03a6, \u03a3) is n + 1-specific model\nof D. Given an action a, let A\u2032\u2032 be the set {holds(f, res((seqn ), s0 )) \u2208 An :\nholds(true, res((seqn ), s0 )) 6\u2208 An \u2227 seqn \u2208 Actn } and A1 be A\u2032\u2032 union\n[{holds(f, res((seqn , a), s0 )) : seqn is any sequence of n actions}\n\u222a{ab(f, a, res((seqn ), s0 )) : seqn is any sequence of n actions}\n\u222a{holds(true, res((seqn , a), s0 )) : seqn is any sequence of n actions}] \u2229 A\nNote that A = An \u222a A1 . By the splitting lemma, A1 is a belief set of A\u2032\u2032 \u222a\nS\n1\nA\nseqn \u2208Actn [\u03a0(Dr ,res((seqn ),s0 ) ]A .\nHence by B.14 there exists an interpretation \u03a62 and a 0-interpretation \u03a60 such\nthat for any sequence of n actions seqn with holds(true, res((seqn ), s0 )) 6\u2208 An , the\nfollowing properties are satisfied:\n1. (\u03a62 , \u0393\u03a61 ([seqn ], \u03a3)) is a 1-specific model of Dr ,\n2. (\u03a60 , \u03c3(An ,(seqn )) ) is in AssoDr (\u03a62 , \u0393\u03a61 ([seqn ], \u03a3)).\n3. A1 =\nS \u2032\n{A(\u03a60 ,\u03c3(A ,(seq )) ,res((seqn ),s0 )) : seqn \u2208 Actn \u2227 holds(true, res((seqn ), s0 )) 6\u2208\nn\nn\nAn } \u222a A\u2032\u2032 .\nDefining \u03a6 such that for any sequence of actions seqn \u2208 Actn if the atomic\nformula holds(true, res((seqn ), s0 )) is not in An , then \u0393\u03a6 ([seqn ], \u03a3) is equal to\n\u0393\u03a61 ([seqn ], \u03a3), we will have that (\u03a6, \u03a3) is an n-specific model of D. Moreover if\nwe take \u03a6 such that for any action a and any sequence of actions seqn \u2208 Actn\n, \u03a6(a, \u0393\u03a6 ([seqn ], \u03a3)) = \u03a62 (a, \u0393\u03a6 ([seqn ], \u03a3)), we will have that for any sequence of\nactions seqn \u2208 Actn , (\u03a6, \u0393\u03a6 ([seqn ], \u03a3)) is a 1-specific model of Dr . Therefore, (\u03a6, \u03a3)\nis an n + 1-specific model of D.\nIt is clear by construction that either the predicate holds(true, res((seqn ), s0 ))\nis a member of An , and by the inertia rules, A1 is the set of predicates of the\nform holds(true, res((seqn , a), s0 )) and holds(f, res((seqn , a), s0 )) with a an action,\n(seqn ) a sequence of n actions and f a fluent literal, or holds(true, res((seqn ), s0 )) 6\u2208\nAn and for any sequence of n actions seqn the following propositions hold:\n1. (\u03a60 , \u03c3(An ,(seqn )) ) \u2208 AssoDr (\u03a6, \u0393\u03a6 ([seqn ], \u03a3)),\n2. A1 = A\u2032(\u03a60 ,\u03c3(A ,(seq )) ,res((seqn ),s0 ))\nn\n\nn\n\nTherefore by definition of An+1\n(\u03a6,\u03a3,s0 ) we have proved that A \u2208 A if and only if it\nn+1\nis in A(\u03a6,\u03a3,s0 ) .\n(\u21d0) Reciprocally, let (\u03a6, \u03a3) be an (n + 1)-specific model of D. Then, by B.13,\ninduction and by the splitting lemma, to prove that An+1\n(\u03a6,\u03a3,s0 ) is a world view of\nn+1\n\u03a0n+1\n,\nit\nsuffices\nto\nshow\nthat\nfor\nany\nA,\nA\n\u2208\nA\nif\nand only if A = An \u222a BA\nD\n(\u03a6,\u03a3,s0 )\nwhere An is the set of predicates in A of \u2264 n-complexity and BA is a belief set of\nthe program\n\n\f54\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\n\n\u03a0 = An \u222a [\u03a0n+1\n\u2212 \u03a0nD ]An+1\nD\n\n(\u03a6,\u03a3,s0 )\n\n[\n\n= An \u222a [\n\n\u03a01(Dr ,res((seqn ),s0 )) ]An+1\n\n(\u03a6,\u03a3,s0 )\n\nseqn \u2208Actn\n\nIn order to prove this, we will show that the beliefs sets of the program \u03a0 are\nexactly the sets of the form An \u222a A1 defined in B.18.\nS\nare the\nFirst note that the belief sets of [ seqn \u2208Actn \u03a01(Dr ,res((seqn ),s0 )) ]An+1\n\nunions of belief sets of [\u03a01(Dr ,res((seqn ),s0 )) ]An+1\n\n(\u03a6,\u03a3,s0 )\n\n(\u03a6,\u03a3,s0 )\n\nwhere seqn is varying over the\n\nset of sequences of n actions. This is so because these programs are independent of\neach other.\nHence, we will calculate the belief sets of each program \u03a01(Dr ,res((seqn ),s0 )) ]An+1\n(\u03a6,\u03a3,s0 )\n\nwhere seqn \u2208 Actn and we will prove that the set of these union is An+1\n(\u03a6,\u03a3,s0 ) .\nTo show this, let seqn be a sequence of n actions. Then there are two possible\ncases:\n1. holds(true, res((seq), s0 )) \u2208 An , and An \u222a [\u03a01(Dr ,res((seqn ),s0 )) ]An+1\n\n(\u03a6,\u03a3,s0 )\n\nhas\n\nonly the belief set BA = An \u222a A1 with A1 the set of predicates of the form\nholds(true, res((seq, a), s0 )) or holds(f, res((seq, a), s0 )) where a is any action\nand f is any fluent literal.\n2. holds(true, res((seq), s0 )) 6\u2208 An and by theorem [B.14], any belief set of\nAn \u222a [\u03a01(Dr ,res((seqn ),s0 )) ]An+1\n(\u03a6,\u03a3,s0 )\n\nis equal to An \u222a A\u2032(\u03a60 ,\u03c3(A ,seq ) ,res((seqn ),s0 )) for some 0-interpretation such\nn\nn\nthat (\u03a60 , \u03c3(An ,seqn ) ) \u2208 AssoDr (\u03a6, \u0393\u03a6 (([seqn , a], \u03a3)).\nTherefore the belief sets of An \u222a [\u03a01(Dr ,res((seqn ),s0 )) ]An+1\nelements on\n\nAn+1\n(\u03a6,\u03a3,s0 ) ,\n\nand this implies that\n\nAn+1\n(\u03a6,\u03a3,s0 )\n\n(\u03a6,\u03a3,s0 )\n\nare precisely the\n\nis a world view of \u03a0n+1\nD .\n\nDefinition B.21\nGiven a domain description D, an initial situation constant s0 and a specific-model\n\u03c9\n(\u03a6, \u03a3) of D, we will denote by A\u03c9\n(\u03a6,\u03a3,s0 ) the following family of sets, A \u2208 A(\u03a6,\u03a3,s0 )\nn+1\nn\nif and only if for any n \u2265 1 there exist An \u2208 A(\u03a6,\u03a3,s0 ) and An+1 \u2208 A(\u03a6,\u03a3,s0 ) with\nS\nAn \u2286 An+1 , and A = n\u22651 An We will denote by AQ\n(\u03a6,\u03a3,s0 ) the family of sets A,\nsuch that A is the union of sets A\u03a0 and AQ where A\u03a0 is an element of A\u03c9\n(\u03a6,\u03a3,s0 )\n\u03a0\nand AQ is a belief set of A\u03a0 \u222a [Q]A\nA\u03c9\n\n(\u03a6,\u03a3,s0 )\n\nLemma B.22\nGiven a consistent domain description D and the initial situation constant s0 , A\nis a world view of \u03a0D , if and only if there exists a specific model (\u03a6, \u03a3) of D such\nthat A = AQ\n(\u03a6,\u03a3,s0 ) .\nProof\nGiven a world view A of \u03a0\u03c9\nD , and A \u2208 A, we will denote by A\u03a0 the subset of A\nrestricted to those predicates in A of the form holds(f, s) or ab(f, a, s) where s is a\nsituation constant and a is an action, then the set A\u03a0 = {A\u03a0 : A \u2208 A} is a world\n\n\fKnowledge and the Action Description Language A\n\n55\n\nview of \u03a0D , and by (B.20) there exists a specific model of \u03a0D such that A\u03a0 is\nA\nA\nequal to A\u03c9\n, applying the splitting lemma\n(\u03a6,\u03a3,s0 ) , since [Q]A is equal to [Q]A\u03c9\nA\nA\nA\nto [\u03a0Q\nD ]A = [\u03a0D ]A \u222a [Q]A we have the result.\n\n(\u03a6,\u03a3,s0 )\n\nTheorem B.23\nLet D be a domain description, s0 be an initial constant situation and (\u03a6, \u03a3)\nbe a specific model of D. Then, given a sequence of actions seq = a1 , . . . , ak ,\nand the situation constant s = res((seq), s0 ), we will have that, for any situation constant s1 and any plan \u03b2, there exists a sequence of actions seq(\u03b2,s) such\nthat \u0393\u03a6 ([seq, seq(\u03b2,s) ], \u03a3) is equal to \u0393\u03a6 (\u03b2, \u0393\u03a6 ([seq], \u03a3) and for any A \u2208 AQ\n(\u03a6,\u03a3,s0 ) ,\nf ind situation(\u03b2, s, s1 ) belongs to A if and only if s1 = res((seq, seq(\u03b2,s) ), s0 ).\n\nProof\nThe proof will be a double induction, using the loop nesting in the plan and the complexity of the plans. Thus, the while-complexity of a plan \u03b2, denoted by wcomp(\u03b2),\nis 0 if \u03b2 is either the empty plan [] or an action a, max(wcomp(\u03b11 ), wcopm(\u03b12 ) If\n\u03b2 = if \u03c6then \u03b11 else \u03b12 or \u03b2 = [\u03b11 |\u03b12 ], 1 + wcomp(\u03b1) if \u03b2 = while \u03c6 do \u03b1.\nTaking wcomp(\u03b2) = 0 we will do induction on comp(\u03b2).\n\u2022 If \u03b2 = [], the result is immediate, because\n\u0393\u03a6 (\u03b2, \u0393\u03a6 ([seq], \u03a3)) = \u0393\u03a6 ([], \u0393\u03a6 ([seq], \u03a3)),\nand for any A \u2208 AQ\n(\u03a6,\u03a3,s0 ) f ind situation([\u03b2], s, s1 ) is in A if and only if\ns1 = s. Therefore taking seq(\u03b2,s) = seq\u2205 the claim follows.\n\u2022 Suppose the theorem is valid for \u2264 n-plans.\n\u2022 Let \u03b2 be an (n + 1)-plan. Then we have the following possibilities\nI) \u03b2 = [a|\u03b1] where \u03b1 is an n-plan and a is an action. Then, by inductive hypothesis \u0393\u03a6 (\u03b1, \u0393\u03a6 ([seq, a], \u03a3)) is equal to \u0393\u03a6 ([seq, a, seq(\u03b1,res(a,s)) ], \u03a3), therefore we will have that\n\u0393\u03a6 (\u03b2, \u0393\u03a6 ([seq], \u03a3)) =\n\u0393\u03a6 ([a, \u03b1], \u0393\u03a6 ([seq], \u03a3)) =\n\u0393\u03a6 (\u03b1, \u0393\u03a6 ([seq, a], \u03a3)) =\n\u0393\u03a6 ([seq(\u03b1,res(a,s)) ], \u0393\u03a6 ([seq, a], \u03a3)) =\n\u0393\u03a6 ([a, seq(\u03b1,res(a,s)) ], \u0393\u03a6 ([seq], \u03a3)) =\n\u0393\u03a6 ([seq(\u03b2,s) ], \u0393\u03a6 ([seq], \u03a3)) =\n\u0393\u03a6 ([seq, seq(\u03b2,s) ], \u03a3).\nMoreover for any A in AQ\n(\u03a6,\u03a3,s0 ) , f ind situation(\u03b2, s, s1 ) is in A if and only if\nf ind situation(\u03b1, res(a, s), s1 ) is in A.\nThus, if we take seq(\u03b2,s) = (seq(\u03b1,res(a,s)) , a), since by inductive hypothesis f ind situation([\u03b1], res(a, s), s1 ) is in A if and only if s1 is equal to\nres(((seq, a), seq(\u03b1,res(a,s)) ), s0 ), we have that f ind situation([\u03b2], s, s1 ) is in\nA if and only if s1 = res((seq, (a, seq(\u03b1,s) )), s0 ) which is equal to the situation\n\n\f56\n\nJorge Lobo, Gisela Mendez and Stuart R. Taylor\nres((seq, seq(\u03b2,s) ), s0 ), and\n\u0393\u03a6 ([\u03b2], \u0393\u03a6 ([seq], \u03a3)) =\n\u0393\u03a6 ([seq, seq(\u03b2,s) ], \u0393\u03a6 ([seq], \u03a3)) =\n\u0393\u03a6 ([seq(\u03b2,s) ], \u0393\u03a6 ([seq], \u03a3))\nII) \u03b2 = [ if \u03c6 then \u03b11 , \u03b12 ] where \u03b11 , \u03b12 are n1 and n2 -plans respectively, with\nn1 +n2 = n then, we have that for any A in AQ\n(\u03a6,\u03a3,s0 ) , f ind situation(\u03b2, s, s1 )\nis in A if and only if either:\ni) holds(\u03c6\u0304, s) holds in AQ\n(\u03a6,\u03a3,s0 ) and f ind situation([\u03b12 ], s, s1 ) is in A,\nor\nii) holds(\u03c6, s) holds in AQ\n(\u03a6,\u03a3,s0 ) and f ind situation([\u03b11 , \u03b12 ], s, s1 ) is in A.\nBy inductive hypothesis \u0393\u03a6 (\u03b12 , \u0393\u03a6 ([seq], \u03a3)) is equal to \u0393\u03a6 ([seq, seq(\u03b12 ,s) ], \u03a3)\nand \u0393\u03a6 ([\u03b11 , \u03b12 ], \u0393\u03a6 ([seq], \u03a3)) is equal to \u0393\u03a6 ([seq, seq([\u03b11 ,\u03b12 ],s) ], \u03a3) for the sequences of actions seq(\u03b12 ,s) and seq([\u03b11 ,\u03b12 ],s)\nHence by [B.19] and inductive hypothesis, f ind situation(\u03b2, s, s1 ) is in A if\nand only if either\ni) \u03c6\u0304 holds in \u0393\u03a6 ([seq], \u03a3), and s1 = res((seq, seq(\u03b12 ,s) ), s0 ).\nor\nii) \u03c6 holds in \u0393\u03a6 ([seq], \u03a3), and s1 = res((seq, seq([\u03b11 ,\u03b12 ],s) ), s0 ).\nThus, if we take seq(\u03b2,s) = seq(\u03b12 ,s) in case i), and in case ii) seq(\u03b2,s) equal\nto seq([\u03b11 ,\u03b12 ],s) we will have that \u0393\u03a6 (\u03b2, \u0393\u03a6 ([seq], \u03a3)) = \u0393\u03a6 ([seq, seq(\u03b2,s) ], \u03a3))\nand f ind situation(\u03b2, s, s1 ), belongs to A if and only if s1 is equal to the\nsituation res((seq, seq(\u03b2,s) ), s0 ).\nIII) \u03b2 = [ if \u03c6 then \u03b11 else \u03b1\u20321 , \u03b12 ], where \u03b11 , \u03b1\u20321 and \u03b12 are n1 , n\u20321 and n2 plans (resp.) with max(n1 , n\u20321 ) + n2 = n. This case is similar to the previous\none.\nIV) \u03b2 = [ while \u03c6 do \u03b11 , \u03b12 ], where \u03b11 and \u03b12 are n1 and n2 -plans\n(resp.), with n1 + n2 = n. Here we may suppose by inductive hypothesis\nthat for any k \u2265 0 the plans [\u03b1k1 ] and [\u03b1k1 , \u03b12 ] verify the theorem, and we\nwill denote by seq(k,\u03b11 ) the sequence seq(\u03b11 ,s) and by seq(k,\u03b12 ) the sequence\nseq([\u03b12 ],res((seq([\u03b1k ],s) ),s0 )) .\n1\n\nUsing the fix-point operator T\u03a0 defined by T\u03a0 (I) = {p : \u2203 a rule p \u2190\nq1 , . . . , qn in \u03a0 with each qi a fact in I}, for any positive logic program \u03a0, we\nknow that if T\u03a0\u21911 is defined to be equal to T\u03a0 (\u2205) and T\u03a0\u2191k+1 = T\u03a0 (T\u03a0\u2191k ),\nS\nthen T\u03a0\u2191\u03c9 , which is the set k\u22651 T\u03a0\u2191k , is a fix-point for T\u03a0 . Moreover A is\nA\na belief set for [\u03a0\u03c9\nif and only if\n(D,s0 ) ]AQ\n(\u03a6,\u03a3,s0 )\n\nA = T[\u03a0\u03c9\n\n]A\n(D,s0 ) AQ\n(\u03a6,\u03a3,s0 )\n\n\u2191\u03c9 .\n\nTherefore, h = f ind situation(\u03b2, s, s1 ) is in A if and only if there exists k \u2265 0\nsuch that h is in T[\u03a0\u03c9\n\u2191k+1 .\n]A Q\n(D,s0 ) A\n(\u03a6,\u03a3,s0 )\n\nLet k0 be the minimum k such that h belongs to T[\u03a0\u03c9\n\n]A\n(D,s0 ) AQ\n(\u03a6,\u03a3,s0 )\n\nhave that h \u2208 T[\u03a0\u03c9\n\n]A\n(D,s0 ) AQ\n(\u03a6,\u03a3,s0 )\n\n\u2191k+1 . We\n\n\u2191k0 +1 , if and only if there exists m such that\n\n\fKnowledge and the Action Description Language A\n\n57\n\nthe following properties are satisfied:\n1) For any j < m holds(\u03c6, res((seq, seq(j,\u03b11 ) ), s0 ) holds in AQ\n(\u03a6,\u03a3,s0 )\n2) holds(\u03c6\u0304, s\u2032 ) holds in AQ\n(\u03a6,\u03a3,s0 )\n\u2032\n3) f ind situation([\u03b1m\n1 ], s, s ) \u2208 A\n\u2032\n4) f ind situation([\u03b12 ], s , s1 ) \u2208 A\nIf we fix m with properties (1), (2), (3) and (4), we have by inductive hypothesis and [B.19] that, h \u2208 A if and only if \u03c6 holds in \u0393\u03a6 ([seq, seq(m,\u03b11 ) ], s0 ),\ns\u2032 = res((seq(m,\u03b11 ) ), s0 ) and s1 = res((seq(m,\u03b12 ) ), s0 ). Hence taking seq(\u03b2,s) =\nseq(m,\u03b12 ) , we will have that \u0393\u03a6 (\u03b2, \u0393\u03a6 ([seq], \u03a3) is equal to \u0393\u03a6 ([seq, seq(\u03b2,s) ], \u03a3)\nand h \u2208 A if and only if s1 = res((seq(\u03b2,s) ), s0 ).\nThe inductive step on wcomp(\u03b2) follows the same reasoning as in the base case.\nCorollary B.24\nGiven a consistent domain description D, the initial situation constant s0 and a\nspecific model of D, (\u03a6, \u03a3), we will have that for any fluent f and any plan \u03b2,\nhold af ter plan(f, \u03b2) holds in AQ\n(\u03a6,\u03a3,s0 ) if and only if f \u2208 \u0393\u03a6 (\u03b2, \u03a3).\nProof\nLet seq(\u03b2,s0 ) be the sequence of actions described in [B.23], such that \u0393\u03a6 (\u03b2, \u03a3) =\n\u0393\u03a6 ([seq(\u03b2,s0 ) ], \u03a3) and f ind situation(\u03b2, s0 , s1 ) holds in AQ\n(\u03a6,\u03a3,s0 ) if and only if s1 =\nres((seq(\u03b2,s0 ) ), s0 ). Then since hold af ter plan(f, \u03b2) holds in AQ\n(\u03a6,\u03a3,s0 ) if and only\nif f ind situation(\u03b2, s0 , s1 ) and holds(f, s1 ) hold in AQ\n(\u03a6,\u03a3,s0 ) , we have by [B.19]\nhold af ter plan(f, \u03b2) holds in AQ\n(\u03a6,\u03a3,s0 ) if and only if f \u2208 \u0393\u03a6 ([seq(\u03b2,s0 ) ], \u03a3) =\n\u0393\u03a6 (\u03b2, \u03a3).\nHence by [B.22] we have the following:\nCorollary B.25\nGiven a simple and consistent domain description D and a plan \u03b2. D |= F after\u03b2\nif and only if \u03a0Q\nD |= hold af ter plan(F, \u03b2).\nTheorem 7.3 Given a simple consistent domain description D and a plan \u03b2. D |=\nF after\u03b2 if and only if \u03a0Q\nD |= hold af ter plan(F, \u03b2).\nProof: Direct from Corollary B.25.\n\n\f"}