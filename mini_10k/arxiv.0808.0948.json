{"id": "http://arxiv.org/abs/0808.0948v1", "guidislink": true, "updated": "2008-08-07T02:45:36Z", "updated_parsed": [2008, 8, 7, 2, 45, 36, 3, 220, 0], "published": "2008-08-07T02:45:36Z", "published_parsed": [2008, 8, 7, 2, 45, 36, 3, 220, 0], "title": "Capacity of a Class of Diamond Channels", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0808.3603%2C0808.1935%2C0808.2542%2C0808.2608%2C0808.2437%2C0808.1919%2C0808.0820%2C0808.2214%2C0808.2744%2C0808.0512%2C0808.1148%2C0808.1596%2C0808.1669%2C0808.1470%2C0808.2328%2C0808.0075%2C0808.1732%2C0808.0254%2C0808.0831%2C0808.2129%2C0808.2022%2C0808.2837%2C0808.0236%2C0808.2551%2C0808.0112%2C0808.1355%2C0808.1389%2C0808.2540%2C0808.0524%2C0808.3158%2C0808.1387%2C0808.3391%2C0808.1429%2C0808.0225%2C0808.3053%2C0808.1471%2C0808.1330%2C0808.1311%2C0808.0882%2C0808.0936%2C0808.2122%2C0808.2256%2C0808.2875%2C0808.0015%2C0808.0691%2C0808.0992%2C0808.0478%2C0808.1674%2C0808.1731%2C0808.2381%2C0808.3293%2C0808.2770%2C0808.2726%2C0808.2341%2C0808.1727%2C0808.1928%2C0808.0008%2C0808.0276%2C0808.2613%2C0808.0375%2C0808.0865%2C0808.2072%2C0808.3190%2C0808.2681%2C0808.3984%2C0808.3964%2C0808.1484%2C0808.1456%2C0808.2124%2C0808.2759%2C0808.4001%2C0808.1273%2C0808.0162%2C0808.0883%2C0808.1618%2C0808.1117%2C0808.0191%2C0808.1323%2C0808.3041%2C0808.1713%2C0808.2970%2C0808.1398%2C0808.2801%2C0808.4097%2C0808.2554%2C0808.3438%2C0808.0948%2C0808.0098%2C0808.3056%2C0808.1514%2C0808.0527%2C0808.1237%2C0808.2034%2C0808.2985%2C0808.0921%2C0808.3017%2C0808.2408%2C0808.4068%2C0808.3893%2C0808.1889%2C0808.1354&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Capacity of a Class of Diamond Channels"}, "summary": "We study a special class of diamond channels which was introduced by Schein\nin 2001. In this special class, each diamond channel consists of a transmitter,\na noisy relay, a noiseless relay and a receiver. We prove the capacity of this\nclass of diamond channels by providing an achievable scheme and a converse. The\ncapacity we show is strictly smaller than the cut-set bound. Our result also\nshows the optimality of a combination of decode-and-forward (DAF) and\ncompress-and-forward (CAF) at the noisy relay node. This is the first example\nwhere a combination of DAF and CAF is shown to be capacity achieving. Finally,\nwe note that there exists a duality between this diamond channel coding problem\nand the Kaspi-Berger source coding problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0808.3603%2C0808.1935%2C0808.2542%2C0808.2608%2C0808.2437%2C0808.1919%2C0808.0820%2C0808.2214%2C0808.2744%2C0808.0512%2C0808.1148%2C0808.1596%2C0808.1669%2C0808.1470%2C0808.2328%2C0808.0075%2C0808.1732%2C0808.0254%2C0808.0831%2C0808.2129%2C0808.2022%2C0808.2837%2C0808.0236%2C0808.2551%2C0808.0112%2C0808.1355%2C0808.1389%2C0808.2540%2C0808.0524%2C0808.3158%2C0808.1387%2C0808.3391%2C0808.1429%2C0808.0225%2C0808.3053%2C0808.1471%2C0808.1330%2C0808.1311%2C0808.0882%2C0808.0936%2C0808.2122%2C0808.2256%2C0808.2875%2C0808.0015%2C0808.0691%2C0808.0992%2C0808.0478%2C0808.1674%2C0808.1731%2C0808.2381%2C0808.3293%2C0808.2770%2C0808.2726%2C0808.2341%2C0808.1727%2C0808.1928%2C0808.0008%2C0808.0276%2C0808.2613%2C0808.0375%2C0808.0865%2C0808.2072%2C0808.3190%2C0808.2681%2C0808.3984%2C0808.3964%2C0808.1484%2C0808.1456%2C0808.2124%2C0808.2759%2C0808.4001%2C0808.1273%2C0808.0162%2C0808.0883%2C0808.1618%2C0808.1117%2C0808.0191%2C0808.1323%2C0808.3041%2C0808.1713%2C0808.2970%2C0808.1398%2C0808.2801%2C0808.4097%2C0808.2554%2C0808.3438%2C0808.0948%2C0808.0098%2C0808.3056%2C0808.1514%2C0808.0527%2C0808.1237%2C0808.2034%2C0808.2985%2C0808.0921%2C0808.3017%2C0808.2408%2C0808.4068%2C0808.3893%2C0808.1889%2C0808.1354&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study a special class of diamond channels which was introduced by Schein\nin 2001. In this special class, each diamond channel consists of a transmitter,\na noisy relay, a noiseless relay and a receiver. We prove the capacity of this\nclass of diamond channels by providing an achievable scheme and a converse. The\ncapacity we show is strictly smaller than the cut-set bound. Our result also\nshows the optimality of a combination of decode-and-forward (DAF) and\ncompress-and-forward (CAF) at the noisy relay node. This is the first example\nwhere a combination of DAF and CAF is shown to be capacity achieving. Finally,\nwe note that there exists a duality between this diamond channel coding problem\nand the Kaspi-Berger source coding problem."}, "authors": ["Wei Kang", "Sennur Ulukus"], "author_detail": {"name": "Sennur Ulukus"}, "author": "Sennur Ulukus", "arxiv_comment": "15 pages, 3 figures, submitted to IEEE Trans. on Information Theory", "links": [{"href": "http://arxiv.org/abs/0808.0948v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0808.0948v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "H.1.1", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0808.0948v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0808.0948v1", "journal_reference": null, "doi": null, "fulltext": "Capacity of a Class of Diamond Channels\u2217\nWei Kang\n\nSennur Ulukus\n\nDepartment of Electrical and Computer Engineering\n\narXiv:0808.0948v1 [cs.IT] 7 Aug 2008\n\nUniversity of Maryland, College Park, MD 20742\nwkang@umd.edu\n\nulukus@umd.edu\n\nOctober 24, 2018\n\nAbstract\nWe study a special class of diamond channels which was introduced by Schein in\n2001. In this special class, each diamond channel consists of a transmitter, a noisy\nrelay, a noiseless relay and a receiver. We prove the capacity of this class of diamond\nchannels by providing an achievable scheme and a converse. The capacity we show\nis strictly smaller than the cut-set bound. Our result also shows the optimality of a\ncombination of decode-and-forward (DAF) and compress-and-forward (CAF) at the\nnoisy relay node. This is the first example where a combination of DAF and CAF is\nshown to be capacity achieving. Finally, we note that there exists a duality between\nthis diamond channel coding problem and the Kaspi-Berger source coding problem.\n\n\u2217\n\nThis work was supported by NSF Grants CCF 04-47613, CCF 05-14846, CNS 07-16311 and CCF 0729127.\n\n1\n\n\f1\n\nProblem Statement and the Result\n\nThe diamond channel was first introduced by Schein in 2001 [1]. The diamond channel\nconsists of one transmitter, two relays and a receiver, where the transmitter and the two\nrelays form a broadcast channel as the first stage and the two relays and the receiver form\na multiple access channel as the second stage. The capacity of the diamond channel in its\nmost general form is open. Schein explored several special cases of the diamond channel, one\nof which [1, Section 3.5] is specified as follows (see Figure 1). The multiple access channel\nconsists of two orthogonal links with rate constraints R1 and R2 , respectively. The broadcast\nchannel contains a noisy branch and a noiseless branch, i.e., with input X and two outputs\nX and Y . We refer to the relay node receiving Y as the noisy relay and the relay node\nreceiving X as the noiseless relay. Schein provided two achievable schemes for this class of\ndiamond channels. In this paper, we will prove the capacity of this special class of diamond\nchannels.\nThe formal definition of the problem is as follows. Consider a channel with input alphabet X and output alphabet Y, which is characterized by the transition probability p(y|x).\nAssume an n-length block code consisting of (f, g, h, \u03c6) where\nf :{1, 2, . . . , M} 7\u2192 X n\n\n(1)\n\ng :Y n 7\u2192 {1, 2, . . . , |g|}\n\n(2)\n\nh :{1, 2, . . . , M} 7\u2192 {1, 2, . . . , |h|}\n\n(3)\n\n\u03c6 :{1, 2, . . . , |g|} \u00d7 {1, 2, . . . , |h|} 7\u2192 {1, 2, . . . , M}\n\n(4)\n\nHere f denotes the encoding function at the transmitter, g and h denote the processing\nfunctions at the noisy and noiseless relays, respectively, and \u03c6 denotes the decoding function\nat the receiver.\nThe encoder sends xn = f (m) into the channel, where m \u2208 {1, 2, . . . , M}. The decoder\nreconstructs m\u0302 = \u03c6(g(Y n ), h(m)). The average probability of error is defined as\nM\n1 X\nP r(m\u0302 6= m|m is sent)\nPe ,\nM m=1\n\n(5)\n\nThe rate triple (R, R1 , R2 ) is achievable if for every 0 < \u01eb < 1, \u03b7 > 0 and every sufficiently\nlarge n, there exists an n-length block code (f, g, h, \u03c6), such that Pe \u2264 \u01eb and\n1\nln M \u2265 R \u2212 \u03b7\nn\n1\nln |g| \u2264 R1 + \u03b7\nn\n1\nln |h| \u2264 R2 + \u03b7\nn\n2\n\n(6)\n(7)\n(8)\n\n\fYn\n\nEncoder\n\nNoisy relay\n(Relay 1)\n\nR1\n\nDecoder\n\nXn\n\nXn\n\nNoiseless relay\n(Relay 2)\n\nR2\n\nFigure 1: The diamond channel.\nThe following theorem characterizes the capacity of the class of diamond channels considered in this paper.\nTheorem 1 The rate triple (R, R1 , R2 ) is achievable in the above channel if and only if the\nfollowing conditions are satisfied\nR \u2264 I(U; Y ) + H(X|U)\n\n(9)\n\nR1 \u2265 I(Z; Y |U, X)\n\n(10)\n\nR2 \u2265 H(X|Z, U)\n\n(11)\n\nR1 + R2 \u2265 R + I(Y ; Z|X, U)\n\n(12)\n\np(u, z, x, y) = p(u, x)p(y|x)p(z|u, y)\n\n(13)\n\nfor some joint distribution\n\nwith cardinalities of alphabets satisfying\n\n2\n\n|U| \u2264 |X | + 4\n\n(14)\n\n|Z| \u2264 |U||Y| + 3 \u2264 |X ||Y| + 4|X | + 3\n\n(15)\n\nThe Achievability\n\nAssume a given joint distribution\np(u, z, x, y) = p(u, x)p(y|x)p(z|u, y)\n\n(16)\n\nand consider that the information theoretic quantities on the right hand sides of (9), (10),\n(11) and (12) are evaluated with this fixed joint probability distribution.\nConsider a message W with rate R. If R \u2264 H(X|Z, U), reliable transmission can be\nachieved by letting g(Y n ) = \u03c6 (constant) and h(W ) = W , i.e., by sending the message\n3\n\n\fR1\nI(U ; Y ) + I(Y ; Z|U )\n\na\u2032\n\na\n\nR + I(Z; Y |U, X) \u2212 H(X|U, Z)\n\nb\n\nI(Z; Y |U, X)\n\nR \u2212 I(U ; Y ) \u2212 I(X; Z|U ) H(X|U, Z)\n\nb\u2032\nR\n\nR + I(Z; Y |U, X)\n\nR2\n\nFigure 2: Rate region of (R1 , R2 ) when H(X|U, Z) \u2264 R \u2264 I(U; Y ) + I(X; Z|U).\nthrough the noiseless relay. Thus, we will only consider the case where\nH(X|Z, U) < R \u2264 I(U; Y ) + H(X|U)\n\n(17)\n\nWe will show that the message can be reliably transmitted with a pair of functions (g, h)\nsuch that ( n1 ln |g|, n1 ln |h|) lies in the inverse pentagon1 with corners a and b in Figure 2.\nHowever, we instead prove reliable transmission with ( n1 ln |g|, n1 ln |h|) lying in the inverse\npentagon with corners a\u2032 and b\u2032 , which contains the inverse pentagon with corners a and\nb and thus imposes a stronger condition to prove. It is straightforward to have reliable\ntransmission with the rate pair at point b\u2032 by letting g(Y n ) = \u03c6 (constant) and h(W ) = W .\nThus, it remains to prove that reliable transmission is possible with the rate pair at point\na\u2032 , i.e.,\nR1 = I(U; Y ) + I(Y ; Z|U)\n\n(18)\n\nR2 = R \u2212 I(U; Y ) \u2212 I(X; Z|U)\n\n(19)\n\nLet us assume that the message W is decomposed as W = (Wa , Wb , Wc ). For a positive\n1\n\nBy \"inverse pentagon\" with corner points a and b, we mean the region in the (R1 , R2 ) space that is to\nthe \"north-east\" of line segment [a, b]. More specifically, this is the region described by inequalities in (10),\n(11) and (12).\n\n4\n\n\fnumber \u01eb, let us define\nMa , |Wa | = exp(n(I(U; Y ) \u2212 3\u01eb))\nM\n= exp(ln M \u2212 n(I(U; Y ) + I(X; Z|U) + 6\u01eb))\nMb , |Wb | =\nMa Mc\nMc , |Wc | = exp(n(I(X; Z|U) \u2212 3\u01eb))\n\n(20)\n(21)\n(22)\n\nRandom codebook generation: We use a superpostion code structure. The size of the\ninner code is Ma . For each inner codeword, we independently generate Mb outer codes. The\nsize of each outer code is Mc .\n\u2022 Independently generate Ma sequences, un (1), un (2), . . . , un (Ma ), according to\nwhere p(ui ) = p(u), for i = 1, 2, . . . , n.\n\nQn\n\ni=1\n\np(ui )\n\n\u2022 For un (j), j = 1, 2, . . . , Ma , independently generate Mb codebooks, C(j, 1), C(j, 2), . . . ,\nC(j, Mb ).\n\u2022 In the codebook C(j, k), j = 1, 2, . . . , Ma , k = 1, 2, . . . , Mb , independently generate Mc\nQ\ncodewords xn (j, k, 1), xn (j, k, 2), . . . , xn (j, k, Mc ) according to ni=1 p(xi |Ui = ui (j)),\nwhere p(xi |U = ui (j)) = p(x|u), for i = 1, 2, . . . , n, j = 1, 2, . . . , Ma , k = 1, 2, . . . , Mb .\nThere will be no overlapping codebooks with high probability when n is sufficiently large,\nbecause\n1\nln Mb Mc < H(X|U)\n(23)\nn\nEncoding at the transmitter : Let W = (Wa , Wb , Wc ) be the message. We send codeword\nX n = f (Wa , Wb , Wc ) , xn (Wa , Wb , Wc ) into the channel.\nProcessing at the noisy relay: First, after having received Y n , seek\n\u00db n = un (\u0174a ) \u2208 {un (1), un (2), . . . , un (Ma )}\n\n(24)\n\n(\u00db n , Y n ) \u2208 T[Un Y ]\n\n(25)\n\nsuch that\n\nwhere the definition of strong typical set can be found in [2, Section 1.2]. If there is not any\nsuch \u00db n , then let \u00db n be an arbitrary sequence in {un (1), un(2), . . . , un (Ma )}. Secondly, conQ\nstruct a conditional rate distortion code according to ni=1 p(zi , yi |\u00fbi ) with encoding function\ng \u2032 (Y n , \u00db n ) and |g \u2032| = L = exp(n(I(Y ; Z|U) + \u03c4 )). Finally send \u00db n and Z n , g \u2032 (Y n , \u00db n ) to\nthe destination, i.e.,\ng(Y n ) = (\u00db n , Z n )\n(26)\nwhere\n|g| = Ma \u00d7 L \u2264 exp(n(I(U; Y ) + I(X; Z|U) + \u03c4 \u2212 3\u01eb))\n\n5\n\n(27)\n\n\fProcessing at the noiseless relay: Let h(f (Wa , Wc , Wb )) = Wb where\n|h| = Mb = exp(ln M \u2212 n(I(U; Y ) + I(X; Z|U) + 6\u01eb))\n\n(28)\n\nDecoding: Decoder collects (\u00db n , Z n ) from the noisy relay and Wb from the noiseless relay.\nThe decoder seeks a codeword xn (Wa , Wb , i) from the codebook C(Wa , Wb ) such that\nn\nn\n(xn (\u0174a , Wb , i), Z n ) \u2208 T[XZ|U\n] (\u00db )\n\n(29)\n\nProbability of error : The error occurs when (\u00db , X\u0302) 6= (U, X). The average probability of\nerror can be decomposed into\nP r(E) \u2264 P r(E1 \u222a E2 \u222a E3 ) = P r(E1 ) + P r(E2 \u2229 E1c ) + P r(E3 \u2229 E1c \u2229 E2c )\n\n(30)\n\nE1 , (U n , X n , Y n , Z n ) \u2208\n/ T[Un XY Z]\n[\nE2 ,\n(\u016bn , Y n ) \u2208 T[Un Y ]\n\n(31)\n\nwhere\n\n(32)\n\n\u016bn 6=U n ,\u016bn \u2208{un (1),un (2),...,un (Ma )}\n\nE3 ,\n\n[\n\nx\u0304n 6=X n ,x\u0304n \u2208C(W\n\nn\nn\n(x\u0304n , Z n ) \u2208 T[XZ|U\n] (U )\n\n(33)\n\na ,Wb )\n\nWe note that\nn\nn\nn\nn\nP r(E1 ) \u2264 P r(U n \u2208\n/ T[Un ] ) + P r((Y n , Z n ) \u2208\n/ T[Yn Z|U ](U n )) + P r(X n \u2208\n/ T[X|Y\nZU ] (Y , Z , U ))\n(34)\n\nwhere\n\u2022 U n is generated in an i.i.d. fashion with probability p(u). Thus, when n is sufficiently\nlarge, we have\nP r(U n \u2208\n/ T[Un ] ) \u2264 \u01eb\n(35)\n\u2022 Z n is a conditional rate distortion code for Y n conditioned on U n . Thus, when n is\nsufficiently large, L = exp(nI(Y ; Z|U) + \u03c4 ), and U n \u2208 T[Un ] , we have\nP r((Y n , Z n ) \u2208\n/ T[Yn Z|U ] (U n )) \u2264 \u01eb\n\n(36)\n\n\u2022 X n can be viewed as being generated according to an i.i.d. conditional probability\np(x|u, y) with respect to (U n , Y n ). Thus, when n is sufficiently large and (Y n , Z n , U n ) \u2208\nT[Yn ZU ] ,\nn\nn\nn\nn\nP r(X n \u2208\n/ T[X|Y\n(37)\nZU ] (Y , Z , U )) \u2264 \u01eb\n6\n\n\fFrom the above calculation, we have\nP r(E1 ) = P r((U n , X n , Y n , Z n ) \u2208\n/ T[Un XZ] ) \u2264 3\u01eb\n\n(38)\n\nFor the second error event, we note that Ma = exp(n(I(U; Y ) \u2212 3\u01eb) and\n\uf8eb\n\n\uf8f6\n\n[\n\nP r(E2 \u2229 E1c ) = P r \uf8ed\n\n(\u016bn , Y n ) \u2208 T[Un Y ] |(Y n ) \u2208 T[Yn ] \uf8f8\n\n\u016bn 6=U n ,\u016bn \u2208{un (1),un (2),...,un (Ma )}\n\n\u2264\n\nMa\nX\n\nP r((un (i), Y n ) \u2208 T[Un Y ] |Y n \u2208 T[Yn ] )\n\ni=1\n\n\u2264 Ma P r(un(i) \u2208 T[Un |Y ] (Y n ))\n\u2264 Ma exp(\u2212nH(U) + n\u01eb) exp(nH(U|Y ) + n\u01eb)\n= exp(\u2212n\u01eb)\n\u2264\u01eb\n\n(39)\n\nfor sufficiently large n. We note that Mc = exp(n(I(X; Z|U) \u2212 3\u01eb), then\n\uf8eb\n\nP r(E3 \u2229 E1c ) = P r \uf8ed\n\n[\n\nx\u0304n 6=X n ,x\u0304n \u2208C(Wa ,Wb )\n\n\u2264\n\nMc\nX\n\n\uf8f6\n\nn\nn\nn\nn \uf8f8\n(x\u0304n , Z n ) \u2208 T[XZ|U\n] (U )|(Z , U) \u2208 T[ZU ]\n\nn\nn\nn\nn\nn\nP r((x(Ma , Mb , i), Z n ) \u2208 T[XZ|U\n] (U )|(Z , U ) \u2208 T[ZU ] )\n\ni=1\n\nn\nn\n\u2264 Mc P r(x(Ma , Mb , i) \u2208 T[X|ZU\n] (Y ))\n\n\u2264 Mc exp(\u2212nH(X|U) + n\u01eb) exp(nH(X|Z, U) + n\u01eb)\n= exp(\u2212n\u01eb)\n\u2264\u01eb\n\n(40)\n\nfor sufficiently large n. Thus, the average probability error is upper bounded as\nP r(E) \u2264 3\u01eb + \u01eb + \u01eb = 5\u01eb\n\n(41)\n\nwhich goes to zero when n goes to infinity.\n\n3\n\nThe Converse\n\nn\nDefine Zi , g and Ui , (Y i\u22121 , Xi+1\n). We note that\n\np(ui , xi , yi , zi ) = p(ui , xi )p(yi |xi )p(zi |yi , ui )\n\n7\n\n(42)\n\n\fWe have\nln M = H(X n )\nn\nX\nn\n=\nH(Xi |Xi+1\n)\ni=1\n\n\u2264\n\nn\nX\n\nn\nI(Y i\u22121 ; Yi ) + H(Xi |Xi+1\n)\n\n=\n\nn\nX\n\nn\nn\nn\nn\nI(Y i\u22121 , Xi+1\n; Yi ) \u2212 I(Xi+1\n; Yi |Y i\u22121 ) + H(Xi |Y i\u22121 , Xi+1\n) + I(Y i\u22121 ; Xi |Xi+1\n)\n\ni=1\n\n1\n=\n\ni=1\nn\nX\n\n=\n\nn\nX\n\nn\nn\nI(Y i\u22121 , Xi+1\n; Yi ) + H(Xi |Y i\u22121 , Xi+1\n)\n\ni=1\n\nI(Ui ; Yi ) + H(Xi |Ui )\n\n(43)\n\ni=1\n\nwhere\n1. Because of the following equality [3, Lemma 7]\nn\nX\n\nn\nI(Xi+1\n; Yi |Y i\u22121 )\n\ni=1\n\n=\n\nn\nX\ni=1\n\n8\n\nn\nI(Y i\u22121 ; Xi |Xi+1\n)\n\n(44)\n\n\fWe have\nln |g| \u2265 H(g)\n\u2265 H(g|h)\n\u2265 H(g|h) \u2212 H(g|h, Y n )\n= I(g; Y n |h)\nn\nX\n=\nI(g; Yi|h, Y i\u22121 )\ni=1\n\n=\n\nn\nX\n\nn\nn\nI(g, Xi+1\n; Yi |h, Y i\u22121 ) \u2212 I(Xi+1\n; Yi |g, h, Y i\u22121 )\n\ni=1\n\nn\n1 X\nn\nn\n=\nI(g, Xi+1\n; Yi |h, Y i\u22121 ) \u2212 I(Y i\u22121 ; Xi |g, h, Xi+1\n)\n\n\u2265\n\ni=1\nn\nX\n\nn\nn\nI(g, Xi+1\n; Yi |h, Y i\u22121 ) \u2212 H(Xi |g, h, Xi+1\n)\n\ni=1\n\n= \u2212H(X n |g, h) +\n\nn\nX\n\nn\nI(g, Xi+1\n; Yi |h, Y i\u22121 )\n\ni=1\n\nn\n2 X\nn\n\u2265\nI(g, Xi+1\n; Yi |h, Y i\u22121 ) \u2212 \u01eb\n\n\u2265\n\ni=1\nn\nX\n\nn\nI(g; Yi|h, Y i\u22121 , Xi+1\n)\u2212\u01eb\n\ni=1\n\nn\n3 X\nn\n, Xi ) \u2212 \u01eb\n\u2265\nI(g; Yi|h, Y i\u22121 , Xi+1\ni=1\n\nn\n4 X\nn\nI(g; Yi|Y i\u22121 , Xi+1\n, Xi ) \u2212 \u01eb\n=\n\n=\n\ni=1\nn\nX\n\nI(Zi ; Yi |Ui , Xi ) \u2212 \u01eb\n\n(45)\n\ni=1\n\nwhere\n1. Because of the following equality [3, Lemma 7]\nn\nX\n\nn\n; Yi |g, h, Y i\u22121 ) =\nI(Xi+1\n\ni=1\n\nn\nX\n\nn\n)\nI(Y i\u22121 ; Xi |g, h, Xi+1\n\n(46)\n\ni=1\n\n2. Due to Fano's inequality.\n3. g is a deterministic function of Y n . Due to the memoryless property, we have\nn\nn\nH(g|Yi, h, Y i\u22121 , Xi+1\n, Xi ) = H(g|Yi, h, Y i\u22121 , Xi+1\n)\n\n9\n\n(47)\n\n\f4. g is a deterministic function of Y n and h is a deterministic function of X n . Due to the\nmemoryless property, we have\nn\nn\nH(g|h, Y i\u22121 , Xi+1\n, Xi ) = H(g|Y i\u22121 , Xi+1\n, Xi )\n\nH(g|h, Y\n\ni\u22121\n\nn\n, Xi+1\n, Xi , Y i )\n\n= H(g|Y\n\ni\u22121\n\nn\n, Xi+1\n, Xi , Y i )\n\n(48)\n(49)\n\nWe have\nln |h| \u2265 H(h|g)\n\u2265 I(h; X n |g)\n= H(X n |g) \u2212 H(X n |g, h)\n1\n\u2265 H(X n |g) \u2212 n\u01eb\nn\nX\nn\n=\nH(Xi |Xi+1\n, g) \u2212 \u01eb\ni=1\n\n\u2265\n=\n\nn\nX\n\ni=1\nn\nX\n\nn\nH(Xi |Y i\u22121 , Xi+1\n, g) \u2212 \u01eb\n\nH(Xi |Ui , Zi ) \u2212 \u01eb\n\n(50)\n\ni=1\n\nwhere\n1. Due to Fano's inequality.\nWe have\nln |g| + ln |h| \u2265 H(g, h)\n\u2265 I(g, h; X n, Y n )\n\u2265 I(X n ; g, h) + I(Y n ; g, h|X n )\n= H(X n ) \u2212 H(X n |g, h) + I(Y n ; g, h|X n )\n1\n\u2265 ln M \u2212 n\u01eb + I(Y n ; g, h|X n)\n2\n= ln M \u2212 n\u01eb + I(Y n ; g|X n )\nn\nX\n= ln M +\n\u2212\u01eb + I(Yi ; g|X n, Y i\u22121 )\ni=1\n\n3\n= ln M +\n= ln M +\n\nn\nX\n\ni=1\nn\nX\n\nn\n\u2212\u01eb + I(Yi ; g|Xi , Y i\u22121 , Xi+1\n)\n\n\u2212\u01eb + I(Yi ; Zi |Xi , Ui )\n\ni=1\n\n10\n\n(51)\n\n\f1. Due to Fano's inequality.\n2. h is a deterministic function of X n\n3. g is a deterministic function of Y n . Due to the memoryless property, we have\nn\nn\nH(g|Xi , Y i\u22121 , Xi+1\n, X i\u22121 ) = H(g|Xi , Y i\u22121 , Xi+1\n)\n\n(52)\n\nn\nn\nH(g|Yi, Xi , Y i\u22121 , Xi+1\n, X i\u22121 ) = H(g|Yi, Xi , Y i\u22121 , Xi+1\n)\n\n(53)\n\nWe note that n1 ln M \u2265 R \u2212 \u03b7, n1 ln |g| \u2264 R1 + \u03b7 and n1 ln |h| \u2264 R2 + \u03b7, for an arbitrary\n\u03b7 > 0. Assume \u01eb \u2192 0, then from (43), (45), (50) and (51), we have\nn\n\n1X\nI(Ui ; Yi ) + H(Xi |Ui )\nR\u2264\nn i=1\n\n(54)\n\nn\n\n1X\nR1 \u2265\nI(Zi ; Yi |Ui , Xi )\nn i=1\n1X\nR2 \u2265\nH(Xi |Ui , Zi )\nn i=1\n\n(55)\n(56)\n\nn\n\n1X\nR1 + R2 \u2265 R +\nI(Yi ; Zi |Xi , Ui )\nn i=1\n\n(57)\n\nDefine a time-sharing random variable Q, which is uniformly distributed on {1, 2, . . . , n}.\nAlso define a set of random variables (X, Y, \u0168 , Z\u0303) such that\nP r(X = x, Y = y, \u0168 = u, Z\u0303 = z|Q = i) = p(Xi = x, Yi = y,Ui = u, Zi = z),\n\ni = 1, 2, . . . , n\n(58)\n\nDefine U = (\u0168 , Q) and Z = (Z\u0303, Q), then\nn\n\n1X\nI(Ui ; Yi ) + H(Xi |Ui )\nR\u2264\nn i=1\n= I(\u0168; Y |Q) + H(X|\u0168, Q)\n\u2264 I(\u0168, Q; Y ) + H(X|\u0168, Q)\n= I(U; Y ) + H(X|U)\n\n(59)\n\nn\n\nR1 \u2265\n\n1X\nI(Zi ; Yi |Ui , Xi )\nn i=1\n\n= I(Z\u0303; Y |\u0168, Q, X)\n= I(Z; Y |U, X)\n11\n\n(60)\n\n\fR2 \u2265\n\n1X\nH(Xi |Ui , Zi )\nn i=1\n\n= H(X|\u0168, Z\u0303, Q)\n= H(X|U, Z)\n\n(61)\n\nn\n\n1X\nI(Yi ; Zi |Xi , Ui )\nR1 + R2 \u2265 R +\nn i=1\n= R + I(Z\u0303; Y |\u0168, X, Q)\n= R + I(Z; Y |U, X)\n\n(62)\n\nwhere (59), (60), (61) and (62) are the same as (9), (10), (11) and (12), concluding the proof.\nFinally, we note that the bounds on the cardinalities of the alphabets in (14) and (15)\ncan be proven in a way similar to [4, Appendix D].\n\n4\n\nRemarks\n\nWe have several remarks regarding this result as follows:\n1. The capacity is strictly smaller than the cut-set bound [5], because first\nR \u2264 R1 + R2 \u2212 I(Y ; Z|U, X)\n\n(63)\n\nAn operational interpretation is that when the noisy relay cannot fully decode the\nmessage, or in other words, when the noisy relay cannot remove the noise completely,\nthe data going through the link from the noisy relay to the receiver contains noise.\nThus, the useful information flowing through the multiple access cut will be strictly\nless than R1 + R2 . Secondly, we note that\nR \u2264 I(U; Y ) + H(X|U) \u2264 H(X)\n\n(64)\n\nAn operational interpretation is that when the noisy relay decodes the message with\na positive rate, the rate of information flowing through the broadcast cut becomes\nstrictly less than H(X).\nConsider the following example. Let X and Y be binary and\nY = X \u2295W\n\n(65)\n\nwhere the sum is a modulo-2 sum and W has a Bernoulli distribution with entropy\n0.5 bits. We assume R1 = R2 = 0.5 bits. The cut-set bound in this example is 1 bit,\n12\n\n\fwhich is not achievable. Because if R is equal to 1 bit, we have,\nR = I(U; Y ) + H(X|U) = H(X) = 1\n\n(66)\n\nthen, U has to be independent of X and Y . Also, we have\nR = R1 + R2 \u2212 I(Y ; Z|U, X) = R1 + R2 = 1\n\n(67)\n\nthen, Z has to be independent of X and Y if U is independent of X and Y . However,\nif U and Z are independent of X and Y , we arrive at the following contradiction,\n0.5 = R2 \u2265 H(X|Z, U) = H(X) = 1\n\n(68)\n\nwhich means that the cut-set bound is not achievable in this example. We note that,\neven in this binary example where |X | = |Y| = 2, the cardinalities of the auxiliary\nrandom variables U and Z are |U| \u2264 6 and |Z| \u2264 15. These large cardinality bounds\nmake it practically impossible to evaluate the capacity of this diamond channel. However, we note that, even though we were not able to compute the exact value of the\ncapacity in this example, we were able to conclude that the capacity is strictly less\nthan the cut-set bound, which is 1 bit.\nWe know that the capacity of a diamond channel with four orthogonal links is equal\nto the cut-set bound in this channel. Our result shows that introducing the broadcast\nnode will reduce the capacity of this all-orthogonal diamond channel. Networks with\nbroadcast nodes have been studied recently from different perspectives, e.g., information theory and network coding [6\u20138]. We note that our diamond channel model is a\nsimple example of a general network with a broadcast node. Thus, we conclude that\nthe cut-set bound in general is not tight in networks with broadcast nodes.\n2. The processing at the noisy relay includes two operations: decode the inner code U n and\ncompress the channel output Y n to Z n conditioned on U n . This processing is essentially\nthe same as Theorem 7 in [9], i.e., combination of DAF and CAF. DAF [9, Theorem 1]\nhas been shown to be optimal in the degraded relay channel [9]. Partial DAF, a special\ncase of [9, Theorem 7] without compression, has been shown to be optimal in semideterministic relay channel [10] and the relay channel with orthogonal transmitter-relay\nlink [11]. Recently, CAF [9, Theorem 6] has been shown to be optimal in two special\nrelay channels [12, 13]. To our knowledge, we are the first to show the optimality of\nthe combination of DAF and CAF in some specific channel, even though the channel\nwe consider is not a three-node relay channel in the strict sense, i.e., as in [9].\n\n13\n\n\fYn\n\nRelay 1\nR1\n\np(x, y)\n\nR0\n\nDecoder\n\nR2\n\nEncoder\n(Relay 2)\n\nXn\n\nFigure 3: Kaspi-Berger rate distortion problem.\n3. If we assume R = H(X) \u2212 R0 , then Theorem 1 can be rewritten as follows\nR \u2264 I(U; Y ) + H(X|U)\n\n\u2190\u2192\n\nR0 \u2265 I(U; X|Y )\n\n(69)\n\nR1 \u2265 I(Z; Y |U, X)\n\n\u2190\u2192\n\nR1 \u2265 I(Z; Y |U, X)\n\n(70)\n\nR2 \u2265 H(X|Z, U)\n\n\u2190\u2192\n\nR2 \u2265 I(X; X|Z, U)\n\n(71)\n\nR1 + R2 \u2265 R + I(Y ; Z|X, U)\n\n\u2190\u2192\n\nR0 + R1 + R2 \u2265 I(X, Y ; U, X, Z)\n\n(72)\n\nfor some joint distribution\np(u, z, x, y) = p(u, x)p(y|x)p(z|u, y)\n\n(73)\n\nWe note that the right hand sides of (69), (70), (71) and (72) in addition to the\ndistribution constraint in (73) are the same as the rate region of the rate-distortion\nproblem studied by Kaspi and Berger as shown in Figure 3 [4, Theorem 2.1, Case C].\nThis duality between our diamond channel coding problem and the Kaspi-Berger source\ncoding problem is similar to the duality between the single-user channel coding problem\nand the Slepian-Wolf source coding problem [2, Section 3.1] by viewing the codebook\ninformation in the channel coding problem as the information sent to all the terminals\nin the source coding problem, e.g., the information with rate R0 in Figure 3. Thus, the\nachievability of our diamond channel coding problem can be obtained from the achievability of Kaspi-Berger source coding problem, in the same way that the achievability\nof the multiple access channel coding problem can be obtained from the achievability\nof fork network coding problem [2, Section 3.2].\n\n14\n\n\fReferences\n[1] B. E. Schein. Distributed Coordination in Network Information Theory. PhD thesis,\nMassachusetts Institute of Technology, 2001.\n[2] I. Csisz\u00e1r and J. K\u00f6rner. Information Theory: Coding Theorems for Discrete Memoryless Systems. Academic Press, 1981.\n[3] I. Csisz\u00e1r and J. K\u00f6rner. Broadcast channels with confidential messages. IEEE Trans.\nInform. Theory, 24(3):339\u2013348, 1978.\n[4] A. H. Kaspi and T. Berger. Rate-distortion for correlated sources with partially separated encoders. IEEE Trans. Inform. Theory, 28(6):828\u2013840, 1982.\n[5] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley and Sons,\n1991.\n[6] A. F. Dana, R. Gowaikar, R. Palanki, B. Hassibi, and M. Effros. Capacity of wireless\nerasure networks. IEEE Trans. Inform. Theory, 52(3):789\u2013804, 2006.\n[7] N. Ratnakar and G. Kramer. The multicast capacity of deterministic relay network\nwith no interference. IEEE Trans. Inform. Theory, 52(6):2425\u20132432, 2006.\n[8] G. Kramer, S. M. S. Tabatabaei Yazdi, and S. A. Savari. Network coding on line\nnetworks with broadcast. In Proc. Conf. Inf. Sciences and Systems (CISS), Princeton,\nNJ, Mar. 2008.\n[9] T. M. Cover and A. El Gamal. Capacity theorems for the relay channel. IEEE Trans.\nInform. Theory, 25:572\u2013584, Sep. 1979.\n[10] A. El Gamal and M. Aref. The capacity of the semideterministic relay channel. IEEE\nTrans. Inform. Theory, 28(3):536, 1982.\n[11] A. El Gamal and S. Zahedi. Capacity of a class of relay channels with orthogonal\ncomponents. IEEE Trans. Inform. Theory, 51(5):1815\u20131817, 2005.\n[12] Y. H. Kim. Capacity of a class of deterministic relay channels. IEEE Trans. Inform.\nTheory, 53(3):1328\u20131329, 2008.\n[13] M. Aleksic, P. Razaghi, and W. Yu. Capacity of a class of modulo-sum relay channels.\nSubmitted to IEEE Transactions on Information Theory, 2007,\nhttp://arxiv.org/pdf/0704.3591.\n\n15\n\n\f"}