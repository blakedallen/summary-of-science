{"id": "http://arxiv.org/abs/1104.5525v1", "guidislink": true, "updated": "2011-04-28T23:03:10Z", "updated_parsed": [2011, 4, 28, 23, 3, 10, 3, 118, 0], "published": "2011-04-28T23:03:10Z", "published_parsed": [2011, 4, 28, 23, 3, 10, 3, 118, 0], "title": "Distributed Delayed Stochastic Optimization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.0949%2C1104.3029%2C1104.4232%2C1104.1467%2C1104.2395%2C1104.3785%2C1104.1961%2C1104.5277%2C1104.2197%2C1104.2165%2C1104.5346%2C1104.2617%2C1104.2509%2C1104.1065%2C1104.3254%2C1104.3780%2C1104.3427%2C1104.1105%2C1104.1738%2C1104.2594%2C1104.4367%2C1104.5185%2C1104.3842%2C1104.5359%2C1104.4373%2C1104.5597%2C1104.5465%2C1104.1737%2C1104.5057%2C1104.5036%2C1104.3962%2C1104.2328%2C1104.2828%2C1104.4356%2C1104.2681%2C1104.4334%2C1104.3380%2C1104.0499%2C1104.4584%2C1104.0825%2C1104.5709%2C1104.5247%2C1104.1389%2C1104.2585%2C1104.4895%2C1104.0153%2C1104.5525%2C1104.0665%2C1104.3378%2C1104.4865%2C1104.1757%2C1104.1988%2C1104.4635%2C1104.2999%2C1104.1740%2C1104.3718%2C1104.0264%2C1104.0710%2C1104.3663%2C1104.5462%2C1104.2135%2C1104.1446%2C1104.3250%2C1104.1708%2C1104.4491%2C1104.5561%2C1104.2662%2C1104.1842%2C1104.3137%2C1104.2951%2C1104.0552%2C1104.2799%2C1104.1300%2C1104.5362%2C1104.5661%2C1104.0732%2C1104.4618%2C1104.0965%2C1104.3569%2C1104.3411%2C1104.2326%2C1104.1521%2C1104.5128%2C1104.4347%2C1104.1915%2C1104.5031%2C1104.4587%2C1104.2025%2C1104.2324%2C1104.2685%2C1104.1718%2C1104.0283%2C1104.4823%2C1104.0755%2C1104.1291%2C1104.2590%2C1104.5615%2C1104.1336%2C1104.0591%2C1104.4326%2C1104.2882&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Distributed Delayed Stochastic Optimization"}, "summary": "We analyze the convergence of gradient-based optimization algorithms that\nbase their updates on delayed stochastic gradient information. The main\napplication of our results is to the development of gradient-based distributed\noptimization algorithms where a master node performs parameter updates while\nworker nodes compute stochastic gradients based on local information in\nparallel, which may give rise to delays due to asynchrony. We take motivation\nfrom statistical problems where the size of the data is so large that it cannot\nfit on one computer; with the advent of huge datasets in biology, astronomy,\nand the internet, such problems are now common. Our main contribution is to\nshow that for smooth stochastic problems, the delays are asymptotically\nnegligible and we can achieve order-optimal convergence results. In application\nto distributed optimization, we develop procedures that overcome communication\nbottlenecks and synchronization requirements. We show $n$-node architectures\nwhose optimization error in stochastic problems---in spite of asynchronous\ndelays---scales asymptotically as $\\order(1 / \\sqrt{nT})$ after $T$ iterations.\nThis rate is known to be optimal for a distributed system with $n$ nodes even\nin the absence of delays. We additionally complement our theoretical results\nwith numerical experiments on a statistical machine learning task.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.0949%2C1104.3029%2C1104.4232%2C1104.1467%2C1104.2395%2C1104.3785%2C1104.1961%2C1104.5277%2C1104.2197%2C1104.2165%2C1104.5346%2C1104.2617%2C1104.2509%2C1104.1065%2C1104.3254%2C1104.3780%2C1104.3427%2C1104.1105%2C1104.1738%2C1104.2594%2C1104.4367%2C1104.5185%2C1104.3842%2C1104.5359%2C1104.4373%2C1104.5597%2C1104.5465%2C1104.1737%2C1104.5057%2C1104.5036%2C1104.3962%2C1104.2328%2C1104.2828%2C1104.4356%2C1104.2681%2C1104.4334%2C1104.3380%2C1104.0499%2C1104.4584%2C1104.0825%2C1104.5709%2C1104.5247%2C1104.1389%2C1104.2585%2C1104.4895%2C1104.0153%2C1104.5525%2C1104.0665%2C1104.3378%2C1104.4865%2C1104.1757%2C1104.1988%2C1104.4635%2C1104.2999%2C1104.1740%2C1104.3718%2C1104.0264%2C1104.0710%2C1104.3663%2C1104.5462%2C1104.2135%2C1104.1446%2C1104.3250%2C1104.1708%2C1104.4491%2C1104.5561%2C1104.2662%2C1104.1842%2C1104.3137%2C1104.2951%2C1104.0552%2C1104.2799%2C1104.1300%2C1104.5362%2C1104.5661%2C1104.0732%2C1104.4618%2C1104.0965%2C1104.3569%2C1104.3411%2C1104.2326%2C1104.1521%2C1104.5128%2C1104.4347%2C1104.1915%2C1104.5031%2C1104.4587%2C1104.2025%2C1104.2324%2C1104.2685%2C1104.1718%2C1104.0283%2C1104.4823%2C1104.0755%2C1104.1291%2C1104.2590%2C1104.5615%2C1104.1336%2C1104.0591%2C1104.4326%2C1104.2882&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We analyze the convergence of gradient-based optimization algorithms that\nbase their updates on delayed stochastic gradient information. The main\napplication of our results is to the development of gradient-based distributed\noptimization algorithms where a master node performs parameter updates while\nworker nodes compute stochastic gradients based on local information in\nparallel, which may give rise to delays due to asynchrony. We take motivation\nfrom statistical problems where the size of the data is so large that it cannot\nfit on one computer; with the advent of huge datasets in biology, astronomy,\nand the internet, such problems are now common. Our main contribution is to\nshow that for smooth stochastic problems, the delays are asymptotically\nnegligible and we can achieve order-optimal convergence results. In application\nto distributed optimization, we develop procedures that overcome communication\nbottlenecks and synchronization requirements. We show $n$-node architectures\nwhose optimization error in stochastic problems---in spite of asynchronous\ndelays---scales asymptotically as $\\order(1 / \\sqrt{nT})$ after $T$ iterations.\nThis rate is known to be optimal for a distributed system with $n$ nodes even\nin the absence of delays. We additionally complement our theoretical results\nwith numerical experiments on a statistical machine learning task."}, "authors": ["Alekh Agarwal", "John C. Duchi"], "author_detail": {"name": "John C. Duchi"}, "author": "John C. Duchi", "arxiv_comment": "27 pages, 4 figures", "links": [{"href": "http://arxiv.org/abs/1104.5525v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1104.5525v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1104.5525v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1104.5525v1", "journal_reference": null, "doi": null, "fulltext": "Distributed Delayed Stochastic Optimization\nAlekh Agarwal\n\nJohn C. Duchi\n\n{alekh,jduchi}@eecs.berkeley.edu\nDepartment of Electrical Engineering and Computer Sciences\nUniversity of California, Berkeley\n\narXiv:1104.5525v1 [math.OC] 28 Apr 2011\n\nAbstract\nWe analyze the convergence of gradient-based optimization algorithms that base their updates on delayed stochastic gradient information. The main application of our results is to\nthe development of gradient-based distributed optimization algorithms where a master node\nperforms parameter updates while worker nodes compute stochastic gradients based on local\ninformation in parallel, which may give rise to delays due to asynchrony. We take motivation\nfrom statistical problems where the size of the data is so large that it cannot fit on one computer;\nwith the advent of huge datasets in biology, astronomy, and the internet, such problems are now\ncommon. Our main contribution is to show that for smooth stochastic problems, the delays are\nasymptotically negligible and we can achieve order-optimal convergence results. In application\nto distributed optimization, we develop procedures that overcome communication bottlenecks\nand synchronization requirements. We show n-node architectures whose optimization\n\u221a error in\nstochastic problems-in spite of asynchronous delays-scales asymptotically as O(1/ nT ) after\nT iterations. This rate is known to be optimal for a distributed system with n nodes even\nin the absence of delays. We additionally complement our theoretical results with numerical\nexperiments on a statistical machine learning task.\n\n1\n\nIntroduction\n\nWe focus on stochastic convex optimization problems of the form\nZ\nF (x; \u03be)dP (\u03be),\nminimize f (x) for f (x) := EP [F (x; \u03be)] =\nx\u2208X\n\n(1)\n\n\u039e\n\nwhere X \u2286 Rd is a closed convex set, P is a probability distribution over \u039e, and F (* ; \u03be) is convex\nfor all \u03be \u2208 \u039e, so that f is convex. The goal is to find a parameter x that approximately minimizes f\nover x \u2208 X . Classical stochastic gradient algorithms [RM51, Pol87] iteratively update a parameter\nx(t) \u2208 X by sampling \u03be \u223c P , computing g(t) = \u2207F (x(t); \u03be), and performing the update x(t + 1) =\n\u03a0X (x(t) \u2212 \u03b1(t)g(t)), where \u03a0X denotes projection onto the set X . In this paper, we analyze\nasynchronous gradient methods, where instead of receiving current information g(t), the procedure\nreceives out of date gradients g(t\u2212\u03c4 (t)) = \u2207F (x(t\u2212\u03c4 (t)), \u03be), where \u03c4 (t) is the (potentially random)\ndelay at time t. The central contribution of this paper is to develop algorithms that-under natural\nassumptions about the functions F in the objective (1)-achieve asymptotically optimal rates for\nstochastic convex optimization in spite of delays.\nOur model of delayed gradient information is particularly relevant in distributed optimization\nscenarios, where a master maintains the parameters x while workers compute stochastic gradients of\nthe objective (1). The architectural assumption of a master with several worker nodes is natural for\ndistributed computation, and other researchers have considered models similar to those in this paper [NBB01, LSZ09]. By allowing delayed and asynchronous updates, we can avoid synchronization\nissues that commonly handicap distributed systems.\n1\n\n\fMaster\n\ng1 (t \u2212 n)\nx(t)\nx(t + 1)\ng2 (t \u2212 n + 1)\n\n1\n\n2\n\ngn (t \u2212 1)\n\nx(t + n \u2212 1)\n\n3\n\nn\n\nFigure 1. Cyclic delayed update architecture. Workers compute gradients cyclically and in parallel,\npassing out-of-date information to master. Master responds with current parameters. Diagram shows\nparameters and gradients communicated between rounds t and t + n \u2212 1.\n\nCertainly distributed optimization has been studied for several decades, tracing back at least\nto seminal work of Tsitsiklis and colleagues ([Tsi84, BT89]) on minimization of smooth functions\nwhere the parameter vector is distributed. More recent work has studied problems in which each\nprocessor P\nor node i in a network has a local function fi , and the goal is to minimize the sum\nf (x) = n1 ni=1 fi (x) [NO09, RNV10, JRJ09, DAW10]. Most prior work assumes as a constraint\nthat data lies on several different nodes throughout a network. However, as Dekel et al. [DGBSX10a]\nfirst noted, in distributed stochastic settings independent realizations of a stochastic gradient can\nbe computed concurrently, and it is thus possible to obtain an aggregated gradient estimate with\nlower variance. Using modern stochastic optimization algorithms (e.g. [JNT08, Lan10]), Dekel et\nal. give a series of reductions to show that in an n-node network it is possible to achieve a speedup\nof O(n) over a single-processor so long as the objective f is smooth.\nOur work is closest to Nedi\u0107 et al.'s asynchronous subgradient method [NBB01], which is an\nincremental gradient procedure in which gradient projection steps are taken using out-of-date gradients. See Figure 1 for an illustration. The asynchronous subgradient method performs non-smooth\nminimization and suffers an asymptotic penalty in convergence rate due to the delays:p\nif the gradients are computed with a delay of \u03c4 , then the convergence rate of the procedure is O( \u03c4 /T ). The\nsetting of distributed optimization provides an elegant illustration of the role played by the delay\nin convergence rates. As in Fig. 1, the\npdelay \u03c4 can essentially be of order n in Nedi\u0107 et al.'s setting,\nwhich gives a convergence\nrate\nof\nO(\nn/T ). A simple centralized stochastic gradient algorithm at\u221a\ntains a rate of O(1/ T ), which suggests something is amiss in the distributed algorithm. Langford\net al. [LSZ09] rediscovered Nedi\u0107 et al.'s results and attempted to remove the asymptotic penalty\nby considering smooth objective functions, though their approach has a technical error (see Appendix C), and even so they do not demonstrate any provable benefits of distributed computation.\nWe analyze similar asynchronous algorithms, but we show that for smooth stochastic problems the\ndelay is asymptotically negligible-the time \u03c4 does not matter-and in fact, with parallelization,\ndelayed updates can give provable performance benefits.\nWe build on results of Dekel et al. [DGBSX10a], who show that when the objective f has\nLipschitz-continuous gradients, then when n processors compute stochastic \u221a\ngradients in parallel\nusing a common parameter x it is possible to achieve convergence rate O(1/ T n) so long as the\nprocessors are synchronized (under appropriate synchrony conditions, this holds nearly indepen2\n\n\fdently of network topology). A variant of their approach is asymptotically robust to asynchrony\nso long as most processors remain synchronized for most of the time [DGBSX10b]. We show results similar to their initial discovery, but we analyze the effects of asynchronous gradient updates\nwhere all the nodes in the network can suffer delays. Application of our main results to the distributed setting provides convergence rates in terms of the number of nodes n in the network and\nthe stochastic process governing the delays. Concretely, we show that under different assumptions\n\u221a\n3 /T + 1/ T n) to\non the network\nand\ndelay\nprocess,\nwe\nachieve\nconvergence\nrates\nranging\nfrom\nO(n\n\u221a\n\u221a\nin T . For problems\u221awith large n, we demonO(n/T + 1/ T n), which is O(1/ nT ) asymptotically\n\u221a\nstrate faster rates ranging from O((n/T )2/3 + 1/ T n) to O(1/T 2/3 + 1/ T n). In either case, the\ntime necessary to achieve \u01eb-optimal solution to the problem (1) is asymptotically O(1/n\u01eb2 ), a factor\nof n-the size of the network-better than a centralized procedure in spite of delay.\nThe remainder of the paper is organized as follows. We begin by reviewing known algorithms\nfor solving the stochastic optimization problem (1) and stating our main assumptions. Then in\nSection 3 we give abstract descriptions of our algorithms and state our main theoretical results,\nwhich we make concrete in Section 4 by formally placing the analysis in the setting of distributed\nstochastic optimization. We complement the theory in Section 5 with experiments on a real-world\ndataset, and proofs follow in the remaining sections.\nNotation For the reader's convenience, we collect our (mostly standard) notation here. We\ndenote general norms by k*k, and the dual norm k*k\u2217 to the norm k*k is defined as kzk\u2217 :=\nsupx:kxk\u22641 hz, xi. The subdifferential set of a function f is\nn\no\n\u2202f (x) := g \u2208 Rd | f (y) \u2265 f (x) + hg, y \u2212 xi for all y \u2208 dom f\n\nWe use the shorthand k\u2202f (x)k\u2217 := supg\u2208\u2202f (x) kgk\u2217 . A function f is G-Lipschitz with respect to\nthe norm k*k on X if for all x, y \u2208 X , |f (x) \u2212 f (y)| \u2264 G kx \u2212 yk. For convex f , this is equivalent\nto k\u2202f (x)k\u2217 \u2264 G for all x \u2208 X (e.g. [HUL96a]). A function f is L-smooth on X if \u2207f is Lipschitz\ncontinuous with respect to the norm k*k, defined as\nk\u2207f (x) \u2212 \u2207f (y)k\u2217 \u2264 L kx \u2212 yk ,\n\nequivalently,\n\nf (y) \u2264 f (x) + h\u2207f (x), y \u2212 xi +\n\nL\nkx \u2212 yk2 .\n2\n\nFor convex differentiable h, the Bregman divergence [Bre67] between x and y is defined as\nDh (x, y) := h(x) \u2212 h(y) \u2212 h\u2207h(y), x \u2212 yi .\n\n(2)\n\nA convex function h is c-strongly convex with respect to a norm k*k over X if\nh(y) \u2265 h(x) + hg, y \u2212 xi +\n\nc\nkx \u2212 yk2\n2\n\nfor all x, y \u2208 X and g \u2208 \u2202h(x).\n\n(3)\n\nWe use [n] to denote the set of integers {1, . . . , n}.\n\n2\n\nSetup and Algorithms\n\nIn this section we set up and recall the delay-free algorithms underlying our approach. We then\ngive the appropriate delayed versions of these algorithms, which we analyze in the sequel.\n3\n\n\f2.1\n\nSetup and Delay-free Algorithms\n\nTo build intuition for the algorithms we analyze, we first describe two closely related first-order\nalgorithms: the dual averaging algorithm of Nesterov [Nes09] and the mirror descent algorithm\nof Nemirovski and Yudin [NY83], which is analyzed further by Beck and Teboulle [BT03]. We\nbegin by collecting notation and giving useful definitions. Both algorithms are based on a proximal\nfunction \u03c8(x), where it is no loss of generality to assume that \u03c8(x) \u2265 0 for all x \u2208 X . We assume\n\u03c8 is 1-strongly convex (by scaling, this is no loss of generality). By definitions (2) and (3), the\ndivergence D\u03c8 satisfies D\u03c8 (x, y) \u2265 21 kx \u2212 yk2 .\nIn the oracle model of stochastic optimization that we assume, at time t both algorithms query\nan oracle at the point x(t), and the oracle then samples \u03be(t) i.i.d. from the distribution P and\nreturns g(t) \u2208 \u2202F (x(t); \u03be(t)). The dual averaging algorithm [Nes09] updates a dual vector z(t) and\nprimal vector x(t) \u2208 X via\no\nn\n1\n\u03c8(x) ,\n(4)\nz(t + 1) = z(t) + g(t) and x(t + 1) = argmin hz(t + 1), xi +\n\u03b1(t + 1)\nx\u2208X\nwhile mirror descent [NY83, BT03] performs the update\nx(t + 1) = argmin\nx\u2208X\n\nn\n\nhg(t), xi +\n\no\n1\nD\u03c8 (x, x(t)) .\n\u03b1(t)\n\n(5)\n\nBoth make a linear approximation to the function being minimized-a global approximation in the\ncase of the dual averaging update (4) and a more local approximation for mirror descent (5)-while\nusing the proximal function \u03c8 to regularize the points x(t).\nWe now state the two essentially standard assumptions [JNT08, Lan10, Xia10] we most often\nmake about the stochastic optimization problem (1), after which we recall the convergence rates of\nthe algorithms (4) and (5).\nAssumption A (Lipschitz Functions). For P -a.e. \u03be, the function F (* ; \u03be) is convex. Moreover, for\nany x \u2208 X , E[k\u2202F (x; \u03be)k2\u2217 ] \u2264 G2 .\nIn particular, Assumption A implies that f is G-Lipschitz continuous with respect to the norm k*k\nand that f is convex. Our second assumption has been used to show rates of convergence based on\nthe variance of a gradient estimator for stochastic optimization problems (e.g. [JNT08, Lan10]).\nAssumption B (Smooth Functions). The function f defined in (1) has L-Lipschitz continuous\ngradient, and for all x \u2208 X the variance bound E[k\u2207f (x) \u2212 \u2207F (x; \u03be)k2\u2217 ] \u2264 \u03c3 2 holds.1\nSeveral commonly used functions satisfy the above assumptions, for example:\n(i) The logistic loss: F (x; \u03be) = log[1+exp(hx, \u03bei)], the objective for logistic regression in statistics\n(e.g. [HTF01]). The objective F satisfies Assumptions A and B so long as k\u03bek is bounded.\n(ii) Least squares or linear regression: F (x; \u03be) = (a \u2212 hx, bi)2 where \u03be = (a, b) for a \u2208 Rd and\nb \u2208 R, satisfies Assumptions A and B as long as \u03be is bounded and X is compact.\nWe also make a standard compactness assumption on the optimization set X .\n1\n\nIf f is differentiable, then F (*; \u03be) is differentiable for P -a.e. \u03be, and conversely, but F need not be smoothly\ndifferentiable [Ber73]. Since \u2207F (x; \u03be) exists for P -a.e. \u03be, we will write \u2207F (x; \u03be) with no loss of generality.\n\n4\n\n\fAssumption C (Compactness). For x\u2217 \u2208 argminx\u2208X f (x) and x \u2208 X , the bounds \u03c8(x\u2217 ) \u2264 R2 /2\nand D\u03c8 (x\u2217 , x) \u2264 R2 both hold.\nUnder Assumptions A or B in addition to Assumption C, the updates (4) and (5) have known\nconvergence rates. Define the time averaged vector x\nb(T ) as\nT\n1X\nx(t + 1).\nx\nb(T ) :=\nT\n\n(6)\n\nt=1\n\nThen under Assumption A, both algorithms satisfy\n\nE[f (b\nx(T ))] \u2212 f (x\u2217 ) = O\n\n\u0012\n\nRG\n\u221a\nT\n\n\u0013\n\n(7)\n\n\u221a\nfor the stepsize choice \u03b1(t) = R/(G t) (e.g. [Nes09, Xia10, NJLS09]). The result (7) is sharp to\nconstant factors in general [NY83, ABRW10], but can be further improved under Assumption B.\nBuilding on work of Juditsky et al. [JNT08] and Lan [Lan10], Dekel et al. [DGBSX10a, Appendix\n\u22121\nA] show that under Assumptions\n\u221a B and C the stepsize choice \u03b1(t) = L + \u03b7(t), where \u03b7(t) is a\ndamping factor set to \u03b7(t) = \u03c3R t, yields for either of the updates (4) or (5) the convergence rate\n\u0012\n\u0013\nLR2\n\u03c3R\n\u2217\nE[f (b\nx(T ))] \u2212 f (x ) = O\n+\u221a\n.\n(8)\nT\nT\n\n2.2\n\nDelayed Optimization Algorithms\n\nWe now turn to extending the dual averaging (4) and mirror descent (5) updates to the setting in\nwhich instead descent (5) updates to the setting in which instead of receiving a current gradient\ng(t) at time t, the procedure receives a gradient g(t \u2212 \u03c4 (t)), that is, a stochastic gradient of the\nobjective (1) computed at the point x(t \u2212 \u03c4 (t)). In the simplest case, the delays are uniform and\n\u03c4 (t) \u2261 \u03c4 for all t, but in general the delays may be a non-i.i.d. stochastic process. Our analysis\nadmits any sequence \u03c4 (t) of delays as long as the mapping t 7\u2192 \u03c4 (t) satisfies E[\u03c4 (t)] \u2264 B < \u221e.\nWe also require that each update happens once, i.e., t 7\u2192 t \u2212 \u03c4 (t) is one-to-one, though this second\nassumption is easily satisfied.\nRecall that the problems we consider are stochastic optimization problems of the form (1).\nUnder the assumptions above, we extend the mirror descent and dual averaging algorithms in the\nsimplest way: we replace g(t) with g(t \u2212 \u03c4 (t)). For dual averaging (c.f. the update (4)) this yields\no\nn\n1\n\u03c8(x) , (9)\nz(t + 1) = z(t) + g(t \u2212 \u03c4 (t)) and x(t + 1) = argmin hz(t + 1), xi +\n\u03b1(t + 1)\nx\u2208X\n\nwhile for mirror descent (c.f. the update (5)) we have\no\nn\n1\nD\u03c8 (x, x(t)) .\nx(t + 1) = argmin hg(t \u2212 \u03c4 (t)), xi +\n\u03b1(t)\nx\u2208X\n\n(10)\n\nA generalization of Nedi\u0107 et al.'s results [NBB01] by combining their techniques with the convergence proofs of dual averaging [Nes09] and mirror descent [BT03] is as follows. Under Assumptions A and C, so long as E[\u03c4 (t)] \u2264 B < \u221e for all t, choosing \u03b1(t) = G\u221aRBt gives rate\n\u221a \u0013\n\u0012\nRG B\n\u2217\n\u221a\n.\n(11)\nE[f (b\nx(T ))] \u2212 f (x ) = O\nT\n5\n\n\f3\n\nConvergence rates for delayed optimization of smooth functions\n\nIn this section, we state and discuss several results for asynchronous stochastic gradient methods.\nWe give two sets of theorems. The first are for the asynchronous method when we make updates to\nthe parameter vector x using one stochastic subgradient, according to the update rules (9) or (10).\nThe second method involves using several stochastic subgradients for every update, each with a\npotentially different delay, which gives sharper results that we present in Section 3.2.\n\n3.1\n\nSimple delayed optimization\n\n\u221a\nIntuitively, the B-penalty due to delays for non-smooth optimization arises from the fact that\nsubgradients can change drastically when measured at slightly different locations, so a small delay\ncan introduce significant inaccuracy. To overcome the delay penalty, we now turn to the smoothness\nassumption B as well as the Lipschitz condition A (we assume both of these conditions along with\nAssumption C hold for all the theorems). In the smooth case, delays mean that stale gradients are\nonly slightly perturbed, since our stochastic algorithms constrain the variability of the points x(t).\nAs we show in the proofs of the remaining results, the error from delay essentially becomes a second\norder term: the penalty is asymptotically negligible. We study both update rules (9) and (10), and\n1\n. Here \u03b7(t) will be chosen to both control the effects of delays and for errors\nwe set \u03b1(t) = L+\u03b7(t)\nfrom stochastic gradient information. We prove the following theorem in Sec. 6.1.\n\u221a\nTheorem 1. Let the sequence x(t) be defined by the update (9). Define the stepsize \u03b7(t) \u221d t + \u03c4\nor let \u03b7(t) \u2261 \u03b7 for all t. Then\nE\n\n\u0014X\nT\nt=1\n\n\u0015\nf (x(t + 1)) \u2212 T f (x\u2217 ) \u2264\n\nT\nT\nX\n1\n1\n\u03c32 X 1\nR2 +\n+ 2LG2 (\u03c4 + 1)2\n+ 2\u03c4 GR.\n\u03b1(T + 1)\n2\n\u03b7(t)\n\u03b7(t \u2212 \u03c4 )2\nt=1\n\nt=1\n\nThe mirror descent update (10) exhibits similar convergence properties, and we prove the next\ntheorem in Sec. 6.2.\nTheorem 2. Use the conditions of Theorem 1 but generate x(t) by the update (10). Then\nE\n\n\u0014X\nT\nt=1\n\n\u0015\nT\nT\nX\n\u03c32 X 1\n1\nf (x(t+1)) \u2212T f (x\u2217 ) \u2264 2LR2 +R2 [\u03b7(1)+\u03b7(T )]+\n+2LG2 (\u03c4 +1)2\n+2\u03c4 GR\n2\n\u03b7(t)\n\u03b7(t \u2212 \u03c4 )2\nt=1\n\nt=1\n\n\u221a\n\nIn each of the above theorems, we can set \u03b7(t) = \u03c3 t + \u03c4 /R. As immediate corollaries, we\nrecall the definition (6) of the averaged sequence of x(t) and use convexity to see that\n\u0013\n\u0012\n\u03c3R\nLG2 \u03c4 2 R2 log T\nLR2 + \u03c4 GR\n\u2217\n\u221a\n+\n+\nE[f (b\nx(T ))] \u2212 f (x ) = O\nT\n\u03c32 T\nT\nfor either update rule. In addition, we can allow the delay \u03c4 (t) to be random:\nCorollary 1. Let the conditions of Theorem 1 or 2 hold, \u221a\nbut allow \u03c4 (t) to be a random mapping\n2\n2\nsuch that E[\u03c4 (t) ] \u2264 B for all t. With the choice \u03b7(t) = \u03c3 T /R the updates (9) and (10) satisfy\n\u0012\n\u0013\nLR2 + B 2 GR\n\u03c3R\nLG2 B 2 R2\nE[f (b\nx(T ))] \u2212 f (x\u2217 ) = O\n+\u221a +\n.\nT\n\u03c32 T\nT\n6\n\n\fWe provide the proof of the corollary in Sec. 6.3. The take-home message from the above corollaries,\nas well as Theorems 1 and 2, is that the penalty in convergence rate due to the delay \u03c4 (t) is\nasymptotically negligible. As we discuss in greater depth in the next section, this has favorable\nimplications for robust distributed stochastic optimization algorithms.\n\n3.2\n\nCombinations of delays\n\nIn some scenarios-including distributed settings similar to those we discuss in the next section-\nthe procedure has access not to only a single delayed gradient but to several with different delays.\nTo abstract away the essential parts of this situation, we assume that the procedure receives n\ngradients g1 , . . . , gn , where each has a potentially different delay \u03c4 (i). Now let \u03bb = (\u03bbi )ni=1 belong\nto the probability simplex, though we leave \u03bb's values unspecified for now. Then the procedure\nperforms the following updates at time t: for dual averaging,\nz(t + 1) = z(t) +\n\nn\nX\ni=1\n\n\u03bbi gi (t \u2212 \u03c4 (i))\n\nand\n\nn\nx(t + 1) = argmin hz(t + 1), xi +\nx\u2208X\n\no\n1\n\u03c8(x) (12)\n\u03b1(t + 1)\n\nwhile for mirror descent, the update is\ng\u03bb (t) =\n\nn\nX\ni=1\n\n\u03bbi gi (t \u2212 \u03c4 (i))\n\nand\n\nx(t + 1) = argmin\nx\u2208X\n\nn\n\nhg\u03bb (t), xi +\n\no\n1\nD\u03c8 (x, x(t)) .\n\u03b1(t)\n\n(13)\n\nThe next two theorems build on the proofs of Theorems 1 and 2, combining several techniques. We\nprovide the proof of Theorem 3 in Sec. 7, omitting the proof of Theorem 4 as it follows in a similar\nway from Theorem 2.\nTheorem 3. Let the sequence\u221ax(t) be defined by the update (12). Under assumptions A, B and C,\n1\n= L + \u03b7(t) and \u03b7(t) \u221d t + \u03c4 or \u03b7(t) \u2261 \u03b7 for all t. Then\nlet \u03b1(t)\nE\n\n\"\n\nT\nX\nt=1\n\n#\n\nf (x(t + 1)) \u2212 T f (x\u2217 ) \u2264 LR2 + \u03b7(T )R2 + 2\n+\n\nT\nX\nt=1\n\nn\nX\n\n\u03bbi \u03c4 (i)GR + 2\n\ni=1\n\n\u03bbi LG2 (\u03c4 (i) + 1)2\n\nT\nX\nt=1\n\ni=1\n\ni=1\nn\nX\n\n1\nE\n2\u03b7(t)\n\nn\nX\n\n2\n\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))]\n\n1\n\u03b7(t \u2212 \u03c4 )2\n\n.\n\u2217\n\nTheorem 4. Use the same conditions as Theorem 3, but assume that x(t) is defined by the update (13) and D\u03c8 (x\u2217 , x) \u2264 R2 for all x \u2208 X . Then\nE\n\n\"\n\nT\nX\nt=1\n\n\u2217\n\n#\n\nf (x(t + 1)) \u2212 T f (x ) \u2264 2R2 (L + \u03b7(T )) + 2\n+\n\nT\nX\nt=1\n\nn\nX\ni=1\n\nn\nX\n\n1\nE\n2\u03b7(t)\n\ni=1\n\n\u03bbi \u03c4 (i)GR + 2\n\nn\nX\n\n\u03bbi LG2 (\u03c4 (i) + 1)2\n\nt=1\n\ni=1\n\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))]\n\n2\n\n.\n\u2217\n\nThe consequences of Theorems 3 and 4 are powerful, as we illustrate in the next section.\n\n7\n\nT\nX\n\n1\n\u03b7(t \u2212 \u03c4 )2\n\n\f4\n\nDistributed Optimization\n\nWe now turn to what we see as the main purpose and application of the above results: developing\nrobust and efficient algorithms for distributed stochastic optimization. Our main motivations here\nare machine learning and statistical applications where the data is so large that it cannot fit on a\nsingle computer. Examples of the form (1) include logistic regression (for background, see [HTF01]),\nwhere the task is to learn a linear classifier that assigns labels in {\u22121, +1} to a series of examples,\nin which case we have the objective F (x; \u03be) = log[1 + exp(h\u03be, xi)] as described in Sec. 2.1(i); or\nlinear regression, where \u03be = (a, b) \u2208 Rd \u00d7 R and F (x; \u03be) = 21 [b \u2212 ha, xi]2 as described in Sec. 2.1(ii).\nBoth objectives satisfy assumptions A and B as discussed earlier. We consider both stochastic and\nonline/streaming scenarios for such problems. In the simplest setting, the distribution P in the\nobjective (1) is the empirical distribution over an observed dataset, that is,\nN\n1 X\nf (x) =\nF (x; \u03bei ).\nN\ni=1\n\nWe divide the N samples among n workers so that each worker has an N/n-sized subset of data.\nIn streaming applications, the distribution P is the unknown distribution generating the data, and\neach worker receives a stream of independent data points \u03be \u223c P . Worker i uses its subset of the\ndata, or its stream, to compute gi , an estimate of the gradient \u2207f of the global f . We make the\nsimplifying assumption that gi is an unbiased estimate of \u2207f (x), which is satisfied, for example,\nwhen each worker receives an independent stream of samples or computes the gradient gi based on\nsamples picked at random without replacement from its subset of the data.\nThe architectural assumptions we make are natural and based off of master/worker topologies,\nbut the convergence results in Section 3 allow us to give procedures robust to delay and asynchrony.\nWe consider two protocols: in the first, workers compute and communicate asynchronously and\nindependently with the master, and in the second, workers are at different distances from the\nmaster and communicate with time lags proportional to their distances. We show in the latter part\nof this\n\u221asection that the convergence rates of each protocol, when applied in an n-node network, are\nO(1/ nT ) for n-node networks (though lower order terms are different for each).\nBefore describing our architectures, we note that perhaps the simplest master-worker scheme is\nto have each worker simultaneously compute a stochastic gradient and send it to the master, which\ntakes a gradient step on the averaged gradient. While the n gradients are computed in parallel,\naccumulating and averaging n gradients at the master takes \u03a9(n) time, offsetting the gains of\nparallelization. Thus we consider alternate architectures that are robust to delay.\nCyclic Delayed Architecture This protocol is the delayed update algorithm mentioned in\nthe introduction, and it parallelizes computation of (estimates of) the gradient \u2207f (x). Formally,\nworker i has parameter x(t) and computes gi (t) = F (x(t); \u03bei (t)), where \u03bei (t) is a random variable\nsampled at worker i from the distribution P . The master maintains a parameter vector x \u2208 X .\nThe algorithm proceeds in rounds, cyclically pipelining updates. The algorithm begins by initiating\ngradient computations at different workers at slightly offset times. At time t, the master receives\ngradient information at a \u03c4 -step delay from some worker, performs a parameter update, and passes\nthe updated central parameter x(t + 1) back to the worker. Other workers do not see this update\nand continue their gradient computations on stale parameter vectors. In the simplest case, each\n\n8\n\n\fx(t)\n\nx(t \u2212 1)\n\ng(t \u2212 1)\n\nx(t \u2212 2)\n\nM\n\ng(t \u2212 2)\n\nM\n\ng(t \u2212 3)\n\nx(t \u2212 3)\n\ng(t \u2212 4)\n\nx(t \u2212 4)\n\n(a)\n\n(b)\n\nFigure 2. Master-worker averaging network. (a): parameters stored at different distances from\nmaster node at time t. A node at distance d from master has the parameter x(t \u2212 d). (b): gradients\ncomputed at different nodes. A node at distance d from master computes gradient g(t \u2212 d).\n\nnode suffers a delay of \u03c4 = n, though our earlier analysis applies to random delays throughout the\nnetwork as well. Recall Fig. 1 for a graphic description of the process.\nLocally Averaged Delayed Architecture At a high level, the protocol we now describe combines the delayed updates of the cyclic delayed architecture with averaging techniques of previous\nwork [NO09, DAW10]. We assume a network G = (V, E), where V is a set of n nodes (workers) and\nE are the edges between the nodes. We select one of the nodes as the master, which maintains the\nparameter vector x(t) \u2208 X over time.\nThe algorithm works via a series of multicasting and aggregation steps on a spanning tree\nrooted at the master node. In the first phase, the algorithm broadcasts from the root towards the\nleaves. At step t the master sends its current parameter vector x(t) to its immediate neighbors.\nSimultaneously, every other node broadcasts its current parameter vector (which, for a depth d\nnode, is x(t \u2212 d)) to its children in the spanning tree. See Fig. 2(a). Every worker receives its new\nparameter and computes its local gradient at this parameter. The second part of the communication\nin a given iteration proceeds from leaves toward the root. The leaf nodes communicate their\ngradients to their parents. The parent takes the gradients of the leaf nodes from the previous\nround (received at iteration t \u2212 1) and averages them with its own gradient, passing this averaged\ngradient back up the tree. Again simultaneously, each node takes the averaged gradient vectors of\nits children from the previous rounds, averages them with its current gradient vector, and passes\nthe result up the spanning tree. See Fig. 2(b) and Fig. 3 for a visual description.\nSlightly more formally, associated with each node i \u2208 V is a delay \u03c4 (i), which is (generally) twice\nits distance from the master. Fix an iteration t. Each node i \u2208 V has an out of date parameter\nvector x(t \u2212 \u03c4 (i)/2), which it sends further down the tree to its children. So, for example, the\nmaster node sends the vector x(t) to its children, which send the parameter vector x(t \u2212 1) to their\n\n9\n\n\f+ 13 g2 (t\n\n1\n3 g1 (t\n\n\u2212 d)\n\u2212 d \u2212 2) + 13 g3 (t \u2212 d \u2212 2)\n\nDepth d\n\ng2 (t \u2212 d \u2212 1)\n\nDepth d + 1\n\n{x(t \u2212 d), g2 (t \u2212 d \u2212 2), g3 (t \u2212 d \u2212 2)}\n\n1\n\n2\n\ng3 (t \u2212 d \u2212 1)\n\n{x(t \u2212 d \u2212 1)}\n\n3\n\n{x(t \u2212 d \u2212 1)}\n\nFigure 3. Communication of gradient information toward master node at time t from node 1 at\ndistance d from master. Information stored at time t by node i in brackets to right of node.\n\nchildren, which in turn send x(t \u2212 2) to their children, and so on. Each node computes\ngi (t \u2212 \u03c4 (i)/2) = \u2207F (x(t \u2212 \u03c4 (i)/2); \u03bei (t)),\nwhere \u03bei (t) is a random variable sampled at node i from the distribution P . The communication\nback up the hierarchy proceeds as follows: the leaf nodes in the tree (say at depth d) send the\ngradient vectors gi (t \u2212 d) to their immediate parents in the tree. At the previous iteration t \u2212 1, the\nparent nodes received gi (t \u2212 d \u2212 1) from their children, which they average with their own gradients\ngi (t\u2212d+1) and pass to their parents, and so on. The master node at the root of the tree receives an\naverage of delayed gradients from the entire tree, with each gradient having a potentially different\ndelay, giving rise to updates of the form (12) or (13).\n\n4.1\n\nConvergence rates for delayed distributed minimization\n\nHaving described our architectures, we can now give corollaries to the theoretical results from the\nprevious sections that show it is possible to achieve asymptotically faster rates (over centralized\nprocedures) using distributed algorithms even without imposing synchronization requirements. We\nallow workers to pipeline updates by computing asynchronously and in parallel, so each worker can\ncompute low variance estimate of the gradient \u2207f (x).\nWe begin with a simple corollary to the results in Sec. 3.1. We ignore the constants L, G, R,\nand \u03c3, which are not dependent on the characteristics of the network. We also assume that each\nworker uses m independent samples of \u03be \u223c P to compute the stochastic gradient as\nm\n\ngi (t) =\n\n1 X\n\u2207F (x(t); \u03bei (j)).\nm\nj=1\n\nUsing the cyclic protocol as in Fig. 1, Theorems 1 and 2 give the following result.\nCorollary 2. Let \u03c8(x) = 12 kxk22 , assume the conditions in Corollary 1, and assume that each\nworker uses m samples\n\u221a \u221a\u03be \u223c P to compute the gradient it communicates to the master. Then with\nthe choice \u03b7(t) = T / m either of the updates (9) or (10) satisfy\n\u0012 2\n\u0013\nB\n1\nB 2m\nE[f (b\nx(T ))] \u2212 f (x\u2217 ) = O\n+\u221a\n.\n+\nT\nT\nTm\n10\n\n\fProof\nThe corollary follows straightforwardly from the realization that the variance \u03c3 2 =\nE[k\u2207f (x) \u2212 gi (t)k22 ] = E[k\u2207f (x) \u2212 \u2207F (x; \u03be)k22 ]/m = O(1/m) when workers use m independent\nstochastic gradient samples.\nIn the above corollary, so long as the bound on the delay B satisfies, say, B = o(T 1/4 ), then \u221a\nthe last\nterm in the bound is asymptotically negligible, and we achieve a convergence rate of O(1/ T m).\nThe cyclic delayed architecture has the drawback that information from a worker can take\nO(n) time to reach the master. While the algorithm is robust to delay and does not need lock-step\ncoordination of workers, the downside of the architecture is that the essentially n2 m/T term in\nthe bounds above can be quite large. Indeed, if each worker computes its gradient over m samples\nwith m \u2248 n-say\nto avoid idling of workers-then the cyclic architecture has convergence\u221arate\n\u221a\n3\nO(n /T + 1/ nT ). For moderate T or large n, the delay penalty n3 /T may dominate 1/ nT ,\noffsetting the gains of parallelization.\nTo address the large n drawback, we turn our attention to the locally averaged architecture\ndescribed by Figs. 2 and 3, where delays can be smaller since they depend only on the height\nof a spanning tree in the network. The algorithm requires more synchronization than the cyclic\narchitecture but still performs limited local communication. Each worker computes gi (t \u2212 \u03c4 (i)) =\n\u2207F (x(t \u2212 \u03c4 (i)); \u03bei (t)) where \u03c4 (i) is the delay of worker i from the master and \u03bei \u223c P . As a result of\nthe communication procedure, the master receives a convex combination of the stochastic gradients\nevaluated at each worker i, for which we gave results in Section 3.2.\nP\nIn this architecture, the master receives gradients of the form g\u03bb (t) = ni=1 \u03bbi gi (t \u2212 \u03c4 (i)) for\nsome \u03bb in the simplex, which puts us in the setting of Theorems 3 and 4. We now make the\nreasonable assumption that the gradient errors \u2207f (x(t)) \u2212 gi (t) are uncorrelated across the nodes\nin the network.2 In statistical applications, for example, each worker may own independent data or\nreceive streaming data from independent sources; more generally, each worker can simply receive\nindependent samples \u03bei \u223c P . We also set \u03c8(x) = 12 kxk22 , and observe\nE\n\nn\nX\ni=1\n\n2\n\n\u03bbi \u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))\n\n=\n2\n\nn\nX\ni=1\n\n\u03bb2i E k\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))k22 .\n\nThis gives the following corollary to Theorems 3 and 4.\n\u221a\n\u221a\nCorollary 3. Set \u03bbi = n1 for all i, \u03c8(x) = 21 kxk22 , and \u03b7(t) = \u03c3 t + \u03c4 /R n. Let \u03c4\u0304 and \u03c4 2 denote\nthe average of the delays \u03c4 (i) and \u03c4 (i)2 , respectively. Under the conditions of Theorem 3 or 4,\n!\n#\n\" T\nX\nR\u03c3 \u221a\nLG2 R2 n\u03c4 2\n\u2217\n2\nT .\nlog T + \u221a\nf (x(t + 1)) \u2212 T f (x ) = O LR + \u03c4\u0304 GR +\nE\n\u03c32\nn\nt=1\n\n\u221a\n\u221a\nThe log T multiplier can be reduced to a constant if we set \u03b7(t) \u2261 \u03c3 T /R n. By using the averaged\n\u221a\nsequence x\nb(T ) (6), Jensen's inequality gives that asymptotically E[f (b\nx(T ))] \u2212 f (x\u2217 ) = O(1/ T n),\nwhich is an optimal dependence on the number of samples \u03be calculated by the method. We also\nobserve in this architecture, the delay \u03c4 is bounded by the graph diameter D, giving us the bound:\n#\n\" T\n\u0012\n\u0013\nX\nR\u03c3 \u221a\nLG2 R2 nD 2\n\u2217\n2\nlog T + \u221a\nf (x(t + 1)) \u2212 T f (x ) = O LR + DGR +\nE\nT .\n(14)\n\u03c32\nn\nt=1\n2\n\nSimilar results continue to hold under weak correlation.\n\n11\n\n\fThe above corollaries are general and hold irrespective of the relative costs of communication\nand computation. However, with knowledge of the costs, we can adapt the stepsizes slightly to give\nbetter rates of convergence when n is large or communication to the master node is expensive. For\nnow, we focus on the cyclic architecture (the setting of Corollary 2), though the same principles\napply to the local averaging scheme. Let C denote the cost of communicating between the master\nand workers in terms of the time to compute a single gradient sample, and assume that we set\nm = Cn, so that no worker node has idle p\ntime. For simplicity, we let the delay be non-random, so\nB = \u03c4 = n. Consider the choice \u03b7(t) = \u03b7 T /(Cn) for the damping stepsizes, where \u03b7 \u2265 1. This\nsetting in Theorem 1 gives\n\u0012 2 3\n\u0013\n\u0012 2 3\n\u0013\n\u03b7 Cn\n\u03b7 Cn\n1\n\u03b7\n\u03b7\n\u2217\n\u221a\n\u221a\n\u221a\nE[f (b\nx(T ))] \u2212 f (x ) = O\n+\n=O\n+\n+\n,\nT\nT\nT Cn \u03b7 T Cn\nT Cn\nwhere the last equality follows since \u03b7 \u2265 1. Optimizing for \u03b7 on the right yields\n)\n(\n)\n!\n(\n2/3 n3\n1\nn\nn7/6\n,1\nand E[f (b\nx(T ))] \u2212 f (x\u2217 ) = O min\n,\n+\u221a\n.\n\u03b7 = max\nT 1/6 C 1/2\nT 2/3 T\nT Cn\n\n(15)\n\nThe convergence rates thus follow two regimes. When \u221aT \u2264 n7 /C 3 , we have convergence rate\nT Cn) convergence. Roughly, in time\nO(n2/3 /T 2/3 ), while once T > n7 /C 3 , we attain O(1/\n\u221a\nproportional to T C, we achieve optimization error 1/ T Cn, which is order-optimal given that we\ncan compute a total of T Cn stochastic gradients [ABRW10]. The scaling of this bound is nicer than\nthat previously: the dependence on network size is at worst n2/3 , which we obtain by increasing the\ndamping factor \u03b7(t)-and hence decreasing the stepsize \u03b1(t) = 1/(L + \u03b7(t))-relative to the setting\nof Corollary 2. We remark that applying \u221a\nthe same technique to Corollary\n\u221a 3 gives convergence rate\n2/3\nscaling as the smaller of O((D/T ) + 1/ T Cn) and O((nCD/T + 1/ T Cn). Since the diameter\nD \u2264 n, this is faster than the cyclic architecture's bound (15).\n\n4.2\n\nRunning-time comparisons\n\nHaving derived the rates of convergence of the different distributed procedures above, we now\nexplicitly study the running times of the centralized stochastic gradient algorithms (4) and (5),\nthe cyclic delayed protocol with the updates (9) and (10), and the locally averaged architecture\nwith the updates (12) and (13). To make comparisons more cleanly, we avoid constants, assuming\nwithout loss that the variance bound \u03c3 2 on E k\u2207f (x) \u2212 \u2207F (x; \u03be)k2 is 1, and that sampling \u03be \u223c P\nand evaluating \u2207F (x; \u03be) requires one unit of time. Noting that E[\u2207F (x; P\n\u03be)] = \u2207f (x), it is clear\n1\n1\n2\nthat if we receive m uncorrelated samples of \u03be, the variance Ek\u2207f (x) \u2212 m m\nj=1 \u2207F (x; \u03bej )k2 \u2264 m .\nNow we state our assumptions on the relative times used by each algorithm. Let T be the\nnumber of units of time allocated to each algorithm, and let the centralized, cyclic delayed and\nlocally averaged delayed algorithms complete Tcent , Tcycle and Tdist iterations, respectively, in time\nT . It is clear that Tcent = T . We assume that the distributed methods use mcycle and mdist samples\nof \u03be \u223c P to compute stochastic gradients and that the delay \u03c4 of the cyclic algorithm is n. For\nconcreteness, we assume that communication is of the same order as computing the gradient of\none sample \u2207F (x; \u03be) so that C = 1. In the cyclic setup of Sec. 3.1, it is reasonable to assume\nthat mcycle = \u03a9(n) to avoid idling of workers (Theorems 1 and 2, as well as the bound (15), show\n2\n= 1/mcycle ). For mcycle = \u03a9(n),\nit is asymptotically beneficial to have mcycle larger, since \u03c3cycle\nm\nmcycle\nthe master requires n units of time to receive one gradient update, so cycle\nn Tcycle = T . In\n12\n\n\fCentralized (4, 5)\nCyclic (9, 10)\nLocal (12, 13)\n\n\u0012r \u0013\n1\nEf (b\nx) \u2212 f (x ) = O\nT !\n!\n3\n2/3\nn\n1\nn\n,\n+\u221a\nO min\n2/3\nT\nT\nTn\n!\n!\n2/3\n2\nD\nn\u03c4\n1\nEf (b\nx) \u2212 f (x\u2217 ) = O min\n,\n+\u221a\nT 2/3 T\nnT\n\u2217\n\nTable 1. Upper bounds on Ef (b\nx) \u2212 f (x\u2217 ) for three computational architectures, where x\nb is the\noutput of each algorithm after T units of time. Each algorithm runs for the amount of time it takes\na centralized stochastic algorithm to perform T iterations as in (16). Here D is the diameter of the\nP\nnetwork, n is the number of nodes, and \u03c4 2 = n1 ni=1 \u03c4 (i)2 is the average squared communication\ndelay for the local averaging architecture. Bounds for the cyclic architecture assume delay \u03c4 = n.\n\nthe locally delayed framework, if each node uses mdist samples to compute a gradient, the master\n2\n= 1/mdist .\nreceives a gradient every mdist units of time, and hence mdist Tdist = T . Further, \u03c3dist\nWe summarize our assumptions by saying that in T units of time, each algorithm performs the\nfollowing number of iterations:\nTcent = T,\n\nTcycle =\n\nTn\n,\nmcycle\n\nand\n\nTdist =\n\nT\n.\nmdist\n\n(16)\n\nPlugging the above iteration counts into the earlier bound (8) and Corollaries 2 and 3 via the\nsharper result (15), we can provide upper bounds (to constant factors) on the expected optimization\naccuracy after T units of time for each of the distributed architectures as in Table 1. Asymptotically\nin the number of units of time T , both the cyclic and locally communicating stochastic optimization\nschemes have the same convergence rate. However, topological considerations show that the locally\ncommunicating method (Figs. 2 and 3) has better performance than the cyclic architecture, though\nit requires more worker coordination. Since the lower order terms matter only for large n or small\nT , we compare the terms n2/3 /T 2/3 and D 2/3 /T 2/3 for the cyclic and locally averaged algorithms,\nrespectively. Since D \u2264 n for any network, the locally averaged algorithm always guarantees better\nperformance than the cyclic algorithm. For specific graph topologies, however, we can quantify the\ntime improvements:\n\u2022 n-node cycle or path: D = n so that both methods have the same convergence rate.\n\u221a\n\u221a\n\u221a\n\u2022 n-by- n grid: D = n, so the distributed method has a factor of n2/3 /n1/3 = n1/3 improvement over the cyclic architecture.\n\u2022 Balanced trees and expander graphs: D = O(log n), so the distributed method has a factor-\nignoring logarithmic terms-of n2/3 improvement over cyclic.\nNaturally, it is possible to modify our assumptions. In a network in which communication\nis cheap, or conversely, in a problem for which the computation of \u2207F (x; \u03be) is more expensive\nthan communication, then the number of samples \u03be \u223c P for which which each worker computes\ngradients is small. Such problems are frequent in statistical machine learning, such as when learning\nconditional random field models, which are useful in natural language processing, computational\nbiology, and other application areas [LMP01]. In this case, it is reasonable to have mcycle =\n13\n\n\fTime to \u01eb accuracy\n\n800\n\n600\n\n400\n\n200\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n8\n\n10\n\n12\n\n15\n\n18\n\n22 26\n\nNumber of workers\nFigure 4. Optimization performance of the delayed cyclic method (9) for the Reuters RCV1 dataset\nwhen we assume that the cost of communication to the master is the same as computing the gradient\nof one term in the objective (17). The number of samples m computed is equal to n for each worker.\nPlotted is the estimated time to \u01eb-accuracy as a function of number of workers n.\n\nO(1), in which case Tcycle = T n and the\n\u221a cyclic delayed architecture has stronger convergence\nguarantees of O(min{n2 /T, 1/T 2/3 } + 1/ T n). In any case, both non-centralized protocols enjoy\nsignificant asymptotically faster convergence rates for stochastic optimization problems in spite of\nasynchronous delays.\n\n5\n\nNumerical Results\n\nThough this paper focuses mostly on the theoretical analysis of the methods we have presented, it\nis important to understand the practical aspects of the above methods in solving real-world tasks\nand problems with real data. To that end, we use the cyclic delayed method (12) to solve a common\nstatistical machine learning problem. Specifically, we focus on solving the logistic regression problem\nmin f (x) =\nx\n\nN\n1 X\nlog(1 + exp(\u2212bi hai , xi))\nN\ni=1\n\nsubject to kxk2 \u2264 R.\n\n(17)\n\nWe use the Reuters RCV1 dataset [LYRL04], which consists of N \u2248 800000 news articles, each\nlabeled with some combination of the four labels economics, government, commerce, and medicine.\nIn the above example, the vectors ai \u2208 {0, 1}d , d \u2248 105 , are feature vectors representing the words\nin each article, and the labels bi are 1 if the article is about government, \u22121 otherwise.\nWe simulate the cyclic delayed optimization algorithm (9) for the problem (17) for several\nchoices of the number of workers n and the number of samples m computed at each worker. We\nsummarize the results of our experiments in Figure 4. To generate the figure, we fix an \u01eb (in this\ncase, \u01eb = .05), then measure the time it takes the stochastic algorithm (9) to output an x\nb such that\nf (b\nx) \u2264 inf x\u2208X f (x) + \u01eb. We perform each experiment ten times.\nAfter computing the number of iterations required to achieve \u01eb-accuracy, we convert the results\nto running time by assuming it takes one unit of time to compute the gradient of one term in the sum\n14\n\n\fdefining the objective (17). We also assume that it takes 1 unit of time, i.e. C = 1, to communicate\nfrom one of the workers to the master, for the master to perform an update, and communicate back\nto one of the workers. In an n node system where each worker computes m samples of the gradient,\nthe master receives an update every max{ m\nn , 1} time units. A centralized algorithm computing\nm samples of its gradient performs an update every m time units. By multiplying the number of\niterations to \u01eb-optimality by max{ m\nn , 1} for the distributed method and by m for the centralized,\nwe can estimate the amount of time it takes each algorithm to achieve an \u01eb-accurate solution.\nWe now turn to discussing Figure 4. The delayed update (9) enjoys speedup (the ratio of time\nto \u01eb-accuracy for an n-node system versus the centralized procedure) nearly linear\np in the number n\nof worker machines until n \u2265 15 or so. Since we use the stepsize choice \u03b7(t) \u221d t/n, which yields\nthe predicted convergence rate given by Corollary 2, the n2 m/T \u2248 n3 /T term in the convergence\nrate presumably becomes non-negligible for larger n. This expands on earlier experimental work\nwith a similar method [LSZ09], which experimentally demonstrated linear speedup for small values\nof n, but did not investigate larger network sizes. Roughly, as predicted by our theory, for nonasymptotic regimes the cost of communication and delays due to using n nodes mitigate some of the\nbenefits of parallelization. Nevertheless, as our analysis shows, allowing delayed and asynchronous\nupdates still gives significant performance improvements.\n\n6\n\nDelayed Updates for Smooth Optimization\n\nIn this section, we prove Theorems 1 and 2. We collect in Appendix A a few technical results\nrelevant to our proof; we will refer to results therein without comment. Before proving either\ntheorem, we state the lemma that is the key to our argument. Lemma 4 shows that certain\ngradient-differencing terms are essentially of second order. As a consequence, when we combine the\nresults of the lemma with Lemma 7, which bounds\u221aE[kx(t) \u2212 x(t + \u03c4 )k2 ], the\n\u221a gradient differencing\nterms become O(log T ) for step size choice \u03b7(t) \u221d t, or O(1) for \u03b7(t) \u2261 \u03b7 T .\nLemma 4. Let assumptions A and B on the function f and the compactness assumption C hold.\nThen for any sequence x(t)\nT\nX\nt=1\n\nT\n\nLX\nkx(t \u2212 \u03c4 ) \u2212 x(t + 1)k2 + 2\u03c4 GR.\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 )), x(t + 1) \u2212 x i \u2264\n2\n\u2217\n\nt=1\n\nConsequently, if E[kx(t) \u2212 x(t + 1)k2 ] \u2264 \u03ba(t)2 G2 for a non-increasing sequence \u03ba(t),\nE\n\n\u0014X\nT\nt=1\n\n\u0015\n\nT\nLG2 (\u03c4 + 1)2 X\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 )), x(t + 1) \u2212 x i \u2264\n\u03ba(t \u2212 \u03c4 )2 + 2\u03c4 GR.\n2\n\u2217\n\nt=1\n\nProof The proof follows by using a few Bregman divergence identities to rewrite the left hand\nside of the above equations, then recognizing that the result is close to a telescoping sum. Recalling\nthe definition of a Bregman divergence (2), we note the following well-known four term equality, a\nconsequence of straightforward algebra: for any a, b, c, d,\nh\u2207f (a) \u2212 \u2207f (b), c \u2212 di = Df (d, a) \u2212 Df (d, b) \u2212 Df (c, a) + Df (c, b).\n\n15\n\n(18)\n\n\fUsing the equality (18), we see that\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 )), x(t + 1) \u2212 x\u2217 i\n\n= Df (x\u2217 , x(t)) \u2212 Df (x\u2217 , x(t \u2212 \u03c4 )) \u2212 Df (x(t + 1), x(t)) + Df (x(t + 1), x(t \u2212 \u03c4 )).\n\n(19)\n\nTo make (19) useful, we note that the Lipschitz continuity of \u2207f implies\nf (x(t + 1)) \u2264 f (x(t \u2212 \u03c4 )) + h\u2207f (x(t \u2212 \u03c4 )), x(t + 1) \u2212 x(t \u2212 \u03c4 )i +\n\nL\nkx(t \u2212 \u03c4 ) \u2212 x(t + 1)k2\n2\n\nso that recalling the definition of Df (2) we have\nDf (x(t + 1), x(t \u2212 \u03c4 )) \u2264\n\nL\nkx(t \u2212 \u03c4 ) \u2212 x(t + 1)k2 .\n2\n\nIn particular, using the non-negativity of Df (x, y), we can replace (19) with the bound\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 )), x(t + 1) \u2212 x\u2217 i \u2264 Df (x\u2217 , x(t))\u2212Df (x\u2217 , x(t\u2212\u03c4 ))+\n\nL\nkx(t \u2212 \u03c4 ) \u2212 x(t + 1)k2 .\n2\n\nSumming the inequality, we see that\nT\nX\nt=1\n\n\u2217\n\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 )), x(t + 1) \u2212 x i \u2264\n\nT\nX\n\nt=T \u2212\u03c4 +1\n\nT\n\nLX\nDf (x , x(t))+\nkx(t \u2212 \u03c4 ) \u2212 x(t + 1)k2 .\n2 t=1\n\u2217\n\n(20)\nTo bound the first Bregman divergence term, we recall that by Assumption C and the strong\nconvexity of \u03c8, kx\u2217 \u2212 x(t)k2 \u2264 2D\u03c8 (x\u2217 , x(t)) \u2264 2R2 , and hence the optimality of x\u2217 implies\nDf (x\u2217 , x(t)) = f (x\u2217 ) \u2212 f (x(t)) \u2212 h\u2207f (x(t)), x\u2217 \u2212 x(t)i \u2264 k\u2207f (x(t))k\u2217 kx\u2217 \u2212 x(t)k \u2264 2GR.\nThis gives the first bound of the lemma. For the second bound, using convexity, we see that\nkx(t \u2212 \u03c4 ) \u2212 x(t + 1)k2 \u2264 (\u03c4 + 1)2\n\n\u03c4\nX\ns=0\n\n1\nkx(t \u2212 s) \u2212 x(t \u2212 s + 1)k2 ,\n\u03c4 +1\n\nso by taking expectations we have E[kx(t) \u2212 x(t + \u03c4 + 1)k2 ] \u2264 (\u03c4 + 1)2 \u03ba(t \u2212 \u03c4 )2 G2 . Since \u03ba is nonincreasing (by\nthe definition of the update scheme) we see that the sum (20) is further bounded by\nL PT\n2\u03c4 GR + 2 t=1 G2 (\u03c4 + 1)2 \u03ba(t \u2212 \u03c4 )2 as desired.\n\n6.1\n\nProof of Theorem 1\n\nThe essential idea in this proof is to use convexity and smoothness to bound f (x(t)) \u2212 f (x\u2217 ), then\nuse the sequence {\u03b7(t)}, which decreases the stepsize \u03b1(t), to cancel variance terms. To begin, we\ndefine the error e(t)\ne(t) := \u2207f (x(t)) \u2212 g(t \u2212 \u03c4 )\nwhere g(t\u2212\u03c4 ) = \u2207F (x(t\u2212\u03c4 ); \u03be(t) for some \u03be(t) \u223c P . Note that e(t) does not have zero expectation,\nas there is a time delay.\n16\n\n\fBy using the convexity of f and then the L-Lipschitz continuity of \u2207f , for any x\u2217 \u2208 X , we have\nf (x(t)) \u2212 f (x\u2217 ) \u2264 h\u2207f (x(t)), x(t) \u2212 x\u2217 i = h\u2207f (x(t)), x(t + 1) \u2212 x\u2217 i + h\u2207f (x(t)), x(t) \u2212 x(t + 1)i\nL\n\u2264 h\u2207f (x(t)), x(t + 1) \u2212 x\u2217 i + f (x(t)) \u2212 f (x(t + 1)) + kx(t) \u2212 x(t + 1)k2 ,\n2\nso that\nL\nkx(t) \u2212 x(t + 1)k2\n2\nL\n= hg(t \u2212 \u03c4 ), x(t + 1) \u2212 x\u2217 i + he(t), x(t + 1) \u2212 x\u2217 i + kx(t) \u2212 x(t + 1)k2\n2\n\nf (x(t + 1)) \u2212 f (x\u2217 ) \u2264 h\u2207f (x(t)), x(t + 1) \u2212 x\u2217 i +\n\n= hz(t + 1), x(t + 1) \u2212 x\u2217 i \u2212 hz(t), x(t + 1) \u2212 x\u2217 i + he(t), x(t + 1) \u2212 x\u2217 i +\n\nL\nkx(t) \u2212 x(t + 1)k2 .\n2\n\nNow, by applying Lemma 5 in Appendix A and the definition of the update (9), we see that\n\u2212 hz(t), x(t + 1) \u2212 x\u2217 i \u2264 \u2212 hz(t), x(t) \u2212 x\u2217 i +\n\n1\n1\n[\u03c8(x(t + 1)) \u2212 \u03c8(x(t))] \u2212\nD\u03c8 (x(t + 1), x(t)),\n\u03b1(t)\n\u03b1(t)\n\nwhich implies\nf (x(t + 1)) \u2212 f (x\u2217 )\n\n1\n[\u03c8(x(t + 1)) \u2212 \u03c8(x(t))]\n\u03b1(t)\nL\n\u2212 LD\u03c8 (x(t + 1), x(t)) \u2212 \u03b7(t)D\u03c8 (x(t + 1), x(t)) + kx(t) \u2212 x(t + 1)k2 + he(t), x(t + 1) \u2212 x\u2217 i\n2\n1\n\u2217\n\u2217\n\u2264 hz(t + 1), x(t + 1) \u2212 x i \u2212 hz(t), x(t) \u2212 x i +\n[\u03c8(x(t + 1)) \u2212 \u03c8(x(t))]\n\u03b1(t)\n\u2212 \u03b7(t)D\u03c8 (x(t + 1), x(t)) + he(t), x(t + 1) \u2212 x\u2217 i .\n(21)\n\n\u2264 hz(t + 1), x(t + 1) \u2212 x\u2217 i \u2212 hz(t), x(t) \u2212 x\u2217 i +\n\nTo get the bound (21), we substituted \u03b1(t)\u22121 = L + \u03b7(t) and then used the fact that \u03c8 is strongly\nconvex, so D\u03c8 (x(t + 1), x(t)) \u2265 21 kx(t) \u2212 x(t + 1)k2 . By summing the bound (21), we have the\nfollowing non-probabilistic inequality:\nT\nX\nt=1\n\nf (x(t + 1)) \u2212 f (x\u2217 )\n\n\u0014\n\u0015\nT\nX\n1\n1\n1\n\u2264 hz(T + 1), x(T + 1) \u2212 x i +\n\u03c8(x(t))\n\u03c8(x(T + 1)) +\n\u2212\n\u03b1(T )\n\u03b1(t \u2212 1) \u03b1(t)\n\u2217\n\nt=1\n\n\u2212\n\nT\nX\n\n\u03b7(t)D\u03c8 (x(t + 1), x(t)) +\n\nt=1\n\n1\n\u2264\n\u03c8(x\u2217 ) +\n\u03b1(T + 1)\n+\n\nT\nX\nt=1\n\n\u0014\n\nT\nX\nt=1\n\nhe(t), x(t + 1) \u2212 x\u2217 i\n\n\u0015 X\nT\n1\n1\n\u2212\n\u03c8(x(t))\n\u2212\n\u03b7(t)D\u03c8 (x(t + 1), x(t))\n\u03b1(t \u2212 1) \u03b1(t)\nt=1\nt=1\n\nT\nX\n\nhe(t), x(t + 1) \u2212 x\u2217 i\n\n(22)\n\n17\n\n\fsince \u03c8(x) \u2265 0 and x(T + 1) minimizes hz(T + 1), xi + \u03b1(T1+1) \u03c8(x). What remains is to control the\nsummed e(t) terms in the bound (22). We can do this simply using the second part of Lemma 4.\nIndeed, we have\nT\nX\nt=1\n\n=\n\nhe(t), x(t + 1) \u2212 x\u2217 i\n\nT\nX\nt=1\n\n(23)\n\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 )), x(t + 1) \u2212 x\u2217 i +\n\nT\nX\nt=1\n\nh\u2207f (x(t \u2212 \u03c4 )) \u2212 g(t \u2212 \u03c4 ), x(t + 1) \u2212 x\u2217 i .\n\nWe can apply Lemma 4 to the first term in (23) by bounding kx(t) \u2212 x(t + 1)k with Lemma 7.\n\u221a\n4G2\nSince \u03b7(t) \u221d t + \u03c4 , Lemma 7 with t0 = \u03c4 implies E[kx(t) \u2212 x(t + 1)k2 ] \u2264 \u03b7(t)\n2 . As a consequence,\nE\n\n\u0014X\nT\nt=1\n\n\u2217\n\n\u0015\n\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 )), x(t + 1) \u2212 x i \u2264 2\u03c4 GR + 2L(\u03c4 + 1)2 G2\n\nT\nX\nt=1\n\n1\n.\n\u03b7(t \u2212 \u03c4 )2\n\nWhat remains, then, is to bound the stochastic (second) term in (23). This is straightforward,\nthough:\nh\u2207f (x(t \u2212 \u03c4 )) \u2212 g(t \u2212 \u03c4 ), x(t + 1) \u2212 x\u2217 i\n\n= h\u2207f (x(t \u2212 \u03c4 )) \u2212 g(t \u2212 \u03c4 ), x(t) \u2212 x\u2217 i + h\u2207f (x(t \u2212 \u03c4 )) \u2212 g(t \u2212 \u03c4 ), x(t + 1) \u2212 x(t)i\n1\n\u03b7(t)\n\u2264 h\u2207f (x(t \u2212 \u03c4 )) \u2212 g(t \u2212 \u03c4 ), x(t) \u2212 x\u2217 i +\nk\u2207f (x(t \u2212 \u03c4 )) \u2212 g(t \u2212 \u03c4 )k2\u2217 +\nkx(t + 1) \u2212 x(t)k2\n2\u03b7(t)\n2\n\nby the Fenchel-Young inequality applied to the conjugate pair 12 k*k2\u2217 and 12 k*k2 . In addition,\n\u2207f (x(t \u2212 \u03c4 )) \u2212 g(t \u2212 \u03c4 ) is independent of x(t) given the sigma-field containing g(1), . . . , g(t\u2212\u03c4 \u22121),\nsince x(t) is a function of gradients to time t \u2212 \u03c4 \u2212 1, so the first term has zero expectation. Also\nrecall that E[k\u2207f (x(t \u2212 \u03c4 )) \u2212 g(t \u2212 \u03c4 )k\u2217 ]2 is bounded by \u03c3 2 by assumption. Combining the above\ntwo bounds into (23), we see that\nT\nX\nt=1\n\nE[he(t), x(t + 1) \u2212 x\u2217 i]\n\nT\nT\nT\nX\n\u03c32 X 1\n1\n1X\n2\n2\n2\n\u2264\n\u03b7(t) kx(t + 1) \u2212 x(t)k + 2LG (\u03c4 + 1)\n+\n+ 2\u03c4 GR.\n2\n\u03b7(t) 2\n\u03b7(t \u2212 \u03c4 )2\nt=1\n\nSince D\u03c8 (x(t + 1), x(t)) \u2265\n1\n1\n\u03b1(t\u22121) \u2212 \u03b1(t) \u2264 0 gives\nT\nX\nt=1\n\n6.2\n\n(24)\n\nt=1\n\nt=1\n\n1\n2\n\nkx(t) \u2212 x(t + 1)k2 , combining (24) with (22) and noting that\n\nT\nT\nX\n1\n1\n\u03c32 X 1\n\u2217\n2\n2\nEf (x(t + 1)) \u2212 f (x ) \u2264\n\u03c8(x ) +\n+ 2LG (\u03c4 + 1)\n+ 2\u03c4 GR.\n\u03b1(T + 1)\n2\n\u03b7(t)\n\u03b7(t \u2212 \u03c4 )2\n\u2217\n\nt=1\n\nt=1\n\nProof of Theorem 2\n\nThe proof of Theorem 2 is similar to that of Theorem 1, so we will be somewhat terse. We define\nthe error e(t) = \u2207f (x(t)) \u2212 g(t \u2212 \u03c4 ), identically as in the earlier proof, and begin as we did in the\n18\n\n\fproof of Theorem 1. Recall that\nf (x(t + 1)) \u2212 f (x\u2217 ) \u2264 hg(t \u2212 \u03c4 ), x(t + 1) \u2212 x\u2217 i + he(t), x(t + 1) \u2212 x\u2217 i +\n\nL\nkx(t) \u2212 x(t + 1)k2 . (25)\n2\n\nApplying the first-order optimality condition to the definition of x(t + 1) (5), we get\nh\u03b1(t)g(t \u2212 \u03c4 ) + \u2207\u03c8(x(t + 1)) \u2212 \u2207\u03c8(x(t)), x \u2212 x(t + 1)i \u2265 0\nfor all x \u2208 X . In particular, we have\n\u03b1(t) hg(t \u2212 \u03c4 ), x(t + 1) \u2212 x\u2217 i \u2264 h\u2207\u03c8(x(t + 1)) \u2212 \u2207\u03c8(x(t)), x\u2217 \u2212 x(t + 1)i\n\n= D\u03c8 (x\u2217 , x(t)) \u2212 D\u03c8 (x\u2217 , x(t + 1)) \u2212 D\u03c8 (x(t + 1), x(t)).\n\nApplying the above to the inequality (25), we see that\nf (x(t + 1)) \u2212 f (x\u2217 )\nL\n1\n[D\u03c8 (x\u2217 , x(t)) \u2212 D\u03c8 (x\u2217 , x(t + 1)) \u2212 D\u03c8 (x(t + 1), x(t))] + he(t), x(t + 1) \u2212 x\u2217 i + kx(t) \u2212 x(t + 1)k2\n\u2264\n\u03b1(t)\n2\n1\n\u2264\n[D\u03c8 (x\u2217 , x(t)) \u2212 D\u03c8 (x\u2217 , x(t + 1))] + he(t), x(t + 1) \u2212 x\u2217 i \u2212 \u03b7(t)D\u03c8 (x(t + 1), x(t))\n(26)\n\u03b1(t)\nwhere for the last inequality, we use the fact that D\u03c8 (x(t + 1), x(t)) \u2265 12 kx(t) \u2212 x(t + 1)k2 , by the\nstrong convexity of \u03c8, and that \u03b1(t)\u22121 = L + \u03b7(t). By summing the inequality (26), we have\n\u0014\n\u0015\nT\nX\n1\n1\n1\n\u2217\n\u2217\nD\u03c8 (x , x(t))\nf (x(t + 1)) \u2212 f (x ) \u2264\nD\u03c8 (x , x(1)) +\n\u2212\n\u03b1(1)\n\u03b1(t) \u03b1(t \u2212 1)\nt=2\nt=1\n\nT\nX\n\n\u2217\n\n\u2212\n\nT\nX\n\n\u03b7(t)D\u03c8 (x(t + 1), x(t)) +\n\nT\nX\nt=1\n\nt=1\n\nhe(t), x(t + 1) \u2212 x\u2217 i .\n\n(27)\n\nComparing the bound (27) with the earlier bound for the dual averaging algorithms (22), we see\nthat the only essential difference is the \u03b1(t)\u22121 \u2212 \u03b1(t \u2212 1)\u22121 terms. The compactness assumption\nguarantees that D\u03c8 (x\u2217 , x(t)) \u2264 R2 , however, so\nT\nX\nt=2\n\n\u0014\n\n\u0015\n1\n1\nR2\nD\u03c8 (x , x(t))\n\u2212\n.\n\u2264\n\u03b1(t) \u03b1(t \u2212 1)\n\u03b1(T )\n\u2217\n\nThe remainder of the proof uses Lemmas 7 and 4 completely identically to the proof of Theorem 1.\n\n6.3\n\nProof of Corollary 1\n\nWe prove this result only for the mirror descent algorithm (10), as the proof for the dual-averagingbased algorithm (9) is similar. We define the error at time t to be e(t) = \u2207f (x(t)) \u2212 g(t \u2212 \u03c4 (t)), and\nobserve that we only need to control the second term involving e(t) in the bound (26) differently.\n\n19\n\n\fExpanding the error terms above and using Fenchel's inequality as in the proofs of Theorems 1\nand 2, we have\nhe(t), x(t + 1) \u2212 x\u2217 i\n\n\u2264 h\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 (t))), x(t + 1) \u2212 x\u2217 i + h\u2207f (x(t \u2212 \u03c4 (t))) \u2212 g(t \u2212 \u03c4 (t)), x(t) \u2212 x\u2217 i\n\u03b7(t)\n1\nk\u2207f (x(t \u2212 \u03c4 (t))) \u2212 g(t \u2212 \u03c4 (t))k2\u2217 +\nkx(t + 1) \u2212 x(t)k2 ,\n+\n2\u03b7(t)\n2\n\nNow we note that conditioned on the delay \u03c4 (t), we have\nE[kx(t \u2212 \u03c4 (t)) \u2212 x(t + 1)k2 | \u03c4 (t)] \u2264 G2 (\u03c4 (t) + 1)2 \u03b1(t \u2212 \u03c4 (t))2 .\nConsequently we apply Lemma 4 (specifically, following the bounds (19) and (20)) and find\nT\nX\nt=1\n\n\u2264\n\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 (t))), x(t + 1) \u2212 x\u2217 i\n\nT\nX\nt=1\n\n[Df (x\u2217 , x(t)) \u2212 Df (x\u2217 , x(t \u2212 \u03c4 (t)))] + G2\n\nT\nX\n(\u03c4 (t) + 1)2 \u03b1(t \u2212 \u03c4 (t))2 .\nt=1\n\nThe sum of Df terms telescopes, leaving only terms not received by the gradient procedure within\nT iterations, and we can use \u03b1(t) \u2264 \u03b7\u221a1 T for all t to derive the further bound\nX\n\nDf (x\u2217 , x(t)) +\n\nt:t+\u03c4 (t)>T\n\nT\nG2 X\n(\u03c4 (t) + 1)2 .\n\u03b7 2 T t=1\n\n(28)\n\nTo control the quantity (28), all we need is to bound the expected cardinality of the set {t \u2208\n[T ] : t + \u03c4 (t) > T }. Using Chebyshev's inequality and standard expectation bounds, we have\nE [card({t \u2208 [T ] : t + \u03c4 (t) > T })] =\n\nT\nX\nt=1\n\nP(t + \u03c4 (t) > T ) \u2264 1 +\n\nT\n\u22121\nX\nt=1\n\nE[\u03c4 (t)2 ]\n\u2264 1 + 2B 2 ,\n(T \u2212 t)2\n\nwhere the last inequality comes from our assumption that E[\u03c4 (t)2 ] \u2264 B 2 . As in Lemma 4, we have\nDf (x\u2217 , x(t)) \u2264 2GR, which yields\nE\n\n\u0014X\nT\nt=1\n\n\u2217\n\n\u0015\n\nh\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 (t))), x(t + 1) \u2212 x i \u2264 6GRB 2 +\n\nG2 (B + 1)2\n\u03b72\n\nWe can control the remaining terms as in the proofs of Theorems 1 and 2.\n\n7\n\nProof of Theorem 3\n\nThe proof of Theorem 3 is not too difficult given our previous work-all we need to do is redefine\nthe error e(t) and use \u03b7(t) to control the variance terms that arise. To that end, we define the\ngradient error terms that we must control. In this proof, we set\ne(t) := \u2207f (x(t)) \u2212\n20\n\nn\nX\ni=1\n\n\u03bbi gi (t \u2212 \u03c4 (i))\n\n(29)\n\n\fwhere gi (t) = \u2207f (x(t); \u03bei (t)) is the gradient of node i computed at the parameter x(t) and \u03c4 (i) is\nthe delay associated with node i.\nUsing Assumption B as in the proofs of previous theorems, then applying Lemma 5, we have\nL\nf (x(t + 1)) \u2212 f (x\u2217 ) \u2264 h\u2207f (x(t)), x(t + 1) \u2212 x\u2217 i + kx(t) \u2212 x(t + 1)k2\n2\n+\n* n\nX\nL\n\u03bbi gi (t \u2212 \u03c4 (i)), x(t + 1) \u2212 x\u2217 + he(t), x(t + 1) \u2212 x\u2217 i + kx(t) \u2212 x(t + 1)k2\n=\n2\ni=1\n\n= hz(t + 1), x(t + 1) \u2212 x\u2217 i \u2212 hz(t), x(t + 1) \u2212 x\u2217 i + he(t), x(t + 1) \u2212 x\u2217 i +\n\nL\nkx(t) \u2212 x(t + 1)k2\n2\n\n1\n1\n\u03c8(x(t + 1)) \u2212\n\u03c8(x(t))\n\u03b1(t)\n\u03b1(t)\n1\nL\n\u2212\nD\u03c8 (x(t + 1), x(t)) + he(t), x(t + 1) \u2212 x\u2217 i + kx(t) \u2212 x(t + 1)k2 .\n\u03b1(t)\n2\n\n\u2264 hz(t + 1), x(t + 1) \u2212 x\u2217 i \u2212 hz(t), x(t) \u2212 x\u2217 i +\n\nWe telescope as in the proofs of Theorems 1 and 2, canceling\ndivergence terms to see that\nT\nX\nt=1\n\nL\n2\n\nkx(t) \u2212 x(t + 1)k2 with the LD\u03c8\n\nf (x(t + 1)) \u2212 f (x\u2217 )\nT\n\nT\n\nX\nX\n1\n\u2264 hz(T + 1), x(T + 1) \u2212 x i +\nhe(t), x(t + 1) \u2212 x\u2217 i\n\u03b7(t)D\u03c8 (x(t + 1), x(t)) +\n\u03c8(x(T )) \u2212\n\u03b1(T )\nt=1\nt=1\n\u2217\n\nT\n\n\u2264\n\nT\n\nX\nX\n1\n\u03c8(x\u2217 ) \u2212\n\u03b7(t)D\u03c8 (x(t + 1), x(t)) +\nhe(t), x(t + 1) \u2212 x\u2217 i .\n\u03b1(T + 1)\nt=1\nt=1\n\n(30)\n\nThis is exactly as in the non-probabilistic bound (22) from the proof of Theorem 1, but the definition (29) of the error e(t) here is different.\nWhat remains is to control the error term in (30). Writing the terms out, we have\n+\n*\nn\nT\nT\nX\nX\nX\n\u03bbi \u2207f (x(t \u2212 \u03c4 (i))), x(t + 1) \u2212 x\u2217\n\u2207f (x(t)) \u2212\nhe(t), x(t + 1) \u2212 x\u2217 i =\nt=1\n\nt=1\n\n+\n\nT\nX\nt=1\n\n* n\nX\ni=1\n\ni=1\n\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))] , x(t + 1) \u2212 x\n\n\u2217\n\n+\n\n(31)\n\nBounding the first term above is simple via Lemma 4: as in the proof of Theorem 1 earlier, we have\n+\u0015\n*\n\u0014X\nn\nT\nX\n\u2217\n\u03bbi \u2207f (x(t \u2212 \u03c4 (i))), x(t + 1) \u2212 x\n\u2207f (x(t)) \u2212\nE\nt=1\n\n=\n\nn\nX\n\ni=1\n\n\u03bbi\n\nT\nX\n\nt=1\ni=1\nn\nX\n\n\u22642\n\nE[h\u2207f (x(t)) \u2212 \u2207f (x(t \u2212 \u03c4 (i))), x(t + 1) \u2212 x\u2217 i]\n\n\u03bbi LG2 (\u03c4 (i) + 1)2\n\ni=1\n\nT\nX\nt=1\n\nn\n\nX\n1\n\u03bbi 2\u03c4 (i)GR.\n+\n\u03b7(t \u2212 \u03c4 )2\n\n21\n\ni=1\n\n\fWe use the same technique as the proof of Theorem 1 to bound the second term from (31).\nIndeed, the Fenchel-Young inequality gives\n+\n* n\nX\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))] , x(t + 1) \u2212 x\u2217\ni=1\n\n=\n\n*\n\nn\nX\ni=1\n\n+\n\u2264\n\n*\n\nn\nX\ni=1\n\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))] , x(t) \u2212 x\n*\n\nn\nX\ni=1\n\n\u2217\n\n+\n\n+\n\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))] , x(t + 1) \u2212 x(t)\n\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))] , x(t) \u2212 x\u2217\n\nn\nX\n1\n+\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))]\n2\u03b7(t)\ni=1\n\n+\n2\n\n+\n\u2217\n\n\u03b7(t)\nkx(t + 1) \u2212 x(t)k2 .\n2\n\nBy assumption, given the information at worker i at time t \u2212 \u03c4 (i), gi (t \u2212 \u03c4 (i))) is independent of\nx(t), so the first term has zero expectation. More formally, this happens because x(t) is a function\nof gradients gi (1), . . . , gi (t \u2212 \u03c4 (i) \u2212 1) from each of the nodes i and hence the expectation of the first\nterm conditioned on {gi (1), . . . , gi (t \u2212 \u03c4 (i) \u2212 1)}ni=1 is 0. The last term is canceled by the Bregman\ndivergence terms in (30), so combining the bound (31) with the above two paragraphs yields\nT\nX\nt=1\n\nn\n\ni=1\n\n+\n\nT\nX\nt=1\n\n8\n\nT\n\nn\n\nt=1\n\ni=1\n\nX\nX\nX\n1\n1\n\u03bbi LG2 (\u03c4 (i) + 1)2\n\u03bbi 2\u03c4 (i)GR\n\u03c8(x\u2217 ) + 2\n+\nEf (x(t + 1)) \u2212 f (x ) \u2264\n\u03b1(t)\n\u03b7(t \u2212 \u03c4 )2\n\u2217\n\n1\nE\n2\u03b7(t)\n\nn\nX\ni=1\n\n2\n\n\u03bbi [\u2207f (x(t \u2212 \u03c4 (i))) \u2212 gi (t \u2212 \u03c4 (i))]\n\n.\n\u2217\n\nConclusion and Discussion\n\nIn this paper, we have studied dual averaging and mirror descent algorithms for smooth and nonsmooth stochastic optimization in delayed settings, showing applications of our results to distributed\noptimization. We showed that for smooth problems, we can preserve the performance benefits of\nparallelization over centralized stochastic optimization even when we relax synchronization requirements. Specifically, we presented methods that take advantage of distributed computational\nresources and are robust to node failures, communication latency, and node slowdowns. In addition, by distributing computation for stochastic optimization problems, we were able to exploit\nasynchronous processing without incurring any asymptotic penalty due to the delays incurred. In\naddition, though we omit these results for brevity, it is possible to extend all of our expected\nconvergence results to guarantees with high-probability.\n\nAcknowledgments\nIn performing this research, AA was supported by a Microsoft Research Fellowship, and JCD\nwas supported by the National Defense Science and Engineering Graduate Fellowship (NDSEG)\n22\n\n\fProgram. We are very grateful to Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao for\nilluminating conversations on distributed stochastic optimization and communication of their proof\nof the bound (8). We would also like to thank Yoram Singer for reading a draft of this manuscript\nand giving useful feedback.\n\nA\n\nTechnical Results about Proximal Functions\n\nIn this section, we collect several useful results about proximal functions and continuity properties\nof the solutions of proximal operators. We give proofs of all uncited results in Appendix B. We\nbegin with results useful for the dual-averaging updates (4) and (9).\nWe define the proximal dual function\n\u001b\n\u001a\n1\n\u2217\n(32)\n\u03c8\u03b1 (z) := sup h\u2212z, xi \u2212 \u03c8(x) .\n\u03b1\nx\u2208X\n\u2217 (z(t)). Further by\nSince \u2207\u03c8\u03b1\u2217 (z) = argmaxx\u2208X {h\u2212z, xi \u2212 \u03b1\u22121 \u03c8(x)}, it is clear that x(t) = \u2207\u03c8\u03b1(t)\n\u2217\nstrong convexity of \u03c8, we have that \u2207\u03c8\u03b1 (z) is \u03b1-Lipschitz continuous [Nes09, HUL96b, Chapter\nX], that is, for the norm k*k with respect to which \u03c8 is strongly convex and its associated dual\nnorm k*k\u2217 ,\nk\u2207\u03c8\u03b1\u2217 (y) \u2212 \u2207\u03c8\u03b1\u2217 (z)k \u2264 \u03b1 ky \u2212 zk\u2217 .\n(33)\n\nWe will find one more result about solutions to the dual averaging update useful. This result has\nessentially been proven in many contexts [Nes09, Tse08, DGBSX10a].\nLemma 5. Let x+ minimize hz, xi + A\u03c8(x) for all x \u2208 X . Then for any x \u2208 X ,\nhz, xi + A\u03c8(x) \u2265 z, x+ + A\u03c8(x+ ) + AD\u03c8 (x, x+ )\nNow we turn to describing properties of the mirror-descent step (5), which we will also use\nfrequently. The lemma allows us to bound differences between x(t) and x(t + 1) for the mirrordescent family of algorithms.\nLemma 6. Let x+ minimize hg, xi + \u03b11 D\u03c8 (x, y) over x \u2208 X . Then kx+ \u2212 yk \u2264 \u03b1 kgk\u2217 .\nThe last technical lemma we give explicitly bounds the differences between x(t) and x(t + \u03c4 ),\nfor some \u03c4 \u2265 1, by using the above continuity lemmas.\nLemma 7. Let Assumption A hold. Define x(t) via the dual-averaging updates (4), (9), or (12)\nor the mirror-descent updates (5), (10), or (13). Let \u03b1(t)\u22121 = L + \u03b7(t + t0 )c for some c \u2208 [0, 1],\n\u03b7 > 0, t0 \u2265 0, and L \u2265 0. Then for any fixed \u03c4 ,\nE[kx(t) \u2212 x(t + \u03c4 )k2 ] \u2264\n\nB\n\n4G2 \u03c4 2\n\u03b7 2 (t + t0 )2c\n\nand\n\nE[kx(t) \u2212 x(t + \u03c4 )k] \u2264\n\n2G\u03c4\n.\n\u03b7(t + t0 )c\n\nProofs of Proximal Operator Properties\n\nProof of Lemma 6\nThe inequality is clear when x+ = y, so assume that x+ 6= y. Since x+\n1\nminimizes hg, xi + \u03b1 D\u03c8 (x, y), the first order conditions for optimality imply\n\u03b1g + \u2207\u03c8(x+ ) \u2212 \u2207\u03c8(y), x \u2212 x+ \u2265 0\n23\n\n\ffor any x \u2208 X . Thus we can choose y = x and see that\n\n\u03b1 hg, y \u2212 xi \u2265 \u2207\u03c8(x+ ) \u2212 \u2207\u03c8(y), x+ \u2212 y \u2265 x+ \u2212 y\n\n2\n\n,\n\nwhere the last inequality follows from the strong convexity of \u03c8. Using H\u00f6lder's inequality gives\n2\nthat \u03b1 kgk\u2217 ky \u2212 xk \u2265 kx+ \u2212 yk , and dividing by ky \u2212 xk completes the proof.\nProof of Lemma 7\nWe first show the lemma for the dual-averaging updates. Recall that\n\u2217\nx(t) = \u2207\u03c8\u03b1(t) (z(t)) and \u2207\u03c8\u03b1\u2217 is \u03b1-Lipschitz continuous. Using the triangle inequality,\n\u2217\n\u2217\nkx(t) \u2212 x(t + \u03c4 )k = \u2207\u03c8\u03b1(t)\n(z(t)) \u2212 \u2207\u03c8\u03b1(t+\u03c4\n) (z(t + \u03c4 ))\n\u2217\n\u2217\n\u2217\n\u2217\n= \u2207\u03c8\u03b1(t)\n(z(t)) \u2212 \u2207\u03c8\u03b1(t+\u03c4\n) (z(t)) + \u2207\u03c8\u03b1(t+\u03c4 ) (z(t)) \u2212 \u2207\u03c8\u03b1(t+\u03c4 ) (z(t + \u03c4 ))\n\u2217\n\u2217\n\u2217\n\u2217\n\u2264 \u2207\u03c8\u03b1(t)\n(z(t)) \u2212 \u2207\u03c8\u03b1(t+\u03c4\n) (z(t)) + \u2207\u03c8\u03b1(t+\u03c4 ) (z(t)) \u2212 \u2207\u03c8\u03b1(t+\u03c4 ) (z(t + \u03c4 ))\n\n\u2264 (\u03b1(t) \u2212 \u03b1(t + \u03c4 )) kz(t)k\u2217 + \u03b1(t + \u03c4 ) kz(t) \u2212 z(t + \u03c4 )k\u2217 .\nIt is easy to check that for c \u2208 [0, 1],\n\u03b1(t) \u2212 \u03b1(t + \u03c4 ) \u2264\n\n(34)\n\nc\u03c4\nc\u03b7\u03c4\n\u2264 1+c .\nc\n2\n1\u2212c\n(L + \u03b7t ) t\n\u03b7t\n\nBy convexity of k*k2\u2217 , we can bound E[kz(t) \u2212 z(t + \u03c4 )k2\u2217 ]:\n\u0014 X\n\u0014 X\n\u03c4\n\u03c4 \u22121\n2\u0015\n2\u0015\n1\n1\n2\n2\n2\n=\u03c4 E\n\u2264 \u03c4 2 G2 ,\nz(t + s) \u2212 z(t + s \u2212 1)\ng(s)\nE[kz(t) \u2212 z(t + \u03c4 )k\u2217 ] = \u03c4 E\n\u03c4\n\u03c4\n\u2217\n\u2217\ns=1\n\ns=0\n\nsince E[k\u2202F (x; \u03be)k2\u2217 ] \u2264 G2 by assumption. Thus, bound (34) gives\n\nE[kx(t) \u2212 x(t + \u03c4 )k2 ] \u2264 2(\u03b1(t) \u2212 \u03b1(t + \u03c4 ))2 E[kz(t)k2\u2217 ] + 2\u03b1(t + \u03c4 )2 E[kz(t) \u2212 z(t + \u03c4 )k2\u2217 ]\n\u2264\n\n2c2 t2 \u03c4 2 G2\n2c2 \u03c4 2 G2\n2G2 \u03c4 2\n2 2\n2\n+\n2G\n\u03c4\n\u03b1(t\n+\n\u03c4\n)\n=\n+\n,\n\u03b7 2 t2+2c\n\u03b7 2 t2c\n(L + \u03b7(t + \u03c4 )c )2\n\nwhere we use Cauchy-Schwarz inequality in the first step. Since c \u2264 1, the last term is clearly\nbounded by 4G2 \u03c4 2 /\u03b7 2 t2c .\nTo get the slightly tighter bound on the first moment\u221ain the statement of the lemma, simply\nuse the triangle inequality from the bound (34) and that EX 2 \u2265 E|X|.\nThe proof for the mirror-descent family of updates is similar. We focus on non-delayed update (5), as the other updates simply modify the indexing of g(t + s) below. We know from\nLemma 6 and the triangle inequality that\nkx(t) \u2212 x(t + \u03c4 )k \u2264\n\n\u03c4\nX\ns=1\n\nkx(t + s) \u2212 x(t + s \u2212 1)k \u2264\n\n\u03c4\nX\ns=1\n\n\u03b1(t + s \u2212 1) kg(t + s)k\u2217\n\nSquaring the above bound, taking expectations, and recalling that \u03b1(t) is non-increasing, we see\n2\n\nE[kx(t) \u2212 x(t + \u03c4 )k ] \u2264\n\n\u03c4 X\n\u03c4\nX\ns=1 r=1\n2\n\n\u03b1(t + s)\u03b1(t + r)E[kg(t + s)k\u2217 kg(t + r)k\u2217 ]\n\n2\n\n\u2264 \u03c4 \u03b1(t) max\nr,s\n\nq\n\nE[kg(t +\n24\n\nq\n\ns)k2\u2217 ]\n\nE[kg(t + r)k2\u2217 ] \u2264 \u03c4 2 \u03b1(t)2 G2\n\n\fby H\u00f6lder's inequality. Substituting the appropriate value for \u03b1(t) completes the proof.\n\nC\n\nError in [LSZ09]\n\nLangford et al. [LSZ09], in Lemma 1 of their paper, state an upper bound on hg(t \u2212 \u03c4 ), x(t \u2212 \u03c4 ) \u2212 x\u2217 i\nthat is essential to the proofs of all of their results. However, the lemma only holds as an equality\nfor unconstrained optimization (i.e. when the set X = Rd ); in the presence of constraints, it fails\nto hold (even as an upper bound). To see why, we consider a simple one-dimensional example with\nX = [\u22121, 1], f (x) = |x|, \u03b7 \u2261 2 and we evaluate both sides of their lemma with \u03c4 = 1 and t = 2.\nThe left hand side of their bound evaluates to 1, while the right hand side is \u22121, and the inequality\nclaimed in the lemma fails. The proofs of their main theorems rely on the application of their\nLemma 1 with equality, restricting those results only to unconstrained optimization. However, the\nresults also require boundedness of the gradients g(t) over all of X as well as boundedness of the\ndistance between the iterates x(t). Few convex functions have bounded gradients over all of Rd ;\nand without constraints the iterates x(t) are seldom bounded for all iterations t.\n\nReferences\n[ABRW10]\n\nA. Agarwal, P. Bartlett, P. Ravikumar, and M. Wainwright, Information-theoretic\nlower bounds on the oracle complexity of convex optimization, Submitted to IEEE\nTransactions on Information Theory, URL http://arxiv.org/abs/1009.0571, 2010.\n\n[Ber73]\n\nD. P. Bertsekas, Stochastic optimization problems with nondifferentiable cost functionals, Journal of Optimization Theory and Applications 12 (1973), no. 2, 218\u2013231.\n\n[Bre67]\n\nL. M. Bregman, The relaxation method of finding the common point of convex sets\nand its application to the solution of problems in convex programming, USSR Computational Mathematics and Mathematical Physics 7 (1967), 200\u2013217.\n\n[BT89]\n\nD. P. Bertsekas and J. N. Tsitsiklis, Parallel and distributed computation: numerical\nmethods, Prentice-Hall, Inc., 1989.\n\n[BT03]\n\nA. Beck and M. Teboulle, Mirror descent and nonlinear projected subgradient methods\nfor convex optimization, Operations Research Letters 31 (2003), 167\u2013175.\n\n[DAW10]\n\nJ. Duchi, A. Agarwal, and M. Wainwright, Dual averaging for distributed optimization:\nconvergence analysis and network scaling, URL\nhttp://arxiv.org/abs/1005.2012, 2010.\n\n[DGBSX10a] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao, Optimal distributed online\nprediction using mini-batches, URL http://arxiv.org/abs/1012.1367, 2010.\n, Robust distributed online prediction, URL http://arxiv.org/abs/1012.1370,\n\n[DGBSX10b]\n2010.\n[HTF01]\n\nT. Hastie, R. Tibshirani, and J. Friedman, The elements of statistical learning,\nSpringer, 2001.\n25\n\n\f[HUL96a]\n[HUL96b]\n\nJ. Hiriart-Urruty and C. Lemar\u00e9chal, Convex Analysis and Minimization Algorithms\nI, Springer, 1996.\n, Convex Analysis and Minimization Algorithms II, Springer, 1996.\n\n[JNT08]\n\nA. Juditsky, A. Nemirovski, and C. Tauvel, Solving variational inequalities with the\nstochastic mirror-prox algorithm, URL http://arxiv.org/abs/0809.0815, 2008.\n\n[JRJ09]\n\nB. Johansson, M. Rabi, and M. Johansson, A randomized incremental subgradient\nmethod for distributed optimization in networked systems, SIAM Journal on Optimization 20 (2009), no. 3, 1157\u20131170.\n\n[Lan10]\n\nG. Lan, An optimal method for stochastic composite optimization, Mathematical Programming Series A (2010), Online first, to appear. URL\nhttp://www.ise.ufl.edu/glan/papers/OPT SA4.pdf.\n\n[LMP01]\n\nJ. Lafferty, A. McCallum, and F. Pereira, Conditional random fields: Probabilistic\nmodels for segmenting and labeling sequence data, Proceedings of the Eightneenth\nInternational Conference on Machine Learning, 2001, pp. 282\u2013289.\n\n[LSZ09]\n\nJ. Langford, A. Smola, and M. Zinkevich, Slow learners are fast, Advances in Neural\nInformation Processing Systems 22, 2009, pp. 2331\u20132339.\n\n[LYRL04]\n\nD. Lewis, Y. Yang, T. Rose, and F. Li, RCV1: A new benchmark collection for text\ncategorization research, Journal of Machine Learning Research 5 (2004), 361\u2013397.\n\n[NBB01]\n\nA. Nedi\u0107, D.P. Bertsekas, and V.S. Borkar, Distributed asynchronous incremental\nsubgradient methods, Inherently Parallel Algorithms in Feasibility and Optimization\nand their Applications (D. Butnariu, Y. Censor, and S. Reich, eds.), Studies in\nComputational Mathematics, vol. 8, Elsevier, 2001, pp. 381\u2013407.\n\n[Nes09]\n\nY. Nesterov, Primal-dual subgradient methods for convex problems, Mathematical\nProgramming A 120 (2009), no. 1, 261\u2013283.\n\n[NJLS09]\n\nA. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, Robust stochastic approximation\napproach to stochastic programming, SIAM Journal on Optimization 19 (2009), no. 4,\n1574\u20131609.\n\n[NO09]\n\nA. Nedi\u0107 and A. Ozdaglar, Distributed subgradient methods for multi-agent optimization, IEEE Transactions on Automatic Control 54 (2009), 48\u201361.\n\n[NY83]\n\nA. Nemirovski and D. Yudin, Problem complexity and method efficiency in optimization, Wiley, New York, 1983.\n\n[Pol87]\n\nB. T. Polyak, Introduction to optimization, Optimization Software, Inc., 1987.\n\n[RM51]\n\nH. Robbins and S. Monro, A stochastic approximation method, Annals of Mathematical Statistics 22 (1951), 400\u2013407.\n\n[RNV10]\n\nS. S. Ram, A. Nedi\u0107, and V. V. Veeravalli, Distributed stochastic subgradient projection algorithms for convex optimization, Journal of Optimization Theory and Applications 147 (2010), no. 3, 516\u2013545.\n26\n\n\f[Tse08]\n\nP. Tseng, On accelerated proximal gradient methods for convex-concave optimization,\nTech. report, Department of Mathematics, University of Washington, 2008.\n\n[Tsi84]\n\nJ. Tsitsiklis, Problems in decentralized decision making and computation, Ph.D. thesis, Massachusetts Institute of Technology, 1984.\n\n[Xia10]\n\nL. Xiao, Dual averaging methods for regularized stochastic learning and online optimization, Journal of Machine Learning Research 11 (2010), 2543\u20132596.\n\n27\n\n\f"}