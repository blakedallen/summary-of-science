{"id": "http://arxiv.org/abs/cond-mat/0201139v1", "guidislink": true, "updated": "2002-01-09T18:55:22Z", "updated_parsed": [2002, 1, 9, 18, 55, 22, 2, 9, 0], "published": "2002-01-09T18:55:22Z", "published_parsed": [2002, 1, 9, 18, 55, 22, 2, 9, 0], "title": "Long-range fractal correlations in literary corpora", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0201237%2Ccond-mat%2F0201028%2Ccond-mat%2F0201003%2Ccond-mat%2F0201358%2Ccond-mat%2F0201567%2Ccond-mat%2F0201280%2Ccond-mat%2F0201491%2Ccond-mat%2F0201268%2Ccond-mat%2F0201582%2Ccond-mat%2F0201079%2Ccond-mat%2F0201315%2Ccond-mat%2F0201319%2Ccond-mat%2F0201230%2Ccond-mat%2F0201262%2Ccond-mat%2F0201057%2Ccond-mat%2F0201475%2Ccond-mat%2F0201222%2Ccond-mat%2F0201305%2Ccond-mat%2F0201157%2Ccond-mat%2F0201471%2Ccond-mat%2F0201011%2Ccond-mat%2F0201408%2Ccond-mat%2F0201479%2Ccond-mat%2F0201518%2Ccond-mat%2F0201066%2Ccond-mat%2F0201407%2Ccond-mat%2F0201189%2Ccond-mat%2F0201267%2Ccond-mat%2F0201200%2Ccond-mat%2F0201530%2Ccond-mat%2F0201094%2Ccond-mat%2F0201257%2Ccond-mat%2F0201438%2Ccond-mat%2F0201421%2Ccond-mat%2F0201499%2Ccond-mat%2F0201372%2Ccond-mat%2F0201446%2Ccond-mat%2F0201183%2Ccond-mat%2F0201483%2Ccond-mat%2F0201162%2Ccond-mat%2F0201528%2Ccond-mat%2F0201332%2Ccond-mat%2F0201501%2Ccond-mat%2F0201363%2Ccond-mat%2F0201106%2Ccond-mat%2F0201550%2Ccond-mat%2F0201089%2Ccond-mat%2F0201077%2Ccond-mat%2F0201151%2Ccond-mat%2F0201176%2Ccond-mat%2F0201524%2Ccond-mat%2F0201148%2Ccond-mat%2F0201575%2Ccond-mat%2F0201413%2Ccond-mat%2F0201352%2Ccond-mat%2F0201284%2Ccond-mat%2F0201448%2Ccond-mat%2F0201404%2Ccond-mat%2F0201213%2Ccond-mat%2F0201329%2Ccond-mat%2F0201195%2Ccond-mat%2F0201206%2Ccond-mat%2F0201357%2Ccond-mat%2F0201169%2Ccond-mat%2F0201225%2Ccond-mat%2F0201412%2Ccond-mat%2F0201004%2Ccond-mat%2F0201347%2Ccond-mat%2F0201069%2Ccond-mat%2F0201175%2Ccond-mat%2F0201493%2Ccond-mat%2F0201416%2Ccond-mat%2F0201462%2Ccond-mat%2F0201046%2Ccond-mat%2F0201245%2Ccond-mat%2F0201300%2Ccond-mat%2F0201051%2Ccond-mat%2F0201422%2Ccond-mat%2F0201279%2Ccond-mat%2F0201067%2Ccond-mat%2F0201377%2Ccond-mat%2F0201187%2Ccond-mat%2F0201134%2Ccond-mat%2F0201111%2Ccond-mat%2F0201231%2Ccond-mat%2F0201509%2Ccond-mat%2F0201293%2Ccond-mat%2F0201061%2Ccond-mat%2F0201058%2Ccond-mat%2F0201587%2Ccond-mat%2F0201533%2Ccond-mat%2F0201234%2Ccond-mat%2F0201020%2Ccond-mat%2F0201426%2Ccond-mat%2F0201348%2Ccond-mat%2F0201375%2Ccond-mat%2F0201009%2Ccond-mat%2F0201073%2Ccond-mat%2F0201191%2Ccond-mat%2F0201139%2Ccond-mat%2F0201591&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Long-range fractal correlations in literary corpora"}, "summary": "In this paper we analyse the fractal structure of long human-language records\nby mapping large samples of texts onto time series. The particular mapping set\nup in this work is inspired on linguistic basis in the sense that is retains\n{\\em the word} as the fundamental unit of communication. The results confirm\nthat beyond the short-range correlations resulting from syntactic rules acting\nat sentence level, long-range structures emerge in large written language\nsamples that give rise to long-range correlations in the use of words.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0201237%2Ccond-mat%2F0201028%2Ccond-mat%2F0201003%2Ccond-mat%2F0201358%2Ccond-mat%2F0201567%2Ccond-mat%2F0201280%2Ccond-mat%2F0201491%2Ccond-mat%2F0201268%2Ccond-mat%2F0201582%2Ccond-mat%2F0201079%2Ccond-mat%2F0201315%2Ccond-mat%2F0201319%2Ccond-mat%2F0201230%2Ccond-mat%2F0201262%2Ccond-mat%2F0201057%2Ccond-mat%2F0201475%2Ccond-mat%2F0201222%2Ccond-mat%2F0201305%2Ccond-mat%2F0201157%2Ccond-mat%2F0201471%2Ccond-mat%2F0201011%2Ccond-mat%2F0201408%2Ccond-mat%2F0201479%2Ccond-mat%2F0201518%2Ccond-mat%2F0201066%2Ccond-mat%2F0201407%2Ccond-mat%2F0201189%2Ccond-mat%2F0201267%2Ccond-mat%2F0201200%2Ccond-mat%2F0201530%2Ccond-mat%2F0201094%2Ccond-mat%2F0201257%2Ccond-mat%2F0201438%2Ccond-mat%2F0201421%2Ccond-mat%2F0201499%2Ccond-mat%2F0201372%2Ccond-mat%2F0201446%2Ccond-mat%2F0201183%2Ccond-mat%2F0201483%2Ccond-mat%2F0201162%2Ccond-mat%2F0201528%2Ccond-mat%2F0201332%2Ccond-mat%2F0201501%2Ccond-mat%2F0201363%2Ccond-mat%2F0201106%2Ccond-mat%2F0201550%2Ccond-mat%2F0201089%2Ccond-mat%2F0201077%2Ccond-mat%2F0201151%2Ccond-mat%2F0201176%2Ccond-mat%2F0201524%2Ccond-mat%2F0201148%2Ccond-mat%2F0201575%2Ccond-mat%2F0201413%2Ccond-mat%2F0201352%2Ccond-mat%2F0201284%2Ccond-mat%2F0201448%2Ccond-mat%2F0201404%2Ccond-mat%2F0201213%2Ccond-mat%2F0201329%2Ccond-mat%2F0201195%2Ccond-mat%2F0201206%2Ccond-mat%2F0201357%2Ccond-mat%2F0201169%2Ccond-mat%2F0201225%2Ccond-mat%2F0201412%2Ccond-mat%2F0201004%2Ccond-mat%2F0201347%2Ccond-mat%2F0201069%2Ccond-mat%2F0201175%2Ccond-mat%2F0201493%2Ccond-mat%2F0201416%2Ccond-mat%2F0201462%2Ccond-mat%2F0201046%2Ccond-mat%2F0201245%2Ccond-mat%2F0201300%2Ccond-mat%2F0201051%2Ccond-mat%2F0201422%2Ccond-mat%2F0201279%2Ccond-mat%2F0201067%2Ccond-mat%2F0201377%2Ccond-mat%2F0201187%2Ccond-mat%2F0201134%2Ccond-mat%2F0201111%2Ccond-mat%2F0201231%2Ccond-mat%2F0201509%2Ccond-mat%2F0201293%2Ccond-mat%2F0201061%2Ccond-mat%2F0201058%2Ccond-mat%2F0201587%2Ccond-mat%2F0201533%2Ccond-mat%2F0201234%2Ccond-mat%2F0201020%2Ccond-mat%2F0201426%2Ccond-mat%2F0201348%2Ccond-mat%2F0201375%2Ccond-mat%2F0201009%2Ccond-mat%2F0201073%2Ccond-mat%2F0201191%2Ccond-mat%2F0201139%2Ccond-mat%2F0201591&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper we analyse the fractal structure of long human-language records\nby mapping large samples of texts onto time series. The particular mapping set\nup in this work is inspired on linguistic basis in the sense that is retains\n{\\em the word} as the fundamental unit of communication. The results confirm\nthat beyond the short-range correlations resulting from syntactic rules acting\nat sentence level, long-range structures emerge in large written language\nsamples that give rise to long-range correlations in the use of words."}, "authors": ["Marcelo A. Montemurro", "Pedro A. Pury"], "author_detail": {"name": "Pedro A. Pury"}, "author": "Pedro A. Pury", "arxiv_comment": "to appear in Fractals", "links": [{"href": "http://arxiv.org/abs/cond-mat/0201139v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cond-mat/0201139v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "nlin.AO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cond-mat/0201139v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cond-mat/0201139v1", "journal_reference": "Fractals 10(4), 451-461 (2002)", "doi": null, "fulltext": "Long-range fractal correlations in literary corpora\n\narXiv:cond-mat/0201139v1 [cond-mat.stat-mech] 9 Jan 2002\n\nMarcelo A. Montemurro\u2217 and Pedro A. Pury\u2020\nFacultad de Matem\u00e1tica, Astronom\u0131\u0301a y F\u0131\u0301sica\nUniversidad Nacional de C\u00f3rdoba\nCiudad Universitaria, 5000 C\u00f3rdoba, Argentina\n(October 18, 2018)\nIn this paper we analyse the fractal structure of long human-language records by mapping large\nsamples of texts onto time series. The particular mapping set up in this work is inspired on linguistic\nbasis in the sense that is retains the word as the fundamental unit of communication. The results\nconfirm that beyond the short-range correlations resulting from syntactic rules acting at sentence\nlevel, long-range structures emerge in large written language samples that give rise to long-range\ncorrelations in the use of words.\n\nII. THE MAPPING OF TEXTS ONTO TIME\nSERIES\n\nI. INTRODUCTION\n\nThe human language faculty reveals as a phenomenon\nof remarkable complexity, intimately interwoven with all\nour superior mental functions [1]. Its role in the modelling and categorisation of factual experience by the\nmind cannot be downplayed. Moreover, it has been suggested that the outburst of symbolic and abstract imagery pervading the archeological records signalling the\ntransition between the middle and upper Paleolithic periods, might be interpreted as the hallmark of an associated, and rather sudden, transition in the linguistic performance of modern humans [2,3]. The evolutionary advantage of complex syntactic communication stems from\nthe capacity that it confers to language of coding complex\ninformation using moderate symbolic resources [4]. In\nthe traditional areas of linguistics much success has been\nattained in dissecting language structure from sentence\nlevel down the morphological rules of word formation.\nHowever, effective linguistic communication is a complex\ncognitive process [5] where sequences of sentences cohere\nin the assemblage of larger meaningful structures. At\nthis point the study of language becomes susceptible of\nbeing complemented by using techniques that have been\nsuccessfully applied on other complex systems in nature\nthat also allow a variety of interdisciplinary approaches.\nIn particular, one question that still remains open to\ndiscussion is the ultimate origin of long-range correlations\nin complex information systems like the genetic code or\nhuman language. The main goal of this work is to examine the existence of long-range correlations in the use\nof words in written language drawing on methods from\nstatistical time series analysis. To that end, we first describe the particular mapping scheme used to translate\nthe given sequence of words present in a text sample into\na time series. Then, we devote a section to explain the\nrescaled range analysis and how it can be used to infer\nthe presence of long-range correlations in time series. Finally, the results of our systematic analysis are presented\nfollowed by concluding remarks.\n\nThe statistical analysis of long symbolic structures had\nan accelerated development right after the first complete\nDNA sequences started to become available, around a\ndecade ago [6]. Basically, the original techniques tried\nto create a random walk out of the particular symbolic\nsequence by mapping each distinct symbol onto a step\nin an independent direction in space. Thus, in a genetic sequence, made up of the concatenation of four independent nucleotides, represented by four letters, each\ninstance of these symbols was translated onto an elementary jump along independent directions in a four dimensional space. Although different alternatives for the particular details of the mappings were put forward, all were\ndevised to keep the original sequence's structure at symbol level translated into the time series. Then, by applying a set of standard statistical techniques, such as\npower spectrum analysis, detrended fluctuation analysis\nor rescaled range analysis among others, it was possible to unravel the existence of long-range correlations in\nbiosequences [6,7].\nWritten samples of natural languages, are also complex information carriers to be read sequentially. Thus,\nsimilar techniques as those employed in genetic sequence analysis were adapted with appropriate modifications [8\u201310]. The different procedures shared one common point, namely, that the mappings were invariably\nperformed at letter level. However, it has never been\ndiscussed in the literature so far whether the mapping\nat letter level is indeed adequate in the case of human\nlanguage. It is crucial to note that the specific coding\nof words in some particular spelling or phonetic system\nis alien to the linguistic structure of communication, as\nis clear by noting that a given text can be readily coded\nin sign languages, for instance. Therefore, a better mapping, founded on linguistics basis, should recognise the\nsymbols at the minimum level intrinsic to the communication process. In what follows we propose the simplest\n1\n\n\fstep in that direction, which consists in taking word tokens as the fundamental units of communication. By\ndoing this we can assert that the mapped sequences do\nnot carry any structure below word level.\nIn order to make clear the details of the actual translation of a given text onto a time series, a brief detour\naround Zipf's analysis is required [11]. It basically consists in counting the number of occurrences of each distinct word in a given text sample, and then producing\na list of all these words ordered by decreasing frequency.\nEach word in this list can be identified by an index equal\nto its rank, r, in the list, that is, the most frequent word\nhas index r = 1, the second in the list is given r = 2, and\nso on. Words that occur the same number of times are\nordered arbitrarily in their corresponding rank interval.\nBy means of Zipf's analysis each word in the original text can be replaced by its corresponding index r.\nThen, at position t starting from the beginning of the\ntext, we have the corresponding index r(t). According\nto this mapping, the whole text becomes a time series\nof ranks, namely {r(t)}Tt=1 , where T stands as the total\nlength of the text. This particular numerical assignment\nmay seem arbitrary at first, and in fact a different choice\nwould certainly work as well. However, the selection of\nthe rank as an indexing key to the words may be rendered more natural if we think of it as the assignment\nthat minimises the effort in lexical access in the rankordered list of words in writing the whole text. When a\nword is required at position t in the text, then it must be\npicked from position r(t) in the list of words ordered by\ndecreasing frequency.\nIn order to compare different time series generated\nby this mapping, it is useful to rescale the series to\nzero mean and unit variance. Thence, we define the\nstandard\nq deviation of the rank sample values as follows\n\nLet \u03be(t), 1 \u2264 t \u2264 T , be the normalised sequence of\nincrements of a process in discrete time. In our case, \u03be(t)\nis the normalised ordered sequence of ranks from a text\ncorpus of T words, as described in the previuos section.\nFrom this sequence, the record\nX(t) =\n\n(3.1)\n\nis constructed. Viewing the \u03be(t) as spatial increments\nin a one-dimensional discrete random walk, X(t) is the\nposition of the walker from the starting point at time t.\nFor any given integer span s > 1 and any initial time t,\na detrended subrecord D(u, t, s), for 0 \u2264 u \u2264 s, can be\ndefined as\nD(u, t, s) = X(t + u) \u2212 X(t) \u2212\n\nu\n(X(t + s) \u2212 X(t)) .\ns\n(3.2)\n\nPs\nIn this quantity, the mean\nw=1 \u03be(t + w)/s was substracted to remove the trend in the subrecord. The cumulated range R(t, s) of the subrecord is defined by\nR(t, s) = max D(u, t, s) \u2212 min D(u, t, s)\n0\u2264u\u2264s\n\n0\u2264u\u2264s\n\n(3.3)\n\nand the variance S 2 (t, s) of the subrecord is defined by\ns\n1X 2\nS 2 (t, s) =\n\u03be (t + w) \u2212\ns w=1\n\n!2\ns\n1X\n\u03be(t + w)\n. (3.4)\ns w=1\n\nFor many time series of natural phenomena the average of the sample values of R(t, s)/S(t, s), carried over\nall admissible starting points t within the sample, follows\nthe Hurst's law: E[R(t, s)/S(t, s)] \u223c sH with H > 1/2.\nHurst's observation is remarkable considering the fact\nthat in the absence of long-run statistical dependence one\nshould find H = 1/2, for processes with finite variance.\nFor example, for a stationary Gaussian process \u03be(t) with\nh\u03be(t)i = 0 and \u03be 2 (t) = 1, Feller [16] has analytically\nproved that\nr\n\u03c0\n\u22121/2\n.\n(3.5)\nlim s\nE[R(t, s)/S(t, s)] =\ns\u2192\u221e\n2\n\n2\n\nr(t) \u2212 hr(t)i\n.\n\u03c3\u0302\n\n\u03be(u)\n\nu=1\n\n\u03c3\u0302 = hr(t)2 i \u2212 hr(t)i , where the symbol h. . .i denotes\nan arithmetic average over the whole series of ranks.\nThen, the quantity \u03be(t) is defined as\n\u03be(t) =\n\nt\nX\n\n(2.1)\n\nIn this way, the sequence of normalised increments \u03be(t)\ncan now be regarded as the sequence of elementary jumps\nin a random walk each taken at a discrete time t.\n\nAdditionally, Mandelbrot and Wallis [15] showed that the\ns1/2 law also applies to processes of independent increments having a variety of distributions: truncated Gaussian, hyperbolic, and (highly skewed) log-normal. Moreover, they also showed that when the increments of the\nprocess are statistical dependent but the dependence is\nlimited to the short run, the s1/2 law holds asymptotically. The effect of strong cyclic components was also\nstudied. When a white Gaussian noise (of zero mean\nand unit variance) is superimposed with a purely periodic process of amplitude A, it can be seen that\n\nIII. RESCALED RANGE ANALYSIS\n\nHydrology is the oldest discipline in which the presence of noncyclic very long-term dependence has been\nreported. Particularly, the rescaled range analysis was\nintroduced by Harold E. Hurst [12] when he was studying\nthe Nile in order to describe the long-term dependence of\nwater levels in the river and reservoirs. Later, this statistical technique was further developed and applied by\nMandelbrot and Wallis [13\u201315].\n\n2\n\n\flim s\n\ns\u2192\u221e\n\n\u22121/2\n\nE[R(t, s)/S(t, s)] =\n\nr\n\n\u03c0\n2\n\n\u0012\n\nA\n1+\n2\n\n\u0013\u22121/2\n\nreckon 2p values of log[R(t, s)/S(t, s)] corresponding to\nthe different starting points t, which are plotted as ordinates (marked by + signs). For each s, the logarithm of\nthe sample average of the quantities R(t, s)/S(t, s) over\nthe different starting points is also marked in the R/S\ndiagram. Then, the value of H is estimated by linear\nregression of log E[R(t, s)/S(t, s)] vs. log s. The slope is\ncalculated using the linear least-square method and the\nerror is evaluated taking the uncertainty in the ordinates,\nassociated with each value of the abscissa (log s), equal to\none quarter of the amplitude between the corresponding\nextreme points.\nThere are two pitfalls related to the R/S diagrams.\nFirst, for small values of log s (short subrecords) there is\nlarge scattering in the values of log[R(t, s)/S(t, s)]. Second, for large values of log s we have very few nonoverlapping subrecords, thus narrowing the R/S diagram. This\ntightening involves a deceiving evaluation of uncertainty\nin the ordinate. Hence, when fitting a straight trendline,\nsmall and large values of log s should be neglected. Given\na value of p we have 2p subsamples each of which have\nT /2p \u2265 2m\u2212p ranks. Therefore, we will retain the values\nof the span s given by 4 \u2264 p \u2264 m \u2212 4 as our criterion\nfor fitting. In this manner, we will only consider average\nover 16 or more samples and each subrecord will have at\nleast 16 ranks.\n\n.\n(3.6)\n\nThe value of the limit as well as the speed with which this\nlimit is attained is dependent on A. When A increases,\nE[R(t, s)/S(t, s)] takes an even longer time to reach the\ns1/2 law. Moreover, the transition to the asymptote is\nnon monotonic, but typically exhibits a series of oscillations.\nThe above comments support the key idea that\ns1/2 law holds for every process for which long-term dependence is unquestionably absent and does not hold for\nprocesses exhibiting noncyclic long-term statistical dependence. Thus, a Hurst exponent of H = 1/2 corresponds to the vanishing of correlations between past and\nfuture spatial increments in the record. For H > 1/2 one\nhas persistent behaviour, which means a positive increment in the past will on the average lead to a positive\nincrement in the future. Conversely a decreasing trend\nin the past implies on the average a sustained decrease in\nthe future. Correspondingly, the case H < 1/2 denotes\nantipersistent behaviour.\nIn all the above discussion the focus was on the scaling\nproperties of the ratio R(t, s)/S(t, s). It is important to\nnote that the introduction of the denominator S(t, s) becomes ever more relevant for processes that deviate from\nthe Gaussian and/or have long-term dependence [15].\nFuthermore, the ratio R(t, s)/S(t, s) has a better sampling stability in\np the sense that the relative deviation, defined by V ar[R(t, s)/S(t, s)]/E[R(t, s)/S(t, s)],\nis smaller than any alternative expression used to study\nlong-term dependence.\n\nA. Results from literary corpora\n\nIn Fig. 1 we show the R/S diagram corresponding to\nthe coded sequence consisting in 885534 ranks from 36\nplays by William Shakespeare. The total number of different ranks, associated with different words in the corpus, is in this case equal to 23150. The mean and standard deviation (\u03c3\u0302) of the original sequence and the third\n(M3) and fourth (M4) moments of the normalised sequence are shown in an inset. For simplicity's sake, we\nonly mark in the diagrams the minimum and maximum\npoints for a given log s, and the corresponding sample average is plotted as a small circle. The trendline is drawn\nas a solid line along the points effectively used in the fitting. The measured value of H and its error are displayed\nin Table I, and confirm the presence of long-range correlation in the series. In the same figure, we can also see as\nsmall squares the diagram for the sample average from\nthe sequence after deleting all ranks outside the interval\n(100, 2000). Strikingly, the corresponding value of H is\nstatistically indistinguishable from that of the original sequence. This fact tells us that the core of long-range correlation is neither supported by the most frequent words\nnor the least used.\nThere are two additional experiments which can provide information in tracking down the source of the correlated behaviour. The first one is a simple random shuffling of all the ranks in the sequence which has the effect of recasting Shakespeare's plays into a nonsensical\n\nIV. ESTIMATION OF H IN LITERARY\nCORPORA\n\nIn this section we shall explain the implementation of\nthe rescaled range analysis, as well as present the results\nobtained after performing the experiments on texts coded\nas a normalised sequences of ranks.\nAs it was stated in the previous section, the method\nis based on the estimation of sample averages of the ratio R(t, s)/S(t, s), for proper choice of different values\nof the integer span s. We shall follow the nomenclature\nand prescriptions introduced by Mandelbrot and Wallis [14] in the construction of the R/S diagrams from the\nexperimental data. We first compute the exponent m\nsuch that 2m \u2264 T < 2m+1 , and then we select the value\nof the span s from the decreasing sequence of integers:\n{T /2p , p = 0, 1, . . . , m \u2212 2}. For each s, we choose the\nstarting points: t = q s + 1, q = 0, 1, . . . , 2p \u2212 1 in order to construct 2p nonoverlapping detrended subrecords\nD(u, t, s) from the Eq. (3.2). Thus, for a given value\nof the index p, we have specified a value of log s which\nis marked on the axis of abscissas. In this way, we can\n3\n\n\fAt this point it may be illustrative to compare a plot\nof the record X(t), the position of the walker as a function of time, both for the Shakespeare's plays and for a\nstochastic sequence generated with H = 0.7 by the successive random addition method [17]. As can be seen in\nFigs. 2 and 3 both records show strong persistence that\nmanifests itself in the long spans of average monotonous\nbehaviour in the records.\n\nrealisation, keeping the same original words without discernible order at any level. As is clear, after this experiment the analysis must yield a Hurst exponent indicating\nthe uncorrelated character of the sequence. This is corroborated also in Fig. 1 where the sample average is plotted as small triangles. Yet more interesting is the analysis\nof a shuffling of Shakespeare's plays that preserves sentence structure, and therefore English grammar. That is,\nby defining a sentence as the sequence of words between\ntwo periods, we can reorder them in a random fashion\nand thus produce a grammatically correct, though hardly\nmeaningful, version of the corpus. The sample averages\nare plotted in this case as small diamonds. The resulting value of H shows that grammar is not sufficient to\ninduce long-range correlations as we can see again from\nTable I. Let us note that for maintaining the readability\nof the graph we only included the extreme R/S points of\nthe original sequence.\n\nFIG. 2. Record of the coded sequence from the Shakespeare\ncorpus.\n\nFIG. 1. R/S diagram corresponding to the Shakespeare\ncorpus. The linear fits are showed only for the original sequence and the shuffling experiments.\n\nsource\nShakespeare a\nDickens b\nDarwin c\nSimon's model d\nMarkovian text e\n\noriginal sequence\n0.687 \u00b1 0.040\n0.738 \u00b1 0.033\n0.745 \u00b1 0.045\n0.550 \u00b1 0.040\n0.533 \u00b1 0.028\n\ntruncation f\n0.658 \u00b1 0.036\n0.660 \u00b1 0.034\n0.678 \u00b1 0.043\n0.519 \u00b1 0.032\n\nsentences shuffled\n0.574 \u00b1 0.035\n0.573 \u00b1 0.025\n0.576 \u00b1 0.033\n\nranks shuffled\n0.524 \u00b1 0.020\n0.520 \u00b1 0.021\n\ng\n\nTABLE I. Values of H from the estimation by linear regression of log E [R/S] vs. log s.\n36 plays: 885534 words\nb\n56 books: 5616403 words\nc\n11 books: 1508483 words\nd\n5 \u00d7 106 words generated after a transient and deleting the ranks \u2264 5\ne\n1.2 \u00d7 106 words generated from table of frequencies corresponding to the Shakespeare corpus with memory of 7 letters\nf\nUnless other specification all ranks outside the interval 100 < rank < 2000 were deleted from the coded sequence\ng\nIn this case the ranks outside the interval 5 < rank < 10000 were deleted\na\n\n4\n\n\fFIG. 3. Record of a fractional brownian motion generated\nwith H = 0.7\n\nFIG. 4. R/S diagram corresponding to the Dickens corpus.\nThe linear fits are showed only for the original sequence and\nthe shuffling experiments.\n\nIn Figs. 4 and 5 we reproduce an identical analysis\nfrom two text corpora gathering a collection of works\nby Dickens and Darwin respectively, whose results are\nsummarised in Table I. The Dickens sequence was obtained from 56 books by the author, and has 5616403\nranks (words) in length (44700 different ranks), whereas\nthe Darwin sequence was obtained from 11 books and\nhas a length of 1508483 ranks (30120 words in the vocabulary). It is worth noticing that the values of H from\nthe original sequences are indistinguishable for these two\ntexts, written in prose and with different styles, although\nthey are slightly greater than the value obtained for the\nShakespeare sequence. However, the three values of H\ncorresponding to the truncation (100 < rank < 2000)\nare statistically equivalent. This fact suggests that the\nlong-range correlations associated to words in the interval\nconsidered are a robust phenomenon over different styles\nand authors. Finally, the shuffling experiments show the\nsame behaviour for the three authors, and are consistent\nwith uncorrelated sequences.\nFIG. 5. R/S diagram corresponding to the Darwin corpus.\nThe linear fits are showed only for the original sequence and\nthe shuffling at sentence level.\nsource\ncorpus\nportion\n\nShakespeare\n0.687 \u00b1 0.040\n0.675 \u00b1 0.044\n0.672 \u00b1 0.041\n\nDickens\n0.738 \u00b1 0.033\n0.715 \u00b1 0.043\n\nB. Tests of consistence\n\nIn this subsection we present three tests of consistency\nof our analysis. First, we performed the same constructions as developed in the previous subsection over portions extracted from the original text corpora. Thus,\nwe split the Shakespeare corpus into two parts of equal\nlength and measured H for each part. On the other\nhand, from the Dickens corpus we extracted a succession of 861038 words from an arbitrary origin and then\n\nTABLE II. Test of consistence: We estimated the H value\nfor portions of two corpora considered in Table I. The portions from the Shakespeare corpus correspond to the first and\nsecond half of the original source. From the Dickens corpus\nwe have taken an embedded succession of 861038 words from\nan arbitrary origin in the original corpus.\n\n5\n\n\fmeasured the corresponding value of H. In Table II the\nvalues from both the original corpora and the parts are\nconfronted. Values indistinguishable of H result from\nthe original corpus and its portions. In interpreting this\nresult we should have in mind that Zipf's analysis of a\nportion of the text generates a quite different table of\nranks from the one corresponding to the entire corpus.\nThis serves as an indication that we are quantifying a\nrobust phenomenon inherent to the fractal structure of\ntexts.\nIn the insets of Figs. 1 and 4 we present the values\nof mean and standard deviation (\u03c3\u0302) corresponding to\nthe set of ranks in the entire corpus of Shakespeare and\nDickens respectively. For the Shakespeare corpus we can\nread that the mean of ranks is equal to 1003, whereas\n\u03c3\u0302 = 2632. On the other hand, Fig. 4 shows for the Dickens corpus that the mean of ranks is equal to 1327 and\n\u03c3\u0302 = 3793. These quantities are calculated from sample\nvalues of a probability distribution P (r), for which Zipf's\nlaw represents a crude approximation. However, by using\nmore accurate analytical descriptions of the probability\ndensity P (r), it is possible to evaluate the statistical moments for the whole distribution, that is, in the limit of\ninfinite vocabulary. In particular, let us mention that the\nstandard deviation calculated for both the Shakespeare\nand Dickens corpora, using the analytical expressions for\nP (r) obtained in Ref. [18], result finite in the infinite vocabulary limit. The reason for this is that for large values\nof r, the probability density develops a fast exponential\ndecay in the case of single-author corpora [18].\nIn Table I we also report the values of H corresponding\nto corpora generated by means of stochastic processes.\nParticularly, we did the analysis explained above over a\nsequence of ranks generated by the Simon's model [19]\nand other corresponding to a Markovian text [20], with\na memory of seven letters. This short-range memory\nis enough to string out groups of few words in grammatical order. However, as we can see from the values\non the table, we do not obtain correlations from neither type of sequence, which is consistent with the expected behaviour for processes with short-range correlations. Consequently, our implementation of the rescaled\nrange analysis allows to clearly distinguish between real\nand stochastic version of texts [21].\n\nble origins for the long-range dependence have been put\nforward, though the conclusions in all cases have been\ninferred from the observed statistical behaviour of single\nliterary works, and from the mapping of texts at letter level. In the present work we use a robust mapping\nwith a strong footing on linguistics, and focus on the\ncorrelations that arise in large corpora comprising many\nindividual books with no thematic linkage. Therefore,\nthe existence of correlations overarching sets of entire\nworks should be attributed to something else than either the relation between ideas expressed by the author\nas it was proposed in Ref. [8], or nonuniformities in the\ndistribution of word's lengths and the associated densities of blank spaces as it was suggested in Ref. [10]. In\nparticular, the latter alternative is ruled out from the\noutset since our mapping does not carry information on\nword structure.\n\nFIG. 6. R/S diagram of the Webster's abridged dictionary\n(1913 edition). The linear fits correspond to the sequences\nwithout the abbreviations.\n\nIn order to gain more insight into the origin of the\nattested long-range correlations, we decided to perform\nanother experiment with a special type of text. A dictionary is a large collection of entries, one for each different\nword, arranged in alphabetical order. Clearly, in general,\nsequences of thematic affinity span at most a few entries,\nfor small groups of words associated by common roots.\nNevertheless, as is confirmed in Fig. 6 where we display\nthe R/S diagram generated from the Webster's abridged\ndictionary (1913 edition), the value of H certifies the\npresence of long-range correlations. Table III compiles\nthe exponent H from the entire dictionary and from the\ntext without the abbreviations (which indicate the grammatical category of entry words), obtaining in both cases\ntotally consistent values for H. This result is rather striking and hints at the presence of a type of long-range organisation in the layout of lexical entries that may be con-\n\nC. The Dictionary\n\nSo far, we still do not have enough evidence as to assert a precise source for the phenomenon being analysed\nin this paper. We have characterised with a robust quantifier the presence of long-range correlations in literary\ntexts beyond sentence level, and in fact over ranges spanning more than one individual work. The shuffling experiments attest where the long-range correlation does not\noriginate, but say nothing on where it does. In previous\nattempts to study this phenomenon, a variety of possi-\n\n6\n\n\fnected with the phenomenon observed in literary works.\nThis last assertion is further supported by performing\na shuffle over the whole dictionary keeping integrity at\nentry level. That is, we do not disrupt the structure of\nentries made up of each word together with the associated definition, but we do mix them in a random manner. The resulting collection of definitions now lacks the\nalphabetical ordering; however, the explicit information\ncontained in the dictionary is still intact. Notwithstanding, the long-range order is completely obliterated by the\nshuffle as accounted for by the fall in the H value down to\nthat of an uncorrelated sequence. This experiment corroborates that beyond the local structures derived from\nsemantic affinity of small groups of related words, the\nordered lexicon also possesses an overall macrostructure\nthat emerges, as a hidden layer, out of the alphabetical\norder of entries.\nA close examination at the structure of the dictionary reveals the source of the long-range order. Let us\ntake for example the following sequence of related entries\nfrom the Webster's abridged dictionary (1913 edition):\nadvice, advisability, advisable, advisableness, advisably,\nadvise, advised, advising, advisedly, advisedness, advisement, adviser, advisership, advisor, and advisory. These\nentries form a small cohesive block of information, and\nall their definitions share many words. In turn, these\nshared terms point to, possibly, remote locations in the\ndictionary where, again, words with common roots form\nsmall clusters. This process carries on to many levels of\ndepth, thus building a complex network of relations at\nword level, which may be closely related to the presence\nof long-range correlation in the succession of words in\nthe dictionary. The entry\u2013level shuffling destroys that\nemerging order, and thereby the correlations.\n\nWebster's Dictionary\noriginal sequence\nsequence without abbr.\nentries shuffled a\n\nV. CONCLUSIONS\n\nIn this work we have addressed the important issue of\nthe emergence of long-range correlations in human written communication. To that end we proposed a simple\nmapping of texts onto random walks that keeps the word\nas the basic unit of communication. Therefore, the time\nseries generated by this mapping retain the structure relevant to the linguistic phenomenon being analysed.\nWe addressed in some detail the rescaled range analysis, which has been successfully applied to the analysis\nof a vast variety of time series exhibiting long-term correlations. By applying this analysis to the coded texts\nwe found conclusive evidence for long-range correlation\nin the use of words over spans as long as the whole corpora under study. It is worth noticing that the corpora\nused in this work are made up of the concatenation of\nindependent literary works by individual authors, and,\ntherefore the long-range effects must emerge as a phenomenon independent of the particular bounds of single\nliterary works. This observation led us to analyse the case\nof a dictionary as a special kind of text, which provided\ninsight into possible sources for the long-range order. The\nalphabetical order of entries in a dictionary corresponds\nonly to the first visible layer of structural organisation.\nYet, by performing the shuffling at entry level we realised\nthat the whole of the long-range order is dependent upon\na more complex structural layer related to the network\nof associations among word clusters.\nAs it has been convincingly shown in Ref. [22] sets\nof literary works also possess higher level structures associated with systematic patterns in word usage, which\nmight also give rise to a complex network topology of relations among groups of words. The detailed mechanisms\nwhereby the onset of these structures takes place in language requires further interdisciplinary research. In the\nlight of the evidence supplied in this work, it is plausible\nthat the ultimate source of these correlations is deeply\nrelated to the structural patterns of word distribution in\nwritten communication. In this view, groups of words\nassociated by affine semantic hierarchies form a complex\narrangement of cohesive clusters even over spans of entire corpora, thereby giving rise to long-range order in\nhuman written records.\n\nH\n0.690 \u00b1 0.031\n0.699 \u00b1 0.036\n0.548 \u00b1 0.025\n\nTABLE III. Values of H for the Webster's abridged dictionary (edition 1913) after coding as sequence of ranks.\na\nfrom the text without abbreviations\n\nACKNOWLEDGMENTS\n\nThe authors are grateful to D. H. Zanette for his critical reading of the manuscript and to F. A. Tamarit for\nuseful suggestions. This work has been partially supported by \"Secretar\u0131\u0301a de Ciencia y Tecnolog\u0131\u0301a de la Universidad Nacional de C\u00f3rdoba\" (fellowship and grant\n194/00). The digital texts analysed in this paper were\nobtained from Project Gutenberg Etext [23]\n\n7\n\n\f\u2217\n\u2020\n\n[1]\n[2]\n\n[3]\n[4]\n[5]\n\n[6]\n\n[7]\n[8]\n[9]\n[10]\n\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n\n[18]\n[19]\n[20]\n[21]\n[22]\n[23]\n\nElectronic mail: mmontemu@famaf.unc.edu.ar\nElectronic mail: pury@famaf.unc.edu.ar\nS. Pinker, The Language Instinct (Harper Collins, New\nYork, 2000).\nP. Mellars, in The Origin and Diversification of Language, eds. N. G. Jabloski and L. C. Aiello (California\nAcademy of Sciences, San Francisco, 1998)\nI. Tattersall and J. H. Matternes, Sci. Am. 282, 38 (January 2000).\nM. A. Nowak, J. B. Plotkin, and V. A. A. Jansen, Nature\n404, 495 (2000).\nA. Aknajian, R. A. Demers, A. K. Farmer, and R. M.\nHarnish, Linguistics. An Introduction to Language and\nCommunication (MIT Press, Cambridge, 1992).\nC.-K. Peng, S. V. Buldyrev, A. L. Goldberger, S. Havlin,\nF. Sciortino, M. Simons, and H. E. Stanley, Nature 356,\n168 (1992); Richard F. Voss, Phys. Rev. Lett. 68, 3805\n(1992); S. V. Buldyrev and A. L. Goldberger, S. Havlin,\nC.-K. Peng, M. Simons, F. Sciortino, and H. E. Stanley,\nibid. 71, 1776 (1993); R. F. Voss, ibid. 71, 1777 (1993).\nRichard F. Voss, Fractals 2, 1 (1994).\nAlain Schenkel, Jun Zhang, and Yi-Cheng Zhang, Fractals 1, 47 (1993).\nM. Amit, Y. Shmerler, E. Eisenberg, M. Abraham, and\nN. Shnerb, Fractals 2, 7 (1994).\nWerner Ebeling and Thorsten P\u00f6schel, Europhys. Lett.\n26, 241 (1994); Werner Ebeling and Alexander Neiman,\nPhysica A 215, 233 (1995).\nG. K. Zipf, Human Behavior and the Principle of Least\nEffort (Addison-Wesley, Reading, 1949).\nH. E. Hurst, Trans. Amer. Soc. Civil Eng. 116, 770\n(1951).\nB. B. Mandelbrot and J. R. Wallis, Water Resources Research 5, 242 (1969).\nB. B. Mandelbrot and J. R. Wallis, Water Resources Research 5, 321 (1969).\nB. B. Mandelbrot and J. R. Wallis, Water Resources Research 5, 967 (1969).\nW. Feller, Ann. Math. Stat. 22, 427 (1951).\nR. F. Voss, Random Fractal Forgeries in Fundamentals\nAlgorithms in Computer Graphics, eds. R. Pynn and A.\nSkjeltorp (Plenum Press, New York, 1985), pp. 1\u201311.\nMarcelo A. Montemurro, Physica A 300, 557 (2001)\nHerbert A. Simon, Biometrika 42, 425 (1955).\nBrian Hayes, Sci. Am. 249, 16 (November 1983).\nA. Cohen and R. N. Mantegna and S. Havlin, Fractals 5,\n95 (1996).\nM. A. Montemurro and D. H. Zanette, Advances in Complex Systems, in press.\nhttp://promo.net/pg\n\n8\n\n\f"}