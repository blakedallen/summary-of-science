{"id": "http://arxiv.org/abs/1112.2113v1", "guidislink": true, "updated": "2011-12-09T15:01:25Z", "updated_parsed": [2011, 12, 9, 15, 1, 25, 4, 343, 0], "published": "2011-12-09T15:01:25Z", "published_parsed": [2011, 12, 9, 15, 1, 25, 4, 343, 0], "title": "Incremental Slow Feature Analysis: Adaptive and Episodic Learning from\n  High-Dimensional Input Streams", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1112.4734%2C1112.2280%2C1112.4908%2C1112.1243%2C1112.3259%2C1112.3200%2C1112.1707%2C1112.2636%2C1112.2277%2C1112.3866%2C1112.4971%2C1112.0856%2C1112.4785%2C1112.4486%2C1112.4219%2C1112.4368%2C1112.2823%2C1112.3380%2C1112.3366%2C1112.3329%2C1112.2758%2C1112.3272%2C1112.4045%2C1112.5249%2C1112.1981%2C1112.5070%2C1112.5185%2C1112.6050%2C1112.5699%2C1112.4474%2C1112.1656%2C1112.4858%2C1112.5946%2C1112.3488%2C1112.5443%2C1112.6429%2C1112.2119%2C1112.6162%2C1112.5504%2C1112.5813%2C1112.5803%2C1112.5647%2C1112.2113%2C1112.5921%2C1112.5691%2C1112.6105%2C1112.2845%2C1112.4071%2C1112.2218%2C1112.1202%2C1112.1001%2C1112.5745%2C1112.6382%2C1112.5642%2C1112.1746%2C1112.3600%2C1112.5026%2C1112.2624%2C1112.0747%2C1112.3857%2C1112.4789%2C1112.1840%2C1112.1780%2C1112.5336%2C1112.2820%2C1112.0386%2C1112.0750%2C1112.6080%2C1112.3245%2C1112.3948%2C1112.4933%2C1112.1325%2C1112.6172%2C1112.6282%2C1112.6327%2C1112.2997%2C1112.5007%2C1112.2896%2C1112.2795%2C1112.2199%2C1112.5537%2C1112.4056%2C1112.3765%2C1112.0470%2C1112.5129%2C1112.3643%2C1112.1086%2C1112.2754%2C1112.5140%2C1112.0412%2C1112.3739%2C1112.0661%2C1112.0626%2C1112.0446%2C1112.2533%2C1112.0835%2C1112.5842%2C1112.0008%2C1112.4518%2C1112.4088%2C1112.4285&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Incremental Slow Feature Analysis: Adaptive and Episodic Learning from\n  High-Dimensional Input Streams"}, "summary": "Slow Feature Analysis (SFA) extracts features representing the underlying\ncauses of changes within a temporally coherent high-dimensional raw sensory\ninput signal. Our novel incremental version of SFA (IncSFA) combines\nincremental Principal Components Analysis and Minor Components Analysis. Unlike\nstandard batch-based SFA, IncSFA adapts along with non-stationary environments,\nis amenable to episodic training, is not corrupted by outliers, and is\ncovariance-free. These properties make IncSFA a generally useful unsupervised\npreprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA\nand MCA updates take the form of Hebbian and anti-Hebbian updating, extending\nthe biological plausibility of SFA. In both single node and deep network\nversions, IncSFA learns to encode its input streams (such as high-dimensional\nvideo) by informative slow features representing meaningful abstract\nenvironmental properties. It can handle cases where batch SFA fails.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1112.4734%2C1112.2280%2C1112.4908%2C1112.1243%2C1112.3259%2C1112.3200%2C1112.1707%2C1112.2636%2C1112.2277%2C1112.3866%2C1112.4971%2C1112.0856%2C1112.4785%2C1112.4486%2C1112.4219%2C1112.4368%2C1112.2823%2C1112.3380%2C1112.3366%2C1112.3329%2C1112.2758%2C1112.3272%2C1112.4045%2C1112.5249%2C1112.1981%2C1112.5070%2C1112.5185%2C1112.6050%2C1112.5699%2C1112.4474%2C1112.1656%2C1112.4858%2C1112.5946%2C1112.3488%2C1112.5443%2C1112.6429%2C1112.2119%2C1112.6162%2C1112.5504%2C1112.5813%2C1112.5803%2C1112.5647%2C1112.2113%2C1112.5921%2C1112.5691%2C1112.6105%2C1112.2845%2C1112.4071%2C1112.2218%2C1112.1202%2C1112.1001%2C1112.5745%2C1112.6382%2C1112.5642%2C1112.1746%2C1112.3600%2C1112.5026%2C1112.2624%2C1112.0747%2C1112.3857%2C1112.4789%2C1112.1840%2C1112.1780%2C1112.5336%2C1112.2820%2C1112.0386%2C1112.0750%2C1112.6080%2C1112.3245%2C1112.3948%2C1112.4933%2C1112.1325%2C1112.6172%2C1112.6282%2C1112.6327%2C1112.2997%2C1112.5007%2C1112.2896%2C1112.2795%2C1112.2199%2C1112.5537%2C1112.4056%2C1112.3765%2C1112.0470%2C1112.5129%2C1112.3643%2C1112.1086%2C1112.2754%2C1112.5140%2C1112.0412%2C1112.3739%2C1112.0661%2C1112.0626%2C1112.0446%2C1112.2533%2C1112.0835%2C1112.5842%2C1112.0008%2C1112.4518%2C1112.4088%2C1112.4285&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Slow Feature Analysis (SFA) extracts features representing the underlying\ncauses of changes within a temporally coherent high-dimensional raw sensory\ninput signal. Our novel incremental version of SFA (IncSFA) combines\nincremental Principal Components Analysis and Minor Components Analysis. Unlike\nstandard batch-based SFA, IncSFA adapts along with non-stationary environments,\nis amenable to episodic training, is not corrupted by outliers, and is\ncovariance-free. These properties make IncSFA a generally useful unsupervised\npreprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA\nand MCA updates take the form of Hebbian and anti-Hebbian updating, extending\nthe biological plausibility of SFA. In both single node and deep network\nversions, IncSFA learns to encode its input streams (such as high-dimensional\nvideo) by informative slow features representing meaningful abstract\nenvironmental properties. It can handle cases where batch SFA fails."}, "authors": ["Varun Raj Kompella", "Matthew Luciw", "Juergen Schmidhuber"], "author_detail": {"name": "Juergen Schmidhuber"}, "author": "Juergen Schmidhuber", "links": [{"href": "http://arxiv.org/abs/1112.2113v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1112.2113v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1112.2113v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1112.2113v1", "arxiv_comment": null, "journal_reference": "Neural Computation, 2012, Vol. 24, No. 11, Pages 2994-3024", "doi": null, "fulltext": "Incremental Slow Feature Analysis: Adaptive and Episodic\n\narXiv:1112.2113v1 [cs.AI] 9 Dec 2011\n\nLearning from High-Dimensional Input Streams\n\nVarun R. Kompella, Matthew Luciw and J\u00fcergen Schmidhuber\n\nTechnical Report No. IDSIA-07-11\nNovember 2011\n\nIDSIA / USI-SUPSI\nIstituto Dalle Molle di studi sull'intelligenza artificiale\nGalleria 2, 6928 Manno, Switzerland\n\nIDSIA was founded by the Fondazione Dalle Molle per la Qualit\u00e0 della Vita and is affiliated with both the Universit\u00e0 della Svizzera italiana (USI)\nand the Scuola unversitaria professionale della Svizzera italiana (SUPSI).\n\n\fTechnical Report No. IDSIA-07-11\n\n1\n\nIncremental Slow Feature Analysis: Adaptive and Episodic\nLearning from High-Dimensional Input Streams\nVarun R. Kompella, Matthew Luciw and J\u00fcergen Schmidhuber\nNovember 2011\nAbstract\nSlow Feature Analysis (SFA) extracts features representing the underlying causes of changes within a\ntemporally coherent high-dimensional raw sensory input signal. Our novel incremental version of SFA\n(IncSFA) combines incremental Principal Components Analysis and Minor Components Analysis. Unlike\nstandard batch-based SFA, IncSFA adapts along with non-stationary environments, is amenable to episodic\ntraining, is not corrupted by outliers, and is covariance-free. These properties make IncSFA a generally\nuseful unsupervised preprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA and\nMCA updates take the form of Hebbian and anti-Hebbian updating, extending the biological plausibility\nof SFA. In both single node and deep network versions, IncSFA learns to encode its input streams (such\nas high-dimensional video) by informative slow features representing meaningful abstract environmental\nproperties. It can handle cases where batch SFA fails.\n\n1\n\nIntroduction\n\nSlow feature analysis (Wiskott and Sejnowski, 2002; Wiskott et al., 2011)(SFA) is an unsupervised learning\ntechnique that extracts features from an input stream with the objective of maintaining an informative\nbut slowly-changing feature response over time. The idea of using temporal stability as an objective in\nlearning systems has motivated some other unsupervised learning techniques (Hinton, 1989; F\u00f6ldi\u00e1k, 1991;\nMitchison, 1991; Schmidhuber, 1992a; Bergstra and Bengio, 2009). SFA is distinguished by its formulation\nof the feature extraction problem as an eigensystem problem, which guarantees that its solution methods\nreliably converge to the best solution, given its constraints (no local minima problem). SFA has shown\nsuccess in problems such as extraction of driving forces of a dynamical system (Wiskott, 2003), nonlinear\nblind source separation (Sprekeler et al., 2010), as a preprocessor for reinforcement learning (Legenstein\net al., 2010; Kompella et al., 2011b), and learning of place-cells, head-direction cells, grid-cells, and spatial\n\n\fTechnical Report No. IDSIA-07-11\n\n2\n\nview cells from high-dimensional visual input (Franzius et al., 2007) - such representations also exist in\nbiological agents (O'Keefe and Dostrovsky, 1971; Taube et al., 1990; Rolls, 1999; Hafting et al., 2005).\nThere are limitations to existing SFA implementations due to their batch processing nature, which\nbecomes especially apparent when attempting to apply it in somewhat uncontrolled environments. To\novercome these issues, we introduce the new Incremental Slow Feature Analysis (IncSFA) (Kompella et al.,\n2011a,b). A few earlier techniques with temporal continuity objective were incremental as well (Hinton,\n1989; Bergstra and Bengio, 2009), but IncSFA follows the SFA formulation and can track solutions of\nbatch SFA (BSFA), over which it has the following advantages:\n\u2022 Adaptation to changing input statistics. BSFA requires all data to be collected in advance. New\ndata cannot be used to modify already learned slow features. Once the input statistics change, IncSFA\ncan automatically adapt its features without outside intervention, while BSFA has to discard previous\nfeatures to process the new data.\nIn open-ended learning settings, an autonomous agent's lifelong input stream follows such a nonstationary distribution. The agent's behavior will typically change over time, thus generating new input\nsequences. Features useful for early behaviors may not be useful for later behavior.\n\u2022 Learn features across episodes. Episodic learning is impossible for BSFA, since it cannot handle\ntemporal discontinuities at episode boundaries. IncSFA, however, may use the final slow features\nfrom the previous episode to initialize its features of the next episode.\n\u2022 Reduced sensitivity to outliers. Real-world environments typically exhibit infrequent, uncontrolled,\ninsignificant external events that should be ignored. BSFA is very sensitive to such events, encoding\neverything that changes slowly within the current batch. IncSFA's plasticity, however, makes it lose\nsensitivity to such events over time.\n\u2022 Covariance-free. BSFA techniques rely upon batch Principal Component Analysis (Jolliffe, 1986)\n(PCA), which requires the data's covariance matrix. Estimating, storing and/or updating covariance matrices can be expensive for high-dimensional data and impractical for open-ended learning.\nIncSFA uses covariance-free techniques. For high-dimensional images, the number of parameters\nto estimate in the covariance matrix is huge: n(n + 1)/2 for dimension n, while a covariance-free\ntechnique only requires n\u00d7m where m is the desired number of principal components (PCs). For example, 100 \u00d7 100 dimensional images lead to 50, 005, 000 free parameters in the covariance matrix,\nwhich is reduced to only 100 \u00d7 100 \u00d7 m eigenvector parameters with covariance-free updating.\nFurthermore, since often only a relatively small number of principal components are needed to explain most of the variance in the data, the other components do not even have to be estimated. With\n\n\fTechnical Report No. IDSIA-07-11\n\n3\n\nIncSFA, dimensionality reduction can be done during PC estimation; no time needs to be wasted on\ncomputing the many insignificant lower-order PCs.\nAnother technical problem with the covariance matrix: in sequences where only a small part of the\ninput changes, computing the principal components of the difference signal's covariance matrix will\nresult in singularity errors, since the matrix won't have full rank.\n\u2022 Biological Plausibility. IncSFA adds further biological plausibility to SFA. SFA itself is linkable to\nbiological systems due to the results in deriving place cell, grid cells, etc., but it is difficult to see how\nBSFA could be realized in the brain. IncSFA's updates, however, can be described in incremental\nHebbian and anti-Hebbian terms.\nThe remainder of this paper is organized as follows. Section 2 reviews SFA and its batch solution.\nSection 3 describes the new incremental SFA. Section 4 details the algorithm and discusses deeper related\nissues, including convergence conditions and parameter setting. Section 5 contains experiments and results, and shows how to utilize IncSFA as part of a hierarchical image processing architecture. Section 6\nconcludes the paper.\n\n2\n2.1\n\nBackground\nSFA: Intuition\n\nWe first review SFA briefly in an intuitive sense. SFA is a form of unsupervised learning (UL). It searches\nfor a set of mappings gi from data x \u2208 RI to output components yi = gi (x) that are separate from\neach other in some sense and express information that is in some sense relevant. In SFA separateness is\nrealized as decorrelation (like in PCA), while relevance is defined in terms of slowness of change over\ntime. Ordering our functions g1 , g2 , ..., gI by slowness, we can discard all but the J < I slowest, to enable\ndimensionality reduction, getting rid of irrelevant information such as quickly changing noise assumed to\nbe useless. The compact relevant data encodings reduce the search space for downstream goal-directed\nlearning procedures (Schmidhuber, 1999; Barlow, 2001). As an example, consider a high-dimensional\ndynamical system: a mobile robot sensing with an onboard camera, where each pixel is considered a\nseparate observation component. SFA will use the video sequences to guide its search over functions that\nencode each image into a small set of state variables, and the robot can use these new state variables to\nquickly develop useful controllers. Fig. 1 provides a visual example of how SFA operates.\nSFA-based UL learns instantaneous features from sequential data (Hinton, 1989; Wiskott and Sejnowski, 2002; Doersch et al., ). Relevance cannot be uncovered without taking time into account, but\nonce it is known, each input frame can be processed on its own. SFA differs from both 1. many well-\n\n\fTechnical Report No. IDSIA-07-11\n\n?\n\n4\n\n?\n\nApply learned\nfeatures\n\nSF2\n1\n\n1\n\n?\n\n4\n\n2\n\nSF1\n\n3 2\n\n3\n4\n\n?\n(a)\n\n(b)\n\nTemporal to spatial\ntransformation\n\n(c)\n\nFigure 1: Intuition of SFA. (A): Consider a zero-mean input signal that spatially resembles white noise.\nAssume the input distributions are Gaussian, shown by the gray area, while the black dots show individual\ndata points. Spatial feature extractors such as PCA will not prefer any direction over any other. (B):\nEschewing unhelpful spatial processing, we examine this input as a time-series; to illustrate here we show\na short sequence of input. Each difference vector becomes a spatial component in the space shown in (C).\nIn this space, the first principal component gives the (linear) direction of quickest change. The second -\nthe minor component - gives the direction of slowest change. We see that recoding the data in terms of\nsubsequent differences and performing an eigendecomposition provides an ordered set of separate features,\nwhich are applied to the original input signal.\nknown unsupervised feature extractors (Abut, 1990; Jolliffe, 1986; Comon, 1994; Lee and Seung, 1999;\nKohonen, 2001; Hinton, 2002), which ignore dynamics, and 2. Other UL systems that both learn and apply\nfeatures to sequences (Schmidhuber, 1992a,c,b; Lindst\u00e4dt, 1993; Klapper-Rybicka et al., 2001; Jenkins and\nMatari\u0107, 2004; Lee et al., 2010; Gisslen et al., 2011), thus assuming that the state of the system itself can\ndepend on past information.\n\n2.2\n\nSFA: Formulation\n\nSFA's optimization problem (Wiskott and Sejnowski, 2002; Franzius et al., 2007) is formally written as\nfollows:\nGiven an I-dimensional sequential input signal x(t) = [x1 (t), ..., xI (t)]T , find a set of J instantaneous\nreal-valued functions g(x) = [g1 (x), ..., gJ (x)]T , which together generate a J-dimensional output signal\ny(t) = [y1 (t), ..., yJ (t)]T with yj (t) := gj (x(t)), such that for each j \u2208 {1, ..., J}\n\u2206j := \u2206(yj ) := h\u1e8fj2 i is minimal\n\n(1)\n\n\fTechnical Report No. IDSIA-07-11\n\n5\n\nunder the constraints\nhyj i =\n\n0\n\n(zero mean),\n\n(2)\n\nhyj2 i =\n\n1\n\n(unit variance),\n\n(3)\n\n\u2200i < j : hyi yj i =\n\n0\n\n(decorrelation and order),\n\n(4)\n\nwith h*i and \u1e8f indicating temporal averaging and the derivative of y, respectively.\nThe problem is to find instantaneous functions gj that generate different output signals varying as slowly\nas possible. The constraints (2) and (3) together avoid a trivial constant output solution. The decorrelation\nconstraint (4) ensures that different functions gj do not code for the same features.\n\n2.3\n\nBatch SFA\n\nSolving this learning problem involves non-trivial variational calculus optimization. But it is simplified\nthrough an eigenvector approach. If the gj are linear combinations of a finite set of nonlinear functions h,\nthen\nyj (t) = gj (x(t)) = wjT h(x(t)) = wjT z(t),\n\n(5)\n\nand the SFA problem now becomes to find weight vectors wj to minimize the rate of change of the output\nvariables,\n\u2206(yj ) = h\u1e8fj2 i = wjT h\u017c\u017cT i wj ,\n\n(6)\n\nsubject to the constraints (2-4). The slow feature learning problem has become linear on the derivative\nsignal \u017c.\nIf the functions of h are chosen such that z has unit covariance matrix and zero mean, the three constraints will be fulfilled if and only if the weight vectors wj are orthonormal. Eq. 6 will be minimized, and\nthe orthonormal constraint satisfied, with the set of J normed eigenvectors of h\u017c\u017cT i with the J smallest\neigenvalues (for any J \u2264 I).\nThe BSFA technique practically implements this solution by using batch principal component analysis\n(PCA) (Jolliffe, 1986) twice. Referring back to Eq. 6, to select h appropriately, a well-known process\ncalled whitening (or sphering), is used to map x to a z with zero mean and identity covariance matrix, thus\ndecorrelating signal components and scaling them so that there is unit variance along each PC direction.\nWhitening serves as a bandwidth normalization, so that slowness can truly be measured (slower change\nwill not simply be due to a low variance direction). Whitening requires the PCs of the input signal (PCA\n#1). The orthonormal basis that minimizes the rate of output change are the minor components \u2013 principal\n\n\fTechnical Report No. IDSIA-07-11\n\n6\n\ncomponents with smallest eigenvalues \u2013 in the derivative space. So another PCA (#2) on \u017c yields the slow\nfeatures (eigenvectors) and their order (via eigenvalues).\n\n3\n\nIncremental SFA\n\nIncSFA also employs the eigenvector tactic, but may update an existing estimate on any amount of new\ndata, even a single data point x(t). A high-level formulation is\n\n(W(t + 1), \u03b8(t + 1)) = IncSF A(W(t), x(t), \u03b8(t)),\n\n(7)\n\nwhere W = (w1 , ..., wJ ) is the matrix of existing slow feature vector estimates, and \u03b8 contains algorithm\nmemory and parameters, which we will discuss later.\nTo replace PCA #1, IncSFA needs to do online whitening of input x. We use Candid Covariance-Free\nIncremental (CCI) PCA (Weng et al., 2003). CCIPCA incrementally updates both the eigenvectors and\neigenvalues necessary for whitening, and does not keep an estimate of the covariance matrix. CCIPCA is\nalso used to reduce dimensionality.\nExcept for low-dimensional derivative signals \u017c, CCIPCA cannot replace PCA #2. It will be unstable, since the slow features correspond to the least significant components. Minor Components Analysis\n(MCA) (Oja, 1992) incrementally extracts the principal components with the smallest eigenvalues. We use\nPeng's low complexity updating rule (Peng et al., 2007). Peng proved its convergence even for constant\nlearning rates-good for open-ended learning. MCA with sequential addition (Chen et al., 2001; Peng and\nYi, 2006) will extract multiple slow features in parallel.\n\n3.1\n\nNeural Updating for PC and MC Extraction\n\nCCIPCA and Peng's MCA are the most appropriate incremental PCA and MCA algorithms for IncSFA.\nTo justify these choices, we briefly review the literature on neural networks that perform incremental PCA\nand MCA.\nWell-known incremental PCA algorithms are Oja and Karhunen's Stochastic Gradient Ascent (SGA) (Oja,\n1985), Sanger's Generalized Hebbian Algorithm (GHA) (Sanger, 1989), and CCIPCA. They all build on\nthe work of Amari (1977) and Oja (1982), who showed that a linear neural unit using Hebbian updating\ncould compute the first principal component of a data set (Amari, 1977; Oja, 1982)1 . However, SGA\n(1985) builds upon Oja's earlier work, GHA (1989) builds upon SGA, and CCIPCA (2003) builds upon\nGHA.\n1 Much earlier work of a non-neural network flavor had shown how the first PC, including the eigenvalue could be learned incrementally (Krasulina, 1970).\n\n\fTechnical Report No. IDSIA-07-11\n\n7\n\nSGA use Gram-Schmidt Orthonormalization (GSO) to incrementally find the subspace of all principal components, but there is no guarantee of finding the components themselves. Sanger used Kreyszig's\n(Kreyszig, 1988) (1988) (faster/more effective) residual vector method for computing multiple components.\nHis provably converging GHA used the residual method for simultaneous computation of all components.\nCCIPCA (Weng et al., 2003) modified GHA to be \"candid\", meaning it maintained an implicit learning\nrate dependant on the data, greatly increasing the algorithm's efficiency so that it became useful for highdimensional inputs, such as in appearance-based computer vision. This incremental PCA updating method\nis the best of the above for IncSFA. It converges (Zhang and Weng, 2001) to both eigenvectors and eigenvalues, necessary since whitening requires both. Due to its candidness, potentially difficult learning rate\n\"hand-tuning\" is minimized.\nAs for MCA: Xu et al. (Xu et al., 1992) were the first to show that a linear neural unit equipped\nwith anti-Hebbian learning could extract minor components. Oja modified SGA's updating method to an\nanti-Hebbian variant (Oja, 1992), and showed how it could converge to the MC subspace. Studying the\nnature of the duality between PC and MC subspaces (Wang and Karhunen, 1996; Chen et al., 1998), Chen,\nAmari and Lin (Chen et al., 2001) (2001) introduced the sequential addition technique, enabling linear\nnetworks to efficiently extract multiple MCs simultaneously. Building upon previous MCA algorithms,\nPeng (2007) (Peng et al., 2007) derived the conditions and a learning rule for extracting MCs without\nchanging the learning rate. Sequential addition was added to this rule so that multiple MCs could be\nextracted (Peng and Yi, 2006). We use this MCA updating method since it gives us the actual minor\ncomponents, not just the subspace they span, and it allows for a constant learning rate, which can be quite\nhigh, leading to a quick reasonable estimate of the true components.\n\n3.2\n\nCCIPCA Updating\n\nGiven zero-mean data u = x\u2212E[x], a PC is a normed eigenvector vi\u2217 of the data covariance matrix E[uuT ].\nEigenvalue \u03bb\u2217i is the variance of the samples along vi\u2217 . By definition, an eigenvector and eigenvalue satisfy\nE[uuT ]vi\u2217 = \u03bb\u2217i vi\u2217 ,\n\n(8)\n\nThe set of eigenvectors are orthonormal, and ordered such that \u03bb\u22171 \u2265 \u03bb\u22172 \u2265 ... \u2265 \u03bb\u2217K .\n\u2217\nThe whitening matrix is generated by multiplying the matrix of principal components V\u0302 = [v1\u2217 , ..vK\n]\n1\n\u02c6\nby the diagonal matrix D\u0302, where component di,i = p \u2217 . After whitening via z(t) = V\u0302D\u0302u(t), then\n\u03bbi\nT\nE[zz ] = I. In IncSFA, we use online estimates of V\u0302 and D\u0302. Both eigenvectors and eigenvalues need to\n\nbe estimated.\nCCIPCA updates V and D from each sample. For inputs ui , the first PC is the expectation of the\nnormalized response-weighted inputs. Eq 8 can be rewritten as\n\n\fTechnical Report No. IDSIA-07-11\n\n8\n\n\u03bb\u2217i vi\u2217 = E [(ui * vi\u2217 ) ui ] ,\n\n(9)\n\nThe corresponding incremental updating equation, where \u03bb\u2217i vi\u2217 is estimated by vi (t), is\n\u0014\nvi (t) = (1 \u2212 \u03b7) vi (t \u2212 1) + \u03b7\n\n\u0015\nui (t) * vi (t \u2212 1)\nui (t) .\nkvi (t \u2212 1)k\n\n(10)\n\nwhere \u03b7 is the learning rate. In other words, both the eigenvector and eigenvalue of the first PC of ui\ncan be found through the sample mean-type updating in Eq. 9. The estimate of the eigenvalue is given by\n\u03bbi = kvi (t)k. Using both a learning rate \u03b7 and retention rate (1 \u2212 \u03b7) automatically controls the adaptation\nof the vector with respect to the magnitude of the data vectors, leading to efficiency and stability.\n\n3.3\n\nLower-Order Principal Components\n\nAny component i > 1 not only must satisfy Eq. 8 but must also be constrained to be orthogonal to the\nhigher-order components. The residual method generates observations in a complementary space so that\nlower-order eigenvectors can be found by the same update rule Eq. 10.\nDenote ui (t) as the observation for component i. When i = 1, u1 (t) = u(t). When i > 1, ui is a\nresidual vector, which has the \"energy\" of u(t) from the higher-order components removed. Solving for\nthe first PC in this residual space solves for the i-th component overall. To create a residual vector, ui\nis projected onto vi to get the energy of ui that vi is responsible for. Then, the energy-weighted vi is\nsubtracted from ui to obtain ui+1 :\n\u0013\n\u0012\nvi (t)\nvi (t)\n.\nui+1 (t) = ui (t) \u2212 uTi (t)\nkvi (t)k kvi (t)k\n\n(11)\n\nTogether, Eq. 10 and Eq. 11 constitute the CCIPCA technique, which was proven to converge to the\ntrue components (Zhang and Weng, 2001). Yet, due to the residual method, the speed of learning is in line\nwith the order: the first PC must be \"sufficiently correct\" before the second PC can start to learn, and so\non.\n\n3.4\n\nMCA Updating\n\nAfter using CCIPCA components to generate an approximately whitened signal z, the derivative is approximated by \u017c(t) = z(t) \u2212 z(t \u2212 1). In this derivative space, the minor components on \u017c are the slow\nfeatures.\nTo find the minor component, Peng's MCA update rule (Peng et al., 2007) is used,\n\n\fTechnical Report No. IDSIA-07-11\n\nwi (t)\n\n9\n\n=\n\n1.5wi (t \u2212 1) \u2212 \u03b7 Ci wi (t \u2212 1)\n\n(12)\n\n\u2212 \u03b7 [wiT (t \u2212 1)wi (t \u2212 1)] wi (t \u2212 1),\nwhere, for the first minor component, C1 = \u017c(t)\u017cT (t).\nFor stability and convergence, the following constraints must be satisfied,\n\n\u03b7\u03bb1 < 0.5,\n1\n||w(0)||2 \u2264\n,\n2\u03b7\nwT (0)w\u2217 6= 0\n\n(13)\n(14)\n(15)\n\nwhere w(0) is the initial feature estimate and w\u2217 the true eigenvector associated with the smallest eigenvalue. Basically, the learning rate must not be too large, and the initial estimate must not be orthogonal to\nthe true component.\n\n3.5\n\nLower-Order Slow Features\n\nSequential addition shifts each observation into a space where the minor component of the current space\nwill be the first PC, and all other PCs are reduced in order by one. It does this by adding the scale of the\nfirst PC to the already estimated slow feature directions. This allows IncSFA to extract more than one slow\nfeature in parallel. Sequential addition updates the matrix Ci , \u2200i > 1 as follows:\n\u0001\n\u0001\nT\nT\nCi (t) = Ci\u22121 (t) + \u03b3(t) wi\u22121 (t)wi\u22121\n(t) / wi\u22121\n(t)wi\u22121 (t)\n\n(16)\n\nNote Eq. 16 introduces parameter \u03b3, which must be larger than the largest eigenvalue of E[\u017c(t)\u017cT (t)].\nTo automatically set \u03b3, we compute the greatest eigenvalue of the derivative signal through another CCIPCA\nrule to update only the first PC. Then, let \u03b3 = \u03bb1 (t) + \u000f for small \u000f.\n\n3.6\n\nLink to Hebbian and Anti-Hebbian Updating\n\nBSFA has been shown to derive slow features that operate like biological grid cells2 from quasi-natural\nimage streams, which are recorded from the camera of a moving agent exploring an enclosure (Franzius\net al., 2007). In rats, grid cells are found in entorhinal cortex (EC) (Hafting et al., 2005), which feeds\ninto the hippocampus. Augmenting the BSFA network with an additional competitive learning (CL) layer\n2 A grid cell has high firing rate when the animal is in certain positions in its closed environment - viewed from above, the pattern\nresembles a grid.\n\n\fTechnical Report No. IDSIA-07-11\n\n10\n\nderives units similar to place, head-direction, and spatial view cells. Place cells and head-direction cells\nare found in rat hippocampus (O'Keefe and Dostrovsky, 1971; Taube et al., 1990), while spatial view cells\nare found in primate hippocampus (Rolls, 1999).\nAlthough BSFA results exhibit the above biological link, it is not clear how this technique might be\nrealized in the brain. In particular, the space required for a covariance matrix of high-dimensional input\nis too large. IncSFA does not require covariance maatrices, and takes the form of biologically plausible\nHebbian and anti-Hebbian updating.\n3.1\n\nHebbian Updating in CCIPCA\n\nHebbian updates of synaptic strengths of some neuron make it more sensitive to expected input activations (Dayan and Abbott, 2001):\nv \u2190 v + \u03b7 g(v, u) u,\n\n(17)\n\nwhere u represents pre-synaptic (input) activity, and g post-synaptic activity (a function of similarity between synaptic weights v and input potentials u). The basic Eq. 17 requires additional care (e.g., normalization of v) to ensure stability during updating. To handle this in one step, learning rate \u03b7 and retention\nrate 1 \u2212 \u03b7 can be used,\nv \u2190 (1 \u2212 \u03b7)v + \u03b7 g(v, u) u.\n\n(18)\n\nwhere 0 \u2264 \u03b7 \u2264 1. With this formulation, Eq. 10 is Hebbian, where the post-synaptic activity is the\nui (t) * vi (t \u2212 1)\nand the presynaptic activity is the input ui .\nnormalized response g(v, u) =\nkvi (t \u2212 1)k\n3.2\n\nAnti-Hebbian Updating in Peng's MCA\n\nThe general form of anti-Hebbian updating simply results from flipping the sign in Eq. 17. In IncSFA\nnotation:\nw \u2190 w \u2212 \u03b7 g(w, \u017c) \u017c.\n\n(19)\n\nTo see the link between Peng's MCA updating and the anti-Hebbian form, in the case of the first MC,\nwe note Eq. 12 can be rewritten as\n\n\fTechnical Report No. IDSIA-07-11\n\nw1\n\n11\n\n\u0002\n\u0003\n\u2190 1.5w1 \u2212 \u03b7 C1 w1 + [w1T w1 ] w1 ,\n\n(20)\n\n\u2190 1.5w1 \u2212 \u03b7 [(\u017c * w1 ) \u017c + (w1 * w1 ) w1 ] ,\n\n(21)\n\n\u2190 1.5w1 \u2212 \u03b7 kw1 k2 w1 \u2212 \u03b7 ((\u017c * w1 ) \u017c) ,\n\u0001\n\u2190 1.5 \u2212 \u03b7 kw1 k2 w1 \u2212 \u03b7 (\u017c * w1 ) \u017c,\n\n(22)\n(23)\n\nwhere (\u017c * w1 ) indicates post-synaptic strength, and \u017c pre-synaptic strength.\nWhen dealing with nonstationary input, as we do in IncSFA due to the simultaneously learning CCIPCA\ncomponents, it is acceptable3 to normalize the magnitude of the slow feature vectors: w \u2190 w /kw k.\ni\n\ni\n\ni\n\nNormalization ensures non-divergence (see Section 4.1). If we normalize, Eq. 20 can be rewritten in the\neven simpler form\n\nw1\n\n\u2190 (1 \u2212 \u03b7)w1 \u2212 \u03b7(\u017c * w1 ) \u017c,\n\n(24)\n\nw1\n\n\u2190 w1 /kw1 k\n\n(25)\n\nan even more basic anti-Hebbian updating with retention rate and learning rate. Now, for all other slow\nfeatures i > 1, the update can be written so sequential addition shows itself to be a lateral competition\nterm:\n\n\uf8eb\n\nwi\n\n\uf8f6\ni\u22121\nX\n\u2190 (1 \u2212 \u03b7)wi \u2212 \u03b7 \uf8ed(\u017c * wi ) \u017c + \u03b3\n(wj * wi )wj \uf8f8 .\n\n(26)\n\nj\n\n4\n\nIncSFA Algorithm\n\nNow we can present the algorithm for a single IncSFA unit. For each time step t = 0, 1, . . .:\n1. Sense: Grab the current raw input as vector x\u0306(t).\n2. Non-Linear Expansion: (optionally) Generate an expanded signal x(t) with I components, e.g. for\na quadratic expansion:\n\nx(t) = [x\u03061 (t), ..., x\u0306d (t), x\u030621 (t), x\u03061 (t)x\u03062 (t), ..., x\u03062d (t)]\n3 Peng: personal communication.\n\n(27)\n\n\fTechnical Report No. IDSIA-07-11\n\n12\n\n3. Mean Estimation and Subtraction: The signal must be centered (zero mean). This can be done\nincrementally if needed. If t = 0, set x\u0304(t) = x(0). Otherwise, update mean vector estimate x\u0304(t):\n\nx\u0304(t) = (1 \u2212 \u03b7) x\u0304(t \u2212 1) + \u03b7 x(t).\n\n(28)\n\n4. Variance Estimation and Normalization: (optionally) The variance of the signal can be normalized. To do so incrementally, the variance estimates \u03c3 = (\u03c31 , ..., \u03c3I ) are updated:\n\n\u03c3i (t) = (1 \u2212 \u03b7) \u03c3i (t \u2212 1) + \u03b7 (xi (t) \u2212 x\u0304i (t))2 , \u2200i\n\n(29)\n\nand normalize each component's variance by dividing by the estimate.\nFor the following steps, u(t) is the processed signal, which has zero mean and unit variance.\n5. CCIPCA: Update estimates of the most significant K principal components of u, where K \u2264 I:\n(a) If t < K, initialize vt (t) = u(t).\n(b) Otherwise do for j = 1, 2, ..., K: Let u1 (t) = u(t); execute CCIPCA equations 10 and 11.\n6. Whitening and Dimensionality Reduction: Let V(t) contain the normed estimates of the K\nprincipal components, ordered by estimated eigenvalue, and create diagonal matrix D(t), where\np\nDi,i = 1/ \u03bbi (t), \u2200i \u2264 K. Then, z(t) = V(t)D(t)u(t).\n7. Derivative Signal: As a forward difference approximation of the derivative, let \u017c(t) = z(t) \u2212 z(t \u2212\n1).\n8. Extract First Principal Component: Use CCIPCA to update the first PC of \u017c (to set sequential\naddition parameter \u03b3(t)).\n9. Update Slowness Measure: The slowness measure of the signal \u017c is computed and updated incrementally to automatically set the learning rate for MCA.\n10. Slow Features: Update estimates of the least significant J PCs of \u017c, where J \u2264 K:\n(a) If t < J, initialize wt = \u017c(t).\n(b) Otherwise, let C1 (t) = \u017c(t)\u017cT (t), and for each i = 1, ..., J, execute incremental MCA updates\nin equation 26.\n11. Normalize Slow Feature Estimates: (optionally, for stability) Each wi \u2190 wi /kwi k.\n12. Output: y(t) = zT (t)W(t) is the SFA output.\n\n\fTechnical Report No. IDSIA-07-11\n\n4.1\n\n13\n\nConvergence of IncSFA\n\nIt is clear that if whitened signal z is drawn from a stationary distribution, the MCA convergence proof (Peng\net al., 2007) applies. But typically the whitening matrix is being learned simultaneously. In this early stage,\nwhile the CCIPCA vectors are learning, care must be taken to ensure that the slow feature estimates will\nnot diverge.\nIt was shown that for any initial vector w(0) within the set S,\n\u001a\nS=\n\nw(t)|w(t) \u2208 RK and kw(t)k2 \u2264\n\n1\n2\u03b7\n\n\u001b\n,\n\n(30)\n\nwill remain in S throughout the dynamics of the MCA updating. kwk must be prevented from getting\ntoo large until the whitening matrix is close to accurate. With respect to lower-order slow features, there\nis additional dependence on the sequential addition technique, parameterized by \u03b3(t) = \u03bb1 (t) + \u000f. This\n\u03b3(t) also needs time to estimate a close value to the first eigenvalue \u03bb1 . Before these estimates become\nreasonably accurate, the input can knock the vector out of S.\nIn practice, normalization of w after each update was found to be the most useful. If kw(0)k = 1\nthen any learning rate \u03b7 \u2264 0.5 ensures non-divergence. Another applicable tactic is clipping. If the signal\nz is thresholded, e.g., from -5 to 5, the potential effect of outliers is controlled. A third tactic is to use a\ngradually increasing MCA learning rate.\nEven if w remains in S, the additional constraint wT (0)w\u2217 6= 0 is needed for the convergence proof.\nBut this is an easy condition to meet, as it is unlikely that any w(t) will be exactly orthogonal to the true\nfeature. In practice, it may be advisable to add a small amount of noise to the MCA update. But we did not\nfind this to be necessary.\nAs for CCIPCA: If the standard conditions on learning rate (Papoulis et al., 1965) (including convergence at zero), the first stage components will converge to the true PCs, leading to a \"nearly-correct\"\nwhitening matrix in reasonable time. So, if x is stationary, the slow feature estimates are likely to become\nquite close to the true slow features in a reasonable amount of updates.\nIn open-ended learning, convergence is not desired. Yet by using a learning rate that is always nonzero,\nthe stability of the algorithm is reduced. This corresponds to the well-known stability-plasticity dilemma (Grossberg, 1980).\n\n\fTechnical Report No. IDSIA-07-11\n\n4.2\n\n14\n\nSetting Learning Rates\n\nIn CCIPCA, if \u03b7 =\n\n1\nt,\n\nEq. 10 will be the most efficient estimator4 of the principal component. But a\n\nlearning rate of 1/t is spatiotemporally optimal if every sample from t = 1, 2, ..., \u221e is drawn from the same\ndistribution, which will not be the case for the lower-order components, and in general for autonomous\nagents. We use an amnesic averaging technique, where the weights of old samples diminuish over time.\nAmnesic averages remain unbiased estimators of the true PCs. For Eq. 10, E[v(n)] \u2192 E[u], as n \u2192 \u221e.\nTo set the CCIPCA learning rate, (and other learning rates, e.g., for the input average x\u0304), we used the\nfollowing three-sectioned amnesic averaging function \u03bc:\n\n\u03bc(t) =\n\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2 0\n\nif t \u2264 t1 ,\n\n\uf8f4\n\uf8f4\n\uf8f3\n\nif t2 < t.\n\nc(t \u2212 t1 )/(t2 \u2212 t1 ) if t1 < t \u2264 t2 ,\nc + (t \u2212 t2 )/r\n\n(31)\n\nEq. 31 combines optimal updating and plasticity for each feature. It uses three stages, defined by points\nt1 and t2 . In the first stage, the learning rate is 1t . In the second, the learning rate is scaled by c to speed up\nlearning of lower-order components. In the third, it changes with t, eventually converging to 1/r.\nUnlike with Peng's MCA, there is no convergence proof for CCIPCA and this type of learning. Instead,\nplasticity introduces an expected error that will not vanish (Weng and Zhang, 2006). To see this, note that\nany component estimate is a weighted sum of all the inputs:\n\nv(t) =\n\nt\nX\n\n\u03c1(t)u(t),\n\n(32)\n\n\u03c4 =1\n\nwhere\n\nPt\n\n\u03c4 =1\n\n\u03c1(t) = 1. Then,\nEkv(t) \u2212 v\u2217 k2 =\n\nt\nX\n\n\u03c12 (t)Ekuk2 =\n\n\u03c4 =1\n\nT\nX\n\n\u03c12 (t) tr(EkuuT k)\n\n(33)\n\nt=1\n\ngives expected estimation error as a function of number of samples T . Eq. 33 can be used to estimate\nthe number of samples needed to get below an acceptable expected error bound, if the signal is stationary.\nOtherwise the process retains the ability to adapt at any future t. This introduces some expected error that is\nlinked to the learning rate into the IncSFA whitening process. Our results show that this is not problematic\nfor many applications, but merely leads to a slight oscillatory behavior around the true features.\nTo prevent divergence while CCIPCA is still learning, we used a slowly rising learning rate for MCA,\nstarting from low \u03b7l at t = 0 and rising to high \u03b7h at t = T ,\n4 The most efficient estimator on average requires the least samples for learning among all unbiased estimators. The sample mean\nis the maximum likelihood estimator (i.e., most efficient unbiased estimator) of the population mean for several distribution types,\ne.g., Gaussian.\n\n\fTechnical Report No. IDSIA-07-11\n\n15\n\n\uf8f1\n\uf8f2 \u03b7 + (\u03b7 \u2212 \u03b7 ) \u2217\nl\nh\nl\n\u03b7(t) =\n\uf8f3 \u03b7h\n\n\u0001\nt 2\nT\n\nif t \u2264 T,\n\n(34)\n\nif T < t.\n\nIdeally, T is a point in time when whitening has stabilized.\nThe upper bound \u03b7b of permissible \u03b7h is defined by the first condition in Eq. 13:\n\u03b7h < \u03b7b =\n\n1\n,\n2\u03bb1\n\n(35)\n\nwhere \u03bb1 is the greatest eigenvalue of the signal. Constant values close to but below the bound can be used\nto achieve faster convergence.\nThe algorithm maintains an incremental estimate of intermediate output slowness. This can be used to\nautomatically adapt the MCA learning rate to changing statistics of the input stream. Since MCA receives\na derivative of the whitened input signal, the greatest eigenvalue \u03bb1 corresponds to the component that\nchanges most rapidly. As a fast approximation, we set\n\u03bb1 \u2248 max \u2206(\u017ci ) = \u2206(\u017cm ),\ni\n\n(36)\n\nwhere zm is the mth dimension of z, which has maximal temporal variation. The \u2206-value (37) measures\ntemporal variation of the signal x(t). It is given by the mean square of that signal's temporal derivative.\nThe smaller the \u2206-value, the slower the variation of the corresponding signal component.\n\u2206(x) = h\u1e8b(t)2 i\n\n(37)\n\nThe \u2206-value is related to Wiskott & Sejnowski's (Wiskott and Sejnowski, 2002) slowness measure of\nthe input signal given by\n\nS(x) =\n\nP p\n\u2206(x)\n2\u03c0\n\n(38)\n\nThe value S for some signal of length P indicates how often a pure sine wave of the same \u2206 value\nwould oscillate.\nNow, from Eq. 36 and Eq. 38, we have\n\n\u2206(\u017cm ) \u221d\n\nS(\u017cm )2\n\n(39)\n\n\u221d\n\nS(\u017cm )2\n\n(40)\n\n\u03bb1\nSince \u03b7b =\n\n1\n2\u03bb1 ,\n\nwe get\n\u03b7b \u221d S(\u017cm )\u22122\n\n(41)\n\n\fTechnical Report No. IDSIA-07-11\n\n16\n\nSelecting \u03b7h close to \u03b7b (see 35), we can write\n\u03b7h = \u03b7b \u2212 \u03c8\n\n(42)\n\n\u03b7h \u221d S(\u017cm )\u22122\n\n(43)\n\nfor some arbitrarily small constant \u03c8.\nFrom Eq. 41 and Eq. 42 we get\n\nWith a working learning rate \u03b7h and the slowness measure estimate for some input, we can automatically adapt \u03b7h for a new signal by tracking how its slowness measure changes.\n\n4.3\n\nDimensionality Reduction Parameter\n\nThe eigenvectors of x associated with the smallest eigenvalues might represent noise dimensions. Instead\nof passing this typically useless information to our second phase, the small eigenvalue directions can be\ndiscarded.\nWhile whitening the I-dimensional input signal, the dimension can be reduced to K \u2264 I. K can be\nautomatically tuned. A method we found to be successful is to set K such that no more than a certain\npercentage of the previously estimated total data variance (the denominator below) is lost. Let \u03b2 be the\nratio of total variance to keep (e.g., 0.95), and compute the smallest K such that\nPK\nk\n\nPI\ni\n\n5\n\n\u03bbk (t)\n\n\u03bbk (t \u2212 1)\n\n> \u03b2.\n\n(44)\n\nExperiments and Results\n\nSome of our experiments are designed to show that IncSFA derives the same features as batch SFA. Others\nshow how IncSFA can work in scenarios where batch SFA is not applicable, and how IncSFA can be\nutilized in high-dimensional video processing applications. Experiments were done either using Python\n(using the MDP toolbox (T. Zito and Berkes, 2008)) or Matlab.\n\n5.1\n\nProof of Concept\n\nAs a basic proof of concept, IncSFA is applied to problem introduced in the original SFA paper (Wiskott\nand Sejnowski, 2002). The input signal is\n\nx\u03061 (t)\n\n= sin(t) + cos(11 t)2 ,\n\n(45)\n\nx\u03062 (t)\n\n= cos(11 t), t \u2208 [0, 2\u03c0],\n\n(46)\n\n\fTechnical Report No. IDSIA-07-11\n\n17\n\nFigure 2: Experiment with a simple non-linear input signal. A learning rate of \u03b7 = 0.08 is used. (a) Input\nSignal (b) Output RMSE plot (c) Batch SFA output of the first slow feature (d)-(f) IncSFA output at t = 2,\n5, 10 epochs. (g) Batch SFA output of the second slow feature (h)-(j) IncSFA output at t = 2, 5, 10 epochs.\nBoth vary quickly over time (see Figure 2(a)). A total of 2, 000 discrete datapoints are used for learning.\nThe slowest feature hidden in the signal is y1 (t) = x\u03061 (t) \u2212 x\u03062 (t)2 = sin(t), and the second is x\u03062 (t)2 .\nBoth BSFA and IncSFA extract these features. Figure 2(b) shows the Root Mean Square Error (RMSE)\nof IncSFA signals compared to the BSFA output, over multiple epochs of training. The RMSE at the end\nof 10 epochs is found to be equal to [0.0360, 0.1078, 0.0377]T .\nFigure 2(c) and (g) shows feature outputs of batch SFA, and (to the right) IncSFA outputs at 2, 5, and\n10 epochs. Figures 2(g)-(j) show this comparison for the second feature.\nThis result show that it is indeed possible to extract multiple slow features in an online way without\nstoring covariance matrices.\n\n5.2\n\nExtraction of a Driving Force from High Dimensional Input\n\nA classic slow feature extraction problem involves uncovering the driving force of a dynamic system hidden\nin a very complex signal. Here, a chaotic time series is derived from a logistic map (T. Zito and Berkes,\n2008):\nx\u0306(t + 1) = (3.6 + 0.13 \u03b3(t))x\u0306(t) (1 \u2212 x(t)),\n\n(47)\n\nwhich is driven by a slowly varying driving force \u03b3(t) made up of two frequency components (5 and 11\n\n\fTechnical Report No. IDSIA-07-11\n\n18\n\nFigure 3: Experiment with a chaotic time series derived from a logistic map. A learning rate of \u03b7 = 0.004\nis used. (a) Driving Force (b) Input (c) Output RMSE plot (d) BSFA output of the slowest feature (e)-(g)\nIncSFA output at t = 15, 30, 60 epochs.\nHz) given by\n\n\u03b3(t) = sin(10\u03c0t) + sin(22\u03c0t).\n\n(48)\n\nTo show the complexity of the signal, figures 3(a) and 3(b) plot the driving force signal \u03b3(t) and the\ngenerated time series x\u0306(t), respectively.\nA total of 1, 000 discrete datapoints are used. The driving force cannot be extracted linearly, so a\nnonlinear expansion is used-temporal in this case. The signal is embedded in 10 dimensional space using\na sliding temporal window of size 10 (the TimeFramesNode from the MDP toolkit (T. Zito and Berkes,\n2008) is used for this). The signal is then spatially quadratically expanded to generate an input signal with\n65 dimensions.\nFigure 3(c) shows the convergence of IncSFA on the BSFA output, Figure 3(d) BSFA output, and\nFigures 3(e)-(g) the outputs of IncSFA at 15, 30 and 60 epochs. The RMSE at 60th epoch is found to be\nequal to 0.0984.\n\n\fTechnical Report No. IDSIA-07-11\n\n19\n\nFigure 4: (a) BSFA output of the first slow feature and (b) the second slow feature (c) IncSFA output of\nthe first slow feature and (d) the second slow feature after 50,000 samples with learning rate \u03b7 = 0.003\n(figures best viewed in color).\n\n5.3\n\nInvariant Spatial Coding from Simple Movement Data\n\nOur simulated agent performs a random walk in a two-dimensional bounded space. Brownian motion is\nused to generate agent trajectories approximately like those of rats. The agent's position p(t) = [x(t), y(t)]\nis updated by a weighted sum of the current velocity and gaussian white noise, with standard deviation\nvr . The momentum term m can assume values between zero and one, so that higher values of m lead to\nsmoother trajectories and more homogeneous sampling of space in less time. Once the agent is predicted\nto cross the spatial boundaries, the current velocity is halved and an alternative random velocity update\nis generated, until a new valid position is reached. Noise variance vr = [3.0, 2.5]T , mass m = 0.75 and\n50, 000 data points are used for generating the training set. A separate test grid dataset samples positions\nand orientations at regular intervals, and is used for evaluation.\nHere is the used movement paradigm:\ncurrV el \u2190 p(t) \u2212 p(t \u2212 1);\nrepeat\nnoise \u2190 GaussianW hiteN oise2d() \u2217 vr ;\np(t + 1) \u2190 p(t) + m \u2217 currV el + (1 \u2212 m) \u2217 noise;\nif not isInsideW alkArea(p(t + 1)) :\ncurrV el \u2190 currV el/2;\nuntil isInsideW alkArea(p(t + 1))\nUnder this movement paradigm (Franzius et al., 2007), SFA yields slow feature outputs in the form of\nhalf-sinusoids, shown in Figure 4. These features collectively encode the agent's x and y position in the\nenvironment. The first slow feature (Figure 4(a)) is invariant to the agent's x position, the second (Figure\n4(b)) to its y position (y axis horizontal). IncSFA's results (Figures 4(c)-(d)) are close to the ones of the\n\n\fTechnical Report No. IDSIA-07-11\n\n20\n\nbatch version, with an RMSE of [0.0536, 0.0914]T .\n\nFeature Adaptation to a Changing Environment\nOutput RMSE\nSlow feature 1\nSlow feature 2\n\n0.4\nRMSE\n\nSlow feature 1\n\n0.3\n0.2\n0.1\n0\n0\n\n20\n\n40\n\n60\n80\nEpochs\n\n(a)\n\n100\n\n120\n\nSlow feature 2\n\n1\nSimilarity to True Feature\n\n0.5\n\nSimilarity to True Feature\n\n5.4\n\n0.8\n0.6\n0.4\n0\n\n20\n\n40\n\n60\nEpochs\n\n80\n\n100\n\n120\n\n1\n0.8\n0.6\n0.4\n0.2\n0\n\n20\n\n40\n\n60\nEpochs\n\n80\n\n100\n\n120\n\n(c)\n\n(b)\n\nFigure 5: (a) RMSE of IncSFA's first two output functions with respect to the true functions for original\nsignal (epochs 1-59), and switched signal (epochs 60-120). (b) Normalized similarity (direction cosine)\nof the first slow feature to the true first slow feature of the current process, over 25 independent runs. (c)\nNormalized similarity of the second incremental slow feature.\nThe purpose of this experiment is to illustrate how IncSFA's features adapt to a sudden shift in the input\nprocess. The input used is the same signal as in Experiment #1, but broken into two partitions. At epoch\n60, the two input lines x1 and x2 are switched such that the x1 signal suddenly carries what x2 used to, and\nvice versa. We wish to show that IncSFA can first learn the slow features of the first partition, then is able\nto adapt to learn the slow features of the second partition.\nThe signal is sampled 500 times per epoch. The CCIPCA learning rate parameters, also used to set the\nlearning rate of the input average x\u0304, were set to t1 = 20, t2 = 200, c = 4, r = 5000. The MCA learning\nrate is \u03b7 = 0.01.\nResults of IncSFA are shown in Fig. 5, demonstrating successful adaptation. To measure convergence\naccuracy, we use the direction cosine (Chatterjee et al., 2000) between the estimated feature w(t) and true\n(unit length) feature w\u2217 ,\n\nDirectionCosine(t) =\n\n|wT (t) * w\u2217 |\n,\nkwT (t)k * kw\u2217 k\n\n(49)\n\nThe direction cosine equals one when the directions align (the feature is correct) and zero when they are\northogonal.\nBSFA results are shown in Fig. 6. The first batch feature catches the meta-dynamics and could actually\nbe used to roughly sense the signal switch. However, the dynamics within each partition are not extracted.\n\n\fTechnical Report No. IDSIA-07-11\n\n21\n\nBatch SFA Feature #1\n\n3\n2\n\n1\nFeature Response\n\nFeature Response\n\nBatch SFA Feature #2\n\n2\n\n1\n0\n\n\u22121\n\n0\n\u22121\n\u22122\n\n\u22122\n\u22123\n2.9\n\n2.95\n\n3\nTime\n\n3.05\n\n\u22123\n2.9\n\n3.1\n4\nx 10\n\n2.95\n\n3\nTime\n\n3.05\n\n3.1\n4\nx 10\n\nFigure 6: Outputs of first two slow features, from epoch 59 through 61, extracted by batch SFA over the\ninput sequence.\n\n5.5\n\nRecovery from Outliers\nIncSFA First Feature's Response\n\n1\n\n3\n\nBSFA First Feature's Response\n\n2\n\n0.5\n\n1\n\n0\n\n0\n\u22120.5\n\n\u22121\n\n\u22121\n0\n\n100\n\n200\n\n300\n\n400\n\n500 \u221220\n\n100\n\n200\n\n300\n\n400\n\n500\n\nFigure 7: First output signals of IncSFA and BSFA on the simple signal with a single outlier.\nAgain, the learning rate setup and basic signal from the previous experiment is used, over 150 epochs,\nwith 500 samples per epoch. A single outlier point is inserted: x1 (100) = x2 (100) = 2000. Figure 7\nshows the first output signal of BSFA and IncSFA, showing that the one outlier point at time 100 (out of\n75,000) is enough to corrupt the first feature of BSFA, whereas IncSFA recovers.\nThe relative lack of sensitivity of IncSFA to outliers is shown in a real-world experiment (Kompella\net al., 2011b), in which a person moves back and forth in front of a stable camera. At only one point in\nthe training sequence, a door in the background is opened, and the BSFA hierarchical network's first slow\nfeature became sensitive to this event. Yet, the AutoIncSFA network's first slow feature encodes the relative\ndistance of the moving interactor.\n\n5.6\n\nHigh-Dimensional Video with Linear IncSFA\n\nIncSFA's scalability is tested with an image sequence of dimension 41\u00d741\u00d73 (color images: see Fig. 8(a)).\nThe agent is located in the middle of a square room with four complex-textured walls. In each episode,\n\n\fTechnical Report No. IDSIA-07-11\n\n22\n\nOu\n\ntpu\n\n\u22120.5\n\n0\n\n0.5\n\nFirst IncSFA Output\n\n(a)\n\n1\n\n\u22121\n\ndI\n\nnc\n\nSF\nA\n\n0\non\n\n\u22121\n\u22121\n\nt\n\n1\n\n0\n\nSe\nc\n\nThird IncSFA Output\n\n1\n\n(b)\n\nFigure 8: (a) Stream of 90 41 \u00d7 41 \u00d7 3 images as the agent completes one turn (360 degrees). Viewed\nrow-wise, left to right. (b) Data projected onto the first three features learned by IncSFA. This gives a\ncompact encoding of the agent's state.\nstarting from a different orientation, the agent rotates slowly (4 degree shifts from one image to the next) by\n360 degrees. At any time, a slight amount of Gaussian noise is added to the image (\u03c3 = 8). The agent has\na video input sensor, and the sequence of image frames with 5, 043 dimensions is fed into a linear IncSFA\ndirectly.\nTo reduce computation time, only the 40 most significant principal components are computed by\nCCIPCA, using learning rate parameters t1 = 20, t2 = 200, c = 4, r = 5000. Computation of the\ncovariance matrix and its full eigendecomposition (including over 5000 eigenvectors and eigenvalues) is\navoided. On the 40 \u00d7 40 whitened difference space, only the first 5 slow features are computed via MCA\nand sequential addition. 500 epochs through the data took approximately 15 minutes using Matlab on a\nmachine with an Intel i3 CPU and 4 GB RAM.\nThe result of projecting the (noise-free) data onto the first three slow features are shown in Fig. 8(b).\nA single linear IncSFA has incrementally compressed this high-dimensional noisy sequence to a nearly\nunambiguous compact form, learning to ignore the details at the pixel level and attend to the true cyclical\nnature underlying the image sequence. A few subsequences have somewhat ambiguous encodings, because\ncertain images associated with slightly different angles are very similar.\n\n\fTechnical Report No. IDSIA-07-11\n\n5.7\n\n23\n\nHigh-Dimensional Video and Episodic Learning\n\n\"Real-world\" learning systems might operate in series of several episodes of interactions with the environment. IncSFA can be readily extended to episodic tasks, with a minor modification: The derivative signal,\nwhich is computed as a difference over a single time step, is simply not computed for the starting sample\nof each episode. The first data point in each episode is used for updating the PCs, but not the slow feature\nvectors.\nHere we present results obtained through a robot's episodic interactions with objects in its field of view.\nTwo plastic cups are placed in the iCub robot's field of view. The robot performs motor babbling in one\njoint using a movement paradigm of Franzius et al. During the course of babbling, it happens to topple the\ncups, in one of two possible orders. The episode ends a short time after it has knocked both down. A new\nepisode begins with the cups upright again and the arm in the beginning position. A total of 50 separate\nepisodes were used as training data.\nLinear IncSFA is used on the entire 80 \u00d7 60 (grayscale) image. Only the 20 most significant principal\ncomponents are computed by CCIPCA, using learning rate parameters t1 = 20, t2 = 200, c = 2, r =\n10000. Only the first 5 slow features are computed via MCA and sequential addition, with learning rate\n0.001. The MCA vectors are normalized after each update during the first 10 epochs, but not thereafter\n(for faster convergence). Each of 25 different trials was over 400 randomly-selected (of the 50 possible)\nepisodes.\nResults are shown in Fig. 9. We measured the slowness of the features on three \"testing\" episodes, after\neach episode of training. The upper left plot shows that all five features get slower over the episodes. After\ntraining completes, we can embed the data in a lower dimension with respect to the learned features. The\nembedding of 20 episodes are shown with respect to the first two PCs as well as the first two slow features.\nSince the cups being toppled or upright are the slow events in the scene, IncSFA's encoding is keyed on\nthe object's state (toppled or upright). PCA does not find such an encoding, being much more sensitive to\nthe arm. Since these events occurs once within each episode, BSFA cannot be used to learn these features.\nFigure 10 shows the average mutual direction cosine between non-identical pairs of slow features, and we\ncan see the features quickly become nearly decorrelated.\nSuch clear object-specific low-dimensional encoding, invariant to the robot's arm position, is useful,\ngreatly facilitating training of a subsequent regressor or reinforcement learner. A video of the experimental\nresult can be found at http://www.idsia.ch/ \u0303luciw/IncSFAArm/IncSFAArm.html.\n\n5.8\n\nHierarchical IncSFA\n\nDeep networks composed of multiple stacked IncSFA nodes, each sensitive to only a small part of the\ninput (i.e., receptive fields), can be used for processing high-dimensional image streams in a biologically\n\n\fTechnical Report No. IDSIA-07-11\n\n24\n\nFigure 9: Experimental result of IncSFA on episodes where the iCub knocks down two cups via motor\nbabbling on one joint. Upper left: The average slowness of the five features at each episode. Upper right:\nafter training, several episodes (each episode is an image sequence where the cups are eventually both\nknocked down) are embedded in the space spanned by the first two PCs. Lower right: the same episodes\nare embedded in the space spanned by the first two slow features. We show some example images and\nwhere they lie in the embedding. The cluster in the upper right (A) represents when both cups are upright.\nWhen the robot knocks down the blue cup first, it moves to the cluster in the upper left (B1). If it instead\nknocks down the brown cup, it moves to the lower right cluster (B2). Once it knocks down both cups, it\nmoves to the lower left area (C).\n\n\fTechnical Report No. IDSIA-07-11\n\n25\n\nFeature Correlation\n\nDecorrelation of the Features\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nEpisode\n\nFigure 10: Average slow feature similarity over episodes.\n\nFigure 11: Example Hierarchical IncSFA Architecture, also showing the structure of an IncSFA node,\nwhich contains a linear IncSFA unit followed by nonlinear expansion followed by another linear IncSFA\nunit.\nplausible way. The computational reason for doing this is that very high-dimensional signals correspond to\nlarge search spaces, and hierarchical setups breaking up the signal can reduce the search burden. And using\nreceptive fields reduces the number of necessary lower-order PCs that have to be computed by CCIPCA,\nwhich should speed the learning.\nFigure 11 shows an example deep network, motivated by the human visual system and based on the one\nspecified by Franzius et al. (Franzius et al., 2007). The network is made up of a converging hierarchy of\n\n\fTechnical Report No. IDSIA-07-11\n\n26\n\nlayers of IncSFA nodes, with overlapping rectangular receptive fields. Each IncSFA node finds the slowest\noutput features from its input within the subspace of all monomials (e.g., of degree two if a quadratic\nexpansion is used) of the node's inputs.\nWe feed IncSFA with images from a high-dimensional video stream generated by the iCub simulator (V. Tikhanoff and Nori, 2008), an OpenGL-based software specifically built for the iCub robot. Our\nexperiment mimics the robot observing a moving interactor agent, which in the simulation takes the form\nof a rectangular flat board moving back and forth in depth over the range [1, 3] (meters) in front of the robot,\nusing a movement paradigm similar to the one discussed in Section 5.3. Figure 12(a) shows the experimental setup in the iCub simulator. Figure 12(b) shows a sample image from the dataset. 20, 000 monocular\nimages are captured from the robot's left eye and downsampled to 83\u00d7100 pixels (input dimension of\n8, 300).\nA three-layer IncSFA network is used to encode the images. Each SFA node operates on a spatial\nreceptive field of the layer below. The first layer uses 15 \u00d7 19 nodes, each with 10 \u00d7 10 image patch\nreceptive field and a 5 pixel overlap. Each node on this layer develops 10 slow features. The second layer\nuses 4 \u00d7 5 nodes, each having a 5 \u00d7 5 receptive field, and developing 5 slow features. The third layer\nuses two nodes, one sensitive to the top half, the other sensitive to the bottom half (5 slow features). The\nforth layer uses a single node and a single slow feature. The network is trained layer-wise from bottom to\ntop, with the lower layers frozen once a new layer begins its training. The CCIPCA output of all nodes\nis clipped to [\u22125, 5], to avoid any outliers that may arise due to close-to-zero eigenvalues in some of the\nreceptive fields that contain unchanging stimuli. Each IncSFA node is trained individually, that is, there is\nno weight sharing among nodes.\nFor comparison, a batch SFA hierarchical network was also trained on this data. Figures 12 show BSFA\nand IncSFA outputs. The expected output is of the form of a sinusoid extending over the range of board\npositions. IncSFA gives a slightly noisy output, probably due to the constant dimensionality reduction\nvalue for all units in each layer of the network, selected to maintain a consistent input structure for the\nsubsequent layer; hence some units with eigenvectors corresponding to very small eigenvalues emerge in\nthe first stage, with receptive fields observing comparatively few input changes, thus slightly corrupting the\nwhitening result, and adding small fluctuations to the overall result.\nFinally, we evaluate how well the IncSFA feature codes for distance. A supervised quadratic regressor\nis trained with ground truth labels on 20% of the dataset, and tested on the other 80%, to measure the\nquality of features for some classifier or reinforcement learner using them (see RMSE plot). Hierarchical\nIncSFA derives the driving forces from a complex and continuous input video stream in a completely online\nand unsupervised manner.\n\n\fTechnical Report No. IDSIA-07-11\n\n27\n\nFigure 12: (a) Experimental Setup: iCub Simulator (b) Sample image from the input dataset (c) Batch-SFA\noutput (d) IncSFA output (\u03b7 = 0.005)\n\n6\n\nConclusions\n\nOur novel Incremental Slow Feature Analysis technique solves SFA problems incrementally without storing covariance matrices. IncSFA's covariance-free Hebbian and anti-Hebbian updates add biological plausibility to SFA itself. While batch SFA cannot handle certain open-ended uncontrolled settings, IncSFA\ncan. This makes it a promising tool for learning autonomous robots. Future work will study online learning\ncontrollers whose experiments actively create data exhibiting novel but learnable regularities measured by\nimprovements of emerging slow features, in line with the formal theory of curiosity (Schmidhuber, 2010).\n\n\fTechnical Report No. IDSIA-07-11\n\n28\n\nAcknowledgments\nThe experimental paradigm used for the distance-encoding high-dimensional video experiment was first\ndeveloped by the first author under the supervision of Mathias Franzius, at the Honda Research Institute\nEurope. We would like to acknowledge Dr. Franzius for his contributions in this regard. We would also\nlike to acknowledge IDSIA researchers Alexander Forster and Kail Frank for enabling the experiment with\nthe iCub robot. Thanks to Marijn Stollenga and Sohrob Kazerounian for their comments on an earlier draft\nof the paper. This work was funded through the 7th framework program of the EU under grants #231722\n(IM-Clever project) and #270247 (NeuralDynamics project), and by Swiss National Science Foundation\ngrant CRSIKO-122697 (Sinergia project).\n\nReferences\nAbut, H., editor (1990). Vector Quantization. IEEE Press, Piscataway, NJ.\nAmari, S. (1977). Neural theory of association and concept-formation. Biological Cybernetics, 26(3):175\u2013\n185.\nBarlow, H. (2001). Redundancy reduction revisited. Network: Computation in Neural Systems, 12(3):241\u2013\n253.\nBergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex cell-like networks.\nAdvances in Neural Information Processing Systems 22, pages 99\u2013107.\nChatterjee, C., Kang, Z., and Roychowdhury, V. (2000). Algorithms for accelerated convergence of adaptive pca. IEEE Transactions on Neural Networks, 11(2):338\u2013355.\nChen, T., Amari, S., and Lin, Q. (1998). A unified algorithm for principal and minor components extraction.\nNeural Networks, 11(3):385\u2013390.\nChen, T., Amari, S., and Murata, N. (2001). Sequential extraction of minor components. Neural Processing\nLetters, 13(3):195\u2013201.\nComon, P. (1994). Independent component analysis, A new concept? Signal Processing, 36:287\u2013314.\nDayan, P. and Abbott, L. (2001). Theoretical neuroscience: Computational and mathematical modeling of\nneural systems.\nDoersch, C., Lee, T., Huang, G., and Miller, E. Temporal continuity learning for convolutional deep belief\nnetworks.\n\n\fTechnical Report No. IDSIA-07-11\n\n29\n\nF\u00f6ldi\u00e1k, P. (1991). Learning invariance from transformation sequences. Neural Computation, 3(2):194\u2013\n200.\nFranzius, M., Sprekeler, H., and Wiskott, L. (2007). Slowness and sparseness lead to place, head-direction,\nand spatial-view cells. PLoS Computational Biology, 3(8):e166.\nGisslen, L., Luciw, M., Graziano, V., and Schmidhuber, J. (2011). Sequential constant size compressors\nfor reinforcement learning. In Fourth Conference on Artificial General Intelligence (AGI).\nGrossberg, S. (1980). How does a brain build a cognitive code?. Psychological Review, 87(1):1.\nHafting, T., Fyhn, M., Molden, S., Moser, M., and Moser, E. (2005). Microstructure of a spatial map in the\nentorhinal cortex. Nature, 7052:801.\nHinton, G. (1989). Connectionist learning procedures. Artificial Intelligence, 40(1-3):185\u2013234.\nHinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Comp.,\n14(8):1771\u20131800.\nJenkins, O. and Matari\u0107, M. (2004). A spatio-temporal extension to isomap nonlinear dimension reduction.\nIn Proceedings of the twenty-first international conference on Machine learning, page 56. ACM.\nJolliffe, I. T. (1986). Principal Component Analysis. Springer-Verlag, New York.\nKlapper-Rybicka, M., Schraudolph, N. N., and Schmidhuber, J. (2001). Unsupervised learning in LSTM\nrecurrent neural networks. In Lecture Notes on Comp. Sci. 2130, Proc. Intl. Conf. on Artificial Neural\nNetworks (ICANN-2001), pages 684\u2013691. Springer: Berlin, Heidelberg.\nKohonen, T. (2001). Self-Organizing Maps. Springer-Verlag, Berlin, 3rd edition.\nKompella, V., Luciw, M., and Schmidhuber, J. (2011a). Incremental slow feature analysis. In International\nJoint Conference of Artificial Intelligence.\nKompella, V. R., Pape, L., Masci, J., Frank, M., and Schmidhuber, J. (2011b). Autoincsfa and vision-based\ndevelopmental learning for humanoid robots. In IEEE-RAS International Conference on Humanoid\nRobots, Bled, Slovenia.\nKrasulina, T. (1970). Method of stochastic approximation in the determination of the largest eigenvalue of\nthe mathematical expectation of random matrices. Automat. Remote Contr, 2:215\u2013221.\nKreyszig, E. (1988). Advanced engineering mathematics. Wiley, New York.\nLee, D. and Seung, H. (1999). Learning the parts of objects by non-negative matrix factorization. Nature,\n401(6755):788\u2013791.\n\n\fTechnical Report No. IDSIA-07-11\n\n30\n\nLee, H., Largman, Y., Pham, P., and Ng, A. (2010). Unsupervised feature learning for audio classification\nusing convolutional deep belief networks. Advances in neural information processing systems, 22:1096\u2013\n1104.\nLegenstein, R., Wilbert, N., and Wiskott, L. (2010). Reinforcement learning on slow features of highdimensional input streams. PLoS Computational Biology, 6(8).\nLindst\u00e4dt, S. (1993). Comparison of two unsupervised neural network models for redundancy reduction.\nIn Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S., editors, Proc. of the\n1993 Connectionist Models Summer School, pages 308\u2013315. Hillsdale, NJ: Erlbaum Associates.\nMitchison, G. (1991). Removing time variation with the anti-hebbian differential synapse. Neural Computation, 3(3):312\u2013320.\nOja, E. (1982). Simplified neuron model as a principal component analyzer. Journal of mathematical\nbiology, 15(3):267\u2013273.\nOja, E. (1985). On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a\nrandom matrix. Journal of Mathematical Analysis and Applications, 106:69\u201384.\nOja, E. (1992). Principal components, minor components, and linear neural networks. Neural Networks,\n5(6):927\u2013935.\nO'Keefe, J. and Dostrovsky, J. (1971). The hippocampus as a spatial map: Preliminary evidence from unit\nactivity in the freely-moving rat. Brain research.\nPapoulis, A., Pillai, S., and Unnikrishna, S. (1965). Probability, random variables, and stochastic processes, volume 196. McGraw-hill New York.\nPeng, D. and Yi, Z. (2006). A new algorithm for sequential minor component analysis. International\nJournal of Computational Intelligence Research, 2(2):207\u2013215.\nPeng, D., Yi, Z., and Luo, W. (2007). Convergence analysis of a simple minor component analysis algorithm. Neural Networks, 20(7):842\u2013850.\nRolls, E. (1999). Spatial view cells and the representation of place in the primate hippocampus. Hippocampus, 9(4):467\u2013480.\nSanger, T. (1989). Optimal unsupervised learning in a single-layer linear feedforward neural network.\nNeural networks, 2(6):459\u2013473.\n\n\fTechnical Report No. IDSIA-07-11\n\n31\n\nSchmidhuber, J. (1992a). Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234\u2013242.\nSchmidhuber, J. (1992b). Learning factorial codes by predictability minimization. Neural Computation,\n4(6):863\u2013879.\nSchmidhuber, J. (1992c). Learning unambiguous reduced sequence descriptions. In Moody, J. E., Hanson,\nS. J., and Lippman, R. P., editors, Advances in Neural Information Processing Systems 4 (NIPS 4), pages\n291\u2013298. Morgan Kaufmann.\nSchmidhuber, J. (1999). Neural predictors for detecting and removing redundant information. In Cruse,\nH., Dean, J., and Ritter, H., editors, Adaptive Behavior and Learning. Kluwer.\nSchmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010). IEEE\nTransactions on Autonomous Mental Development, 2(3):230\u2013247.\nSprekeler, H., Zito, T., and Wiskott, L. (2010). An extension of slow feature analysis for nonlinear blind\nsource separation.\nT. Zito, N. Wilbert, L. W. and Berkes, P. (2008). Modular toolkit for data processing (mdp): a python data\nprocessing framework. Frontiers in Neuroinformatics, 2.\nTaube, J., Muller, R., and Ranck, J. (1990). Head-direction cells recorded from the postsubiculum in freely\nmoving rats. i. description and quantitative analysis. The Journal of Neuroscience, 10(2):420.\nV. Tikhanoff, A. Cangelosi, P. F. G. M. L. N. and Nori, F. (2008). An open-source simulator for cognitive\nrobotics research: The prototype of the icub humanoid robot simulator.\nWang, L. and Karhunen, J. (1996). A unified neural bigradient algorithm for robust pca and mca. International journal of neural systems, 7(1):53.\nWeng, J. and Zhang, N. (2006). Optimal in-place learning and the lobe component analysis. In Neural\nNetworks, 2006. IJCNN'06. International Joint Conference on, pages 3887\u20133894. IEEE.\nWeng, J., Zhang, Y., and Hwang, W. (2003). Candid covariance-free incremental principal component\nanalysis. 25(8):1034\u20131040.\nWiskott, L. (2003). Estimating driving forces of nonstationary time series with slow feature analysis. Arxiv\npreprint cond-mat/0312317.\nWiskott, L., Berkes, P., Franzius, M., Sprekeler, H., and Wilbert, N. (2011). Slow feature analysis. Scholarpedia, 6(4):5282.\n\n\fTechnical Report No. IDSIA-07-11\n\n32\n\nWiskott, L. and Sejnowski, T. (2002). Slow feature analysis: Unsupervised learning of invariances. Neural\nComputation, 14(4):715\u2013770.\nXu, L., Oja, E., and Suen, C. (1992). Modified hebbian learning for curve and surface fitting. Neural\nNetworks, 5(3):441\u2013457.\nZhang, Y. and Weng, J. (2001). Convergence analysis of complementary candid incremental principal\ncomponent analysis. Michigan State University.\n\n\f"}