{"id": "http://arxiv.org/abs/1106.0898v1", "guidislink": true, "updated": "2011-06-05T13:35:24Z", "updated_parsed": [2011, 6, 5, 13, 35, 24, 6, 156, 0], "published": "2011-06-05T13:35:24Z", "published_parsed": [2011, 6, 5, 13, 35, 24, 6, 156, 0], "title": "Hessian-Free Methods for Checking the Second-Order Sufficient Conditions\n  in Equality-Constrained Optimization and Equilibrium Problems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1106.2116%2C1106.2194%2C1106.3950%2C1106.0957%2C1106.1305%2C1106.0840%2C1106.1100%2C1106.4152%2C1106.2650%2C1106.0883%2C1106.1497%2C1106.5458%2C1106.5116%2C1106.1557%2C1106.3740%2C1106.4628%2C1106.1463%2C1106.3393%2C1106.0690%2C1106.0693%2C1106.4550%2C1106.5078%2C1106.2407%2C1106.3491%2C1106.4930%2C1106.0113%2C1106.4046%2C1106.3099%2C1106.5448%2C1106.3373%2C1106.5829%2C1106.3178%2C1106.1278%2C1106.4687%2C1106.3139%2C1106.0247%2C1106.5335%2C1106.1595%2C1106.2886%2C1106.1153%2C1106.1038%2C1106.4125%2C1106.2435%2C1106.3484%2C1106.3510%2C1106.1617%2C1106.2591%2C1106.0078%2C1106.5522%2C1106.5111%2C1106.5051%2C1106.2251%2C1106.1279%2C1106.5177%2C1106.5379%2C1106.1342%2C1106.1258%2C1106.0138%2C1106.1733%2C1106.3242%2C1106.2406%2C1106.4404%2C1106.4430%2C1106.3631%2C1106.1240%2C1106.1942%2C1106.4611%2C1106.3993%2C1106.4763%2C1106.1041%2C1106.4265%2C1106.4327%2C1106.5271%2C1106.5574%2C1106.3098%2C1106.3894%2C1106.6269%2C1106.1124%2C1106.4113%2C1106.4971%2C1106.1158%2C1106.4922%2C1106.0865%2C1106.0536%2C1106.2862%2C1106.1048%2C1106.5639%2C1106.4958%2C1106.2646%2C1106.3007%2C1106.1994%2C1106.1404%2C1106.0473%2C1106.0058%2C1106.3374%2C1106.2044%2C1106.4506%2C1106.2786%2C1106.0898%2C1106.5421%2C1106.4212&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Hessian-Free Methods for Checking the Second-Order Sufficient Conditions\n  in Equality-Constrained Optimization and Equilibrium Problems"}, "summary": "Verifying the Second-Order Sufficient Condition (SOSC), thus ensuring a\nstationary point locally minimizes a given objective function (subject to\ncertain constraints), is an essential component of non-convex computational\noptimization and equilibrium programming. This article proposes three new\n\"Hessian-free\" tests of the SOSC that can be implemented efficiently with\ngradient evaluations alone and reveal feasible directions of negative curvature\nwhen the SOSC fails. The Bordered Hessian Test and a Matrix Inertia test, two\nclassical tests of the SOSC, require explicit knowledge of the Hessian of the\nLagrangian and do not reveal feasible directions of negative curvature should\nthe SOSC fail. Computational comparisons of the new methods with classical\ntests demonstrate the relative efficiency of these new algorithms and the need\nfor careful study of false negatives resulting from accumulation of round-off\nerrors.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1106.2116%2C1106.2194%2C1106.3950%2C1106.0957%2C1106.1305%2C1106.0840%2C1106.1100%2C1106.4152%2C1106.2650%2C1106.0883%2C1106.1497%2C1106.5458%2C1106.5116%2C1106.1557%2C1106.3740%2C1106.4628%2C1106.1463%2C1106.3393%2C1106.0690%2C1106.0693%2C1106.4550%2C1106.5078%2C1106.2407%2C1106.3491%2C1106.4930%2C1106.0113%2C1106.4046%2C1106.3099%2C1106.5448%2C1106.3373%2C1106.5829%2C1106.3178%2C1106.1278%2C1106.4687%2C1106.3139%2C1106.0247%2C1106.5335%2C1106.1595%2C1106.2886%2C1106.1153%2C1106.1038%2C1106.4125%2C1106.2435%2C1106.3484%2C1106.3510%2C1106.1617%2C1106.2591%2C1106.0078%2C1106.5522%2C1106.5111%2C1106.5051%2C1106.2251%2C1106.1279%2C1106.5177%2C1106.5379%2C1106.1342%2C1106.1258%2C1106.0138%2C1106.1733%2C1106.3242%2C1106.2406%2C1106.4404%2C1106.4430%2C1106.3631%2C1106.1240%2C1106.1942%2C1106.4611%2C1106.3993%2C1106.4763%2C1106.1041%2C1106.4265%2C1106.4327%2C1106.5271%2C1106.5574%2C1106.3098%2C1106.3894%2C1106.6269%2C1106.1124%2C1106.4113%2C1106.4971%2C1106.1158%2C1106.4922%2C1106.0865%2C1106.0536%2C1106.2862%2C1106.1048%2C1106.5639%2C1106.4958%2C1106.2646%2C1106.3007%2C1106.1994%2C1106.1404%2C1106.0473%2C1106.0058%2C1106.3374%2C1106.2044%2C1106.4506%2C1106.2786%2C1106.0898%2C1106.5421%2C1106.4212&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Verifying the Second-Order Sufficient Condition (SOSC), thus ensuring a\nstationary point locally minimizes a given objective function (subject to\ncertain constraints), is an essential component of non-convex computational\noptimization and equilibrium programming. This article proposes three new\n\"Hessian-free\" tests of the SOSC that can be implemented efficiently with\ngradient evaluations alone and reveal feasible directions of negative curvature\nwhen the SOSC fails. The Bordered Hessian Test and a Matrix Inertia test, two\nclassical tests of the SOSC, require explicit knowledge of the Hessian of the\nLagrangian and do not reveal feasible directions of negative curvature should\nthe SOSC fail. Computational comparisons of the new methods with classical\ntests demonstrate the relative efficiency of these new algorithms and the need\nfor careful study of false negatives resulting from accumulation of round-off\nerrors."}, "authors": ["W. Ross Morrow"], "author_detail": {"name": "W. Ross Morrow"}, "author": "W. Ross Morrow", "links": [{"href": "http://arxiv.org/abs/1106.0898v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1106.0898v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "65, 90, 91", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1106.0898v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1106.0898v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "HESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER\nSUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED\nOPTIMIZATION AND EQUILIBRIUM PROBLEMS\n\narXiv:1106.0898v1 [math.OC] 5 Jun 2011\n\nW. ROSS MORROW\n\nAbstract. Verifying the Second-Order Sufficient Condition (SOSC), thus ensuring a stationary point locally minimizes a given objective function (subject to certain constraints),\nis an essential component of non-convex computational optimization and equilibrium programming. This article proposes three new \"Hessian-free\" tests of the SOSC that can be\nimplemented efficiently with gradient evaluations alone and reveal feasible directions of\nnegative curvature when the SOSC fails. The Bordered Hessian Test and a Matrix Inertia test, two classical tests of the SOSC, require explicit knowledge of the Hessian of the\nLagrangian and do not reveal feasible directions of negative curvature should the SOSC\nfail. Computational comparisons of the new methods with classical tests demonstrate the\nrelative efficiency of these new algorithms and the need for careful study of false negatives\nresulting from accumulation of round-off errors.\n\n1. INTRODUCTION\nVerifying the Second-Order Sufficient Condition (SOSC) to certify local optimality of\npoints computed by optimization software is an important but underdeveloped component\nof computational optimization. Existing optimization solvers compute points satisfying\na First-Order or Second-Order Necessary Condition (FONC/SONC), typically without\nchecking the corresponding SOSC. As a result, there remain cases in which computed\npoints are not optimizers such as, for example, Example 1.\nWhile verifying the SOSC is important for non-convex optimization, verifying the SOSC\nis essential for equilibrium models increasingly being employed in Economics, Operations\nResearch, and Engineering. While much of the theory of computing equilibria relies on\nassumptions of convexity to ensure the first-order conditions imply optimality rather than\njust stationarity [26], examples of non-convex games are rapidly appearing in important\napplications. So long as equilibrium programming methods for general, non-convex games\nare restricted to solving the combined first-order conditions, algorithms can compute simultaneous first-order points but cannot distinguish equilibria from other types of first-order\npoints; see, e.g., Example 2. Verifying the SOSC is thus fundamental to properly computing\nequilibria.\nAt least two ways to test the SOSC have been known for some time. The classical\n\"Bordered Hessian Test\" (BHT) [41, 58, 69, 57] can, in principle, be used to verify or reject\nDate: June 7, 2011.\n1\n\n\f2\n\nW. ROSS MORROW\n\nthe SOSC at points computed by optimization software. Computationally implementing\nthe BHT relies on a set of nested LU factorizations that can be efficiently taken with\nLU factorization updating. Determining the inertia of a \"KKT matrix\" [46, 68] is another\n\"classical\" way to verify or reject the SOSC. Efficient implementations of this \"Inertia test\"\nare available in the form of existing factorization packages that can compute matrix inertia,\nand may be easily integrated into SQP solvers for constrained optimization problems.\nBoth of these \"classical\" tests require an explicit representation of the Hessian of the\nLagrangian, which may not be available. Popular optimization solvers including MINOS,\nLANCELOT, SNOPT, KNITRO, and matlab can utilize gradient evaluations alone through quasiNewton updates, finite-differences, or automatic differentiation [68]. The Hessian of the\nLagrangian will thus not be available for verifying the SOSC when gradient evaluations\nalone are used. \"Hessian-free\" algorithms requiring only Hessian-vector products [68] could\nseamlessly integrate SOSC checks with optimization and equilibrium solvers that do not\nrequire users to provide formulas for the second derivatives of the objective or the constraint functions. Such algorithms can fully exploit sparsity patterns in the objective and\nconstraint functions, as well as be implemented using directional finite differences to approximate Hessian-vector products.\nThis article presents three new Hessian-free algorithms for verifying or rejecting the\nSOSC at first-order points in smooth equality-constrained optimization or equilibrium\nproblems. The first algorithm is based on Cholesky factorization, the most efficient and\nstable method for testing Hessian positive-definiteness without constraints [44, 49]. The\nsecond algorithm is based on an \"oblique\" Gram-Schmidt orthogonalization, and the third\nalgorithm is a modification of the Projected Conjugate Gradient algorithm developed for\nconstrained quadratic programming [47, 68]. By being Hessian-free, these algorithms can\ntake full advantage of sparsity patterns in the Hessian of the Lagrangian when it is known\nand may significantly reduce the number of gradient evaluations necessary to verify or\nreject the SOSC when the Hessian is not available at all.\nAnother important feature of an algorithm to verify the SOSC is how easily a feasible\ndirection of negative curvature can be computed should the SOSC fail. Certain optimization algorithms use feasible directions of negative curvature to promote global convergence\nto second-order necessary points; see, e.g., [18]. Recovery of a feasible direction of negative curvature when an the SOSC fails enables \"warm restarts\" of optimization algorithms\nthat may not already take advantage of second-order information. The classical BHT and\nInertia tests do not provide an obvious path to computing such directions when the SOSC\nfails. In contrast, the Hessian-free algorithms proposed here make computation of such\ndirections straightforward when the SOSC fails. Indeed, two of the Hessian-free algorithms\npresented here fail precisely by finding such directions.\nFinally, useful algorithms for verifying the SOSC should not return false-positives or\nfalse-negatives due to the accumulation of round-off errors. Unfortunately the numerical\naccuracy of verifying the SOSC with any approach has not yet been addressed. Section\n5 provides computational comparisons of the different algorithms including an example\nthat illustrates significant potential for erroneous results due to round-off errors when the\nconstraint gradients are very nearly linearly dependent. In particular, no method can be\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\nconsidered accurate for certain problems with very nearly linearly independent constraint\ngradients, even with relatively few variables and constraints.\n2. OPTIMIZATION AND EQUILIBRIUM PROBLEMS\n2.1. Optimization Problems. This article considers the equality-constrained, continuous variable optimization problem\nminimize\n(1)\n\nwith respect to\nsubject to\n\nf (x)\nx \u2208 RN\nc(x) = 0\n\nwhere f : RN \u2192 R, c : RN \u2192 RM , and N, M \u2208 N, M < N . The objective (f ) and\nconstraints (c) are nonlinear and twice continuously differentiable functions of N variables\nx = (x1 , . . . , xN ). The component functions of c are denoted by ci , for i \u2208 {1, . . . , M }.\nTheorem 1. (Optimality Conditions [68, Theorems 12.1, 12.5, 12.6]) Consider Problem\n(1), and assume the Linear Independence Constraint Qualification (LICQ) is satisfied [68,\nDef. 12.4]. Define the \"Lagrangian\" L(x, \u03bb) = f (x) \u2212 \u03bb> c(x), and let A(x) = Dc(x) \u2208\nRM \u00d7N denote the Jacobian matrix of the constraint function evaluated at x \u2208 RN .\nFONC: If x\u2217 \u2208 RN is a local solution to Problem (1), then there exists some \u03bb\u2217 \u2208 RM\nP\n\u2217\n\u2217\n\u2217\nsuch that \u2207L(x\u2217 , \u03bb\u2217 ) = 0. That is, \u2207f (x\u2217 ) = M\ni=1 \u03bbi \u2207ci (x ) and c(x ) = 0.\nSONC: Moreover, w> H(x\u2217 , \u03bb\u2217 ) w \u2265 0 for all w \u2208 C(x\u2217 ) = { h : A(x\u2217 )h = 0 } where\nH(x, \u03bb) denotes the Hessian of the Lagrangian (or simply \"Hessian\"):\nH(x, \u03bb) = \u22072x,x L(x, \u03bb) = \u22072x,x f (x) \u2212\n\nM\nX\n\n\u03bbm \u22072x,x ci (x).\n\ni=1\n\nSOSC: On the other hand, suppose x\u2217 \u2208 RN and \u03bb\u2217 \u2208 RM satisfy the FONC and\nw> H(x\u2217 , \u03bb\u2217 ) w > 0 for all w \u2208 C(x\u2217 ). Then x\u2217 is an isolated local solution to Problem (1).\nNote that this SOSC also applies to inequality constrained optimization problems at\nstrictly complementary stationary points [68, Def. 12.5], and as the continuous part of the\noptimality conditions for mixed-integer nonlinear optimization problems. The remainder\nof this article assumes that x\u2217 and \u03bb\u2217 satisfy the FONC and denotes H(x\u2217 , \u03bb\u2217 ), A(x\u2217 ),\nand C(x\u2217 ) by simply H, A, and C, respectively. Below the SOSC is denoted compactly\nusing the symbol H \u001fC 0.\nMany existing codes for solving problem (1) solve a variant of the FONC without verifying the SOSC at computed points [65]. Sequential Quadratic Programming (SQP) methods\n[68, Chapter 18] solve a sequence of local quadratic model problems that, in the equalityconstrained case, corresponds to applying Newton's method to solve the FONC. SQP\nmethods are currently implemented in NPSOL [39], SNOPT [37], filterSQP [30, 29, 31],\nKNITRO [13, 15], and matlab [48, 73, 72, 40, 28]. Augmented Lagrangian methods [68,\nChapter 17], [7] \"penalize\" the Lagrangian with a measure of the constraint violation and\n\n\f4\n\nW. ROSS MORROW\n\nsolve the FONC for a sequence of penalized problems with a variant of Newton's method.\nThis method is currently implemented in the MINOS [62, 63, 64] and LANCELOT [17, 16]\ncodes. Like Augmented Lagrangian methods, Interior-Point methods [68, Chapter 19] for\ninequality-constrained problems solve the FONC for a sequence of approximate problems\nusing a variant of Newton's method. KNITRO [15], LOQO [89, 88], IPOPT [90], and matlab\n[12, 14] currently contain implementations of interior-point methods.\nObtaining a solution to the FONC is not sufficient to declare the computed point a local\nsolution to (1), as shown in Example 1 below. In practice, sufficient decrease conditions\non a merit function (or a filter mechanism) bias existing solvers towards computing constrained minimizers of f [20, 68, 78]. Indeed, this bias towards optimizers is certainly one\nreason separate codes for verifying the SOSC do not currently exist. Sufficient decrease\nconditions certainly rule out some types of convergence: for example, these conditions rule\nout converge to local constrained maximizers of f . However, algorithms with sufficient\ndecrease conditions can still converge to saddle points, as in Example 1.\nExample 1. Consider minimizing f (x) = x3 over R. x\u2217 = 0 satisfies the FONC (f 0 (0) =\n0) and the SONC (f 00 (0) = 0), but does not satisfy the SOSC. Indeed, f is not locally\nminimized at x\u2217 = 0, or at any other finite x. Applying SQP [68, Chapter 18.1] to this\nproblem starting at any x0 > 0 generates the sequence xn = xn\u22121 /2 = x0 /2n \u2192 0 as\nn \u2192 \u221e. Moreover this sequence satisfies the Armijo Condition [68, Eqn. 3.4], a popular\nsufficient decrease condition. \u0004\nThere also exist \"second-order\" algorithms that converge only to points at which the\nHessian is positive semi-definite [18]. Such algorithms make use of feasible directions of\nnegative curvature\u2212vectors d \u2208 C satisfying d> Hd < 0\u2212to promote converge to SONC\npoints. Any such algorithm must (periodically) compute a direction of negative curvature\nand thus, by definition, contains a check of the SOSC: should no direction be found, the\nSOSC must hold. While this is certainly sufficient when the Hessian is known explicitly,\nmany practical applications do not have such knowledge. Implementations of secondorder algorithms that rely on quasi-Newton approximations to the Hessian only determine\nwhether there is a direction of negative curvature for the approximation, rather than the\ntrue Hessian, and thus cannot by themselves verify the SOSC. Many other large-scale\nHessian-free codes apply Conjugate-Gradient (CG) type iterations to solve constrained\nquadratic subproblems [47]. Section 4.3 below, however, demonstrates that convergence of\nCG methods alone is insufficient to verify the SOSC and thus it is conceivable that CG\nmethods could miss indefiniteness in H (over C) in some exceptional circumstances. A\npost-convergence verification of the SOSC at computed points appears to be required for\nproper application of Hessian-free methods for large-scale optimization.\n\n2.2. Equilibrium Problems. Verifying the SOSC is also currently vital for solving nonlinear, non-convex equality constrained (generalized Nash) equilibrium problems; that is,\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\ncollections of K \u2208 {2, 3, . . . } coupled optimization problems:\nminimize\n(2)\n\nwith respect to\nsubject to\n\nfk (x1 , . . . , xK )\nxk \u2208 RNk\nck (x1 , . . . , xK ) = 0\n\nwhere fk : RN \u2192 R, ck : RN \u2192 RMk , Nk , Mk \u2208 N, Mk < Nk , and N = N1 + * * * + NK\nfor all k \u2208 {1, . . . , K}. A (local) equilibrium is a point (x\u22171 , . . . , x\u2217K ) \u2208 RN such that x\u2217k \u2208\nRNk (locally) solves Problem (2) for all k. Originating in game theory, such equilibrium\nmodels have been used by Economists and Operations Researchers to study electric power\n[50, 51, 52, 24] and other energy sectors [35, 34], new and used vehicles [5, 42, 43, 84,\n71, 55, 6, 3, 53], entertainment goods [45], and food services [8, 66, 67, 83, 85]. Recent\n\"market-systems\" research in engineering design is also applying the equilibrium framework\n[59, 82, 79, 80, 81, 33]. See [26] for a review of similar applications and economic history.\nFirst and second order optimality conditions can be employed in the computation of equilibria. In particular, a FONC for a (local) equilibrium follows from combining the FONC\nfor each underlying optimization problem, resulting in a single nonlinear system (e.g., [60])\nor, more generally, Nonlinear Complementarity Problem (NCP) (e.g., [27, 51]); see [26].\n\"Simultaneously stationary\" points that solve combined FONC can often be computed using standard methods for nonlinear systems or NCPs such as trust-region Newton methods\n[20, 18], non-smooth Newton methods [74, 21, 61], or semi-smooth methods [61]; see, e.g.\n[2, 51, 60]. However the sufficient decrease conditions that enforce convergence to minimizers or saddle points in optimization problems now apply to a residual norm instead of\nan objective function, and thus cannot preferentially select equilibria over non-equilibrium\nstationary points.\nExample 2. Morrow & Skerlos [60, Example 10] compute equilibrium prices for a twofirm market with heterogeneous consumers. Both firms set the price for a single \"branded\"\nproduct whose only non-price attribute is \"brand\". There are three types of consumers,\ntwo of which are brand-loyal and the other is brand-indifferent. Demand within each type\nis modeled using a Logit model [86, Chapter 3]. The resulting optimal pricing problem is\nan unconstrained nonlinear optimization problem with a non-convex, multi-modal objective. The combined FONC for equilibrium has nine solutions, with only four of these nine\nfirst-order points locally maximizing both firms' profits. Newton's method applied to the\ncombined FONC cannot distinguish between any of these nine points; any of the five spurious \"solutions\" could be mistaken for equilibria if the SOSC were not verified. Again, no\ngeneral, globally-convergent method for computing only equilibria in this type of non-convex\ngame is currently known. \u0004\nSome studies compute equilibria using \"sequential optimization\", \"tattonement\", or\n\"Jacobi/Gauss-Seidel\" methods [59, 51]; see the discussion of \"practitioners methods\" in\n[26] for algorithmic details. In general, sequential optimization should enable some \"filtering\" of simultaneously stationary points, as the use of optimization algorithms would\n\n\f6\n\nW. ROSS MORROW\n\navoid convergence to simultaneously stationary points that are minimizers of some objectives and maximizers of others. However, there are no existing results guaranteeing\nany sort of convergence behavior from sequential optimization methods in either convex\nor non-convex equilibrium problems [26]. Because there do exist algorithms proven to\nconverge to solutions of the combined FONC, computing equilibria through solving the\ncombined FONC is currently theoretically preferable to sequential optimization methods.\nFurthermore, sequential optimization is likely to be efficient only when the optimization\nproblems are weakly coupled; much effort could be wasted when the optimizers for the\ncoupled problems strongly depend on one another.\nWhile strong methods for computing equilibrium points in games with convex objectives\nand feasible sets exist and are preferable to solving the FONC alone, the alternatives to\nsolving the combined first-order conditions when the players' objectives and feasible sets\nare non-convex are currently limited. Until methods guaranteed to compute equilibria in\nnon-convex games are developed, general equilibrium programming must be undertaken\nwith checks of the SOSC.\n2.3. Benefits of Hessian-Free Algorithms. A \"Hessian-free\" algorithm for checking the\nSOSC will require only matrix-vector products with H, rather than requiring H explicitly\n[68, pg. 170]. Hessian-free algorithms for checking the SOSC have at least two major\nadvantages over algorithms that require the Hessians explicitly.\nFirst, multiplying by H can be more efficient than working directly with the elements\nof H, even when H is known [87, pg. 244]. For example: if H \u2208 RN \u00d7N is diagonal,\nthen Hx can be computed in N flops rather than the 2N 2 flops required for arbitrary\nH \u2208 RN \u00d7N . For very large and sparse H the efficiency gained by algorithms that require\nonly multiplications by H can reduce computational burden by an order of magnitude,\nwithout adding the significant overhead required by sparse factorization methods to track\nentries and maintain sparsity. The benefits of this property is well-known and lauded in\nnumerical linear algebra.\nSecond, the second-order derivatives of the objective and constraints required to explicitly compute H can be challenging to derive, difficult to program, and computationally\nintensive to implement for complex optimization and equilibrium problems [65, 54, 68].\nMatrix-vector products Hs, however, can be obtained with the gradient of the Lagrangian\nalone using finite-difference approximations [10, 70, 68]:\n\u0011\n1 \u0010\n\u2207L(x + \u03c3s, \u03bb) \u2212 \u2207L(x, \u03bb)\nHs \u2248\n\u03c3||s||\nfor small \u03c3; see [20, 10] to choose effective scales \u03c3. In fact, this relationship underlies the\neffectiveness of some Newton-Krylov solvers for very large and complex nonlinear systems\n[10, 70].\nOf course, an approximation to H itself could be constructed using finite differences with\nN gradient evaluations. Indeed, this complete approximation is required to use the BHT\nand Inertia tests when the second derivatives are not explicitly provided; quasi-Newton\napproximations such as the BFGS approximation cannot be used. The new Hessian-free\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\nalgorithms presented below, however, require at most L \u2264 N evaluations of the gradient\nof the Lagrangian. For highly constrained problems (L \u001c N ) this represents a significant\ndecrease in function evaluations and thus overall computational burden.\n3. THREE TESTS OF THE SOSC\nVerifying the SOSC for unconstrained problems requires verifying the positive-definiteness\nof the Hessian matrix H. Computationally, this is best accomplished by attempting to\ntake a Cholesky factorization of H, a stable, efficient, and symmetry-exploiting algorithm\n[44, 87, 49]. At least three equivalent tests exist for evaluating the \"constrained positivedefiniteness\" H \u001fC 0 of the Hessian of the Lagrangian in Problem (1).\nThe most direct test of the SOSC H \u001fC 0 is to verify the positive-definiteness of a\nreduced L \u00d7 L Hessian matrix, where L = N \u2212 M :\nSOSC Test 1 ((Reduced Matrix Test [57])). H \u001fC 0 if, and only if, W> HW \u2208 RL\u00d7L is\npositive-definite for any matrix W \u2208 RN \u00d7L whose columns form a basis of C.\nThe three algorithms described in Sections 4.1, 4.2, and 4.3 below are implementations\nof this test.\nThe remaining two tests involve two (N + M ) \u00d7 (N + M ) matrices:\n\u0014\n\u0015\n\u0014\n\u0015\n0 A\nH A>\nB=\nand\nK\n=\nA> H\nA 0\nwhere in both cases 0 \u2208 RM \u00d7M . B is the well-known \"Bordered Hessian\" [58, 57, 69].\nK appears in the FONC for equality-constrained quadratic programs and is thus often\nreferred to as a \"KKT matrix\" [68, Chapter 16]. More generally, K is an example of a\nsaddle-point matrix or equilibrium system; for general information see the extensive review\nin [4].\nThe \"Bordered Hessian Test\" (BHT) is a classical test of H \u001fC 0 particularly popular\nin economics that uses B:\nSOSC Test 2 ((Bordered Hessian Test [41, 58, 57])). H \u001fC 0 if, and only if,\n\u0001 the last L\nleading principle minors of B all have sign (\u22121)M ; specifically, sign(det Bi ) = (\u22121)M\nfor all i = 1, . . . , L, where Bi is the submatrix of B formed by taking the first 2M + i rows\nand columns.\nNote that the BHT requires L determinant calculations, and thus L LU-factorizations.\nSection 4.4 below outlines an efficient procedure for verifying the BHT using updated LU\nfactorization.\nA symmetry-exploiting, single-factorization test based on the KKT matrix K can also\nbe derived. The inertia of a matrix is a triple containing the number of positive, negative,\nand zero eigenvalues [44, 68]. Gould [46] proves the following equation:\nLemma 1. ([46]) inertia(K) = inertia(W> HW) + (M, M, 0) for any basis W of C.\nBecause H is positive-definite over C if, and only if, inertia(W> HW) = (L, 0, 0) \u2212 i.e.,\nall of (W> HW)'s eigenvalues are positive \u2212 Lemma 1 establishes the following test:\n\n\f8\n\nW. ROSS MORROW\n\nTable 1. Several methods for verifying the SOSC elaborated on in Section 4. \"Hessian-Free\" refers to the ability of an algorithm to operate with\nHessian-vector products, rather than the actual elements of the Hessian.\n\"Basis of C?\" refers to the requirement that an algorithm start by computing a basis of C. \"Dir. of Neg. Curvature\" refers to the ability of an\nalgorithm to reveal or compute a feasible direction of negative curvature\nwhen the SOSC fails.\nTest\n1\n1\n1\n2\n3\n\nAlgorithm\nAlg. 2 (Sec. 4.1)\nAlg. 4 (Sec. 4.2)\nAlg. 5 (Sec. 4.3)\nSec. 4.4\nSec. 4.5\n\nHessianFree?\nYes\nYes\nYes\nNo\nNo\n\nBasis\nof C?\nYes\nYes\nNo\nNo\nNo\n\nDir. of Neg.\nCurvature\nYes\nYes\nYes\nNo\nNo\n\nSOSC Test 3 ((The Inertia Test [46])). H \u001fC 0 if, and only if, inertia(K) = inertia(B) =\n(N, M, 0).\nSection 4.5 describes how this test can be implemented using an inertia-revealing symmetricindefinite factorization to compute the inertia of K [11, 46, 1, 23].\n4. FIVE ALGORITHMS\nThis section derives several algorithms for verifying the SOSC; see Table 1. The focus\nis on three new Hessian-free algorithms for Test 1, (Sections 4.1, 4.2, and 4.3) rather than\nthe existing factorization-based BHT and Inertia tests (Section 4.4 and 4.5). Appendix A\ndiscusses ways to compute a basis W of C, needed for the algorithms described in Sections\n4.1 and 4.2; Algorithm 5 provides a Hessian-free method that does not require computing\na basis of C.\n4.1. An Implicit, Projected Cholesky Factorization for Test (1). The most direct\nway to verify H \u001fC 0 given a basis W of C is to attempt to take a Cholesky factorization\nof W> HW. In the nth step the Cholesky factorization derived in [87, Lecture 23],\n\u0014\n\u0015\nIn\u22121\n0\n>\n>\nW HW = L1 * * * Ln\u22121\nL>\n> H W\nn\u22121 * * * L1\n0\nWn:L\nn\nn:L\nwhere\n\uf8ee\n\nIn\u22121\nLn = \uf8f0 0\n0\n\n0\n\u221a\n\u03b1n\n\u221a\n>\nWn+1:L\nHn wn / \u03b1n\n\n0\n0\n\n\uf8f9\n\uf8fb,\n\nIL\u2212n\u22121\n\nH1 = H,\n(3)\n\n>\nHn = Hn\u22121 \u2212 Hn\u22121 wn\u22121 wn\u22121\nHn\u22121 /\u03b1n ,\n\nand \u03b1n = wn> Hn wn for all n \u2208 {1, . . . , L}. H \u001fC 0 if, and only if, \u03b11 , . . . , \u03b1L > 0.\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\nAlgorithm 1 ((\"Classical\"\nCholesky)).\n1 V \u2190 HW\n2 for n = 1, . . . , L,\n3\n\nAlgorithm 2 ((\"Modified\"\nCholesky)).\n1 V \u2190 HW\n2 for n = 1, . . . , L,\n\nImplicit\n\nfor m = 1, . . . , n \u2212 1,\n>\nvm\nwn /\u03b1m\n\n4\n\n\u03b2\u2190\n\n5\n\nvn \u2190 vn \u2212 \u03b2vm\n\n3\n\n\u03b1 \u2190 wn> vn\n\n4\n\nif \u03b1 \u2264 0, fail,\n\nImplicit\n\n5\n\nfor m = n + 1, . . . , L,\n\n6\n\n\u03b1n \u2190\n\nwn> vn\n\n6\n\n\u03b2 \u2190 vn> wm /\u03b1\n\n7\n\nif \u03b1n \u2264 0, fail,\n\n7\n\nvm \u2190 vm \u2212 \u03b2vn\n\nFigure 1. \"Classical\" (Left) and \"Modified\" (Right) Gram-Schmidt style\nImplicit Cholesky Algorithms.\nObserving that the Cholesky factors Lk do not need to be explicitly computed to compute\nthe \u03b1 numbers determining whether the Cholesky process succeeds or fails, the Cholesky\nprocess can be reduce to the following:\n> v > 0 for all m < n, \u03b1 = w> v\nLemma 2. For any n \u2208 {1, . . . , L} such that \u03b1m = wm\nm\nn\nn n\nwhere\nn\u22121\nX \u0012 v> wn \u0013\nm\n(4)\nvn = Hn wn = Hwn \u2212\nvm .\n\u03b1m\nm=1\n\nProof. The formula for \u03b1n is its definition accepting vn = Hn wn ; the second formula for\nvn follows from Eqn. (3) by induction.\n\u0003\nTwo \"implicit\" Cholesky algorithms that implement Eqn. (4) are given in Algs. 1 and\n2. While W> HW is not explicitly formed, Algs. 1 and 2 implicitly form the upper (or\nlower) triangle of W> HW. Note that Algs. 1 and 2 only need to compute the products\nHwn , rather than work with the elements of H explicitly, and is thus Hessian-free. Finally,\nfeasible directions of negative curvature can be computed easily should either algorithm\nreject the SOSC:\nLemma 3. Suppose Algorithm 1 or 2 fails in the nth step, for some n \u2208 {1, . . . , L}, with\n\u03b1n < 0. Then d = s1 w1 + * * * + sn wn is a feasible direction of negative curvature, where\nsn = 1 and\n(5)\n\nsm =\n\n>w\n>\nsm+1 (vm\nm+1 ) + * * * + sn (vm wn )\n\u03b1m\n\nfor all m \u2208 {1, . . . , n \u2212 1}.\nImplementing this formula requires storing the \u03b1 values computed in Alg. 2 and would\n>w\nbenefit from storing the triangular array of inner products vm\nm+k that are computed as\npart of the Implicit Cholesky process, rather than re-computing them to find a direction\nof negative curvature.\n\n\f10\n\nW. ROSS MORROW\n\nAlgorithm 3 ((\"Classical\"\n\nAlgorithm 4 ((\"Modified\"\ntion)).\n1 V\u2190W\n2 for n = 1, . . . , L,\n\nDiagonaliza-\n\ntion)).\n\n1\n2\n3\n\nV\u2190W\nfor n = 1, . . . , L,\nfor m = 1, . . . , n \u2212 1,\nz>\nm vn /\u03b1m\n\n4\n\n\u03b2\u2190\n\n5\n\nvn \u2190 vn \u2212 \u03b2vm\nzn \u2190 Hvn\n\nDiagonaliza-\n\n3\n\nz \u2190 Hvn\n\n4\n\n\u03b1n \u2190 vn> z\n\n5\n\nif \u03b1n \u2264 0, fail,\n\n6\n\nfor m = n + 1, . . . , L,\n\n7\n\n\u03b1n \u2190\n\nvn> zn\n\n7\n\n>\n\u03b2 \u2190 wm\nz/\u03b1m\n\n8\n\nif \u03b1n \u2264 0, fail,\n\n8\n\nvm \u2190 vm \u2212 \u03b2vn\n\n6\n\nFigure 2. \"Classical\" (Left) and \"Modified\" (Right) Gram-Schmidt style\nDiagonalization algorithms. Note that the vectors z1 , . . . , zL can overwrite\nw1 , . . . , wL .\n>\nProof. The idea is straightforward: if d = Ws where s solves L>\nn\u22121 * * * L1 s = en , then\n>\nd \u2208 range(W) = C and d Hd = \u03b1n < 0. Eqn. (5) follows from applying back substitution\nto solve for this s.\n\u0003\n\n4.2. A Diagonalization Method for Test (1). The following facts furnish a different\nversion of Test (1):\nLemma 4. (i) Let the columns of W \u2208 RN \u00d7L be any basis for C, let S \u2208 RL\u00d7L be any\nnonsingular matrix, and set V = WS. W> HW is positive-definite if, and only if, V> HV\nis positive-definite. (ii) Let V be any basis of C such that V> HV is diagonal. H \u001fC 0 if,\nand only if, all of V> HV's diagonal entries are positive.\nProof. Claim (i) follows from Sylvester's Law of Inertia and claim (ii) is trivial.\n\n\u0003\n\nThus, given any basis W of C, the definiteness of H over C can be revealed by finding a\nnonsingular S such that V = WS and V> HV is diagonal. An \"oblique\" Gram-Schmidt\nprocess makes this possible:\nLemma 5. Set v1 = w1 and recursively define\nn\u22121\nX \u0012 v> Hwn \u0013\nk\nvk\n(6)\nvn = wn \u2212\n\u03b1k\nk=1\n\nfor any n \u2208 {2, . . . , L}, so long as \u03b1k = vk> Hvk 6= 0 for all k \u2208 {1, . . . , n \u2212 1}. For\n> Hv = 0 for all m, k \u2208 {1, . . . , n}, m 6= k.\nv1 , . . . , vn so defined, vm\nk\nProof. The proof is a straightforward induction.\n\n\u0003\n\nAgain, two algorithms for Test 1 based on Eqn. (6) are provided in Algs. 3 and 4. Note\nthat Algs. 3 and 4 require exactly the same linear-algebraic operations as Algs. 1 and 2,\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\nbut on different quantities. In particular, Algs. 3 and 4 are also Hessian-free. As written,\nAlgs. 3 and 4 require an additional N \u00d7 L matrix of storage for z1 , . . . , zL , although these\nvectors can be written over w1 , . . . , wL if the basis of C is not needed after the SOSC\ncheck. Note, however, no additional computation is required to extract a feasible direction\nof negative curvature from Algs. 3 or 4 when the SOSC fails:\nLemma 6. Suppose Algorithm 3 or 4 fails in step n \u2264 L with \u03b1 < 0. Then vn is a feasible\ndirection of negative curvature.\nThus, Alg. 3 or 4 is a useful re-organization of Alg. 1 or 2 (respectively) if directions of\nnegative curvature are important.\n4.3. Projected Conjugate Gradients for Test (1). The Conjugate Gradient (CG)\nalgorithm is a widely-used iterative method for solving symmetric positive-definite linear\nsystems [44, 87]. The Projected Conjugate Gradient (PCG) algorithm is an equivalent\nalgorithm for solving the FONC for equality-constrained quadratic programming problems\n[47, 68]. Verifying the SOSC with PCG is based on the following converse question:\nCan (P)CG verify that a matrix is (constrained) positive-definite?\nThis section describes a Hessian-free approach for checking Test (1) based on existing\nPCG algorithms. For simplicity, the majority of the derivation neglects constraints, and\nconsiders how CG can be adapted to verify or reject the positive-definiteness of a matrix\nH \u2208 RN \u00d7N . The extension to the constrained case is a straightforward adaptation of this\ndiscussion and existing PCG methods as described in, e.g. [68, Chapter 16]. CG applied\nto the system Hx = b, b 6= 0 is denoted by CG(H, b); see [44, pg. 527] or [68, pg. 112]\nfor a formal algorithm. Because CG started at x0 6= 0 is equivalent to CG(H, b0 ) started\nat 0, where b0 = b \u2212 Hx0 , it is assumed that CG(H, b) always starts at 0.\nWhile CG(H, b) converges if H is definite [44, 87, 68], convergence of CG(H, b) alone\nis not an indicator of definiteness:\nExample 3. Let H = diag(1, \u22121) and b = (1, 2). CG(H, b) converges in two steps, despite\nbeing applied to an indefinite matrix. However if b = (1, 1), then CG(H, b) breaks down\nin the first step. \u0004\nThis example shows that convergence (or divergence) of CG alone cannot be used as a\nbasis for verifying positive-definiteness.\nThe foundation for using CG(H, b) as a positive-definiteness check is, in fact, Lemma\n4(ii). In exact arithmetic CG(H, b) constructs a set of N 0 \u2264 N linearly independent, H0\nconjugate vectors pi and evaluates p>\ni Hpi for all i \u2208 {1, . . . , N } [44, 87]. Verifying that\n>\npi Hpi > 0 for all i verifies positive-definiteness, by Lemma 4(ii), assuming that N 0 = N .\nExample 4. Again let H = diag(1, \u22121) and b = (1, 2). CG(H, b) encounters p>\n2 Hp2 < 0\nin the second step, and thus discovers indefiniteness in H. \u0004\n0\nEnsuring that p>\ni Hpi > 0 for all i \u2208 {1, . . . , N } is already a component of some CG\ncodes [68].\n\n\f12\n\nW. ROSS MORROW\n\nUnfortunately CG(H, b) may converge for some N 0 < N prior to building a basis of\nand thus cannot always determine whether H is positive-definite. Indeed this \"early\nconvergence\" is the central benefit of CG for solving large linear systems.\nRN\n\nExample 5. Consider again H = diag(1, \u22121), and take b = (1, 0). CG(H, b) converges\nin a single step (1 = N 0 < N = 2), and cannot identify that H is indefinite.\u0004\nIn fact, in exact arithmetic, CG(H, b) always converges with N 0 < N when b lies in a\nproper invariant subspace of H. (A proper invariant subspace of H is a proper subspace\nW of RN such that Hw \u2208 W for all w \u2208 W.) The same issue presents in finite-precision\narithmetic, where CG(H, b) \"converges\" with N 0 < N when the residual Hx \u2212 b or the\nsearch directions become \"small\".\nWhen CG(H, b) converges with N 0 < N , the search can be continued consistent with\nLemma 4(ii) by \"restarting\" with a new right-hand-side b0 that is H-conjugate to all previous search directions. The continued search should proceed until (a) positive-definiteness\nhas been rejected, (b) the restarted process itself converges, or (c) the remaining dimensions\nof the space have been searched without rejecting positive-definiteness.\nExample 6. To continue the case in Example 5 above choose b0 = (0, \u00b11). b0 \u22a5 Hb and\n(b0 )> Hb0 < 0; thus the \"restarted\" CG(H, b0 ) would identify indefiniteness. \u0004\nFigure 3 gives a formal algorithm implementing the continued PCG approach\u2212with\nconstraints\u2212to verify (or reject) H \u001fC 0. Algorithm 5 is Hessian-free and, like Algorithm\n4, does not require any additional computation to extract a feasible direction of negative\ncurvature:\nLemma 7. Suppose Algorithm 5 fails in step j \u2264 L with \u03b7 < 0. Then pj is a feasible\ndirection of negative curvature.\nAlgorithm 5, being modeled on existing PCG algorithms [47, 68], allows flexibility\nin choosing an operator projC : RN \u2192 C that orthogonally projects vectors from RN\nonto C \u2282 RN . This projection can always be done using a basis W of C by setting\nprojC (r) = W(W> W)\u22121 W> r, a formula that is particularly simple when W is orthonormal. However, projection onto C can also be accomplished without computing a basis for\nC [47]. This is an important distinguishing feature of Algorithm 5 relative to Algorithms 2\nand 4: In principle, PCG provides a Hessian-free way to verify H \u001fC 0 if it is impractical\nto project onto C using a basis of C computed from the constraint gradients.\nGould et al [47] study two projectors in the traditional PCG algorithm (in the context\nof finite-precision arithmetic). The first sets projC r = r \u2212 A> v where AA> v = Ar can be\nsolved using a Cholesky factorization of AA> . A QR factorization or SVD of A> could also\nbe used to solve minv\u2208RM ||A> v\u2212r||2 , the equivalent least-squares problem defining v. The\nsecond method considered in [47] solves an \"augmented system\" with symmetric-indefinite\nfactorization; see [47, 68, 32]. Any necessary factorizations need only be taken once, prior to\nexecuting Algorithm 5. Continuation, however, requires projecting onto the intersection of\nC with the image under H of the previous search directions. Computationally, this amounts\nto appending certain rows to A or, equivalently, columns to A> . Using Householder QR\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\nAlgorithm 5 ((Continued Projected Conjugate Gradients)).\n1 choose b \u2208 C\n2\n\ni\u21900\n\n3\n\nwhile i \u2264 L,\n\n4\n\npcg(b, C, i)\n\n5\n\nif pcg(b,C,i) failed, exit\n\n6\n\nelse if pcg(b,C,i) converged,\n\n7\n\nC \u2190 C \u2229 span{qk : k = i + 1, . . . , j}\u22a5\n\n8\n\nchoose b \u2208 C\n\n9\n\ni\u2190j+1\n\nAlgorithm 6 ((Projected Conjugate Gradient Step)).\npcg(b, C, i)\n1\n\n\u221a\nr \u2190 b, \u03c9 \u2190 r> r, r \u2190 r/ \u03c9, p \u2190 r\n\n2\n\nfor j = i + 1, . . . , L\n\n3\n\n\u03c4 \u2190 \u03c9, qj \u2190 Hp, \u03b7 \u2190 p> qj ,\n\n4\n\nif \u03b7 \u2264 0, fail\n\n5\n\nr \u2190 r \u2212 (\u03c4 /\u03b7)vj , s \u2190 projC (r), \u03c9 \u2190 r> s,\n\n13\n\nif |\u03c9| \u2264 tol, converged\n\n14\n\np \u2190 s + (\u03c9/\u03c4 )p\n\nFigure 3. Formal algorithm for the continued PCG method for verifying\n(or rejecting) H \u001fC 0.\nfactorization of A> provides an efficient and stable way to applying and updating this\nprojector.\n4.4. Updated LU Factorizations for the BHT, Test (2). The BHT requires computing the sign of L determinants, each typically computed using LU factorization; see\n[44, pg. 97] or [49, pg. 279]. Explicitly forming the L LU factorizations to compute the\nleading principal minor determinants for the BHT, however, requires an unacceptable and\nunnecessary amount of work asymptotically proportional to L(N + M )3 .\nAn efficient LU updating scheme for computing the associated determinant signs follows\nfrom a recursive definition of the leading principle submatrices of B. The matrices Bi ,\ni = 2, . . . , L (defined in Test 2) satisfy the recursion\n\u0014\n\u0015\nBi\u22121 bi\nBi =\nb>\n\u03b3i\ni\n\nfor some\n\nbi \u2208 R2M +i\u22121\n.\n\u03b3i \u2208 R\n\n\f14\n\nW. ROSS MORROW\n\nAn LU factorization of Bi can then be constructed by applying the pivots and row eliminations from earlier LU factorizations to bi and then zeroing out b>\ni with 2M + i \u2212 1 new\nrow eliminations [44, Section 3.2].\nFor brevity the full LU updating process is not described here; deriving a formal statement of this update is straightforward, if a bit tedious. The first LU factorization can be\nspecially designed to account for the leading M \u00d7 M submatrix of zeros in B and standard pivoting strategies can be used to enhance stability. Existing codes such as LAPACK\n[56], LUSOL [36, 75] PARDISO [76], or HSL's LA15 [19] can be exploited to take the first\nLU factorization as well as, in some cases, implement the factorization updates. While\ncomputing determinants can be inaccurate due to numerical over- and under-flow [49, pg.\n279], computing the sign of the determinant is exact for the computed LU factorization.\nIn any case, the BHT ultimately requires factoring the L matrices Bi and may thus be\ninappropriate for large scale problems without significant sparsity. Furthermore, the BHT\ndoes not provide an obvious route to computing a direction of negative curvature should\nthe SOSC fail.\n4.5. Block Symmetric-Indefinite (LDL) Factorization for the Inertia Test (3).\nImplementing the Inertia Test (3) is, in principle, rather straightforward. The inertia of\nK can be computed using a stable symmetric-indefinite block \"LDL\" factorization [11, 1,\n49, 23] of the form PKP> = LDL> where P is a permutation matrix, L is unit lower\ntriangular, and D is a block-diagonal matrix with 1 \u00d7 1 and 2 \u00d7 2 blocks; see [49, Chapter\n11]. By Sylvester's Law of Inertia, inertia(K) = inertia(D) [44, pg. 403] and computing the\ninertia of the computed D is efficient and exact even in inexact arithmetic; see [49, Problem\n11.2]. Indeed, symmetric-indefinite codes [23, 77] already have options to compute inertia,\nand the matrix inertia is used in some optimization solvers to promote global convergence\n[38, 15, 78]. However accuracy of the Inertia test in inexact arithmetic depends on whether\nthe inertia of the D computed using finite-precision arithmetic actually equals the inertia\nof K. The Inertia test requires factoring a single (N + M ) \u00d7 (N + M ) matrix and may thus\nbe applicable to large scale problems only when there is significant sparsity. There are,\nhowever, extremely efficient codes for forming this factorization. Like the BHT, it is not\nobvious how to compute a direction of negative curvature from K when the Inertia Test\nfails.\n5. EXAMPLES\nThis section presents several computational examples. Sections 5.3 and 5.4 illustrate the\nrelative performance characteristics of the different algorithms discussed above on dense\ntest problems for which H, A, and the truth of H \u001fC 0 is known explicitly. Section 5.5\ndemonstrates the advantages of the Hessian-free properties of Cholesky and Diagonalization\nusing a test problem from the COPS collection [22].\n5.1. Computational Details. Algs. 2, 4, 5, the updated LU BHT, and the Inertia test\nhave been implemented in C making extensive use of BLAS [9] routines and LAPACK's routines\nfor QR (dgeqrf/dormqr), LU (dgetrf), and symmetric-indefinite block LDL (dsytrf)\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\nfactorizations [56]. The basis W of C used in Algorithms 2 and 4 is computed using a\nQR factorization of A> , primarily for consistency with the updated QR approach to the\nPCG algorithm. Using the SVD would probably provide more stable (though also more\nexpensive) computations of W. The updated LU BHT described above was verified using\na \"naive\" BHT that computes sign(det(Bi )) via L independent LU factorizations. The\ntime savings from updating LU factorizations ranged from 10-90% depending on N and\nM . All computations were undertaken on an Apple MacPro tower with dual quad-core\n2.26 GHz \"Nehalem\" processors (each with an 8 MB cache) and 32 GB of RAM running\nMac OS X 10.6.6 and Apple's implementations of BLAS and LAPACK.\n5.2. Generating Dense Random Test Problems. Random test problems were obtained as follows: Let N \u2208 {2, 3, . . . }, M \u2208 {1, . . . , N \u2212 1}, and P \u2208 {0, . . . , N }. Choose\nsymmetric positive-definite \u039b+ \u2208 RP \u00d7P , symmetric negative-definite \u039b\u2212 \u2208 R(N \u2212P )\u00d7(N \u2212P ) ,\northogonal Q \u2208 RN \u00d7N , upper-triangular R \u2208 RM \u00d7M , and set\n\u0014\n\u0015\n\u0014 \u0015\n\u039b+ 0\n0\n>\n>\n(7)\nH=Q\nQ\nand\nA =Q\n0 \u039b\u2212\nR\nWhether H \u001fC 0 for such H and A is known analytically:\nLemma 8. Let H and A be defined as in Eqn. (7) and let C = null(A). (i) H \u001fC 0\nif, and only if, L = N \u2212 M \u2264 P . (ii) For uniformly drawn M \u2208 {1, . . . , N \u2212 1} and\nP \u2208 {0, . . . , N }, P(H \u001fC 0) = (N + 2)/(2(N + 1)).\nThe techniques described by Higham [49, pg, 517-518] are used to draw random orthogonal matrices (Q) and random symmetric matrices with known eigenvalues (\u039b+ , \u039b\u2212 ). Note\nthat any such construction is likely to be dense, and thus does not take advantage of the\nHessian-free characteristics of the Cholesky, Diagonalization, or PCG algorithms.\n5.3. A Well-Conditioned Example.\nExample 7. Let A be defined by an R with off diagonal elements ri,j , j > i, drawn from a\nstandard normal distribution and diagonal elements ri,i drawn from a normal distribution\nwith mean zero and variance (M \u2212 i)2 . \u0004\nOver 20,000 numerical tests were run for Ex. 7. 90 distinct values of N were drawn\nfrom {10, ..., 5000} and, for each N , at least 200 M, P pairs were drawn. Slightly more\nthan half (\u223c 50.3%) of these trials have H \u001fC 0, with H \u0007C 0 in the remaining trials; see\nLemma 8(ii). Every Hessian matrix H drawn has eigenvalues \u03bb satisfying 0.1 \u2264 |\u03bb| \u2264 100.\nThe tolerance for convergence in the PCG approach (Alg. 5) was 10\u221210 .\nIn Ex. 7, all methods correctly verified or rejected H \u001fC 0 in most tests. No method\nreturned a false positive in any trial, and only the Inertia test returned false negatives.\nIndeed, the performance of the Inertia test degrades when H \u001fC 0 as N grows, as shown\nin Table 5.3. For N \u2248 5, 000, more than 3% of the SOSC checks using the Inertia test gives\na false negative. No other algorithm returned a single false negative.\nFigs. 4 and 5 compare the time required by each method to computationally verify\nH \u001fC 0 for Ex. 7. The Inertia test, generally the fastest method, is used as a benchmark\n\n\f16\n\nW. ROSS MORROW\n\nTable 2. Percent of trials in Example 7 for which H \u001fC 0 yet the Inertia\nTest incorrectly declares H \u0007C 0; i.e. false negatives (FN).\nN\nFN\n\n77\n1.0%\n\n242\n0.5%\n\n478\n2.0%\n\n679\n1.1%\n\n899\n1.0%\n\n908\n1.0%\n\n1153\n1.0%\n\n1161\n2.1%\n\nN\nFN\n\n1530\n1.0%\n\n1538\n1.0%\n\n1718\n2.9%\n\n2306\n2.0%\n\n2460\n1.1%\n\n3687\n2.7%\n\n4637\n1.9%\n\n4903\n3.3%\n\nTable 3. Percent of trials in Example 7 for which H \u001fC 0 and PCG was\ncontinued at least once.\nAll N\n68%\n\nN \u2265 20\n76%\n\nN \u2265 50 N \u2265 100 N \u2265 1, 000\n88%\n94%\n99%\n\nfor comparison; see Fig. 4. Cholesky (Alg. 2) and Diagonalization (Alg. 4) are as fast or\nfaster than the Inertia test for highly constrained problems with M \u2265 0.75N . However if M\nis small relative to N , Algs. 2 and 4 can take 10-20 times longer than the Inertia test. Alg.\n4 is slightly slower than Alg. 2 in part because the Hessian-vector products Hw are done\none-by-one in Alg. 4, instead of \"all-at-once\" in Alg. 2. When H is known the level-3 BLAS\nroutines used to form HW optimize efficient cache memory usage. Generally speaking\nthese results are more encouraging than they appear: dsytrf, the code for the Inertia\ntest, is highly optimized while the implementations of Algs. 2 and 4 have not yet been\noptimized. \"Blocked\" variants of Algs. 2 and 4 that fully exploit memory traffic efficiencies\nin the level-3 BLAS are conceptually easy to derive, and will be even more competitive with\nthe Inertia test from the perspective of compute time.\nThe PCG approach (Alg. 5) and the BHT are as fast or faster than the Inertia test only\nfor M \u2248 N , and can take more than 100 times longer than the Inertia test on problems\nwith N \u2265 1000 and small M . Though it is not shown in the plots, continuation is an\nimportant part of the PCG approach; see Table 5.3. Alg. 5 converged at least once in over\n68% of the trials for which H \u001fC 0, and would thus \"fail\" in more than 68% of our cases if\nit were not continued. Moreover as N grows, Alg. 5 tends to converge more often. Based\non the trials undertaken for Ex. 7, we would \"expect\" Alg. 5 to converge in \u223c N 0.85 cases.\nSpecifically, Alg. 5 converged no more than N 0.9 times as N grew in the trials undertaken\nfor Ex. 7, and converged at least \u223c N 0.8 times in more than 50% of the trials. While these\npredictions should not be extrapolated beyond this example, they clearly demonstrate the\nnecessity of continuation in the PCG approach to verifying the SOSC.\n5.4. A Poorly Conditioned Example.\nExample 8. Let A be defined by an R with all elements drawn from a standard normal\ndistribution. \u0004\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\nCompute Time in the\nInertia Test (seconds)\n100\n1\n0.01\n0.0001\n10\n\n100\n1000\nProblem Size (N)\n\nFigure 4. Compute times for the Inertia test applied in Example 7 in trials\nfor which H \u001fC 0 is true.\n\n100\n\nCompute Time Relative to the Inertia Test\nCholesky\n\nDiagonalization\n\nPCG\n\nBHT\n\n10\n1\n0.25\n10\n\n100\n1000\nProblem Size (N)\n\n10\n\n100\n1000\nProblem Size (N)\n\n10\n\n100\n1000\nProblem Size (N)\n\n10\n\n100\n1000\nProblem Size (N)\n\nFigure 5. Compute times for Example 7 in trials for which H \u001fC 0 is\ntrue, relative to the Inertia test. Solid black lines give the maximum and\nminimum ratios or times for a given N over the values of M tested. Thin\ngrey lines in the four relative plots represent the maximum ratio of times\nfor subsets of trials such that M \u2265 0.25N (lightest), M \u2265 0.5N (midtone),\nand M \u2265 0.75N (darkest).\nAgain, over 20,000 numerical tests were run for Ex. 8 with the same character as for\nEx. 7, discussed above.\nEx. 8 demonstrates that numerical accuracy is far from guaranteed for computational\nSOSC checks. Fig. 6 illustrates the fraction of correct tests results and thus also of false\nnegatives. The Cholesky, Diagonalization, PCG, and BHT tests appear more stable than\nthe Inertia test. However all methods have a false negative rate close to 70% for problems\nwith as few as N \u2248 100 variables. With N \u2248 1, 000 variables, each test is so overcome by\nroundoff error that virtually no correct results are obtained. As with Ex. 7, there were no\nfalse positives.\nThe relatively poorer performance of the Inertia test in both examples deserves some\ndiscussion. Weyl's Eigenvalue Pertubation Theorem provides one way to control numerical\nerror in the Inertia test:\n\n\f18\n\nW. ROSS MORROW\n\nAlgs. 1, 2, 3,\nand BHT\nInertia Test\n\n0.8\n\n0.0\n0.2\n\n0.6\n\n0.4\n\n0.4\n\n0.6\n\n0.2\n\n0.8\n\n0.0\n10\n\n100\n\n1000\n\nProblem Size (N)\n\nFraction of\nFalse Negatives\n\nFraction of\nCorrect Results\n\n1.0\n\n1.0\n5000\n\nFigure 6. Fraction of correct test results (left axis) and, symmetrically,\nfalse negatives (right axis), over all trials in Example 8 for which H \u001fC 0.\nTheorem 2. (Weyl's Theorem [25, Theorem 4.1]) If \u03bb1 \u2265 * * * \u2265 \u03bbn and \u03bc1 \u2265 * * * \u2265 \u03bcn are\nthe eigenvalues of X and X + E, respectively, for any n \u00d7 n symmetric matrix X and any\nn \u00d7 n matrix E, then |\u03bbi \u2212 \u03bci | \u2264 ||E||2 for all i.\nCorollary 1. Let P(K + E)P> = LDL> be the computed LDL factorization of K where\nerrors are accumulated into a perturbation E of K; see [49, Chapter 11]. If |\u03bc|min =\nmin{|\u03bc| : \u03bc \u2208 \u03bb(K + E)} > ||E||2 then inertia(K) = inertia(D).\nProof. The Weyl Theorem states that \u03bbi = \u03bci + \u000fi for some |\u000fi | \u2264 ||E||2 for all i. If\n|\u03bci | > ||E||2 for all i, then also sign{\u03bbi } = sign{\u03bci }. Hence inertia(K) = inertia(K + E) =\ninertia(D).\n\u0003\nIf, however, |\u03bc|min \u2264 ||E||2 then inertia(K + E) = inertia(D) may not be equal to\ninertia(K). That is, if the magnitude of any eigenvalue of K + E is less than the norm of\nthe accumulated round-off errors, this eigenvalue of the perturbed KKT matrix may have\nthe wrong sign relative to the true eigenvalue. As N + M grows, the likelihood of both a\nsmall |\u03bc|min and large ||E||2 increases.\nNote that K can have small eigenvalues even if H does not. Specifically, K has small\neigenvalues whenever A has nearly linearly dependent columns, as the simple example in\nAppendix B shows.\nUsing the \"exact\" basis Q1:L of C in the Cholesky and Diagonalization tests eliminated\nthe false negatives seen in Fig. 6. This suggests that the numerical errors in these two\ntests, as well as perhaps the PCG test, are entirely a consequence of numerical errors\nin the QR factorization of A> . Two remarks along these lines must be made: First,\nusing Q1:L is a device of the artificial numerical examples. In practice, some factorization\nmust be used compute a basis for C from A. Second, the effect of error in the computed\nbasis of C will depend on the spectrum of H. For example if H is positive-definite these\nrepresentational errors in would be irrelevant to the accuracy of the test. On the other\nhand, false negatives can be obtained only if the numerical errors in the representation of C\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\nmagnify the contribution of H's negative eigenvalues to the quadratic forms. Understanding\nwhen the construction of a basis of C is sufficiently accurate relative to H will be essential\nto understanding when SOSC checks using the Cholesky and Diagonalization approaches\nare themselves numerically accurate.\nUnfortunately there will also be numerical errors associated with the Cholesky and\nDiagonalization approaches themselves for some Hessians H, independent of what errors\nare made in the construction of a representation of C. What specific properties of H to\nmonitor and control in this respect are not yet known and will be the subject of future\ninvestigations.\n5.5. A Hessian-Free Example from the COPS Collection. The \"Thomson Problem\"\nof finding the minimal energy configuration of K \u2208 {2, 3, . . . } points on a sphere is a a\nlarge-scale equality-constrained optimization problem from the COPS collection [22]:\nminimize\n\nf (x1 , . . . , xK ) =\n\nwith respect to\n\n3\n\nK\u22121\nX\n\nK\nX\n\nk=1 m=k+1\n\n(8)\nsubject to\n\n1\n||xk \u2212 xm ||2\n\nx1 , . . . , x K \u2208 R\n||xk ||22 = 1\n\nfor all n \u2208 {1, . . . , K}\n\nProb. (8) depends only on the Euclidean norms of K vectors in R3 , which are invariant over\northogonal transformations of R3 . Thus, Prob. (8) as stated has infinitely many solutions:\nspecifically, if (x1 , . . . , xK ) is a solution then so is (Qx1 , . . . , QxK ) for each rotation or\nreflection Q of R3 . An \"orthogonally-invariant\" Thomson problem can be obtained by\nrestricting the locations of the first and second points in a manner inspired by Householder\nQR factorization:\nminimize\n\nf (x1 , . . . , xK ) =\n\nwith respect to\n\n3\n\nK\u22121\nX\n\nK\nX\n\nk=1 m=k+1\n\n(9)\n\nsubject to\n\nx1 , . . . , xK \u2208 R\n\n||xk ||22 /2 = 1/2 for all\nx1,2 = x1,3 = x2,3 = 0\n\n1\n||xk \u2212 xm ||2\n\nn \u2208 {1, . . . , K}\n\nProblem (9) has N = 3K variables and M = K + 3 constraints.\nProb. (9) was solved for specific values of K between 3 and 333 using matlab's Hessianfree interior-point algorithm. The SOSC was then verified at the computed (x1 , . . . , xK )\nand \u03bb using our C implementations of Cholesky, Diagonalization, and the Inertia test. The\nHessian-free nature of the Cholesky and Diagonalization algorithms was exploited by using\ndirectional finite differences to approximate Hessian-vector products Hs. For the Inertia\ntest the full Hessian H was approximated with finite-differences. The PCG and BHT\nalgorithms were not used; the results in Section 5.3 suggest they are not competitive.\nIn this case the Hessian-free SOSC checks provided by the Cholesky and Diagonalization\nalgorithms tended to reduced time to verify the SOSC relative to the Inertia test by just\n\n\f20\n\nW. ROSS MORROW\n\n35%\n30%\n25%\n20%\n15%\n\nCholesky\n\n10%\n5%\n\nDiagonalization\n0\n\n200\n\n400\n\n600\n\nProblem Size (N)\n\n800\n\n1000\n\nFigure 7. Left: Computed solution to Prob. (9) for K = 100. Right:\nPercent reduction in time to computationally verify H \u001fC 0 for various\ninstances of Prob. (9). When only the black dot is visible, the gray and\nblack dots coincide. Note that N = 3K, where K is the number of points\ndistributed over the surface of the unit sphere.\nover 20%; see Fig. 7. Recall that the Inertia test was the fastest method on the dense test\nproblems above with \u223c 30% of the variables constrained. Thus the Hessian-free application\nof the Cholesky and Diagonalization algorithms results in significant computational savings.\n6. CONCLUSIONS\nThis article has presented three novel Hessian-free algorithms for verifying (or rejecting)\nthe SOSC for constrained continuous optimization. These algorithms also make computation of feasible directions of negative curvature easy when the SOSC fails, a feature\nnot available in classical tests. Numerical trials have demonstrated (1) the inefficiency of\nthe Bordered Hessian Test, (2) the relative speed of the Inertia test, (3) the computational efficiency of the new algorithms, especially when their Hessian-free properties can\nbe exploited, and (4) the potential for significant loss of accuracy due to round-off error\nusing any method, even on small problems. Future work will optimize implementations of\nthe new algorithms and undertake a detailed mathematical analysis of round-off errors to\ndetermine computable certificates of test accuracy.\nReferences\n1. Cleve Ashcraft, Roger G. Grimes, and John G. Lewis, Accurate symmetric indefinite linear equation\nsolvers, SIAM journal of matrix analysis and applications 20 (1998), no. 2, 513\u2013561.\n2. Hande Y. Benson, Arun Sen, David F. Shanno, and Robert J. Vanderbei, Interior-point algorithms,\npenalty methods, and equilibrium problems, Computational Optimization and Applications 34 (2006),\nno. 2, 155\u2013182.\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\n3. Antonio M. Bento, Lawrence H. Goulder, Mark R. Jacobsen, and Roger H. von Haefen, Distributional\nand efficiency impacts of increased us gasoline taxes, American Economic Review 99 (2009), no. 3,\n667\u2013699.\n4. Michele Benzi, Gene H Golub, and Jorg Leisen, Numerical solution of saddle point problems, Acta\nNumerica 14 (2005), 1\u2013137.\n5. Steven Berry, James Levinsohn, and Ariel Pakes, Automobile prices in market equilibrium, Econometrica\n63 (1995), no. 4, 841\u2013890.\n6.\n, Differentiated products demand systems from a combination of micro and macro data: The\nnew car market, Journal of Political Economy 112 (2004), no. 1, 68\u2013105.\n7. Dimitri P. Bertsekas, Nonlinear programming, Athena Scientific, 1999.\n8. David Besanko, Sachin Gupta, and Dipak Jain, Logit demand estimation under competitive pricing\nbehavior: An equilibrium framework, Management Science 44 (1998), no. 11, 1533\u20131547.\n9. BLAS, Basic linear algebra subprograms, 2010.\n10. Peter N. Brown and Youcef Saad, Hybrid krylov methods for nonlinear systems of equations, SIAM\nJournal of Scientific and Statistical Computing 11 (1990), no. 3, 450\u2013481.\n11. James R. Bunch and Linda Kaufman, Some stable methods for calculating inertia and solving symmetric\nlinear systems, Mathematics of Computation 31 (1977), no. 137, 163\u2013179.\n12. Richard H. Byrd, Jean Charles Gilbert, and Jorge Nocedal, A trust region method based on interior\npoint techniques for nonlinear programming, Mathematical Programming, Series A 89 (2000), no. 1,\n149\u2013185.\n13. Richard H. Byrd, Nicholas I. M. Gould, Jorge Nocedal, and Richard A. Waltz, An algorithm for\nnonlinear optimization using linear programming and quality constrained subproblems, Mathematical\nProgramming, Series B 100 (2004), no. 1, 27\u201348.\n14. Richard H. Byrd, Mary E. Hribar, and Jorge Nocedal, An interior point algorithm for large-scale\nnonlinear programming, SIAM Journal on Optimization 9 (1999), no. 4, 877\u2013900.\n15. Richard H. Byrd, Jorge Nocedal, and Richard A. Waltz, KNITRO: An integrated package for nonlinear\noptimization, Tech. report, Ziena Inc., 2006.\n16. Andrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint, Lancelot.\n17.\n, LANCELOT: A fortran package for large-scale nonlinear optimization (release a)., Springer\nSeries in Computational Mathematics, vol. 17, Springer-Verlag, 1992.\n18.\n, Trust region methods, SIAM, 2000.\n19. Science & Technology Facilities Council, HSL mathematical software library catalogue: LA15 v 1.2.0,\nTech. report, Research Councils UK, 2010.\n20. John E. Dennis and Robert B. Schnabel, Numerical methods for unconstrained optimization and nonlinear equations, SIAM, 1996.\n21. Steven P. Dirkse and Michael C. Ferris, The PATH solver: A non-monotone stabilization scheme for\nmixed complementarity problems, Optimization Methods and Software 5 (1995), 123\u2013156.\n22. Elizabeth D. Dolan, Jorge J. More, and Todd S. Munson, Benchmarking optimization software with\nCOPS 3.0, Tech. Report ANL/MCS-273, Argonne National Laboratory, February 2004.\n23. Iain S. Duff, MA57 - a code for the solution of sparse symmetric definite and indefinite systems, ACM\nTransactions on Mathematical Software 30 (2004), no. 2, 118\u2013144.\n24. Andreas Ehrenmann and Karsten Neuhoff, A comparison of electricity market designs in networks,\nOperations Research 57 (2009), no. 2, 274\u2013286.\n25. Stanley C. Eisenstat and Ilse C. F. Ipsen, Relative perturbation results for eigenvalues and eigenvectors\nof diagonalisable matrices, SIAM Journal of Matrix Analysis and Applications 20 (1998), no. 1, 149\u2013158.\n26. Francisco Facchinei and Christian Kanzow, Generalized nash equilibrium problems, Annals of Operations\nResearch 175 (2009), 177\u2013211.\n27. Michael C. Ferris and Jong-Shi Pang, Engineering and economic applications of complementarity problems, SIAM Review 39 (1997), no. 4, 669\u2013713.\n28. Roger Fletcher, Practical methods of optimization, Wiley and Sons, 1987.\n\n\f22\n\nW. ROSS MORROW\n\n29. Roger Fletcher, Nicholas I. M. Gould, Sven Leyffer, Philippe L. Toint, and Andreas Wachter, Global\nconvergence of a trust-region sqp-filter algorithm for general nonlinear programming, SIAM Journal on\nOptimization 13 (2002), no. 2, 635\u2013659.\n30. Roger Fletcher and Sven Leyffer, Nonlinear programming without a penalty function, Mathematical\nProgramming, Series A 91 (2002), no. 2, 239\u2013269.\n31. Roger Fletcher, Sven Leyffer, and Philippe L. Toint, On the global convergence of a filter-SQP algorithm,\nSIAM Journal on Optimization 13 (2002), no. 1, 44\u201359.\n32. Anders Forsgren, Philip E. Gill, and Joshua D. Griffin, Iterative solution of augmented systems arising\nin interior methods, SIAM Journal on Optimization 18 (2007), no. 2, 666\u2013690.\n33. Bart D. Frischknecht, Katie Whitefoot, and Panos Y. Papalambros, On the Suitability of Econometric Demand Models in Design for Market Systems, ASME Journal of Mechanical Design 132 (2010),\nno. 121007, 1\u201311.\n34. Steven A. Gabriel, Supat Kiet, and Jifang Zhuang, A mixed complementarity-based equilibrium model\nof natural gas markets, Operations Research 53 (2005), no. 5, 799\u2013818.\n35. Steven A. Gabriel, Andy S. Kydes, and Peter Whitman, The national energy modeling system: A\nlarge-scale energy-economic equilibrium model, Operations Research 49 (2001), no. 1, 14\u201325.\n36. P. E. Gill, W. Murray, M. A. Saunders, and M. H. Wright, Maintaining LU factors of a general sparse\nmatrix, Linear Algebra and its Applications 88/89 (1987), 239\u2013270.\n37. Philip E. Gill, Walter Murray, and Michael A. Saunders, SNOPT: An SQP algorithm for large-scale\nconstrained optimization, SIAM Review 47 (2005), no. 1, 99\u2013131.\n38. Philip E. Gill, Walter Murray, Michael A. Saunders, and Margaret H. Wright, Inertia-controlling methods for general quadratic programming, SIAM Review 33 (1991), no. 1, 1\u201336.\n39.\n, User's guide for NPSOL 5.0: A FORTRAN package for nonlinear programming, Tech. Report\nSOL 86-6, Stanford University, 2001.\n40. Philip E. Gill, Walter Murray, and Margaret H. Wright, Practical optimization, Academic Press, 1981.\n41. Robert Pollack Gillespie, Partial differentiation, Interscience Publishers, 1951.\n42. Pinelopi K. Goldberg, Product differentiation and oligopoly in international markets: The case of the\nu.s. automobile industry, Econometrica 63 (1995), no. 4, 891\u2013951.\n, The effects of the corporate average fuel efficiency standards in the US, The Journal of Industrial\n43.\nEconomics 46 (1998), no. 1, 1\u201333.\n44. Gene H Golub and Charles F Van Loan, Matrix computations, The Johns Hopkins University Press,\n1996.\n45. Austan Goolsbee and Amil Petrin, The consumer gains from direct broadcast satellites and the competition with cable tv, Econometrica 72 (2004), no. 2, 351\u2013381.\n46. Nicholas I. M. Gould, On practical conditions for the existence and uniqueness of solutions to the general\nequality quadratic programming problems, Mathematical Programming 32 (1985), 90\u201399.\n47. Nicholas I. M. Gould, Mary E. Hribar, and Jorge Nocedal, On the solution of equality constrained\nquadratic programming problems arising in optimization, SIAM Journal of Scientific Computing 23\n(2001), no. 4, 1376\u20131395.\n48. S. P. Han, A globally convergent method for nonlinear programming, Journal of Optimization Theory\nand Applications 22 (1977), no. 3, 297\u2013309.\n49. Nicholas J. Higham, Accuracy and stability of numerical algorithms, 2nd edition ed., SIAM, 2002.\n50. Benjamin F. Hobbs and J. S. Pang, Nash-cournot equilibria in electric power markets with piecewise\nlinear demand functions and joint constraints, Operations Research 55 (2007), no. 1, 113\u2013127.\n51. Xinmin Hu and Daniel Ralph, Using EPECs to model bilevel games in restructured electricity markets\nwith locational prices, Operations Research 55 (2007), no. 5, 809\u2013827.\n52. Xinmin Hu, Daniel Ralph, Eric K. Ralph, Peter Bardsley, and Michael C. Ferris, Electricity generation\nwith looped transmission networks: Bidding to an iso, 2007.\n53. Mark R. Jacobsen, Evaluating u. s. fuel economy standards in a model with producer and household\nheterogeneity, Working Paper, Stanford University, January 2010.\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\n54. Kenneth L. Judd, Numerical methods in economics, MIT Press, 1998.\n55. Andrew Kleit, Impacts of long-range increases in the corporate average fuel economy (cafe) standard,\nEconomic Inquiry 42 (2004), no. 2, 279\u2013294.\n56. LAPACK, Lapack - linear algebra package, 2010.\n57. David G. Luenberger and Yinyu Ye, Linear and nonlinear programming, third edition ed., Springer,\n2009.\n58. Daniel L. McFadden, Appendix A: Definite quadratic forms subject to constraint, Production Economics:\nA Dual Approach to Theory and Applications (Melvyn Fuss and Daniel L. McFadden, eds.), vol. I: The\nTheory of Production, Amsterdam: North-Holland, 1978.\n59. Jeremy J. Michalek, Panos Y. Papalambros, and Steven J. Skerlos, A Study of Fuel Efficiency and\nEmission Policy Impact on Optimal Vehicle Design Decisions, ASME Journal of Mechanical Design\n126 (2004), 1062\u20131070.\n60. W. Ross Morrow and Steven J. Skerlos, Fixed-point approaches to computing bertrand-nash equilibrium\nprices under mixed-logit demand, Operations Research (Forthcoming).\n61. Todd S. Munson, Algorithms and environments for complementarity, Ph.D. thesis, University of Wisconsin, Madison, 2000.\n62. Bruce A. Murtagh and Michael A. Saunders, Large-scale linearly constrained optimization, Mathematical Programming 14 (1978), no. 1, 41\u201372.\n63.\n, A projected lagrangian algorithm and its implementation for sparse nonlinear constraints,\nMathematical Programming Study 16 (1982), 84\u2013117.\n, MINOS 5.5 user's guide, Tech. Report SOL 83-20R, Stanford University, 1998.\n64.\n65. Stephen G. Nash, Nonlinear Programming, ORMS Today (1998).\n66. Aviv Nevo, Mergers with differentiated products: The case of the ready-to-eat cereal industry, The\nRAND Journal of Economics 31 (2000), no. 3, 395\u2013421.\n67.\n, Measuring market power in the ready-to-eat cereal industry, Econometrica 69 (2001), no. 2,\n307\u2013342.\n68. Jorge Nocedal and Stephen J. Wright, Numerical optimization, Springer-Verlag, 2006.\n69. Panos Y. Papalambros and Douglass J. Wilde, Principles of optimal design: Modeling and computation,\nCambridge University Press, 2000.\n70. Michael Pernice and Homer F. Walker, NITSOL: A newton iterative solver for nonlinear systems, SIAM\nJournal of Scientific Computing 19 (1998), no. 1, 302\u2013318.\n71. Amil Petrin, Quantifying the benefits of new products: The case of the minivan, Journal of Political\nEconomy 110 (2002), no. 4, 705\u2013729.\n72. M. J. D. Powell, The convergence of variable matric methods for nonlinearly constrained optimization\ncalculations, Nonlinear Programming (O. L. Mangasarian, R.R. Meyer, and S.M. Robinson, eds.), vol. 3,\nAcademic Press, 1978.\n, A fast algorithm for nonlinearly constrained optimization calculations, Numerical Analysis\n73.\n(G. A. Watson, ed.), Lecture Notes in Mathematics, vol. 630, Springer-Verlag, 1978.\n74. Daniel Ralph, Global convergence of damped newton's method for nonsmooth equations, via the path\nsearch, Mathematics of Operations Research 19 (1994), 352\u2013389.\n75. Michael A. Saunders, LUSOL: Sparse LU for Ax = b.\n76. Olaf Schenk and Klaus Gartner, Solving unsymmetric sparse systems of linear equations with PARDISO,\nJournal of Future Generation Computer Systems 20 (2004), no. 3, 475\u2013487.\n, On fast factorization pivoting methods for symmetric indefinite systems, Electronic Transac77.\ntions on Numerical Analysis 23 (2006), 158\u2013179.\n78. Olaf Schenk, Andreas Wachter, and Martin Weiser, Inertia-revealing preconditioning for large-scale\nnonconvex constrained optimization, SIAM Journal of Scientific Computing 31 (2008), no. 2, 939\u2013960.\n79. Ching-Shin Norman Shiau and Jeremy J. Michalek, Optimal Product Design Under Price Competition,\nASME Journal of Mechanical Design 131 (2009), no. 071003, 1\u201310.\n\n\f24\n\nW. ROSS MORROW\n\n80.\n81.\n\n82.\n\n83.\n84.\n85.\n86.\n87.\n88.\n89.\n90.\n\n, Should Designers Worry About Market Structure?, ASME Journal of Mechanical Design 131\n(2009), no. 011011, 1\u20139.\nChing-Shin Norman Shiau, Jeremy J. Michalek, and Chris T. Hendrickson, A Structural Analysis of\nVehicle Design Responses to Corporate Average Fuel Economy Policy, Transportation Research A:\nPolicy and Practice 43 (2009), 814\u2013828.\nSteven J. Skerlos, Jeremy J. Michalek, and W. Ross Morrow, Sustainable Design Engineering and\nScience: Selected Challenges and Case Studies, Sustainability Science and Engineering, Volume 1:\nDefining Principles (M. A. Abraham, ed.), Elsevier Science, 2005.\nHoward Smith, Supermarket choice and supermarket competition in market equilibrium, The Review of\nEconomic Studies 71 (2004), 235\u2013263.\nK Sudhir, Competitive pricing behavior in the auto market: A structural analysis, Marketing Science\n20 (2001), no. 1, 42\u201360.\nRaphael Thomadsen, The effect of ownership structure on prices in geographically differentiated markets,\nThe RAND Journal of Economics 36 (2005), no. 4, 908\u2013929.\nKenneth Train, Discrete choice methods with simulation, Cambridge University Press, 2003.\nLloyd N. Trefethen and David Bau, Numerical linear algebra, SIAM, 1997.\nRobert J. Vanderbei, Loqo user's manual - version 4.05, Tech. Report ORFE-99-??, Princeton University, 2006.\nRobert J. Vanderbei and David F. Shanno, An interior point algorithm for nonconvex nonlinear programming, Computational Optimization and Applications 13 (1999), no. 1-3, 231\u2013252.\nAndreas Wachter and Lorenz T. Biegler, On the implementation of an interior-point filter line search\nalgorithm for large-scale nonlinear programming, Mathematical Programming 106 (2006), no. 1, 25\u201357.\n\nAppendix A. Computing a Basis for C given A\nBoth Algorithm 2 and 4 require a basis W for C, rather than the constraint gradients A.\nThere are several ways to compute such a basis from A. Implicitly, each method considers\nthe equation AW = 0 and uses some factorization of A (or A> ) to find a formula for W.\nSeveral examples follow:\n\u2022 SVD of A: If A = U[ \u03a3 0 ]V> is a full Singular Value Decomposition (SVD; [87,\nLecture 4]) of A, then W = VM +1:N is an orthonormal basis for C.\n\u2022 QR factorization of A> : Similarly if A> = QR then W = QM +1:N is an orthonormal basis for C.\n\u2022 QR or LU factorization of A: If A = Q[ R S ] for upper-triangular R, or if\nPA = L[ U S ] for upper-triangular U and some permutation matrix P, then\n\u0014 \u22121 \u0015\n\u0014 \u22121 \u0015\nR S\nU S\nW=\nor\nW=\nI\nI\nare bases for C. In both cases, I \u2208 RL\u00d7L .\nStability considerations typically suggest that either of the first two approaches are preferable to the third.\nAppendix B. K has Small Eigenvalues When A is Nearly Rank Deficient\nLet H \u2208 RN \u00d7N and A0 \u2208 R(M \u22121)\u00d7N be arbitrary (but full-rank). Let am \u2208 RN denote\nthe mth row of A0 , for m \u2208 {1, . . . , M \u2212 1}. Choose \u03b21 , . . . , \u03b2M \u22121 (not all zero), \u000f \u2208 RN\n\n\fHESSIAN-FREE METHODS FOR CHECKING THE SECOND-ORDER SUFFICIENT CONDITIONS IN EQUALITY-CONSTRAINED OP\n\nPM \u22121\nwith small 2-norm, and set aM = m=1\n\u03b2m am + \u000f. The \u03b2's and \u000f can be chosen so that\n||aM ||2 = 1, even if ||\u000f||2 is very small. Define\n\uf8ee\n\uf8f9\n\uf8ee\n\uf8f9\nH (A0 )> aM\n0 0 \u2212\u000f\n0\n0 \uf8fb\nK = \uf8f0 A0\nand\nF = \uf8f00 0 0 \uf8fb ,\n>\n0 0 0\naM\n0\n0\nnoting that K + F is\n\uf8eb\n\uf8ee\n0\n\uf8edK + \uf8f00\n0\n\nsingular. Specifically,\n\uf8f9\uf8f6 \uf8ee \uf8f9 \uf8ee\n\uf8f9\uf8ee \uf8f9\n0 \u2212\u000f\n0\nH (A0 )> aM \u2212 \u000f\n0\n0 0 \uf8fb\uf8f8 \uf8f0\u2212\u03b2 \uf8fb = \uf8f0 A0\n0\n0 \uf8fb \uf8f0\u2212\u03b2 \uf8fb\n0 0\n0\n0\na>\n0\n0\nM\n\uf8ee PM \u22121\n\uf8f9 \uf8ee \uf8f9\n0\n\u2212 m=1 \u03b2m am + aM \u2212 \u000f\n\uf8fb = \uf8f00\uf8fb .\n=\uf8f0\n0\n0\n0\n\nThus, K + F has a zero eigenvalue and Weyl's Theorem states that K has an eigenvalue \u03bb\nwith |\u03bb| \u2264 ||F||2 = ||\u000f||2 .\nDepartments of Mechanical Engineering and Economics, Iowa State University, Ames, Iowa\n50011\nE-mail address: wrmorrow@iastate.edu\n\n\f"}