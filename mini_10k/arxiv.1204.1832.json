{"id": "http://arxiv.org/abs/1204.1832v2", "guidislink": true, "updated": "2012-04-12T06:26:56Z", "updated_parsed": [2012, 4, 12, 6, 26, 56, 3, 103, 0], "published": "2012-04-09T08:47:58Z", "published_parsed": [2012, 4, 9, 8, 47, 58, 0, 100, 0], "title": "Mathematical Modeling of Competitive Group Recommendation Systems with\n  Application to Peer Review Systems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1010.2444%2C1010.4049%2C1010.0564%2C1010.1926%2C1010.4633%2C1010.3033%2C1010.4485%2C1010.0273%2C1010.4630%2C1010.2767%2C1010.2042%2C1204.5367%2C1204.1998%2C1204.6281%2C1204.4224%2C1204.6082%2C1204.3278%2C1204.4101%2C1204.4353%2C1204.6251%2C1204.3786%2C1204.2379%2C1204.5210%2C1204.5482%2C1204.2576%2C1204.0019%2C1204.0144%2C1204.3352%2C1204.2836%2C1204.3601%2C1204.3743%2C1204.5756%2C1204.5980%2C1204.4688%2C1204.6029%2C1204.6288%2C1204.1825%2C1204.1003%2C1204.1452%2C1204.1643%2C1204.1949%2C1204.5044%2C1204.5540%2C1204.2495%2C1204.4075%2C1204.6653%2C1204.0539%2C1204.1835%2C1204.4445%2C1204.0542%2C1204.3434%2C1204.4490%2C1204.6646%2C1204.1955%2C1204.6510%2C1204.1067%2C1204.1832%2C1204.6511%2C1204.4364%2C1204.5860%2C1204.3151%2C1204.2785%2C1204.1704%2C1204.1026%2C1204.1085%2C1204.4091%2C1204.1256%2C1204.6312%2C1204.6131%2C1204.2707%2C1204.4438%2C1204.3960%2C1204.6041%2C1204.3544%2C1204.2287%2C1204.0216%2C1204.0985%2C1204.5068%2C1204.5302%2C1204.1463%2C1204.1468%2C1204.3913%2C1204.3321%2C1204.0476%2C1204.5363%2C1204.6050%2C1204.6419%2C1204.0658%2C1204.0075%2C1204.2134%2C1204.4856%2C1204.4532%2C1204.5817%2C1204.2619%2C1204.0892%2C1204.3689%2C1204.0432%2C1204.1692%2C1204.0639%2C1204.6336%2C1204.3612&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Mathematical Modeling of Competitive Group Recommendation Systems with\n  Application to Peer Review Systems"}, "summary": "In this paper, we present a mathematical model to capture various factors\nwhich may influence the accuracy of a competitive group recommendation system.\nWe apply this model to peer review systems, i.e., conference or research grants\nreview, which is an essential component in our scientific community. We explore\nnumber of important questions, i.e., how will the number of reviews per paper\naffect the accuracy of the overall recommendation? Will the score aggregation\npolicy influence the final recommendation? How reviewers' preference may affect\nthe accuracy of the final recommendation? To answer these important questions,\nwe formally analyze our model. Through this analysis, we obtain the insight on\nhow to design a randomized algorithm which is both computationally efficient\nand asymptotically accurate in evaluating the accuracy of a competitive group\nrecommendation system. We obtain number of interesting observations: i.e., for\na medium tier conference, three reviews per paper is sufficient for a high\naccuracy recommendation. For prestigious conferences, one may need at least\nseven reviews per paper to achieve high accuracy. We also propose a\nheterogeneous review strategy which requires equal or less reviewing workload,\nbut can improve over a homogeneous review strategy in recommendation accuracy\nby as much as 30% . We believe our models and methodology are important\nbuilding blocks to study competitive group recommendation systems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1010.2444%2C1010.4049%2C1010.0564%2C1010.1926%2C1010.4633%2C1010.3033%2C1010.4485%2C1010.0273%2C1010.4630%2C1010.2767%2C1010.2042%2C1204.5367%2C1204.1998%2C1204.6281%2C1204.4224%2C1204.6082%2C1204.3278%2C1204.4101%2C1204.4353%2C1204.6251%2C1204.3786%2C1204.2379%2C1204.5210%2C1204.5482%2C1204.2576%2C1204.0019%2C1204.0144%2C1204.3352%2C1204.2836%2C1204.3601%2C1204.3743%2C1204.5756%2C1204.5980%2C1204.4688%2C1204.6029%2C1204.6288%2C1204.1825%2C1204.1003%2C1204.1452%2C1204.1643%2C1204.1949%2C1204.5044%2C1204.5540%2C1204.2495%2C1204.4075%2C1204.6653%2C1204.0539%2C1204.1835%2C1204.4445%2C1204.0542%2C1204.3434%2C1204.4490%2C1204.6646%2C1204.1955%2C1204.6510%2C1204.1067%2C1204.1832%2C1204.6511%2C1204.4364%2C1204.5860%2C1204.3151%2C1204.2785%2C1204.1704%2C1204.1026%2C1204.1085%2C1204.4091%2C1204.1256%2C1204.6312%2C1204.6131%2C1204.2707%2C1204.4438%2C1204.3960%2C1204.6041%2C1204.3544%2C1204.2287%2C1204.0216%2C1204.0985%2C1204.5068%2C1204.5302%2C1204.1463%2C1204.1468%2C1204.3913%2C1204.3321%2C1204.0476%2C1204.5363%2C1204.6050%2C1204.6419%2C1204.0658%2C1204.0075%2C1204.2134%2C1204.4856%2C1204.4532%2C1204.5817%2C1204.2619%2C1204.0892%2C1204.3689%2C1204.0432%2C1204.1692%2C1204.0639%2C1204.6336%2C1204.3612&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper, we present a mathematical model to capture various factors\nwhich may influence the accuracy of a competitive group recommendation system.\nWe apply this model to peer review systems, i.e., conference or research grants\nreview, which is an essential component in our scientific community. We explore\nnumber of important questions, i.e., how will the number of reviews per paper\naffect the accuracy of the overall recommendation? Will the score aggregation\npolicy influence the final recommendation? How reviewers' preference may affect\nthe accuracy of the final recommendation? To answer these important questions,\nwe formally analyze our model. Through this analysis, we obtain the insight on\nhow to design a randomized algorithm which is both computationally efficient\nand asymptotically accurate in evaluating the accuracy of a competitive group\nrecommendation system. We obtain number of interesting observations: i.e., for\na medium tier conference, three reviews per paper is sufficient for a high\naccuracy recommendation. For prestigious conferences, one may need at least\nseven reviews per paper to achieve high accuracy. We also propose a\nheterogeneous review strategy which requires equal or less reviewing workload,\nbut can improve over a homogeneous review strategy in recommendation accuracy\nby as much as 30% . We believe our models and methodology are important\nbuilding blocks to study competitive group recommendation systems."}, "authors": ["Hong Xie", "John C. S. Lui"], "author_detail": {"name": "John C. S. Lui"}, "author": "John C. S. Lui", "arxiv_comment": "35 pages", "links": [{"href": "http://arxiv.org/abs/1204.1832v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.1832v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.PF", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.1832v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.1832v2", "journal_reference": null, "doi": null, "fulltext": "arXiv:1204.1832v2 [cs.IR] 12 Apr 2012\n\nMathematical Modeling of Competitive Group\nRecommendation Systems with Application to Peer Review\nSystems\nHong Xie John C.S. Lui\nComputer Science & Engineering Department\nThe Chinese University of Hong Kong\nEmail: {hxie,cslui}@cse.cuhk.edu.hk\nNovember 2, 2018\nAbstract\nIn this paper, we present a mathematical model to capture various factors which may influence the accuracy\nof a competitive group recommendation system. We apply this model to peer review systems, i.e., conference or\nresearch grants review, which is an essential component in our scientific community. We explore number of important\nquestions, i.e., how will the number of reviews per paper affect the accuracy of the overall recommendation? Will\nthe score aggregation policy influence the final recommendation? How reviewers' preference may affect the accuracy\nof the final recommendation? To answer these important questions, we formally analyze our model. Through this\nanalysis, we obtain the insight on how to design a randomized algorithm which is both computationally efficient\nand asymptotically accurate in evaluating the accuracy of a competitive group recommendation system. We obtain\nnumber of interesting observations: i.e., for a medium tier conference, three reviews per paper is sufficient for a high\naccuracy recommendation. For prestigious conferences, one may need at least seven reviews per paper to achieve\nhigh accuracy. We also propose a heterogeneous review strategy which requires equal or less reviewing workload,\nbut can improve over a homogeneous review strategy in recommendation accuracy by as much as 30% . We believe\nour models and methodology are important building blocks to study competitive group recommendation systems.\n\n1 Introduction\nIn recent years, recommendation systems [20] have received a lot of attention in both commercial and academic\ncommunities. Researchers investigate various algorithmic and complexity issues [7, 10, 20, 22, 23], at the same time,\nwe also see successful applications of recommendation systems in commercial products. In general, recommendation\nsystems take into account a user's preference and make a recommendation so as to maximize the user's utility. Group\nrecommendation systems [3], on the other hand, take into account the preferences of all users in a group to make\na single recommendation. In recent years, we have seen successful group recommendations in commercial areas\n[1, 3, 12, 13, 15, 16, 18].\nIn this paper, we consider a special class of recommendation system which we call the competitive group recommendation system: there are N users and the system will make a single recommendation to k users only, where k \u2264 N ,\nwhile N \u2212k users will receive the complement of the recommendation. Competitive group recommendation systems\nhave many important applications. In here, we consider an application which is dearest to many researchers' heart:\npeer review systems for conferences or research grant proposals. To a certain degree, the progress of our scientific\ncommunity depends on the accuracy this type of recommendation systems. To the best of our knowledge, this is the\nfirst paper which provides a formal mathematical analysis to such recommendation systems.\nA peer review system can be briefly described as follows: there are N candidates (papers or grant proposals), a\ngroup of reviewers is asked to review these candidates. Each reviewer evaluates a subset of these candidates based on\nher preference, and will provide a rating for each candidate. The system will use some policies to aggregate all ratings\nof all candidates, and will only recommend a subset k candidates for acceptance, while all other candidates will receive\na rejection, which is the complement of the acceptance recommendation. For such systems, there are many interesting\n1\n\n\fquestions to explore, e.g., to achieve high accuracy, how many reviews each candidate should receive? What is the\nprobability that the best candidate will be accepted or rejected? How reviewers' preference may influence the final\nrecommendation? Is one rating aggregation policy more accurate than others?\nOur contribution can be summarized as follows:\n\u2022 We propose a mathematical model to understand the accuracy of a competitive group recommendation system\nand apply it to conference review systems.\n\u2022 We formally analyze the model. Through this analysis, we gain the insight to create a randomized algorithm\nto evaluate the model. We show our algorithm is computationally efficient and also provides performance\nguarantees.\n\u2022 We apply our model to a conference review system and show many interesting insights, i.e., for a medium\ntiter conference, three reviews per paper can guarantee a highly accurate recommendation, but for prestigious\nconferences, we need at least seven reviews per paper.\n\u2022 We propose a two round heterogeneous review strategy which outperforms the homogeneous review strategy by\nas much as 30% in recommendation accuracy with the the same or less reviewing workload.\nThis is the outline of the paper. In Section 2, we present the mathematical model of competitive group recommendation systems. In Section 3, we present analysis and derive theoretical results of the model. In Section 4, we propose\na randomized algorithm which is computationally efficient and provides performance guarantees in evaluating a competitive group recommendation system. In Section 5, we evaluate the performance of a conference review system and\nexplore various factors that influence its accuracy. Related work is given in Section 6 and Section7 concludes.\n\n2 Mathematical Model\nLet us present the mathematical model of a competitive group recommendation system and we focus on a particular\napplication scenario, a conference review system which is a representative example of peer review systems. Let\nP = {P1 , . . . , PN } be a finite set of N candidate papers. Let Qi \u2208 (1, m) represent the intrinsic quality of paper Pi .\nHigher value of intrinsic quality implies higher quality. Hence, if Qi > Qj , it means paper Pi is better than Pj . Without\nany loss of generality, let us assume Q1 > Q2 > * * * > QN . It is important to emphasize that reviewers of these papers\ndo not have any a-prior knowledge of Qi , \u2200i. The conference can only accept k papers, where 1 \u2264 k \u2264 N . Let AI (k)\nand A(k) denote the set of the k accepted papers according to the intrinsic quality or according to the conference\nrecommendation criteria respectively. It is clear that AI (k) = {P1 , P2 , * * * , Pk }, and if a conference recommendation\nsystem is perfect, we should have AI (k) = A(k). But in general, many factors or reviewers' preference may influence\nthe final recommendation, hence AI (k) 6= A(k). To measure the accuracy of a recommendation system, we aim to\ndetermine how many papers in A(k) are also in AI (k). Formally, we seek to derive the following probability mass\nfunction (pmf) :\nPr[|AI (k) \u2229 A(k)| = i],\nfor i = 0, 1, . . . , k.\nIntuitively, if Pr[|AI (k) \u2229 A(k)| = k] occurs with a high probability, then the conference recommendation system is\naccurate and at the same time, robust against different scoring and human factors.\n\nLet R be a finite set of M reviewers. We assume that the reviewers are independent. Reviewers do not have a\ndirect knowledge of Qi , \u2200i, the intrinsic quality of papers, and they evaluate papers based on their own preference.\nA reviewer submits a score for each paper after reviewing. Scores are discrete and take on value in {1, . . . , m}.\nPaper Pi , for i = 1, . . . , N , is assigned to ni \u2265 1 reviewers. Hence, paper Pi receives ni reviewing scores. Let\nS i = {S1i , . . . , Sni i } denote the set of ni scores of paper Pi , where Sji \u2208 {1, . . . , m}. Let R(Sji ) represent the reviewer\nwho submits score Sji . Let eij \u2208 {1, . . . , l} represent the expertise level (or familiarity) that reviewer R(Sji ) selects on\ntopics related to paper Pi . Reviewer R(Sji ) submits score Sji in conjunction with expertise level eij . Again, we adopt\nthe convention that higher values represent higher quality or expertise level. There are number of interesting questions\none can explore, i.e., how M , the number of reviewers (or the size of a technical program committee), as well as ni , \u2200i,\nthe number of reviews for each paper, may affect the accuracy of the final recommendation?\nLet V be the voting rule that is used by the conference recommendation system to rank papers based on their\nreviewing scores. Generally, a voting rule works in two steps. It first aggregates the reviewing scores of each paper into\n\n2\n\n\fa combined overall score. Then it ranks all papers based on their respective combined overall scores. Let \u03b3i = V(S i )\nbe the combined overall score of paper Pi derived from S i under the voting rule V. ThereP\ncan be many voting rules.\nA simple and often used voting rule is the average score rule. In this case, we have \u03b3i = S\u2208S i S/|S i |. By sorting\n\u03b31 , . . . , \u03b3N , we obtain a ranked list of all papers. Again, there are number of interesting questions to explore, i.e., what\nare some effective voting rules? Can one voting rule be more accurate than others?\nSpecifying the voting rule is not enough. Recall that the system can only accept k papers. It may happen that\nthe combined overall score of the k th ranked paper, equals to that of (k + 1)-th ranked paper. In this case, we need\nto specify a tie-breaking rule to decide which paper should be selected. Let T denote the tie-breaking rule. It is\ninteresting to explore whether the recommendation results are sensitive to a particular tie-breaking rule.\nTo answer the above questions, let us now present probabilistic models in describing the intrinsic quality (or the\nself-selection effect), the reviewing behavior, as well as critical degree of reviewers.\n\n2.1 Model Intrinsic Quality via Self-selection\nIt is well-known that paper submission has the self-selection effect. In other words, authors tend to submit their high\nquality papers to some highly prestigious and selective conferences, while lower tier conferences may receive papers\nwith lower quality, or candidate papers have high variance in quality. The intrinsic quality of a submitted paper can be\ndescribed as a random variable, and one can vary its mean or variance to reflect the self-selection effect. Specifically,\na high value of mean and a small value of variance imply that the submitted papers are of high intrinsic qualities and\nthese qualities have small variation only. On the other hand, a low value of mean and a large value of variance imply\nthat the submitted papers have low intrinsic qualities and these qualities have high variability.\nWe use Qi \u2208 (1, m) to denote the intrinsic quality of paper Pi . Assume Q1 , ..., QN are independent random\nvariables. Let D(Qi ) denote the probability distribution of Qi . The probability distribution D(Qi ) is described by a\ntruncated normal distribution N (qi , \u03c3i2 ), where qi \u2208 (1, m) is the mean and \u03c3i2 is the variance. Since the value of the\nintrinsic quality Qi is in (1, m), thus D(Qi ) is obtained by truncating N (qi , \u03c3i2 ) to keep those values in (1, m) and\nscaling up the kept values by 1/Pr[1 < X < m], where X is a random variable with probability distribution N (qi , \u03c3i2 ).\nIt should be clear that after truncation, the mean qi and the variance \u03c3i2 can still reflect the self-selection effect. Here,\nwe use the following parameters to reflect four representative types of self-selectivity:\nHigh self-selectivity: the mean qi and variance \u03c3i2 are specified by\nqi = m, \u03c3i2 = 1,\n\nfor i = 1, . . . , N .\n\n(1)\n\nThis indicates that papers tend to have high intrinsic quality (or high mean), and most of the probability mass concentrates around high intrinsic quality. Top tier conferences fall into this category.\nMedium self-selectivity: the mean qi and variance \u03c3i2 are\nqi = (m + 1)/2, \u03c3i2 = 1,\n\nfor i = 1, . . . , N .\n\n(2)\n\nThis reflects that papers tend to have an average intrinsic quality and most of the probability mass concentrates around\nthe average intrinsic quality. Medium tier conferences fall into this category.\nLow self-selectivity: the mean qi and variance \u03c3i2 are\nqi = 1, \u03c3i2 = 1,\n\nfor i = 1, . . . , N .\n\n(3)\n\nThis indicates that papers tend to have low intrinsic quality (or low mean), and most of the probability mass concentrates around low quality. Low tier conferences fall into this category.\nRandom self-selectivity: the variance \u03c3i2 is\n\u03c3i2 = \u221e,\n\nfor i = 1, . . . , N .\n\n(4)\n\nSo D(Qi ) converges to a uniform distribution on (1, m). This means that the intrinsic qualities of submitted papers are\nuniformly distributed. Newly started conferences fall into this category. This is because a newly started conference has\nnot built up a reputation yet, thus researchers are not sure if it is a good conference, which results in random quality in\nsubmission.\n\n3\n\n\f2.2 Model for Reviewing Behavior\nWhen reviewing a paper, a reviewer needs to evaluate its quality. Here we assume that each reviewer is fair, unbias and\ncritical. We consider two most important factors that affect the evaluation. The first one is Qi , the intrinsic quality of\npaper Pi , and the second one is the critical degree of the reviewer. Specifically, when Qi is high, the evaluated quality\nof Pi is more likely to be high. And the higher the critical degree of the reviewer, the more likely that the evaluated\nquality tends to be close to the intrinsic quality of that paper. The reviewing behavior can be described by a random\nvariable and one can vary its mean and variance to reflect the intrinsic quality and critical degree.\nTo illustrate, consider a paper Pi and one of its score Sji . Recall that the reviewer who submits score Sji is denoted\nby R(Sji ). Let cij \u2208 [0, 1] denote the critical degree of reviewer R(Sji ). With the usual convention, higher value\nrepresents higher critical degree. The score Sji is a random variable with probability distribution D(Sji ), which should\nhave the following two properties:\nProperty 1: The mean should be equal to Qi . The physical meaning is that a reviewer is unbias.\nProperty 2: The variance should reflect the critical degree of a reviewer. Specifically, the higher the critical degree,\nthe lower the variance for the probability distribution D(Sji ).\n\u0001\nIn our study, the probability distribution D(Sji ) is obtained by mapping a normal distribution N Qi , \u03c3 2 (cij ) to\na discrete distribution. Note that the standard variance \u03c3(cij ) is a monotonic decreasing function of cij and we will\nspecify it in later section. The probability distribution mapping can be described by the following two steps:\nDiscretization \u0001: Transform a normal distribution into a discrete distribution. We transform the normal distribution\nN Qi , \u03c3 2 (cij ) into a discrete random variable L with probability distribution\n\u0001\ne Qi , \u03c3 2 (ci ) with values in {1, ..., m}. The pmf of L is:\nN\nj\n\u0001\n\u0001\n\u03a6 (l + 0.5 \u2212 Qi )/\u03c3(cij ) \u2212 \u03a6 (l \u2212 0.5 \u2212 Qi )/\u03c3(cij )\nPr[l \u2212 0.5 \u2264 X \u2264 l + 0.5]\n\u0001 ,\n\u0001\nPr[L = l] =\n=\nPr[0.5 \u2264 X \u2264 m + 0.5]\n\u03a6 (m + 0.5 \u2212 Qi )/\u03c3(cij ) \u2212 \u03a6 (0.5 \u2212 Qi )/\u03c3(cij )\nfor l = 1, . . . , m\n\n(5)\n\n\u221a\n\u0001\nRx\nwhere \u03a6(x) = \u2212\u221e exp(\u2212t2 /2)/ 2\u03c0dt and the probability distribution of X is N Qi , \u03c3 2 (cij ) . Note that this\ndiscrete distribution satisfies Property 2 but not Property 1. In the following step, we adjust the distribution so that it\nsatisfies Property 1 also.\n\u0001\ne Qi , \u03c3 2 (ci ) such that its mean equals to Qi . The idea is that if E[L] < Qi ,\nAdjustment: Adjust the distribution N\nj\n\u0001\ne Qi , \u03c3 2 (ci ) by scaling up the probability:\nthen we increase the mean of N\nj\n\nPr[L = l],\n\nfor all l = \u230aQi \u230b + 1, . . . , m.\n\n\u0001\ne Qi , \u03c3 2 (ci ) , we obtain\nElse, we decrease the mean by scaling them down. Applying this idea to adjust the mean of N\nj\nthe probability distribution D(Sji ). The pmf of D(Sji ) is:\nPr[Sji\n\n(\n[1 \u2212 \u03b2(Qi , cij )]Pr[L = l], \u2200l = 1, . . . , \u230aQi \u230b\n= l] =\n,\n[1 + \u03b1(Qi , cij )]Pr[L = l], \u2200l = \u230aQi \u230b + 1, . . . , m\n\n(6)\n\nwhere \u03b1(Qi , cij ) and \u03b2(Qi , cij ) are:\nP\u230aQi \u230b\n\nPr[L = l](Qi \u2212 E[L])\n,\n(7)\n= Pl=1\n\u230aQi \u230b\nl=1 Pr[L = l](E[L] \u2212 l)\nPm\nl=\u230aQi \u230b+1 Pr[L = l](Qi \u2212 E[L])\ni\n\u03b2(Qi , cj ) =\n,\n(8)\nP\u230aQi \u230b\nl=1 Pr[L = l](E[L] \u2212 l)\n\u0001\ne Qi , \u03c3 2 (ci ) . Note that D(S i ) satisfies both\nand L is a discrete random variable with probability distribution N\nj\nj\nProperty 1 and 2.\n\u03b1(Qi , cij )\n\n4\n\n\f2.3 Model for Critical Degree\nTo model the critical degree of a reviewer, we classify papers and reviewers into \"types\". Specifically, a paper can\nbe of many types (e.g., system paper, theory paper, etc), and reviewers can be of many types also (e.g., prefer system\npaper, or theory paper, etc). If a paper-reviewer pairing is of the same type, then the expertise level and the critical\ndegree of the reviewer will be of high values, else they will be of low values.\nTo illustrate, consider a paper P \u2208 P and a reviewer R \u2208 R. Assume reviewer R reviews paper P . Let u \u2208 [0, 1]\ndenote the matching degree between reviewer R and paper P and let e, c denote the corresponding expertise level\nand critical degree respectively. Using our usual convention, higher value represents higher matching degree. The\nmatching degree couples the expertise level and the critical degree in the following manner:\ne\nc\n\n=\n=\n\n\u03ba,\nif \u03bc \u2208 [(\u03ba \u2212 1)/l, \u03ba/l), for \u03ba = 1, . . . , l,\nf (\u03bc) \u2208 [0, 1],\nwhere \u03bc \u2208 [0, 1].\n\n(9)\n\nNote that e = l when \u03bc = 1 and f (\u03bc) is a monotonic increasing function of \u03bc. There are number of choices for function\nf (\u03bc), e.g., f (\u03bc) = \u03bc, or f (\u03bc) = \u03bc2 , etc. We will specify it in later section.\nAgain there are number of interesting questions to explore, i.e., is the conference recommendation system sensitive\nto the paper-reviewer matching? Will a small percentage of reviewers who prefer theory create a large inaccuracy in\nthe final recommendation of a system-oriented conference (or vice versa)?\n\n3 Theoretical Analysis\nRecall that AI (k) and A(k) denote the set of k accepted papers according to the intrinsic quality or according to\nthe conference recommendation criteria respectively. In this section, we first derive the following probability mass\nfunction (pmf) :\nPr[|AI (k) \u2229 A(k)| = i],\nfor i = 0, 1, . . . , k.\n\nWith this pmf, we can then derive the expectation E[|AI (k) \u2229 A(k)|] and variance Var[|AI (k) \u2229 A(k)|]. The above\nprobability measures can provide us with a lot of insights, e.g., if Pr[|AI (k)\u2229A(k)| = k] occurs with a high probability,\nor E[|AI (k) \u2229 A(k)|] \u2248 k, then the conference recommendation system is very accurate and robust against different\nhuman factors, or if Var[|AI (k) \u2229 A(k)|] is of small value, then the conference recommendation system is very stable\nlikely to be close to the expectation. To derive this pmf, let us first consider the following special case. The purpose is\nto show the general idea of derivation and to illustrate the underlying computational complexity. We will consider the\nderivation of the general case later.\n\n3.1 Derivation of the Special Case\nLet us consider a conference recommendation system which has only one type of papers and one type of reviewers\n(e.g., all papers are theoretical and all reviewers prefer theoretical topics). Hence, the critical degree of all reviewers\nare the same, say c. The intrinsic quality of each paper is specified as follows:\nQi = m \u2212 i(m \u2212 1)/(N + 1),\n\nfor i = 1, . . . , N .\n\n(10)\n\nEach paper will have the same number of reviews, or ni = n, \u2200i. The voting rule V is the average score rule and we\nuse a random rule to tie-break any papers whose scores are the same.\n3.1.1 Theoretical derivation\nThe score set for paper Pi is {S1i , . . . , Sni }. Recall that a score is described by a random variable, and its probability distribution is uniquely determined by the intrinsic quality of the corresponding paper and critical degree of the\ncorresponding reviewer. Since the critical degree of each reviewer is the same c, thus S1i , . . . , Sni are i.i.d. random\nvariables. By specifying Qi and cij in Eq. (6) with Eq. (10) and cij = c respectively, we obtain the pmf of score Sji .\nThis is stated in the following lemma.\n\n5\n\n\fLemma 1 The pmf of score Sji , for i = 1, . . . , N , j = 1, . . . , n, is\n\uf8f1\n\uf8f4\n\uf8f2[1 \u2212 \u03b2(m \u2212\nPr[Sji = l] = [1 + \u03b1(m \u2212\n\uf8f4\n\uf8f3\n0,\n\ni(m\u22121)\nN +1 , c)]Pr[L\ni(m\u22121)\nN +1 , c)]Pr[L\n\n= l], \u2200l = 1, . . . , \u230aQi \u230b\n= l], \u2200l = \u230aQi \u230b + 1, . . . , m ,\notherwise\n\n(11)\n\n\u0001\ne m \u2212 i(m \u2212 1)/(N + 1), \u03c3 2 (c) whose pmf is\nwhere L is a discrete random variable with probability distribution N\nderived by Eq. (5), and \u03b1(m \u2212 i(m \u2212 1)/(N + 1), c), \u03b2(m \u2212 i(m \u2212 1)/(N + 1), c) are derived by Eq. (7) and (8)\nrespectively.\nPn\nThe average score of each paper is \u03b3i = j=1 Sji /n, \u2200i. The probability mass function (pmf) of the average score\nof each paper is specified in the following lemma.\nLemma 2 The pmf of the averages score \u03b3i , \u2200i, is\n\u0014\n\u0015 X\nYn\nl\nPr \u03b3i =\nPr[Sji = sj ],\n=\nPn\nj=1\ns\n=l\nn\nj=1 j\nand its cumulative distribution function (CDF) is\n\u0014\n\u0015 X\nYn\nl\nPr \u03b3i \u2264\nPr[Sji = sj ],\n=\nPn\nj=1\nn\nj=1 sj \u2264l\n\nfor all l = n, . . . , nm,\n\n(12)\n\nfor all l = n, . . . , nm,\n\n(13)\n\nwhere Pr[Sji = sj ] is specified in Eq. (11).\n\nProof: Note that, the ratings of each\nPn has been\nPn paper are independent random variables. The distribution of each rating\nderived in Lemma 1. Since \u03b3i = j=1 Sji /n, thus by enumerating all the cases satisfying the condition j=1 Sji = l,\nwe could obtain the pmd of \u03b3i , or\n\u0015\n\u0014\nhXn\ni X\nYn\nl\nPr[Sji = sj ].\n= Pr\nSji = l =\nPr \u03b3i =\nPn\nj=1\ns\n=l\nj=1\nn\nj=1 j\nPn\nSimilarly, by enumerating all the cases satisfying the condition j=1 Sji \u2264 l, we could obtain the CDF of \u03b3i , or\n\u0014\n\u0015\ni X\nhXn\nYn\nl\nPr \u03b3i \u2264\nPr[Sji = sj ],\nSji \u2264 l =\n= Pr\nPn\nj=1\ns\n\u2264l\nj=1\nj\nn\nj=1\n\nwhich completes the proof.\nBased on the results derived in Lemma 1 and Lemma 2, the probability that a specific set of papers is accepted is\nstated as follows.\nTheorem 1 Let {Pi1 , . . . , Pik } be a set of k papers. The probability that the accepted paper set equals to {Pi1 , . . . , Pik }\nis:\n!\nnm\nX\nY\nY\nY\nPr [\u03b3j \u2264 (l \u2212 1)/n] +\nPr[A(k) = {Pi1 , . . . , Pik }] =\nPr [\u03b3i \u2264 l/n] \u2212\nPr [\u03b3i \u2264 (l \u2212 1)/n]\nl=n\n\ni\u2208I\n\nX\n\nF \u2286I, G\u2286I, F , G6=\u2205\n\nY\n\n\u03ba\u2208I\\G\n\ni\u2208I\n\nj\u2208I\n\n\u0012\n\u0013\u22121 X\nnm Y\nY\n|F \u222a G|\n(1 \u2212 Pr [\u03b3i \u2264 l/n])\nPr [\u03b3j = l/n]\n|F |\nl=n i\u2208I\\F\n\nPr [\u03b3\u03ba \u2264 (l \u2212 1)/n] ,\n\nj\u2208F \u222aG\n\n(14)\n\nwhere I = {i1 , . . . , ik } is the index set of {Pi1 , . . . , Pik }, and I = {1, . . . , N }\\I is the complement of I. And\nPr[\u03b3i = l/n] is specified in Eq. (12), and Pr[\u03b3i \u2264 l/n] is specified in Eq. (13).\n6\n\n\fProof: Let H = {Pi1 , . . . , Pik }. Let H = {P1 , . . . , PN }\\H be the complement of H. Let \u03b3min (H) = min{\u03b3i , i \u2208 I}\ndenote the minimum average score of paper set H. Let \u03b3max (H) = max{\u03b3i , i \u2208 I} denote the maximum average score\nof paper set H. The probability that the accepted paper set equals to {Pi1 , . . . , Pik } can be divided into the following\nthree parts:\nPr[A(k) = {Pi1 , . . . , Pik }] = Pr[A(k) = H, \u03b3min (H) < \u03b3max (H)]+\n\nPr[A(k) = H, \u03b3min (H) > \u03b3 max (H)]+\n\nPr[A(k) = H, \u03b3 min (H) = \u03b3 max (H)].\n\n(15)\n\nLet us derive these three terms one by one.\nAccording to our voting rule, the accepted paper set A(k) equals to H is 0 conditioned on that \u03b3 min (H) is less\nthan \u03b3 max (H). Thus,\nPr[A(k) = H, \u03b3min (H) < \u03b3max (H)] =\n\n=\n\nPr[\u03b3min (H) < \u03b3max (H)]Pr[A(k) = H | \u03b3 min (H) < \u03b3 max (H)]\n0\n\n(16)\n\nAccording to our voting rule, the accepted paper set A(k) equals to H is 1 conditioned on \u03b3 min (H) > \u03b3 max (H).\nThus,\nPr[A(k) = H, \u03b3min (H) > \u03b3max (H)] = Pr[\u03b3min (H) > \u03b3max (H)]Pr[A(k) = H | \u03b3min (H) > \u03b3max (H)]\n= Pr[\u03b3min (H) > \u03b3max (H)].\n\n(17)\n\nPn\n\ni\ni\nNote that \u03b3i =\ni=1 Sj /n, \u2200i. Since the scores Sj , \u2200i, j, are independent random variables, thus the average\nscores \u03b31 , . . . , \u03b3N are also independent random variables. Based on this fact, we derive the analytical expression\nof Pr[\u03b3 min (H) > \u03b3max (H)] as\nXnm\n\u0002\n\u0003\nPr [\u03b3min (H) = l/n] Pr \u03b3max (H) < l/n\nPr[\u03b3min (H) > \u03b3max (H)] =\nl=n\n\u0011Y\nXnm\u0010Y\nY\nPr[\u03b3j \u2264 (l \u2212 1)/n]. (18)\n=\nPr[\u03b3i \u2264 l/n]\u2212\nPr[\u03b3i \u2264 (l\u22121)/n]\nl=n\n\ni\u2208I\n\ni\u2208I\n\nj\u2208I\n\nThe remaining task is to derive the last term of Eq. (15). When \u03b3min (H) = \u03b3max (H) occurs, tie breaking will\nbe performed on the set of papers with the average score equal to \u03b3min (H). Let us provide some notations first. Let\nF = {i | \u03b3i = \u03b3min (H), i \u2208 I} be the index set of the papers that belong to set H and with average scores equal to\nminimum average score of H. Let G = {i | \u03b3i = \u03b3min (H), i \u2208 I} be the index set of the papers that belong to set H\nand with average scores equal to the minimum average score of H. Thus, tie breaking will perform on the papers with\nindex set F \u222a G, from which only |F | papers will be selected for acceptance. By enumerating all possible tie breaking\npaper sets, we can divide the the last term of Eq. (15) into the following form:\nX\nPr[F \u222a G]Pr[F | F \u222a G],\n(19)\nPr[A(k) = H, \u03b3min (H) = \u03b3max (H)] =\nF \u2286I, G\u2286I, F , G6=\u2205\n\nwhere Pr[F \u222a G] is the probability that tie breaking is performed on papers with index set F \u222a G, and Pr[F |F \u222a G] is\nthe conditional probability that papers with index set F is selected for acceptance under the condition that tie breaking\nis performed on the papers with index set F \u222a G. Since the tie-breaking rule is the random rule, under which we just\nrandomly pick |F | papers, thus\n\u0012\n\u0013\u22121\n|F \u222a G|\nPr[F |F \u222a G] =\n.\n(20)\n|F |\n\nBecause the average scores \u03b31 , . . . , \u03b3N are independent random variables, we can derive Pr[F \u222a G] as:\nPr[F \u222a G] = Pr[\u03b3i > \u03b3 min (H)\nPr[\u03b3i = \u03b3 min (H)\n\nfor all i \u2208 I \\F ]\u00d7\nfor all i \u2208 F \u222a G ]\u00d7\n\nPr[\u03b3i < \u03b3 min (H) for all i \u2208 I \\G ]\nnm Y\nY\nY\nX\n(1 \u2212 Pr [\u03b3i \u2264 l/n])\nPr [\u03b3j = l/n]\nPr [\u03b3\u03ba \u2264 (l \u2212 1)/n] .\n=\nl=n i\u2208I\\F\n\nj\u2208F \u222aG\n\n7\n\n\u03ba\u2208I\\G\n\n(21)\n\n\fCombining Eq. (19) \u2212 (21) we obtain\n\n\u0013\u22121 Y\nnm \u0012\nX\n|F \u222a G|\nPr[A(k) = H, \u03b3 min (H) = \u03b3 max (H)] =\n(1 \u2212 Pr [\u03b3i \u2264 l/n])\n|F |\ni\u2208I\\F\nF \u2286I, G\u2286I, F , G6=\u2205 l=n\nY\nY\nPr [\u03b3\u03ba \u2264 (l \u2212 1)/n] .\nPr [\u03b3j = l/n]\nX\n\nj\u2208F \u222aG\n\n(22)\n\n\u03ba\u2208I\\G\n\nCombining Eq. (15) \u2212 (18), and (22), we obtain Eq. (14).\nTo illustrate the analytical expression of Pr[A(k) = {Pi1 , . . . , Pik }], consider a simple example where n = 1,\nm = 2, N = 3 and k = 1. Table 1 shows the analytical expression Pr[A(1) = {Pi1 }].\n{Pi1 }\n\nPr[A(1) = {Pi1 }]\nPr[\u03b31 = 2]Pr[\u03b32 = 1]Pr[\u03b33 = 1] + Pr[\u03b31 = 2]Pr[\u03b32 = 2]Pr[\u03b33 = 1]/2+\n\n{P1 } Pr[\u03b31 = 2]Pr[\u03b33 = 2]Pr[\u03b32 = 1]/2 + Pr[\u03b31 = 1]Pr[\u03b33 = 1]Pr[\u03b32 = 1]/3\nPr[\u03b31 = 2]Pr[\u03b33 = 2]Pr[\u03b32 = 2]/3\nPr[\u03b32 = 2]Pr[\u03b31 = 1]Pr[\u03b33 = 1] + Pr[\u03b32 = 2]Pr[\u03b31 = 2]Pr[\u03b33 = 1]/2+\n{P2 } Pr[\u03b32 = 2]Pr[\u03b33 = 2]Pr[\u03b31 = 1]/2 + Pr[\u03b32 = 1]Pr[\u03b33 = 1]Pr[\u03b31 = 1]/3\nPr[\u03b32 = 2]Pr[\u03b33 = 2]Pr[\u03b31 = 2]/3\nPr[\u03b33 = 2]Pr[\u03b32 = 1]Pr[\u03b31 = 1] + Pr[\u03b33 = 2]Pr[\u03b32 = 2]Pr[\u03b31 = 1]/2+\n{P3 } Pr[\u03b33 = 2]Pr[\u03b31 = 2]Pr[\u03b32 = 1]/2 + Pr[\u03b33 = 1]Pr[\u03b31 = 1]Pr[\u03b32 = 1]/3\nPr[\u03b33 = 2]Pr[\u03b31 = 2]Pr[\u03b32 = 2]/3\nTable 1: Examples of the analytical expression of Pr[A(1) = {Pi1 }], where i1 = 1, . . . , 3.\nUp to now, we have derived the probability of a specific set of accepted papers. Let us derive the pmf of a general\nset of k papers:\nPr[|AI (k) \u2229 A(k)| = i],\nfor i = 0, 1, . . . , k,\nwhich is shown in the following theorem.\nTheorem 2 The pmf of |AI (k) \u2229 A(k)| is:\nPr[|AI (k) \u2229 A(k)| = i] =\n\nX\n\nF \u2286AI (k),\n|F |=i\n\nX\n\nG\u2286AI (k),\n|G|=k\u2212i\n\nPr[A(k) = F \u222a G],\n\nfor all i = 0, 1, . . . , k,\n\n(23)\n\nwhere AI (k) = {P1 , . . . , PN }/AI (k) is the complement of AI (k) and Pr[A(k) = F \u222a G] is given in Eq. (14).\nProof: The paper set A(k) can be divided into two disjoint subsets of which one is A(k) \u2229 AI (k) and the other\none is A(k) \u2229 AI (k). Note that we have derived the pmf that a specific set of accepted papers in Eq. (14). Then\nby enumerating subsets of A(k) with cardinality i and all the subsets of AI (k) with cardinality k \u2212 i we can obtain\nprobability Pr[|AI (k) \u2229 A(k)| = i], or\nX\nX\nPr[A(k) = F \u222a G],\nPr[|AI (k) \u2229 A(k)| = i] =\nF \u2286AI (k),\nG\u2286AI (k),\n|F |=i\n\n|G|=k\u2212i\n\nfor all i = 0, 1, . . . , k,\n\nwhere Pr[A(k) = F \u222a G] is given in Eq. (14).\n\n8\n\n\fPr[AI (1) \u2229 A(1) = i]\n\ni\n\nPr[\u03b31 = 2]Pr[\u03b32 = 1]Pr[\u03b33 = 1] + Pr[\u03b31 = 2]Pr[\u03b32 = 2]Pr[\u03b33 = 1]/2+\n1\n\nPr[\u03b31 = 2]Pr[\u03b33 = 2]Pr[\u03b32 = 1]/2 + Pr[\u03b31 = 1]Pr[\u03b33 = 1]Pr[\u03b32 = 1]/3\nPr[\u03b31 = 2]Pr[\u03b33 = 2]Pr[\u03b32 = 2]/3\nPr[\u03b32 = 2]Pr[\u03b31 = 1]Pr[\u03b33 = 1] + Pr[\u03b33 = 2]Pr[\u03b32 = 1]Pr[\u03b31 = 1]+\n\n0\n\nPr[\u03b32 = 2]Pr[\u03b31 = 2]Pr[\u03b33 = 1]/2 + Pr[\u03b33 = 2]Pr[\u03b31 = 2]Pr[\u03b32 = 1]/2\nPr[\u03b32 = 2]Pr[\u03b33 = 2]Pr[\u03b31 = 1] + Pr[\u03b32 = 1]Pr[\u03b33 = 1]Pr[\u03b31 = 1] \u00d7 2/3\nPr[\u03b32 = 2]Pr[\u03b33 = 2]Pr[\u03b31 = 2] \u00d7 2/3\n\nTable 2: Examples of the analytical expression of Pr[AI (1) \u2229 A(1) = i], where i = 0, 1.\nTo illustrate the analytical expression of Pr[|AI (k) \u2229 A(k)| = i], let us consider the same example with Table 1.\nTable 2 shows the analytical expression Pr[AI (1) \u2229 A(1) = i], where i = 0, 1.\n\nNow we have derived the analytical expression of the pmf of |AI (k) \u2229 A(k)|, then it is easy to obtain the analytical\nexpression of E[|AI (k) \u2229 A(k)|] and Var[|AI (k) \u2229 A(k)|] according to the definition of expectation and variance.\n\nExamining Eq. (23), we can see that Pr[A(k) = F \u222a G] is an essential part of the analytical expression of the\npmf. The analytical expression of Pr[A(k) = F \u222a G] is given in Eq. (14), which is quite complicated, and these\nanalytical expressions cannot be reduced to a simple form. Thus, it is not easy to use these analytical results to gain\nsome insight of a conference recommendation system. An alternative way is to compute the numerical results of the\npmf of |AI (k) \u2229 A(k)| based on the analytical expressions in Eq. (23). After we obtain the numerical results of the\npmf of |AI (k) \u2229 A(k)|, we can then compute its expectation and variance. Unfortunately, computing the numerical\nresults of Eq. (23) is computationally expensive, which is shown in the following theorem.\nTheorem 3 The computational complexity in calculating the numerical results of the pmf of |AI (k) \u2229 A(k)| based on\nEq. (23) is exponential, or \u0398(2N ).\nProof: Examining Eq. (23), we can see that the calculation of Pr[A(k) = F \u222a G] is the core part on the calculation of\nthe the pmf of |AI (k) \u2229 A(k)|. Assume the running time of calculating Pr[A(k) = F \u222a G] is t, then by Equation (23),\nthe running time of calculating the pmf is\n\u0013!\n\u0012\u0012 \u0013 \u0013\nk \u0012 \u0013\u0012\nX\nk\nN \u2212k\nN\nt =\u0398\nt .\n(24)\n\u0398\ni\nk\n\u2212\ni\nk\ni=0\nIn the following we analyze the running time of calculating the numerical result of Pr[A(k) = F \u222a G] based on its\nanalytical expression derived by Eq. (14). Examining Eq. (14), we can see that there are two basic computations of\nEq. (14), of which the first one is\n\u0014\n\u0015\n\u0015!\n\u0015\n\u0014\nY \u0014\nl\u22121\nl\u22121 Y\nl Y\nPr \u03b3j \u2264\n,\nPr \u03b3i \u2264 \u2212 Pr \u03b3i \u2264\nn\nn\nn\ni\u2208I\n\ni\u2208I\n\nj\u2208I\n\nlet us assume the running time of calculating this basic part is t1 . The second basic computation is\n\u0014\n\u0015\u0013 Y\n\u0015\n\u0015\n\u0014\n\u0014\nY\u0012\nl\nl Y\nl\u22121\n1\u2212Pr \u03b3i \u2264\n,\nPr \u03b3j =\nPr \u03b3\u03ba \u2264\nn\nn\nn\ni\u2208I\\F\n\nj\u2208F \u222aG\n\n\u03ba\u2208I\\G\n\nlet us assume the running time of calculating this basic part is t2 . The running time of computing Pr[A(k) = F \u222a G]\nby Eq. (14) is\nt = \u0398(npt1 + (2k \u2212 1)(2N \u2212k \u2212 1)npt2 )\n= \u0398(npt1 + 2k\u22121 2N \u2212k\u22121 npt2 )\n\n= \u0398(npt1 + 2N \u22122 npt2 ) = \u0398(2N npt2 /4).\n9\n\n\fBy letting t = \u0398(2N npt2 /4) in Eq. (24), we can obtain the result stated in this theorem.\nTo illustrate the complexity, consider the number of mathematical operations (i.e., addition, subtraction, multiplication and division) that we need in the computation of the numerical results of the pmf of |AI (k) \u2229 A(k)|. Table\n3 illustrates the computational complexity of three conferences: Recsys'11, Sigcomm'11, WWW'11 and IEEE Infocom'11. Since the expectation and variance are based on these operations, so their computational complexity are also\nexponential.\nConference\n\nN\n\nk\n\nACM Recsys'11\n\n110\n\n22\n\n20%\n\nACM Sigcomm'11 223\n\n32\n\n13%\n\n81\n\n12%\n\nIEEE Infocom'11 1823 291\n\n16%\n\nWWW'11\n\n658\n\nAcceptance % Complexity\n\u223c 2161\n\u223c 2312\n\u223c 2902\n\n\u223c 22593\n\nTable 3: Examples of computational complexity\nIn summary, we have the following conclusions in analyzing the above conference recommendation system:\n\u2022 We can analytically derive the pmf of:\nPr[|AI (k) \u2229 A(k)| = i],\n\nfor i = 0, 1, . . . , k,\n\n\u2022 The analytical expression is complex and it is not easy to obtain insights of the underlying recommendation\nsystem.\n\u2022 Computing the numerical results based on these analytical results is computational expensive.\n\n3.2 Derivation for the General Case\nFor the general case, we can derive the analytical experssions of the pmf, expectation and variance of |AI (k) \u2229 A(k)|\nwith similar methods used in the special case. The analytical expressions for the general case will have a similar form\ncompared with the analytical expressions derived in the special case. Furthermore, it is reasonable to expect that for\nthe general case, there can be different types of paper and that reviewers are not homogeneous (e.g, they may have\ndifferent topics preference). Also, tie breaking rules will be more complicated than the random rule. Hence we expect\nthe analytical expressions for the general case will be more complicated. Thus, one may not easily obtain insight by\nexamining the analytical expression of the general case. Instead, let us focus on finding a practical approach to solve\nthe general case, and that it should be computational inexpensive to obtain numerical results of:\nPr[|AI (k) \u2229 A(k)| = i],\n\nfor i = 0, 1, . . . , k.\n\nIn the following section, we present this practical approach, and to show that not only we can have a computational\nefficient approach to compute all probability measures, but more importantly, provides performance guarantees on our\noperation.\n\n4 Randomized Algorithm\nIn this section, we present a randomized algorithm to evaluate the pmf, expectation and variance of |AI (k)\u2229A(k)|. Our\nrandomized algorithm is computationally efficient with performance guarantee. We will use the following notations\nb\nb\nto describe our algorithm. We define I(k) = |AI (k) \u2229 A(k)|. Let b\nPr[I(k) = i], E[I(k)]\nand Var[I(k)]\ndenote the\napproximate value of Pr[I(k) = i], E[I(k)] and Var[I(k)] respectively. We first present the algorithm, then show its\nperformance guarantee.\n\n10\n\n\f4.1 Randomized Algorithm\nOur algorithm is stated in Algorithm 1. The main idea of this randomized algorithm is that we first approximate\nb\nb\nthe pmf Pr[I(k) = i], then we use the approximate value b\nPr[I(k) = i] to compute E[I(k)]\nand Var[I(k)].\nWe can\nAlgorithm 1 : Randomized Algorithm\n1: for all i = 0, . . . , k, li \u2190 0\n2: for j = 1 to K do\n3:\nfor all i = 1, . . . , N , generate the intrinsic quality for paper Pi by simulating the paper submission process\naccording to a self-selectivity type.\n4:\nproduce the intrinsic top-k paper set AI (k).\n5:\nassign papers to reviewers, generate the degree of criticality based on the paper-reviewer matching degree.\n6:\nfor i = 1, . . . , N , generate score set S i for paper Pi by simulating the scoring process.\n7:\nsimulate the decision making process, i.e., applying the voting rule V and the tie breaking rule T to produce the\nset A(k) based on the score sets {S 1 , . . . , S N }\n8:\nif the cardinality of the intersection of AI (k) and A(k) is equal to i, then li \u2190 li + 1.\n9: end for\n10: for all i = 0, . . . , k, b\nPr[I(k) = i] \u2190 li /K\nP\nb\n11: E[I(k)]\n\u2190 ki=0 ib\nPr[I(k) = i]\n\u00112\nPk \u0010\nb\nb\nb\n12: Var[I(k)]\n\u2190 i=0 i \u2212 E[I(k)]\nPr[I(k) = i]\n\nstate two properties of this algorithm. The first one is its running time complexity and the other one is its theoretical\nperformance guarantee. The following theorem states its running time complexity.\nTheorem 4 The computational complexity of our randomized algorithm is \u0398(KN log N ), where K is the number of\nsimulation round and N is the number of submitted papers.\nProof: We prove this theorem by examining the complexity of each step of our Algorithm 1. The complexity\nof\nPN\nstep 10 \u2212 12 and 1 are the same, say \u0398(k). The complexity of step 3 \u2212 8 are \u0398(N ), \u0398(N log N ), \u0398( i=1 ni ),\nPN\n\u0398( i=1 ni ), \u0398(N log N ), and \u0398(1) respectively. Since each reviewer only review a small subset of the submitted\nP\npapers, namely ni \u226a N , thus we have \u0398( N\ni=1 ni ) = \u0398(N ). By adding the complexity of step 1 \u2212 12 up we could\nobtain the theorem.\nThe remaining technical issue is how to set the parameter K. Specifically, how many simulation rounds K can\nproduce a good approximation of the pmf, expectation and variance? Let us proceed to answer this question by\nderiving the theoretical performance guarantee for Algorithm 1.\n\n4.2 Theoretical Performance Guarantee\nFirst, we derive a loose bound on the number of simulation rounds K needed but have good performance guarantee.\nThen we show how one can have a tight bound on K, and tradeoff between K and its performance guarantee. The\nfollowing theorem states the loose bound on K.\nTheorem 5 When the following condition holds:\nK \u2265 max\n\ni=0,1,...,k,\nPr[I(k)=i]6=0\n\n3 ln(2(k + 1)/\u03b4)\n,\nPr[I(k) = i]\u01eb2\n\nthen Algorithm 1 guarantees:\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i]\nb\nE[I(k)]\n\u2212 E[I(k)]\n\nb\nVar[I(k)]\n\u2212 Var[I(k)]\n\n\u2264\n\n\u01ebPr[I(k) = i],\n\n\u2264\n\n\u01ebE[I(k)],\n\n\u2264\n\n\u01eb(1 + \u01eb)Var[I(k)],\n\n11\n\n\u2200i = 0, . . . , k,\n\n(25)\n\n\fwith probability at least 1 \u2212 \u03b4.\nProof: By applying Lemma 4, we obtain that\nb\nPr[I(k) = i] \u2212 Pr[I(k) = i] \u2264 \u01ebPr[I(k) = i],\n\nholds for all x = 0, 1, . . . , k, with probability at least 1 \u2212 \u03b4. By applying Lemma 5 and 4 we have\nb\nE[I(k)]\n\u2212 E[I(k)] \u2264 \u01ebE[I(k)]\n\nholds with probability at least 1 \u2212 \u03b4. By applying Lemma 7 and 4 we have\n\nb\nVar[I(k)]\n\u2212 Var[I(k)] \u2264 \u01eb(1 + \u01eb)Var[I(k)]\n\nholds with probability at least 1 \u2212 \u03b4. Please refer appendix for the proofs of these lemmas.\nWhen one examines the Inequality (25), one can see that the bound of K is useful when Pr[I(k) = i] is not small\nfor i = 0, 1, . . . , k. Consider the case when Pr[I(k) = i] \u2264 2N for some i = 0, 1, . . . , k, then we have K \u2265 2N . For\nsuch cases, K is too large. In the following theorem, we show a tight bound on K.\nTheorem 6 When K \u2265 3 ln(2(k + 1)/\u03b4)/\u01eb2 , Algorithm 1 guarantees:\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i]\n\n\u2264\n\nb\nVar[I(k)]\n\u2212 Var[I(k)]\n\n\u2264\n\nb\nE[I(k)]\n\u2212 E[I(k)]\n\nwith probability at least 1 \u2212 \u03b4.\n\n\u2264\n\np\n\u01eb Pr[I(k) = i], \u2200i = 0, . . . , k,\np\n\u01eb k(k + 1)E[I(k)]/2\n\u0010\n\u0011\np\n\u01eb(k + 1) \u01ebVar[I(k)] + (2k + 1)Var[I(k)]/6\n\nProof: By applying Lemma 9, we obtain that\np\nb\nPr[I(k) = i] \u2212 Pr[I(k) = i] \u2264 \u01eb Pr[I(k) = i]\n\nholds for all i = 0, 1, . . . , k, with probability at least 1 \u2212 \u03b4. By applying Lemma 10, and 9 we have\nb\nE[I(k)]\n\u2212 E[I(k)] \u2264 \u01eb\n\np\nk(k + 1)E[I(k)]/2\n\nholds with probability 1 \u2212 \u03b4. By applying Lemma 12 and 9 we have\n\u0010\n\u0011\np\nb\nVar[I(k)]\n\u2212 Var[I(k)] \u2264 \u01eb(k + 1) \u01ebVar[I(k)] + (2k + 1)Var[I(k)]/6\n\nhold with probability at least 1 \u2212 \u03b4. Detailed proof of these lemmas are in the appendix.\n\n5 Evaluation of Peer Review System\nIn this section we evaluate the accuracy of conference recommendation systems. We consider a conference with two\nhundred submissions, or N = 200, and only k = 30 submissions will be accepted. In other words, a 15% acceptance\nrate. In consistent with realistic conference review systems, we set m = 5, or the rating set is {1, . . . , 5}. The\nsimulation rounds K in Algorithm 1 is set to 109 . In the following, we start our evaluation from a simple case, then we\nextend it step by step and evaluate the impact of various factors on the overall accuracy in the final recommendation.\n\n12\n\n\f5.1 Probability distribution, expectation and variance of |AI (k) \u2229 A(k)|\nHere we consider a homogenous conference recommendation system, in which papers and reviewers are homogeneous\nand each paper is reviewed by the same number of reviewers, or ni = n, \u2200i. Specifically, each reviewer is non-biased\nwith the same critical degree, or cij = c, \u2200i, j. Hence the function \u03c3(cij ), \u2200i, j, within Equation (6) has the same\nvalue \u03c3(c) and further we set \u03c3(c) = 1. We select one type of self-selectivity, say medium self-selectivity specified by\nEquation (2), to study here. The voting rule V is the average score rule and the tie breaking rule is the least variance\nrule that selects the paper with the least variance. If there is still tie, paper is randomly selected.\nDefinition 1 The reviewing workload of a conference recommendation system is the sum of all reviews, or\nW =\n\nXN\n\ni=1\n\nni .\n\nDefinition 2 Let Ii be the random variable indicating\nIi = |AI (i) \u2229 A(k)|.\n\n(26)\n\nIn other words, if i = 1, I1 reflects the event that the best paper is accepted by this conference recommendation system.\nWhen i = 5, I5 reflects the event that the top five submitted papers are finally accepted.\nThe numerical results of the pmf of |AI (30)\u2229A(30)| are shown in Fig. 1. The numerical results of the expectation\nand variance of I1 , I5 , I10 and I30 are shown in Table 4.\nIn Fig. 1, the horizontal axis represents the number of top 30 papers that got finally accepted, or |AI (30) \u2229 A(30)|.\nThe vertical axis shows the corresponding probability. From Fig. 1 we could see that when we increase the number\nof reviews per paper, or n, the probability mass function shifts toward the right. In other words, the more reviews\neach paper received, the higher the accuracy of the conference recommendation system. From Table 4, we have the\nfollowing observations. When each paper is reviewed by three reviewers, approximately 19.8 papers from the top\n30 papers will get accepted. It is interesting to note that the chance of accepting the best paper is invariant of the\nreviewing workload, since E[I1 ] = 0.98 when n = 3 and it improves to 0.9999 when n = 10. This statement also holds\nfor the top ten papers. From Table 4, we can see that as we increase n, we decrease the variance, which reflects that\nthe conference recommendation system is more accurate.\nLessons learned: If reviewers are non-biased, fair and critical, and the quality of submitted papers is of medium selfselectivity as specified by Eq. (2), we have a pretty accurate conference recommendation system. There are number of\ninteresting questions to explore further, i.e., are these results dependent on the distribution of intrinsic quality, or the\nself-selectivity type of papers? Are these results sensitive to any voting rule? Let us continue to explore.\n\nPr[|AI(30) \u2229 A(30)|]\n\n0.25\n\nN = 200, k = 30\nMedium self\u2212selectivity\nn=3\nn=4\nn=6\nn=8\nn = 10\n\n0.2\n0.15\n0.1\n0.05\n0\n\n0\n\n10\n\nI\n\n20\n\n|A (30) \u2229 A(30)|\n\n30\n\nFigure 1: pmf of |AI (30)\u2229A(30)| when n = 3, 4, 6, 8, 10 and papers are submitted with medium self-selectivity.\n\n13\n\n\fE[I1 ]\nE[I5 ]\nE[I10 ]\nE[I30 ]\nVar[I1 ]\nVar[I5 ]\nVar[I10 ]\nVar[I30 ]\n\nn=3\n0.9832\n4.6854\n8.7763\n19.8270\n0.0165\n0.2942\n1.0793\n4.1210\n\nn=4\n0.9931\n4.8284\n9.1643\n20.960\n0.0067\n0.1701\n0.7793\n3.7710\n\nn=6\n0.9986\n4.9352\n9.5576\n22.400\n0.0014\n0.0654\n0.4370\n3.2934\n\nn=8\n0.9996\n4.9723\n9.7446\n23.328\n0.0004\n0.0280\n0.2586\n2.9580\n\nn = 10\n0.9999\n4.9869\n9.8426\n23.980\n0.0001\n0.0132\n0.1609\n2.7121\n\nTable 4: Expectation and variance of I1 , I5 , I10 , and I30 when we increase number of reviews per paper, n =\n3, 4, 6, 8, 10, and papers are submitted with medium self-selectivity.\n\n5.2 Effect of Intrinsic Quality\nHere, we explore the effect of intrinsic quality (or self-selectivity) of papers on the conference recommendation system. Specifically, we consider four representative types of self-selectivity of papers: high, medium, low and random\nself-selectivity specified by Eq. (1) \u2212 (4) respectively. We explore the effect of self-selectivity of papers on the homogeneous conference recommendation system specified in Section 5.1 with each paper receiving three reviews, or\nn = 3. We use the following notations to present our results:\nH-S-S: high self-selectivity specified in Eq. (1).\nM-S-S: medium self-selectivity specified in Eq. (2).\nL-S-S: low self-selectivity specified in Eq. (3).\nR-S-S: random self-selectivity specified in Eq. (4).\nThe numerical results of the pmf of |AI (30) \u2229 A(30)| is shown in Fig. 2. The numerical results of the expectation and\nvariance of I1 , I5 , I10 and I30 are shown in Table 5.\nIn Fig. 2, the horizontal axis represents the number of top 30 papers that got finally accepted, or |AI (30) \u2229 A(30)|.\nThe vertical axis shows the corresponding probability. From Fig. 2 we could see that as the self-selectivity type\nvaries in the order of high, low, medium, random self-selectivity, the corresponding mass probability distribution\ncurve moves towards right. In other words, the accuracy of the conference recommendation system corresponding\nto the random self-selectivity is the highest followed by medium, low and high self-selectivity. From Table 5 we\nhave the following observations. When papers are submitted with medium, low or random self-selectivity, around\n20 papers from the top 30 papers will be accepted. It is interesting to note that the chance of accepting the best\npaper is invariant to these three self-selectivity types, since the corresponding three values of E[I1 ] are all around\n0.98. This statement also holds for top ten papers. But when papers are submitted with high self-selectivity, the\naccuracy of the conference recommendation system is remarkably lower than that the other three self-selectivity types.\nSpecifically, only a small number, around 13.2, of papers from the top 30 papers will be accepted. Even the best paper\nwill get rejected with high probability, around 0.46. This statement also holds for top ten or top five papers. The\nvariance corresponding to the high self-selectivity is the highest among those four, which reflects that the results of the\nconference recommendation system is the least accurate and more likely to depart from the expectation. Hence, for a\nprestigious conference, i.e., SIGCOMM, assume papers are submitted with high self-selectivity, and if the conference\ninsists to have a small technical program committee with reviewers having moderate reviewing workload, say n = 3,\nthe final recommendation may not be accurate.\nLessons learned: When reviewers are non-biased fair and critical, the above simple conference review system is quite\naccurate except when papers are of high self-selectivity. In that case, one may explore other means to improve the\naccuracy. Again, there are number of interesting questions to explore, i.e., how large the reviewing workload do we\nneed to have an accurate recommendation?\n\n5.3 Effect of Number of Reviews per Paper\nHere we consider a homogeneous conference recommendation system specified in Section 5.1. We select one probability measure, expectation of acceptance, to study. The numerical results of E[I1 ], E[I5 ], E[I10 ] and E[I30 ] are\n\n14\n\n\fPr[|AI(30) \u2229 A(30)|]\n\n0.2\n\nN = 200, k = 30, n = 3\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n0.15\n0.1\n0.05\n0\n\n0\n\n10\n\n20\n\n|AI(30) \u2229 A(30)|\n\n30\n\nFigure 2: pmf of |AI (30) \u2229 A(30)| when n = 3 and papers are submitted with high, medium, low or random selfselectivity.\n\nE[I1 ]\nE[I5 ]\nE[I10 ]\nE[I30 ]\nVar[I1 ]\nVar[I5 ]\nVar[I10 ]\nVar[I30 ]\n\nH-S-S\n0.5401\n2.6250\n5.0687\n13.2258\n0.2437\n1.2236\n2.3933\n5.4420\n\nM-S-S\n0.9832\n4.6950\n8.7736\n19.8270\n0.0165\n0.2942\n1.0793\n4.1210\n\nL-S-S\n0.9788\n4.6082\n8.5162\n18.9798\n0.0208\n0.3698\n1.2491\n4.2888\n\nR-S-S\n0.9846\n4.7599\n9.0810\n21.5821\n0.0151\n0.2314\n0.8275\n3.5473\n\nTable 5: Expectation and Variance of I1 , I5 , I10 , and I30 when n = 3 and papers are submitted with high, medium,\nlow or random self-selectivity.\nshown in Fig. 3.\nIn Fig. 3, the horizontal axis represents the number of reviews per paper, or n. The vertical axis shows the\ncorresponding expectation. From Fig. 3, we have the following observations. When we increase the number of\nreviews per paper, or n, the expectation increased, which reflects the improvement in accuracy of the conference\nrecommendation system. As the self-selectivity type varies in the order of high, low, medium, random self-selectivity,\nthe expectation curve shifts toward up. In other words, the accuracy corresponding to the random self-selectivity is the\nhighest followed by medium, low, high self-selectivity. It is interesting to note that the best paper is invariant of the\nworkload, except for papers with high self-selectivity. This statement also holds for the top five or ten papers. When\npapers are highly self-selective, the accuracy of the conference recommendation system is remarkably lower than the\nother three self-selectivity types. Especially, when the reviewing workload is low, say n = 3, with less than 15 papers\nfrom the top 30 papers will get accepted. This holds even for the best paper, which only has a probability of less than\n0.6 of being accepted when n = 3. Same can be said for the top five or top ten papers. In closing, for papers with\nhigh self-selectivity, and we may have to increase the reviewing workload to at least n \u2265 7 such that we have a strong\nguarantee that the best paper will be accepted.\nLessons learned: If reviewers are non-biased and fair, using the above simple conference review system achieves\nrelatively high accuracy except when papers are of high self-selectivity. In that case, we have to increase the workload\nto n \u2265 7 to improve the system. Again, there are number of interesting questions to explore, i.e., is increasing the\nworkload the only way to improve the conference recommendation system? Can we improve it by using different\nvoting rule or tie breaking rule?\n\n15\n\n\f5\n\n0.9\n\n4.5\nE[I5]\n\n1\n\nE[I1]\n\n0.8\n\n3.5\n\n0.7\n0.6\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\nN = 200, k = 30\nAverage score\nLeast variance\n\n0.5\n0.4\n\n4\n\n2\n\n4\n6\n8\n10\naverage workload (n)\n\n3\nN = 200, k = 30\nAverage score\nLeast variance\n\n2.5\n2\n2\n\n12\n\n(a) # of top 1 papers get in\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n4\n6\n8\n10\naverage workload (n)\n\n12\n\n(b) # of top 5 papers get in\n\n10\n\n25\n\n8\n\n20\n30\n\nE[I ]\n\nE[I10]\n\n9\n\n7\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n6\nN = 200, k = 30\nAverage score\nLeast variance\n\n5\n4\n2\n\n4\n6\n8\n10\naverage workload (n)\n\n15\n\n10\n\n12\n\n(c) # of top 10 papers get in\n\nN = 200, k = 30\nAverage score\nLeast variance\n2\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n4\n6\n8\n10\naverage workload (n)\n\n12\n\n(d) # of top 30 papers get in\n\nFigure 3: Expectation of I1 , I5 , I10 , and I30 when we vary number of reviews per paper n = 2, . . . , 12, with papers are\nfrom high, low, medium or random self-selectivity.\n\n5.4 Effect of Voting Rules\nIn this section we explore the effect of voting rules on the accuracy of conference recommendation system. Specifically, we will evaluate the performance of the following three representative voting rules:\nAverage score rule (Vas ): specified by\nX\n\u03b3i =\nS/|S i |.\n(27)\ni\nS\u2208S\n\nEliminate the highest & lowest score rule (Vehl ): eliminate the highest and lowest score of each paper, and calculate\nthe average score of each paper by the remaining scores, or\n\u0010X\n\u0011\n\u03b3i =\nS \u2212 max {S i } \u2212 min {S i } /(|S i | \u2212 2).\n(28)\ni\nS\u2208S\n\nPunish low scores rule (Vpl ): punish the low scores. Specifically, a low score, or 1, brings the an extra punishment of\ndecreasing its score by \u03b7, or\nX\n\u03b3i =\nS/|S i | \u2212 \u03b7|{S | S = 1, S \u2208 S i }|.\n(29)\ni\nS\u2208S\n\nLet us set the punishment \u03b7 to be 0.5 throughout this paper.\nWe use the least variance rule for tie breaking and we choose expectation as our performance measure. We\nevaluate the accuracy of these three voting rules on the conference recommendation system specified in Section 5.1.\nThe numerical results of expectation of I30 are shown in Fig. 4. The numerical results of expectation of I1 and I5 for\n\n16\n\n\fVas\nVehl\nVpl\nHigh self-selectivity\nE[I1 ]\n0.5793 0.5793 0.5793\nE[I5 ]\n2.8078 2.8078 2.8077\nE[I10 ] 5.4028 5.4026 5.4026\nMedium self-selectivity\nE[I1 ]\n0.9832 0.9911 0.9832\nE[I5 ]\n4.6950 4.7733 4.6950\nE[I10 ] 8.7763 8.9641 8.7763\nLow self-selectivity\nE[I1 ]\n0.9788 0.9741 0.9746\nE[I5 ]\n4.6082 4.5616 4.5691\nE[I10 ] 8.5162 8.4032 8.4297\nRandom self-selectivity\nE[I1 ]\n0.9846 0.9873 0.9846\nE[I5 ]\n4.7599 4.7937 4.7599\nE[I10 ] 9.0810 9.1791 9.0810\nTable 6: Expectation of E[I1 ], E[I5 ], and E[I10 ] for three voting rules: Vas , Vehl , and Vpl when n = 3\nhigh self-selectivity submissions are shown in Fig. 5. The numerical results of E[I1 ], E[I5 ], and E[I10 ] when n = 3\nare shown in Table 6.\nIn Fig. 4 and 5 the horizontal axis represents the number of reviews per paper, or n. The vertical axis shows\ncorresponding expectation. From Fig. 4, we have the following observations. When we increase n, the expectation\nE[I30 ] corresponding to each voting rule increased, which reflects that the accuracy of the conference recommendation\nsystem increased. From Fig. 4(a), we see that when submitted papers are of high self-selectivity, the expectation curves\noverlapped together. In other words, for high self-selectivity papers, these three voting rules are similar. From Fig.\n4(b) we see that for submitted papers with medium self-selectivity, the average score rule and punish low scores rule\nhave the same degree of accuracy, and the eliminate highest and lowest score rule has slightly higher accuracy than\nthe other two voting rules. This statement also holds for the random self-selectivity submissions as shown in Fig. 4(d).\nFrom Fig. 4(c), we see that when submitted papers are of low self-selectivity, all three voting rules have nearly the\nsame accuracy but the punish low scores rule has slightly lower accuracy. From Table 6, we observe that when n = 3,\nthe chance of accepting the best paper is invariant of these voting rules, unless when submitted papers are of high\nselectivity, since E[I1 ] is around 0.98 for medium, low and random self-selectivity papers. This statement also holds\nfor the top five and the top ten papers. When papers are submitted with high self-selectivity, the chance of accepting\nthe best paper is low, in fact, it is less than 0.6. The same statements holds for the top five and top ten papers. From\nFig. 5, we see that when papers submitted with high self-selectivity, the expectation curves overlapped together. And\nfor each voting rule we have to increase the reviewing workload to at least seven such that we have a strong guarantee\nthat the best paper or the top five papers will get accepted.\nLessons learned: These three voting rules have comparable accuracy, no rule can outperform others remarkably. Thus\nthe improvement of conference recommendation system by voting rules is limited. For each voting rule, we till have\nto increase the average workload to at least seven to have a strong guarantee that the best paper will get in. Again,\nthere are number of interesting questions to explore, i.e., how about the tie-breaking rules?\n\n5.5 Effect of Tie Breaking Rules\nIn this section we explore the effect of tie breaking rules on conference recommendation system. Specifically, we\nevaluate the performance of the following four representative tie breaking rules:\nLeast variance (Tvar ): select one with least variance, if there is still tie, perform random selection.\nLargest max score (Tmaxs ): select one with the largest max score, if there is still tie, perform random selection.\nLargest min score (Tmins ): select one with the largest min score, if there is still tie, perform random selection.\nLargest medium score (Tmeds ): select one with the largest medium score, if there is still tie, perform random selection.\n17\n\n\f25\n\n22\n\n24\n23\n\n30\n\nE[I ]\n\n18\n\nN = 200, k = 30\nLeast variance\n\n30\n\nE[I ]\n\n20\n\n21\n\n16\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\n14\n4\n\n6\n8\n10\naverage workload (n)\n\n19\n\n12\n\n23\n\n25\n\nN = 200, k = 30\nLeast variance\n\n20\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n4\n\n6\n8\n10\naverage workload (n)\n\n6\n8\n10\naverage workload (n)\n\n12\n\n24\n\n30\n\nE[I ]\n\n22\n\n30\n\nE[I ]\n\n26\n\n19\n\n4\n\n(b) Medium self-selectivity\n\n24\n\n21\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\n20\n\n(a) High self-selectivity\n\n18\n\nN = 200, k = 30\nLeast variance\n\n22\n\nN = 200, k = 30\nLeast variance\n\n23\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\n22\n21\n\n12\n\n(c) Low self-selectivity\n\n4\n\n6\n8\n10\naverage workload (n)\n\n12\n\n(d) Random self-selectivity\n\nFigure 4: E[I30 ] for three voting rules: Vas , Vehl , and Vpl .\n\n5\n\n0.9\n\n4.5\n\n0.8\n\n4\n\n5\n\nE[I1]\n\nN = 200, k = 30\nLeast variance\n\n0.7\n\n4\n\n6\n8\n10\naverage workload (n)\n\nN = 200, k = 30\nLeast variance\n\n3.5\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\n0.6\n0.5\n\nE[I ]\n\n1\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\n3\n2.5\n\n12\n\n(a) # of top 1 paper get in\n\n4\n\n6\n8\n10\naverage workload (n)\n\n12\n\n(b) # of top 5 papers get in\n\nFigure 5: Expectation of E[I1 ], E[I5 ] for three voting rules: Vas , Vehl , and Vpl for high self-selectivity submission.\n\n18\n\n\fE[I1 ]\nE[I5 ]\nE[I10 ]\nE[I1 ]\nE[I5 ]\nE[I10 ]\nE[I1 ]\nE[I5 ]\nE[I10 ]\nE[I1 ]\nE[I5 ]\nE[I10 ]\n\nTvar\nTmaxs Tmins\nHigh self-selectivity\n0.5793 0.5793 0.5793\n2.8078 2.8077 2.8077\n5.4026 5.4026 5.4026\nMedium self-selectivity\n0.9832 0.9880 0.9832\n4.6951 4.7514 4.6951\n8.7764 8.9311 8.7765\nLow self-selectivity\n0.9788 0.9787 0.9788\n4.6083 4.5989 4.6074\n8.5164 8.4723 8.5102\nRandom self-selectivity\n0.9846 0.9858 0.9846\n4.7599 4.7747 4.7599\n9.0810 9.1239 9.0810\n\nTmeds\n0.5793\n2.8078\n5.4026\n0.9907\n4.7755\n8.9831\n0.9886\n4.6040\n8.3990\n0.9872\n4.7926\n9.1761\n\nTable 7: Expectation of E[I1 ], E[I5 ], and E[I10 ] for four tie breaking rules: Tvar , Tmaxs , Tmins , and Tmeds when\nn = 3.\nTo evaluate the performance of these four tie breaking rules, let us select a voting rule: average score rule and\nwe use expectation as our performance measure. We evaluate the performance of these four tie breaking rules on the\nconference recommendation system specified in Section 5.1. The numerical results of E[I30 ] are shown in Fig. 6.\nThe numerical results of E[I1 ] and E[I5 ] for high self-selectivity papers are shown in Fig. 7. The numerical results of\nE[I1 ], E[I5 ], and E[I10 ] when n = 3 are shown in Table 7.\nIn Fig. 6 and 7, the horizontal axis represents the number of reviews per paper, or n. The vertical axis shows the\ncorresponding expectation. From Fig. 6, we could have the following observations. When we increase the reviewing\nworkload, the expectation E[I30 ] corresponding to each tie breaking rule increased, which reflects that the accuracy of\nthe conference recommendation system increased. From Fig. 6(a) and 6(c), we could see that when submitted papers\nare of high or low self-selectivity, the expectation curves corresponding to these four tie breaking rules overlapped\ntogether. In other words, these four rules have the same accuracy. From Fig. 6(b) and 6(d), we see that when submitted\npapers are of medium or random self-selectivity, the expectation curves corresponding to these four tie breaking rules\nbunched together and the largest medium score rule has a slightly higher accuracy than others. From Table 7, we\nsee that when n = 3, the chance of accepting the best paper is invariant of tie breaking rules, unless when submitted\npapers are of high self-selectivity, since values of E[I1 ] corresponding to medium, low or random self-selectivity are\nall around 0.98. This statement also holds for the top five and top ten papers. When submitted papers are of high\nself-selectivity, the chance of the best paper being accepted is low, or less than 0.6. The same statements holds for the\ntop five and top ten papers. From Fig. 7, we see that for each tie breaking rule, we still have to increase the reviewing\nworkload n to at least 7 so as to have a strong guarantee that the best paper or the top five papers will get accepted.\nLessons learned: These four tie breaking rules have comparable accuracy, no rule can outperform others remarkably.\nThus, the improvement of conference recommendation system by tie breaking rules is limited. For each tie breaking\nrule, we till have to increase the reviewing workload n to at least seven to have a strong guarantee that the best paper\nwill get in.\n\n5.6 Effect of Reviewers Type \u2013 two types case\nWe extend the homogeneous conference recommendation system specified in Section 5.1 to a heterogeneous conference recommendation system, for which papers and reviewers can be of different types. Specifically, we consider\npapers and reviewers are of two types (e.g. system or theory). If paper-reviewer matches in the same type, the critical\ndegree is high, else the critical degree is low. Here we explore the effect of the fraction that paper-reviewer matches in\nthe same type on the accuracy of the recommendation. We use expectation as our performance measure and we set the\nnumber of review per paper in this heterogeneous recommendation system be four, or n = 4. We vary the fraction of\n\n19\n\n\f22\n\n25\n\n20\n\n24\n23\n\nE[I30]\n\n18\nN = 200, k = 30\nAverage score\n\nN = 200, k = 30\nAverage score\n\n22\n\nE[I30]\n\n16\n\n21\n\nLeast variance\nLargest max score\nLargest min score\nLargest medium score\n\n14\n12\n10\n\n2\n\n4\n6\n8\n10\naverage workload (n)\n\nLeast variance\nLargest max score\nLargest min score\nLargest medium score\n\n20\n19\n18\n\n12\n\n2\n\n(a) High self-selectivity\n\n4\n6\n8\n10\naverage workload (n)\n\n12\n\n(b) Medium self-selectivity\n\n26\n\n24\n\n25\n22\n20\n\n2\n\n4\n6\n8\n10\naverage workload (n)\n\nN = 200, k = 30\nAverage score\n\n23\n\nLeast variance\nLargest max score\nLargest min score\nLargest medium score\n\n18\n16\n\nE[I30]\n\nE[I30]\n\n24\n\nN = 200, k = 30\nAverage score\n\nLeast variance\nLargest max score\nLargest min score\nLargest medium score\n\n22\n21\n20\n\n12\n\n2\n\n(c) Low self-selectivity\n\n4\n6\n8\n10\naverage workload (n)\n\n12\n\n(d) Random self-selectivity\n\nFigure 6: Expectation of E[I30 ] for four tie breaking rules: Tvar , Tmaxs , Tmins , and Tmeds .\n\n1\n\n5\n\n0.9\n\n4.5\n4\n\nE[I1]\n\n0.8\n0.7\n\n0.5\n2\n\n4\n6\n8\n10\naverage workload (n)\n\nN = 200, k = 30\nAverage score\n\n5\n\nLeast variance\nLargest max score\nLargest min score\nLargest medium score\n\n0.6\n\n0.4\n\nE[X ]\n\nN = 200, k = 30\nAverage score\n\n3.5\n\nLeast variance\nLargest max score\nLargest min score\nLargest medium score\n\n3\n2.5\n2\n\n12\n\n(a) # of top 1 paper get in\n\n2\n\n4\n6\n8\n10\naverage workload (n)\n\n12\n\n(b) # of top 5 papers get in\n\nFigure 7: Expectation of E[I1 ], E[I5 ] for four tie breaking rules: Tvar , Tmaxs , Tmins , and Tmeds for high selfselectivity submission.\n\n20\n\n\fpaper-reviewer that matches in the same type from 0.1 to 1. Before showing our results, let us specify \u03c3(cij ) derived\nin Eq. (6): if papers-reviewer matches in the same type, \u03c3(cij ) = 0.5, otherwise \u03c3(cij ) = 2. The numerical results of\nE[I1 ], E[I5 ], E[I10 ], and E[I30 ] are shown in Fig. 8.\nIn Fig. 8, the horizontal axis represents the the fraction that paper-reviewer matches in the same type. The vertical\naxis shows the corresponding expectation. From Fig. 8, we have the following observations. When we increase the\nmatching fraction, the expectation E[I1 ], E[I5 ], E[I10 ], and E[I30 ] increased. In other words, the accuracy of the\nconference recommendation system increased when more paper-reviewer matches in the same type. It is interesting to\nsee that the expectation E[I1 ], E[I5 ], E[I10 ] and E[I30 ] increase in a linear rate and the rate corresponding to the high\nself-selectivity is the highest, and the random self-selectivity is the lowest. When papers are submitted with medium,\nlow or random self-selectivity, the chance of accepting the best paper is invariant of the matching fraction, since the\ncorresponding values of E[I1 ] are approximately equal to 1 when the matching fraction is 0 and increased slightly\nwhen matching faction increased to 1. This statement also holds for the top five papers. When submitted papers are\nhighly self-selective, the accuracy of the system is very sensitive to the matching fraction, this is because E[I1 ] \u2248 0.5,\nE[I10 ] \u2248 5 and E[I30 ] \u2248 13 when matching fraction is 0, and E[I1 ] \u2248 0.9, E[I10 ] \u2248 8 and E[I30 ] \u2248 20 when matching\nfraction is 1.\nLessons learned: If reviewers' preference and papers match, it can significantly affect the accuracy of the conference\nrecommendation system. This is especially true for submitted papers which are of high self-selectivity (or prestigious\nconferences). To achieve this, it is especially important to select the appropriate technical program committee members\nto match the research topics since it can greatly influence the accuracy of the final recommendation.\n\n5\n\n0.9\n\n4.5\nE[I5]\n\n1\n\n0.8\n\n4\n\n3.5\n\nE[I1]\n\n0.7\n0.6\n0.5\n0.4\n0\n\n3\n\nH\u2212S\u2212S\nN = 200\nM\u2212S\u2212S\nk = 30, n = 4\nL\u2212S\u2212S\nAverage score\nR\u2212S\u2212S\nLeast variance\n0.2\n0.4\n0.6\n0.8\n1\nmatching fraction\n\n2.5\n2\n0\n\n(a) # of top 1 papers get in\n\nH\u2212S\u2212S\nN = 200\nM\u2212S\u2212S\nk = 30, n = 4\nL\u2212S\u2212S\nAverage score\nR\u2212S\u2212S\nLeast variance\n0.2\n0.4\n0.6\n0.8\n1\nmatching fraction\n(b) # of top 5 papers get in\n\n25\n\n10\n9\n\n20\nE[I ]\n\n7\n\n30\n\nE[I10]\n\n8\n\n6\n5\n4\n3\n0\n\nH\u2212S\u2212S\nN = 200\nM\u2212S\u2212S\nk = 30, n = 4\nL\u2212S\u2212S\nAverage score\nR\u2212S\u2212S\nLeast variance\n0.2\n0.4\n0.6\n0.8\n1\nmatching fraction\n\n15\n\n10\n0\n\n(c) # of top 10 papers get in\n\nH\u2212S\u2212S\nN = 200\nM\u2212S\u2212S\nk = 30, n = 4\nL\u2212S\u2212S\nAverage score\nR\u2212S\u2212S\nLeast variance\n0.2\n0.4\n0.6\n0.8\n1\nmatching fraction\n(d) # of top 30 papers get in\n\nFigure 8: Impact of paper-reviewing matching\n\n21\n\n\f5.7 Effect of Reviewers type \u2013 many types case\nWe now generalize the types of papers or reviewers of a conference recommendation system specified in Section 5.6\nto many types. We explore the effect of reviewer types on the accuracy of different voting rules. Specifically, we\nconsider the following four representative voting rules: Average score (Vas ), Eliminate the highest & lowest (Vehl ) ,\nPunish low scores (Vpl ) as given by Equation (27) \u2212 (29). We also consider the Weighted average (Vwa ) rule: The\naverage score of paper Pi is weighted on eij , where j = 1, . . . , ni , the declared expertise level of reviewer R(Sji ) who\nselects topics related to Pi , or\nhXni\ni\u22121\nXni\n\u03b3i =\n.\n(30)\nSji eij\neij\nj=1\n\nj=1\n\nWe consider the least variance rule for tie breaking and we use expectation as our performance measure. In this\nevaluation, papers and reviewers are randomly matched. Before showing our results, let us specify two functions that\nrelated to model for reviewers types: the first one is \u03c3(cij ) within the probability distribution for score Sji derived in\nEq. (6), which is specified by the following linear function\n\u03c3(cij ) = 0.5 + 1.5[1 \u2212 cij ],\n\nthe second one is a monotonic increasing function f (\u03bc) within Eq. (9), which is specified by the following linear\nfunction\nf (\u03bc) = \u03bc.\nWe set l to be 3, thus eij \u2208 {1, 2, 3}. The numerical results of E[I30 ] are shown Fig. 9.\n\nIn Fig. 9, the horizontal axis represents the number of reviews per paper, or n. The vertical axis shows the\ncorresponding expectation. From Fig. 9, we have the following observations. When submitted papers are of high\nself-selectivity , the expectation curves corresponding to these four voting rules overlapped together. In other words,\nthese four rules have the same accuracy for high self-selectivity submissions. When submitted papers of low selfselectivity, the punish low scores rule has the lowest accuracy than the other three voting rules which have nearly the\nsame accuracy. When submitted papers of medium self-selectivity, the weighted average scoring rule and the eliminate\nhighest and lowest score rule have nearly the same accuracy, and the weighted average scoring rule has slightly higher\naccuracy than the average scoring rule and punish low scores rule . This statement also holds for the submitted papers\nwith random self-selectivity.\nLessons learned: When papers and reviewers are of many types and papers and reviewers are randomly matched,\nweighted average score rule can have slightly higher accuracy than average score rule and punish low scores rule\n, and it has nearly the same accuracy with eliminate highest and lowest score rule. We have the following similar\nobservations in Section 5.4: these four voting rules have comparable accuracy, no rule can outperform others, thus the\nimprovement of conference recommendation system by voting rules is limited. Again, there are number of interesting\nquestions to explore, i.e., is the accuracy sensitive to anomaly behavior?\n\n5.8 Effect of Anomaly Behavior \u2013 Random Scoring\nLet us explore the effect of an anomaly behavior on the accuracy of a conference recommendation system. Specifically,\nwe consider one potential anomaly behavior: random scoring behavior, under which a misbehaving reviewer gives a\nrandom score to any paper she reviews regardless of the quality of that paper. We vary the fraction of anomaly behavior\nfrom 0.1 to 1 and we use expectation as our performance measure. We explore the effect of anomaly behavior on the\nconference recommendation system specified in Section 5.1 with number of reviews per paper n = 4. The numerical\nresults of E[I1 ], E[I5 ], E[I10 ], and E[I30 ] are shown in Fig. 10.\nIn Fig. 10, the horizontal axis represents the fraction of these misbehaving reviewers. The vertical axis shows\nthe corresponding expectation. From Fig. 10, we have the following observations. When we increase the fraction of\nanomaly reviewers, the expectation decreased. In other words, the more anomaly reviewers, the lower is the accuracy\nof the conference recommendation system. It is interesting to note that the accuracy of the conference recommendation\nsystem decreases in a nearly linear rate. From Fig. 10(a), we see that for low self-selectivity papers, the chance of\naccepting the best paper can withstand a small fraction of misbehaving reviewers, say around 10%. This statement\nholds for the top five papers. When submitted papers are of high self-selectivity, around 40% of misbehaving reviewers\ncan drastically disrupt the accuracy of a conference recommendation system. Because for this case, less than ten papers\nfrom the top 30 papers will get accepted, and less than four papers from the top ten papers will get accepted. Even\n\n22\n\n\f22\n\n25\n24\n\n20\n\n23\nE[I30]\n\n18\n16\n\n4\n\n6\n8\n10\naverage workload (n)\n\n22\n\nN = 200, k = 30\nLeast variance\n\n30\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\nWeighted average\n\n14\n12\n\nE[I ]\n\nN = 200, k = 30\nLeast variance\n\n21\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\nWeighted average\n\n20\n19\n18\n\n12\n\n4\n\n(a) High self-selectivity\n\n6\n8\n10\naverage workload (n)\n\n12\n\n(b) Medium self-selectivity\n\n24\n\n26\n\n23\n\n25\n\n22\n\nE[I30]\n\n24\n\n21\n\nE[I30]\n\nN = 200, k = 30\nLeast variance\n\n20\n\n22\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\nWeighted average\n\n19\n18\n17\n4\n\n6\n8\n10\naverage workload (n)\n\nN = 200, k = 30\nLeast variance\n\n23\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\nWeighted average\n\n21\n20\n\n12\n\n4\n\n(c) Low self-selectivity\n\n6\n8\n10\naverage workload (n)\n\n12\n\n(d) Random self-selectivity\n\nFigure 9: Expectation of I30 for four voting rules: Vas , Vehl , Vpl and Vwa .\nthe best paper can only be accepted with the probability of less than 0.4. When papers are submitted with medium,\nlow or random self-selectivity, around 60% misbehaving reviewers can drastically disrupt the accuracy of a conference\nrecommendation system.\nAn interesting question is that which voting rule is more robust against this type of misbehaving reviewers? Here\nwe evaluate three voting rules: average score rule, eliminate highest and lowest and punish low scores given by\nEquation (27) \u2212 (29). We use expectation as our performance measure. The numerical results of E[I30 ] are shown in\nFig. 11.\nIn Fig. 11, the horizontal axis represents of the fraction of misbehaving reviewers. The vertical axis shows the\ncorresponding expectation. From Fig. 11, we have the following observations. When we increase the fraction of\nanomaly reviewers, the expectation decreased. From Fig. 11(c), we see that when submitted papers are of low selfselectivity, the expectation curves corresponding to these three voting rules overlapped together. In other words, for\nthe low self-selectivity papers, these three voting rules have the same robustness. From Fig. 11(b), 11(d), we see that\nwhen submitted papers are of medium or random self-selectivity, eliminate highest and lowest score rule are slightly\nmore robust than the other two rules. From Fig. 11(a), we observe that when submitted papers of high self-selectivity,\nthese three voting rules have the same robustness when the fraction of anomaly reviewer is less than 30%, and when it\nis higher than 30%, eliminate highest and lowest voting rule are more robust than the other two rules.\nLessons learned: Random anomaly behavior can significantly affect the accuracy of the conference recommendation\nsystem. This is especially true for submitted papers which are of high self-selectivity (or prestigious conferences).\nThe conference recommendation system suffers significantly from this kind of anomaly behavior, say and 20% of\nmisbehaving reviewers will reduce the probability of the best paper to be accepted to around 0.5. These four voting\nrules have comparable robustness, no rule can outperform others remarkably, thus to defend this kind of anomaly\n\n23\n\n\fbehavior by voting rules may not be effective. Again, there are number of interesting questions to explore, i.e., how to\nimprove the accuracy of the conference recommendation system?\n\nN = 200, k = 30\nAverage score\nLeast variance\n\n1\n\nE[I ]\n\n1.2\n1\n\nN = 200, k = 30\nAverage score\nLeast variance\n\n5\n4\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\nE[I5]\n\n0.8\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n3\n\n0.6\n\n2\n\n0.4\n\n1\n\n0.2\n0\n\n0.2\n0.4\n0.6\n0.8\n1\nfraction of anomaly reviewers\n\n0\n\n(a) # of top 1 papers get in\n\nN = 200, k = 30\nAverage score\nLeast variance\n\nE[I10]\n\n8\n\n(b) # of top 5 papers get in\n\n25\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\nN = 200, k = 30\nAverage score\nLeast variance\n\n20\nE[I30]\n\n10\n\n0.2\n0.4\n0.6\n0.8\n1\nfraction of anomaly reviewers\n\n6\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n15\n\n4\n\n10\n\n2\n\n5\n0\n\n0.2\n0.4\n0.6\n0.8\n1\nfraction of anomaly reviewers\n\n0\n\n(c) # of top 10 papers get in\n\n0.2\n0.4\n0.6\n0.8\n1\nfraction of anomaly reviewers\n(d) # of top 30 papers get in\n\nFigure 10: Impact of random anomaly behavior when n = 4.\n\n5.9 Effect of Anomaly Behavior \u2013 Bias Scoring\nLet us explore the effect of another potential anomaly behavior on the accuracy of a conference recommendation\nsystem. Specifically, we consider bias scoring behavior, under which a misbehaving reviewer gives a high score m\nto a paper if the evaluated quality is low, say less than three, otherwise gives a low score 1 to those papers whose\nevaluated quality is above three. We vary the fraction of anomaly behavior from 0 to 0.3 and we use expectation as\nour performance measure. We explore the effect of this type of anomaly behavior on the conference recommendation\nsystem specified in Section 5.1 with number of reviews per paper n = 4. The numerical results of E[I1 ], E[I5 ], E[I10 ],\nand E[I30 ] are shown in Fig. 12.\nIn Fig. 12, the horizontal axis represents the fraction of these misbehaving reviewers. The vertical axis shows\nthe corresponding expectation. From Fig. 12, we have the following observations. When we increase the fraction\nof anomaly reviewers slightly, the expectation decreased remarkably. From Fig. 12(a), we see that for low selfselectivity papers, the chance of accepting the best paper can withstand a small fraction of misbehaving reviewers,\nsay around 6%, but for the other three self-selectivity types, even a small fraction of misbehaving reviewers may lead\nto high inaccuracy. For example, when 6% of reviewers are misbehaving, the best paper only has less than 70%\nchance of being accepted for the highly self-selective paper submissions and it only has less than 80% chance of being\naccepted for the submitted papers which are of medium or random self-selectivity. Similar deterioration can be said\nfor the top five papers. Around 15% of misbehaving reviewers can drastically disrupt the accuracy of a conference\n\n24\n\n\f16\n14\n\n25\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\n20\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\nE[I30]\n\n18\n\nE[I30]\n\n12\n\n15\n\n10\n8\n\n10\n\nN = 200\n6 k = 30, n = 4\nLeast variance\n4\n0\n0.2\n0.4\n0.6\n0.8\n1\nfraction of anomaly reviewers\n\nN = 200\nk = 30, n = 4\n5 Least variance\n0\n0.2\n0.4\n0.6\n0.8\n1\nfraction of anomaly reviewers\n\n(a) High self-selectivity\n\n30\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\n25\n\nAverage score\nEliminate h\u2212est & l\u2212est\nPunish low scores\n\n20\n\nE[I30]\n\n15\n\n30\n\nE[I ]\n\n20\n\n(b) Medium self-selectivity\n\n15\n\n10\n\n10 N = 200\nk = 30, n = 4\n5 Least variance\n0\n0.2\n0.4\n0.6\n0.8\n1\nfraction of anomaly reviewers\n\nN = 200\nk = 30, n = 4\n5 Least variance\n0\n0.2\n0.4\n0.6\n0.8\n1\nfraction of anomaly reviewers\n(c) Low self-selectivity\n\n(d) Random self-selectivity\n\nFigure 11: Robustness of three voting rules: Vas , Vehl , and Vpl when n = 4.\nrecommendation system, since less than 15 papers from top 30 papers will get accepted.\nLessons learned: Bias scoring anomaly behavior can significantly affect the accuracy of the conference recommendation system. A small fraction of this kind of misbehaving reviewers can decrease the accuracy of the conference\nrecommendation system dramatically. This is especially true for submitted papers which are of high, medium, or\nrandom self-selectivity (prestigious, medium, or newly started conferences). The conference recommendation system\nsuffers severely from this kind of anomaly behavior, say 15% of misbehaving reviewers will disrupt the accuracy of\nthe conference recommendation system.\n\n5.10 Improving Conference Recommendation Systems\nIn reality, most conferences use homogeneous review strategy, with which each paper is reviewed by the same number\nof reviewers. One obvious advantage of this strategy is its fairness for all papers. But its efficiency in using the\nworkload is low. Here we propose a heterogeneous review strategy that that can increase the efficiency of homogeneous\nreview strategy. In other words, with the same reviewing workload W , it has a much higher chance to finally include\nthe top k papers in the final acceptance recommendation. Assume that the the total reviewing workload for the\nhomogeneous review strategy is W = N \u2217 n. Our heterogeneous review strategy works in two rounds:\n\nRound 1: Eliminate half of the submitted papers using only half of the workload. Specifically, each paper will receive\n\u230an/2\u230b reviews in the first round. After this reviewing round, apply a voting rule and tie breaking rule to eliminate N/2\npapers.\nRound 2: Select k papers from the remaining N/2 papers to accept. Each paper entering this round will receive\n2\u2308n/2\u2309 reviews. After the reviewing process finished, combine their reviews in round 1 and round 2. Then apply a\n25\n\n\f1\n\n5\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n4\n5\n\nE[I ]\n\n1\n\nE[I ]\n\n0.8\n0.6\n\n0.4 N = 300, k = 30\nAverage score\nLeast variance\n0.2\n0\n0.06 0.12 0.18 0.24\nanomaly fraction\n\n3\n2 N = 300, k = 30\nAverage score\nLeast variance\n1\n0\n0.06 0.12 0.18 0.24\nanomaly fraction\n\n0.3\n\n(a) # of top 1 papers get in\n\n10\n\n25\n\n30\n\nE[I ]\n\n6\n4 N = 300, k = 30\nAverage score\nLeast variance\n2\n0\n0.06 0.12 0.18 0.24\nanomaly fraction\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n20\n\n1o\n\nE[I ]\n\n0.3\n\n(b) # of top 5 papers get in\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n8\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n15\n10 N = 300, k = 30\nAverage score\nLeast variance\n5\n0\n0.06 0.12 0.18 0.24\nanomaly fraction\n\n0.3\n\n(c) # of top 10 papers get in\n\n0.3\n\n(d) # of top 30 papers get in\n\nFigure 12: Impact of bias scoring anomaly behavior when n = 4.\nvoting rule and a tie breaking rule to select the top k papers to accept.\nDefinition 3 Let E[Ii | hom] and E[Ii | hetero] represent the expectation of Ii under homogeneous or heterogeneous\nreview strategy applied respectively, where Ii is defined by Definition 2.\nDefinition 4 The improvement of heterogeneous review strategy over homogeneous review strategy is:\n\u2206E[Ii ] = E[Ii | hetero] \u2212 E[Ii | hom],\nand the improvement ratio is:\n\u2206E[Ii ]/E[Ii | hom].\nWe evaluate these two strategies on a conference recommendation system specified in Section 5.1. The numerical\nresults of \u2206E[I30 ] and \u2206E[I30 ]/E[I30 | hom] are shown in Fig. 13, where the horizontal axis represents the average\nreviewing workload n. The vertical axis shows the corresponding improvement or improvement ratio. From Fig. 13,\nwe see an improvement of heterogeneous review strategy over homogeneous review strategy. When the reviewing\nworkload is small, say n = 3, with heterogeneous review strategy at least one more paper from the top 30 papers will\nget accepted. For papers with high self-selectivity, we have more improvement wherein three or more papers from the\ntop 30 papers will get accepted. When the average reviewing workload increased to six, the improvement becomes\nstabilized. When the reviewing workload is three, the improvement is the highest, or around four more papers from\nthe top 30 papers will get accepted for papers submitted with high self-selectivity, and the improvement ratio for this\ncase is around 30%.\nAn interesting question is that with heterogeneous review strategy, how large the average reviewing workload do\nwe need. We apply our heterogeneous strategy to the conference recommendation system specified in Section 5.3. We\n26\n\n\fuse expectation as performance measure. The numerical results of E[I1 ], E[I5 ], E[I10 ] and E[I30 ] are shown in Fig.\n14, where the horizontal axis represents the average reviewing workload n. The vertical axis shows the corresponding\nexpectation. From Fig. 14, we see that we need to increase the workload to at least five such that we have a strong\nguarantee that the best paper will get accepted, which reduces the average review workload by two as compared with\nthe homogeneous review strategy, in which we need to increase the average reviewing workload to at least seven as\nstated in Section 5.3.\nSummary: Lessons learned: Our heterogeneous review strategy uses equal or less resource (e.g., reviewing workload) than the homogeneous review strategy and at the same time, achieve higher accuracy.\n\n0.2\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n0.1\n\n30\n\n\u2206E[I ] / E[I\n\n2\n1\n0\n\nN = 30, k = 30\nAverage score\nLeast variance\n\n30\n\n30\n\n\u2206E[I ]\n\n3\n\n0.3\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n| hom]\n\nN = 30, k = 30\nAverage score\nLeast variance\n\n4\n\n2\n\n4\n6\n8\n10\naverage workload (n)\n\n0\n\n12\n\n(a) Improvement\n\n2\n\n4\n6\n8\n10\naverage workload (n)\n\n12\n\n(b) Improvement ratio\n\nFigure 13: Improvement of heterogeneous review strategy on homogeneous review strategy\n\n6 Related Work\nIn [8, 9, 14, 21], authors studied peer review systems. Typically, the main issue is the reviewer assignment problem\nwhich contains three phrases: specifying the assignment constraint, computing the matching degree between reviewers\nand submissions, and optimizing the assignment with constraints. Disciplines like information retrieval [8], artificial\nintelligence [11, 21] and operations research [5, 9, 14], etc address these assignment problems.\nAuthors in [4, 19, 24, 25] worked on the group recommendation systems and address issues on rating scale and\n[2, 15, 6] on preference aggregation. Rating is used to show individuals' preferences, and in [25], authors stated\nthat discrete rating scales (number of rating points) outperform continuous rating scales. In [4], authors evaluated\nthe reliability of rating scales and showed evidence that more rating points will have a more reliable rating. In [19],\nauthors stated that the best rating scale is around five to ten rating points. Preference aggregation is the process to\nmerge the preference of multiple people so as to make recommendations. Basically the aggregation method can be\ndivided into two classes based on the preference type: cardinal ranking or ordinal ranking. For cardinal ranking case,\nweighted average strategy[16] is the most popular strategy, and it is used in PolyLen. The second class is the ordinal\nranking preference, for which, each individual's preference is shown by a ranked list of a subset of the candidates. For\nthis case, users' preferences are treated as a set of constraints and a preference aggregation approach attempts to find\nrecommendations that satisfy the constraints of all users[2, 6, 15]. As far as we know, our work is the first that study\nthe mathematical modeling of competitive group recommendation systems and apply it to peer review systems.\n\n7 Conclusions\nThis is the first paper that provides a mathematical model and analysis on a competitive group recommendation system.\nWe apply it to a conference peer review system and show how various factors may influence the overall accuracy of\nthe final recommendation. We formally analyze the model and through this analysis, we gain the insight on developing\na randomized algorithm, which is both computationally efficient and can provide performance guarantees on various\nperformance measures. Number of interesting observations are found, e.g., for a medium tier conference, three reviews\n27\n\n\f5\n\n0.9\n\n4.5\nE[I5]\n\n1\n\nE[I ]\n\n1\n\n0.8\n0.7\n\nN = 30, k = 30\nAverage score\nLeast variance\n\n0.6\n2\n\n3.5\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n4\n6\n8\n10\naverage workload (n)\n\n4\n\nN = 30, k = 30\nAverage score\nLeast variance\n\n3\n2.5\n\n12\n\n2\n\n(a) # of top 1 papers get in\n\n4\n6\n8\n10\naverage workload (n)\n\n12\n\n(b) # of top 5 papers get in\n\n10\n\n26\n24\n\n9\n\n22\n\nE[I30]\n\nE[I10]\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n8\n\n20\n\n7\nN = 30, k = 30\nAverage score\nLeast variance\n\n6\n5\n\n2\n\n18\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n4\n6\n8\n10\naverage workload (n)\n\nN = 30, k = 30\nAverage score\nLeast variance\n\n16\n14\n\n12\n\n2\n\n(c) # of top 10 papers get in\n\nH\u2212S\u2212S\nM\u2212S\u2212S\nL\u2212S\u2212S\nR\u2212S\u2212S\n\n4\n6\n8\n10\naverage workload (n)\n\n12\n\n(d) # of top 30 papers get in\n\nFigure 14: Expectation of I1 , I5 , I10 and I30 with heterogeneous review strategy.\nper paper are sufficient to achieve high accuracy in the final recommendation, but for some prestigious conferences,\nwe need at least seven reviews per paper. Lastly, we propose a heterogeneous review strategy that requires equal or\nless reviewing workload but can produce a more accurate recommendation than the homogeneous review strategy.\nWe believe our model and methodology are important building blocks for researchers to study competitive group\nrecommendation systems.\n\nReferences\n[1] S. Amer-Yahia, S. Roy, A. Chawlat, G. Das, and C. Yu. Group recommendation: Semantics and efficiency. Proc.\nof VLDB, 2009.\n[2] J. Baskin and S. Krishnamurthi. Preference aggregation in group recommender systems for committee decisionmaking. In Proc. of ACM RecSys, 2009.\n[3] L. Boratto and S. Carta. State-of-the-art in group recommendation and new approaches for automatic identification of groups. Information Retrieval and Mining in Distributed Environments, pages 1\u201320, 2011.\n[4] G. Churchill Jr and J. Peter. Research design effects on the reliability of rating scales: a meta-analysis. Journal\nof Marketing Research, pages 360\u2013375, 1984.\n[5] W. Cook, B. Golany, M. Kress, M. Penn, and T. Raviv. Optimal allocation of proposals to reviewers to facilitate\neffective ranking. Management Science, pages 655\u2013661, 2005.\n\n28\n\n\f[6] W. Cook, B. Golany, M. Penn, and T. Raviv. Creating a consensus ranking of proposals from reviewers partial\nordinal rankings. Computers & operations research, 34(4):954\u2013965, 2007.\n[7] M. Deshpande and G. Karypis. Item-based top-n recommendation algorithms. ACM TOIS, 22(1):143\u2013177, 2004.\n[8] S. Dumais and J. Nielsen. Automating the assignment of submitted manuscripts to reviewers. In Proc. of ACM\nSIGIR, 1992.\n[9] D. Hartvigsen, J. Wei, and R. Czuchlewski. The conference paper-reviewer assignment problem. Decision\nSciences, 30(3):865\u2013876, 1999.\n[10] J. Herlocker, J. Konstan, L. Terveen, and J. Riedl. Evaluating collaborative filtering recommender systems. ACM\nTOIS, 22(1):5\u201353, 2004.\n[11] S. Hettich and M. Pazzani. Mining for proposal reviewers: lessons learned at the national science foundation. In\nProc. of SIGKDD'06.\n[12] A. Jameson. More than the sum of its members: challenges for group recommender systems. In Proc. of ACM\nworking conference on Advanced visual interfaces, 2004.\n[13] A. Jameson, S. Baldes, and T. Kleinbauer. Two methods for enhancing mutual awareness in a group recommender\nsystem. In Proceedings of ACM working conference on Advanced visual interfaces, 2004.\n[14] M. Karimzadehgan and C. Zhai. Constrained multi-aspect expertise matching for committee review assignment.\nIn Proc. of CIKM'09.\n[15] F. Lorenzi, F. dos Santos, P. Ferreira, and A. Bazzan. Optimizing preferences within groups: a case study on\ntravel recommendation. Advances in Artificial Intelligence-SBIA, pages 103\u2013112, 2008.\n[16] J. Masthoff. Group modeling: Selecting a sequence of television items to suit a group of viewers. User Modeling\nand User-Adapted Interaction, 14(1):37\u201385, 2004.\n[17] M. Mitzenmacher and E. Upfal. Probability and computing: Randomized algorithms and probabilistic analysis.\nCambridge Univ Pr, 2005.\n[18] M. Oconnor, D. Cosley, J. Konstan, and J. Riedl. Polylens: A recommender system for groups of users. In Proc.\nof ECSCW 2001.\n[19] C. Preston and A. Colman. Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences. Acta psychologica, 104(1):1\u201315, 2000.\n[20] P. Resnick and H. Varian. Recommender systems. Communications of the ACM, 40(3):56\u201358, 1997.\n[21] M. Rodriguez and J. Bollen. An algorithm to determine peer-reviewers. In Proc. of ACM CIKM, 2008.\n[22] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. Item-based collaborative filtering recommendation algorithms.\nIn Proc. of WWW'01.\n[23] J. Schafer, J. Konstan, and J. Riedi. Recommender systems in e-commerce. In Proc. of ACM EC, 1999.\n[24] E. I. Sparling and S. Sen. Rating: how difficult is it? In Proc. of ACM RecSys, 2011.\n[25] E. Svensson. Comparison of the quality of assessments using continuous and discrete ordinal rating scales.\nBiometrical Journal, 42(4):417\u2013434, 2000.\n\n29\n\n\fAppendix\nTheorem 7 (Chernoff Bound[17])\nLet X1 , . . . , Xn be independent random variables with Xi = 1 with probability p\nPn\nand 0 otherwise. Let X = i=1 Xi and let \u03bc = E[X] = np. Then we have\nPr[|X \u2212 \u03bc| \u2265 \u01eb\u03bc] \u2264 2e\u2212\u03bc\u01eb\n\n2\n\n/3\n\n.\n\nIn the following lemma we derive a loose bound on the number of simulation rounds K but have a good performance guarantee on the pmd of I(k), or Pr[I(k) = i], \u2200i by carefully applying Theorem 7.\nLemma 3 When the following condition holds:\nK \u2265 max\n\ni\u2208{0,...,k}\nPr[I(k)=i]6=0\n\n3 ln(2(k + 1)/\u03b4)\n,\nPr[I(k) = i]\u01eb2\n\n(31)\n\nthen for each i = 0, 1, . . . , k,\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i] \u2264 \u01ebPr[I(k) = i],\n\nholds with probability at least 1 \u2212 \u03b4/(k + 1).\n\nProof: Without lose of any generality, consider the performance guarantee on the approximation of Pr[I(k) = i],\nwhere i = 0, 1, . . . , k. Our goal is to show\nb\nPr[I(k) = i] \u2212 Pr[I(k) = i] \u2264 \u01ebPr[I(k) = i].\n\nLet Iij be an indicator random variable defined by\n\u001a\n1 if in j-th round, |AI (k) \u2229 A(k)| = i\nIij =\n,\n0 otherwise\nwhere j = 1, . . . , K. There are two cases to explore:\nCase 1: Pr[I(k) = i] = 0. The physical meaning implies that the event I(k) = i never happen, which result in Iij = 0\nPK\nfor all j = 0, 1, . . . , K. Hence, b\nPr[I(k) = i] = j=1 Iij /K = 0. Then we have\nb\nPr[I(k) = i] \u2212 Pr[I(k) = i] \u2264 \u01ebPr[I(k) = i].\n\nCase 2: Pr[I(k) = i] 6= 0. Since each round runs independently, thus random variables Ii1 , . . . , IiK are independent\nPK\nrandom variables with Iij = 1 with probability Pr[I(k) = i] and 0 otherwise. Let Ii = j=1 Iij . Then E[Ii ] =\nKPr[I(k) = i]. From Algorithm 1 we could see that b\nPr[I(k) = i], the approximate value of Pr[I(k) = i], is given by\nb\nPr[I(k) = i] = Ii /K. Then by applying Theorem 7 we have,\nh\ni\nPr |b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2265 \u01ebPr[I(k) = i]\n= Pr [|Ii \u2212 KPr[I(k) = i]| \u2265 \u01ebKPr[I(k) = i]]\n\n\u2264 2e\u2212KPr[I(k)=i]\u01eb\n\n2\n\n/3\n\n,\n\nby substituting K with Inequality (31), we have\nh\ni\nPr |b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2265 \u01ebPr[I(k) = i]\n\u2264 \u03b4/(k + 1).\n\n30\n\n\fFinally the proof of this lemma can be completed by\nh\ni\nPr |b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2264 \u01ebPr[I(k) = i]\nh\ni\n\u2265 1 \u2212 Pr |b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2265 \u01ebPr[I(k) = i]\n\u2265 1 \u2212 \u03b4/(k + 1).\n\nThis lemma is proved.\nLemma 3 shows the performance guarantee on Pr[I(k) = i] with success probability at least 1 \u2212 \u03b4/(k + 1) for\neach specific i = 0, 1, . . . , k. Then, what is the success probability for all i = 0, 1, . . . , k? The answer of this question\nis stated in the following lemma.\nLemma 4 When the following condition holds:\nK \u2265 max\n\ni\u2208{0,...,k}\nPr[I(k)=i]6=0\n\n3 ln(2(k + 1)/\u03b4)\n,\nPr [I(k) = i]\u01eb2\n\nthen\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i] \u2264 \u01ebPr[I(k) = i],\n\nholds for all x = 0, 1, . . . , k, with probability at least 1 \u2212 \u03b4.\n\nb\nProof: Let Ei denote the event that |Pr[I(k)\n= i] \u2212 Pr[I(k) = i]| \u2264 \u01ebPr[I(k) = i] holds. From Lemma 3 we see that\nfor each i = 0, 1, . . . , k, the condition\n|b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2264 \u01ebPr[I(k) = i]\n\nholds with probability at least 1 \u2212 \u03b4/(k + 1), thus\n\nPr[Ei ] \u2265 1 \u2212 \u03b4/(k + 1).\nOur goal is to derive the probability that condition\n|b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2264 \u01ebPr[I(k) = i]\n\nholds for all i = 0, 1, . . . , k. Specifically, the probability that events E0 , E1 , . . . , Ek all happens, or\nPr[E0 , . . . , Ek ].\nNote that the physical meaning of Ei is that b\nPr[I(k) = i], the approximate value of Pr[I(k) = i], is close to the\ntrue value of Pr[I(k) = i]. Thus the physical meaning of \u2229i\u2208F Ei , where F \u2286 {0, . . . , k}, is that the approximate\nvalue of Pr[I(k) = i], is close to the true value of Pr[I(k) = i], for all i \u2208 F . Thus \u2229i\u2208F Ei happens implies that\nP\nP\nP\nb\ni\u2208F Pr[I(k) = i].\ni\u2208F Pr[I(k) = i] is close to the real value of\ni\u2208F Pr[I(k) = i], the approximate value of\nSince\nX\nX\nX\nX\nb\nb\nPr[I(k) = i] \u2212\nPr[I(k) = i] =\nPr[I(k) = i] \u2212\nPr[I(k) = i],\ni\u2208F\n\ni\u2208F\n\ni\u2208F\n\ni\u2208F\n\nP\n\nwhere F = {0, 1, . . . , k}\\F is the complement of F , thus we have that i\u2208F b\nPr[I(k) = i] is close to the real value\nP\nP\nP\nPr[I(k) = i] is close the real value of i\u2208F Pr[I(k) = i]. Thus \u2229i\u2208F Ei\nof i\u2208F Pr[I(k) = i] if and only if i\u2208F b\nP\nP\nimplies that i\u2208F b\nPr[I(k) = i] is close the real value of i\u2208F Pr[I(k) = i].\nP\nb\n= i]\nThe event \u2229i\u2208G Ei , where G \u2286 F , is more likely to happen given the prior information that i\u2208F Pr[I(k)\nP\nP\nb\nis close the real value of i\u2208F Pr[I(k) = i] than given noting at all. Since \u2229i\u2208F Ei implies that i\u2208F Pr[I(k) = i]\nP\nis close the real value of i\u2208F Pr[I(k) = i], thus \u2229i\u2208G Ei , where G \u2286 F, is more likely to happen given prior\ninformation that \u2229i\u2208F Ei happens than given nothing at all. Or mathematically,\nPr[\u2229i\u2208G Ei | \u2229i\u2208F Ei ] \u2265 Pr[\u2229i\u2208G Ei ],\n31\n\n\fwhere G \u2286 F. Based on this fact, we have\nPr[E0 , . . . , Ek ] =\n\nYk\n\ni=0\n\nPr[Ei | \u2229j\u2208Fi Ej ] \u2265\n\nYk\n\ni=0\n\nPr[Ei ],\n\nwhere Fi = {0, . . . , k}/{0, . . . , i}. By substituting Pr[Ei ] with Pr[Ei ] \u2265 1 \u2212 \u03b4/(k + 1) we have\nPr[E0 , . . . , Ek ] \u2265 [1 \u2212 \u03b4/(k + 1)]k+1 \u2265 1 \u2212 \u03b4,\n\nwhich completes the proof.\nb\nIn the following lemma we derive the error bound of expectation, or |E[I(k)]\n\u2212 E[I(k)]|, under the condition that\nb\n|Pr[I(k) = i] \u2212 Pr[I(k) = i]| \u2264 \u01ebPr[I(k) = i] holds for all i = 0, 1, . . . , k.\nLemma 5 When the following condition holds:\n\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i] \u2264 \u01ebPr[I(k) = i],\n\nfor all i = 0, 1, . . . , k, then\n\nb\nE[I(k)]\n\u2212 E[I(k)] \u2264 \u01ebE[I(k)]\n\nholds.\nProof: The proof is quite straightforward,\n\nb\nE[I(k)]\u2212E[I(k)]\n=\n\u2264\n\n\u2264\nwhich complets the proof.\n\nXk\n\ni=0\n\nXk\n\n\u0010\n\u0011\nb\ni Pr[I(k)\n= i]\u2212Pr[I(k) = i]\n\ni b\nPr[I(k) = i]\u2212Pr[I(k) = i]\n\ni=0\nXk\n\n\u01ebiPr[I(k) = i] = \u01ebE[I(k)],\n\ni=0\n\nb\nIn the following, we let \u2206E[I(k)] = E[I(k)]\n\u2212 E[I(k)] and \u2206pi = b\nPr[I(k) = i] \u2212 Pr[I(k) = i]. In the following\n2\nwe first derive the bound of (\u2206E[I(k)]) , and then apply the bound of (\u2206E[I(k)])2 to derive the bound of variance,\nb\nor |Var[I(k)]\n\u2212 Var[I(k)]|. The bound of (\u2206E[I(k)])2 is stated in the following lemma.\nLemma 6 When the following condition holds:\n\nb\nPr[I(k)\n= i]\u2212Pr[I(k) = i] \u2264 \u01ebPr[I(k) = i]\n\nfor all i = 0, 1, . . . , k, then\n\n2\n\n(\u2206E[I(k)]) \u2264 \u01eb2 Var[I(k)]\n\nholds.\n\n(32)\n\nP\nProof: First we have |\u2206pi | \u2264 \u01ebPr[I(k) = i]. It is straightforward to show ki=0 \u2206pi = 0, or\nXk\nXk\nXk\nb\nPr[I(k) = i]\nPr[I(k) = i] \u2212\n\u2206pi =\ni=0\n\ni=0\n\ni=0\n\n= 0.\n\nBased on this fact, we could have\n(\u2206E[I(k)])2 =\n=\n\u2264\n\n\u0012X\nk\n\ni=0\n\n\u0012X\nk\n\ni=0\n\n\u0012X\nk\n\ni=0\n\ni\u2206pi\n\n\u00132\n\ni\u2206pi \u2212 E[I(k)]\n\nXk\n\n|i \u2212 E[I(k)]| |\u2206pi |\n32\n\ni=0\n\n\u00132\n\n.\n\n\u2206pi\n\n\u00132\n(33)\n\n\fThen by substituting |\u2206pi | with |\u2206pi | \u2264 \u01ebPr[I(k) = i] we have\n2\n\n(\u2206E[I(k)]) \u2264\nby applying Cauchy's Inequality we have,\n\u0012X\nk\n2\n(\u2206E[I(k)]) \u2264\n\n\u0012X\nk\n\n\u00132\n|i \u2212 E[X]|\u01ebPr[I(k) = i]\n\ni=0\n\n2\n\ni=0\n\n\u01eb Pr[I(k) = i]\n\n= \u01eb2 Var[I(k)],\n\n\u0013 \u0012X\n\nk\ni=0\n\n2\n\n(i \u2212 E[X]) Pr[I(k) = i]\n\n\u0013\n\nwhich completes the proof.\nb\nIn the following lemma we apply Lemma 6 to derive the error bound of variance, or |Var[I(k)]\n\u2212 Var[I(k)]|.\n\nLemma 7 When the following condition holds:\n\nb\nPr[I(k)\n= i]\u2212Pr[I(k) = i] \u2264 \u01ebPr[I(k) = i]\n\nfor all i = 0, 1, . . . , k, then\n\nb\nVar[I(k)]\n\u2212 Var[I(k)] \u2264 \u01eb(1 + \u01eb)Var[I(k)]\n\nholds.\n\nb\nProof: First we can write Var[I(k)]\n\u2212 Var[I(k)] in the following form:\nb\nVar[I(k)]\n\u2212 Var[I(k)] =\n=\n\nSince\n\nPk\n\ni=0\n\n\u2206pi = 0, thus we have\n\nXk\n\ni=0\nXk\n\ni=0\n\ni2 \u2206pi \u2212 2E[I(k)]\u2206E[I(k)] \u2212 (\u2206E[I(k)])2\nXk\n\u2206pi \u2212 (\u2206E[I(k)])2\n(i \u2212 E[I(k)])2 \u2206pi \u2212 (E[I(k)])2\ni=0\n\nb\nVar[I(k)]\n\u2212 Var[I(k)] =\n\u2264\n\nXk\n\ni=0\n\nXk\n\ni=0\n\n(i \u2212 E[I(k)])2 \u2206pi \u2212 (\u2206E[I(k)])2\n\n(i \u2212 E[I(k)])2 |\u2206pi | + (\u2206E[I(k)])2 ,\n\n(34)\n\nby substituting \u2206pi with |\u2206pi | \u2264 \u01ebPr[I(k) = i], and substituting (\u2206E[I(k)])2 with Inequality (32) we have\n\nwhich completes the proof.\n\nb\n|Var[I(k)]\n\u2212 Var[I(k)]| \u2264 \u01eb(1 + \u01eb)Var[I(k)],\n\nIn the following lemmas, we show how to tradeoff the number of simulation rounds K with the performance\nguarantees. The tradeoff between K and the performance guarantee on pmf is stated in the following theorem.\nLemma 8 When the following condition holds:\nK \u2265 3 ln(2(k + 1)/\u03b4)/\u01eb2 ,\nthen for each i = 0, . . . , k,\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i] \u2264 \u01eb\n\nholds with probability at least 1 \u2212 \u03b4/(k + 1).\n\n33\n\np\nPr[I(k) = i],\n\n(35)\n\n\fProof: Without lose of any generality, consider the performance guarantee on the approximation of Pr[I(k) = i],\nwhere i = 0, 1, . . . , k. Our goal is to show\np\nb\nPr[I(k) = i] \u2212 Pr[I(k) = i] \u2264 \u01eb Pr[I(k) = i].\n\nFollowing similar method in Lemma 3 and the same notations defined in the proof of Lemma 3, we could have the\nfollowing:\nCase 1: Pr[I(k) = i] = 0. Then b\nPr[I(k) = i] = 0. Hence\np\nb\nPr[I(k) = i] \u2212 Pr[I(k) = i] \u2264 \u01eb Pr[I(k) = i].\nCase 2: Pr[I(k) = i] 6= 0. Following similar method in Lemma 3 we have\ni\nh\np\nPr |b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2264 \u01eb Pr[I(k) = i]\nh\ni\np\n\u2265 1 \u2212 Pr |b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2265 \u01eb Pr[I(k) = i]\n\"\n#\n\u01ebKPr[I(k) = i]\n= 1 \u2212 Pr |Ii \u2212 KPr[I(k) = i]| \u2265 p\nPr[I(k) = i]\n\u2265 1 \u2212 2e\u2212K\u01eb\n\n2\n\n/3\n\n,\n\nby substituting K with Inequality (35) we can prove this lemma.\nUsing the same method in Lemma 3, we can prove the following lemma.\nLemma 9 When K \u2265 3 ln(2(k + 1)/\u03b4)/\u01eb2 , then\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i] \u2264 \u01eb\n\nholds for all i = 0, 1, . . . , k with probability at least 1 \u2212 \u03b4.\n\np\nPr[I(k) = i]\n\nb\nIn the following lemma we derive\np the error bound of expectation, or |E[I(k)] \u2212 E[I(k)]|, under the condition that\n|b\nPr[I(k) = i] \u2212 Pr[I(k) = i]| \u2264 \u01eb Pr[I(k) = i] holds for all i = 0, 1, . . . , k.\nLemma 10 When the following condition holds:\n\nholds for all i = 0, 1, . . . , k, then\n\nholds.\n\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i] \u2264 \u01eb\nb\nE[I(k)]\n\u2212 E[I(k)] \u2264 \u01eb\n\np\nPr[I(k) = i]\n\np\n(k + 1)kE[I(k)]/2\n\np\nProof: First we have \u2206pi \u2264 \u01eb Pr[I(k) = i]. Then we have the following\nb\nE[I(k)]\n\u2212 E[I(k)] \u2264\n\n\u2264\n\nby applying Cauchy's Inequality we have,\nb\nE[I(k)]\n\u2212 E[I(k)] \u2264 \u01eb\n\nXk\n\ni=0\nXk\n\ni=0\n\ns\u0012\nXk\n\ni=0\n\ni\n\ni|\u2206pi |\np\n\u01ebi Pr[I(k) = i],\n\n\u0013\u0012X\nk\n\ni=0\n\niPr[I(k) = i]\n\np\n= \u01eb (k + 1)kE[I(k)]/2,\n34\n\n\u0013\n\n\fwhich completes the proof.\n2\nb\nIn the\np following lemma we derive the bound of (\u2206E[I(k)]) under the condition that |Pr[I(k) = i] \u2212 Pr[I(k) =\ni]| \u2264 \u01eb Pr[I(k) = i] holds for all i = 0, 1, . . . , k.\n\nLemma 11 When the following condition holds:\n\nholds for all i = 0, 1, . . . , k, then\n\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i] \u2264 \u01eb\n\np\nPr[I(k) = i]\n\n(\u2206E[I(k)])2 \u2264 \u01eb2 (k + 1)Var[I(k)]\n\nholds.\n\n(36)\n\np\nProof: First we have \u2206pi \u2264 \u01eb Pr[I(k) = i]. From Inequality (33) we have\n\u0012X\n\u00132\nk\n2\n(\u2206E[I(k)]) \u2264\n|i \u2212 E[I(k)]| |\u2206pi | ,\ni=0\n\np\nby substituting |\u2206pi | with \u2206pi \u2264 \u01eb Pr[I(k) = i] we have\n\u00132\n\u0012X\np\nk\n2\n(\u2206E[I(k)]) \u2264\n|i \u2212 E[I(k)]| \u01eb Pr[I(k) = i] ,\ni=0\n\nby applying Cauchy's Inequality we have,\n\u0012X\nk\n2\n(\u2206E[I(k)]) \u2264\n\ni=0\n\n\u01eb\n\n2\n\n2\n\n\u0013 \u0012X\n\nk\ni=0\n\n= \u01eb (k + 1)Var[I(k)],\n\n\u0013\n(i \u2212 E[I(k)]) Pr[I(k) = i]\n2\n\nwhich completes the proof.\nb\nIn the following lemma we apply Lemma 11 to derive\npthe error bound of variance, or |Var[I(k)] \u2212 Var[I(k)]|,\nb\nunder the condition that |Pr[I(k) = i] \u2212 Pr[I(k) = i]| \u2264 \u01eb Pr[I(k) = i] holds for all i = 0, 1, . . . , k.\n\nLemma 12 When the following condition holds:\n\nholds for all i = 0, 1, . . . , k, then\n\nholds.\n\nb\nPr[I(k)\n= i] \u2212 Pr[I(k) = i] \u2264 \u01eb\n\nb\nVar[I(k)]\n\u2212 Var[I(k)] \u2264 \u01eb(k + 1)\n\np\nPr[I(k) = i]\n\n\u0011\n\u0010p\n(2k + 1)Var [I(k)]/6 + \u01ebVar [I(k)]\n\np\nProof: First we have \u2206pi \u2264 \u01eb Pr[I(k) = i]. From Inequality (34) we have\nXk\nb\nVar[I(k)]\n\u2212 Var[I(k)] \u2264\n(i \u2212 E[I(k)])2 |\u2206pi |+ (\u2206E[I(k)])2 ,\ni=0\np\nby substituting \u2206pi with |\u2206pi | \u2264 \u01eb Pr[I(k) = i] we have\nXk\np\nb\n(i \u2212 E[I(k)])2 \u01eb Pr[I(k) = i] + (\u2206E[I(k)])2 ,\n|Var[I(k)]\n\u2212 Var[I(k)]| \u2264\ni=0\n\nby applying Cauchy's Inequality we have,\n\u0012X\n\u00131/2\n\u00131/2 \u0012X\nk\nk\n2\n2\nb\n|Var[I(k)] \u2212 Var[I(k)]| \u2264 \u01eb\n(i \u2212 E[I(k)]) Pr[I(k) = i]\n+ (\u2206E[I(k)])2\n(i \u2212 E[I(k)])\ni=0\ni=0\n\u0011\n\u0010\np\n\u2264 \u01eb (k + 1) (2k + 1)Var[I(k)]/6 + (\u2206E[I(k)])2 ,\n\nthe proof can be completed by substituting (\u2206E[X])2 with Inequality (36).\n\n35\n\n\f"}