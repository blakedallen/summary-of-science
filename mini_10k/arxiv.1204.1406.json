{"id": "http://arxiv.org/abs/1204.1406v1", "guidislink": true, "updated": "2012-04-06T04:37:38Z", "updated_parsed": [2012, 4, 6, 4, 37, 38, 4, 97, 0], "published": "2012-04-06T04:37:38Z", "published_parsed": [2012, 4, 6, 4, 37, 38, 4, 97, 0], "title": "An Effective Information Retrieval for Ambiguous Query", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.2026%2C1204.0795%2C1204.3058%2C1204.5457%2C1204.4262%2C1204.2462%2C1204.2103%2C1204.5867%2C1204.2010%2C1204.0405%2C1204.0766%2C1204.1379%2C1204.5422%2C1204.6575%2C1204.3416%2C1204.1478%2C1204.3391%2C1204.4545%2C1204.2746%2C1204.0456%2C1204.1232%2C1204.0464%2C1204.2020%2C1204.1650%2C1204.5064%2C1204.4630%2C1204.4571%2C1204.4661%2C1204.4563%2C1204.3550%2C1204.5033%2C1204.4647%2C1204.5607%2C1204.6604%2C1204.6385%2C1204.0757%2C1204.5716%2C1204.1580%2C1204.6079%2C1204.6585%2C1204.6496%2C1204.1072%2C1204.0393%2C1204.4009%2C1204.1437%2C1204.3950%2C1204.1297%2C1204.5053%2C1204.1391%2C1204.4003%2C1204.2223%2C1204.0905%2C1204.3748%2C1204.5107%2C1204.5628%2C1204.4463%2C1204.2602%2C1204.0299%2C1204.5972%2C1204.6257%2C1204.2135%2C1204.0953%2C1204.0115%2C1204.4608%2C1204.3905%2C1204.2586%2C1204.0537%2C1204.3939%2C1204.5511%2C1204.2866%2C1204.0416%2C1204.4005%2C1204.1652%2C1204.3722%2C1204.1755%2C1204.4360%2C1204.2505%2C1204.4052%2C1204.2015%2C1204.2319%2C1204.4292%2C1204.0679%2C1204.1806%2C1204.1366%2C1204.3709%2C1204.0256%2C1204.0207%2C1204.3506%2C1204.4234%2C1204.1406%2C1204.2173%2C1204.1058%2C1204.0209%2C1204.0661%2C1204.6348%2C1204.0334%2C1204.4717%2C1204.0420%2C1204.0108%2C1204.2352%2C1204.5067&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "An Effective Information Retrieval for Ambiguous Query"}, "summary": "Search engine returns thousands of web pages for a single user query, in\nwhich most of them are not relevant. In this context, effective information\nretrieval from the expanding web is a challenging task, in particular, if the\nquery is ambiguous. The major question arises here is that how to get the\nrelevant pages for an ambiguous query. We propose an approach for the effective\nresult of an ambiguous query by forming community vector based on association\nconcept of data minning using vector space model and the freedictionary. We\ndevelop clusters by computing the similarity between community vectors and\ndocument vectors formed from the extracted web pages by the search engine. We\nuse Gensim package to implement the algorithm because of its simplicity and\nrobust nature. Analysis shows that our approach is an effective way to form\nclusters for an ambiguous query.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.2026%2C1204.0795%2C1204.3058%2C1204.5457%2C1204.4262%2C1204.2462%2C1204.2103%2C1204.5867%2C1204.2010%2C1204.0405%2C1204.0766%2C1204.1379%2C1204.5422%2C1204.6575%2C1204.3416%2C1204.1478%2C1204.3391%2C1204.4545%2C1204.2746%2C1204.0456%2C1204.1232%2C1204.0464%2C1204.2020%2C1204.1650%2C1204.5064%2C1204.4630%2C1204.4571%2C1204.4661%2C1204.4563%2C1204.3550%2C1204.5033%2C1204.4647%2C1204.5607%2C1204.6604%2C1204.6385%2C1204.0757%2C1204.5716%2C1204.1580%2C1204.6079%2C1204.6585%2C1204.6496%2C1204.1072%2C1204.0393%2C1204.4009%2C1204.1437%2C1204.3950%2C1204.1297%2C1204.5053%2C1204.1391%2C1204.4003%2C1204.2223%2C1204.0905%2C1204.3748%2C1204.5107%2C1204.5628%2C1204.4463%2C1204.2602%2C1204.0299%2C1204.5972%2C1204.6257%2C1204.2135%2C1204.0953%2C1204.0115%2C1204.4608%2C1204.3905%2C1204.2586%2C1204.0537%2C1204.3939%2C1204.5511%2C1204.2866%2C1204.0416%2C1204.4005%2C1204.1652%2C1204.3722%2C1204.1755%2C1204.4360%2C1204.2505%2C1204.4052%2C1204.2015%2C1204.2319%2C1204.4292%2C1204.0679%2C1204.1806%2C1204.1366%2C1204.3709%2C1204.0256%2C1204.0207%2C1204.3506%2C1204.4234%2C1204.1406%2C1204.2173%2C1204.1058%2C1204.0209%2C1204.0661%2C1204.6348%2C1204.0334%2C1204.4717%2C1204.0420%2C1204.0108%2C1204.2352%2C1204.5067&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Search engine returns thousands of web pages for a single user query, in\nwhich most of them are not relevant. In this context, effective information\nretrieval from the expanding web is a challenging task, in particular, if the\nquery is ambiguous. The major question arises here is that how to get the\nrelevant pages for an ambiguous query. We propose an approach for the effective\nresult of an ambiguous query by forming community vector based on association\nconcept of data minning using vector space model and the freedictionary. We\ndevelop clusters by computing the similarity between community vectors and\ndocument vectors formed from the extracted web pages by the search engine. We\nuse Gensim package to implement the algorithm because of its simplicity and\nrobust nature. Analysis shows that our approach is an effective way to form\nclusters for an ambiguous query."}, "authors": ["R. K. Roul", "S. K. Sahay"], "author_detail": {"name": "S. K. Sahay"}, "author": "S. K. Sahay", "arxiv_comment": "11 Pages, 1 figure", "links": [{"href": "http://arxiv.org/abs/1204.1406v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.1406v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.1406v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.1406v1", "journal_reference": "AJCSIT, Vol. 2, No. 3, P. 26-30, 2012", "doi": null, "fulltext": "arXiv:1204.1406v1 [cs.IR] 6 Apr 2012\n\nAn Effective Information Retrieval for Ambiguous\nQuery\nR.K. Roul\u2217 and S.K. Sahay\u2020\nBITS, Pilani - K.K. Birla, Goa Campus, Zuarinagar, Goa - 403726, India.\n\nAbstract\nSearch engine returns thousands of web pages for a single user query, in which most of\nthem are not relevant. In this context, effective information retrieval from the expanding web\nis a challenging task, in particular, if the query is ambiguous. The major question arises here\nis that how to get the relevant pages for an ambiguous query. We propose an approach for\nthe effective result of an ambiguous query by forming community vector based on association\nconcept of data minning using vector space model and the freedictionary. We develop clusters\nby computing the similarity between community vectors and document vectors formed from\nthe extracted web pages by the search engine. We use Gensim package to implement the\nalgorithm because of its simplicity and robust nature. Analysis shows that our approach is an\neffective way to form clusters for an ambiguous query.\n\nKeywords: Information Retrieval, Clustering, Vector space model, Gensim.\n\n1\n\nIntroduction\n\nOn the web, search engines are key for the information retrieval (IR) for any user query. However,\nresolving ambiguous query is a challenging task, hence a vibrant area of research. Due to short and\nambiguity in the user query, retrieving the information as per the intention of user in large volume\nof web is not straight forward. The ambiguities in queries is due to the short query length, which\nis on an average is 2.33 times on a popular search engine [1]. In this context, Sanderson [2] reports\nthat 7%-23% of the queries frequently occur in two search engines are ambiguous with the average\nlength one. For e.g. the familiar word Java which is ambiguous as it has multiple senses viz.\nJava coffee, Java Island and Java programming language etc. In the user query, ambiguities can\nalso exists which do not appear in surface. Because of such ambiguities, search engine generally\ndoes not understand in what context user is looking for the information. Hence, it returns huge\namount of information, in which most of the retrieved pages are irrelevant to the user. These huge\n\u2217\n\u2020\n\nemail: rkroul@bits-goa.ac.in\nemail: ssahay@bits-goa.ac.in\n\n1\n\n\famount of heterogeneous information retrieve not only increases the burden for search engine but\nalso decreases its performance.\nIn this paper we propose an approach to improve the effectiveness of search engine by making\nclusters of word sense based on association concept of data mining, using vector space model of\nGensim [6] and the freedictionary [13]. The association concept on which the clusters has formed\ncan be describe as follows. Suppose, if user queried for the word Apple, which is associated\nin multiple context viz. computer, fruit, company etc. Each of this context associated with Apple is again associate with different word senses viz. computer is associated with the keyboard,\nmouse, monitor etc. Hence computer can be taken as community vector or cluster whose components/elements are the associated words keyboard, mouse, monitor, etc. Here, each element in\nthe cluster represent the sense of computer vector for apple. So, if a user looking for apple as a\ncomputer, s'he may look for 'apple keyboard' or 'apple mouse' or 'apple monitor' etc. We use\nMinipar [16] to transform a complete sentence into a dependency tree and for the classification of\nwords and phrases into lexical categories.\nThe paper is organized as follows. In section 2 we examine the related work on the information\nretrieval based on clustering technique. In section 3 we briefly discuss the Gensim package for the\nimplementation of our approach. In section 4 we present our approach for the effective information\nretrieval in the context of user query. Section 5 contains analysis of the algorithm. Finally Section\n6 is the conclusion of the paper.\n\n2\n\nRelated Work\n\nRanking and Clustering are the two most popular methods for information retrieval on the web. In\nranking, a model is designed using training data, such that model can sort new objects according to\ntheir relevance's. There are many ranking models [14] which can be roughly categorized as querydependent and query-independent models. In the other method i.e. clustering, an unstructured\nset of objects form a group, based on the similarity among each other. One of the most popular\nalgorithms on clustering is k-means algorithm. However, the problem of this algorithm is that an\ninappropriate choice of clusters (k) may yield poor results. In case of an ambiguous query, word\nsense discovery is one of the useful method for IR in which documents are clustered in corpus.\nDiscovering word senses by clustering the words according to their distributional similarity is done\nby Patrick et al, 2002. The main drawback of this approach is that they require large training data\nto make proper cluster and its performance is based on cluster centroid, which changes whenever\na new web page is added to it. Hence identifying relevant cluster will be a tedious work.\nHerrera et al., 2010 gave an approach, which uses several features extracted from the document\ncollection and query logs for automatically identifying the users goal behind their queries. This\napproach success to classifies the queries into different categories like navigational, informational\nand transactional (B. J. Jansen et al., 2008) but fails to classify the ambiguous query. As query logs\nhas been used, it may raise privacy concerns as long sessions are recorded and may led to ethical\nissues surrounding the users data collections. Lilyaa et.al [15] uses statistical relational learning\n(SRL) for the short ambiguous query based only on a short glimpse of user search activity, captured\n\n2\n\n\fin a brief search session. Many research has been done to map user queries to a set of categories\n(Powell et al., 2003; Dolin et al., 1998; Yu et al., 2001). But all of the above techniques fails to\nidentify the user intention behind the user query.\nThe Word Sense Induction (Roberto Navigli et.al, 2010) method is a graph based clustering\nalgorithm, in which snippets are clustered based on dynamic and finer grained notion of sense.\nThe approach (Ahmed Sameh et al, 2010) with the help of modified Lingo algorithm, identifying\nfrequent phrases as a candidate cluster label, the snippets are assigned to those labels. In this\napproach semantic recognition is identified by WordNet which enables recognition of synonyms\nin snippets. Clusters formation by the above two approaches not contain all the relevant pages of\nuser choice. Our work uses free dictionary and association concept of data mining has been added\nto our approach to form clusters. Secondly it can handle the dynamic nature of the web as Gensim\nhas been used. Hence the user intention behind the ambiguous query can be identified in simple\nand efficient manner.\nIn 2008, Jiyang Chen et. al. purposed an unsupervised approach to cluster results by word\nsense communities. Clusters are made based on dependency based keywords which are extracted\nfor large corpus and manual label are assigned to each cluster. In this paper we form the community vector and eliminate the problem of manual assignment of the cluster lable. We use Gensim\npackage to avoid the dependency of the large training corpus size [5], and its ease of implementing\nvector space model (e.g. LSI, LDA).\n\n3\n\nGensim\n\nGensim package is a python library for vector space modeling, aims to process raw, unstructured\ndigital texts (\"plain text\"). It can automatically extract semantic topics from documents, used\nbasically for the Natural Language Processing (NLP) community. Its memory (RAM) independent\nfeature with respect to the corpus size allows to process large web based corpora. In Gensism one\ncan easily plugin his own input corpus and data stream and other vector space algorithms can be\ntrivially incorporated in it.\nIn Gensim, many unsupervised algorithms are based on word co-occurrence patterns within a\ncorpus of training documents. Once these statistical patterns are found, any plain text documents\ncan be succinctly expressed in the new semantic representation and can be queried for the topical\nsimilarity against other documents and so on. In addition it has following salient features\n\u2022 Straightforward interfaces, scalable software framework, low API learning curve and prototyping.\n\u2022 Efficient implementations of several popular vector space algorithms, calculation of TFIDF (term frequency-inverse document frequency), distributed incremental Latent Semantic\nAnalysis, distributed incremental incremental Latent Dirichlet Allocation(LDA).\n\u2022 I/O wrappers and converters around several popular data formats.\n\n3\n\n\fVector Space Model:\nIn vector space model, each document is defined as a multidimensional vector of keywords\nin euclidean space whose axis correspond to the keyword i.e., each dimension corresponds to a\nseparate keyword [4]. The keywords are extracted from the document and weight associated with\neach keyword determines the importance of the keyword in the document. Thus, a document is\nrepresented as,\n\nDj = (w1j , w2j , w3j , w4j , ..........wnj )\nwhere wij is the weight of term i in document j indicating the relevance and importance of the\nkeyword.\nTF-IDF Concept: TF is the measure of how often a word appears in a document and IDF is the\nmeasure of the rarity of a word within the search index. Combining TF-IDF is used to measure the\nstatistical strength of the given word in reference to the query. Mathematically,\nni\nTFi = P\nk nk\nwhere, ni is the number of occurrences of the considered terms and nk is the number of occurrences\nof all terms in the given document\nIDFi = log\n\nN\ndfi\n\nwhere, N is the number of occurrences of the considered terms and dfi is the number of documents\nthat contain term i.\nTF-IDF = TFi \u00d7 log\n\nN\ndfi\n\nCosine Similarity Measure: It is a technique to measure the similarity between the document\nand the query. The angle (\u03b8) between the document vector and the query vector determines the\nsimilarity between the document and the query and it is written as\nP\nwq,j wij\nqP\ncos \u03b8 = qP\n(1)\n2\n2\nwq,j\nwi,j\nqP\n\n2\nwq,j\nand\n\nqP\n\n2\nwi,j\nis the length of the query and document vector respectively.\n\nIf \u03b8 = 0\u25e6 then the document and query is similar. As \u03b8 changes from 0o to 90o , the similarity\nbetween the document and query decreases i.e. D2 will be more similar to query than D1 , if the\nangle between D2 and query is smaller than the angel between D1 and query.\n\n4\n\n\f4\n\nOur Approach\n\nOur approach for an ambiguous query is described below in five steps and depicted in the flow\nchart (Fig. 1).\n1. Web page extraction and preprocessing: Submit the ambiguous query to a search engine and\nextract top n pages. Preprocess the retrieve corpus as follows:\n\u2022 Remove the stop and unwanted words.\n\u2022 Select noun as the keywords from the corpus using Minipar [16] and ignore other categories, such as verbs, adjectives, adverbs and pronounce.\n\u2022 Do stemming using porter algorithm [12].\n\u2022 Save each processed n pages as documents Dk , where k = 1, 2, 3, .....n.\n2. Document vectors: Compute TF and IDF score for all the keywords of each Dk using Gensim and make document vectors of all the retrieved pages.\n3. Cluster formation: We use the freedictionary with the option start with to form the community vector of the queried word as follows\n\u2022 Submit the ambiguous query (say apple) to the freedictionary, preprocess the retrieved\ndata i.e. remove the queried, stop & unwanted words. After stemming, save all the\nnoun as keywords (Wj ) in a file Fc , where j = 1, 2, 3, ......m\n\u2022 Now submit each Wj again to the freedictionary, preprocess the retrieved data and save\nthe noun as keywords along with the queried word in a community file FWj .\n\u2022 Search all the words of FWj in Dk using regular expression search technique.\n\u2022 Delete those words in FWj which are not present in Dk .\n\u2022 Wj is the formed community vectors (clusters) whose elements are the words saved in\nthe file FWj\n\u2022 Compute TF-IDF for each word in FWj in compare with Dk to form community vectors.\n4. Similarity check: Compute the cosine similarities between the formed documents and community vectors using eq. 1.\n5. Assignment of Documents to the Clusters: Assign the documents to that cluster which has\nmaximum similarity.\n\n5\n\n\fAmbiguous Query\n\nExtract top 'n' pages from a search engine\nand do the preprocessing.\n\nFreeDictionary\n\nPreprocess the retrieve data then save all the nouns\n(except queried word) as keywords ( Wj) in a file Fc\n\nExtract nouns as keywords from each 'n'\npages and save it in separate documents ( D k)\n\nAgain submit each word from the file Fc to FreeDictionary.\nPreprocess the retrieved data for each word and then\nsave the nouns and the queried word in a separate file FW j\nSearch each word of F W j in Dk\n\nDelete those words from FW which are not present in D k\nj\n\nCompute TF and IDF for each keyword of\nD kto form document vectors.\n\nCompute TF and IDF for each word of FW in compare\nj\nwith Dk to form community vectors.\n\nCompute similarities between each community vector and document vector\n\nAssign documents to clusters which has maximum similarities\n\nFigure 1: An effective IR for an ambiguous query\n\n5\n\nTest Results\n\nTo illustrate our approach we took four sample documents as shown in Table 1. We preprocess\nthe documents and extracted ten keywords (apple, computer, tree, keyboard, mouse, juice, country,\nvegetables, fruit, monitor) from the sample (Table 2). After assigning a token ID to each selected\nkeyword (Table 3) TF & IDF are computed which is shown in Table 4. In Table 5 computed weight\n(TF-IDF) of all the four sample documents are given.With the calculated weight and respective\ntoken IDs, document vectors are generated (Table 6).\nThe community vectors are formed as described in the section 4 (Table 7) and the corresponding TF-IDF and weights are calculated (Table 8). Cosine similarity are calculated defined by the\neq. 1. Now the similarity between each community vector (C1, C2) and the set of document vectors (D1, D2, D3 and D4) are computed and maximum values of the similarity between community\n6\n\n\fand document vectors form the cluster. From our experimental result, we found that (D1, D3 ) and\n(D2, D4) associated with C1 and C2 respectively i.e. two clusters are generated (Table 9 and 10).\nAs an example, from the Table 10 we say that if the user search the ambiguous word apple,\ns'he will get two clusters C1 and C2, containing most relevant documents.\n\n6\n\nConclusion\n\nFor an ambiguous query, we propose an effective approach for the IR by forming the clusters of\nrelevant web pages. For cluster formation we use standard vector space model and the freedictionary. From our approach we find that user intention behind ambiguous query can be identify\nsignificantly. This unsupervised approach not only handles the corpus by extracting and analyzing\nsignificant terms, but also form desire clusters for real time query. Further we would extend our\nwork for the multi word query and improving these clusters using ranking techniques.\n\nAcknowledgment\nWe are thankful to Bharat Deshpande and our colleague Aruna Govada and K.V. Santhilata for\ntheir useful discussions and valuable suggestions.\n\nReferences\n[1] B. J. Jansen, et al. Real life information retrieval: a study of user queries on the web, ACM\nSIGIR Forum. Vol. 32, No. 1, pp. 5-17, 1998.\n[2] M. Sanderson, Ambiguous queries: Test collections need more sense, In SIGIR-08.\n[3] B.J. Jansen, Determining the informational, navigational and transactional intent of web\nqueries, Information Processing and Management (Science Direct) 44, 1251 - 1266, 2008\n[4] http://www.scribd.com/doc/10552567/TFIDF\n[5] http://www.miislita.com/term-vector/term-vector-3.html\n[6] http://www.nlp.fi.muni.cz/projekty/gensim/intro.html\n[7] J. Chen, O. R. Zaiane and R. Goebel, An Unsupervised Approach to Cluster Web Search\nResults based on Word Sense Communities, 2008 IEEE/WIC/ACM, International Conference\non Web Intelligence and Intelligent Agent Technology.\n[8] P. Pantel and D. Lin, Discovering Word Senses from Text, SIGKDD'02, July 23-26, 2002,\nEdmonton, Ablerta, Canada.\n\n7\n\n\f[9] Herrera, M.R et al., 2010. Exploring features for the automatic identification of user goals in\nweb search. Inform Process Management, 46 pp. 131-142. DOI: 10.1016/j.ipm.2009.09.003\n[10] R. Navigil et al., Inducing Word Senses to Improve Web Search Result Clustering,\nhttp://www.hitwise.com/us/press-center/press-releases/google-searches-apr-09\n[11] A. Sameh and A. Kadray, 2010, Semantic Web Search results clusterinh Using Lingo and\nWordNet, Internation Journal of Research and Reviews in Computer Sciences\n[12] http://tartarus.org/martin/PorterStemmer/def.txt\n[13] http://www.thefreedictionary.com/\n[14] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval. Addison Wesley, May\n1999.\n[15] http://www.cs.utexas.edu/\u223cml/papers/beyondSearch.pdf\n[16] D. Lin Principar: an efficient, broad-coverage, principle-based parser. In Proceedings of the\n17th international conference on computational linguistic, pp. 482-488, 1994.\n\nAppendix\nD1 apple computer released new wireless keyboard and apple trees\nare more in our country.\nD2 all vegetables trees are different from apple trees.\nD3 the apple mouse is a multi-button USB mouse manufactured and\nsold by apple Inc.\nD4 apple as juice or fruit is very tasty and apple launch new LED monitor.\nTable 1: Sample documents taken for experiment.\n\nD1\nD2\nD3\nD4\n\napple computer keyboard apple tree country\nvegetable tree apple tree\napple mouse mouse apple\napple juice fruit apple monitor\nTable 2: Documents after preprocessing.\n\n8\n\n\fKeyword Token ID\napple\n0\ncomputer\n1\ntree\n2\nkeyboard\n3\nmouse\n4\njuice\n5\ncountry\n6\nvegetable\n7\nfruit\n8\nmonitor\n9\nTable 3: Keywords & respective token IDs.\n\nKeyword D1\napple\n2\ncomputer 1\ntree\n1\nkeyboard 1\nmouse\n0\njuice\n0\ncountry\n1\nvegetable 0\nfruit\n0\nmonitor\n0\n\nTF1\n0.33\n0.16\n0.16\n0.16\n0\n0\n0.16\n0\n0\n0\n\nD2\n1\n0\n2\n0\n0\n0\n0\n1\n0\n0\n\nTF2\n0.25\n0\n0.5\n0\n0\n0\n0\n0.25\n0\n0\n\nD3\n2\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\nTF3\n0.5\n0\n0\n0\n0.5\n0\n0\n0\n0\n0\n\nD4\n2\n0\n0\n0\n0\n1\n0\n0\n1\n1\n\nTF4\n0.4\n0\n0\n0\n0\n0.2\n0\n0\n0.2\n0.2\n\nTable 4: Calculation of TF-IDF for each documents.\n\n9\n\nIDF\n0\n0.602\n0.301\n0.602\n0.602\n0.602\n0.602\n0.602\n0.602\n0.602\n\n\fKeyword\napple\ncomputer\ntree\nkeyboard\nmouse\njuice\ncountry\nvegetable\nfruit\nmonitor\n\nD1\n0\n0.09632\n0.04816\n0.09632\n0\n0\n0.09632\n0\n0\n0\n\nD2\n0\n0\n0.1505\n0\n0\n0\n0\n0.1505\n0\n0\n\nD3\n0\n0\n0\n0\n0.301\n0\n0\n0\n0\n0\n\nD4\n0\n0\n0\n0\n0\n0.1204\n0\n0\n0.1204\n0.1204\n\nTable 5: Weight: TF x IDF.\n\nDocuments Corresponding Document Vectors\nD1\n[(0, 0), (1, 0.09632), (2, 0.04816), (3, 0.09632), (4, 0),\n(5, 0), (6, 0.09632), (7, 0), (8, 0), (9, 0)]\nD2\n[(0, 0), (1, 0), (2, 0.1505), (3, 0), (4, 0), (5, 0),\n(6, 0), (7, 0.1505), (8, 0), (9, 0)]\nD3\n[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0.301), (5, 0), (6, 0),\n(7, 0), (8, 0), (9, 0)]\nD4\n[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0.1204), (6, 0),\n(7, 0), (8, 0.1204), (9, 0.1204)]\nTable 6: Representation of document as vectors.\n\n10\n\n\fCommunity Vector Associated Keywords\n[(ID,Frequency)]\nComputer (C1)\ncomputer, keyboard,\n[(1,1), (3,1), (4,2), (9,1)]\nmouse, monitor\nFruit (C2)\nfruit, tree, vegetable, juice [(8,1), (2,3), (7,1), (5,1)]\nTable 7: Community vectors formed from communities as [ID, Frequency].\n\nKeyword\n\nC1 TF C1\n\nC2 TF C2 IDF\n\napple\ncomputer\ntree\nkeyboard\nmouse\njuice\ncountry\nvegetable\nfruit\nmonitor\n\n0\n1\n0\n1\n2\n0\n0\n0\n0\n1\n\n0\n0\n3\n0\n0\n1\n0\n1\n1\n0\n\n0\n0.25\n0\n0.25\n0.5\n0\n0\n0\n0\n0.25\n\n0\n0\n0.75\n0\n0\n0.25\n0\n0.25\n0.25\n0\n\n0\n0.602\n0.301\n0.602\n0.602\n0.602\n0.602\n0.602\n0.602\n0.602\n\nWeight = TF\u00d7 IDF\nC1\nC2\n0\n0\n0.1505 0\n0\n0.22575\n0.1505 0\n0.301\n0\n0\n0.1505\n0\n0\n0\n0.1505\n0\n0.1505\n0.1505 0\n\nTable 8: TF-IDF calculation for community vector.\n\nDocument/Community C1\n\nC2\n\nD1\nD2\nD3\nD4\n\n0.18165\n0.77149\n0.0\n0.5041\n\n0.41939\n0.0\n0.75593\n0.21828\n\nResultant Cluster\n(Max(C1,C2))\nC1 (Computer)\nC2 (Fruit)\nC1 (Computer)\nC2 (Fruit)\n\nTable 9: Similarity between each community and document is tabulated.\n\nQuery (apple) Community (sense) Cluster\nCluster 1\nComputer\nD1, D3\nCluster 2\nFruit\nD2, D4\nTable 10: Final clustering of relevant documents.\n\n11\n\n\f"}