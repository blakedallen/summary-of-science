{"id": "http://arxiv.org/abs/1011.2366v1", "guidislink": true, "updated": "2010-11-10T12:56:08Z", "updated_parsed": [2010, 11, 10, 12, 56, 8, 2, 314, 0], "published": "2010-11-10T12:56:08Z", "published_parsed": [2010, 11, 10, 12, 56, 8, 2, 314, 0], "title": "Nonparametric estimation of genewise variance for microarray data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.5804%2C1011.2790%2C1011.5912%2C1011.3795%2C1011.4803%2C1011.6046%2C1011.0733%2C1011.6083%2C1011.2262%2C1011.5757%2C1011.5653%2C1011.1429%2C1011.0414%2C1011.4398%2C1011.2384%2C1011.0766%2C1011.4545%2C1011.1882%2C1011.5798%2C1011.6360%2C1011.1547%2C1011.4337%2C1011.6483%2C1011.4496%2C1011.5934%2C1011.0827%2C1011.4548%2C1011.0391%2C1011.4177%2C1011.6670%2C1011.1570%2C1011.0741%2C1011.2507%2C1011.2949%2C1011.0826%2C1011.5108%2C1011.4200%2C1011.5055%2C1011.1097%2C1011.2540%2C1011.6423%2C1011.6074%2C1011.1661%2C1011.2064%2C1011.4051%2C1011.0566%2C1011.0517%2C1011.2616%2C1011.1153%2C1011.6531%2C1011.3913%2C1011.5963%2C1011.3739%2C1011.0145%2C1011.1504%2C1011.1553%2C1011.2095%2C1011.5799%2C1011.6389%2C1011.2769%2C1011.5694%2C1011.0449%2C1011.2099%2C1011.3456%2C1011.0014%2C1011.6115%2C1011.3671%2C1011.3807%2C1011.2970%2C1011.6143%2C1011.3799%2C1011.1162%2C1011.3920%2C1011.5420%2C1011.0142%2C1011.2643%2C1011.6516%2C1011.0857%2C1011.1124%2C1011.0836%2C1011.6289%2C1011.1595%2C1011.2220%2C1011.5246%2C1011.2366%2C1011.3233%2C1011.0202%2C1011.2549%2C1011.4710%2C1011.1619%2C1011.0269%2C1011.1954%2C1011.5898%2C1011.2785%2C1011.6185%2C1011.2724%2C1011.3908%2C1011.4633%2C1011.1110%2C1011.2294%2C1011.4879&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Nonparametric estimation of genewise variance for microarray data"}, "summary": "Estimation of genewise variance arises from two important applications in\nmicroarray data analysis: selecting significantly differentially expressed\ngenes and validation tests for normalization of microarray data. We approach\nthe problem by introducing a two-way nonparametric model, which is an extension\nof the famous Neyman--Scott model and is applicable beyond microarray data. The\nproblem itself poses interesting challenges because the number of nuisance\nparameters is proportional to the sample size and it is not obvious how the\nvariance function can be estimated when measurements are correlated. In such a\nhigh-dimensional nonparametric problem, we proposed two novel nonparametric\nestimators for genewise variance function and semiparametric estimators for\nmeasurement correlation, via solving a system of nonlinear equations. Their\nasymptotic normality is established. The finite sample property is demonstrated\nby simulation studies. The estimators also improve the power of the tests for\ndetecting statistically differentially expressed genes. The methodology is\nillustrated by the data from microarray quality control (MAQC) project.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.5804%2C1011.2790%2C1011.5912%2C1011.3795%2C1011.4803%2C1011.6046%2C1011.0733%2C1011.6083%2C1011.2262%2C1011.5757%2C1011.5653%2C1011.1429%2C1011.0414%2C1011.4398%2C1011.2384%2C1011.0766%2C1011.4545%2C1011.1882%2C1011.5798%2C1011.6360%2C1011.1547%2C1011.4337%2C1011.6483%2C1011.4496%2C1011.5934%2C1011.0827%2C1011.4548%2C1011.0391%2C1011.4177%2C1011.6670%2C1011.1570%2C1011.0741%2C1011.2507%2C1011.2949%2C1011.0826%2C1011.5108%2C1011.4200%2C1011.5055%2C1011.1097%2C1011.2540%2C1011.6423%2C1011.6074%2C1011.1661%2C1011.2064%2C1011.4051%2C1011.0566%2C1011.0517%2C1011.2616%2C1011.1153%2C1011.6531%2C1011.3913%2C1011.5963%2C1011.3739%2C1011.0145%2C1011.1504%2C1011.1553%2C1011.2095%2C1011.5799%2C1011.6389%2C1011.2769%2C1011.5694%2C1011.0449%2C1011.2099%2C1011.3456%2C1011.0014%2C1011.6115%2C1011.3671%2C1011.3807%2C1011.2970%2C1011.6143%2C1011.3799%2C1011.1162%2C1011.3920%2C1011.5420%2C1011.0142%2C1011.2643%2C1011.6516%2C1011.0857%2C1011.1124%2C1011.0836%2C1011.6289%2C1011.1595%2C1011.2220%2C1011.5246%2C1011.2366%2C1011.3233%2C1011.0202%2C1011.2549%2C1011.4710%2C1011.1619%2C1011.0269%2C1011.1954%2C1011.5898%2C1011.2785%2C1011.6185%2C1011.2724%2C1011.3908%2C1011.4633%2C1011.1110%2C1011.2294%2C1011.4879&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Estimation of genewise variance arises from two important applications in\nmicroarray data analysis: selecting significantly differentially expressed\ngenes and validation tests for normalization of microarray data. We approach\nthe problem by introducing a two-way nonparametric model, which is an extension\nof the famous Neyman--Scott model and is applicable beyond microarray data. The\nproblem itself poses interesting challenges because the number of nuisance\nparameters is proportional to the sample size and it is not obvious how the\nvariance function can be estimated when measurements are correlated. In such a\nhigh-dimensional nonparametric problem, we proposed two novel nonparametric\nestimators for genewise variance function and semiparametric estimators for\nmeasurement correlation, via solving a system of nonlinear equations. Their\nasymptotic normality is established. The finite sample property is demonstrated\nby simulation studies. The estimators also improve the power of the tests for\ndetecting statistically differentially expressed genes. The methodology is\nillustrated by the data from microarray quality control (MAQC) project."}, "authors": ["Jianqing Fan", "Yang Feng", "Yue S. Niu"], "author_detail": {"name": "Yue S. Niu"}, "author": "Yue S. Niu", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/10-AOS802", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1011.2366v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.2366v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/10-AOS802 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.2366v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.2366v1", "journal_reference": "Annals of Statistics 2010, Vol. 38, No. 5, 2723-2750", "doi": "10.1214/10-AOS802", "fulltext": "arXiv:1011.2366v1 [math.ST] 10 Nov 2010\n\nThe Annals of Statistics\n2010, Vol. 38, No. 5, 2723\u20132750\nDOI: 10.1214/10-AOS802\nc Institute of Mathematical Statistics, 2010\n\nNONPARAMETRIC ESTIMATION OF GENEWISE VARIANCE\nFOR MICROARRAY DATA1\nBy Jianqing Fan, Yang Feng and Yue S. Niu\nPrinceton University, Columbia University and University of Arizona\nEstimation of genewise variance arises from two important applications in microarray data analysis: selecting significantly differentially expressed genes and validation tests for normalization of microarray data. We approach the problem by introducing a two-way\nnonparametric model, which is an extension of the famous Neyman\u2013\nScott model and is applicable beyond microarray data. The problem\nitself poses interesting challenges because the number of nuisance parameters is proportional to the sample size and it is not obvious how\nthe variance function can be estimated when measurements are correlated. In such a high-dimensional nonparametric problem, we proposed two novel nonparametric estimators for genewise variance function and semiparametric estimators for measurement correlation, via\nsolving a system of nonlinear equations. Their asymptotic normality\nis established. The finite sample property is demonstrated by simulation studies. The estimators also improve the power of the tests for detecting statistically differentially expressed genes. The methodology\nis illustrated by the data from microarray quality control (MAQC)\nproject.\n\n1. Introduction. Microarray experiments are one of widely used technologies nowadays, allowing scientists to monitor thousands of gene expressions simultaneously. One of the important scientific endeavors of microarray\ndata analysis is to detect statistically differentially expressed genes for downstream analysis [Cui, Hwang and Qiu (2005), Fan et al. (2004), Fan and Ren\n(2006), Storey and Tibshirani (2003), Tusher, Tibshirani and Chu (2001)].\nStandard t-test and F -test are frequently employed. However, due to the\ncost of the experiment, it is common to see a large number of genes with\nReceived September 2009; revised January 2010.\nSupported by NSF Grant DMS-07-14554 and NIH Grant R01-GM072611.\nAMS 2000 subject classifications. Primary 62G05; secondary 62P10.\nKey words and phrases. Genewise variance estimation, gene selection, local linear regression, nonparametric model, correlation correction, validation test.\n1\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2010, Vol. 38, No. 5, 2723\u20132750. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\na small number of replications. Even in customized arrays where only several hundreds of genes expressions are measured, the number of replications\nis usually limited. As a result, we are facing a high-dimensional statistical\nproblem with a large number of parameters and a small sample size.\nGenewise variance estimation arises at the heart of microarray data analysis. To select differentially expressed genes among thousands of genes, the\nt-test is frequently employed with a stringent control of type I errors. The\ndegree of freedom is usually small due to limited replications. The power\nof the test can be significantly improved if the genewise variance can be\nestimated accurately. In such a case, the t-test becomes basically a z-test.\nA simple genewise variance estimator is the sample variance of replicated\ndata, which is not reliable due to a relatively small number of replicated\ngenes. They have direct impact on the sensitivity and specificity of t-test\n[Cui, Hwang and Qiu (2005)]. Therefore, novel methods for estimating the\ngenewise variances are needed for improving the power of the standard t-test.\nAnother important application of genewise variance estimation arises from\ntesting whether systematic biases have been properly removed after applying\nsome normalization method, or selecting the most appropriate normalization\ntechnique for a given array. Fan and Niu (2007) developed such validation\ntests (see Section 4), which require the estimation of genewise variance. The\nmethods of variance estimation, like pooled variance estimator, and REML\nestimator [Smyth, Michaud and Scott (2005)], are not accurate enough due\nto the small number of replications.\nDue to the importance of genewise variance in microarray data analysis, conscientious efforts have been made to accurately estimate it. Various\nmethods have been proposed under different models and assumptions. It has\nbeen widely observed that genewise variance is to a great extent related to\nthe intensity level. Kamb and Ramaswami (2001) proposed a crude regression estimation of variance from microarray control data. Tong and Wang\n(2007) discussed a family of shrinkage estimators to improve the accuracy.\nLet Rgi and Ggi , respectively, be the intensities of red (Cy3) and green\n(Cy5) channels for the ith replication of the gth gene on a two-color microarray data. The log-ratios and log-intensities are computed, respectively,\nas\nYgi = log2 (Ggi /Rgi ) and\n\nXgi = 21 log2 (Ggi Rgi ),\ni = 1, . . . , I, g = 1, . . . , N,\n\nwhere I is the number of replications for each gene and N is the number\nof genes with replications. For the purpose of estimating genewise variance,\nwe assume that there is no systematic biases or the systematic biases have\nbeen removed by a certain normalization method. This assumption is always\n\n\fGENEWISE VARIANCE ESTIMATION\n\n3\n\nmade for selecting significantly differentially expressed genes or validation\ntest under the null hypothesis. Thus, we have\nYgi = \u03b1g + \u03c3gi \u01ebgi\nwith \u03b1g denoting the log-ratio of gene expressions in the treatment and control samples. Here, (\u01ebg1 , . . . , \u01ebgI )T follows a multivariate normal distribution\nwith \u01ebgi \u223c N (0, 1) and Corr(\u01ebgi , \u01ebgj ) = \u03c1 when i 6= j. It is also assumed that\nobservations from different genes are independent. Such a model was used\nin Smyth, Michaud and Scott (2005).\nIn the papers by Wang, Ma and Carroll (2009) and Carroll and Wang\n(2008), nonparametric measurement-error models have been introduced to\naggregate the information of estimating the genewise variance:\n(1)\n\nYgi = \u03b1g + \u03c3(\u03b1g )\u03b5gi ,\ncorr(\u03b5gi , \u03b5gi \u2032 ) = 0,\n\ng = 1, . . . , N, i = 1, . . . , I.\n\nThe model is intended for the analysis of the Affymetrix array (one-color\narray) data in which \u03b1g represents the expected intensity level, and Ygi is\nthe ith replicate of observed expression level of gene g. When it is applied to\nthe two-color microarray data as in our setting, in which \u03b1g is the relative\nexpression profiles between the treatment and control, several drawbacks\nemerge: (a) the model is difficult to interpret as the genewide variance is a\nfunction of the log-ratio of expression profiles; (b) errors-in-variable methods\nhave a very slow rate of convergence for the nonparametric problem and the\nobserved intensity information Xgi is not used; (c) they are usually hard\nto be implemented robustly and depend sensitively on the distribution of\n\u03c3(\u03b1g )\u03b5gi and the i.i.d. assumption on the noise; (d) in many microarray\napplications, \u03b1g = 0 for most g and hence \u03c3(\u03b1g ) are the same for most\ngenes, which is unrealistic. Therefore, our model (2) below is complementary\nto that of Wang, Ma and Carroll (2009) and Carroll and Wang (2008), with\nfocus on the applications to two-color microarray data.\nTo overcome these drawbacks in the applications to microarray data and\nto utilize the observed intensity information, we assume that \u03c3gi = \u03c3(Xgi ) for\na smooth function \u03c3(*). This leads to the following two-way nonparametric\nmodel:\n(2)\n\nYgi = \u03b1g + \u03c3(Xgi )\u01ebgi ,\n\ng = 1, . . . , N, i = 1, . . . , I,\n\nfor estimating genewise variance. This model is clearly an extension of the\nNeyman\u2013Scott problem [Neyman and Scott (1948)], in which the genewise\nvariance is a constant. The Neyman\u2013Scott problem has many applications\nin astronomy. Note that the number of nuisance parameters {\u03b1g } is proportional to the sample size. This imposes an important challenge to the\n\n\f4\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nnonparametric problem. It is not even clear whether the function \u03c3(*) can\nbe consistently estimated.\nTo estimate the genewise variance in their microarray data analysis, Fan et\nal. (2004) assumed a model similar to (2). But in the absence of other available techniques, they had to impose that the treatment effect {\u03b1g } is also a\nsmooth function of the intensity level so that they can apply nonparametric\nmethods to estimate genewise variance [Ruppert et al. (1997)]. However, this\nassumption is not valid in most microarray applications, and the estimator\nof genewise variance incurs big biases unless {\u03b1g } is sparse, a situation that\nFan et al. (2004) hoped. Fan and Niu (2007) approached this problem in\nanother simple way. When the noise in the replications is small, that is,\nXgi \u2248 X\u0304g , where X\u0304g is the sample mean for the gth gene. Therefore, they\nP\nsimply smoothed the pair {(X\u0304g , r\u0304g )}, where r\u0304g = Ii=1 (Ygi \u2212 \u0232g )2 /(I \u2212 1).\nThis also leads to a biased estimator, which is denoted as \u03be\u0302 2 (x). One asks\nnaturally whether the function \u03c3(*) is estimable and how it can be estimated\nin the general two-way nonparametric model.\nWe propose a novel nonparametric approach to estimate the genewise\nvariance. We first study a benchmark case when there is no correlation between replications, that is, \u03c1 = 0. This corresponds to the case with independent replications across arrays [Fan, Peng and Huang (2005), Huang, Wang\nand Zhang (2005)]. It is also applicable to those dealt by the Neyman\u2013\nScott problem. By noticing E{(Ygi \u2212 \u0232g )2 |Xgi } is a linear combination of\n\u03c3 2 (Xgi ), we obtain a system of linear equations. Hence, \u03c3 2 (*) can be estimated via nonparametric regression of a proper linear combination of\n{(Ygi \u2212 \u0232g )2 , i = 1, . . . , I} on {Xgi }. The asymptotic normality of the estimator is established. In the case that the replication correlation does not\nvanish, the system of equations becomes nonlinear and cannot be analytically solved. However, we are able to derive the correlation corrected estimator, based on the estimator without genewise correlation. The genewise\nvariance function and the correlation coefficient of repeated measurements\nare simultaneously estimated by iteratively solving a nonlinear equation.\nThe asymptotic normality of such estimators is established.\nModel (2) can be applied to the microarrays in which within-array replications are not available. In that case, we can aggregate all the microarrays\ntogether and view them as a super array with replications [Fan, Peng and\nHuang (2005), Huang, Wang and Zhang (2005)]. In other words, i in (2)\nindexes arrays and \u03c1 can be taken as 0, namely (2) is the across-array replication with \u03c1 = 0.\nThe structure of this paper is as follows. In Section 2, we discuss the\nestimation schemes of the genewise variance and establish the asymptotic\nproperties of the estimators. Simulation studies are given in Section 3 to\nverify the performance of our methods in the finite sample. Applications to\n\n\fGENEWISE VARIANCE ESTIMATION\n\n5\n\nthe data from Microarray Quality Control (MAQC) project are showed in\nSection 4 to illustrate the proposed methodology. In Section 5, we give a\nshort summary. Technical proofs are relegated to the Appendix.\n2. Nonparametric estimators of genewise variance.\n2.1. Estimation without correlation. We first consider the specific case\nwhere there is no correlation among the replications Yg1 , . . . , YgI of the same\ngene g under model (2). This is usually applicable to the across-array replication and stimulates our procedure for the more general case with the\nreplication correlation. In the former case, we have\nX\nE[(Ygi \u2212 Y \u0304g )2 |X] = (I \u2212 1)2 \u03c3 2 (Xgi )/I 2 +\n\u03c3 2 (Xgj )/I 2 ,\ni = 1, . . . , I.\nj6=i\n\nWe will discuss in Section 2.2.4 the case that I = 2. For I > 2, we have\nI different equations with I unknowns \u03c3 2 (Xg1 ), \u03c3 2 (Xg2 ), . . . , \u03c3 2 (XgI ) for a\ngiven gene g. Solving these I equations, we can express the unknowns in\nterms of {E[(Ygi \u2212 \u0232g )2 |X]}Ii=1 , estimable quantities. Let\nrg = ((Yg1 \u2212 \u0232g )2 , . . . , (YgI \u2212 \u0232g )2 )T\n\nand \u03c3 2g = (\u03c3 2 (Xg1 ), . . . , \u03c3 2 (XgI ))T .\n\nThen, it can easily be shown that \u03c32g = BE[rg |X], where B is the coefficient\nmatrix:\nB = ((I 2 \u2212 I)I \u2212 E)/(I \u2212 1)(I \u2212 2)\nwith I being the I \u00d7 I identity matrix and E the I \u00d7 I matrix with all\nelements 1. Define\n\u25b3\n\nZg = (Zg1 , . . . , ZgI )T = Brg .\nThen we have\n(3)\n\n\u03c3 2 (Xgi ) = E[Zgi |X].\n\nNote that the left-hand side of (3) depends only on Xgi , not other variables.\nBy the the double expectation formula, it follows that the variance function\n\u03c3 2 (*) can be expressed as the univariate regression\n(4)\n\n\u03c3 2 (x) = E[Zgi |Xgi = x],\n\ni = 1, . . . , I.\n\nUsing the synthetic data {(Xgi , Zgi ), g = 1, . . . , N } for each given i, we can\napply the local linear regression technique [Fan and Gijbels (1996)] to obtain\na nonparametric estimator \u03b7\u0302i2 (x) of \u03c3 2 (*). Explicitly, for a given kernel K\nand bandwidth h,\n\u0012\n\u0013\nN\nX\nXgi \u2212 x\n2\nWN,i\n\u03b7\u0302i (x) =\n(5)\nZgi ,\ni = 1, . . . , I,\nh\ng=1\n\n\f6\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nwith\nSN,2 \u2212 uSN,1\n2 ,\nSN,2 SN,0 \u2212 SN,1\nP\nl\nwhere Kh (u) = h\u22121 K(u/h) and SN,l = N\ng=1 Kh (Xgi \u2212x)[(Xgi \u2212x)/h] , whose\n2\ndependence on i is suppressed. Thus, we have I estimators \u03b7\u03021 (x), . . . , \u03b7\u0302I2 (x)\nfor the same genewise variance function \u03c3 2 (*). Each of these I estimators\n\u03b7\u0302i2 (x) is a consistent estimator of \u03c3 2 (x). To optimally aggregate those I\nestimators, we need the asymptotic properties of \u03b7(x) = (\u03b7\u030212 (x), . . . , \u03b7\u0302I2 (x))T .\nDenote\nZ \u221e\nZ \u221e\n2\nK 2 (u) du,\nu K(u) du,\ndK =\ncK =\nWN,i (u) = h\u22121 K(u)\n\n\u2212\u221e\n\n\u2212\u221e\n\n\u03c31 = E[\u03c3(Xgi )]\n\nand \u03c32 = E[\u03c3 2 (Xgi )].\n\nAssume that Xgi are i.i.d. with marginal density fX (*) and \u03b5gi are i.i.d.\nrandom variables from the standard normal distribution. In the following\nresult, we assume that I is fixed, but N diverges.\nTheorem 1. Under the regularity conditions in the Appendix, for a fixed\npoint x, we have\nD\n\n\u03a3\u22121/2 (\u03b7 \u2212 (\u03c3 2 (x) + b(x) + oP (h2 ))e) \u2212\u2192 N (0, I),\n\nprovided that h \u2192 0 and N h \u2192 \u221e, where e = (1, 1, . . . , 1)T and\nwith b(x) =\n\nh2\n2\n\u2032\u2032\n2 cK (\u03c3 (x)) ,\n\n\u03a3 = V1 I + V2 (E \u2212 I)\n\n\u001a\n\u001b\n2\n4 + 4(I \u2212 1)(I \u2212 3)\ndK\n2\n2\n\u03c3\n\u03c3\n(x)\n+\n\u03c3\n2\u03c3 4 (x) +\n,\n2\nN hfX (x)\n(I \u2212 1)(I \u2212 2)2\n(I \u2212 1)(I \u2212 2) 2\n\u001a\n\u001b\n1\n8\n2(I \u2212 3)\n4\n2\n4\n2\nV2 =\n\u03c3 .\n\u03c3 (x) \u2212\n\u03c32 \u03c3 (x) +\nN (I \u2212 1)2\n(I \u2212 1)2\n(I \u2212 1)2 (I \u2212 2) 2\nV1 =\n\nNote that V2 is one order of magnitude smaller than V1 . Hence, the estimators \u03b7\u030212 (x), . . . , \u03b7\u0302I2 (x) are asymptotically independently distributed as\nN (\u03c3 2 (x) + b(x), V1 ). Their dependence is only in the second order. The best\nlinear combination of I estimators is\n(6)\n\n\u03b7\u0302 2 (x) = [\u03b7\u030212 (x) + \u03b7\u030222 (x) + * * * + \u03b7\u0302I2 (x)]/I\n\nwith the asymptotic distribution\n(7)\n\nN (\u03c3 2 (x) + b(x), V1 /I + (1 \u2212 1/I)V2 ).\n\n\fGENEWISE VARIANCE ESTIMATION\n\n7\n\nSee also the aggregated estimator (16) with \u03c1 = 0, which has the same\nasymptotic property as the estimator (8). See Remark 1 below for additional discussion.\nTheorem 1 gives the asymptotic normality of the proposed nonparametric\nestimators under the presence of a large number of nuisance parameters\n{\u03b1g }N\ng=1 . With the newly proposed technique, we do not have to impose any\nassumptions on \u03b1g such as sparsity or smoothness. This kind of local linear\nestimator can be applied to most two-color microarray data, for instance,\ncustomized arrays and Agilent arrays.\n2.2. Variance estimation with correlated replications.\n2.2.1. Aggregated estimator. We now consider the case with correlated\nwith-array replications. There is a lot of evidence that correlation among\nwithin-array replicated genes exists [Smyth, Michaud and Scott (2005), Fan\nand Niu (2007)]. Suppose that within-array replications have a common\ncorrelation corr(Ygi , Ygj |X) = \u03c1 when i 6= j. Observations across different\ngenes or arrays are independent. Then the conditional variance of (Ygi \u2212 \u0232g )\ncan be expressed as\nvar[(Ygi \u2212 Y \u0304g )|X]\n(8)\n\n= (I \u2212 1)2 \u03c3 2 (Xgi )/I 2 + 2\u03c1\n+ 2(I \u2212 1)\u03c1\n\nX\nj6=i\n\nX\n\n\u03c3(Xgj )\u03c3(Xgk )/I 2\n\n1\u2264j<k\u2264I,\nj6=i,k6=i\n\n\u03c3 2 (Xgj )/I 2 \u2212\n\nX\n\n\u03c3(Xgi )\u03c3(Xgj )/I 2 .\n\nj6=i\n\nThis is a complex system of nonlinear equations and the analytic form cannot\nbe found. Innovative ideas are needed.\nUsing the same notation as that in the previous section, it can be calculated that\n2 X\n\u03c1\u03c3(Xgi )\u03c3(Xgj )\nE[Zgi |X] = \u03c3 2 (Xgi ) \u2212\nI \u22121\nj6=i\n\n+\n\n2\n(I \u2212 1)(I \u2212 2)\n\nX\n\n\u03c1\u03c3(Xgj )\u03c3(Xgk ).\n\n1\u2264j<k\u2264I,\nj6=i,k6=i\n\nTaking the expectation with respect to Xgj for all j 6= i, we obtain\n(9)\n\n\u25b3\n\nE[Zgi |Xgi = x] = \u03c3 2 (x) \u2212 2\u03c1\u03c31 \u03c3(x) + \u03c1\u03c312 = \u03b7 2 (x),\n\nwhere \u03c31 = E[\u03c3(X)].\n\n\f8\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nHere, we can directly apply the local linear approach to all aggregated\n2\ndata {(Xgi , Zgi )}I,N\ni,g=1 , due to the same regression function (9). Let \u03b7\u0302A (*) be\nthe local linear estimator of \u03b7 2 (*), based on the aggregated data. Then\n\u0013\n\u0012\nN X\nI\nX\nXgi \u2212 x\n2\n(10)\nZgi\n\u03b7\u0302A (x) =\nWN\nh\ng=1 i=1\n\nwith\nWN (u) = h\u22121 K(u)\nwhere SNI ,l =\nto (9):\n(11)\n\nPN PI\ng=1\n\ni=1 Kh (Xgi\n\nSNI ,2 \u2212 uSNI ,1\n,\n2\nSNI ,0 SNI ,2 \u2212 SNI\n,1\n\n\u2212 x)[(Xgi \u2212 x)/h]l . There are two solutions\n\n\u03c3\u0302A (x, \u03c1)(1),(2) = \u03c1\u0302\u03c3\u03021 \u00b1\n\nq\n\n2 (x),\n\u03c1\u03022 \u03c3\u030212 \u2212 \u03c1\u0302\u03c3\u030212 + \u03b7\u0302A\n\nNotice that given the sample X and Y, \u03c3\u0302A (x, \u03c1)(1),(2) are continuous in both\nx and \u03c1. For \u03c1 < 0, \u03c3\u0302A (x, \u03c1)(1) should be used since the standard deviation\nshould be nonnegative. Since \u03c3\u0302A (x, \u03c1)(1) > \u03c3\u0302A (x, \u03c1)(2) for every x and \u03c1, by\nthe continuity of the solution in \u03c1, we can only use the same solution when\n\u03c1 changes continuously. Then \u03c3\u0302A (x, \u03c1)(1) should always be used regardless\nof \u03c1. From now on, we drop the superscript and denote\nq\n2 (x).\n(12)\n\u03c3\u0302A (x) = \u03c1\u03c31 + \u03c12 \u03c312 \u2212 \u03c1\u03c312 + \u03b7\u0302A\nThis is called the aggregated estimator. Note that in (12), \u03c1, \u03c31 and \u03c3(*) are\nall unknown.\n\n2.2.2. Estimation of correlation. To estimate \u03c1, we assume that there\nare J independent arrays (J \u2265 2). In other words, we observed data from\n(2) independently J times. In this case, the residual maximum likelihood\n(REML) estimator introduced by Smyth, Michaud and Scott (2005) is as\nfollows:\nPN 2\nPN 2\ng=1 sB,g \u2212\ng=1 sW,g\n\u03c1\u03020 = PN\n(13)\nPN 2 ,\n2\ng=1 sB,g + (I \u2212 1)\ng=1 sW,g\nP\nP\nwhere s2B,g = I(J \u2212 1)\u22121 Jj=1 (\u0232gj \u2212 \u0232g )2 with \u0232gj = I \u22121 Ii=1 Ygij and \u0232g =\nP\nJ \u22121 Jj=1 \u0232gj is the between-arrays variance and s2W,g is the within-array\nvariance:\nJ\n\ns2W,g =\n\nI\n\nXX\n1\n(Ygij \u2212 \u0232gj )2 .\nJ(I \u2212 1)\nj=1 i=1\n\n\f9\n\nGENEWISE VARIANCE ESTIMATION\n\nAs discussed in Smyth, Michaud and Scott (2005), the estimator \u03c1\u03020 of \u03c1\nis consistent when var(Ygij |X) = \u03c3g is the same for all i = 1, . . . , I and j =\n1, . . . , J . However, this assumption is not valid under the model (2) and a\ncorrection is needed. We propose the following estimator:\nPN 2\nPN 2\n\u03c32\ng=1 sB,g \u2212\ng=1 sW,g\n\u03c1\u0302 = 2 * PN\n(14)\nPN 2 .\n2\n\u03c31\ng=1 sB,g + (I \u2212 1)\ng=1 sW,g\nThe consistency of \u03c1\u0302 is given by the following theorem.\n\nTheorem 2. \u221a Under the regularity condition in the Appendix, the estimator \u03c1\u0302 of \u03c1 is N -consistent:\n\u03c1\u0302 \u2212 \u03c1 = OP (N \u22121/2 ).\nWith a consistent estimator of \u03c1, \u03c31 , \u03c32 and \u03c3A (*) can be solved by the\nfollowing iterative algorithm:\n2 (*) as an initial estimate of \u03c3 2 (*).\nStep 1. Set \u03b7\u0302A\nA\nStep 2. With \u03c3\u0302A (*), compute\n\n(15)\n\n\u03c3\u03021 = N \u22121\n\nN\nX\ng=1\n\n\u03c3\u0302A (Xgi ),\n\n\u03c3\u03022 = N \u22121\n\nN\nX\n\n2\n\u03c3\u0302A\n(Xgi ),\n\n\u03c1\u0302 = \u03c1\u03020 \u03c3\u03022 /\u03c3\u030212 .\n\ng=1\n\nStep 3. With \u03c3\u03021 , \u03c3\u03022 and \u03c1\u0302, compute \u03c3\u0302A (*) using (12).\nStep 4. Repeat steps 2 and 3 until convergence.\nThis provides simultaneously the estimators \u03c3\u03021 , \u03c3\u03022 , \u03c1\u0302 and \u03c3\u0302A (*). From our\nnumerical experience, this algorithm converges quickly after a few iterations.\n2 (x) is given by\nWhen the algorithm converges, the estimator \u03c3A\nq\n2 (x).\n\u03c3\u0302A (x) = \u03c1\u0302\u03c3\u03021 + \u03c1\u03022 \u03c3\u030212 \u2212 \u03c1\u0302\u03c3\u030212 + \u03b7\u0302A\n(16)\nNote that the presence of multiple arrays is only used to estimate the\ncorrelation \u03c1 for the replications. It is not needed for estimating the genewise\nvariance function. In the case of the presence of J arrays, we can take the\naverage of the J estimates from each array.\n2.2.3. Asymptotic properties. Following a similar idea as the case with2 (x).\nout correlation, we can derive the asymptotic property of \u03b7\u0302A\nTheorem 3. Under the regularity conditions in the Appendix, for a fixed\npoint x, we have\nD\n\n2\n{V \u2217 }\u22121/2 {\u03b7\u0302A\n(x) \u2212 [\u03b7 2 (x) + \u03b2(x)] + oP (h2 )} \u2212\u2192 N (0, 1),\n\n\f10\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nprovided that h \u2192 0 and N h \u2192 \u221e, with \u03b2(x) =\n\nh2\n2\n\u2032\u2032\n2 cK (\u03b7 (x))\n\nand\n\nI \u22121 \u2032\n1\nV2 ,\nV \u2217 = V1\u2032 +\nI\nI\nwhere\nV1\u2032 =\n\ndK\n{2\u03c3 4 (x) \u2212 8\u03c1\u03c31 \u03c3 3 (x) + C2 \u03c3 2 (x) + C3 \u03c3(x) + C4 },\nN hfX (x)\n\nV2\u2032 =\n\n1\n{D0 \u03c3 4 (x) + D1 \u03c3 3 (x) + D2 \u03c3 2 (x) + D3 \u03c3(x) + D4 }\nN\n\nwith coefficients C2 , . . . , C4 , D0 , . . . , D4 defined in the Appendix.\n2 (x) can be derived from that of \u03b7\u0302 2 (x).\nThe asymptotic normality of \u03c3\u0302A\nA\np\n2\n2\n2 \u03c3 2 \u2212 \u03c1\u03c3 2 + z)2 .\n\u03c1\nMore specifically, \u03c3\u0302A (x) = \u03c6(\u03b7A (x)) with \u03c6(z) = (\u03c1\u03c31 +\n1\n1\np\nThe derivative of \u03c6(*) with respect to z is \u03c8(z) = \u03c1\u03c31 / \u03c12 \u03c312 \u2212 \u03c1\u03c312 + z + 1.\nThen, by the delta method, we have\nD\n\n2\n{V \u2217 }\u22121/2 (\u03c3\u0302A\n(x) \u2212 \u03c6(\u03b7 2 (x) + \u03b2(x) + oP (h2 ))) \u2212\u2192 N (0, \u03c8 2 (\u03b7 2 (x))).\n\nRemark 1. An alternative approach when correlation exists is to apply\nthe same correlation correction idea to {Xgi , Zgi }N\ng=1 for every replication i,\nresulting in the estimator \u03c3\u0302i2 (x). In this case, it can be proved that the best\nlinear combination of the estimator is\n(17)\n\n\u03c3\u0302 2 (x) = [\u03c3\u030212 (x) + \u03c3\u030222 (x) + * * * + \u03c3\u0302I2 (x)]/I.\n\nThis estimator has the same asymptotic performance as the aggregated estimator. However, we prefer the aggregated estimator due to the following\nreasons: the equation (16) only needs to be solved once by using the algo2 (*) can be\nrithm in Section 2.2.2, all data are treated symmetrically, and \u03b7\u0302A\nestimated more stably.\n2.2.4. Two replications. The aforementioned methods apply to the case\nwhen there are more than two replications. For the case I = 2, the equations\nfor var[(Ygi \u2212 \u0232g )|X] collapse into one. In this case, it can be shown using\nthe same arguments before that\n(18)\n\nvar[(Ygi \u2212 \u0232g )|Xgi = x] = 41 \u03c3 2 (x) + 14 \u03c32 \u2212 21 \u03c1\u03c31 \u03c3(x),\n\ni = 1, 2,\n\nwhere \u03c32 = E[\u03c3 2 (Xgi )]. In this case, the left-hand side is always equal to\nvar[(Yg1 \u2212 Yg2 )/2|Xgi = x].\n\n\f11\n\nGENEWISE VARIANCE ESTIMATION\n\nLet \u03b7\u0302 2 (x) be the local linear estimator of the function on the right-hand\nN\nN\nside by smoothing {(Yg1 \u2212 Yg2 )2 /4}N\ng=1 on {Xg1 }g=1 and {Xg2 }g=1 . Then\nthe genewise variance is a solution to the following equation:\nq\n(19)\n\u03c3\u0302(x) = \u03c1\u0302\u03c3\u03021 + \u03c1\u03022 \u03c3\u030212 \u2212 \u03c3\u03022 + 4\u03b7\u0302 2 (x).\nThe algorithm in Section 2.2.2 can be applied directly.\n\n3. Simulations and comparisons. In this section, we conduct simulations\nto evaluate the finite sample performance of different variance estimators\n2 (x). First, the bias problem of the naive nonparametric\n\u03be\u0302 2 (x), \u03b7\u0302 2 (x) and \u03c3\u0302A\nvariance estimator \u03be\u0302 2 (x) is demonstrated. It is shown that this bias issue\ncan be eliminated by our newly proposed methods. Then we consider the\n2 (x) under different configurations of the within-array\nestimators \u03b7\u0302 2 (x) and \u03c3\u0302A\nreplication correlation.\n3.1. Simulation design. In all the simulation examples, we set the number of genes N = 2000, each gene having I = 3 within-array replications\nand J = 4 independent arrays. For the purpose of investigating the genewise\nvariance estimation, the data are generated from model (2). The details of\nsimulation scheme are summarized as follows:\n\u03b1g : The expression levels of the first 250 genes are generated from the standard double exponential distribution. The rest are 0s. These expression\nlevels are the same over 4 arrays in each simulation, but may vary over\nsimulations.\nX: The intensity is generated from a mixture distribution: with probability\n0.7 from the distribution 0.0004(x \u2212 6)3 I(6 < x < 16) and 0.3 from the\nuniform distribution over [6, 16].\n\u03b5: \u03b5gi is generated from the standard normal distribution.\n\u03c3 2 (*): The genewise variance function is taken as\n\u03c3 2 (x) = 0.15 + 0.015(12 \u2212 x)2 I{x < 12}.\n\nThe parameters are taken from Fan, Peng and Huang (2005). The kernel\n3 3\nfunction is selected as 70\n81 (1 \u2212 |x| ) I(|x| \u2264 1). In addition, we fix the bandwidth h = 1 for all the numerical analysis.\nFor every setting, we repeat the whole simulation process for T times and\nevaluate the estimates of \u03c3 2 (*) over K = 101 grid points {xk }K\nk=1 on the\ninterval [6, 16]. For the kth grid point, we define\nBk = \u03c3\u0304 2 (xk ) \u2212 \u03c3 2 (xk )\nSk = T \u22121\n\nwith \u03c3\u0304 2 (xk ) = T \u22121\n\nT\nX\n[\u03c3\u0302t2 (xk ) \u2212 \u03c3\u0304 2 (xk )]2 ,\nt=1\n\nT\nX\nt=1\n\n\u03c3\u0302t2 (xk ),\n\n\f12\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nand MSEk = Bk2 + Sk . Let f (*) be the density function of intensity X. Let\nBias2 =\n\nK\nX\nk=1\n\nand\n\nK\n.X\nBk2 f (xk )\nf (xk ),\n\nVAR =\n\nk=1\n\nMISE =\n\nK\nX\nk=1\n\nK\nX\nk=1\n\nK\n.X\nSk f (xk )\nf (xk )\nk=1\n\nK\n.X\nMSEk f (xk )\nf (xk )\nk=1\n\n2\n\nbe the integrated squared bias (Bias ), the integrated variance (VAR), and\nthe integrated mean squared error (MISE) of the estimate \u03c3\u0302 2 (*), respectively.\nFor the tth simulation experiment, we define\nISEt =\n\nK\nK\n.X\nX\n(\u03c3\u0302t2 (xk ) \u2212 \u03c3 2 (xk ))2 f (xk )\nf (xk )\nk=1\n\nk=1\n\nbe the integrated squared error for the tth simulation.\n\n3.2. The bias of naive nonparametric estimator. A naive approach is to\nregard \u03b1g in (2) as a smooth function of Xgi , namely, \u03b1g = \u03b1(Xgi ). The\nfunction \u03b1(*) can be estimated by a local linear regression estimator, result2 }N\ning in an estimated function \u03b1\u0302(*). The squared residuals {rgi\ng=1 is then\n2\nN\n\u02c6\nfurther smoothed on {Xgi }g=1 to obtain an estimate \u03be (x) of the variance\nfunction \u03c3 2 (*), where rgi = \u0176gi \u2212 \u03b1\u0302(Xgi ) [Ruppert et al. (1997)].\nTo provide a comprehensive view of the performances of the naive and\nthe new estimators, we first compare the performances of \u03be\u02c62 (x) and \u03b7\u0302 2 (x)\nunder the smoothness assumption of the gene effect \u03b1g . Data from the naive\nnonparametric regression model is also generated with\n\u0012\n\u0013\n1\n\u03b1(x) = exp \u2212\nI{12 < x < 14}.\n1 \u2212 (x \u2212 13)2\nThis allows us to understand the loss of efficiency when \u03b1g is continuous\nin Xgi . This usually does not occur for microarray data, but can appear in\nother applications. Note that \u03b1(*) is zero in most of the region and thus is\nreasonably sparse. Here, the number of simulations is taken to be T = 100.\nThe data is generated with the assumption that \u03c1 = 0, in which case the\n2 (x) have the same performance (see also\nvariance estimators \u03b7\u0302 2 (x) and \u03c3\u0302A\nTable 2 below). Thus, we only report the performance of \u03b7\u0302 2 (x).\nIn Table 1, we report the mean integrated squared bias (Bias2 ), the mean\nintegrated variance (VAR), and the mean integrated squared error (MISE)\nof \u03be\u0302 2 (x) and \u03b7\u0302 2 (x) with and without the smoothness assumption on the\ngene effect \u03b1g . From the left panel of Table 1, we can see that when the\n\n\f13\n\nGENEWISE VARIANCE ESTIMATION\n\nTable 1\nMean integrated squared bias (Bias2 ), mean integrated variance (VAR), mean integrated\nsquared error (MISE) over 100 simulations for variance estimators \u03be\u02c62 (x) and \u03b7\u0302 2 (x). Two\ndifferent gene effect functions \u03b1(*) are implemented. All quantities are multiplied by 1000\nSmooth gene effect\nBias\n\u03be\u02c62 (x)\n\u03b7\u0302 2 (x)\n\n2\n\nNonsmooth gene effect\n2\n\nVAR\n\nMISE\n\nBias\n\n0.14\n0.24\n\n0.15\n0.80\n\n16.00\n0.00\n\n0.01\n0.57\n\nVAR\n\nMISE\n\n1.47\n0.22\n\n17.47\n0.23\n\nTable 2\nMean integrated squared bias (Bias2 ), mean integrated variance (VAR), mean integrated\nsquared error (MISE) over 1000 simulations for different variance estimators \u03b7\u0302 2 (x) and\n2\n\u03c3\u0302O\n(x). Seven different correlation schemes are simulated: \u03c1 = \u22120.4, \u03c1 = \u22120.2, \u03c1 = 0,\n\u03c1 = 0.2, \u03c1 = 0.4, \u03c1 = 0.6 and \u03c1 = 0.8. All quantities are multiplied by 1000\n\u03c1\n\nBias\n\n2\n\nVAR\n\nMISE\n\n2\n\n\u03b7\u0302 (x)\n2\n\u03c3\u0302A\n(x)\n2\n\u03c3\u0302O (x)\n\u03b7\u0302 2 (x)\n2\n\u03c3\u0302A\n(x)\n2\n\u03c3\u0302O (x)\n\u03b7\u0302 2 (x)\n2\n\u03c3\u0302A\n(x)\n2\n\u03c3\u0302O (x)\n\n\u22120.4\n\n\u22120.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n5.93\n0.00\n0.00\n0.44\n0.27\n0.27\n6.37\n0.27\n0.27\n\n1.48\n0.00\n0.00\n0.33\n0.25\n0.25\n1.81\n0.25\n0.25\n\n0.00\n0.00\n0.00\n0.24\n0.24\n0.24\n0.24\n0.24\n0.24\n\n1.48\n0.00\n0.00\n0.16\n0.22\n0.22\n1.64\n0.22\n0.22\n\n5.91\n0.00\n0.00\n0.10\n0.20\n0.20\n6.01\n0.21\n0.20\n\n13.31\n0.00\n0.00\n0.05\n0.19\n0.18\n13.37\n0.19\n0.18\n\n23.67\n0.00\n0.01\n0.02\n0.20\n0.23\n23.69\n0.20\n0.24\n\nsmoothness assumption is valid, the estimator \u03be\u02c62 (x) outperforms \u03b7\u0302 2 (x). The\nreason is that the mean function \u03b1(Xgi ) depends on the replication and is\nnot a constant. Therefore, model (2) fails and \u03b7\u0302 2 (x) is biased. One should\ncompare the results with those on the second row of the right panel where the\nmodel is right for \u03b7\u0302 2 (x). In this case, \u03b7\u0302 2 (x) performs much better. Its variance\nis about 3/2 as large as the variance in the case that mean is generated from\na smooth function \u03b1(Xgi ). This is expected. In the latter case, to eliminate\n\u03b1g , the degree of freedom reduces from I = 3 to 2, whereas in the former\ncase, \u03b1(Xgi ) can be estimated without losing the degree of freedom, namely\nthe number of replications is still 3. The ratio 3/2 is reflected in Table 1.\nHowever, when the smoothness assumption does not hold, there is serious\nbias in the estimator \u03be\u0302 2 (x), even though that \u03b1g is still reasonably sparse.\nThe bias is an order of magnitude larger than those in the other situations.\n\n\f14\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nTo see how variance estimators behave, we plot typical estimators \u03be\u0302 2 (x)\nand \u03b7\u0302 2 (x) with median ISE value among 100 simulations in Figure 1. The\nsolid line is the true variance function while the dotted and dashed lines represent \u03be\u02c62 (x) and \u03b7\u0302 2 (x), respectively. On the left panel of Figure 1, we can\nsee that estimator \u03be\u02c62 (x) outperforms the estimator \u03b7\u0302 2 (x) when the smoothness assumption is valid. The region where the biases occur has already\nbeen explained above. However, \u03be\u02c62 (x) will generate substantial bias when\nthe nonparametric regression model does not hold, and at the same time,\nour nonparametric estimator \u03b7\u0302 2 (x) corrects the bias very well.\n3.3. Performance of new estimators. In this example, we consider the\nsetting in Section 3.1 that the smoothness assumption of the gene effect \u03b1g\n2 (x)\nis not valid. For comparison purpose only, we add an oracle estimator \u03c3\u0302O\nin which we assume that \u03c31 , \u03c32 and \u03c1 are all known. We now evaluate the\n2 (x) and \u03c3\u0302 2 (x) when the correlation\nperformance of the estimators \u03b7\u0302 2 (x), \u03c3\u0302A\nO\nbetween within-array replications varies. To be more specific, seven different\ncorrelation settings are considered: \u03c1 = \u22120.4, \u22120.2, 0, 0.2, 0.4, 0.6, 0.8, with\n\u03c1 = 0 representing across-array replications. In this case, we increase the\nnumber of simulations to T = 1000. Again, we report Bias2 , VAR and MISE\nof the three estimators for each correlation setting in Table 2. When \u03c1 = 0,\nall the three estimators give the same bias and variance. This is consistent\nwith our theory. We can see clearly from the table that, when \u03c1 6= 0, the\n2 (x) produces much smaller biases than \u03b7\u0302 2 (x). In fact, when |\u03c1|\nestimator \u03c3\u0302A\nas small as 0.2, the bias of \u03b7\u0302 2 (x) already dominates the variance.\n2 (x) and \u03c3\u0302 2 (x) are almost\nIt is worth noticing that the performance of \u03c3\u0302O\nA\nalways the same, which indicates that our algorithm for estimating \u03c1, \u03c31\n\nFig. 1. Variance estimators \u03be\u02c62 (x) and \u03b7\u0302 2 (x) with median performance when different\ngene effect function \u03b1(*) are implemented. Left panel: smooth \u03b1(*) function. Right panel:\nnonsmooth \u03b1(*) function.\n\n\f15\n\nGENEWISE VARIANCE ESTIMATION\n\nand \u03c32 is very accurate. To see this more clearly, the squared bias, variance\n2 (x) under the seven correlation\nand MSE of the estimator \u03c1, \u03c31 and \u03c32 in \u03c3\u0302A\nsettings are reported in Table 3. Here, the true value of \u03c31 and \u03c32 is 0.4217\nand 0.1857. For example, when \u03c1 = 0.8, the bias of \u03c1\u0302 is less than 0.002\n2 (x), which is acceptable because the convergence threshold in the\nfor \u03c3\u0302A\nalgorithm is set to be 0.001.\n2 (x) with the median\nIn Figure 2, we render the estimates \u03b7\u0302 2 (x) and \u03c3\u0302A\nISE under four different correlation settings: \u03c1 = \u22120.4, \u03c1 = 0, \u03c1 = 0.6 and\n\u03c1 = 0.8. We omit the other correlation schemes since they all have similar\nperformance. The solid lines represent the true variance function. The dotted\n2 (x), respectively. For the case \u03c1 =\nlines and dashed lines are for \u03b7\u0302 2 (x) and \u03c3\u0302A\n0, the two estimators are indistinguishable. When \u03c1 < 0, \u03b7\u0302 2 (x) overestimates\nthe genewise variance function, whereas when \u03c1 > 0, it underestimates the\ngenewise variance function.\n4. Application to human total RNA samples using Agilent arrays. Our\nreal data example comes from Microarray Quality Control (MAQC) project\n[Patterson et al. (2006)]. The main purpose of the original paper is on comparison of reproducibility, sensitivity and specificity of microarray measurements across different platforms (i.e., one-color and two-color) and testing\nsites. The MAQC project use two RNA samples, Stratagene Universal Human Reference total RNA and Ambion Human Brain Reference total RNA.\nThe two RNA samples have been assayed on three kinds of arrays: Agilent,\nCapitalBio and TeleChem. The data were collected at five sites. Our study\nfocuses only on the Agilent arrays. At each site, 10 two-color Agilent microarrays are assayed with 5 of them dye swapped, totaling 30 microarrays.\nTable 3\n2\nSquared bias, variance and MSE of \u03c1\u0302, \u03c3\u03021 and \u03c3\u03022 in the estimate \u03c3\u0302A\n(x).\n6\nAll quantities are multiplied by 10\n\u03c1\n2\n\u03c3\u0302A\n(x)\n\n\u03c1\u0302\n\n\u03c3\u03021\n\n\u03c3\u03022\n\nBias2\nVAR\nMSE\nBias2\nVAR\nMSE\nBias2\nVAR\nMSE\n\n\u22120.4\n\n\u22120.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0.07\n7.90\n7.97\n0.24\n11.65\n11.89\n0.14\n10.34\n10.47\n\n0.04\n16.91\n16.95\n0.23\n11.52\n11.75\n0.14\n10.17\n10.31\n\n0.01\n28.65\n28.66\n0.19\n11.79\n11.99\n0.12\n10.45\n10.57\n\n0.00\n36.17\n36.17\n0.14\n12.46\n12.60\n0.09\n11.12\n11.20\n\n0.00\n35.68\n35.68\n0.11\n13.64\n13.75\n0.08\n12.24\n12.32\n\n0.00\n27.21\n27.21\n0.05\n15.55\n15.59\n0.05\n13.96\n14.00\n\n3.90\n20.44\n24.35\n2.47\n18.66\n21.12\n0.67\n16.16\n16.83\n\n\f16\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\n2\nFig. 2. Median performance of variance estimators \u03b7\u0302 2 (x), \u03c3\u0302 2 (x) and \u03c3\u0302A\n(x) when\n\u03c1 = \u22120.4, 0, 0.6 and 0.8.\n\n4.1. Validation test. In the first application, we revisit the validation test\nas considered in Fan and Niu (2007). For the purpose of the validation tests,\nwe use gProcessedSignal and rProcessedSignal values from Agilent Feature\nExtraction software as input. We follow the preprocessing scheme described\nin Patterson et al. (2006) and get 22,144 genes from a total of 41,675 noncontrol genes. Among those, 19 genes with each having 10 replications are used\nfor validation tests. Under the null hypothesis of no experimental biases, a\nreasonable model is\n(20)\n\nYgi = \u03b1g + \u03b5gi ,\n\n\u03b5gi \u223c N (0, \u03c3g2 ),\n\ni = 1, . . . , I, g = 1, . . . , G.\n\nWe use the notation G to denote the number of genes that have I replications. For our data, G = 19 and I = 10. Note that G can be different from\nN , the total number of different genes. The validation test statistics in Fan\n\n\f17\n\nGENEWISE VARIANCE ESTIMATION\n\nand Niu (2007) include weighted statistics\n)\n( I\n( I\n)\nG\nG\n.\n.\nX\nX\nX\nX\n|Ygi \u2212 \u0232g | \u03c3g ,\n(Ygi \u2212 \u0232g )2 \u03c3g2 ,\nT2 =\nT1 =\ng=1\n\ng=1\n\ni=1\n\ni=1\n\nand unweighted test statistics\n)\u22121/2\n)(\n( G I\nG\nG\nX\nX\nXX\n,\n\u03c3g4\n2(I \u2212 1)\n\u03c3g2\nT3 =\n(Ygi \u2212 \u0232g )2 \u2212 (I \u2212 1)\ng=1\n\ng=1\n\ng=1 i=1\n\n!1/2 )\n),(\n( G I\nG\nG\nX\nX\nXX\n,\n\u03c3g2\n\u03baI\n\u03c3g\nT4 =\n|Ygi \u2212 \u0232g | \u2212 \u03bbI\ng=1\n\ng=1\n\ng=1 i=1\n\np\n\nP\nwhere \u03bbI = 2I(I \u2212 1)/\u03c0 and \u03ba2I = var( Ii=1 |\u03b5gi \u2212 \u03b5\u0304g |/\u03c3g ). Under the null\nhypothesis, the test statistic T1 is \u03c72 distributed with degree of freedom\n(I \u2212 1)G and T2 , T3 and T4 are all asymptotically normally distributed. As\na result, the corresponding p-values can be easily computed.\nHere, we apply the same statistics T1 , T2 , T3 and T4 but we replace the\npooled sample variance estimator by the aggregated local linear estimator\n\u03c3\u0302g2\n\n=\n\nI\nX\ni=1\n\nI\n.X\n\n2\n\u03c3\u0302A\n(Xgi )f\u02c6(Xgi )\n\nf\u02c6(Xgi ),\n\ni=1\n\nwhere f\u02c6 is the estimated density function of Xgi . The difference between the\nnew variance estimator and the simple pooled variance estimator is that we\nconsider the genewise variance as a nonparametric function of the intensity\nlevel. The latter estimator may drag small variances of certain arrays to\nmuch higher levels by averaging, resulting in a larger estimated genewise\nvariance and smaller test statistics or bigger p-values.\nIn the analysis here, we first consider all thirty arrays. The estimated\ncorrelation among replicated genes is \u03c1\u0302 = 0.69. The p-values based on the\nnewly estimated genewise variance are depicted in Table 4. As explained in\nFan and Niu (2007), T4 is the most stable test among the four. It turns out\nthat none of the arrays needs further normalization, which is the same as\nFan and Niu (2007). Furthermore, we separate the analysis into two groups:\nthe first group using 15 arrays without dye-swap, which has the estimated\ncorrelation \u03c1\u0302 = 0.66, and the second group using 15 arrays with dye-swap,\nresulting in an estimated correlation \u03c1\u0302 = 0.34. The p-values are summarized\nin Table 5. Results show that array AGL-2-D3 and array AGL-2-D5 need\nfurther normalization if 5% significance level applies. The difference is due\nto decreased estimated \u03c1 for the dye swap arrays and p-values are sensitive\nto the genewise variance. We also did analysis by separating data into 6\n\n\f18\n\nJ. FAN, Y. FENG AND Y. S. NIU\nTable 4\nComparison of p-values for T1 , . . . , T4 for MAQC project data\nconsidering all 30 arrays together\np-values\nSlide name\n\nT1\n\nT2\n\nT3\n\nT4\n\nAGL-1-C1\nAGL-1-C2\nAGL-1-C3\nAGL-1-C4\nAGL-1-C5\nAGL-1-D1\nAGL-1-D2\nAGL-1-D3\nAGL-1-D4\nAGL-1-D5\nAGL-2-C1\nAGL-2-C2\nAGL-2-C3\nAGL-2-C4\nAGL-2-C5\nAGL-2-D1\nAGL-2-D2\nAGL-2-D3\nAGL-2-D4\nAGL-2-D5\nAGL-3-C1\nAGL-3-C2\nAGL-3-C3\nAGL-3-C4\nAGL-3-C5\nAGL-3-D1\nAGL-3-D2\nAGL-3-D3\nAGL-3-D4\nAGL-3-D5\n\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.8387\n0.3525\n1.0000\n0.8820\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.9999\n0.9011\n0.1824\n1.0000\n0.8070\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.9996\n0.8953\n0.3902\n1.0000\n0.8848\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.9999\n0.9182\n0.1905\n1.0000\n0.7952\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\ngroups: with and without dye swap, and three sites of experiments. Due to\nthe small sample size, the six estimates of \u03c1 range from 0.08 to 0.74, and we\nalso find that array AGL-2-D3 needs further normalization.\n4.2. Gene selection. To detect the differentially expressed genes, we follow the filter instruction and get 19,802 genes out of 41,000 unique noncontrol genes as in Patterson et al. (2006), that is, I = 1. The dye swap result\nwas averaged before doing the one-sample t-test. Thus, at each site, we have\nfive microarrays.\n\n\f19\n\nGENEWISE VARIANCE ESTIMATION\nTable 5\nComparison of p-values for T1 , . . . , T4 for MAQC project data\nconsidering the arrays with and without dye-swap separately\np-values\nSlide name\n\nT1\n\nT2\n\nT3\n\nT4\n\nAGL-1-C1\nAGL-1-C2\nAGL-1-C3\nAGL-1-C4\nAGL-1-C5\nAGL-1-D1\nAGL-1-D2\nAGL-1-D3\nAGL-1-D4\nAGL-1-D5\nAGL-2-C1\nAGL-2-C2\nAGL-2-C3\nAGL-2-C4\nAGL-2-C5\nAGL-2-D1\nAGL-2-D2\nAGL-2-D3\nAGL-2-D4\nAGL-2-D5\nAGL-3-C1\nAGL-3-C2\nAGL-3-C3\nAGL-3-C4\nAGL-3-C5\nAGL-3-D1\nAGL-3-D2\nAGL-3-D3\nAGL-3-D4\nAGL-3-D5\n\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.0152\n1.0000\n0.7806\n0.2170\n0.0002\n1.0000\n0.1236\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.9493\n1.0000\n0.8074\n0.2984\n0.0000\n1.0000\n0.0662\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n1.0000\n1.0000\n0.9999\n1.0000\n0.9999\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.9943\n1.0000\n1.0000\n0.3931\n0.8060\n0.6622\n0.1651\n0.0001\n1.0000\n0.0669\n0.9996\n0.9988\n0.9977\n1.0000\n0.9999\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n0.9136\n1.0000\n0.6584\n0.2217\n0.0000\n1.0000\n0.0300\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\nFor each site, significant genes are selected based on these 5 dye-swaped\naverage arrays. For all N = 19,802 genes, there are no within-array replications. However, model (2) is still reasonable, in which i indexes the array.\nHence, the \"within-array correlation\" becomes \"between-array correlation\"\nand is reasonably assumed as \u03c1 = 0.\nIn our nonparametric estimation for the variance function, all the 19,802\ngenes are used to estimate the variance function, which gives us enough\n2 (x) is close to the inherent true\nreason to believe that the estimator \u03c3\u0302A\nvariance function \u03c3 2 (x).\n\n\f20\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nWe applied both the t-test and z-test to each gene to see if the logarithm\nof the expression ratio is zero, using the five arrays collected at each location. The number of differentially expressed genes detected by using the\ntwo different tests under three Fold Changes (FC) and four significant levels\nare given in Table 6. Large numbers of genes are identified as differentially\nexpressed, which is expected when comparing a brain sample and a tissue\npool sample [Patterson et al. (2006)]. We can see clearly that the z-test as2 (x) leads to more differentially\nsociated with our new variance estimator \u03c3\u0302A\nexpressed genes. For example, at site 1, using \u03b1 = 0.001, among the fold\nchanges at least 2, t-test picks 8231 genes whereas z-test selects 8875 genes.\nThis gives an empirical power increase of (8875 \u2212 8231)/19,802 \u2248 3.25% in\nthe group with observed fold change at least 2.\nTo verify the accuracy of our variance estimation in the z-test, we compare\nthe empirical power increase with the expected theoretical power increase.\nThe expected theoretical power increase is computed as\n(21)\n\nave{Pz (\u03bcg /\u03c3g ) \u2212 Ptn\u22121 (\u03bcg /\u03c3g )},\n\ntaking the average of power increases across all \u03bcg 6= 0. However, in the absence of the availability, we replace \u03bcg by its estimate, which is the sample\naverage of n = 5 observed log-expression ratios. Table 7 depicts the results\nat three different sites, in which the columns \"Theo\" refer to the expected\ntheoretical power increase defined by (21), with \u03bcg replaced by \u0232g and \u03c3g replaced by its estimate from the genewise variance function, and the columns\n\"Emp\" refer to the empirical power increase.\nThere are two things worth noticing here. First, for expected theoretical\npower increase, we use the sample mean \u0232g = \u03bcg + \u01edg instead of the real gene\neffect \u03bcg , which is not observable, so it inevitably involves the error term \u01edg .\nTable 6\nComparison of the number of significantly differentially expressed genes\np < 0.05\n\nAgilent 1\n\nAgilent 2\n\nAgilent 3\n\nFC > 1.5\nFC > 2\nFC > 4\nFC > 1.5\nFC > 2\nFC > 4\nFC > 1.5\nFC > 2\nFC > 4\n\np < 0.01\n\np < 0.005\n\np < 0.001\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\n12692\n8802\n3493\n12282\n8644\n3600\n12502\n8689\n3585\n\n12802\n8879\n3493\n12678\n8877\n3649\n12692\n8832\n3603\n\n12464\n8654\n3431\n11217\n7908\n3188\n11994\n8344\n3378\n\n12752\n8872\n3493\n12587\n8875\n3649\n12576\n8810\n3602\n\n12313\n8556\n3376\n10502\n7452\n2964\n11694\n8150\n3278\n\n12722\n8869\n3493\n12536\n8861\n3649\n12519\n8800\n3602\n\n11744\n8231\n3231\n8270\n6125\n2422\n10788\n7591\n2985\n\n12646\n8858\n3493\n12421\n8828\n3649\n12374\n8762\n3600\n\n\f21\n\nGENEWISE VARIANCE ESTIMATION\nTable 7\nComparison of expected theoretical and empirical power difference (in percentage)\n\u03b1 = 0.05\n\nAgilent 1\nAgilent 2\nAgilent 3\nAverage\n\n\u03b1 = 0.01\n\n\u03b1 = 0.005\n\n\u03b1 = 0.001\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\n2.52\n4.03\n3.02\n3.19\n\n0.61\n7.56\n2.56\n3.58\n\n6.08\n10.11\n7.14\n7.78\n\n3.75\n17.61\n7.39\n9.58\n\n8.06\n13.61\n9.42\n10.36\n\n5.59\n22.86\n10.19\n12.88\n\n13.66\n23.75\n15.94\n17.79\n\n11.74\n37.63\n18.18\n22.51\n\nSecond, the power functions Pz (\u03bc) and Pt (\u03bc) depend sensitively on \u03bc and the\ntails of the assumed distribution. Despite of these, the expected theoretical\nand empirical power increases are in the same bulk and the averages are very\nclose. This provides good evidence that our genewise variance estimation has\nan acceptable accuracy.\nWe also apply SIMEX and permutation SIMEX methods in Carroll and\nWang (2008) to the MAQC data, to illustrate its utility. As mentioned in the\nIntroduction, their model is not really intended for the analysis of two-color\nmicroarray data. Should we only use the information on log-ratios (Y ), the\nmodel is very hard to interpret. In addition, one might question why the\ninformation on X (observed intensity levels) is not used at all. Nevertheless,\nwe apply the SIMEX methods of Carroll and Wang (2008) to only the logratios Y in the two-color data and produce similar tables to the Tables 6\nand 7.\nFrom the results, we have the following understandings. First, all the numbers for z-test in Tables 8 and 9 at all significance levels are approximately\nthe same. In fact, the p-values are very small so that numbers at different\nsignificance levels are about the same. That indicates that both SIMEX and\npermutation SIMEX method are tending to estimate genewise variance very\nsmall, making the test statistics large for all the time. On the other hand,\nour method estimates the genewise variance moderately so that the numbers are not exactly the same for different significance levels. Second, in the\nimplementation, we found that the SIMEX and permutation SIMEX is computationally expensive (more than one hour) while our method only takes\na few minutes. Third, from Tables 10 and 11 we can see that the expected\ntheoretical power increase and the empirical ones are reasonably close, which\nare in lines with our method.\n5. Summary. The estimation of genewise variance function is motivated\nby the downstream analysis of microarray data such as validation test and\nselecting statistically differentially expressed genes. The methodology proposed here is novel by using across-array and within-array replications. It\n\n\f22\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nTable 8\nComparison of the number of significantly differentially expressed genes using SIMEX\nmethod\np < 0.05\n\nAgilent 1\n\nAgilent 2\n\nAgilent 3\n\nFC > 1.5\nFC > 2\nFC > 4\nFC > 1.5\nFC > 2\nFC > 4\nFC > 1.5\nFC > 2\nFC > 4\n\np < 0.01\n\np < 0.005\n\np < 0.001\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\n12692\n8802\n3493\n12282\n8644\n3600\n12502\n8689\n3585\n\n12820\n8879\n3493\n12721\n8878\n3649\n12760\n8836\n3603\n\n12464\n8654\n3431\n11217\n7908\n3188\n11994\n8344\n3378\n\n12820\n8879\n3493\n12721\n8878\n3649\n12760\n8836\n3603\n\n12313\n8556\n3376\n10502\n7452\n2964\n11694\n8150\n3278\n\n12820\n8879\n3493\n12721\n8878\n3649\n12760\n8836\n3603\n\n11744\n8231\n3231\n8270\n6125\n2422\n10788\n7591\n2985\n\n12820\n8879\n3493\n12721\n8878\n3649\n12760\n8836\n3603\n\nTable 9\nComparison of the number of significantly differentially expressed genes using\npermutation SIMEX\np < 0.05\n\nAgilent 1\n\nAgilent 2\n\nAgilent 3\n\nFC > 1.5\nFC > 2\nFC > 4\nFC > 1.5\nFC > 2\nFC > 4\nFC > 1.5\nFC > 2\nFC > 4\n\np < 0.01\n\np < 0.005\n\np < 0.001\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\nt-test\n\nz-test\n\n12692\n8802\n3493\n12282\n8644\n3600\n12502\n8689\n3585\n\n12820\n8879\n3493\n12721\n8878\n3649\n12760\n8836\n3603\n\n12464\n8654\n3431\n11217\n7908\n3188\n11994\n8344\n3378\n\n12820\n8879\n3493\n12721\n8878\n3649\n12760\n8836\n3603\n\n12313\n8556\n3376\n10502\n7452\n2964\n11694\n8150\n3278\n\n12820\n8879\n3493\n12721\n8878\n3649\n12760\n8836\n3603\n\n11744\n8231\n3231\n8270\n6125\n2422\n10788\n7591\n2985\n\n12820\n8879\n3493\n12721\n8878\n3649\n12760\n8836\n3603\n\nTable 10\nComparison of expected theoretical and empirical power difference using SIMEX method\n(in percentage)\n\u03b1 = 0.05\n\nAgilent 1\nAgilent 2\nAgilent 3\nAverage\n\n\u03b1 = 0.01\n\n\u03b1 = 0.005\n\n\u03b1 = 0.001\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\n2.43\n7.16\n4.18\n4.59\n\n2.06\n3.41\n2.88\n2.78\n\n7.17\n19.20\n11.71\n12.69\n\n5.42\n12.06\n7.38\n8.29\n\n10.30\n26.17\n16.45\n17.64\n\n7.34\n16.90\n9.89\n11.38\n\n20.71\n43.46\n31.38\n31.85\n\n13.44\n30.42\n17.57\n20.48\n\n\f23\n\nGENEWISE VARIANCE ESTIMATION\n\nTable 11\nComparison of expected theoretical and empirical power difference using permutation\nSIMEX method (in percentage)\n\u03b1 = 0.05\n\nAgilent 1\nAgilent 2\nAgilent 3\nAverage\n\n\u03b1 = 0.01\n\n\u03b1 = 0.005\n\n\u03b1 = 0.001\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\nTheo\n\nEmp\n\n1.89\n4.84\n2.89\n3.20\n\n2.86\n7.37\n4.91\n5.05\n\n5.66\n13.44\n8.34\n9.15\n\n6.43\n17.22\n10.13\n11.26\n\n8.19\n18.97\n11.87\n13.01\n\n8.59\n22.50\n13.11\n14.74\n\n16.75\n36.90\n23.44\n25.70\n\n15.07\n37.26\n21.31\n24.55\n\ndoes not require any specific assumptions on \u03b1g such as sparsity or smoothness, and hence reduces the bias of the conventional nonparametric estimators. Although the number of nuisance parameters is proportional to the\nsample size, we can estimate the main interest (variance function) consistently. By increasing the degree of freedom largely, both the validation tests\nand z-test using our variance estimators are more powerful in identifying\narrays that need to be normalized further and more capable of selecting\ndifferentially expressed genes.\nOur proposed methodology has a wide range of applications. In addition\nto the microarray data analysis with within-array replications, it can be also\napplied to the case without within-array replications, as long as the model\n(2) is reasonable. Our two-way nonparametric model is a natural extension\nof the Neyman\u2013Scott problem. Therefore, it is applicable to all the problems\nwhere the Neyman\u2013Scott problem is applicable.\nThere are possible extensions. For example, the SIMEX idea can be applied on our model in order to take into account the measurement error.\nWe can also make adaptations to our methods when we have a prior correlation structure among replications other than the identical correlation\nassumption.\nAPPENDIX\nThe following regularity conditions are imposed for the technical proofs:\n1. The regression function \u03c3 2 (x) has a bounded and continuous second\nderivative.\n2. The kernel function K is a bounded symmetric density function with a\nbounded support.\n3. h \u2192 0, N h \u2192 \u221e.\n4. E[\u03c3 8 (X)] exists and the marginal density fX (*) is continuous.\nWe need the following conditional variance\u2013covariance matrix of the random vector Zg in our asymptotic study.\n\n\f24\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nLemma 1. Let \u03a9 be the variance\u2013covariance matrix of Zg conditioning\non all data X. Then, respectively, the diagonal and off-diagonal elements are\nX\n2\n\u03c3 2 (Xgk )\u03c3 2 (Xgl )\n\u03a9ii = 2\u03c3 4 (Xgi ) +\n(I \u2212 1)2 (I \u2212 2)2\nk6=l\n(22)\nX\n4(I \u2212 3)\n2\n\u03c3\n(X\n)\n\u03c3 2 (Xgj ),\ni = 1, . . . , I,\n+\ngi\n(I \u2212 1)(I \u2212 2)2\nj6=i\n\n\u03a9ij =\n\n4\n\u03c3 2 (Xgi )\u03c3 2 (Xgj )\n(I \u2212 1)2\nX\n2\n\u03c3 2 (Xgk )\u03c3 2 (Xgl )\n+\n2\n2\n(I \u2212 1) (I \u2212 2)\nk6=l\nk,l6=i,j\n\n(23)\n\u2212\n\nX\n4\n\u03c3 2 (Xgk )(\u03c3 2 (Xgi ) + \u03c3 2 (Xgj )),\n(I \u2212 1)2 (I \u2212 2)\nk6=i,j\n\ni, j = 1, . . . , I, i 6= j.\nProof. Let A be the variance\u2013covariance matrix of rg conditioning on\nall data X. By direct computation, the diagonal elements are given by\nAii = var[(Ygi \u2212 \u0232g )2 |X]\n(24)\n\n=\n\n2(I \u2212 1)4 4\n4(I \u2212 1)2 X 2\n2 X 4\n\u03c3\n(X\n)\n+\n\u03c3 (Xgi )\u03c3 2 (Xgk ) + 4\n\u03c3 (Xgk )\ngi\n4\n4\nI\nI\nI\nk6=i\n\n+\n\n4\nI4\n\nX\n\n\u03c3 2 (Xgl )\u03c3 2 (Xgk ),\n\nk6=i\n\ni = 1, . . . , I,\n\nl,k6=i,l<k\n\nand the off-diagonal elements are given by\nAij = cov{[(Ygi \u2212 \u0232g )2 , (Ygj \u2212 \u0232g )2 ]|X}\n=\n(25)\n\n2(I \u2212 1)2 4\n4(I \u2212 1)2 2\n4\n[\u03c3\n(X\n)\n+\n\u03c3\n(X\n)]\n+\n\u03c3 (Xgi )\u03c3 2 (Xgj )\ngi\ngj\nI4\nI4\n4(I \u2212 1) X 2\n\u2212\n\u03c3 (Xgk )(\u03c3 2 (Xgi ) + \u03c3 2 (Xgj ))\nI4\nk6=i,j\n\n+\n\n4\nI4\n\nX\n\nk,l6=i,j;l<k\n\n\u03c3 2 (Xgl )\u03c3 2 (Xgk ) +\n\n2 X 4\n\u03c3 (Xgk ).\nI4\nk6=i,j\n\nUsing \u03a9 = BABT , we can obtain the result by direct computation. \u0003\n\n\fGENEWISE VARIANCE ESTIMATION\n\n25\n\nThe proofs for Theorems 1 and 3 follow a similar idea. Since Theorem 1\ndoes not involve a lot of coefficients, we will show the proof of Theorem 1\nand explain the difference in the proof of Theorem 3.\nProof of Theorem 1. First of all, the bias of \u03b7i2 (x) comes from the\nlocal linear approximation. Since {(Xgi , Zgi )}N\ng=1 is an i.i.d. sequence, by (4)\nand the result in Fan and Gijbels (1996), it follows that\nE{\u03b7i2 (x)|X} = \u03c3 2 (x) + b(x) + oP (h2 ).\nSimilarly, the asymptotic variance of \u03b7i2 (x) also follows from Fan and Gijbels\n(1996).\nWe now prove the off-diagonal elements in matrix var[\u03b7|X]\ncov[(\u03b7\u0302i2 (x), \u03b7\u0302j2 (x))|X] = V2 + oP (1/N ).\nP\nRecalling that \u03b7\u0302i2 (x) = N\ng=1 WN,i ((Xgi \u2212 x)/h)Zgi , we have\n\u0012\n\u0012\n\u0013\n\u0013\nN\nX\nXgi \u2212 x\nXgj \u2212 x\n2\n2\nWN,i\ncov[(\u03b7\u0302i (x), \u03b7\u0302j (x))|X] =\nWN,j\ncov[(Zgi , Zgj )|X].\nh\nh\n(26)\n\ng=1\n\nThe equality follows by the fact that cov[(Zgi , Zg\u2032 j )|X] = 0 when g 6= g \u2032 .\nRecall \u03a9ij = cov[(Zgi , Zgj )|X] and define RN,g = N * WN,j ((Xgj \u2212 x)/h)\u03a9ij .\nThus,\n\u0013\n\u0012\nN\nX\nXgi \u2212 x\n(27)\nRN,g .\nWN,i\nN * cov[(\u03b7\u0302i2 (x), \u03b7\u0302j2 (x))|X] =\nh\ng=1\n\nThe right-hand side of (27) can be seen as local linear smoother of the synthetic data {(Xgi , RN,g )}N\ng=1 . Although RN,g involves N at the first glance,\nits conditional expectation E[RN,g |Xgi = x] and conditional variance\nvar[RN,g |Xgi = x] do not grow with N . Since {(Xgi , RN,g )}N\ng=1 is an i.i.d.\nsequence, by the results in Fan and Gijbels (1996), we obtain\nN * cov[(\u03b7\u0302i2 (x), \u03b7\u0302j2 (x))|X] = E[RN,g |Xgi = x] + oP (1).\n\nTo calculate E[RN,g |Xgi = x], we apply the approximation WN,i (u) = K(u)(1+\noP (1))/(N hfX (x)) in the example of Fan and Gijbels [(1996), page 64] and\nhave the following arguments\nE[RN,g |Xgi = x]\n\u0015\n\u0014\n1\nhKh (Xgj \u2212 x)\u03a9ij |Xgi = x (1 + oP (1))\n=E N *\nN hfX (x)\nZ\n= (fX (x))\u22121 K(u)\u03a9ij |Xgi =x (x + hu, s)fX (x + hu) du ds + oP (1)\n= N V2 + oP (1),\n\n\f26\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\nwhere s represents all the integrating variables corresponding to Xg1 , . . . , XgI\nexcept Xgi and Xgj . That justifies (26).\nTo prove the multivariate asymptotic normality\nD\n\n\u03a3\u22121/2 (\u03b7 \u2212 (\u03c3 2 (x) + b(x) + oP (h2 ))e) \u2212\u2192 N (0, II ),\n\n(28)\n\nwe employ Cram\u00e9r\u2013Wold device: for any unit vector a = (a1 , . . . , aI )T in RI ,\n( I\n)\n\u0012\n\u0013\nN\nX X\nX\n\u2212\nx\n\u25b3\nD\ngi\nai\nWN,i\nF \u2217 = {aT \u03a3a}\u22121/2\n(Zgi \u2212 \u03c3 2 (Zgi )) \u2212\u2192 N (0, 1).\nh\ng=1\ni=1\n\ne g = PI ai Qg,i .\nDenote by Qg,i = WN,i ((Xgi \u2212 x)/h)(Zgi \u2212 \u03c3 2 (Xgi )) and Q\ni=1\neg }N is i.i.d. distributed. To show the asymptotic\nNote that the sequence {Q\ng=1\nnormality of F \u2217 , it is sufficient to check Lyapunov's condition:\nPN\ne 4\ng=1 E[|Qg | |X]\n= 0.\nlim PN\neg |2 |X])2\nN \u2192\u221e (\nE[|Q\ng=1\n\nTo facilitate the presentation, we first note that sequences {Qg,i }N\ng=1 are i.i.d.\n2\nand satisfy Lyapunov's condition for each fixed i. Denote \u03b4N,i\n=\nPN\n2\n2\n2\n\u22121\ng=1 E[|Qg,i | |X]. And recall that \u03b4N,i = var[\u03b7\u0302i (x)|X] = OP ((N h) ). Let\nc\u2217 be a generic constant which may vary from one line to another. We have\nthe following approximation:\nN\nX\ng=1\n\nE[|Qg,i |4 |X] = c\u2217 N \u22123 E{Kh4 (Xgi \u2212 x)[(Zgi \u2212 \u03c3 2 (Xgi ))4 |X]}(1 + oP (1))\n= OP ((N h)\u22123 ).\n\nPN\n4\n4\nTherefore,\ng=1 E[|Qg,i | |X] = o(\u03b4N,i ). By the marginal Lyapunov conditions, we have the following inequality:\nN\nX\ng=1\n\ne 4g |X] \u2264 c\u2217\nE[Q\n\nN\nI X\nX\ni=1 g=1\n\nE[|Qg,i |4 |X] = c\u2217 I * oP ((N h)\u22122 ) = oP ((N h)\u22122 ).\n\nFor the denominator, we have the following arguments:\nN\nX\ng=1\n\neg |2 |X] =\nE[|Q\n=\n\nX\ni\n\nX\n\na2i\n\nN\nX\ng=1\n\nE[Q2g,i |X] +\n\na2i var[\u03b7\u0302i2 (x)|X] +\n\ni\n\n\u2217\n\nX\ni6=j\n\nX\ni6=j\n\n= OP ((N h)\u22121 ) + OP (N \u22121 )\n= OP ((N h)\u22121 ).\n\nai aj\n\nN\nX\ng=1\n\nE[Qg,i Qg,j |X]\n\nai aj cov[(\u03b7\u0302i2 (x), \u03b7\u0302j2 (x))|X]\n\n\fGENEWISE VARIANCE ESTIMATION\n\n27\n\nNote that the second to last equality holds by the asymptotic conditional\nvariance\u2013covariance matrix \u03a3. Therefore Lyapunov's condition is justified.\nThat completes the proof. \u0003\nProof of Theorem 2.\n\nFirst of all, for each given g,\n\nEs2B,g = I var(\u0232gj ) = \u03c32 + \u03c1(I \u2212 1)\u03c312 .\nNote that by (8), we have\nE(Ygij \u2212 \u0232gj )2 = I \u22122 [I(I \u2212 1)\u03c32 + \u03c1(I \u2212 1)(I \u2212 2)\u03c312 \u2212 2(I \u2212 1)2 \u03c1\u03c312 ]\n= I \u22121 (I \u2212 1)(\u03c32 \u2212 \u03c1\u03c312 ).\n\nThus, for all g, we have\nEs2W,g = \u03c32 \u2212 \u03c1\u03c312 .\nSince {s2B,g } and {s2W,g } are i.i.d. sequences across the N genes, by the\ncentral limit theorem, we have\nN\n1 X 2\nsB,g = \u03c32 + \u03c1(I \u2212 1)\u03c312 + OP (N \u22121/2 ),\nN\ng=1\n\nN\n1 X 2\nsW,g = \u03c32 \u2212 \u03c1\u03c312 + OP (N \u22121/2 ).\nN\ng=1\n\nTherefore,\n\u03c1\u03020 =\n\n\u03c32 + \u03c1(I \u2212 1)\u03c312 \u2212 \u03c32 + \u03c1\u03c312 + OP (N \u22121/2 )\n\u03c32 + \u03c1(I \u2212 1)\u03c312 + (I \u2212 1)(\u03c32 \u2212 \u03c1\u03c312 ) + OP (N \u22121/2 )\n\n= \u03c1\u03c312 /\u03c32 + OP (N \u22121/2 ).\n\n\u0003\n\nNote that\n\u0012\n\u0013\nN X\nI\nX\n2 Xgi \u2212 x\n2\nWN\nvar[\u03b7\u0302A (x)|X] =\nvar[Zgi |X]\nh\nProof of Theorem 3.\n\ng=1 i=1\n\n+\n\nN X\nI\nX\ng=1 i6=j\n\nWN\n\n\u0012\n\n\u0013\n\u0013\n\u0012\nXgj \u2212 x\nXgi \u2212 x\nWN\ncov[(Zgi , Zgj )|X].\nh\nh\n\n2 (x)|\nFollowing similar steps in the proof of Theorem 1, one can verify var[\u03b7\u0302A\nX] = V1\u2032 /I + (1 \u2212 1/I)V2\u2032 + oP ((N h)\u22121 ), where the coefficients C2 , . . . , C4 , D0 ,\n\n\f28\n\nJ. FAN, Y. FENG AND Y. S. NIU\n\n. . . , D4 are as follows:\nC2 =\n\n4(1 + \u03c12 )\u03c32 + [4\u03c1(I \u2212 2) + 4\u03c12 (2I \u2212 3)]\u03c312\n,\nI \u22121\n\n8\u03c12 (I \u2212 3)\u03c313 + 8(\u03c12 + \u03c1)\u03c31 \u03c32\n,\nI \u22121\n2\nC4 =\n{(1 + \u03c12 )\u03c322 + 2(\u03c12 + \u03c1)(I \u2212 3)\u03c312 \u03c32\n(I \u2212 1)(I \u2212 2)\n\nC3 = \u2212\n\n\u0013\n\u0012\n2(1 + \u03c12 )\n4\u03c1\n2\n+\n,\nD0 = 2 \u03c1 \u2212\nI \u22121\n(I \u2212 1)2\nD1 =\nD2 =\n\n8\n{(2I \u2212 4)\u03c1 \u2212 (I 2 \u2212 4I + 5)\u03c12 }\u03c31 ,\n(I \u2212 1)2\n\n4\n{(I \u2212 3)2 \u03c12 + ((I \u2212 2)2 + 1)\u03c1 \u2212 2(I \u2212 2)}\u03c32\n(I \u2212 1)2 (I \u2212 2)\n+\n\nD3 = \u2212\nD4 =\n\n+ (I \u2212 3)(I \u2212 4)\u03c12 \u03c314 },\n\n4(I \u2212 3)\n{(3(I \u2212 2)(I \u2212 3) + 2)\u03c12 \u2212 2(I \u2212 2)\u03c1}\u03c312 ,\n(I \u2212 1)2 (I \u2212 2)\n\n8(I \u2212 3)2\n{(\u03c12 + \u03c1)\u03c31 \u03c32 + (I \u2212 4)\u03c12 \u03c313 },\n(I \u2212 1)2 (I \u2212 2)\n4\n\n\u2212 2)2\n\u001a\n\u0012\n\u0013\nI \u22122\n2\n\u00d7 (1 + \u03c1 )\n\u03c322\n2\n\u0012\n\u0013\n\u0012\n\u0013 \u001b\nI \u22122\n2\n2\n2 I \u22122\n+ 6(\u03c1 + \u03c1)\n\u03c31 \u03c32 + 12\u03c1\n\u03c314 .\n3\n4\n\n(I\n\n\u2212 1)2 (I\n\n\u0003\n\nAcknowledgments. The authors thank the Editor, the Associate Editor\nand two referees, whose comments have greatly improved the scope and\npresentation of the paper.\nREFERENCES\nCarroll, R. J. and Wang, Y. (2008). Nonparametric variance estimation in the analysis\nof microarray data: A measurement error approach. Biometrika 95 437\u2013449. MR2422697\nCui, X., Hwang, J. T. and Qiu, J. (2005). Improved statistical tests for differential gene\nexpression by shrinking variance components estimates. Biostatistics 6 59\u201375.\nFan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Applications. Chapman and Hall, London. MR1383587\nFan, J. and Niu, Y. (2007). Selection and validation of normalization methods for c-DNA\nmicroarrays using within-array replications. Bioinformatics 23 2391\u20132398.\n\n\fGENEWISE VARIANCE ESTIMATION\n\n29\n\nFan, J., Peng, H. and Huang, T. (2005). Semilinear high-dimensional model for normalization of microarray data: A theoretical analysis and partial consistency (with\ndiscussion). J. Amer. Statist. Assoc. 100 781\u2013813. MR2201010\nFan, J. and Ren, Y. (2007). Statistical analysis of DNA microarray data in cancer research. Clinical Cancer Research 12 4469\u20134473.\nFan, J., Tam, P., Vande Woude, G. and Ren, Y. (2004). Normalization and analysis\nof cDNA micro-arrays using within-array replications applied to neuroblastoma cell\nresponse to a cytokine. Proc. Natl. Acad. Sci. USA 101 1135\u20131140.\nHuang, J., Wang, D. and Zhang, C. (2005). A two-way semi-linear model for normalization and significant analysis of cDNA microarray data. J. Amer. Statist. Assoc. 100\n814\u2013829. MR2201011\nKamb, A. and Ramaswami, A. (2001). A simple method for statistical analysis of intensity\ndifferences in microarray-deried gene expression data. BMC Biotechnology 1 8.\nNeyman, J. and Scott, E. (1948). Consistent estimates based on partially consistent\nobservations. Econometrica 16 1\u201332. MR0025113\nPatterson, T. et al. (2006). Performance comparison of one-color and two-color platforms within the MicroArray Quality Control (MAQC) project. Nature Biotechnology\n24 1140\u20131150.\nRuppert, D., Wand, M. P., Holst, U. and H\u00f6ssjer, O. (1997). Local polynomial\nvariance function estimation. Technometrics 39 262\u2013273. MR1462587\nSmyth, G., Michaud, J. and Scott, H. (2005). Use of within-array replicate spots for\nassessing differential expression in microarray experiments. Bioinformatics 21 2067\u2013\n2075.\nStorey, J. D. and Tibshirani, R. (2003). Statistical significance for genome-wide studies.\nProc. Natl. Acad. Sci. USA 100 9440\u20139445. MR1994856\nTong, T. and Wang, Y. (2007). Optimal shrinkage estimation of variances with applications to microarray data analysis. J. Amer. Statist. Assoc. 102 113\u2013122. MR2293304\nTusher, V. G., Tibshirani, R. and Chu, G. (2001). Significance analysis of microarrays\napplied to the ionizing radiation response. Proc. Natl. Acad. Sci. 98 5116\u20135121.\nWang, Y., Ma, Y. and Carroll, R. J. (2009). Variance estimation in the analysis of\nmicroarray data. J. Roy. Statist. Soc. Ser. B 71 425\u2013445.\nJ. Fan\nDepartment of Operations Research\nand Financial Engineering\nPrinceton University\nPrinceton, New Jersey 08544\nUSA\nE-mail: jqfan@princeton.edu\n\nY. Feng\nDepartment of Statistics\nColumbia University\n1255 Amsterdam Avenue, 10th Floor\nNew York, New York 10027\nUSA\nE-mail: fy2158@columbia.edu\n\nY. S. Niu\nDepartment of Mathematics\nUniversity of Arizona\n617 N. Santa Rita Ave.\nP.O. Box 210089\nTucson, Arizona 85721-0089\nUSA\nE-mail: yueniu@math.arizona.edu\n\n\f"}