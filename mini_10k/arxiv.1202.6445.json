{"id": "http://arxiv.org/abs/1202.6445v1", "guidislink": true, "updated": "2012-02-29T05:23:33Z", "updated_parsed": [2012, 2, 29, 5, 23, 33, 2, 60, 0], "published": "2012-02-29T05:23:33Z", "published_parsed": [2012, 2, 29, 5, 23, 33, 2, 60, 0], "title": "Principal Component Pursuit with Reduced Linear Measurements", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.1690%2C1202.6097%2C1202.1183%2C1202.3021%2C1202.2271%2C1202.0770%2C1202.4525%2C1202.3340%2C1202.5043%2C1202.1454%2C1202.6586%2C1202.3370%2C1202.4596%2C1202.1329%2C1202.0657%2C1202.0587%2C1202.6493%2C1202.3726%2C1202.4047%2C1202.0218%2C1202.2893%2C1202.5652%2C1202.3287%2C1202.6622%2C1202.3415%2C1202.0493%2C1202.3732%2C1202.0649%2C1202.6351%2C1202.3735%2C1202.5545%2C1202.5537%2C1202.0472%2C1202.1081%2C1202.1672%2C1202.2643%2C1202.0160%2C1202.5594%2C1202.3399%2C1202.1970%2C1202.1417%2C1202.5327%2C1202.2195%2C1202.6445%2C1202.3443%2C1202.2467%2C1202.5553%2C1202.6173%2C1202.4966%2C1202.4380%2C1202.1511%2C1202.4776%2C1202.1169%2C1202.4522%2C1202.5768%2C1202.3389%2C1202.3575%2C1202.5380%2C1202.2869%2C1202.5610%2C1202.5150%2C1202.2674%2C1202.3347%2C1202.6023%2C1202.1467%2C1202.1558%2C1202.0165%2C1202.3368%2C1202.5993%2C1202.2776%2C1202.3640%2C1202.1087%2C1202.5372%2C1202.1659%2C1202.1189%2C1202.3004%2C1202.1823%2C1202.0630%2C1202.1312%2C1202.6189%2C1202.2894%2C1202.1879%2C1202.5379%2C1202.5636%2C1202.6640%2C1202.4964%2C1202.0586%2C1202.5844%2C1202.6209%2C1202.0796%2C1202.1673%2C1202.4794%2C1202.2088%2C1202.3003%2C1202.6069%2C1202.5286%2C1202.5121%2C1202.4575%2C1202.4675%2C1202.1154%2C1202.3626&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Principal Component Pursuit with Reduced Linear Measurements"}, "summary": "In this paper, we study the problem of decomposing a superposition of a\nlow-rank matrix and a sparse matrix when a relatively few linear measurements\nare available. This problem arises in many data processing tasks such as\naligning multiple images or rectifying regular texture, where the goal is to\nrecover a low-rank matrix with a large fraction of corrupted entries in the\npresence of nonlinear domain transformation. We consider a natural convex\nheuristic to this problem which is a variant to the recently proposed Principal\nComponent Pursuit. We prove that under suitable conditions, this convex program\nguarantees to recover the correct low-rank and sparse components despite\nreduced measurements. Our analysis covers both random and deterministic\nmeasurement models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.1690%2C1202.6097%2C1202.1183%2C1202.3021%2C1202.2271%2C1202.0770%2C1202.4525%2C1202.3340%2C1202.5043%2C1202.1454%2C1202.6586%2C1202.3370%2C1202.4596%2C1202.1329%2C1202.0657%2C1202.0587%2C1202.6493%2C1202.3726%2C1202.4047%2C1202.0218%2C1202.2893%2C1202.5652%2C1202.3287%2C1202.6622%2C1202.3415%2C1202.0493%2C1202.3732%2C1202.0649%2C1202.6351%2C1202.3735%2C1202.5545%2C1202.5537%2C1202.0472%2C1202.1081%2C1202.1672%2C1202.2643%2C1202.0160%2C1202.5594%2C1202.3399%2C1202.1970%2C1202.1417%2C1202.5327%2C1202.2195%2C1202.6445%2C1202.3443%2C1202.2467%2C1202.5553%2C1202.6173%2C1202.4966%2C1202.4380%2C1202.1511%2C1202.4776%2C1202.1169%2C1202.4522%2C1202.5768%2C1202.3389%2C1202.3575%2C1202.5380%2C1202.2869%2C1202.5610%2C1202.5150%2C1202.2674%2C1202.3347%2C1202.6023%2C1202.1467%2C1202.1558%2C1202.0165%2C1202.3368%2C1202.5993%2C1202.2776%2C1202.3640%2C1202.1087%2C1202.5372%2C1202.1659%2C1202.1189%2C1202.3004%2C1202.1823%2C1202.0630%2C1202.1312%2C1202.6189%2C1202.2894%2C1202.1879%2C1202.5379%2C1202.5636%2C1202.6640%2C1202.4964%2C1202.0586%2C1202.5844%2C1202.6209%2C1202.0796%2C1202.1673%2C1202.4794%2C1202.2088%2C1202.3003%2C1202.6069%2C1202.5286%2C1202.5121%2C1202.4575%2C1202.4675%2C1202.1154%2C1202.3626&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper, we study the problem of decomposing a superposition of a\nlow-rank matrix and a sparse matrix when a relatively few linear measurements\nare available. This problem arises in many data processing tasks such as\naligning multiple images or rectifying regular texture, where the goal is to\nrecover a low-rank matrix with a large fraction of corrupted entries in the\npresence of nonlinear domain transformation. We consider a natural convex\nheuristic to this problem which is a variant to the recently proposed Principal\nComponent Pursuit. We prove that under suitable conditions, this convex program\nguarantees to recover the correct low-rank and sparse components despite\nreduced measurements. Our analysis covers both random and deterministic\nmeasurement models."}, "authors": ["Arvind Ganesh", "Kerui Min", "John Wright", "Yi Ma"], "author_detail": {"name": "Yi Ma"}, "author": "Yi Ma", "arxiv_comment": "32 pages, preliminary version submitted to ISIT'12", "links": [{"href": "http://arxiv.org/abs/1202.6445v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.6445v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.6445v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.6445v1", "journal_reference": null, "doi": null, "fulltext": "Principal Component Pursuit with Reduced Linear\nMeasurements\nArvind Ganesh\u2217 , Kerui Min\u2217 , John Wright\u2020 , and Yi Ma\u2217,\u2021\n\narXiv:1202.6445v1 [cs.IT] 29 Feb 2012\n\n\u2217\n\nDept. of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign\n\u2020\n\nDept. of Electrical Engineering, Columbia University\n\u2021\n\nVisual Computing Group, Microsoft Research Asia\n\nAbstract\nIn this paper, we study the problem of decomposing a superposition of a low-rank matrix and a\nsparse matrix when a relatively few linear measurements are available. This problem arises in many\ndata processing tasks such as aligning multiple images or rectifying regular texture, where the goal\nis to recover a low-rank matrix with a large fraction of corrupted entries in the presence of nonlinear\ndomain transformation. We consider a natural convex heuristic to this problem which is a variant to\nthe recently proposed Principal Component Pursuit. We prove that under suitable conditions, this\nconvex program guarantees to recover the correct low-rank and sparse components despite reduced\nmeasurements. Our analysis covers both random and deterministic measurement models.\n\n1\n\nIntroduction\n\nLow-rank matrix recovery and approximation has been a popular area of research in many different\nfields. The popularity of low-rank matrices can be attributed to the fact that they arise in one of\nthe most commonly used data models in real applications, namely when very high-dimensional data\nsamples are assumed to lie approximately on a low-dimensional linear subspace. This model has been\nsuccessfully employed in various problems such as face recognition [1], system identification [2], and\ninformation retrieval [3], for instance.\nThe most popular tool for low-rank matrix approximation is the Principal Component Analysis (PCA)\n[4, 5]. The basic idea of PCA is to find the \"best low-rank approximation\" (in an l2 -sense) to a given\ninput matrix. Essentially, PCA finds a rank-r approximation to a given data matrix D \u2208 Rm\u00d7n by\nsolving the following problem:\nmin kD \u2212 Lk s.t. rank(L) \u2264 r,\nL\n\nwhere k * k denotes the matrix spectral norm. It is well-known that the solution to this problem can\nbe easily obtained by computing the Singular Value Decomposition (SVD) of D and retaining only the\nr largest singular values and the corresponding singular vectors. Besides the ease of computation, the\nPCA estimate has been shown to be optimal in the presence of isotropic Gaussian noise. However, the\nbiggest drawback of PCA is that it breaks down even when one entry of the matrix is corrupted by an\nerror of very large magnitude. Unfortunately, such large-magnitude, non-Gaussian errors often exist in\nreal data. For instance, occlusions in images corrupt only a fraction of the pixels in an image, but the\nmagnitude of corruption can be quite large.\nThere have been many works in the literature that try to make PCA robust to such gross, nonGaussian errors and many models and solutions have been proposed. We here consider the specific\nproblem of recovering a low-rank matrix L0 \u2208 Rm\u00d7n from corrupted observations D = L0 + S0 , where\nS0 \u2208 Rm\u00d7n is a sparse matrix whose non-zero entries may have arbitrary magnitude. This problem has\nbeen studied in detail recently by various works in the literature [6, 7, 8]. It has been shown that under\n\n1\n\n\frather broad conditions, the following convex program succeeds in recovering L0 from D:\nmin kLk\u2217 + \u03bbkSk1\nL,S\n\ns.t. D = L + S,\n\n(1)\n\nwhere k * k\u2217 denotes the nuclear norm1 , k * k1 denotes the l1 -norm2 , and \u03bb > 0 is a weighting factor. This\nmethod has been dubbed Principal Component Pursuit (PCP) in [6]. In addition to being computationally tractable, it comes with very strong theoretical guarantees of recovery. Furthermore, follow-up\nworks have shown that PCP is stable in the presence of additive Gaussian noise [9] and can recover L0\neven when the corruption matrix S0 is not so sparse [10].\nBesides being of theoretical interest, this convex optimization framework for low-rank matrix recovery\nhas been employed very successfully to solve real problems in computer vision such as photometric stereo\n[11]. However, in practice, much more data, especially imagery data, can be viewed as low-rank only\nafter some transformation is applied. For instance, an image of a building facade will become a low-rank\nmatrix after the perspective distortion is rectified [12] or a set of face images of the same person will\nbecome linearly correlated only after they are proper aligned [13]. With our terminology here, we can\nwrite as D \u25e6 \u03c4 = L0 + S0 where \u03c4 belongs to certain transformation group. As the transformation \u03c4 is\nalso unknown, one natural way to recover L0 , S0 and \u03c4 together is to approximate the nonlinear equation\nwith its linearization at the current estimate of \u03c4\u0302 :\nD \u25e6 \u03c4\u0302 +\n\np\nX\n\nJi d\u03c4i = L + S,\n\ni=1\n\nwhere {Ji } is the Jacobian of D \u25e6 \u03c4 with respect to the parameters {\u03c4i } of \u03c4 . Then one can incrementally\nupdate the estimate for \u03c4 with \u03c4\u0302 + d\u03c4 by solving the following convex program:\nmin kLk\u2217 + \u03bbkSk1\n\nL,S,d\u03c4i\n\ns.t. D +\n\np\nX\n\nJi d\u03c4i = L + S.\n\n(2)\n\ni=1\n\nEmpirically this scheme has been shown to work rather effectively in practice in both the image rectification problem [12] and the image alignment problem [13].\nAlthough the convex program was proposed in the same spirit as PCP, we note that the linear\nconstraint is different, and hence, the theoretical guarantees for PCP shown in [6, 7, 8] do not directly\napply to this case. In this work, we attempt to fill the gap between theory and practice and try to\nunderstand under what conditions, the above extended version of PCP is expected to work correctly.\nLet Q be the linear subspace in Rm\u00d7n that is the orthogonal complement to the span of all the Ji 's,\nthen its dimension is q = mn \u2212 p. Clearly, we can rewrite the above program in the following form:\nmin kLk\u2217 + \u03bbkSk1\nL,S\n\ns.t. PQ D = PQ (L + S),\n\n(3)\n\nwhere PQ is the orthogonal projection onto the linear subspace Q. Clearly, this program is a variation\nto PCP (1) in which the number of linear constraints has been reduced from mn to q = mn \u2212 p. Indeed,\nif Q is the entire space, then it reduces to the PCP. If Q is a linear subspace of matrices with support in\n\u03a9 \u2286 [m] \u00d7 [n], then we have the special case of recovering L0 from D, when only a subset of the entries in\nD are available. This case is akin to the low-rank matrix completion problem [14, 15, 16], and theoretical\nguarantees have been derived in [6, 17]. However, to the best of our knowledge, the case with a general\nsubspace Q has not yet been analyzed in detail in the literature.\nOur motivation to study when the convex program (3) succeeds with such reduced linear constraints\nis at least twofold. First, the relationships between Q and L0 and S0 will provide us better understanding\nabout what type of images and signals for which techniques such as those used in [12, 13] are expected\nto work well. Second, we want to know how many general linear measurements we could reduce without\n1 The\n2 The\n\nsum of all singular values.\nsum of absolute values of all matrix entries.\n\n2\n\n\fsacrificing the robustness of PCP for recovering the low-rank matrix L0 . In these applications, the\nnumber of constraints reduced corresponds to the dimension of the transformation group. In the image\nrectification problem, the dimension of the transformation group p is typically fixed with respect to the\nsize of the matrix; in the image alignment problem, however, the dimension typically grows linearly in\nm (or n). In either case, we need to know if the program (3) can tolerate up to a constant fraction of\ngross errors.\n\n1.1\n\nNotation\n\nWe first establish a set of notations that will be used throughout this work. We will assume that the\nmatrices L0 , S0 and D in (3) have size m \u00d7 n. Without any loss of generality, we assume that n \u2264 m.\nWe denote the rank of L0 by r. Let L0 = U \u03a3V \u2217 be the reduced Singular Value Decomposition (SVD)\nof L0 . We define a linear subspace T as follows:\n.\nT = {U X \u2217 + Y V \u2217 : X \u2208 Rn\u00d7r , Y \u2208 Rm\u00d7r }.\n(4)\nBasically, T contains all matrices that share a common row space or column space with L0 . We denote\nby \u03a9 the support of S0 . By a slight abuse of notation, we also represent by \u03a9 the subspace of matrices\nwhose support is contained in the support of S0 . For any subspace S \u2286 Rm\u00d7n , PS : Rm\u00d7n \u2192 Rm\u00d7n\ndenotes the orthogonal projection operator onto S.\nP\nFor any X, Y \u2208 Rm\u00d7n , we define their inner product as hX, Y i = trace(X \u2217 Y ) = ij Xij Yij . We let\nk * kF and k * k denote the matrix Frobenius norm and spectral norm, respectively. We also denote the\nl\u221e -norm of a matrix X as kXk\u221e = maxij |Xij |. We say that an event E occurs with high probability if\nP[E c ] \u2264 C m\u2212\u03b1 , for some positive numerical constants C and \u03b1. Here, E c denotes the event complement\nto E.\n\n1.2\n\nMain Assumptions\n\nObviously, successful recovery is not always guaranteed except under proper assumptions on the lowrank L0 , sparse S0 , and the subspace Q involved. For instance, if the matrix L0 is itself a sparse matrix,\nthen there is a fundamental ambiguity in the solution to be recovered. Here, we outline some of our\nassumptions that we will use throughout this paper. The assumptions we make here on L0 and S0 are\nessentially the same as those for PCP [6]. For completeness, we list them below.\nWe assume that each entry of the matrix belongs to the support of the sparse matrix S0 independently\nwith probability \u03c1. We denote this as supp(S0 ) \u223c Ber(\u03c1). For simplicity, we assume the signs of the\nnonzero entries are also random.3 For the low-rank matrix L0 , we assume the subspace T defined in(4)\nis incoherent to the standard basis (and hence the sparse matrix S0 ). To be precise, let us denote the\nstandard basis in Rm and Rn by \u0113i and ej , respectively, where i \u2208 [m] and j \u2208 [n]. We assume (as in\n[14]) that\nr\n\u03bcr\n\u03bcr\n\u03bcr\n\u2217\n2\n\u2217\n\u2217\n2\n, max kV ej k2 \u2264\n, kU V k\u221e \u2264\n,\n(5)\nmax kU \u0113i k2 \u2264\nm\nn\nmn\nj\u2208[n]\ni\u2208[m]\nfor some \u03bc > 0 and for all (i, j) \u2208 [m] \u00d7 [n]. We recall that r = rank(L0 ). It follows from the above\nassumptions that for any (i, j) \u2208 [m] \u00d7 [n]\nr\n2\u03bcr\n\u2217\nkPT \u0113i ej kF \u2264\n.\n(6)\nn\nFurthermore, it can be shown that kPT \u22a5 Xk \u2264 kXk for any X \u2208 Rm\u00d7n .\nIn addition to the above assumptions, we define the following two properties of linear subspaces. We\nsay that a linear subspace S \u2286 Rm\u00d7n is\n3 The random sign assumption is not entirely necessary for obtaining the same qualitative results. One can follow the\nderandomization process in [6] to remove this assumption if needed.\n\n3\n\n\f\u2022 \u03bd-coherent if there exists an orthonormal basis {Gi } for S satisfying\nmax kGi k2 \u2264\ni\n\n\u2022 \u03b3-constrained if\n\n\u03bd\n.\nn\n\n(7)\n\nmax kPS \u0113i e\u2217j k2F \u2264 \u03b3.\ni,j\n\n(8)\n\nIf S is a random subspace, we say it is \u03bd-coherent and \u03b3-constrained if Eqns. (7) and (8) hold with high\nprobability, respectively.\nIn this paper, we will deal with two different assumptions on the subspace Q as outlined below. We\nwill see later that it is in fact convenient to make our assumptions on the subspace Q\u22a5 , rather than on\nQ itself. This is partly motivated from the model in (2) that was used in [12, 13], where the Ji 's are\nessentially a basis for Q\u22a5 . So, any assumptions on Q\u22a5 can be easily interpreted in terms of the Ji 's\nand this would help us make the connection to these applications more directly. We denote by p the\ndimension of the subspace Q\u22a5 .\n\u2022 Random subspace model. Let G1 , G2 , . . . , Gp \u2208 Rm\u00d7n be an orthonormal basis for Q\u22a5 . We\nassume that this basis set is chosen uniformly at random from all possible orthobasis sets of size\np in Rm\u00d7n . It can be shown that each of the Gi 's are identical in distribution to H/kHkF , where\nthe entries of H \u2208 Rm\u00d7n are i.i.d. according to a Gaussian distribution with mean 0 and variance\n1/mn.\n\u2022 Deterministic subspace model. Under this model, we assume that Q\u22a5 is a fixed subspace\nwhich is \u03bd-coherent, for some \u03bd \u2265 1.\n\n1.3\n\nMain Results\n\nWith the above notation, we now briefly describe the main results we prove in this work. Although\nour results and proof methodology resemble those in [6], there are some important differences here.\nParticularly, we will see that the assumptions we make on the subspace Q greatly influences the kind of\nguarantees for recovery that can be derived.\nAs mentioned earlier, we will consider two different assumptions on the subspace Q. In the first one,\nwe assume a random subspace model for Q\u22a5 . The main result that we prove in this work under this\nrandom subspace model is summarized as the following theorem.\nTheorem 1 (Random Reduction). Fix any Cp > 0, and let Q\u22a5 be a p-dimensional random subspace\nof Rm\u00d7n (n \u2264 m), L0 a rank-r, \u03bc-incoherent matrix, and supp(S0 ) \u223c Ber(\u03c1). Then, provided that\nr < Cr\n\nn\n,\n\u03bc log2 m\n\np < Cp n,\n\n\u03c1 < \u03c10 ,\n\n(9)\n\nwith high probability (L0 , S0 ) is the unique optimal solution to (3) with \u03bb = m\u22121/2 . Here, Cr > 0 and\n\u03c10 \u2208 (0, 1) are numerical constants.\nRemark 1. In Theorem 1, \"with high probability\" means with probability at least 1 \u2212 \u03b2(Cp )m\u2212c , with\nc > 0 numerical.\nThe scaling in this result covers several applications of interest: in [12], p is a fixed constant, while in\n[13], p scales linearly with n. Therefore, the above result already covers both these applications in terms\nof the number of reduced constraints. It states that with such reduced constraints, the convex program\n(3) can recover the low-rank matrix L0 essentially under the same conditions as PCP. In particular, it\ncan tolerate up to a constant fraction of errors.\nIn a work that is closely related to this one [18], we have shown that one can expect the convex\nprogram (3) to work under much more highly compressive scenario. More precisely, the dimension of\n4\n\n\fthe subspace Q only needs to be on the order of (mr + k) log2 m which is only a polylogarithmic factor\nmore than the intrinsic degrees of freedom of the unknown L0 and S0 . One nice feature about the work\nof [18] is that the proof framework is very modular and the techniques are even applicable to more\ngeneral structured signals beyond low-rank and sparse ones. Nevertheless, that result does not subsume\nthe result here because in such highly compressive scenario, we cannot expect to tolerate error up to a\nconstant fraction of the matrix entries. Obtaining the results in Theorem 1 and Theorem 2 seems to\nrequire arguments that are specially tailored to the PCP problem.\nThere is a common limitation for all results that are based on a random assumption for Q or Q\u22a5 : the\nrandom assumption does not hold in many real applications. For instance, in [12, 13], the subspace Q\u22a5\nis typically spanned by a set of image Jacobians, which may not behave like random matrices. Therefore,\nit is desirable to have deterministic conditions on Q\u22a5 (or Q) that can be verified for the given data.\nWe need theoretical guarantees for recovery when Q\u22a5 is a deterministic subspace. This is the second\nscenario that we will consider in this work, for which we have the following result:\nTheorem 2 (Deterministic Reduction). Fix any p \u2208 Z+ , \u03b1 \u2265 1, and \u03bd \u2265 1. Then there exists\nCr > 0 such that if Q\u22a5 is a \u03bd-coherent p-dimensional subspace of Rm\u00d7n (n \u2264 m \u2264 \u03b1n), L0 is a rank-r, \u03bcincoherent matrix, and supp(S0 ) \u223c Ber(\u03c1), with high probability (L0 , S0 ) is the unique optimal solution\nto (3) with \u03bb = m\u22121/2 , provided that\n(\u0012\n)\n\u00131/2 \u0012\n\u00131/3\nn\nn\nn\nr < Cr min\n, \u03c1 < \u03c10 ,\n(10)\n,\n,\n\u03bd 2 p2 \u03b1\n\u03b1\u03bd\u03bcp\n\u03bc log m\nwhere Cr , \u03c10 \u2208 (0, 1) are numerical constants.\nRemark 2. Here, \"with high probability\" means with probability at least 1 \u2212 \u03b2(p, \u03b1, \u03bd)m\u2212c , with c > 0\nnumerical.\nThe \u03bd-coherence condition essentially requires there exists an orthonormal basis for Q\u22a5 whose spectral\nnorms are bounded above by O(n\u22121/2 ). This is a condition that can be verified directly once the subspace\nQ or Q\u22a5 is given (say as the span of the Jacobians). This condition is also significantly weaker than the\nrandom subspace assumption in Theorem 1.\nBecause the assumptions are weaker, the orders of growth in Theorem 2, quite a bit more restrictive\nthan those in Theorem 1. Nevertheless, this result can be very useful for the practical problems that\nwe encountered in image rectification where the dimension of the transformation group is typically fixed\n(i.e. does not change with the matrix dimension). Theorem 2 suggests we should expect the program to\nwork at least for deformation groups whose dimension is fixed. Although empirical results suggest that\nit could even grow as O(n), we leave that for future investigation.\nThe remainder of this paper is organized as follows: In Section 2, we derive the optimality\nconditions for (L0 , S0 ) to be the optimal solution to the convex program (3). In particular, we derive the\nconditions that a certain dual certificate must satisfy that would establish our main result. In Section\n3, we provide a constructive procedure for the aforementioned dual certificate. In Section 4, we describe\nour main assumptions and the detailed steps of the proof of Theorem 1. In Section 5, we outline the\nproof of Theorem 2. Although the proof for both the deterministic case will follow a common strategy as\nthe random case, there are a few important differences. In particular, we will highlight the parts where\nthe proof deviates significantly from that of Theorem 1.\n\n2\n\nExistence of Dual Certificate\n\nIn this section, we prove the following lemma that establishes necessary and sufficient conditions for\n(L0 , S0 ) to be the optimal solution to (3).\n\n5\n\n\fLemma 1. Assume that dim(Q\u22a5 \u2295 T \u2295 \u03a9) = dim(Q\u22a5 )+ dim(T )+ dim(\u03a9). (L0 , S0 ) is the unique optimal\nsolution to (3) if there exists a pair (W, F ) \u2208 Rm\u00d7n \u00d7 Rm\u00d7n satisfying\nU V \u2217 + W = \u03bb(sgn(S0 ) + F ) \u2208 Q,\n\n(11)\n\nwith PT W = 0, kW k < 1, P\u03a9 F = 0, and kF k\u221e < 1.\nProof. Consider a feasible solution to (3) of the form (L0 + HL , S0 \u2212 HS ). Clearly, we have that PQ HL =\nPQ HS . Under the conditions mentioned in the lemma, we will show that this pair does not minimize\nthe cost function in (3), unless HL = HS = 0.\nWe first use the fact that k * k\u2217 and k * k1 are convex functions. Consider any pair (W0 , F0 ) \u2208\nRm\u00d7n \u00d7 Rm\u00d7n satisfying PT W0 = 0, kW0 k \u2264 1, P\u03a9 F0 = 0, and kF0 k\u221e \u2264 1. Then, U V \u2217 + W0 is a\nsubgradient to k * k\u2217 at L0 , and sgn(S0 ) + F0 is a subgradient to k * k1 at S0 . Therefore,\nkL0 + HL k\u2217 + \u03bbkS0 \u2212 HS k1 \u2265 kL0 k\u2217 + \u03bbkS0 k1 + hU V \u2217 + W0 , HL i \u2212 \u03bbhsgn(S0 ) + F0 , HS i.\nBy H\u00f6lder's inequality (and the duality of norms), it is possible to choose W0 and F0 such that\nhW0 , HL i = kPT \u22a5 HL k\u2217 ,\n\nhF0 , HS i = \u2212kP\u03a9\u22a5 HS k1 .\n\nThen, we have\nkL0 + HL k\u2217 + \u03bbkS0 \u2212 HS k1\n\n\u2265\n\nkL0 k\u2217 + \u03bbkS0 k1 + hU V \u2217 , HL i \u2212 \u03bbhsgn(S0 ), HS i\n+kPT \u22a5 HL k\u2217 + \u03bbkP\u03a9\u22a5 HS k1 .\n\nBy assumption, we have\nU V \u2217 = \u03bb(sgn(S0 ) + F ) \u2212 W,\n\nwith \u03bb(sgn(S0 ) + F ) \u2208 Q. Substituting for U V \u2217 and using PQ HL = PQ HS , we get\nhU V \u2217 , HL i = \u03bbhsgn(S0 ), HS i + \u03bbhF, HS i \u2212 hW, HL i.\nSubstituting this in the above inequality, we get\nkL0 + HL k\u2217 + \u03bbkS0 \u2212 HS k1\n\n\u2265 kL0 k\u2217 + \u03bbkS0 k1 + kPT \u22a5 HL k\u2217 + \u03bbkP\u03a9\u22a5 HS k1\n+\u03bbhF, HS i \u2212 hW, HL i.\n\nLet \u03b2 = max{kW k, kF k\u221e} < 1. Using H\u00f6lder's inequality, we get\nkL0 + HL k\u2217 + \u03bbkS0 \u2212 HS k1\n\n\u2265\n\nkL0 k\u2217 + \u03bbkS0 k1 + (1 \u2212 \u03b2)kPT \u22a5 (HL )k\u2217\n+(1 \u2212 \u03b2)\u03bbkP\u03a9\u22a5 (HS )k1 .\n\nFor non-zero HL , HS , the last term on the right hand side above can be zero only if HL \u2208 T \\{0} and\nHS \u2208 \u03a9\\{0}. Since \u03a9 \u2229 T = {0}, HL 6= HS . We also have PQ (HL \u2212 HS ) = 0. This implies that\nHL \u2212 HS \u2208 Q\u22a5 , which is a contradiction since Q\u22a5 \u2229 (T \u2295 \u03a9) = {0}. Thus, we have\nkL0 + HL k\u2217 + \u03bbkS0 \u2212 HS k1 > kL0 k\u2217 + \u03bbkS0 k1 ,\nfor any non-zero feasible perturbation (HL , HS ).\nIt is often convenient to relax the equality constraints on the dual certificate given in (11). Thus,\nsimilar to the proof outline in [6, 16], we now provide a slightly relaxed dual certificate condition.\nFact 1. Let S1 and S2 be two linear subspaces in Rm\u00d7n with S1 \u2286 S2 . Then, for any X \u2208 Rm\u00d7n , we\nhave PS1 X = PS1 PS2 X, and consequently, kPS1 XkF \u2264 kPS2 XkF .\n6\n\n\fLemma 2. Suppose that dim(Q\u22a5 \u2295 T \u2295 \u03a9) = dim(Q\u22a5 ) + dim(T ) + dim(\u03a9). Let \u0393 = Q \u2229 T \u22a5 so that\n\u0393\u22a5 = Q\u22a5 \u2295 T . Assume that kP\u03a9 P\u0393\u22a5 k < 1/2 and \u03bb < 1. Then, (L0 , S0 ) is the unique optimal solution\nto (3) if there exists a pair (W, F ) \u2208 Rm\u00d7n \u00d7 Rm\u00d7n satisfying\nU V \u2217 + W = \u03bb(sgn(S0 ) + F + P\u03a9 D) \u2208 Q,\n\n(12)\n\nwith PT W = 0, kW k < 1/2, P\u03a9 F = 0, kF k\u221e < 1/2, and kP\u03a9 DkF \u2264 1/4.\nProof. Proceeding along the same lines as in the proof of Lemma 1, for any feasible perturbation\n(HL , HS ), we get\nkL0 + HL k\u2217 + \u03bbkS0 \u2212 HS k1\n\n1\nkL0 k\u2217 + \u03bbkS0 k1 + kPT \u22a5 HL k\u2217\n2\n\u03bb\n+ kP\u03a9\u22a5 HS k1 + \u03bbhP\u03a9 D, HS i\n2\n1\nkL0 k\u2217 + \u03bbkS0 k1 + kPT \u22a5 HL k\u2217\n2\n\u03bb\n\u03bb\n+ kP\u03a9\u22a5 HS k1 \u2212 kP\u03a9 HS kF .\n2\n4\n\n\u2265\n\n\u2265\n\nWe note that\nkP\u03a9 HS kF\n\n\u2264\n\u2264\n\u2264\n\u2264\n\nkP\u03a9 P\u0393 HS kF + kP\u03a9 P\u0393\u22a5 HS kF\n1\nkP\u03a9 P\u0393 HL kF + kHS kF\n2\n1\n1\nkP\u0393 HL kF + kP\u03a9 HS kF + kP\u03a9\u22a5 HS kF\n2\n2\n1\n1\nkPT \u22a5 HL kF + kP\u03a9 HS kF + kP\u03a9\u22a5 HS kF .\n2\n2\n\nIn the second step above, we have used the fact that P\u0393 HL = P\u0393 HS (since \u0393 \u2286 Q), and the final\ninequality follows from Fact 1. Thus, we have\nkP\u03a9 HS kF \u2264 2kPT \u22a5 HL kF + kP\u03a9\u22a5 HS kF \u2264 2kPT \u22a5 HL k\u2217 + kP\u03a9\u22a5 HS k1 .\nPutting it all together, we get\nkL0 + HL k\u2217 + \u03bbkS0 \u2212 HS k1 \u2265 kL0 k\u2217 + \u03bbkS0 k1 +\n\n\u03bb\n1\u2212\u03bb\nkPT \u22a5 HL k\u2217 + kP\u03a9\u22a5 HS k1 .\n2\n4\n\nThe desired result follows from the fact that Q\u22a5 \u2229 (T \u2295 \u03a9) = {0}.\n\n3\n\nProof Strategy\n\nBy Lemma 2, in order for us to prove either Theorem 1 or 2, it is sufficient to produce a dual certificate\nW \u2208 Rm\u00d7n satisfying\n\uf8f1\nW \u2208 T \u22a5,\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 PQ\u22a5 W = \u2212PQ\u22a5 (U V \u2217 ),\nkW k < 1/2,\n(13)\n\uf8f4\n\u2217\n\uf8f4\nkP\n(U\nV\n\u2212\n\u03bbsgn(S\n)\n+\nW\n)k\n\u2264\n\u03bb/4,\n\uf8f4\n\u03a9\n0\nF\n\uf8f4\n\uf8f3\nkP\u03a9\u22a5 (U V \u2217 + W )k\u221e < \u03bb/2.\n\nTo prove Theorems 1 and 2 under the above conditions, we try to construct the dual certificate W\nby following a similar strategy as that in the original PCP [6]. However, the extra projection of the\nobservations onto the subspace Q adds significant difficulty to various technical parts of the proof. In\n7\n\n\fthis section, we will outline the basic components for constructing such a certificate and then provide\ndetailed proofs for each of the component in next sections. For simplicity, throughout our discussion\n.\nbelow, we set \u0393 = Q \u2229 T \u22a5 so that \u0393\u22a5 = Q\u22a5 \u2295 T .\nAs the support of the sparse matrix is distributed as \u03a9 \u223c Ber(\u03c1) for some small \u03c1 \u2208 (0, 1). This is,\nof course, equivalent to assuming that \u03a9c \u223c Ber(1 \u2212 \u03c1). Suppose that \u03a91 , \u03a92 , . . . , \u03a9j0 are independent\nSj0\nsupport sets such that \u03a9j \u223c Ber(q) for all j. Then, \u03a9c and j=1\n\u03a9j have the same probability distribution\n.\nif \u03c1 = (1 \u2212 q)j0 . We now propose a construction for the dual certificate W = W L + W S + W Q as follows.\nWe use a combination of the golfing scheme proposed in [16] and the least norm approach.\n1. Construction of W L using the golfing scheme. Starting with Y0 = 0, we iteratively define\nYj = Yj\u22121 + q \u22121 P\u03a9j P\u0393\u22a5 (U V \u2217 \u2212 Yj\u22121 ),\n\n(14)\n\nW L = P\u0393 Yj0 ,\n\n(15)\n\nand set\nwhere j0 = \u23082 log m\u2309.\n2. Construction of W S by least norm solution. We define W S by the following least norm problem:\nWS\n=\nsubj. to\n\narg minX kXkF\nP\u03a9 X = \u03bbsgn(S0 )\nP\u0393\u22a5 X = 0.\n\n(16)\n\n3. Construction of W Q by least squares. We define W Q by the following least squares problem:\nWQ\nsubj. to\n\n=\n\narg minX kXkF\nPQ\u22a5 X = \u2212PQ\u22a5 (U V \u2217 )\nP\u03a0 X = 0,\n\n(17)\n\nwhere \u03a0 = \u03a9 \u2295 T .\nWe note that under our assumptions (see Section 1.2), both the least squares programs above are\nfeasible with high probability under both the random subspace model and the deterministic subspace\nmodel. This is because we will later show that the spectral norms of the linear operators P\u03a9 P\u0393\u22a5 and\nPQ\u22a5 P\u03a0 can be bounded below unity with high probability.\nThus, to prove that W L + W S + W Q is a valid dual certificate, we have to establish the following:\nkW L + W S + W Q k < 1/2,\nkP\u03a9 (U V \u2217 + W L )kF \u2264 \u03bb/4,\n\nkP\u03a9\u22a5 (U V \u2217 + W L + W S + W Q )k\u221e < \u03bb/2.\n\n(18)\n(19)\n(20)\n\nLemma 3. Assume that \u03a9 \u223c Ber(\u03c1) for some small \u03c1 \u2208 (0, 1) and the assumptions (5) and (7) hold\ntrue. Then, the matrix W L obeys, with high probability,\n1. kW L k < 1/4,\n2. kP\u03a9 (U V \u2217 + W L )kF < \u03bb/4,\n3. kP\u03a9\u22a5 (U V \u2217 + W L )k\u221e < \u03bb/4.\nLemma 4. In addition to the assumptions in the previous lemma, assume that the signs of the non-zero\nentries of S0 are i.i.d. random. Then, the matrix W S obeys, with high probability,\n1. kW S k < 1/8,\n8\n\n\f2. kP\u03a9\u22a5 W S k\u221e < \u03bb/8.\nLemma 5. Assume that \u03a9 \u223c Ber(\u03c1) for some small \u03c1 \u2208 (0, 1) and the assumptions (5) and (7) hold\ntrue. Then, the matrix W Q obeys, with high probability,\n1. kW Q k < 1/8,\n2. kP\u03a9\u22a5 W Q k\u221e < \u03bb/8.\nThe above lemmas together establish a valid dual certificate that satisfies Eqn. (18) to Eqn. (20).\n\n4\n\nRandom Reduction: Proof of Theorem 1\n\nIn this section, we provide a detailed proof of Lemmas 3, 4, and 5 for the case when Q is a random\nsubspace. Before proceeding to the main steps of the proof, we first establish some important properties\nand relationships among the different quantities involved in the problem.\n\n4.1\n\nPreliminaries\n\nLemma 6. Let Q\u22a5 be a linear subspace distributed according to the random subspace model described\nearlier. Then, for any (i, j) \u2208 [m] \u00d7 [n], with high probability,\nr\np log(mnp)\n\u2217\nkPQ\u22a5 \u0113i ej kF \u2264 4\n.\n(21)\nmn\nProof. For any (i, j) \u2208 [m] \u00d7 [n], we have\nv\nu p\nuX\n\u221a\n|hGk , e \u0304i e\u2217j i|2 \u2264 p max kGk k\u221e .\nkPQ\u22a5 \u0113i e\u2217j kF = t\nk\n\nk=1\n\n(22)\n\nWe now derive a bound for kGk k\u221e . Suppose that M \u2208 Rm\u00d7n is a random matrix whose entries are i.i.d.\naccording to the standard normal distribution. Let us define\n1\nM,\nH=\u221a\nmn\nand G = H/kHkF . Clearly, G is identical in distribution to G1 , G2 , . . . , Gp . We know that, for any\n(i, j) \u2208 [m] \u00d7 [n],\nr\n2\n2 e\u2212t /2\nP[|Mij | > t] \u2264\n.\n\u03c0\nt\nTherefore, using a union bound,we get\nr\n2 mn \u2212t2 /2\ne\n,\nP[kM k\u221e > t] \u2264\n\u03c0 t\nor equivalently,\n\u0014\n\u0015 r\n2 mn \u2212t2 /2\nt\nP kHk\u221e > \u221a\ne\n.\n\u2264\n\u03c0 t\nmn\n\nNow, if we have p random matrices H1 , H2 , . . . , Hp , independent and identical in distribution to H, then\n\u0015 r\n\u0014\n2 mnp \u2212t2 /2\nt\n\u221a\n\u2264\ne\n.\nP max kHk k\u221e >\nk\nmn\n\u03c0 t\n9\n\n\fSetting t =\n\np\n4 log(mnp), we get\n\"\n\nP max kHk k\u221e >\nk\n\nr\n\nThus, with high probability, we have that\n\n# r\n4 log(mnp)\n1\n1\np\n\u2264\n.\nmn\n2\u03c0 mnp log(mnp)\n\nmax kHk k\u221e \u2264\nk\n\nr\n\n4 log(mnp)\n.\nmn\n\nIt can be shown that kHk kF \u2265 1/2 with high probability. Thus, we have that\nr\n16 log(mnp)\n,\nmax kGk k\u221e \u2264\nk\nmn\nwith high probability. The desired result follows from Eqn. (22).\nLemma 7. Assume that p < mn/4. Let Q\u22a5 be a linear subspace distributed according to the random\nsubspace model. Then, with high probability, we have\n!\np\n\u221a\np + (m + n)r\n\u221a\n.\n(23)\nkPQ\u22a5 PT k \u2264 8\nmn\nProof. Firstly, we note that Q\u22a5 is identical in distribution to a subspace spanned by p independent\nrandom matrices, each of whose entries are i.i.d. according to a Gaussian distribution with mean zero\nand variance 1/mn. Let H : Rp \u2192 Rm\u00d7n be a linear operator defined as follows:\nH(x) =\n\np\nX\n\nxk Hk ,\n\nk=1\n\nwhere the Hk 's are independent random matrices each of whose entries are i.i.d. according to a Gaussian\ndistribution with mean zero and variance 1/mn. Then, we have that PQ\u22a5 has the same distribution as\nthe operator H(H\u2217 H)\u22121 H\u2217 . Therefore, we have\n!#\n\"\nr\nr\np\n(m + n)r\n+\nP kPQ\u22a5 PT k > 8\nmn\nmn\n\"\n!#\nr\nr\np\n(m + n)r\n\u2217\n\u22121 \u2217\n= P kH(H H) H PT k > 8\n+\nmn\nmn\n!#\n\"\nr\nr\np\n(m + n)r\n\u2217\n\u22121\n\u2217\n+\n\u2264 P kH(H H) kkH PT k > 8\nmn\nmn\n\"\n!#\nr\nr\n\u0002\n\u0003\np\n(m + n)r\n\u2217\n\u22121\n\u2217\n\u2264 P kH(H H) k > 4 + P kH PT k > 2\n.\n+\nmn\nmn\nSuppose that R \u2208 Rmn\u00d7p is a random matrix whose entries are i.i.d. according to a Gaussian\ndistribution with mean zero and variance 1/mn. It is easy to see that if we vectorize all the matrices,\nthen R is the matrix analogue\nkH(H\u2217 H)\u22121 k has the same distribution\n\u221a of the operator H. Therefore,\n\u2032\n\u22121\n\u2032\nas (\u03c3min (R)) . Let R = mn R. Clearly, the entries of R are i.i.d according to the standard normal\ndistribution. Using the concentration results for 1-Lipschitz functions (see Proposition 2.18 in [19]) and\nthe distribution of singular values of random Gaussian matrices [20], it is possible to show that\n\u0002\n\u0003\n\u221a\n2\n\u221a\nP \u03c3min (R\u2032 ) \u2264 mn \u2212 p \u2212 t \u2264 e\u2212t /2 ,\n10\n\n\ffor any t \u2265 0. Consequently, we have that\nr\n\u0014\n\u0015\n2\np\nP \u03c3min (R) \u2264 1 \u2212\n\u2212 t \u2264 e\u2212mnt /2 .\nmn\nSetting t = 1/4 and by our assumption that p < mn/4, we get\n\u0015\n\u0014\n\u0002\n\u0003\n1\n= P kH(H\u2217 H)\u22121 k \u2265 4 \u2264 e\u2212mn/32 .\nP \u03c3min (R) \u2264\n4\n\nWe now note that kH\u2217 PT k = kPT Hk is identical in distribution to kM k, where M \u2208 R(m+n)r\u00d7p is a\nrandom matrix whose entries are i.i.d. N (0, 1/mn). This is because the isotropic Gaussian distribution\nis rotation-invariant. Hence, without any loss of generality we can assume that the operator PT preserves\nonly the first dim(T ) = (m + n)r components of the basis elements H1 , . . . , Hp . Once again, invoking\nProposition 2.18 in [19], we can show that\n\"\n#\np\n\u221a\np + (m + n)r\n2\n\u221a\nP kM k \u2265\n+ t \u2264 e\u2212mnt /2 .\nmn\nSetting t = max\n\nnp\no\np\np/mn , (m + n)r/mn , it follows that\n\n!#\np\n\u221a\np + (m + n)r\n\u221a\nP kM k \u2265 2\nmn\n!#\n\"\np\n\u221a\np\n+\n(m\n+\nn)r\n\u221a\nP kH\u2217 PT k \u2265 2\nmn\nn\no\nmin e\u2212p/2 , e\u2212(m+n)r/2 .\n\"\n\n=\n\u2264\nPutting it all together, we get\n\n\"\n\nThus, we have that\n\nr\n\nr\n\n(m + n)r\nP kPQ\u22a5 PT k > 8\nmn\nn\no\n\u2264 e\u2212mn/32 + min e\u2212p/2 , e\u2212(m+n)r/2 .\nkPQ\u22a5 PT k \u2264 8\n\np\n+\nmn\n\n!#\n\n!\np\n\u221a\np + (m + n)r\n\u221a\nmn\n\nwith high probability.\nLemma 8. Let Q\u22a5 be a linear subspace distributed according to the random subspace model and\n\u03a9 \u223c Ber(\u03c1). Then, with high probability, we have\nr !\nr\np\n5\u03c1\nkPQ\u22a5 P\u03a9 k \u2264 8\n.\n(24)\n+\nmn\n4\nProof. Proceeding along the same lines of the proof of the previous lemma and conditioned on \u03a9, we get\n\"\n#\nr !\nr\np\n5\u03c1\n5\nP kPQ\u22a5 P\u03a9 k > 8\n|\u03a9| \u2264 \u03c1mn\n+\nmn\n4\n4\no\nn\n\u2264 e\u2212mn/32 + min e\u2212p/2 , e\u22125mn\u03c1/8 .\n11\n\n\fUsing Bernstein's inequality, it is possible to show that\nmn\u03c1\u03b4 2\nP [|\u03a9| > mn\u03c1(1 + \u03b4)] \u2264 2 exp \u2212\n1 \u2212 \u03c1 + 2\u03b4\n3\n\n!\n\n\u0012\n\u0013\n3\n2\n\u2264 2 exp \u2212 mn\u03c1\u03b4 ,\n5\n\nfor any \u03b4 \u2208 (0, 1). We set \u03b4 = 1/4. Thus, we have\n\"\nr !#\nr\np\n5\u03c1\nP kPQ\u22a5 P\u03a9 k > 8\n+\nmn\n4\n#\n\"\nr !\nr\n\u0014\n\u0015\n5\n5\np\n5\u03c1\n|\u03a9| \u2264 \u03c1mn + P |\u03a9| > \u03c1mn\n+\n\u2264 P kPQ\u22a5 P\u03a9 k > 8\nmn\n4\n4\n4\no\nn\n\u2264 e\u2212mn/32 + min e\u2212p/2 , e\u22125mn\u03c1/8 + 2 e\u22123mn\u03c1/80 .\nThus, we have that\n\nkPQ\u22a5 P\u03a9 k \u2264 8\n\nr\n\np\n+\nmn\n\nr\n\n5\u03c1\n4\n\n!\n\nwith high probability.\nLemma 9. Let \u03a9 \u223c Ber(\u03c1). Then, with high probability,\nkP\u03a9 PT k2 \u2264 \u03c1 + \u01eb,\n\n(25)\n\nm\nprovided that 1 \u2212 \u03c1 \u2265 C0 \u01eb\u22122 \u03bcr log\nfor some numerical constant C0 > 0.\nn\n\nProof. See Corollary 2.7 in [6].\nWe now prove the following two results that would help us establish incoherence relations with\nsubspaces obtained by a direct sum of two incoherent subspaces.\nLemma 10. Let S1 and S2 be any two linear subspaces in Rm\u00d7n satisfying kPS1 PS2 k \u2264 \u03b1 < 1. We\ndefine S = S1 \u2295 S2 . Then, for any X \u2208 Rm\u00d7n , we have\nkPS Xk2F \u2264 (1 \u2212 \u03b1)\u22121 (kPS1 Xk2F + kPS2 Xk2F ).\n\n(26)\n\nProof. We denote by vec : Rm\u00d7n \u2192 Rmn , the operation of converting a matrix to a vector by stacking\nits columns one below another. Suppose that d1 and d2 are the dimensions of the subspaces S1 and\nS2 , respectively. Then, there exist matrices B1 \u2208 Rmn\u00d7d1 and B2 \u2208 Rmn\u00d7d2 whose columns constitute\northonormal bases for S1 and S2 , respectively.\n.\nLet M = [B1 B2 ]. Clearly, the columns of M constitute a basis for the subspace S in Rmn . Hence,\nfor any X \u2208 Rm\u00d7n , its projection onto S can be expressed as follows:\nvec(PS X) = M (M \u2217 M )\u22121 M \u2217 vec(X).\nWe note that kB1\u2217 vec(X)k2 = kPS1 XkF and kB2\u2217 vec(X)k2 = kPS2 XkF . Therefore,we have\nkPS Xk2F\n\n=\n=\n\u2264\n=\n\nkvec(PS X)k22\nkM (M \u2217 M )\u22121 M \u2217 vec(X)k22\n\nkM (M \u2217 M )\u22121 k2 * kM \u2217 vec(X)k22\nkM (M \u2217 M )\u22121 k2 * (kPS1 Xk2F + kPS2 Xk2F )\n12\n\n\f.\nLet M \u2020 = (M \u2217 M )\u22121 M \u2217 denote the Moore-Penrose pseudoinverse of M . It is evident that kM \u2020 k =\n\u2217\nkM (M M )\u22121 k. But we know that kM \u2020 k = (\u03c3min (M ))\u22121 , where \u03c3min (M ) is the smallest non-zero\nsingular value of M . Using the fact that B1 and B2 have orthonormal columns, we can show that\n(\u03c3min (M ))2 = \u03bbmin (M \u2217 M ) \u2265 1 \u2212 \u03b1, where \u03bbmin (M \u2217 M ) is the smallest eigenvalue of M \u2217 M .4 Therefore,\nwe have\nkPS Xk2F \u2264 (\u03c3min (M ))\u22122 (kPS1 Xk2F + kPS2 Xk2F )\n\u2264 (1 \u2212 \u03b1)\u22121 (kPS1 Xk2F + kPS2 Xk2F ).\nSuppose that kPQ\u22a5 PT k < 1/2.5 Then, it follows that\n\u0013\n\u0012\n8p log(mnp) \u03bcr\n,\n+\nkP\u0393\u22a5 \u0113i e\u2217j k2F \u2264 4\nmn\nn\n\n(27)\n\nwith high probability, for all (i, j) \u2208 [m] \u00d7 [n]. In other words, with high probability, when Q\u22a5 is\ndistributed according\nto the \u0011random subspace model, we have that the subspace \u0393\u22a5 is \u03b3-constrained\n\u0010\n8p log(mnp)\nwith \u03b3 = 4\n+ \u03bcr\nmn\nn . We further note that \u03b3 log m = O(1/ log m) under the conditions of\nTheorem 1. This fact will be used frequently in our proof below.\nLemma 11. Let S1 , S2 and S3 be any three linear subspaces in Rm\u00d7n satisfying dim(S1 \u2295 S2 \u2295 S3 ) =\ndim(S1 )+ dim(S2 )+ dim(S3 ), and kPS1 PS2 k \u2264 \u03b11,2 < 1, kPS2 PS3 k \u2264 \u03b12,3 < 1 and kPS3 PS1 k \u2264 \u03b13,1 < 1.\nWe define S = S1 \u2295 S2 . Then, we have\ns\n\u03b122,3 + \u03b123,1\n.\n(28)\nkPS PS3 k \u2264\n1 \u2212 \u03b11,2\nProof. The proof is a simple application of Lemma 10. We note that, for any X \u2208 Rm\u00d7n ,\nkPS PS3 Xk2F\n\n\u2264 (1 \u2212 \u03b11,2 )\u22121 (kPS1 PS3 Xk2F + kPS2 PS3 Xk2F\n\u2264 (1 \u2212 \u03b11,2 )\u22121 (kPS1 PS3 k2 + kPS2 PS3 k2 )kXk2F\n\u2264 (1 \u2212 \u03b11,2 )\u22121 (\u03b123,1 + \u03b122,3 )kXk2F .\n\nIt follows that\nkPS PS3 k \u2264\n\ns\n\n\u03b122,3 + \u03b123,1\n.\n1 \u2212 \u03b11,2\n\nLemma 12. Let \u03a9 \u223c Ber(\u03c1) and \u0393\u22a5 be \u03b3-constrained. Then, for any \u01eb \u2208 (0, 1), with high probability,\nkP\u0393\u22a5 \u2212 \u03c1\u22121 P\u0393\u22a5 P\u03a9 P\u0393\u22a5 k \u2264 \u01eb,\n\n(29)\n\nprovided that \u03c1 \u2265 C * \u01eb\u22122 \u03b3 log m for some numerical constant C > 0.\nProof. The proof is very similar to that of Theorem 4.1 in [14]. We highlight the main steps here. For\neach (i, j) \u2208 [m] \u00d7 [n], we define binary random variables \u03b4ij , each takes value 1 if (i, j) \u2208 \u03a9, and 0\notherwise. We note that\nX\n\u03b4ij P\u0393\u22a5 \u0113i e\u2217j \u2297 P\u0393\u22a5 \u0113i e\u2217j ,\nP\u0393\u22a5 P\u03a9 P\u0393\u22a5 =\nij\n\nE[P\u0393\u22a5 P\u03a9 P\u0393\u22a5 ] =\n\n4 Since\n5 From\n\n\u03c1 P\u0393\u22a5 ,\n\nM has full column rank, M \u2217 M is positive definite.\nLemma 7 and the assumptions of Theorem 1, this is true with high probability for sufficiently large m, n.\n\n13\n\n\fwhere \u2297 denotes the outer or tensor product between matrices. Applying a concentration result for\noperators of the above form, as established in [21], we have, with high probability,\ns\nlog(mn)\nmax kP\u0393\u22a5 \u0113i e\u2217j kF\n(30)\nkP\u0393\u22a5 \u2212 \u03c1\u22121 P\u0393\u22a5 P\u03a9 P\u0393\u22a5 k \u2264 C \u2032\nij\n\u03c1\ns\n\u03b3 log(mn)\n\u2032\n\u2264C\n,\n(31)\n\u03c1\nprovided that the right hand side is smaller than 1. Here, C \u2032 > 0 is a numerical constant. The desired\nresult follows by noting that n \u2264 m, and bounding the right hand side by \u01eb \u2208 (0, 1).\nLemma 13. Let Z \u2208 \u0393\u22a5 be fixed, \u0393\u22a5 be \u03b3-constrained, and \u03a9 \u223c Ber(\u03c1). Then, with high probability,\nkZ \u2212 \u03c1\u22121 P\u0393\u22a5 P\u03a9 Zk\u221e \u2264 \u01ebkZk\u221e,\n\n(32)\n\nprovided that \u03c1 \u2265 C0 * \u01eb\u22122 \u03b3 log m for some numerical constant C0 > 64/3.\nProof. Let \u03b4ij be a sequence of independent Bernoulli random variables such that\n\u001a\n1, if (i, j) \u2208 \u03a9,\n\u03b4ij =\n0, otherwise.\n.\nWe define Z \u2032 = Z \u2212 \u03c1\u22121 P\u0393\u22a5 P\u03a9 Z. Then,\nX\nZ\u2032 =\n(1 \u2212 \u03c1\u22121 \u03b4ij )Zij P\u0393\u22a5 \u0113i e\u2217j .\nij\n\nFor any (i0 , j0 ) \u2208 [m] \u00d7 [n], we can express Zi\u20320 j0 as a sum of independent random variables as shown\nbelow:\nX\nRij , Rij = (1 \u2212 \u03c1\u22121 \u03b4ij )Zij hP\u0393\u22a5 \u0113i e\u2217j , \u0113i0 e\u2217j0 i.\nZi\u20320 j0 =\nij\n\nIt is easy to show that the Rij 's are zero-mean random variables with variance given by\nVar(Rij ) = (1 \u2212 \u03c1)\u03c1\u22121 |Zij |2 |hP\u0393\u22a5 \u0113i e\u2217j , \u0113i0 e\u2217j0 i|2 .\nTherefore,\nX\n\nVar(Rij ) =\n\n(1 \u2212 \u03c1)\u03c1\u22121\n\n\u2264\n\n\u22121\n\nij\n\n=\n\u2264\n\n(1 \u2212 \u03c1)\u03c1\n(1 \u2212\n\n(1 \u2212\n\nX\nij\n\n|Zij |2 |hP\u0393\u22a5 \u0113i e\u2217j , \u0113i0 e\u2217j0 i|2\n\nkZk2\u221e\n\nX\nij\n\n|h\u0113i e\u2217j , P\u0393\u22a5 \u0113i0 e\u2217j0 i|2\n\n\u03c1)\u03c1 kZk2\u221e kP\u0393\u22a5 \u0113i0 e\u2217j0 k2F\n\u03c1)\u03c1\u22121 \u03b3kZk2\u221e,\n\u22121\n\nwhere the last inequality holds with high probability. Furthermore, we have\n|Rij | \u2264 \u03c1\u22121 kZk\u221e |hP\u0393\u22a5 \u0113i e\u2217j , \u0113i0 e\u2217j0 i|\n\n\u2264 \u03c1\u22121 kZk\u221e kP\u0393\u22a5 \u0113i e\u2217j kF kP\u0393\u22a5 \u0113i0 e\u2217j0 kF\n\u2264 \u03c1\u22121 \u03b3kZk\u221e ,\n\nwith high probability. Thus, using Bernstein's inequality, we obtain\n\u0003\n\u0002\nP |Zi\u20320 j0 | > \u01ebkZk\u221e \u2264 2 exp \u2212\n14\n\n\u01eb2 \u03c1\n\u0001\n2\u03b3 3\u01eb + 1 \u2212 \u03c1\n\n!\n\n.\n\n\fChoosing \u01eb < 1, we can reduce the above expression to\n\u0012\n\u0013\n\u0003\n\u0002 \u2032\n3\u01eb2 \u03c1\nP |Zi0 j0 | > \u01ebkZk\u221e \u2264 2 exp \u2212\n.\n8\u03b3\n\nIf \u03c1 \u2265 C0 \u01eb\u22122 \u03b3 log m for some numerical constant C0 > 64/3, then we have\n\u0012\n\u0013\n\u0003\n\u0002\n3C0 log m\nP |Zi\u20320 j0 | > \u01ebkZk\u221e \u2264 2 exp \u2212\n.\n32\nApplying a union bound, we get\n\nP [kZ \u2032 k\u221e > \u01ebkZk\u221e ] \u2264\n\u2264\n\n\u0011\n\u0010\nlog m\n2mn exp \u2212 3C032\n3C0\n2m(2\u2212 32 ) .\n\n(33)\n\nSince C0 > 64/3, we obtain the desired result.\nThe following lemma is a restatement of Theorem 6.3 in [14].\nLemma 14. Let Z \u2208 Rm\u00d7n be fixed, and \u03a9 \u223c Ber(\u03c1). Then, with high probability,\ns\nm log m\nkZk\u221e ,\nkZ \u2212 \u03c1\u22121 P\u03a9 Zk \u2264 C0\u2032\n\u03c1\n\n(34)\n\nprovided that \u03c1 \u2265 C0\u2032 lognm , where C0\u2032 > 0 is a numerical constant.\n\n4.2\n\nProof of Lemma 3\n\n.\nBefore proceeding to the actual proof, we introduce some additional notation. Let Zj = U V \u2217 \u2212 P\u0393\u22a5 Yj ,\n\u22a5\nwhere Yj 's are defined in Eqn. (14). Evidently, Zj \u2208 \u0393 for all j \u2265 0. The recursive relation between\nthe Yj 's can then be expressed as\nZj = (P\u0393\u22a5 \u2212 q \u22121 P\u0393\u22a5 P\u03a9j P\u0393\u22a5 )Zj\u22121 ,\n\nZ0 = U V \u2217 .\n\n(35)\n\nLet us assume that \u01eb \u2208 (0, e\u22121 ). From Lemma 13, we have that\nkZj k\u221e \u2264 \u01ebkZj\u22121 k\u221e ,\nwith high probability, provided that\nq \u2265 C0 \u01eb\u22122 \u03b3 log m,\n\n(36)\n\n\u2217\n\nwhere C0 > 64/3 is a numerical constant. Since Z0 = U V , with high probability, we have\nkZj k\u221e\n\n\u2264 \u01ebj p\nkU V \u2217 k\u221e\n\u03bcr\nj\n\u2264 \u01eb\nmn .\n\nThe second inequality above follows from our assumptions about the matrices U and V . Furthermore,\nwhen Eqn. (36) holds, we also have, with high probability,\nkZj kF \u2264 \u01ebkZj\u22121 kF\n\n(37)\n\nusing Lemma 12. Once again, since Z0 = U V \u2217 , we deduce that\nkZj kF\n\n\u2264 \u01ebj \u221a\nkU V \u2217 kF\nj\n= \u01eb r\n\nwith high probability.\n15\n\n(38)\n\n\f4.2.1\n\nBounding kW L k\n\nWe first introduce a few notions before deriving a bound on kW L k. We let R denote the linear subspace\nobtained by projecting all the points in Q\u22a5 onto T \u22a5 . By a slight abuse of notation, we denote this by\nR = PT \u22a5 Q \u22a5 .\n\n(39)\n\nWe note that if Q\u22a5 is a random p-dimensional subspace in Rm\u00d7n , then with probability one, R is a\np-dimensional subspace of T \u22a5 . It is easy to verify that for any X \u2208 Rm\u00d7n , we have\nP\u0393\u22a5 X\nWe note that\nYj0 =\n\n= PT X + PR X.\nj0\nX\nj=1\n\nq \u22121 P\u03a9j Zj\u22121 .\n\n(40)\n\nThus, we have\nkW L k =\n\u2264\n=\n\nkP\u0393 Yj0 k\nj0\nX\nj=1\n\nj0\nX\nj=1\n\n\u2264\n\nj0\nX\nj=1\n\nq \u22121 P\u0393 P\u03a9j P\u0393\u22a5 Zj\u22121\nP\u0393 (q \u22121 P\u03a9j \u2212 I)P\u0393\u22a5 Zj\u22121\nP\u0393\u22a5 (q \u22121 P\u03a9j \u2212 I)P\u0393\u22a5 Zj\u22121 +\n\nj0\nX\nj=1\n\n(q \u22121 P\u03a9j \u2212 I)P\u0393\u22a5 Zj\u22121 .\n\nThe second term in the above inequality can be bounded with high probability using Lemma 14 as\nfollows:\ns\nj0\nj0\nX\nm log m X\n\u2032\n\u22121\n\u2264 C0\n(q P\u03a9j \u2212 I)P\u0393\u22a5 Zj\u22121\nkZj\u22121 k\u221e\nq\nj=1\nj=1\ns\nr\nj0\nm log m X j\u22121 \u03bcr\n\u2032\n\u2264 C0\n\u01eb\nq\nmn\nj=1\ns\n\u03bcr log m\n\u2264 C0\u2032\n(1 \u2212 \u01eb)\u22121 ,\nqn\nprovided that\n\u001b\n\u001a\nlog m\n, C0 \u01eb\u22122 \u03b3 log m .\nq \u2265 max C0\u2032\nn\n\nOn the other hand, each term in the summation in the first term can be split as\n\n\u2264\n\n\u2264\n\nP\u0393\u22a5 (q \u22121 P\u03a9j \u2212 I)P\u0393\u22a5 Zj\u22121\n\nPT (q \u22121 P\u03a9j \u2212 I)P\u0393\u22a5 Zj\u22121 + PR (q \u22121 P\u03a9j \u2212 I)P\u0393\u22a5 Zj\u22121\n\n2 (q \u22121 P\u03a9j \u2212 I)P\u0393\u22a5 Zj\u22121 + PR (q \u22121 P\u03a9j \u2212 I)Zj\u22121 .\n\nWe have already seen how the first term in the above inequality can be bounded with high probability.\nHence, we now focus on the second term. We first state the matrix Bernstein inequality (see Theorem\n1.4 in [22]) that will enable us to derive a bound on the second term.\n16\n\n\fTheorem 3 (Matrix Bernstein Inequality). Let M1 , . . . , Mk \u2208 Rd1 \u00d7d2 be k independent random matrices\nsatisfying\nE[Mi ] = 0, kMi k \u2264 S almost surely, i = 1, . . . , k.\n(41)\nWe set\n\n2\n\n\u03c3 = max\nThen, for any t > 0, we have\n\"\nP\n\nk\nX\ni=1\n\nMi\n\n(\n\nk\nX\n\nE[Mi\u2217 Mi ]\n\n,\n\nk\nX\n\nE[Mi Mi\u2217 ]\n\ni=1\n\ni=1\n\n#\n\n\u0012\n> t \u2264 (d1 + d2 ) exp \u2212\n\n)\n\nt2\n2\u03c3 2 + 3St\n\n.\n\n\u0013\n\n(42)\n\n.\n\n(43)\n\nUsing Theorem 3, we will now show that, with high probability,\n\u221a\nPR (q \u22121 P\u03a9j \u2212 I)Zj\u22121 \u2264 C\u0303p m log mkZj\u22121 k\u221e .\nThe proof is as follows.\n.\nFor every (i, l) \u2208 [m] \u00d7 [n], let us define Mil = Hil (Zj\u22121 )il PR \u0113i e\u2217l , where the Hil 's are independent\nrandom variables distributed as follows:\n\u001a\n1,\nw.p. 1 \u2212 q\nHil =\n1 \u2212 q \u22121 , w.p. q\nP\nWe note that i,l Mil has the same distribution as PR (I \u2212q \u22121 P\u03a9j )Zj\u22121 . Since the Hil 's are independent\nzero-mean random variables that are independent of Zj\u22121 , we have that, for any (i, l) \u2208 [m] \u00d7 [n],\nE[Mil | Zj\u22121 ] = 0.\nWe record two useful bounds. We have that\n1 \u2212 \u03c1 = P [\u222aj {(i, l) \u2208 \u03a9j }] \u2264 j0 q.\n\n(44)\n\nSo q \u2265 (1 \u2212 \u03c1)/j0 . Since |Hil | \u2264 q \u22121 almost surely, and j0 \u2265 C/ log m, we have\n|Hil | \u2264 O(log m) almost surely.\n\n(45)\n\nkPR \u0113i e\u2217l k \u2264 kPR \u0113i e\u2217l kF \u2264 1,\n\n(46)\n\nWe also have\nfor any (i, l) \u2208 [m] \u00d7 [n]. It follows that kMil k \u2264 O(log m)kZj\u22121 k\u221e almost surely.\nNow we bound the variance term. It can be shown that E[Hil2 ] = O(log m). Let B1 , . . . , Bp be such\nan orthonormal basis for R. Then, we have\nX\n\nE[Mil Mil\u2217 ] =\n\nX\n\nE[Hil2 ]PR [\u0113i e\u2217l ](PR [e \u0304i e\u2217l ])\u2217 (Zj\u22121 )2il\n\n(47)\n\nX\n\n(48)\n\nil\n\nil\n\n\u2264 O(log m)kZj\u22121 k2\u221e\n=\n\nO(log m)kZj\u22121 k2\u221e\n\n= O(log m)kZj\u22121 k2\u221e\n\u2264 O(log m)pkZj\u22121 k2\u221e\n\nil\n\nPR [\u0113i e\u2217l ](PR [\u0113i e\u2217l ])\u2217\n\np\nX X\ns=1\n\nil\n\nX\n\nBt Bt\u2217\n\n!\n\nBs hBs , \u0113i e\u2217l i\n\np\nX\nt=1\n\n!\u2217\n\nBt hBt , \u0113i e\u2217l i\n\n(49)\n(50)\n\nt\n\n17\n\n(51)\n\n\fA similar bound holds for the other variance term E[Mil\u2217 Mil ].\nNow, using the matrix Bernstein inequality, we have\n\n\u2264\n\n\u0001\nP kPR (q \u22121 P\u03a9j \u2212 I)Zj\u22121 k > t|Zj\u22121 , Q\n\u0012\n\u0013\nt2\n(m + n) exp \u2212\n.\nC1 p log mkZj\u22121 k2\u221e + C2 log mkZj\u22121 k\u221e t\n\nTherefore, removing the conditioning, we have that, with high probability,\n\u221a\nkPR (q \u22121 P\u03a9j \u2212 I)Zj\u22121 k \u2264 C\u0303 m log mkZj\u22121 k\u221e ,\nfor any j, for some numerical constant C\u0303 > 0.\nThuswe have that, with high probability,\nj0\nX\nj=1\n\nj0\nX\n\u221a\nC\u0303 m log mkZj\u22121 k\u221e\n\nkPR (q \u22121 P\u03a9j \u2212 I)Zj\u22121 k \u2264\n\nj=1\n\nr\n\u221a\n\u03bcr\nC\u0303 m log m\n(1 \u2212 \u01eb)\u22121\nmn\nr\n\u03bcr\n(1 \u2212 \u01eb)\u22121 .\nC\u0303 log m\nn\n\n\u2264\n\u2264\n\nUnder the assumptions of Theorem 1, the bound on the right hand side can be made arbitrarily small.\nThis gives us the desired bound.\n4.2.2\n\nBounding kP\u03a9 (U V \u2217 + W L )kF\n\nWe now prove the second part of Lemma 3. First, we note that P\u03a9 Yj0 = 0 by construction. Therefore,\nConsequently, we have\n\nP\u03a9 (U V \u2217 + P\u0393 Yj0 ) = P\u03a9 (U V \u2217 \u2212 P\u0393\u22a5 Yj0 ) = P\u03a9 Zj0 .\nkP\u03a9 (U V \u2217 + P\u0393 Yj0 )kF\n\n=\n\u2264\n\u2264\n\u2264\n\n(52)\n\nkP\u03a9 Zj0 kF\nkZ\u221a\nj0 kF\n\u01eb\u221aj0 r\nr\nm2 .\n\nThe last step follows from the fact that \u01eb < e\u22121 and j0 \u2265 2 log m.\n4.2.3\n\nBounding kP\u03a9\u22a5 (U V \u2217 + W L )k\u221e\n\nWe now prove the final part of Lemma 3. We note that\nU V \u2217 + W L = U V \u2217 + P\u0393 Yj0 = Yj0 + Zj0 .\n\n(53)\n\nSince we have already proved that kZj0 kF < \u03bb/8, it is sufficient to show that kYj0 k\u221e < \u03bb/8. We have\nkYj0 k\u221e\n\n= k\n\nj0\nX\nj=1\n\n\u2264 q \u22121\n\u2264 q \u22121\n\u2264 q \u22121\n\nq \u22121 P\u03a9j Zj\u22121 k\u221e\n\nj0\nX\nj=1\n\nj0\nX\nj=1\n\nj0\nX\nj=1\n\n18\n\nkP\u03a9j Zj\u22121 k\u221e\nkZj\u22121 k\u221e\n\u01ebj\u22121\n\nr\n\n\u03bcr\nmn\n\n\fNotice that q \u2265 (1 \u2212 \u03c1)/j0 \u2265 4/ log m for \u03c1 < 1/2,\nkYj0 k\u221e\n\n\u2264\n\u2264\n\nlog m\n4(1 \u2212 \u01eb)\n\u03bb/8,\n\nr\n\n\u03bcr\nmn\n\nfor sufficiently small \u01eb and for some numerical constant Cr .\n\n4.3\n\nProof of Lemma 4\n\nWe recall the notation that \u0393\u22a5 = Q\u22a5 \u2295 T . By Lemma 11, we have that\nkP\u0393\u22a5 P\u03a9 k2 \u2264\n\nkP\u03a9 PQ\u22a5 k2 + kP\u03a9 PT k2\n.\n1 \u2212 kPQ\u22a5 PT k\n\nLet us assume that m, n are sufficiently large so that the following conditions hold true:\np\nmn\n!\nr\nr\np\n(m + n)r\n8\n+\nmn\nmn\n\u03c12 (1 \u2212 \u03c1)\n\n5\u03c1\n,\n4\n1\n,\n2\n\n<\n<\n\n\u2265 C0 *\n\n(54)\n(55)\n\u03bcr log m\n,\nn\n\n(56)\n\nwhere C0 > 0 is the numerical constant from Lemma 9. We also assume that \u03c1 < 1/5. We note that it\nis possible to satisfy all of the above inequalities under the assumptions on p and r given in Theorem 1,\nand because \u03c1 is a fixed constant in the interval (0, 1). Using Lemma 11, it is easy to verify that under\nthese assumptions, with high probability, we have that\n\u221a\n(57)\nkP\u0393\u22a5 P\u03a9 k \u2264 \u03b7 \u03c1,\nwhere \u03b7 > 0 is a numerical constant.\nThe basic steps of the proof closely follow that of Lemma 2.9 in [6]. We recognize that using the\nconvergent Neumann series, W S can be expressed as follows:\nX\nW S = \u03bb(I \u2212 P\u0393\u22a5 )P\u03a9\n(P\u03a9 P\u0393\u22a5 P\u03a9 )k [sgn(S0 )].\n(58)\nk\u22650\n\nAs mentioned in Section 1.2, we assume that the signs of the non-zero entries of S0 are independent,\nsymmetric \u00b11 random variables.\n4.3.1\n\nBounding kW S k\n\nIt is easy to show that\nWS\n\n=\n\n\u03bbsgn(S0 ) \u2212 \u03bbP\u03a9\u22a5 P\u0393\u22a5 P\u03a9\n\n:=\n\nW1S \u2212 W2S\n\nX\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 )k [sgn(S0 )]\n\nk\u22650\n\nWe now show that each of these components have spectral norm smaller than 1/16 with high probability.\nThis gives us the desired bound on kW S k.\nFor the first term, we can use standard arguments about the norms of random matrices with i.i.d.\nentries (see [23]) to show that, with high probability,\n\u221a\nksgn(S0 )k \u2264 4 n\u03c1.\n19\n\n\f\u221a\nSince \u03bb = m\u22121/2 , we have that kW1S k \u2264 4 \u03c1 with high probability. Thus, for sufficiently small \u03c1, we\nS\nhave that kW1 k < 1/16.\nWe use a discretization argument to bound kW2S k. Let Nm and Nn be 1/2-nets for the unit spheres\nin Rm and Rn , respectively. It can be shown that the sizes of Nm and Nn are at most 6m and 6n ,\nrespectively (see Theorem 4.16 in [19]). Then, we have that\nkW2S k \u2264\n\n4\n\n=\n\n4\n\n=\n\n=\n=\n\n4\n\nmax\n\nx\u2217 W2S y\n\nmax\n\nxy\u2217 , W2S\n*\n\nx\u2208Nm ,y\u2208Nn\nx\u2208Nm ,y\u2208Nn\n\nmax\n\nx\u2208Nm ,y\u2208Nn\n\n4\u03bb\n4\u03bb\n\nxy\u2217 , \u03bbP\u03a9\u22a5 P\u0393\u22a5 P\u03a9\n\n*\n\nmax\n\nx\u2208Nm ,y\u2208Nn\n\nX\n\nk\u22650\n\nX\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 )k [sgn(S0 )]\n\nk\u22650\n\nk\n\n+\n\n+\n\n\u2217\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 ) P\u03a9 P\u0393\u22a5 P\u03a9\u22a5 [xy ], sgn(S0 )\n\nhH(x, y), sgn(S0 )i .\n\nmax\n\nx\u2208Nm ,y\u2208Nn\n\nFor any (x, y) \u2208 Nm \u00d7 Nn , we bound kH(x, y)kF as follows:\nkH(x, y)kF\n\n=\n\nX\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 )k P\u03a9 P\u0393\u22a5 P\u03a9\u22a5 [xy\u2217 ]\n\nk\u22650\n\n\u2264\n\nX\n\nk\u22650\n\n\uf8eb\n\n\u2264 \uf8ed\n\n\u2264\n\nF\nk\n\n\u2217\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 ) P\u03a9 P\u0393\u22a5 P\u03a9\u22a5 [xy ]\n\nX\n\nk\u22650\n\nF\n\n\uf8f6\n\nk(P\u03a9 P\u0393\u22a5 P\u03a9 )kk \uf8f8 kP\u03a9 P\u0393\u22a5 kkP\u03a9\u22a5 [xy\u2217 ]kF\n\nkP\u03a9 P\u0393\u22a5 k\n.\n1 \u2212 kP\u03a9 P\u0393\u22a5 k2\n\nConditioned on Q and \u03a9, we use Hoeffding's inequality to get\n\u0012\nP [|hH(x, y), sgn(S0 )i| > t|\u03a9, Q] < 2 exp \u2212\n\n2t2\nkH(x, y)k2F\n\n\u0013\n\n.\n\nSubsequently, using a union bound over Nm \u00d7 Nn , we obtain\n\u0014\n\u0015\nP\nmax\n|hH(x, y), sgn(S0 )i| > t|\u03a9, Q\n(59)\nx\u2208Nm ,y\u2208Nn\n\u0013\n\u0012\n2t2\n(60)\n< 2 * 6m+n * exp \u2212\nmaxx\u2208Nm ,y\u2208Nn kH(x, y)k2F\n\u0013\n\u0012\n2t2 (1 \u2212 kP\u03a9 P\u0393\u22a5 k2 )2\nm+n\n.\n(61)\n\u22642*6\n* exp \u2212\nkP\u03a9 P\u0393\u22a5 k2\n\u221a\nLet E1 be the event {kP\u03a9 P\u0393\u22a5 k \u2264 \u03b7 \u03c1}. We know that this event occurs with high probability. Thus,\nremoving the conditioning on \u03a9 and Q, we have\n\u0014\n\u0015\n\u0012\n\u0013\n2t2 (1 \u2212 \u03b7 2 \u03c1)2\nP\nmax\n|hH(x, y), sgn(S0 )i| > t\n< 2 * 6m+n * exp \u2212\nx\u2208Nm ,y\u2208Nn\n\u03b72 \u03c1\n+P[E1c ].\n\n20\n\n\fTherefore,\n\u0014\nP 4\u03bb\n\nmax\n\nx\u2208Nm ,y\u2208Nn\n\n|hH(x, y), sgn(S0 )i| > t\n\n\u0015\n\n\u0012 2\n\u0013\nt (1 \u2212 \u03b7 2 \u03c1)2\n2 * 6m+n * exp \u2212\n8\u03bb2 \u03b7 2 \u03c1\nc\n+P[E1 ].\n\n<\n\n\u221a\nand substituting \u03bb = 1/ m, we get\n\u0015\n\u221a\n\u0014\n\u0001\ns\u03b7 16\u03c1\n< 2 * exp 2m(log 6 \u2212 s2 ) + P[E1c ].\nP kW2S k >\n2\n1\u2212\u03b7 \u03c1\n\u221a\nLet us choose any s > log 6. Then, for sufficiently small \u03c1, we have that kW2S k < 1/16 with high\nprobability.\nSetting t =\n\n4.3.2\n\n\u221a\ns\u03b7 16\u03c1\n1\u2212\u03b7 2 \u03c1\n\nBounding kP\u03a9\u22a5 W S k\u221e\n\nOnce again, using the convergent Neumann series expansion for W S , we have\n*\n+\nX\nS\n\u2217\nk\nmax c \u0113i ej , \u03bb(I \u2212 P\u0393\u22a5 )P\u03a9\nkP\u03a9\u22a5 W k\u221e =\n(P\u03a9 P\u0393\u22a5 P\u03a9 ) [sgn(S0 )]\n(i,j)\u2208\u03a9\n\nk\u22650\n\n= \u03bb max c\n(i,j)\u2208\u03a9\n\n*\n\nX\n\nk\u22650\n\nk\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 )\n\n+\n\nP\u03a9 P\u0393\u22a5 [\u0113i e\u2217j ], sgn(S0 )\n\n= \u03bb max c |hXi,j , sgn(S0 )i| .\n(i,j)\u2208\u03a9\n\nConditioned on Q and \u03a9, we use Hoeffding's inequality to get\n\u0012\nP [|hXi,j , sgn(S0 )i| > t|\u03a9, Q] < 2 exp \u2212\n\n2t2\nkXi,j k2F\n\nUsing a union bound, we obtain\n\u0014\n\u0015\n\u0012\nP max |hXi,j , sgn(S0 )i| > t|\u03a9, Q < 2mn exp \u2212\ni,j\n\n\u0013\n\n.\n\n2t2\nmaxi,j kXi,j k2F\n\nWe obtain a bound on kXi,j kF as follows:\nkXi,j kF\n\n=\n\nX\n\nk\u22650\n\n\u2264\n\u2264\n\u2264\nThus, we get\n\nX\n\nk\u22650\n\n\uf8eb\n\uf8ed\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 )k P\u03a9 P\u0393\u22a5 [\u0113i e\u2217j ]\nk\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 )\n\nX\n\nk\u22650\n\nF\n\u2217\nP\u03a9 P\u0393\u22a5 [\u0113i ej ] F\n\n\uf8f6\n\nkP\u03a9 P\u0393\u22a5 P\u03a9 kk \uf8f8 kP\u03a9 P\u0393\u22a5 [\u0113i e\u2217j ]kF\n\nkP\u03a9 P\u0393\u22a5 kkP\u0393\u22a5 [\u0113i e\u2217j ]kF\n.\n1 \u2212 kP\u03a9 P\u0393\u22a5 k2\n\n\u0014\n\u0015\nP max |hXi,j , sgn(S0 )i| > t|\u03a9, Q\ni,j\n\n<\n\n2t2 (1 \u2212 kP\u03a9 P\u0393\u22a5 k2 )2\n2mn exp \u2212\nkP\u03a9 P\u0393\u22a5 k2 maxi,j kP\u0393\u22a5 [\u0113i e\u2217j ]k2F\n21\n\n!\n\n.\n\n\u0013\n\n.\n\n\fRemoving the conditioning on Q and \u03a9, we get\n\"\n#\nr\ns log(mn) kP\u03a9 P\u0393\u22a5 k maxi,j kP\u0393\u22a5 [\u0113i e\u2217j ]kF\nS\nP kP\u03a9\u22a5 W k\u221e > \u03bb\n< 2(mn)1\u2212s\n2\n1 \u2212 kP\u03a9 P\u0393\u22a5 k2\nConsider the two events:\nE1\n\n:=\n\nE2\n\n:=\n\n\u221a\n{kP\u03a9 P\u0393\u22a5 k \u2264 \u03b7 \u03c1} ,\n\u001b\n\u001a\n\u221a\nmax kP\u0393\u22a5 \u0113i e\u2217j kF \u2264 \u03b3 ,\ni,j\n\n\u0010\n\n\u0011\nwhere we recall that \u03b3 = 4 8p log(mnp)\n+ \u03bcr\nmn\nn . We have already shown that E1 and E2 occur with high\nprobability. Substituting for the various bounds and setting s = 2, we get\n\u0014\n\u221a \u0015\np\n\u03b7 \u03c1\nP kP\u03a9\u22a5 W S k\u221e > \u03bb \u03b3 log(mn)\n1 \u2212 \u03b72 \u03c1\n2\n<\n+ P[(E1 \u2229 E2 )c ].\nmn\nUnder the conditions of Theorem 1, and for sufficiently large m, n and sufficiently small \u03c1, we get that\nkP\u03a9\u22a5 W S k\u221e < \u03bb/8 with high probability.\n\n4.4\n4.4.1\n\nProof of Lemma 5\nBounding ||W Q ||\n\nUsing the convergent Neumann series expansion, we can write the analytical expression for W Q as follows:\nX\nW Q = P \u03a0\u22a5\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k (PQ\u22a5 (\u2212U V \u2217 )),\n(62)\nk\u22650\n\nwhere we recall that \u03a0 = \u03a9 \u2295 T . It follows that\nkW Q kF \u2264\n\nX\n\nk\u22650\n\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k kPQ\u22a5 (U V \u2217 )kF .\n\nConsidering the first term of the product on the right hand side,\nX\n\nk\u22650\n\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k\n\n\u2264\n\u2264\n\nX\n\nk\u22650\n\nX\n\nk\u22650\n\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k\nkPQ\u22a5 P\u03a0 k2k .\n\nFrom Lemma 11, we have that, for any \u01eb > 0, with high probability,\nkPQ\u22a5 P\u03a0 k2\n\u2264\n\n\uf8eb\nr !2\nr\np\n5\u03c1\n64\n\uf8ed\n\u221a\n+\n+\n1\u2212 \u03c1+\u01eb\nmn\n4\n\nr\n\np\n+\nmn\n\nr\n\n(m + n)r\nmn\n\n!2 \uf8f6\n\n\uf8f8.\n\nAssume that \u03c1 < 1/4, and fix \u01eb = 3\u03c1. For m, n large enough, we can assume that max{p/mn , r(m +\nn)/mn} < \u03c1. Then, we have that, with high probability,\nkPQ\u22a5 P\u03a0 k2 \u2264\n22\n\n832 \u03c1\n\u221a .\n1\u22122 \u03c1\n\n\fTherefore, for sufficiently small \u03c1, we have that\nkPQ\u22a5 P\u03a0 k2 \u2264\n\n1\n,\n4\n\n(63)\n\nwith high probability. Consequently,\nX\n\nk\u22650\n\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k \u2264\n\n4\n,\n3\n\n(64)\n\nwith high probability.\nWe bound kPQ\u22a5 (U V \u2217 )kF as follows. As explained earlier, suppose we vectorize all matrices, then\nPQ\u22a5 has the same distribution as H(H \u2217 H)\u22121 H \u2217 , where H \u2208 Rmn\u00d7p is a random Gaussian matrix with\ni.i.d. entries \u223c N (0, 1/mn). Therefore, we have\nkPQ\u22a5 (U V \u2217 )kF = kH(H \u2217 H)\u22121 H \u2217 vec(U V \u2217 )k2 \u2264 kH(H \u2217 H)\u22121 k kH \u2217vec(U V \u2217 )k2 ,\nwhere the above equality is in distribution. We have already shown in the proof of Lemma 7 that\n\u0002\n\u0003\nP kH(H \u2217 H)\u22121 k \u2265 4 \u2264 e\u2212mn/32 .\n\nWe note that H \u2217 vec(U V \u2217 ) is a p-dimensional vector whose components are i.i.d. and have the same\ndistribution as hG, U V \u2217 i, where G \u2208 Rm\u00d7n is a random Gaussian matrix whose entries are i.i.d. \u223c\nN (0, 1/mn). It is easy to see that hG, U V \u2217 i is distributed according to N (0, r/mn), and therefore, we\nhave\nr\npr\n.\nE[kH \u2217 vec(U V \u2217 )kF ] \u2264 (E[kH \u2217 vec(U V \u2217 )k2F ])1/2 =\nmn\nSince k * kF is a 1-Lipschitz function, we use Proposition 2.18 in [19] to get\nr\n\u0015\n\u0014\n2\nr\n\u2217\n\u2217\n\u2217\n\u2217\n\u2264 e\u2212t /2 .\nP kH vec(U V )kF \u2265 E(kH vec(U V )kF ) + t *\nmn\n\u221a\nSetting t = 6 log m, we get\n!\nr\nr\n1\npr\n6r log m\n\u2217\n\u2217\n\u2264 3.\n+\nP kH vec(U V )kF \u2265\nmn\nmn\nm\n\n(65)\n\nPutting it all together, we conclude that\n16\nkW k \u2264 kW kF \u2264\n3\nQ\n\nQ\n\nr\n\npr\n+\nmn\n\nr\n\n6r log m\nmn\n\n!\n\n,\n\n(66)\n\nwith high probability. Clearly, for sufficiently large m, the right hand side can be made arbitrarily small\nunder the conditions of Theorem 1 and hence, we have the desired bound.\n4.4.2\n\nControlling kP\u03a9\u22a5 W Q k\u221e\n\nIt is easy to show that the analytical expression for W Q can be written slightly differently as follows:\nX\nW Q = P\u03a0\u22a5 PQ\u22a5\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k (PQ\u22a5 (\u2212U V \u2217 )).\n(67)\nk\u22650\n\n23\n\n\fConsider any (i, j) \u2208 [m] \u00d7 [n]. Then,\n|hW Q , \u0113i e\u2217j i|\n\n=\n\n\u2264\n\u2264\n\n*\n\nX\n\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k (PQ\u22a5 (\u2212U V \u2217 )), PQ\u22a5 P\u03a0\u22a5 \u0113i e\u2217j\n\nk\u22650\n\n+\n\nX\n\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k PQ\u22a5 (U V \u2217 )\n\nX\n\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k kPQ\u22a5 (U V \u2217 )kF kPQ\u22a5 P\u03a0\u22a5 \u0113i e\u2217j kF .\n\nk\u22650\n\nk\u22650\n\nkPQ\u22a5 P\u03a0\u22a5 \u0113i e\u2217j kF\nF\n\nWe have already derived bounds for the first two terms. For the final term in the product, we use the\nsame technique we employed to bound kPQ\u22a5 (U V \u2217 )kF . Using the fact that kP\u03a0\u22a5 \u0113i e\u2217j kF \u2264 1, we can\nshow that\n!#\n\"\nr\nr\n1\np\n6\nlog\nm\n16\n\u2264 3 + e\u2212mn/32 .\n+\nP kPQ\u22a5 P\u03a0\u22a5 \u0113i e\u2217j kF >\n3\nmn\nmn\nm\nUsing a union bound, we get\n\"\nP max\ni,j\n\nkPQ\u22a5 P\u03a0\u22a5 \u0113i e\u2217j kF\n\n16\n>\n3\n\nr\n\np\n+\nmn\n\nr\n\n6 log m\nmn\n\n!#\n\n\u2264 mn(m\u22123 + e\u2212mn/32 ).\n\nPutting all the bounds together, we have that, with high probability,\n\u221a\n\u00112\np\n256 r \u0010\u221a\np + 6 log m .\nkP\u03a9\u22a5 W Q k\u221e \u2264\n9 mn\n\n(68)\n\nSince \u03bb = m\u22121/2 , it easy to show that under the assumptions of Theorem 1, the right hand side in the\nabove inequality can be made smaller than C \u2032 \u03bb, for any fixed C \u2032 > 0. Thus, we have the desired bound.\n\n5\n\nDeterministic Reduction: Proof of Theorem 2\n\nIn this section, we provide the proof for Theorem 2 under the deterministic subspace model for Q\u22a5 . We\nwill adopt the same optimality conditions established in Lemma 2, and the same proof strategy outlined\nin Section 3, namely the construction of W = W L + W S + W Q . To avoid redundancy, wherever possible,\nwe will only highlight the parts that differ from the previous proof in Section 4 and refer the interested\nreader to Section 4 for more details. First, we derive the various incoherence relations associated with\nour fixed subspace Q\u22a5 . Then, we will prove Lemmas 3, 4 and 5 using these relations.\n\n5.1\n\nPreliminaries\n\nIn this subsection, we provide several lemmas that will be used later in our proof.\nLemma 15. If X \u2208 Rm\u00d7n is a rank-r matrix, then\nkPQ\u22a5 Xk2F \u2264 \u03bd\n\n24\n\npr\nkXk2F .\nn\n\n(69)\n\n\fProof.\nkPQ\u22a5 Xk2F\n\n=\n\np\nX\ni=1\n\n\u2264\n\u2264\n\u2264\n\n|hGi , Xi|2\n\n\u0011\n\u0010\np max kGi k2 kXk2\u2217\ni\n\u0010\n\u0011\npr max kGi k2 kXk2F\ni\n\npr\n\u03bd kXk2F .\nn\n\nCorollary 1. For any \u03bd-coherent subspace Q\u22a5 , we have the following:\n1. kPQ\u22a5 \u0113i e\u2217j k2F \u2264 \u03bd np ;\n2. kPQ\u22a5 PT k2 \u2264 2\u03bd pr\nn;\n2\n\n3. kPQ\u22a5 (U V \u2217 )k2F \u2264 2\u03bd prn .\nProof. The first two results follow from the fact that the \u0113i e\u2217j are rank-1 matrices, and rank (PT X) \u2264\n2r \u2200X \u2208 Rm\u00d7n . The last result can be derived from the second one as shown below:\nkPQ\u22a5 (U V \u2217 )k2F \u2264 kPQ\u22a5 PT k2 kU V \u2217 k2F \u2264 2\u03bd\n\npr2\n.\nn\n\nLemma 16. Under the assumptions made in Theorem 2, we have that\nkPQ\u22a5 P\u03a9 k < 1/2,\n\n(70)\n\nwith high probability, provided that \u03c1 < \u03c10 and \u03bd 2 p3 log m/n \u2264 C. Here, C > 0 and \u03c10 \u2208 (0, 1) are\nnumerical constants.\nProof. Please refer to Section 5.5 for a detailed proof.\n\n5.2\n\nProof of Lemma 3 (deterministic case)\n\nWe use the same framework from Section 4.2.1 to bound the corresponding norms of W L . We note that\nto bound kW L k in the previous case, the only key property of Q\u22a5 that was critical to the proof was that\n\u0393\u22a5 = Q\u22a5 \u2295 T is O(\u03bcr/n)-constrained. More specifically, the latter property is used in Lemma 12 and\nLemma 13.\nIn the deterministic case, by assumption, Q\u22a5 is \u03bd-coherent, where \u03bd is a constant. In the following\nlemma, we will show that \u0393\u22a5 is O(\u03bcr/n)-constrained as well under our assumptions. We will show that,\nthe proof of Lemma 3 can be directly adopted for the deterministic case from the that with the random\nsubspace model.\nLemma 17. If Q\u22a5 is \u03bd-coherent, then\nkP\u0393\u22a5 \u0113i e\u2217j kF\n\n\u22644\n\nr\n\n\u03bdp\n+\nn\n\nr\n\n2\u03bcr\nn\n\n!\n\n.\n\nIn other words, if Q\u22a5 is \u03bd-coherent, then \u0393\u22a5 is \u03b3-constrained for \u03b3 = 16\n\n25\n\n(71)\n\u0010p\n\u00112\np\n\u03bdp/n + 2\u03bcr/n .\n\n\fProof. Let us assume that kPQ\u22a5 PT k < 1/2. This is true for sufficiently large n under the assumptions\nof Theorem 2. Using the convergent Neumann series expansion, it is possible to show that\n\u0001\nP\u0393\u22a5 \u0113i e\u2217j = (I \u2212 PQ\u22a5 PT )\u22121 PQ\u22a5 PT \u22a5 + (I \u2212 PT PQ\u22a5 )\u22121 PT PQ (\u0113i e\u2217j ),\n\nand therefore,\n\nkP\u0393\u22a5 \u0113i e\u2217j kF \u2264 kI \u2212 PQ\u22a5 PT )\u22121 kkPQ\u22a5 PT \u22a5 (\u0113i e\u2217j )kF + kI \u2212 PT PQ\u22a5 )\u22121 kkPT PQ (\u0113i e\u2217j )kF .\nFrom Eqn. (6) and Corollary 1, we have\nkPQ\u22a5 PT \u22a5 (\u0113i e\u2217j )kF\n\n\u2264 kPQ\u22a5 (\u0113i e\u2217j )kF + kPQ\u22a5 PT (\u0113i e\u2217j )kF\n\u2264 kPQ\u22a5 (\u0113i e\u2217j )kF + kPT (\u0113i e\u2217j )kF\nr\nr\n\u03bdp\n2\u03bcr\n\u2264\n+\n.\nn\nn\n\nSimilarly, we have\nkPT PQ (\u0113i e\u2217j )kF\n\n\u2264\n\nWe also have that\n\nr\n\n\u03bdp\n+\nn\n\nr\n\n(I \u2212 PQ\u22a5 PT )\u22121 = (I \u2212 PT PQ\u22a5 )\u22121 =\n\n2\u03bcr\n.\nn\n\nX\n\n(PQ\u22a5 PT )k < 2.\n\nk\u22650\n\nHence, we have\nkP\u0393\u22a5 ei e\u2217j kF\n\n\u22644\n\nr\n\n\u03bdp\n+\nn\n\nr\n\n2\u03bcr\nn\n\n!\n\n.\n\n(72)\n\nIt can be easily shown that the results in Section 4.1 all hold for the deterministic case as well with\nthe modified value for \u03b3 derived above. Consequently, the proof of Lemma 3 from Section 4.2 can be\ndirectly adopted for the deterministic case as well.\n\n5.3\n\nProof of Lemma 4 (deterministic case)\n\nWe now provide a proof of Lemma 4 under our deterministic subspace model. Since the basic framework\nof the proof is very similar to that in Section 4.3, we will derive only the important steps here and refer\nthe interested reader to Section 4.3 for more details.\nControlling kP\u03a9\u22a5 W S k\u221e\n\nUsing the convergent Neumann series, we have\nX\nW S = \u03bb (I \u2212 P\u0393\u22a5 ) P\u03a9\n(P\u03a9 P\u0393\u22a5 P\u03a9 )k [sgn(S0 )] .\nk\u22650\n\nTherefore, we have\nS\n\nkP\u03a9\u22a5 W k\u221e\n\n= \u03bb max c\n(i,j)\u2208\u03a9\n\n= \u03bb max c\n(i,j)\u2208\u03a9\n\n= \u03bb max c\n(i,j)\u2208\u03a9\n\n*\n*\nD\n\n\u0113i e\u2217j , (I\n\nX\n\nk\u22650\n\n\u2212 P\u0393\u22a5 ) P\u03a9\nk\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 )\n\nk\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 ) [sgn(S0 )]\n\nk\u22650\n\n+\n\nP\u03a9 P\u0393\u22a5 (\u0113i e\u2217j ), sgn(S0 )\n\nE\nH (i,j) , sgn(S0 ) .\n26\n\nX\n\n+\n\n\fWe now bound kH (i,j) kF as follows:\nkH (i,j) kF\n\n\u2264\n\nX\n\nk\u22650\n\n\uf8eb\n\n\u2264 \uf8ed\n\n\u2264\n\n(P\u03a9 P\u0393\u22a5 P\u03a9 )k P\u03a9 P\u0393\u22a5 (\u0113i e\u2217j )\n\nX\n\nk\u22650\n\nF\n\n\uf8f6\n\nk(P\u03a9 P\u0393\u22a5 P\u03a9 )kk \uf8f8 kP\u03a9 P\u0393\u22a5 kkP\u0393\u22a5 (\u0113i e\u2217j )kF\n\nkP\u03a9 P\u0393\u22a5 kkP\u0393\u22a5 (\u0113i e\u2217j )kF\n.\n1 \u2212 kP\u03a9 P\u0393\u22a5 k2\n\nConditioned on \u03a9, using Hoeffding's inequality, we have\n\u0012\nh\ni\nP hH (i,j) , sgn(S0 )i > t | \u03a9 < 2 exp \u2212\nApplying a union bound, we get\n\u0014\n\u0015\nP max hH (i,j) , sgn(S0 )i > t | \u03a9\n\u2264\ni,j\n\n\u2264\n\n\u0012\n2mn exp \u2212\n\n2t2\nkH (i,j) k2F\n\n\u0013\n\n.\n\n2t2\nmaxi,j kH (i,j) k2F\n\n\u0013\n\n!\n\u00012\n2t2 1 \u2212 kP\u03a9 P\u0393\u22a5 k2\n.\n2mn exp \u2212\nkP\u03a9 P\u0393\u22a5 k2 maxi,j kP\u0393\u22a5 (\u0113i e\u2217j )k2F\n\nRemoving the conditioning on \u03a9, we get\n\"\n#\nr\ns log(mn) kP\u03a9 P\u0393\u22a5 k maxi,j kP\u0393\u22a5 (\u0113i e\u2217j )kF\nS\nP kP\u03a9\u22a5 W k\u221e > \u03bb\n< 2(mn)1\u2212s ,\n2\n1 \u2212 kP\u03a9 P\u0393\u22a5 k2\n\b\n\u221a\nwhere s > 0. Consider the event E = kP\u03a9 P\u0393\u22a5 k \u2264 \u03b7 \u03c1 . Just like under the random subspace model, it\nis not difficult to show that the event E occurs with high probability for some fixed \u03b7 > 0. Furthermore,\nwe have already shown that \u0393\u22a5 is a \u03b3-constrained subspace with \u03b3 log m = O(1/ log m). Setting s = 2,\nwe get\n\u0014\n\u221a \u0015\np\n\u03b7 \u03c1\n2\nS\nP kP\u03a9\u22a5 W k\u221e > \u03bb \u03b3 log(mn)\n<\n+ P[E c ].\n1 \u2212 \u03b72 \u03c1\nmn\nThus, we have the desired bound.\n\nControlling kW S k The proof is identical to the one in Section 4.3.1.\n\n5.4\n\nProof of Lemma 5 (deterministic case)\n\nWe now prove Lemma 5 under our deterministic subspace model. Once again, the basic structure of the\nproof is very similar to the one used in Section 4.4. So, we only provide the relevant bounds here and\nrefer the interested reader to Section 4.4 for the detailed steps involved.\nControlling kW Q k\n\nThe proof framework is the same as the one in Section 4.4.1. We note that the\nP\nk\nkey step is to bound kPQ\u22a5 (U V \u2217 )kF and\nk\u22650 (PQ\u22a5 P\u03a0 PQ\u22a5 ) , where we recall that \u03a0 = \u03a9 \u2295 T . From\nCorollary 1, we already know that\nr\n2\u03bdpr2\n\u2217\n.\nkPQ\u22a5 (U V )kF \u2264\nn\n\n27\n\n\fFor the other quantity, we have that\nX\n\nk\u22650\n\n(PQ\u22a5 P\u03a0 PQ\u22a5 )k \u2264\n\nBy Lemma 11, we have\nkPQ\u22a5 P\u03a0 k2 \u2264\nFrom Lemma 9, we know that kP\u03a9 PT k \u2264\n\n\u221a\n\n1\n1 \u2212 kPQ\u22a5 P\u03a0 k2\n\n.\n\nkPQ\u22a5 P\u03a9 k2 + kPQ\u22a5 PT k2\n.\n1 \u2212 kP\u03a9 PT k\n\n\u03c1 + \u01eb with high probability, provided that\n\n(1 \u2212 \u03c1) \u2265 C0 * \u01eb\u22122\n\n\u03bcr log m\n.\nn\n\nSuppose that the above condition holds with \u01eb = \u03c1, and assume that\n2\u03bdpr\n< \u03c1.\nn\nWe note that both the assumptions above can be true for sufficiently large m and n under the assumptions\nof Theorem 2. Under these assumptions, along with Lemma 16, we have\nkPQ\u22a5 P\u03a0 k2 \u2264\n\n1/4 + \u03c1\n\u221a ,\n1 \u2212 2\u03c1\n\nwith high probability. Thus, we have that kPQ\u22a5 P\u03a0 k2 \u2264 1/2 with high probability, provided that \u03c1 is\nsufficiently small. Putting all these bounds together, we get\nr\n2\u03bdpr2\nQ\n,\nW\n\u22642\nn\nwith high probability. Under the assumptions of Theorem 2, the right hand side can be made arbitrarily\nsmall, and hence, we have the desired result.\nControlling kP\u03a9\u22a5 W Q k\u221e Once again, the proof framework is identical to that used in Section 4.4.2.\nThe key step here is to bound max(i,j)\u2208\u03a9c kPQ\u22a5 P\u03a0\u22a5 \u0113i e\u2217j kF . We first use the Neumann series to rewrite\nP\u03a0 \u0113i e\u2217j as\n\u0001\nP\u03a0 \u0113i e\u2217j = (I \u2212 P\u03a9 PT )\u22121 P\u03a9 PT \u22a5 + (I \u2212 PT P\u03a9 )\u22121 PT P\u03a9\u22a5 (\u0113i e\u2217j ).\n\nNow, for any (i, j) \u2208 [m] \u00d7 [n], we have\n\nkP\u03a9 PT \u22a5 \u0113i e\u2217j kF\n\n=\n\nkP\u03a9 PT \u0113i e\u2217j kF\n\nkPT P\u03a9\u22a5 \u0113i e\u2217j kF = kPT \u0113i e\u2217j kF\n\nr\n\n2\u03bcr\n,\nn\nr\n2\u03bcr\n\u2264\n.\nn\n\u2264\n\nFurthermore, by the assumption we used earlier (to bound kW Q k), we have that kP\u03a9 PT k <\nhigh probability. Therefore, we have\n(I \u2212 P\u03a9 PT )\u22121\n\n=\n\n(I \u2212 PT P\u03a9 )\u22121\n\n=\n\nX\n\nk\u22650\n\n(P\u03a9 PT )k\n\n1\n\u221a\n1 \u2212 2\u03c1\n\u2264 2\n\u2264\n\n28\n\n\u221a\n2\u03c1 with\n\n\fwith high probability, provided that \u03c1 \u2264 1/8. Thus, we get\nr\n2\u03bcr\n\u2217\nkP\u03a0 \u0113i ej kF \u2264 4\n,\nn\nwith high probability. Consequently, for any (i, j) \u2208 \u03a9c , we have\nkPQ\u22a5 P\u03a0\u22a5 \u0113i e\u2217j kF\n\n\u2264\n\u2264\n\nkPQ\u22a5 \u0113i e\u2217j kF + kPQ\u22a5 P\u03a0 \u0113i e\u2217j kF\nr\nr\n\u03bdp\n2\u03bcr\n+4\n,\nn\nn\n\nwith high probability.\nProceeding along the same lines as in Section 4.4.2, we have that\n!\nr\nr\nr\n2\u03bdpr2\n\u03bdp\n2\u03bcr\nQ\n\u2217\n,\n+4\n|hW , \u0113i ej i| \u2264 2\nn\nn\nn\nwith high probability, for any (i, j) \u2208 \u03a9c . Therefore, we have that\n!\nr\nr\nr\n2\u03bdpr2\n\u03bdp\n2\u03bcr\nQ\n,\nkP\u03a9\u22a5 W k\u221e \u2264 2\n+4\nn\nn\nn\nwith high probability. Since \u03bb = m\u22121/2 , under the assumptions made in Theorem 2, the right hand side\ncan be made smaller than \u03bb/8, provided that n is sufficiently large.\n\n5.5\n\nProof of Lemma 16\n\nConsider the linear operator\nA = PQ\u22a5 P\u03a9 PQ\u22a5 \u2212 \u03c1PQ\u22a5 .\nIt can be easily shown that\nE [A] = 0.\nFirst, we derive a bound for the spectral norm of A. Let \u03b4ij be a sequence of independent Bernoulli\nrandom variables such that\n\u001a\n1, if (i, j) \u2208 \u03a9,\n\u03b4ij =\n0, otherwise.\nThen, we can rewrite A as\n\nA=\n\nX\nij\n\nAij ,\n\nwhere\n\n\u03c1\nP \u22a5,\nmn Q\nand \u2297 denotes the outer or tensor product between matrices. Then, we have that\nAij = \u03b4ij PQ\u22a5 (\u0113i e\u2217j ) \u2297 PQ\u22a5 (\u0113i e\u2217j ) \u2212\n\nkAij k \u2264 kPQ\u22a5 (\u0113i e\u2217j ) \u2297 PQ\u22a5 (\u0113i e\u2217j )k +\n\u2264 kPQ\u22a5 (\u0113i e\u2217j )k2F +\n\u03c1\n\u03bdp\n++\nn\nmn\n, S,\n\n\u03c1\nmn\n\n\u2264\n\nwhere in Eqn. (74) we used the fact that kA \u2297 Bk \u2264 kAkF kBkF .\n29\n\n\u03c1\nmn\n\n(73)\n(74)\n(75)\n(76)\n\n\fWe now bound the variance terms.\n\u03c32 =\n\nX\ni,j\n\n=\n\n\u0002 \u0003\nE A2ij\n\nX\ni,j\n\n(77)\n\n\u00032 2\u03c12 PQ\u22a5 (\u0113i e\u2217j ) \u2297 PQ\u22a5 (\u0113i e\u2217j )\n\u03c12\n\u03c1 PQ\u22a5 (\u0113i e\u2217j ) \u2297 PQ\u22a5 (\u0113i e\u2217j ) \u2212\n+ 2 2 PQ\u22a5\nmn\nm n\n\u0002\n\n!\n\n(78)\n\nWe let P\u03a9ij denote the orthogonal projector onto the subspace span(\u0113i e\u2217j ). Clearly, we have\nX\nP\u03a9ij .\nP\u03a9 =\n(i,j)\u2208\u03a9\n\nFurthermore, we note that\nPQ\u22a5 (\u0113i e\u2217j ) \u2297 PQ\u22a5 (\u0113i e\u2217j ) = PQ\u22a5 P\u03a9ij PQ\u22a5 .\nThus, we get\nX 2\u03c12 PQ\u22a5 (\u0113i e\u2217j ) \u2297 PQ\u22a5 (\u0113i e\u2217j )\ni,j\n\nmn\n\n2\u03c12 PQ\u22a5\n\n=\n\n=\n\n\u03c1\n\n=\n\n\u0011\nP\u03a9ij PQ\u22a5\n\n2\u03c12 PQ\u22a5\n.\nmn\n\nX\ni,j\n\ni,j\n\ni,j\n\nmn\n\n=\nSimilarly, we have\nX \u0002\n\u00032\n\u03c1 PQ\u22a5 (\u0113i e\u2217j ) \u2297 PQ\u22a5 (\u0113i e\u2217j )\n\n\u0010P\n\nPQ\u22a5 P\u03a9ij PQ\u22a5 P\u03a9ij PQ\u22a5\n\n\uf8f6\n\uf8eb\nX\nP\u03a9ij PQ\u22a5 P\u03a9ij \uf8f8 PQ\u22a5 .\n\u03c1PQ\u22a5 \uf8ed\ni,j\n\nLet X \u2208 Rm\u00d7n be any matrix satisfying kXkF = 1. Then,\nX\ni,j\n\nP\u03a9ij PQ\u22a5 P\u03a9ij X\n\n=\n\nX\ni,j\n\nF\n\n=\n\nX\ni,j\n\nP\u03a9ij\n\np\nX\n\nP\u03a9ij\n\np\nX\n\nk=1\n\nk=1\n\nhGk , P\u03a9ij Xi Gk\n\n!\n\nhP\u03a9ij Gk , Xi Gk\n\n!\n\nwhere we recall that the Gi 's constitute an orthonormal basis for Q\nnow bound kP\u03a9ij Gk kF as follows:\nkP\u03a9ij Gk kF\n\n= | h\u0113i e\u2217j , PQ\u22a5 Gk i |\n= | hPQ\u22a5 \u0113i e\u2217j , Gk i |\n\n\u22a5\n\n,\nF\n\nsatisfying maxi kGi k2 < \u03bd/n. We\n\n\u2264 kGk kkPQ\u22a5 \u0113i e\u2217j k\u2217\n\u221a\n\u2264 kGk k n kPQ\u22a5 \u0113i e\u2217j kF\nr\nr\n\u03bd \u221a\n\u03bdp\n\u2264\nn\nn\nn\nr\np\n.\n= \u03bd\nn\n30\n\nF\n\n\fCombining the above bound with H\u00f6lder's inequality, we get\nX\ni,j\n\n\u2264\n\nP\u03a9ij PQ\u22a5 P\u03a9ij (X)\nF\n\nX\n\ni,j,k\n\n\u2264\u03bd\n\nr\n\n\u2264\u03bd\n\nr\n\nhP\u03a9ij Gk , Xi P\u03a9ij Gk\n\n(79)\nF\n\n\uf8eb\n\n\uf8f6\n\np \uf8ed\nP\u03a9ij \uf8f8\nn\ni,j\nX\n\np\nX\n\nGk\n\nk=1\n\n!\n\n(80)\nF\n\np3\n.\nn\n\n(81)\n\nTherefore, we have that the variance in Eqn. (78) can be bounded as\nr\n2\u03c12\n\u03c12\np3\n2\n+\n+\n\u03c3 \u2264 \u03c1\u03bd\nn\nmn mn\nr\np3\n\u2264 2\u03c1\u03bd\nn\nApplying the matrix Bernstein inequality (Theorem 3), we get\n\u0012\n\u0013\nt2\nP [kAk > t] \u2264 2m2 exp \u2212 2\n2\u03c3 + 3St\n\u2264\n\nt2\n\n2\n\n2m exp \u2212\n\nC1 \u03c1\u03bd\n\nLet us set t = \u03c1. Now, suppose that\n\np\np3 /m + C2 \u03bdpt/m\n\n\u03bd 2 p3 log m\n\u2264 C3 \u03c12 ,\nn\n\n!\n\n.\n\n(82)\n\nwhere C3 > 0 is a numerical constant. Then, under the conditions of Theorem 2, kAk is bounded from\nabove by \u03c1 with high probability. Since A = PQ\u22a5 P\u03a9 PQ\u22a5 \u2212 \u03c1PQ\u22a5 , this implies that\nkPQ\u22a5 P\u03a9 PQ\u22a5 k \u2264 2\u03c1,\n\n(83)\n\nkPQ\u22a5 P\u03a9 k < 1/2,\n\n(84)\n\nwith high probability. It follows that\n\nwith high probability, provided that \u03c1 is sufficiently small and \u03bd 2 p3 log m/n \u2264 C, where C is a numerical\nconstant.\n\nReferences\n[1] J. Wright, A. Yang, A. Ganesh, Y. Ma, and S. Sastry, \"Robust face recognition via sparse representation,\" IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 2, Feb 2009.\n[2] M. Fazel, H. Hindi, and S. Boyd, \"Rank minimization and applications in system theory,\" in American Control Conference, June 2004.\n[3] C. Papadimitriou, P. Raghavan, H. Tamaki, and S. Vempala, \"Latent semantic indexing: A probabilistic analysis,\" Journal of Computer and System Sciences, vol. 61, no. 2, Oct 2000.\n31\n\n\f[4] C. Eckart and G. Young, \"The approximation of one matrix by another of lower rank,\" Psychometrika, vol. 1, pp. 211\u2013218, 1936.\n[5] I. Jolliffe, Principal Component Analysis.\n\nSpringer-Verlag, 1986.\n\n[6] E. Cand\u00e8s, X. Li, Y. Ma, and J. Wright, \"Robust principal component analysis?\" Journal of the\nACM, vol. 58, no. 7, May 2011.\n[7] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. Willsky, \"Rank-sparsity incoherence for matrix\ndecomposition,\" SIAM Journal on Optimization, vol. 21, no. 2, pp. 572\u2013596, 2011.\n[8] D. Hsu, S. M. Kakade, and T. Zhang, \"Robust matrix decomposition with sparse corruptions,\"\nIEEE Transactions on Information Theory, vol. 57, no. 11, pp. 7221\u20137234, 2011.\n[9] Z. Zhou, X. Li, J. Wright, E. Cand\u00e8s, and Y. Ma, \"Dense error correction for low-rank matrices via\nprincipal component pursuit,\" in IEEE International Symposium on Information Theory, 2010.\n[10] A. Ganesh, J. Wright, X. Li, E. Cand\u00e8s, and Y. Ma, \"Dense error correction for low-rank matrices\nvia principal component pursuit,\" in IEEE International Symposium on Information Theory, 2010.\n[11] L. Wu, A. Ganesh, B. Shi, Y. Matsushita, Y. Wang, and Y. Ma, \"Robust photometric stereo via\nlow-rank matrix completion and recovery,\" in Asian Conference on Computer Vision, 2010.\n[12] Z. Zhang, A. Ganesh, X. Liang, and Y. Ma, \"TILT: Transform Invariant Lowrank Textures,\" International Journal of Computer Vision, 2011. [Online]. Available:\nhttp://dx.doi.org/10.1007/s11263-012-0515-x\n[13] Y. Peng, A. Ganesh, J. Wright, W. Xu, and Y. Ma, \"RASL: Robust Alignment by Sparse and\nLow-rank decomposition,\" To appear in IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011.\n[14] E. Cand\u00e8s and B. Recht, \"Exact matrix completion via convex optimzation,\" Found. of Comput.\nMath., 2008.\n[15] E. Cand\u00e8s and T. Tao, \"The power of convex relaxation: Near-optimal matrix completion,\" to\nappear in IEEE Transactions on Information Theory, 2009.\n[16] D. Gross, \"Recovering low-rank matrices from few coefficients in any basis,\" preprint, 2009.\n[17] X. Li, \"Compressed sensing and matrix completion with constant proportion of corruptions,\" 2011,\navailable at http://arxiv.org/abs/1104.1041.\n[18] J. Wright, A. Ganesh, K. Min, and Y. Ma, \"Compressive principal component pursuit,\" 2012,\nsubmitted.\n[19] M. Ledoux, The Concentration of Measure Phenomenon.\n\nAmerican Mathematical Society, 2001.\n\n[20] M. Rudelson and R. Vershynin, \"Non-asymptotic theory of random matrices: extreme singular\nvalues,\" in Proc. of International Congress of Mathematicians, 2010.\n[21] E. Cand\u00e8s and J. Romberg, \"Sparsity and incoherence in compressive sampling,\" Inverse Problems,\nvol. 23, no. 3, pp. 969\u2013985, 2007.\n[22] J. Tropp, \"User-friendly tail bounds for sums of random matrices,\" Foundations of Computational\nMathematics, 2011. [Online]. Available: http://dx.doi.org/10.1007/s10208-011-9099-z\n[23] R. Vershynin, \"Introduction to the non-asymptotic analysis of random matrices,\" 2011, available at\nhttp://www-personal.umich.edu/ romanv/papers/non-asymptotic-rmt-plain.pdf.\n\n32\n\n\f"}