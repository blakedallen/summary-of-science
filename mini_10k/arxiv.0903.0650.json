{"id": "http://arxiv.org/abs/0903.0650v1", "guidislink": true, "updated": "2009-03-03T23:05:04Z", "updated_parsed": [2009, 3, 3, 23, 5, 4, 1, 62, 0], "published": "2009-03-03T23:05:04Z", "published_parsed": [2009, 3, 3, 23, 5, 4, 1, 62, 0], "title": "Compressive Sensing Using Low Density Frames", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0903.4319%2C0903.1777%2C0903.5056%2C0903.0650%2C0903.3430%2C0903.2796%2C0903.0317%2C0903.0833%2C0903.5393%2C0903.1535%2C0903.5348%2C0903.1589%2C0903.3378%2C0903.1480%2C0903.5253%2C0903.3638%2C0903.1891%2C0903.4210%2C0903.3265%2C0903.1863%2C0903.2591%2C0903.3006%2C0903.0172%2C0903.2220%2C0903.3806%2C0903.5115%2C0903.4333%2C0903.4651%2C0903.5511%2C0903.4738%2C0903.1935%2C0903.1115%2C0903.3251%2C0903.1279%2C0903.2285%2C0903.1145%2C0903.3688%2C0903.4480%2C0903.3798%2C0903.0687%2C0903.1429%2C0903.1137%2C0903.5437%2C0903.3272%2C0903.5297%2C0903.4746%2C0903.5128%2C0903.2919%2C0903.1258%2C0903.3017%2C0903.2951%2C0903.3562%2C0903.4343%2C0903.1247%2C0903.2958%2C0903.2072%2C0903.4220%2C0903.3425%2C0903.5252%2C0903.5470%2C0903.2243%2C0903.1941%2C0903.3386%2C0903.2686%2C0903.0641%2C0903.3074%2C0903.3556%2C0903.1175%2C0903.0206%2C0903.3476%2C0903.2064%2C0903.3722%2C0903.3441%2C0903.0267%2C0903.4927%2C0903.3471%2C0903.0993%2C0903.0718%2C0903.5408%2C0903.2920%2C0903.4701%2C0903.1668%2C0903.3613%2C0903.4728%2C0903.0966%2C0903.5073%2C0903.4016%2C0903.2847%2C0903.0147%2C0903.3548%2C0903.4218%2C0903.4708%2C0903.0995%2C0903.0096%2C0903.4941%2C0903.4141%2C0903.1893%2C0903.2868%2C0903.1192%2C0903.3075%2C0903.4872&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Compressive Sensing Using Low Density Frames"}, "summary": "We consider the compressive sensing of a sparse or compressible signal ${\\bf\nx} \\in {\\mathbb R}^M$. We explicitly construct a class of measurement matrices,\nreferred to as the low density frames, and develop decoding algorithms that\nproduce an accurate estimate $\\hat{\\bf x}$ even in the presence of additive\nnoise. Low density frames are sparse matrices and have small storage\nrequirements. Our decoding algorithms for these frames have $O(M)$ complexity.\nSimulation results are provided, demonstrating that our approach significantly\noutperforms state-of-the-art recovery algorithms for numerous cases of\ninterest. In particular, for Gaussian sparse signals and Gaussian noise, we are\nwithin 2 dB range of the theoretical lower bound in most cases.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0903.4319%2C0903.1777%2C0903.5056%2C0903.0650%2C0903.3430%2C0903.2796%2C0903.0317%2C0903.0833%2C0903.5393%2C0903.1535%2C0903.5348%2C0903.1589%2C0903.3378%2C0903.1480%2C0903.5253%2C0903.3638%2C0903.1891%2C0903.4210%2C0903.3265%2C0903.1863%2C0903.2591%2C0903.3006%2C0903.0172%2C0903.2220%2C0903.3806%2C0903.5115%2C0903.4333%2C0903.4651%2C0903.5511%2C0903.4738%2C0903.1935%2C0903.1115%2C0903.3251%2C0903.1279%2C0903.2285%2C0903.1145%2C0903.3688%2C0903.4480%2C0903.3798%2C0903.0687%2C0903.1429%2C0903.1137%2C0903.5437%2C0903.3272%2C0903.5297%2C0903.4746%2C0903.5128%2C0903.2919%2C0903.1258%2C0903.3017%2C0903.2951%2C0903.3562%2C0903.4343%2C0903.1247%2C0903.2958%2C0903.2072%2C0903.4220%2C0903.3425%2C0903.5252%2C0903.5470%2C0903.2243%2C0903.1941%2C0903.3386%2C0903.2686%2C0903.0641%2C0903.3074%2C0903.3556%2C0903.1175%2C0903.0206%2C0903.3476%2C0903.2064%2C0903.3722%2C0903.3441%2C0903.0267%2C0903.4927%2C0903.3471%2C0903.0993%2C0903.0718%2C0903.5408%2C0903.2920%2C0903.4701%2C0903.1668%2C0903.3613%2C0903.4728%2C0903.0966%2C0903.5073%2C0903.4016%2C0903.2847%2C0903.0147%2C0903.3548%2C0903.4218%2C0903.4708%2C0903.0995%2C0903.0096%2C0903.4941%2C0903.4141%2C0903.1893%2C0903.2868%2C0903.1192%2C0903.3075%2C0903.4872&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider the compressive sensing of a sparse or compressible signal ${\\bf\nx} \\in {\\mathbb R}^M$. We explicitly construct a class of measurement matrices,\nreferred to as the low density frames, and develop decoding algorithms that\nproduce an accurate estimate $\\hat{\\bf x}$ even in the presence of additive\nnoise. Low density frames are sparse matrices and have small storage\nrequirements. Our decoding algorithms for these frames have $O(M)$ complexity.\nSimulation results are provided, demonstrating that our approach significantly\noutperforms state-of-the-art recovery algorithms for numerous cases of\ninterest. In particular, for Gaussian sparse signals and Gaussian noise, we are\nwithin 2 dB range of the theoretical lower bound in most cases."}, "authors": ["Mehmet Ak\u00e7akaya", "Jinsoo Park", "Vahid Tarokh"], "author_detail": {"name": "Vahid Tarokh"}, "author": "Vahid Tarokh", "arxiv_comment": "11 pages, 6 figures, Submitted to IEEE Transactions on Signal\n  Processing", "links": [{"href": "http://arxiv.org/abs/0903.0650v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0903.0650v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0903.0650v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0903.0650v1", "journal_reference": null, "doi": null, "fulltext": "DRAFT\n\n1\n\nCompressive Sensing Using Low Density Frames\n\narXiv:0903.0650v1 [cs.IT] 3 Mar 2009\n\nMehmet Ak\u00e7akaya, Jinsoo Park and Vahid Tarokh\n\nAbstract- We consider the compressive sensing of a sparse or\ncompressible signal x \u2208 RM . We explicitly construct a class of\nmeasurement matrices, referred to as the low density frames,\nand develop decoding algorithms that produce an accurate\nestimate x\u0302 even in the presence of additive noise. Low density\nframes are sparse matrices and have small storage requirements. Our decoding algorithms for these frames have O(M )\ncomplexity. Simulation results are provided, demonstrating that\nour approach significantly outperforms state-of-the-art recovery\nalgorithms for numerous cases of interest. In particular, for\nGaussian sparse signals and Gaussian noise, we are within 2\ndB range of the theoretical lower bound in most cases.\nIndex Terms- Low density frames, compressive sensing, sum\nproduct algorithm, expectation maximization, Gaussian scale\nmixtures\n\nmin ||x||1\n\nI. I NTRODUCTION\nET x = (x1 , . . . , xM ) \u2208 RM with ||x||0 = |{xi : xi 6=\n0}| = L. x is said to be sparse if L << M . Consider\nthe equation\ny = Ax + n,\n(1)\n\nL\n\nwhere A is an N \u00d7 M measurement matrix and n \u2208 RN is a\nnoise vector. When N < M , y is called the compressively\nsensed version of x with measurement matrix A. In this\npaper, we are interested in coming up with a good estimate\nx\u0302 for a sparse vector x from the observed vector y and the\nmeasurement matrix A.\nWe refer to the case n = 0 as noiseless compressive sensing.\nThis is the only case when x can be perfectly recovered. In\nparticular, it can be shown [13], [30] that if A has the property\nthat every of its N columns are linearly independent, then a\ndecoder can recover x uniquely from N = 2L samples by\nsolving the `0 minimization problem\nmin ||x||0\n\ns. t.\n\ny = Ax.\n\n(2)\n\nHowever, solving this `0 minimization problem for general\nA is NP-hard [42]. An alternative solution method proposed\nin the literature is the `1 regularization approach, where\nmin ||x||1\n\ns. t.\n\ny = Ax,\n\nIt can be shown that there is a relationship between the\nsolution to problem (1) and minimum Hamming distance\nproblem in coding theory [1], [2], [35]. This approach was\nfurther exploited in [50]. Using this connection, we constructed ensembles of measurement matrices1 and associated\ndecoding algorithms that solves the `0 minimization problem\nwith complexity O(M N ) for L = O(M ) with N = O(L) in\nthe noiseless case [1], [2].\nFor problem (1) with non-zero n, referred to as noisy\ncompressive sensing, the `1 regularization approach of (3) can\nalso be applied. For a measurement matrix A that satisfies\na property called the restricted isometry principle (RIP), the\nquadratic program\n\n(3)\n\nis solved instead. Criteria has been established in the literature\nas to when the solution of (3) coincides with that of (2) in the\nnoiseless case for various classes of measurement matrices\n[13], [16]. In an important contribution, for A belonging to\nthe classes of Gaussian and partial Fourier ensembles, Cand\u00e8s\nand Tao showed [13] that this recovery problem can be solved\nfor L = O(M ) with N = O(L) as long as the observations\nare not contaminated with (additive) noise.\nM. Ak\u00e7akaya, J. Park and V. Tarokh are with the School of Engineering\nand Applied Sciences, Harvard University, Cambridge, MA, 02138. (e-mails:\n{akcakaya, vahid}@seas.harvard.edu, park10@fas.harvard.edu)\n\ns. t.\n\n||Ax \u2212 y||2 \u2264 \u000f,\n\ncan be solved for a parameter \u000f related to ||n||2 , and an\nestimate x\u0302QP can be obtained such that ||x\u0302QP \u2212 x||2 \u2264 C1 \u000f,\nwhere C1 is a constant that depends on A [11]. If n \u223c\nN (0, \u03c3 2 IN ), another approach is the Dantzig Selector\nmin ||x||1\n\ns. t.\n\n||A\u2217 (Ax \u2212 y)||\u221e \u2264 \u03b3,\n\nwhere \u03b3 is a function of \u03c3 and M . This gives\nP an estimate\nx\u0302DS such that En ||x\u0302DS \u2212 x||22 \u2264 C2 (log M ) i min(x2i , \u03c3 2 ),\nwhere C2 is a constant that depends on A [12]. Both these\nmethods may not be easily implemented in real time with\nthe limitations of today's hardware. To improve the running\ntime of `1 methods, some authors have investigated using\nsparse matrices for A [6]. Using the expansion properties\nof the graphs represented by such matrices, it was shown\nin [6] that it is possible to obtain an estimate x\u0302E such that\n||x\u0302E \u2212 x||1 \u2264 C3 ||n||1 , where C3 is a constant that depends\non A.\nAnother strand of work studies recovery algorithms based\non the matching pursuit algorithm [44]. Recently, variants\nof this algorithm, e.g. Subspace Pursuit [17] and CoSaMP\n[30], have been proposed. Both algorithms provably work for\nmeasurement matrices satisfying RIP, and guarantee perfect reconstruction in the noiseless setting for N = O(L log(M/L))\nas the `1 recovery methods do. For the noisy problem, they\nalso offer similar guarantees to `1 methods. These algorithms\nhave complexity O(L log R), where L is the complexity of\nmatrix-vector multiplication (O(M N ) for Gaussian matrices,\nO(N log N ) for partial Fourier ensembles) and R is a precision parameter bounded above by ||x||2 (which is O(N ) for a\nfixed signal-to-noise ratio). In [7], Sparse Matching Pursuit\n(SMP) was proposed for sparse A and this algorithm has\nO(M log(M/L)) complexity .\n1 We use the terms \"frame\" and \"measurement matrix\" interchangeably\nthroughout the rest of the paper.\n\n\f2\n\nDRAFT\n\nYet another direction in compressive sensing is the use of\nthe Bayesian approach. In [23], the idea of relevance vector\nmachine (RVM) [40] has been applied to compressive sensing.\nAlthough simulation results indicate that the algorithm has\ngood performance, it has complexity O(M N 2 ).\nIn this paper, we study the construction of measurement\nmatrices that can be stored and manipulated efficiently in\nhigh dimensions, and fast decoding algorithms that generate\nestimates with small `2 distortion. The ensemble of measurement matrices are a generalization of LDPC codes and we\nrefer to them as low density frames (LDFs). For our decoding\nalgorithms, we combine various ideas from coding theory,\nstatistical learning theory and theory of estimation. Simulation\nresults are provided indicating an excellent distortion performance at high levels of sparsity and for high levels of noise.\nThe outline of this paper is given next. In Section II, we\nintroduce low density frames and study their basic properties.\nIn Section III, we introduce various concepts used in algorithms and describe the decoding algorithms. In Section IV, we\nprovide extensive simulation results for a number of different\ntesting criteria. Finally in Section V, we make our conclusions\nand provide directions for future research.\nII. L OW D ENSITY F RAMES\nLet F = {\u03c61 , \u03c62 , * * * , \u03c6M } be a frame consisting of\nM \u2265 N non-zero vectors which span RN . Let \u03c6i =\n(\u03c61,i , * * * , \u03c6N,i ) for i = 1, 2, * * * , M . This frame could be\nrepresented in matrix form as an N \u00d7 M matrix\n\uf8eb\n\uf8f6\n\u03c61,1 \u03c61,2 * * * \u03c61,M\n\uf8ec \u03c62,1 \u03c62,2 * * * \u03c62,M \uf8f7\n\uf8ec\n\uf8f7\nF=\uf8ec .\n(4)\n\uf8f7.\n..\n..\n..\n\uf8ed ..\n\uf8f8\n.\n.\n.\n\u03c6N,1 \u03c6N,2 * * * \u03c6N,M\nA low density frame (LDF) F is defined by a matrix F\nwhere the vast majority of elements of each column and each\nrow of F are zeroes. Formally, we define a (dv , dc )-regular\nLDF as a matrix F that has dc non-zero elements in each row\nand dv non-zero elements in each column. Clearly M dv =\nN dc . We also note that the redundancy of the frame is r =\nM/N = dc /dv . We will restrict ourselves to binary regular\nLDFs, where the non-zero elements of F are ones.\nThe density \u03c1 of a frame F is the ratio of the number of\nnon-zero entries of F to the dimension of F. In this paper,\nwe consider regular LDFs for which \u03c1 = (M dv )/(M N ) =\ndv /N << 1.\nAs with LDPC codes, it is natural to represent LDFs\nusing bipartite graphs. Furthermore, there is a well-established\nliterature on inference in graphical models. Some of these\nmethods can be used as a basis for recovery algorithms in\nthe context of compressive sensing. To this end, we next\nsummarize two important ideas from graphical models, namely\nfactor graphs and the sum-product algorithm, and show how\nLDFs can be viewed as factor graphs.\nA. Factor Graphs\nFactor graphs are used to represent factorizations of functions of several variables [9], [24]. Let f (w) be a function of\n\nFig. 1.\n\nFactor graph representing the function in Equation 6.\n\nseveral variables that can be factored as\nY\nf (w) =\nfs (ws ).\n\n(5)\n\ns\n\nIn this factorization each factor fs is only a factor of ws , the\nsubset of variable nodes w.\nA factor graph depicting (5) consists of variable nodes represented by circles, factor nodes represented by bold squares,\nand undirected edges connecting each factor node to all the\nvariable nodes involved in that factor.\nAs an example, the factor graph representing\nf (w) = fa (w1 , w2 , w3 )fb (w2 , w3 , w4 )fc (w1 , w4 )\n\n(6)\n\nis depicted in Figure 1.\nB. Sum-Product Algorithm\nThe natural inference algorithm for factor graphs is the sumproduct algorithm [9], [21]. This algorithm is an exact interference algorithm for tree-structured graphs (i.e. graphs with\nno cycles), and is usually described over discrete alphabets.\nHowever, the ideas also apply to continuous random variables\nwith the sum being replaced by integration. In doing so, the\ncomputational cost of implementation increases and this issue\nwill be addressed later.\nSuppose the goal is to find the marginal density p(w) for a\nparticular variable w. In particular, we have\nX\np(w) =\np(w).\nw\\w\n\nOne treats w to be the root node of a tree, and looks at the\nsubtrees connected to w via factor nodes. Using this approach,\nthe joint distribution can be written as\nY\np(w) =\nFs (w, Ws ),\n(7)\ns\u2208ne(w)\n\nwhere ne(w) is the neighborhood of w, i.e. the set of factor\nnodes that are connected to w, and Ws is the set of variable\nnodes in the subtree connected to the factor node fs in ne(w)\n[9]. Fs (w, Ws ) represents the product of the factors in the\nsubtree associated with fs . Interchanging the summation and\nthe products yields\nY\np(w) =\n\u03bcfs \u2192w (w),\ns\u2208ne(w)\n\nwhere \u03bcfs \u2192w (w) is the message sent from factor node fs to\nvariable node w. One can show [9] that\nX\nY\n\u03bcfs \u2192w (w) =\nfs (w, ws )\n\u03bcwm \u2192fs (wm ),\nws \\w\n\nm\u2208ne(fs )\\w\n\n\fDRAFT\n\n3\n\nwhere ws are all variable nodes connected to the factor node\nfs , including w, and ne(fs ) are the set of variable nodes\nconnected to the factor node fs . One can also show [9]\nY\n\u03bcwm \u2192fs (wm ) =\n\u03bcfl \u2192wm (wm ).\nl\u2208ne(wm )\\fs\n\nThus there are two types of messages, one type going\nfrom factor nodes to variable nodes denoted \u03bcf \u2192w and the\nother going from variable nodes to factor nodes denoted\n\u03bcw\u2192f . The message propagation starts from the leaves of the\nfactor graph. A leaf variable node sends an identity message\n\u03bcw\u2192f (w) = 1 to its parent, whereas a leaf factor node f sends\n\u03bcf \u2192w (w) = f (w), a description of f to its parent. These\nexpressions for messages give an efficient way of calculating\nthe marginal probability distribution. We note that in writing\nout the factorization in (7), it is essential that the graph has\na tree structure so that the factors in the joint probability\ndistribution p(w) can be partitioned into groups, each of which\nis associated with a single factor node in ne(w).\nThe algorithm is easily modified to calculate the marginal\nfor every variable node in the graph [9]. This modification\nresults in only twice as many calculations as calculating a\nsingle marginal. A more interesting case is when there are\nobserved variables in the graph, v. In this case the sum-product\nalgorithm could be used to calculate posterior marginals\np(wi |v = v\u0302).\nC. Graphical Representation of Low Density Frames\nThe main connection between the `0 minimization problem\nand coding theory involves the description of the underlying\ncode [1], V of F, where\nV = {d \u2208 RM : Fd = 0}.\nOne can view V as the set of vectors whose product with each\nrow of F \"checks\" to 0. In the works of Tanner, it was noted\nthat this relationship between the checks and the codewords\nof a code can be represented by a bipartite graph [39]. This\nbipartite graph consists of two disjoint sets of vertices, V\nand C, where V contains the variable nodes and C contains\nthe factor nodes representing checks that codewords need to\nsatisfy. Thus we have |V | = M and |C| = N . Furthermore\nnode j in V will be connected to node i in C if and only if the\n(i, j)th element of F is non-zero. Thus the number of edges\nof the graph is equal to the number of non-zero elements in\nthe measurement matrix F. For an LDF, this leads to a sparse\nbipartite graph.\nA simple example of this graphical representation is depicted in Figure 2. For representation of LDFs, it is convenient\nto use a factor node, depicted by a \u0001, called a parity check\nnode. This node has the property that the variable nodes\nconnected to it should sum to zero. We also note that for the\npurposes of decoding, it is more convenient to use syndromes\n[26] that represent the measurement vector, r. This is done by\nconnecting a variable node representing the j th component of\nr to the j th check node. In this case, the parity check node has\nthe property that the variable nodes connected to it sum to rj .\nThus the graph now represents the set {d \u2208 RM : Fd = r}\nwhich is a coset of the underlying code of the frame.\n\nFig. 2.\n\nA frame F and its graphical representation.\n\nIt is important to note that the graph representing an LDF\nwill have cycles. Without the tree structure, the sum-product\nalgorithm will only be an approximate inference algorithm.\nHowever, it has been empirically shown that for sparse graphs\nthis approximate algorithm works very well [25], [33], [48].\nIII. S UM P RODUCT A LGORITHM WITH E XPECTATION\nM AXIMIZATION\nIt is well-known in coding theory literature, that the standard decoding algorithm for codes on graphs is the sumproduct algorithm (SPA) [21], [24], [25]. Given a set of\nobservations, this algorithm can be used to approximate the\nposterior marginal distributions. In fact, when there is no\nnoise, variants of this algorithm [38] has been successfully\nadapted to compressive sensing [35], [50]. However, when\nwe are interested in the practical case of noisy observations,\nthese algorithms no longer can be applied in a straightforward\nmanner. Some authors have tried to circumvent this difficulty\nby using a two-point Gaussian mixture approach [36], however\nthe complexity of this algorithm may grow quickly as the\nnumber of Gaussian components in the mixtures could grow\nexponentially, unless some approximation is applied. However,\nusing these approximations degrades the performance of the\nLDF approach.\nIn this paper, we consider Gaussian Scale Mixture (GSM)\npriors with Jeffreys' non-informative hyperprior to obtain an\nalgorithm that provides estimates for the noisy compressive\nsensing problem\nr = Fx + n,\nas well as the noiseless problem. Throughout the paper we\nassume that\nn \u223c N (0, \u03c3 2 IN ).\nHowever simulation results (not included in this paper) indicate that the algorithms still work well even for non-Gaussian\nnoise. We define the signal-to-noise ratio (SNR) as\nSNR = 10 log10\n\n||Fx||2\n||Fx||2\n= 10 log10 2\n.\n2\nEn ||n||\n\u03c3 N\n\nA. Gaussian Scale Mixtures\nThe main difficulty in using the sum-product algorithm\n(SPA) in compressive sensing setting is that the variables of\ninterest are continuous. Nonetheless SPA can be employed\nnaturally when the underlying continous random variables are\nGaussian [47]. Since any Gaussian pdf N (x|a, A) can be\n\n\f4\n\nDRAFT\n\ndetermined by its mean a and variance A, these constitute the\nmessages in this setting. At the variable nodes, the product\nof Gaussian probability density functions (pdf) will result in a\n(scaled) Gaussian pdf, and at the check nodes, the convolution\nof Gaussian pdfs will also result in a Gaussian pdf. i.e.\nN (x|a1 , A1 ) \u2217 N (x|a2 , A2 ) \u221d N (x|a1 + a2 , A1 + A2 ),\nand\nN (x|a1 , A1 ) * N (x|a2 , A2 ) \u221d N (x|b, B),\nwhere \u221d denotes normalization up to a constant, and\n\nFig. 3. Contour plots for a Gaussian distribution (left), a GSM with \u03b21 = \u03b22\ndistributed according to Jeffreys' prior (middle), a GSM with \u03b21 and \u03b22 i.i.d.\nwith Jeffreys' prior (right).\n\n\u22121 \u22121\nB = (A\u22121\n,\n1 + A2 )\n\u22121\nb = B(A\u22121\n1 a1 + A2 a2 ).\n\nWe note that all the underlying operations for SPA preserve\nthe Gaussian structure.\nIt is well-known that the Gaussian pdf is not \"sparsityenhancing\". Thus some authors propose the use of the Laplacian prior\nY\nY\u03bb\nexp(\u2212\u03bb|xi |).\n(8)\np(x) =\npxi (xi ) =\n2\ni\ni\nClearly with this prior and for Gaussian noise n\n\u0001\np(x|y) \u221d p(y|x)p(x) \u221d exp \u2212 ||y \u2212 Ax||22 \u2212 \u03bb0 ||x||1 , (9)\nand maximization of p(x|y) is equivalent to minimizing\n||y \u2212 Ax||22 + \u03bb0 ||x||1 ,\nwhich is the objective function for the LASSO algorithm\n[43], [46]. However, a straightforward implementation of this\nalgorithm may not be computationally feasible.\nIn this paper, we consider the family of Gaussian Scale\nMixtures (GSM) densities [4], given by\np\nx = \u03b2u,\n\u221a\nwhere u is a zero-mean Gaussian and \u03b2 is a positive scalar\nrandom variable. Hence\npx|\u03b2 (x|\u03b2) \u223c N (x|0, \u03b2),\nand\n\nZ\npx (x) =\n\nwhich has no parameters to optimize. We note that this is an\nimproper density, i.e. it cannot be normalized. In Bayesian\nstatistics, these kind of improper priors are used frequently,\nsince only the relative weight of the prior determines the aposteriori density [34]. This density also has a singularity at\nthe origin. This fact is usually ignored as long as it does\nnot create computational problems [32]. As an alternative one\nmight set the prior to 0 in a small interval \u03b2 \u2208 [0, \u03b2min ). We\nalso note that with this choice for p\u03b2i (\u03b2i ), pxi (xi ) \u221d 1/|xi |,\nwhich is a very heavy-tailed density.\nTo enhance sparsity in each coordinate, it is important to\nhave independent \u03b2i for all i [41]. As depicted in the middle\nsubplot of Figure 3, compared to a Gaussian distribution, a\nGSM with \u03b2i distributed according to Jeffreys' prior has a\nmuch sharper peak at the origin even when \u03b21 = \u03b22 . However,\nthe subplot on the right demonstrates that if the \u03b2i s are indeed\nindependent, the GSM will be highly concentrated not only\naround the origin, but along the coordinate axes as well, which\nis a desired property if we have no further information about\nthe locations of the sparse coefficients of x. In our model, we\nwill assume that\np(x, \u03b2) =\n\nM\nY\n\np(xi |\u03b2i )\n\ni=1\n\nM\nY\n\np(\u03b2i )\n\ni=1\n\nin order to enhance sparsity in all coordinates. This independence assumption is natural and commonly used in the\nliterature [18], [43], [46].\n\n\u221e\n\npx|\u03b2 (x|\u03b2)p\u03b2 (\u03b2)d\u03b2.\n0\n\nThis family of densities are symmetric, zero-mean and have\nheavier tails than a Gaussian, and have been successfully used\nin image processing [8], [18], [32], and learning theory [40].\nIn order to completely specify our model, we need to choose\na pdf for p\u03b2 (\u03b2). In this paper, we use\n!\np\n\u2202 2 log px|\u03b2 (x|\u03b2)\np\u03b2 (\u03b2) \u221d det(I(\u03b2)), I(\u03b2) = E \u2212\n\u03b2\n\u2202\u03b2 2\nwhere I(\u03b2) is the Fisher information matrix. This is referred\nto as the Jeffreys' prior, which can be shown to be a scalar\ninvariant prior suitable for sparse estimation [34]. In our\nmodel, the prior is given by\np\u03b2i (\u03b2i ) =\n\n1\n,\n\u03b2i\n\nB. Expectation Maximization\nThe expectation maximization (EM) algorithm is a method\nfor finding maximum-likelihood (ML) estimates of parameters\nin a model with observed and hidden variables [29]. Let y be\nthe observed data and let z be the hidden data. Let the probability density function of (y, z) be f (y, z|\u03b8), parametrized\nby the vector \u03b8. The EM algorithm iteratively improves on\nan initial estimate \u03b8 (0) using a two-step procedure. In the\nexpectation step (E-step), we calculate\n\u0010\n\u0011\nQ(\u03b8|\u03b8 (k) ) = Ez log f (y, z|\u03b8)|y, \u03b8 (k)\ngiven an estimate \u03b8 (k) from the previous iteration. It is important to distinguish the two arguments of the Q function are\ndifferent. The second argument is the conditioning argument\nfor the expectation and is fixed during the E-step. In the second\n\n\fDRAFT\n\n5\n\nSince in our setting, the underlying variables are Gaussian,\nthe density p(xi |y, \u03b2 (t) ) produced by the SPA is also Gaussian, with mean \u03bci and variance \u03bdi . One can explicitly write\nout Q(\u03b2i |\u03b2 (t) ) as\n\u0013\n\u0012\n(t)\n(t)\nQ(\u03b2i |\u03b2 ) = Exi log p(xi , \u03b2i ) y, \u03b2\n\u0010\n\u0011\nx2 1 \u0001\u0001\n1\n= Exi log \u221a\ny, \u03b2 (t)\nexp(\u2212 i )\n2\u03b2i \u03b2i\n2\u03c0\u03b2i\n1\n3\n= C2 \u2212 log \u03b2i \u2212\nEx (x2 |y, \u03b2 (t) )\n2\n2\u03b2i i i\n3\n1\n= C2 \u2212 log \u03b2i \u2212\n(\u03bc2 + \u03bdi ),\n(12)\n2\n2\u03b2i i\nFig. 4.\nThe factor graph for a (3,6)-regular LDF with the appropriate\nhyperpriors.\n\nwhere C2 is independent of \u03b2i .\nFor the M-step, we find\n\u03b2 (t+1) = arg max Q(\u03b2|\u03b2 (t) ).\n\u03b2\n\nstep, called the maximization step or M-step, a new estimate\n\u03b8 (k+1) = arg max Q(\u03b8|\u03b8 (k) )\n\u03b8\n\nis calculated.\nIt can be shown that the estimates monotonically increases\nthe likelihood with respect to the observed data y [29],\nf (y|\u03b8 (k+1) ) \u2265 f (y|\u03b8 (k) ).\nWhen \u03b8 is itself a random\nvariable, the M-step maximizes\n\u0001\nQ(\u03b8|\u03b8 (k) ) + log f (\u03b8) , and the EM algorithm can be used to\nfind a maximum a-posteriori (MAP) estimate of \u03b8 [28].\nC. SuPrEM Algorithm I\nThe factor graph for decoding purposes is depicted in Figure\n4. Here, r is the vector of observed variables, x is the vector\nof hidden variables and \u03b2 is the vector of parameters. We\nnext propose the Sum Product with Expectation Maximization\n(SuPrEM) Algorithm I. At every iteration t, this algorithm\nuses a combination of the Sum-Product Algorithm (SPA) and\n(t)\nEM algorithm to generate estimates for the hyperpriors {\u03b2k },\n(t)\nas well as a point estimate {x\u0302k }. In the EM stage of the\nalgorithm, Q(\u03b2|\u03b2 (t) ) for the E-step is given by\nQ(\u03b2|\u03b2\n\n(t)\n\n\u0012\n\n(t)\n\n= C1 +\n= C1 +\n\ni=1\nM\nX\n\n\u0012\nEx log p(xi , \u03b2i ) y, \u03b2\n\u0012\nExi\n\n(t)\n\n\u0013\n\n\u0013\nlog p(xi , \u03b2i ) y, \u03b2 (t) ,\n\n(10)\n\ni=1\n\n\u0001\nwhere C1 = Ex log p(y|x) y, \u03b2 (t) is a term\u0001independent of\n\u03b2. Let Q(\u03b2i |\u03b2 (t) ) = Exi log p(xi , \u03b2i )|y, \u03b2 (t) . We have\nQ(\u03b2|\u03b2 (t) ) = C1 +\n\nM\nX\ni=1\n\n(t+1)\n\n\u03b2i\n\n= arg max Q(\u03b2i |\u03b2 (t) ) =\n\u03b2i\n\n\u03bc2i + \u03bdi\n3\n\n(13)\n\nWe summarize SuPrEM I in Algorithm 1. The inputs to\nthe algorithm contain a stopping criterion T and a messagepassing schedule S. The stopping criterion does not really\naffect the behavior of the algorithm, and there are a few\nalternatives for a reasonable criterion, which are discussed in\nSection IV. It turns out the message-passing schedule is rather\nimportant for achieving the maximum performance. To this\nend, we develop a message-passing schedule that attains such\ngood performance and we describe this schedule in detail in\nAppendix I. For all our simulations, we use this fixed schedule.\nSimulation results indicate that with this fixed schedule, the\nalgorithm is robust in various different scenarios. The overall\ncomplexity of SuPrEM is O(M ) for a fixed number of\niterations. We also note that in the presence of noise, the\noutput of the algorithm will not be exactly sparse and a sparse\nestimate can be constructed using soft-thresholding techniques\nsuch as those described in [18].\n\n\u0013\n\n) = Ex log p(x, y, \u03b2) y, \u03b2\n\u0012\n\u0013\n\u0001\n= Ex log p(y|x)p(x|\u03b2)p(\u03b2) y, \u03b2 (t)\n\u0012\n\u0013\n\u0012\n\u0013\n= Ex log p(y|x) y, \u03b2 (t) + Ex log p(x, \u03b2) y, \u03b2 (t)\nM\nX\n\nClearly Q(\u03b2|\u03b2 (t) ) can be maximized by maximizing each\nQ(\u03b2i |\u03b2 (t) ). Hence we have the simple local update rule\n\nQ(\u03b2i |\u03b2 (t) )\n\n(11)\n\nD. SuPrEM Algorithm II\nWhen the ratio L/N is relatively large, SuPrEM I does not\nperform well, in particular for high SNRs, since it does not\nenforce strict sparsity. Thus we propose SuPrEM Algorithm\nII that enforces sparsity at various stages of the algorithm and\nsends messages between the nodes of the underlying graph\naccordingly. To this end, we keep a set of candidate variable\nnodes O that are likely to have non-zero values, and modify\nthe messages from the variable nodes that do not belong to\na specified subset of O denoted by O1 . Similar ideas have\nbeen used in developing state-of-the-art recovery algorithms\nfor compressive sensing, such as Subspace Pursuit [17] and\nCoSaMP [30]. The full description is given in Algorithm 2.\nThe main modification to SuPrEM I is the addition of a\n(t)\nsparsification step. Intuitively \u03b2k is the reliability of the\n(t)\nhypothesis x\u0302k 6= 0. Throughout the algorithm we maintain\n\n\f6\n\nDRAFT\n\nAlgorithm 1 SuPrEM Algorithm I\nInputs: The observed vector r, the measurement matrix F,\nthe noise level \u03c3 2 , a stopping criterion T , and a messagepassing schedule S.\n(0)\n1. Initialization: Let \u03b2k = |(FT r)k |2 /d2v . Initial outgoing\n(0)\nmessages from variable node xk is (0, \u03b2k ).\n2. Check Nodes: For i = 1, 2, . . . , N\nLet {i1 , i2 , . . . , idc } be the indices of the variable nodes\nconnected to the ith check node ri . Let the message coming\nfrom variable node xij to the check node ri at tth iteration\n(t)\n(t)\nbe (\u03bcij , \u03bdij ) for j = 1, . . . , dc . Then the outgoing message\nfrom check node ri to variable node xij is\nPdc\n(t) Pdc\n(t)\n2\n(ri \u2212 k=1,k6\n=j \u03bcik ,\nk=1,k6=j \u03bdik + \u03c3 ).\nThe messages are sent according to the schedule S.\n3. Variable Nodes: For k = 1, 2, . . . , M\nLet {k1 , k2 , . . . , kdv } be the indices of the check nodes\nconnected to the k th variable node xk . Let the incoming\nmessage from the check node rkj to the variable node xk at\n(t)\n(t)\nthe tth iteration be (\u03bckj , \u03bdkj ) for j = 1, . . . , dv .\na. EM update: Let\n\u0013\u22121\n\u0012\n\u0012\n(t) \u0013\nPdv 1\n(t)\n(t) Pdv \u03bckj\n(t)\n1\n+\n,\n\u03bc\n=\nV\nVk =\n(t\u22121)\nj=1 (t) .\nj=1 (t)\nk\nk\n\u03bdk\n\n\u03b2k\n\nj\n\n\u03bdk\n\nj\n\n(t)\n(t)\n(\u03bck )2 +Vk\n\n(t)\n\u03b2k\n\nThen the EM update is\n=\n.\n3\nb. Message updates: The outgoing message from variable\nnode xk to check node rki at the (t + 1)th iteration is given\n(t+1)\n(t+1)\nby (\u03bcki , \u03bdki ), where\n\u0012\n\u0013\u22121\nPdv\n(t+1)\n1\n1\n\u03bdki\n=\nj=1,j6=i (t) + (t)\n\u03bdk\n\n\u03b2k\n\nj\n\nand\n(t+1)\n\n\u03bcki\n\n(t+1)\n\n\u0012\n\n= \u03bdki\n\n(t)\n\u03bck\nj\n\nPdv\n\nj=1,j6=i \u03bd (t)\nk\n\n\u0013\n.\n\nj\n\nThe messages are sent according to the schedule S.\n4. Iterations:Repeat (2) and (3) until stopping criterion T is\nreached.\n5. Decisions: For the k th variable node xk , let the incoming\n(T )\n(T )\nmessages be (\u03bckj , \u03bdkj ) for j = 1, . . . , dv . Let\n\u0013\u22121\n\u0012\nPdv\n1\n1\nV\u0302k =\nj=1 (T ) + (T )\n\u03bdk\n\n\u03b2k\n\nj\n\nand\n\u0012\nx\u0302k = V\u0302k\n\n(T )\n\nPdv\n\nj=1\n\n\u03bck\n\nj\n(T )\n\u03bdk\nj\n\n\u0013\n.\n\nOutput: The estimate is x\u0302 = (x\u03021 , x\u03022 , . . . , x\u0302M )T .\n\na list of variable nodes O1 that correspond to the largest\nL coefficients of x\u0302(t) at iteration t. We also keep a list of\nvariable nodes O2 corresponding to the L largest elements of\n\u03b2 (t) , i.e. those with the largest reliabilities of the hypothesis\n(t)\nx\u0302k 6= 0. In the sparsification stage, these two sets are merged,\nO = O1 \u222a O2 . The addition and deletion of elements from\nO allow refinements to be made with each iteration. We note\nL \u2264 |O| \u2264 2L at any given iteration. Decisions are made on\nthe elements of O, and O1 is updated. Finally for variable\nnodes not in O1 , the mean value of the messages is forced to\n\nAlgorithm 2 SuPrEM Algorithm II\nInputs: The observed vector r, the measurement matrix F,\nthe sparsity level L, a stopping criterion T , the noise level\n\u03c3 2 (optional), and a message-passing schedule S.\n(0)\n1. Initialization: Let \u03b2k = |(FT r)k |2 /d2v and let O1 = \u2205.\n(0)\nInitial outgoing messages from variable node xk is (0, \u03b2k ).\n2. Check Nodes: Same as in Algorithm I.\n3. Variable Nodes: Same as in Algorithm I.\n4. Sparsification:\na. After the \u03b2k s have been updated, find the indices of the L\nlargest \u03b2k s. Let these indices be O2 .\nb. Merge O1 and O2 , i.e. Let O = O1 \u222a O2 .\nc. For all indices in k \u2208 O make a decision on x\u0302k (as in Step\n5 of Algorithm I). For all indices k \u2208\n/ O, let x\u0302k = 0.\nd. Identify the indices corresponding to the L largest (in\nabsolute value) coefficients of x\u0302. Update O1 to be this set of\nL indices.\ne. The variable vertices k \u2208 O1 , send out their messages as\nwas decided in Step 3 of Algorithm I. The variable vertices\nk\u2208\n/ O1 , send out their messages with 0 mean and the\nvariance that was decided in Step 3 of Algorithm 1.\n5. Decisions: Make decisions only the vertices in O. Once\nthese are calculated, keep the L indices with the largest\n|x\u0302k |, k \u2208 O. Set all other indices to 0.\n6. Iterations: Repeat (2), (3), (4) and (5) until stopping\ncriterion T is reached.\nOutput: The estimate is x\u0302 = (x\u03021 , x\u03022 , . . . , x\u0302M )T .\n\nbe 0, but the variance (i.e. the uncertainty about the estimate\nitself) is kept. By modifying the messages this way, we not\nonly enforce sparsity at the final stage, but also throughout the\nalgorithm.\nWe note that the noise level \u03c3 2 is an optional input to\nthe algorithm. Our simulations indicate that the algorithm\nworks without this knowledge also. However, if this extra\nstatistical information is available, it is easily incorporated into\nthe algorithm in a natural way and results in a performance\nincrease.\nSuPrEM II has complexity O(M ). The only significant\noperation different than those in SuPrEM I is the determination\nof the largest L elements of \u03b2 and x\u0302. This could be done\nwith O(M ) complexity, as described in [15] (Chapter 9). A\nmore straightforward implementation for this stage might use\nsorting of the relevant coefficients, which would result in a\nhigher complexity of O(M log M ) for the overall algorithm.\nE. Reweighted Algorithms\nFor high L/N ratios, simulation results show that SuPrEM\nI and SuPrEM II still perform well. However more iterations\nare needed to achieve very low distortion levels, which may be\nundesirable. Thus we propose a modification to SuPrEM I and\nSuPrEM II to speed up the convergence that uses estimates\ngenerated within a few iterations. In compressive sensing,\nemploying prior estimates to improve the final solution has\nbeen used for `1 approximation [14], but this increases the\nrunning time by a factor of reweighing steps.\n\n\fDRAFT\n\n7\n\nNext, we motivate for our reweighing approach. In our\n(0)\nalgorithms, the initial choice of {\u03b2k = |(FT r)k |2 /d2v } is\nbased on the intuition that \u03b2k must be proportional to |xk |2 .\n(0)\nBy providing a better estimate for the initial {\u03b2k }, the rate\nof convergence may be improved. The algorithm is initiated\nwith \u03b2 (0) as above and is run for Tr1 iterations. At the end\n0\nof this stage, we re-initialize \u03b2 (0) to be\n(0)0\n\n\u03b2k\n\n(Tr1 ) 2\n\n= x\u0302k\n\n2\n\n+ (FT (r \u2212 Fx\u0302(Tr1 ) ))k /d2v ,\n\nand the algorithm is run for Tr2 iterations. This process is\nrepeated\nPRrecursively until convergence or R times. We note\nthat\nk=1 Trk = T , where T is the original number of\nfixed iterations. Thus the total number of iterations remains\nunchanged when we use reweighing.\nIV. S IMULATION D ETAILS\nA. Simulation Setup\nIn our simulations we used LDFs with parameters (3, 6),\n(3, 12) and (3, 24) for M/N = 2, 4, 8 and M = 10000. We\nconstructed these frames using the progressive edge growth\nalgorithm [22], avoiding cycles of length 4 when possible 2 .\nSimulations will be presented for SNR= 12, 24, 36 dB, as well\nas the noiseless case. For various choices of L and SNR, we\nran 1000 Monte-Carlo simulations for each value, where x\nis generated as a signal with L non-zero elements that are\npicked from a Gaussian distribution. The support of x is picked\nuniformly at random.\n\u221a Once x is generated, it is normalized\nsuch that ||Fx||2 = N . Thus SNR= 10 log10 \u03c312 .\nLet G be the genie decoder that has full information about\nsupp(x) = {i : xi 6= 0}. Let the output of this decoder be\nx\u0302genie = G(r) obtained by solving the least squares problem\ninvolving r and the matrix formed by the columns of F\nspecified by supp(x). We define the following genie distortion\nmeasure:\n||x \u2212 x\u0302genie ||22\nd \u0304g (x, x\u0302genie ) =\n.\n||x||22\nThis distortion measure is invariant to the scaling of x for a\nfixed SNR. For any other recovery algorithm that outputs an\nestimate x\u0302, we let\n||x \u2212 x\u0302e ||22\nd \u0304e (x, x\u0302e ) =\n,\n||x||22\nwhere the subscript e denotes the estimation procedure. We\nwill be interested in the performance of an estimation procedure with respect to the genie decoder. To this end, we define\nDe/g (x, x\u0302e , x\u0302genie ) =\n\n||x \u2212 x\u0302e ||22\nd \u0304e (x, x\u0302e )\n=\n.\n||x \u2212 x\u0302genie ||22\nd \u0304g (x, x\u0302genie )\n\nWe will be interested in this quantity averaged over K\nMonte-Carlo simulations, and converted to dB. The closer\nthis quantity is to 0 dB means the closer the performance\nof the estimation procedure is to the performance of the genie\ndecoder.\n2 We also tested LDFs with 4 cycles and this does not seem to have an\nadverse effect on the average distortion in the presence of noise.\n\nIn other cases, such as the noiseless case, we will be\ninterested in the empirical probability of recovery. For K\nMonte-Carlo simulations, this is given by\nPrec =\n\nK\n1 X\nI(x \u223c x\u0302e ),\nK\nk=1\n\nwhere I(*) is the indicator function for (*) (1 if (*) is true, 0\notherwise). We will define the relation x \u223c x\u0302e to be true only\nif supp(x) = supp(x\u0302e ), unless otherwise specified.\nA number of different stopping criterion can be used for T :\n1) x\u0302 converges, 2) The minimum value of {||r \u2212 Fx\u0302(t) ||2 }t\ndoes not change for T d iterations , 3) A fixed number of\niterations T is reached. In our simulations we use criterion\ntwo with T d = 30 and T = 500. These values were chosen to\nmake sure that the algorithms did not stop too prematurely. The\nmessage passing schedule S is described in detail in Appendix\nI. Finally, for the reweighted algorithm we use 10 reweighings\nwith Tr1 = * * * = Tr10 = T /10.\nB. Simulation Results\nSimulation results are presented in Figure 5 for exactly\nsparse signals.\nFor comparison to our algorithms, we include results for\nCoSaMP [30] and `1 based methods [10], [11], [13], [16],\n[19]. For these algorithms we used partial Fourier matrices as\nmeasurement matrices. The choice of these matrices is based\non their small storage requirements (in comparison to Gaussian\nmatrices), while still satisfying restricted isometry principles.\nFor CoSaMP, we used 100 iterations of the algorithm (and 150\niterations of Richardson's iteration for calculating least squares\nsolutions). For `1 based methods, we used the L1MAGIC\npackage in the noiseless case. In the noisy case, we used both\nL1MAGIC, and the GPSR package (with Barzilai-Borwein\nGradient Projection with continuation and debiasing). Since\nthese two methods approximately perform the same, we include the results for GPSR here. In the implementation of\nGPSR we fine-tune the value of \u03c4 and observe that \u03c4 =\n0.001||FT r||\u221e gives the best performance.\nSince the outputs of `1 based methods and SuPrEM I are\nnot sparse, we threshold x to its L largest coefficients and\npostulate these are the locations of the sparse coefficients.\nFor all methods, we solve the least squares problem involving\nr and the matrix formed by the columns of F specified by\nthe final estimate for the locations of the sparse coefficients.\nFor partial Fourier matrices we use Richardson's iteration to\ncalculate this vector, whereas for LDFs we use the LSQR\nalgorithm which also has O(M ) complexity [31].\nC. Discussion of The Results\nThe simulation results indicate that the SuPrEM algorithms\noutperform the other state-of-the-art algorithms. In the low\nSNR regime (SNR = 12 dB), SuPrEM algorithms and the\n`1 methods have similar performance. In moderate and high\nSNR regimes, we see that SuPrEM algorithms significantly\noutperform the other algorithms both in terms of distortion\nand in terms of the maximum sparsity they can work at.\n\n\f8\n\nFig. 5.\n\nDRAFT\n\nPerformance comparison of recovery algorithms for sparse signals with Gaussian non-zero components.\n\nFurthermore for different values of N , the maximum sparsity\nscales as L = O(N/ log(M/N )), which is the same scaling\nas those of other methods. As we discussed previously the\nperformance of SuPrEM I degrades as sparsity and SNR\nincreases. We also observe that the reweighted SuPrEM II\nalgorithm outperforms the regular SuPrEM II algorithm, even\nthough the maximum number of iterations are the same.\n\nWe also note that for both partial Fourier matrices and\nLDFs, the quantity d \u0304g (x, x\u0302genie ) is almost the same for a fixed\nL and SNR. This means that De/g (x, x\u0302e , x\u0302genie ) provides\nan objective performance criterion in terms of relative meansquare error with respect to the genie bound, as well as in\nterms of absolute distortion error d \u0304e (x, x\u0302e ).\n\nFinally, compared to the other methods for the noiseless\nproblem, the SuPrEM algorithms can recover signals that\nhave a higher number of non-zero elements. In this case, the\nreweighted algorithm performs the best, and converges faster.\nWe also note that the results presented for CoSaMP and `1\nbased methods for the noiseless case are optimistic, since we\ndeclare success in recovery if d \u0304e (x, x\u0302e ) < 10\u22126 . We needed\nto introduce this measure, since these algorithms tend to miss\na small portion of the support of x containing elements of\nsmall magnitude.\n\nD. Simulation Results for Natural Images\nFor the testing of compressible signals, instead of using\nartificially generated signals, we used real-world compressible signals. In particular, we compressively sensed the db2\nwavelet coefficients of the 256\u00d7256 (raw) peppers image using\nN = 17000 measurements. Then we used various recovery\nalgorithms to recover the wavelet coefficients, and we did the\ninverse wavelet transform to recover the original image.\nFor SuPrEM algorithms, we used a rate (3, 12) LDF with\nM = 68000 (the wavelet coefficients vector was padded\n\n\fDRAFT\n\n9\n\nFig. 6. Performance comparison of recovery algorithms with a 256 \u00d7 256 natural image whose db2 wavelet coefficients are compressively sensed with N\n= 17000 measurements.\n\nwith zeros to match the dimension). We set L = 8000 (the\nmaximum sparsity the algorithm converged at) for SuPrEM II.\nWe ran the algorithm first with \u03c3 = 0. We also accomodated\nfor noise, and estimated the per measurement noise to be\n\u221a 2 and ran the algorithm again3 . We ran our\n\u03c3 = 0.1 ||r||\nN\nalgorithms for just 50 iterations. For the reweighted SuPrEM II\nalgorithm, we let \u03c3 = 0 and we reweighed after 5 steps of the\nalgorithm for a total of 10 reweighings. For SMP, we used the\nSMP package [7]. We used a matrix generated by this package,\nand L = 8000. For the remaining methods, we used partial\nFourier matrices whose rows were chosen randomly. For `1\n3 With this value of \u03c3, SuPrEM I also provides a similar performance.\nHowever since the output in this case is very similar to that of SuPrEM II,\nwe do not include it in the figure.\n\nwith equality constraints, we used the L1MAGIC package. For\nLASSO, we used the GPSR package and \u03c4 = 0.001||FT r||\u221e ,\nas described previously, and we thresholded the output to\nL = 8000 sparse coefficients and solved the appropriate least\nsquares problem to get the final estimate. For CoSaMP and\nSubspace Pursuit, we used 100 iterations of the algorithm (and\n150 iterations for the Richardson's iteration for calculating\nthe least square solutions). For these algorithms, we used\nL = 3000 for CoSaMP, and L = 3500 for Subspace Pursuit.\nThese are slightly lower than the maximum sparsities they\nconverged at (L = 3500 and L = 4000 respectively), but the\nvalues we used resulted in better visual quality and PSNR\nvalues. The results are depicted in Figure 6.\nThe PSNR values for the methods are as follows: 23.41\n\n\f10\n\nDRAFT\n\ndB for SuPrEM II, 23.83 dB for SuPrEM II (with non-zero\n\u03c3 2 ), 24.79 for SuPrEM II (reweighted), 20.18 dB for CoSaMP,\n19.51 dB for SMP, 21.62 dB for `1 , 23.61 dB for LASSO,\n21.27 dB for Subspace Pursuit. Among the algorithms that\nassume no knowledge of noise, we see that SuPrEM II outperforms the other algorithms both in terms of PSNR value and\nin terms of visual quality. The two algorithms that accomodate\nnoise, SuPrEM II (in this case SuPrEM I also produces a\nsimilar output) and LASSO have similar PSNR values. Finally,\nthe reweighted SuPrEM II also assumes no knowledge of\nnoise, and outperforms all other methods by about 1 dB and\nalso in terms of visual quality, without requiring more running\ntime.\n\nopen problem is to analyze the performance of the iterative\ndecoding algorithms for the LDFs theoretically, which may\nin turn lead to useful design tools (like Density Evolution\n[33]) that might help with the construction of LDFs with\nirregular degree distributions. Adaptive measurements using\nthe soft information available about the estimates, as well as\nonline decoding (similar to Raptor Codes [37]) is another open\nresearch area. Finally, if further information is available about\nthe statistical properties of a class of signals (such as blocksparse signals or images represented on wavelet trees as in\n[5]), the decoding algorithms may be changed accordingly to\nimprove performance.\nA PPENDIX I\nD ETAILS O N T HE M ESSAGE -PASSING S CHEDULE\n\nE. Further Results\nWe studied the effect of the change of degree distributions.\nFor a given M/N ratio, we need to keep the ratio of dc /dv\nfixed however the values can be varied. Thus we compared\nthe performance of dv = 3 LDFs to dv = 5 LDFs, and\nobserved that the latter actually performed sligthly better.\nHowever, having a higher dv means more operations are\nrequired. We also observed that the number of iterations\nrequired for convergence was slightly higher. Thus we chose\nto use dv = 3 LDFs that allowed faster decoding. We also\nnote that increasing dv too much (while keeping M/N fixed)\nresults in performance deterioration, since the graph becomes\nless sparse, and we run into shorter cycles which affect the\nperformance of SPA.\nWe also tested the performance of our constructions and\nalgorithms at M = 100000. With L/M and N/M fixed,\ninterestingly the performance improves as M \u2192 \u221e for\nGaussian sparse signals for a fixed maximum number of\n500 iterations. This is in line with intuitions drawn from\nShannon Theory [3]. Another interesting observation is that\nthe number of iterations remain unchanged in this setting. In\ngeneral, we observed that the number of iterations required for\nconvergence is only a function of L/M and does not change\nwith M .\nV. C ONCLUSION\nIn this paper, we constructed an ensemble of measurement\nmatrices with small storage requirements. We denoted the\nmembers of this ensemble as Low Density Frames (LDF). For\nthese frames, we provided sparse reconstruction algorithms\nthat have O(M ) complexity and that are Bayesian in nature.\nWe evaluated the performance of this ensemble of matrices and\ntheir decoding algorithms, and compared their performance to\nother state-of-the-art recovery algorithms and their associated\nmeasurement matrices. We observed that in various cases\nof interest, SuPrEM algorithms with LDFs outperformed the\nother algorithms with partial Fourier matrices. In particular,\nfor Gaussian sparse signals and Gaussian noise, we are within\n2 dB range of the theoretical lower bound in most cases.\nThere are various interesting research problems in this area.\nOne is to find a deterministic message-passing schedule that\nperforms as well as (or better than) our probabilistic messagepassing schedule and that is amenable to analysis. Another\n\nA message-passing schedule determines the order of messages passed between variable and check nodes of a factor\ngraph. Traditionally, with LDPC codes, the so-called \"flooding\" schedule is used. In this schedule, at each iteration,\nall the variable nodes pass messages to their neighboring\ncheck nodes. Subsequently, all the check nodes pass messages\nto their neighboring variable nodes. For a cycle-free graph,\nSPA with a flooding schedule correctly computes a-posteriori\nprobabilities [9], [49]. An alternative schedule is the \"serial\"\nschedule, where we go through each variable node serially and\ncompute the messages to the neighboring nodes. The order in\nwhich we go through variable nodes could be lexicographic,\nrandom or based on reliabilities.\nIn this section, we propose the following schedule based\non the intuition derived from our simulations and results from\nLDPC codes [27], [49]: For the first iteration, all the check\nnodes send messages to variable nodes and vice-versa in\na flooding schedule. After this iteration, with probability 12\neach check node is \"on\" or \"off\". If a check node is off,\nit marks the edges connected to itself as an \"inactive\", and\nsends back the messages it received to the variable nodes. If\na check node is on, it marks the edges connected to itself as\n\"active\" and computes a new message. At the variable nodes,\nwhen calculating the new beta, we only use the information\ncoming from active edges. That is for k = 1, 2, . . . , M , let\n{k1 , k2 , . . . , kdv } be the indices of the check nodes connected\nto the k th variable node xk . Let the incoming message from\nthe check node rkj to the variable node xk at the tth iteration\n(t)\n(t)\nbe (\u03bckj , \u03bdkj ) for j = 1, . . . , dv . We will have\n\u0012\n\u0013\u22121\nX\n1\n1\n(t)\n\u03bbk =\n+\n,\n(t)\n(t\u22121)\n\u03b2k\n(k,kj ) is an active edge \u03bdkj\n(t) !\n\n(t)\n\u03bck\n\n=\n\nX\n\n\u03bckj\n\n(k,kj ) is an active edge\n\n\u03bdkj\n\n(t)\n\u03bbk\n\n(t)\n\nand\n\n(t)\n\n(t)\n\n,\n\n(\u03bck )2 + \u03bbk\n.\n3\nThus when there is no active edge, we do not perform a\n\u03b2 update. For the special case when there is only one active\n(t)\nedge (k, kj ), we let \u03bck = \u03bckj . This is because the intrinsic\n(t)\n\n\u03b2k =\n\n\fDRAFT\n\n11\n\n(t\u22121)\n\ninformation is more valuable, and the estimate on \u03b2k\ntends\nto be not as reliable. When we calculate the point estimate,\nwe use all the information at the node, including the reliable\nand unreliable edges, i.e.\n\u0013\u22121\n\u0012X\ndv\n1\n1\n(t)\n+ (t)\n,\nV\u0302k =\n(t)\n\u03b2k\nj=1 \u03bdkj\n(t)\nx\u0302k\n\n=\n\n(t)\nV\u0302k\n\n(t) \u0013\n\n\u0012X\ndv\n\n\u03bckj\n\nj=1\n\n\u03bdkj\n\n(t)\n\n.\n\nIt is noteworthy that the flooding schedule and serial schedules tend to converge to local minima and they do not perform\nas well as this schedule we proposed.\nR EFERENCES\n[1] M. Ak\u00e7akaya and V. Tarokh, \"A Frame Construction and A Universal\nDistortion Bound for Sparse Representations,\" IEEE Trans. Sig. Proc.,\nvol. 56, pp. 2443-2550, June 2008.\n[2] M. Ak\u00e7akaya and V. Tarokh, \"On Sparsity, Redundancy and Quality of\nFrame Representations,\" IEEE Int. Symposium on Information Theory\n(ISIT), Nice, France, June 2007.\n[3] M. Ak\u00e7akaya and V. Tarokh, \"Shannon theoretic limits on noisy compressive sampling,\" arXiv:0711.0366v1 [cs.IT], Nov. 2007.\n[4] D. Andrews and C. Mallows, \"Scale mixtures of normal distributions,\"\nJ. R. Stat. Soc., vol. 36, pp. 99 - 102, 1974.\n[5] R. Baraniuk, V. Cevher, M. Duarte, and C. Hegde, \"Model-based\ncompressive sensing,\" arXiv:0808.3572v2, Sept. 2008.\n[6] R. Berinde, A. C. Gilbert, P. Indyk, H. Karloff, and M. J. Strauss,\n\"Combining geometry and combinatorics: A unified approach to sparse\nsignal recovery,\" preprint, 2008.\n[7] R. Berinde, P. Indyk, and M. Ruz\u0303i\u0107, \"Practical near-optimal sparse recovery in the ell-1 norm,\" Proc. Allerton Conference on Communication,\nControl, and Computing, Monticello, IL, September 2008.\n[8] J. M. Bioucas-Dias, \"Bayesian Wavelet-Based Image Deconvolution:\nA GEM Algorithm Exploiting a Class of Heavy-Tailed Priors,\" IEEE\nTrans. Image Proc., vol. 15, pp. 937-951, Apr. 2006.\n[9] C. M. Bishop, Pattern Recognition and Machine Learning, First Edition,\nSpringer, New York, NY, 2006.\n[10] E. J. Cand\u00e8s, J. Romberg, \"Practical signal recovery from random\nprojections,\" presented at the Wavelet Appl. Signal Image Process. XI,\nSPIE Conf., San Diego, CA, 2005.\n[11] E. J. Cand\u00e8s, J. Romberg, T. Tao, \"Stable signal recovery for incomplete\nand inaccurate measurements,\" Commun. Pure Appl. Math., vol. 59, pp.\n1207-1223, Aug. 2006.\n[12] E. J. Cand\u00e8s and T. Tao, \"The Dantzig selector: statistical estimation\nwhen p is much larger than n,\" Annals of Statistics, 35, pp. 2313-2351,\nDec. 2007.\n[13] E. J. Cand\u00e8s, T. Tao, \"Decoding by Linear Programming,\" IEEE Trans.\nInf. Theory, vol. 51, pp. 4203-4215, Dec. 2005.\n[14] E. J. Cand\u00e8s, M. Wakin and S. Boyd, \"Enhancing sparsity by reweighted\nl1 minimization,\" J. Fourier Anal. Appl., vol. 14, pp. 877-905.\n[15] T. Cormen, C. Lesierson, L. Rivest, and C. Stein, Introduction to\nAlgorithms, Second Edition, MIT Press, Cambridge, MA, 2001.\n[16] D. L. Donoho, \"Compressed Sensing,\" IEEE Trans. Inf. Theory, vol. 52,\npp. 1289-1306, April 2006.\n[17] W. Dai and O. Milenkovic, \"Subspace pursuit for compressive\nsensing: Closing the gap between performance and complexity,\"\narXiv:0803.0811v2 [cs.NA], March 2008.\n[18] M. A. T. Figueiredo and R. Nowak, \"Wavelet-based image estimation:\nAn empirical bayes approach using Jeffreys noninformative prior,\" IEEE\nTrans. Image Proc., vol. 10, pp. 1322-1331, Sep. 2001.\n[19] M. A. T. Figueiredo, R. D. Nowak and S. J. Wright, \"Gradient projection\nfor sparse reconstruction: Application to compressed sensing and other\ninverse problems,\" IEEE Journal of Selected Topics in Signal Processing,\nvol. 1, pp. 586-598, Dec. 2007.\n[20] A. K. Fletcher, S. Rangan and V. K. Goyal, \"Necessary and Sufficient\nConditions on Sparsity Pattern Recovery,\" arXiv:0804.1839v1 [cs.IT],\nApr. 2008.\n[21] R. G. Gallager, Low-Density Parity-Check Codes, MIT Press, Cambridge, MA, 1963.\n\n[22] X.-Y. Hu, E. Eleftheriou and D. M. Arnold, \"Regular and irregular\nprogressive edge-growth tanner graphs,\" IEEE Trans. Inf. Theory, vol.\n51, pp. 386-398, Jan. 2005.\n[23] S. Ji, Y. Xue and L. Carin, \"Bayesian compressive sensing,\" IEEE Trans.\non Sig. Proc., vol. 56, pp. 2346-2356, June 2008.\n[24] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger, \"Factor Graphs and\nthe Sum-Product Algorithm,\" IEEE Trans. Inf. Theory, vol. 47, pp. 498519, Feb. 2001.\n[25] D. J. C. MacKay, \"Good error correcting codes based on very sparse\nmatrices,\" IEEE Trans. Inf. Theory, vol. 45, pp. 399-431, Mar. 1999.\n[26] D. J. C. MacKay, Information Theory, Inference, and Learning Algorithms, First Edition, Cambridge University Press, Cambridge, UK,\n2002.\n[27] Y. Mao and A. H. Banihashemi, \"Decoding Low-Density Parity-Check\nCodes With Probabilistic Scheduling,\" IEEE Comm. Letters, vol. 5, pp.\n414-416, Oct. 2001.\n[28] G. J. McLachlan and T. Krishnan, The EM Algorithm and Extensions,\nFirst Edition, John Wiley & Sons, New York, NY, 1997.\n[29] T. K. Moon, \"The EM algorithm in signal processing,\" IEEE Sig. Proc.\nMag., vol. 13, pp. 47-60, Nov. 1996.\n[30] D. Needell and J. A. Tropp, \"CoSaMP: Iterative signal recovery from\nincomplete and inaccurate samples,\" arXiv:0803.2392v2 [math.NA],\nApr. 2008.\n[31] C. C. Paige and M. A. Saunders, \"LSQR: Sparse Linear Equations and\nLeast Squares Problems,\" ACM Transactions on Mathematical Software\n(TOMS), vol. 8, pp.195-209, June 1982.\n[32] J. Portilla, V. Strela, M. J. Wainwright and E. P. Simoncelli, \"Image\nDenoising Using Scale Mixtures of Gaussians in the Wavelet Domain,\"\nIEEE Trans. Image Proc., vol. 12, pp. 1338-1351, Nov. 2003.\n[33] T. J. Richardson and R.L. Urbanke, \"The capacity of low-density paritycheck codes under message passing decoding,\" IEEE Trans. Inf. Theory,\nvol. 47, no. 2, pp. 599-618, Feb. 2001.\n[34] C. Robert, The Bayesian Choice: A Decision Theoretic Motivation, First\nEdition, New York, NY, Springer-Verlag, 1994.\n[35] S. Sarvotham, D. Baron, and R. Baraniuk, \"Sudocodes - Fast Measurement and Reconstruction of Sparse Signals,\" Proc. IEEE Int. Symp. on\nInf. Theory (ISIT), Seattle, WA, July 2006.\n[36] S. Sarvotham, D. Baron, and R. Baraniuk, \"Compressed Sensing Reconstruction via Belief Propagation,\" preprint, 2006.\n[37] A. Shokrollahi, \"Raptor codes,\" IEEE Trans. Inf. Theory, vol. 52, pp.\n2551-2567, June 2006.\n[38] M. Sipser and D. A. Spielman,\"Expander codes,\" IEEE Trans. Inf.\nTheory, vol. 42, pp. 1710-1722, Nov. 1996.\n[39] R. M. Tanner, \" A Recursive Approach to Low Complexity Codes,\"\nIEEE Trans. Inf. Theory, vol. 27, pp. 533-547, Sept. 1981.\n[40] M. E. Tipping, \"Sparse Bayesian learning and the relevance vector\nmachine,\" Journal of Machine Learning Research, vol. 1, pp. 211-244,\n2001.\n[41] M. E. Tipping, \"Bayesian inference: An introduction to principles and\npractice in machine learning,\" in O. Bousquet, U. von Luxburg, and\nG. Rtsch (Eds.), Advanced Lectures on Machine Learning, pp. 41-62,\nSpringer, 2004.\n[42] J. A. Tropp, \"Topics in Sparse Approximation\", Ph.D. dissertation,\nComputational and Applied Mathematics, UT-Austin, August 2004.\n[43] J. A. Tropp, \"Just relax: Convex programming methods for identifying\nsparse signals\", IEEE Trans. Inf. Theory, vol. 51, no. 3, pp. 1030-1051,\nMar. 2006.\n[44] J. A. Tropp, A. C. Gilbert, \"Signal recovery from partial information\nvia Orthogonal Matching Pursuit\", IEEE Trans. Inf. Theory, vol. 53,\npp.4655-4666, Dec. 2007.\n[45] M. J. Wainwright, \"Information-Theoretic Limits on Sparsity Recovery\nin the High-Dimensional and Noisy Setting,\" Technical Report, UC\nBerkeley, Department of Statistics, Jan. 2007.\n[46] M. J. Wainwright, \"Sharp thresholds for noisy and high-dimensional\nrecovery of sparsity using `1 -constrained quadratic programming,\" Technical report, UC Berkeley, Department of Statistics, May 2006.\n[47] Y. Weiss and W. T. Freeman, \"Correctness of belief propagation in\nGaussian graphical models of arbitrary topology,\" Proc. Adv. Neural\nInform. Processing Syst., vol. 12, Dec. 1999.\n[48] N. Wiberg, \"Codes and decoding on general graphs,\" Ph.D. dissertation,\nLink\u00f6ping University, Sweden, 1996.\n[49] H. Xiao and A. H. Banihashemi, \"Graph-Based Message-Passing Schedules for Decoding LDPC Codes,\" IEEE Trans. on Comm., vol. 52, pp.\n2098-2105, Dec. 2004.\n[50] W. Xu and B. Hassibi, \"Efficient compressive sensing with deterministic\nguarantees using expander graphs,\" Proc. IEEE Inf. Theory Workshop,\nLake Tahoe, CA, Sept. 2007.\n\n\f"}