{"id": "http://arxiv.org/abs/1012.3476v1", "guidislink": true, "updated": "2010-12-15T21:23:09Z", "updated_parsed": [2010, 12, 15, 21, 23, 9, 2, 349, 0], "published": "2010-12-15T21:23:09Z", "published_parsed": [2010, 12, 15, 21, 23, 9, 2, 349, 0], "title": "Adaptive Parallel Tempering for Stochastic Maximum Likelihood Learning\n  of RBMs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.2366%2C1012.5613%2C1012.3461%2C1012.4497%2C1012.2894%2C1012.1681%2C1012.4862%2C1012.4390%2C1012.1098%2C1012.1661%2C1012.5578%2C1012.2381%2C1012.2405%2C1012.2946%2C1012.1759%2C1012.4876%2C1012.4905%2C1012.5455%2C1012.1040%2C1012.0242%2C1012.4051%2C1012.2326%2C1012.0176%2C1012.5073%2C1012.3158%2C1012.3207%2C1012.4073%2C1012.0796%2C1012.1347%2C1012.3824%2C1012.1472%2C1012.0642%2C1012.2687%2C1012.3409%2C1012.2852%2C1012.6041%2C1012.0866%2C1012.3106%2C1012.0710%2C1012.4772%2C1012.0496%2C1012.5878%2C1012.3338%2C1012.2425%2C1012.4334%2C1012.3997%2C1012.3788%2C1012.5678%2C1012.5214%2C1012.0899%2C1012.2281%2C1012.4040%2C1012.4268%2C1012.1283%2C1012.4684%2C1012.1199%2C1012.4687%2C1012.1293%2C1012.5683%2C1012.1987%2C1012.2521%2C1012.3365%2C1012.3805%2C1012.1989%2C1012.5431%2C1012.4825%2C1012.5466%2C1012.2668%2C1012.2392%2C1012.3381%2C1012.0298%2C1012.3558%2C1012.5502%2C1012.0627%2C1012.3739%2C1012.1879%2C1012.0683%2C1012.2209%2C1012.3175%2C1012.1988%2C1012.3497%2C1012.5606%2C1012.2601%2C1012.1838%2C1012.4438%2C1012.1605%2C1012.2841%2C1012.0323%2C1012.3442%2C1012.2700%2C1012.1635%2C1012.2016%2C1012.0296%2C1012.0274%2C1012.2127%2C1012.2187%2C1012.1425%2C1012.3476%2C1012.5191%2C1012.1612%2C1012.5506&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adaptive Parallel Tempering for Stochastic Maximum Likelihood Learning\n  of RBMs"}, "summary": "Restricted Boltzmann Machines (RBM) have attracted a lot of attention of\nlate, as one the principle building blocks of deep networks. Training RBMs\nremains problematic however, because of the intractibility of their partition\nfunction. The maximum likelihood gradient requires a very robust sampler which\ncan accurately sample from the model despite the loss of ergodicity often\nincurred during learning. While using Parallel Tempering in the negative phase\nof Stochastic Maximum Likelihood (SML-PT) helps address the issue, it imposes a\ntrade-off between computational complexity and high ergodicity, and requires\ncareful hand-tuning of the temperatures. In this paper, we show that this\ntrade-off is unnecessary. The choice of optimal temperatures can be automated\nby minimizing average return time (a concept first proposed by [Katzgraber et\nal., 2006]) while chains can be spawned dynamically, as needed, thus minimizing\nthe computational overhead. We show on a synthetic dataset, that this results\nin better likelihood scores.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.2366%2C1012.5613%2C1012.3461%2C1012.4497%2C1012.2894%2C1012.1681%2C1012.4862%2C1012.4390%2C1012.1098%2C1012.1661%2C1012.5578%2C1012.2381%2C1012.2405%2C1012.2946%2C1012.1759%2C1012.4876%2C1012.4905%2C1012.5455%2C1012.1040%2C1012.0242%2C1012.4051%2C1012.2326%2C1012.0176%2C1012.5073%2C1012.3158%2C1012.3207%2C1012.4073%2C1012.0796%2C1012.1347%2C1012.3824%2C1012.1472%2C1012.0642%2C1012.2687%2C1012.3409%2C1012.2852%2C1012.6041%2C1012.0866%2C1012.3106%2C1012.0710%2C1012.4772%2C1012.0496%2C1012.5878%2C1012.3338%2C1012.2425%2C1012.4334%2C1012.3997%2C1012.3788%2C1012.5678%2C1012.5214%2C1012.0899%2C1012.2281%2C1012.4040%2C1012.4268%2C1012.1283%2C1012.4684%2C1012.1199%2C1012.4687%2C1012.1293%2C1012.5683%2C1012.1987%2C1012.2521%2C1012.3365%2C1012.3805%2C1012.1989%2C1012.5431%2C1012.4825%2C1012.5466%2C1012.2668%2C1012.2392%2C1012.3381%2C1012.0298%2C1012.3558%2C1012.5502%2C1012.0627%2C1012.3739%2C1012.1879%2C1012.0683%2C1012.2209%2C1012.3175%2C1012.1988%2C1012.3497%2C1012.5606%2C1012.2601%2C1012.1838%2C1012.4438%2C1012.1605%2C1012.2841%2C1012.0323%2C1012.3442%2C1012.2700%2C1012.1635%2C1012.2016%2C1012.0296%2C1012.0274%2C1012.2127%2C1012.2187%2C1012.1425%2C1012.3476%2C1012.5191%2C1012.1612%2C1012.5506&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Restricted Boltzmann Machines (RBM) have attracted a lot of attention of\nlate, as one the principle building blocks of deep networks. Training RBMs\nremains problematic however, because of the intractibility of their partition\nfunction. The maximum likelihood gradient requires a very robust sampler which\ncan accurately sample from the model despite the loss of ergodicity often\nincurred during learning. While using Parallel Tempering in the negative phase\nof Stochastic Maximum Likelihood (SML-PT) helps address the issue, it imposes a\ntrade-off between computational complexity and high ergodicity, and requires\ncareful hand-tuning of the temperatures. In this paper, we show that this\ntrade-off is unnecessary. The choice of optimal temperatures can be automated\nby minimizing average return time (a concept first proposed by [Katzgraber et\nal., 2006]) while chains can be spawned dynamically, as needed, thus minimizing\nthe computational overhead. We show on a synthetic dataset, that this results\nin better likelihood scores."}, "authors": ["Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio"], "author_detail": {"name": "Yoshua Bengio"}, "author": "Yoshua Bengio", "arxiv_comment": "Presented at the \"NIPS 2010 Workshop on Deep Learning and\n  Unsupervised Feature Learning\"", "links": [{"href": "http://arxiv.org/abs/1012.3476v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1012.3476v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1012.3476v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1012.3476v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:1012.3476v1 [stat.ML] 15 Dec 2010\n\nAdaptive Parallel Tempering for Stochastic\nMaximum Likelihood Learning of RBMs\n\nGuillaume Desjardins, Aaron Courville and Yoshua Bengio\nDept. IRO\nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9al, QC\n{desjagui,courvila,bengioy}@iro.umontreal.ca\n\nAbstract\nRestricted Boltzmann Machines (RBM) have attracted a lot of attention of late,\nas one the principle building blocks of deep networks. Training RBMs remains\nproblematic however, because of the intractibility of their partition function. The\nmaximum likelihood gradient requires a very robust sampler which can accurately\nsample from the model despite the loss of ergodicity often incurred during learning. While using Parallel Tempering in the negative phase of Stochastic Maximum\nLikelihood (SML-PT) helps address the issue, it imposes a trade-off between computational complexity and high ergodicity, and requires careful hand-tuning of the\ntemperatures. In this paper, we show that this trade-off is unnecessary. The choice\nof optimal temperatures can be automated by minimizing average return time (a\nconcept first proposed by (Katzgraber et al., 2006)) while chains can be spawned\ndynamically, as needed, thus minimizing the computational overhead. We show\non a synthetic dataset, that this results in better likelihood scores.\n\n1\n\nIntroduction\n\nRestricted Boltzmann Machines (RBM) (Freund & Haussler, 1994; Welling et al., 2005; Hinton\net al., 2006) have become a model of choice for learning unsupervised features for use in deep feedforward architectures (Hinton et al., 2006; Bengio, 2009) as well as for modeling complex, highdimensional distributions (Welling et al., 2005; Taylor & Hinton, 2009; Larochelle et al., 2010).\nTheir success can be explained in part through the bi-partite structure of their graphical model.\nUnits are grouped into a visible layer v and a hidden layer h, prohibiting connections within the\nsame layer. The use of latent variables affords RBMs a rich modeling capacity, while the conditional\nindependence property yields a trivial inference procedure.\nRBMs are parametrized by an energy function E(v, h) which is converted to probability through\nthe Boltzmann distribution, after marginalizing\nP out the hidden units. The probability of a given\n1\nconfiguration p(v)\nP is thus given by p(v) = Z h exp(\u2212E(v, h)), where Z is the partition function\ndefined as Z = v,h exp(\u2212E(v, h)).\nDespite their popularity, direct learning of these models through maximum likelihood remains problematic. The maximum likelihood gradient with respect to the parameters \u03b8 of the model is:\n\u2202 log p(v)\n\u2202\u03b8\n\n= \u2212\n\nX\nh\n\np(h|v)\n\nX\n\u2202E(v, h)\n\u2202E(v\u2212 , h\u2212 )\n+\np(v\u2212 , h\u2212 )\n\u2202\u03b8\n\u2202\u03b8\n\u2212\n\u2212\n\n(1)\n\nv ,h\n\nThe first term is trivial to calculate and is referred to as the positive phase, as it raises the probability\nof training data. The second term or negative phase is intractable in most applications of interest,\n1\n\n\fas it involves an expectation over p(v, h). Many learning algorithms have been proposed in the\nliterature to address this issue:\n\u2022 Contrastive Divergence (CD) (Hinton, 1999; Hinton, 2002) replaces the expectation with a\nfinite set of negative samples, which are obtained by running a short Markov chain initialized at positive training examples. This yields a biased, but low-variance gradient which\nhas been shown to work well as a feature extractor for deep networks such as the Deep\nBelief Network (Hinton et al., 2006).\n\u2022 Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD)\n(Younes, 1998; Tieleman, 2008) on the other hand, relies on a persistent Markov chain\nto sample the negative particles. The chain is run for a small number of steps between\nconsecutive model updates, with the assumption that the Markov chain will stay close to\nits equilibrium distribution as the parameters evolve. Learning actually encourages this\nprocess, in what is called the \"fast-weight effect\" (Tieleman & Hinton, 2009).\n\u2022 Ratio Matching and Score Matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007) avoid the issue\nof the partition function altogether by replacing maximum likelihood by another learning\nprinciple, based on matching the change in likelihood to that implied by the empirical\ndistribution.\n(Marlin et al., 2010) recently compared these algorithms on a variety of tasks and found SML to\nbe the most attractive method when taking computational complexity into account. Unfortunately,\nthese results fail to address the main shortcomings of SML. First, it relies on Gibbs sampling to\nextract negative samples: a poor choice when sampling from multi-modal distributions. Second,\nto guarantee convergence, the learning rate must be annealed throughout learning in order to offset\nthe loss of ergodicity 1 incurred by the Markov chain due to parameter updates (Younes, 1998;\nDesjardins et al., 2010). Using tempering in the negative phase of SML (Desjardins et al., 2010;\nCho et al., 2010; Salakhutdinov, 2010b; Salakhutdinov, 2010a) appears to address these issues to\nsome extent. By performing a random walk in the joint (configuration, temperature) space, negative\nparticles can escape regions of high probability and travel between disconnected modes. Also, since\nhigh temperature chains are inherently more ergodic, the sampler as a whole exhibits better mixing\nand results in better convergence properties than traditional SML.\nTempering is still no panacea however. Great care must be taken to select the set of temperatures\nT = {T1 , ..., TM ; T1 < Ti < TM \u2200i \u2208 [1, M ], M \u2208 N} over which to run the simulation. Having\ntoo few or incorrectly spaced chains can result in high rejection ratios (tempered transition), low\nreturn rates (simulated tempering) or low swap rates between neighboring chains (parallel tempering), which all undermine the usefulness of the method. In this work, we show that the choice of\nT can be automated for parallel tempering, both in terms of optimal temperature spacing, as well\nas the number of chains to simulate. Our algorithm relies heavily on the work of (Katzgraber et al.,\n2006), who were the first to show that optimal temperature spacing can be obtained by minimizing\nthe average return time of particles under simulation.\nThe paper is organized as follows. We start with a brief review of SML, then explore the details\nbehind SML with Parallel Tempering (SML-PT) as described in (Desjardins et al., 2010). Following\nthis, we show how the algorithm of Katzgraber et al. can be adapted to the online gradient setting for\nuse with SML-PT and show how chains can be created dynamically, so as to maintain a given level\nof ergodicity throughout training. We then proceed to show various results on a complex synthetic\ndataset.\n\n2\n2.1\n\nSML with Optimized Parallel Tempering\nParallel Tempered SML (SML-PT)\n\nWe start with a very brief review of SML, which will serve mostly to anchor our notation. For\ndetails on the actual algorithm, we refer the interested reader to (Tieleman & Hinton, 2009; Marlin\net al., 2010). RBMs are parametrized by \u03b8 = {W, b, c}, where bi is the i-th hidden bias, cj the\n1\nWe use the term \"ergodicity\" rather loosely, to reflect the amount of time required for the states sampled\nby the Markov chain, to reflect the true expectation we wish to measure.\n\n2\n\n\fj-th visible bias and Wij is the weight connecting units hi to P\nvj . They belong to the family of loglinear models whose energy function is given by E(x) = \u2212 k \u03b8k \u03c6k (x), where \u03c6k are functions\nassociated with each parameter \u03b8k . In the case of RBMs, x = (v, h) and \u03c6(v, h) = (hvT , h, v).\nFor this family of model, the gradient of Equation 1 simplifies to:\n\u2202 log p(v)\n\u2202\u03b8\n\n= Ep(h|v) [\u03c6(v, h)] \u2212 Ep(v,h) [\u03c6(v, h)].\n\n(2)\n\nAs was mentioned previously, SML approximates the gradient by drawing negative phase samples\n(i.e. to estimate the second expectation) from a persistent Markov chain, which attempts to track\nchanges in the model. If we denote the state of this chain at timestep t as vt\u2212 and the i-th training\n\u2212\nexample as v(i) , then the stochastic gradient update follows \u03c6(v(i) , h\u0303) \u2212 \u03c6(\u1e7dt+k\n, h\u0303\u2212\nt+k ), where\n\u2212\n(i)\nh\u0303 = E[h|v = v ], and \u1e7dt+k is obtained after k steps of alternating Gibbs starting from state vt\u2212\n\u2212\nand h\u0303\u2212\nt+k = E[h|v = vt+k ].\nTraining an RBM using SML-PT maintains the positive phase as is. During the negative phase\nhowever, we create and sample from an an extended set of M persistent chains, {p\u03b2i (v, h)|i \u2208\ni E(x))\nrepresents a smoothed\n[1, M ], \u03b2i \u2265 \u03b2j \u21d0\u21d2 i < j}. Here each p\u03b2i (v, h) = exp(\u2212\u03b2\nZ(\u03b2i )\nversion of the distribution we wish to sample from, with the inverse temperature \u03b2i = 1/Ti \u2208 [0, 1]\ncontrolling the degree of smoothing. Distributions with small \u03b2 values are easier to sample from as\nthey exhibit greater ergodicity.\nAfter performing k Gibbs steps for each of the M intermediate distributions, cross-temperature\nstate swaps are proposed between neighboring chains using a Metropolis-Hastings-based swap acceptance criterion. If we denote by xi the joint state (visible and hidden) of the i-th chain, the swap\nacceptance ratio ri for swapping chains (i,i + 1) is given by:\nri = max(1,\n\np\u03b2i (xi+1 )p\u03b2i+1 (xi )\n)\np\u03b2i (xi )p\u03b2i+1 (xi+1 )\n\n(3)\n\nAlthough one might reduce variance by using free-energies to compute swap ratios, we prefer using\nenergies as the above factorizes nicely into the following expression:\nri = exp((\u03b2i \u2212 \u03b2i+1 ) * (E(xi ) \u2212 E(xi+1 ))),\n\n(4)\n\nWhile many swapping schedules are possible, we use the Deterministic Even Odd algorithm (DEO)\n(Lingenheil et al., 2009), described below.\n2.2\n\nReturn Time and Optimal Temperatures\n\nConventional wisdom for choosing the optimal set T has relied on the \"flat histogram\" method\nwhich selects the parameters \u03b2i such that the pair-wise swap ratio ri is constant and independent of\nthe index i. Under certain conditions (such as when sampling from multi-variate Gaussian distributions), this can lead to a geometric spacing of the temperature parameters (Neal, 1994). (Behrens\net al., 2010) has recently shown that geometric spacing is actually optimal for a wider family of\ndistributions characterized by E\u03b2 (E(x)) = K1 /\u03b2 + K2 , where E\u03b2 denotes the expectation over\ninverse temperature and K1 , K2 are arbitrary constants.\nSince this is clearly not the case for RBMs, we turn to the work of (Katzgraber et al., 2006) who\npropose a novel measure for optimizing T . Their algorithm directly maximizes the ergodicity of\nthe sampler by minimizing the time taken for a particle to perform a round-trip between \u03b21 and \u03b2M .\nThis is defined as the average \"return time\" \u03c4rt . The benefit of their method is striking: temperatures\nautomatically pool around phase transitions, causing spikes in local exchange rates and maximizing\nthe \"flow\" of particles in temperature space.\nThe algorithm works as follows. For Ns sampling updates:\n\u2022 assign a label to each particle: those swapped into \u03b21 are labeled as \"up\" particles. Similarly, any \"up\" particle swapped into \u03b2M becomes a \"down\" particle.\n3\n\n\f\u2022 after each swap proposal, update the histograms nu (i), nd (i), counting the number of \"up\"\nand \"down\" particles for the Markov chain associated with \u03b2i .\nnu (i)\n\u2022 define fup (i) = nu (i)+n\n, the fraction of \"up\"-moving particles at \u03b2i . By construction,\nd (i)\nnotice that fup (\u03b21 ) = 1 and fup (\u03b2M ) = 0. fup thus defines a probability distribution of\n\"up\" particles in the range [\u03b21 , \u03b2M ].\n\u2022 The new inverse temperature parameters \u03b2 0 are chosen as the ordered set which assigns\nequal probability mass to each chain. This yields an fup curve which is linear in the chain\nindex.\n\nThe above procedure is applied iteratively, each time increasing Ns so as to fine-tune the \u03b2i 's.\nTo monitor return time, we can simply maintain a counter \u03c4i for each particle xi , which is (1)\nincremented at every sampling iteration and (2) reset to 0 whenever xi has label \"down\" and is\nPM\nswapped into \u03b21 . A lower-bound for return time is then given by \u03c4\u0302rt = i=0 \u03c4i .\n2.3\n2.3.1\n\nOptimizing T while Learning\nOnline Beta Adaptation\n\nWhile the above algorithm exhibits the right properties, it is not very well suited to the context of\nlearning. When training an RBM, the distribution we are sampling from is continuously changing.\nAs such, one would expect the optimal set T to evolve over time. We also do not have the luxury of\nperforming Ns sampling steps after each gradient update.\nOur solution is simple: the histograms nu and nd are updated using an exponential moving average,\nwhose time constant is in the order of the return time \u03c4\u0302rt . Using \u03c4\u0302rt as the time constant is crucial\nas it allows us to maintain flow statistics at the proper timescale. If an \"up\" particle reaches the i-th\nchain, we update nu (i) as follows:\nt\nt\nt\nnt+1\nu (i) = nu (i)(1 \u2212 1/\u03c4\u0302rt ) + 1/\u03c4\u0302rt ,\n\n(5)\n\nt\nis the estimated return time at time t.\nwhere \u03c4\u0302rt\n\nUsing the above, we can estimate the set of optimal inverse temperatures \u03b2i0 . Beta values are updated\nby performing a step in the direction of the optimal value: \u03b2it+1 = \u03b2it + \u03bc(\u03b2i0 \u2212 \u03b2it ), where \u03bc is\na learning rate on \u03b2. The properties of (Katzgraber et al., 2006) naturally enforce the ordering\nconstraint on the \u03b2i 's.\n2.3.2\n\nChoosing M and \u03b2M\n\nAnother important point is that (Katzgraber et al., 2006) optimizes the set T while keeping the\nbounds \u03b21 and \u03b2M fixed. While \u03b21 = 1 is a natural choice, we expect the optimal \u03b2M to vary\nduring learning. For this reason, we err on the side of caution and use \u03b2M = 0, relying on a chain\nspawning process to maintain sufficiently high swap rates between neighboring parallel chains.\nSpawning chains as required by the sampler should therefore result in increased stability, as well as\ncomputational savings.\n(Lingenheil et al., 2009) performed an interesting study where they compared the round trip rate\n1/\u03c4rt to the average swap rate measured across all chains. They found that the DEO algorithm,\nwhich alternates between proposing swaps between chains {(i, i + 1); \u2200 even i} followed by {(i, i +\n1); \u2200 odd\nPi}), gave rise to a concave function with a broad maximum around an average swap rate\nof r\u0304 = i ri \u2248 0.4\nOur temperature adaptation therefore works in two phases:\n1. The algorithm of Katzgraber et. al is used to optimize {\u03b2i ; 1 < i < M }, for a fixed M.\n2. Periodically, a chain is spawned whenever r\u0304 < r\u0304min , a hyper-parameter of the algorithm.\nEmpirically, we have observed increased stability when the index j of the new chain is selected such\nthat j = argmaxi (|fup (i)\u2212fup (i+1)|), i \u2208 [1, M \u22121]. To avoid a long burn-in period, we initialize\nthe new chain with the state of the (j + 1)-th chain and choose its inverse temperature as the mean\n(\u03b2j + \u03b2j+1 )/2. A small but fixed burn-in period allows the system to adapt to the new configuration.\n4\n\n\f3\n\nResults and Discussion\n\nWe evaluate our adaptive SML-PT algorithm (SML-APT) on a complex, synthetic dataset. This\ndataset is heavily inspired from the one used in (Desjardins et al., 2010) and was specifically crafted\nto push the limits of the algorithm.\nIt is an online dataset of 28x28 binary images, where each example is sampled from a mixture\nP5\nmodel with probability density function fX (x) = m=1 wm fYm (x). Our dataset thus consists of 5\nmixture components whose weights wm are sampled uniformly in the unit interval and normalized\nto one. Each mixture component Ym is itself a random 28x28 binary image, whose pixels are\nindependent random variables having a probability pm of being flipped. From the point of view of\na sampler performing a random walk in image space, pm is inversely proportional to the difficulty\nof finding the mode in question. The complexity of our synthetic dataset comes from our particular\nchoice of wm and pm . 2 Large wm and small pm lead to modes which are difficult to sample and in\nwhich a Gibbs sampler would tend to get trapped. Large pm values on the other hand will tend to\nintercept \"down\" moving particles and thus present a challenge for parallel tempering.\nFigure 1(a) compares the results of training a 10 hidden unit RBM, using standard SML, SML-PT\nwith {10, 20, 50} parallel chains and our new SML-APT algorithm. We performed 105 updates\n(followed by 2 * 104 steps of sampling) with mini-batches of size 5 and tested learning rates in\n{10\u22123 , 10\u22124 }, \u03b2 learning rates in {10\u22123 , 10\u22124 , 10\u22125 }. For each algorithm, we show the results\nfor the best performing hyper-parameters, averaging over 5 different runs. Results are plotted with\nrespect to computation time to show the relative computational cost of each algorithm.\n\n(a) Log-likelihood\n\n(b) Return Time\n\nFigure 1: (a) Comparison of training likelihood as a function of time for standard SML, SML-PT\nwith 10/20/50 chains and the proposed SML-APT (initialized with 10 chains). SML-APT adapts\nthe temperature set T = {T1 , ..., TM ; T1 < Ti < TM } to minimize round trip time between chains\nT1 and TM , and modifies the number of chains M to maintain a minimal average swap rate. The\nresulting sampler exhibits greater ergodicity and yields better likelihood scores than standard SML\nand SML-PT, without requiring a careful hand-tuning of T . (b) Average return time of each algorithm. SML-APT successfully minimizes this metric resulting in a return time similar to SML-PT\n10, while still outperforming SML-PT 50 in terms of likelihood. Errors bars represent standard error\non the mean.\nAs we can see, standard SML fails to learn anything meaningful: the Gibbs sampler is unable to\ncope with the loss in ergodicity and the model diverges. SML-PT on the other hand performs much\nbetter. Using more parallel chains in SML-PT consistently yields a better likelihood score, as well\nas reduced variance. This seems to confirm that using more parallel chains in SML-PT increases\nthe ergodicity of the sampler. Finally, SML-APT outperforms all other methods. As we will see\nin Figure 2, it does so using only 20 parallel chains. Unfortunately, the computational cost seems\nsimilar to 50 parallel chains. We hope this can be reduced to the same cost as SML-PT with 20\n2\n\nw = [0.3314, 0.2262, 0.0812, 0.0254, 0.3358] and p = [0.0001, 0.0137, 0.0215, 0.0223, 0.0544]\n\n5\n\n\fchains in the near future. Also interesting to note, while the variance of all methods increase with\ntraining time, SML-APT seems immune to this issue.\nWe now compare the various metrics being optimized by our adaptive algorithm. Figure 1(b) shows\nthe average return time for each of the algorithms. We can see that SML-APT achieves a return\ntime which is comparable to SML-PT with 10 chains, while achieving a better likelihood score than\nSML-PT 50.\nWe now select the best performing seeds for SML-PT with 50 chains and SML-APT, and show in\nFigure 2, the resulting fup (i) curves obtained at the end of training.\n\n(a) SML-APT\n\n(b) SML-PT 50\n\nFigure 2: Return time is minimized by tagging each particle with a label: \"up\" if the particle visited\nT1 more recently than TM and \"down\" otherwise. Histograms nu (i) and nd (i) track the number of\nup/down particles at each temperature Ti . Temperatures are modified such that the ratio fup (i) =\nnu (i)/(nu (i) + nd (i)) is linear in the index i. (a) fup curve obtained with SML-APT, as a function\nof temperature index (blue) and inverse temperature (red). SML-APT achieves a linear fup in the\ntemperature index i. (b) Typical fup curve obtained with SML-PT (here using 50 chains). fup is not\nlinear in the index i, which translates to larger return times as shown in Fig. 1(b).\nThe blue curve plots fup as a function of beta index, while the red curves plots fup as a function of\n\u03b2. We can see that SML-APT results in a more or less linear curve for fup (i), which is not the case\nfor SML-PT. In Figure 3(a) we can see the effect on the pair-wise swap statistics ri . As reported in\n(Katzgraber et al., 2006), optimizing T to maintain a linear fup leads to temperatures pooling around\nthe bottleneck. In comparison, SML-PT fails to capture this phenomenon regardless of whether it\nuses 20 or 50 parallel chains (figures 3(b)-3(c)).\n\n(a) SML-APT\n\n(b) SML-PT 20\n\n(c) SML-PT 50\n\nFigure 3: Pairwise swap statistics obtained after 105 updates. Minimizing return time causes SMLAPT to pool temperatures around bottlenecks, achieving large swap rates (0.9) around bottenecks\nwith relatively few chains. SML-PT on the other hand results in a much flatter distribution, requiring\naround 50 chains to reach swap rates close to 0.8.\n6\n\n\fFinally, Figure 4 shows the evolution of the inverse temperature parameters throughout learning. We\ncan see that the position of the bottleneck in temperature space changes with learning. As such, a\nmanual tuning of temperatures would be hopeless in achieving optimal return times.\n\n1.0\n\n0.8\n\nBetas\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n0\n\n20k\n\n40k\n\n60k\n\n80k\n\n100k\n\n120k\n\nNumber of Updates\n\nFigure 4: Graphical depiction of the set {\u03b2i ; i \u2208 [1, M ]}, of inverse temperature parameters used\nby SML-APT during learning. Temperatures pool around a bottleneck to minimize return time,\nwhile new chains are spawned to maintain a given average swap rate. Note that the last 20k updates\nactually correspond to a pure sampling phase (i.e. a learning rate of 0).\n\n4\n\nConclusion\n\nWe have introduced a new adaptive training algorithm for RBMs, which we call Stochastic Maximum Likelihood with Adaptive Parallel Tempering (SML-APT). It leverages the benefits of PT in\nthe negative phase of SML, but adapts and spawns new temperatures so as to minimize return time.\nThe resulting negative phase sampler thus exhibits greater ergodicity. Using a synthetic dataset,\nwe have shown that this can directly translate to a better and more stable likelihood score. In the\nprocess, SML-APT also greatly reduces the number of hyper-parameters to tune: temperature set\nselection is not only automated, but optimal. The end-user is left with very few dials: a standard\nlearning rate on \u03b2i and a minimum average swap rate r\u0304min below which to spawn.\nMuch work still remains. In terms of computational cost, we would like a model trained with SMLAPT and resulting in M chains, to always be upper-bounded by SML-PT initialized with M chains.\nObviously, the above experiments should also be repeated with larger RBMs on natural datasets,\nsuch as MNIST or Caltech Silhouettes.3 .\nAcknowledgments\nThe authors acknowledge the support of the following agencies for research funding and computing\nsupport: NSERC, Compute Canada and CIFAR. We would also like to thank the developers of\nTheano 4 , for developing such a powerful tool for scientific computing, especially gradient-based\nlearning.\n\nReferences\nBehrens, G., Friel, N., & Hurn, M. (2010). Tuning Tempered Transitions. ArXiv e-prints.\nBengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2, 1\u2013127.\nAlso published as a book. Now Publishers, 2009.\n3\n4\n\nhttp://people.cs.ubc.ca/ bmarlin/data/index.shtml\nhttp://deeplearning.net/software/theano/\n\n7\n\n\fCho, K., Raiko, T., & Ilin, A. (2010). Parallel tempering is efficient for learning restricted boltzmann machines.\nProceedings of the International Joint Conference on Neural Networks (IJCNN 2010). Barcelona, Spain.\nDesjardins, G., Courville, A., & Bengio, Y. (2010). Tempered Markov chain monte carlo for training of\nrestricted Boltzmann machine. Proceedings of AISTATS 2010 (pp. 145\u2013152).\nFreund, Y., & Haussler, D. (1994). Unsupervised learning of distributions on binary vectors using two layer\nnetworks (Technical Report UCSC-CRL-94-25). University of California, Santa Cruz.\nHinton, G. E. (1999). Products of experts. Proceedings of the Ninth International Conference on Artificial\nNeural Networks (ICANN) (pp. 1\u20136). Edinburgh, Scotland: IEE.\nHinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation,\n14, 1771\u20131800.\nHinton, G. E., Osindero, S., & Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527\u20131554.\nHyv\u00e4rinen, A. (2005). Estimation of non-normalized statistical models using score matching. Journal of\nMachine Learning Research, 6, 695\u2013709.\nHyv\u00e4rinen, A. (2007). Some extensions of score matching. Computational Statistics and Data Analysis, 51,\n2499\u20132512.\nKatzgraber, H. G., Trebst, S., Huse, D. A., & Troyer, M. (2006). Feedback-optimized parallel tempering monte\ncarlo. Journal of Statistical Mechanics: Theory and Experiment, 2006, P03018.\nLarochelle, H., Bengio, Y., & Turian, J. (2010). Tractable multivariate binary density estimation and the\nrestricted Boltzmann forest. Neural Computation, 22, 2285\u20132307.\nLingenheil, M., Denschlag, R., Mathias, G., & Tavan, P. (2009). Efficiency of exchange schemes in replica\nexchange. Chemical Physics Letters, 478, 80 \u2013 84.\nMarlin, B., Swersky, K., Chen, B., & de Freitas, N. (2010). Inductive principles for restricted boltzmann\nmachine learning. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and\nStatistics (AISTATS 2010).\nNadler, W., & Hansmann, U. H. E. (2007). Generalized ensemble and tempering simulations: A unified view.\nPhys. Rev. E, 75, 026109.\nNeal, R. M. (1994). Sampling from multimodal distributions using tempered transitions (Technical Report\n9421). Dept. of Statistics, University of Toronto.\nSalakhutdinov, R. (2010a). Learning deep boltzmann machines using adaptive mcmc. Proceedings of the\nTwenty-seventh International Conference on Machine Learning (ICML-10) (pp. 943\u2013950). ACM.\nSalakhutdinov, R. (2010b). Learning in Markov random fields using tempered transitions. NIPS'09.\nTaylor, G., & Hinton, G. (2009). Factored conditional restricted Boltzmann machines for modeling motion\nstyle. Proceedings of the 26th International Conference on Machine Learning (ICML'09) (pp. 1025\u20131032).\nMontreal: Omnipress.\nTieleman, T. (2008). Training restricted boltzmann machines using approximations to the likelihood gradient.\nICML 2008 (pp. 1064\u20131071). Helsinki, Finland: ACM.\nTieleman, T., & Hinton, G. (2009). Using fast weights to improve persistent contrastive divergence. ICML\n2009 (pp. 1033\u20131040). ACM.\nWelling, M., Rosen-Zvi, M., & Hinton, G. E. (2005). Exponential family harmoniums with an application to\ninformation retrieval. NIPS'04. Cambridge, MA: MIT Press.\nYounes, L. (1998). On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity\nrates. Stochastics and Stochastics Models (pp. 177\u2013228).\n\n8\n\n\f"}