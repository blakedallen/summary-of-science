{"id": "http://arxiv.org/abs/1205.2097v1", "guidislink": true, "updated": "2012-05-09T20:11:25Z", "updated_parsed": [2012, 5, 9, 20, 11, 25, 2, 130, 0], "published": "2012-05-09T20:11:25Z", "published_parsed": [2012, 5, 9, 20, 11, 25, 2, 130, 0], "title": "Three lectures on free probability", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1205.3983%2C1205.3730%2C1205.0638%2C1205.0959%2C1205.3070%2C1205.1348%2C1205.1294%2C1205.2371%2C1205.4141%2C1205.1500%2C1205.1793%2C1205.0976%2C1205.1936%2C1205.1737%2C1205.2037%2C1205.3051%2C1205.1081%2C1205.3461%2C1205.2614%2C1205.3752%2C1205.0241%2C1205.3757%2C1205.3662%2C1205.3640%2C1205.3203%2C1205.4140%2C1205.0124%2C1205.2971%2C1205.0355%2C1205.1122%2C1205.1843%2C1205.2241%2C1205.0078%2C1205.2363%2C1205.1003%2C1205.1240%2C1205.0488%2C1205.0330%2C1205.4076%2C1205.0822%2C1205.2757%2C1205.3231%2C1205.4256%2C1205.2334%2C1205.4234%2C1205.0727%2C1205.4037%2C1205.0868%2C1205.2162%2C1205.2112%2C1205.4157%2C1205.3349%2C1205.3729%2C1205.0400%2C1205.3082%2C1205.0170%2C1205.2527%2C1205.3505%2C1205.0027%2C1205.1091%2C1205.3028%2C1205.1423%2C1205.0586%2C1205.1924%2C1205.0545%2C1205.3247%2C1205.4335%2C1205.0409%2C1205.1998%2C1205.1488%2C1205.0592%2C1205.1248%2C1205.4370%2C1205.3425%2C1205.3854%2C1205.1053%2C1205.1710%2C1205.4481%2C1205.2429%2C1205.3111%2C1205.2097%2C1205.1119%2C1205.4338%2C1205.3532%2C1205.1355%2C1205.0747%2C1205.3848%2C1205.2003%2C1205.0218%2C1205.0566%2C1205.2916%2C1205.2240%2C1205.2121%2C1205.2514%2C1205.0038%2C1205.1424%2C1205.0106%2C1205.2867%2C1205.2564%2C1205.4216%2C1205.2891&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Three lectures on free probability"}, "summary": "These are notes from a three-lecture mini-course on free probability given at\nMSRI in the Fall of 2010 and repeated a year later at Harvard. The lectures\nwere aimed at mathematicians and mathematical physicists working in\ncombinatorics, probability, and random matrix theory. The first lecture was a\nstaged rediscovery of free independence from first principles, the second dealt\nwith the additive calculus of free random variables, and the third focused on\nrandom matrix models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1205.3983%2C1205.3730%2C1205.0638%2C1205.0959%2C1205.3070%2C1205.1348%2C1205.1294%2C1205.2371%2C1205.4141%2C1205.1500%2C1205.1793%2C1205.0976%2C1205.1936%2C1205.1737%2C1205.2037%2C1205.3051%2C1205.1081%2C1205.3461%2C1205.2614%2C1205.3752%2C1205.0241%2C1205.3757%2C1205.3662%2C1205.3640%2C1205.3203%2C1205.4140%2C1205.0124%2C1205.2971%2C1205.0355%2C1205.1122%2C1205.1843%2C1205.2241%2C1205.0078%2C1205.2363%2C1205.1003%2C1205.1240%2C1205.0488%2C1205.0330%2C1205.4076%2C1205.0822%2C1205.2757%2C1205.3231%2C1205.4256%2C1205.2334%2C1205.4234%2C1205.0727%2C1205.4037%2C1205.0868%2C1205.2162%2C1205.2112%2C1205.4157%2C1205.3349%2C1205.3729%2C1205.0400%2C1205.3082%2C1205.0170%2C1205.2527%2C1205.3505%2C1205.0027%2C1205.1091%2C1205.3028%2C1205.1423%2C1205.0586%2C1205.1924%2C1205.0545%2C1205.3247%2C1205.4335%2C1205.0409%2C1205.1998%2C1205.1488%2C1205.0592%2C1205.1248%2C1205.4370%2C1205.3425%2C1205.3854%2C1205.1053%2C1205.1710%2C1205.4481%2C1205.2429%2C1205.3111%2C1205.2097%2C1205.1119%2C1205.4338%2C1205.3532%2C1205.1355%2C1205.0747%2C1205.3848%2C1205.2003%2C1205.0218%2C1205.0566%2C1205.2916%2C1205.2240%2C1205.2121%2C1205.2514%2C1205.0038%2C1205.1424%2C1205.0106%2C1205.2867%2C1205.2564%2C1205.4216%2C1205.2891&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "These are notes from a three-lecture mini-course on free probability given at\nMSRI in the Fall of 2010 and repeated a year later at Harvard. The lectures\nwere aimed at mathematicians and mathematical physicists working in\ncombinatorics, probability, and random matrix theory. The first lecture was a\nstaged rediscovery of free independence from first principles, the second dealt\nwith the additive calculus of free random variables, and the third focused on\nrandom matrix models."}, "authors": ["Jonathan Novak", "Michael LaCroix"], "author_detail": {"name": "Michael LaCroix"}, "author": "Michael LaCroix", "arxiv_comment": "66 pages, 16 figures, for proceedings of the MSRI semester \"Random\n  matrix theory, interacting particle systems and integrable systems.\"", "links": [{"href": "http://arxiv.org/abs/1205.2097v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1205.2097v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.MP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1205.2097v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1205.2097v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:1205.2097v1 [math.CO] 9 May 2012\n\nTHREE LECTURES ON FREE PROBABILITY\nJONATHAN NOVAK\nWITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nContents\n0. Introduction\n1. Lecture One: Discovering the Free World\n1.1. Counting connected graphs\n1.2. Cumulants and connectedness\n1.3. Cumulants and independence\n1.4. Central Limit Theorem by cumulants\n1.5. Geometrically connected graphs\n1.6. Non-crossing cumulants\n1.7. Non-crossing independence\n1.8. The medium is the message\n1.9. A brief history of the free world\n2. Lecture Two: Exploring the Free World\n2.1. Random walk on the integers\n2.2. P\u00f3lya's theorem\n2.3. Kesten's problem\n2.4. The classical algorithm\n2.5. Voiculescu's algorithm\n2.6. Solution of Kesten's problem\n2.7. Spectral measures and free convolution\n2.8. Free Poisson limit theorem\n2.9. Semicircle flow\n3. Lecture Three: Modelling the Free World\n3.1. Algebraic model of a free arcsine pair\n3.2. Algebraic model of a free semicircular pair\n3.3. Algebraic versus asymptotic models\n3.4. Random matrix model of a free semicircular pair\n3.5. Random matrix model of a free pair with one semicircle\n3.6. Random matrix model of an arbitrary free pair\n3.7. GUE + GUE\n3.8. GUE + deterministic\n3.9. randomly rotated + diagonal\nReferences\n\n1\n\n2\n2\n2\n4\n7\n9\n10\n12\n19\n21\n22\n24\n24\n25\n27\n30\n33\n37\n41\n43\n44\n45\n45\n45\n47\n49\n55\n57\n63\n63\n64\n64\n\n\f2\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n0. Introduction\nThese are notes from a three-lecture mini-course on free probability given at\nMSRI in the Fall of 2010 and repeated a year later at Harvard. The lectures were\naimed at mathematicians and mathematical physicists working in combinatorics,\nprobability, and random matrix theory. The first lecture was a staged rediscovery of\nfree independence from first principles, the second dealt with the additive calculus\nof free random variables, and the third focused on random matrix models.\nMost of my knowledge of free probability was acquired through informal conversations with my thesis supervisor, Roland Speicher, and while he is an expert\nin the field the same cannot be said for me. These notes reflect my own limited\nunderstanding and are no substitute for complete and rigorous treatments, such as\nVoiculescu, Dykema and Nica [44], Hiai and Petz [18], and Nica and Speicher [28].\nIn addition to these sources, the expository articles of Biane [4], Shlyakhtenko [35]\nand Tao [41] are very informative.\nI would like to thank the organizers of the MSRI semester \"Random Matrix\nTheory, Interacting Particle Systems and Integrable Systems\" for the opportunity\nto participate as a postdoctoral fellow. Special thanks are owed to Peter Forrester\nfor coordinating the corresponding MSRI book series volume in which these notes\nappear. I am also grateful to the participants of the Harvard random matrices\nseminar for their insightful comments and questions.\nI am indebted to Michael LaCroix for making the illustrations which accompany\nthese notes.\n1. Lecture One: Discovering the Free World\n1.1. Counting connected graphs. Let mn denote the number of simple, undin\nrected graphs on the vertex set [n] = {1, . . . , n}. We have mn = 2( 2 ) , since each\npair of vertices is either connected by an edge or not. A more subtle quantity is the\nnumber cn of connected graphs on [n]. The sequence (cn )n\u22651 is listed as A01187 in\nSloane's Online Encyclopedia of Integer Sequences; its first few terms are\n1, 1, 4, 38, 728, 26 704, 1 866 256, . . . .\nPerhaps surprisingly, there is no closed formula for cn . However, cn may be understood in terms of the transparent sequence mn in several ways, each of which\ncorresponds to a combinatorial decomposition.\nFirst, we may decompose a graph into two disjoint subgraphs: the connected\ncomponent of a distinguished vertex, say n, and everything else, i.e. the induced\nsubgraph on the remaining vertices. Looking at this the other way around, we may\nbuild\u0001 a graph as follows. From the vertices 1, . . . , n \u2212 1 we can choose k of these in\nn\u22121\nways, and then build an arbitrary graph on these vertices in mk ways. On\nk\nthe remaining n \u2212 1 \u2212 k vertices together with n, we may build a connected graph\nin cn\u2212k ways. This construction produces different graphs for different values of\nk, since the size of the connected component containing the pivot vertex n will be\ndifferent. Moreover, as k ranges from one to n \u2212 1 we obtain all graphs in this\nfashion. Thus we have\nmn =\n\nn\u22121\nX\u0012\nk=0\n\n\u0013\nn\u22121\nmk cn\u2212k ,\nk\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n2\n3\n\n2\n1\n\n3\n\n2\n1\n\n3\n\n2\n1\n\n3\n\n2\n1\n\n3\n\n3\n\n2\n1\n\n3\n\n2\n1\n\n3\n\n2\n1\n\n3\n\n1\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n3\n\n1\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n1\n\n3\n\n4\n\n1\n4\n\n3\n\n1\n4\n\n3\n\n1\n\n3\n\n4\n\n1\n4\n\n3\n\n1\n4\n\n3\n\n1\n4\n\n3\n\n1\n4\n\nFigure 1. Thirty eight of sixty four graphs on four vertices are connected.\nor equivalently\ncn = mn \u2212\n\nn\u22121\nX\u0012\nk=1\n\n\u0013\nn\u22121\nmk cn\u2212k .\nk\n\nWhile this is not a closed formula, it allows the efficient computation of cn given\nc1 , . . . , cn\u22121 .\nA less efficient but ultimately more useful recursion can be obtained by viewing\na graph as the disjoint union of its connected components. We construct a graph\nby first choosing a partition of the underlying vertex set into disjoint non-empty\nsubsets B1 , . . . , Bk , and then building a connected graph on each of these, which\ncan be done in c|B1 | . . . c|Bk | ways. This leads to the formula\nmn =\n\nX Y\n\n\u03c0\u2208P(n) B\u2208\u03c0\n\nc|B| ,\n\n\f4\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nwhere the summation is over the set of all partitions of [n]. We can split off the\nterm of the sum corresponding to the partition [n] = [n] to obtain the recursion\ncn = mn \u2212\n\nX Y\n\n\u03c0\u2208P(n) B\u2208\u03c0\nb(\u03c0)\u22652\n\nc|B| ,\n\nin which we sum over partitions with at least two blocks.\nThe above reasoning is applicable much more generally. Suppose that mn is the\nnumber of \"structures\" which can be built on a set of n labelled points, and that cn\nis the number of \"connected structures\" on these points of the same type. Then the\nquantities mn and cn will satisfy the above (equivalent) relations. This fundamental\nenumerative link between connected and disconnected structures is ubiquitous in\nmathematics and the sciences, see [38, Chapter 5]. Prominent examples come from\nenumerative algebraic geometry [32], where connected covers of curves are counted\nin terms of all covers, and quantum field theory [10], where Feynman diagram sums\nare reduced to summation over connected terms.\n1.2. Cumulants and connectedness. The relationship between connected and\ndisconnected structures is well-known to probabilists, albeit from a different point\nof view. In stochastic applications, mn = mn (X) = E[X n ] is the moment sequence\nof a random variable X, and the quantities cn (X) defined by either of the equivalent\nrecurrences\nn\u22121\nX\u0012\n\n\u0013\nn\u22121\nmk (X)cn\u2212k (X)\nk\nk=0\nX Y\nmn (X) =\nc|B| (X)\nmn (X) =\n\n\u03c0\u2208P(n) B\u2208\u03c0\n\nare called the cumulants of X. This term was suggested by Harold Hotelling and\nsubsequently popularized by Ronald Fisher and John Wishart in an influential\n1932 article [11]. Cumulants were, however, investigated as early as 1889 by the\nDanish mathematician and astronomer Thorvald Nicolai Thiele, who called them\nhalf-invariants. Thiele introduced the cumulant sequence as a transform of the\nmoment sequence defined via the first of the above recurrences, and some years\nlater arrived at the equivalent formulation using the second recurrence. The latter\nis now called the moment-cumulant formula. Thiele's contributions to statistics\nand the early theory of cumulants have been detailed by Anders Hald [16, 17].\nCumulants are now well-established and frequently encountered in probability\nand statistics, sufficiently so that the first four have been given names: mean,\nvariance, skewness, and kurtosis1. The formulas for mean and variance in terms of\nmoments are simple and familiar,\nc1 (X) = m1 (X)\nc2 (X) = m2 (X) \u2212 m1 (X)2 ,\nwhereas the third and fourth cumulants are more involved,\n1In practice, statisticians often define skewness and kurtosis to be the third and fourth cumulants scaled by a power of the variance.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n-3.0\n\n-2.0\n\n-1.0\n\n0.0\n\n1.0\n\n2.0\n\n3.0\n\nFigure 2. The Gaussian density\n\nc3 (X) = m3 (X) \u2212 3m2 (X)m1 (X) + 2m1 (X)3\n\nc4 (X) = m4 (X) \u2212 4m3 (X)m1 (X) \u2212 3m2 (X)2 + 12m2 (X)m1 (X)2 \u2212 6m1 (X)4 .\nIt is not immediately clear why the cumulants of a random variable are of interest.\nIf a random variable X is uniquely determined by its moments, then we may think\nof the moment sequence\n(m1 (X), m2 (X), . . . , mn (X), . . . )\nas coordinatizing X. Passing from moments to cumulants then amounts to a (polynomial) change of coordinates. Why is this advantageous?\nAs a motivating example, let us compute the cumulant sequence of the most\nimportant random variable, the standard Gaussian X. The distribution of X has\ndensity given by the bell curve\nt2\n1\n\u03bcX (dt) = \u221a e\u2212 2 dt\n2\u03c0\n\ndepicted in Figure 2\nWe will now determine the moments of X. Let z be a complex variable, and define\nZ\nMX (z) := etz \u03bcX (dt).\nR\n\n2\n\n\u2212 t2\n\nSince e\ndecays rapidly as |t| \u2192 \u221e, MX (z) is a well-defined entire function of z\nwhose derivatives can be computed by differentiation under the integral sign,\nZ\nZ\n\u2032\ntz\n\u2032\u2032\nMX (z) = te \u03bcX (dt), MX (z) = t2 etz \u03bcX (dt), . . . .\nR\n\nR\n\nIn particular, the nth derivative of MX (z) at z = 0 is\n\n\f6\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n(n)\n\nMX (0) =\n\nZ\n\ntn \u03bcX (dt) = mn (X),\n\nR\n\nso we have the Maclaurin series expansion\nMX (z) =\n\n\u221e\nX\n\nmn (X)\n\nn=0\n\nzn\n.\nn!\n\nThus, the integral MX (z) acts as a generating function for the moments of X. On\nthe other hand, this integral may be explicitly evaluated. Completing the square\nin the exponent of the integrand we find that\nMX (z) = e\n\nz2\n2\n\nZ\nR\n\n2 dt\n1\ne\u2212 2 (t\u2212z) \u221a ,\n2\u03c0\n\nwhence\nMX (z) = e\n\nz2\n2\n\n=\n\n\u221e\nX\nz 2k\n2k k!\nk=0\n\nby translation invariance of Lebesgue measure. We conclude that the odd moments\nof X vanish while the even ones are given by the formula\n(2k)!\n= (2k \u2212 1) * (2k \u2212 3) * * * * * 5 * 3 * 1.\n2k k!\nThis is the number of partitions of the set [2k] into blocks of size two, also called\n\"pairings\": we have 2k \u2212 1 choices for the element to be paired with 1, then 2k \u2212 3\nchoices for the element to be paired with the smallest remaining unpaired element,\netc. Alternatively, we may say that mn (X) is equal to the number of 1-regular\ngraphs on n labelled vertices. It now follows from the fundamental link between\nconnected and disconnected structures that the cumulant cn (X) is equal to the\nnumber of connected 1-regular graphs. Consequently, the cumulant sequence of a\nstandard Gaussian random variable is simply\nm2k (X) =\n\n(0, 1, 0, 0, 0, . . . )\nThe fact that the universality of the Gaussian distribution is reflected in the\nsimplicity of its cumulant sequence signals cumulants as a key concept in probability\ntheory. In Thiele's own words [17],\nThis remarkable proposition has originally led me to prefer the\nhalf-invariants over every other system of symmetrical functions.\nThis sentiment persists amongst modern-day probabilists. To quote Terry Speed\n[37],\nIn a sense which it is hard to make precise, all of the important\naspects of distributions seem to be simpler functions of cumulants\nthan of anything else, and they are also the natural tools with which\ntransformations of systems of random variables can be studied when\nexact distribution theory is out of the question.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n7\n\n1.3. Cumulants and independence. The importance of cumulants stems, ultimately, from their relationship with stochastic independence. Suppose that X\nand Y are a pair of random variables whose moment sequences have been given\nto us by an oracle, and our task is to compute the moments of X + Y . Since\nE[X a Y b ] = E[X a ]E[Y b ], this can be done using the formula\nn \u0012 \u0013\nX\nn\nmn (X + Y ) =\nmk (X)mn\u2212k (Y ),\nk\nk=0\n\nwhich is conceptually clear but computationally inefficient because of its dependence\non n. For example, if we want to compute m100 (X + Y ) we must evaluate a sum\nwith 101 terms, each of which is a product of three factors. Computations with\nindependent random variables simplify dramatically if one works with cumulants\nrather than moments. Indeed, Thiele called cumulants \"half-invariants\" because\nX, Y independent =\u21d2 cn (X + Y ) = cn (X) + cn (Y ) \u2200n \u2265 1.\n\nThanks to this formula, if the cumulant sequences of X and Y are given, then each\ncumulant of X + Y can be computed simply by adding two numbers. The mantra\nto be remembered is:\ncumulants linearize addition of independent random variables.\nFor example, this fact together with the computation we did above yields that the\nsum of two iid standard Gaussians is a Gaussian of variance two.\nIn order to precisely understand the relationship between cumulants and independence, we need to extend the relationship between moments and cumulants to\na relationship between mixed moments and mixed cumulants. Mixed moments are\neasy to define: given a set of (not necessarily distinct) random variables X1 , . . . , Xn ,\nmn (X1 , . . . , Xn ) := E[X1 . . . Xn ].\nIt is clear that mn (X1 , . . . , Xn ) is a symmetric, multilinear function of its arguments. The new notation for mixed moments is related to our old notation for pure\nmoments by\nmn (X) = mn (X, . . . , X),\nwhich we may keep as a useful shorthand.\nWe now define mixed cumulants recursively in terms of mixed moments using\nthe natural extension of the moment-cumulant formula:\nmn (X1 , . . . , Xn ) =\n\nX Y\n\n\u03c0\u2208P(n) B\u2208\u03c0\n\nFor example, we have\n\nc|B| (Xi : i \u2208 B).\n\nm2 (X1 , X2 ) = c2 (X1 , X2 ) + c1 (X1 )c1 (X2 ),\nfrom which we find that the second mixed cumulant of X1 and X2 is their covariance,\nc2 (X1 , X2 ) = m2 (X1 , X2 ) \u2212 m1 (X1 )m2 (X2 ).\n\nMore generally, the recurrence\n\n\f8\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\ncn (X1 , . . . , Xn ) = mn (X1 , . . . , Xn ) \u2212\n\nX Y\n\n\u03c0\u2208P(n) B\u2208\u03c0\nb(\u03c0)\u22652\n\nc|B| (Xi : i \u2208 B)\n\nfacilitates a straightforward inductive proof that cn (X1 , . . . , Xn ) is a symmetric,\nn-linear function of its arguments, which explains Thiele's reference to cumulants\nas his preferred system of symmetric functions.\nThe fundamental relationship between cumulants and stochastic independence\nis the following: X and Y are independent if and only if all their mixed cumulants\nvanish,\nc2 (X, Y ) = 0\nc3 (X, X, Y ) = c3 (X, Y, Y ) = 0\nc4 (X, X, X, Y ) = c4 (X, X, Y, Y ) = c4 (X, Y, Y, Y ) = 0\n..\n.\nThe forward direction of this theorem,\nX, Y independent =\u21d2 mixed cumulants vanish,\nimmediately yields Thiele's linearization property, since by multilinearity we have\ncn (X + Y ) = cn (X + Y, . . . , X + Y )\n= cn (X, . . . , X) + mixed cumulants + cn (Y, . . . , Y )\n= cn (X) + cn (Y ).\nConversely, let X, Y be a pair of random variables whose mixed cumulants vanish.\nLet us check in a couple of concrete cases that this condition forces X and Y to\nobey the algebraic identities associated with independent random variables. In the\nfirst non-trivial case, n = 2, vanishing of mixed cumulants reduces the extended\nmoment-cumulant formula to\nm2 (X, Y ) = c1 (X)c1 (Y ) = m1 (X)m1 (Y ),\nwhich is consistent with the factorization rule E[XY ] = E[X]E[Y ] for independent\nrandom variables. Now let us try an n = 4 example. We compute m4 (X, X, Y, Y )\ndirectly from the extended moment cumulant formula. Referring to Figure 3, we\nfind that vanishing of mixed cumulants implies\nm4 (X, X, Y, Y ) = c2 (X, X)c2 (Y, Y ) + c2 (X, X)c1 (Y )c1 (Y ) + c2 (Y, Y )c1 (X)c1 (X)\n+ c1 (X)c1 (X)c1 (Y )c1 (Y ),\nwhich reduces to the factorization identity E[X 2 Y 2 ] = E[X 2 ]E[Y 2 ].\nOf course, if we compute m4 (X, Y, X, Y ) using the extended moment-cumulant\nformula we should get the same answer, and indeed this is the case, but it is\nimportant to note that the contributions to the sum come from different partitions,\nas indicated in Figure 4.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n9\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nFigure 3. Graphical evaluation of m4 (X, X, Y, Y ).\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nFigure 4. Graphical evaluation of m4 (X, Y, X, Y ).\n1.4. Central Limit Theorem by cumulants. We can use the theory of cumulants presented thus far to prove an elementary version of the Central Limit\nTheorem. Let X1 , X2 , X3 . . . be a sequence of iid random variables, and let X be a\nstandard Gaussian. Suppose that the common distribution of the variables Xi has\nmean zero, variance one, and finite moments of all orders. Put\nSN :=\n\nX1 + * * * + XN\n\u221a\n.\nN\n\nThen, for each positive integer n,\nlim mn (SN ) = mn (X).\n\nN \u2192\u221e\n\nSince moments and cumulants mutually determine one another, in order to prove\nthis CLT it suffices to prove that\n\n\f10\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nlim cn (SN ) = cn (X)\n\nN \u2192\u221e\n\nfor each n \u2265 1. Now, by multilinearity of cn and independence of the Xi 's, we have\n1\n\ncn (SN ) = cn (N \u2212 2 (X1 + * * * + XN ))\nn\n\n= N \u2212 2 (cn (X1 ) + * * * + cn (XN ))\nn\n\n= N 1\u2212 2 cn (X1 ),\n\nwhere the last line follows from the fact that the Xi 's are equidistributed. Thus: if\nn = 1,\n1\n\nc1 (SN ) = N 2 c1 (X1 ) = 0;\nif n = 2,\nc2 (SN ) = c2 (X1 ) = 1;\nif n > 2,\ncn (SN ) = N negative number cn (X1 ).\nWe conclude that\nlim cn (SN ) = \u03b4n2 ,\n\nN \u2192\u221e\n\nwhich we have already identified as the cumulant sequence of a standard Gaussian\nrandom variable.\n1.5. Geometrically connected graphs. Let us now consider a variation on our\noriginal graph-counting question. Given a graph G on the vertex set [n], we may\nrepresent its vertices by n distinct points on the unit circle (say, the nth roots of\nunity) and its edges by straight line segments joining these points. This is how we\nrepresented the set of four-vertex graphs in Figure 1. We will denote this geometric\nrealization of G by |G|. The geometric realization of a graph carries extra structure\nwhich we may wish to consider. For example, it may happen that |G| is a connected\nset of points in the plane even if the graph G is not connected in the usual sense of\ngraph theory. Let \u03ban denote the number of geometrically connected graphs on [n].\nThis is sequence A136653 in Sloane's database; its first few terms are\n1, 1, 4, 39, 748, 27 162, 1 880 872, . . . .\nSince geometric connectivity is a weaker condition than set-theoretic connectivity,\n\u03ban grows faster than cn ; these sequences diverge from one another at n = 4, where\nthe unique disconnected but geometrically connected graph is the \"crosshairs\"\ngraph shown in Figure 5.\nConsider now the problem of computing \u03ban . As with cn , we can address this\nproblem by means of a combinatorial decomposition of the set of graphs with n\nvertices. However, this decomposition must take into account the planar nature\nof geometric connectivity, which our previous set-theoretic decompositions do not.\nConsequently, we must formulate a new decomposition.\nGiven a graph G on [n], let \u03c0(G) denote the partition of [n] induced by the\nconnected components of G (i and j are in the same block of \u03c0(G) if and only if\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n11\n\n2\n\n3\n\n1\n\n4\nFigure 5. The crosshairs graph.\n\n3\n4\n\n2\n\n5\n\n1\n\n6\n\n1 2 3 4 5 6 7 8\n\n1 2 3 4 5 6 7 8\n\n8\n7\nFigure 6. Partition fusion accounts for geometric connectedness.\n\nthey are in the same connected component of G), and let \u03c0(|G|) denote the partition\nof [n] induced by the geometrically connected components of |G| (i and j are in the\nsame block of \u03c0(|G|) if and only if they are in the same geometrically connected\ncomponent of |G|). How are \u03c0(G) and \u03c0(|G|) related? To understand this, let us\nview our geometric graph realizations as living in the hyperbolic plane rather than\nthe Euclidean plane. Thus Figure 1 depicts line systems in the Klein model, in\nwhich the plane is an open disc and straight lines are chords of the boundary circle.\nWe could alternatively represent a graph in the Poincar\u00e9 disc model, where straight\nlines are arcs of circles orthogonal to the boundary circle, or in the Poincar\u00e9 halfplane model, where space is an open-half plane and straight lines are arcs of circles\northogonal to the boundary line. The notion of geometric connectedness does not\ndepend on the particular realization chosen. The half-plane model has the useful\nfeature that the geometric realization |G| essentially coincides with the pictorial\nrepresentation of \u03c0(G), and we can see clearly that crossings in |G| correspond\nexactly to crossings in \u03c0(G). Thus, \u03c0(|G|) is obtained by fusing together crossing\nblocks of \u03c0(G). The resulting partition \u03c0(|G|) no longer has any crossings - by\nconstruction, it is a non-crossing partition, see figure 6.\nWe can now obtain a recurrence for \u03ban . We construct a graph by first choosing\na non-crossing partition of the underlying vertex set into blocks B1 , . . . , Bk and\nthen building a geometrically connected graph on each block, which can be done in\n\u03ba|B1 | . . . \u03ba|Bk | ways. This leads to the formula\n\n\f12\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nmn =\n\nX\n\nY\n\n\u03c0\u2208NC(n) B\u2208\u03c0\n\n\u03ba|B| ,\n\nwhere the summation is over non-crossing partitions of [n]. Just as before, we can\nsplit off the term of the sum corresponding to the partition with only one block to\nobtain the recursion\n\u03ba n = mn \u2212\n\nX\n\nY\n\n\u03c0\u2208NC(n) B\u2208\u03c0\nb(\u03c0)\u22652\n\n\u03ba|B| ,\n\nin which we sum over non-crossing partitions with at least two blocks.\n1.6. Non-crossing cumulants. We have seen above that the usual set-theoretic\nnotion of connectedness manifests itself probabilistically as the cumulant concept.\nWe have also seen that set-theoretic connectedness has an interesting geometric\nvariation, which we called geometric connectedness. This begs the question:\nIs there a probabilistic interpretation of geometric connectedness?\nLet X be a random variable, with moments mn (X). Just as the classical cumulants cn (X) were defined recursively using the relation between all structures and\nconnected structures, we define the non-crossing cumulants of X recursively using\nthe relation between all structures and geometrically connected structures:\nmn (X) =\n\nX Y\n\nNC(n) B\u2208\u03c0\n\n\u03ba|B| (X).\n\nWe will call this the non-crossing moment-cumulant formula. Since connectedness\nand geometric connectedness coincide for structures of size n = 1, 2, 3, the first\nthree non-crossing cumulants of X are identical to its first three classical cumulants.\nHowever, for n \u2265 4, the non-crossing cumulants become genuinely new statistics of\nX.\nOur first step in investigating these new statistics is to look for a non-crossing\nanalogue of the most important random variable, the standard Gaussian. This\nshould be a random variable whose non-crossing cumulant sequence is\n0, 1, 0, 0, . . . .\nIf this search leads to something interesting, we may be motivated to further investigate non-crossing probability theory. If not, we will reject the idea as a will-o'the-wisp.\nFrom the non-crossing moment-cumulant formula, we find that the moments of\nthe non-crossing Gaussian X are given by\nmn (X) =\n\nX\n\nY\n\n\u03c0\u2208NC(n) B\u2208\u03c0\n\n\u03b4|B|,2 =\n\nX\n\n1.\n\n\u03c0\u2208NC2 (n)\n\nThat is, mn (X) is equal to the number of partitions in NC(n) all of whose blocks\nhave size 2, i.e. non-crossing pairings of n points. We know that there are no\npairings at all on an odd number of points, so the odd moments of X must be\nzero, which indicates that X likely has a symmetric distribution. The number\nof pairings on n = 2k points is given by a factorial going down in steps of two,\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n13\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n(1,-1,1,-1,1,-1)\n\n(1,1,-1,-1,1,-1)\n\n(1,1,-1,-1,1,-1)\n\n(1,1,-1,1,-1,-1)\n\n(1,1,-1,1,-1,-1)\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n(1,-1,1,1,-1,-1)\n\n(1,1,-1,1,-1,-1)\n\n(1,1,1,-1,-1,-1)\n\n(1,1,1,-1,-1,-1)\n\n(1,1,1,-1,-1,-1)\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n1 2 3 4 5 6\n\n(1,-1,1,1,-1,-1)\n\n(1,1,-1,1,-1,-1)\n\n(1,1,1,-1,-1,-1)\n\n(1,1,1,-1,-1,-1)\n\n(1,1,1,-1,-1,-1)\n\nFigure 7. Construction of the function f from pairings to bitstrings.\n(2k \u2212 1)!! = (2k \u2212 1) * (2k \u2212 3) * * * * 5 * 3 * 1, so the number of non-crossing pairings\nmust be smaller than this double factorial.\nIn order to count non-crossing pairings on 2k points, we construct a function\nf from the set of all pairings on 2k points to length 2k sequences of \u00b11's. This\nfunction is easy to describe: if i < j constitute a block of \u03c0, then the ith element of\nf (\u03c0) is +1 and the j th element of f (\u03c0) is \u22121. See Figure 7 for an illustration of this\nfunction in the case k = 3. By construction, f is a surjection from the set of pairings\non 2k points onto the set of length 2k sequences of \u00b11's all of whose partial sums\nare non-negative and whose total sum is zero. We leave it to the reader to show\nthat the fibre of f over any such sequence contains exactly one non-crossing pairing,\nso that f restricts to a bijection from non-crossing pairings onto its image. The\nimage sequences can be neatly enumerated using the Dvoretzky-Motzkin-Raney\ncyclic shift lemma, as in [14, \u00a77.5]. They are counted by the Catalan numbers\n\u0012 \u0013\n2k\n1\n,\nCatk =\nk+1 k\n\nwhich are smaller than the double factorials by a factor of 2k /(k+1)!. This indicates\nthat the distribution of X decays even more rapidly than the Gaussian distribution\nand might even be compactly supported.\nWe have discovered that\n(\n0, if n odd\nmn (X) =\nCat n2 , if n even.\n\nThe Catalan numbers are ubiquitous in enumerative combinatorics, see [38, Exercise\n6.19] as well as [39], and their appearance in this context is the first sign that we\nare onto something interesting. We are now faced with an inverse problem: we are\nnot trying to calculate the moments of a random variable given its distribution,\nrather we know that the moment sequence of X is\n0, Cat1 , 0, Cat2 , 0, Cat3 , 0, . . . .\nand we would like to write down its distribution \u03bcX . Equivalently, we are looking\nfor an integral representation of the entire function\n\n\f14\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nMX (z) =\n\n\u221e\nX\n\nCatn\n\nn=0\n\nwhich has the form\n\nMX (z) =\n\n\u221e\nX\nz 2n\nz 2n\n=\n(2n)! n=0 n!(n + 1)!\n\nZ\n\netz \u03bcX (dt),\n\nR\n\nwith \u03bcX a probability measure on the real line. The solution to this problem can\nbe extracted from the classical theory of Bessel functions.\nThe modified Bessel function I\u03b1 (z) of order \u03b1 is one of two linearly independent\nsolutions to the modified Bessel equation\n\u0012\n\u0013\nd2\nd\nz2 2 + z\n\u2212 (z 2 + \u03b12 ) F = 0,\ndz\ndz\nthe other being the Macdonald function\n\u03c0 I\u2212\u03b1 (z) \u2212 I\u03b1 (z)\n.\n2\nsin(\u03b1\u03c0)\nThe modified Bessel equation (and hence the functions I\u03b1 , K\u03b1 ) appears in many\nproblems of physics and engineering since it is related to solutions of Laplace's\nequation with cylindrical symmetry. An excellent reference on this topic is [1,\nChapter 4].\nInterestingly, Bessel functions also occur in the combinatorics of permutations:\na remarkable identity due to Ira Gessel asserts that\nK\u03b1 (z) =\n\ndet[Ii\u2212j (2z)]ki,j=1 =\n\n\u221e\nX\n\nn=0\n\nlisk (n)\n\nz 2n\n,\n(n!)2\n\nwhere lisk (n) is the number of permutations in the symmetric group S(n) with no\nincreasing subsequence of length k + 1. Gessel's identity was the point of departure\nin the work of Jinho Baik, Percy Deift and Kurt Johansson who, answering a\nquestion posed by Stanislaw Ulam, proved that the limit distribution of the length of\nthe longest increasing subsequence in a uniformly distributed random permutation\nis given by the (\u03b2 = 2) Tracy-Widom distribution. This non-classical distribution\nwas isolated and studied by Craig Tracy and Harold Widom in a series of works\non random matrix theory in the early 1990's where it emerged as the limiting\ndistribution of the top eigenvalue of large random Hermitian matrices. It has a\ndensity which may also be described in terms of Bessel functions, albeit indirectly.\nConsider the ordinary differential equation\nd2\nu = 2u3 + xu\ndx2\nfor a real function u = u(x), which is known as the Painlev\u00e9 II equation after\nthe French mathematician (and two-time Prime Minister of France) Paul Painlev\u00e9.\nIt is known that this equation has a unique solution, called the Hastings-McLeod\nsolution, with the asymptotics u(x) \u223c \u2212 Ai(x) as x \u2192 \u221e, where\nr\n2 3\n1 x\nK1 ( x2 )\nAi(x) =\n\u03c0 3 3 3\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n15\n\nis a scaled specialization of the Macdonald function known as the Airy function.\nDefine the Tracy-Widom distribution function by\nR\u221e\n\n2\n\nF (t) = e\u2212 t (x\u2212t)u(x) dx ,\nwhere u is the Hastings-McLeod solution to Painlev\u00e9 II. The theorem of Baik, Deift\nand Johansson asserts that\n1\nlis \u221a\n1/6 (n) = F (t)\nn! 2 n+tn\nfor any t \u2208 R. From this one may conclude, for example, that the probability a\npermutation drawn uniformly at random from the symmetric group S(n2 ) avoids\nthe pattern 1 2 . . . 2n + 1 converges to F (0) = 0.9694 . . . . We refer the interested\nreader to Richard Stanley's survey [40] for more information on this topic.\nNineteenth century mathematicians knew how to describe the modified Bessel\nfunction both as a series,\nlim\n\nn\u2192\u221e\n\nI\u03b1 (z) =\nand as an integral,\n\n\u221e\nX\n\n( z2 )2n+\u03b1\n,\nn!\u0393(n + 1 + \u03b1)\nn=0\n\n( z )\u03b1\nI\u03b1 (z) = \u221a 2\n\u03c0\u0393(\u03b1 + 21 )\n\nZ\u03c0\n\ne(cos \u03b8)z (sin \u03b8)2\u03b1 d\u03b8.\n\n0\n\nFrom the series representation we find that\nI1 (2z)\n,\nz\nand consequently we have the integral representation\nMX (z) =\n\n2\nMX (z) =\n\u03c0\n\nZ\u03c0\n\ne2(cos \u03b8)z sin2 \u03b8d\u03b8.\n\n0\n\nThis is one step removed from what we want: it tells us that the Catalan numbers\nare the even moments of the random variable X = 2 cos(Y ), where Y is a random\nvariable with distribution\n2\nsin2 \u03b8d\u03b8\n\u03c0\nsupported on the interval [0, \u03c0]. However, this is a rather interesting intermediate\nstep since the above measure appears in number theory, where it is called the\nSato-Tate distribution, see Figure 8.\nThe Sato-Tate distribution arises in the arithmetic statistics of elliptic curves.\nThe location of integer points on elliptic curves is a classical topic in number theory.\nFor example, Diophantus of Alexandria wrote that the equation\n\u03bcY (d\u03b8) =\n\ny 2 = x3 \u2212 2\nhas the solution x = 3, y = 5, and in the 1650's Pierre de Fermat claimed that there\nare no other positive integer solutions. This is the striking assertion that 26 is the\nonly number one greater than a perfect square and one less than a perfect cube,\n\n\f16\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n\n0.0\n\n1.0\n\n\u03c0\n2\n\n2.0\n\n3.0 \u03c0\n\nFigure 8. The Sato-Tate density\nsee Figure 9. That this is indeed the case was proved by Leonhard Euler in 1770,\nalthough according to some sources Euler's proof was incomplete and the solution\nto this problem should be attributed to Axel Thue in 1908.\nModern number theorists study solutions to elliptic Diophantine equations by\nreducing modulo primes. Given an elliptic curve\ny 2 = x3 + ax + b,\n3\n\n2\n\na, b \u2208 Z,\n\nlet \u2206 = \u221216(4a + 27b ) be sixteen times the discriminant of x3 + ax + b, and let\nSp be the number of solutions of the congruence\ny 2 \u2261 x3 + ax + b\n\nmod p\n\nwhere p is a prime which does not divide \u2206. In his 1924 doctoral thesis, Emil Artin\nconjectured that\n\u221a\n|Sp \u2212 p| \u2264 2 p\n\nfor all such good reduction primes. This remarkable inequality states that the num\u221a\nber of solutions modulo p is roughly p itself, up to an error of order p. Artin's\nconjecture was proved by Helmut Hasse in 1933. Around 1960, Mikio Sato and\nJohn Tate became interested in the finer question of the distribution of the cen\u221a\ntred and scaled solution count (Sp \u2212 p)/ p for typical elliptic curves E (meaning\nthose without complex multiplication) as p ranges over the infinitely many primes\nnot dividing the discriminant of E. Because of Hasse's theorem, this amounts to\nstudying the distribution of the angle \u03b8p defined by\nSp \u2212 p\n= 2 cos \u03b8p\n\u221a\np\nin the the interval [0, \u03c0]. Define a sequence \u03bcE\nN of empirical probability measures\nassociated to E by\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n17\n\nFigure 9. Diophantine perspectives on twenty-six\n\n\u03bcE\nN =\n\n1 X\n\u03b4\u03b8 p ,\n\u03c0(N )\np\u2264N\n\nwhere \u03c0(N ) is the number of prime numbers less than or equal to N . Sato and\nTate conjectured that, for any elliptic curve E without complex multiplication, \u03bcE\nN\nconverges weakly to the Sato-Tate distribution as N \u2192 \u221e. This is a universality\nconjecture: it posits that certain limiting behaviour is common to a large class of\nelliptic curves irrespective of their fine structural details. Major progress on the\nSato-Tate conjecture has been made within the last decade; we refer the reader to\nthe surveys of Barry Mazur [23] and Ram Murty and Kumar Murty [25] for further\ninformation.\nThe random variable we seek is not the Sato-Tate variable Y , but twice its cosine,\nX = 2 cos(Y ). Making the substitution s = arccos(\u03b8) in the integral representation\nof MX (z) obtained above, we obtain\n2\nMX (z) =\n\u03c0\n\nZ1\n\ne2sz\n\n\u22121\n\nand further substituting t = 2s this becomes\n1\nMX (z) =\n2\u03c0\n\nZ2\n\n\u22122\n\np\n1 \u2212 s2 ds,\n\netz\n\np\n4 \u2212 t2 dt.\n\nThus the random variable X with even moments the Catalan numbers and vanishing\nodd moments is distributed in the interval [\u22122, 2] with density\n1 p\n4 \u2212 t2 dt,\n2\u03c0\nwhich is both symmetric and compactly supported. This is another famous distribution: it is called the Wigner semicircle distribution after the physicist Eugene\n\u03bcX (dt) =\n\n\f18\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n0.30\n0.25\n0.20\n0.15\n0.10\n0.05\n\n-2.0\n\n-1.0\n\n0.0\n\n1.0\n\n2.0\n\nFigure 10. The Wigner semicircle density\nWigner, who considered it in the 1950's in a context ostensibly unrelated to elliptic\ncurves. The density of \u03bcX is shown in Figure 10 - note that it is not a semicircle,\nbut rather half an ellipse of semi-major axis two and semi-minor axis 1/\u03c0.\nWigner was interested in constructing models for the energy levels of complex\nsystems, and hit on the idea that the eigenvalues of large symmetric random matrices provide a good approximation. Wigner considered N \u00d7 N symmetric matrices\nXN whose entries XN (ij) are independent random variables, up to the symmetry\nconstraint XN (ij) = XN (ji). Random matrices of this form are now known as\nWigner matrices, and their study remains a topic of major interest today. Wigner\nstudied the empirical spectral distribution of the eigenvalues of XN , i.e. the probability measure\n\u03bcN =\n\nN\n1 X\n\u03b4\u03bbk (N )\nN\nk=1\n\nwhich places mass 1/N at each eigenvalue of XN . Note that, unlike in the setting\nabove where we considered the sequence of empirical measures associated to a fixed\nelliptic curve E, the measure \u03bcN is a random measure since XN is a random matrix.\nWigner showed that the limiting behaviour of \u03bcN does not depend on the details of\nthe random variables which make up XN . In [45], he made the following hypotheses:\n(1) Each XN (ij) has a symmetric distribution;\n(2) Each XN (ij) has finite moments of all orders, each of which is bounded by\na constant independent of N, i, j;\n(3) The variance of XN (ij) is 1/N .\nWigner prove that, under these hypotheses, \u03bcN converges weakly to the semicircle\nlaw which now bears his name. We will see a proof of Wigner's theorem for random\nmatrices with (complex) Gaussian entries in Lecture Three. The universality of the\nspectral structure of real and complex Wigner matrices holds at a much finer level,\nand under much weaker hypotheses, both at the edges of the semicircle [36] and in\nthe bulk [9, 42].\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n19\n\n1.7. Non-crossing independence. Our quest for the non-crossing Gaussian has\nbrought us into contact with interesting objects (random permutations, elliptic\ncurves, random matrices) and the limit laws which govern them (Tracy-Widom\ndistribution, Sato-Tate distribution, Wigner semicircle distribution). This motivates us to continue developing the rudiments of non-crossing probability theory -\nperhaps we have hit on a framework within which these objects may be studied.\nOur next step is to introduce a notion of non-crossing independence. We know\nthat classical independence is characterized by the vanishing of mixed cumulants.\nImitating this, we will define non-crossing independence via the vanishing of mixed\nnon-crossing cumulants. Like classical mixed cumulants, the non-crossing mixed\ncumulant functionals are defined recursively via the multilinear extension of the\nnon-crossing moment-cumulant formula,\nmn (X1 , . . . , Xn ) =\n\nX\n\nY\n\n\u03c0\u2208NC(n) B\u2208\u03c0\n\nThe recurrence\n\n\u03ban (X1 , . . . , Xn ) = mn (X1 , . . . , Xn ) \u2212\n\n\u03ba|B| (Xi : i \u2208 B).\n\nX\n\nY\n\n\u03c0\u2208NC(n) B\u2208\u03c0\n\n\u03ba|B| (Xi : i \u2208 B)\n\nand induction establish that \u03ban (X1 , . . . , Xn ) is a symmetric multilinear function of\nits arguments. Two random variables X, Y are said to be non-crossing independent\nif their mixed non-crossing cumulants vanish:\n\u03ba2 (X, Y ) = 0\n\u03ba3 (X, X, Y ) = \u03ba3 (X, Y, Y ) = 0\n\u03ba4 (X, X, X, Y ) = \u03ba4 (X, X, Y, Y ) = \u03ba4 (X, Y, Y, Y ) = 0\n..\n.\nAn almost tautological consequence of this definition is:\nX, Y non-crossing independent =\u21d2 \u03ban (X + Y ) = \u03ban (X) + \u03ban (Y ) \u2200n \u2265 1.\nThus, just as classical cumulants linearize the addition of classically independent\nrandom variables,\nnon-crossing cumulants linearize addition of non-crossing independent random variables.\nWe can also note that the semicircular random variable X, whose non-crossing\ncumulant sequence is 0, 1, 0, 0, . . . , plays the role of the standard Gaussian with\nrespect to this new notion of independence. For example, since non-crossing cumulants linearize non-crossing independence, the sum of two non-crossing independent semicircular random variables is a semicircular random variable of variance\ntwo. The non-crossing analogue of the Central Limit Theorem asserts that, if\nX1 , X2 , . . . is a sequence of non-crossing independent and identically distributed\nrandom variables with mean zero and variance one, then the moments of\nSN =\n\nX1 + * * * + XN\n\u221a\nN\n\n\f20\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nX X Y Y\n\nFigure 11. Graphical evaluation of m4 (X, X, Y, Y ) using noncrossing cumulants.\nconverge to the moments of the standard semicircular X as N \u2192 \u221e. The proof\nof this fact is identical to the proof of the classical Central Limit Theorem given\nabove, except that classical cumulants are replaced by non-crossing cumulants.\nOf course, we don't really know what non-crossing independence means. For example, if X and Y are non-crossing independent, is it true that E[XY ] = E[X]E[Y ]?\nThe answer is yes, since classical and non-crossing mixed cumulants agree up to\nand including order three,\nc1 (X) = \u03ba1 (X),\n\nc2 (X, Y ) = \u03ba2 (X, Y ),\n\nc3 (X, Y, Z) = \u03ba3 (X, Y, Z).\n\nBut what about higher order mixed moments?\nWe observed above that, in the classical case, vanishing of mixed cumulants\nallows us to recover the familiar algebraic identities governing the expectation of\nindependent random variables. We do not have a priori knowledge of the algebraic\nidentities governing the expectation of non-crossing independent random variables,\nso we must discover them using the vanishing of mixed non-crossing cumulants.\nLet us see what this implies for the mixed moment m4 (X, X, Y, Y ) = E[X 2 Y 2 ].\nReferring to Figure 11 we see that in this case the non-crossing moment-cumulant\nformula reduces to\nm4 (X, X, Y, Y ) = \u03ba2 (X, X)\u03ba2 (Y, Y ) + \u03ba2 (X, X)\u03ba1 (Y )\u03ba1 (Y ) + \u03ba2 (Y, Y )\u03ba1 (X)\u03ba1 (X)\n+ \u03ba1 (X)\u03ba1 (X)\u03ba1 (Y )\u03ba1 (Y )\nwhich is exactly the formula we obtained for classically independent random variablesusing the classical moment-cumulant formula.\nHowever, when we use the non-crossing moment-cumulant formula to evaluate the\nsame mixed moment with its arguments permuted, we instead get\nm4 (X, Y, X, Y ) = \u03ba2 (X, X)\u03ba1 (Y )\u03ba1 (Y )+\u03ba2 (Y, Y )\u03ba1 (X)\u03ba1 (X)+\u03ba1 (X)\u03ba1 (X)\u03ba1 (Y )\u03ba1 (Y ),\nsee Figure 12.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n21\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nX Y X Y\n\nFigure 12. Graphical evaluation of m4 (X, Y, X, Y ) using noncrossing cumulants.\nSince m4 (X, X, Y, Y ) = m4 (X, Y, X, Y ), we are forced to conclude that the two\nexpressions obtained are equal, which in turn forces\n\u03ba2 (X, X)\u03ba2 (Y, Y ) = 0.\nThus, if X, Y are non-crossing independent random variables, at least one of them\nmust have vanishing variance, and consequently must be almost surely constant.\nThe converse is also true - one can show that a (classical or non-crossing) mixed\ncumulant vanishes if any of its entries are constant random variables. So we\nhave classified pairs of non-crossing independent random variables: they look like\n{X, Y } = {arbitrary, constant}. Such pairs of random variables are of no interest\nfrom a probabilistic perspective. It would seem that non-crossing probability is a\ndead end.\n1.8. The medium is the message. If \u03a9 is a compact Hausdorff space then the\nalgebra A(\u03a9) of continuous functions X : \u03a9 \u2192 C is a commutative C \u2217 -algebra.\nThis means that in addition to its standard algebraic structure (pointwise addition, multiplication and scalar multiplication of functions) A(\u03a9) is equipped with\na norm satisfying the Banach algebra axioms and an antilinear involution which is\ncompatible with the norm, kX \u2217 Xk = kXk2 . The norm comes from the topology\nof the source, kXk = sup\u03c9 |X(\u03c9)|, and the involution comes from the conjugation\nautomorphism of the target, X \u2217 (\u03c9) = X(\u03c9). Conversely, a famous theorem of\nIsrael Gelfand asserts that any unital commutative C \u2217 -algebra A can be realized\nas the algebra of continuous functions on a compact Hausdorff space \u03a9(A) in an\nessentially unique way. In fact, \u03a9(A) may be constructed as the set of maximal\nideals of A equipped with a suitable topology. The associations \u03a9 7\u2192 A(\u03a9) and\nA 7\u2192 \u03a9(A) are contravariantly functorial and set up a dual equivalence between\nthe category of compact Hausdorff spaces and the category of unital commutative\nC \u2217 -algebras.\nThere are many situations in which one encounters a category of spaces dually\nequivalent to a category of algebras. In a wonderful book [26], the mathematicians\ncollectively known as Jet Nestruev develop the theory of smooth real manifolds\n\n\f22\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nentirely upside-down: the theory is built in the dual algebraic category, whose objects Nestruev terms smooth complete geometric R-algebras, and then exported to\nthe geometric one by a contravariant functor. In many situations, given a category\nof spaces dually equivalent to a category of algebras it pays to shift our stance\nand view the the algebraic category as primary. In particular, the algebraic point\nof view is typically easier to generalize. This is the paradigm shift driving Alain\nConnes' non-commutative geometry programme, and the reader is referred to [7]\nfor much more information.\nThis paradigm shift is precisely what is needed in order to salvage non-crossing\nprobability theory. In probability theory, the notion of space is that of a Kolmogorov\ntriple (\u03a9, F , P ) which models the probability to observe a stochastic system in a\ngiven state or collection of states. The dual algebraic object associated to a Kolmogorov triple is L\u221e (\u03a9, F , P ), the algebra of essentially bounded complex random\nvariables X : \u03a9 \u2192 C. Just like in the case of continuous functions on a compact\nHausdorff space, this algebra has a very special structure: it is a commutative\nvon\nR\nNeumann algebra equipped with a unital faithful tracial state, \u03c4 [X] = \u03a9 XdP .\nMoreover, there is an analogue of Gelfand's theorem in this setting which says that\nany commutative von Neumann algebra can be realized as the algebra of bounded\ncomplex random variables on a Kolmogorov triple in an essentially unique way.\nThis is the statement that the categories of Kolmogorov triples and commutative\nvon Neumann algebras are dual equivalent.\nNon-crossing independence was rendered trivial by the commutativity of random\nvariables. We can rescue it from the abyss by following the lead of non-commutative\ngeometry and dropping commutativity in the dual category: we shift our stance\nand define a non-commutative probability space to be a pair (A, \u03c4 ) consisting of\na possibly non-commutative complex associative unital algebra A together with a\nunital linear functional \u03c4 : A \u2192 C. If we reinstate commutativity and insist that A\nis a von Neumann algebra and \u03c4 a faithful tracial state, we are looking at essentially\nbounded random variables on a Kolmogorov triple, but a general non-commutative\nprobability space need not be an avatar of any classical probabilistic entity.\nAs a nod to the origins of this definition, and in order to foster analogies with\nclassical probability, we refer to the elements of A as random variables and call\n\u03c4 the expectation functional. This prompts some natural questions. Before this\nsubsection we only discussed real random variables - complex numbers crept in\nwith the abstract nonsense. What is the analogue of the notion of real random\nvariable in a non-commutative probability space? Probabilists characterize random\nvariables in terms of their distributions. Can we assign distributions to random\nvariables living in a non-commutative probability space? Is it possible to give\nmeaning to the phrase \"the distribution of a bounded real random variable living in\na non-commutative probability space is a compactly supported probability measure\non the line\"? We will deal with some of these questions at the end of Lecture Two.\nFor now, however, we remain in the purely algebraic framework, where the closest\nthing to the distribution of a random variable X \u2208 A is its moment sequence\nmn (X) = \u03c4 [X n ]. As in [44, Page 12],\nThe algebraic context is not used in the pursuit of generality, but\nrather of transparence.\n1.9. A brief history of the free world. Having cast off the yoke of commutativity, we are free - free to explore non-crossing probability in the new framework\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n23\n\nprovided by the non-commutative probability space concept. Non-crossing probability has become Free Probability, and will henceforth be referred to as such.\nAccordingly, non-crossing cumulants will now be referred to as free cumulants, and\nnon-crossing independence will be termed free independence.\nThe reader is likely aware that free probability is a flourishing area of contemporary mathematics. This first lecture has been historical fiction, and is essentially\nan extended version of [27]. Free probability was not discovered in the context of\ngraph enumeration problems, or by tampering with the cumulant concept, although\nin retrospect it might have been. Rather, free probability theory was invented by\nDan-Virgil Voiculescu in the 1980's in order to address a famous open problem in\nthe theory of von Neumann algebras, the free group factors isomorphism problem.\nThe problem is to determine when the von Neumann algebra of the free group on\na generators is isomorphic to the von Neumann algebra of the free group on b generators. It is generally believed that these are isomorphic von Neumann algebras if\nand only if a = b, but this remains an open problem. Free probability theory (and\nits name) originated in this operator-algebraic context.\nVoiculescu's definition of free independence, which was modelled on the free\nproduct of groups, is the following: random variables X, Y in a non-commutative\nprobability space (A, \u03c4 ) are said to be freely independent if\n\u03c4 [f1 (X)g1 (Y ) . . . fk (X)gk (Y )] = 0\nwhenever f1 , g1 , . . . , fk , gk are polynomials such that\n\u03c4 [f1 (X)] = \u03c4 [g1 (X)] = * * * = \u03c4 [fk (X)] = \u03c4 [gk (Y )] = 0.\nThis should be compared with the definition of classical independence: random variables X, Y in a non-commutative probability space (A, \u03c4 ) are said to be classically\nindependent if they commute, XY = Y X, and if\n\u03c4 [f (X)g(Y )] = 0\nwhenever f and g are polynomials such that \u03c4 [f (X)] = \u03c4 [g(Y )] = 0. These two\ndefinitions are antithetical: classical independence has commutativity built into\nit, while free independence becomes trivial if commutativity is imposed. Nevertheless, both notions are accommodated within the non-commutative probability\nspace framework.\nThe precise statement of equivalence between classical independence and vanishing of mixed cumulants is due to Gian-Carlo Rota [31]. In the 1990's, knowing\nboth of Voiculescu's new free probability Theory and Rota's approach to classical\nprobability theory, Roland Speicher made the beautiful discovery that by excising\nthe lattice of set partitions from Rota's foundations and replacing it with the lattice of non-crossing partitions, much of Voiculescu's theory could be recovered and\nextended by elementary combinatorial methods. In particular, Speicher showed\nthat free independence is equivalent to the vanishing of mixed free cumulants. The\ncombinatorial approach to free probability is exhaustively applied in [28], while the\noriginal analytic approach of Voiculescu is detailed in [44].\n\n\f24\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n2. Lecture Two: Exploring the Free World\nLecture One culminated in the notion of a non-commutative probability space\nand the realization that this framework supports two types of independence: classical independence and free independence. From here we can proceed in several\nways. One option is to prove an abstract result essentially stating that these are\nthe only notions of independence which can occur. This result, due to Speicher,\nplaces classical and free independence on equal footing. Another possibility is to\npresent concrete problems of intrinsic interest where free independence naturally\nappears. We will pursue the second route, and examine problems emerging from the\ntheory of random walks on groups which can be recast as questions about free random variables. In the course of solving these problems we will develop the calculus\nof free random variables and explore the terrain of the free world.\n2.1. Random walk on the integers. The prototypical example of a random walk\non a group is the simple random walk on Z: a walker initially positioned at zero\ntosses a fair coin at each tick of the clock - if it lands heads he takes a step of\n+1, if it lands tails he takes a step of \u22121. A random walk is said to be recurrent\nif it returns to its initial position with probability one, and transient if not. Is the\nsimple random walk on Z recurrent or transient?\nLet \u03b1(n) denote the number of walks which return to zero for the first time after\nn steps, and let \u03c6(n) = 2\u2212n \u03b1(n) denote the corresponding probability that the first\nreturn occurs at time n. Note that \u03b1(0) = \u03c6(0) = 0, and define\n\u221e\nX\n\nF (z) =\n\n\u03c6(n)z n .\n\nn=0\n\nThen\nF (1) =\n\n\u221e\nX\n\nn=0\n\n\u03c6(n) \u2264 1\n\nis the probability we seek. The radius of convergence of F (z) is at least one, and\nby Abel's theorem\nF (1) = lim F (x)\nx\u21921\n\nas x approaches 1 in the interval [0, 1).\nLet \u03bb(n) denote the number of length n loops on Z based at 0, and let \u03c1(n) =\n2\u2212n \u03bb(n) be the corresponding probability of return at time n (regardless of whether\nthis is the first return or not). Note that \u03bb(0) = \u03c1(0) = 1. We have\n\u03bb(n) =\n\n(\n\nFrom Stirling's formula, we see that\n\n0, if n odd\n\u0001\nn\nn , if n even\n2\n\n1\n\u03c1(2k) \u223c \u221a\n\u03c0k\nas k \u2192 \u221e. Thus the radius of convergence of\n\n.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\nR(z) =\n\n\u221e\nX\n\n25\n\n\u03c1(n)z n\n\nn=0\n\nis one.\nWe can decompose the set of loops of given length according to the number of\nsteps taken to the first return. This produces the equation\n\u03bb(n) =\n\nn\nX\n\n\u03b1(k)\u03bb(n \u2212 k).\n\nn\nX\n\n\u03c6(k)\u03c1(n \u2212 k).\n\nk=0\n\nEquivalently, since all probabilities are uniform,\n\u03c1(n) =\n\nk=0\n\nSumming on z, this becomes the identity\nR(z) \u2212 1 = F (z)R(z)\n\nin the algebra of holomorphic functions on the open unit disc in C. Since R(z) has\nnon-negative coefficients, it is non-vanishing for x \u2208 [0, 1) and we can write\nF (x) = 1 \u2212\n\n1\n,\nR(x)\n\n0 \u2264 x < 1.\n\nThus\n1\n.\nlimx\u21921 R(x)\nIf R(1) < \u221e, then by Abel's theorem limx\u21921 R(x) = R(1) and we obtain F (1) < 1.\nOn the other hand, if R(1) = \u221e, then limx\u21921 R(x) = \u221e and we get F (1) = 1.\nThus the simple random walk\nP is transient or recurrent according to the convergence\nor divergence of the series\n\u03c1(n). From the Stirling estimate above we find that\nthis sum diverges, so the simple random walk on Z is recurrent.\nF (1) = lim F (x) = 1 \u2212\nx\u21921\n\n2.2. P\u00f3lya's theorem. In the category of abelian groups, coproduct is direct sum:\na\n\nGi =\n\ni\u2208I\n\nM\n\nGi .\n\ni\u2208I\n\nIn 1921, George P\u00f3lya [30] proved that the simple random walk on\nZd = Z \u2295 * * * \u2295 Z\n|\n{z\n}\nd\n\nis recurrent for d = 1, 2 and transient for d > 2. This striking result can be deduced\nsolely from an understanding of the simple random walk on Z.\nLet us give a proof of P\u00f3lya's theorem. Let \u03bbd (n) denote the number of length\nn loops on Zd based at 0d . Let \u03c1d (n) denote the probability of return to 0d after n\nsteps,\n\u03c1d (n) =\n\n1\n\u03bbd (n).\n(2d)n\n\n\f26\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nAs above, the simple random walk on Zd is recurrent if the sum\nand transient otherwise. Form the loop generating function\nLd (z) =\n\n\u221e\nX\n\nP\n\n\u03c1d (n) diverges,\n\n\u03bbd (n)z n .\n\nn=0\n\nWe aim to prove that\nLd\n\n\u0012\n\n1\n2d\n\n\u0013\n\n=\n\n\u221e\nX\n\n\u03c1d (n)\n\nn=0\n\ndiverges for d = 1, 2 and converges for d > 2.\nWhile the ordinary loop generating function is hard to analyze directly, the\nexponential loop generating function\nEd (z) =\n\n\u221e\nX\n\nn=0\n\n\u03bbd (n)\n\nzn\nn!\n\nis quite accessible. Indeed, as in the last subsection we have\n(\n0, if n odd\n\u0001\n\u03bb1 (n) =\n,\nn\nn , if n even\n2\n\nso that\n\nE1 (z) =\n\n\u221e\nX\nz 2k\n= I0 (2z)\nk!k!\n\nk=0\n\nis precisely the modified Bessel function of order zero. Since a loop on Zd is just a\nshuffle of loops on Z, the product formula for exponential generating functions [38]\nyields\nEd (z) = E1 (z)d = I0 (2z)d .\nWhat we have is the exponential generating function for the loop counts \u03bbd (n),\nand what we want is the ordinary generating function of this sequence. The integral\ntransform\nLf (z) =\n\nZ\u221e\n\nf (tz)e\u2212t dt,\n\n0\n\nwhich looks like the Laplace transform of f but with the z-parameter in the wrong\nplace, converts exponential generating functions into ordinary generating functions.\nThis can be seen by differentiating under the integral sign and using the fact that\nthe moments of the exponential distribution are the factorials,\nZ\u221e\n\ntn e\u2212t dt = n!.\n\n0\n\nThis trick is constantly used in quantum field theory in connection with Borel\nsummation of divergent series [10]. In particular, we have\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\nLd (z) =\n\nZ\u221e\n\nEd (tz)e\u2212t dt =\n\nZ\u221e\n\n27\n\nI0 (2tz)d e\u2212t dt.\n\n0\n\n0\n\nThus it remains only to show that the integral\nLd\n\n\u0012\n\n1\n2d\n\n\u0013\n\nZ\u221e \u0012 \u0013d\nt\n= I0\ne\u2212t dt\nd\n0\n\nis divergent for d = 1, 2 and convergent for d > 2. This in turn amounts to\nunderstanding the asymptotics of I0 (t/d) as t \u2192 \u221e along the real line.\nWe already encountered Bessel functions in Lecture One, and we know that\n1\nI0 (t/d) =\n\u03c0\n\nZ\u03c0\n\net(\n\ncos \u03b8\nd )\n\nd\u03b8.\n\n0\n\nThis is an integral of Laplace type,\nZb\n\netf (\u03b8) d\u03b8,\n\na\n\nand Laplace integrals localize as t \u2192 \u221e with asymptotics given by the classical\nsteepest descent formula (maximum at an endpoint case),\nZb\na\n\netf (\u03b8) d\u03b8 \u223c\n\nr\n\n\u03c0\netf (a) .\n2t|f \u2032\u2032 (a)|\n\nFor our integral, this specializes to\nr\n\n1\net/d , t \u2192 \u221e,\n2\u03c0 3/2 t\nfrom which it follows that Ld ((2d)\u22121 ) diverges or converges according to the divergence or convergence of the integral\nI0 (t/d) \u223c\n\nZ\u221e\n\nt\u2212d/2 dt.\n\n1\n\nThis integral diverges for d = 1, 2 and converges for d \u2265 3, which proves P\u00f3lya's\nresult. In fact, the probability that the simple random walk on Z3 returns to its\ninitial position is already less than thirty five percent.\n2.3. Kesten's problem. The category of abelian groups is a full subcategory of\nthe category of groups. In the category of groups, coproduct is free product:\na\ni\u2208I\n\nGi = \u2217i\u2208I Gi .\n\nThus one could equally well ask about the recurrence or transience of the simple\nrandom walk on\n\n\f28\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nFigure 13. Balls of increasing radius in F2 .\n\nFd = |Z \u2217 *{z\n* * \u2217 Z},\nd\n\nthe free group on d generators. Whereas the Cayley graph of the abelian group\nZd is the (2d)-regular hypercubic lattice, the Cayley graph of the free group Fd is\nthe (2d)-regular tree, see Figure 13. What is the free analogue of P\u00f3lya's theorem?\nWe will see that the random walk on Fd can be understood entirely in terms of\nthe random walk on F1 = Z, just like in the abelian category. However, the tools\nwe will use are quite different, and the concept of free random variables plays the\ncentral role.\nThe study of random walks on groups was initiated by Harry Kesten in his 1958\nPh.D. thesis, with published results appearing in [20]. A good source of information\non this topic, with many pointers to the literature, is Laurent Saloff-Coste's survey\narticle [33]. Kesten related the behaviour of the simple random walk on a finitelygenerated group G to other properties of G, such as amenability. A countable\ngroup is said to be amenable if it admits a finitely additive G-invariant probability\nmeasure. The notion of amenability was introduced by John von Neumann in\n1929. Finite groups are amenable since they can be equipped with the uniform\nmeasure P (g) = |G|\u22121 . For infinite groups the situation is not so clear, and many\ndifferent characterizations of amenability have been derived. For example, Alain\nConnes showed that a group is amenable if and only if its von Neumann algebra is\nhyperfinite. Kesten proved that G is non-amenable if and only if the probability\n\u03c1G (n) that the simple random walk on G returns to its starting point at time n\ndecays exponentially in n. We saw above that for G = Z the return probability\nhas square root decay, so Z is amenable. In fact, amenability is preserved by direct\nsum so all abelian groups are amenable. Is the free group Fd amenable? Let \u03bbd (n)\ndenote the number of length n loops on Fd based at id. We will refer to the problem\nof finding an explicit expression for the loop generating function\nLd (z) = 1 +\n\n\u221e\nX\n\nn=1\n\n\u03bbd (n)z n\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n29\n\nas Kesten's problem. Presumably, if we can obtain an explicit expression for this\nfunction then we can read off the asymptotics of \u03c1d (n), which is the coefficient of z n\nin Ld (z/2d), via the usual methods of singularity analysis of generating functions.\nWe begin at the beginning: d = 2. Let A and B denote the generators of F2 ,\nand let A = A[F2 ] be the group algebra consisting of formal C-linear combinations\nof words in these generators and their inverses, A\u22121 and B \u22121 . The identity element\nof A is the empty word, which is identified with id \u2208 F2 . Introduce the expectation\nfunctional\n\u03c4 [X] = coefficient of id in X\nfor each X \u2208 A. Then (A, \u03c4 ) is a non-commutative probability space. A loop\nid \u2192 id in F2 is simply a word in A, A\u22121 , B, B \u22121 which reduces to id. Thus the\nnumber of length n loops in F2 is\n\u03bb2 (n) = mn (X + Y ) = \u03c4 [(X + Y )n ],\nwhere X, Y \u2208 A are the random variables\nX = A + A\u22121 ,\n\nY = B + B \u22121 .\n\nWe see that the loop generating function for F2 is precisely the moment generating\nfunction for the random variable X + Y in the non-commutative probability space\n(A, \u03c4 ),\nL2 (z) = 1 +\n\n\u221e\nX\n\nmn (X + Y )z n .\n\nn=1\n\nWe want to compute the moments of the sum X + Y of two non-commutative\nrandom variables, and what we know are the moments of its summands:\nmn (X) = mn (Y ) =\n\n(\n\n0, if n odd\n\u0001\nn\nn , if n even\n\n.\n\n2\n\nNow we make the key observation: the random variables X, Y are freely independent. Indeed, suppose that f1 , g1 , . . . , fk , gk are polynomials such that\n\u03c4 [f1 (X)] = \u03c4 [g1 (Y )] = * * * = \u03c4 [fk (X)] = \u03c4 [gk (Y )] = 0.\n\nThis means that fi (X) = fi (A + A\u22121 ) is a Laurent polynomial in A with zero\nconstant term, and gj (Y ) = gj (B + B \u22121 ) is a Laurent polynomial in B with zero\nconstant term. Since there are no relations between A and B, an alternating product\nof polynomials of this form cannot produce any occurrences of the empty word, and\nwe have\n\u03c4 [f1 (X)g1 (Y ) . . . fk (X)gk (Y )] = 0.\nThis is precisely Voiculescu's definition of free independence.\nWe conclude that the problem of computing \u03bb2 (n) is a particular case of the\nproblem of computing the moments mn (X + Y ) of the sum of two free random\nvariables given their individual moments, mn (X) and mn (Y ). This motivates us\nto solve a fundamental problem in free probability theory:\n\n\f30\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nGiven a pair of free random variables X and Y , compute the moments of X + Y\nin terms of the moments of X and the moments of Y .\nWe can, in principle, solve this problem using the fact that free cumulants linearize the addition of free random variables, \u03ban (X + Y ) = \u03ban (X) + \u03ban (Y ). This\nsolution is implemented as the following recursive algorithm.\nInput: \u03ba1 (X), . . . , \u03ban\u22121 (X), \u03ba1 (Y ), . . . , \u03ban\u22121 (Y ).\nStep 1 : Compute mn (X), mn (Y ).\nStep 2 : Compute \u03ban (X), \u03ban (Y ) using\n\u03ban (X) = mn (X) \u2212\n\u03ban (Y ) = mn (Y ) \u2212\n\nX\n\nY\n\n\u03ba|\u03b2| (X)\n\nX\n\nY\n\n\u03ba|\u03b2| (Y ).\n\n\u03c0\u2208NC(n) B\u2208\u03c0\nb(\u03c0)\u22652\n\n\u03c0\u2208NC(n) B\u2208\u03c0\nb(\u03c0)\u22652\n\nStep 3 : Add,\n\u03ban (X + Y ) = \u03ban (X) + \u03ban (Y ).\nStep 4 : Compute mn (X + Y ) using\nmn (X + Y ) = \u03ban (X + Y ) +\n\nX\n\nY\n\n\u03c0\u2208NC(n) B\u2208\u03c0\nb(\u03c0\u22652\n\n\u03ba|B| (X + Y ).\n\nOutput: mn (X + Y ).\nThis recursive algorithm is conceptually simple but virtually useless as is. In\nparticular, it is not clear how to coax it into computing the loop generating function\nL2 (z). We need to develop an additive calculus of free random variables which\nparallels the additive calculus of classically independent random variables.\n2.4. The classical algorithm. If X, Y are classically independent random variables, we can compute the moments of their sum X + Y using the recursive algorithm above, replacing free cumulants with classical cumulants. But this is not\nwhat probabilists do in their daily lives. They have a much better algorithm which\nuses analytic function theory to efficiently handle the recursive nature of the naive\nalgorithm. The classical algorithm associates to X and Y analytic functions MX (z)\nand MY (z) which have the property that MX+Y (z) := MX (z)MY (z) encodes the\nmoments of X + Y as its derivatives at z = 0. We will give a somewhat roundabout\nderivation of this algorithm, which is presented in this way specifically to highlight\nthe analogy with Voiculescu's algorithm presented in the next section.\nThe classical algorithm for summing two random variables is developed in two\nstages. In the first stage, the relation between the moments and classical cumulants\nof a random variable is packaged as an identity in the ring of formal power series\n\u221e\nC[[z]]. Suppose that (mn )\u221e\nn=1 and (cn )n=1 are two numerical sequences related by\nthe chain of identities\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\nX Y\n\nmn =\n\n\u03c0\u2208P(n) B\u2208\u03c0\n\nc|B| ,\n\n31\n\nn \u2265 1.\n\nThe \u03c0 th term of the sum on the right only depends on the \"spectrum\" of \u03c0, i.e.\nthe integer vector \u039b(\u03c0) = (1b1 (\u03c0) , 2b2 (\u03c0) , . . . , nbn (\u03c0) ), where bi (\u03c0) is the number\nof blocks of size i in \u03c0. We may view \u039b(\u03c0) as the Young diagram with bi rows of\nlength i. Consequently, we can perform a change of variables to push the summation\nforward onto a sum over Young diagrams with n boxes provided we can compute\nthe \"Jacobian\" of the map \u039b : P(n) \u2192 Y(n) sending \u03c0 on its spectrum:\nX\n\nmn =\n\nb1 +2b2 +***+nbn =n\n\ncb11 cb22 . . . cbnn |\u039b\u22121 (1b1 , 2b2 , . . . , nbn )|.\n\nThe volume of the fibre of \u039b over any given Young diagram can be explicitly computed,\n|\u039b\u22121 (1b1 , 2b2 , . . . , nbn )| =\n\n(1!)b1 (2!)b2\n\nn!\n,\n. . . (n!)bn b1 !b2 ! . . . bn !\n\nso that we have the chain of identities\nmn\n=\nn!\n\nX\n\nb1 +2b2 +***+nbn =n\n\n(c1 /1!)b1 (c2 /2!)b2 . . . (cn /n!)bn\n,\nb1 !b2 ! . . . bn !\n\nn \u2265 1.\n\nWe can bundle these identities as a single relation between power series. Summing\non z we obtain\n\u0013\n\u221e \u0012\nX\nX\n(c1 /1!)b1 (c2 /2!)b2 . . . (cn /n!)bn n\nzn\nz\n=1+\n1+\nmn\nn!\nb1 !b2 ! . . . bn !\nn=1\nn=1\nb1 +2b2 +***+nbn =n\n\u0012 \u221e\n\u00131\n\u0012 \u221e\n\u00132\n1 X zn\n1 X zn\n=1+\n+\n+ ...\ncn\ncn\n1! n=1 n!\n2! n=1 n!\n\u221e\nX\n\n=e\n\nP\u221e\n\nn=1\n\nn\n\ncn zn!\n\n.\n\nWe conclude that the chain of moment-cumulant formulas is equivalent to the single\nidentity M (z) = eC(z) in C[[z]], where\nM (z) = 1 +\n\n\u221e\nX\n\nn=1\n\nmn\n\nzn\n,\nn!\n\nC(z) =\n\n\u221e\nX\n\nn=1\n\ncn\n\nzn\nn!\n\nThis fact is known in enumerative combinatorics as the exponential formula. In\nother branches of science it goes by other names, such as the the polymer expansion\nformula or the linked cluster theorem. In the physics literature, the exponential\nformula is often invoked using colourful phrases such as \"connected vacuum bubbles\nexponentiate\" [34]. The exponential formula seems to have been first written down\nprecisely by Adolf Hurwitz in 1891 [19].\nThe exponential formula becomes particularly powerful when combined with\ncomplex analysis. Suppose that X, Y are classically independent random variables\nliving in a non-commutative probability space (A, \u03c4 ). Suppose moreover that an\noracle has given us probability measures \u03bcX , \u03bcY on the real line which behave like\ndistributions for X, Y insofar as\n\n\f32\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n\u03c4 [X n ] =\n\nZ\n\ntn \u03bcX (dt),\n\n\u03c4 [Y n ] =\n\nR\n\nZ\n\ntn \u03bcY (dt),\n\nn \u2265 1.\n\nR\n\nLet us ask for even more, and insist that \u03bcX , \u03bcY are compactly supported. Then\nthe functions2\nZ\nZ\ntz\nMX (z) = e \u03bcX (dt), MY (z) = etz \u03bcY (dt)\nR\n\nR\n\nare entire, and their derivatives can be computed by differentiation under the integral sign. Consequently, we have the globally convergent power series expansions\nMX (z) = 1 +\n\n\u221e\nX\n\nmn (X)\n\nn=1\n\nzn\n,\nn!\n\nMY (z) = 1 +\n\n\u221e\nX\n\nmn (Y )\n\nn=1\n\nzn\n.\nn!\n\nSince MX (0) = MY (0) = 1 and the zeros of holomorphic functions are discrete,\nwe can restrict to a complex domain D containing the origin on which MX (z), MY (z)\nare non-vanishing. Let Hol(D) denote the algebra of holomorphic functions on D.\nThe following algorithm produces a function MX+Y (z) \u2208 Hol(D) whose derivatives\nat z = 0 are the moments of X + Y .\nInput: \u03bcX and \u03bcY .\nStep 1 : Compute\nMX (z) =\n\nZ\n\ntz\n\ne \u03bcX (dt), MY (z) =\n\nR\n\nZ\n\netz \u03bcY (dt).\n\nR\n\nStep 2 : Solve\nMX (z) = eCX (z) ,\n\nMY (z) = eCY (z)\n\nin Hol(D) subject to CX (0) = CY (0) = 0.\nStep 3 : Add,\nCX+Y (z) := CX (z) + CY (z).\nStep 4 : Exponentiate,\nMX+Y (z) := eCX+Y (z) .\nOutput: MX+Y (z).\nIn Step One, we try to compute the integral transforms MX (z), MY (z) in terms of\nelementary functions, like ez , log(z), sin(z), cos(z), sinh(z), cosh(z), . . . etc, or other\nclassical functions like Bessel functions, Whittaker functions, or anything else that\ncan be looked up in [1]. This is often feasible if the distributions \u03bcX , \u03bcY have known\ndensities, and we saw some examples in Lecture One.\n2The restriction of M to the real axis, M (\u2212x), is the two-sided Laplace transform, while\nX\nX\nthe restriction of MX to the imaginary axis, MX (\u2212iy), is the Fourier transform.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n33\n\nThe equations in Step Two have unique solutions. The required functions\nCX (z), CY (z) \u2208 Hol(D) are the principal branches of the logarithms of MX (z), MY (z)\non D, and can be represented as contour integrals:\nI z \u2032\n\u2032\nMY (\u03b6)\nMX\n(\u03b6)\nd\u03b6, CY (z) = log MY (z) =\nd\u03b6\nCX (z) = log MX (z) =\n0 MY (\u03b6)\n0 MX (\u03b6)\nfor z \u2208 D. Since log has the usual formal properties associated with the logarithm,\nif Step One outputs a reasonably explicit expression then so will Step Two.\nStep Two is the crux of the algorithm. It is performed precisely to change\ngears from a moment computation to a cumulant computation. Appealing to the\nexponential formula, we conclude that the holomorphic functions CX (z), CY (z)\npassed to Step Three by Step Two have Maclaurin series\nI\n\nCX (z) =\n\nz\n\n\u221e\nX\n\nn=1\n\ncn (X)\n\nzn\n,\nn!\n\nCY (z) =\n\n\u221e\nX\n\ncn (Y )\n\nn=1\n\nzn\n,\nn!\n\nwhere cn (X), cn (Y ) are the cumulants of X and Y . Since cumulants linearize the\naddition of independent random variables, the new function CX+Y (z) := CX (z) +\nCY (z) defined in Step Three encodes the cumulants of X + Y as its derivatives at\nz = 0.\nIn Step Four we define a new function MX+Y (z) \u2208 Hol(D) by MX+Y (z) :=\nCX+Y (z)\ne\n. The exponential formula and the moment-cumulant formula now combine\nin the reverse direction to tell us that the Maclaurin series of MX+Y (z) is\nMX+Y (z) = 1 +\n\n\u221e\nX\n\nn=1\n\nmn (X + Y )\n\nzn\n.\nn!\n\nIn summary, assuming that X, Y are classically independent random variables\nliving in a non-commutative probability space (A, \u03c4 ) with affiliated distributions\n\u03bcX , \u03bcY having nice properties, the classical algorithm takes these distributions\nas input and outputs a function MX+Y (z) analytic at z = 0 whose derivatives\nare the moments of X + Y . It works by combining the exponential formula and\nthe moment-cumulant formula to convert the moment problem into the (linear)\ncumulant problem, adding, and then converting back to moments. An optional\nFifth Step is to extract the distribution \u03bcX+Y from MX+Y (z) using the Fourier\ninversion formula:\n1\n\u03bcX+Y ([a, b]) = lim\nT \u2192\u221e 2\u03c0\n\nZT\n\ne\u2212iat \u2212 e\u2212ibt\nMX+Y (it)dt.\nit\n\n\u2212T\n\n2.5. Voiculescu's algorithm. We wish to develop a free analogue of the classical\nalgorithm. Suppose that X, Y are freely independent random variables living in\na non-commutative probability space (A, \u03c4 ) possessing compactly supported real\ndistributions \u03bcX , \u03bcY . The free algorithm should take these distributions as input,\nbuild a pair of analytic functions which encode the moments of X and Y respectively, and then convolve these somehow to produce a new analytic function which\nencodes the moments of X + Y . A basic hurdle to be overcome is that, even assuming we know how to construct \u03bcX and \u03bcY , we don't know what to do with them.\nWe could repeat Step One of the classical algorithm to obtain analytic functions\n\n\f34\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nMX (z), MY (z) whose derivatives at z = 0 are the moments of X and Y . If we then\nperform Step Two we obtain analytic functions CX (z), CY (z) whose derivatives encode the classical cumulants of X and Y . But classical cumulants do not linearize\naddition of free random variables.\nThe classical algorithm is predicated on the existence of a formal power series\nidentity equivalent to the chain of classical moment-cumulant identities. We need\na free analogue of this, namely a power series identity equivalent to the chain of\nnumerical identities\nX\n\nmn =\n\nY\n\n\u03c0\u2208NC(n) B\u2208\u03c0\n\n\u03ba|B| ,\n\nn \u2265 1.\n\nProceeding as in the classical case, rewrite this in the form\nmn =\n\nX\n\nb1 +2b2 +***+nbn =n\n\n\u03bab11 \u03bab22 . . . \u03babnn |\u039b\u22121 (1b1 , 2b2 , . . . , nbn ) \u2229 NC(n)|,\n\nwhere as above \u039b : P(n) \u2192 Y(n) is the surjection which sends a partition \u03c0 with\nbi blocks of size i to the Young diagram with bi rows of length i. Now we have to\ncompute the volume of the fibres of \u039b intersected with the non-crossing partition\nlattice. The solution to this enumeration problem is again known in explicit form,\n|\u039b\u22121 (1m1 , 2m2 , . . . , nmn ) \u2229 NC(n)| =\n\nn!\n.\n(n + 1 \u2212 (b1 + b2 + * * * + bn ))!b1 !b2 ! . . . bn !\n\nThis formula allows us to obtain the desired power series identity, though the manipulations required are quite involved and require either the use of Lagrange inversion or an understanding of the poset structure of NC(n). In any event, what\nultimately comes out of the computation is the fact that two numerical sequences\nsatisfy the chain of free moment-cumulant identities if and only if the ordinary (not\nexponential) generating functions\nL(z) = 1 +\n\n\u221e\nX\n\nmn z n ,\n\nK(z) = 1 +\n\nn=1\n\nsolve the equation\n\n\u221e\nX\n\n\u03ban z n\n\nn=1\n\nL(z) = K(zL(z))\nin the formal power series ring C[[z]]. This is the free analogue of the exponential\nformula.\nAs in the classical case, we wish to turn this formal power series encoding into\nan analytic encoding. Suppose that X, Y admit distributions \u03bcX , \u03bcY supported\nin the real interval [\u2212r, r]. We then have |mn (X)|, |mn (Y )| \u2264 rn , so the moment\ngenerating functions\nLX (z) = 1 +\n\n\u221e\nX\n\nmn (X)z n ,\n\nLY (z) = 1 +\n\nn=1\n\n\u221e\nX\n\nmn (Y )z n ,\n\nn=1\n1\nr ).\n\nare absolutely convergent in the open disc D(0,\nOne can use the relation between\nmoments and free cumulants to show that the free cumulant generating functions\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\nKX (z) = 1 +\n\n\u221e\nX\n\n\u03ban (X)z n ,\n\nKY (z) = 1 +\n\nn=1\n\n\u221e\nX\n\n35\n\n\u03ban (Y )z n\n\nn=1\n\nare also absolutely convergent on a (possibly smaller) neighbourhood of z = 0.\nHowever, it turns out that the correct environment for the free algorithm is a\nneighbourhood of infinity rather than a neighbourhood of zero. This is because\nwhat we really want is an integral transform which realizes ordinary generating\nfunctions in the same way as the Fourier (or Laplace) transform realizes exponential\ngenerating functions. Access to such a transform will allow us to obtain closed forms\nfor generating functions by evaluating integrals, just like in classical probability.\nSuch an object is well-known in analysis, where it goes by the name of the Cauchy\n(or Stieltjes) transform. The Cauchy transform of a random variable X with real\ndistribution \u03bcX is\nZ\n1\n\u03bcX (dt).\nGX (z) =\nz\u2212t\nR\n\nThe Cauchy transform is well-defined on the complement of the support of \u03bcX ,\nand differentiating under the integral sign shows that GX (z) is holomorphic on its\ndomain of definition. In particular, if \u03bcX is supported in [\u2212r, r] then GX (z) admits\nthe convergent Laurent expansion\nGX (z) =\n\n\u221e\n1X\nz n=0\n\nR\n\n\u221e\ntn \u03bcX (dt) X mn (X)\n=\nzn\nz n+1\nn=0\n\non |z| > r. This is an ordinary generating function for the moments of X with z \u22121\nplaying the role of the formal variable.\nTo create an interface between the free moment-cumulant formula and the Cauchy\ntransform, we must re-write the formal power series identity L(z) = K(zL(z)) as\nan identity in C((z)) = Quot C[[z]], the field of formal Laurent series. Introduce\nthe formal Laurent series\nG(z) =\n1\nz\n\nThe automorphism z 7\u2192\nthe identity\n\n\u221e\nX\n1 1\nmn\n.\nL( ) =\nn+1\nz z\nz\nn=0\n\ntransforms the non-crossing exponential formula into\nK(G(z))\n= z.\nG(z)\n\nSetting\nV (z) =\nthis becomes the identity\n\n\u221e\n\nK(z)\n1 X\n\u03ban+1 z n ,\n= +\nz\nz n=0\nV (G(z)) = z\n\nin C((z)).\n\n\f36\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nWe have now associated two analytic functions to X. The first is the Cauchy\ntransform GX (z), which is defined as an integral transform and admits a convergent\nLaurent expansion in a neighbourhood of infinity in the z-plane. The second is the\nVoiculescu transform VX (w), which is defined by the convergent Laurent series\n\u221e\n\n1 X\nVX (w) = +\n\u03ban+1 wn\nw n=0\n\nin a neighbourhood of zero in the w-plane. The Voiculescu transform is a meromorphic function with a simple pole of residue one at w = 0. The Voiculescu transform\nless its principal part, RX (w) = VX (w) \u2212 w1 , is an analytic function known as the\nR-transform of X. From the formal identities V (G(z)) = z, G(V (w)) = w and\nthe asymptotics GX (z) \u223c z1 as |z| \u2192 \u221e and VX (w) \u223c w1 as |w| \u2192 0, we expect\nto find a neighbourhood D\u221e of infinity in the z-plane and a neighbourhood D0 of\nzero in the w-plane such that GX : D\u221e \u2192 D0 and VX : D0 \u2192 D\u221e are mutually\ninverse holomorphic bijections. The existence of the required domains hinges on\nidentifying regions where the Cauchy and Voiculescu transforms are injective, and\nthis can be established through a complex-analytic argument, see [24, Chapter 4].\nWith these pieces in place, we can state Voiculescu's algorithm for the addition\nof free random variables.\nInput: \u03bcX and \u03bcY .\nStep 1 : Compute\nGX (z) =\n\nZ\nR\n\n1\n\u03bcX (dt), GY (z) =\nz\u2212t\n\nZ\n\n1\n\u03bcY (dt)\nz\u2212t\n\nR\n\nStep 2 : Solve the first Voiculescu functional equations,\n(GX \u25e6 VX )(w) = w, (GY \u25e6 VY )(w) = w\nsubject to VX (w) \u223c w1 near w = 0.\n\nStep 3 : Remove principal part,\nRX (w) = VX (w) \u2212\n\n1\n,\nw\n\nRY (w) = VY (w) \u2212\n\n1\n,\nw\n\nadd,\nRX+Y (w) := RX (w) + RY (w),\nrestore principal part,\nVX+Y (w) := RX+Y (w) +\n\n1\n.\nw\n\nStep 4 : Solve the second Voiculescu functional equation,\n(VX+Y \u25e6 GX+Y )(z) = z,\nsubject to GX+Y (z) \u223c z1 near z = \u221e.\nOutput: GX+Y (z).\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n37\n\nVoiculescu's algorithm is directly analogous to the classical algorithm presented\nin the previous section. The analogy can be succinctly summarized as follows:\nThe R-transform is the free analogue of the logarithm of the Fourier transform.\nIn Step One, we try to compute the integral transforms GX (z), GY (z) in terms\nof elementary functions.\nStep Two changes gears from a moment computation to a cumulant computation. Since free cumulants linearize the addition of free random variables, the new\nfunction VX+Y (w) := RX (w) + RY (w) + w1 defined in Step Three encodes the free\ncumulants of \u03ban (X + Y ) as its Laurent coefficients of non-negative degree.\nIn Step Four we define a new function GX+Y (z) by solving the second Voiculescu\nfunctional equation. The free exponential formula and the free moment-cumulant\nformula combine in the reverse direction to tell us that the Laurent series of\nGX+Y (z) is\nGX+Y (z) =\n\n\u221e\nX\nmn (X + Y )\n.\nz n+1\nn=0\n\nAn optional Fifth Step is to extract the distribution \u03bcX+Y from GX+Y (z) using\nthe Stieltjes inversion formula:\n\u03bcX+Y (dt) = \u2212\n\n1\nlim IGX+Y (t + i\u03b5).\n\u03c0 \u03b5\u21920\n\n2.6. Solution of Kesten's problem. Our motivation for building up the additive\ntheory of free random variables came from Kesten's problem: explicitly determine\nthe loop generating function of the free group F2 , and more generally of the free\ngroup Fd , d \u2265 2. This amounts to computing the moment generating function\nLd (z) = 1 +\n\n\u221e\nX\n\nmn (Sd )z d\n\nn=1\n\nof the sum\n\nSd = X 1 + * * * + X d\n\nof fid (free identically distributed) random variables with moments\n(\n0, n odd\n\u0001\n\u03c4 [Xin ] =\nn\nn/2 , n even.\n\nVoiculescu's algorithm gives us the means to obtain this generating function provided we can feed it the required input, namely a compactly supported probability\nmeasure on R with moment sequence\n\u0012 \u0013 \u0012 \u0013 \u0012 \u0013\n2\n4\n6\n0,\n, 0,\n, 0,\n, 0, . . . .\n1\n2\n3\nAs we saw above, the exponential generating function of this moment sequence,\nM (z) =\n\n\u221e\nX\nz 2k\n= I0 (2z),\nk!k!\n\nk=0\n\n\f38\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n0.80\n0.70\n0.60\n0.50\n0.40\n0.30\n0.20\n0.10\n-2.0\n\n-1.0\n\n0.0\n\n1.0\n\n2.0\n\nFigure 14. The arcsine density\ncoincides with the modified Bessel function of order zero. From the integral representation\n1\nI0 (2z) =\n\u03c0\n\nZ\u03c0\n\ne2(cos \u03b8)z d\u03b8\n\n0\n\nwe conclude that a random variable X with odd moments zero and even moments\nthe central binomial coefficients is given by X = 2 cos(Y ), where Y has uniform\ndistribution over [0, \u03c0]. Making the same change of variables that we did in Lecture\nOne, we obtain\n1\nMX (z) =\n\u03c0\n\nZ2\n\n\u22122\n\n1\ndt,\netz \u221a\n4 \u2212 t2\n\nso that \u03bcX is supported on [\u22122, 2] with density\n1\n\u221a\ndt.\n\u03c0 4 \u2212 t2\nThis measure is known as the arcsine distribution because its cumulative distribution function is\n\u03bcX (dt) =\n\nZ\n\n1 arcsine( x2 )\n+\n.\n2\n\u03c0\n\u22122\nSo to obtain the loop generating function L2 (z) for the simple random walk on F2 ,\nwe should run Voiculescu's algorithm with input \u03bcX = \u03bcY = arcsine.\nLet us warm up with an easier computation. Suppose that X, Y are not fid\narcsine random variables, but rather fid \u00b11-Bernoulli random variables:\nx\n\n\u03bcX (dt) =\n\n\u03bcX = \u03bcY =\n\n1\n1\n\u03b4\u22121 + \u03b4+1 .\n2\n2\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n39\n\nWe will use Voiculescu's algorithm to obtain the distribution of X + Y . If X, Y\nwere classically iid Bernoullis, we would of course obtain the binomial distribution\n1\n1\n1\n\u03b4\u22122 + \u03b40 + \u03b4+2\n4\n2\n4\ngiving the distribution of the simple random walk on Z at time two. The result is\nquite different in the free case.\nStep One. Obtain the Cauchy transform,\n\u03bcX+Y =\n\nGX (z) = GY (z) =\n\n\u0012\n\u0013\n\u221e\nX\n1\nz\n1\n1\n1\n= 2\n.\n+\n=\n2n+1\n2 z+1 z\u22121\nz \u2212 1 n=0 z\n\nStep Two. Solve the first Voiculescu functional equation. From Step One, this\nis\nwV 2 (w) \u2212 V (w) \u2212 w = 0,\n\nwhich has roots\n\n\u221a\n\u221a\n1 + 4w2\n1\n1 \u2212 1 + 4w2\n3\n5\n7\n= +w\u2212w +2w \u22125w +. . . ,\n= \u2212w+w3 \u22122w5 +. . . .\n2w\nw\n2w\nWe identify the first of these as the Voiculescu transform VX (w) = VY (w).\nStep Three. Compute the R-transform,\n1+\n\nRX (w) = RY (w) =\n\n1+\n\n\u221a\n\u221a\n1 + 4w2\n1 + 4w2 \u2212 1\n1\n\u2212 =\n,\n2w\nw\n2w\n\nand sum to obtain\nRX+Y (w) = RX (w) + RY (w) =\n\n\u221a\n1 + 4w2 \u2212 1\n.\nw\n\nNow restore the principal part,\n\u221a\n1 + 4w2\n1\n=\n.\nw\nw\nStep Four. Solve the second Voiculescu functional equation. From Step Three,\nthis is the equation\nVX+Y (w) = RX+Y (w) +\n\nwhich has roots\n\np\n1 + 4G(z)2\n= z,\nG(z)\n\n\u00b11\n\u00b11 \u00b12 \u00b16 \u00b120 \u00b170 \u00b1252\n=\n+ 3 + 5 + 7 + 9 + 11 + . . . .\n2\nz\nz\nz\nz\nz\nz\nz \u22124\nThe positive root is identified as GX+Y (z).\nFinally, we perform the optional fifth step to recover the distribution \u03bcX+Y whose\nCauchy transform is GX+Y (z). This can be done in two ways. First, we could notice\nthat\n\u0001 the non-zero Laurent coefficients of GX+Y are the central binomial coefficients\n2k\nk , and we just determined that these are the moments of the arcsine distribution.\nAlternatively we could use Stieltjes inversion:\n\u221a\n\n\f40\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n\u03bcX+Y (dt) = \u2212\n\n1\n1\n1\n1\n1\n\u03b4|t|\u22642 .\nlim p\n= \u221a\n= \u2212 I\u221a\n2\n2\n\u03c0 \u03b5\u21920 (t + i\u03b5) \u2212 4\n\u03c0\nt \u22124\n\u03c0 4 \u2212 t2\n\nWe conclude that the sum of two fid Bernoulli random variables has arcsine\ndistribution. Note the surprising feature that the outcome of a free coin toss has\ncontinuous distribution over [\u22122, 2]. More generally, we can say that the sum\nSd = X1 + * * * + X2d\n\nof 2d fid \u00b11-Bernoulli random variables, i.e. the sum of 2d free coin tosses, encodes\nall information about the simple random walk on Fd in its moments.\nLet us move on to the solution of Kesten's problem for F2 . Here X, Y are fid\narcsine random variables.\nStep One. The Cauchy transform GX (z) = GY (z) is the output of our last\napplication of the algorithm, namely\n1\nGX (z) = GY (z) = \u221a\n.\nz2 \u2212 4\nStep Two. Solve the first Voiculescu functional equation to obtain\n\u221a\n\n1 + 4w2\n1\n= + 2w \u2212 2w3 + . . . .\nw\nw\nStep Three. Switch to R-transforms, add, switch back to get the Voiculescu\ntransform of X + Y ,\nVX (w) = VY (w) =\n\n\u221a\n1\n2 1 + 4w2 \u2212 1\n= + 4w \u2212 4w3 + . . . .\nz\nw\nStep Four. Solve the second Voiculescu functional equation to obtain\nVX+Y (w) =\n\n\u221a\n28 232 2092\n\u2212z + 2 z 2 \u2212 12\n1\n4\n= + 3 + 5 + 7 + 9 + ....\nz 2 \u2212 16\nz z\nz\nz\nz\nWe can now calculate the loop generating function for F2 ,\nGX+Y (z) =\n\n\u221a\n1\n1\n\u22121 + 2 1 \u2212 12z 2\nL2 (z) = GX+Y ( ) =\n= 1 + 4z 2 + 28z 4 + 232z 6 + 2092z 8 + . . . .\nz\nz\n1 \u2212 16z 2\nMore generally, we can run through the above steps for general d to obtain the loop\ngenerating function\np\n\u2212(d \u2212 1) + d 1 \u2212 4(2d \u2212 1)z 2\nLd (z) =\n1 \u2212 16z 2\nfor the free group Fd , d \u2265 2, which in turn leads to the probability generating\nfunction\nLd (\n\np\n\u2212(d \u2212 1) + d 1 \u2212 (2d \u2212 1)( zd )2\nz\n.\n)=\n2d\n1 \u2212 4( dz )2\n\nApplying standard methods from analytic combinatorics [12], this expression leads\nto the asymptotics\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n\u03c1d (n) \u223c constd * n\n\n\u2212 32\n\n41\n\n\u0012 \u221a \u0013n\n2 d\nd+1\n\nfor the return probability of the simple random walk on Fd , d \u2265 2. From this we\ncan conclude that the simple random walk on Fd is transient for all d \u2265 2, and\nindeed that Fd is non-amenable for all d \u2265 2.\n2.7. Spectral measures and free convolution. Voiculescu's algorithm outputs\na function GX+Y (z) which encodes the moments of the sum of two freely independent random variables X and Y . As input, it requires a pair of compactly supported\nreal measures \u03bcX , \u03bcY which act as distributions for X and Y in the sense that\n\u03c4 [X n ] =\n\nZ\n\ntn \u03bcX (dt),\n\n\u03c4 [Y n ] =\n\nR\n\nZ\n\ntn \u03bcY (dt).\n\nR\n\nIn our applications of Voiculescu's algorithm we were able to find such measures\nby inspection. Nevertheless, it is of theoretical and psychological importance to\ndetermine sufficient conditions guaranteeing the existence of measures with the\nrequired properties.\nIf X : \u03a9 \u2192 C is a random variable defined on a Kolmogorov triple (\u03a9, F , P ), its\ndistribution \u03bcX is the pushforward of P by X,\n\u03bcX (B) = (X\u2217 P )(B) = P (X \u22121 (B))\nfor any Borel (or Lebsegue) set B \u2286 C. One has the general change of variables\nformula\nE[f (X)] =\n\nZ\n\nf (z)\u03bcX (dz)\n\nC\n\nfor any reasonable f : C \u2192 C. If X is essentially bounded and real-valued, \u03bcX\nis compactly supported in R. As a random variable X living in an abstract noncommutative probability space (A, \u03c4 ) is not a function, one must obtain \u03bcX by\nsome other means.\nThe existence of distributions is too much to expect within the framework of a\nnon-commtative probability space, which is a purely algebraic object. We need to\ninject some analytic structure into (A, \u03c4 ). This is achieved by upgrading A to a\n\u2217-algebra, i.e. a complex algebra equipped with a map \u2217 : A \u2192 A satisfying\n(X \u2217 )\u2217 = X,\n\n(\u03b1X + \u03b2Y )\u2217 = \u03b1X \u2217 + \u03b2Y \u2217 ,\n\n(XY )\u2217 = Y \u2217 X \u2217 .\n\nThis map, which is an abstraction of complex conjugation, is required to be compatible with the expectation \u03c4 in the sense that\n\u03c4 [X \u2217 ] = \u03c4 [X].\nA non-commutative probability space equipped with this extra structure is called\na non-commutative \u2217-probability space.\nIn the framework of a \u2217-probability space we can single out a class of random\nvariables analogous to real random variables in classical probability. These are the\n\n\f42\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nfixed points of \u2217, X \u2217 = X. A random variable with this property is called selfadjoint. Self-adjoint random variables have real expected values, \u03c4 [X] = \u03c4 [X \u2217 ] =\n\u03c4 [X], and more generally \u03c4 [f (X)] \u2208 R for any polynomial f with real coefficients.\nThe identification of bounded random variables requires one more upgrade.\nGiven a \u2217-probability space (A, \u03c4 ), we can introduce a Hermitian form B : A\u00d7 A \u2192\nC defined by\nB(X, Y ) = \u03c4 [XY \u2217 ].\nIf we require that \u03c4 has the positivity property \u03c4 [XX \u2217 ] \u2265 0 for all X \u2208 A, then we\nobtain a semi-norm\nkXk = B(X, X)1/2\non A, and we can access the Cauchy-Schwarz inequality\n|B(X, Y )| \u2264 kXkkY k.\nOnce we have Cauchy-Schwarz, we can prove the monotonicity inequalities\n|\u03c4 [X]| \u2264 |\u03c4 [X 2 ]|1/2 \u2264 |\u03c4 [X 4 ]|1/4\n\n|\u03c4 [X 3 ]| \u2264 |\u03c4 [X 4 ]|1/4 \u2264 |\u03c4 [X 6 ]|1/6\n\n|\u03c4 [X 5 ]| \u2264 |\u03c4 [X 6 ]|1/6 \u2264 |\u03c4 [X 8 ]|1/8\n..\n.\n\nfrom which the chain of inequalities\n|\u03c4 [X]| \u2264 |\u03c4 [X 2 ]|1/2 \u2264 |\u03c4 [X 4 ]|1/4 \u2264 |\u03c4 [X 6 ]|1/6 \u2264 |\u03c4 [X 8 ]|1/8 \u2264 . . .\ncan be extracted. From this we conclude that the limit\n\u03c1(X) := lim |\u03c4 [X 2k ]|1/(2k)\nk\u2192\u221e\n\nexists in R\u22650 \u222a {\u221e}. This limit is called the spectral radius of X. A random\nvariable X \u2208 A is said to be bounded if its spectral radius is finite, \u03c1(X) < \u221e.\nIn the framework of a non-commutative \u2217-probability space (A, \u03c4 ) with nonnegative expectation, bounded self-adjoint random variables play the role of essentially bounded real-valued random variables in classical probability theory. With\nsome work, one may deduce from the Riesz representation theorem that to each\nbounded self-adjoint X corresponds a unique Borel measure \u03bcX supported in [\u2212\u03c1(X), \u03c1(X)]\nsuch that\nZ\n\u03c4 [f (X)] = f (t)\u03bcX (dt)\nR\n\nfor all polynomial functions f : C \u2192 C. The details of this argument, in which a\nreverse-engineered Cauchy transform plays the key role, are given in Tao's notes\n[41]. The measure \u03bcX is often called the spectral measure of X, but we will refer to it as the distribution of X. There is also a converse to this result: given\nany compactly supported measure \u03bc on R, there exists a bounded self-adjoint random variable X living in some non-commutative \u2217-probability space (A, \u03c4 ) whose\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n43\n\ndistribution is \u03bc. Consequently, given two compactly supported real probability\nmeasures \u03bc, \u03bd we may define a new measure \u03bc \u229e \u03bd as \"the distribution of the random variable X + Y , where X and Y are freely independent bounded self-adjoint\nrandom variables with distributions \u03bc and \u03bd, respectively.\" Since the sum of two\nbounded self-adjoint random variables is again bounded self-adjoint, \u03bc \u229e \u03bd is another compactly supported real probability measure. Moreover, \u03bc \u229e \u03bd does not\ndepend on the particular random variables chosen to realize \u03bc and \u03bd. Thus we get\na bona fide binary operation \u229e on the set of compactly supported real measures,\nwhich is known as the additive free convolution. For example, we computed above\nthat\nBernoulli \u229e Bernoulli = Arcsine.\nThe additive free convolution of measures is induced by the addition of free\nrandom variables. As such, it is the free analogue of the classical convolution of\nmeasures induced by the addition of classically independent random variables. Like\nclassical convolution, free convolution can be defined for unbounded measures, but\nthis requires more work [2].\n2.8. Free Poisson limit theorem. Select positive real numbers \u03bb and \u03b1. Consider the measure\n\u03bb\n\u03bb\n)\u03b40 + \u03b4\u03b1\nN\nN\n\u03bb\n\u03bb\nwhich consists of an atom of mass 1 \u2212 N\nplaced at zero and an atom of mass N\nplaced at \u03b1. For N sufficiently large, \u03bcN is a probability measure. Its moment\nsequence is\n\u03bcN = (1 \u2212\n\nmn (\u03bcN ) =\nThe N -fold classical convolution of \u03bcN\n\n\u03bb n\n\u03b1 , n \u2265 1.\nN\nwith itself,\n\n\u03bc\u2217N\nN = \u03bcN \u2217 * * * \u2217 \u03bcN ,\n|\n{z\n}\nN\n\nconverges weakly to the Poisson measure of rate \u03bb and jump size \u03b1 as N \u2192 \u221e. This\nis a classical limit theorem in probability known as the Poisson Limit Theorem, or\nthe Law of Rare Events.\nLet us obtain a free analogue of the Poisson Limit Theorem. This should be a\nlimit law for the iterated free convolution\n\u03bc\u229eN\nN = \u03bcN \u229e * * * \u229e \u03bcN .\n{z\n}\n|\nN\n\nFrom the free moment-cumulant formula, we obtain the estimate\n\u0012\n\u0013\n\u0013\n\u0012\n1\n\u03bb n\n1\n\u03ban (\u03bcN ) = mn (\u03bcN ) + O\n= \u03b1 +O\n.\nN2\nN\nN2\nSince free cumulants linearize free convolution, we have\n\u0012 \u0013\n1\n\u229eN\nn\n.\n\u03ban (\u03bcN ) = N \u03ban (\u03bcN ) = \u03bb\u03b1 + O\nN\n\n\f44\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nThus\nlim \u03ban (\u03bcN ) = \u03bb\u03b1n ,\n\nN \u2192\u221e\n\nand it remains to determine the measure \u03bc with this free cumulant sequence. The\nVoiculescu transform of \u03bc is\nV\u03bc (w) =\n\n\u221e\n\n1\n\u03bb\u03b1\n1 X n+1 n\n+\n\u03bb\u03b1\nw = +\n,\nw n=0\nw 1 \u2212 \u03b1w\n\nso the second Voiculescu functional equation V\u03bc (G\u03bc (z)) = z yields\n1\n\u03bb\u03b1\n+\n= z.\nG\u03bc (z) 1 \u2212 \u03b1G\u03bc (z)\nThis equation has two solutions, and the one which behaves like 1/z for |z| \u2192 \u221e\nis the Cauchy transform of \u03bc. We obtain\np\nz + \u03b1(1 \u2212 \u03bb) \u2212 (z \u2212 \u03b1(1 + \u03bb))2 \u2212 4\u03bb\u03b12\n.\nG\u03bc (z) =\n2\u03b1z\nApplying Stieltjes inversion, we find that the density of \u03bc is given by\n(\n(1 \u2212 \u03bb)\u03b40 + \u03bbm(t)dt, 0 \u2264 \u03bb \u2264 1\n\u03bc(dt) =\nm(t)dt, \u03bb > 1\nwhere\n1 p\n4\u03bb\u03b12 \u2212 (t \u2212 \u03b1(1 + \u03bb))2 .\n2\u03c0\u03b1t\nThis measure is known as the Marchenko-Pastur distribution after the Ukrainian\nmathematical physicists Vladimir Marchenko and Leonid Pastur, who discovered it\nin their study of the asymptotic eigenvalue distribution of a certain class of random\nmatrices.\nm(t) =\n\n2.9. Semicircle flow. Given r > 0, let \u03bcr be the semicircular measure of radius\nr,\n2 p 2\nr \u2212 t2 dt.\n\u03c0r2\nTaking r = 2 yields the standard semicircular distribution. Let \u03bc be an arbitrary\ncompactly supported probability measure on R. The function\n\u03bcr (dt) =\n\nf\u03bc : {positive real numbers} \u2192 {compactly supported real measures}\ndefined by\nf\u03bc (r) = \u03bc \u229e \u03bcr\nis called the semicircle flow. The semicircle flow has very interesting dynamics: in\none of his earliest articles on free random variables [43], Voiculescu showed that\n\u2202G(r, z)\n\u2202G(r, z)\n+ G(r, z)\n= 0,\n\u2202r\n\u2202z\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n45\n\nwhere G(r, z) is the Cauchy transform of f\u03bc (r) = \u03bc \u229e \u03bcr . Thus the free analogue of\nthe heat equation is the complex inviscid Burgers equation. For a detailed analysis\nof the semicircle flow, see [3].\n3. Lecture Three: Modelling the Free World\nFree random variables are of interest for many reasons. First and foremost,\nVoiculescu's free probability theory is an intrinsically appealing subject worthy of\nstudy from a purely esthetic point of view. Adding to this are the many remarkable\nconnections between free probability and other parts of mathematics, including\noperator algebras, representation theory, and random matrix theory. This lecture\nis an exposition of Voiculescu's discovery that random matrices provide asymptotic\nmodels of free random variables. We follow the treatment of Nica and Speicher\n[28].\n3.1. Algebraic model of a free arcsine pair. In Lecture Two we gave a grouptheoretic construction of a pair of free random variables each of which has an arcsine\ndistribution. In this example, the algebra of random variables is the group algebra\nA = A[F2 ] of the free group on two generators A, B, and the expectation \u03c4 is the\ncoefficient-of-id functional. We saw that the random variables\nX = A + A\u22121 ,\n\nY = B + B \u22121\n\nare freely independent, and each has an arcsine distribution:\n(\n0, if n odd\n\u0001\n\u03c4 [X n ] = \u03c4 [Y n ] =\n.\nn\nn , if n even\n2\n\n3.2. Algebraic model of a free semicircular pair. We can give a linear-algebraic\nmodel of a pair of free random variables each of which has a semicircular distribution. The ingredients in this construction are a complex vector space V and an\ninner product B : V \u00d7 V \u2192 C. Our random variables will be endomorphisms of the\ntensor algebra over V,\nF(V) =\n\n\u221e\nM\n\nV\u2297n ,\n\nn=0\n\nwhich physicists and operator algebraists call the full Fock space over V after the\nRussian physicist Vladimir Fock. We view the zeroth tensor power V \u22970 as the\nline in V spanned by a distinguished unit vector v\u2205 called the vacuum vector. Let\nA = End F(V ). This is a unital algebra, with unit the identity operator I : F(V ) \u2192\nF(V ). To make A into a non-commutative probability space we need an expectation.\nWe get an expectation by lifting the inner product on V to the inner product\nF(B) : F(V) \u00d7 F(V) \u2192 C defined by\nF(B)(v1 \u2297 * * * \u2297 vm , w1 \u2297 * * * \u2297 wn ) = \u03b4mn B(v1 , w1 ) . . . B(vn , wn ).\n\nNote that this inner product makes A = End F(B) into a \u2217-algebra: for each X \u2208 A,\nX \u2217 is that linear operator for which the equation\nF(B)(Xs, t) = F(B)(s, X \u2217 t)\n\n\f46\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nholds true for every pair of tensors s, t \u2208 F(V). The expectation on A is the linear\nfunctional \u03c4 : A \u2192 C defined by\n\u03c4 [X] = F(B)(Xv\u2205 , v\u2205 ).\nThis functional is called vacuum expectation. It is unital because\n\u03c4 [I] = F(B)(Iv\u2205 , v\u2205 ) = B(v\u2205 , v\u2205 ) = 1.\nThus (A, \u03c4 ) is a non-commutative \u2217-probability space.\nTo construct a semicircular element in (A, \u03c4 ), notice that to every non-zero vector\nv \u2208 V is naturally associated a pair of linear operators Rv , Lv : F(V ) \u2192 F(V ) whose\naction on decomposable tensors is defined by tensoring,\nRv (v\u2205 ) = v\n\nRv (v1 \u2297 * * * \u2297 vn ) = v \u2297 v1 \u2297 * * * \u2297 vn ,\n\nn \u2265 1,\n\nand insertion-contraction\n\nLv (v\u2205 ) = 0\n\nLv (v1 ) = B(v1 , v)v\u2205\n\nLv (v1 \u2297 v2 \u2297 * * * \u2297 vn ) = B(v1 , v)v2 \u2297 * * * \u2297 vn ,\n\u2297n\n\nn \u2265 2.\n\n\u2297n+1\n\nSince Rv maps V\n\u2192V\nfor each n \u2265 0, it is called the raising (or creation)\noperator associated to v. Since Lv maps V\u2297n \u2192 V\u2297n\u22121 for each n \u2265 1 and kills\nthe vacuum, it is called the lowering (or annihilation) operator associated to v. We\nhave Rv\u2217 = Lv , and also\nLv Rw = B(w, v)I\nfor any vectors v, w \u2208 V.\nLet v \u2208 V be a unit vector, B(v, v) = 1, and consider the self-adjoint random\nvariable\nXv = Lv + Rv .\nWe claim that Xv has a semicircular distribution:\n(\n0, if n odd\nmn (Xv ) = \u03c4 [Xvn ] =\nCat n2 , if n even\n\n.\n\nTo see this, we expand\nX\n\n\u03c4 [Xvn ] = \u03c4 [(Xv + Yv )n ] =\n\nW \u2208{Lv ,Rv\n\n\u03c4 [W ],\n}n\n\nwhere the summation is over all words of length n in the operators Lv , Rv . Only\na very small fraction of these words have non-zero vacuum expectation. Using the\nrelation Lv Rv = I to remove occurrences of the substring Lv Rv , we see that any\nsuch word can be placed in normally ordered form\nW = Rv . . . Rv Lv . . . Lv\n| {z } | {z }\na\n\nb\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n47\n\nwith a + b \u2264 n. Since the lowering operator kills the vacuum vector, the vacuum\nexpectation of W can only be non-zero if b = 0. On the other hand, since V\u2297a is\nF(B)-orthogonal to V\u22970 for a > 0, we must also have a = 0 to obtain a non-zero\ncontribution. Thus the only words which contribute to the above sum are those\nwhose normally ordered form is that of the identity operator. If we replace each\noccurrence of Lv in W with a +1 and each occurrence of Rv in W with a \u22121,\nthe condition that W reduces to I becomes the condition that the corresponding\nbitstring has total sum zero and all partial sums non-negative. There are no such\nbitstrings for n odd, and as we saw in Lecture One when n is even the required\nbitstrings are counted by the Catalan number Catn/2 .\nNow let V1 and V2 be B-orthogonal vector subspaces of V, each of dimension\nat least one, and choose unit vectors x \u2208 V1 , y \u2208 V2 . According to the above\nconstruction, the random variables\nX = Lx + Rx ,\n\nY = Ly + Ry\n\nare semicircular. In fact, they are freely independent. To prove this, we must\ndemonstrate that\n\u03c4 [f1 (X)g1 (Y ) . . . fk (X)gk (Y )] = 0\nwhenever f1 , g1 , . . . , fk , gk are polynomials such that\n\u03c4 [f1 (X)] = \u03c4 [g1 (Y )] = * * * = \u03c4 [fk (X)] = \u03c4 [gk (Y )] = 0.\n\nThis hypothesis means that fi (X) = fi (Lx + Rx ) is a polynomial in Lx , Rx none of\nwhose terms are words which normally order to I, and similarly gj (Y ) = gj (Ly +Ry )\nis a polynomial in Ly , Ry none of whose terms are words which normally order to\nI. Consequently, the alternating product\nf1 (X)g1 (Y ) . . . fk (X)gk (Y )\nis a polynomial in the operators Lx , Rx , Ly , Ry whose terms are words W of the\nform\nWx1 Wy1 . . . Wxk Wyk\nwith Wxi a word in Lx , Rx which does not normally order to I and Wyj a word in\nLy , Ry which does not normally order to I. Thus the only way that W can have\na non-zero vacuum expectation is if we can use the relations Lx Ry = B(y, x)I and\nLy Rx = B(x, y)I to normally order W as\nB(x, y)m B(y, x)n I\nwith m, n non-negative integers at least one of which is positive. But, since x, y are\nB-orthogonal, this is the zero element of A, which has vacuum expectation zero.\n3.3. Algebraic versus asymptotic models. We have constructed algebraic models for a free arcsine pair and a free semicircular pair. Perhaps these should be\ncalled examples rather than models, since the term model connotes some degree of\nimprecision or ambiguity and algebra is a subject which allows neither.\n\n\f48\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nSuppose that X, Y are free random variables living in an abstract non-commutative\nprobability space (A, \u03c4 ). An approximate model for this pair will consist of a sequence (AN , \u03c4N ) of concrete or canonical non-commutative probability spaces together with a sequence of pairs XN , YN of random variables from these spaces such\nthat XN models X and YN models Y , i.e.\n\u03c4 [f (X)] = lim \u03c4 [f (XN )],\nN \u2192\u221e\n\n\u03c4 [g(Y )] = lim \u03c4 [g(YN )]\nN \u2192\u221e\n\nfor any polynomials f, g, and such that free independence holds in the large N\nlimit, i.e.\nlim \u03c4 [f1 (XN )g1 (YN ) . . . fk (XN )gk (YN )] = 0\n\nN \u2192\u221e\n\nwhenever f1 , g1 , . . . , fk , gk are polynomials such that\nlim \u03c4N [f1 (XN )] = lim \u03c4N [g1 (YN )] = * * * = lim \u03c4N [fk (XN )] = lim \u03c4N [gk (YN )] = 0.\n\nN \u2192\u221e\n\nN \u2192\u221e\n\nN \u2192\u221e\n\nN \u2192\u221e\n\nThe question of which non-commutative probability spaces are considered concrete or canonical, and could therefore serve as potential models, is subjective and\ndetermined by individual experience. Three examples of concrete non-commutative\nprobability spaces are:\nGroup probability spaces: (A, \u03c4 ) consists of the group algebra A = A[G]\nof a group G, and \u03c4 is the coefficient-of-identity expectation. This noncommutative probability space is commutative if and only if G is abelian.\nClassical probability spaces: (A, \u03c4 ) consists\nT\u221e of the algebra of complex random variables A = L\u221e\u2212 (\u03a9, F , P ) = p=1 Lp (\u03a9, F , P ) defined on a Kolmogorov triple which have finite absolute moments of all orders, and \u03c4 is the\nclassical expectation \u03c4 [X] = E[X]. Classical probability spaces are always\ncommutative.\nMatrix probability spaces: (A, \u03c4 ) consists of the algebra A = MN (C) of\nN \u00d7 N complex matrices X = [X(ij)], and expectation is the normalized\nN)\ntrace, \u03c4 [X] = trN [X] = X(11)+***+X(N\n. This non-commutative probabilN\nity space is commutative if and only if N = 1.\nThe first class of model non-commutative probability spaces, group probability spaces, is algebraic and we are trying to move away from algebraic examples.\nThe second model class, classical probability spaces, has genuine randomness but\nis commutative. The third model class, matrix probability spaces, has a parameter\nN that can be pushed to infinity but has no randomness. By combining classical probability spaces and matrix probability spaces we arrive at a class of model\nnon-commutative probability spaces which incorporate both randomness and a parameter which can be made large. Thus we are led to consider random matrices.\nThe space of N \u00d7N complex random matrices is the non-commutative probability\nspace (AN , \u03c4N ) = (L\u221e\u2212 (\u03a9, F , P ) \u2297 MN (C), E \u2297 trN ). A random variable XN in\nthis space may be viewed as an N \u00d7 N matrix whose entries XN (ij) belong to\nthe algebra L\u221e\u2212 (\u03a9, F , P ). The expectation \u03c4N [XN ] is the expected value of the\nnormalized trace:\n\u0015\n\u0014\nXN (11) + * * * + XN (N N )\n.\n\u03c4N [XN ] = (E \u2297 trN )[XN ] = E\nN\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n49\n\nWe have already seen indications of a connection between free probability and\nrandom matrices. The fact that Wigner's semicircle law assumes the role of the\nGaussian distribution in free probability signals a connection between these subjects. Another example is the occurrence of the Marchenko-Pastur distribution in\nthe free version of the Poisson limit theorem - this distribution is well-known in\nrandom matrix theory in connection with the asymptotic eigenvalue distribution of\nWishart matrices. In Lecture One, we were led to free independence when we tried\nto solve a counting problem associated to graphs drawn in the plane. The use of\nrandom matrices to enumerate planar graphs has been a subject of much interest in\nmathematical physics since the seminal work of Edouard Br\u00e9zin, Claude Itzykson,\nGiorgio Parisi and Jean-Bernard Zuber [5], which built on insights of Gerardus 't\nHooft. Then, when we examined the dynamics of the semicircle flow, we found\nthat the free analogue of the heat equation is the complex Burgers equation. This\npartial differential equation actually appeared in Voiculescu's work [43] before it\nemerged in random matrix theory [22] and the discrete analogue of random matrix\ntheory, the dimer model [21].\nIn the remainder of these notes, we will model a pair of free random variables X, Y living in an abstract non-commutative probability space using sequences\nXN , YN of random matrices living in random matrix space. This is first carried out\nin the important special case where X, Y are semicircular random variables, then\nadapted to allow Y to have arbitrary distribution while X remains semicircular,\nand finally relaxed to allow X, Y to have arbitrary specified distributions. The random matrix models of free random variables which we describe below were used by\nVoiculescu in order to resolve several previously intractable problems in the theory\nof von Neumann algebras, see [24, 44] for more information. Random matrix models\nwhich approximate free random variables in a stronger sense than that described\nhere were subsequently used by Uffe Haagerup and Steen Thorbj\u00f8rnsen [15] to resolve another operator algebras conjecture, this time concerning the Ext-invariant\nof the reduced C \u2217 -algebra of F2 . An important feature of the connection between\nfree probability and random matrices is that it can sometimes be inverted to obtain\ninformation about random matrices using the free calculus. For each of the three\nmatrix models constructed we give an example of this type.\n3.4. Random matrix model of a free semicircular pair. In this subsection\nwe construct a random matrix model for a free semicircular pair X, Y .\nIn Lecture One, we briefly discussed Wigner matrices. A real Wigner matrix is a\nsymmetric matrix whose entries are centred real random variables which are independent up to the symmetry constraint. A complex Wigner matrix is a Hermitian\nmatrix whose entries are centred complex random variables which are independent\nup to the complex symmetry constraint. Our matrix model for a free semicircular\npair will be built out of complex Wigner matrices of a very special type: they will\nbe GUE random matrices.\nTo construct a GUE random matrix XN , we start with a Ginibre matrix ZN . Let\n(\u03a9, F , P ) be a Kolmogorov triple. The N 2 matrix elements ZN (ij) \u2208 L\u221e\u2212 (\u03a9, F , P )\nof a Ginibre matrix are iid complex Gaussian random variables of mean zero and\nvariance 1/N . Thus ZN is a random variable in the non-commutative probability\nspace (AN , \u03c4N ) = (L\u221e\u2212 (\u03a9, F , P ) \u2297 MN (C), E \u2297 trN ). The symmetrized random\n\u2217\n) is again a member of random matrix space. The joint\nmatrix XN = 12 (ZN + ZN\ndistribution of the eigenvalues of XN can be explicitly computed, and is given by\n\n\f50\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nP (\u03bbN (1) \u2208 I1 , . . . , \u03bbN (N ) \u2208 IN ) \u221d\n\nZ\n\nI1\n\n...\n\nZ\n\ne\u2212N\n\n2\n\nH(\u03bb1 ,...,\u03bbN )\n\nd\u03bb1 . . . d\u03bbN\n\nIN\n\nfor any intervals I1 , . . . , IN \u2286 R, where H is the log-gas Hamiltonian [13]\nH(\u03bb1 , . . . , \u03bbN ) =\n\nN\n1\n1 X \u03bb2i\n\u2212 2\nN i=1 2\nN\n\nX\n\n1\u2264i6=j\u2264N\n\nlog |\u03bbi \u2212 \u03bbj |.\n\nThe random point process on the real line driven by this Hamiltonian is known as\nthe Gaussian Unitary Ensemble, and XN is termed a GUE random matrix. GUE\nrandom matrices sit at the nexus of the two principal strains of complex random\nmatrix theory: they are simultaneously Hermitian Wigner matrices and unitarily\ninvariant matrices. The latter condition means that the distribution of a GUE matrix in the space of N \u00d7 N Hermitian matrices is invariant under conjugation by\nunitary matrices. The spectral statistics of a GUE random matrix can be computed\nin gory detail from knowledge of the joint distribution of eigenvalues, and virtually\nany question can be answered. The universality programme in random matrix theory seeks to show that, in the limit N \u2192 \u221e and under mild hypotheses, Hermitian\nWigner matrices as well as unitarily invariant Hermitian matrices exhibit the same\nspectral statistics as GUE matrices.\nGiven the central role of the GUE in random matrix theory, it is fitting that our\nmatrix model for a free semicircular pair is built from a pair of independent GUE\nmatrices. The first step in proving this is to show that a single GUE matrix XN\nin random matrix space (AN , \u03c4N ) is an asymptotic model for a single semicircular\nrandom variable X living in an abstract non-commutative probability space (A, \u03c4 ).\nIn other words, we need to prove that\nlim\n\nN \u2192\u221e\n\nn\n\u03c4N [XN\n]\n\n= lim (E \u2297\nN \u2192\u221e\n\nn\ntrN )[XN\n]\n\n(\n0, if n odd\n=\nCat n2 , if n even\n\n.\n\nIn order to establish this, we will not need access to the eigenvalues of XN . Rather,\nwe work with the correlation functions of its entries.\nLet XN = [XN (ij)] be a GUE random matrix. Mixed moments of the random\nvariables XN (ij), i.e. expectations of the form\n\u0014Y\n\u0015\nn\nE\nXN (i(k)j(k))\nk=1\n\nwhere i, j are functions [n] \u2192 [N ], are called correlation functions. All correlations\nmay be computed in terms of pair correlations (i.e. covariances)\n\u03b4ik \u03b4jl\nN\nusing a convenient combinatorial formula known as Wick's formula. This formula,\nnamed for the Italian physicist Gian-Carlo Wick, is yet another manifestation of\nthe moment-cumulant/exponential formulas. It asserts that\nE[XN (ij)XN (kl)] = E[XN (ij)XN (lk)] =\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n\u0014Y\n\u0015\nn\nE\nXN (i(k)j(k))) =\nk=1\n\nX\n\nY\n\n51\n\nE[XN (i(r)j(r))XN (i(s)j(s))]\n\n\u03c0\u2208P2 (n) {r,s}\u2208\u03c0\n\nfor any integer n \u2265 1 and functions i, j : [n] \u2192 [N ]. The sum on the right hand\nside is taken over all pair partitions of [n], and the product is over the blocks of \u03c0.\nFor example,\nE[XN (i(1)j(1))XN (i(2)j(2))XN (i(3)j(3))] = 0\nsince there are no pairings on three points, whereas\nE[XN (i(1)j(1))XN (i(2)j(2))XN (i(3)j(3))XN (i(4)j(4))]\n=E[XN (i(1)j(1))XN (i(2)j(2))]E[XN (i(3)j(3))XN (i(4)j(4))]\n+E[XN (i(1)j(1))XN (i(3)j(3))]E[XN (i(2)j(2))XN (i(4)j(4))]\n+E[XN (i(1)j(1))XN (i(4)j(4))]E[XN (i(2)j(2))XN (i(3)j(3))],\ncorresponding to the three pair partitions {1, 2}\u2294{3, 4}, {1, 3}\u2294{2, 4}, {1, 4}\u2294{2, 3}\nof [4]. The Wick formula is a special feature of Gaussian random variables which,\nultimately, is a consequence of the moment formula\nE[X n ] =\n\nX\n\n1\n\n\u03c0\u2208P2 (n)\n\nfor a single standard real Gaussian X which we proved in Lecture One. A proof of\nthe Wick formula may be found in Alexandre Zvonkin's expository article [46].\nWe now compute the moments of the trace of a GUE matrix XN using the Wick\nformula, and then take the N \u2192 \u221e limit. We have\nn\n\u03c4N [XN\n]=\n\n1\nN\n\n1\n=\nN\n\nX\n\nE[XN (i(1)i(2))XN (i(2)i(3))) . . . XN (i(n)i(1))]\n\ni:[n]\u2192[N ]\n\nX\n\ni:[n]\u2192[N ]\n\n\u0014Y\n\u0015\nn\nE\nXN (i(k)i\u03b3(k)) ,\nk=1\n\nwhere \u03b3 = (1 2 . . . n) is the full forward cycle in the symmetric group S(n). Let\nus apply the Wick formula to each term of this sum, and then use the covariance\nstructure of the matrix elements. We obtain\n\u0014Y\n\u0015\nn\nE\nXN (i(k)i\u03b3(k)) =\nk=1\n\nX\n\nY\n\n\u03c0\u2208P2 (n) {r,s}\u2208\u03c0\nn\n\n= N\u2212 2\n\nX\n\nE[XN (i(r)i\u03b3(r))XN (i(s)i\u03b3(s))]\nY\n\n\u03b4i(r)i\u03b3(s) \u03b4i(s)i\u03b3(r) .\n\n\u03c0\u2208P2 (n) {r,s}\u2208\u03c0\n\nNow, any pair partition of [n] can be viewed as a product of disjoint two-cycles\nin S(n). For example, the three pair partitions of [4] enumerated above may be\nviewed as the fixed point free involutions\n(1 2)(3 4), (1 3)(2 4), (1 4)(2 3)\n\n\f52\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nin S(4). This is a useful shift in perspective because partitions are inert combinatorial objects whereas permutations are functions which act on points. Our\ncomputation above may thus be re-written as\n\u0014Y\n\u0015\nn\nn\nE\nXN (i(k)i\u03b3(k)) = N \u2212 2\nk=1\n\nX\n\nn\nY\n\n\u03b4i(k)i\u03b3\u03c0(k) .\n\n\u03c0\u2208P2 (n) k=1\n\nPutting this all together and changing order of summation, we obtain\nn\n\nn\n\u03c4N [XN\n] = N 1\u2212 2\n\nX\n\nX\n\nX\n\nX\n\nn\nY\n\n\u03b4i(k)i\u03b3\u03c0(k)\n\ni:[n]\u2192[N ] \u03c0\u2208P2 (n) k=1\nn\n\n= N 1\u2212 2\n\nn\nY\n\n\u03b4i(k)i\u03b3\u03c0(k) ,\n\n\u03c0\u2208P2 (n) i:[n]\u2192[N ] k=1\n\nfrom which we see that the internal sum is non-zero if and only if the function\ni : [n] \u2192 [N ] is constant on the cycles of the permutation \u03b3\u03c0 \u2208 S(n). In order to\nbuild such a function, we must specify one of N possible values to be taken on each\ncycle. We thus obtain\nX\nn\nn\nN c(\u03b3\u03c0)\u22121\u2212 2 ,\n]=\n\u03c4N [XN\n\u03c0\u2208P2 (n)\n\nwhere c(\u03c3) denotes the number of cycles in the disjoint cycle decomposition of a\n3\npermutation \u03c3 \u2208 S(n). For example, when n = 3 we have \u03c4n [XN\n] = 0 since there\n4\nare no fixed point free involutions in S(3). In order to compute \u03c4N [XN\n], we first\ncompute the product of \u03b3 with all fixed point free involutions in S(4),\n(1 2 3 4)(1 2)(3 4) = (1 3)(2)(4)\n(1 2 3 4)(1 3)(2 4) = (1 4 3 2)\n(1 2 3 4)(1 4)(2 3) = (2 4)(1)(3),\n\nand from this we obtain\n1\n.\nN2\nn\nMore generally, \u03c4N [XN\n] = 0 whenever n is odd since there are no pairings on an\nodd number of points. When n = 2k is even the product \u03b3\u03c0 has the form\n4\n\u03c4N [XN\n]=2+\n\n\u03b3\u03c0 = (1 2 . . . 2k)(s1 t1 )(s2 t2 ) . . . (sk tk ).\nIn this product, each transposition factor (si ti ) acts either as a \"cut\" or as a \"join\",\nmeaning that it may either cut a cycle of (1 2 . . . 2k)(s1 t1 ) . . . (si\u22121 ti\u22121 ) in two,\nor join two disjoint cycles together into one. More geometrically, we can view the\nproduct \u03b3\u03c0 as a walk of length k on the (right) Cayley graph of S(2k); this walk\nis non-backtracking and each step taken augments the distance from the identity\npermutation by \u00b11, see Figure 15.\nA cut (step towards the identity) occurs when si and ti reside on the same cycle\nin the disjoint cycle decomposition of (1 2 . . . 2k)(s1 t1 ) . . . (si\u22121 ti\u22121 ), while a\njoin (step away from the identity) occurs when si and ti are on different cycles. In\ngeneral, the number of cycles in the product will be\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n53\n\nFigure 15. Walks corresponding to the products \u03b3\u03c0 in S(4).\n\nc(\u03b3\u03c0) = 1 + #cuts \u2212 #joins,\nso c(\u03b3\u03c0) is maximal at c(\u03b3\u03c0) = 1 + k when it is acted on by a sequence of k cut\nn\ntranspositions. In this case we get a contribution of N 1+k\u22121\u2212k = N 0 to \u03c4 [XN\n]. In\nfact, we always have\n#cuts \u2212 #joins = k \u2212 2g\nfor some non-negative integer g, leading to a contribution of the form N \u22122g and\nresulting in the formula\n2k\n\u03c4N [XN\n]=\n\nX \u03b5g (2k)\ng\u22650\n\nN 2g\n\nwhere \u03b5g (2k) is the number of products \u03b3\u03c0 of the long cycle with a fixed point free\ninvolution in S(2k) which terminate at a point of the sphere \u2202B(id, 2k \u22121\u22122g). We\nare only interested in the first term of this expansion, \u03b50 (2k), which counts fixed\npoint free involutions in S(2k) entirely composed of cuts. It is not difficult to see\nthat (s1 t1 ) . . . (sk tk ) is a sequence of cuts for \u03b3 if and only if it corresponds to a\nnon-crossing pair partition of [2k], and as we know the number of these is Catk .\nWe have now shown that\n(\n0, if n odd\nn\nn\nlim \u03c4N [XN ] = lim (E \u2297 trN )[XN ] =\n.\nN \u2192\u221e\nN \u2192\u221e\nCat n2 , if n even\nfor a GUE matrix XN . This establishes that XN is an asymptotic random matrix\nmodel of a single semicircular random variable X. It remains to use this fact to\nconstruct a sequence of pairs of random matrices which model a pair X, Y of freely\nindependent semicircular random variables.\n\n\f54\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nWhat should we be looking for? Let X (1) , X (2) be a pair of free semicircular\nrandom variables. Let e : [n] \u2192 [2] be a function, and apply the free momentcumulant formula to the corresponding mixed moment:\n\u03c4 [X (e(1)) . . . X (e(n)) ] =\n\nX\n\nY\n\n\u03c0\u2208NC(n) B\u2208\u03c0\n\nX\n\n=\n\n\u03ba|B| (X (e(i)) : i \u2208 B)\n\nY\n\n\u03b4e(r)e(s) .\n\n\u03c0\u2208NC2 (n) {r,s}\u2208\u03c0\n\nThis reduction occurs because X (1) , X (2) are free, so that all mixed free cumulants\nin these variables vanish. Moreover, these variables are semicircular so only order\ntwo pure cumulants survive. We can think of the function e as a bicolouring of [n].\nThe formula for mixed moments of a semicircular pair then becomes\nX\n\n\u03c4 [X (e(1)) . . . X (e(n)) ] =\n\n1,\n\n(e)\n\n\u03c0\u2208NC2 (n)\n(e)\n\nwhere \u03c0 \u2208 NC2 (n) is the set of non-crossing pair partitions of [n] which pair elements of the same colour. This is very much like the Wick formula for Gaussian expectations, but with Gaussians replaced by semicirculars and summation restricted\nto non-crossing pairings. We need to realize this structure in the combinatorics of\nGUE random matrices.\n(e)\nThis construction goes as follows. Let ZN (ij), 1 \u2264 e \u2264 2, 1 \u2264 i, j \u2264 N be a\n2\ncollection of 2N iid centred complex Gaussian random variables of variance 1/N .\n(1)\n(1)\n(2)\n(2)\nForm the corresponding Ginibre matrices ZN = [ZN (ij)], ZN = [ZN (ij)] and\n(2)\n(1)\n(2)\n(1)\n(2)\n(1)\nGUE matrices XN = 21 (ZN + (ZN )\u2217 ), XN = 21 (ZN + (ZN )\u2217 ). The resulting\ncovariance structure of matrix elements is\n(q)\n\n(p)\n\n(p)\n\n(q)\n\nE[XN (ij)XN (kl)] = E[XN (ij)XN (lk)] =\n(1)\n\n\u03b4ik \u03b4jl \u03b4pq\n.\nN\n\n(2)\n\nWe can prove that XN , XN are asymptotically free by showing that\n(e(1))\n\nlim \u03c4N [XN\n\nN \u2192\u221e\n\n(e(n))\n\n. . . XN\n\n(e)\n\n] = |NC2 (n)|,\n\nand this can in turn be proved using the Wick formula and the above covariance\nstructure. Computations almost exactly like those appearing in the one-matrix case\nlead to the formula\n(e(1))\n\n\u03c4N [XN\n\n(e(n))\n\n. . . XN\n\n]=\n\nX\n\nn\n\nN c(\u03b3\u03c0)\u22121\u2212 2 ,\n\n(e)\n\n\u03c0\u2208P2 (n)\n(e)\n\nwith the summation being taken over the set P2 (n) of pairings on [n] which respect the colouring e : [n] \u2192 [2]. Arguing as above, each such pairing makes a\ncontribution of the form N \u22122g for some g \u2265 0, and those which make contributions\non the leading order N 0 correspond to sequences of cut transpositions for the full\nforward cycle \u03c0, which we know come from non-crossing pairings. So in the limit\n(e)\nN \u2192 \u221e this expectation converges to |NC2 (n)|, as required.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n55\n\n3.5. Random matrix model of a free pair with one semicircle. In the previous subsection we modelled a free pair of semicircular random variables X, Y living\nin an abstract non-commutative probability space (A, \u03c4 ) using a sequence of independent GUE random matrices XN , YN living in random matrix space (AN , \u03c4N ).\nIt is reasonable to wonder whether we have not overlooked the possibility of\nmodelling X, Y in a simpler way, namely using deterministic matrices. Indeed, we\nhave\nZ\n\u03c4 [X n ] = tn \u03bcX (dt)\nR\n\nwith\n1 p\n4 \u2212 t2 dt\n2\u03c0\nthe Wigner semicircle measure, and this fact leads to a deterministic matrix model\nfor X. For each N \u2265 1, define the N th classical locations LN (1) < LN (2) < * * * <\nLN (N ) of \u03bcX implicitly by\n\u03bcX (dt) =\n\nLZN (i)\n\n\u03bcX (dt) =\n\ni\n.\nN\n\n\u22122\n\nThat is, we start at t = \u22122 and integrate along the semicircle until a mass of\ni/N is achieved, at which time we mark off the corresponding location LN (i) on\nthe t-axis. The measure \u03bcN which places mass 1/N at each of the N th classical\nlocations converges weakly to \u03bcX as N \u2192 \u221e. Consequently, the diagonal matrix\nXN with entries XN (ij) = \u03b4ij LN (i) is a random variable in deterministic matrix\nspace (MN (C), trN ) which models X,\nn\nlim trN [XN\n] = \u03c4 [X n ].\n\nN \u2192\u221e\n\nSince X and Y are equidistributed, putting YN := XN we have that XN models\nX and YN models Y . However, XN and YN are not asymptotically free. Indeed,\nasymptotic freeness of XN and YN would imply that\nlim trN [XN YN ] = lim trN [XN ] lim trN [XN ] = 0,\n\nN \u2192\u221e\n\nbut instead we have\n\nN \u2192\u221e\n\nN \u2192\u221e\n\nLN (1)2 + * * * + LN (N )2\n,\nN\nthe mean squared classical locations of the Wigner measure, which is strictly positive and increasing in N . Thus while XN and YN model X and Y respectively,\nthey cannot model the free relation between them. However, this does not preclude\nthe possibility that a pair of free random variables can be modelled by one random\nand one deterministic matrix.\nLet X and Y be a pair of free random variables with X semicircular, and Y of\narbitrary distribution. Let XN be a sequence of GUE matrices modelling X, and\nsuppose that YN is a sequence of deterministic matrices modelling Y ,\ntrN [XN YN ] =\n\n\f56\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nlim trN [YNn ] = \u03c4 [Y n ].\n\nN \u2192\u221e\n\nXN lives in random matrix space (AN , \u03c4N ) = (L\u221e\u2212 (\u03a9, F , P ) \u2297 MN (C), E \u2297 trN )\nwhile YN lives in deterministic matrix space (MN (C), trN ), so a priori it is meaningless to speak of the potential asymptotic free independence of XN and YN .\nHowever, we may think of a deterministic matrix as a random matrix whose entries\nare constant random variables in L\u221e\u2212 (\u03a9, F , P ). This corresponds to an embedding of deterministic matrix space in random matrix space satisfying \u03c4N |MN (C) =\n(E \u2297 trN )|MN (C) = trN . From this point of view, YN is a random matrix model of\nY and we can consider the possibility that XN , YN \u2208 AN are asymptotically free\nwith respect to \u03c4N . We now show that this is indeed the case.\nAs in the previous subsection, we proceed by identifying the combinatorial structure governing the target pair X, Y and then looking for this same structure in the\nN \u2192 \u221e asymptotics of XN , YN . Our target is a pair of free random variables\nwith X semicircular and Y arbitrary. Understanding their joint distribution means\nunderstanding the collection of mixed moments\n\u03c4 [X p(1) Y q(1) . . . X p(n) Y q(n) ],\nwith n \u2265 1 and p, q : [n] \u2192 {0, 1, 2, . . . }. This amounts to understanding mixed\nmoments of the form\n\u03c4 [XY q(1) . . . XY q(n) ],\nsince we can artificially insert copies of Y 0 = 1A to break up powers of X greater\nthan one. We can expand this expectation using the free moment-cumulant formula\nand simplify the resulting expression using the fact that mixed cumulants in free\nrandom variables vanish. Further simplification results from the fact that, since X\nis semicircular, its only non-vanishing pure cumulant is \u03ba2 (X) = 1. This leads to\na formula for \u03c4 [XY q(1) . . . XY q(n) ] which is straightforward but whose statement\nrequires some notions which we have not covered (in particular, the complement\nof a non-crossing partition, see [28]). However, in the case where \u03c4 is a tracial\nexpecation, meaning that \u03c4 [AB] = \u03c4 [BA], the formula in question can be stated\nmore simply as\n\u03c4 [XY q(1) . . . XY q(n) ] =\n\nX\n\n\u03c4\u03c0\u03b3 [Y q(1) , . . . , Y q(n) ].\n\n\u03c0\u2208NC2 (n)\n\nHere, as in the last subsection, we think of a pair partition \u03c0 \u2208 P2 (n) as a product\nof disjoint two-cycles in the symmetric group S(n), and \u03b3 is the full forward cycle\n(1 2 . . . n). Given a permutation \u03c3 \u2208 S(n), the expression \u03c4\u03c3 [A1 , . . . , AN ] is defined\nto be the product of \u03c4 extended over the cycles of \u03c3. For example,\n\u03c4(1\n\n6 2)(4 5)(3) [A1 , A2 , A3 , A4 , A5 , A6 ]\n\n= \u03c4 [A1 A6 A2 ]\u03c4 [A4 A5 ]\u03c4 [A3 ].\n\nThis definition is kosher since \u03c4 is tracial. We now have our proof strategy: we will\nprove that XN , YN are asymptotically free by showing that\nq(1)\n\nlim \u03c4N [XN YN\n\nN \u2192\u221e\n\nq(n)\n\n. . . XN YN\n\n]=\n\nX\n\n\u03c0\u2208NC2 (n)\n\n\u03c4\u03c0\u03b3 [Y q(1) , . . . , Y q(n) ].\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n57\n\nThe computation proceeds much as in the last section - we expand everything\nin sight and apply the Wick formula. We have\nq(1)\n\nq(n)\n\n\u03c4N [XN YN . . . XN YN ]\n1 X\nq(n)\nq(1)\n=\nE[XN (a(1)a(2))YN (a(2)a(3)) . . . XN (a(2n \u2212 1)a(2n))YN (a(2n)a(1))],\nN a\n\nthe summation being over all functions a : [2n] \u2192 [N ]. Let us reparameterize each\nterm of the sum with i, j : [n] \u2192 [N ] defined by\n(a(1), a(2), . . . , a(2n \u2212 1), a(2n)) = (i(1), j(1), . . . , i(n), j(n)).\nOur computation so far becomes\nq(1)\n\u03c4N [XN YN\n\nq(n)\n. . . XN YN ]\n\n\u0014Y\n\u0015Y\nn\nn\n1 X\nq(k)\n=\nE\nXN (i(k)j(k))\nYN (j(k)i\u03b3(k)).\nN i,j\nk=1\n\nk=1\n\nApplying the Wick formula, the calculation evolves as follows:\nq(1)\n\n\u03c4N [XN YN\n\nq(n)\n\n. . . XN YN\n\n1 X X\n=\nN i,j\n\nY\n\n]\n\nE[XN (i(r)j(r))XN (i(s)j(s))]\n\nn\n\nX X\n\nn\nY\n\nq(k)\n\nYN\n\n(j(k)i\u03b3(k))\n\nk=1\n\n\u03c0\u2208P2 (n) {r,s}\u2208\u03c0\n\n= N \u22121\u2212 2\n\nn\nY\n\nq(k)\n\n\u03b4i(k)j\u03c0(k) YN\n\n(j(k)i\u03b3(k))\n\ni,j \u03c0\u2208P2 (n) k=1\n\nn\n\n= N \u22121\u2212 2\n\nn\nX XY\n\n\u03c0\u2208P2 (n) j\nn\n\n= N \u22121\u2212 2\n\nX\n\nq(k)\n\nYN\n\n(j(k)j\u03c0\u03b3(k))\n\nk=1\nq(1)\n\nTr\u03c0\u03b3 [YN\n\nq(n)\n\n, . . . , YN\n\n]\n\n\u03c0\u2208P2 (n)\n\n=\n\nX\n\nn\n\nq(1)\n\nN c(\u03c0\u03b3)\u22121\u2212 2 tr\u03c0\u03b3 [YN\n\nq(n)\n\n, . . . , YN\n\n].\n\n\u03c0\u2208P2 (n)\n\nAs in the previous subsection, the dominant contributions to this sum are of order\nN 0 and come from those pair partitions \u03c0 \u2208 P2 (n) for which c(\u03c0\u03b3) is maximal, and\nthese are the non-crossing pairings. Hence we obtain\nX\nq(1)\nq(n)\n\u03c4\u03c0\u03b3 [Y q(1) , . . . , Y q(n) ],\nlim \u03c4N [XN YN . . . XN YN ] =\nN \u2192\u221e\n\n\u03c0\u2208NC2 (n)\n\nas required.\n\n3.6. Random matrix model of an arbitrary free pair. In the last section we\nsaw that a pair of free random variables can be modelled by one random and one\ndeterministic matrix provided that at least one of the target variables is semicircular. In this case, the semicircular target is modelled by a sequence of GUE random\nmatrices.\nIn this section we show that any pair of free random variables can be modelled\nby one random and one deterministic matrix, provided each target variable can\n\n\f58\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nbe individually modelled by a sequence of deterministic matrices. The idea is to\nrandomly rotate one of the deterministic matrix models so as to create the free\nrelation.\nLet X, Y be a pair of free random variables living in an abstract non-commutative\nprobability space (A, \u03c4 ). We make no assumption on their moments. What we\nassume is the existence of a pair of deterministic matrix models\nn\n\u03c4 [X n ] = lim trN [XN\n],\nN \u2192\u221e\n\n\u03c4 [Y n ] = lim trN [YNn ].\nN \u2192\u221e\n\nIf X, Y happen to have distributions \u03bcX , \u03bcY which are compactly supported probability measures on R, then such models can always be constructed. In particular,\nthis will be the case if X, Y are bounded self-adjoint random variables living in a\n\u2217-probability space.\nAs in the previous subsection, we view XN , YN as random matrices with constant\nentries so that they reside in random matrix space (AN , \u03c4N ), with the E part of\n\u03c4N = E \u2297 trN acting trivially. As we saw above, there is no guarantee that XN , YN\nare asymptotically free. On the other hand, we also saw that special pairs of free\nrandom variables can be modelled by one random and one deterministic matrix.\nTherefore it is reasonable to hope that making XN genuinely random might lead to\nasymptotic freeness. We have to randomize XN in such a way that its moments will\nbe preserved. This can be achieved via conjugation by a unitary random matrix\nUN \u2208 AN ,\nThe deterministic matrix XN\nmoments since\n\n\u2217\nXN 7\u2192 UN XN UN\n.\n\u2217\nand its randomized version UN XN UN\nhave the same\n\n\u2217 n\n\u2217 n\n\u03c4N [(UN XN UN\n) ] = (E \u2297 trN )[(UN XN UN\n) ]\nn \u2217\n= (E \u2297 trN )[UN XN\nUN ]\n\n\u2217\nn\n= (E \u2297 trN )[UN\nU N XN\n]\nn\n= (E \u2297 trN )[XN\n]\nn\n= \u03c4N [XN\n].\n\n\u2217\nConsequently, the sequence UN XN UN\nis a random matrix model for X.\n\u2217\nWe aim to prove that UN XN UN and YN are asymptotically free. Since we are\nmaking no assumptions on the limiting variables X, Y , we cannot verify this by\n\u2217\nlooking for special structure in the limiting mixed moments of UN XN UN\nand YN ,\nas we did above. Instead, we must verify asymptotic freeness directly, using the\ndefinition:\n\u2217\n\u2217\nlim \u03c4N [f1 (UN XN UN\n)g1 (YN ) . . . fn (UN XN UN\n)gn (YN )] = 0\n\nN \u2192\u221e\n\nwhenever f1 , g1 , . . . , fn , gn are polynomials such that\n\u2217\n\u2217\nlim \u03c4N [f1 (UN XN UN\n)] = lim \u03c4n [g1 (YN )] = * * * = lim \u03c4N [fn (UN XN UN\n)] = lim \u03c4n [gn (YN )] = 0.\n\nN \u2192\u221e\n\nN \u2192\u221e\n\nN \u2192\u221e\n\nN \u2192\u221e\n\nThough the brute force verification of this criterion may seem an impossible task,\nwe will see that it can be accomplished for a well-chosen sequence of unitary random\nmatrices UN . Let us advance as far as possible before specifying UN precisely.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n59\n\nAs an initial reduction, note the identity\n\u2217\n\u2217\n\u03c4N [f1 (UN XN UN\n)g1 (YN ) . . . fn (UN XN UN\n)gn (YN )]\n\u2217\n\u2217\n= \u03c4N [UN f1 (XN )UN\ng1 (YN ) . . . UN fn (XN )UN\ngn (YN )].\n\nSince the fi 's and gj 's are polynomials and \u03c4N is linear, the right hand side of this\nequation may be expanded as a sum of monomial expectations,\n\u2217\n\u2217\n\u03c4N [UN f1 (XN )UN\ng1 (YN ) . . . UN fn (XN )UN\ngn (YN )]\nX\np(n) \u2217 q(n)\np(1) \u2217 q(1)\n=\nc(pq)\u03c4N [UN XN UN YN . . . UN XN UN YN ]\np,q\n\nweighted by some scalar coefficients c(pq), the sum being over functions p : [n] \u2192\n{0, . . . , max deg fi }, q : [n] \u2192 {0, . . . , max deg gj }. Each monomial expectation can\nin turn be expanded as\np(1)\n\np(n)\n\nq(1)\n\nq(n)\n\n\u2217\n\u2217\nYN ]\nYN . . . UN XN UN\n\u03c4N [UN XN UN\nX\n1\nq(n)\np(1)\n\u2217\n(a(4n \u2212 1)a(4n))YN (a(4n)a(1))]\nE[UN (a(1)a(2))XN (a(2)a(3)) . . . UN\n=\nN a\n1 X\nq(n)\np(1)\n=\nE[UN (a(1)a(2))XN (a(2)a(3)) . . . U N (a(4n)a(4n \u2212 1))YN (a(4n)a(1))].\nN a\n\nLet us reparameterize the summation index a : [4n] \u2192 [N ] by a quadruple of\nfunctions i, j, i\u2032 , j \u2032 : [n] \u2192 [N ] according to\n(a(1), a(2), a(3), a(4), . . . , a(4n \u2212 3), a(4n \u2212 2), a(4n \u2212 1), a(4n))\n\n=(i(1), j(1), j \u2032 (1), i\u2032 (1), . . . , i(n), j(n), j \u2032 (n), i\u2032 (n)).\n\nOur monomial expectations then take the more streamlined form\nq(1)\n\np(1)\n\np(n)\n\nq(n)\n\n\u2217\n\u2217\n\u03c4N [UN XN UN\nYN . . . UN XN UN\nYN ]\n\u0015Y\n\u0014\nn\nn\nY\n1 X\np(k)\nq(k)\n=\nXN (j(k)j \u2032 (k))YN (i\u2032 (k)i\u03b3(k)),\nE\nUN (i(k)j(k))U N (i\u2032 (k)j \u2032 (k))\nN\n\u2032 \u2032\ni,j,i ,j\n\nk=1\n\nk=1\n\nwhere as always \u03b3 = (1 2 . . . n) is the full forward cycle in the symmetric group\nS(n). In order to go any further with this calculation, we must deal with the\ncorrelation functions\n\u0014Y\n\u0015\nn\nE\nUN (i(k)j(k))U N (i\u2032 (k)j \u2032 (k)) .\nk=1\n\nof the matrix elements of UN . We would like to have an analogue of the Wick\nformula which will enable us to address these correlation functions. A formula of\nthis type is known for random matrices sampled from the Haar probability measure\non the unitary group U(N ).\n\n\f60\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\nHaar-distributed unitary matrices are the second most important class of random matrices after GUE matrices. Like GUE matrices,\nthey can be construc\u221a\ntively obtained from Ginibre matrices. Let Z\u0303N = N ZN be an N \u00d7 N random matrix whose entries Z\u0303N (ij) are iid complex Gaussian random variables of\nmean zero and variance one. This is a renormalized version of the Ginibre matrix which we previously used to construct a GUE random matrix. The Ginibre\nmatrix Z\u0303N is almost surely non-singular. Applying the Gram-Schmidt orthonormalization procedure to the columns of Z\u0303N , we obtain a random unitary matrix\nUN whose distribution in the unitary group U(N ) is given by the Haar probability measure. The entries UN (ij) are bounded random variables, so UN is a\nnon-commutative random variable living in random matrix space (AN , \u03c4N ). The\neigenvalues \u03bbN (1) = ei\u03b8N (1) , . . . , \u03bbN (N ) = ei\u03b8N (N ) , 0 \u2264 \u03b8N (1) \u2264 * * * \u2264 \u03b8N (N ) \u2264 2\u03c0\nof UN form a random point process on the unit circle with joint distribution\nP (\u03b8N (1) \u2208 I1 , . . . , \u03b8N (N ) \u2208 IN ) \u221d\n\nZ\n\n...\n\nZ\n\ne\u2212N\n\n2\n\nH(\u03b81 ,...,\u03b8N )\n\nd\u03b81 . . . d\u03b8N\n\nIN\n\nI1\n\nfor any intervals I1 , . . . , IN \u2286 [0, 2\u03c0], where H is the log-gas Hamiltonian [13]\nH(\u03b81 , . . . , \u03b8N ) = \u2212\n\n1\nN2\n\nX\n\n1\u2264i6=j\u2264N\n\nlog |ei\u03b8i \u2212 ei\u03b8j |.\n\nThe random point process on the unit circle driven by this Hamiltonian is known as\nthe Circular Unitary Ensemble, and UN is termed a CUE random matrix. As with\nGUE random matrices, almost any question about the spectrum of CUE random\nmatrices can be answered using this explicit formula, see e.g. [8] for a survey of\nmany interesting results.\nWe are not interested in the eigenvalues of CUE matrices, but rather in the correlation functions of their matrix elements. These can be handled using a Wick-type\nformula known as the Weingarten formula, after the American physicist Donald H.\nWeingarten3. Like the Wick formula, the Weingarten formula is a combinatorial\nrule which reduces the computation of general correlation functions to the computation of a special class of correlations. Unfortunately, the Weingarten formula is\nmore complicated than the Wick formula. It reads:\n\u0015\n\u0014Y\nn\nE\nUN (i(k)j(k))U N (i\u2032 (k)j \u2032 (k)) =\nk=1\n\nX\n\n\u03c1,\u03c3\u2208S(n)\n\n\u0014Y\n\u0015\nn\n\u03b4i\u03c3,i\u2032 \u03b4j\u03c1,j \u2032 E\nUN (kk)U N (k\u03c1\u22121 \u03c3(k)) .\nk=1\n\nNote that his formula only makes sense when N \u2265 n, and instead of a sum over\nfixed point free involutions we are faced with a double sum over all of S(n). Worse\nstill, the Weingarten formula does not reduce our problem to the computation of\npair correlators, but only to the computation of arbitrary permutation correlators\n\u0014Y\n\u0015\nn\nE\nUN (kk)U N (k\u03c0(k)) ,\nk=1\n\n\u03c0 \u2208 S(n),\n\n3Further information regarding Weingarten and his colleagues in the first Fermilab theory\ngroup may be found at http://bama.ua.edu/\u223clclavell/Weston/\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n61\n\nand these have a rather complicated structure. Their computation is the subject\nof a large literature both in physics and mathematics, a unified treatment of which\nmay be found in [6]. We delay dealing with these averages for the moment and\npress on in our calculation.\nWe return to the expression\nq(1)\n\np(1)\n\np(n)\n\nq(n)\n\n\u2217\n\u2217\nYN ]\nYN . . . UN XN UN\n\u03c4N [UN XN UN\n\u0015Y\n\u0014\nn\nn\nX\nY\n1\np(k)\nq(k)\n\u2032\n\u2032\n=\nXN (j(k)j \u2032 (k))YN (i\u2032 (k)i\u03b3(k)),\nE\nUN (i(k)j(k))U N (i (k)j (k))\nN\n\u2032 \u2032\ni,j,i ,j\n\nk=1\n\nk=1\n\nand apply the Weingarten formula. The calculation evolves as follows:\nq(1)\n\np(1)\n\n\u2217\nYN\n\u03c4N [UN XN UN\n\n=\n\n1 X\nN\n\u2032 \u2032\n\nX\n\ni,j,i ,j \u03c1,\u03c3\u2208S(n)\n\n=\n\n=\n\n=\n\n1\nN\n1\nN\n\nX\n\n\u03c1,\u03c3\u2208S(n)\n\nX\n\n\u03c1,\u03c3\u2208S(n)\n\nX\n\n\u03c1,\u03c3\u2208S(n)\n\np(n)\n\nq(n)\n\n\u2217\n. . . U N XN U N\nYN ]\n\u0015Y\n\u0014Y\nn\nn\nq(k)\np(k)\n\u22121\nXN (j(k)j \u2032 (k))YN (i\u2032 (k)i\u03b3(k))\n\u03b4i\u03c3,i\u2032 \u03b4j\u03c1,j \u2032 E\nUN (kk)U N (k\u03c1 \u03c3(k))\nk=1\n\nk=1\n\n\u0015X Y\n\u0014Y\nn\nn\np(k)\nq(k)\n\u22121\nXN (j(k)j\u03c1(k))YN (i\u2032 (k)i\u03c3 \u22121 \u03b3(k))\nE\nUN (kk)U N (k\u03c1 \u03c3(k))\nk=1\n\ni\u2032 ,j k=1\n\n\u0014Y\n\u0015\nn\np(1)\np(n)\np(1)\np(n)\n\u22121\nE\nUN (kk)U N (k\u03c1 \u03c3(k)) Tr\u03c1 (XN , . . . , XN ) Tr\u03c3\u22121 \u03b3 (YN , . . . , YN )\nk=1\n\n\u0015\n\u0014Y\nn\n\u22121\np(1)\np(n)\np(1)\np(n)\nE\nUN (kk)U N (k\u03c1\u22121 \u03c3(k)) N c(\u03c1)+c(\u03c3 \u03b3)\u22121 tr\u03c1 (XN , . . . , XN ) tr\u03c3\u22121 \u03b3 (YN , . . . , YN ).\nk=1\n\nQ\nAt this point we are forced to deal with the permutation correlators E[ UN (kk)U N (k\u03c0(k))].\nPerhaps the most appealing presentation of these expectations is as a power series\nin N \u22121 . It may be shown [29] that\n\u0014Y\n\u0015\nn\n\u221e\n1 X\ncn,r (\u03c0)\nE\nUN (kk)U N (k\u03c0(k)) = n\n,\n(\u22121)r\nN r=0\nNr\nk=1\n\nfor any \u03c0 \u2208 S(n), where the coefficient cn,r (\u03c0) equals the number of factorizations\n\u03c0 = (s1 t1 ) . . . (sr tr )\nof \u03c0 into r transpositions (si ti ) \u2208 S(n), si < ti , which have the property that\nt1 \u2264 * * * \u2264 tr .\nThis series is absolutely convergent for N \u2265 n, but divergent for N < n. This will\nnot trouble us since we are looking for N \u2192 \u221e asymptotics with n fixed. Indeed,\nlet |\u03c0| = n \u2212 c(\u03c0) denote the distance from the identity permutation to \u03c0 in the\nCayley graph of S(n). Then, since any permutation is either even or odd, we have\n\n\f62\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n\u0014Y\n\u0015\n\u221e\nn\ncn,r (\u03c0)\n1 X\n(\u22121)r\nE\nUN (kk)U N (k\u03c0(k)) = n\nN r=0\nNr\nk=1\n\n\u221e\n(\u22121)|\u03c0| X cn,|\u03c0|+2g (\u03c0)\nN 2g\nN n+|\u03c0| g=0\n\u0013\n\u0012\na(\u03c0)\n1\n= n+|\u03c0| + O\n,\nN\nN n+|\u03c0|+2\n\n=\n\nwhere a(\u03c0) = (\u22121)|\u03c0| cn,|\u03c0| (\u03c0) is the leading asymptotics. We may now continue\nour calculation:\np(1)\n\nq(1)\n\np(n)\n\nq(n)\n\n\u2217\n\u2217\n\u03c4N [UN XN UN\nYN . . . UN XN UN\nYN ]\n\u0012\n\u0012\n\u0013\u0013\n\u22121\nX\n\u22121\na(\u03c1 \u03c3))\n1\np(1)\np(n)\np(1)\np(n)\n=\nN c(\u03c1)+c(\u03c3 \u03b3)\u22121 tr\u03c1 (XN , . . . , XN ) tr\u03c3\u22121 \u03b3 (YN , . . . , YN )\n\u22121 \u03c3| + O\n\u22121 \u03c3|+2\nn+|\u03c1\nn+|\u03c1\nN\nN\n\u03c1,\u03c3\u2208S(n)\n\u0012\n\u0013\u0013\nX \u0012\n\u22121\n\u22121\n1\np(n)\np(1)\np(n)\np(1)\na(\u03c1\u22121 \u03c3) + O\n=\nN |\u03b3|\u2212|\u03c1|\u2212|\u03c1 \u03c3|\u2212|\u03c3 \u03b3| tr\u03c1 (XN , . . . , XN ) tr\u03c3\u22121 \u03b3 (YN , . . . , YN ).\n2\nN\n\u03c1,\u03c3\u2208S(n)\n\nPutting everything together, we have shown that\n\u2217\n\u2217\n\u03c4N [UN f1 (XN )UN\ng1 (YN ) . . . UN fn (XN )UN\ngn (YN )]\n\u0012\n\u0012\n\u0013\u0013\nX\n\u22121\n\u22121\n1\na(\u03c1\u22121 \u03c3) + O\nN |\u03b3|\u2212|\u03c1|\u2212|\u03c1 \u03c3|\u2212|\u03c3 \u03b3| tr\u03c1 (f1 (XN ), . . . , fn (XN )) tr\u03c3\u22121 \u03b3 (g1 (YN ), . . . , gn (YN )),\n=\n2\nN\n\u03c1,\u03c3\u2208S(n)\n\nand it remains to show that the N \u2192 \u221e limit of this complicated expression is\nzero. To this end, consider the order |\u03b3| \u2212 |\u03c1| \u2212 |\u03c1\u22121 \u03c3| \u2212 |\u03c3 \u22121 \u03b3| of the \u03c1, \u03c3 term\nin this sum. The positive part, |\u03b3| = n \u2212 1, is simply the length of any geodesic\njoining the identity permutation to \u03b3 in the Cayley graph of S(n). The negative\npart, \u2212|\u03c1| \u2212 |\u03c1\u22121 \u03c3| \u2212 |\u03c3 \u22121 \u03b3|, is the length of a walk from the identity to \u03b3 made up\nof three legs: a geodesic from id to \u03c1, followed by a geodesic from \u03c1 to \u03c3, followed\nby a geodesic from \u03c3 to \u03b3. Thus the order of the \u03c1, \u03c3 term is at most N 0 , and this\noccurs precisely when \u03c1 and \u03c3 lie on a geodesic from id to \u03b3, see Figure 16. Thus\n\u2217\n\u2217\ng1 (YN ) . . . UN fn (XN )UN\ngn (YN )]\nlim \u03c4N [UN f1 (XN )UN\nX\na(\u03c1\u22121 \u03c3)\u03c4\u03c1 (f1 (X), . . . , fn (X))\u03c4\u03c3\u22121 \u03b3 (g1 (Y ), . . . , gn (Y )).\n=\nN \u2192\u221e\n\n|\u03c1|+|\u03c1\u22121 \u03c3|+|\u03c3\u22121 \u03b3|=|\u03b3|\n\nSince\n\u03c4 [f1 (X)] = \u03c4 [g1 (Y )] = * * * = \u03c4 [fn (X)] = \u03c4 [gn (Y )] = 0,\n\nin order to show that the sum on the right has all terms equal to zero it suffices\nto show that the condition |\u03c1| + |\u03c1\u22121 \u03c3| + |\u03c3 \u22121 \u03b3| = |\u03b3| forces either \u03c1 or \u03c3 \u22121 \u03b3 to\nhave a fixed point. This is because \u03c4\u03c1 and \u03c4\u03c3\u22121 \u03b3 are products determined by the\ncycle structure of the indexing permutation. Since \u03c1, \u03c3 lie on a geodesic id \u2192 \u03b3, we\nhave |\u03c1| + |\u03c3 \u22121 \u03b3| \u2264 |\u03b3| = n \u2212 1, so that one of \u03c1 or \u03c3 \u22121 \u03b3 is a product of at most\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n63\n\n\u03b3\n\n\u03b3\n\n\u03c3\n\n\u03c3\nn-1\n\u03c1\n\n\u03c1\n\nid\n\nid\n\nFigure 16. Only geodesic paths survive in the large N limit\n(n \u2212 1)/2 transpositions. In the extremal case, all of these transpositions are joins,\nleading to a permutation consisting of an (n \u2212 1)-cycle and a fixed point.\n3.7. GUE + GUE. Imagine that we had been enumeratively lazy in our construction of the GUE matrix model of a free semicircular pair, and had only shown that\n(2)\n(1)\ntwo iid GUE matrices XN , XN are asymptotically free without determining their\nindividual limiting distributions. We could then appeal to the free central limit\ntheorem to obtain that the limit distribution of the random matrix\n(1)\n\nSN =\n\n(n)\n\nXN + * * * + XN\n\u221a\n,\nN\n\n(i)\n\nwhere the XN 's are iid GUE samples, is standard semicircular. On the other hand,\n(i)\nsince the matrix elements of the XN 's are independent Guassians whose variances\nadd, we see that the rescaled sum SN is itself an N \u00d7 N GUE random matrix for\neach finite N . Thus we recover Wigner's semicircle law (for GUE matrices) from\nthe free central limit theorem.\n3.8. GUE + deterministic. Let XN be an N \u00d7 N GUE random matrix. Let YN\nbe an N \u00d7 N deterministic Hermitian matrix whose spectral measure \u03bdN converges\nweakly to a compactly supported probability measure \u03bd. Let \u03c3 be the limit distribution of the random matrix XN + YN . Since XN , YN are asymptotically free, we\nhave\n\u03c3 = \u03bc \u229e \u03bd,\nwhere \u03bc is the Wigner semicircle.\n\n\f64\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n3.9. randomly rotated + diagonal. Consider the 2N \u00d7 2N diagonal matrix\n\uf8ee\n\n\uf8ef\n\uf8ef\n\uf8ef\n=\uf8ef\n\uf8ef\n\uf8f0\n\n\uf8f9\n\n1\n\u22121\n\n\uf8fa\n\uf8fa\n\uf8fa\n..\nD2N\n\uf8fa\n.\n\uf8fa\n\uf8fb\n1\n\u22121\nwhose diagonal entries are the first 2N terms of an alternating sequence of \u00b11's, all\nother entries being zero. Let U2N be a 2N \u00d7 2N CUE random matrix, and consider\nthe random Hermitian matrix\n\u2217\nA2N = U2N D2N U2N\n+ D2N .\n\nLet \u03bc2N denote the spectral measure of A2N . We claim that \u03bc2N converges weakly\nto the arcsine distribution\n1\ndt,\n\u03bc(dt) = \u221a\n\u03c0 4 \u2212 t2\n\nt \u2208 [\u22122, 2],\n\nas N \u2192 \u221e.\n\u2217\nProof: Set X2N = U2N D2N U2N\nand Y2N = D2N . Then XN , YN is a random\nmatrix model for a pair of free random variables X, Y each of which has the \u00b11Bernoulli distribution\n1\n1\n\u03b4\u22121 + \u03b4+1 .\n2\n2\nThus the limit distribution of their sum is\nBernoulli \u229e Bernoulli = Arcsine.\nReferences\n1. G. E. Andrews, R. Askey, R. Roy, Special Functions, Encyclopedia of Mathematics and its\nApplications 71, Cambridge University Press, 2000.\n2. H. Bercovici, D. Voiculescu, Free convolution of measures with unbounded support, Indiana\nUniversity Mathematics Journal 42(3) (1993), 733-773.\n3. P. Biane, On the free convolution with a semicircular distribution, Indiana University Mathematics Journal 46(3) (1997), 705-718.\n4. P. Biane, Free probability and combinatorics, ICM 2002, Vol. II, 765-774.\n5. E. Br\u00e9zin, C. Itzykson, G. Parisi, J.-B. Zuber, Planar diagrams, Communications in Mathematical Physics 59 (1978), 35-51.\n6. B. Collins, S. Matsumoto, J. Novak, An invitation to Weingarten calculus, SpringerBriefs in\nMathematics, in preparation.\n7. A. Connes, Non-Commutative Geometry, available at http://www.alainconnes.org/docs/book94bigpdf.pdf\n8. P. Diaconis, Patterns in eigenvalues: the 70th Josiah Willard Gibbs lecture, Bulletin (New\nSeries) of the American Mathematical Society 40(2) (2003), 155-178.\n9. L. Erd\u0151s, B. Schlein, H.-T. Yau, Universality of random matrices and local relaxation flow,\nInventiones Mathematicae 185 (2011), 75-119.\n10. P. Etingof, Mathematical ideas and notions of quantum field theory, available at\nhttp://math.mit.edu/\u223cetingof/lect.ps\n11. R. A. Fisher, J. Wishart, The derivation of the pattern formulae of two-way partitions from\nthose of simpler patterns, Proceedings of the London Mathematical Society 33(1) (1932),\n195\u2013208.\n12. P. Flajolet, R. Sedgewick, Analytic Combinatorics, Cambridge University Press, 2009.\n\n\fTHREE LECTURES ON FREE PROBABILITY\n\n65\n\n13. P. J. Forrester, Log-Gases and Random Matrices, London Mathematical Society Monographs\nvolume 34, Princeton University Press 2010.\n14. R. L. Graham, D. E. Knuth, O. Patashnik, Concrete Mathematics: a Foundation for Computer Science, Second Edition, Addison-Wesley (1989).\n\u2217 (F )] is not\n15. U. Haagerup, S. Thorbj\u00f8rnsen, A new application of random matrices: Ext[Cred\n2\na group, Annals of Mathematics 162 (2005), 711-775.\n16. A. Hald, T. N. Thiele's contributions to statistics, International Statistical Review 49(1)\n(1981), 1\u201320.\n17. A. Hald, The early history of cumulants and the Gram-Charlier series, International Statistical Review 68(2) (2000), 137\u2013153.\n18. F. Hiai, D. Petz, The semicircle law, free random variables, and entropy,\n19. A. Hurwitz, \u00dcber Riemann'sche Fl\u00e4chen mit gegebenen Verzweigungspunkten, Mathematische\nAnnalen 39 (1891), 1\u201366.\n20. H. Kesten, Symmetric random walks on groups, Transactions of the American Mathematical\nSociety 92(2) (1959), 336-354.\n21. R. Kenyon, A. Okounkov, Limit shapes and the complex Burgers equation, Acta Mathematica\n199 (2007), 263-302.\n22. A. Matytsin, On the large N limit of the Itzykson-Zuber integral, Nuclear Physics B 411\n(1994), 805-820.\n23. B. Mazur, Controlling our errors, Nature 443 (2006), 38-39.\n24. J. A. Mingo, R. Speicher, Free Probability and Random Matrices, Fields Institute Monographs\n(to appear).\n25. M. R. Murty, V. K. Murty, The Sato-Tate conjecture and generalizations, available at\nhttp://www.ias.ac.in/pubs/splpubs/pjubileebook/639.pdf\n26. J. Nestruev, Smooth Manifolds and Observables, Springer Graduate Texts in Mathematics\n220.\n27. J. Novak, P. \u015aniady, What is. . . a free cumulant? Notices of the American Mathematical\nSociety 58(2) (2011), 300\u2013301.\n28. A. Nica, R. Speicher, Lectures on the Combinatorics of Free Probability, London Mathematical\nSociety Lecture Note Series 335 (2006).\n29. J. Novak, Jucys-Murphy elements and the unitary Weingarten function, Banach Center Publications 89 (2010), 231-235.\n30. G. P\u00f3lya, \u00dcber eine Aufgabe der Wahrscheinlichkeitsrechnung betrefend die Irrfhart im\nStrassenetz, Mathematisch Annalen 84 (1921), 149-160.\n31. G.-C. Rota, On the foundations of combinatorial theory I. Theory of M\u00f6bius functions,\nZeitschrift Wahrscheinlicheitstheorie 2 (1964), 340-368.\n32. M.\nRoth,\nCounting\ncovers\nof\nan\nelliptic\ncurve,\navailable\nat\nhttp://www.mast.queensu.ca/\u223cmikeroth/notes/covers.pdf\n33. L. Saloff-Coste, Probability on groups: random walks and invariant diffusions, Bulletin of the\nAmerican Mathematical Society 48(9) (2001), 968-977.\n34. S. Samuel, U(N ) Integrals, 1/N , and the De Wit - 't Hooft anomalies, Journal of Mathematical\nPhysics 21(12) (1980), 2695-2703.\n35. D.\nShlyakhtenko,\nNotes\non\nfree\nprobability\ntheory,\navailable\nat\nhttp://xxx.lanl.gov/pdf/math/0504063v1.pdf\n36. A. Soshnikov, Universality at the edge of the spectrum in Wigner random matrices, Communications in Mathematical Physics 207 (1999), 697-733.\n37. T. P. Speed, Cumulants and partition lattices, Australian Journal of Statistics 25(2) (1983),\n378\u2013388.\n38. R. P. Stanley, Enumerative Combinatorics, Volume 2, Cambridge Studies in Advanced Mathematics, Cambridge University Press (1999).\n39. R. P. Stanley, Catalan addendum, available at http://www-math.mit.edu/\u223crstan/ec/catadd.pdf\n40. R. P. Stanley, Increasing and decreasing subsequences and their variants, ICM 2006, Madrid.\n41. T.\nTao,\n254A,\nNotes\n5:\nFree\nProbability,\navailable\nat\nhttp://terrytao.wordpress.com/2010/02/10/245a-notes-5-free-probability\n42. T. Tao, V. Vu, Random matrices: Universality of local eigenvalue statistics, Acta Mathematica 206 (2011), 127-204.\n43. D.-V. Voiculescu, Addition of certain non-commuting random variables, Journal of Functional\nAnalysis 66 (1986), 323-346.\n\n\f66\n\nJONATHAN NOVAK WITH ILLUSTRATIONS BY MICHAEL LACROIX\n\n44. D.-V. Voiculescu, K. Dykema, A. Nica, Free Random Variables, CRM Monograph Series\n(1992)\n45. E. P. Wigner, On the distribution of the roots of certain symmetric matrices, Annals of\nMathematics 67(2) (1958), 325-327.\n46. A. Zvonkin, Matrix integrals and map enumeration: an accessible introduction, Mathematical\nand Computer Modelling 26 (1997), 281-304.\nDepartment of Mathematics, Massachusetts Institute of Technology, 77 Massachusetts\nAvenue, Cambridge, MA 02139-4307\nE-mail address: jnovak@math.mit.edu\n\n\f"}