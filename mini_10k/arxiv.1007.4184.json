{"id": "http://arxiv.org/abs/1007.4184v1", "guidislink": true, "updated": "2010-07-23T18:09:42Z", "updated_parsed": [2010, 7, 23, 18, 9, 42, 4, 204, 0], "published": "2010-07-23T18:09:42Z", "published_parsed": [2010, 7, 23, 18, 9, 42, 4, 204, 0], "title": "An Introductory Course on Quantum Mechanics", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.4682%2C1007.1826%2C1007.0910%2C1007.0161%2C1007.0661%2C1007.3118%2C1007.4587%2C1007.3263%2C1007.4536%2C1007.4460%2C1007.0929%2C1007.1080%2C1007.2151%2C1007.0570%2C1007.3986%2C1007.0567%2C1007.4996%2C1007.0249%2C1007.2198%2C1007.2244%2C1007.3145%2C1007.2678%2C1007.0450%2C1007.5006%2C1007.4886%2C1007.1970%2C1007.4798%2C1007.2801%2C1007.1315%2C1007.1221%2C1007.2623%2C1007.0034%2C1007.2805%2C1007.4038%2C1007.5053%2C1007.4087%2C1007.5182%2C1007.2971%2C1007.4173%2C1007.3956%2C1007.2515%2C1007.4225%2C1007.4031%2C1007.3407%2C1007.0232%2C1007.0853%2C1007.3036%2C1007.4315%2C1007.0002%2C1007.5438%2C1007.0389%2C1007.3941%2C1007.2725%2C1007.3539%2C1007.4453%2C1007.0186%2C1007.3799%2C1007.2778%2C1007.2553%2C1007.1540%2C1007.4243%2C1007.2116%2C1007.0737%2C1007.3392%2C1007.1613%2C1007.0494%2C1007.3612%2C1007.0490%2C1007.2978%2C1007.3084%2C1007.2657%2C1007.1892%2C1007.1394%2C1007.3679%2C1007.4440%2C1007.4743%2C1007.3359%2C1007.0965%2C1007.4739%2C1007.5217%2C1007.4994%2C1007.3709%2C1007.0212%2C1007.1361%2C1007.4184%2C1007.4748%2C1007.3699%2C1007.1312%2C1007.0344%2C1007.2025%2C1007.3601%2C1007.3005%2C1007.3635%2C1007.4271%2C1007.2859%2C1007.0524%2C1007.3706%2C1007.1639%2C1007.1924%2C1007.5447%2C1007.3303&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "An Introductory Course on Quantum Mechanics"}, "summary": "This is a very gentle introductory course on quantum mechanics aimed at the\nfirst years of the undergraduate level. The basic concepts are introduced, with\nmany applications and illustrations. Contains 12 short chapters of equal\nlength, ideal for a one term course. The license allows reuse of figures and\ntext under the Attribution-Noncommercial-ShareAlike conditions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.4682%2C1007.1826%2C1007.0910%2C1007.0161%2C1007.0661%2C1007.3118%2C1007.4587%2C1007.3263%2C1007.4536%2C1007.4460%2C1007.0929%2C1007.1080%2C1007.2151%2C1007.0570%2C1007.3986%2C1007.0567%2C1007.4996%2C1007.0249%2C1007.2198%2C1007.2244%2C1007.3145%2C1007.2678%2C1007.0450%2C1007.5006%2C1007.4886%2C1007.1970%2C1007.4798%2C1007.2801%2C1007.1315%2C1007.1221%2C1007.2623%2C1007.0034%2C1007.2805%2C1007.4038%2C1007.5053%2C1007.4087%2C1007.5182%2C1007.2971%2C1007.4173%2C1007.3956%2C1007.2515%2C1007.4225%2C1007.4031%2C1007.3407%2C1007.0232%2C1007.0853%2C1007.3036%2C1007.4315%2C1007.0002%2C1007.5438%2C1007.0389%2C1007.3941%2C1007.2725%2C1007.3539%2C1007.4453%2C1007.0186%2C1007.3799%2C1007.2778%2C1007.2553%2C1007.1540%2C1007.4243%2C1007.2116%2C1007.0737%2C1007.3392%2C1007.1613%2C1007.0494%2C1007.3612%2C1007.0490%2C1007.2978%2C1007.3084%2C1007.2657%2C1007.1892%2C1007.1394%2C1007.3679%2C1007.4440%2C1007.4743%2C1007.3359%2C1007.0965%2C1007.4739%2C1007.5217%2C1007.4994%2C1007.3709%2C1007.0212%2C1007.1361%2C1007.4184%2C1007.4748%2C1007.3699%2C1007.1312%2C1007.0344%2C1007.2025%2C1007.3601%2C1007.3005%2C1007.3635%2C1007.4271%2C1007.2859%2C1007.0524%2C1007.3706%2C1007.1639%2C1007.1924%2C1007.5447%2C1007.3303&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This is a very gentle introductory course on quantum mechanics aimed at the\nfirst years of the undergraduate level. The basic concepts are introduced, with\nmany applications and illustrations. Contains 12 short chapters of equal\nlength, ideal for a one term course. The license allows reuse of figures and\ntext under the Attribution-Noncommercial-ShareAlike conditions."}, "authors": ["Bram Gaasbeek"], "author_detail": {"name": "Bram Gaasbeek"}, "author": "Bram Gaasbeek", "arxiv_comment": "204 pages, 65 figures", "links": [{"href": "http://arxiv.org/abs/1007.4184v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1007.4184v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.atom-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.ed-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.pop-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1007.4184v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1007.4184v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:1007.4184v1 [quant-ph] 23 Jul 2010\n\nAn introduction to Quantum\nMechanics\n\n1\n\n\fContents\nWelcome!\n\n6\n\nI\n\n7\n\nThe Laws of Quantum Mechanics\n\n1 Intro: what is Quantum Mechanics?\n1.1 Unequal starts: reading guide . . . . . . . . .\n1.2 Quantum Mechanics, what's up? . . . . . . .\n1.3 How it all began... two slits . . . . . . . . . .\n1.4 Another experiment: the photoelectric effect .\n1.5 The de Broglie relations . . . . . . . . . . . .\n1.6 Conclusion . . . . . . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n8\n9\n9\n11\n13\n16\n18\n\n2 Schr\u00f6dinger's equation\n2.1 Operators and functions . . . . . . . . .\n2.1.1 An operator, what's that? . . . .\n2.1.2 Eigenfunctions and eigenvalues. .\n2.2 Wavefunctions and Schr\u00f6dinger . . . . .\n2.2.1 The wave function . . . . . . . .\n2.2.2 Why complex? . . . . . . . . . .\n2.2.3 Time evolution . . . . . . . . . .\n2.3 Using the Schr\u00f6dinger equation . . . . .\n2.3.1 The Hamiltonian . . . . . . . . .\n2.3.2 A real computation: a particle in\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\na\n\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\nbox\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n28\n29\n29\n30\n32\n32\n33\n34\n37\n37\n37\n\n3 The measurement\n3.1 Inner products and bra(c)kets . .\n3.1.1 Inner product of functions\n3.1.2 Column- and row vectors\n3.1.3 Bras and kets . . . . . . .\n3.2 The measurement . . . . . . . . .\n3.3 More precise . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n42\n43\n43\n44\n45\n46\n49\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n2\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n\f3\n\nCONTENTS\n3.3.1\n3.3.2\n3.3.3\n3.3.4\n3.3.5\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n51\n52\n54\n55\n58\n\n4 Observables\n4.1 Hermitian operators and the Hilbert space . . . . . .\n4.1.1 Hermitian operators . . . . . . . . . . . . . .\n4.1.2 A space of states . . . . . . . . . . . . . . . .\n4.2 Observables . . . . . . . . . . . . . . . . . . . . . . .\n4.2.1 Non-eigenstates . . . . . . . . . . . . . . . . .\n4.3 Working with the Hilbert space . . . . . . . . . . . .\n4.3.1 Example . . . . . . . . . . . . . . . . . . . . .\n4.3.2 Recap: the postulates of quantum mechanics.\n4.3.3 Conclusion . . . . . . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n60\n61\n61\n61\n63\n65\n66\n66\n69\n71\n\nII\n\nExample: the harmonic oscillator . .\nA neat trick . . . . . . . . . . . . . .\nSidestep: getting familiar with bras .\nThe fling . . . . . . . . . . . . . . .\nConclusion . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\nAtoms and Wave Packets\n\n73\n\n5 The hydrogen atom\n5.1 Operators in three dimensions . . . .\n5.1.1 3D sensation . . . . . . . . .\n5.1.2 Angular momentum . . . . .\n5.2 Story: cracking the mystery of atoms\n5.2.1 The end of the world . . . . .\n5.2.2 Balmer and friends . . . . . .\n5.2.3 Bohr . . . . . . . . . . . . . .\n5.2.4 A better explanation? . . . .\n5.3 The quantum approach . . . . . . .\n5.3.1 A better coordinate system .\n5.3.2 The angular equation . . . .\n5.3.3 The radial equation . . . . .\n5.3.4 Conclusion . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n74\n75\n75\n76\n77\n77\n78\n79\n81\n81\n82\n84\n85\n87\n\n6 The commutator\n6.1 Commute along, cowboy . . . .\n6.2 Asking those questions... . . . .\n6.2.1 Z-supremacy? . . . . . .\n6.2.2 CoSCO . . . . . . . . .\n6.2.3 Compatible observables\n6.3 Filling in the gaps . . . . . . .\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n92\n93\n94\n94\n95\n96\n97\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n\fCONTENTS\n\n4\n\n6.3.1\n6.3.2\n6.3.3\n\nGap 1: Changing basis . . . . . . . . . . . . . . . . . . . . . . . . . . 97\nGap 2: Another CoSCO . . . . . . . . . . . . . . . . . . . . . . . . . 99\nGap 3: The angular momentum commutation relations . . . . . . . 100\n\n7 Position and momentum basis\n7.1 Fourier's trick . . . . . . . . . . . . . . . . . . . . .\n7.1.1 Fourier series . . . . . . . . . . . . . . . . .\n7.1.2 Fourier transform . . . . . . . . . . . . . . .\n7.1.3 Delta function . . . . . . . . . . . . . . . .\n7.2 Changing habits . . . . . . . . . . . . . . . . . . .\n7.2.1 Momentum basis . . . . . . . . . . . . . . .\n7.2.2 Position basis . . . . . . . . . . . . . . . . .\n7.3 Using the position and momentum representation .\n7.3.1 Example . . . . . . . . . . . . . . . . . . . .\n7.3.2 Projection, inner product and completeness\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n103\n104\n104\n107\n109\n111\n111\n113\n115\n115\n117\n\n8 The uncertainty principle\n8.1 Fourier transform of Gaussians . . . . . . . . .\n8.1.1 Variance . . . . . . . . . . . . . . . . . .\n8.1.2 Fourier transform of Gaussians . . . . .\n8.1.3 Cauchy-Schwarz inequality . . . . . . .\n8.2 The uncertainty principle . . . . . . . . . . . .\n8.2.1 Commuting observables, revisited . . . .\n8.2.2 The Gaussian distribution . . . . . . . .\n8.2.3 The uncertainty principle of Heisenberg\n8.3 Proof and applications . . . . . . . . . . . . . .\n8.3.1 Proof . . . . . . . . . . . . . . . . . . .\n8.3.2 Applications . . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n121\n122\n122\n123\n125\n126\n126\n126\n127\n128\n128\n129\n\nIII\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\nOther Applications\n\n9 Reflection and transmission of particles\n9.1 Particles in constant potentials . . . . .\n9.1.1 Probability current . . . . . . . .\n9.2 Tunnelling and nuclear decay . . . . . .\n9.3 Particles and barriers . . . . . . . . . . .\n9.3.1 The potential step . . . . . . . .\n9.3.2 The potential barrier . . . . . . .\n9.3.3 Application: Scanning Tunnelling\n9.3.4 Application: It's warm outside! .\n\n134\n. . . . . . .\n. . . . . . .\n. . . . . . .\n. . . . . . .\n. . . . . . .\n. . . . . . .\nMicroscope\n. . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n135\n136\n137\n139\n141\n142\n147\n149\n149\n\n\f5\n\nCONTENTS\n10 Spin\n10.1 Tools . . . . . . . . . . . . . . . . . .\n10.1.1 Magnetic moment . . . . . .\n10.1.2 Larmor precession . . . . . .\n10.2 Spin . . . . . . . . . . . . . . . . . .\n10.2.1 Zeeman effect . . . . . . . . .\n10.2.2 Stern-Gerlach experiment . .\n10.2.3 The Zeeman effect, ctd. . . .\n10.3 Working with spin . . . . . . . . . .\n10.3.1 Bra's and ket's . . . . . . . .\n10.3.2 Angular momentum . . . . .\n10.3.3 Quantum Larmor precession\n11 Many particles\n11.1 Product and sum of vector spaces . .\n11.2 Many particles . . . . . . . . . . . .\n11.2.1 Direct product . . . . . . . .\n11.2.2 Identical particles . . . . . .\n11.2.3 The Pauli exclusion principle\n11.3 Applications . . . . . . . . . . . . . .\n11.3.1 The atomic structure . . . . .\n11.3.2 Application: Lasers . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n12 Metals, insulators and all that's in between\n12.1 Particle statistics . . . . . . . . . . . . . . . .\n12.1.1 Population numbers . . . . . . . . . .\n12.2 Metals, insulators and all that's in between .\n12.2.1 Conductors . . . . . . . . . . . . . . .\n12.2.2 Insulators . . . . . . . . . . . . . . . .\n12.2.3 Semiconductors . . . . . . . . . . . . .\n12.3 A simple model . . . . . . . . . . . . . . . . .\nOutro\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n152\n153\n153\n154\n155\n155\n157\n163\n164\n164\n164\n166\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n171\n. 172\n. 173\n. 174\n. 176\n. 178\n. 179\n. 179\n. 183\n\n.\n.\n.\n.\n.\n.\n.\n\n187\n. 188\n. 188\n. 190\n. 191\n. 192\n. 192\n. 194\n199\n\nBonus track: philosophical note\n200\n12.4 Quantum mystery and God's dice . . . . . . . . . . . . . . . . . . . . . . . . 200\n12.5 A jungle of interpretations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\nFigure credits\n\n204\n\n\fWelcome!\nWelcome to this course on Quantum Mechanics. It is aimed at students who have not\nhad any exposure to the subject before. Hopefully you enjoy this learning material. Extra\nmaterial can be found on the course website:\nhttp://www.bramm.be/qmcourse\n\nBackground material\nThe course assumes familiarity with basic calculus skills like differentiating, integrating,\nand complex numbers. Also, the reader should be familiar with the notion of vector spaces\nand matrices. On the physics side, it is probably useful to have knowledge of classical\nmechanics and electromagnetism. If you know these subjects, you should definitely be\nable to master this course.\n\nStyle\nThis course is written in a rather loose style. The emphasis is mainly on physical understanding and not on mathematical rigor. Also, all chapters (with exception of the first\none) have a fixed outline:\n\u2022\n\nFirst, we introduce some necessary tools. Typically some math or a short\nphysics review.\n\n\u2022\n\nAfter that, a story is told. This is the theory/history part of the chapter.\n\n\u2022\n\nFinally, we combine the two previous ingredients (tools and story) to get to the\nphysics of the chapter. Typically this involves a computation and some applications.\n\nGood luck!\nBram Gaasbeek, juli 23, 2010\n\n6\n\n\fPart I\n\nThe Laws of Quantum Mechanics\n\n7\n\n\fChapter 1\n\nIntro: what is Quantum\nMechanics?\nIn this chapter...\nIn this chapter you will learn the basic 'quantum concepts'. You may find them a bit\ndifferent from things you have learned before, but don't be afraid, they are far from incomprehensible. After reading the first few chapters, you may even see quantum mechanics\nas a quite natural description of the world on its smallest scale.\n\n8\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n1.1\n\n9\n\nUnequal starts: reading guide\n\nAs with any course, the biggest problem is the unequal start. Some students have learned\nmore than others, some less. As a consequence, every course will always start off too fast\nfor some and too slow for others.\nTo solve this problem, there are several appendices to this chapter. They contain\nthings you should know before reading this course. However, you might know quite some\nbit of it already. The descriptions below should help you decide which appendices you\nshould read, and (more important) which not.\n\u2022 Appendix 1: Complex numbers recap.\nOnly needed if your knowledge of complex numbers is poor. Review of complex\nconjugate, norm, inverse, ei\u03b8 , etcetera.\n\u2022 Appendix 2: Moving waves\nReview of traveling waves like y(x, t) = sin(kx \u2212 \u03c9t) or y(x, t) = ei(kx\u2212\u03c9t) . Needed\nif you are not familiar with the concepts 'wave amplitude', 'wave number', 'wave\nspeed' or 'complex wave'.\n\u2022 Appendix 3: Maxwell's equations.\nReviews how electromagnetism can be summarized in the four laws of Maxwell. If\nyou don't know these laws by hard but understand them, you certainly do not need\nto read this appendix.\n\u2022 Appendix 4: Light is an electromagnetic wave.\nNeeded if you never learned about this fact, or if you have never seen how the\nMaxwell equations imply the existence of waves.\n\u2022 Appendix 5: What is interference?\nNeeded if you do not know the calculation of interference of (light)waves falling\nthrough two narrow slits.\nIf you know all of this, congratulations; forget about those appendices and read on. If\nthere are some things you don't know yet, take your time to read them carefully - they\nwill help making sense of what follows. Again: restrict your attention to what you don't\nknow - ignore the rest. After catching up, return to this point.\n\n1.2\n\nQuantum Mechanics, what's up?\n\nAs a warmup, some quick questions and answers about Quantum Mechanics...\nWhy would I learn Quantum mechanics?\nLet's think a bit about the physics you have learned so far. Throughout the past years,\nyou got to know how bodies experience and exert forces, how gases behave, how electromagnetic forces act and all that. On top of that, you learned that molecules constitute all\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n10\n\nthe matter we see in daily life. These molecules are arranged in gases, solids and liquids,\nmixed up in one big soup - the world around us. Better even, those molecules are made\nof atoms, and each single atom is a cloud of electrons flying around a nucleus of protons\nand neutrons. That's pretty impressive. Indeed, this insight explains almost the entire\nreality around us in terms of a very small set of particles: electrons, protons and neutrons.\nHowever impressive and unifying this fact may be, it does leave us with some very real\nquestions. Questions that you -most likely- have not gotten an answer to yet. Why do\nthose particles stick together in atoms? Why do they sit together very very close, but not\non top of each other? And what are these particles really? Just points? Or do they have\nsome size? How do they behave? And so on. Precisely these questions are answered by\nquantum mechanics.\nWhat will I learn in this course?\nSo, in this course, you will learn what those particles are, really. This means we will\ndescribe reality on the smallest scale. This description will be different from the physics\nyou have learned so far. That does not mean one or the other is wrong. They are just\ndescriptions of different physical situations. If you want to describe the dynamics of\nindividual particles (or just a few of them) then you have to use quantum mechanics. For\ngases, solids, and all that, we don't use this description: it would be both impractical,\nand impossible. Indeed: how could we describe a gas by mathematically keeping track of\nall particles of such a system at the same time? We can't, that's why -for larger systemswe use gas laws, heat laws, Newton's laws, and so on. Those laws are very accurate for\nmacroscopic systems, but one should always bear in mind that they actually follow from\nthe dynamics and physical laws that govern the microscopic world - that of particles. That\nis what makes Quantum Mechanics (short: 'QM') - the fundamental theory of microscopic\nsystems - so interesting.\nIs QM difficult?\nQM may be more difficult than the physics you have learned up to this point, but definitely\nnot 'impossible to understand' or 'comprehensible to only few'. You may wonder why it\nis more difficult. Why do we understand -for example- gas laws more easily then the\nquantum mechanical laws that describe fundamental particles? Actually, there is a simple\nanswer to that question. Gas laws deal with systems of our scale, of our daily life. We\nhave quite some feel for it, even before studying physics. You were not really shocked when\nlearning that the pressure of a gas rises when you compress it, right? This is just because\nyou have experienced this law many times (f.e. when you inflate a bike tire) without really\nrealizing it. However, none of us 'feel' or 'see' fundamental particles in ordinary daily life.\nIf we did, the laws of quantum mechanics would feel very natural. You would just say 'ah\nyes, thats indeed what it looks like' or 'ah, yes, a particle indeed moves like that' - in the\nsame way some people react on gas laws. Just keep in mind that our gas laws (or any\nother macroscopic physics) would look equally strange and funny for -say- some exotic\nsubmicroscopic life form only used to the laws of QM.\nHow sure are we about QM?\nOr better even: how did we ever discover the laws of quantum mechanics? Like any other\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n11\n\nbranch of science: by doing experiments and thinking about the results. You may know\nthat the basic laws of quantum mechanics were discovered about a hundred years ago.\nPrehistory, that is, in terms of technology and science. Since their discovery, the laws of\nquantum mechanics have been confirmed by numerous experiments, often with a higher\nprecision than anywhere else. So, it is no exaggeration to state that quantum mechanics\nis the best-tested theory of physics. On top of that, the number of inventions directly\nusing quantum mechanics is significantly bigger than most people think, and very likely\nto keep on growing in the very near future. In conclusion, despite its beauty and exotic\ncharacter, quantum mechanics is not at all 'hypothetical' or 'merely philosophical'. It is\nan accurate, tested and elegant description of the smallest and most fundamental part of\nthe reality around us, that of elementary particles.\n\n1.3\n\nHow it all began... two slits\n\nQuantum mechanics was born together with a series of remarkable experiments. One of\nthem was the so-called two-slit experiment1 . The first version of this experiment was\nactually carried out by Thomas Young in 1803. Some people back then were arguing\nabout the true nature of light. In our daily life, we see that light travels on straight lines\n- you can't look around a street corner, right? This suggests light might be made up of\ntiny particles or 'bullets' flying through space. However, some people -like Mr. Youngwere convinced light has some wave aspect to it. Indeed, as you learned in your course on\nelectromechanics2 , you know that light is a wave in the electric and magnetic field. So to\ndescribe it, you need to give the distortion in the E and B field, depending on space and\ntime. So you need to give E(~x, t) and B(~x, t) to describe a traveling piece of light. This\ncontrasts with a particle, which can be described by only its position depending on time:\n~x(t). Anyhow, Mr. Young did not know all the fancy formulas of Maxwell, so how did he\nfind out light is a traveling wave, not a traveling particle?\nSo, how did he find out? \u2013 The two-slit experiment.\nHe set up the following experiment: take a light source, a thin wall and a projection screen,\nas in Figure 1.1. If you cut -for example- the shape of a cat away from the wall, and shine\non it, you see a bright shape on the projection screen, in the shape of a cat. Now, what\nhappens if instead two very narrow slits are drilled, on a small distance of each other?\nNaively, you expect two very narrow bright stripes, on a small distance of each other.\nWell, strange enough, that's not what you see. Instead, you see an alternating series of\nbright and dark bands. Those bands are precisely what you see when waves create an\ninterference pattern. So Young could do nothing but conclude that indeed, light must be\na wave, even though he didn't know what kind of wave. Nice, huh?\n1\n2\n\nwhich you either learned about before, or have read about in Appendix 5\nor else in Appendix 4\n\n\f12\n\nCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\nBut what about that cat shaped hole then? Well, for larger holes, the interference\npattern becomes much more complicated. It is still there, really, but you barely see it.\nMoreover, it turns out that for larger holes, the spot on the screen gets the same form\nas the hole. From this, it looks like light just consists of 'bullets' passing through, even\nthough that's not really true. Got it?\nwhat you EXPECT:\n\nlight source\n\nlight source\n\nwhat you OBSERVE:\n\nlight source\n\nFigure 1.1: If you shine with a light source on a hole with the shape of a cat, light will\nimpact along a cat-shaped region (left). Logically, you would guess that two very narrow\nslits lead to two vary narrow bands of light impacts (middle). Striking enough, what you\nobserve is a whole series of bands (right). The conclusion is inevitable: light must be a\nwave.\n\nThe surprise\nAnyhow, nowadays we understand Young's experiment very well, since now we know\nprecisely what kind of wave light is: an electromagnetic wave. But here comes the surprise.\nLet's do the two-slit experiment again, but now for electrons. More precise, tell your\nengineering friend to build a nice electron-source: an apparatus shooting electrons. He'll\nreadily do that for you. Of course, as we can't see flying electrons with our eyes, we have\nto replace the projection screen by a set of detectors, which can measure the electrons\narriving. So the setup is exactly that of Figure 1.1, but with an electron source and a\ndetecting screen instead of a light source and a projection screen. When sending electrons\nthrough the cat-shaped hole, the detectors will tell you that a cat-shaped region is under\nfire. Ah, ok, that's what you expect. Now do as Young did for light: cut out two narrow\nslits. What do the detectors tell you now? Let's think about it. Electrons really are\nparticles (so imagine flying 'bullets') so two slits should result in just two bands where\nelectrons are detected. However, that is not what happens. Precisely like for light, you\nsee an alternating series of bands. On one band, there is much detection, then on the next\nthere is few to none, and so on. Even if lower the intensity of the source (so that there\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n13\n\nis more time between the electrons, and you are sure they fly at the slits one by one, not\nfeeling the previous or next one) the result is the same. Bands, not just two stripes.\nInterpretation\nThere is only one conclusion we can draw. Each electron has to be a wave. Well, more\nprecise, it is just a small piece of wave: a wavepacket. To a brutal and large observer like\nus, that small packet looks like a point particle in most experiments. However, it does have\na non-zero size. It is a very small blob. And when each such wave packet goes through\nthe two slits, it interferes with itself. (The part of the wave packet that went through one\nslit interferes with the part that went through the other slit.) So the wavelike nature of\nthe electron is only made visible because of the specific setup of the two-slit experiment.\nIn most other experiments, we don't notice, and the electron just looks like a point, even\nif, in fact, it is spread out over a small region. Now, if you repeat the experiment for\nneutrons and protons, you find precisely the same result. So they are all wave packets.\nPeople like to call this the wave aspect of particles.\n\n1.4\n\nAnother experiment: the photoelectric effect\n\nInitially, the two-slit experiment completely stunned physicists. But the origin of QM lies\nat several experiments and puzzles that (roughly) arose around the beginning of the 20th\ncentury. One of them was the photoelectric effect. To see this effect, do the following.\nTake two pieces of metal. Take a direct current source, and connect it two the pieces of\nmetal, as shown in Figure 1.2. Since the metal pieces are not connected, no current can\nflow through the circuit. That is indeed what you see experimentally (your ammeter will\nindicate the current is zero). Well, wait: now do the following. Take a light source, of\nwhich you can adjust the color (the frequency). Since you can set it to shine at only one\ncolor, you call such a source monochromatic, which is Greek for one-color. Start with\nvery red (low frequency) light, and shine on the negatively charged piece of metal (see\nfigure). You still don't see anything special. Now if you increase the frequency of the light\n(go from red to yellow to green, towards blue light) there is suddenly some frequency/color\nat which you detect a current running. What? The only explanation is that the light strips\noff electrons from the metal; those fly through the air towards the (attracting) positive\npole, making you detect a current.\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n14\n\nlight\n+\n\n-\n\nsource\n\nammeter\n\nFigure 1.2: A setup showing the photoelectric effect. A source is connected to two pieces\nof metal. They get charged (electrons are displaced) but no net current flows through the\ncircuit. When light shines on the negatively charged metal piece, things change. At least:\nif the light has a high enough frequency, it strips electrons out of the metal, so they can\ncross the gap, and reach the other side. This will cause a current to flow, as detected by\nthe ammeter. On the other hand, if the frequency is too low, nothing happens, not even\nwhen one drastically increases the intensity of the light - the current stays exactly zero.\n\nNow scientists partly understood this experiment. They knew light (or EM radiation\nin general) is a form of energy, so it is not so strange that it can strip off electrons from\na metal. The strange thing, however, is that it only starts doing so beyond a certain\nfrequency (which depends only on the type of metal you are using). If the frequency\nis too low, there is zero current, regardless of the intensity! So if you are below that\ncritical frequency, no matter how much light you shine on the metal, you don't detect a\ncurrent. Once above the critical frequency (which depends on the metal), the current is\nproportional to the applied intensity, so the amount of electrons ripped off grows with the\namount of radiation you shine on the metal. So what is going on here?\n\nUnderstanding the photoelectric effect\nThis peculiar behavior remained unexplained for quite some while. Then, Albert Einstein\nproposed a simple explanation. Imagine that a ray of light consists of very many separate\npieces. Let's call these fundamental bits of light photons. If you shine on the metal, all\nthese photons bump into the atomic grid of the metal, one by one. Now on top of that,\nassume each photon has an energy proportional to its frequency. So\nE = h\u03bd\n\n(1.1)\n\nwhere E is the energy of the photon, and \u03bd is its frequency (color). The factor of proportionality between the two, h is called Planck's constant. Ok, suppose this is true.\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n15\n\nEinstein then reasoned further as follows. Say there is some energy \u03a6 needed to strip off\none electron. Then if the individual photons have an energy E that is smaller than \u03a6, a\nsingle photon cannot strip off one electron. And as they arrive one by one (however fast\nthat may be) each of them has insufficient energy to kick an electron out. So nothing\nhappens. Only if the individual photons have an energy E greater than \u03a6, each of them\ncan strip off one electron. In this case, the result is obvious: the more photons you send,\nthe more electrons are released by the metal. Said differently: a higher intensity of light\nmeans more photons per second, so more electrons released, and a bigger current detected.\nA popular comparison is the 'ball and the fence'-story. Imagine you want to kick a ball\nover a fence. If you kick too soft, it will not go over. Even if you repeat this, the result\nwill be rather poor: the ball will just bounce back over and over again. However, if you\nkick hard enough, it will fly over the fence at once.\nAs you see: the explanation of the photoelectric effect is not particularly difficult. To\nunderstand it, you don't need to be an Einstein at all. Note that the above provides more\nthan just an explanation. It also gives a prediction. Since an electron needs an energy \u03a6\nto be kicked out (this energy is actually called the work function), and since the photons\ngives an energy h\u03bd, the kinetic energy K of the electron after being kicked loose is given\nby:\nK = h\u03bd \u2212 \u03a6\n(1.2)\nSo the explanation of Einstein predicts a simple linear relation between the kinetic energy\nof the released electrons and the frequency of the light falling on the metal. With a\nslightly more careful setup one can measure the kinetic energy of the released electrons\n(by letting them cross a potential difference). And indeed, their kinetic energy precisely\nobeys the above relation. This result (and many other experiments) confirm that the\nphoton-explanation has to be correct.\nInterpretation\nSo what does the photoelectric effect tell us? Light must be made up of many individual\nparticles, each with its own energy. They are really separate objects. How can we reconcile\nthis with the knowledge that light is a wave? Each photon has to be a little piece of the\nelectromagnetic wave. If you put many of these 'elementary pieces of wave' together, it just\nlooks like one big wave. So the particle-like behavior of light is only made visible because of\nthe specific setup of the photoelectric effect. People like to call this the particle aspect\nof light.\n\nPutting the pieces together\nThe experiments we have just seen force us to think very differently about particles and\nradiation. Before the advent of quantum mechanics, people thought there was a very clear\ndifference between radiation and particles. They thought particles didn't have any size\nand should be described by their position as a function of time. They thought radiation\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n16\n\nis a disturbance of the background, and should to be described by an amplitude (of the\nelectric and magnetic fields) as a function of place and time. Quantum mechanics learns\nus that this is not correct. Instead, everything we see around us is built of wave packets.\nIf you see one such packet, as it tends to be very small, it may look like just a point of\nzero size. For radiation, if you see many of these wave packets at once, it looks like just\none big wave, without obvious constituents - even if they really are still there. The idea\nthat everything is built of wavepackets is really the core idea of quantum mechanics. We\ncan actually summarize this entire chapter by one single drawing:\nwave\n\nwavepacket\n\nall\nry sm\ne\nv\nis a\nparticle\n\nThis picture will be made more quantitative in the next section.\n\n1.5\n\nThe de Broglie relations\n\nFrom the photoelectric effect, we know that photons have an energy proportional to the\nfrequency. This is a quantitative expression of the particle aspect of light. There is also a\nformula expressing the wave aspect of particles. Here is a way to obtain it experimentally.\nYou know (or have read in Appendix 5) that a two-slit interference experiment yields\nbright bands (a lot of detection) at positions x on the screen satisfying\nx=\n\nn\u03bbL\nd\n\n(1.3)\n\nSo from the a double-slit experiment, you can deduce the wavelength of the waves from\nthe positions x of constructive interference. (Given that you also know L and d.) So if\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n17\n\nyou do the double-slit experiment with particles, you can measure the wavelength of these\ntiny wave packets. It turns out that the following holds:\n\u03bb=\n\nh\np\n\n(1.4)\n\nwhere p is the momentum of the particles falling through the slits (their mass times their\nspeed) and h is again the Planck constant. In conclusion: the photoelectric effect and the\ndouble slit experiment show that\n\u03bb=\n\nh\np\n\nand f =\n\nE\nh\n\n(1.5)\n\nThese equations are called the de Broglie relations.3 The first relation gives the wavelength associated to the wave packet of a particle. The second relation gives the energy\nof a photon in relation to its frequency. But actually, both relations hold for photons and\nfor particles. So you can read the de Broglie relations in two ways. First, for a particle\nwith energy E and momentum p, they give the wavelength and the frequency of the wave\npacket of the particle. Second, for a photon with frequency f and wavelength \u03bb, they\ngive the energy and momentum of the photon. So a photon does not only have a certain\namount of energy, it also has a tiny bit of momentum. Again, this puts particles and\nwaves on a very symmetric footing.\nWe can also rewrite the above two equations a bit. Recall that the wavelength of a\npiece of wave is related to the wavenumber k by the relation \u03bb = 2\u03c0\nk . Also, recall that the\nangular frequency \u03c9 of a wave is defined as \u03c9 = 2\u03c0f . Then the above two equations\ncan be written as\np = ~k and E = ~\u03c9\n(1.6)\nwith ~ =\n~ are:4\n\nh\n2\u03c0\n\nis called the reduced Planck constant. The experimental values of h and\nh = 6.626 * 10\u221234 J*s = 4.136 * 10\u221215 eV*s\nh\n= 1.055 * 10\u221234 J*s = 6.582 * 10\u221216 eV*s\n~=\n2\u03c0\n\n(1.7)\n(1.8)\n\nThis constant is not just small, it's tiny - that's one of the reasons why the quantumbehavior of particles is only manifest in very precise and specific experiments.\n\n3\n\nThe pronunciation of 'de Broglie' is somewhere in between 'duh Broy' and 'duh Brey'.\nTo see the conversion between units, recall that an electron volt (eV) is the energy you get from letting\nan electron cross a potential difference of one volt, so 1eV = qe (1V ) = (1, 6 * 10\u221219 C)(1V ) = 1, 6 * 10\u221219 J\n4\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n1.6\n\n18\n\nConclusion\n\nThe main conclusion of the chapter is the fact that the fundamental building block of\nnature is a wavepacket (a lump of wave). This is quite nice, since it rids us of the classical\nidea that nature consists of two separate building blocks (particles and waves). Whether\nwe are talking about a photon, an electron, a proton, ... all these are a wave packet. Since\nand during the discovery of quantum mechanics, this fact has been proven again and again\nin experiments. Most textbooks mention not only the two experiments above, but several\nothers. Many of those were equally important in the development of quantum mechanics.\nWe have omitted those here, as they all lead to the same conclusion. If you are interested,\ngo ahead and look them up. If you got the point though, maybe you don't really need to.\n\nExcercises\n1. Explain to yourself (or better: to someone else) what the conclusion of this chapter\nwas, and how it logically follows from the discussed experiments.\n2. What is the momentum carried by a red photon? (Red light has a frequency f =\n450 * 1012 Hz.) How many such photons do you need to obtain a momentum of\n1kg*m/s (=a decently served tennis ball)?\n3. For every wave, \u03bbf = v with \u03bb the wavelength, f the frequency and v the wave\nspeed. Using that the speed of light is c = 3 * 108 m/s, what is the wavelength of a\nred photon?\n4. You are doing the double-slit experiment with electrons. The distance between the\nslits is 1 cm, and the impact screen is at 1 meter from the slits. What speed should\nthe electrons have in order to obtain an interference pattern with the bright bands\nseparated by 0.1 m? (The mass of an electron is me = 9.11 * 10\u221231 kg.)\n5. The work function \u03a6 of silver is about 4.6 eV. What frequency do photons need to\nhave in order to strip off electrons from a chunk of silver? Explain what happens at\nlower frequencies. What happens at higher frequencies? What is the kinetic energy\nof an electron stripped off by a photon of frequency 2 * 1015 Hz?\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n19\n\nAppendix 1: Complex numbers recap\nAs you probably know, a complex number is a number of the form a + bi where a and b\nare real numbers, and i is the imaginary unit (sometimes denoted I or j) which satisfies\ni2 = \u22121. In such a decomposition, a is called the real part of z, and b is the imaginary\npart:\na = Re z b = Im z\n(1.9)\nThe product and sum of two complex numbers a + bi and c + di are given by:\n(a + bi) + (c + di) = (a + b) + (c + d)i\n\n(1.10)\n2\n\n(a + bi) . (c + di) = ac + adi + bci + bdi = (ac \u2212 bd) + (ad + bc)i\n\n(1.11)\n\nThe complex conjugate of a complex number z = a + bi is given by z\u0304 = a \u2212 bi. With\nthis, the real and imaginary part of a complex number can be written as\nz \u2212 z\u0304\n(1.12)\n2i\n\u221a\n\u221a\nThe norm of a complex number is defined by |z| = zz = a2 + b2 . Also, each complex\nnumber can be written in the form z = |z|ei\u03b8 with\nRez =\n\nz + z\u0304\n2\n\nImz =\n\nei\u03b8 = cos \u03b8 + i sin \u03b8\n\n(1.13)\n\nThe angle \u03b8 is called the argument of z.\n\u03b8 = arg z\n\n(1.14)\n\nAll these quantities can be shown very graphically, as shown\nin the figure on the side.\nFor computations, there are some important properties\nthat will show up again and again throughout this course.\nFirst, the inverse z \u22121 of a complex number (defined by requiring z \u22121 z = 1) is given by\nz \u22121 =\n\nz\u0304\n|z|\n\n(1.15)\n\nAlso, the conjugate of the sum and the product of two complex numbers z1 and z2 are given by:\nz1 + z2 = z\u03041 + z\u03042\n\nz1 z2 = z\u03041 z\u03042 .\n\n(1.16)\n\nSimilarly, the norm of the product of two complex numbers is given by\n|z1 z2 | = |z1 ||z2 |.\n\n(1.17)\n\nHowever, for the norm of the sum, one needs to be careful, since |z1 + z2 | =\n6 |z1 | + |z2 |. If\nyou have forgotten some of these things, you could do the following short exercises:\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n20\n\n\u2022 Show that the norm of z = |z|ei\u03b8 really is |z|. (Write out real and imaginary parts\nexplicitly, sum the squares.)\n\u2022 Check that the expression for the inverse z \u22121 of a complex number indeed implies\nzz \u22121 = 1.\n\u2022 Show the properties (1.16) and (1.17) listed above.\n\u2022 Use these properties and (1.15) to show that (z1 z2 )\u22121 = z1\u22121 z2\u22121 .\n\u2022 Find an example of two complex numbers, such that |z1 + z2 | =\n6 |z1 | + |z2 |. (You\ncan even find examples in the real numbers, which are just a subset of the complex\nnumbers.)\n\nAppendix 2: Moving waves\nWaves are all around us. Waves on water, sound, light, ... . In general , one can define\na wave as a disturbance, throughout some medium. What this disturbance is, can\ndiffer significantly: for water waves, it is a disturbance in the height of the surface, for\nsound it is a disturbance in the pressure of the air, for light (see Appendix 4) it is a\ndisturbance in the electric and magnetic fields. If we denote the quantity that is disturbed\nby y then a wave is described by a function y(x, t). Indeed, the disturbance typically\ndepends on the position x, and changes throughout the time t. 5 A nice feature is\nthat for very, very many systems, 'small' waves (small disturbances) have the shape of\na sine-function.6 Superposing such sine-waves (=taking the sum, the total disturbance)\nthen gives a large class of possible disturbances traveling through space. So our central\nquestion here is: How can we describe such a sine-shaped wave? Well, such a wave\ncan always be put into the form\ny(x, t) = A sin(kx \u2212 \u03c9t + \u03c6)\n\n(1.18)\n\nLet us dissect this expression. The right hand side indeed depends on the position x and\ntime t. The other variables A, k, \u03c9, \u03c6 are constants. Since A multiplies the sine-function,\nit says how large the disturbance is, so A is called the amplitude of the wave. To see the\nmeaning of the other constants, first note that at time t = 0, the wave looks like\ny(x, t = 0) = A sin(kx + \u03c6)\n\n(1.19)\n\n5\nPossibly, the position is described by more than just one coordinate. You need two coordinates for a\nwave on the 2 dimensional surface of water, 3 for a sound wave moving through a 3 dimensional room,\netcetera. For simplicity, we consider only one coordinate x here. To avoid confusion, we stress again that\ny denotes the quantity being disturbed, it is not a spatial coordinate like x.\n6\nOr a cosine function, which is of course just the same shape, but shifted.\n\n\f21\n\nCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\nThis is a sine-function indeed. The distance between two peaks is called the wavelength\n\u03bb, and is given by\n2\u03c0\n\u03bb=\n.\n(1.20)\nk\nTo check this, just note that if there is a peak at x1 , there will also be a peak at x1 + \u03bb,\nsince\ny(x1 + \u03bb, 0) = A sin(kx1 + 2\u03c0 + \u03c6) = A sin(kx1 + \u03c6) = y(x1 , 0).\n(1.21)\nThe object k itself is called the wave number. What about \u03c6? This constant determines\nhow far the sine-function (1.19) is shifted to the left at t = 0. It is called the phase of the\nwave. Now let us take a different point of view. What if we stay put at x = 0, and watch\nhow the disturbance at this specific position changes throughout time. Clearly, it is given\nby the function\ny(x = 0, t) = A sin(\u2212\u03c9t + \u03c6)\n(1.22)\nThis looks very similar: again it is a harmonically oscillating behavior. Also, the disturbance is periodic: it repeats itself throughout time. Indeed, the disturbance at t and t + 2\u03c0\n\u03c9\nare the same:\ny(0, t + 2\u03c0/\u03c9) = A sin(\u2212\u03c9t + 2\u03c0 + \u03c6) = A sin(\u2212\u03c9t + \u03c6) = y(0, t).\n\n(1.23)\n\nThe time T = 2\u03c0\n\u03c9 is called the period of the wave. The number \u03c9 is called the (angular)\nfrequency. Here is what happens when we look at the entire wave, evolving through time.\nIt starts of like the wave (1.19) at t = 0, and then it runs along the x-axis (maintaining its\nshape) with a constant speed v. To see what the speed is, you can use the following trick.\nTo follow the wave on its movement, you should sit at one point (a peak for instance) and\nride along. To ride along means you always see the same disturbance along your way. So\nyou want to see y(x, t) to be constant, which is realized by kx \u2212 \u03c9t = constant. Reworking\nthis expression, you get\nx=\n\nconstant \u03c9\nconstant + \u03c9t\n=\n+ t\nk\nk\nk\n\nThis means you have to move at speed\n\n\u03c9\nk.\n\nSo the wave speed is given by v =\n\n(1.24)\n\u03c9\nk.\n\nComplex waves\nIn quite some fields of physics, it turns out that the propagating waves in a system are\nmost naturally described by a complex wave. This does not mean something wacky is\ngoing on, it just means this gives a convenient description, with simple equations etcetera.\nFor example, in electrical circuits it can be handy to describe an alternating current by a\ncomplex wave. Maybe you have once seen this trick, and realized that such a description\nsaves a lot of ugly trigoniometric manipulations that would be needed when working with\nordinary (real) functions. However, if you really wanted to, you could do so, meaning that\nnothing strange is going on. It is just a matter of choice. Real functions are easier to\n\n\f22\n\nCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\ny\n\nt=0 t=\u03b4t t=2\u03b4t\nv\nx\n\u03bb\n\nFigure 1.3: A moving wave. The displacement y(x, t) is shown for times t = 0, t = \u03b4t and\nt = 2\u03b4t. Clearly, the wave is moving to the right with increasing time. The wave speed is\ngiven by v = \u03c9k\n\nunderstand, but complex functions save a lot of work. Anyway, how would you describe\nsuch a complex wave? Well, very similar to the above: by a function\ny(x, t) = Aei(kx\u2212\u03c9t+\u03c6)\n\n(1.25)\n\nIm(f)\n\nx\nRe(f)\nFigure 1.4: A graphical representation of a complex wave z = eix . The real and imaginary\nparts of z are shown separately. The total figure has a helix shape.\nActually, if we allow A to be a complex number, we can absorb the phase ei\u03c6 in it.\nSuch a wave can be visualized as in Figure 1.4. Note that the real and imaginary parts\nare just ordinary waves of the type we just discussed. Again, k is the wavenumber, \u03c9 the\n(angular) frequency. Can we also describe a complex wave in three dimensions? Sure, this\nwould be given by\n~\n\nY (x, y, z, t) = Aei(kx x+ky y+kz z\u2212\u03c9t+\u03c6) = Aei(k~x\u2212\u03c9t+\u03c6)\n\n(1.26)\n\n(We capitalized the name of the wave to avoid confusion.) In this case, the vector ~k =\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n23\n\n(kx , ky , kz ) is called the wave vector.\n\u221a 2 It 2can2 be shown that such a wave travels in the\n~k|\nkx +ky +kz\n|\n.\ndirection of ~k, with speed v = \u03c9 =\n\u03c9\nTo conclude, a remark about units. It is often stated that functions like the sine and\nexponential function can only have dimensionless arguments. Since x and t have units\nof length and time (meters and seconds), k and \u03c9 necessarily have units m\u22121 and s\u22121 .\nWith this, you can verify that the period T and wavelength \u03bb of a wave (with the above\nexpressions) then have units time and length, as you would expect.\n\nAppendix 3: Maxwell's equations\nYou have probably seen the laws of electromagnetism several times throughout your life.\nThey just happen to be very important to a lot of interesting physical phenomena. Quite\nsome physics books phrase these laws in terms of physical situations. Like: 'the magnetic\nfield around a conductor carrying a current, is given by ...' or 'a changing magnetic\nflux will create an electric field inside the conductor, given by...'. These phrases are very\nimportant, but are not the most elegant description, since they require a lot of words and\nexplanation. A quite nice fact is that all these laws can be summarized in a very compact\nway: only four equations. They are called Maxwell's equations, although they are only a\ndifferent version of the laws discovered by others before him. They look as follows:\n~ *E\n~ =\n\u2022 \u2207\n\n\u03c1\n\u03b50\n\n(Gauss's law)\n\n~ *B\n~ = 0 (Gauss's law for magnetism)\n\u2022 \u2207\n~ \u00d7E\n~ = \u2212 \u2202 B~ (Faraday's law)\n\u2022 \u2207\n\u2202t\n~ \u00d7B\n~ = \u03bc0 J~ + \u03bc0 \u03b50 \u2202 E~ (Amp\u00e8re's law)\n\u2022 \u2207\n\u2202t\n~ and B\n~ are the electric and magnetic fields, \u03c1 is the charge density, and J~ the\nHere, E\nelectrical current. The constants \u03b50 and \u03bc0 are the electrical permittivity, and the magnetic\n~ is a vector of derivatives: \u2207\n~ = ( d , d , d ). So for example\npermeability. The object \u2207\ndx dy dz\n~E\n~ = dEx + dEy + dEz . So what do these equations say? First of all, note that the relevant\n\u2207\ndx\n\ndy\n\ndz\n\n~ B,\n~ \u03c1, J~) depend on the position ~x and time. So they are local equations:\nvariables (E,\nthey say how all involved quantities are related at every position and time. Also, the\n~ B:\n~ the equations of Maxwell are four differential\nequations involve derivatives of E,\nequations. Sometimes, people say the above expressions are the 'differential' form of the\nlaws of electromagnetism. It might not be immediately clear how to extract real physics\n(say, the force between two conductors carrying a current) from them, but sure they are\nvery compact and elegant! That's why people like them so much. A very important\nconsequence that can be extracted from them, is described in the following appendix.\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n24\n\nAppendix 4: Light is an electromagnetic wave\nA question Maxwell asked himself, was the following. What happens if there are no charges\nor currents present? Could something interesting happen then? In that case (\u03c1 = 0, ~j = 0)\nthe four equations become:\n~\n\u2207*E\n\n=\n\n~\n\u2207\u00d7E\n\n=\n\n~\n\u2207*B\n\n=\n\n~\n\u2207\u00d7B\n\n=\n\n0\n\n(1.27)\n\n~\n\u2202B\n\u2212\n\u2202t\n0\n\n(1.28)\n(1.29)\n\n~\n\u2202E\n\u03bc0 \u03b50\n\u2202t\n\n(1.30)\n\nTaking the curl (\u2207\u00d7) of the second and fourth equation and then using them again gives:\n~\n\u2207\u00d7\u2207\u00d7E\n\n~\n\u2207\u00d7\u2207\u00d7B\n\n~\n\u2202\n~ = \u2212\u03bc0 \u03b50 \u2202 2 E\n= \u2212 \u2202t\n\u2207\u00d7B\n\u2202t2\n\n=\n\n\u2202\n\u03bc0 \u03b50 \u2202t\n\u2207\n\n~ =\n\u00d7E\n\n2~\n\u2212\u03bco \u03b5o \u2202\u2202tB\n2\n\nAfter this, using the vector identity\n\u0010\n\u0011\n\u0010\n\u0011\n~ = \u2207 \u2207*V\n~ \u2212 \u22072 V\n~\n\u2207\u00d7 \u2207\u00d7V\n\n(1.31)\n(1.32)\n\n(1.33)\n\n~ (~x)) and the first and third equation (\u2207E\n~ = \u2207B\n~ = 0)\n(valid for every vector function V\nwe get\n~\n\u22022E\n~\n\u2212 c2 * \u22072 E\n\u2202t2\n~\n\u22022B\n~\n\u2212 c2 * \u22072 B\n\u2202t2\n\n= 0\n\n(1.34)\n\n= 0\n\n(1.35)\n\nwhere c = \u221a\u03bc10 \u03b50 . You may recognize that these equations are standard wave equations.\nThat means they have solutions with the shape of a sine-function, like sin(kx \u2212 \u03c9t). For\nexample, if one takes the electric field to be\n~ = (0, E0 sin(kx \u2212 \u03c9t), 0).\nE\n\n(1.36)\n\nThen the x- and z- component of (1.34) are trivially solved, and the y-component becomes:\n\u03c9 2 \u2212 c2 k2 = 0\n\n(1.37)\n\nwhich can be satisfied by taking \u03c9 = ck. From (1.28) and (1.30) we see that we should\ntake\n~ = (0, 0, B0 sin(kx \u2212 \u03c9t))\nB\n(1.38)\n\n\f25\n\nCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n\u03bb\nE\nk\nB\nFigure 1.5: This figure shows an electromagnetic wave, with the electric and magnetic\nfields along the x-axis shown at a specific instant in time. If we let time run, the wave\nmoves ('shifts') to the right.\nwith B0 = Ec0 . (Just check by plugging in that this works.) Clearly, the above also\nsatisfies the wave equation (1.34). So what does this look like? This solution to the\nMaxwell equations is drawn in Figure 1.5. It is wave of oscillating electric and magnetic\nfields, traveling through space in the x-direction. Note that E and B are perpendicular to\nthe direction of motion. We can intuitively understand why such a wave is possible. From\nelectromagnetism, we know that a changing magnetic field induces a changing electric\nfield, and vice versa. So the above wave precisely embodies this principle - even without\nany charges or currents being present. So the equations of Maxwell imply the existence of\nelectromagnetic waves. That's nice, but there is more. The speed of this wave is given by\nv=\n\n\u03c9\n=c\nk\n\n(1.39)\n\nand the numerical value of c is given by plugging in the values of the permittivity and\npermeability:\n1\nc= \u221a\n= 2, 998 \u00d7 108 m/s\n(1.40)\n\u03bc0 \u03b50\nThat's precisely the numerical value of the speed of light! From this, Maxwell concluded\nthat light has to be a form of electromagnetic radiation. A very neat and surprising\nconclusion, and - better even - very right. Today we know that light is only one form\nof electromagnetic radiation. Microwaves, radio waves, X-rays,... are all electromagnetic\nwaves, with the single difference of comprising waves of different frequency regions.\n\nAppendix 5: What is interference?\nAn important property of waves is that they can be put 'on top of each other'. Such a sum\nof two waves is called a superposition. This can be seen in water waves for example. If a\nduck on water creates waves y1 (x, t) and another duck makes waves y2 (x, t) then the total\ndisturbance pattern they create is given by y1 (x, t) + y2 (x, t). A very interesting thing\n\n\fCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\n26\n\nthat occurs when two disturbances are superposed is interference. It could be that at\nsome position and time, y1 and y2 are exactly opposite. This means the total disturbance\nis zero, even if y1 and y2 individually would have been nonzero. Such a situation is called\ndestructive interference. Similarly, if y1 and y2 have the same sign, the total wave will\nbe larger than the individual disturbances. This is called constructive interference. You\ncan easily reproduce these patterns yourself - by throwing two stones into an undisturbed\npond for example. An experiment that makes constructive and destructive interference\nparticularly clear is the double-slit experiment, shown in Figure 1.6. In this case,\nincoming waves fall on two narrow slits/holes. At the other side of the slits, the waves\ncome out. In that region, the waves emerging from the two slits interfere. A question\nworth asking is what the total wave pattern will look like on a wall at some distance of the\nslits. For ease, we suppose the waves are of a single wavelength \u03bb. Also, suppose the wall\nis at a distance L >> \u03bb of the slits. Since the waves start out the same way at the slits,\nthey must have the same phase there. This means that the disturbances at the two slits\nshow maximum positive displacement (a crest) and maximal negative displacement (a\ntrough) at the same moments in time. More precise, the wave amplitude y(x, t) satisfies:\ny(slit 1, t) = y(slit 2, t)\n\n\u2200t\n\n(1.41)\n\nConsider the pieces of wave traveling upwards, towards a position x on the screen. Note\nthat the distance the two pieces of wave have to travel is not the same. This is called the\npath difference. From 1.6 it is clear that the path difference \u2206 is given by\n\u2206 = d sin \u03b8\n\n(1.42)\n\nHere is the crux of the story: if the path difference is a multiple of \u03bb, the two waves will\nhave the same phase at x, so they will interfere constructively. If the path difference is\n\u03bb/2 plus a multiple of \u03bb, the wave will be exactly opposites when arriving at x. So in that\ncase, destructive interference will occur. So\n\u2206=\n\u2206=\n\nn\u03bb (n integer)\n\u0001\nn + 21 \u03bb (n integer)\n\n\u21d2 constructive interference at x\n\n\u21d2 destructive interference at x\n\n(1.43)\n(1.44)\n\nSo combining (1.42) with (1.43), we see that constructive interference occurs if\nd sin \u03b8 = n\u03bb\n\n(1.45)\n\nNote that the position x on the wall is hidden in \u03b8. If L is large, \u03b8 is small and sin \u03b8 \u2248\ntan \u03b8 = x/L. In this approximation, the above condition can be rewritten as\nx=\n\nn\u03bbL\nd\n\n(1.46)\n\nAt points x satisfying the above relation, constructive interference occurs, so the combined waves give rise to a wave with large amplitude. In between such points (at x =\n\n\f27\n\nCHAPTER 1. INTRO: WHAT IS QUANTUM MECHANICS?\n\nx\n\n\u03b8\n\nd\n\nL\n\n\u03b8\n\n\u03bb\n\u0394\n\n\u0394\n\nFigure 1.6: An interference experiment: two narrow slits/holes, through which waves fall.\nThe distance between the two holes is d. The distance between the holes and the wall\non the right is L. We are interested at the total (superposed) wave arriving at position\nx, corresponding to an angle \u03b8 (shown at two places). The path difference is clearly\n\u2206 = d sin \u03b8. On the right, the waves emerging from the two holes and traveling towards\none specific x are shown, at a particular instant in time. Since the waves emerging from\nthe holes necessarily have the same phase. Indeed, here, they both show a crest at the left\nmost position. However, at the height of the wall, the two waves are out of phase. The\nupper wave shows a trough, whereas the lower wave shows a crest. This means destructive\ninterference occurs at this particular x. This is not so surprising, since the path difference\nis precisely half of the wavelength. (Shown in the right corner.)\n\n\u0001\nn + 12 \u03bbL/d) destructive interference occurs - the waves of the lower and upper slit exactly cancel each other. This means there is no disturbance at all at these points. In\nbetween the two extremes, there is a smooth transition. This pattern shows alternating\nbands (regions) of constructive and destructive interference, and is called the interference\npattern.\n\n\fChapter 2\n\nSchr\u00f6dinger's equation\nIn this chapter...\nThe introductory chapter gave a quick qualitative view on one of the central theses of\nquantum mechanics, namely that the fundamental building blocks of all matter and radiation are tiny wave packets. As a consequence, we know that we should actually describe\nparticles as waves, not just a moving points. The question that now arises is obvious: how\ncan do so? How do we describe such a wave? And what is the dynamics of such a wave;\nhow does it evolve in time? These questions will be addressed in this chapter. We will\nmove on from a qualitative to a quantitative description.\nAs mentioned in the foreword, all chapters will from now on have a fixed structure.\nThe tool part of this chapter will be an introduction on the mathematics of operators.\nThen we tell the story of the Schr\u00f6dinger equation. Finally, in the computation part we\nwill do our first real quantum mechanical description of a particle. Good luck!\n\n28\n\n\fCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\n2.1\n\n29\n\nOperators and functions\n\nIn this section, we will learn about operators. It will be a very central tool for the\nfollowing chapters. First we explain what an operator is, then we give some examples. We\nend with the notion of an eigenvector and eigenvalue of an operator.\n\n2.1.1\n\nAn operator, what's that?\n\nAbout the most ubiquitous object in mathematics is the 'function'. Take for example the\nreal function f (x) = x2 . It maps each number to another number. More pictorially, we\ncould say that such a function is a machine: if you feed it a number x, it will return to you\na different number, x2 . Now what is an operator? Its a machine too, but now the in- and\noutput aren't numbers, but functions. Lets give a simple example. Given a nice smooth\nfunction f , we can compute its derivative f \u2032 . We can see the process of differentiation as\na map:\ndifferentiation : f \u2192 f \u2032\n(2.1)\nWe can do this for every nice smooth function, so 'differentiation' is an operator: if you\nfeed it a function, it will return a new function, f \u2032 . If we denote this differentiation\noperator with D, we have\nD : f \u2192 D(f ) = f \u2032\n(2.2)\n\nSo D(x2 ) = 2x, D(ex ) = ex , etcetera. We can give another example of an operator: take\na number n, and define the operator X n as follows\nX n : f \u2192 X n (f ) = xn * f\n\n(2.3)\n\nSince X n sends every function to a new function (xn times the original one) it's an operator\nindeed. Take for example X 2 . We have X 2 (sin x) = x2 sin x, X 2 (ex ) = x2 ex , etcetera. 1\nNow both the above examples are of a special type: they are linear operators. We call\nan operator L linear if for all functions f and g and for all numbers a and b\nL(af + bg) = aL(f ) + bL(g).\n\n(2.4)\n\nThis indeed holds for the differentiation operator D, and also for the operators X n . In\nfact, all operators we will meet from here on will be linear operators. They are the ones\nthat are relevant for quantum mechanics. So, we will drop the adjective 'linear' from now\non, and just talk about 'operators'. Also, we will try to stick to the following notation:\na capital letter is an operator, while a minuscule denotes a function. (Numbers will be\ndenoted by either a capital or a minuscule.) Besides that, we will also be more careful\n1\nFor notation naggers: there is actually some abuse of notation here: we use xn and sin x as shorthand\nfor the functions x \u2192 xn and x \u2192 sin x. In principle we could write out things more carefully, but this\nwould make all formulas look ugly.\n\n\f30\n\nCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\nFUNCTION\n\nOPERATOR\n\nMACHINE\n\nf(x) = x2\n\nD f = f'\n\nFigure 2.1: A pictorial way to think about functions and operators. Both are machines.\nA function has a number as in- and output. An operator takes a function as input and\ngives another function in return.\n\nwith the differentiation operator, and denote with a subscript the variable with respect to\nwhich the derivative is taken:\n\u2202f\n(2.5)\nDx : f \u2192\n\u2202x\nand analogous for any other variable.\n\n2.1.2\n\nEigenfunctions and eigenvalues.\n\nIf you had any course on linear algebra, you are probably familiar with the notion of an\neigenvector. Given a matrix A, we say v is an eigenvector with eigenvalue \u03bb if\nAv = \u03bbv.\n(Being lazy, we didn't put an arrow on the vector v.) For example:\n\u0012\n\u0013\u0012\n\u0013\n\u0012\n\u0013\n2 0\n1\n1\n=2\n\u22122 1\n\u22122\n\u22122\n\n(2.6)\n\n(2.7)\n\n\u0012\n\n\u0013\n\u0012\n\u0013\n1\n2 0\nSo the vector\nis an eigenvector of the matrix\nwith eigenvalue 2.\n\u22122\n\u22122 1\nHowever simple the idea of eigenvector is, it is the basic concept for a lot of interesting\nmath. Not surprising, it will turn out to be a very useful step to generalize it to the\ncontext of operators. There, the input and output are functions, not vectors. So it could\nbe the case that for a certain function f and an operator A:\nA(f ) = \u03bbf.\n\n(2.8)\n\n\f31\n\nCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\nwith \u03bb a number. In that case, we say that the function f is eigenfunction of the operator\nA, with eigenvalue \u03bb. If A is a linear operator (and all operators we will meet from here\non will be so) then every multiple of an eigenfunction is an eigenfunction again. Indeed,\nin the case of a linear operator A and an eigenfunction f , we have\nA(kf ) = kA(f ) = k\u03bbf = \u03bb(kf )\n\n(2.9)\n\nfor every number k. The parallel with ordinary eigenvectors in linear algebra is very\nf1\n\nA\n\nf2\nf3\n\nA\n\nf4\n\nFigure 2.2: An example of two functions f1 and f3 , acted upon by an operator A. Function\nf1 is sent to a multiple of itself: it is an eigenfunction of the operator A. Function f3 is\nsent to something completely different, and clearly is not an eigenfunction of A.\nstrong. To make the resemblance even more explicit, we can rewrite the condition to be\nan eigenfunction in a slightly shorter way:\nAf = \u03bbf.\n\n(2.10)\n\nHere it is understood that the operator A works on the object to its right - just like a\nmatrix working on a vector. From now on we will always use this notation. So if we don't\nput brackets around its argument explicitly, it is understood that an operator works\non the object standing to its right. So for example we may simply write ABf instead\nof A(B(f )). At first sight, it may look like this could give rise to ambiguous expressions,\nbut just like with matrix products, the notation works quite well. For example, given\ntwo operators A and B we can define a new operator, denoted by AB, which acts on any\nfunction with B first, and then with A:\nAB : f \u2192 (AB)(f ) \u2261 A(B(f ))\n\n(2.11)\n\nLet's do some random examples. The function e7x is an eigenfunction of Dx , with eigenvalue 7, because\nDx e7x = 7e7x .\n(2.12)\n\n\fCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\n32\n\nAnother one: sin(x) is an eigenfunction of the operator Dx Dx , with eigenvalue \u22121, because\nDx Dx sin(x) = Dx (Dx (sin(x))) = Dx (cos(x)) = \u2212 sin(x).\n\n(2.13)\n\nIn the first step we have added brackets, to make connect with the previous notation.\nAnother example. Define the unity operator '1' as the operator that sends each function\nto itself:\n1:f \u2192f\n(2.14)\n\n(In fact, this is just the operator X 0 .) It is obvious that all functions are eigenfunctions of\nthe unity operator, with eigenvalue 1. Last example: take a function depending on some\nvariables, but not on x. So, for example a function g(y, z). If we take the derivative with\nrespect to x, we get\n\u2202g(y, z)\nDx g(y, z) =\n=0\n(2.15)\n\u2202x\nSo any such function g(y, z) is an eigenfunction of Dx , with eigenvalue 0.\nSums of operators\n\nA last remark: we can also add operators, and multiply them by numbers, in a quite\ntrivial way. The operator A + B is defined as follows: it sends every function to Af + Bf .\nIn the same way, for any number c, the operator cA is defined as follows: it sends every\nfunction f to cAf (=c times the function Af ). As an application of these rules, you can\ncheck that the operator X 2 \u2212 2X 3 acts as:\nX 2 \u2212 2X 3 : f \u2192 (x2 \u2212 2x3 ) * f\n\n(2.16)\n\njust like you would guess naively. In general, to any function g(x) we can associate the\noperator g(X), defined as follows:\ng(X) : f \u2192 g(x) * f\n\n(2.17)\n\nSo for example exp(X)x = ex x. That's the math for this chapter, let's move on to the\nstory part!\n\n2.2\n2.2.1\n\nWavefunctions and Schr\u00f6dinger\nThe wave function\n\nIn the previous chapter, we saw how the two-slit experiment and the photoelectric effect\nforce us to describe both radiation and matter as wave packets. Especially for particles\nthis requires a new framework. From here on, our main goal will be to answer the question:\nhow can we describe the wave aspect of particles? Here particle can mean: electron,\nproton, neutron, or any other of the (more rare) particles that make up our universe. Let's\n\n\f33\n\nCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\ntake for example one single particle on a specific moment time t. The pioneers of quantum\nmechanics did the following (very reasonable) step: let us describe this particle by a\nfunction:\n\u03c8(x)\n(2.18)\nThe symbol \u03c8 is the Greek letter Psi, pronounced as 'sigh'. Like for a wave, it gives the\ndisturbance as a function of space. As we want to describe a wave packet, we expect \u03c8 to\nslope of fast outside some region which is the location of the wave packet. Schematically,\nyou might see something like this\ndisturbance\n\n\u03c8\n\nspatial direction\n\nFor simplicity we have drawn only one coordinate axis. In principle the disturbance\n\u03c8 will depend on the tree different coordinates, but for now we will describe disturbances\ndepending on one single coordinate. So a wave packet is just a function \u03c8(x). But then\nwhat is that disturbance \u03c8? For radiation, there are two oscillating fields: E and B. For\na sound wave, there is a space-(and time) dependent perturbation of the pressure and the\ndensity. Here however, we are dealing with a fundamentally new concept: for sure \u03c8 is\nnot a type of disturbance that you have met before. So, since a better name is lacking, \u03c8\nis simply called the wave function of the particle. After all, it is a function describing\nthe particle's wave packet. A peculiar thing is that the wave function is taken to be a\ncomplex function.\n\n2.2.2\n\nWhy complex?\n\nNotice that in the examples just given (radiation and sound waves) there were several\noscillating quantities. This is very general: most waves consist of two types of disturbance,\nwhich are constantly being transformed into each other, at each point in space. For\nelectromagnetism, a changing electric field creates the magnetic field, and the changing\nmagnetic field creates the electric field again, and so on - and this happens at each point\nin space. For a wave of water, there is the movement (velocity) of water on one hand\nand the vertical displacement on the other hand. Again, these two types of disturbance\n\n\f34\n\nCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\nare constantly transformed into each other. So many waves are described by two real\nfunctions (disturbances). This already suggests it is possible to group these together into\none single complex function.\nThere is another argument to do this 'grouping'. As we have stressed, for each position\nin space, the wave exhibits a cyclic behavior. The specific stage of the cycle can be\nexpressed by a phase (an angle). On the other hand, a wave often has a notion of total\namplitude (a total of the oscillating quantities). As you know, a phase and an amplitude\ncan be economically summarized in a complex number, via the argument and the norm\nof the complex number.\nSo this suggests one can economically describe a wave by a single complex function.\nIndeed, in -for example- electromechanics alternating currents are often described by complex waves. This is not for fun, or to increase the complexity of the situation. There,\nsuch a description really is quite natural and simplifies a lot of computations. Indeed:\nwave phenomena naturally involve trigoniometric functions (cos x, sin x,...) and instead of\nusing tedious trigoniometric identities, one can see these functions as the real/imaginary\npart of a single imaginary function like eix . (See Appendix 2 of the previous chapter.) For\nwater waves and pure electromagnetism, a complex description of waves is less usual. One\nreason is that there, the constituents of these waves have a very clear physical meaning\n(electric and magnetic fields, displacement of water, ...) and people like to hold on to\nthose concepts. Of the many scientific fields in which people deal with waves, some use\ncomplex descriptions, some don't. It's a bit of a trade-off. A complex description can lead\nto easier expressions (sometimes, not always) but is of course slightly less intuitive.\nBack to quantum mechanics and the wave function of a single particle. There, the\ndisturbance is not of a kind we know already. So there is no point at trying to give a\ndescription in terms of real fields - this would just make the mathematical expressions\nlook more ugly. This suggests one should just go ahead and use a complex function to\ndescribe these wave packets. And indeed you will see this works pretty well. Just recall\nthat there is nothing fancy about using complex functions: it does not means some intrinsic\nstrangeness is involved. Complex functions are just the most natural way to deal with\nwaves - that is why we like to use them.\n\n2.2.3\n\nTime evolution\n\nWe now know we should describe a particle by a complex function, the wave function. The\nnext question is obvious: how does such a wave function evolve in time? Otherwise stated,\nsay you have a wave function at a given time t0 , call this \u03c8(x, t0 ), how will it move/change\nits shape as time runs? So what is \u03c8(x, t) for later times t? A solution to this problem\nwould be to find a relation of the form\n\u2202t \u03c8(x, t)\n\nin terms of\n\n\u03c8(x, t)\n\n(2.19)\n\nIn other words, we want to find a differential equation relating the state \u03c8(x, t) of the\nwave function to its change \u2202t \u03c8(x, t) in time.\n\n\f35\n\nCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\nTo get a hint\n\nTo get a hint what such a relation might look like, lets first try to generalize an (infinite)\nreal wave to the complex description we will use. An infinite wave with amplitude A,\nfrequency \u03c9 and wave number k is typically described by\nA sin(kx \u2212 \u03c9t).\n\n(2.20)\n\nIndeed, this is an oscillation, moving to the right with speed v = \u03c9/k. Now the complex\nfunction\nAei(kx\u2212\u03c9t)\n(2.21)\nis an oscillation as well. Take some time to try to visualize this wave. At each point in\nspace the value of the function is a complex number, with modulus A, and with a phase\nthat evolves with angular frequency \u03c9. So the above function describes a complex wave\nwith angular frequency \u03c9, wavenumber k, and amplitude A. If you draw the real and\nimaginary parts separately, you get a helix shape, like in the following figure:\nIm(f)\n\nx\nRe(f)\n\nFigure 2.3: The complex wave f (x) = eix , showing its real and imaginary parts separately.\nThroughout time, the wave (2.21) will move: the above helix will run to the left or to\nthe right. Also, note that (2.21) is an eigenfunction of the operators\nDt \u2261\n\n\u2202\n\u2202t\n\nwith eigenvalue \u2212 i\u03c9\n\n(2.22)\n\nand of\n\n\u2202\nwith eigenvalue ik.\n(2.23)\n\u2202x\nLoosely speaking, the values of \u03c9 and k can be obtained by acting with the time- and spatial\nderivative on the wave. Moreover, we know from the first chapter that the frequency and\nwave number of a particle are related to the energy and momentum,\nDx \u2261\n\nE = ~\u03c9\n\np = ~k\n\n(2.24)\n\nCombining with the above, we expect 'some' relation between the quantities E and p of\nthe particle and operators acting on its wave function:\nDt \u2194 \u2212i\u03c9 = \u2212i\n\nE\n~\n\nand Dx \u2194 ik = i\n\np\n~\n\n(2.25)\n\n\fCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\n36\n\nOn the other hand, we know from classical mechanics that the total energy E and momentum p (= mv) of a particle are related by the following expression:\nmv 2\np2\n= V (x) +\n.\n2\n2m\nwhere V (x) is the potential energy of the particle at position x.\nE = V (x) +\n\n(2.26)\n\nA guess\nNow, using (2.25) to replace E and p in 2.26, we get\n1\n(\u2212i~)2 Dx2 .\n(2.27)\n2m\nOf course, this equation looks a bit strange. However, if we let both sides act on \u03c8, we\nget the following\n(\u2212i~)2 2\ni~Dt \u03c8(x, t) = V (x)\u03c8(x, t) +\nDx \u03c8(x, t)\n(2.28)\n2m\nwhich can be rewritten as:\ni~Dt \" = \" V (x) +\n\n~2 \u2202 2 \u03c8(x, t)\n\u2202\u03c8(x, t)\n= V (x)\u03c8(x, t) \u2212\n(2.29)\n\u2202t\n2m \u2202x2\nThis expression is called the Sch\u00f6dinger equation, after the physicist Erwin Schr\u00f6dinger\nwho wrote it down first.2 It has the shape of an evolution equation: it tells you what the\n\u2202\n) of a wave function is, given the wave function on that moment. That\ntime evolution ( \u2202t\nis precisely what we were looking for! It turns out that almost all particles obey the above\ntime evolution equation: electrons, protons, neutrons, ... . Hence, the above equation is\nreally the core of quantum mechanics - much of the rest of the course will be using this\nequation.\ni~\n\nDisclaimer\nIf you have vaguely understood the motivation and arguments leading to the Schr\u00f6dinger\nequation, do congratulate yourself. Do not think that the arguments present are even\nclose to being solid though, we did not derive the equation. Such a derivation doesn't\neven exist. To see why, just think back of how the laws of Newton were introduced to\nyou a long time ago. People can try to convince you that those formulas are more or less\nreasonable, but their exact form can not derived in a mathematical way: they are only\nfound by doing lots of experiments, by many people thinking long and hard about how\nto put the experimental results into a small set of formulas. Newton did this for gravity\nand classical mechanics (building on the work of many others), and Schr\u00f6dinger (equally\ndependent on others' ideas and results) did so for quantum mechanics.\nOf course, things will get more clear after using the Schr\u00f6dinger equation. Good news:\nin the next section we will do our first real quantum mechanics computation.\n2\n\nPronounced 'SHROEding-uh'\n\n\f37\n\nCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\n2.3\n2.3.1\n\nUsing the Schr\u00f6dinger equation\nThe Hamiltonian\n\nLets look at the Schr\u00f6dinger equation more carefully. On the right hand side there is\nV (x)\u03c8(x, t) \u2212\n\n~2 \u2202 2 \u03c8(x, t)\n2m \u2202x2\n\n(2.30)\n\nThis whole object is a function, depending on x and t. If you know V (x), and someone\ngives you some input \u03c8(x, t), you can readily compute the above object for him. Wait\nwait wait, we have heard that before: this sound like an operator! Indeed, using notation\nof the first section\n\n, we can define the operator\nH \u2261 V (X) \u2212\n\n~2 2\nD\n2m x\n\n(2.31)\n\n2\n\n~\ntimes its second derivative. This\nwhich sends any function to V (x) time itself, plus \u2212 2m\noperator is called the Hamiltonian operator. Using this operator, we can then rewrite\nthe Schr\u00f6dinger equation in the following concise form:\n\ni~\n\n\u2202\u03c8(x, t)\n= H\u03c8(x, t)\n\u2202t\n\n\u2202\nSome people like to write partial derivatives like \u2202t\nand\nstylize even further:\ni~\u2202t \u03c8 = H\u03c8.\n\n(2.32)\n\u2202\n\u2202x\n\nsimply as \u2202t and \u2202x . They can\n(2.33)\n\nIn this form the Schr\u00f6dinger equation looks less scary - although the notation is more\nabstract.\n\n2.3.2\n\nA real computation: a particle in a box\n\nWe are now ready for our first real computation. Imagine a particle enclosed in a small\nspace, or a \"box\" - for instance an electron trapped in a small region. What does its\nwave function look like, and how does it evolve in time?Let's see if we can answer these\nquestions. First let's try to see if we can make the word \"box\" more precise. Consider the\nfollowing potential\n\u001a\n0\nfor 0 < x < L\n(2.34)\nV (x) =\n+\u221e\neverywhere else\nBetween x = 0 and x = L, the potential is constant, so the particle moves freely. Outside\nthat region however, the potential energy is infinite, meaning the particle can not go\nthere. In terms of the wave function: we impose that \u03c8 has to be zero outside [0, L].\nIf we also demand the wave function to be continuous, it also has to vanish on x = 0\n\n\f38\n\nCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\nV\n\nV\n\nV\n\n0\n\nV\nL\n\n0\n\nx\n\nand x = L. Since outside the box \u03c8 = 0, the Schr\u00f6dinger equation is trivially satisfied\nthere, so we can focus on solving the equation inside the box. For a general initial wave\nfunction \u03c8(x) \u2261 \u03c8(x, t = 0) it is not so clear how to explicitly solve the Schr\u00f6dinger\nequation. So it is hard to tell what the wave function will look like at a later moment.\nHowever, something special happens if the initial wave function is an eigenfunction of the\nHamiltonian operator. In that case,\nH\u03c8(x) = E\u03c8(x)\n\n(2.35)\n\nfor some number E. In that case, the time evolution of the wave packet is rather easy: it\nis given by.\n\u03c8(x, t) = e\u2212iEt/~ \u03c8(x)\n(2.36)\nYou can easily check that the above \u03c8(x, t) indeed satisfies the Schr\u00f6dinger equation\ni~\u2202t \u03c8(x, t) = H\u03c8(x, t). So for each wave function that is an eigenfunction of the Hamiltonian, we can write down a fully time-dependent solution to the Schr\u00f6dinger equation.\nFor this reason, (2.35) is called the time-independent Schr\u00f6dinger equation: it's an\nequation not involving t, and the time evolution of its solutions is very easy. Let's try to\nsolve that equation for a particle in a box. Here V (x) = 0 in the relevant region, so that\n~2 2\n~2 \u2202 2\nDx = \u2212\n(2.37)\n2m\n2m \u2202x2\nCan we find eigenfunctions of this operator? Well, you know that the functions sin x and\ncos x are proportional so their second derivative. This suggests we should look in that\ndirection. Putting in some extra constants, we find that all functions of the form:\nH=\u2212\n\n\u03c8(x) = A sin(kx) + B cos(kx)\nsatisfy E\u03c8 = \u2212\n\n~2 \u2202 2 \u03c8\n2m \u2202x2\n\n(2.38)\n\n, given that\nk2 =\n\n2Em\n.\n~2\n\n(2.39)\n\n\f39\n\nCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\nOnly the function sin(kx) vanishes at x = 0 and demanding that also \u03c8(L) = 0 imposes\nk=\n\nn\u03c0\nL\n\nwith n a positive natural number.\n\n(2.40)\n\nSo the eigenfunctions of H (satisfying the boundary conditions \u03c8(0) = \u03c8(L) = 0) are\ngiven by\nn\u03c0x\n\u03c8n (x) = A sin\n(2.41)\nL\nand they have corresponding eigenvalues\nEn =\n\nn2 ~2 \u03c0 2\n~2 k2\n=\n2m\n2mL2\n\n(2.42)\n\nA priori, the prefactors Aqhave arbitrary values. For reasons that will be clear later, we\nwill put the A's equal to L2 . If we now use (2.36), we see that the full time-dependent\nsolutions to the Schr\u00f6dinger equation are\nr\nn\u03c0x\n2\n\u2212iEn t/~\nsin\n(2.43)\n\u03c8n (x, t) = e\nL\nL\nThe first few \u03c8n are shown in figure below.\n\nE\n\u03c84\n\nE4\n\u03c83\n\nE3\n\n\u03c82\n\u03c81\n0\n\nL\n\nx\n\nE2\nE1\n\nThe time-dependent solutions are just the above graphs, but 'rotating' in time: multiplying by e\u2212iEn t/~ keeps the norm of the wave function the same throughout time, but\nthe argument runs with angular velocity \u03c9 = E~n . This is a pretty nice result. We find\nan infinite series of allowed wave functions for a particle in a box. The solutions are labeled by the integer n, and each such solution is an eigenfunction of the hamiltonian, with\neigenvalue En . Let's try to understand this result better.\n\n\fCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\n40\n\nInterpretation\nIt is very striking that only a discrete set of wave functions (obeying the Schr\u00f6dinger\nequation and the matching conditions) can be found. By discrete we mean the set of\nsolutions is labeled by an integer, not by a continuous parameter. Each such solution is\ncalled a state. So a particle in a box can only be in a discrete series of states. As we\nsaw before, the Hamiltonian operator is closely related to the concept of energy, so it is\nnatural to see the eigenvalue of a state under H as its energy. So according to quantum\nmechanics, the energy of a particle in a box can only take on a discrete set of values En .\nLet's compare this to the classical (incorrect) description. If a fundamental particle\nreally was a point, it would just look like a bullet bouncing from side to side in the box,\np\n. The states would then just be labeled by the momentum p of the\nwith speed v = m\np2\nparticle. So in such a situation, any value of p would be allowed, and the energy E = 2m\nof the particle could take on any positive value.\nThis means the classical (point particle) description is drastically different from the\n(correct) quantum description. How can we check the quantum description is really true?\nImagine a particle jumping from one energy level to a lower one. In such a process, the\nenergy difference has to be released - for example in the form of radiation: the emission\nof a photon. The energy of such a released photon can be measured. This way, we can\nmeasure the energy differences between the different states of a system. Sure enough,\nfor particles enclosed in a small region these energy differences take on only very special\nvalues, confirming the spectrum is actually discrete, just like QM predicts. Of course, the\ndetails of that spectrum depend on the exact shape of the potential in which the particle\nis locked up. The potential discussed here is a bit simplistic/idealized. There is more than\none dimension (here we considered only an x-dependent wave function) and it is technically\nnot possible to build such a perfect box potential. However, very similar systems can be\nbuilt. A quantum dot f.e. is a configuration in which an electron is locked up in a very\nsmall zone of a material. These dots can have f.e. a nice round shape. The calculation to\nsolve the Schr\u00f6dinger equation is a bit more difficult there, but the result is very similar:\none finds a discrete series of states, each with its own energy. And the computed energy\nlevels indeed match experimental results, backing up the quantum description, once again.\n\n\fCHAPTER 2. SCHR\u00d6DINGER'S EQUATION\n\n41\n\nExercises\n1. In the beginning of the chapter, we promised that all operators relevant to QM are\nlinear operators. Check that the Hamiltonian operator is linear, by explicitly writing\nout its action on a linear combination of functions.\n2. What is the eigenvalue of the function f (x) = 1 under the operator Dx ? And of the\nfunction f (x) = x under the operator XDx ? Give an example of a function which\nis not an eigenfunction of these operators (should be easy).\n3. Under what condition (on n and m) is the function xn an eigenfunction of X m Dx2 ?\nWhat's the corresponding eigenvalue?\n4. Consider an electron locked up in a box, with length L = 10 nm (1 nm =10\u22129 m, the\nelectron mass is me = 9, 1110\u221231 kg). What is the energy E2 of the second level?3\nAnd what's the energy of the first (lowest) level? If electron drops down from the\nsecond to the first level, the corresponding energy difference has to be released, for\nexample in the form of a photon. What would be the frequency of this photon?\n5. The nucleus is a very small region where protons and neutrons are squeezed together\n(or 'locked up') in a region of about 1 fm = 10\u221215 m. Be very crude and pretend\nthat a nucleus really is a 'particle in a box' potential trapping the nucleons. If\na neutron (mn = 1, 67 * 10\u221227 kg) drops from the third to the first level, what\nenergy is released? To convince you that the 'particle in a box' approximation is\nnot completely ridiculous here: it is indeed possible for a nucleus to be in an excited\nstate - with one or more nuclei occupying a higher energy state (such states are\ntypically produced in nuclear decay processes). When decaying, typically a gamma\nray escapes, with an energy around 1 MeV (= 109 eV). Is this comparable to what\nyou found with your 'toy calculation' ? (If not, you may have done something wrong\nwith units - check the footnote.)\n6. In classical mechanics, the potential (and all other energy levels) can often be shifted\nwithout changing the physics. To see how things work out here: solve the problem of\na particle in a box, but now with V =(some constant) instead of V = 0 in the allowed\nzone. What happens to the energy levels? Try to see what happens in general if you\nsend H \u2192 H+constant: do the eigenstates change, and their eigenvalues? What\nabout time evolution?\n\n3\nBe careful with units here! Unless you are using the de relation E = hf (where you can express both\nthe energy and Planck constant in eV-units) you have to express Planck's constant in standard units: J*s\nand not eV*s, since all other quantities (lengths, masses, ...) are expressed in standard units. Of course,\nat the end of your calculation you may choose to re-expess the energy you found in eV again.\n\n\fChapter 3\n\nThe measurement\nIn this chapter...\nWe are now ready to introduce another central aspect of quantum mechanics: performing\nmeasurements on a particle's wave function. The tool we need to introduce is the notion\nof inner product between functions. We then tell the story of the measurement in QM. In\nthe last section, make all this more concrete with the study of the particle in a harmonic\npotential.\n\n42\n\n\f43\n\nCHAPTER 3. THE MEASUREMENT\n\n3.1\n3.1.1\n\nInner products and bra(c)kets\nInner product of functions\n\nIn quantum mechanics, a very central role is played by complex functions. A simple but\nimportant property is that the sum of two complex functions is again a function, and a\ncomplex multiple of a complex function is a complex function as well. This means the\nspace of complex functions forms a (complex) vector space. Many vector spaces have\nnotion of distance, angles and all that. All these useful concepts are usually derived from\none single notion: the inner product. Can you define an inner product for complex\nfunctions as well? The answer is: yes, you can! It is usually denoted by hf, gi and is a\nmap from any two complex functions f and g to a complex number, as follows:\nZ +\u221e\nf (x)g(x)dx\n(3.1)\nhf, gi \u2261\n\u2212\u221e\n\nHere the bar on f denotes complex conjugation. Some people denote complex conjugation\nwith an asterisk, so they write f (x)\u2217 instead of f (x). The result of the above integral is\nindeed a single complex number. You may worry that the above integral is not always\nfinite. Here is a relief: we will use inner products like the above specifically for wave\nfunctions f and g. Since wave functions fall off fast outside some region (they are wave\npackets) they have finite inner products hf, gi amongst themselves. So in the case of our\ninterest, the above object will be well-defined. Roughly speaking, the inner product is\nlarge if the wave packets f and g have a similar shape and location, otherwise it is small.\nThe inner product has a special property, called bilinearity:\nhf, ag1 + bg2 i = ahf, g1 i + bhf, g2 i\n\n(3.2)\n\nhaf1 + bf2 , gi = \u0101hf1 , gi + b\u0304hf2 , gi.\n\n(3.3)\n\nand\nThis is sometimes put as follows: the inner product is linear in its second argument\nand anti-linear in its first argument. Lets check that for example the second of these\nproperties is true:\nZ +\u221e\naf1 (x) + bf2 (x)g(x)dx\n(3.4)\nhaf1 + bf2 , gi =\n\u2212\u221e\nZ +\u221e\n=\naf1 (x) + bf2 (x)g(x)dx\n(3.5)\n\u2212\u221e\n\n= \u0101hf1 , gi + b\u0304hf2 , gi\n\n(3.6)\n\nHere we first used the properties of complex conjugation (see Appendix 1 on complex\nnumbers) and then the linearity of an integral. Another essential property of the inner\nproduct (which you can easily check) is the so-called conjugate symmetry:\nhf, gi = hg, f i\n\n(3.7)\n\n\f44\n\nCHAPTER 3. THE MEASUREMENT\nOf course, we can also take the inner product of a function with itself:\nZ +\u221e\nZ +\u221e\nhf, f i =\n|f (x)|2 dx\nf (x)f (x)dx =\n\u2212\u221e\n\n(3.8)\n\n\u2212\u221e\n\nThe last expression, being the integral of a real and positive function |f (x)|2 , has to be\nreal and positive. Hence,\nhf, f i \u2265 0.\n(3.9)\np\nThis means we can always take the square root hf, f i. This gives a real number, and\nsays how \"big\" the (complex) function f is. (Something like the total size of the complex\nfunction f .) It is called the norm of the function f , and is denoted by kf k:\np\nkf k = hf, f i\n(3.10)\nUsing our fresh and shiny inner product tool, we can introduce yet another new concept\n(besides the norm): the Hermitian conjugate of any operator A. It is denoted by A\u2020\nand defined as following: A\u2020 is the operator such that for every pair of functions f and g,\nhA\u2020 f, gi = hf, Agi.\n\n(3.11)\n\n(A\u2020 )\u2020 = A\n\n(3.12)\n\n(aA + bB)\u2020 = \u0101A\u2020 + b\u0304B \u2020\n\n(3.13)\n\nOne can show that\nand\nfor all operators A and B, and all complex numbers a and b. Side remark: as mentioned\nin Appendix 1 of the first chapter, we denote complex conjugation either by a bar on\ntop (like in the above expression) or by an asterisk, at will. So both c\u0304 and c\u2217 denote the\nconjugate of c. Another property of Hermitian conjugation that one can show, is\n(AB)\u2020 = B \u2020 A\u2020\n\n(3.14)\n\nSo the order of operators switches upon taking the Hermitian conjugate.\n\n3.1.2\n\nColumn- and row vectors\n\nMaybe you recognized several terms of the previous section. Indeed, you might be familiar\nwith the inner product of (complex) vectors from the context of linear algebra. Given two\nn\u2212dimensional complex vectors a = (a1 , ..., an ) and b = (b1 , ..., bn ) it is defined as\nha, bi =\n\nn\nX\n\n\u0101i bi\n\n(3.15)\n\ni=1\n\nYou can check that this, just like the inner product of functions, is a bilinear operation.\nThe only difference is that here it maps two vectors, not two functions, to a complex\n\n\fCHAPTER 3. THE MEASUREMENT\n\n45\n\nnumber. In linear algebra, one usually denotes vectors in a column version. For example\nthe vector ~v with components 1 and i is written as:\n\u0012 \u0013\n1\n(3.16)\nv=\ni\nNext to that, vectors also have a row version, called their Hermitian conjugate and\ndenoted by ~v \u2020 . It is obtained by taking the complex conjugate of all components, and\nthen writing the numbers in a row. For the above vector, this gives:\n\u0001\nv \u2020 = 1 \u2212i\n(3.17)\n\nNow what is the point of making two different versions of the same object? Well, if\nyou write a column vector after a row vector, the ordinary matrix multiplication of the\ntwo objects gives you precisely the inner product of the two vectors. So, by using the\nright version of the vectors, people know where to take the inner product: it just happens\nautomatically if you perform the matrix multiplications. For example: if someone\u0012wants to\n\u0013\n1 0\ncalculate the inner product of the above vector v with the matrix product of A =\n1 1\n\u0012 \u0013\n1\ntimes the vector w =\n, he should write down\n0\n\u0013 \u0012 \u0013\n\u0012 \u0013 \u0012\n1\n1 0\n1\ni\n*\n,\nh\n0\n1 1\ni\n\n(3.18)\n\nThis looks horrible. However, with the row-column trick he can just write the above\nexpression as\n\u0012\n\u0013\u0012 \u0013\n\u0001 1 0\n1\nv = 1 \u2212i\n(3.19)\n1 1\n0\n\nThis looks more appealing, and is completely clear: just take the matrix product twice,\nand the result you get is indeed the required inner product. It is important to remember\nthat the difference between row and column vectors is not fundamental: it is just a trick\nto indicate: ah, if the two are next to each other, take the (inner- or matrix-) product.\n\n3.1.3\n\nBras and kets\n\nLet's try to repeat that convenient trick for the inner product of functions. Doing so, we\nwill also upgrade our notation. Up to this point, we were a bit sloppy with the difference\nbetween a state of a particle and a function. For sure, the state of a particle is described\nby a complex function (the particle's wave function) but very strictly speaking it is not\nthe same thing. A similar example: a temperature can be described by a number, but a\ntemperature is more than 'just a number': it has a special meaning. To be more careful\nwith this difference: if a particle has wave function f , we will say that it is in the state\n\n\fCHAPTER 3. THE MEASUREMENT\n\n46\n\n|f i. So using the vertical bar and a right bracket, we indicate that we are talking about\na state, not just any function. (In the same way that an \u25e6 F , \u25e6 C or K indicates that the\nnumber in front is a temperature, not just any number.) Now, analogous to linear algebra,\nfor a state |f i, we define its hermitian conjugate, and denote it as hf |. In some sense,\nit really is the same object (it specifies the state of a particle) but in a different version just like row versus column. Here the visual difference is not row or column, but the shape\nof the brackets. Just like for ordinary vector spaces, the only use is the following: if you\nwrite a conjugate hg| next to |f i, it is understood that you should take the inner product.\nA state written in the form |f i, is called a ket, and if you write it in the (conjugate) form\nhf |, you call it a bra. The reason for these funny terms is as follows: if you write a 'bra'\nand then a 'ket', you have to take the bracket (which is slang for 'inner product'). Yes,\nthis proves that physicists pull bad jokes.\nhf | |gi\n=\nhf |gi\n=\nhf, gi\n(bra and then ket) = (short notation) = (bracket)\nThis notation is a bit strange when you first meet it. Don't worry: its use will become\nclear later on. Hopefully you understood that there is not something deep behind it: it is\njust a convenient bookkeeping on where to take the inner product, that's all.\nWhen operator meets (brac)ket\nNow what about operators? When acting with an operator A on a function f you get a\nnew function Af . In the context of states: |f i describes a state with wave function f, and\n|Af i describes a state with wave function Af . So this means we can see every operator\nnot only as a map between functions, but also as a map between states, as follows:\nA : |f i \u2192 A|f i = |Af i\n\n(3.20)\n\nIn this last line, A|f i means: 'act with operator A on state |f i', and |Af i means again:\n'the state with wave function Af '. This is a bit of a trivial remark - mostly a nuance in\nnotation and interpretation. Time for the story part!\n\n3.2\n\nThe measurement\n\nIn the first chapter, we learned about the two slit experiment for electrons. Since the result\nof the experiment was very clearly pointing at interference (even if single particles went\nthrough) we concluded that each particle has to be a wave packet. This conclusion has\nbeen confirmed by many other experiments since - particles really are extended objects,\ndescribed by a wave function. Yet, there is a subtlety that we have not touched upon so\nfar. How does detection occur when one of the electrons arrives at the wall? In principle,\nthe detecting wall measures the position of the incoming particle. But how does this work,\ngiven that the particle arriving is -in fact- an extended object? What outcome does a\n\n\f47\n\nCHAPTER 3. THE MEASUREMENT\n\nmeasuring device give? Of course, if a measuring device could speak and gesture, it would\nanswer: \"Well, the particle is spread out a bit, and is more or less here\", waving its hand\nat the location of the wave packet. But that is not what it is made to do, it is made\nto give you an exact answer. It turns out that a measuring device will give you a very\nprecise answer for the position, one particular x. But what will it say? Which x will it\ngive you? Well, the chance for a device to give you x as the particle's position\nis proportional to the amplitude |\u03c8(x)|2 of the wave function there. So in some\nsense, the measuring device does a good job: the position it gives you is usually a place\nwhere the wave function is big, which is indeed an indication for where the wave function\nis located. Since the function |\u03c8(x)|2 is proportional to the chance to detect the particle's\nposition to be x, we call this function the probability amplitude.\n|\u03c8| 2\n\nRe \u03c8\n\nx\n\nx1\n\nx2\n\nsignal\ncollector\n\nOn this figure, several things are shown: a wave packet (consisting of a single lump\nhere) arrives at a detector wall. The detector wall consists of many individual detectors\n(the holes) connected to a data collector. To indicate the shape of the wave function,\nits real part Re \u03c8 is shown. The total probability amplitude |\u03c8|2 = (Re \u03c8)2 + (Im \u03c8)2 is\nalso shown. The odds for a detector to give you x1 as an answer is really small as the\namplitude |\u03c8(x1 )|2 is very small there. An answer like x2 is much more likely to be given.\nVaguely speaking: a measuring device will give a random, but reasonable anwer. Now\nif you like to experiment, the above explanation probably brings the following question to\nyour mind. What if, right after measuring a first time, you measure again? This could in\nprinciple be done by making a 'transparent' detecting wall that does not stop the particle,\nand placing a second detecting wall behind it. Is there again some statistical chance for the\npossible outcomes? The answer is no! It turns out that if you do so, you will get the same\nanswer. So no statistical chances anymore, nothing like that, just a 100% sure that you get\nthe same outcome as the first measurement. If this confuses you, don't worry, many great\nminds in physics of the last century were very puzzled by this fact, and the only answer\nthey could come up with is the following. Apparently, performing a measurement on\na wave function drastically alters it: it seems to collapse onto the state corresponding\n\n\f48\n\nCHAPTER 3. THE MEASUREMENT\n\nto your measurement outcome. So if you measure its position, and get as answer x1 , the\nwave packet simultaneously collapses into a wave function that is very sharply localized\naround x1 , even if it wasn't so before.\n\nmeasurement\nwith result x1\n\n|\u03c8 f |2\n\n|\u03c8 i |2\nx1\n\nx\n\nThis phenomenon is called the collapse of the wave function. There are some\ntheories on how and why it happens, but despite all the debate, there is no consensus on\nwhich one is true. The great difficulty is of course talking about a particle (microscopic\nobject) and a detector (macroscopic object) at the same time. To be consistent you would\nneed to describe the detector in the quantum mechanical language as well. That is of\ncourse a very hard task, and can only be modeled poorly. Maybe the next generation\nphysicists will be able to solve this problem - lets hope so. Here is the good news: even\nthough we don't know why it happens, it is very sure that this happens - to very high\naccuracy - so this gives a partial relief.\nThe two slit experiment, revisited\nTo illustrate the above behaviour, let's go back to the two slit experiment. Say we use an\nelectron source and put it at a very low intensity, to make sure it shoots out the electrons\none by one. As a particle arrives at the slit, its wave function is still a nice blob. Then, as\nit passes through the holes, the two parts of the wave functions (passing through the left\nand right slit) interfere with each other. The result is a \"striped\" wave function: only on\nplaces of constructive interference, the amplitude |\u03c8|2 is big. Now the striped wave function\narrives at the detecting wall. The chance for the detecting grid to answer to you that the\nparticle arrived at position x is large at the stripes only, since the probability amplitude is\nlarge there only. So very probably, it will answer you a position -say some x1 - lying within\none of the stripe's positions. Now a second electron comes through. The shape of its wave\nfunction after passing through the slits is identical to that of the first electron. However,\nthe result of the measurement is not necessarily the same. The measurement may yield\nany outcome x2 , with probability |\u03c8(x2 )|2 . Obviously, after a lot of particles have gone\nthrough the slits, the total detection rates will be nicely proportional to the probability\namplitude |\u03c8(x)|2 of the individual particles. So even though individual measurements\ngive very few information about the wave functions, the total detection count reveals\nthis information very accurately. This way, the wave property of the individual particles\n\n\f49\n\nCHAPTER 3. THE MEASUREMENT\n\ninevitably becomes apparent. So we conclude the first interpretation of the experiment\nwas correct, although the physics involved has a statistical subtlety. Got it?\nWave packets look like point particles\nAlso, the above explanation should make clear why particles appear as points to us in\nmany experiments. This is because the position measurement gives very sharp results,\nand simultaneously collapses the wave function on a very localized one. In a more lyrical\nphrasing: the measurement forces the wave function to act like a particle, or better: you\nmay easily get the impression they are strict points indeed, although this is more an\nartifact of the measuring process.\n\n3.3\n\nMore precise\n\nSo far the story part, let's now make all this more concrete and precise. We said that the\nchance to measure a particle with wave function \u03c8 to be at position x is proportional to\n|\u03c8(x)|2 . By this we actually mean: the chance to measure the particle to be in a very\nsmall interval [x, x + \u2206x] is given by \u2206x * |\u03c8(x)|2 . For a larger interval the probability is\ngiven by the integral of |\u03c8(x)|2 :\nP ([a, b]) =\n\nZ\n\nb\na\n\n|\u03c8(x)|2 dx.\n\n(3.21)\n\nwhere by P ([a, b]) we mean the chance to detect the particle in the position-interval [a, b].\nFor this reason, people say that |\u03c8(x)|2 is a probability density.\nConsequence 1\nThe total chance getting a measurement result between x = \u2212\u221e and x = +\u221e has to be\n1 of course. This means we need\nZ +\u221e\n|\u03c8(x)|2 dx = 1\n(3.22)\n\u2212\u221e\n\nIn the terminology of the first section: the square of the norm of \u03c8 should be 1:\nh\u03c8|\u03c8i = 1\n\n(3.23)\n\nIf this is not the case for some wave function, you should multiply it by a number so that\nit does. Concretely, you can do this by dividing \u03c8 by its norm:\n\u03c8\u2192p\n\n\u03c8\nh\u03c8|\u03c8i\n\n(3.24)\n\n\f50\n\nCHAPTER 3. THE MEASUREMENT\n\nThis is called normalization of the wave function. If a wave function (or state) is\nnot normalized, that isn't a deep problem, but in that case you can't immediately use\nthe probability law (3.21). You can now understand why we put the A's involved for\nthe particle in a box to a specific value: we wanted the wave functions to be properly\nnormalized. More precise: the norm of the particle-in-a-box wave functions can be shown\nto be\nL\n(3.25)\nh\u03c8n , \u03c8n i = |A|2\n2\nq\nSo to achieve normalization, we have to put A = L2\nConsequence 2\nAnother thing. Lets try to calculate the expectation value of the position. So given a wave\nfunction, what is the average position you would measure? This is the number you would\nget if you would measure the position again and again and take the average - at least if\nafter each measurement you are able to put the particle back in its original state, undoing\nthe effect of the collapse of the wave function. More practical: it is the average of all the\nconsecutive position measurements of a large collection of particles, which you manage to\nput in the same state. (Just like in the two slit experiment for example.) In words, this\nquantity is given by summing for all intervals [x, x + \u2206x] the chance of finding the particle\nin that interval times the corresponding position x. So\nZ +\u221e\nx * |\u03c8(x)|2 dx\n(3.26)\nhXi\u03c8 =\n\u2212\u221e\n\nHere we have denoted the expectation value of the position for a wave function \u03c8 by hXi\u03c8 .\nNow the above can be written in a very compact way using the bra-ket notation:\nhXi\u03c8 = h\u03c8|X|\u03c8i\n\n(3.27)\n\nIndeed, acting with the operator X on the function \u03c8(x) gives x * \u03c8(x), so the state X|\u03c8i\n( =|X\u03c8i) has wave function x\u03c8(x) and\nh\u03c8|X|\u03c8i = h\u03c8|X\u03c8i\nZ \u221e\n\u03c8(x) (x * \u03c8(x)) dx\n=\nZ\u2212\u221e\n\u221e\nx|\u03c8(x)|2 dx\n=\n\n(3.28)\n(3.29)\n(3.30)\n\n\u2212\u221e\n\nThe object h\u03c8|X|\u03c8i is sometimes called the sandwich of the operator X inside the state\n\u03c8. This origin of this term is probably clear. In general, we call the object h\u03c8|A|\u03c6i the\nsandwich of operator A between states \u03c8 and \u03c6:\n\n\f51\n\nCHAPTER 3. THE MEASUREMENT\n\n<\u03c8|A|\u03c6>\nFigure 3.1: Two sandwiches\n\nWe can cryptically summarize the above paragraph by stating that the expectation\nvalue of the position for a state is given by the sandwich of the position operator X inside\nthat state. If you understand this sentence, you have clearly gotten the above. We are\nnow ready for another important example: a particle in a harmonic potential. There, we\nwill be able to apply all the bra-ket machinery.\n\n3.3.1\n\nExample: the harmonic oscillator\n\nLet's consider a particle with mass m in a quadratic potential:\nV (x) =\n\n1\nm\u03c9 2 x2\n2\n\n(3.31)\n\nClassically, an object in such a potential swings perpetually around its equilibrium, with\nfrequency \u03c9. That is indeed what a marble would do in such a potential (ignoring friction).\nBut to see what a elementary particle in such a potential looks like, we need to use the\nquantum mechanical description. The key is of course the SE (short for Schr\u00f6dinger\nEquation):\ni~\u2202t \u03c8(x, t) = H\u03c8(x, t)\n(3.32)\nFor this potential, the Hamiltonian is given by\nH=\u2212\n\n~2 2\nP2\nm\u03c9 2 2\nDx + V (X) =\n+\nX\n2m\n2m\n2\n\n(3.33)\n\nIn the last step we have also defined the momentum operator\nP = \u2212i~Dx .\n\n(3.34)\n\nGeneral recipe\nThe strategy is exactly the same as for the particle in a box. Assume we can solve the\ntime-independent SE. That is, assume we can find energy eigenstates |\u03c8n i:\nH|\u03c8n i = En |\u03c8n i\n\n(3.35)\n\nThen their fully time dependent counterparts are given by\n\u03c8n (x, t) = e\u2212iEn t/~ \u03c8n (x)\n\n(3.36)\n\n\f52\n\nCHAPTER 3. THE MEASUREMENT\nV(x)\n\n\u03c8(x)\n\nx\n\nFigure 3.2: The (classical) movement of a macroscopic object in a harmonic potential is\nshown by the oscillating point. For a microscopic particle things will look differently: the\nparticle will take on some specific wave function \u03c8(x) inside the potential.\n\nand these automatically satisfy the SE. So the problem is reduced to one real task: finding\nall the wave functions \u03c8n (x) that solve to the equation\n\u2212\n\n1\n~2 d2\n\u03c8n (x) + m\u03c9 2 X 2 \u03c8n (x) = En \u03c8n (x).\n2\n2m dx\n2\n\n(3.37)\n\nNow there is bad news and good news. The bad news: unlike for the particle in a box, we\nnow have a nasty potential term in the Hamiltonian. This makes the above equation is\nnot easy to solve. The good news: there is a neat trick to solve the problem in an elegant\nfashion. Here goes.\n\n3.3.2\n\nA neat trick\n\nLet us define the following operators:\na =\n\nr\n\nr\n\nm\u03c9\n2~\n\n\u0012\n\nX+\n\ni\nP\nm\u03c9\n\n\u0013\n\n\u0013\n\u0012\nm\u03c9\ni\na =\nP\nX\u2212\n2~\nm\u03c9\nr\n\u0013\n\u0012\n\u0012\n\u0013\n\u0012\n\u0013 r\ni \u2217 \u2020\ni\nm\u03c9\nm\u03c9\n\u2020\n\u2020\nP = a\u2020\nP\nX +\nX\u2212\n=\n(a) =\n2~\nm\u03c9\n2~\nm\u03c9\n\u2020\n\n(3.38)\n\n(3.39)\n\nIn the first step, we used rule (3.13) for Hermitian conjugation, namely\n(aA + bB)\u2020 = \u0101A\u2020 + b\u0304B \u2020\n\nfor all numbers a, b and operators A, B.\n\n(3.40)\n\nand in the second step, we used the fact that X \u2020 = X and P \u2020 = P , a property you can\neasily verify (see exercises) - but just buy it for now. If we now compose those operators\n\n\f53\n\nCHAPTER 3. THE MEASUREMENT\ninto a new operator a\u2020 a, we get:\n\u2020\n\naa =\n=\n=\n\n\u0013\u0012\n\u0013\n\u0012\ni\nm\u03c9\ni\nP\nX\u2212\nP\nX+\n2~\nm\u03c9\nm\u03c9\n\u0012\n\u0013\nm\u03c9\ni\n(XP \u2212 P X) + P 2\nX2 +\n2~\nm\u03c9\n\u0012\n\u0013\nm\u03c9\n~2\n~\n2\n2\n(XD \u2212 DX) \u2212 2 2 D\nX +\n2~\nm\u03c9\nm \u03c9\n\n(3.41)\n\nIn the last line we did not write the index x under the differential operator D - since it\nis clear we mean differentiation with respect to x. If X and D were ordinary numbers,\nthe object XD \u2212 DX would be zero. But they are operators, and hence XD is not\nnecessarily the same as DX! Lets think about this more careful. The operator XD takes\nthe derivative of a function, and then multiplies by the identity function x.\nXD : f \u2192 XDf = xf \u2032\n\n(3.42)\n\nBut operator DX does something else: first it multiplies by x, then it takes the derivative\nof that object. Using the product rule of differentiation, we get\nDX : f \u2192 DXf = D(x * f ) = f + xf \u2032\n\n(3.43)\n\nSo XD and DX are really different operators. In general, for operators A and B, the\nobject AB \u2212 BA is called the commutator, and is denoted by [A, B]:\n[A, B] = AB \u2212 BA\n\n(3.44)\n\nIf the commutator of two operators is zero, then AB = BA, and we say the two operators\ncommute. In words: for commuting operators their order doesn't matter. If the commutator of two operators is not zero, we say they don't commute, and then AB 6= BA.\nOk, back to the above. As XD 6= DX, we see that D and X don't commute. In fact, for\nany f we have\n(XD \u2212 DX)f = xf \u2032 \u2212 (f + xf \u2032 ) = \u2212f = \u22121 * f\n(3.45)\nSo here the commutator is given by:\n[X, D] = \u22121\n\n(3.46)\n\nwhere 1 is the unity operator we met before. Using this, we conclude that\n\u0012\n\u0013\n~2\n~\nm\u03c9\n2\n2\n\u2020\n+\nD\nX \u2212\na a=\n2~\nm\u03c9 m2 \u03c9 2\n\n(3.47)\n\nSo the Hamiltonian can be written as\n\u0012\n\n1\nH = ~\u03c9 a a +\n2\n\u2020\n\n\u0013\n\n(3.48)\n\n\f54\n\nCHAPTER 3. THE MEASUREMENT\n\nWoah, what a coincidence! In terms of the a's, the Hamiltonian looks quite simple. With\na similar calculation to the above, you can check that\n[a, a\u2020 ] = 1.\n\n(3.49)\n\nAnd from (3.48) and (3.49) one can show\n[H, a] = \u2212~\u03c9a\n\nand\n\nh\n\ni\nH, a\u2020 = ~\u03c9a\u2020\n\n(3.50)\n\nGiving us even more remarkable results. But what do we get from all these formulas?\nHold on for a moment, we will extract some important consequences ater the next section.\nFirst, we need one last sidestep.\n\n3.3.3\n\nSidestep: getting familiar with bras\n\nSuppose you have a state |\u03c8i. Then a|\u03c8i is another state. Here is a question: what is the\nbra state corresponding to this ket state a|\u03c8i? Recall that the bra and ket are related by\nHermitian conjugation:\nh\u03c8| = |\u03c8i\u2020\n(3.51)\n\nSo the bra version of a|\u03c8E i is given by (a|\u03c8i)\u2020 . From formula (3.14) we know that for two\noperators (AB)\u2020 = B \u2020 A\u2020 . It turns out that for the product of a state and an operator,\nthe same property holds. We will not prove this, and use it as a given rule. In particular\nthis means that\n(a|\u03c8i)\u2020 = (|\u03c8i)\u2020 a\u2020 = h\u03c8|a\u2020\n(3.52)\nSo the object on the right hand side is the bra version of the ket a|\u03c8i. Now, recall\nthat the norm squared of a state is obtained by putting the bra and the ket together:\nk |\u03c8i k2 = h\u03c8|\u03c8i. This means that the squared norm of the state a|\u03c8i is given by\nk a|\u03c8ik2 = h\u03c8|a\u2020 a|\u03c8i\n\n(3.53)\n\nYou can read the object on the right hand side in different ways. Either you see it as the\nbra h\u03c8|a\u2020 followed by the ket a|\u03c8i, or you see it as the operator a\u2020 a sandwiched inside the\nh\u03c8| and |\u03c8i. What viewpoint you take does not really matter - all really describe the same\nobject. In mathematical terms: building objects like in the above line is an associative\noperation: it does not matter which objects you want to group or put around brackets, so\n(h\u03c8|a\u2020 )(a|\u03c8i) = h\u03c8|(a\u2020 a)|\u03c8i = h\u03c8|(a\u2020 a|\u03c8i) = ...\n\n(3.54)\n\nand so on. This might seem a bit suspicious at first, so a comparison with ordinary vectors\nand matrices might be in place. Imagine you want to compute the norm of the vector v,\nwith\n\u0012\n\u0013\u0012 \u0013\n1 0\n1\nv=\n(3.55)\ni 1\n0\n\n\f55\n\nCHAPTER 3. THE MEASUREMENT\nYou can do this in two ways. Of course, you can compute that v =\nnorm is given by\n2\n\n\u2020\n\nkvk = v v =\n\n1 \u2212i\n\n\u0001\n\n\u0012\n\n1\ni\n\n\u0013\n\n\u0012\n\n1\ni\n\n\u0013\n\nand hence the\n\n=2\n\nHowever, you can also directly take the conjugate of (3.55), so that\n\u0012\n\u0013\u0012\n\u0013\u0012 \u0013\n\u0001 1 \u2212i\n1 0\n1\n2\n\u2020\nkvk = v v = 1 \u2212i\n0 1\ni 1\n0\n\n(3.56)\n\n(3.57)\n\nIf you compute the above line, you will again find 2 as an answer. But what we want to\nstress, is that it does not matter in what order you perform the matrix products. You can\nstart out left, or you can multiply the matrices in the middle first, whatever you like. So\nhere too, you can view the object in different (equivalent) ways, thanks to associativity.\nThe situation with (3.53) is precisely the same.\n\n3.3.4\n\nThe fling\n\nWe can now start using all the above. Suppose there is a (normalized) state |\u03c8E i that is an\nenergy eigenstate (=eigenstate of the Hamiltonian) with energy E. Then a|\u03c8E i is another\nstate, and from the above we know that its norm is given by h\u03c8E |a\u2020 a|\u03c8E i. Because the\nnorm of that state is positive (this is always the case), we must have\n0 \u2264 h\u03c8E |a\u2020 a|\u03c8E i\nExpressing a\u2020 a from the above expression in terms of the Hamiltonian, we get\n\u0013\n\u001d\n\u001c\n\u0012\n1\nH\n\u2212\n\u03c8E\n0 \u2264\n\u03c8E\n~\u03c9 2\n\u001c\n\u0013\n\u0012\n\u001d\n1\nE\n=\n\u03c8E\n\u2212\n\u03c8E\n~\u03c9 2\n\u0012\n\u0013\nE\n1\n=\n\u2212\nh\u03c8E |\u03c8E i\n~\u03c9 2\n\u0013\n\u0012\n1\nE\n\u2212\n=\n~\u03c9 2\n\n(3.58)\n\n(3.59)\n(3.60)\n(3.61)\n(3.62)\n\nIn the first step we have used that |\u03c8E i is an eigenstate of H with eigenvalue E (by\nassumption). Then, we dragged some things out of the bracket to the left: this is OK\nbecause they are all just numbers.The last step then uses the normalization of |\u03c8E i. In\nfact, we have also used the linearity property of the bracket implicitly in the first two\nsteps. If you now compare the first and last part, you see that E \u2265 ~\u03c9/2. So every state\n\n\f56\n\nCHAPTER 3. THE MEASUREMENT\n\nhas an energy of at least 21 ~\u03c9. That is already a nice result. But there is more. What if\nwe act with the Hamiltonian on this state a|\u03c8E i?\nH(a |\u03c8E i) = ([H, a] + aH) |\u03c8E i\n\n= (\u2212~\u03c9a + aE) |\u03c8E i\n\n= (E \u2212 ~\u03c9)(a |\u03c8E i)\n\nThe second step uses (3.50). In the last step, we dragged E to the left. This is OK because\nit is just a number, so there is no problem when you pass it to the other side of an operator\nlike a. Comparing the first and last part, you see the state a |\u03c8E i is an eigenstate of the\nHamiltonian with energy E \u2212 ~\u03c9. So it is a state with one 'step'(= ~\u03c9) energy less than\n|\u03c8E i. Similarly, you can show that\nH(a\u2020 |\u03c8E i) = (E + ~\u03c9)(a\u2020 |\u03c8E i)\n\n(3.63)\n\nMeaning that a\u2020 |\u03c8E i is an energy eigenstate as well, but with one 'step' more energy. This\nmeans that (given one energy eigenstate) we can build many more energy eigenstates, with\nlower or higher energy, by acting with the operators a\u2020 and a. That's why a\u2020 and a are\ncalled the raising and lowering operator, as they raise and lower the energy of a state\nby one energy step ~\u03c9.\nNow there might seem to be a contradiction here: by acting repeatedly with a, we can\nget a series of states with lower and lower energy, but we just found that the minimum\nenergy is Emin = 12 ~\u03c9. This can only be reconciled if there is some state |0i, for which\na |0i = 0\n\n(3.64)\n\nso that the lowering process ends there. The right hand side here is the zero ket, a state\nwith wave function equal to zero, meaning there is no particle at all. Any subsequent\napplication of the lowering operator will just give the zero ket again, instead of additional\nenergy eigenstates. So the state |0i necessarily is the state with lowest energy:\n1\nH |0i = ~\u03c9 |0i\n2\n\n(3.65)\n\nStarting from this lowest energy state we can now build up a series of states (using a\u2020 ),\nwhich have higher and higher energy. (We may have to normalize them properly, but that\ndoes not affect their energy.) We denote this tower of states by\n{|0i , |1i , |2i , ..., |ni , ...}\n\n(3.66)\n\nTheir energies are given by\n\u0012\n\n1\nH |ni = ~\u03c9 n +\n2\n\n\u0013\n\n|ni .\n\n(3.67)\n\nSo, in a very elegant way we have solved the spectrum of the harmonic oscillator. We have\nconstructed a series of energy eigenstates, and know that these are the only ones around.\n\n\f57\n\nCHAPTER 3. THE MEASUREMENT\n\nThe time-dependent solutions are given by multiplying each state |ni by e\u2212iEn t/~ , like in\n(3.36). So this solves the problem of describing a particle in a harmonic potential.\nOf course, you may have some difficulties imagining what these states look like. If so,\nyou will find relief in the next section.\n\nTheir shape\nMaybe we should start with the lowest energy state. The lowest energy state satisfies\na |0i = 0 so by definition of a the wave function \u03c8n=0 (x) corresponding to |0i satisfies\nx\u03c80 (x) +\n\n~ d\u03c80\n(x) = 0\nm\u03c9 dx\n\n(3.68)\n\nwhich can be shown to be solved by the wave function\nm\u03c9\n\n\u03c80 (x) \u221d e\u2212 2~\n\nx2\n\n(3.69)\n\nor (after normalization)\n\u03c80 (x) =\nand its time evolution is given by\n\n\u0010 m\u03c9 \u0011 1\n\n4\n\n\u03c0~\n\nm\u03c9\n\n2\n\ne\u2212 2~ x .\n\n(3.70)\n\n\u03c80 (x, t) = \u03c80 (x)e\u2212iE0 t/~ = \u03c80 (x)e\u2212i\u03c9t/2\n\n(3.71)\n\nIn a similar fashion, one can explicitly find the wave function of the other energy eigenstates\n|2i, |3i, etcetera. This is a bit tedious, and we will just show the result pictorially:\n\n\u03c84(x)\n\u03c83(x)\n\u03c82(x)\n\u03c81(x)\n\u03c80(x)\n\nWhat about measuring the position of a particle in one of these states? As you know,\nthe chance to get an outcome x from a measuring device is proportional to |\u03c8(x)|2 . From\nthe figure you see \u03c8n (x) becomes more and more spead out with increasing n. So for\nhigher energy states, it becomes more and more likely to measure the particle to be far\naway from the origin. Intuitively, the particle spreads more throughout the potential as it\n\n\f58\n\nCHAPTER 3. THE MEASUREMENT\n\ngains energy, somewhat like a classical oscillator gaining amplitude with increasing energy.\nWhat about the expectation value of the position? This is given by\nZ \u221e\nx|\u03c8n (x)|2 dx\n(3.72)\nhXi\u03c8n =\n\u2212\u221e\n\nAs you see from the figure, the wave functions \u03c8n (x) are all symmetric or anti-symmetric:\n\u03c8n (x) = \u00b1\u03c8n (\u2212x). Hence the squared amplitude has |\u03c8n (x)|2 = |\u03c8n (\u2212x)|2 . So the\nintegrand x|\u03c8n (x)|2 above is an anti-symmetric function: the value at x and \u2212x are exactly\nopposite. Such functions have integral zero, since the contributions from the positive and\nnegative integration domain exactly cancel. So\nhXi\u03c8n = 0 \u2200n\n\n(3.73)\n\nSo we find that the expectation value of the position measurement is zero, for all states.\nThis is not so surprising: after all the system is left- right symmetric. Of course, this does\nnot mean you will always get 0 as an outcome of the measurement: the above is just a\n'weighted average' - which happens to be zero.\n\n3.3.5\n\nConclusion\n\nSummarising, there are several special aspects to this system. First, it is remarkable that\nthe lowest energy is not zero. Classically, a particle at rest in the potential has zero\nkinetic and potential energy, and hence zero total energy. For a wave function, this turns\nout not to be the case: the lowest energy it can reach is 12 ~\u03c9. Some people express this (in a\nsomewhat obscure, but poetic phrasing) as: \"Quantum mechanically, a harmonic oscillator\ncan never be at rest.\" By this they just mean: \"A particle in a harmonic potential can\nnot have zero energy.\" Besides that, we also see some similarities with the particle in a\nbox. Again, only certain energy levels are admitted. There is no state with energy 3.1~\u03c9\nfor example. This means the transitions between different energy levels of the particle can\nrelease only specific amounts of energy: only multiples of ~\u03c9. In the classical theory of\n(point-like) electrons and electromagnetism, an electron that is harmonically oscillating\nat frequency \u03c9 emits radiation with frequency \u03c9, hereby lowering its energy. However,\nthis loss of energy can occur in arbitrary small steps. According to the correct (quantum\nmechanical) description, the situation is different. Here the particle can emit energy too,\nbut it is clear that this radiation should always have energy n~\u03c9 for some natural number\nn. Do you smell n photons of frequency \u03c9 coming out of the particle? Nice, huh?\nYou may now understand better where the name 'Quantum Mechanics' comes from.\nIn the situation here, for the particle in a box and in many more examples we will see\nthat discrete amounts of energy are typically involved. We meet discrete energy levels,\na photon is a discrete package of energy, and so on. The word 'quantum' (derived from\nLatin) precisely means 'discrete amount' or 'small package'. So Quantum Mechanics is\nin essence the description of systems involving discrete amounts of energy: the world of\nparticles.\n\n\fCHAPTER 3. THE MEASUREMENT\n\n59\n\nExercises\n1. Prove (3.12), (3.13) and (3.14). (Hint: you'll need the conjugate symmetry and\nbilinearity.)\n2. Compute [X 2 , Dx ].\n3. Prove that if an energy eigenstate is normalized, then it will still be normalized at\nany later moment in time. (Just write down its time evolution and take the norm.)\n\u0002\n\u0003\n4. Verify [H, a] = \u2212~\u03c9a and\nH, a\u2020 = ~\u03c9a\u2020 .\n5. Compute \u03c81 (x) from the expression for \u03c80 (x). (You don't need to normalize it.)\nTry to draw this function. Does it match the graph shown in section 3.3.4? What\nis the time-evolution of this state?\n\n6. Imagine you can create an electric field of the form E = \u2212\u03bax with \u03ba = 100V /m2 .\n(Check that the units are right.) Put a proton (mp = 1, 67*10\u221227 kg, qp = +1, 6*10\u221219\nC) in that field - it will stay trapped around x = 0. What frequency should a photon\nhave to be able to kick up the proton from the lowest to the next-to-lowest energy\nlevel?\n7. Prove that the operators X and P are Hermitian. (For the second: think of partial\nintegration. Why can you drop the boundary terms for a wave packet?)\n\n\fChapter 4\n\nObservables\nIn this chapter...\nIn the previous chapter, we learned how a position measurement works out in quantum\nmechanics. In this chapter, we will extend this further, and explain how general measurements (such as the energy or velocity of a particle) work. Doing so, we will learn\nmore about the space of states of a particle. We first introduce the concepts 'Hermitian\noperator' and 'state basis'. Then follows the story part (which generalizes the previous\nchapter to other measurements) and we conclude by a simple example.\n\n60\n\n\f61\n\nCHAPTER 4. OBSERVABLES\n\n4.1\n4.1.1\n\nHermitian operators and the Hilbert space\nHermitian operators\n\nYou know by now that to each operator A, we can associate another one, its Hermitian\nconjugate A\u2020 . It turns out that for some special operators, the following holds:\nA = A\u2020\n\n(4.1)\n\nBy this we just mean that for every function f , the outputs Af and A\u2020 f are equal.\nSuch operators are called Hermitian operators. As we will learn in this chapter, such\noperators are very important in quantum mechanics. One special property they exhibit,\nis that they only have real eigenvalues. Indeed, suppose a Hermitian operator L has an\neigenvalue \u03bb. Then there exists a function f such that Lf = \u03bbf , and\n\u03bbhf, f i = h\u03bbf, f i = hLf, f i = hf, Lf i = \u03bbhf, f i.\n\n(4.2)\n\nIn these steps we used bilinearity of the inner product, Hermiticity of L, and the fact that\nf is an eigenfunction. Since hf, f i is nonzero (true for every nonzero f ) the left and right\nside of the above equation gives \u03bb = \u03bb, so \u03bb must be real. This shows that every eigenvalue\n\u03bb of a Hermitian operator indeed has to be real. Besides that, an important property of\nHermitian operators is that their eigenfunctions are orthogonal. By this we mean\nthat eigenfunctions with different eigenvalues have an inner product equal to zero. Indeed,\nimagine functions f1 and f2 with eigenvalues \u03bb1 and \u03bb2 (6= \u03bb1 ) under a Hermitian operator\nL. Then\n(4.3)\n\u03bb2 hf1 , f2 i = hf1 , Lf2 i = hLf1 , f2 i = \u03bb1 hf1 , f2 i = \u03bb1 hf1 , f2 i\nIn the last step we used the fact that an eigenvalue \u03bb1 has to be real. Once again combining\nthe first and last side of the equation, we now get\n(\u03bb2 \u2212 \u03bb1 )hf1 , f2 i = 0\n\n(4.4)\n\nWhich (as \u03bb2 6= \u03bb1 ) means that hf1 , f2 i = 0, so indeed f1 and f2 are orthogonal.\n\n4.1.2\n\nA space of states\n\nIn the previous chapters, we solved two systems: a particle in a box and a particle in\na harmonic potential. In either case, we found an infinite tower of states. Each state\ndescribes a specific wave function the particle can be in. You may wonder what the\nmeaning is of sums of these wave functions - or in general: any linear combination. These\nare complex functions as well. And they satisfy the same good properties: they are smooth\nand have a decent fall-off (so they can be normalized). On these grounds, we could be\ntempted include these combinations as valid wave functions as well. Doing so, the space\n\n\f62\n\nCHAPTER 4. OBSERVABLES\n\nof states becomes a complex vector space! That space is called the Hilbert space. So if\n\u03c81 (x) and \u03c82 (x) are the two wave functions (i.e.: elements of the Hilbert space) then\n\u03c7(x) = a\u03c81 (x) + b\u03c82 (x)\n\n(4.5)\n\nis a wave function as well, for any value of the complex numbers a and b. You may still\nneed to normalize this wave function \u03c7(x), but that is just a matter of multiplying by a\nconstant - not really a deep issue. In the language of kets:\n|\u03c7i = a|\u03c81 i + b|\u03c82 i\n\n(4.6)\n\ngives a new state |\u03c7i built from the states |\u03c81 i and |\u03c82 i. So if you take the sum of two\nkets, it is understood that you mean the state whose wave function is the sum of the wave\nfunctions:\na|\u03c81 i + b|\u03c82 i = |a\u03c81 + b\u03c82 i\n(4.7)\nOf course, we can also take linear combinations of more than two elements. This way, all\nlinear combinations of the original set of states become elements of the Hilbert space. So\nthe tower of states we found for the harmonic oscillator and the particle in a box form a\nbasis of the Hilbert space.\nFrom a 'the more the merrier' point of view, the above is not really an unpleasant fact.\nHowever, something might bother you. We were very proud that we could explicitly write\ndown the time-evolution of the states we found. But now we have a whole new zoo of\nstates that we have to consider. How do all these evolve in time? This turns out to work\nout nicely. Let us look at the Schr\u00f6dinger equation once again:\ni~\u2202t \u03c8(x, t) = H\u03c8(x, t)\n\n(4.8)\n\nThis differential equation is of a special kind: it is a linear differential equation. That\nmeans that if you have two solutions satisfying it, every linear combination satisfies it too.\nLet us make this more concrete. Say at t = 0 we consider two wave functions \u03c81 (x, 0)\nand \u03c82 (x, 0) that are energy eigenfunctions with values E1 and E2 . We know that their\ntime-evolution (dictated by the SE) is just given by\n\u03c81 (x, t) = \u03c81 (x, 0)e\u2212iE1 t/~\n\n\u03c82 (x, t) = \u03c82 (x, 0)e\u2212iE2 t/~\n\n(4.9)\n\nBut what if at t = 0 we start out with the state \u03c7(x, 0) = a\u03c81 (x, 0) + b\u03c8(x, 0). How does\nthis evolve in time: what is \u03c7(x, t)? A priori this could be everything, but because of\nlinearity, the function\n\u03c7(x, t) = a\u03c81 (x, t) + b\u03c82 (x, t) = a\u03c81 (x, 0)e\u2212iE1 t/~ + b\u03c82 (x, 0)e\u2212iE1 t/~\n\n(4.10)\n\nautomatically satisfies the SE since \u03c81 (x, t) and \u03c82 (x, t) do. In conclusion: the time\nevolution of a linear combination is just the linear combination of the time evolutions.\nThis means we do not need to do any extra work when describing the entire Hilbert space.\nTime for the story...\n\n\f63\n\nCHAPTER 4. OBSERVABLES\n\n4.2\n\nObservables\n\nIn the previous chapter, we learned about the probability amplitude and how measuring\na particle's position deforms its wave function. Of course, besides position there are many\nother properties of a particle that you may measure: energy, velocity, angular momentum,\nor any combination like 'position times momentum' or 'position squared' and so on. Every\nquantity of a particle that you can measure (or 'observe') is called an observable. It\nturns out that every observable is associated to a unique operator. So there is a strict\ncorrespondence between the numerical quantities we can observe in experiments with\nparticles, and operators acting on the space of wave functions (Hilbert space). Here is a\nlist of pairs we have met so far:\nenergy (E) \u2194 Hamiltonian operator H\n\nposition (x) \u2194 position operator X\n\nmomentum (p) \u2194 momentum operator P ( = \u2212i~Dx )\n\n(4.11)\n\nNow comes a very nice surprise: it turns out that all observables -including the aboveare Hermitian. That is a very nice property: we just learned that the eigenfunctions of a\nHermitian operator are orthogonal. If you normalize those states, you get an orthonormal\nbasis.\nThe value of such an orthonormal basis can not be stressed enough. To see why, think\nback of an ordinary complex vector space for a little moment. If you have a basis {e1 , ...en }\nthen every element v in the vector space can be written as\nv = v1 e1 + ... + vn en\n\n(4.12)\n\nfor precisely one combination {v1 , ...vn } of complex numbers. (We don't write any vector\narrows, out of laziness.) So the advantage of a basis is clear: it allows specifying every\nelement of the vector space uniquely, by a set of numbers. The problem is that (for a\ngiven vector) it is not immediately clear how to find the above decomposition. But if a\nvector space is endowed with an inner product, things are different. One can try to find a\nspecial basis, namely an orthonormal one. By orthonormal we mean that\nhei , ej i = \u03b4ij\n\n(4.13)\n\nHere \u03b4ij is the Kronecker delta function, which equals one if i = j and equals zero if i 6= j.\nFor such a basis, and any vector v in the vector space,\nhe1 , vi = he1 , v1 e1 + ... + vn en i = v1 * 1 + 0 + ... + 0 = v1 .\n\n(4.14)\n\nWhere we used linearity of the inner product and orthonormality of the basis. So the\nnumber v1 (and analogously any of the vi ) can be obtained just by taking the inner\n\n\f64\n\nCHAPTER 4. OBSERVABLES\n\nproduct of v with e1 (or any of the ei ). This is a very strong fact: not only is every\nvector uniquely specified by a set of numbers, this decomposition can be obtained easily\nby taking the inner product. The quantities vi are called the projection of v on ei . They\ntell you how much v is directed along the vector ei . An example in two dimensions, the\nvector ~v = 2~e1 + ~e2 and its projections:\ne2\n\nv = 2 e1 +e 2\n\n< v ,e2> =1\ne1\n< v ,e 1> = 2\n\nThings are very similar for a Hilbert space. There too, we have a basis. There too,\nwe want to be able to write every element in terms of this basis. But in the context of\nthe Hilbert space, the advantage of an orthonormal basis is even more drastic. In that\ncase the set of basis elements is typically infinite. If someone gives you a function, and\nasks to decompose it as a combination of basis elements, you will have a very hard time.\nFor an orthonormal basis, things are not that harsh: you can obtain that decomposition\nin a straightforward manner. Indeed, the projections on the basis elements are just given\nby the inner product, which requires just performing a definite integral. Later on, we will\nperform some concrete decompositions. What you should remember from this, is that the\nset of eigenstates of an observable provides a very good basis of the Hilbert space: an\northonormal one. So every observable is a bit like a fairy, it can help making your deepest\nwish come true.\nThink back of -say- the particle in a box for a moment. There we found eigenstates\nof the Hamiltonian. You now know that this operator is Hermitian, so you conclude that\nthe different energy states we found were orthonormal. Indeed, if you like performing\nintegrals, feel very welcome to try to compute the inner product between different energy\nstates. It will always give you zero.\nNow imagine that -for a given problem- you have found the eigenstates/ eigenfunctions\nof some observable O. What is the physical meaning of these eigenstates? Say f.e. that you\nhave a eigenstate under the operator O with eigenvalue \u03bb. It is natural to interpret this\nas a state for which the observable corresponding to O has value \u03bb. Actually,\nwe used a specific case of this interpretation already: we interpreted eigenstates of the\nHamiltonian with eigenvalue E as states with energy E. Similarly, an eigenstate of the\nmomentum operator P with eigenvalue p will just mean a particle with momentum p,\netcetera.\n\n\f65\n\nCHAPTER 4. OBSERVABLES\n\n4.2.1\n\nNon-eigenstates\n\nAlso the negation of the above interpretation makes sense. A state that is not an eigenstate\nof O is a particle for which the measurable quantity corresponding to O does not have\na specific value. Let's be more concrete. Suppose the eigenstates of an observable O are\ngiven by states |\u03c8n i, with corresponding eigenvalues \u03bbn . If we have also normalized the\n|\u03c8n i, then they form an orthonormal basis:\nh\u03c8n |\u03c8m i = \u03b4mn .\n\n(4.15)\n\nX\n\n(4.16)\n\nNow a state of the form\n|\u03c8i =\n\nn\n\ncn |\u03c8n i\n\n(with the cn 's complex numbers) is clearly not an eigenstate of O. (Unless only one of the\nci is nonzero.) If you measure the observably quantity corresponding to O of a particle in\nthe above state, your apparatus will give you any of the values \u03bbi at random, but the\nprobability of getting a specific \u03bbi as an answer is given by\n|ci |2\nP\u03c8 (\u03bbi ) = probability to measure \u03bbi = P\n2\nn |cn |\n\n(4.17)\n\nJust as with measuring the position, the measurement drastically changes the state. If your\nmeasurement results in some \u03bbi , the state will collapse onto the corresponding eigenstate,\nso\nif measurement gives outcome \u03bbi then |\u03c8i \u2192 |\u03c8i i\n(4.18)\nLet us look at the denominator in (4.17) for a moment. This object is just the norm of\nthe state |\u03c8i.\n!\n!\nX\nX\ncm |\u03c8m i\n(4.19)\nc\u0304n h\u03c8n |\nh\u03c8|\u03c8i =\nm\n\nn\n\n=\n\n=\n\nXX\n\nc\u0304n cm \u03b4nm\n\nn\n\nm\n\nX\n\n|ci |2\n\ni\n\n(4.20)\n\n(4.21)\n\nFrom this, it follows that expression (4.17) does not change if one replaces the state |\u03c8i\nby any complex multiple a|\u03c8i of itself: both the numerator and denominator will scale up\nwith a factor |a|2 . This implies that the seemingly different states |\u03c8i and a|\u03c8i give the\nsame measurement probabilities, so they represent the same physical object. Stated\nmore roughly: they are just different names for the same thing. This is of course related to\nthe issue of normalization. States |\u03c8i and a|\u03c8i may look differently, after normalization,\n\n\f66\n\nCHAPTER 4. OBSERVABLES\n\nthey are the same object.1 If we demand |\u03c8i to be normalized from the beginning, (4.17)\nsimplifies a bit. In that case the denominator on the right equals one, so that\nP\u03c8 (\u03bbi ) = |ci |2\n\n(for normalized |\u03c8i)\n\n(4.22)\n\nNote that the total chance of measuring any of the \u03bbi is one, as should be. The expectation\nvalue of the observable O is then given by\nP\nX\n\u03bbi |cn |2\nhOi\u03c8 =\n\u03bbi P\u03c8 (\u03bbi ) = Pi\n(4.23)\n2\nn |cn |\ni\n\nYou can check that the above can be rewritten in a compact fashion using the sandwich\nof O:\nh\u03c8|O|\u03c8i\nhOi\u03c8 =\n(4.24)\nh\u03c8|\u03c8i\nwhich is very similar to the expression for the position expectation value. For a normalized\nstate, the denominator is again one. So this is how the measurement works for observables\nother than the position. Time for an example!\n\n4.3\n4.3.1\n\nWorking with the Hilbert space\nExample\n\nTo illustrate all the above, let's go back to -say- the harmonic oscillator. Recall that we\nfound\n\u0013\n\u0012\n1\n|ni\n(4.25)\nH|ni = ~\u03c9 n +\n2\n\u0001\nor by using H = ~\u03c9 a\u2020 a + 12 ,\na\u2020 a|ni = n|ni\n(4.26)\nFor this reason, the object a\u2020 a is called the number operator - it just gives you the\nstate number. (More precise: the n-th energy eigenstate is an eigenstate of the number\noperator as well, with eigenvalue n.) If we require that |ni is a normalized state, we have\nhn|a\u2020 a|ni = nhn|ni = n\n\n(4.27)\n\nWe also found that a|ni is a state with one quantum less energy. So necessarily a|ni \u223c\n|n \u2212 1i. There could be a proportionality constant involved here: |n \u2212 1i is normalized\nbut a|ni may not be so. Indeed, the above equation precisely shows that a|\u03c8i has norm\nsquared equal to n. So we conclude\n\u221a\n(4.28)\na|ni = n|n \u2212 1i\n1\n\nTo be overly precise: even if one asks |\u03c8i to be normalized, there is still some freedom of renaming:\nall states a|\u03c8i with a = ei\u03b8 (a complex number of unit modulus) are still normalized, and all represent the\nvery same physical state.\n\n\fCHAPTER 4. OBSERVABLES\nSimilarly, acting with a\u2020 on this equation and renaming n \u2192 n \u2212 1 shows that\n\u221a\na\u2020 |ni = n + 1|n + 1i.\n\n67\n\n(4.29)\n\nActually, there is a nice way to make all these things more concrete. Let us denote the\nstate |ni by the n-th basis vector:\n\uf8eb \uf8f6\n0\n\uf8ec .. \uf8f7\n\uf8ec . \uf8f7\n\uf8ec \uf8f7\n\uf8ec 0 \uf8f7\n\uf8f7\n|ni \u2194 \uf8ec\n(4.30)\n\uf8ec 1 \uf8f7\n\uf8ec \uf8f7\n\uf8ec 0 \uf8f7\n\uf8ed \uf8f8\n..\n.\n\nwhere the 1 is on the n-th position. Obviously, to include all states we need to take the\nvector on the right hand side to be infinitely long. This is a bit funny, but the notation\nworks fine. A general state in the Hilbert space is then denoted in vector form as follows:\n\uf8eb\n\uf8f6\nc0\n\uf8ec c1 \uf8f7\n\uf8ec\n\uf8f7\nX\n\uf8ec\n\uf8f7\n|\u03c8i =\ncn |ni \u2194 \uf8ec c2 \uf8f7\n(4.31)\n\uf8ec c3 \uf8f7\nn\n\uf8ed\n\uf8f8\n..\n.\n\nWe start labeling by 0 because the state labels start from zero on. In a similar way, we\ncan denote the bra version of states by a row vector. For the state |\u03c8i above, we have\n\u0001\nh\u03c8| \u2194 c\u03040 c\u03041 c\u03042 . . .\n(4.32)\n\nIn this way, the norm squared of a state is just the matrix product:\n\uf8eb\n\uf8f6\nc0\n\uf8f7\n\u0001\uf8ec\n\uf8ec c1 \uf8f7 X\nh\u03c8|\u03c8i = c\u03040 c\u03041 c\u03042 . . . \uf8ec c \uf8f7 =\n|cn |2\n\uf8ed 2 \uf8f8\nn\n..\n.\n\n(4.33)\n\nWith this notation, we can represent operators as matrices acting on the vectors. For\nexample, looking at (4.28) it follows that the lowering operator is given by\n\u221a\n\uf8f6\n\uf8eb\n1 \u221a0\n0\n0 ...\n0\n\uf8ec 0 0\n2 \u221a0\n0 ... \uf8f7\n\uf8f7\n\uf8ec\n\uf8ec 0 0\n3 \u221a0 . . . \uf8f7\n0\n(4.34)\na\u2194\uf8ec\n\uf8f7\n\uf8f7\n\uf8ec 0 0\n0\n0\n4\n.\n.\n.\n\uf8f8\n\uf8ed\n..\n..\n..\n.. ..\n.\n.\n.\n.\n.\n\n\f68\n\nCHAPTER 4. OBSERVABLES\nThe raising operator is a similar object:\n\uf8eb\n0\n0\n0\n\u221a0\n\uf8ec 1 0\n0\n0\n\uf8ec\n\u221a\n\uf8ec 0\n\u2020\n2\n0\n0\na \u2194\uf8ec\n\u221a\n\uf8ec 0\n0\n3 0\n\uf8ed\n..\n..\n..\n.\n.\n.\n\n0\n0\n0\n0\n..\n.\n\n...\n...\n...\n...\n..\n.\n\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\n(4.35)\n\nIn fact, this is just the Hermitian conjugate of the previous matrix - this just stems from\nthe fact that operators a and a\u2020 are Hermitian conjugate\n\u0001 operators. If you perform the\n(infinite) matrix product, you see that H = ~\u03c9 a\u2020 a + 12 should be written as\n\uf8eb\n\uf8f6\n0\n0\n0\n0 ...\n0 + 21\n\uf8ec 0\n0\n0\n0 ... \uf8f7\n1 + 12\n\uf8ec\n\uf8f7\n1\n\uf8ec 0\n0\n2+ 2\n0\n0 ... \uf8f7\nH \u2194 ~\u03c9 \uf8ec\n(4.36)\n\uf8f7\n1\n\uf8ec 0\n\uf8f7\n0\n.\n.\n.\n0\n0\n3\n+\n\uf8ed\n\uf8f8\n2\n..\n..\n..\n.. ..\n.\n.\n.\n.\n.\n\nThere are several observations here. First, this is a Hermitian matrix. This is actually\ndue to the fact that H is a Hermitian operator. Second, H is a diagonal matrix. This\nexpresses that the basis we use consists of eigenstates of H. More than ever, the parallel\nwith linear algebra is clearly present - although here we work with infinitely big matrices.\nLet us now consider a particular state |\u03c8i = |0i + |1i:\n\uf8eb \uf8f6\n1\n\uf8ec 1 \uf8f7\n\uf8ec \uf8f7\n|\u03c8i = \uf8ec 0 \uf8f7\n(4.37)\n\uf8ed \uf8f8\n..\n.\n\nThis state is not normalized however: h\u03c8|\u03c8i = 2. So to normalize, we need to divide by a\nconstant:\n\uf8eb \u221a \uf8f6\n1/\u221a2\n\uf8ec\n|\u03c8i \uf8ec 1/ 2 \uf8f7\n\uf8f7\n(4.38)\n|\u03c8i \u2192 \u221a = \uf8ec 0 \uf8f7\n\uf8f8\n2 \uf8ed\n..\n.\nThe state equally combines both first energy levels, so if you measure the energy, you\nexpect that there is 12 chance to detect an energy E0 , and 21 for E1 . Indeed:\nP (E \u2192 E0 ) = |c0 |2 =\n\n1\n= |c1 |2 = P (E \u2192 E1 )\n2\n\n(4.39)\n\nNow say you measured the energy of the particle to equal E0 . The state then collapses\nonto that level: |\u03c8i \u2192 |\u03c8 \u2032 i = |0i after the measurement. If you measure the energy again,\n\n\f69\n\nCHAPTER 4. OBSERVABLES\nyou are sure that you get E0 as an answer again, because now\nP (E \u2192 E0 ) = |c\u20320 |2 = 1\n\n(4.40)\n\nand all the other are zero. Indeed, the wave function collapsed. Now what was the original\nexpectation value of the energy? One-half chance on either of the energies means that\n1\n~\u03c9 + 32 ~\u03c9\nE0 + E1\n= 2\n= ~\u03c9\n(4.41)\n2\n2\njust their average. Is this equal to the sandwich of H inside |\u03c8i? Yes it is,\n1\n(h0| + h1|)H(|0i + |1i)\nh\u03c8|H|\u03c8i =\n2\n1\n=\n(h0| + h1|)(E1 |0i + E2 |1i)\n2\n1\n=\n(E0 + E1 ).\n(4.42)\n2\nFor the time evolution, we note that the time dependent version of |0i and |1i are given\nby\ne\u2212iE0 t/~ |0i and e\u2212iE1 t/~ |1i\n(4.43)\n\nhEi\u03c8 =\n\nsuch that\n\n1\n1\n(4.44)\n|\u03c8(t)i = \u221a e\u2212iE0 t/~ |0i + \u221a e\u2212iE1 t/~ |1i\n2\n2\nis the time evolution of the initial state |\u03c8i.\nA last remark: the above implies that the energy (or any other observable) of a particle\nneeds not be sharply defined: a particle may be 'smeared out' over several values of this\nobservable. This is very similar to the fact that a wave function does not have a sharply\ndefined position: simply because it is spread out over some region in space. This does not\nmean the quantum description is fuzzy or so: states of particles are well defined; it's just\nthe naive point particle idea (where position, energy, ... are all unambiguously defined)\nwhich you need to get rid of.\n\n4.3.2\n\nRecap: the postulates of quantum mechanics.\n\nSome people like to summarize. Some people like to highlight. For those souls, get your\nsharpened markers out, we are about to summarize this and all previous chapters in just\none section. Why will we do so? Well, the summary below is actually due to the pioneers\nof quantum mechanics, who had been trying to understand the theory so hard that -on\nthe moment they got it all straight- they couldn't resist listing the set of rules they met on\nthe way. This list bears the pompous name 'the postulates of quantum mechanics'.\nThese postulates are true not only for the examples we have done so far, but still hold for\nall other examples and situations we will describe in this course. So, OK, maybe they did\nhave some reason to pick such a solemn name. Here goes.\nWhen we wish to describe a particle quantum mechanically, we use the following rules:\n\n\f70\n\nCHAPTER 4. OBSERVABLES\nPostulate 1\n\nAll possible states form a complex vector space, the Hilbert space. Complex multiples\nof a state |\u03c8i correspond to the same physical state, and we prefer working with the\nnormalized version: h\u03c8|\u03c8i = 1.\nPostulate 2\nThe time evolution of a state is given by the Schr\u00f6dinger equation\ni~\u2202t |\u03c8i = H|\u03c8i\n\n(4.45)\n\nwhere H is the Hamiltonian.\nPostulate 3\nEvery observable (=measurable quantity of the system) corresponds to a Hermitian operator.\nPostulate 4\nIf the system is in a state that is an eigenstate of an observable O, with eigenvalue \u03bb,\nthen measurement of the quantity corresponding to O will yield \u03bb as an outcome. The\neigenstates |\u03c8i i of an observable O form an orthonormal basis of the Hilbert space, so\nevery state in the Hilbert space can be written as\nX\n|\u03c8i =\nci |\u03c8i i.\n(4.46)\ni\n\nFor such a (general) state, the outcome of measuring O is probabilistic. The chance that\nthe measuring device gives \u03bbi as an answer is given by\n|ci |2\nP\u03c8 (\u03bbi ) = P\n2\nn |cn |\n\n(4.47)\n\n(If the state is properly normalized, the denominator falls away (it's one) and the above\nexpression looks somewhat simpler.) After measuring O, the state of the system collapses\ninto the eigenstate corresponding to the measured eigenvalue.\n|\u03c8i \u2192 measuring O results in some \u03bba \u2192 state collapses into |\u03c8a i\n\n(4.48)\n\nPostulate 5\nThe probabilistic nature of the measurement means we can associate to a general state not\nan exact value of an observable O, but merely an expectation value. This is the average\nresult one would obtain when measuring O again and again, each time making sure to put\n\n\fCHAPTER 4. OBSERVABLES\n\n71\n\nback the system in its original state. The expectation value is given by the sandwich of\nthe operator O inside the state:\nh\u03c8|O|\u03c8i\nhOi\u03c8 =\n(4.49)\nh\u03c8|\u03c8i\n\n(Again, if the state is normalized, the denominator can be left out.)\n\n4.3.3\n\nConclusion\n\nThat's it! These are the laws of quantum mechanics. If you recognized all the above\npostulates (they all showed up earlier), and understood their meaning, congratulations.\nTo get here was the hardest task of this course. From now on, we will use the developed\nframework to get more insight into the quantum-world. So we will make the things we\nhave learned more concrete, and extend to more general situations. What does this last\nthing mean? Well, so far we have been considering a particle living in one dimension x.\nA real particle (as described in the next chapter f.e.) lives in 3 dimensions, x, y, z. So\nto be more precise, particle states are wave functions \u03c8(x, y, z). Also, we have not yet\nlived up to our promise to explain the structure of atoms. These and other things will be\ndone in the next chapters. So some other things will be added, we will do more examples,\nbut the framework (the postulates above) stays exactly the same. Even the most involved\ncomputations in particle physics are, in fact, a mere application of the above rules. Nice,\nno?\n\n\f72\n\nCHAPTER 4. OBSERVABLES\n\nExercises\n1. Without looking at the text, try to recall as much from the postulates of quantum\nmechanics as you can.\n2. The number operator for a particle in a harmonic potential is a Hermitian operator.\nWhat can you conclude about the eigenvalues of this operator. Is this indeed the\ncase?\n3. For the harmonic oscillator, write out the operators X and P in (infinite) matrix\nform. Compute the expectation values of hn|X|ni and hn|P |ni. Did you expect this\nresult? (Note that we 'computed' hn|X|ni already in the previous chapter. Your\ncomputation here was a bit more precise.) By taking matrix products, find h|XP |i.\n4. Write down your favorite quantum state of a particle in a harmonic potential that\nis not and energy eigenstate. What is the expectation value of the energy for this\nstate?\n5. Consider an observable O (which does not involve time explicitly) and a state |\u03c8(t)i\n(which is not necessarily an eigenstate of O or the Hamiltonian). Obviously, h\u03c8|O|\u03c8i\nmay depend on time, since the state depends on time. Show that\n\u2202t h\u03c8(t)|O|\u03c8(t)i =\n\ni\nh\u03c8(t)|[H, O]|\u03c8(t)i.\n~\n\n(Hint: you'll need the Schr\u00f6dinger equation.) By taking O to be the unity operator,\nprove that the norm of a state does not change in time.\n\n\fPart II\n\nAtoms and Wave Packets\n\n73\n\n\fChapter 5\n\nThe hydrogen atom\nIn this chapter...\nOne of the main motivations of the development of quantum mechanics, also mentioned\nin the introduction, was getting a better understanding of atoms and molecules. How can\nwe explain the behavior of these crucial building blocks of nature? Nice enough, we have\nalready collected the necessary pieces to start answering this puzzle: in this chapter, we\nwill describe in detail the most elementary atom: hydrogen.\nWe start with some more this-and-thats on Hilbert spaces, then comes the usual bite\nof (hi)story, and in the last part we get down to business - describing the hydrogen atom\nquantum mechanically.\n\ne-\n\np+\n\nFigure 5.1: A naive depiction of the atom. This chapter, we will learn better.\n\n74\n\n\f75\n\nCHAPTER 5. THE HYDROGEN ATOM\n\n5.1\n5.1.1\n\nOperators in three dimensions\n3D sensation\n\nIn the previous chapter, we used only one spatial variable, x, on which wave functions\ncould depend. That describes particles living in one dimension only, so on a line. Doing\nso makes life easy, and such a description can even be quite good for systems where the\ny and z dependence is trivial. But what if one really wants to describe a wave function\npropagating in three dimensions? In such case, we need to let the wave function depend\non three coordinates\n\u03c8 = \u03c8(x, y, z).\n(5.1)\nSo that is just a minor change. As there are more variables now, we need to be a bit more\ncareful with derivatives. To avoid confusion, we will write an index for each differential\noperator:\n\u2202\n\u2202\n\u2202\nDy =\nDz =\n.\n(5.2)\nDx =\n\u2202x\n\u2202y\n\u2202z\nAs the order of taking derivatives never matters, these differential operators all commute:\n\u2202 \u2202\n\u2202 \u2202\nf=\nf \u21d2 [Dx , Dy ] = 0\n\u2202x \u2202y\n\u2202y \u2202x\n\n(5.3)\n\nand similarly [Dx , Dz ] = [Dy , Dz ] = 0. Now to each derivative, we can associate a corresponding momentum operator:\nPx = \u2212i~Dx\n\nPy = \u2212i~Dy\n\nPz = \u2212i~Dz\n\n(5.4)\n\nThis makes sense, as for a particle in 3d you can measure three different components of\n~ = (\u2202x , \u2202y , \u2202z ), the above formula can\nthe momentum. Using the grad(ient) operator \u2207\nbe written in vector form:\n~\nP~ = \u2212i~\u2207\n(5.5)\nSo here left- and right hand side are vectors of operators. Some people are a bit lazy,\nand drop the vector arrows on top. We will do so too, sometimes.\nExperimentally, one can also measure the total size of the momentum of a particle.\n(Regardless its direction.) Which operator corresponds to this observable? Classically,\nthe square of the total momentum is given by\n|p|2 = p2x + p2y + p2z .\n\n(5.6)\n\nThis suggests that the corresponding quantum operator is given by\nP 2 = Px2 + Py2 + Pz2 = \u2212~2 \u22072 = \u2212~2 (\u2202x2 + \u2202y2 + \u2202z2 )\n\n(5.7)\n\n\fCHAPTER 5. THE HYDROGEN ATOM\n\n76\n\nNote that this is a single operator again - not a vector. Ah, so when going from one\nto three dimensions, the operator for the momentum squared just becomes a grad(ient)\nsquared. This immediately suggests the form of the Schr\u00f6dinger equation in 3d:\ni~\u2202t \u03c8(~x, t) =\n\n\u2212~2 2\n\u2207 \u03c8(~x, t) + V (~x)\u03c8(~x, t)\n2m\n\n(5.8)\n\nThe changes are: \u03c8 now depends on three coordinates ~x, the same for the potential, and\nthe derivative squared is now a grad squared. Not all too different, right?\n\n5.1.2\n\nAngular momentum\n\nWhen passing from 1D to 3D, there is another thing that changes: in 3 dimensions, objects\ncan rotate. In the same way that linear momentum is the 'amount of motion', angular\nmomentum measures an object's 'amount of rotation'. The angular momentum is a vector\n~l, and for a point particle with position ~x and momentum p~ it is given by:\n~l = ~x \u00d7 p~\n\n(5.9)\n\nor writing out the cross product in components:\n~l = (ypz \u2212 zpy , zpx \u2212 xpz , xpy \u2212 ypz ).\n\n(5.10)\n\nAgain the corresponding quantum operator is a straightforward guess:\n~ =X\n~ \u00d7 P~\nL\n\n(5.11)\n\n~ we mean the vector of the position operators X, Y and Z. Writing out the\nwhere by X\nmomentum operator, we get\n~ = \u2212i~X\n~ \u00d7 \u2207.\n~\nL\n(5.12)\nSo for example the x component of the angular momentum operator is given by:\nLx = Y Pz \u2212 ZPy = \u2212i~(Y \u2202z \u2212 Z\u2202y )\n\n(5.13)\n\nMini-example\nThe function f (x, y, z) = y + iz is an eigenfunction of Lx , with eigenvalue ~, since\nLx f (x, y, z) = \u2212i~(yi \u2212 z) = ~(y + iz) = ~f\n\n(5.14)\n\nOf course, such a function is not a decent wave function: it is nowhere near normalizable.\nBut well - it's just an example.\n\n\fCHAPTER 5. THE HYDROGEN ATOM\n\n5.2\n\n77\n\nStory: cracking the mystery of atoms\n\nThe idea of atoms making up the basic constituents of nature is not so new. Even some\ngreek philosophers thought of the possibility of fundamental, indivisible pieces of matter1 .\nAlmost two millennia later, the first scientifically motivated atomic theories came to life.\nThrough a long and fascinating history of discoveries, it became clear that there really have\nto be basic constituents of all matter: atoms. First a mere model, more and more proofs\nof their existence accumulated, up to their classification, and finally even the possibility\nof 'looking' inside those strange building blocks. At the end of a long debate, scientists\ngot convinced that atoms have to be made of a small positive core, with a cloud of\nelectrons around it. For a moment, one could have thought that the mystery of atoms was\nunraveled. Electric attraction indeed explains that such a cloud of electrons would stay\nclose to a positive core, orbiting around it. It could not be more beautiful, the smallest\nknown scale in almost perfect similarity to our very own solar system. However prefect\nthis may seem, there were some serious problems indicating that the mystery was not\ncompletely solved yet.\n\n5.2.1\n\nThe end of the world\n\nFrom your basic electromagnetic course, you know that opposite charges attract. A bit\nsimilar to gravitation keeps a planet circling around a star, a light negative charge would\nneatly orbit around a heavy positive one. But there is a problem. The circular movement\nmeans that the orbiting charge experiences an acceleration: the centripetal force realized\nby the electrostatic attraction. Now a more advanced course on electromagnetism (whether\nyou had one or not) will readily tell you that an accelerated charge necessarily radiates\naway energy. Roughly speaking, the circular movement of the charge gives rise to a\nchanging electric field, in such a way that electromagnetic waves are created. This is\nsimilar to a moving object in water: it creates waves that carry away energy from the\nmoving object. So an orbiting charge will gradually lose its energy, and is deemed to\n(after some time) end up at the positive core. One could think this process just takes\nlong, and therefore does not occur on visible timescales, but it turns out that such a\ncollapse would go very quickly. So if the classical view on the electron and nucleus was\ncorrect, they were doomed to collapse quickly. Sounds very much like the end of the world,\nwhich we luckily don't see, so something else has to be going on inside the atoms around\nus.\n1\n\nActually, 'atom' comes from the Greek word for indivisible.\n\n\f78\n\nCHAPTER 5. THE HYDROGEN ATOM\n\nE\n\np+\n\ne-\n\nE\n\nFigure 5.2: If electrons were point particles, they would lose all their energy by the emitted\nradiation, and spiral down to the nucleus. In no time, all atoms would collapse, and the\nuniverse would end in a drastic collapse. Obviously, this is not what happens.\n\n5.2.2\n\nBalmer and friends\n\nNevertheless, there are some things that do make sense if you think of an atom in the\n'planetary' way. If electrons are orbiting around some core, you expect that they can\nchange their kinetic energy. So if one invests energy in an electron, it will just move to\na larger orbit. If it falls back to the smaller orbit, the absorbed energy will be emitted\nagain. This suggests atoms can absorb and emit energy. And such was indeed observed.\nAt the end of the 19th century, the pioneers in spectroscopy had found clear indications\nthat some gases absorb light (so, energy) and emit it again. To the scientists working on\nthe atomic model, it was clear that this absorption of energy had to do something with\nthe electrons of an atom absorbing it, and spitting it out again later. But there was a very\nstrange peculiarity. Gases did not seem to absorb any radiation, but only at very specific\nwavelengths. For hydrogen gas, this behavior was put into an empiric formula by Johann\nBalmer. He found that absorbed wavelengths \u03bbabs were all of the form\n\u0012\n\u0013\n1\n1\n1\n\u221d\n\u2212\nwith n some integer > 2.\n(5.15)\n\u03bbabs\n22 n2\nOther scientists later found some other absorbed wavelengths, and these are were all of\nthe form\n\u0013\n\u0012\n1\n1\n1\n\u2212\nwith n1 < n2 integers.\n(5.16)\n= RH\n\u03bbabs\nn21 n22\nThe above equation is called the Rydberg formula, after the discoverer of this generalization of Balmer's formula. The number RH is the Rydberg constant. This formula\nvery strongly suggests that the an electron in a hydrogen atom can only 'sit' on special\n\n\fCHAPTER 5. THE HYDROGEN ATOM\n\n79\n\nenergy levels. To see that, first note that for light, the inverse wavelength is proportional\nto the frequency. Indeed, the product of wavelength and frequency is the wave speed (light\nspeed):\nf\u03bb = c\n(5.17)\nSo because the speed of light c is just a constant, we have \u03bb1 \u223c f . Next the frequency\nof a light quantum (a photon) is proportional to energy: E = hf . So \u03bb\u22121\nabs is really a\nmeasure for an energy difference: the energy of a photon emitted in a transition between\ntwo electron orbits. So the Rydberg formula implies that the allowed energy differences\nare given by\n\u0013\n\u0012\n1\n1\n\u2212\nwith n1 < n2 positive integers.\n(5.18)\n\u2206E \u221d\nn21 n22\nThis then suggests the only energy levels occupied by the (hydrogen) electron are given\nby\n1\nE = E0 2 with n positive and integer.\n(5.19)\nn\nWith E0 a constant. If you track down the constants involved in all proportionalities, you\ncan check that E0 = hcRH .\n\n5.2.3\n\nBohr\n\nThis observed discreteness of the atomic spectrum of hydrogen (and other atoms) is of\ncourse not so easily explained by classical mechanics. The only situation in which physicists\nhad been confronted with discrete amounts of energy was the photon-hypothesis. As\nexplained in the introduction, the photoelectric effect (amongst other experimental results)\nshowed that light has to consist of discrete packets, of which the energy is determined\ncompletely by the frequency of the light:\nE = hf (= ~\u03c9).\n\n(5.20)\n\nIn part inspired by this result, the physicist Niels Bohr cooked up the following explanation\nfor the energy spectrum of hydrogen. He suggested that electrons around an atomic\nnucleus can only move on circular orbits, whose angular momentum is an integer multiple\nof Planck's constant ~. This assumption was never derived or even slightly motivated.\nProfessor Bohr just pulled it out of the hat. Any motivation (or lack of it) aside, let us try\nto see what the consequence is of such an assumption. If we take the origin of the system\nat the nucleus, the length of the position vector ~r of the electron is equal to the orbital\nradius r. The momentum is of course given by mv with m the electron mass, and v its\nvelocity. (Although the direction of the velocity changes throughout the circular motion,\nits size remains the same.) Since the velocity is perpendicular to the position vector we\nhave |~r \u00d7 ~\np| = |~r||~\np| and the size of the angular momentum is given by\nl = |~l| = |~r \u00d7 p~| = rmv.\n\n(5.21)\n\n\f80\n\nCHAPTER 5. THE HYDROGEN ATOM\n\nn=3\n\nn=2\n\ne-\n\nn=1\n\nE\n\np+\n\nFigure 5.3: According to the Bohr model, electrons can move on specific orbits only. As\na consequence, an electron falling from one energy level to another (here from n = 3\nto n = 2) can only emit a specific amount of energy. For the same reason, only specific\namounts of energy can be absorbed by an atom. This explains the findings of spectroscopic\nexperiments and the formula of Rydberg.\n\nSo Bohr's hypothesis amounts to\nrmv = n~ with n a positive integer.\n\n(5.22)\n\nWhich orbits satisfy this relation? First note that the centripetal force is given by the\nattraction between the nucleus and the electron. Putting in the charge of the proton (+e)\nand the electron (\u2212e):\nmv 2\ne2\nFcentr = Fcoul \u21d2\n=k\n(5.23)\nr\nr\nSo that\nke2\n(5.24)\nr=\nmv 2\nfor every circular orbit. This allows us to rewrite Bohr's hypothesis (5.22) as\nke2\n= n~.\nv\n\n(5.25)\n\nWe are now getting close. The energy of the system is given by\nE = Ekin + Epot =\n\n1\ne2\n1\nmv 2 \u2212 k = \u2212 mv 2\n2\nr\n2\n\n(5.26)\n\n\f81\n\nCHAPTER 5. THE HYDROGEN ATOM\n\nwhere in the last step we used (5.23). Plugging in Bohr's hypothesis (5.25), we find that\nthe energy of the electron is given by\n\u0012 2 \u00132\n1\nke\n1\nE=\u2212 m\n= \u2212E0 2\n(5.27)\n2\nn~\nn\n\u0010 2 \u00112\nwith n a positive integer and E0 = 12 m ke~\na numerical constant with units of energy.\nWoah, ye pirate, Bohr! Using his seemingly random hypothesis, we find a discrete set of\nenergies. And not just some set, precisely the one matching Rydberg's formula. Indeed,\nit has the right n12 form, and moreover the numerical value of E0 turns out to coincide\nsharply with the Rydberg constant RH .\n\n5.2.4\n\nA better explanation?\n\nThat's all very nice of Bohr. The great trick he pulls is just assuming a discrete behavior\nof the angular momentum, which then in the end of course leads to a discrete behavior\nof the energy. However perfect the final result matches with experiment, it just shifts the\nproblem to another one: why is the angular momentum discrete, then? It also does not\nanswer the problem raised in the 'end of the world' section. The model still describes an\nelectron moving on circles, which should radiate away all its energy.\nWhat can we do about that? Well, first of all we know that the electron should\nbe described by a smeared out wave function, not a circular orbit of a point particle.\nIn the examples we have met so far, solving the SE of such a wave function leads to\nan eigenfunction problem. Moreover, in the examples we have seen, this eigenfunction\nproblem only had a special set of solutions. So a wave function 'locked up' in a attractive\npotential naturally has a discrete set of energy states. This is a much more satisfying\nanswer than the magic Bohr pulled out of his hat. But can we also reproduce this result\nhere? It's to say, if we solve the SE for the hydrogen atom, do we get the right spectrum?\nWe will see in the next section...\n\n5.3\n\nThe quantum approach\n\nLet us try to apply what we have learned in the previous chapters, and understand the\nhydrogen atom using quantum mechanics. We know that the nucleus is very small, and\nthat it creates a potential for the bound electron:\nV = \u2212k\n\ne2\n.\nr\n\n(5.28)\n\np\nWith r = x2 + y 2 + z 2 the distance to the origin. This means the Schr\u00f6dinger equation\nfor the electron wave function reads\n\u0013\n\u0012\n~2\n\u2206 \u03c8\n(5.29)\ni~\u2202t \u03c8 = V \u2212\n2m\n\n\f82\n\nCHAPTER 5. THE HYDROGEN ATOM\n\np\nHere V = \u2212ke2 / x2 + y 2 + z 2 is the potential, \u03c8 = \u03c8(x, y, z) the three-dimensional wave\nfunction, and \u2206 = \u2202x2 + \u2202y2 + \u2202z2 the Laplacian. This gives a first situation where we have\nto deal with a realistic (and fully 3D) problem. In principle, the only thing one need to\ndo to understand the quantum behavior of an electron in a hydrogen atom is to solve the\nabove equation. Again, we first look for energy-eigenstates. The time-evolution of these\nstates is trivial (e\u2212iEt/~ ), and the most general state of the Hilbert space is just given by\ntaking linear combinations. So the only real problem is solving:\n\u0013\n\u0012\n~2\n\u2206 \u03c8.\n(5.30)\nE\u03c8 = V \u2212\n2m\n2\n\n~\nPhrased differently: we want to find the eigenfunctions of the hamiltonian H = V \u2212 2m\n\u2206.\n\n5.3.1\n\nA better coordinate system\n\nThe above equation is not so easy to solve. The naughty element is the potential, which\ninvolves a square root. Actually, since the potential (and hence the entire problem) is\nspherically symmetric, it is natural to switch to an adapted coordinate frame. Spherical\ncoordinates (r, \u03b8, \u03c6) are related to Cartesian ones by\nx = r cos \u03c6 sin \u03b8\n\n(5.31)\n\ny = r sin \u03c6 sin \u03b8\n\n(5.32)\n\nz = r cos \u03b8\n\n(5.33)\n\np\n\n(5.34)\n\nor\nr =\n\nx2 + y 2 + z 2\n\n\u03c6 = arctan(y/x)\n\u03b8 = arccos\n\n(5.35)\nz\n\np\n\nx2\n\n+ y2 + z2\n\n!\n\n(5.36)\n\nThe ranges are r \u2208 [0, \u221e[, \u03b8 \u2208 [0, \u03c0] and \u03c6 \u2208 [0, 2\u03c0[. If we want to rewrite the energyeigenstate equation (5.30) in terms of (r, \u03b8, \u03c6), three things change. First, we need\n2\nto replace\np \u03c8(x, y, z) by \u03c8(r, \u03b8, \u03c6). Next, the potential just becomes \u2212ke /r instead of\n2\n\u2212ke / x2 + y 2 + z 2 . The last thing is to rewrite the Laplacian in terms of derivatives \u2202r ,\n\u2202\u03b8 and \u2202\u03c6 . Doing so is unusually nasty (not even fun) and we only sketch the procedure\nhere. For example, to express \u2202z2 , we remark that\n\u2202z =\n\n\u2202r\n\u2202\u03b8\n\u2202\u03c6\n\u2202\n=\n\u2202r +\n\u2202\u03b8 +\n\u2202\u03c6\n\u2202z\n\u2202z\n\u2202z\n\u2202z\n\n(5.37)\n\nThis is just the chain rule for derivatives. (Both sides are operators, so one should imagine\nthem acting on a function. Writing a function on the right of both sides, you indeed see\n\n\f83\n\nCHAPTER 5. THE HYDROGEN ATOM\n\nnothing but the chain rule.) You can then compute the three partial derivatives occurring\non the right-hand side, using the expressions for r(x, y, z), \u03b8(x, y, z) and \u03c6(x, y, z) and\nnext express the results back in spherical coordinates. This way you can obtain\n\u2202z = cos \u03b8 \u2202r \u2212\n\nsin \u03b8\n\u2202\u03b8\nr\n\n(5.38)\n\nNow to get \u2202z2 , one must be a bit careful. Once again, recall that we are dealing with\noperators. So for example the square of an operator (r\u2202r ) would be\n(r\u2202r )2 = (r\u2202r )(r\u2202r ) = r(\u2202r + r\u2202r2 ) 6= r 2 \u2202r2\n\n(5.39)\n\nIf you are confused by these steps, just write a general function f on the right of every\nterm. In the same fashion, one can obtain an expression for \u2202z2 . (We omit it here.)\nDoing the same for \u2202x2 and \u2202y2 and taking the sum, one obtains the Laplacian in spherical\ncoordinates:\n\u0012\n\u0013\n\u0012\n\u0013\n\u22022\n\u2202\n1\n\u2202\n1\n1 \u2202\n2 \u2202\n.\n(5.40)\nr\n+ 2\nsin \u03b8\n+ 2 2\n\u2206= 2\nr \u2202r\n\u2202r\nr sin \u03b8 \u2202\u03b8\n\u2202\u03b8\nr sin \u03b8 \u2202\u03c62\n\nBack to the equation\nAfter this sidestep, let us move on to what we are here for, solving\n\u0013\n\u0012\n~2\n\u2206 \u03c8(r, \u03b8, \u03c6)\nE\u03c8(r, \u03b8, \u03c6) = V (r) \u2212\n2m\n\n(5.41)\n\nWith \u2206 given by (5.40). Let us suppose that the wave function \u03c8(r, \u03b8, \u03c6) can be written\nas\n\u03c8(r, \u03b8, \u03c6) = R(r)F (\u03b8, \u03c6)\n(5.42)\nThis is called separation of variables. In terms of R and F , the energy eigenstate\nr2)\nequation (5.41) becomes (after multiplying by \u22122m\n~2\n\u0012\n\u0013\n\u22022R\n\u2202R 2m 2\nF\nr 2 2 + 2r\n\u2212 2 r (V (r) \u2212 E)R\n(5.43)\n\u2202r\n\u2202r\n~\n\u0012\n\u0013\n1 \u2202\n\u2202F\n1 \u22022F\n+ R\n(sin \u03b8\n)+\n=0\n(5.44)\nsin \u03b8 \u2202\u03b8\n\u2202\u03b8\nsin2 \u03b8 \u2202\u03c62\nThis looks pretty disastrous, but there is an important simplification. Formally, the above\nsays\nF (some function of r) + R(some function of \u03b8 and \u03c6) = 0\n(5.45)\nSo assuming R and F are not zero - a typical kind of physics sloppiness:\n(some function of \u03b8 and \u03c6)\n(some function of r)\n=\u2212\n.\nR\nF\n\n(5.46)\n\n\fCHAPTER 5. THE HYDROGEN ATOM\n\n84\n\nThe last equation is very special. On the left hand side things only depend on r, whereas\non the right hand side only on \u03b8 and \u03c6. This can only be true if both sides are equal to\nsome constant, say C. This then reduces our problem to solving\n1 \u2202\n\u2202F\n1 \u22022F\n(sin \u03b8\n)+\n= \u2212CF\nsin \u03b8 \u2202\u03b8\n\u2202\u03b8\nsin2 \u03b8 \u2202\u03c62\n\n(5.47)\n\n\u2202R 2m 2\n\u22022R\n+ 2r\n\u2212 2 r (V (r) \u2212 E)R = CR\n(5.48)\n2\n\u2202r\n\u2202r\n~\nAh, we have cracked down the original equation in two parts! That's good news. The first\npart is called the angular equation, the second is the radial equation. We will solve\nthem separately.\nr2\n\n5.3.2\n\nThe angular equation\n\nFor those who were masochistic enough to have done the Laplacian calculation, another\ngoodie. Verify that the angular momentum operator, defined in the beginning of the\nchapter, has the following form in spherical coordinates:\n\u0015\n\u0012\n\u0013\n\u0014\n\u2202\n1 \u22022\n1 \u2202\n2\n2\n2\n2\n2\n(5.49)\nsin \u03b8\n+\nL = Lx + Ly + Lz = \u2212~\nsin \u03b8 \u2202\u03b8\n\u2202\u03b8\nsin2 \u03b8 \u2202\u03c62\nIf you have better things to do in your life than performing tedious algebra, just believe\nthe above statement. It will leave with more energy to discover something special: the\nangular equation (5.47) is just an eigenvalue equation of the angular momentum operator:\nL2 F = ~2 CF\n\n(5.50)\n\nNow this equation turns out to be well-studied in physics. It shows up in virtually any\nproblem with spherical symmetry (quantum or not). It only has solutions for the following\nvalues of C:\nC = l(l + 1) where l = 0, 1, 2, ...\n(5.51)\nStriking enough, for each such C (except for C = 0), there are several solutions. So to be\nclear on which solution we are talking about, not only do we need to specify l, but also an\nextra label distinguishing the different solutions corresponding to that l. It turns out that\nthe different solutions corresponding to the same l are all eigenfunctions of the operator\nLz , but with different eigenvalues. This means we can use the eigenvalue with respect to\nLz as the extra label needed to specify the solution uniquely. This label is conventionally\ncalled m, although it has nothing to do with mass. The corresponding eigenvalues of Lz\nare ~m. In conclusion: the angular equation L2 F = ~2 CF has an infinite set of solutions,\ndenoted Ylm (\u03b8, \u03c6), where the labels l and m are related to the eigenvalues of the solution\nwith respect to L and Lz :\n(5.52)\nL2 Ylm = ~2 l(l + 1) Ylm\n\n\f85\n\nCHAPTER 5. THE HYDROGEN ATOM\nand\nLz Ylm = ~m Ylm\n\n(5.53)\n\nIt turns out that the values m can take are bound by \u00b1l: so m = 0, \u00b11, \u00b12, ... \u00b1 l. The\nfunctions Ylm (\u03b8, \u03c6) are called spherical harmonics. To see where this name comes from:\na function that behaves 'well' under the Laplacian is called harmonic, and L2 is just the\nangular (=spherical) part of the Laplacian.\nThe general form of the spherical harmonics Ylm (\u03b8, \u03c6) can be written down explicitly.\nThis expression is not so illuminating, so we just restrict to giving a table of the first few\nsolutions (smallest l and m):\nm\nY\nql\n\nl\n\nm\n\n0\n\n0\n\n1\n\n0\n\n1\n\n\u00b11\n\n2\n\n0\n\n2\n\n\u00b11\n\n2\n\n\u00b12\n\n1\n4\u03c0\n\nq\n\n3\n\n4\u03c0\nq\n\n\u2213\nq\n\ncos \u03b8\n\n3\n8\u03c0\n\nsin \u03b8e\u00b1i\u03c6\n\n5\n(3 cos2 \u03b8 \u2212 1)\nq 16\u03c0\n15\ncos \u03b8 sin \u03b8e\u00b1i\u03c6\nq 8\u03c0\n15\n2\n\u00b12i\u03c6\n32\u03c0 sin \u03b8e\n\nYou also see that m runs from \u2212l to +l indeed. Next, all the spherical harmonics have\na simple \u03c6-dependence, of the form eim\u03c6 . This guarantees they are eigenfunctions of\nLz = \u2212i~\u2202\u03c6 with eigenvalue ~m.\n\n5.3.3\n\nThe radial equation\n\nHaving solved the angular equation, let us move on to the radial part. Using C = l(l + 1),\nwe can write the radial equation as\n2\n\u2202 2 R 2 \u2202R l(l + 1)\n+\n\u2212\nR + R \u2212 b2 R = 0\n2\n2\n\u2202r\nr \u2202r\nr\nar\n\n(5.54)\n\n2\n\n2mE\n~\n2\n. Note that the \u2212b2 is proportional to the energy E. Classiwith a = mke\n2 and \u2212b =\n~2\ncally, an elektron is bound to an attractive potential is its total energy (kinetic+potential)\nis negative. This is also the case in quantum mechanics. So we really are interested in\nsolutions where E is negative, meaning b is a real constant. For large r the middle terms\nare small and the equation becomes\n\n\u22022R\n\u2212 b2 R = 0\n\u2202r 2\n\n(5.55)\n\n\f86\n\nCHAPTER 5. THE HYDROGEN ATOM\n\nSo R \u221d e\u00b1br . The solution with a plus sign is a bit pathetic: the wave function would\nthen blow up for large r. We conclude there is only one healthy asymptotic behavior:\nR \u221d e\u2212br . This gives a hint on what solutions to the full equation (5.54) might look like:\nR(r) = (some function of r) * e\u2212br .\n\n(5.56)\n\nConcretely, by defining\nR(r) =\n\nu(r) \u2212br\ne\nr\n\n(5.57)\n\nthe radial equation becomes\n\u2202u\n\u22022u\n+\n\u2212 2b\n2\n\u2202r\n\u2202r\n\n\u0012\n\n2\nl(l + 1)\n\u2212\nar\nr2\n\n\u0013\n\nu = 0.\n\n(5.58)\n\nNow what kind of solutions do we want? We need u(r) to be a function of r, but it should\nbe decent enough, so that the asymptotic behavior is still dominated by the exponential.\nStudying the asymptotic behavior more carefully, one can show that u has to be a polynomial. (Otherwise, R(r) will blow up for large r.) So the problem of solving the radial\nequation has been reduced to: 'what polynomials satisfy the equation above?' Ah, again\nthis is a well-understood problem for mathematicians. Don't you just love them? The\nproblem only has solutions when\n1\n= n with n a positive integer\n(5.59)\nab\nAlso, we need l + 1 \u2264 n for a solution to exist. For these special values of a and b, the\nsolution of (5.58) is given by\nu(r) = r l+1 L2l+1\nn\u2212l\u22121 (2br)\n\n(5.60)\n\nwith L\u03b1\u03b2 the so-called generalised Laguerre polynomials. They are defined by\nx\u2212\u03b1 ex d\u03b2 \u0010 \u2212x \u03b1+\u03b2 \u0011\n(5.61)\ne x\nL\u03b1\u03b2 (x) =\n\u03b2! dx\u03b2\nand the first relevant combinations become:\nn l\nr l+1 L2l+1\nn\u2212l\u22121 (2br)\n1 0\nr\n2 0\nr(\u22122br + 2)\n2 1\nr2\n3 0 r(2b2 r 2 \u2212 6br + 3)\n3 1\nr 2 (\u22122br + 4)\n3 2\nr3\nAgain, you can check for your favorite element of the above list that it satisfies the corresponding differential equation. Returning to the original function R, the solution corresponding to a particular value of n and l is\n2l+1\n2l+1\n(2br)e\u2212br\n(2br)e\u2212br = r l Ln\u2212l\u22121\nR(r) = r l Ln\u2212l\u22121\n\n(5.62)\n\n\f87\n\nCHAPTER 5. THE HYDROGEN ATOM\n\n5.3.4\n\nConclusion\n\nCongratulations! You have gotten through solving the radial and angular part of the\nSchr\u00f6dinger equation of the hydrogen atom. The conclusion is: there is an infinite set of\nsolutions, labeled by the numbers n, l and m. They are given by\n\u03c8n,l,m(r, \u03b8, \u03c6) = R(r)F (\u03b8, \u03c6)\n=\n\n2l+1\n(2br)e\u2212br\nN r l Ln\u2212l\u22121\n\n(5.63)\nYlm (\u03b8, \u03c6).\n\n(5.64)\n\nWhere\nn = 1, 2, 3, ...\n\n(5.65)\n\nl = 0, 1, ...n \u2212 1\n\n(5.66)\n\nm = \u2212l, \u2212l + 1, ...l \u2212 1, l\n\n(5.67)\n\nA constant N has been included to take account of the normalization of these solutions.\nYou may wonder what those numbers n, l and m mean physically. Because the operators\nL2 and Lz involve only radial coordinates, equations (5.52) and (5.53) also imply that\nL2 \u03c8n,l,m = ~2 l(l + 1)\u03c8n,l,m\n\n(5.68)\n\nLz \u03c8n,l,m = ~m\u03c8n,l,m\n\n(5.69)\n\nHence the state |\u03c8n,l,m i has (squared) angular momentum ~2 l(l+1), and the z-component\nof the angular momentum is ~m. The label l is called the orbital quantum number\nand m is the magnetic quantum number. Furthermore, using the expressions for b\nand a, the energy is given by\nE=\u2212\nDefining again E0 =\n\nmk 2 e4\n2~2\n\n~2 (1/an)2\nmk2 e4\n~2 b2\n=\u2212\n=\u2212 2 2\n2m\n2m\n2~ n\n\n(5.70)\n\n(\u2248 13, 6eV ), the energy is given by\nE = \u2212E0\n\n1\n.\nn2\n\n(5.71)\n\nSo the label n determines the energy. It is called the principal quantum number.\nAlthough it took us quite a bit of work to get here, this is a very beautiful result.\nNot only have we found the wave functions that can be taken by the electron of the\nhydrogen atom, we have also found the corresponding values of the energy. On top of\nthat, these energies precisely coincide with the measured energy levels. (Since the result\nis in accordance with the empirical formula of Rydberg.) In contrast to Bohr, we did not\nhave to make some funny assumptions. Also, these wave functions just sit there: they\nare not an orbiting point particle as Bohr assumed, which had the problem of inevitably\nradiating away its energy and ending up in the nucleus. So in some sense the quantumapproach also solves the 'end of the world'-problem. More precise: there is a state (n = 1\n\n\fCHAPTER 5. THE HYDROGEN ATOM\n\n88\n\nand l = m = 0) that has the lowest energy: this is the ground state. When the electron\nis in this state, there is no possible process taking it to a lower energy state. The system\nthus is stable, and cannot decay or collapse. This contrasts with the numerous plagues\none encounters when trying to describe the atomic structure using classical mechanics.\n\nClosing remarks\nWe have obtained our desired solutions, but it might be clarifying to spend some more\ntime on them. First, as you may know, the different possible states of the electron in the\natom are called orbitals. There is an alternative notation to indicate the principal and\norbital quantum number of each orbital: write the value of n and then the letter s, p, d,\nf, ... for l = 0, 1, 2, 3, .... So for example\n1s \u2194 n = 1, l = 0\n\n(5.72)\n\n2p \u2194 n = 2, l = 1\n..\n.\n\n(5.74)\n\n2s \u2194 n = 2, l = 0\n\n(5.73)\n\n(5.75)\n\netcetera. The strange letters indicating the orbital quantum number derive from some\npurely visual aspects in spectroscopic experiments: they stand for s(harp), p(rincipal),\nd(iffuse), f(undamental). Here are some figures showing the norm of the wave function\nfor the lowest energy states of the hydrogen atom. The brighter the color, the larger the\namplitude.\n\n\f89\n\nCHAPTER 5. THE HYDROGEN ATOM\n\nSome very nice things can be seen here. First, note that the higher the energy, the\nfurther away the wave function is located from the nucleus. Indeed, for n = 3, the\nwave functions are more spread out ('larger') than for n = 2 or n = 1. This is the\nquantum analog of the classical expectation that for higher energies, the electron should\nbe occupying a larger orbit. Let us be a bit more quantitative on the size of these orbitals.\nUsing the fact that L\u03b1\u03b2 is a polynomial of degree \u03b2, you can check that the leading term\nin the wave functions (5.64) is given by\n\u03c8n,l,m \u223c r n\u22121 e\u2212r/(na)\n\n(5.76)\n\n1\n. The function on the right falls off quickly due to the expowhere we have used b = na\nnential. The region over which it has an appreciable size, can be estimated by looking at\nthe location of its maximum. There always is one, and you can check that it is given by\n\n\u2202 n\u22121 \u2212r/(na)\n(r\ne\n)=0\n\u2202r\n\n\u21d4\n\nr = n2 a \u2212 na\n\n(5.77)\n\nSo we see that for larger n the wave function is approximately limited to a region of size\nr \u223c an2 :\nstate size \u223c n2 a\n(5.78)\nThe constant a is called the Bohr radius. Its numerical value is\na = 0.529 * 10\u221210 m\n\n(5.79)\n\nand this is indeed the approximate size of the hydrogen atom. (Others atoms are larger\nsince they contain more electrons). So the size of the wave function grows quadratically\nwith n. This explains to some extent the stunning success of Bohr's naive model. His\nmysterious assumption describes classical orbits, of which the radii just happen to coincide\nwith the actual size of the quantum wave function.2 On top of that, the corresponding\nclassical energy to such an orbit just so happens to coincide with the right (quantum\nmechanical) value. For more complicated systems (atoms with more than one electron for\nexample) there are no similar 'smart guesses', and a quantum description is the only tool\nto get the right predictions.\nOne other observation is that the solution we found, predicts that there are in general\nseveral states with the same energy. (Since the energy only depends on n, not on l or\nm.) This phenomenon is called degeneracy and finally is something that really contrasts\nwith the Bohr description. It is possible to do very precise measurements and see the\ndifferent states corresponding to the same energy. This again confirms that the quantum\nmechanical description really is the right one.\nFinally, some comments on the angular part of the wave function. You see that for\nincreasing l the spherical harmonics get more and more wobbly: higher and higher powers\n2\n\nJust combine (5.24) and (5.25) to get an expression for r, only involving n and some constants, but\nnot v. You get precisely the relation r = n2 a.\n\n\fCHAPTER 5. THE HYDROGEN ATOM\n\n90\n\nof sin \u03b8 and cos \u03b8 occur. This trend continues for greater l. Also the \u03c6 dependence eim\u03c6\nbecomes more and more oscillating. This is the quantum mechanical manifestation of\nangular momentum. The stronger a wave function varies along the angular coordinates,\nthe more angular momentum it contains. This is a bit similar to the linear momentum of\na state. Since P = \u2212i~\u2207, the momentum of a state is proportional to its variation along\nthe coordinate axes, its linear 'wobbliness'. Of course, these statements are a bit shady:\nthey only give a heuristic feel for the manifestation of angular and linear momentum in\nthe language of wave functions. The right way to do this, is of course looking at the\neigenvalues (or otherwise the expectation values) of all these operators.\nYou can now say you truly understand what the hydrogen atom is. Historically, the\nfact that its structure can be fully derived from the Schr\u00f6dinger equation without any\nextra assumptions was a very important sign that his equation (and quantum mechanics)\nare the right way to describe particles.\n\n\fCHAPTER 5. THE HYDROGEN ATOM\n\n91\n\nExercises\n1. What is the wavelength a photon should have to excite a hydrogen atom from the\nthird to the fourth level?\n2. Pretend an atom has a cubic shape, and that you can stack hydrogen atoms tightly\nnext to each other. How many atoms then fit in a cubic centimeter? Estimate the\nnumber of sand particles in the Sahara desert. (Surface: \u223c 2500 km \u00d7 4000 km,\ndepth \u223c 100 m.) Would you say an atom is pretty small or very small?\n3. Using (5.49) and the fact that Lz = \u2212i~\u2202\u03c6 in spherical coordinates, check for your\nfavorite element of the table of spherical harmonics that it is an eigenvalue of L2\nand Lz , with the right eigenvalues.\n4. Verify the transition of (5.54) to (5.58).\n5. If you bombard the electron with too much energy, it will be kicked away the hydrogen nucleus. Concretely, the states with high n are only very weakly bound, and\nthey are very close to an ionized state. How much energy do you need to kick an\nelectron from n = 0 to the 'highest state' n = \u221e? If you impact a 20 eV photon on\na hydrogen atom, how much kinetic energy will the (now free) electron have?\n6. Challenge: consider a hydrogen-like atom: a nucleus with charge +Z and a single\nelectron around it. (For example, the case Z = 2 corresponds to He+ , and Z = 1 is\njust the hydrogen atom.) The potential is then Z times bigger. Skim through the\ncomputation of this chapter, and check which things change. Find the expression for\nthe wave function. Hint: the functions occurring remain the same but everything\n(prefactors, function arguments) gets rescaled. If you do all well, you should in the\n2\nend find that the energy levels of such a system are given by En = \u2212E0 Zn2 .\n7. Consider a particle in a box - a true one, in three dimensions, with volume L3 . Write\ndown the Hamiltonian: what is the kinetic part, what is the potential V (x, y, z)?\nWrite down the time-independent Schr\u00f6dinger equation. To simplify things, use\nseparation of variables: put\n\u03c8(x, y, z) = \u03c81 (x)\u03c82 (y)\u03c83 (z)\nShow that the equation breaks apart in three separate equations. Do you recognise\nthese equations? What are the solutions? Write down the most general solution of\nthe 3D system, and show that the energy levels are of the form\n~2 \u03c0 2\n* (n21 + n22 + n23 )\n2mL2\nwith n1 , n2 and n3 integers.\n\n\fChapter 6\n\nThe commutator\nIn this chapter...\nWe will come back to the result we got for the hydrogen atom. We address some small\nissues, giving another view on the situation which (hopefully) extends your understanding.\nA technical tool that will be important for this discussion is the commutator. This concept\nhas come up already several times, and it will re-appear even more often, so that is why\nit deserved this chapter's title. We start by improving our commutator computing skills,\nthen (as promised) we spend some more time pondering about the result of the previous\nchapter, and we conclude by several small illustrating computations.\n\n92\n\n\f93\n\nCHAPTER 6. THE COMMUTATOR\n\n6.1\n\nCommute along, cowboy\n\nIn the chapter on the measurement, we introduced the commutator of two operators. It\nis defined as\n[A, B] = AB \u2212 BA.\n(6.1)\n\nIf A and B were ordinary numbers, the above would obviously be zero. For operators\nhowever, the order in which you let them act on a function might matter; in that case the\ncommutator is nonzero. Here in this section we briefly give some facts on commutators.\nMost of them are easy to check, just by writing out everything explicitly; it is a good\nexercise to do so. First of all, commutating is a linear operation:\n[aA + bB, C] = a[A, C] + b[B, C]\n\n(6.2)\n\nfor complex numbers a, b, and operators A, B, C. Similarly\n[A, bB + cC] = b[A, B] + c[A, C]\nThe commutator of products is given by the 'drag out sideways' -rule (\n\n(6.3)\n):\n\n[A, BC] = [A, B]C + B[A, C] and [AB, C] = A[B, C] + [A, C]B.\n\n(6.4)\n\nWhich can again be seen by just writing out both sides - carefully retaining the order of\noperators within in each term. We have also seen that\n[X, Dx ] = \u22121\n\n(6.5)\n\nWe showed this property before, but just to refresh your memory, a concrete mini-example\nillustrating it (for -say- the function x2 ):\n[X, Dx ]x2 = XDx x2 \u2212 Dx Xx2 = X(2x) \u2212 Dx (x3 ) = 2x2 \u2212 3x2 = \u22121(x2 ).\n\n(6.6)\n\nOf course the same holds for other variables: [Y, Dy ] = [Z, Dz ] = \u22121. As a consequence\n(using Pi = \u2212i~Di )\n[X, Px ] = [Y, Py ] = [Z, Pz ] = i~\n(6.7)\nThe above is sometimes called the canonical commutation relation. On the other\nhand, mixed position-momentum commutators are zero:\n[X, Py ] = 0\n\n(6.8)\n\nand similar for any other combination of two different variables. To see this, just note\nthat for any function f (x, y, z),\nXDy f = X(\u2202y f ) = x\u2202y f = \u2202y (xf ) = Dy Xf\n\n(6.9)\n\nand similar for any other combination of two different variables. If the before last step is\nnot immediately clear, just use the product rule for differentiation to see that\n\u2202y (xf ) = (\u2202y x)f + x(\u2202y f ) = 0 + x\u2202y f.\nWe're done with the first part. Not all too harsh, right?\n\n(6.10)\n\n\fCHAPTER 6. THE COMMUTATOR\n\n6.2\n\n94\n\nAsking those questions...\n\nIn the precious chapter, we uncovered the quantum structure of the hydrogen atom. You\nmay have felt a bit like a passive audience though: most results fell out of the sky. In\nparticular, the fact that we have to use three labels to specify each state seems to have\nemerged 'miraculously'. Of course, you may be satisfied with this, but if you are critical,\nthere are several questions you can ask. First, notice that the third quantum number (m)\ndescribes the z\u2212component of the angular momentum. Why this one? What is special\nabout this direction? Didn't we start out with a perfectly spherically symmetric problem?\nAlso, why did we find three quantum numbers, and not (say) four? How could we have\nseen this coming in advance? These are all quite good questions. Below, we will answer\nthem in detail. Hopefully this adds some extra understanding to the matter.\n\n6.2.1\n\nZ-supremacy?\n\nWhy did we find the states of an electron in the hydrogen atom to be eigenstates of Lz ,\nand not Lx or Ly ? Here is the explanation. Recall that we found that given the total\nangular momentum (labeled by l) there were several solutions to the angular equation.\nSo given some l, there are several solutions to\nL2 F (\u03b8, \u03c6) = ~2 l(l + 1)F (\u03b8, \u03c6).\n\n(6.11)\n\nNow here is the key observation: the above equation is linear - just like the Schr\u00f6dinger\nequation. So if you have two solutions F1 (\u03b8, \u03c6) and F2 (\u03b8, \u03c6) then any linear combination\naF1 (\u03b8, \u03c6)+bF2 (\u03b8, \u03c6) is a solution too. Thus, when we said there is more than one solution,\nwe should actually have said: there are infinitely many solutions, and they form a vector\nspace. This vector space can best be described by picking a basis - but which one you\nchoose is up to you. In the previous chapter we picked a basis, namely functions Ylm ,\nwhich (on top of L2 ) were also eigenfunctions of the operator Lz , with eigenvalues m. Of\ncourse, it is possible to pick any other basis. In fact, it is equally possible to pick basis\nelements that are eigenfunctions of L2 and Lx (or of L2 and Lx ). In such a case, you\nwould label your basis states by l and their eigenvalue under Lx (or Ly ). So there really\nis nothing special about the z-direction. Using Lz just happens to provide the simplest\nlooking basis of the space of solutions of (6.11). That is why people like to use that one.\nOriginally, this comes from the fact that in the (r, \u03b8, \u03c6) system, Lz has a very simple form,\n\u2212i\u2202\u03c6 . The other two angular momentum operators look a bit more difficult, and so do\ntheir eigenfunctions - but nothing fundamental distinguishes them. As an illustration, we\nwill actually perform the change of basis explicitly in the computation part of the chapter.\n\nOrthogonal states\nHere is another question that might have come to your mind. Are the states we found\nan orthogonal basis? As emphasized in the chapter on observables, such a basis is very\n\n\fCHAPTER 6. THE COMMUTATOR\n\n95\n\nvaluable. We have also shown that eigenstates of an observable with different eigenvalues\nare automatically orthogonal. Here, things look a bit different at first: since there are\nseveral states with the same n. How does this work? First, let us be more clear on what\nwe mean by orthonormal in this situation. Denoting by |n l mi the state corresponding to\n\u03c8nlm (x), we can express orthonormality by:\nhn l m|n\u2032 l\u2032 m\u2032 i = \u03b4nn\u2032 \u03b4ll\u2032 \u03b4mm\u2032 .\n\n(6.12)\n\nLet us think about this. States with different n have a different energy. That assures that\nh\u03c8nlm |\u03c8n\u2032 l\u2032 m\u2032 i = 0 whenever n 6= n\u2032 . But what if n = n\u2032 ? Ah, remember that if l is the\neigenvalue under L2 . This is a Hermitian operator too, as it corresponds to the observable\n'angular momentum'. Hence, states with different eigenvalues l under L2 are orthogonal\ntoo. And in the same fashion, eigenstates with a different m, have different eigenvalues\nunder the (also hermitian) operator Lz , so that is sufficient to guarantee orthogonality\ntoo. In conclusion, the inner product h\u03c8nlm |\u03c8n\u2032 l\u2032 m\u2032 i will be zero whenever one of the three\nquantum numbers n, l, or m are different. Stated differently: the above inner product is\nonly nonzero when taking the inner product of a state with itself. If one also makes sure\nstates are normalized (h\u03c8nlm |\u03c8nlm i = 1) the equation (6.12) is indeed satisfied.\n\n6.2.2\n\nCoSCO\n\nLet's imagine what would happen if you were a computing beast which could solve differential equations simply by looking at them. Then, studying the hydrogen atom quantum\nmechanically would go a bit like this. You write down the Hamiltonian for an electron\nbound to a nucleus. You glance at that time independent Schr\u00f6dinger equation (the eigenfunction problem) and instantly solve it - without having to use silly tricks (splitting it\ninto a radial and angular equation) which normal humans have to rely on: you just write\ndown an infinite tower of solutions (which span the Hilbert space) labelled by their energy,\nand that's it.\nBut suddenly something bothers you: there are several states belonging to each energy\nlevel. The entire set of solutions with the same energy actually forms a vector space, since\nthe sum of eigenfunctions with the same eigenvalue is again an eigenfunction. Such a\nvector space is called a subspace of the total Hilbert space. This is not a problem per\nse, but you'd like to find a decent, orthogonal basis of the Hilbert space - and picking a\nrandom basis of energy eigenstates -like you just did- is no good: states with the same\nenergy eigenvalue are not necessarily orthogonal.\nSo how do you find a basis that is orthogonal then? You glance again (still in beast\nmode) and immediately realize that for every subspace, you can find a special set of\nstates which are eigenstates of both L2 and Lz . Picking these special states as your basis\n(elements of this basis are now labeled by their eigenvalue under H, L2 and Lz ) you have\nnow obtained an orthogonal basis! Indeed, each two elements of your basis have a different\neigenvalue with respect to at least one Hermitian operator (H or L2 or Lz ) so they are\n\n\f96\n\nCHAPTER 6. THE COMMUTATOR\n\nguaranteed to be orthogonal. This finally gives you an orthonormal basis |n lmi. (Being\nmere humans, we needed more work to obtain this basis.)\nSince all states of the Hilbert space basis can be labeled by their eigenvalues under the\nobservables H, L2 and Lz , these are called a complete set of commuting observables,\nor CoSCO for short. Let's clarify. The term 'complete' means they specify every state\nuniquely: no two different states can be found with the same eigenvalues. Next (as\nsuggested by the name) such a set of observables necessarily commutes. This is not too\nhard to show. Suppose one has two observables A and B and a basis of states |iji, which\nare eigen to A with eigenvalues ai and eigen to B with eigenvalues bj . So\nA|iji = ai |iji\n\nand B|iji = bj |iji\n\n\u2200i , \u2200j\n\n(6.13)\n\nSo i specifies the A-subspace to which\na state belongs, and j the B-subspace. Then it\nP P\nfollows that for a every state |\u03c8i = i j \u03b1ij |iji in the Hilbert space\nAB|\u03c8i = AB\n\nXX\ni\n\n= A\n\nXX\ni\n\n=\n\nj\n\nXX\ni\n\n=\n\nj\n\nXX\ni\n\n= B\n\nj\n\nj\n\nbj \u03b1ij |iji\n\nai bj \u03b1ij |iji\nbj ai \u03b1ij |iji\n\nXX\ni\n\n\u03b1ij |iji\n\nj\n\nai \u03b1ij |iji\n\n= BA\u03b1ij |iji = BA|\u03c8i\n\n(6.14)\n\nAt several points the linearity of A and B was used. (This is OK because all operators in\nquantum mechanics are linear.) So AB|\u03c8i = BA|\u03c8i for all states |\u03c8i in the Hilbert space,\nmeaning\nAB = BA so [A, B] = 0\n(6.15)\nIndeed, A and B must commute. This explains the second C in CoSCO.\n\n6.2.3\n\nCompatible observables\n\nThe above combined with our finding of the states |n l mi implies that all the commutators\nbetween H, L2 and Lz have to be zero:\n[H, L2 ] = [H, Lz ] = [L2 , Lz ] = 0.\n\n(6.16)\n\nBut also a converse of the previous section is true: If the commutator [A, B] of two\nobservables is zero, then it is possible to find states that are eigenstates of A and B in the\n\n\f97\n\nCHAPTER 6. THE COMMUTATOR\n\nsame time. Because of this property, commuting observables A and B are sometimes called\ncompatible observables. But not any two observables are compatible. For example, we\nwill see in the computation part that [Ly , Lz ] and [Lx , Lz ] are not zero. This means that\nit is not possible to find a basis of states that are simultaneous eigenstates of more than\none component of the angular momentum. So generally, if a state is an eigenstate of one\nof the components of the angular momentum, it can not be an eigenstate of any of the two\nother components. With respect to one of these two other components, it must then be\na superposition of several eigenstates, and measuring one of these other components will\nthus always yield a probabilistic outcome. This has very important consequences: in fact,\nit is the underlying reason for the so-called 'uncertainty principle', which we will discuss in\nChapter 8. Anyhow, we have now gotten the answer on the last question we posed at the\nbeginning of this section. We now know that there are only 3 quantum numbers necessary\n(and not -say- 4) to describe states of the hydrogen atom because there are precisely three\ncompatible observables for that system, no more and no less. Which set you take (f.e.\n(H, L2 , Lz ) or (H, L2 , Lx ) or ...) still has some freedom, but you always need three of\nthem.\n\n6.3\n\nFilling in the gaps\n\nIn this section we fill in three gaps that we left open in the story part. First, we explicitly perform the basis change that we were talking about in the 'z-supremacy'-section.\nNext, we give a baby-example of another problem where a CoSCO shows up. Finally, we\ncompute the commutation relations between different components of the angular momentum to show that they are non-zero. This proves the different components of the angular\nmomentum are non-compatible observables.\n\n6.3.1\n\nGap 1: Changing basis\n\nLet us denote the states |2, 1, \u22121i, |2, 1, 0i and |2, 1, 1i by the vectors\n\uf8eb \uf8f6\n\uf8eb \uf8f6\n\uf8eb \uf8f6\n0\n0\n1\n\uf8ed 0 \uf8f8 , \uf8ed 1 \uf8f8 and \uf8ed 0 \uf8f8\n1\n0\n0\n\nSo the vector\n\n\uf8eb\n\n\uf8f6\na\n\uf8ed b \uf8f8\nc\n\n(6.17)\n\n(6.18)\n\nis a shorthand for the state a |2, 1, \u22121i + b |2, 1, 0i + c |2, 1, 1i. The advantage of this symbolic notation is the following. Operators acting on these three states can be represented\nby a matrix acting on the corresponding vector. Indeed, the action of Lz for example can\n\n\f98\n\nCHAPTER 6. THE COMMUTATOR\nbe written as\n\n\uf8eb\n\n\uf8f6\n1 0 0\nLz = ~ \uf8ed 0 0 0 \uf8f8\n0 0 \u22121\n\n(6.19)\n\nSo for example the equation Lz |2, 1, \u22121i = (\u2212~)|2, 1, \u22121i is represented in matrix form by\n\uf8eb\n\uf8f6\uf8eb \uf8f6\n\uf8eb \uf8f6\n1 0 0\n0\n0\n\uf8ed\n\uf8f8\n\uf8ed\n\uf8f8\n\uf8ed\n~\n0 0 0\n0\n= (\u2212~)\n0 \uf8f8\n(6.20)\n0 0 \u22121\n1\n1\n\nSimilarly, the operator L2 acts on these states as\n\uf8eb\n\uf8f6\n1 0 0\nL2 = 2~2 \uf8ed 0 1 0 \uf8f8\n0 0 1\n\n(6.21)\n\nbecause all three states are eigenstates of L2 with eigenvalue ~2 l(l + 1) = 2~2 . Using the\nspherical coordinate expression for Lx :\nLx = i~ (sin \u03c6\u2202\u03b8 + cot \u03b8 cos \u03c6\u2202\u03c6 )\n\n(6.22)\n\nand its explicit action on the three states under consideration, one can check that Lx acts\nas the matrix\n\uf8f6\n\uf8eb\n0 1 0\n~ \uf8ed\n(6.23)\n1 0 1 \uf8f8\nLx = \u221a\n2\n0 1 0\n\nLet us find the eigenvalues and eigenvectors of this matrix. First of all, the determinant\nof Lx \u2212 \u03bb1 is (\u2212\u03bb3 + ~2 \u03bb), which has solutions \u03bb = \u2212~, \u03bb = 0 and \u03bb = ~. This means\nthe eigenvalues of the matrix Lx are \u2212~, 0 and ~, just as for Lz . On the contrary, the\neigenvectors are different. The eigenvector corresponding to \u03bb = \u2212~ can be found by\ndemanding\n\uf8eb \u221a\n\uf8f6\uf8eb \uf8f6\n2 \u221a1\n0\na\n~ \uf8ed\n\u221a\n(6.24)\n2 \u221a1 \uf8f8 \uf8ed b \uf8f8 = 0\n1\n2\nc\n0\n1\n2\nWhich is solved by\n\n\uf8eb\n\n1\n\u221a\n\n\uf8f6\n\nv1 = \uf8ed \u2212 2 \uf8f8\n1\n\n(6.25)\n\nor any multiple of that vector. Similarly, one can find the eigenvectors corresponding to\nthe other two eigenvalues. They are\n\uf8eb\n\uf8eb\n\uf8f6\n\uf8f6\n1\n1\n\u221a\nv2 = \uf8ed 0 \uf8f8 and v3 = \uf8ed 2 \uf8f8\n(6.26)\n\u22121\n1\n\n\fCHAPTER 6. THE COMMUTATOR\n\n99\n\nWe are now very close to our original goal. Remember that we set out to find a basis of\nstates that are eigenstates of the operator Lx instead of Lz . By our above calculation, we\nhave actually done so, in a very economic fashion. Indeed, the states corresponding to the\nvectors v1 , v2 and v3 are precisely eigenstates of Lx , with eigenvalues \u2212~, 0 and ~. For\nconvenience, we will denote these states\u221a by |\u03c81 i, |\u03c82 i and |\u03c83 i. For example: v1 corresponds to the state |\u03c81 i = 1|2, 1, \u22121i \u2212 2|2, 1, 0i + |2, 1, 1i, and one can check (although\nthat is a bit tedious) that the corresponding wave function indeed is an eigenstate of Lx\nwith eigenvalue \u2212~. If we would switch to a notation where the vectors\n\uf8eb \uf8f6\uf8eb \uf8f6\uf8eb \uf8f6\n1\n0\n0\n\uf8ed 0 \uf8f8\uf8ed 1 \uf8f8\uf8ed 0 \uf8f8\n(6.27)\n0\n0\n1\n\nwould represent the states |\u03c81 i, |\u03c82 i and |\u03c83 i, than Lx would have precisely the form Lz\nhad before:\n\uf8eb\n\uf8f6\n1 0 0\n~\uf8ed 0 0 0 \uf8f8.\n(6.28)\n0 0 \u22121\n\nThis confirms that one may just as well work with Lx eigenstates by a simple change of\nbasis. We can easily make this basis orthonormal by getting the normalization right. We\nillustrate this with |\u03c81 i: redefine\n\u221a\n(6.29)\n|\u03c81 i = N (1|2, 1, \u22121i \u2212 2|2, 1, 0i + |2, 1, 1i)\nwith N a suitable constant. Demanding h\u03c81 |\u03c81 i = 1 gives\n\u221a\n1 = (h2, 1, \u22121| \u2212 2h2, 1, 0| + h2, 1, 1|)N\u0304\n\u221a\n*N (1|2, 1, \u22121i \u2212 2|2, 1, 0i + |2, 1, 1i)\n|N |2 (1 + 2 + 1)\n1\n\u21d2 |N | = \u221a\n2\n=\n\nSo we could take for example N = \u221a12 or N ei\u03b8 with any value of \u03b8 - as we have seen before\nthat they are all considered as a the same physical state anyway. The very same thing\ncan be done to normalize the other two vectors.\nNote: in the above example we have only changed basis from Lz to Lx eigenstates for\nthe n = 2 level; the general principle is hopefully clear.\n\n6.3.2\n\nGap 2: Another CoSCO\n\nHere, we do a baby example involving the use of a CoSCO. Consider the operator D 2 . Its\neigenfunctions with eigenvalue \u22121 are given by eix and e\u2212ix . Since these are two linearly\ndifferent functions, they form an eigenspace of dimension two. More general, for each\n\n\f100\n\nCHAPTER 6. THE COMMUTATOR\n\neigenvalue \u2212a2 there are two eigenfunctions eiax and e\u2212iax . This means that the label a\nis not enough to specify which function we are talking about. Can we find a (Hermitian)\noperator that commutes with D 2 ? Yes, we can define the reflection operator R as\nR : f (x) \u2192 Rf (x) = f (\u2212x).\n\n(6.30)\n\nSo R maps a function to the function obtained by flipping around the vertical axis:\n\nf(x)\n\nR f(x) = f(-x)\n\nx\n\nOne can check that this commutes with our other operator:\n[R, D 2 ] = 0.\n\n(6.31)\n\nHere comes the nice thing: the eigenvalues of R are 1 and \u22121, and they split the degenerate\neigenspaces of D 2 cleanly into two. Explicitly:\nR(eiax + e\u2212iax ) = e\u2212iax + eiax\nR(eiax \u2212 e\u2212iax ) = e\u2212iax \u2212 eiax = \u2212(eiax \u2212 e\u2212iax )\n\n(6.32)\n\nSo the functions e\u2212iax +eiax and e\u2212iax \u2212eiax provide a very nice basis: they are simultaneous\neigenfunctions of D 2 and R, uniquely labeled by some a and \u00b11. That's all. Of course, the\nexample has some shortcomings. We did not specify the total Hilbert space that we wanted\nto find a basis for. Also, the basis elements are is a bit special - they are not normalizable.\nThis is somewhat unusual, but similar situations will be dealt with in the next chapter.\nThe only point however was to show the CoSCO mechanism of using multiple labels to\ndescribe states uniquely. The obtained basis then diagonalizes all the operators of the\nCSCO. By 'diagonalizing', we just mean 'to form/pick a basis of eigenfunctions' - just like\nin linear algebra.\n\n6.3.3\n\nGap 3: The angular momentum commutation relations\n\nIn the story part, we mentioned that the different components of the angular momentum\ndo not commute, so that it is not possible to find simultaneous eigenstates. Here, we\nexplicitly find these commutators. This also puts many of the formulae of first section\ninto practice. Let us start with [Lx , Ly ]:\n[Lx , Ly ] = [Y Pz \u2212 ZPy , ZPx \u2212 XPz ]\n\n(6.33)\n\n\fCHAPTER 6. THE COMMUTATOR\n\n101\n\nusing linearity, one gets four commutators. One of them for example is [Y Pz , ZPx ]. Now\nusing the 'drag out sideways' ( ) rule from above, we can rewrite that commutator as a\nsum of four terms:\n[Y Pz , ZPx ] = Z[Y Pz , Px ] + [Y Pz , Z]Px\n= ZY [Pz , Px ] + Z[Y, Px ]Pz + Y [Pz , Z]Px + [Y, Z]Pz Px\n\n(6.34)\n\nPhooh, this smells nasty. Luckily, the last expression is not as horrible as is looks. The\nonly term involving a nonzero commutator is the Y [Pz , Z]Px term. So the entire above\nexpression just equals \u2212i~Y Px . From this, we can conclude the following trick. Just look\nif there are a position and momentum operator of the same coordinate occurring in a\ncommutator. Such terms are the only possible reason for a commutator to be nonzero.\nAnother term of (6.33) is [ZPy , XPz ]. Here, the only problem arises from the z-coordinate.\nPulling out the two other operators, we get\n[ZPy , XPz ] = Py X[Z, Pz ] = i~XPy\n\n(6.35)\n\nThe two remaining terms of (6.33) have no possible problem: only mixed commutators,\nor position-position or momentum-momentum. So they are zero. In conclusion:\n[Lx , Ly ] = i~XPy \u2212 i~Y Px = i~Lz\n\n(6.36)\n\nAnalogously, one can find the two other commutators. Writing all results together, we\nhave:\n[Lx , Ly ] = i~Lz ,\n[Ly , Lz ] = i~Lx ,\n[Lz , Lx ] = i~Ly\nThe above relations are called the angular momentum commutation relations. They\nshow that the different components of the angular momentum aren't compatible observables. Computing them used everything we learned about commutators, so if you have\nreached/understood this page, you can rightfully call yourself a true commander of commutator craft. Good job!\n\n\fCHAPTER 6. THE COMMUTATOR\n\n102\n\nExercises\n1. Explain in detail to someone (or yourself) what a CoSCO is. Can you reconstruct\nthe reasoning?\n2. Use the above angular momentum commutation relations and the 'pull out sideways'\nrule to show that [L2 , Lx ] = 0.\n3. Compute [f (X), P ] where f is a function.\n4. Show the Baker-Campbell-Hausdorff formula, which states that eA Be\u2212A =\nB + [A, B] + 2!1 [A, [A, B]] + 3!1 [A, [A, [A, B]]] + .... (Hint 1: the exponential of an\noperator is defined by the series expansion. Hint 2: The expression on the left can\nbe seen as the function F (s) = esX Y e\u2212sX , evaluated at s = 1. The value there can\nalso be obtained by series expanding F (s) around 0. What relation is there between\ntwo consecutive coefficients of the series expansion?)\n5. Find the states with n = 2 which are eigenstates of Ly .\n6. Make up two operators and computer their commutator.\n7. It was stated that two observables commute if and only if they can be simultaneously\ndiagonalized (=admit a basis of states which are all eigenstates of both operators).\nWhat is the linear algebra analog of this statement? Can you prove this?\n\n\fChapter 7\n\nPosition and momentum basis\nIn this chapter...\nIn the previous chapters, we have learned that a particle is not a point but an object\nsmeared out over some region in space. This means the position is not sharply defined. In\nthis chapter we will think a bit more careful about the momentum of a particle. We will\nsee that (just like position) the momentum of a particle is not sharply defined. In fact, we\nwill see that a particle can be thought of as a packet spread out over different momenta,\nin the very same way that it is a packet spread out over different positions. An important\nmathematical tool will be the notion of Fourier transform. This will allow us to switch\nback and forth between the two descriptions, and give some more feel about the nature of\nthe wave function.\n\n103\n\n\f104\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n7.1\n\nFourier's trick\n\nAs mentioned in the introduction, we will need a mathematical tool called Fourier transform. We introduce this tool here. If you have not learned about this before, be sure to\nread the following carefully. It is an important technique for physics and mathematics,\nand understanding it well will surely benefit you later on. If you have seen Fourier transforms before, just see the following section as a short recap. We work in two steps: first\nwe study the baby brother of Fourier transform: Fourier series. After that, we are in a\nbetter position to understand the (closely related) topic of Fourier transforms.\n\n7.1.1\n\nFourier series\n\nImagine you have a function that is periodic, that is: it consists of a single piece, which is\nrepeated over and over. For instance, it could look like this:\n\nT\n\ny\n\nx\n\nT\nThe length (in the x-direction) of the repeated piece is the period of the function.1\nFor the function above, the period T has been indicated on two places. Suppose that the\nperiod of a function is equal to 2\u03c0. In that case, you can capture all the information of\nthe function effectively by giving its value on the interval [\u2212\u03c0, \u03c0]. Indeed, the value of the\nfunction anywhere outside the interval (say at some x + 2\u03c0k, with x \u2208 [\u2212\u03c0, +\u03c0] and k an\ninteger) is just given by\nf (x + 2\u03c0k) = f (x).\n(7.1)\nYou know that there is a very important pair of functions functions that are of this\nkind (periodic with period 2\u03c0): the trigoniometric functions sin x and cos x. In fact, all\nfunctions sin(nx) and cos(nx) (with n a positive integer) are periodic with period 2\u03c0.2\nThe central question that lies at the heart of Fourier series, is the following:\nCan we write the periodic function f as a linear combination of the functions\nsin(nx) and cos(nx)?\n1\n\nFor traveling waves, the period refers to the duration of the time cycle, and the wavelength refers to\nthe periodic behavior in the spatial direction. Here, as we are considering a function of one variable only,\nyou may call the length of a cycle either wavelength or period - the second one being slightly more popular.\n2\nActually, the functions sin(nx) and cos(nx) have period 2\u03c0/n, but this implies they are also periodic\nover a distance of 2\u03c0 - as this is just a multiple of their period.\n\n\fCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n105\n\nThis question essentially asks how (and if) we can decompose an arbitrary periodic function\ninto harmonic functions (sines and cosines). Because harmonic functions have some really\nnice properties, such a decomposition is obviously an interesting trick. It turns out that\nfor almost every well behaved periodic function, this is indeed possible. An illustration\nof how this works is shown in Figure 7.1. Let us try to see how this works in general.\nWith some good faith, suppose such a decomposition is indeed possible. So assume that\nfor every well-behaved 2\u03c0-periodic function f there exist numbers ai and bi (with i \u2265 1)\nand an a0 such that\nf (x) =\n\n\u221e\n\na0 X\n[an cos(nx) + bn sin(nx)]\n+\n2\n\n(7.2)\n\nn=1\n\nWe have also included a constant function (first term), the last part is a sum over infinitely\nmany harmonic functions. For this infinite sum to be well defined, we need the an and bn\nto go to zero fast enough to ensure convergence. This is a mathematical problem which\nwe will not touch upon here. A more important question one immediately comes up with\nis: given that this decomposition is possible, how do I find the values of the an and bn ?\nHere is a crucial observation: you can show that for all n and all m\nZ \u03c0\nsin(nx) cos(mx)dx = 0\n(7.3)\n\u2212\u03c0\nZ \u03c0\nsin(nx) sin(mx)dx = \u03c0\u03b4nm\n(7.4)\nZ \u2212\u03c0\n\u03c0\ncos(nx) cos(mx)dx = \u03c0\u03b4nm\n(7.5)\n\u2212\u03c0\n\nwhere \u03b4nm is again the Kronecker delta:\n\u03b4nm =\n\n\u001a\n\n1 if n = m\n0 if n 6= m\n\n(7.6)\n\nIn words: if one integrates a product of two different harmonic functions (n 6= m or sine\ntimes cosine) the result will always be zero. Aha! Here is the miracle: if you multiply\n(7.2) by cos kx and integrate x over [\u2212\u03c0, \u03c0], you get:\nZ\n1 \u03c0\nf (x) cos(kx) dx\n\u2200k \u2265 0\n(7.7)\nak =\n\u03c0 \u2212\u03c0\nThis expression also incorporates the expression for the constant term a0 . In a similar\nfashion you can show that\nZ\n1 \u03c0\nbk =\nf (x) sin(kx) dx\n\u2200k \u2265 1\n(7.8)\n\u03c0 \u2212\u03c0\nIn conclusion: by performing simple integrals of the function f , we can find the constants\nan and bn . This gives a straightforward way to decompose any (well-behaved) periodic\nfunction into harmonics. Nice trick, Mr. Fourier!\n\n\f106\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\nf 5(x)\n\ncos(2x)\n\nsin(x)\n\ncos(x)\n\nsawtooth(x)\n\nf (x) = sin(x)\n1\n\nFigure 7.1: A Fourier series decomposes an arbitrary periodic function into harmonics.\nThe left side shows some of the first harmonic functions involved in Fourier series. The\nright shows a function with the shape of a sawtooth. The line f1 = sin x includes only one\nterm of the Fourier series. The function f5 is a sum of more harmonic functions. Including\neven more terms, you can approximation the sawtooth with arbitrary precision.\n\nShort story\nYou might wonder about the possible uses of such a decomposition. If you ever have a\ncourse on differential equations, you are quite likely to learn the above trick, and see how\nit helps solving problems like heat evolution, the evolution of waves, etcetera. Another\nnice application are voice recognition and synthesizers. So either your favorite action\nmovie scene (involving some fancy voice technology to open a safe) or your favorite music\ngroup might very well rely on Fourier transforms. As follows: every tone (of a voice or an\ninstrument) typically has a frequency associated to it. However, a sound is seldom 'pure',\nbut consists of several superposed frequencies. These extra frequencies are sometimes\ncalled overtones. Their relative amplitude make the sound to what it is: they give it the\ntypical total sound called timbre. It is precisely this what allows us to distinguish between\ndifferent instruments and different voices: not just their pitch, but the combination of\ndifferent frequencies. Here, Fourier analysis comes into play. Imagine someone gives you a\nsingle sound signal (an amplitude as a function of time), and asks you to analyze it. This\nis not an easy task. However, if you use Fourier analysis (by f.e. some software that does\nthe integrals/decomposition for you) you get a very nice overview of the frequencies that\nmake up the signal. This allows you to very economically summarize the signal - as if you\ngot it's fingerprint. This way, a synthesizer can store large numbers of different sounds,\nnot relying on a sound database, but merely keeping some characteristic numbers for each\none of them. For speaker recognition, the idea is similar: the composition of someone's\nvoice can be analyzed and allows pretty well to uniquely characterize it, allowing for a\nrather secure identification method, just like a fingerprint does. Just to give a slight hint\non the versatility of this technique.\n\n\fCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n7.1.2\n\n107\n\nFourier transform\n\nYou may wonder: can make the above trick useful in the context of wave functions too?\nThe answer is yes, but there are two important differences with the above. First, a wave\nfunction is not typically a periodic function. Second, a wave function is a complex function\nwhile the above only dealt with real functions. This means we will have to adapt the above\ntrick slightly, to a technique called Fourier transform . We start out with the remark\nthat the complex analogue of sin(kx) and cos(kx) is the function\neikx\n\nwith\n\nk real\n\n(7.9)\n\nSo it is natural to go and try to decompose a complex function into several such eikx .\nSince in the context of a wave function no specific periodicity is required, k can take on\nany real value. This suggests we need an integral (not just a sum) over different k's. More\nprecise, we want to write\nZ +\u221e\n1\ndk g(k) eikx\n(7.10)\nf (x) = \u221a\n2\u03c0 \u2212\u221e\nHere, g(k) is a complex function, the Fourier transform of f , and has the same role as the\ncoefficients an and bn before. The prefactor is just there for convenience.3 The function\ng(k) describes to what extent each of the waves eikx is 'present' in the shape of f . So if\nf (x) has a piece that looks like a (complex) oscillation eik0 x , then g(k) will typically be\nlarge around this k0 . Just like with a Fourier series, we can obtain g by performing an\nintegral of f . It turns out that g can be obtained as follows:\nZ \u221e\n1\ndx f (x)e\u2212ikx\n(7.11)\ng(k) = \u221a\n2\u03c0 \u2212\u221e\nNotice how the last two expressions are nicely symmetric (up to the minus sign). For this\nreason, the second one is called inverse Fourier transform. Let us try to show how\nthe above definition of g indeed is the correct one. In words, we want to show that the\nabove definition of g substituted into (7.10) indeed gives f as a result. If we plug in the\ndefinition, we get\n\u0014\n\u0015\nZ +\u221e\nZ \u221e\n1\n1\n\u2032\n\u2032 \u2212ikx\u2032\n\u221a\ndk \u221a\ndx f (x )e\neikx\n(7.12)\n2\u03c0 \u2212\u221e\n2\u03c0 \u2212\u221e\nand we want to show that this indeed equals f (x). (We have changed the integration\nvariable to x\u2032 , since x is already taken - its the argument of function f .)\n3\n\nAlso, note that we have written the integration measure dk before the integrand and not after it. In\ncase you have not met this before: it is a different notation but has an identical meaning. If you are\nconfused by it, just scratch away the dk and write it at the end of the expression.\n\n\f108\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\n\nProof that (7.12)= f (x).\nIf we rearrange, and rewrite the range of integration as a limit, expression (7.12) becomes:\n1\n2\u03c0\n\nZ\n\n\u221e\n\n\u2032\n\n\u2032\n\ndx f (x ) lim\n\n\u2212\u221e\n\nZ\n\nK\n\nK\u2192\u221e \u2212K\n\n\u2032\n\ndkeik(x\u2212x )\n\n(7.13)\n\nWe have interchanged the integrals, which -in the context of physics- is no problem. (Mathematicians will tell you that this may not always be correct but we're not going to be that\ncareful.) Now the integral on the right can be done explicitly (renaming y = x \u2212 x\u2032 ):\nZ\n\nK\n\niky\n\ndk e\n\u2212K\n\n\u0014\n\n1 iky\n=\ne\nik\n\n\u0015k=K\n\n=2\n\nk=\u2212K\n\nsin(Ky)\ny\n\n(7.14)\n\nThe function on the right side (not including the factor 2) is called the sinc function. It's\nshape is shown in Figure 7.2. There, you can see that the central peak is getting higher\nand more narrow with increasing K. The limit for large K occurring in (7.13) goes to (\u03c0\ntimes) a very special function, the Dirac delta function (denoted \u03b4):\nsin Ky\n= \u03c0\u03b4(y)\nK\u2192\u221e\ny\nlim\n\n(7.15)\n\nWe will say more about this function in the next section, but we already give one of its\nproperties, namely:\nZ \u221e\nf (x)\u03b4(x) dx = f (0)\n(7.16)\n\u2212\u221e\n\nfor every function f . This is a peculiar property, but as promised, the next section will\nexplain this more carefully, so don't worry - just assume it is true for now. With this\ninformation, we can rewrite (7.13) further. In terms of the new variable y = x \u2212 x\u2032 , the\nright hand side becomes\nZ \u221e\nZ \u221e\n1\nsin \u03a9y\n1\n=\ndy f (x \u2212 y) lim 2\ndy f (x \u2212 y)2\u03c0\u03b4(y)\n(7.17)\n\u03a9\u2192\u221e\n2\u03c0 \u2212\u221e\ny\n2\u03c0 \u2212\u221e\nUsing the special property of the delta-function, we can rewrite this last expression as\nZ \u221e\n1\ndy f (x \u2212 y) 2\u03c0\u03b4(y) = f (x \u2212 y)|y=0 = f (x)\n(7.18)\n2\u03c0 \u2212\u221e\nAh! This is precisely what we wanted. The above line shows that the definition of g(k)\nis indeed the right one, and that it correctly gives the decomposition of f (x) into a sum\n(integral) of oscillating functions eikx . This decomposition will turn out to have some nice\napplications in quantum mechanics, as we will see in the rest of this chapter. Before we\ngo there, some more words on this mysterious delta function.\n\n\f109\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\n\ny\n\u03a9=1\n\u03a9=2\n\u03a9=4\n1\n\u03c0\n\n-\u03c0\n\nFigure 7.2: The sinc function y =\n\n7.1.3\n\nsin(\u03a9x)\n,\nx\n\nx\n\nplotted for several values of \u03a9.\n\nDelta function\n\nConsider the following series of functions \u03c7n (with n a positive integer):\n\u001a\n1\n1\nn if x \u2208 [\u2212 2n\n, 2n\n]\n\u03c7n (x) =\n0 everywhere else\n\n(7.19)\n\nThese functions have the following property: for large n they are only nonzero within a\nvery narrow interval. Despite this narrowing, the surface under each of these functions\nis always 1, due to the fact that the height of the bump grows bigger and bigger (Figure\n7.3). Explicitly:\n\ny\n\u03c73\n\u03c72\n\u03c71 1\n-1\u20442\n\n1\u20442\n\nx\n\nFigure 7.3: The functions \u03c7n .\n\n\f110\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\n\nZ\n\n\u221e\n\n\u03c7n (x)dx = 1\n\n\u2212\u221e\n\n\u2200n\n\n(7.20)\n\nNow here is an alternative (but equivalent) definition of the Dirac delta function we met\nabove:\n\u03b4(x) = lim \u03c7n (x).\n(7.21)\nn\u2192\u221e\n\nNow if you like performing limits, you will immediately see that\n\u001a\n\u221e\nif x = 0\n\u03b4(x) =\n0 everywhere else\n\n(7.22)\n\nThis is definitely not an ordinary function. In fact, the object \u03b4 is not a function, but a\ndistribution. A distribution is an object that makes not so much sense on itself, only\nwhen it standing inside an integral. Indeed, although the above prescription looks a bit\nstrange, the integral of \u03b4 is perfectly fine:\nZ \u221e\nZ \u221e\nZ \u221e\n\u03c7n (x) dx = lim 1 = 1\n(7.23)\nlim \u03c7n (x) dx = lim\n\u03b4(x) dx =\n\u2212\u221e n\u2192\u221e\n\n\u2212\u221e\n\nn\u2192\u221e\n\nn\u2192\u221e \u2212\u221e\n\nAlso, using the fact that for a continuous function f\nZ x+\u01eb\nlim\nf (y)dy = 2\u01ebf (x)\n\n(7.24)\n\n\u01eb\u21920 x\u2212\u01eb\n\nwe get\n\nZ\n\n\u221e\n\nf (x)\u03b4(x) dx =\n\n\u2212\u221e\n\n=\n\nlim\n\nZ\n\n\u221e\n\nf (x)\u03c7n (x) dx\n\nn\u2192\u221e \u2212\u221e\nZ 1/2n\n\nlim n\n\nn\u2192\u221e\n\n(7.26)\n\n\u22121/2n\n\n1\nlim n f (0)\nn\u2192\u221e n\n= f (0).\n\n=\n\nf (x) dx\n\n(7.25)\n\n(7.27)\n(7.28)\n\nThis result is precisely the property we used in the section about Fourier series! In words:\nthe central peak of \u03b4(x) picks out the region around zero as the only contribution to the\nintegral, giving f (0) as a result. From this property, one can derive that\nZ \u221e\nf (x)\u03b4(x \u2212 a) dx = f (a).\n(7.29)\n\u2212\u221e\n\nTo see this, just rewrite this last expression in terms of a new integration variable y = x\u2212a.\nHopefully, you are not too much confused by the peculiar character of this object. Just\nkeep in mind that the delta function is not an ordinary function, but a more general\nobject: a distribution. As physicists are less careful about some things (like interchanging\nsums and integrals an limits in all the above) they tend to treat the object \u03b4(x) just as a\nfunction, and even named it that way. You will get used to these properties later on.\n\n\fCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n111\n\nSmall remark\nA last remark: you may wonder whether the definition used here is really equivalent to\nthe one from section on Fourier transforms. We will not really show this, but you may be\nconvinced by looking back at Figure 7.2. There too, we were dealing with an infinite series\nof functions (labeled by K instead of n). There too, a central peak was getting higher and\nnarrower, while maintaining the area under the graph constant. Hopefully you see that\nsuch a series of functions will converge to the same 'object', the Dirac delta function. Just\nnotice that all the above properties derived above only depended on the limiting behavior\nof the \u03c7n , and not so much on their specific shape. So far the tools, let us get back to the\nworld of quantum mechanics.\n\n7.2\n\nChanging habits\n\nIn the previous chapters, you learned that the states of a particle form a complex vector\nspace, the Hilbert space. So far, we have studied three systems: a particle in a box, a\nparticle in a harmonic potential, and the hydrogen atom. In each case we have found energy\neigenstates as a natural basis (modulo soms complications that arose for the Hydrogen\natom). Now let's try to change habits a bit, and see if we can also express states in terms\nof another basis. Here we will describe two other natural bases: the momentum basis\nand the position basis. Just to make things easier, we will work in one dimension - this\navoids working with vectors - but all can be readily generalized to three dimensions.\n\n7.2.1\n\nMomentum basis\n\nWe have met the momentum operator already several times:\nP = \u2212i~Dx\n\n(7.30)\n\nYou can easily check that this operator has eigenfunctions\neipx/~\n\n(7.31)\n\nwith eigenvalue p. Now imagine someone gives you the wave function \u03c8(x) of a particle,\nand asks you whether you can write that wave function as a sum of momentum eigenstates\neipx/~ . You think about it for a moment, and then you realize: of course I can! This is\nprecisely what a Fourier transform does. So yes, you can write as the decomposition\nZ \u221e\n1\ndp \u03c8(p)eipx/~\n(7.32)\n\u03c8(x) = \u221a\n2\u03c0~ \u2212\u221e\nwhere \u03c8(p) is given by\n1\n\u03c8(p) = \u221a\n2\u03c0~\n\nZ\n\n\u221e\n\u2212\u221e\n\ndx \u03c8(x)e\u2212ipx/~\n\n(7.33)\n\n\f112\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\n\nCompared to the original expression for the Fourier transform, some extra ~'s have popped\nup in the prefactors. This is just a convention, and due to the fact that we decompose into\neipx/~ instead of the ordinary eikx . Furthermore, we have denoted the Fourier transform\n\u03c8(p) by the same letter \u03c8 - in principle this is an ambiguous notation, but we will always\nwrite the argument explicitly to avoid confusion. So \u03c8(p) and \u03c8(x) are different functions.\nThe Fourier transform \u03c8(p) is called the momentum space representation of the wave\np\nfunction. For each p, it tells you to what extent the wave function looks like ei ~ x . To\ncontrast, the wave function \u03c8(x) which we worked with up to now, is called the position\nrepresentation. The momentum representation (like with the position representation)\nis a function spread out over a certain region. So just like the fact that a wave function\ndoesn't have a sharp position, it has a smeared out (complex) function describing its\nmomentum. This means we have a very symmetric way to describe a particle: either by\nits wave function in position space \u03c8(x), or by its wave function in momentum space \u03c8(p).\nBra-ket notation\nWe can also rewrite the above in bra-ket notation. First, we define the momentum state\n|pi by taking the corresponding wave function as follows:\n|pi\n\n\u2194\n\neipx/~\n\u221a\n2\u03c0~\n\n(7.34)\n\nWith this, we can write the state |\u03c8i corresponding to the wave function \u03c8(x) as follows:\nZ \u221e\ndp \u03c8(p)|pi\n(7.35)\n|\u03c8i =\n\u2212\u221e\n\nIf you see the integral as an infinite sum, the right hand side is a linear combination of all\nthe different |pi. The \u03c8(p) can thus be seen as the coefficients of all the |pi in this linear\ncombination and is given explicitly by (7.33). With this, we have reached the goal of this\nsection: decomposing an arbitrary state |\u03c8i with respect to the basis of all momentum\neigenstates |pi.\nMeasurements\nBeside giving the particle's momentum 'components', the function \u03c8(p) has another meaning. Given a state |\u03c8i and a device that measures the momentum, the chance to receive\nan outcome between momenta p0 and p1 is given by\nZ p1\n|\u03c8(p)|2 dp\n(7.36)\nP (p0 < (outcome of measurement) < p1 ) =\np0\n\nSo the object |\u03c8(p)|2 acts as a probability density, just like |\u03c8(x)|2 . Yes, yes: it is all\nvery symmetric between x and p. Here is an example of a wave function \u03c8(x) (real part\nshown) and its momentum representation \u03c8(p):\n\n\f113\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n\u03c8(p)\n\n\u03c8(x)\nx\n\np\n\n7.2.2\n\nPosition basis\n\nWe now know there is something like a momentum basis. Let us now try to hunt for\nits counterpart, the position basis. First, think about this question more careful. The\neigenstates we found for the momentum operator had a very clear physical meaning: they\nwere infinite waves, with a sharp and well-defined momentum. Similarly, we are now\nlooking for functions that can describe a wave function which is sharply localized, so that\nit has an exact position. We did recently meet such 'infinitely sharp' function: the delta\nfunction. Indeed, the set of functions\nfx0 (x) = \u03b4(x \u2212 x0 )\n\n(7.37)\n\nare all infinitly peaked. The peak of each fx0 is at x0 , as on that place the function value\nis \u03b4(x0 \u2212 x0 ) = \u03b4(0) = \u221e. So we suggest that the function fx0 describes a wave function\nof a state with exact (sharp) position x0 . We denote such a sharp position-state by |x0 i,\nso\nwave function fx0 \u2194 state |x0 i \u2194 state sharply localized at position x0\n\n(7.38)\n\nLet us do a check on this. For consistency, we should show that the position-states |x0 i are\nindeed eigenstates of the position operator X, just in the same way the momentum-states\n|pi are eigenstates of the momentum operator P . So we want to show:\nX|x0 i = x0 |x0 i\n\n(7.39)\n\nfor all x0 . Using that X acts as multiplication with the identity function (so X : f (x) \u2192\nx * f (x)) the wave functions of both sides are\nx \u03b4(x \u2212 x0 ) = x0 \u03b4(x \u2212 x0 ).\n\n(7.40)\n\nAs we have said before, \u03b4 is a distribution, and such an object makes better sense when\nstanding inside an integral. Let us see what both sides of (7.40) give inside an integral\nwith a random function h(x):\nZ \u221e\nZ \u221e\n(xh(x)) \u03b4(x \u2212 x0 ) dx = x0 h(x0 )\nh(x) (x \u03b4(x \u2212 x0 )) dx =\n\u2212\u221e\n\u2212\u221e\nZ \u221e\nZ \u221e\nh(x) \u03b4(x \u2212 x0 ) dx = x0 h(x0 )\nh(x) (x0 \u03b4(x \u2212 x0 )) dx = x0\n\u2212\u221e\n\n\u2212\u221e\n\n\fCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n114\n\nThe result is the same! As the above holds for any function h(x), we have to conclude\nthat the objects x\u03b4(x \u2212 x0 ) and x0 \u03b4(x \u2212 x0 ) are indeed equal. This proves (7.40) and hence\n(7.39).\nBra-ket notation\nCan we now express any state as a linear combination of position states |x0 i? By analogy\nwith momentum eigenstates, you might guess that we can write an arbitrary state |\u03c8i as\nfollows:\nZ \u221e\ndx0 \u03c8(x0 )|x0 i.\n(7.41)\n|\u03c8i =\n\u2212\u221e\n\nwhere \u03c8(x0 ) is the wave function, evaluated at x0 . This expression makes sense since the\nweight given to each position state |x0 i in the above 'sum' is just \u03c8(x0 ): precisely the\nvalue of the wave function at position x0 . Better even, we can actually prove the above\nequation by showing that the state on the right side really has wave function \u03c8(x). To\nsee this, note that each |x0 i has wave function \u03b4(x \u2212 x0 ), so the total wave function of the\nright hand side is\nZ\n\u221e\n\n\u2212\u221e\n\ndx0 \u03c8(x0 )\u03b4(x \u2212 x0 ),\n\n(7.42)\n\nwhich just equals \u03c8(x) by the standard property of the delta-function. Expression (7.41)\ncompletes our second goal: writing out an arbitrary state |\u03c8i as a linear combination\n(integral) of position-basis elements. This means we now understand how to write an\narbitrary state in several bases:\n\u2022 a basis of energy eigenstates (as in the first chapters),\n\u2022 a basis of momentum eigenstates |pi (each with weight \u03c8(p))\n\u2022 or a basis of position eigenstates |xi (each with weight \u03c8(x)).\nThe last two give the momentum- and space representation we wanted to discover. Before\nwe do some concrete examples, one last remark.\n\nCaricature states\nWe just got to know two new kinds of states: momentum states |pi and position states\n|xi. (From now on, we drop the subscript 0 on the position state, so by |xi we mean the\nposition state located at position x.) Both are a bit peculiar: one is infinite in extend\n(an infinite wave) the other is infinitely sharp (a delta function). We already mentioned\nthat real particles are never in such a state, but always a superposition (integral) of these,\nwhich results in a nice, finite and smooth wave function. So what are the states |pi and |xi\nthen? The best way to describe them is as caricature states. They describe a particle,\nif you could squeeze it into a perfect state with exact momentum, or into a perfect state of\nexact position. A very good comparison are point masses in Newtonian mechanics. When\n\n\fCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n115\n\ndealing with problems of bodies moving and interacting, you were often told to 'treat the\nbodies involved as point masses'. This is not a realistic description, but allows for simple\ncalculations. A point mass does not exist, but the mathematics of such an object is much\neasier than that of a (physical, extended) object. In very much the same way, the states\n|pi and |xi do not 'exist in nature', but they have some nice properties, like providing a\nclean basis in which you can express a physical state |\u03c8i. That is why we study them,\nnothing more, nothing less.\n\n7.3\n7.3.1\n\nUsing the position and momentum representation\nExample\n\nWe will now use all the above in the simplest example we have met so far: the particle in\na box. Recall that we found energy eigenstates labelled by n = 1, 2, ...:\nr\nn\u03c0x\n2\n\u03c8n (x) =\nsin\n(7.43)\nL\nL\nThese satisfy H\u03c8n (x) = En \u03c8n , and their time evolution dictated by the SE is given by\n\u03c8n (x, t) = e\u2212iEn t/~ \u03c8n (x)\nThe most general element in the Hilbert space is given by\nX\n\u03c8(x) =\nan \u03c8n (x)\n\n(7.44)\n\n(7.45)\n\nn\n\nand its time evolution is just given by \u03c8(x, t) =\nPosition basis\n\nP\n\nn an \u03c8n (x, t).\n\nThe last equation describes a general element |\u03c8i in the Hilbert space, decomposed in\nenergy eigenstates. Here, we will try to express this very state |\u03c8i in the position- or\nmomentum basis. Let us start with the easy one: the position-basis. First, note that\nevery energy eigenstate |\u03c8n i can be written as\n!\nr\nZ L\nZ \u221e\n2\nn\u03c0x\ndx\ndx \u03c8n (x)|xi =\n|\u03c8n i =\nsin\n|xi\n(7.46)\nL\nL\n0\n\u2212\u221e\n\n\fCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n116\n\nThe range of integration has changed in the last step: this is because the wave functions\n\u03c8n are actually zero outside [0, L]. Hence, for the state |\u03c8i itself, we have:\nX\n|\u03c8i =\nan |\u03c8n i\n(7.47)\nn\n\n=\n\nX\n\nan\n\nn\n\n=\n\nZ\n\n! !\nn\u03c0x\n2\n|xi\nsin\ndx\nL\nL\n0\n!\nr\nn\u03c0x\n2X\nan sin\n|xi\nL n\nL\n\nZ\n\nL\n\ndx\n0\n\nL\n\nr\n\n(7.48)\n(7.49)\n\nIn the same way the\nare the coefficients\nof |\u03c8i with respect to the energy-eigenstate\n\u0010qan P\n\u0011\nn\u03c0x\n2\nbasis, the function\ndescribes the 'coefficients' of the same state when\nn an sin L\nL\nexpressed in the position basis. In simple terms: this object is just the (position) wave\nfunction of |\u03c8i. Can we also do the same for momentum? That is, can we also find a\nfunction \u03c8(p) such that\nZ\n|\u03c8i =\n\n\u221e\n\ndp \u03c8(p)|pi\n\n(7.50)\n\n\u2212\u221e\n\nAgain we first try to do this for each |\u03c8n i individually, so we will first look for functions\n\u03c8n (p) satisfying\nZ \u221e\ndp \u03c8n (p)|pi\n(7.51)\n|\u03c8n i =\n\u2212\u221e\n\nFrom the general formula (7.33) for the Fourier transform, we get\nZ \u221e\n1\n\u03c8n (p) = \u221a\ndx e\u2212ipx/~ \u03c8n (x)\n2\u03c0~ \u2212\u221e\n!\nr\nZ L\nn\u03c0x\n2\n1\n\u2212ipx/~\nsin\ndx e\n= \u221a\nL\nL\n2\u03c0~ 0\n!\nr\nZ L\ni n\u03c0x\n\u2212i n\u03c0x\nL \u2212 e\nL\ne\n1\n=\ndx e\u2212ipx/~\n\u03c0~L 0\n2i\nr\nZ L\ni\nh\n\u2212p\n\u2212p\nn\u03c0\nn\u03c0\n1\n1\n=\ndx eix( ~ + L ) \u2212 eix( ~ \u2212 L )\n2i \u03c0~L 0\nr\n\u0014\n\u0015\n1\n1\n1 ixc1\n1 ixc2 x=L\n=\ne\ne\n\u2212\n2i \u03c0~L ic1\nic2\nx=0\n\u0001\n\u0001\nn\u03c0\nn\u03c0\nand c2 = \u2212p\nIn the last step we have defined c1 = \u2212p\n~ + L\n~ \u2212 L . With these\nsions, you can write out the end result and simplify it a little bit, but we don't\neffort here. Just note that the end result depends on p - via the objects c1 and\n\n(7.52)\n\n(7.53)\nexpresdo this\nc2 . We\n\n\f117\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\n\nhave just done our first Fourier transform! Congratulations. With this, the state |\u03c8i can\nthen be written as follows:\nX\n|\u03c8i =\nan |\u03c8n i\nn\n\nX\n\n=\n\nan\n\n=\n\n\u221e\n\n\u2212\u221e\n\nn\n\nZ\n\n\u0012Z\n\n\u221e\n\n\u2212\u221e\n\ndp\n\nX\nn\n\n\u0013\ndp \u03c8n (p)|pi\n!\n\nan \u03c8n (p) |pi\n\n(7.54)\n\nwhere the object between brackets -with \u03c8n (p) given in (7.53)- is just one function of p.\nThis is the finish line! We have succeeded in writing |\u03c8i not only in energy representation\n(7.45) but also in the position basis (7.49) and momentum basis (7.54). If you managed\nto follow this last derivation, you have probably understood most of this chapter so far.\n\n7.3.2\n\nProjection, inner product and completeness\n\nAs a last application of what we have learned in this chapter, we give the so-called completeness relations. These are very useful equations, and provide a nice mnemonic for\nthe formula of Fourier transform. First, recall that in a vector space we could take the\nprojection of a vector on a basis element:\nh~ei , ~v i = vi\n\n(7.55)\n\nIf the basis vectors ei are orthonormal, we can get back the original vector ~v by summing\nup all its projections along the basis vectors:\nX\nX\n~v =\nei vi =\n~ei h~ei , ~v i\n(7.56)\ni\n\ni\n\nOr if we write the right hand side with a Hermitian conjugate:4\n!\nX\nX\n\u2217\n\u2217\n~ei~ei ~v\n~v =\n~ei (~ei ~v ) =\n\n(7.57)\n\ni\n\ni\n\nIn the last step, we used associativity of matrix multiplication. This is true for all ~v , which\nimplies\nX\n~ei e~i \u2217 = 1\n(7.58)\ni\n\n4\n\na \u2020.\n\nIn the context of ordinary linear algebra, the Hermitian adjoint is denoted by an asterisk \u2217 instead of\n\n\f118\n\nCHAPTER 7. POSITION AND MOMENTUM BASIS\n\nwhere 1 stands for the identity\nFor example,\n\u0012 matrix.\n\u0013\n\u0012 \u0013 in a two dimensional vector space\n1\n0\nwith orthonormal basis e1 =\nand e2 =\nwe have\n0\n1\nX\ni\n\n~ei e~i \u2217 =\n\n\u0012\n\n1\n0\n\n\u0013\n\n1 0\n\n\u0001\n\n+\n\n\u0012\n\n0\n1\n\n\u0013\n\n0 1\n\n\u0001\n\n=\n\n\u0012\n\n1 0\n0 1\n\n\u0013\n\n.\n\n(7.59)\n\nEquation (7.58) is the completeness relation (for vector spaces). The name is derived from\nthe fact that such a relation only holds for a complete (and orthonormal) basis. Again,\nit is possible to extend this to the context of quantum mechanics. First, we give some\nresults that you can show yourself (see exercises):\nhx|\u03c8i = \u03c8(x)\n\nhp|\u03c8i = \u03c8(p)\neipx\nhx|pi = \u221a\n2\u03c0~\nhx1 |x2 i = \u03b4(x1 \u2212 x2 )\nhp1 |p2 i = \u03b4(p1 \u2212 p2 )\n\n(7.60)\n(7.61)\n(7.62)\n(7.63)\n(7.64)\n\nEspecially the last two equations are very suggestive: the states |xi and |pi are 'orthonormal': only then with a Dirac delta function instead of a Kronecker delta. This suggests\nmaybe here too, we have something like a completeness relation. Indeed, it turns out that\nZ \u221e\ndx |xihx| = 1\n(7.65)\n\u2212\u221e\nZ \u221e\ndp |pihp| = 1.\n(7.66)\n\u2212\u221e\n\nWhere 1 is the identity operator (sends every state to itself). These are the completeness\nrelations for quantum mechanics.RYou might be a bit puzzled if the objects on the left side\n\u221e\nare really operators. The object \u2212\u221e dx|xihx| for example is to be read as the command:\nstarting with a state, take the inner product with hx|. Then, multiply this (complex)\nnumber by the corresponding state |xi and then sum over all x. This operation indeed\nsends a state to a state. Hopefully, you see the full parallel with (7.58). Here are nice\n\n\fCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n119\n\nconsequences of the above equations. First:\n|\u03c8i = 1|\u03c8i\n\u0013\n\u0012Z \u221e\ndx |xihx| |\u03c8i\n=\nZ \u221e\u2212\u221e\ndx |xihx|\u03c8i\n=\n\u2212\u221e\nZ \u221e\ndx |xi\u03c8(x)\n=\n\u2212\u221e\nZ \u221e\ndx \u03c8(x)|xi.\n=\n\n(7.67)\n(7.68)\n(7.69)\n(7.70)\n(7.71)\n\n\u2212\u221e\n\nAh! If you compare the first and last part, you see that we precisely get the decomposition\nof |\u03c8i into position states. That's a really short way to get there. Another example:\n\u03c8(x) = hx|\u03c8i\n\n= hx|1|\u03c8i\n\u0013\n\u0012Z \u221e\ndp |pihp| |\u03c8i\n= hx|\n\u2212\u221e\nZ \u221e\ndp hx|pihp|\u03c8i\n=\n\u2212\u221e\nZ \u221e\n1\n= \u221a\ndp eipx/~ \u03c8(p)\n2\u03c0~ \u2212\u221e\n\n(7.72)\n(7.73)\n(7.74)\n(7.75)\n(7.76)\n\nIn the before last step, when pulling out the integral to the left, we have used linearity of\nthe inner product. Once again comparing first- and last part, we see that we have fully\nrediscovered the Fourier transform, out of the hat! This ultrashort 'proof' of the Fourier\ntransform formula is a very clear sign of the power of the bra-ket notation. You may even\nstart to like it one day... .\n\nConclusion\nIn this chapter, you have learned to view a wave packet in a different way: as a superposition of different infinite waves (momentum-eigenstates), each with some weight. This\nshould caution you a bit when speaking about a particle's momentum - just like with\nposition the particle's state comprises an extended region of momenta. Of course, you can\nstill speak of the expectation value of both, these are well-defined.\nWhen you think back of the de Broglie relations, you may be a bit confused: there, the\nparticle's momentum, energy, wavelength occur. Actually, it is best to see the variables\nover there as expectation values, for a state which is sufficiently centered around these\nvalues.\n\n\fCHAPTER 7. POSITION AND MOMENTUM BASIS\n\n120\n\nExercises\n1. Show (7.60) - (7.64).\n2. Argue that the probability density of the momentum and position representation\nare actually mere applications of the Postulate on the measurement, with the only\ndifference that sums are replaced by integrals, and probabilities by probability densities.\n3. Check that (7.3) is true (should not be much work). Can you find an easy way\nto show (7.4) for the cases n 6= m? (Hint: the states \u03c8n of the particle in a box\nhave different eigenvalues under H. What does this imply? Can you relate this\nto what we want to show here?) To show the case n = m, you may want to use\nsin2 mx = 1\u2212cos2 2mx . Use the result to check that we properly normalized the states\np\nof the particle in the box by putting the prefactor 2/L.\n\n4. Consider the function f (x), which has period 2\u03c0, has f (x) = \u22121 if \u2212\u03c0 < x < 0 and\nf (x) = 1 if 0 < x < \u03c0. This function is called the square wave. What does this\nfunction look like: a sine or a cosine function? So which component of the series do\nyou expect to be largest? Now go ahead and compute its Fourier series. (You best\nsplit the integrals in two parts. Which Fourier components are zero?)\n\u221a\n1\n5. Consider the wave function \u03c8n (x) = x which equals n on \u22121\n2n < x < 2n and\nzero elsewhere. What is the Fourier transform of \u03c8n (x)? Which function do you\nrecognize?\n\n\fChapter 8\n\nThe uncertainty principle\nIn this chapter...\nIn the chapter on commutators we showed that compatible observables (observables admitting a basis elements, the elements of which are eigenstates to both operators) always\ncommute. In this chapter, we spend some time on a logical consequence of this: if operators don't commute, it is impossible to find such a basis of common eigenstates. A\nquantitative consequence is the so-called uncertainty principle. This is an important principle, with a very high degree of quantum flavor to it: it will help you to get a better feel\nfor the nature of wave functions.\nIn the tool part, we show an important example of a Fourier transform which already suggests the uncertainty principle. In the story, we explain in more detail what\nthe principle is, where it comes from. We then give a proof and conclude with several\nconsequences/applications.\n\n121\n\n\f122\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\n8.1\n\nFourier transform of Gaussians\n\nAs promised in the intro, the tool part consists of an extra example on Fourier transforms.\nTwo other small things we will review are the notion of variance, and the Cauchy-Schwarz\ninequality. Ready? Go!\n\n8.1.1\n\nVariance\n\nIn statistics a function \u03c1(x) is called a probability density function if for each x, \u03c1(x)\nis a measure for the chance to have x as an outcome. The usage of this term in quantum\nmechanics is just a special case of this. For the total probability to be one, we need:\nZ \u221e\n\u03c1(x) dx = 1\n(8.1)\n\u2212\u221e\n\nOne then defines the expectation value of x, as follows:\nZ \u221e\nx\u03c1(x) dx.\nhxi =\n\n(8.2)\n\n\u2212\u221e\n\nFor example, if \u03c1(x) is the chance to win an amount x at the lottery, then hxi will give\nthe amount you win on average per game. (Which is always lower than the cost of you\nticket, alas.) Equally interesting, one may ask how much your winnings will typically be\naway from the average winnings. A quantity expressing this, is given by\nZ \u221e\n2\n(x \u2212 hxi)2 \u03c1(x) dx.\n(8.3)\nh(x \u2212 hxi) i =\n\u2212\u221e\n\nThis quantity indeed says something about the typical deviation: it gives the expectation\nof the squared deviation from average. This quantity is the variance Var(x), and its\nsquare root is called the uncertainty \u2206x:\nh(x \u2212 hxi)2 i = Var(x) = (\u2206x)2\n\n(8.4)\n\nOf course you see that the above is what we have been using in the context of quantum\nmechanics. Just write the Rprobability density |\u03c8(x)|2 instead of the probability density\n\u221e\nfunction \u03c1. The quantity \u2212\u221e |\u03c8|2 x dx then nicely gives the expectation value of the\nposition:\nZ\nZ\nhXi = h\u03c8|X|\u03c8i =\n\n\u221e\n\n\u03c8 \u2217 (x)X\u03c8(x) dx =\n\n\u2212\u221e\n\nSimilarly the uncertainty of the position is given by\np\n\u2206X = h(X \u2212 hXi)2 i\n\n\u221e\n\nx|\u03c8(x)|2 dx.\n\n(8.5)\n\n\u2212\u221e\n\n(8.6)\n\n\f123\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\nwith\n2\n\nh(X \u2212 hXi) i =\n\nZ\n\n\u221e\n\n\u2212\u221e\n\n(x \u2212 hXi)2 |\u03c8(x)|2 dx.\n\n(8.7)\n\nMore general, for any observable O, we can define the expectation value hOi = h\u03c8|O|\u03c8i\n(see chapter on observables), and the corresponding uncertainty is\np\n\u2206O = h(O \u2212 hOi)2 i\n(8.8)\n\nAlthough the name sounds somewhat mystical, there is nothing deep about the notion of\nuncertainty: it just expresses how far possible measurement outcomes of the observable O\non a given state |\u03c8i are lying apart. For example: a wave packet that is rather localized\nwill have a small uncertainty \u2206X, a packet which is more spread out will have a much\nlarger position uncertainty. Last remark: although the last two expressions don't seem to\ncontain the state |\u03c8i, they really do depend on it as the expectation values hi are to be\ntaken between h\u03c8| and |\u03c8i. To avoid confusion, some people write subscripts:\nq\n(8.9)\nh\u2206Oi\u03c8 = h(O \u2212 hOi\u03c8 )2 i\u03c8\n\n8.1.2\n\nFourier transform of Gaussians\n\nA very important function in math and physics, is the Gaussian distribution. It is given\nby\nx2\n1\ne\u2212 2\u03c32\n(8.10)\nf (x) = \u221a\n2\u03c0\u03c3 2\nand has variance Var(x) = \u03c3 2 . Also, using the property\nr\nZ \u221e\n\u03c0\n\u2212\u03b1x2\n(8.11)\ne\ndx =\n\u03b1\n\u2212\u221e\n\nit is easy to show that f is a properly normalized probability density function.\n1.0\n\n\u03bc = 0, \u03c3 2 = 0.2,\n\u03bc = 0, \u03c3 2 = 1.0,\n\u03bc = 0, \u03c3 2 = 5.0,\n\n0.8\n\n0.6\n\nf (x)\n0.4\n\n0.2\n\n0.0\n\u22125\n\n\u22124\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\n\nx\n\n1\n\n2\n\n3\n\n4\n\n5\n\nFigure 8.1: The Gaussian distribution, for different values of the variance\n\n\f124\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\nThe fact that f is normalized, means that its square root\n\u03c6(x) =\n\n2\n1\n\u2212 x2\n4\u03c3\ne\n(2\u03c0\u03c3 2 )1/4\n\n(8.12)\n\nR\u221e\nis a normalized wave function: \u2212\u221e |\u03c6(x)|2 dx = 1. Now we ask the question: what is the\nFourier transform of this function? In other words: what is the momentum representation\n\u03c6(p) of the state |\u03c6i? We know that\nZ \u221e\n1\ndx \u03c6(x)e\u2212ipx/~\n(8.13)\n\u03c6(p) = \u221a\n2\u03c0~ \u2212\u221e\nso we get\n\nZ \u221e\n2\n1\n1\n\u2212 x 2 +ipx/~\n4\u03c3\n(8.14)\ndx\ne\n\u03c6(p) = \u221a\n2\u03c0~ (2\u03c0\u03c3 2 )1/4 \u2212\u221e\nAt first sight, performing the integral is not so obvious. However, there is a smart way\nto recast it into a form that allows to use (8.11). This is done by completing the square\nstanding in the exponent, as follows. Go to a new variable x\u2032 = x \u2212 i\u03c3p/~. Then\n\u20322\n\nx =\nand\n\n\u0012\n\n2i\u03c3 2 p\nx\u2212\n~\n\n\u00132\n\n= x2 \u2212 4ix\u03c3 2\n\np\np2\n\u2212 4\u03c3 4 2\n~\n~\n\n(8.15)\n\n2\n\u2212x2\np\n\u2212x\u20322\n2p\n+\n\u03c3\n=\n+\nix\n.\n4\u03c3 2\n4\u03c3 2\n~\n~2\n\n(8.16)\n\nExponentiating, we get\n\u2212x\u20322\n\n\u2212x2\n\np\n\ne 4\u03c32 = e 4\u03c32 eix ~ e\u03c3\n\n2\n2p\n~2\n\n(8.17)\n\nwhich is the integrand of (8.14) up to the last (x-independent) factor. This means\nterms of x\u2032 and dx\u2032 (= dx),\nZ \u221e\n2\nx\u20322\n2p\n1\n1\n\u2032 \u2212 4\u03c3\n2 e\u2212\u03c3 ~2\ndx\ne\n\u03c6(p) = \u221a\n2\u03c0~ (2\u03c0\u03c3 2 )1/4 \u2212\u221e\nZ \u221e\n2\nx\u20322\n1\n1\n\u2212\u03c32 p2\n\u2032 \u2212 4\u03c3 2\n~\ne\ndx\ne\n= \u221a\n2\u03c0~ (2\u03c0\u03c3 2 )1/4\n\u2212\u221e\n\u221a\np2\n2\n1\n1\ne\u2212\u03c3 ~2 * 4\u03c3 2 \u03c0\n= \u221a\n1/4\n2\n2\u03c0~ (2\u03c0\u03c3 )\nwhere in the last step we have used (8.11). Rearranging, and defining \u03c3 \u2032 =\nget\n2\n1\n\u2212 p \u20322\n4\u03c3 .\n\u03c6(p) =\ne\n(2\u03c0\u03c3 \u20322 )1/4\n\n~\n2\u03c3 ,\n\nthat in\n\n(8.18)\n(8.19)\n(8.20)\n\nwe finally\n(8.21)\n\nComparing to (8.12), you see that this distribution is again Gaussian, but now with\nvariance \u03c3 \u20322 instead of \u03c3 2 . This result is shown in the next figure:\n\n\f125\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\n\u03c6(x)\n\n\u03c6(p)\n\n2\u03c3\n\nx\n\n2\u03c3'\n\np\n\nThis is quite remarkable: if a Gaussian wave packet is sharply peaked, its momentum\nrepresentation will be very spread out, and vice versa. This is already a first glimpse of\nthe uncertainty principle which we are about to meet. Before we go there, another small\nrecap which might be useful:\n\n8.1.3\n\nCauchy-Schwarz inequality\n\nYou probably remember the famous Cauchy-Schwarz inequality from a course on linear\nalgebra. It states that for two vectors ~x and ~y,\n|~x * ~y| \u2264 k~xk . k~y k\n\n(8.22)\n\nIn words: the magnitude of the inner product of two vectors is always smaller than the\nproduct of their lengths. The reason for this inequality is very simple. For vectors, the\ninner product is just given by ~x * ~y = k~xk . k~y k . cos \u03b8 with \u03b8 the angle between the two\nvectors. Taking absolute values on both sides, you then get\n|~x * ~y | = k~xk . k~y k . | cos \u03b8 | \u2264 k~xk . k~y k\n\n(8.23)\n\nThis inequality can also be expressed\nin terms of inner products only. Writing h~x, ~y i\np\ninstead of ~x * ~y and using k~xk = h~x, ~xi, we get\n|h~x, ~y i|2 \u2264 h~x, ~xi h~y , ~y i.\n\n(8.24)\n\nIt turns out that this inequality is not only true for ordinary vector spaces Rn , but for any\nspace with an inner product. This means that also in the context of the Hilbert space\n|h\u03c7|\u03c8i|2 \u2264 h\u03c7|\u03c7i h\u03c8|\u03c8i\n\n(8.25)\n\nfor all states |\u03c7i and |\u03c8i. We will use this form of the Cauchy-Schwarz inequality later on\nin this chapter. Time for the story part...\n\n\fCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\n8.2\n8.2.1\n\n126\n\nThe uncertainty principle\nCommuting observables, revisited\n\nIn the chapter on the commutator, we learned that if two observables are compatible\n(=there exists a basis of states |\u03b1i , \u03b2j i, labeled by i and j, that are simultaneous eigenstates\nto both A and B) then these operators commute: [A, B] = 0:\nA|\u03b1i , \u03b2j i = \u03b1i |\u03b1i , \u03b2j i\n\u21d2\n\nand\n[A, B] = 0\n\nB|\u03b1i , \u03b2j i = \u03b2j |\u03b1i , \u03b2j i\n\n(8.26)\n\nSo in the case of commuting observables, a particle can have a definite value for both of\nthem at the same time: this is indeed true for all the states |\u03b1i , \u03b2j i. (Of course, it does\nnot have to be this way: if the particle is in a superposition - say a state proportional to\n|\u03b1i , \u03b2j i + |\u03b1i\u2032 , \u03b2j \u2032 i then it isn't an eigenstate of any of the two observables. But the fact\nthat you can find an entire basis of states which are simultaneously eigen is quite special.)\nNow the question we want to address in this chapter is the following: what happens\nif [A, B] 6= 0? Well, by the negation of the above expression, if two operators do not\ncommute, they are incompatible and you can not find a basis of simultaneous eigenstates.\nSo simultaneous eigenstates will be rather rare in this case. In practice, there are often\nno such states for non-commuting observables. So in such a situation we expect that an\neigenstate |\u03b1i of A can (generally) only be a sum of different eigenstates of B:\nX\n|\u03b2i i with B|\u03b2i i = \u03b2i |\u03b2i i\n(8.27)\n|\u03b1i =\ni\n\nSo for incompatible observables, if a state has a definite value with respect to one of them,\nit usually does not have a definite value with respect to the other one.\nTo make this more concrete, let us think of two observables that do not commute. Ah!\nPosition and momentum are such a pair. Recall that\n[X, P ] = i~\n\n(8.28)\n\nFrom the above, we expect that eigenstates of X will not be eigenstates of P . After the\nprevious chapter, we know both sets of eigenspace very well. The first are the position\neigenstates |xi (delta functions) and the second are the momentum eigenstates |pi (infinite\nwaves). For sure, these are two very different sets of functions: they do not have any\noverlap. There is not a single state that is both a position eigenstate and a momentum\neigenstate.\n\n8.2.2\n\nThe Gaussian distribution\n\nSo the above suggests that for every state, at least one of the observables X and P has\nto be 'spread out'. They can not both have a definite value. Let us now look back at the\n\n\f127\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\ncomputation we did in the first section. We found that for a Gaussian wave function\n\u03c6(x) =\n\n2\n1\n\u2212 x2\n4\u03c3 ,\ne\n(2\u03c0\u03c3 2 )1/4\n\n(8.29)\n\nthe momentum representation is also Gaussian\n\u03c6(p) =\n\n2\n1\n\u2212 p \u20322\n4\u03c3\ne\n(2\u03c0\u03c3 \u20322 )1/4\n\n(8.30)\n\nand the variances \u03c3 2 and \u03c3 \u20322 are related by\n\u03c3 \u20322 =\n\n~2\n.\n4\u03c3 2\n\n(8.31)\n\nRemember that the square root of the variance is the uncertainty, so \u2206X = \u03c3 and \u2206P = \u03c3 \u2032 .\nIn terms of these, the above relation becomes\n\u2206X\u2206P =\n\n8.2.3\n\n~\n2\n\n(8.32)\n\nThe uncertainty principle of Heisenberg\n\nThe relation (8.32) is a very striking result. It implies that a narrow distribution in\nposition space necessarily has a broad distribution in momentum space. It turns out that\nthis phenomenon also occurs for many other cases - not only for Gaussian wave packets.\nEvery time, the product \u2206X\u2206P is of the order of ~. Actually, the Gaussian wave packets\nhave a bit of a special role: they realize the lowest value for \u2206X\u2206P . For other wave\nfunctions, the product of the uncertainties tends to be bigger. This behavior has leads to\nthe following inequality:\n~\n(8.33)\n\u2206X\u2206P \u2265\n2\nfor every state |\u03c8i. This relation is called the uncertainty principle and is due to the\nphysicist Werner Heisenberg. It is the central result of this chapter. We will actually prove\nit in the next part. Before we go there, some small remarks:\n\u2022 Note that \u2206X and \u2206P are quantities that depend on the specific state of the particle.\nThis is because the expectation values hi are always to be taken with respect to some\nstate. The uncertainty principle states that there is a universal relation between\nthose two quantities which is satisfied for every physical state.\n\u2022 This relation has an immediate consequence for measurements. Remember that\nmeasuring a quantity collapses a state into a narrow region around it. For example,\nmeasuring the position collapses a state in a very narrow region centered around\nthe value you read of from the apparatus. Ideally, this is a delta-function (exact\nposition eigenstate) but in real (imperfect) measuring devices this is just a 'very\n\n\f128\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\nvery narrow function'. For such a wave function, the uncertainty \u2206X will be small.\nThe uncertainty principle then implies that \u2206P is very large. This means that a\nconsecutive measurement of momentum can yield a very broad range of results. This\nmeans it is very hard to predict the outcome of this second measurement. Of course,\na similar thing occurs when you first measure the momentum and directly after that\nthe position.\n\u2022 The last lines explained that there are consequences of the uncertainty principle\nfor the process of measurement. However, it is important to bear in mind that\nthe inequality also has a meaning besides the context of measurement. It is not\njust a limitation of a researcher to measure particular quantities of a system, but\nit is a statement about the nature of the wave function. Actually, it can be seen\nas a mathematical statement, relating the variances of a function and its Fourier\ntransform. That's why we can prove it.\n\n\u03c8(p)\n\n\u03c8(x)\nx\n\n\u0394x\n\np\n\n\u0394p\n_\n_\n\u0394x\u0394p \u2265 h\n2\n\nFigure 8.2: The uncertainty principle: a relation between the variances of the position\nand momentum representation of the wave function.\n\n8.3\n8.3.1\n\nProof and applications\nProof\n\nConsider two operators A and B, and a state |\u03c8i. Then\nkA|\u03c8ik2 kB|\u03c8ik2 = h\u03c8|A\u2020 A|\u03c8ih\u03c8|B \u2020 B|\u03c8i \u2265 |h\u03c8|A)(B|\u03c8i|2\n\n(8.34)\n\nIn the first step we used the definition of the norm, and the fact that (A|\u03c8i)\u2020 = h\u03c8|A\u2020\nand similarly (B|\u03c8i)\u2020 = h\u03c8|B \u2020 . The last step is nothing but the good old Cauchy-Schwarz\n\n\f129\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\ninequality, reviewed earlier in this chapter, applied on the states A|\u03c8i and B|\u03c8i. First,\nlook at the left hand side of (8.34). If we restrict A and B to be observables, they have to\nbe hermitean, so A\u2020 = A and B \u2020 = B. This means the left hand side is equal to\nkA|\u03c8ik2 kB|\u03c8ik2 = h\u03c8|AA|\u03c8ih\u03c8|BB|\u03c8i = hA2 i\u03c8 hB 2 i\u03c8 .\n\n(8.35)\n\nNow the right hand side of (8.34). This is the norm squared of the complex number\nh\u03c8|AB|\u03c8i. Since the norm of a complex number is alway greater than the square of its\nimaginary part, we have\n|h\u03c8|AB|\u03c8i|2 \u2265\n\n1\nh\u03c8|AB \u2212 BA|\u03c8i\n2i\n\n2\n\n(8.36)\n\nIf we now plug in the above two equations in (8.34), we get:\n1\n|h[A, B]i\u03c8 |2\n(8.37)\n4\nwhich is already very close to the general uncertainty relation. For the last step, note that\nthe above equation is true for all operators A, B, and all states |\u03c8i. In particular, it will\nalso be true for the operators \u00c3 = A \u2212 hAi\u03c8 and B\u0303 = B \u2212 hBi\u03c8 . Since h\u00c32 i = (\u2206A)2 ,\nhB\u0303 2 i = (\u2206B)2 and [\u00c3, B\u0303] = [A, B], application of the above equation to \u00c3 and B\u0303 gives\nhA2 i\u03c8 hB 2 i\u03c8 \u2265\n\n1\n(\u2206A)2\u03c8 (\u2206B)2\u03c8 \u2265 |h[A, B]i\u03c8 |2\n4\nor, upon taking square roots, and dropping subscripts\n\n(8.38)\n\n1\n|h[A, B]i|\n(8.39)\n2\nThis is the general form of the uncertainty relation. It holds for any pair of observables A\nand B. For the operators X and P in particular, you can check that it yields the result\n\u2206A\u2206B \u2265\n\n~\n(8.40)\n2\nAlso, note that (as we suggested above) there may be some simultaneous eigenstates\nstates of noncommuting observables. Indeed, this is possible if this particular state is\nan eigenstate with value 0 of the operator [A, B]. In that case, the uncertainty principle\nimposes no conditions on the uncertainties of A and B for that state.\n\u2206X\u2206P \u2265\n\n8.3.2\n\nApplications\n\nApplication: uncertainty principle for other observables\nAnother meaningful uncertainty relation can be stated in three dimensions for any pair of\nthe angular momentum operators. For example the uncertainties of Jx and Jy are related\nby:\n1\n1\n(8.41)\n\u2206Jx \u2206Jy \u2265 |h[Jx , Jy ]i| = |Jz |\n2\n2\n\n\fCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\n130\n\nAnother variant is obtained as following. The observables X and P are in many ways\nrelated. This is why they are sometimes called 'conjugate quantities'. Another pair of\nquantities that display such a 'paired' behavior in many situations are time and energy.\nThis inspired some physicists to write\n\u2206E\u2206t \u2265\n\n~\n2\n\n(8.42)\n\nThis relation is a bit more subtle though, as the quantity time (t) is not an observable, it\nis a parameter. Hence, the above equation does not make immediate sense. However, in\nthe context of particle physics and decay processes it turns out that a similar relation does\nactually hold, in the following way. If a system decays from one state to another, then\nthe typical time after which this decay occurs and the uncertainty of the energy released\nare indeed related by the above inequality. To understand this better: an unstable state\ntypically does not have a very sharply defined energy, but rather a small range of energies.\nThis means the energy released (energy of the initial unstable state minus the final energy)\nis not always the same, and has a small uncertainty (\u2206E), called the natural linewidth.\nIt turns out that the typical time of existence of an unstable state (denoted \u2206t) and the\nlinewidth (\u2206E) are indeed related by the above formula. This means the more unstable a\nstate is (if it decays faster) the bigger the linewidth is. Conclusion: the above uncertainty\nrelation has some experimental meaning, but you should be a bit careful with the meaning\nof \u2206E and \u2206t there. (Note that it is not just another application of the general uncertainty\nrelation: time (t) is not an observable, it's a parameter!)\nApplication: exotic atoms.\nWith the uncertainty principle in our hands, we can understand the structure of atoms a\nbit better. Think again of the hydrogen system. The Hamiltonian was\nH = \u2212V (r) +\n\nke2\n~2\nP2\n=\u2212\n\u2212\n\u2206\n2m\nr\n2m\n\n(8.43)\n\nImagine a wave packet needs to be put in the lowest energy state possible. You know\nthe outcome of this problem, but let us ignore this for a moment. If a packet wants to\nhave a low potential energy (first term of the Hamiltonian), it needs to be well localized\naround the nucleus (around r = 0). Such a packet will have a rather small \u2206X. However,\nfrom the uncertainty principle, we know this implies \u2206P is large. This means that -when\nviewed in the momentum perspective- the wave packet necessarily contains components\nof large momentum P . (Otherwise it can't have a large momentum variance.) This in\nturn is disadvantageous for the total energy, as it blows up the kinetic part P 2 /2m. It\nis precisely a balancing between the two extremes that gives an atom a finite size: the\noptimal way in between. A question one can ask, is the following: what if the electron was\nheavier? What would this change in the story? Well. As the mass of a particle is standing\nin the denominator of the kinetic term, a larger mass means the relative cost of kinetic\n\n\f131\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\ne\u03bcp+\n\np+\n\nFigure 8.3: A muonic hydrogen atom is almost identical to an ordinary hydrogen atom.\nThe only difference is that the electron is replaced by it's heavier brother, the muon. This\nparticle sits in much smaller wave functions, and is much stronger bound to the nucleus.\n\nenergy is getting lower - at least if one fixes the total momentum.1 So taking the same\nwave function (hence the same P 2 ) but for a particle with a higher mass m will have less\nkinetic energy P 2 /2m. In such a case, the particle might just as well sit a bit closer to the\nnucleus, lowering its potential energy, while not driving up his kinetic energy too much.\nBased on this reasoning, one expects a 'heavier electron' (if such existed) to sit closer to\nthe nucleus, and hence to be better bound. Recalling the expression for the Bohr radius\nand the binding energy of the lowest energy state (n = 1),\nE(n = 1) = \u2212E0\na =\n\nme k2 e4\n1\n=\n\u2212E\n=\n\u2212\n0\n12\n2~2\n\n~2\nme ke2\n\n(8.44)\n(8.45)\n\nwe see that the binding energy is indeed proportional to me , and the Bohr radius a is\ninversely proportional to it. This confirms our reasoning.\nYou may wonder if all this philosophizing is of any use. After all, the electron mass is\njust a fixed number, there is no way of changing it. Well, here is a reason why we would\nactually like to think about it. Apart from the particles you already know (electron,\nproton, neutron and photon) there are quite some other, more rare particles. They are\ngenerally short lived (getting created in some very special processes like particle collisions\nand nuclear decay) and rapidly decay to the more familiar particles. An example is the\nmuon. It lives for about 2 microseconds, weighs 200 times as much as the electron, but\nhas all other characteristics identical to the electron. It is actually possible to bind such\na muon to a nucleus (during its short lifetime) forming a so-called muonic atom. The\n1\n\nThis is just the same statement as the fact that in classical mechanics, the kinetic energy p2 /2m\ndecreases with mass - if one fixes the momentum. Of course this is something very different from keeping\nthe velocity fixed while decreasing the mass. In that case, the kinetic energy mv 2 /2 decreases, as expected.\n\n\f132\n\nCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\np+\n\n(a)\n\n(b)\n\n(c)\n\nFigure 8.4: The H2+ molecule. (a): Two nuclei far apart (no lowering of the energy). (b):\nTwo nuclei close together (strong repulsion). (c): Intermediate distance: lowest energy\nstate (bound system).\n\nenergy-levels and atomic size are quite well described by the above two formula, if one at\nleast puts in a mass 200 times the electron mass. This confirms both the above reasoning,\nand that the muon nicely obeys the Schr\u00f6dinger equation, like all other particles!\nApplication: what the universe is made of.\nWith a reasoning similar to the one above, we can also understand the nature of chemical\nbonds a bit better. Try to think of the simplest molecule: H2+ . This consists of two\nhydrogen nuclei, and one electron 'circling around' and binding the system together. How\ndoes the electron manage to keep the system together, and how does the system get its\nsize? If you look at Figure 8.4 you see that there are three players in the game. First,\nthere is the attraction of the electron to both nuclei. Second, there is the mutual repulsion\nbetween the two nuclei. Last, there is the uncertainty principle, or better: the electron\nwants to minimize its kinetic energy. If the two nuclei are too close together, their mutual\nrepulsion will drive them apart. If they are far away of each other, the electron might just\nas well sit on one of the nuclei only, and the system breaks apart. However, if the two nuclei\nare at an intermediate distance, something interesting happens. On the first place, the\nelectron can orbit around both nuclei, getting a low potential energy. On the second place,\nas the wave function of the electron is now larger, \u2206X is big, and the uncertainty principle\nallows a very small \u2206P . In other words: the electron can take on a very narrow range of\nlow momenta, thus having only a very small kinetic energy. By 'taking on low momenta',\nwe mean that \u03c8(p) is only nonzero for very small p, which necessarily corresponds to small\n\u2206p. This system obviously has a lower energy than any other option, meaning it forms\na genuine bound state. This is what keeps the molecule together. The very same effects\nare responsible for the structure of any other molecule, even if more electrons and more\nnuclei are involved. A rule of thumb is that electrons like to occupy a lot of space, taking\non low momenta, while combining this with a wave function close enough to the nuclei.\nThis is precisely what a chemical bond is. These bonds make molecules to what they are,\nand hence give structure to what most of the (known) universe is made of.\n\n\fCHAPTER 8. THE UNCERTAINTY PRINCIPLE\n\n133\n\nExercises\n1. Explain to someone what the uncertainty principle is. What are the implications for\nmeasurements? How is it related to commutativity?\n2. By using linearity, show that Var(x) = hX 2 i \u2212 hXi2 . Since the variance is a positive\nquantity (it's the expectation value of a square) this implies hX 2 i > hXi2 . Check\nthis property for the wave function \u03c8(x) which equals one on the interval [\u22121/2, 1/2]\nand equals zero outside that interval.\n3. For a particle in a box, L gives a vague idea of the position uncertainty. How large do\nyou estimate the momentum uncertainty to be then? What is the (classical) energy\nof a particle with a momentum lying somewhere around that value? Compare this\nto the expression for the energy level that we found. (Just the form, not the exact\nprefactors - this exercise is only a vague estimate.)\n4. A team experimentalists can prepare atoms in an excited state. These atoms decay\nto a stable state, with a lifetime of 10\u22128 s. What is the uncertainty of the energy of\nthe photons produced in their experiment?\n5. In the last exercise of the previous chapter, you computed the Fourier transform\nof the rectangular wave functions \u03c8n (x). (You should have gotten something proportional to a sinc function.) Estimate roughly the variance of both \u03c8n and its\ntransform (i.e. give a number characterizing the magnitude of this variance) and\nverify that again \u2206X\u2206P \u223c ~.\n6. Compute \u2206X and \u2206P for the first excited level of the harmonic oscillator. (Hint:\nexpress X and P in terms of creation and annihilation operators. Be careful with\nprefactors like in (4.28) and (4.29)!) Do these satisfy the uncertainty principle?\nWhat about the ground state? (Look at its wave function; it has a special form!)\n\n\fPart III\n\nOther Applications\n\n134\n\n\fChapter 9\n\nReflection and transmission of\nparticles\nIn this chapter...\nThe chapters of this Part 3 will study different kinds of systems then what we have met so\nfar. If you think about it, all the examples we have done considered particles which were\ntrapped in a potential. In order of appearance: a box-potential, an oscillator-potential,\nand the Coulomb potential of the hydrogen atom. If a particle is not trapped/bound in a\npotential, we call it a free particle. This chapter will precisely deal with such free particles.\nWe will see that -just like trapped particles- they have some special and unexpected\nproperties. One of them is the phenomenon of tunnelling, where a particle can move trough\na 'wall' (potential barrier) which according to classical mechanics would be impenetrable.\nIn the first part, we prepare ourselves a bit: we study the SE in potentials which\nconsist of several constant regions and we introduce the notion of probability current.\nThen, we explain the phenomenon of tunnelling and why it is relevant in nature. Finally,\nwe compute the probability for a particle to actually tunnel though a potential barrier.\n\n135\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n9.1\n\n136\n\nParticles in constant potentials\n\nFor this chapter, we will study particles in potentials, which are composed of several\nregions in which the potential is constant. To understand the behavior of such systems,\nwe need to solve the SE in each part separately, and then glue together the solutions.\nHere, we will tackle the first part of that problem: solving the SE in a region where the\npotential is constant.\nSo imagine we a particle, with total energy E, traveling through a region where the\npotential is constant: V (x) = V . Because the particle has energy E, we have\nH|\u03c8i = E |\u03c8i\n\n(9.1)\n\nwith \u03c8(x) the wave function of the state |\u03c8i. This means that\nH\u03c8(x) = \u2212\n\n~2 \u2202 2 \u03c8(x)\n+ V \u03c8(x) = E\u03c8(x).\n2m \u2202x2\n\n(9.2)\n\nso\n\n2m(E \u2212 V )\n\u2202 2 \u03c8(x)\n=\u2212\n\u03c8(x).\n(9.3)\n2\n\u2202x\n~2\nThere are two possibilities. Either the value of the potential is lower than the total energy\nof the particle, or it is higher. We treat those cases separately.\nCase 1: V < E\nIn this case, E \u2212 V is positive, and we define a real and positive parameter k:\nk2 =\n\n2m(E \u2212 V )\n~2\n\n(9.4)\n\nWith this definition equation (9.3) becomes:\n\u2202 2 \u03c8(x)\n= \u2212k2 \u03c8(x)\n\u2202x2\n\n(9.5)\n\nThis is a differential equation that we have met before. We can write the solution as a\nsum of a sine and a cosine, or in terms of complex exponentials. Here we will choose for\nthe latter, so we write the general solution of the above equation as\n\u03c8(x) = Aeikx + Be\u2212ikx\n\n(9.6)\n\nwith A and B complex numbers. (If you use eix = cos x + i sin x, the above expression can\nbe turned into a sum of a sine and a cosine again, be it with different coefficients.) This\nmeans the full, time dependent solution is given by:\n\u03c8(x, t) = \u03c8(x)e\u2212iEt/~ = Aeikx\u2212i\u03c9t + Be\u2212ikx\u2212i\u03c9t\n\n(9.7)\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n137\n\nwith \u03c9 = E/~. This is just the sum of two waves, one traveling to the left, and the other\ntraveling to the right. Using\nkx \u2212 \u03c9t = c \u21d2 x = (c/k) + (\u03c9/k)t\n\n(9.8)\n\nwe see that the wave of amplitude A is traveling to the right, with speed v = \u03c9/k, and\nthe wave with amplitude B is traveling to the left, with speed v = \u2212\u03c9/k.\nThe interpretation of this should be quite clear: if a particle has enough energy to be\nin a certain region (so E > V ), then the solutions are just waves. By superposition of\nsuch waves, we can build wave packets traveling around in such a region.\nCase 2: V > E\nHere, it is V \u2212 E which is positive, so we define a real and positive parameter \u03bc as follows:\n2m(V \u2212 E)\n.\n~2\n\n(9.9)\n\n\u2202 2 \u03c8(x)\n= \u03bc2 \u03c8(x)\n\u2202x2\n\n(9.10)\n\n\u03c8(x) = Ae\u03bcx + Be\u2212\u03bcx\n\n(9.11)\n\n\u03bc2 =\nEquation (9.3) now becomes:\n\nThe solutions are now:\nwith A and B again complex numbers. These solutions are not ordinary waves anymore,\nbut represent wave functions that are either damped or blowing up for large x. This\nmeans something special is happening - we will say more about this later. Anyhow, you\nmight have expected something special to happen: if in a certain region V > E, it means\nthat the particle does not have enough energy to be at that place - according to classical\nmechanics. So it is not strange that we do not find any ordinary wave solutions here\nanymore. This behavior will be explained later in this chapter.\n\n9.1.1\n\nProbability current\n\nAnother thing we will use in this chapter is the notion of probability current. Typically,\nin a system where there is a density \u03c1 in play (energy density, particle density, ...) people\nlike to write an equation of the following form:\n\u2202\u03c1\n~ * ~j = \u2212(\u2202x jx + \u2202y jy + \u2202z jz )\n= \u2212\u2207\n\u2202t\n\n(9.12)\n\nThis is called the continuity equation. In just one dimension, it becomes:\n\u2202\u03c1\n= \u2212\u2202x jx\n\u2202t\n\n(9.13)\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n138\n\nR\u221e\nThis can easily be shown to imply that the total quantity Q = \u2212\u221e \u03c1(x) dx is conserved.\nBy conserved, we mean that it does not grow or shrink in the course of time: \u2202t Q =\n0. (Showing this needs the assumption jx (+\u221e) = jx (\u2212\u221e) = 0.) The object j(x) is\nthen called the conserved current associated to Rthe density \u03c1. For example: if \u03c1E (x)\n\u221e\nrepresents an energy density, and the total energy \u2212\u221e \u03c1E (x) dx of a system is conserved\n(does not grow or shrink with time) then there will typically be a conserved current\njE (x) such that together with \u03c1E it satisfies the continuity\nR \u221e equation. Similarly, if \u03c1p (x)\nis a particle density, and the total number of particles \u2212\u221e \u03c1p (x) dx is conserved, then\nthere typically is some conserved current jp (x) in play. In quantum mechanics, there is a\n2\ncontinuity equation as well. Recall\nof a probability density,\nR \u221e that |\u03c8(x)|R \u221ehas the meaning\n2\nand since the total probability \u2212\u221e dx \u03c1(x) = \u2212\u221e dx|\u03c8(x)| is constant (it equals one for\nany time t) it is conserved. This means we can probably set up a continuity equation for\nthe probability density. Let us try to find it. By the product rule of derivatives,\n\u2202\u03c8 \u2217\n\u2202\u03c8 \u2217\n\u2202\n|\u03c8|2 =\n\u03c8+\n\u03c8\n\u2202t\n\u2202t\n\u2202t\n\n(9.14)\n\nTo proceed, let us consider a freely moving particle. This means we consider V (x) = 0.\nIn that case, the SE tells us that\ni~\n\u2202\u03c8\n=\n\u2206\u03c8\n(9.15)\n\u2202t\n2m\nand by taking the complex conjugate\n\u2202\u03c8 \u2217\n\u2212i~\n=\n\u2206\u03c8 \u2217 .\n\u2202t\n2m\n\n(9.16)\n\nIf this step is not obvious, try to write out \u03c8 as a sum of its real and imaginary part, write\nout the derivatives, and then apply complex conjugation to arrive at the above expression.\nPlugging these two equations into (9.14), we get\n\u2202\n|\u03c8|2 =\n\u2202t\n=\n\ni~ \u2217\n(\u03c8 \u2206\u03c8 \u2212 \u03c8\u2206\u03c8 \u2217 )\n2m\ni~ ~ \u2217 ~\n~ \u2217)\n\u2207(\u03c8 \u2207\u03c8 \u2212 \u03c8 \u2207\u03c8\n2m\n\n(9.17)\n(9.18)\n\nP\nIf the second step is not clear, write out the derivatives in components, like \u2206\u03c8 = i \u2202i2 \u03c8\nP\n2\n~ v) =\nor \u2207(~\ni \u2202i vi . Ah! The above equation is a continuity equation indeed, for \u03c1 = |\u03c8|\nand\n~ \u2212 \u03c8 \u2207\u03c8\n~ \u2217 ) = ~ Im(\u03c8 \u2217 \u2207\u03c8)\n~\n~j = i~ (\u03c8 \u2217 \u2207\u03c8\n(9.19)\n2m\nm\nThe interpretation of ~j is then as follows: if a wave packet is moving from one place to the\nother, the probability is flowing from this one region to the other. The size of the flow at\neach position x is precisely given by ~j(x).\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n9.2\n\n139\n\nTunnelling and nuclear decay\n\nIn this story, we will fresh up our knowledge about the nucleus. You know it consists of\nprotons and neutrons. You also know that these are bound together very tightly. You\nmay wonder how this is possible: after all, the protons are all positively charged, so they\nrepel each other. The secret (which you may or may not have heard of) is the so-called\nstrong force. This is a force between particles, just like electromagnetism and gravitation,\nbut with some special features. First of all: it has a very short range. As you know, the\ninfluence of gravitation and electromagnetism can work on very large distances. Arbitrary\nlarge, in fact. Both forces have a 1/r 2 behavior - which gets small for large r but never\nvanishes. However, for the strong force, this is not true. It can only work between particles\nthat are very close together, at a distance of about 10\u221215 m, approximately the size of the\nnucleus of an atom. Another special feature is that this force is always attractive, and\nonly felt by protons and neutrons. That means electrons do not feel this force at all.\nThis is a bit similar to the fact that the electric force is not felt by the neutron (because\nit is neutral). Lastly, this force is very strong - or better: within its small range, it\ndominates the electromagnetic force by far. This in turn is a bit similar to the fact that\nthe electromagnetic force between individual particles is much much larger than their\nmutual gravitational force - that's why we can safely ignore gravitation when dealing with\nsmall systems. To summarize:\nForce:\nStrong\nElectromagnetic\nGravitational\n\nRange:\n\u223c 10\u221215 m\n\u221e\n\u221e\n\nStrength:\nvery strong\nintermediate\nvery weak\n\nAffected particles:\np+ , n0\ncharged particles\nall particles\n\nWith this information, you can understand better why a nucleus sticks together. If\nneutrons and protons are very very close together, they are within the range of eachs\nothers attractive strong force. This force is much larger than the electrostatic repulsion\nbetween the protons, so the nucleons stick together tightly and well-bound. This way,\nthey form a nucleus, and the size of that nucleus is of the same order as the range of the\nstrong force.\nThis sounds very cosy, but it turns out that if there are too many nucleons sitting\ntogether, the nucleus gets crowded, and some of the nucleons might want to get out.\nDividing the nucleons over two nuclei might be energetically more advantageous than\npacking them all together in a single crowded one. This is precisely what is known as\nradioactivity. Typically, elements with too many nucleons - such as uranium, for example\n- have the tendency to fall apart after a while.1 By falling apart, they release some energy:\nthe energy of the initial state minus the energy of the final state. So in a large bunch of\natoms of an unstable kind, there are all the time decays occuring. Each such decay releases\n1\n\nAlso some smaller nuclei can be unstable, but this is for other reasons.\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n140\n\na bit of energy. It is this ongoing energy emission - in the form of radiation - what signals\nthat a chemical element is radioactive.\nHopefully you now have a better understanding of what the nucleus is. However,\nthere is one logical gap in the explanation. We have said that the strong force (within its\nrange) is always bigger than the electrostatic repulsion. So it seems that once the protons\nand neutrons are locked up in a nucleus, they can never get out. Not even when the end\nproduct has a lower energy and thus is favorable. From this we would incorrectly conclude\nthat there are no unstable nuclei. How can we understand this better? To answer this\nquestion, look at Figure 9.1. There, the total potential (of the electrostatic force + the\nstrong force) is shown. If a nucleon was a point particle, it would just tend to move hence\nand forth in the potential well. Every time it reaches the boundary of the nucleus, it slows\ndown, stops and moves back. So in this description, a single particle (or a cluster of them)\ncan never escape, and we can not explain radioactivity.\nHowever, quantum mechanics tells us this description is wrong. We know that the\nprotons and neutrons (just like electrons) are each described by a wave function, which\nhas a nonzero size. Now the result of such a wave packet hitting a potential barrier is very\ninteresting. There is always a small part of the wave function that crosses the barrier, even\nif the energy of the wave packet is much lower than the height of the potential barrier.\nThis is the so-called tunnelling effect. It is shown in Figure 9.2. We will study this\neffect quantitatively at the end of this chapter, and give an expression for the amount of\nthe wave packet that is let through the barrier. For now, knowing this effect is enough\nto understand the puzzle here. Indeed, the tunnel effect implies there is always a small\nchance for a a particle to cross a barrier, and hence for one or more nucleons of an unstable\natom to leave the nucleus. This means nuclei can decay (at least: if the end result has a\nlower energy) even if the barrier to escape the nucleus is classically impenetrable.\nThis quantum effect was first studied by George Gamow, who used the tunnel effect to\ndescribe the process of alpha decay. In alpha decay, two neutrons and two protons leave\nthe nucleus at once. These four particles are exactly the same as a helium nucleus, and\nsuch a cluster is also called an 'alpha particle'. Using the tunnelling probability formula\n(which we will derive shortly) Gamow managed to explain a relation between the emitted\nenergy in alpha decay process and the decay time. The decay time is the typical time\nan unstable atom lives before decaying and depends on the specific isotope. This relation\nmatches very well with experiments, confirming the quantum mechanical computation.\nBefore we go to the calculations, two small remarks on the explanation above. First,\nyou may wonder why we can describe the nucleus as a point when we are dealing with\nelectrons in an atom. Indeed, in the hydrogen case we just took the nucleus to be a point\ncharge. The above explanation suggests we should describe all particles in the nucleus\nby their wave function, not as a single point. This would obviously make the problem of\nstudying electrons in an atom much harder. What saves the day is of course the strength of\nthe strong force, which squeezes the wave functions of nucleons together very tightly. This\nmakes the nucleus look extremely small - much smaller than the atom itself. A popular\ncomparison is that is the nucleus sitting in the atom is as small as an ant sitting in a\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n141\n\nV\nVp, nucl\nVn, nucl\n\n0\n\nr\nEp , En\n\nFigure 9.1: The total nuclear potential for a proton and a neutron. Both feel the attractive\nstrong force. This creates the deep pit around the origin. On top of that, the proton\nexperiences the repulsive coulomb potential. For both particles the typical energy lies\nbelow the height of the barrier, meaning they can not escape.\n\nlarge football stadium.2 That is why we can safely treat it as a single point when we are\ndescribing the electrons. A second pitfall we want to prevent is a too literal interpretation\nof Figure 9.2 in the context of the nucleus. Just like the wave functions in a box, the wave\nfunctions of nucleons stretch out over the entire nucleus. They are not just small wave\npackets bouncing around, so Figure 9.2 gives a bit a simplistic image of this situation. For\ncases where a free particle hits a potential barrier, this figure is precisely what happens,\nand it will be this situation which we will describe below. So the two situations (free\nparticle hitting a potential barrier and an alpha particle leaving the nucleus) both involve\nthe same tunnelling effect - although the situations are not strictly identical. Got it?\n\n9.3\n\nParticles and barriers\n\nWe will study two problems in this part. The first one considers a particle, traveling\nfreely, until at some point in space it encounters a sudden potential step. We will ask\nwhat happens to the particle. If it travels onwards (into the region where V is higher), we\nsay the particle is transmitted. When it is sent back to where it came from, we say that\n2\n\nThe size of the total atom is of course determined by the approximate size of the cloud of electrons.\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\nV(x)\n\n142\n\nV(x)\n\nx\nV(x)\n\nx\nV(x)\n\nx\n\nx\n\nFigure 9.2: When a classical (point) particle hits a potential higher than its energy, it is\nreflected. When a wave function hits a barrier, there is always a part which is transmitted.\nThis implies there is a nonzero chance for the particle to go through the barrier.\n\nthe particle has been reflected. We want to find out the relative probabilities for those\ntwo options. The other problem deals with a potential barrier: a narrow but high bump\nin the potential. Classically, the particle will be reflected by the bump if it is higher than\nthe particles' energy, but according to quantum mechanics, there will always be a nonzero\nchance for the particle to boldly cross the barrier. This is precisely the tunnel effect we\nwere talking about before.\n\n9.3.1\n\nThe potential step\n\nThis describes a particle traveling in a potential that has a step at some place, say at\nx = 0:\n\u001a\n0 for x < 0\nV (x) =\nV for x \u2265 0\nAlso, let us take the energy E of the particle to be larger than V . We then know from the\nfirst part of the chapter, that the solution of the SE in each part of the potential is given\nby ordinary waves:\n\u001a\n\u03c81 (x) = A1 eik1 x + B1 e\u2212ik1 x for x < 0\n\u03c8(x) =\n\u03c82 (x) = A2 eik2 x + B2 e\u2212ik2 x for x \u2265 0\n\np\n\u221a\nwith k1 = 2mE/~ and k2 = 2m(E \u2212 V )/~. The time evolution of this solution is just\ngiven by multiplying the above wave functions by e\u2212iEt . To have a healthy solution we\nneed to glue together the part \u03c81 on the left to the part \u03c82 on the right. In particular, to\nhave a descent wave function we need the function \u03c8(x) to be continuous at x = 0. Also,\nto ensure the wave function does not have a strange kink, we will demand the derivative\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n143\n\nV(x)\nV\n\nx\nFigure 9.3: The potential step. When a particle with energy E > V hits this barrier,\nthere is a chance R to be reflected and a chance T to be transmitted. This contrasts with\nthe description of particles as points. There a particle will always be transmitted when\nE >V.\n\nof \u03c8 to be continuous at x = 0 as well. These requirements give the following system of\nequations:\n\u03c81 (0) = \u03c82 (0) \u21d2 A1 + B1 = A2 + B2\n\u03c81\u2032 (0)\n\n=\n\n\u03c82\u2032 (0)\n\n\u21d2 k1 (A1 \u2212 B1 ) = k2 (A2 \u2212 B2 )\n\n(9.20)\n(9.21)\n\nNow some more physical input. The solution (9.20) contains four parts: two waves in the\nregion x < 0 (one going to the left and one to the right), and similarly two waves in the\nregion x > 0. We want to describe a particle coming from the left, falling on the potential\nstep, and check how much of the initial wave is reflected and how much is transmitted.\nThis means that in the region x > 0 we only want to consider waves that are going to the\nright. Hence we will put\nB2 = 0.\n(9.22)\nAlso, we only want to study the ratios of the transmitted and reflected waves compared to\nthe incoming wave. This means we can just normalize with respect to the incoming wave,\nand put the amplitude of the incoming wave to one:\nA1 = 1.\n\n(9.23)\n\nThe conditions (9.21) then become\n1 + B1 = A2\nwhich has as solutions\nA2 =\n\nand\n\n2k1\n,\nk1 + k2\n\nk1 (1 \u2212 B1 ) = k2 A2\nB1 =\n\nk1 \u2212 k2\n.\nk1 + k2\n\n(9.24)\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n144\n\nWave packets\nIt should be clear that the coefficients A2 and B1 are closely related to the probabilities\nfor a particle to be transmitted or reflected by the barrier. If A2 is large, a large portion\nof an incident wave will be going into the region x > 0, which can be interpreted as a large\nchance for the particle to be transmitted.\nHowever, real particles are never an infinite wave, but a limited packet. (Recall that we\ndescribed an infinite wave as mere caricature of a wave packet of which the momentum is\nsharply peaked around some value.) But we know from the chapter on Fourier transforms\nthat such a packet can nevertheless be described by a superposition of infinite waves. So\nwe only need a small step to convert the above results into a statement about particles.\nSay we start out with a wave packet \u03c8(x) that is initially located in some region on the\nside x < 0, and that is is traveling to the right. In momentum representation, we can\nwrite\nZ \u221e\n1\ndk1 eik1 x \u03c8(k1 ).\n(9.25)\n\u03c8(x) = \u221a\n2\u03c0 \u2212\u221e\nIn this last equation, we the waves eikx in terms of the wavenumber, and not in terms\nof the momentum p = ~k. This means we do not have to include any extra factors ~\nanywhere: we can just use the original expression for a Fourier transform which we met\nin the first section of Chapter 8. The normalization condition phrased in terms of \u03c8(k1 )\nis\nZ \u221e\ndk1 |\u03c8(k1 )|2 = 1\n(9.26)\n\u2212\u221e\n\nThe previous section showed that a wave eik1 x which falls on the potential step is broken\nup in two parts:\neik1 x \u2192 B1 e\u2212ik1 x + A2 eik2 x\n(9.27)\n(We suppress the time evolution for elegance.) This is true for all k, so for the wave packet\n(9.25) in its entirety will break up into\nZ \u221e\nZ \u221e\n1\n1\ndk1 B1 e\u2212ik1 x \u03c8(k1 ) + \u221a\ndk1 A2 eik2 x \u03c8(k1 )\n(9.28)\n\u2192\u221a\n2\u03c0 \u2212\u221e\n2\u03c0 \u2212\u221e\nWhat we want to do, is to find the amplitudes of the two parts. First, let us consider the\nreflected part. Its total amplitude (=the norm squared) will be interpreted as the chance\nfor reflection:\nZ \u221e\nR=\ndk1 |B1 |2 |\u03c8(k1 )|2 .\n(9.29)\n\u2212\u221e\n\nSimilarly the total amplitude of the transmitted part can be interpreted as the chance for\nthe particle to go over the potential step. We compute the values of R and T below.\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n145\n\nFinding R and T\nWe start with the reflected part. To do the integral (9.29) note that B1 depends on k1 ,\nso it is not just a constant. However, things simplify if we assume that the initial wave\nfunction \u03c8(k1 ) is sharply peaked around a single value (say k\u03041 ) and zero at other places.\nThen the relevant range of integration in the above integral becomes a very small region\naround k\u03041 , and we can take B1 to be approximately constant: namely, just its value when\nevaluated for k\u03041 . Using this, and the normalization (9.26), we get\nZ \u221e\n2\nR = |B1 |\ndk1 |\u03c8(k1 )|2 = |B1 |2\n(9.30)\n\u2212\u221e\n\nAs said, it is understood that B1 is evaluated at the peak value k\u03041 of the incident wave\nfunction. Now let us consider the transmitted part, the last term in (9.28). Let us denote\nthat wave packet by \u03c8T (x). Because of the relation between k1 and k2\nk12 \u2212 k22 =\n\n2Em 2(E \u2212 U )m\n2U m\n\u2212\n=\n2\n2\n~\n~\n~2\n\nwe can change the integration variable to k2 , and get\nZ \u221e\nZ \u221e\n\u2202k1\nik2 x\ndk2\ndk1 A2 e\n\u03c8(k1 ) =\n\u03c8T (x) =\nA2 eik2 x \u03c8(k1 (k2 ))\n\u2202k2\n\u2212\u221e\n\u2212\u221e\n\n(9.31)\n\n(9.32)\n\nIn the object on the right hand side, the integration variable (k2 ) is the same object\nas what occurs in the exponentials - unlike the integral on the left. So the momentum\nrepresentation \u03c8T (k2 ) is to be read off from the integral on the right. Using (9.31) we can\nk2\n1\nsee that \u2202k\n\u2202k2 = k1 and we find\n\u03c8T (k2 ) =\n\nk2\nA2 \u03c8(k1 )\nk1\n\n(9.33)\n\nwhere k1 depends on k2 via (9.31). With this, the total amplitude of the transmitted part\nbecomes\nZ \u221e\nZ \u221e\n\u2202k2 k22\nk2\n2\n2\ndk1\n(9.34)\ndk2 22 |A2 |2 |\u03c8(k1 )|2 =\nT =\n2 |A2 | |\u03c8(k1 )|\n\u2202k\nk\nk\n1 1\n\u2212\u221e\n\u2212\u221e\n1\n\nNow again we use the fact that \u03c8(k1 ) is sharply peaked, so we can approximate the objects\ninside the integral by using in the value k\u03041 and the corresponding k\u03042 :\nZ \u221e\nk\u03041 k\u030422\nk\u03042\n2\nT =\n(9.35)\n|A2 |\ndk1 |\u03c8(k1 )|2 = |A2 |2 .\n2\nk\u03042 k\u03041\nk\u03041\n\u2212\u221e\n\nWhere A2 is to be evaluated at k\u03041 and k\u03042 . In conclusion, the chances for our incident\nparticle to be reflected or transmitted, are given by:\n\u00132\n\u0012\n4k\u03041 k\u03042\nk\u03042\nk\u03041 \u2212 k\u03042\n2\n.\n(9.36)\nand T = |A2 |2 =\nR = |B1 | =\n(k\u03041 + k\u03042 )2\nk\u03041 + k\u03042\nk\u03041\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n146\n\nGetting here was a bit nasty, but as a rather non-trivial check, you can verify that these\nprobabilities nicely add up to one:\nR + T = 1.\n(9.37)\nTwo more remarks to wrap this all up. First: you may wonder why we changed integration\nvariables twice when deriving T . Once from k1 to k2 and once from k2 to k1 . Is this really\nnecessary? Couldn't we just have left k1 there all the time? The answer is no. We had\nto switch to k2 to correctly read off the wave function \u03c8T (k2 ), since the waves in the\nintegrand were expressed in termsRof eik2 x and not eik1 x . The second change of variables\n\u221e\nwas there to pull out the integral \u2212\u221e dk1 |\u03c8(k1 )|2 = 1 again, like we did in the reflection\npart. A second comment is about time dependence. We have at no point written any of the\ne\u2212iEt/~ factors. Is that ok? Well, since we were considering waves with different k, these\nalso have different energies E. If the wave function \u03c8(k) is sharply peaked around one k,\nthe energies of the different components are very comparable, and the time dependence\nreally is just an irrelevant factor e\u2212iEt/~ showing up everywhere. So no, it does not change\nmuch if we include this factor or not, but yes, to be correct we should have. This would\nintroduce some phases that are present in the outgoing wave packets, but which of course\ndo not change the norms, and hence not the outcome. The main goal of this derivation\nwas not to include all the details, but we did manage to find expressions for R and T .\nAnd as we anticipated, they are just proportional to B1 and A2 .\nProbability currents\nThe above construction of wave packets has two faces. On one hand, it makes clear what\nis happening: a wave packet gets split and the amplitude of both parts precisely give the\nprobability for transmission or reflection to occur. On the other hand, it is a bit a long\nway round to get the end result. Is there a shorter way to obtain T and R? You bet. The\ntool we can use is the probability current we met in the beginning of this chapter. The\ncurrent on the left side (x < 0) and right side (x > 0) are:\n\u0012\n\u0013\n~k1\n~\n\u2217 \u2202\u03c81 (x)\nIm \u03c81 (x)\n(1 \u2212 |B1 |2 )\n(9.38)\n=\nj1 (x) =\nm\n\u2202x\nm\n\u0012\n\u0013\n~\n~k2\n\u2217 \u2202\u03c82 (x)\nj2 (x) =\nIm \u03c82 (x)\n|A2 |2\n(9.39)\n=\nm\n\u2202x\nm\n1\nThe current j1 consists of two parts. The positive piece j1+ (x) = ~k\nm corresponds to a\nprobability current moving to the right. This is the incoming wave. The negative piece\n2\n1\nj1\u2212 = ~k\nm |B1 | is the reflected part. The current j2 is just the probability current corresponding to the transmitted wave. Relative to the incoming current j1+ , the transmitted\n\n\f147\n\nCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\nand reflected currents are\nT\n\n=\n\nR =\n\nj2\n4k1 k2\nk2\n2\n+ = k |A2 | = (k + k )2 .\nj1\n1\n1\n2\n\u00132\n\u0012\n\u2212\nj1\nk1 \u2212 k2\n2\n+ = |B1 | =\nk\nj1\n1 + k2\n\n(9.40)\n(9.41)\n\nWhich confirms our previous results via a nice shortcut. Nevertheless, the interpretation\nof T and R remains most clear in the wave packet view. We just bare in mind that infinite\nwaves are not fully physical (not normalizable) - but the wave functions constructed from\nthem are.\nA last comment on the result of the computation. Note that (unlike what you would\nguess from classical mechanics) there is always some nonzero chance for reflection. So\neven if a particle can easily cross a potential step, there is a (small) chance for it to be\nbounced right back. There are several experimental situations where particles meet a\nsmall potential step, typically when particles are traveling from one type of material into\nanother one. Even if this step is small enough for all particles to pass (given their energy)\nthere are some which are reflected - in accordance to the above expression.\n\n9.3.2\n\nThe potential barrier\n\nWe now consider a potential barrier:\n\u001a\nV\nV (x) =\n\nfor \u2212 a < x < a\n0 everywhere else\n\nand a particle coming in from the left. The solution\n\uf8f1\n\uf8f2 \u03c81 = A1 eikx + B1 e\u2212ikx\n\u03c8(x) =\n\u03c8 = A2 e\u2212\u03bcx + B2 e\u03bcx\n\uf8f3 2\n\u03c83 = A3 eikx\n\nof the SE in the three regions is:\nfor\nfor\nfor\n\nx < \u2212a\n\u2212a<x<a\nx>a\n\n(9.42)\n\nwith\n\np\n\u221a\n2m(V \u2212 E)\n2mE\n\u03bc=\nk=\n~\n~\nDemanding continuity at x = \u2212a and x = a amounts to\n\n(9.43)\n\n\u03c81 (\u2212a) = \u03c82 (\u2212a) \u21d2 A1 e\u2212ika + B1 eika = A2 e\u03bca + B2 e\u2212\u03bca\n\u03c81\u2032 (\u2212a)\n\n=\n\n\u03c82\u2032 (\u2212a)\n\n\u2212ika\n\n\u21d2 ikA1 e\n\n\u2212\u03bca\n\n\u03c82 (a) = \u03c83 (a) \u21d2 A2 e\n\u03c82\u2032 (a)\n\n=\n\n\u03c83\u2032 (a)\n\n\u2212 ikB1 e\n\u03bca\n\n+ B2 e\n\n\u2212\u03bca\n\n\u21d2 \u2212\u03bcA2 e\n\nika\n\n\u03bca\n\n= \u2212\u03bcA2 e\nika\n\n= A3 e\n\u03bca\n\n+ \u03bcB2 e\n\nika\n\n= ikA3 e\n\n(9.44)\n\u2212\u03bca\n\n+ \u03bcB2 e\n\n(9.45)\n(9.46)\n(9.47)\n\nThis is a homogeneous system of 4 equations with 5 unknown parameters A1 , B1 , A2 , B2\nand A3 . We will put A1 = 1, just like we did for the potential step. This gives 4 equations\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n148\n\nand 4 unknown parameters. Solving for A3 yields\n|A3 |2 =\n\n(2k/\u03bc)2\n(1 \u2212 k2 /\u03bc2 )2 sinh (2\u03bca) + (2k/\u03bc)2 cosh2 (2\u03bca)\n2\n\n(9.48)\n\nThe currents on the left and right are now\n~k\n(1 \u2212 |B1 |2 ) = j1+ \u2212 j1\u2212\nm\n~k\n|A3 |2\nm\n\nj1 =\nj3 =\n\n(9.49)\n(9.50)\n\nso we see that |A3 |2 (= j1+ /j3 ) is the transmission probability. Striking enough, even if\nE > V (which is implicitly expressed by the reality of \u03bc) there is a nonzero chance for the\nparticle to cross the barrier. In the limit \u03bca >> 1 (broad barrier, large energy deficit) the\ntransmission probability (9.48) becomes\nT = |A3 |2 = 4e\u22124\u03bca\n\n\u0012\n\nk\u03bc\nk 2 + \u03bc2\n\n\u00132\n\n= 4e\u22124\u03bca\n\nE\nV\n\n\u0012\n\n1\u2212\n\nE\nV\n\n\u0013\n\n(9.51)\n\nso the transmission probability decreases exponentially with the barrier width a. The\nincoming, reflected, and transmitted probability currents are shown on the figure below:\n\nV(x)\nV\n\n-a\n\na\n\nx\n\nj1j1+\n\nj\n\n3\n\nFigure 9.4: The incoming, reflected, and transmitted probability currents at a potential\nbarrier. The amplitude of the transmitted current gives the probability for a particle to\ntunnel through the barrier - even if this is not allowed classically.\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n9.3.3\n\n149\n\nApplication: Scanning Tunnelling Microscope\n\nA nice application of the tunnel effect is the Scanning Tunnelling Microscope. This is an\nextremely high-resolution microscope, which relies on the tunnelling effect. The device\nconsists of a sharp 'scanning tip', which is put at an elevated electrical potential with\nrespect to the sample one wishes to study. By bringing the tip close to the sample,\nelectrons will tunnel through the empty space between the tip and the sample - even\nwhen the two are not touching. This way, a current arises. The size of the current tells\nsomething about the distance between the tip and the sample. This way, one can 'feel'\nthe shape of the sample, at a distance. (This distance is very small: the tip has to be\nextremely close the sample to realize a nonzero tunnelling current.) A modern STM has\nsuch a good resolution, that it can easily distinguish single atoms on the sample surface.\nPretty impressive!\n\nStearing mechanism\n\nStearing signal to tip\n\nTunneling\ncurrent amplifier\n\nDistance control\nData processing\n\nTip\n\nDisplay\n\nTunneling current\nTunneling voltage\n\nFigure 9.5: A Scanning tunnelling microscope. The amplitude of the tunnelling current\nallows to scan the surface in an extremely sensitive way. This allows to see objects as\nsmall as individual atoms.\n\n9.3.4\n\nApplication: It's warm outside!\n\nYou probably know that our beloved sun is a very large fusion reactor. At its extremely\nhigh temperature, hydrogen nuclei fuse to helium nuclei. These are more strongly bound,\nso there is an energy gain in the process. This energy is radiated away and eventually\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n150\n\nreached us on earth. If you use what you have learned in this chapter, you may see a\nproblem. The very barrier which prevents particles from the nucleus to escape is also\nthere when two nuclei try to merge. So classically, it is very hard (almost impossible)\nfor two hydrogen nuclei to fuse into a helium nucleus. Ah - this means that once again\ntunnelling plays a crucial role. Indeed, the tunnelling process gives rise a very small but\nnonzero fusion probability, just enough to make the sun into the bright shiny object we\nsee in the sky. So the next time you and your friends are enjoying a sunny day, make sure\nto mention that none of this would be possible without quantum tunnelling. Caution: for\nthe well-being of you social life you might better not say things like that out loud. Well,\nthere still is the sweet inner joy of insight, right?\n\n\fCHAPTER 9. REFLECTION AND TRANSMISSION OF PARTICLES\n\n151\n\nExercises\n1. An electron with energy 6 eV hits a 10\u221210 m long barrier of 8 eV high. What is the\nchance for the electron to go through it?\n2. The same question, but now the barrier is 0.01 m long. Do you have to be afraid that\nyour cup of tea spontaneously empties itself by tunnelling of particles (or molecules)\nthrough the sides?\n3. A proton with energy 2 keV suddenly travels into a region where the electric potential\nis 10 V higher. What would happen if a proton was a charged point particle? Using\nquantum mechanics, what is the chance for the proton to be reflected?\n4. Prove that the charge corresponding to a conserved current is indeed conserved in\ntime. Can you do the three dimensional case? What expression would you take for\nQ? Can you show it's conserved?\n5. Study a situation where a particle (coming from the left) drops of a potential step.\nSo V (x) = V on the left, and V = 0 on the right. Solve the SE, write boundary\nconditions and find R and T using probability conditions. Compare the result to\nwhat you expect classically.\n6. Challenge: try to solve the potential well: a particle stuck in a potential which has\nV (x) = \u2212V for \u2212L < x < L and zero elsewhere. Can you recover the particle in a\nbox by taking the appropriate limit?\n\n\fChapter 10\n\nSpin\nIn this chapter...\nBy now, you have become quite experienced in dealing with particles in potentials. As\na consequence, you also know how to deal with electric fields. Just write down the cor~ = F = q E),\n~ solve the Schr\u00f6dinger equation of a\nresponding potential (such that \u2212\u2207V\nparticle moving in this V , et voila. In fact, this is precisely what we did in the chapter on\nthe hydrogen atom. For a random electric field and the corresponding potential, solving\nthe SE may be a hard problem, but you can always do this in principle. You or your\ncomputer just need to be good enough at solving differential equations.\nThat is very nice, but it leaves an important question unanswered. What about magnetic fields? You know that there is no such thing as 'the magnetic potential' so the\ninfluence of magnetic fields can not be summarized by giving a potential V . So what\nabout those situations, in which a particle is immersed in a magnetic field? It is clear that\nthere will be some new input here. Indeed, we will meet a new and important concept:\nspin. This is a property of particles which not only determines their behavior in magnetic\nfields, but also in other situations. Reason enough to have a closer look!\n\n152\n\n\f153\n\nCHAPTER 10. SPIN\n\n10.1\n10.1.1\n\nTools\nMagnetic moment\n\nAs you very well know, a free electric charge will start to move when immersed in an electric\nfield. If the electric field is created by two distant source charges of opposite sign, the free\ncharged particle will move towards the source charge with opposite sign. Something nice\nhappens when you attach two small opposite charges to each other to form an electric\ndipole still immersed in the same electric field. They will each pull towards one side,\nand stay put by force balance. Hence, the only thing that happens is the alignment of\nthe electric dipole with the electric field. Now let us compare this with magnetism. In\nthat case there are no monopoles, that is: there are no isolated magnetic charges. But of\ncourse there are magnetic dipoles - which can be thought of as a little magnet. Just like\nthe electric dipoles, they tend to align with an external field, which is magnetic in this\ncase. So the energy for a magnetic dipole to be aligned with an external field is lower than\nwhen it is anti-aligned. If we denote the magnetic moment (=strength of the dipole) by\n~ , and the external field by B,\n~ the energy W of the dipole is\nM\n~ *B\n~\nW = \u2212M\n\n(10.1)\n\n~ k B,\n~ so that alignment is preferred. The situation is shown\nThis is indeed lowest when M\nin the figure below:\n\nFigure 10.1: An electric dipole aligns with the external E-field. Similarly, a magnetic\ndipole aligns with the external B-field. Notice that the electric dipole is conventionally\ndefined as a vector pointing from the negative to the positive particle - not to be confused\nwith the definition of the electric field, which points from positive to negative.\n\n\f154\n\nCHAPTER 10. SPIN\n\n10.1.2\n\nLarmor precession\n\nWe just refreshed the well-known fact that a magnetic dipole tends to align with an\nexternal magnetic field. This of course can only happen if the dipole can dispose of the\nenergy released by aligning. Otherwise, it will typically perform an oscillating movement,\nnot able to find its equilibrium value because of conservation of energy. Similarly, a ball\ncan only roll to the bottom of a pit and stay there because of the energy lost while rolling\ndown. If there was no friction, the ball would be doomed to roll in and out constantly. In\nthe following, we will show that a magnetic field indeed can keep a dipole in an oscillatory\nmotion, or more precise: a rotation. This phenomenon is called Larmor precession.\nTo be concrete, let us consider a small electric loop current - like a charge running in\na circle. As you know from electromagnetism, the circle movement of a charge creates a\n~ is the angular momentum of the circling charge, the induced\nmagnetic field: a dipole. If S\n~ is proportional to it:\nmagnetic moment M\n~ = \u03b3S\n~\nM\n\n(10.2)\n\nThe proportionality constant depends on the details of the loop current (which we do not\nconsider here) and is called the gyromagnetic ratio. As we just refreshed, a magnetic\ndipole feels a force trying to align it with the external B-field. This can be expressed as a\n~\ntorque, trying to turn the dipole in the direction of B:\n~ \u00d7B\n~\n~\u03c4 = M\n\n(10.3)\n\nThe direction of the torque is shown in Figure 10.2. Just like a force causes the momentum\nto change, the torque causes the angular momentum to change:\n~\u03c4 =\n\nd~\nd\n(~r \u00d7 ~p) = S\ndt\ndt\n\n(10.4)\n\n~ as before denotes the angular momentum ~r \u00d7~p of the orbiting charge. Combining\nWhere S\nthe last three expressions, we get\nd~\n~ \u00d7 B.\n~\nS = \u03b3S\ndt\n\n(10.5)\n\n~ = (0, 0, B) with B the\nTaking the magnetic field directed along the z-axis, we get B\nmagnitude of the field. Hence, the components of the above equation are:\nd\nSx = \u03b3(Sy Bz \u2212 Sz By ) = \u03b3Sy B\ndt\nd\nSy = \u03b3(Sz Bx \u2212 Sx Bz ) = \u2212\u03b3Sz B\ndt\nd\nSz = \u03b3(Sx By \u2212 Sy Bx ) = 0\ndt\n\n(10.6)\n(10.7)\n(10.8)\n\n\f155\n\nCHAPTER 10. SPIN\n\nB\n\nB\n\n\u03bc\n\u03bc\n\u03c4\n\n0\n\nFigure 10.2: Left: direction of the torque, exerted on a magnetic dipole in an external field.\n~ and ~\nBoth B\n\u03bc are in the plane of the paper, while ~\u03c4 is pointing directly towards you. Right:\nthe circular movement arising from the torque: Larmor precession. This movement is quite\nsimilar to that of a planet in its orbit. In that case the linear momentum is constantly\nchanging due to the force acting. For Larmor precession the angular momentum of the\ndipole is constantly changing due to the torque acting.\n\nThe last equation implies Sz is constant in time. The first two combine to\n\u2212\u03b3 2 B 2 Sx , which has solutions of the form\nSx = A cos(\u03c9t \u2212 \u03c6)\n\nd2 Sx\ndt2\n\n=\n\n(10.9)\n\nwith \u03c9 = |\u03b3|B. This number is known as the Larmor frequency. The total dynamics is\nthus given by\n~ = (A cos(\u03c9t \u2212 \u03c6), \u00b1A sin(\u03c9t \u2212 \u03c6), Sz )\nS\n(10.10)\n\nwith the sign of Sy determined by the sign of \u03b3. The total motion is shown in Figure 10.2.\n\n10.2\n10.2.1\n\nSpin\nZeeman effect\n\nIn the previous example we considered a magnetic dipole arising from an orbiting charge.\nWhere could we naturally meet such a system? Precisely, the electron in an atom! Indeed,\nit is charged and every state with l \u2265 1 has nonzero angular momentum:\nL2 |n, l, mi = ~2 l(l + 1)|n, l, mi\n\n(10.11)\n\nFrom this you might infer all states with l \u2265 1 have a magnetic dipole moment. For\na classical magnetic dipole that is caused by an angular moment, the motion is just\nthe Larmor precession described above. But what happens according to the full-fledged\nquantum mechanical description? First, write\n~ = \u03b3L\n~\nM\n\n(10.12)\n\n\f156\n\nCHAPTER 10. SPIN\n\nB=0\n\nB\u22600\nE\n|1>\n\n|1> , |2> , |3>\n\n|2>\n|3>\n\nFigure 10.3: Under the presence of a B-field, the degenerate energy levels of an electron\nare split into several distinct levels. This confirms that the electron's angular momentum\ncreates a magnetic moment that couples to the external B-field. However, this is not\nenough to explain the observed splitting. It seems that there is an extra contribution. This\nis due to the intrinsic angular momentum of the electron (the spin) and the corresponding\nintrinsic magnetic moment.\n\nwhere \u03b3 is the gyromagnetic ratio relating the angular momentum and the magnetic\n~ is the quantum mechanical operator\nmoment of the 'orbiting' electron. This makes M\n~ \u2020 = \u03b3\u2217L\n~ \u2020 = \u03b3L\n~ = M\n~ so that M\ncorresponding to the magnetic moment. Moreover, M\nis Hermitian - meaning it is an observable. As we noted before, the classical energy of a\n~ * B.\n~ This suggests that for the quantum description,\ndipole in a B-field is given by W = \u2212M\nwe have to add to our Hamiltonian a piece:\n~ *B\n~ = \u2212\u03b3 L\n~ *B\n~\nHL = \u2212M\n\n(10.13)\n\nSo that the total Hamiltonian for an atomic electron in a B-field (0, 0, B) becomes\n\u0013\n\u0012\n~2\n\u2206 + V \u2212 \u03b3BLz *\n(10.14)\nH = H0 + HL = \u2212\n2m\nThe electron states |n l mi will also be eigenstates of this new Hamiltonian, but with a\ndifferent energy. Indeed:\nH|n, l, mi = (H0 + HL )|n, l, mi = (En \u2212 \u03b3B~m)|n, l, mi\n\n(10.15)\n\nThis is a very interesting result. Without the B-field, all states with the same main\nquantum number n have the same energy En . However, in the presence of an external\nmagnetic field the total energy E = En \u2212 \u03b3B~m also depends on m. Hence, the energy\ndegeneracy is partly lifted: it now depends on n and also on m. Originally, it was Zeeman\nwho discovered this shift of the energy levels of atoms in the presence of a B-field. The\nZeeman effect for hydrogen partly agrees with the above reasoning: indeed, spectral lines\nsplit up due to the magnetic field. That's nice, but there is one big problem: the splitting\n\n\f157\n\nCHAPTER 10. SPIN\n\nis not just \u2212\u03b3B~m. It seems as if on top of the angular momentum of the electron(s),\nthere is an extra contribution to the magnetic moment, not due to the angular momentum\nof the wave function. After a long history of guesses and other experiments, physicists\nfound a resolution to this paradox: the electron must have an intrinsic angular moment\nand a corresponding intrinsic magnetic moment. This property is called the spin of\nthe electron. A way to visualize this spin, is shown in Figure 10.4. Imagine an electron\nwould be a small charged sphere, rapidly rotating around one axis. Then it would have\nan angular moment, and a magnetic moment due to the rotating charge of the sphere.\nThis picture is not really correct (an electron obviously is not just a charged sphere) but\nit gives a nice classical idea on how such an intrinsic angular momentum may arise.\n\nI\n\nFigure 10.4: A current in a loop creates a magnetic moment. Similarly, a rotating charged\nsphere would do so. This gives a pictorial idea on how the intrinsic magnetic moment of\nan electron might arise. Be careful: this picture is too simplistic. We will give a correct\nand more detailed description below.\n\n10.2.2\n\nStern-Gerlach experiment\n\nAnother important experiment in the discovery of the electron spin, was that of Stern and\nGerlach. They prepared vaporized silver atoms in a furnace, and fired these into a region\nwith an inhomogeneous magnetic field:\n~ = (0, 0, B(z))\nB\n\nwith \u2202z B(z) > 0\n\n(10.16)\n\n~ , the force exerted\nThe setup is shown in Figure 10.5. If the atom has magnetic moment M\non it by passing through the field is given by\n~ = \u2212\u2207W\n~\n~ M\n~ * B)\n~\nF\n= \u2207(\n\n(10.17)\n\nThe last expression only has a nonzero z-component, so the force acting on the atom is\ngiven by\nFz = Mz \u2202z B(z).\n(10.18)\nThere are several remarks in place. First, the force is only due to the gradient of the\nmagnetic field. Also note it is proportional to the z-component of the magnetic moment\n\n\f158\n\nCHAPTER 10. SPIN\ndetection\nmagnet\n\nfurnace\n\nB\n\nbeam of\n\nexpected\n(classical)\n\nsilver atoms\nmagnet\n\nobserved\n\nFigure 10.5: The Stern-Gerlach experiment. The B-field is directed along the z-axis, and\nalso increases in strength in that direction: \u2202z B(z) > 0. On the right, the result of the\nexperiment is shown: detection occurs at two central bright spots. This contrasts with\nthe classical expectation, which predicts detection along a wide and smeared out region.\n\nonly. From the part on Larmor precession, we know that the effect of a B-field on the\nmagnetic moment is rather simple: precession occurs in the x- and y- direction, but Mz\nstays constant. In conclusion: the atom is deflected by the field, in the direction of the\nforce Fz \u221d Mz . If Mz > 0, the atoms will be sent upwards, for Mz < 0 they bend off to\nlower z. Since the magnetic moment of the atoms can be oriented in any direction, we\nexpect Mz to take on a continuous region of values. This would imply the particles hit\nthe wall on the right of the detector along a continuous region. This is shown with the\ndashed line ('classical expectation') on the right in Figure 10.5.\nThe outcome\nAs shown in the figure, the outcome of the experiment is rather different. Instead of\nparticles impacting along a broad region of the screen, they only hit two well-localized\nspots. The location of the spots correspond to the following magnetic moments:\nMz = \u00b1\u03b3e\n\n~\n2\n\n(10.19)\n\nWith \u03b3e is a numerical constant. This is a pretty shocking result if you are a die-hard\nfan of classical physics. Just by sending a beam of ordinary silver atoms through some\nmagnetic field, it splits in two very sharp lines! Obviously, it's hard to explain this with\nclassical physics.\nCan we understand this result with the spin picture? The answer is yes. First, one can\nshow that the magnetic moment of a silver atom is due to only one of its electrons. All\nother electrons pair up in states with opposite spin, and also the total angular momentum\nof all the electrons in silver is zero. This will be explained better in next chapter, just\nbuy for now that the experiment really measures the intrinsic magnetic moment of one\n\n\f159\n\nCHAPTER 10. SPIN\n\nelectron only. If we define the constant \u03b3e to be the electron's (intrinsic) gyromagnetic\nratio, equation (10.19) readily implies that\nSz = \u00b1\n\n~\n2\n\n(10.20)\n\nHence, from the Stern-Gerlach experiment, we draw two conclusions\n\u2022 The size of the intrinsic angular momentum of the electron is equal to ~/2.\n\u2022 The direction of the spin is very restrained: it can only point upwards or downwardsnothing in between.\nLet us put this in a quantum mechanical style. The electron (ignoring the spatial dependence of the wave function) can be in two states: spin up and spin down, which we\ndenote as follows\n\u0012 \u0013\n\u0012 \u0013\n1\n0\nspin up \u2194\nspin down \u2194\n(10.21)\n0\n1\nWe also define the operator corresponding to the z-component of the intrinsic angular\nmomentum (the spin operator) to be\n\u0012\n\u0013\n~ 1 0\n(10.22)\nSz =\n2 0 \u22121\nThis operator acts on the spin up and spin down states by ordinary matrix multiplication.\nDenoting the spin up and spin down state by | \u2191i and | \u2193i, we have\n\u0012\n\u0012 \u0013\n\u0013\u0012 \u0013\n~ 1 0\n~\n~ 1\n1\nS| \u2191i =\n(10.23)\n= | \u2191i\n=\n0\n2 0 \u22121\n2 0\n2\nand similarly\nS| \u2193i =\n\n~\n2\n\n\u0012\n\n1 0\n0 \u22121\n\n\u0013\u0012\n\n0\n1\n\n\u0013\n\n=\u2212\n\n~\n2\n\n\u0012\n\n0\n1\n\n\u0013\n\n~\n= \u2212 | \u2191i\n2\n\n(10.24)\n\nFrom this we conclude that the spin up state | \u2191i has eigenvalue ~/2 under the operator\nSz , and the spin down state | \u2193i has eigenvalue \u2212~/2. This corresponds precisely to the\ntwo spots on the detection screen. When the electron is in the up state, it is deflected\nupwards by the Stern-Gerlach apparatus. When it is in the down state, it will hit the spot\nlower on the screen.\n\n\fCHAPTER 10. SPIN\n\n160\n\nFigure 10.6: Again relying on the naive depiction of an electron as a rotating sphere,\nwe can see the spin up and spin down states as two different rotations. Rotating in one\ndirection gives spin up, rotating in the opposite direction gives spin down. Again, we\nstress this is a simplistic visualization of the spin.\n\nSuperpositions\nAccording to the rules of quantum mechanics, all linear combinations of these two should\nbe physical states too. So any state of the form\n\u0012 \u0013\n\u0012 \u0013 \u0012\n\u0013\n1\n0\n\u03b1\n\u03b1\n+\u03b2\n=\n(10.25)\n0\n1\n\u03b2\nis a good state too, for any complex numbers \u03b1 and \u03b2. (If you require normalization, you\nneed |\u03b1|2 + |\u03b2|2 = 1.) What happens to such a state when it enters the apparatus? As we\nhave mentioned several times by now, the Schr\u00f6dinger equation (for whatever system of\nHamiltonian) is linear. Hence, if a state is the sum of two parts, each part\n\u0013 evolve in\n\u0012 will\n1\npart will\nits own way - as if the other was not there. From this we infer that the \u03b1\n0\n\u0012 \u0013\n0\ntravel upwards, and the \u03b2\nwill fly downwards. Hence the wave function will split\n1\nin two parts. The total amplitudes of the two parts of the wave function are given by |\u03b1|2\nand |\u03b2|2 . You know that the measuring device (etecting screen) will then report impact\non the upper spot with probability |\u03b1|2 , and impact on the lower spot with probability\n|\u03b2|2 . So actually, we should say that an electron can be:\n\u25e6 in the up state\n\u25e6 in the down state\n\u25e6 in a superposition\nHowever, for any of these possibilities there will only be one place at which detection\noccurs, so it always looks as if the electron was strictly up or down, even if the state\nentering the apparatus was a superposition. So just by adding the notion of spin to our\nquantum theory, we can successfully explain the stunning experiment of Stern and Gerlach.\n\n\f161\n\nCHAPTER 10. SPIN\nPlaying with spins\n\nThis clarifies the z-component of the spin, but what about the other components, x and y?\nWell, Stern-Gerlach apparatuses are like Lego pieces: you can put them together in many\nways, and have lots of fun. Consider drilling a hole in the detecting screen, at the spin\nup spot, and behind it placing another Stern-Gerlach apparatus, as in Figure 10.7. The\noutcome is not so surprising: the screen after the second apparatus will only detect spin\nups. Indeed: the hole served as some kind of a filter - letting through only the | \u2191i states.\nHence, at a second splitting, there will only be particles going upwards, as confirmed by\nexperiment. Now do something special. Keep the first one, but twist the second apparatus\nover an angle of 90\u25e6 , so that the B-field is in the x-direction. That way it selects states\nby Sx , the x-component of their intrinsic magnetic moment. The outcome is surprising:\nagain two spots are seen on the final screen: one with magnetic moment Sx = ~2 , and one\n\u0012 \u0013\n1\n~\nspot for Sx = \u2212 2 . How can this be? Somehow, the state\nof definite Sz does not\n0\nhave a definite Sx . It is a superposition of an spin up and spin down when viewed in the\nx-direction. A similar result holds when studying Sy . Well, let us now look at it from the\nquantum side. Both Sx and Sy are -like Sz - physical quantities: they can be measured.\nThey have to correspond to operators acting on the spin state. \u0012So, just\n\u0013 like Sz they have\n\u03b1\nto be represented by a two-by-two matrix acting on the states\n. So we need\n\u03b2\nSx =\n\n\u0012\n\na x bx\ncx dx\n\n\u0013\n\nand\n\nSy =\n\n\u0012\n\na y by\ncy dy\n\n\u0013\n\n(10.26)\n\nfor complex numbers ax , ... , dy . How can we guess the form of Sx and Sy ? Because the\noperators Sx and Sy are observables, they have to be Hermitian. Hence the corresponding\nmatrices need to be so too. So we need\n\u0012\n\u0013 \u0012 \u2217 \u2217 \u0013\na b\na c\n.\n(10.27)\n=\nb\u2217 d\u2217\nc d\nThis is a very restricting property. You can easily check that (up to linear combinations)\nthere are only 4 Hermitian 2-by-2 matrices:\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\n1 0\n0 1\n0 \u2212i\n1 0\n,\n,\n,\n.\n(10.28)\n0 1\n1 0\ni 0\n0 \u22121\nThe first one is trivial: the identity matrix. This not really an interesting operator. The\nlast one you should also recognize: it's our friend Sz modulo the factor ~/2 which has\nbeen stripped of. A fond guesser might then conjecture that\n\u0012\n\u0012\n\u0013\n\u0013\n~ 0 \u2212i\n~ 0 1\nand Sy =\n(10.29)\nSx =\ni 0\n2 1 0\n2\n\n\f162\n\nCHAPTER 10. SPIN\n\nsecond SG apparatus\nfirst SG apparatus\nonly spin up detected\n\nB\nB\n\nwall\n\nFigure 10.7: Playing with different SG apparatuses. Top: drilling a hole at the spin-up\nspot and placing another appartus after that. The outcome is as expected: only spin-ups\nare detected in the second device. Bottom: when placing a second, but rotated device,\nthings become more interesting. Again, two spots emerge, one at Sx = ~/2 and one at\nSx = \u2212~/2. This means that a state with spin up along the z-axis does not have a definite\nvalue for Sx .\n\nWell, this actually is a good guess. To convince you a bit, we will use the above guess to\nexplain the outcome in Figure 10.7. On the first place,\n\u0012 \u0013\n\u0012\n\u0012 \u0013\n\u0013\u0012 \u0013\n~ 0 1\n~ 0\n1\n1\nSx\n=\n(10.30)\n=\n0\n0\n2 1 0\n2 1\nso clearly\n\n\u0012\n\n1\n0\n\n\u0013\n\nis not and state with definite Sx . However, it can be broken in two parts\n\u0012\n\n1\n0\n\n\u0013\n\n=\n\n\u0012\n\n1/2\n1/2\n\n\u0013\n\n+\n\n\u0012\n\n1/2\n\u22121/2\n\n\u0013\n\n(10.31)\n\n\f163\n\nCHAPTER 10. SPIN\nAnd these parts are eigenstates of Sx :\n\u0012\n\u0012\n\u0013\n\u0013\n~ 1/2\n1/2\nSx\n=\n1/2\n2 1/2\n\u0012\n\u0013\n\u0012\n\u0013\n~\n1/2\n1/2\nSx\n= \u2212\n\u22121/2\n2 \u22121/2\n\n(10.32)\n\nThis explains the outcome\n\u0012 \u0013 of the 'twisted' Stern-Gerlach experiment. The first part selects\n1\nthe state to be in\n. This is an eigenstate of Sz , but not of Sx . Hence, when passing\n0\n\u0012\n\u0013\n1/2\nthrough the second B-field, the state gets split again. The part\nhad positive Sx ,\n1/2\n\u0012\n\u0013\n\u22121/2\nhence feels a force Fx > 0 and goes to positive x. The part\nhas negative Sx ,\n1/2\nso is subject to Fx < 0 and goes the other way. Similar reasonings hold when considering\nSz and Sy or Sx and Sy : if an electron is in a definite state with respect to one of the\nthree operators Sx , Sy of Sz , it will be a superposition of the up and down states along\nany other direction.\n\n10.2.3\n\nThe Zeeman effect, ctd.\n\nWe can now see why Zeeman's experiment gave a 'strange' outcome. Consider a free\nelectron (so not bound in an atom). Put it in a magnetic field: (0, 0, B). The Hamiltonian\nof the electron is:\n~ *M\n~ = \u2212\u03b3e B\n~ *S\n~ = \u2212\u03b3e BSz\nH = \u2212B\n(10.33)\nHence, the spin-up and the spin-down states are energy eigenstates:\n~\nH| \u2191i = \u2212\u03b3e B | \u2191i\n2\n\n~\nH| \u2193i = \u03b3e B | \u2193i\n2\n\n(10.34)\n\nTheir energy difference is given by:\n\u2206E = \u03b3e B\n\n\u0012\n\u0013\n~\n~\n\u2212 \u2212\u03b3e B\n= \u03b3e B~\n2\n2\n\n(10.35)\n\nSo in conclusion, when you want to know the energy of an electron in an atom, not only\ndo you need to consider the energy shift due to its angular momentum (proportional to\nthe quantum number m) you also need to specify its spin state (up or down) to get the\nright expression. If you carefully account for both of those, your prediction very tightly\ncoincides with Zeeman's measurements.\nThis concludes the story part. You now understand the outcome of both the Zeeman\nand Stern-Gerlach experiment thanks to the concept of spin. Of course, you may not be\nfully comfortable with it yet. To get more grip on it, we will work out some more details\nand concrete examples in the last part of this chapter.\n\n\f164\n\nCHAPTER 10. SPIN\n\n10.3\n10.3.1\n\nWorking with spin\nBra's and ket's\n\nA nice thing about spin, is that realizes the bra and ket formalism in a very simple way.\nFor a general spin ket |si = \u03b1| \u2191i + \u03b2| \u2193i, we define its bra as follows:\n\u0012\n\u0013\n\u0001\n\u03b1\n|si =\n\u21d2\nhs| = \u03b1\u2217 \u03b2 \u2217\n(10.36)\n\u03b2\n\nThe inner product of states is nothing but the inner product of vectors:\n\u0012 \u0013\n\u0001 c\nhs1 |s2 i = a b\n= ac + bd\nd\n\nIn particular, the square of the norm of a spin state is given by\n\u0012\n\u0013\n\u0001 \u03b1\n2\n\u2217\n\u2217\nks1 k = hs1 |s1 i = \u03b1 \u03b2\n= |\u03b1|2 + |\u03b2|2\n\u03b2\n\n(10.37)\n\n(10.38)\n\n\u0012\n\n\u0013\n\u03b1\nThis clarifies our remark in the previous section: a spin state\nis normalized if and\n\u03b2\nonly if |\u03b1|2 + |\u03b2|2 = 1. This is a nice moment to write down the spin up and down states\nalong the x and y axes. Denoting them by |\u00b1ix and |\u00b1iy , we have\n\u221a \u0013\n\u221a \u0013\n\u0012\n\u0012\n1/ \u221a2\n1/ \u221a2\nand |\u00b1iy =\n(10.39)\n|\u00b1ix =\n\u00b11/ 2\n\u00b1i/ 2\nYou can verify their normalisation and that\n~\nSx |\u00b1ix = \u00b1 |\u00b1ix\n2\n\n~\nand Sy |\u00b1iy = \u00b1 |\u00b1iy .\n2\n\n(10.40)\n\nAlso, it is nice to check explicitly that the inner product between the two eigenstates of\nany Si is zero. This has to be so, since these pairs are eigenstates (eigenvectors) of a\nHermitian operator (matrix) with different eigenvalues.\n\n10.3.2\n\nAngular momentum\n\nIf you are very (very) critical, you may remark that the Zeeman and Stern-Gerlach experiment do not strictly prove the existence of an electron's angular momentum. They clearly\nshow an electron has an intrinsic magnetic dipole moment - how else could it get an extra\n~ *B\n~ in the presence of a magnetic field? From this observation then, we just\nenergy term M\nassumed the underlying reason for this dipole is an intrinsic angular momentum - which\nwe called spin. This is a very natural thing to do, but how do we show more directly\n\n\f165\n\nCHAPTER 10. SPIN\n\nthat this spin really is a form of angular momentum? Or more general, if one has three\noperators (Ox , Oy , Oz ) what makes one conclude that these give the angular momentum of\na quantum system? Classically, an object is an angular momentum if it gives an object's\n'amount of rotation' but in quantum mechanics we can not use this pictorial definition. It\nmight be good at this point to go back quite a bit, to the point where we computed the\n~ = (Lx , Ly , Lz ). We found\ncommutation relations of the angular momentum operators L\nthat\n[Lx , Ly ] = i~Lz and cyclic\n(10.41)\nBy 'cyclic' we mean that you can shift all three variables x \u2192 y \u2192 z to get the other\ntwo commutators. These commutation relations are actually quite special. They are so\nspecial, that whatever set of operators satisfying similar commutation relations is defined\nto be a generalised angular momentum. This gives a nice and strict definition of\n~ = (Sx , Sy , Sz ) satisfy\nangular momentum in quantum mechanics. Do the spin operators S\nthe above commutation relations? You bet: with the matrix representation it is easy to\nshow that\n[Sx , Sy ] = i~Sz and cyclic\n(10.42)\nHence, we can comfortably state that spin really is a (generalized) angular momentum,\njust because of the structure of the commutators of its three components.\nIn analogy with the total angular momentum operator L2 , we can also write out the\noperator S 2 :\n\u0013\u0012\n\u0013\n\u0012\n\u0012\n\u0013\nX\n1\n1 0\n1 0\n21\n23\n2\n~\n~\n+1\n(10.43)\n=~\nSi Si = ~\nS = SS =\n0 1\n4 0 1\n2 2\ni\n\nSo every spin state |si is an eigenvector of S 2 , with eigenvalue ~2 21\n\u0013\n\u0012\n1\n2\n21\nS |si = ~\n+ 1 |si.\n2 2\n\n1\n2\n\n\u0001\n+1 :\n\n(10.44)\n\nThe reason to write 3/4 in a funny way, is to make clear the (formal) resemblance to the\natomic energy states, where we found\nL2 |n, l, mi = ~2 l(l + 1)|n, l, mi.\n\n(10.45)\n\nSo just like we labeled |n, l, mi with an orbital angular momentum quantum number l, spin\nstates can be interpreted as carrying an intrinsic angular momentum quantum number\n1/2. Because this value is the same for all spin states (all spin states are eigenstates of\nS 2 with the same eigenvalue) this is an intrinsic property of the particle itself. People say\nthat the electron is a spin- 12 particle.\n\n\f166\n\nCHAPTER 10. SPIN\n\n10.3.3\n\nQuantum Larmor precession\n\nAh, we have arrived at our final computation. We start out with a small question. The\noperators Sx , Sy , and Sz describe the spin of a particle along three main axes. What\nabout all other directions? In spherical coordinates, every direction can be described by\na unit vector ~u = (sin \u03b8 cos \u03c6, sin \u03b8 sin \u03c6, cos \u03b8). Hence it is natural to define the operator\nof the spin along direction ~u as:\n\u0013\n\u0012\n~\ncos \u03b8\nsin \u03b8e\u2212i\u03c6\n~\nSu = ~u * S =\n.\n(10.46)\n2 sin \u03b8ei\u03c6 \u2212 cos \u03b8\nA trivial example: the x-direction is given by angles \u03b8 = \u03c02 , \u03c6 = 0. This corresponds to\nthe unit vector ~ux = (1, 0, 0), and the associated operator written out above is nothing\nbut Sx . For a general direction ~u and the corresponding operator Su , the spin eigenstates\nalong that direction are given by\n\u0013\n\u0013\n\u0012\n\u0012 \u2212i\u03c6/2\n\u2212e\u2212i\u03c6/2 sin(\u03b8/2)\ne\ncos(\u03b8/2)\n(10.47)\nand |\u2212iu =\n|+iu =\nei\u03c6/2 cos(\u03b8/2)\nei\u03c6/2 sin(\u03b8/2)\nAgain, a\u0012trivial\n\u03b8 = \u03c6 = 0, and the above expressions give\n\u0013 check: for the\n\u0012 z-direction,\n\u0013\n1\n0\n|+iz =\nand |\u2212iz =\nas should be. We have now all necessary tools to\n0\n1\ndescribe the quantum mechanical version of Larmor precession. Let us take a state, which\nis spin up along a direction ~u:\n|\u03c8(t = 0)i = |+iu\n\u0013\n\u0012 \u2212i\u03c6/2\ne\ncos(\u03b8/2)\n=\nei\u03c6/2 sin(\u03b8/2)\n\n(10.48)\n(10.49)\n\n= e\u2212i\u03c6/2 cos(\u03b8/2)|+iz + ei\u03c6/2 sin(\u03b8/2)|\u2212iz\n\n(10.50)\n\nWe now want to study how this state evolves through time, given that we put a magnetic\nfield (0, 0, B). Well, earlier we found that the magnetic field gives an energy E + = \u03b3e B ~2\nto the up state and E \u2212 = \u2212\u03b3e B 2~ for the down state. The Schr\u00f6dinger equation i\u2202t |\u03c8i =\nH|\u03c8i then tells us that the time evolution of the above state is given by\n|\u03c8(t)i = e\u2212i\u03c6/2 cos(\u03b8/2)e\u2212iE\n\n+ t/~\n\n|+iz + ei\u03c6/2 sin(\u03b8/2)e\u2212iE\n\n\u2212 t/~\n\n|\u2212iz\n\n(10.51)\n\nFrom this, we can compute the expectation value of the different spin components:\n~\nsin \u03b8 cos(\u03c6 + \u03c90 t)\n2\n~\nhSy i\u03c8 (t) = h\u03c8(t)|Sy |\u03c8(t)i = sin \u03b8 sin(\u03c6 + \u03c90 t)\n2\n~\nhSx i\u03c8 (t) = h\u03c8(t)|Sz |\u03c8(t)i = cos \u03b8\n2\nhSx i\u03c8 (t) = h\u03c8(t)|Sx |\u03c8(t)i =\n\n(10.52)\n(10.53)\n(10.54)\n\n\f167\n\nCHAPTER 10. SPIN\n\nSz |+>z =+h/2\n\nz\ny\n\nprojection\non x\n\nx\n\nSx|+>z = 0\n\nFigure 10.8: The spin of a particle does not point. Even though the expectation values\ndefine a vector (which does point) one would draw several incorrect conclusions from\ntreating spin as a vector. The most obvious pitfall is concluding that Sx |+iz ' =\u2032 0 by\nprojecting the spin on the x-axis: this is of course wrong. The right decomposition is\nshown on the right. Clearly this is not the way ordinary vectors work.\n\n+\n\nWhere \u03c90 = 2E~ = \u03b3e B, which is nothing but the classical Larmor frequency. The above\nresult is very nice: if you look back, you see that the expectation values of the spin components have the same time dependence as the components of the classical counterpart.\nSuch similarities show up quite often in quantum mechanics. Recall for example how the\n(semi-)classical radius of the electron orbit (the Bohr radius) was quite good an indicator\nfor the size of the correct quantum wave function. In general, classical intuition can often\n'predict' the evolution expectation values of systems, and orders of magnitude. But this\nis only a guidance. Equally often, the classical intuition is of no good at all - think of the\ntunneling effect for example.\nAlso with spin, one should be careful. The above result on Larmor precession may\nseems so natural, that one would almost think that the naive picture of spin (an electron\nrotating around an axis) captures the situation accurately. Although this would make the\nconcept of spin more easy to grasp, this idea really is incorrect. Spin is a bit more tricky\nthan that. Take a look at Figure 10.8. Say there is an electron with spin up along axis z.\nYou may be tempted to represent this as an arrow pointing upwards along z. (And such\nis done quite often.) This is not bad, but some students tend to infer from this drawing\nthat the components of the spin along axis x are zero. So they might write\nSx |+iy = 0.\n\n(WRONG)\n\n(10.55)\n\nThis is very wrong: we have seen in this chapter that the spin state - expressed in the\nbasis of the x direction will be a superposition:\n1\n1\n|+iz = \u221a |+ix + \u221a |\u2212ix\n2\n2\n\n(10.56)\n\n\f168\n\nCHAPTER 10. SPIN\nso that\n\n\u0012\n\u0013\n~\n1\n~ 1\n\u221a |\u2212ix 6= 0\nSx |+iz = \u221a |+ix + \u2212\n2 2\n2\n2\n\n(10.57)\n\nSo if you really want to use arrows, make sure you understand the right side of Figure 10.8.\nBut even then, you might get into trouble when trying to draw complex spin components so there probably all intuition breaks down. The safest thing is to remember the following\n(not so catchy) catchphrase:\nThe spin of a particle is represented by two (possibly complex) numbers. You\ncan decompose this as a linear combination of a spin-up and a spin-down state,\nwith respect to any direction. In particular, if the state is up (or down) along\na direction ~u, then it is eigen under the operator Su with eigenvalue ~/2 (or\n\u2212~/2). However, this does not mean this spin points up (or down) along ~u.\nSpin does not point in any direction; it is a state - not a vector.\nEspecially the last sentence is important. You might refute this: don't the expectation\n~ define a vector for each state - like in the equation (10.54)? This thing points,\nvalues of S\nright? Well, that is very true, but then recall that a state is more than just it's expectation\nvalue. In a similar way, we can associate to each wave function a well-defined number hxi,\nbut this does not yet mean a particle is a point.\nSo this makes spin a bit subtle to understand. There is a pretty common saying that\nthere is nothing we understand in life - we just get used to things. That is probably true\nhere too - and don't worry, you will get used to the idea of spin at some point.\n\nApplication: Sunspots\nDue to the high temperature, many of the atoms in the solar atmosphere are excited or\neven (partly) ionized. As a result, there is an abundant absorbing/emitting of photons\nby the electrons kicked up/falling down from one level to another. This is visible in the\nlight that reaches: the spectrum has so-called spectral lines (less intensity at specific\nfrequencies). It is possible to identify which lines are due to which elements and hence\ndeduce the composition of the sun without actually having to go there. (Historically, this\nis how we got to know the sun is composed of mainly hydrogen and helium. Before it\nwas thought the sun had a composition similar to earth.) However, if you look at those\nspectral lines carefully, you can see that the degenerate lines (coming from transitions\nbetween degenerate energy levels) are actually split! With what you have learned, you\nwould guess that the sun has a magnetic field, and that the splitting you are seeing is\nactually the Zeeman effect. This is indeed true. By doing precise measurements, you can\nobserve that this magnetic field varies along the solar surface. Maybe you have once heard\nof the small but numerous dark spots on the sun (not so imaginatively called 'sunspots').\nIt turns out the magnetic field is significantly larger at these places. This very crucial:\nthe darkness of the spots turns out to be caused by the magnetic field, which influences\nthe dynamics of the gas and plasma around it - in a way that the temperature is lowered\n\n\fCHAPTER 10. SPIN\n\n169\n\nwhich in turn makes that region shine less bright. The exact dynamics of the magnetic\nfield of the sun and the formation of sunspots is a vast field of research; and the underlying\nexperimental observations would definitely not be possible without (an understanding of)\nthe Zeeman effect!\n\n\f170\n\nCHAPTER 10. SPIN\n\nExercises\n1. Explain to yourself (without looking at the text) how the Zeeman and Stern-Gerlach\nexperiments work and why the lead to the idea of spin.\n2. In the chapter on the uncertainty principle, you learned that non-commuting observables are incompatible. How is this related to spin and the 'twisted' version of\nthe Stern-Gerlach experiment (Figure 10.7)?\n3. Check that the commutation relations of the spin operators satisfy the angular momentum commutation relations. Are these still satisfied if you absorb an extra\nfactor - say multiply all three matrices by 2? (Now look again at the transition\n(10.19)\u2192(10.20). Could you also have taken \u03b3e only half as large, and Sz = \u00b1~?)\n4. Check the completeness relations for the basis {|+iu , |\u2212iu }\n5. Find the eigenvalues and eigenvectors of the matrix (10.46). Do you find the vectors\nstated in the text?\n6. A typical value of the B-field in a solar spot is 0.15 Tesla. Ignoring angular momentum, what is the difference between the two spin states of an electron of an atom near\nthat spot? (The size of the gyromagnetic ratio of the electron spin is \u03b3e = 1.76 * 1011\ns\u22121 T\u22121 .) What is the frequency of the emitted photon when the electron goes from\none spin state to another?\nP\n7. Check the following property of the Pauli matrices: \u03c3a \u03c3b = \u03b4ab *I + i c \u03b5abc \u03c3c where\nI is the 2 \u00d7 2 identity matrix. With this, prove (~a * ~\u03c3 )(~b * ~\u03c3 ) = (~a * ~b) I + i~\u03c3 * (~a \u00d7 ~b).\n8. It is natural to guess that the total angular momentum of an electron is given by\n~ + S:\n~ just the sum of its orbital and intrinsic angular momentum. Because\nJ~ = L\n~\n~ on the spin state of the electron, [S,\n~ L]\n~ = 0.\nJ works on the wave function, and S\nShow that this implies that J~ is indeed a angular momentum operator (i.e. that it\nsatisfies the right commutation relations).\n9. Given a set of operators Jx , Jy , Jz which satisfy the angular momentum commutation\nrelations, define the following two operators:\nJ+ = Jx + iJy\n\nand\n\nJ\u2212 = Jx \u2212 iJy\n\n. Show that [Jz , J\u00b1 ] = \u00b1~J\u00b1 . Show also Jz J\u00b1 |mi = ~ (m \u00b1 1) J\u00b1 |mi if m is an\neigenstate of Jz with eigenvalue ~m. Interpret this result. (Hint: think back of the\nsituation with the creation and annihilation operators for the harmonic oscillator.)\n\n\fChapter 11\n\nMany particles\nIn this chapter. . .\nWe are about to explore a topic left untouched so far: describing the quantum behaviour\nof collections of particles, instead of just a single one. An important concept governing the\ndynamics of such systems is the notion of identical particles. This will lead to a division of\nparticles in two types: bosons and fermions. These concepts go a long way understanding\nthe structure of larger systems. It will soon become clear that they have very direct\nconsequences on the touch and feel of our own (macroscopic) world.\n\n171\n\n\fCHAPTER 11. MANY PARTICLES\n\n11.1\n\n172\n\nProduct and sum of vector spaces\n\nWe will give a short review on the direct sum and direct product of vector spaces. You\nmight have seen these before, but for what follows, it's useful to have it all fresh in your\nmind.\nAs an ode to those golden times of high school math, we will illustrate the two concepts\nwith an example involving marbles - oh yes. Imagine you have a collection of marbles.\nThey are of three kinds: yellow, blue and red. You can summarize the content of your\ncollection with three numbers: (ny , nb , nr ). These quantities give the total number of\nyellow, blue and red marbles you have. The space of all possible (ny , nb , nr ) forms a vector\nspace. (We ignore the issue of these numbers being integer or not.) For example: (1, 2, 2)\nand (2, 4, 1) give two possible marble collections, but also their sum (3, 6, 3) describes a\npossible collection. The same for multiples. Imagine you are even more lucky - and not\nonly have marbles, but also three empty jars. Woah, full of enthusiasm you put your\nmarbles in the three jars. That sure is some good fun, but a sudden problem strikes you.\nHow can I now describe my collection, now that it is in the jars? Here is a first way to\ndo so. On top of specifying (ny , nb , nr ), you can give another vector, which gives the total\nweight of marbles in each jar: (w1 , w2 , w3 ). These numbers too form a vector space, by\nvery similar arguments. Better even, your collection (and how it is put in the jars) can be\ncompactly summarised by one long vector:\n(ny , nb , nr , w1 , w2 , w3 ).\n\n(11.1)\n\nThis just glues the two pieces of information together in one single object. That is possible\nbecause the two pieces of information are completely independent: the number of marbles\nper color and the weight of the marbles in each jar have nothing to do with each other.\nMaking a new vector space in this fashion, is called the direct sum of vector spaces.\nBeing satisfied with this answer, you are just about to go and play with the marbles\nand jars, but another issue strikes you. You could also describe the marbles in the jars\nin another way. Why not say how much marbles there are in each jar, per color ? So\nsomething like:\n\uf8eb\n\uf8f6\nw1,b w1,y w1,r\n\uf8ed w2,b w2,y w2,r \uf8f8\n(11.2)\nw3,b w3,y w3,r\nwith for example w2,y the total weight of yellow marbles in jar 2. In this alternative\ndescription, we get more information on the system. This is because we are not keeping\ntrack of the weights in jars and the types of marbles separately anymore: we give mixed\ninformation on how many of each type is in each jar. This new vectorspace (in the form\nof 3-by-3 matrices) is called the direct product of the original two vector spaces.\nWhat you should remember from this somewhat pathetical example is the following.\nEvery pair of vector spaces can be combined into a new (larger) one. There are two ways\nto do this. The first (direct sum) glues vectors together. It means that you keep track\n\n\f173\n\nCHAPTER 11. MANY PARTICLES\n\nof both vectors (marble types and jar weights) in a rather trivial way. These two bits of\ninformation need not have any relation at all. The second possibility is taking the direct\nproduct. In that case, you keep mixed information (here: how much of each of the marble\ntype is in each of the jars). This construction is suited for situation in which you want to\nkeep track of a finer sort of information.\nIn a slightly more mathematical language: let there be given two vector spaces V and\nW of dimension n and m, with bases v1 , . . . vn and w1 , . . . wm . The direct sum of these\nvector spaces is another vector space, denoted V \u2295 W , with basis\n{v1 , . . . , vn , w1 , . . . , wm }\n\n(11.3)\n\nObviously, this vector space has dimension n + m. Next to this, we can also construct the\ndirect product, denoted V \u2297 W . This space has as a formal basis:\n\uf8f1\n\uf8fc\n\uf8f4\n\uf8f2 v1 w1 , v1 w2 . . . v1 wm \uf8f4\n\uf8fd\n..\n..\n..\n..\n(11.4)\n.\n.\n.\n.\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8fe\nvn w1 , vn w2 , . . . vn wm\n\nand has dimension nm.\n\n11.2\n\nMany particles\n\nSo here we start our quest to understand systems with more than one particle. For\nconcreteness, consider two particles. How can we describe those two together? Since both\nof them are wavepackets (\u03c81 and \u03c82 ), we could guess that the total system should be\ndescribed by the sum of the two wavefunctions \u03c81 + \u03c82 . Something like this is suggested\nin Figure 11.1. However, such a description quickly runs into trouble: how could we then\ndistinguish between those two particles and just one particle of which the wavefunction\njust happens to consist of two lumps? We need something else. A good starting point is\nthe following question: what if we measure their positions? In that case we get a concrete\noutcome like (x1 , x2 ). Here x1 is the measured position of one particle, and x2 the detected\nposition of the other. From our experience with single particle systems, we can guess that\nsuch an outcome is not certain, but only has a probability associated to it. From this we\nguess there must be a function\n\u03c8(x1 , x2 )\n(11.5)\nsuch that |\u03c8(x1 , x2 )|2 is the probability density to measure the particles' positions to\nbe (x1 , x2 ). This turns out to be the right description. So when we correctly want to\ndescribe two particles at once, we should not consider two separate wavefunctions, but one\ncollective wavefunction. As an illustration: the probability to measure a1 < x1 < b1\nand simultaneously a2 < x2 < b2 is given by\nZ x1 =b1 Z x2 =b2\n|\u03c8(x1 , x2 )|2 dx1 dx2\n(11.6)\nP (a1 < x1 < b1 & a2 < x2 < b2 ) =\nx1 =a1\n\nx2 =a2\n\n\f174\n\nCHAPTER 11. MANY PARTICLES\n\nx1\n\nx\nx2\nFigure 11.1: Left: if the wavefunction \u03c812 of two particles would be just the sum of\nthe wavefunctions, such a state could not be distinguished from a single particle whose\nwavefunction happens to consist of to lumps. Right: if we describe the two-particle system\nwith a function \u03c812 = \u03c8(x1 , x2 ) (real part shown) this problem does not occur. So this\ngives the right description.\n\nHence, the normalisation of a collective wavefunction takes the form\nZ \u221eZ \u221e\n|\u03c8(x1 , x2 )|2 dx1 dx2 = 1\n\u2212\u221e\n\n(11.7)\n\n\u2212\u221e\n\nBecause a collective wavefunction has more arguments (here two: x1 and x2 ) it is somewhat harder to visualise than a single wavefunction. An attempt at drawing a particular\n\u03c8(x1 , x2 ) is made in Figure 11.1 on the right.\n\n11.2.1\n\nDirect product\n\nWe have just seen that the wavefunction of a system of two particles keeps track of both\nparticles at once, not just the conjunction of two individual wavefunctions. If you got the\nmarble story, you might smell a direct product coming up here. Say that a basis of the\nHilbert space of particle one is given by |\u03c71 i, ..., |\u03c7n i, and a basis for the states of the\nsecond particle by |\u03c61 i, ..., |\u03c6m i. Then the combined system of the two particles has as\nbasis\n|\u03c7i i|\u03c6j i with 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m.\n(11.8)\nThe state |\u03c7i i|\u03c6j i is understood as a situation in which the first particle is in state |\u03c7i i\nand the second is in |\u03c6j i. The inner product between two collective states is defined in\nthe following natural way:\n(h\u03c7i |h\u03c6j |)(|\u03c7i\u2032 i|\u03c6j \u2032 i) = h\u03c7i |\u03c7i\u2032 ih\u03c6j |\u03c6j \u2032 i\n\n(11.9)\n\nBut this is not enough: we need the combined states to form a Hilbert space. So we\nalso want to allow linear combinations of the above basis of states. This means the most\n\n\f175\n\nCHAPTER 11. MANY PARTICLES\ngeneral state of the two-particle system can be written as\nX\n|\u03c8i =\ncij |\u03c7i i|\u03c6j i\n\n(11.10)\n\nij\n\nP\nP P\n(By ij we mean ni=1 m\nj=1 of course.) For this |\u03c8i, the chance to measure the first\nparticle to be in state |\u03c8i i and the second in |\u03c7j i is given by\nP (1 : |\u03c7i i & 2 : |\u03c6j i) = |cij |2\n\n(11.11)\n\nHence \u03c8 is normalised if\nX\nij\n\n|cij |2 = 1.\n\n(11.12)\n\nWe can now better understand the collective wavefunction \u03c8(x1 , x2 ) from above. A (continuous) basis we can take for any particle is the position basis |xi. Hence, we can take\nas a basis for the collective states all products |x1 i|x2 i. These are wavefunctions sharply\npeaked at one specific x1 and x2 . So just like\nZ\n|\u03c8i = dx \u03c8(x)|xi\n(11.13)\nfor a single particle, we have\n|\u03c8i =\n\nZ Z\n\ndx1 dx2 \u03c8(x1 , x2 )|x1 i|x2 i\n\n(11.14)\n\nfor two particles. So the collective wavefunction we were talking about before, is just the\nposition representation of the combined system:\n\u03c8(x1 , x2 ) = (hx1 |hx2 |)|\u03c8i\nThis means we can also obtain (11.14) by inserting two completeness relations:\n\u0013\n\u0012Z\nZ\ndx1 |x1 ihx1 | dx2 |x2 ihx2 | |\u03c8i\n|\u03c8i =\nZ Z\n=\ndx1 dx2 \u03c8(x1 , x2 )|x1 i|x2 i\n\n(11.15)\n\n(11.16)\n\nBefore moving on, we want to point out a very classical pitfall. Although this is so for all\nbasis states, not every state can be written in the product form |\u03c8i = |\u03c7i|\u03c6i. We illustrate\nthis subtlety with an example. Consider two particles, each of which can be in states |ai\nand |bi. For example, having both particles in state a is written as\n|ai1 |ai2\n\n(11.17)\n\n\f176\n\nCHAPTER 11. MANY PARTICLES\n\nwith the subscript denoting we are talking about particle 1 and 2 respectively. In the same\nway, the system can be in state\n|ai1 |bi2\n(11.18)\nThe system could also be in a (normalised) superposition of the above two:\n\u0012\n\u0013\n1\n1\n1\n1\n\u221a |ai1 |ai2 + \u221a |ai1 |bi2 = |ai1 \u221a |ai2 + \u221a |bi2\n(11.19)\n2\n2\n2\n2\n\u221a\nThis is a product form again! Indeed, if we rename the state (|ai + |bi)/ 2 as |ci, the\nabove state is just |ai1 |ci2 . In contrast, you can show that the state\n1\n1\n\u221a |ai1 |ai2 + \u221a |bi1 |bi2\n2\n2\n\n(11.20)\n\ncan never be rewritten as a product |di1 |ei2 - whatever definition of d and e you would try.\nSo if someone says you a system of two particles is in a state |\u03c8i you can not in general\ndecompose this state as |\u03c8i = |\u03c7i|\u03c6i. The only decomposition that is always possible is\n(11.10).\n\n11.2.2\n\nIdentical particles\n\nIn the previous section, we did not at all specify which two particles or systems we were\ntalking about. They could have been anything. Let us be more specific here, and say that\nthe two objects under consideration are two particles of the same species: so two protons,\nor two electrons, two muons - whatever. You may or may not have thought about this\nbefore, but there is a very special thing about particles. They don't have hair and they\ndon't wear clothes, so if you see two particles of the same species, they are really identical.\nTo illustrate this, imagine studying two electrons. The chance to detect electron 1 around\nsome x and electron 2 around x\u2032 is proportional to the probability density:\nP (1 : x & 2 : x\u2032 ) \u221d |\u03c8(x, x\u2032 )|2\n\n(11.21)\n\nIn a similar fashion, you would say that the chance to find electron 1 around x\u2032 and electron\n2 around x to be like\nP (1 : x\u2032 & 2 : x) \u221d |\u03c8(x\u2032 , x)|2\n(11.22)\nThis could a priori be two different probabilities. However, if you think more critical,\nyou may remark that identical particles neccesarily are indistinguishable. If you detect\none electron at site x and another at x\u2032 , how would you say which ones they are? The\nnames that you gave them (electron 1 and electron 2) are not written on them, so if you\ndo measurements, you can never tell which detection corresponded to which electron. So\npurely on the basis of consistency, we need to have\n|\u03c8(x, x\u2032 )|2 = |\u03c8(x\u2032 , x)|2\n\nfor identical particles.\n\n(11.23)\n\n\f177\n\nCHAPTER 11. MANY PARTICLES\n\nIndeed, both quantities describe one and the same measurement outcome (an electron\nfound at x and an electron found at x\u2032 ) so they have to yield equal probabilities. So we\nconlude\n\u03c8(x, x\u2032 ) = \u03b7 \u03c8(x\u2032 , x)\n(11.24)\nwith \u03b7 a complex number of modulus one. In fact, by interchanging twice, we get the same\nstate again, so \u03b7 2 = 1 implying \u03b7 = \u00b11. Inspired by this result, we define the exchange\noperator P . Take again two identical particles. If particle 1 is in state |\u03c7i and particle\n2 in state |\u03c6i, then P acts as follows:\nP |\u03c7i1 |\u03c6i2 = |\u03c6i1 |\u03c7i2\n\n(11.25)\n\nThat is, P puts particle 1 in the state of particle 2 and vice versa. Obviously, P 2 = 1: if\nyou interchange the states of the particles twice the state is again as before. From P 2 = 1,\nit follows that P can only have eigenvalues \u00b11. (Indeed, if P |\u03c8i = \u03bb|\u03c8i, and P 2 = 1 then\n|\u03c8i = P 2 |\u03c8i = \u03bb2 |\u03c8i so \u03bb2 = 1.) If the eigenvalue under P of the pair of identical particles\nis \u22121, we call them fermions, and if their eigenvalue is +1, we call the particles bosons.\nSo\n|\u03c7i1 |\u03c6i2 = |\u03c6i1 |\u03c7i2\n\n|\u03c7i1 |\u03c6i2 = \u2212|\u03c6i1 |\u03c7i2\n\n(bosons)\n(fermions)\n\n(11.26)\n(11.27)\n\nThis is a very fundamental classifications of elementary particles. Electrons, neutrons\nand protons are all fermions. Photons are bosons. As you know, there are many other\nparticles, and all of these are either bosons or fermions. Let us consider fermions as an\nexample. The two-particle wave function is:\nZ Z\n|\u03c8i =\ndx dx\u2032 \u03c8(x, x\u2032 )|xi1 |x\u2032 i2\n(11.28)\nand since |xi1 |x\u2032 i2 = \u2212|x\u2032 i1 |xi2 only the antisymmetric part of \u03c8 contributes to the\nintegral. So without any loss we can keep only the antisymmetric part of \u03c8(x, x\u2032 ):\n\u03c8(x, x\u2032 ) = \u2212\u03c8(x\u2032 , x)\n\n(11.29)\n\nThis way, we see that \u03b7 = \u22121 for fermions. In a similar fashion, \u03b7 = 1 for bosons and\n\u03c8(x, x\u2032 ) is symmetric in that case. So for either bosons or fermions, the chance to detect\nparticle 1 here and particle 2 there is equal to the chance to detect particle 2 here and\nparticle 1 there - as should be for indistinguishable particles.\n\n\f178\n\nCHAPTER 11. MANY PARTICLES\n\nx\nFigure 11.2: Two particles: one is at location x1 and the other is at location x2 . Because the particles are identical, this can not be distinguished from a situation where we\ninterchange the particles' positions. This has important consequences for the collective\nwavefunction: it should either be symmetric or anti-symmetric in terms of the position\nvariables.\n\n11.2.3\n\nThe Pauli exclusion principle\n\nSo far we have only considered two particles. We will not consider more than two particles\nin a general way. This is a bit sloppy - expressions with many indices and that sort of visual\nfoltering- and it gives few extra extra insight: all rules are just a quite straightforward\nextrapolation of the above conclusions. Taking direct products, bosons vs. fermions,\nminus signs popping up, that kind of things. So we skip the general treatment here.\nA special case we will treat is a system of N fermions, all of the same kind. If every\nfermion separately can be in states |\u03c8i i, then the Hilbert space of the entire system is\nspanned by\n|\u03c8i1 i|\u03c8i2 i...|\u03c8iN i\n(11.30)\nThe above state describes a situation where fermion 1 is in state |\u03c8i1 i, fermion 2 in |\u03c8i2 i,\netcetera. Now by the definition of a fermion, exchanging the states of two fermions is\nminus the original state. So for example, interchanging the states of fermion 1 and 3, we\nget:\n|\u03c8i1 i|\u03c8i2 i|\u03c8i3 i...|\u03c8iN i = \u2212|\u03c8i3 i|\u03c8i2 i|\u03c8i1 i...|\u03c8iN i\n(11.31)\n\nUp to the sign, the right hand side is to be read as: fermion 1 is in state i3 and fermion 3\nin state i1 . Once more, note that this rule is not so strange: both above states yield the\nsame observations (you'd measure indistinguishable electrons in states i1 , ..., iN ) so they\nhave to be identical up to a phase factor, which by definition is -1 for fermions.\nFrom the above equation, we conclude that there is some redundancy in the states (11.30).\nAny two combinations that are a permutation of the individual states have to be proportional to each other. So many of them are just the copies of another up to a possible\nsign.\nBut there is a more striking remark. Suppose the system is a state like (11.30) but\nnow we put fermions 1 and 2 in the same state, so i1 = i2 . Hence the total system\nis |\u03c8i1 i|\u03c8i1 i|\u03c8i3 i...|\u03c8iN i. If we blindly interchange the states of fermions 1 and 2 (even\nthough they are the same) the above rule tells us we get a minus sign:\n|\u03c8i1 i|\u03c8i1 i|\u03c8i3 i...|\u03c8iN i = \u2212|\u03c8i1 i|\u03c8i1 i|\u03c8i3 i...|\u03c8iN i\n\n(11.32)\n\nThis is a bit of a funny equation: an object can only equal its opposite if it is zero. So a\nstate with i1 = i2 can not exist! In general, we conclude:\n\n\fCHAPTER 11. MANY PARTICLES\n\n179\n\nIt is impossible for two fermions (of the same species) to be in the\nsame quantum state.\nThis is a very profound statement, with far-reaching consequences (we will meet some\nlater on in this chapter). It was first formulated by Pauli, and is called the exclusion\nprinciple.\nIn the unlikely case you mix up the meanings of bosons and fermions, you can use a\ntrick. A fermion is firm (it 'repels' other fermions) while a boson likes to get close on\n(it likes to sit together with other bosons). OK, there are better sounding rhymes in the\nworld but well. Time to use all these new goodies for some applications.\n\n11.3\n\nApplications\n\nWith the ideas and techniques presented in the theory part, you can say a very great deal\nabout larger physical systems. By 'large' we mean: an atom with two or more electrons, a\nmolecule, or even an large piece of metal. The Pauli exclusion principle is the final piece of\nthe quantum mechanical jigsaw that we have been putting together in this course. Below\nare some nice applications.\n\n11.3.1\n\nThe atomic structure\n\nYou know that the energy eigenstates of a single electron in an atom are given by |n l mi.\nIn the absence of a magnetic field, the energy depends on n only. If an electron is in\na high energy state n > 1, it will typically fall down after a while, until it has reached\nn = 1. With each transition, the energy difference is released in the form of radiation. So\na hydrogen atom at rest will typically be in the n = 1 state, the ground state. Now what\nabout larger atoms? These have more electrons in them, and the nuclear charge is now\nZ 6= 1 but it turns out you can still find states of the form |n, l, mi. A bold guess would\nbe that in such systems then, all electrons would sit in the lowest energy state. So you\nwould write\n|\u03c8i = |1 0 0i1 ...|1 0 0iN (WRONG)\n(11.33)\nIn words: the system is given by all (N ) electrons sitting in the lowest energy state |1 0 0i.\nHowever, this is not what Pauli taught us to do. No two electrons can sit in the same\nstate, so the above configuration is just not possible. If no two electrons can sit in the same\nstate, what is the best we can do (energy-wise)? Ah, just filling up the n lowest energy\nstates of course. Also, we know that electrons have a spin degree of freedom. Hence, in\neach energy level, there can be a spin-up electron and a spin-down. As an example of this,\nconsider the Lithium atom. Two electrons can sit in the |1 0 0i state, if at least they have\nopposite spins - this way they are in a different state, and the exclusion principle poses\nno problem. The third electron however is the odd one out: we can not do anything but\nputting it in a higher energy level. So the ground state of the 3-electron system of Lithium\n\n\f180\n\nCHAPTER 11. MANY PARTICLES\nis given by\n|\u03c8iLi = |1 0 0 \u2191i1 |1 0 0 \u2193i2 |2 0 0 si3\n\n(11.34)\n\nHere, we have just put the spin states as an extra parameter in the list of quantum\nnumbers. Note that the spin state s of the third electron can be whatever it likes to be\n- any superposition of up and down is allowed. Also, the spin up and down (of the first\ntwo electrons) can be with respect to any direction - as long as they are opposites, they\ndescribe states without overlap. In principle, the above notation is fine, but one has to\nkeep in mind that it is equals to \u2212|1 0 0 \u2193i1 |1 0 0 \u2191i2 |2 0 0 si3 because this just interchanges\nstates of electrons 1 and 2. To prevent any confusion, people like to write states explicity\nin an anti-symmetrized form. Here, this amounts to summing\nup all six permutations,\n\u221a\nwith the right signs, and each with an individual weight 1/ 6:\n|\u03c8iLi =\n+\n+\n\n1\n\u221a |1 0 0 \u2191i|1 0 0 \u2193i|2 0 0 si \u2212\n6\n1\n\u221a |1 1 0 \u2193i|2 0 0 si|1 0 0 \u2191i \u2212\n6\n1\n\u221a |2 0 0 si|1 0 0 \u2191i|1 0 0 \u2193i \u2212\n6\n\n1\n\u221a |1 0 0 \u2191i|2 0 0 si|1 0 0 \u2193i\n6\n1\n\u221a |1 0 0 \u2193i|1 0 0 \u2191i|2 0 0 si\n6\n1\n\u221a |2 0 0 si|1 0 0 \u2193i|1 0 0 \u2191i\n6\n\nIn principle, this is just the same state as the one up higher, but it is explicitly antisymmetrised, and hence we can freely drop all subscripts denoting the particle numbers,\nthese are not relevant anymore in the above notation. In a similar fashion, we can construct\nthe states of the electrons in arbitrary large atoms. This 'stacking' of electrons in energy\nlevels explains a very wide range of chemical properties of elements. As shown in Figure\n11.3 it can describe the configuration of electrons around larger atoms. These kind of\ndiagrams can in turn be used to explain which elements can be bound to which others,\nand so on. So combining the exclusion principle with our earlier results opens up the vast\nworld of chemistry. That's quite pretty... .\nOf course, there are some things one should bear in mind when considering larger\natoms. There are several effects that shift the energy levels of the states. These make the\nenergy of a state depend on all quantum numbers, not just n. So the most economic way\nof filling an atom's orbitals becomes a bit more subtle. You may remember some rules\nfrom chemistry that tell you in which order to fill the orbitals. These rules are precisely a\nconsequence of the energy shifts we are talking about. Here we mention two main effects\n(in order of importance) determining the shifts and splittings of the energy levels:\nEffect 1: Coulomb repulsion\nIn larger atoms, the Coulomb force is not acting between the nucleus and the electrons, but\nalso amongst each pair of electrons. This is an extra energy contribution, and hence one\nneeds to put the corresponding term into the Hamiltonian. Physically, what is happening,\nis that the electrons in low energy states (small n and small l) are covering up a part of\n\n\f181\n\nCHAPTER 11. MANY PARTICLES\n\nn=3\n\nn=2\nn=1\n\nFigure 11.3: The atomic structure of magnesium. The twelve electrons sit in the lowest\npossible energy states. On the left these are pictorially shown as Bohr orbits. The correct\ndescription involving states is shown on the right. States with the same n are called a\nshell, and states with the same n and l comprise a subshell. So for magnesium two shells\n(n = 1, 2) are completely filled and one shell (n = 3) is partly filled.\n\nthe charge of the nucleus. So electrons in more extended wavefunctions (larger n or larger\nl) see a lower effective nuclear charge. An approximate equation expressing this, is\nZeff = Z \u2212 S\n\n(11.35)\n\nwith Zeff the nuclear charge, seen by an electron that has S electrons below itself. This\nis called the shield effect. It is shown in Figure 11.4. Hence, the binding energies of\nelectrons in high orbits are lower than one would expect. Indeed, a lower (effective) nuclear\ncharge means that they are less attracted to the nucleus, and hence less well bound. Also,\nfor states with the same n but a different l will not have the same energy any more. A\nlarger l corresponds to a more extended wavefunction (recall from your chemistry class\nthe dumbbell shape p-orbital versus the spherical s-orbital) and will be more strongly\nexperience the shield effect.\nEffect 2: Spin-orbit interactions\n~ of a state creates a magnetic moment. For example,\nRecall that the angular momentum L\nwe found a Zeeman effect, even when not including spin. But there are also magnetic\n~ of the electrons. Because any pair of magnetic moments tend\nmoments due to the spin S\n~ S,\n~ which is called the\nto align, there is an extra term in the Hamiltonian of the form \u2212L\n\n\fCHAPTER 11. MANY PARTICLES\n\n182\n\nFigure 11.4: The shield effect, as experienced by the outer electron in lithium. The\nelectrons in the lowest energy states cover up the nucleus charge +Ze. So for the outer\nelectron, the effective nuclear charge is (Z \u2212 S)e, where S is the number of shielding\nelectrons. Here Z = 3 and S = 2. Note that S depends on the particular electron we are\nconsidering.\n\nspin-orbit coupling.1 Actually, even in the case of just one electron, this term is present.\nThe effect of this term is less drastic than the shield effect, but for precise results, one\nshould include it. The structure of atomic energy spectra obtained when including this\neffect is called the fine structure. Typically, it will involve small shifts of energy levels\n(and splits the degeneracy with respect to the spin) which can be observed in precise\nexperiments.\nConsequences\nThere are several consequences of the above effects. From the physics side, we see that\nthe structure of electronic states is more involved than just antisymmetrising the singleelectron results. Most important, the energy levels turn out not to depend on n only.\nIn the shield effect, the angular momentum plays a role, and upon including spin-orbit\ncoupling also the spin state becomes relevant. In the end, few is left of the degeneracy\nthat we might have expected. Actually this is not only an effect on larger atoms: spinorbit coupling also takes place in ordinary hydrogen, so even in that situation the fine\nstructure is non-trivial: there is a very small splitting of the energy states of equal n. We\nstress again that the Coulomb interaction between the electrons is the largest effect. Since\nthis is absent in the single electron case, the result there is only subject to the spin-orbit\n1\nThe word 'coupling' is a very popular term to indicate that two quantities influence each other. In\nthe context of quantum mechanics, it means that the there is a term in the Hamiltonian involving the two\nquantities. This means that the energy depends on the relative values of those quantities, so they 'feel'\neach other, they are not independent degrees of freedom.\n\n\fCHAPTER 11. MANY PARTICLES\n\n183\n\nFigure 11.5: Classical depiction of the spin-orbit coupling. Both the orbital angular\n~ of the electron and the intrinsic angular momentum S\n~ lead to a magnetic\nmomentum L\nmoment. These two magnetic moments tend to align. So a parallel state as shown here\nhas lower energy than when the two are anti-parallel. This means states with the same l\nbut different spin have different energies. With precise measurements these (very small)\nenergy differences can be measured.\n\ninteraction. So only if one really does precise measurements, the spectral lines of hydrogen\nare split. But to a very good accuracy, the energy levels are given by the n-dependent\nresult we found in chapter 6. The size of the splitting is about a thousand times smaller\nthan the energy differences between states with different n.\nFrom the physicist's side this means that much more work needs to be done to describe\nmany-electron atoms properly. In practice, one will have to rely on numerical techniques,\nsuch as the Hartree-Fock methods. But to apply those techniques, one really needs to\nknow what one is doing. So there is also quite some theoretical input. In conclusion, the\nstudy of atoms is a rich field. If you want to know more about it, be sure to take a class\non atomic physics once. With this course here, you sure have enough background to\nembark it.\n\n11.3.2\n\nApplication: Lasers\n\nAs you know, electrons in high energy states tend to fall back after a while, emitting a\nenergy: a photon. For this reason, energy levels that are not the ground state are called\nexcited states. The energy of the emitted photon from a transition between states is\ngiven by\n\u2206E = Ei \u2212 Ef\n(11.36)\nwhere Ei and Ef stand for the energy levels of the initial and final state of the electron.\nSuch a process is called spontaneous emission. There is however a trick to make this\nspontanteous decay process happen earlier. If you send a photon onto an atom that\nis excited, and this photon has exactly energy \u2206E, the decay process will occur right\naway. That is: two photons will instantly leave the atom, both of energy \u2206E. This\n\n\f184\n\nCHAPTER 11. MANY PARTICLES\n\nphenomenon is called stimulated emission. The process of stimulated emission was\npredicted a long time ago, by Einstein. This process was soon verified some years after,\nand soon raised the interest of many scientists for practical applications. What if you\nwould be able to put N atoms all in the same excited state. If one of them decays, the\nescaping photon would trigger all others to decay as well. This cascade process would\nalmost instantly release N photons, all of the same energy \u2206E. The result would be: an\nintense flash of light, consisting of photons that are all of identical energy (= identical\nwavelength). Repeating the process, you can get a continuous beam of very 'pure' light.\nSince the photons all have the same phase and frequency, such a source is called coherent\nand monochromous. This invention got the name LASER: Light Amplification by\nStimulated Emission of Radiation. Since the realisation of the first lasers, it has become\nan important player in eye surgery, metal cutting industry, measuring devices, printers,\nCD and DVD readers, and so on. Why do we mention this here? Well, to stress that\nthis important invention would never work if photons were fermions. For each individual\nflash, the photons exiting the laser are all in the same state: same energy, same position,\netcetera. This would be impossible under the asiocial behaviour of fermions. Only bosons\nlike to sit together in the same state, fermions just can't. So the next time you watch\nyour favourite high definition action movie DVD, be sure to thank those photons in the\nreading beam for being sociable bosons, otherwise we might still be working with those\ncharming but impractical black tapes of our parent's generation!\n\nFlashlamp (pump source)\nhighly\nreflective\nmirror\n\nPartially\nreflective\nmirror\nLaser\noutput\n\nFigure 11.6: A possible design of a laser. A flash lamp (purple light) lifts the atoms below\nit to an excited energy level. The decay may consist of several steps: typically first a\nsmall step (no visible light) and then a larger one - for example emitting red light. By\nstimulated emission, each photon rips out an entire cascade of identical photons of all the\nexcited atoms met on the way, giving a very monochromous and coherent light beam.\n\n\fCHAPTER 11. MANY PARTICLES\n\n185\n\nExercises\n1. Without looking, try to reconstruct as much as you can of the reasoning which leads\nto the distinction between bosons and fermions, and their defining property.\n2. In a many-particle system, the Hamiltonian will depend on the positions and momenta of all the particles. Also, the Hilbert space will be bigger. However, the\nSchr\u00f6dinger equation still holds. Convince yourself that the postulates of Quantum\nmechanics still apply despite these generalizations.\n3. In the text, the inner product of the product space was defined, based on the inner\nproduct of the single particle spaces. How would you have defined it if we had taken\nthe direct sum of the spaces?\n4. Someone is arguing with you whether quantum mechanics is different from classical\nmechanics. He goes: 'Ok, particles are tiny pieces of wave. But there are waves in\nclassical physics too'. Explain to him that (as just one possible counter argument)\nthe quantum physics of many particles is not a classical theory of waves. (Hint: look\nback at Figure 11.1.) In some sense, the conceptual shift to multi-particle quantum\nmechanics departs much further from our intuition than the step we made to describe\na single particle - luckily at least the mathematical framework stays the same in this\nstep.\n5. Symmetrize the state |ai|bi|ci|di (four bosons), respecting normalization. You may\nassume that |ai, |bi, |ci, |di are all properly normalized.\n6. Consider two fermions, both in a harmonic potential. The Hamiltonian is then\nH = H1 + H2 = \u2212\n\nm\u03c9 2 2\n~2 2\nm\u03c9 2 2\n~2 2\nDx1 +\nX1 \u2212\nDx2 +\nX2 .\n2m\n2\n2m\n2\n\nThis Hamiltonian acts on states of the form \u03c8(x1 , x2 ). If we denote by |ni1 the n-th\nharmonic oscillator state of the first particle (which is described by wave function\nwith argument x1 ) then H1 |ni1 = En |ni1 . Similarly, H2 |ni2 = En |ni2 for the\nsecond particle. Show that the state |ni1 |mi2 is an eigenstate of H. What is the\ncorresponding energy? What is the time evolution of this state? What is the antisymmetrised version? Which states are forbidden by Pauli's principle? The solution\nto the combined system is so easy here because the Hamiltonian doesn't impose\nany interaction between the particles: the potential only depends on the particle's\npositions, not their relative distance.\n7. Consider two particles, but now with Hamiltonian:\nH =\u2212\n\n~2 2\nK\nm\u03c9 2\nm\u03c9 2\n~2 2\nDx1 +\n(X1 \u2212 a1 )2 \u2212\nDx2 +\n(X2 \u2212 a2 )2 +\n2m\n2\n2m\n2\n|(X1 \u2212 X2 )|\n\n\fCHAPTER 11. MANY PARTICLES\n\n186\n\nwith a1 , a2 and K constants. What is the classical meaning of such a Hamiltonian?\nHint: interpret the three potential terms. For which molecule might this Hamiltonian\nbe a model? Here, the two particles are coupled: they feel each other as the energy\ndepends on their relative position, so the solutions aren't simply products of the\nsingle-particle solutions anymore - so you don't have to solve the system here.\n8. In the exercises of Chapter 5, you computed the spectrum of particles in a three\ndimensional box. Here, we will look for the Fermi energy of such a system, when N\n(noninteracting) fermions are in such a potential with volume V . To find the Fermi\nenergy, note that at T = 0, the N lowest energy states are occupied. Each state is\ncharacterized by a vector ~n = (nx , ny , nz ), and the energy of a state is essentially a\nconstant times the length of this vector. So roughly speaking, at zero temperature,\nall states which are filled have their ~n inside a sphere with radius R. (At least, the\nregion where all components nx , ny , nz are positive - one eight of a sphere that\nis.) By computing the sphere volume, estimate the number of states lying inside\nthe sphere:N (R). Invert this to find the radius corresponding to N fermions at zero\ntemperature. The energy of a state on this sphere is then the Fermi energy of the\nsystem. Prove that\n\u0012\n\u00132/3\n~2 3\u03c0 2 N\nEF =\n2m\nV\n.\n\n\fChapter 12\n\nMetals, insulators and all that's in\nbetween\nIn this chapter. . .\nIn this last chapter, we will move up one step in scale, and derive properties of metals (but also insulators and semiconductors) from the microscopical quantum laws. The\nunderstanding of these systems has an impact on our daily life which can hardly be overestimated. An obvious example is the development of computers. These would still be\nlimited to constructions of rotating wheels or vacuum tubes if not at some point the transistor was invented - a simple but versatile application of the physics of semiconducting\nmaterials. Again, quantum mechanics is the gateway to truly understand the underlying\nphysics.\n\n187\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n12.1\n\n188\n\nParticle statistics\n\nWhen describing large systems, one typically relies on statistical approaches. This is not\nonly a useful technique, it is often the only way out. For example, a container of gas can\nin principle be described by the motion of all individual molecules, but this is so complex\nthat it is not feasible in practice. What you do then, is describe how much of the molecules\nwould on average have this or that momentum, be on this or that position, etcetera. You\nmake statistical statements. This is the domain of Statistical Mechanics - on which may\nhave had a course already. If you haven't, don't worry: we'll briefly review the concepts\nwe need in this context.\n\n12.1.1\n\nPopulation numbers\n\nA large collection of particles or molecules is typically described by a temperature. The\nnotion of temperature is intimately connected to statistical properties. For example, if a\n(monoatomic, ideal) gas is at temperature T , the average energy per particle is given by\nhEi =\n\n3\nkT\n2\n\n(12.1)\n\nwith k the Boltzmann constant. Not all individual particles have energy 32 kT though:\non the contrary, many particles will have a much larger or much lower energy. Again, the\nbrackets (not to be confused with the quantum mechanical expectation value) denote that\nthe above is only an average over the entire collection of particles.\nLuckily, the above can be refined a bit. Suppose that the states that can be taken\nby the individual particles form a discrete set, with corresponding energies Ei . Then the\nBoltzmann distribution states that the number ni of particles in energy level Ei is\ngiven by:\n1 Ei\n(12.2)\nni = e\u2212 kT\nZ\nThe numbers ni are also called occupation numbers. This law should be understood\nas follows. If a the energy Ei of state i is big, then the exponential will be very small,\nso the state is only occupied by a few particles. States with lower Ei have a larger\noccupation number ni . The effect of high temperature is to make states of large energy\nmore easily accessible, since the exponential suppression becomes less harsh. Again, the\nabove expression is a statistical one: it expresses how many particles will on average be\nin Ei if the system as a whole is at temperature T . In particular, ni doesP\nnot have to\nbe an integer. The proportionality constant Z has to be chosen such that i ni equals\nto total number of particles. This constant can be ignored if one only considers ratio's of\npopulation numbers:\nEi \u2212Ej\ne\u2212Ei /kT\nni\n= \u2212E /kT = e\u2212 kT\n(12.3)\nnj\ne j\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\nE\n\n189\n\nE\n\nn(E)\nFigure 12.1: On the left a system at temperature T with several energy levels and some\nparticles is shown. The specific arrangement is not determined by the temperature: the\nonly thing we can talk about is the average occupation number of each level of this system.\nThis is given by a function n(E): the Botzmann distribution. Here, we have inverse-plotted\nthe function to highlight the similarity to the figure on the left.\n\nBosons and fermions\nFrom the previous chapter, we know that particles can not always be in one state together.\nThis is only the case for bosons. A fermionic state can either be occupied or empty,\nnothing else. A more careful analysis for those cases shows that for bosons and fermions\nthe Boltzmann distribution has to be replaced by:\nni =\n\n1\ne\n\nEi \u2212\u03bc\nkT\n\n(12.4)\n\u00b11\n\nwith \u03bc is a constant, explained below. The lower sign (minus) is to be used for bosons. Such\na distribution of particles over states is called Bose-Einstein-statistics. The upper sign\n(plus) is to be used for fermions. Such a distribution is called Fermi-Dirac statistics.\nClearly, in that case all ni are smaller than one, as should be for fermions.\nThe number \u03bc in the above expression is again a normalization constant - although\nit has now been absorbed into the exponential. It is to be taken such that the sum of\nall occupation numbers equals the number of particles of the system. It is called the\nchemical potential. It is interesting to see what happens at low temperatures. First,\nconsider bosons. Denote the lowest energy level by E0 . Of course, we need \u03bc < E0 , to\nensure that all occupation numbers are positive. If you now let T \u2192 0 and also \u03bc \u2192 E0 ,\nthen the number of particles in the lowest energy state (n0 ) becomes large (the denominator\nbecomes 1 \u2212 1 since \u03bc \u2192 E0 ) while all others (n1 , n1 , ... ) go to zero (the denominator\nbecomes \u221e \u2212 1 because T \u2192 0). This means all particles settle in the lowest energy state.\nThis phenomenon is called Bose-Einstein condensation. This interesting phenomenon\nis only possible because bosons can all sit in the same state. Now, consider fermions. You\ncan check that in the limit T \u2192 0, the occupation numbers ni \u2192 1 for all energy levels\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n190\n\nwith Ei < \u03bc and that ni \u2192 0 for all Ei > \u03bc. This is not so strange: at low temperatures,\nthe fermions seek the lowest energy configuration: stacking all lowest energy levels with\none particle each is the best you can do.\nLow occupation limit\nYou may ask yourself what the use of (12.2) is if we have (12.4). After all, every particle\n(even an entire atom or molecule!) is either a boson or a fermion. Why don't we see that\na gas consists of bosons or fermions? From the above distributions you might infer that\nboth cases behave drastically different. The answer is that for most gases the average\noccupation number of a state is extremely low. Not because there are few particles - on\nthe contrary - but there is an even more overwhelming amount of states. Hence, all ni will\nbe very small. This can only be due to the fact that e(Ei \u2212\u03bc)/kT is very large, so for all these\nsystems the \u00b11 in the numerator becomes negligible, and (12.4) neatly reduces to (12.2).\nPhrased differently: only if there are very few states available (\u03bc high, i.e.: comparable to\nparticle energies) or if the temperature is very low (small eEi /kT ) the difference between\nbosons and fermions becomes visible. For typical gas systems neither is the case, so the\ngas is well described by the ordinary Boltzmann distribution. So far the Stat-Mech recap,\ntime for the story...\n\n12.2\n\nMetals, insulators and all that's in between\n\nIn this course, we have met two types of systems: bound particles trapped in a potential,\nand free particles which can move around freely - possibly meeting some obstacles. In the\nfirst case, the spectrum typically consists of a discrete set of states that can be taken. In\nsharp contrast, the spectrum of a free particle is continuous: by adjusting its momentum\none can in principle give it any energy. Here, we meet a third type of systems: crystals.\nBy a 'crystal' we mean a solid in which the constituting atoms are arranged in a periodic\nand ordered way (a lattice). Electrons in such an environment are surely bound: they\ncan not just walk out of the crystal. However, they are not strictly attached to one\nsingle atom: within the grid, they can freely move around. In some sense this puts\ncrystals somewhere in between free and bound systems. And this is also reflected in the\nspectrum. For a crystal, the energy states that can be occupied by the electrons consist\nof several continuous regions, energy bands, which are separated from each other by\nregions without accessible states: band gaps. An example is shown in Figure 12.2.\nThere, electrons can sit in energies in three bands. The bands are separated by two gaps.\nThere are no states of which the energy lies in this gap regions, so no electron can occupy\na state with that energy - they are 'forbidden' energy zones.\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n191\n\nE\n\nforbidden\nforbidden\n\nFigure 12.2: The typical allowed energy levels for a crystal. Several continuous energy\nregions can be filled with electrons: these are the energy bands. In between them, there\nare no states available: these are the gaps.\n\nLevel filling\nWe just explained that every crystal structure has energy bands and band gaps associated\nto it. The placement of these regions on the energy spectrum depend on several things:\nthe atoms making up the crystal (their atomic numbers) the spatial arrangement of the\natoms (the lattice type) and possibly also impurities. It are these data that decide where\nthe energy bands and band gaps lie. But the location of these energy regions is not the\nend of the story. An important property is how far these energy levels are filled up by the\nelectrons present in the crystal. Once again, we note that the exclusion principle prevents\nany two electrons from sitting in the same energy level. Hence, the best electrons can do\nis filling up all the lowest energy states available, not putting any two in the same state.\nPictorially speaking, the electron filling defines a surface: the highest energy level that has\nto be occupied, given that all levels below have already been filled. This surface is called\nthe Fermi surface, and the corresponding energy is the Fermi energy.1 Now we have\nall necessary ingredients to classify into three large categories: conductors, insulators and\nsemiconductors. We review them in that order.\n\n12.2.1\n\nConductors\n\nConductors are materials that easily produce an electric current when an external potential\nis applied. What is the mechanism behind this current? As follows: the electrical field\ngives some electrons an extra energy, they are accelerated, and lifted up to a slightly\nhigher energy level. These electrons of a slightly higher energy tend to have a momentum\ndirection determined by the electric field. Hence they give rise to a macroscopic current.\nHowever, it is not always possible for an electron to take on a slightly higher energy\nlevel. To do so, there needs to be a vacant state available, right above the original state\nof the electron. Clearly, the electrons occupying low energy states can not do this: all\n1\n\nCombining with the tool part, you may remark that a low temperatures these should tend to the\nchemical potential.\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n192\n\nstates above them are already taken. So these electrons do not take part in the process of\nconduction. For electrons near the Fermi surface, the situation is different. If the Fermi\nsurface lies somewhere inside an energy band then the electrons there can jump up to\nhigher (unoccupied) energy states. In this scenario, the material can conduct a current\nvery well: it is a conductor. This case is shown on the left in Figure 12.3.\n\n12.2.2\n\nInsulators\n\nWe just explained that if the Fermi surface lies somewhere inside an energy band, the\nelectrons near that surface can jump up in energy and conduct a current. Another possibility that can occur, is that the Fermi surface lies at the top of an energy band. This\nenergy band is then completely filled, and is called the valence band. In such a case, the\nlowest vacant energy states lie much higher: at the bottom of the next energy band called\nthe conduction band. So electrons need to jump up over the entire band gap to reach\nthose states. If the band gap is wide, the applied potential is typically insufficient for a\nsignificant number of electrons to cross the band. Hence, the current will consist of only\nvery few (or even none) of the electrons. Such a material does not conduct well, or even\nnot at all: it is an insulator.\n\nE\nconduction band\n\nempty band\n\nempty band\nhalf-filled\nband\nfilled band\n\nvalence band\n\nlarge gap\n\nconduction band\n\nhighest filled\nband\n\nvalence band\n\nfilled band\n\nempty band\nsmall gap\nhighest filled\nband\nfilled band\n\nFigure 12.3: The different types of crystals according to their electrical properties. Left:\na conductor has a broad band, in the middle of which lies the Fermi surface. This leaves\na lot of available states open right above the occupied states. Such a situation easily\nconducts an electric current. Middle: when a large gap separates the (filled) valence band\nfrom the (empty) conduction band, electrons can hardly overcome that gap. This means\nfew or electrons will participate in the current: the material is an insulator.\n\n12.2.3\n\nSemiconductors\n\nThe third category of materials is a bit of an in-between. The Fermi surface again lies at\nthe top of an energy level, but now the valence band and the conduction band are much\ncloser together. The energy gap to be crossed is then much smaller. Here, the role of\ntemperature becomes very crucial. In the beginning of this chapter we met an expression\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n193\n\nni\n1\n0.8\n0.6\n0.4\n0.2\nT=0\n\nT1\n\nT2\n\nT3\n\n\u03bc\n\nE\n\nFigure 12.4: The Fermi-Dirac distribution for different temperatures. At zero temperature,\nthe system takes on the lowest possible energy state. This means all states below some\n\u03bc are filled and all higher states are empty. When raising the temperature, thermal\nexcitations start to show up: some electrons migrate to higher energies. The distribution\nsmoothly transitions to a more spread out distribution, the tail of which has a e\u2212Ei /kT\nshape.\n\nfor the occupation of states for a collection of Fermions at some temperature T , which can\nbe rewritten as follows:\n1\n(12.5)\nni = (E \u2212\u03bc)/kT\ne i\n+1\nThe shape of the distribution at several temperatures is shown in Figure 12.4.\nBack to the band gap of a semiconductor. Since the occupation numbers are low in the\nconduction band, the statistics can be safely approximated by the Boltzmann distribution.\nIndeed: (E \u2212 \u03bc) >> kT there, whence\n1\ne(E\u2212\u03bc)/kT\n\n+1\n\n\u223c e\u2212(E\u2212\u03bc)/kT\n\n(12.6)\n\nDenoting the size of the energy gap by \u2206E, we find that the occupation number ncond of\nthe conduction band is proportional to:\nncond. \u223c e\u2212\u2206E/kT\n\n(12.7)\n\nsince (E \u2212 \u03bc) \u2248 \u2206E for low-lying energy levels in that band. This factor strongly depends\non the temperature, so we conclude that the conductivity of the semiconductor is varies\nstrongly with temperature. Also, note that for each electron in the conduction band, a\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n194\n\nhole is left in the valence band. In the background of the lattice, this hole looks like a\npositive charge, and hence it can take part in the conduction too.\nBesides the temperature, the conductivity of a semiconductor is strongly influenced\nby impurities. That is because some impurities donate freely moving electrons (or holes).\nSince only very few electrons of the semiconductor take part in the conduction, the addition of a small amount of impurities may easily donate an equally large or much larger\namount of available electrons. So adding impurities allows to manipulate the properties\nof semiconductors strongly. This is the underlying reason of the widespread use of semiconductors in electronics. In particular, simple semiconductor based applications like the\ntransistor have revolutionized our world and this has even lead some people to state it is\nthe single most important invention of the 20th century. Again, the physics underlying\nthis invention would not have been understood without the advent of quantum mechanics.\nTime for an example...\n\n12.3\n\nA simple model\n\nStriking enough, we can obtain a (semi-realistic) example of a system containing energy\nbands and gaps without all too much effort, by the so-called Kronig-Penney model. As\nwe explained, a crystal is a solid material in which the atoms are arranged in a periodic\nand ordered fashion. By consequence, the Coulomb potential felt by electrons is periodic\nas well. We now mimic such a periodic potential by V (x) as shown in Figure 12.5. The\npotential is zero over a distance 2a, peaks at some V (x) = V over a distance b, and this\nis repeated. The period of the potential is given by c = 2a + b:\nV (x + c) = V (x)\n\n(12.8)\n\nWe now introduce a translation operator T , which is shifts any function horizontally over\na distance c:\nT f (x) = f (x + c)\n(12.9)\n\u2202\n=\nFor example T V (x) = V (x + c) = V (x). Using this and the fact that \u2202x\nfollows from the chain rule) we get\n\u0012\n\u0013\n~2 \u2202 2\nT H\u03c8(x) = T \u2212\n+ V (x) \u03c8(x)\n2m \u2202x2\n\u0012\n\u0013\n~2\n\u22022\n=\n\u2212\n+\nV\n(x\n+\nc)\n\u03c8(x + c)\n2m \u2202(x + c)2\n\u0013\n\u0012\n~2 \u2202 2\n+ V (x) \u03c8(x + c)\n=\n\u2212\n2m \u2202x2\n= HT \u03c8(x)\n\n\u2202\n\u2202(x+c)\n\n(which\n\n(12.10)\n(12.11)\n(12.12)\n(12.13)\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n195\n\nV(x)\nV\nx\n\nb\n\n2a\n\nFigure 12.5: A periodic potential. The value is alternating between V (x) = 0 and V (x) =\nV . The width of the peaks is b and they are separated by a distance 2a.\n\nSo [H, T ] = 0. This means that if |\u03c8i is an eigenstate of H with eigenvalue E, the state\nT |\u03c8i will be so too, and with the same eigenvalue:\nH(T |\u03c8i) = T H|\u03c8i = T E|\u03c8i = E(T |\u03c8i).\n\n(12.14)\n\nIf we assume that there is only one state for each energy, |\u03c8i and T |\u03c8i must be the same\nphysical states. So their wave functions \u03c8(x) and \u03c8(x + c) must be equal up to a phase\nfactor:\n\u03c8(x) = ei\u03b1 T \u03c8(x) = ei\u03b1 \u03c8(x + c)\n(12.15)\nThe equation \u03c8(x) = ei\u03b1 \u03c8(x + c) is very restrictive. It can be shown that it can only be\nsatisfied for functions of the following form:\n\u03c8(x) = eiKx u(x)\n\n(12.16)\n\nwith u(x) a periodic function. Wave functions of this kind are called Bloch waves. They\nare not necessary periodic over a distance c, since the 'rotating' part eiKx can have any\nperiodicity (not necessary c). Also note that the above equation implies that\n\u03c8(x + c) = eiKc \u03c8(x).\n\n(12.17)\n\nNow, let us put a particle of energy E in the potential. For concreteness, take E < V , the\nother case works out in a similar fashion. Defining\n\u03ba2 =\n\n2m\n(V \u2212 E) and\n~2\n\nk2 =\n\n2m\nE\n~2\n\n(12.18)\n\nthe solution in region 1 (see Figure 12.5) is given by:\n\u03c81 (x) = A1 e\u03bax + B1 e\u2212\u03bax\nikx\n\n= A2 e\n\n\u2212ikx\n\n+ B2 e\n\nfor\nfor\n\n\u2212 b \u2212 a < x < \u2212a\n\u2212a<x<a\n\n(12.19)\n(12.20)\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\nFrom the above and (12.17) it follows that the solution in region 2 looks like:\ni\nh\nfor a < x < a + b\n\u03c82 (x) = eiKc A1 e\u03ba(x\u2212c) + B1 e\u2212\u03ba(x\u2212c)\ni\nh\nfor a + b < x < a + c\n= eiKc A2 eik(x\u2212c) + B2 e\u2212ik(x\u2212c)\n\n196\n\n(12.21)\n(12.22)\n\nDemanding continuity of the wave function and its derivative at the points x = \u2212a and\nx = a gives the following four constraints:\nA1 e\u2212\u03baa + B1 e\u03baa = A2 e\u2212ika + B2 eika\n\u2212\u03baa\n\n\u03baA1 e\n\n\u03baa\n\n\u2212 \u03baB1 e\n\n\u2212ika\n\n(12.23)\nika\n\n= ikA2 e\n\u2212 ikB2 e\ni\nh\nA2 eika + B2 e\u2212ika = eiKc A1 e\u03ba(a\u2212c) + B1 e\u2212\u03ba(a\u2212c)\ni\ni\nh\nh\nik A2 eika \u2212 B2 e\u2212ika = eiKc \u03ba A1 e\u03ba(a\u2212c) \u2212 B1 e\u2212\u03ba(a\u2212c)\n\n(12.24)\n(12.25)\n(12.26)\n\nThis is a homogenous system of four equations in the unknowns A1 , B1 , A2 , B2 . The\ncondition to have solutions is thus\ne\u2212\u03baa\ne\u03baa\n\u2212e\u2212ika\n\u2212eika\n\u2212\u03baa\n\u03baa\n\u2212ika\n\u03bae\n\u2212\u03bae\n\u2212ike\nikeika\niKc\n\u03ba(a\u2212c)\niKc\n\u2212\u03ba(a\u2212c)\nika\n\u2212e e\n\u2212e e\ne\ne\u2212ika\n\u2212\u03baeiKc e\u03ba(a\u2212c) \u03baeiKc e\u2212\u03ba(a\u2212c)\nikeika\n\u2212ike\u2212ika\n\n=0\n\n(12.27)\n\nTo evaluate the determinant, we simplify it a bit. First divide away the factors e\u2212\u03baa , e\u03baa ,\n\u2212e\u2212ika and \u2212eika from the four rows, and use 2a \u2212 c = \u2212b to simplify some exponents.\nThe first row then looks like (1, 1, 1, 1). Then put (column 1+ column 2)/2 and (column\n1 - column 2)/2 instead of the original columns 1 and 2, and similarly for the columns 3\nand 4. Taking away a last factor i from column 4, we get:\n1\n0\n1\n0\n0\n\u03ba\n0\nk\n\u2212eiKc cosh(\u03bab) eiKc sinh(\u03bab) \u2212 cos(2ka)\n\u2212 sin 2ka\n\u03baeiKc sinh(\u03bab) \u2212\u03baeiKc cosh \u03bab k sin(2ka) \u2212k cos(2ka)\n\n=0\n\n(12.28)\n\nA lot of zeros have popped up! By expanding this as a sum of cofactors of the elements of\nfirst row, the above is just a sum of two 3-by-3 determinants. These are not too hard: they\neach lead to four terms. Collecting terms and simplifying expressions like sin2 \u03b8+cos2 \u03b8 = 1\nand cosh2 x\u2212sinh2 x = 1, the whole thing becomes less and less ugly. The end result reads:\n\u03ba2 \u2212 k2\nsin(2ka) sinh(\u03bab) = cos(Kc)\n(12.29)\n2\u03bak\nSo to recap: for the system to have smooth solutions at energy E, the above equation\nneeds to be satisfied. Since the right hand side lies between -1 and 1, the left hand side\nmust be so to in order to allow solutions. In conclusion, we need\ncos(2ka) cosh(\u03bab) +\n\n\u2212 1 \u2264 f (E) \u2264 1,\n\n(12.30)\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n197\n\nf(E)\n1\n\nE\n-1\n\nenergy bands\nFigure 12.6: The function f (E) for a specific value of a and b. There are no states with\nenergy E in the regions where |f (E)| > 1. So these zones are gaps, as indicated on\nthe figure. In the regions where |f (E)| < 1 there are energy states available, so these\nconstitute the energy bands.\n\nwith\n\n\u03ba2 \u2212 k2\nsin(2ka) sinh(\u03bab)\n(12.31)\n2\u03bak\nThe function f (E) is shown in Figure 12.6 for some arbitrary values of the parameters a, b\nand V . It is immediately clear that there are genuine energy bands and energy gaps. This\nbehavior occurs rather generic: the parameters a, b and V need not to be finely tuned\nto arrive at pictures similar to the Figure 12.6. So even for a very simplistic potential\nwe obtain a semi-realistic energy spectrum. One can upgrade this model by taking more\nCoulomb-like potentials (of which the charge is partly shielded by the other electrons) by\nmaking the model three-dimensional and by including imperfections of the crystal. So\nwith the very same principles, one can obtain even more realistic results.\nf (E) = cos(2ka) cosh(\u03bab) +\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n198\n\nExercises\n1. Explain that the electrons crystals are somewhere in between bound state and free.\nHow is this reflected in the energy spectrum?\n2. It is sometimes stated that a conductor is a limiting case of a semiconductor, where\nthe valence and conduction band are right on top of each other, without gap. Explain.\n3. The surface of the sun is at about 5800K. For every hydrogen atom in the 2s state,\nhow many atoms can you find in the 1s state? The Boltzmann constant is k =\n8.62 * 10\u22125 eV/K= 1, 38 * 10\u221223 J/K.\n4. Redo the periodic potential, but now for the case E > V . (There is a shortcut\nto obtain this result: if E > V , then \u03ba becomes imaginary. Putting \u03ba = ik2 and\nusing sinh(ix) = i sin x and cosh(ix) = cos x you should be able to find the analog\nof (12.29) in terms of k and k2 . This should correspond to what you found in the\n'brute' way.)\n5. For the Kronig-Penney model, we found the energy spectrum. Recall that the states\nare also characterized by their momentum k. (Just like for a free particle, the energy\nis a function of the wave number.) So actually, the spectrum can also be depicted\nas lines in a E \u2212 k diagram, instead of just bands on an energy axis. Look at Figure\n(12.7): the set of allowed states are lines in this diagram. The dots denote states\n(actually, they form a continuous spectrum, but well) and a white dot denotes an\nempty state. How do you see the energy gap on the pictures? The material shown\nis an N-type semiconductor: a semiconductor doped by a material which donates\nsome extra electrons to the lattice. The occupation of states is shown for the same\nmaterial, in three different situations. Explain which physical situations you see on\nthe three different pictures. In the situation where a net current is flowing (due to\nan external E-field) in which direction is it going?\nE\n\nE\n\nk\n\nE\n\nk\n\nk\n\nFigure 12.7: An N-type semiconductor in different physical situations.\n\n\fOutro\nThat's it! This is the end of the course. You can now say that you know quite a bit about\nquantum mechanics. When you think back, you may realize that quantum mechanics is not\njust a funny and strange theory invented by even stranger physicists. It is an accurate and\ncomprehensible theory about nature on its smallest scale. Moreover, it is the beating heart\nof the reality as we see it. Indeed, atoms, molecules, crystalline solids, radiation: all these\ncrucial ingredients of nature are very strongly determined by the quantum mechanical\nlaws that govern their behavior. Also, the role of quantum mechanics in technological\napplications is much bigger than most people think. Lasers, microprocessors, precision\nthermometers and STM microscopes: none of these inventions could have been possible\nwithout understanding the laws quantum mechanics. Hopefully you have a more broad\nvision on this now, but far more important: I hope you have enjoyed this textbook.\n\n199\n\n\fBonus track: philosophical note\n12.4\n\nQuantum mystery and God's dice\n\nIn the outro, emphasis was put on the practical value of quantum mechanics in quite some technological applications. This\nis a fact which seems to be a bit ignored by popular culture.\nOn the contrary, to a large public, quantum mechanics has\na somewhat mysterious or even transcendental status. This\ndespite the fact that the theory came to life at the very beginning of the 20th century - the absolute prehistory on the\nscientific and technological timescale. So where does this esoteric status of quantum mechanics come from? An important\nelement in this quantum-mystery is the widespread idea that\nquantum mechanics is not a deterministic theory. This contrasts to more or less every other theory in physics, and would\nsurely be an uttermost mysterious conclusion of one of the most refined theories of nature.\nLet us think about this more carefully. Indeterministic ... where did we see that word\npopping up in this course? For sure there are no 'unknown' parameters in the Schr\u00f6dinger\nequation. Whether we are talking about one particle, or a bunch of them, or even an\nentire lump of matter, the rules of quantum mechanics give a very clear and deterministic\nprescription on how this system will evolve in time. If you know the system's Hamiltonian,\nyou can solve the time evolution, and say what it will look like at any later moment in\ntime. So in that aspect, quantum mechanics is not even slightly different from -say- the\nNewtonian description of point particles. It is only a bit more complex, and a lot more\naccurate.\n'Ah, but what about the measurement', you say. We did learn indeed that measuring\nthe position of a particle for example yields a more-or-less unpredictable outcome. The\nanswer will typically be some value lying inside the approximate location of the wave function, but we can never predict what the exact outcome will be. Back then, we summarized\nthis by the statement that a measuring device gives a 'reasonable, but random answer'. It\nis this aspect of the theory which people refer to when they say that quantum mechanics\nis indeterministic. And it is a puzzling feature indeed, which has caused a lot of confusion\nover the decades. A famous quote of Einstein in this context is: 'God does not play dice'.\n200\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n201\n\nThis summarizes his and many others' dissatisfaction with the issue.\nSo when considering the issue of determinism, it seems like quantum mechanics shows\ntwo faces. On one hand, every system (small or big) evolves perfectly deterministic by the\nSchr\u00f6dinger equation, on the other hand, the act of measurement introduces a random\noutcome, and collapses the system onto the state that is was measured to be in. So\nmeasuring devices seem to play a very special role here. Of course this is a bit paradoxical.\nIf the laws of quantum mechanics are correct (and there are very good reasons to believe\nthis) also measuring devices have to behave according to those laws. So they too, need to\nbehave deterministic. What is going on here?\nIn principle, the issue is almost of mathematical clarity. Either measuring devices\nevolve according to the deterministic laws of quantum mechanics, or they have a special\nstatus that allows them to give fundamentally indeterministic answers. The problem is of\ncourse that studying the behavior of a measuring device on the basis of quantum mechanics\nis a horrible problem: it is practically impossible to do any realistic computation for such\na large and complex system. There are some attempts to do so, but not with all too great\nsuccess - and it will probably stay like that for a while.\nSo the problem of deciding whether or not quantum mechanics is a deterministic theory\nis essentially one of computational complexity. Actually, there is quite a lot to this phrase\nof Einstein. What he refers to is of course that it seems unnatural for God2 to include\n'random' events in the evolution of any system - as if he was throwing dice for every\nmeasurement. Funny enough, when you think about dice, you may remark that these\n(depite being our most universal symbol for randomness) do not behave random either.\nThey look like they do, but you know they just obey Newton's laws of mechanics. Let us\nillustrate this with a discussion between Newton and a gambler.\nGambler: Hey Newton, do you have any fancy theory to predict the outcome\nof my dice? If you have, I'll learn physics right away!\nNewton: Well, actually, I have. They fall, bounce and twist just according to\nmy laws of mechanics.\nGambler: Oh, really? Can you predict what I will get right now? (Throws\ndice.) Aha, a double six! did you see that one coming?\nNewton: Not really. In principle, if you would give me all data, the momentum\nyou gave the dice, their shape and that of the surface they land on, I could\npredict the outcome. But even with all the right data, it's a very nasty computation. Probably even harder than computing orbits of planets.\nGambler: (disappointed) So actually, you can not say anything interesting\nabout it.\nNewton: Well, yes. There are probability laws. These will tell you that you\nare much more likely to throw a total of seven than a total of twelve, which\nyou got just now. These laws are of great value, even though they are not able\nto predict any individual outcome.\n2\n\na term that he used as a metaphor for the most fundamental laws of nature\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n202\n\nGambler: Ah, I see. So I better take a course on statistics then?\nNewton: If you want to be a better gambler, yes. Good luck with it!\nGambler: Thanks, Sir Newton, good luck with your laws!\nIn a very similar way, you may believe that measuring devices too behave deterministic.3\nOnly in the very end, the dazzlingly complex behavior gives rise to a relatively simple\nprobabilistic law: P \u223c |\u03c8|2 . But how could such a simple law be the end result of such\na huge complexity? Well, after all a measuring device is a very special system, designed\nto do a very special job. In the same way, a dice is precisely designed to give a nice equal\nchance distribution resulting from its dynamics - despite the fact that it was probably\nnot designed with the active purpose to 'develop a solid object, of which the dynamical\nand elastic characteristics are such that the final stage after a throw on a smooth surface\nends up in an almost perfectly homogeneous probability distribution over the possible\noutcomes'. In the same way, the simplicity of the probabilistic law of measurements in\nquantum mechanics may be the result of 'accidental' smart design of the instrument.4\n\n12.5\n\nA jungle of interpretations\n\nSo far a partly demystifying view on the whole subject. However, don't think there is\na consensus on this matter though. There is a jungle of possible interpretations of the\nphysics of the measurement in quantum mechanics. Just check out 'interpretation of\nquantum mechanics' on any search engine, and this jungle opens up. There are roughly\nthree categories of interpretations:\n\u2022 Indeterministic theories. These take the process of measurement as an indication\nthat quantum mechanics is intrinsically non-deterministic. This leads to very deep\nquestions on what reality is, what an observer is and what connects them. Probably\nthe most hard and (sorry for the word) confusing interpretation, these theories seem\nto dominate the view on the matter presented to the large public. Without doubt\nthe origin of the widespread mystery surrounding quantum mechanics.\n\u2022 Indifferent theories. These do not even pose the question whether quantum mechanics is deterministic or not, and categorize the issue as irrelevant and philosophical. Here, the arguments (quite rightful) are that quantum mechanics is a perfectly\nhealthy theory. It is consistent, allows to describe systems and the (be it probabilistic) outcomes of measurements. Where this probabilistic law comes from, is not a\nquestion the physicist should ask him- or herself. A quote supporting this: 'Shut\nup and calculate.' If you are not fond of philosophy, this might be your favorite\nviewpoint.\n3\n\nThis similarity was very likely not intended by Einstein's phrase, but the possible analogy between\nthe underlying mechanisms is rather striking.\n4\nNo intelligent design pun intended.\n\n\fCHAPTER 12. METALS, INSULATORS AND ALL THAT'S IN BETWEEN\n\n203\n\n\u2022 Deterministic theories. These assume that the process of measurement is governed\nby the same quantum laws that describe other systems. Some arguments for this\nview were presented above. Conceptually, this is probably the least confusing option.\n(Unless you are more dogmatic and like 'indifferent theories' better.) However, the\nquestion on how this precisely works (i.e. how the probability rule emerges exactly)\nis a very hard one, and remains largely unanswered.\nAs said, each of the categories comprises numerous theories and variations, and probably equally many of them do not fall strictly into one of the three types outlined here.\nMaybe you will develop a view of your own one day. Feel free to do so, the interpretation\nof the measurement of quantum mechanics is one of the (many) open problems of physics.\n\n\fFigure credits\nThanks\nIn creating this work, I have benefitted from the free software, media and knowledge\nprovided by the Latex team, the Inkscape team and the Wikimedia foundation. I also\nwant to thank my colleges and friends for their moral support. Many of the images\nwere based on existing ones. Here is a list of attributions. The images presented in this\ndocument are under the Creative Commons Attribution-Share Alike 3.0 Unported license,\nunless required differently by the license of their originals.\nFig.\n1.1\n1.1\n1.6\n\nOriginal work\nDoubleslitexperiment\nBlocked cat\nComplex number\n\n1.5\n1.6\n\nOnde electromagnetique\nDouble-slit schematic\n\n2.3.2\n\nInfinite potential well\n\n2.3.2\n3.3.4\n5.3\n5.3.4\n8.1\n9.5\n\nParticle in a box wavefunctions\nHarmOsziFunktionen\nBohr-atom-PAR\nHAtomOrbitals\nNormal Distribution PDF\nRastertunnelmikroskop-schema\n\n10.3\n10.4\n11.6\n12.4\n(dice)\n\nZeeman effect\nGloriole\nLasercons\nFD e mu\nWuerfel rot\n\nAuthor/user name\nKoantum, Trutz Behn\nBinnette\nKarol Ossowski,\nMesserWoland\nSuperManu\nPeter Suppenhuhn, Trutz\nBehn\nBenjamin D. Esham for the\nWikimedia Commons\nPapa November\nAllenMcC.\nPAR, JabberWok\nFlorianMarquard\nInductiveload\nMichael Schmid, Grzegorz\nPietrzak\nBogdan\nNae'blis\nDrBob\nUnc.hbar\nW.J.Pilsak\n204\n\nLicense\nby-sa\nGFDL/by-sa\nGFDL/by-sa\nGFDL/by-sa\npublic domain\npublic domain\nby-sa\nby-sa\nGFDL\nGFDL/by-sa\npublic domain\nby-sa\nby-sa\nGFDL\nGFDL/by-sa\npublic domain\nby-sa\n\n\fI\n\n\f"}