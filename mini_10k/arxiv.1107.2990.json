{"id": "http://arxiv.org/abs/1107.2990v2", "guidislink": true, "updated": "2013-12-03T00:32:22Z", "updated_parsed": [2013, 12, 3, 0, 32, 22, 1, 337, 0], "published": "2011-07-15T04:24:38Z", "published_parsed": [2011, 7, 15, 4, 24, 38, 4, 196, 0], "title": "Solving the At-Most-Once Problem with Nearly Optimal Effectiveness", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.3302%2C1107.1482%2C1107.4773%2C1107.3552%2C1107.1234%2C1107.3131%2C1107.1064%2C1107.0196%2C1107.4998%2C1107.3483%2C1107.2561%2C1107.1261%2C1107.2990%2C1107.0867%2C1107.4792%2C1107.5270%2C1107.3850%2C1107.4554%2C1107.3388%2C1107.2666%2C1107.2021%2C1107.5198%2C1107.0319%2C1107.0604%2C1107.2591%2C1107.1035%2C1107.0658%2C1107.2748%2C1107.4570%2C1107.2326%2C1107.4786%2C1107.2239%2C1107.5256%2C1107.4290%2C1107.1725%2C1107.3675%2C1107.3712%2C1107.1216%2C1107.4813%2C1107.2787%2C1107.2475%2C1107.1351%2C1107.4700%2C1107.4951%2C1107.4036%2C1107.5775%2C1107.3598%2C1107.1088%2C1107.3877%2C1107.5333%2C1107.4906%2C1107.5445%2C1107.3604%2C1107.0081%2C1107.1024%2C1107.2405%2C1107.0888%2C1107.0167%2C1107.3274%2C1107.5795%2C1107.1015%2C1107.0385%2C1107.5030%2C1107.0803%2C1107.4032%2C1107.3731%2C1107.2771%2C1107.1948%2C1107.2059%2C1107.1127%2C1107.3578%2C1107.4787%2C1107.5608%2C1107.3748%2C1107.0182%2C1107.4423%2C1107.1027%2C1107.1581%2C1107.3525%2C1107.0916%2C1107.0664%2C1107.2311%2C1107.2549%2C1107.3453%2C1107.2540%2C1107.0900%2C1107.2415%2C1107.5885%2C1107.1313%2C1107.5063%2C1107.2640%2C1107.4458%2C1107.3264%2C1107.4110%2C1107.5060%2C1107.4999%2C1107.0726%2C1107.1714%2C1107.2502%2C1107.3321%2C1107.3509&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Solving the At-Most-Once Problem with Nearly Optimal Effectiveness"}, "summary": "We present and analyze a wait-free deterministic algorithm for solving the\nat-most-once problem: how m shared-memory fail-prone processes perform\nasynchronously n jobs at most once. Our algorithmic strategy provides for the\nfirst time nearly optimal effectiveness, which is a measure that expresses the\ntotal number of jobs completed in the worst case. The effectiveness of our\nalgorithm equals n-2m+2. This is up to an additive factor of m close to the\nknown effectiveness upper bound n-m+1 over all possible algorithms and improves\non the previously best known deterministic solutions that have effectiveness\nonly n-log m o(n). We also present an iterative version of our algorithm that\nfor any $m = O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$ is both\neffectiveness-optimal and work-optimal, for any constant $\\epsilon > 0$. We\nthen employ this algorithm to provide a new algorithmic solution for the\nWrite-All problem which is work optimal for any\n$m=O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.3302%2C1107.1482%2C1107.4773%2C1107.3552%2C1107.1234%2C1107.3131%2C1107.1064%2C1107.0196%2C1107.4998%2C1107.3483%2C1107.2561%2C1107.1261%2C1107.2990%2C1107.0867%2C1107.4792%2C1107.5270%2C1107.3850%2C1107.4554%2C1107.3388%2C1107.2666%2C1107.2021%2C1107.5198%2C1107.0319%2C1107.0604%2C1107.2591%2C1107.1035%2C1107.0658%2C1107.2748%2C1107.4570%2C1107.2326%2C1107.4786%2C1107.2239%2C1107.5256%2C1107.4290%2C1107.1725%2C1107.3675%2C1107.3712%2C1107.1216%2C1107.4813%2C1107.2787%2C1107.2475%2C1107.1351%2C1107.4700%2C1107.4951%2C1107.4036%2C1107.5775%2C1107.3598%2C1107.1088%2C1107.3877%2C1107.5333%2C1107.4906%2C1107.5445%2C1107.3604%2C1107.0081%2C1107.1024%2C1107.2405%2C1107.0888%2C1107.0167%2C1107.3274%2C1107.5795%2C1107.1015%2C1107.0385%2C1107.5030%2C1107.0803%2C1107.4032%2C1107.3731%2C1107.2771%2C1107.1948%2C1107.2059%2C1107.1127%2C1107.3578%2C1107.4787%2C1107.5608%2C1107.3748%2C1107.0182%2C1107.4423%2C1107.1027%2C1107.1581%2C1107.3525%2C1107.0916%2C1107.0664%2C1107.2311%2C1107.2549%2C1107.3453%2C1107.2540%2C1107.0900%2C1107.2415%2C1107.5885%2C1107.1313%2C1107.5063%2C1107.2640%2C1107.4458%2C1107.3264%2C1107.4110%2C1107.5060%2C1107.4999%2C1107.0726%2C1107.1714%2C1107.2502%2C1107.3321%2C1107.3509&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present and analyze a wait-free deterministic algorithm for solving the\nat-most-once problem: how m shared-memory fail-prone processes perform\nasynchronously n jobs at most once. Our algorithmic strategy provides for the\nfirst time nearly optimal effectiveness, which is a measure that expresses the\ntotal number of jobs completed in the worst case. The effectiveness of our\nalgorithm equals n-2m+2. This is up to an additive factor of m close to the\nknown effectiveness upper bound n-m+1 over all possible algorithms and improves\non the previously best known deterministic solutions that have effectiveness\nonly n-log m o(n). We also present an iterative version of our algorithm that\nfor any $m = O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$ is both\neffectiveness-optimal and work-optimal, for any constant $\\epsilon > 0$. We\nthen employ this algorithm to provide a new algorithmic solution for the\nWrite-All problem which is work optimal for any\n$m=O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$."}, "authors": ["Sotirios Kentros", "Aggelos Kiayias"], "author_detail": {"name": "Aggelos Kiayias"}, "author": "Aggelos Kiayias", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.tcs.2013.04.017", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1107.2990v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.2990v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Updated Version. A Brief Announcement was published in PODC 2011. An\n  Extended Abstract was published in the proceeding of ICDCN 2012. A full\n  version was published in Theoretical Computer Science, Volume 496, 22 July\n  2013, Pages 69 - 88", "arxiv_primary_category": {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "F.1.2; F.2.m", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.2990v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1107.2990v2", "journal_reference": "Theoretical Computer Science, Volume 496, 22 July 2013, Pages\n  69-88, ISSN 0304-3975", "doi": "10.1016/j.tcs.2013.04.017", "fulltext": "Solving the At-Most-Once Problem with Nearly Optimal\nEffectiveness\nSotirios Kentrosa,1, Aggelos Kiayiasa,2\n\narXiv:1107.2990v2 [cs.DC] 3 Dec 2013\n\na Computer\n\nScience and Engineering, University of Connecticut, Storrs, USA\n\nAbstract\nWe present and analyze a wait-free deterministic algorithm for solving the at-most-once problem:\nhow m shared-memory fail-prone processes perform asynchronously n jobs at most once. Our\nalgorithmic strategy provides for the first time nearly optimal effectiveness, which is a measure\nthat expresses the total number of jobs completed in the worst case. The effectiveness of our algorithm equals n \u2212 2m + 2. This is up to an additive factor of m close to the known effectiveness\nupper bound n \u2212 m + 1 over all possible algorithms and improves on the previously best known\ndeterministic solutions that have effectiveness onlypn \u2212 log m * o(n). We also present an iterative\nversion of our algorithm that for any m = O( 3+\u01eb n/ log n) is both effectiveness-optimal and\nwork-optimal, for any constant \u01eb > 0. We then employ this algorithm to provide apnew algorithmic solution for the Write-All problem which is work optimal for any m = O( 3+\u01eb n/ log n).\nKeywords: at-most-once problem, task allocation, write-all, I/O automata, asynchronous shared\nmemory, deterministic algorithms, distributed computing\n\n1. Introduction\nThe at-most-once problem for asynchronous shared memory systems was introduced by Kentros et al. [26] as the problem of performing a set of n jobs by m fail-prone processes while\nmaintaining at-most-once semantics.\nThe at-most-once semantic for object invocation ensures that an operation accessing and altering the state of an object is performed no more than once. This semantic is among the standard\nsemantics for remote procedure calls (RPC) and method invocations and it provides important\nmeans for reasoning about the safety of critical applications. Uniprocessor systems may trivially\nprovide solutions for at-most-once semantics by implementing a central schedule for operations.\n\nEmail addresses: skentros@engr.uconn.edu (Sotirios Kentros), aggelos@kiayias.com (Aggelos\nKiayias)\n1 Research supported in part by the State Scholarships Foundation of Greece.\n2 Research supported in part by NSF awards 0831304, 0831306 and EU projects RECUP and CODAMODA\n3 NOTICE: This is the authors version of a work that was accepted for publication in Theoretical Computer Science.\nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other\nquality control mechanisms may not be reflected in this document. Changes may have been made to this work since\nit was submitted for publication. A definitive version was subsequently published in Theoretical Computer Science,\n[Volume 496, 22 July 2013] DOI:10.1016/j.tcs.2013.04.017\n\n1\n\n\fThe problem becomes very challenging for autonomous processes in a system with concurrent\ninvocations on multiple objects. At-most-once semantics have been thoroughly studied in the\ncontext of at-most-once message delivery [8, 30, 33] and at-most-once process invocation for\nRPC [6, 31, 37]. However, finding effective solutions for asynchronous shared-memory multiprocessors, in terms of how many at-most-once invocations can be performed by the cooperating processes, is largely an open problem. Solutions for the at-most-once problem, using only\natomic read/write memory, and without specialized hardware support such as conditional writing, provide a useful tool in reasoning about the safety properties of applications developed for\na variety of multiprocessor systems, including those not supporting bus-interlocking instructions\nand multi-core systems. Specifically, in recent years, attention has shifted from increasing clock\nspeed towards chip multiprocessing, in order to increase the performance of systems. Because of\nthe differences in each multi-core system, asynchronous shared memory is becoming an important abstraction for arguing about the safety properties of parallel applications in such systems.\nIn the next years, one can expect chip multiprocessing to appear in a wide range of applications,\nmany of which will have components that need to satisfy at-most-once semantics in order to\nguarantee safety. Such applications may include autonomous robotic devices, robotic devices\nfor assisted living, automation in production lines or medical facilities. In such applications performing specific jobs at-most-once may be of paramount importance for safety of patients, the\nworkers in a facility, or the devices themselves. Such jobs could be the triggering of a motor\nin a robotic arm, the activation of the X-ray gun in an X-ray machine, or supplying a dosage of\nmedicine to a patient.\nPerhaps the most important question in this area is devising algorithms for the at-most-once\nproblem with good effectiveness. The complexity measure of effectiveness [26] describes the\nnumber of jobs completed (at-most-once) by an implementation, as a function of the overall\nnumber of jobs n, the number of processes m, and the number of crashes f . The only deter1\nministic solutions known, exhibit very low effectiveness (n log m \u2212 1)log m (see [26]) which for\nmost choices of the parameters is very far from optimal (unless m = O(1)). Contrary to this,\nthe present work presents the first wait-free deterministic algorithm for the at-most-once problem\nwhich is optimal up to additive factors of m. Specifically our effectiveness is n\u2212 (2m\u2212 2) which\ncomes close to an additive factor of m to the known upper bound over all possible algorithms\nfor effectiveness n \u2212 m + 1 (from [26]). We also demonstrate how to construct an algorithm\nwhich has effectiveness n \u2212 O(m2 log n log m) and work p\ncomplexity O(n + m3+\u01eb log n), and\n3+\u01eb\nn/ log n), for any constant \u01eb > 0\nis both effectiveness and work optimal when m = O(\n(work complexity counts the total number of basic operations performed by the processes). Finally we show how to use this algorithm in order to solve the Write-All problem [23] with work\ncomplexity O(n + m3+\u01eb log n).\nRelated Work: A wide range of works study at-most-once semantics in a variety of settings. Atmost-once message delivery [8, 30, 33, 38] and at-most-once semantics for RPC [6, 31\u201333, 37],\nare two areas that have attracted a lot of attention. Both in at-most-once message delivery and\nRPCs, we have two entities (sender/client and receiver/server) that communicate by message\npassing. Any entity may fail and recover and messages may be delayed or lost. In the first case\none wants to guarantee that duplicate messages will not be accepted by the receiver, while in\nthe case of RPCs, one wants to guarantee that the procedure called in the remote server will be\ninvoked at-most-once [37].\nIn Kentros et al. [26], the at-most-once problem for asynchronous shared memory systems\nand the correctness properties to be satisfied by any solution were defined. The first algorithms\n2\n\n\fthat solve the at-most-once problem were provided and analyzed. Specifically they presented\ntwo algorithms that solve the at-most-once problem for two processes with optimal effectiveness and a multi-process algorithm, that employs a two-process algorithm as a building block,\nand solves the at-most-once problem with effectiveness n \u2212 log m * o(n) and work complexity\nO(n + m log m). Subsequently Censor-Hillel [22] provided a probabilistic algorithm in the same\nsetting with optimal effectiveness and expected work complexity O(nm2 log m) by employing a\nprobabilistic multi-valued consensus protocol as a building block.\nFollowing the conference version of this paper [25] and motivated by the difficulty of implementing wait-free deterministic solutions for the at-most-once problem that are effectiveness\noptimal, Kentros et al. [24] introduced the strong at-most-once problem and studied its feasibility. The strong at-most-once problem refers to the setting where effectiveness is measured only\nin terms of the jobs that need to be executed and the processes that took part in the computation\nand crashed. The strong at-most-once problem demands solutions that are adaptive, in the sense\nthat the effectiveness depends only on the behavior of processes that participate in the execution.\nIn this manner trivial solutions are excluded and, as demonstrated in [24], processes have to solve\nan agreement primitive in order to make progress and provide a solution for the problem. Kentros et al. [24] prove that the strong at-most-once problem has consensus number 2 as defined by\nHerlihy [21] and observe that it belongs in the Common2 class as defined by Afek et al. [1]. As\na result, there exists no wait-free deterministic solution for the strong at-most-once problem in\nthe asynchronous shared memory model, using atomic read/write registers. Kentros et al. [24]\npresent a randomized k-adaptive effectiveness optimal solution for the strong at-most-once problem, with expected work complexity of O(n + k 2+\u01eb log n) for any small constant \u01eb, where k the\nnumber of processes that participate in the execution.\nDi Crescenzo and Kiayias in [11] (and later Fitzi et al. [14]) demonstrate the use of the\nat-most-once semantic in message passing systems for the purpose of secure communication.\nDriven by the fundamental security requirements of one-time pad encryption, the authors partition a common random pad among multiple communicating parties. Perfect security can be\nachieved only if every piece of the pad is used at most once. The authors show how the parties maintain security while maximizing efficiency by applying at-most-once semantics on pad\nexpenditure.\nDucker et al. [12] consider a distributed task allocation problem, where players that communicate using a shared blackboard or an arbitrary directed communication graph, want to assign\nthe tasks so that each task is performed exactly once. They consider synchronous execution\nwithout failures and examine the communication and round complexity required to solve the\nproblem, providing relevant lower and upper bounds. If crashes are introduced in their model,\nthe impossibility results from Kentros et al. [26] will apply to the at-most-once version of their\nproblem.\nAnother related problem is the semi-matching problem [7, 10, 20]. The semi-matching problem known also as the load balancing problem has been extensively studied under various names\nin the network scheduling literature. Recently it has received renewed attention after a paper by\nHarvey et al. [20], where the name semi-matching was introduced. Semi-matching can be seen\nas an abstraction of the problem of matching clients with servers, each of which can process a\nsubset of clients. The goal is to match each client with at-most-one server. Clients and servers are\nabstracted as the vertices of a bipartite graph, and a synchronous, failure-free, message-passing\nmodel of computation is assumed, where edges represent communication links.\nOne can also relate the at-most-once problem to the consensus problem [13, 21, 29, 35]. Indeed, consensus can be viewed as an at-most-once distributed decision. Another related problem\n3\n\n\fis process renaming, see Attiya et al. [4] where each process identifier should be assigned to at\nmost one process.\nThe at-most-once problem has also many similarities with the Write-All problem for the\nshared memory model [3, 9, 18, 23, 28, 36]. First presented by Kanellakis and Shvartsman [23],\nthe Write-All problem is concerned with performing each job at-least-once. Most of the solutions\nfor the Write-All problem, exhibit super-linear work even when m \u226a n. Malewicz [36] was\nthe first to present a solution for the Write-All problem that has linear work for a non-trivial\nnumber of processors. The algorithm presented by Malewicz [36] has work O(n + m4 log n)\nand uses test-and-set operations. Later Kowalski and Shvartsman [28] presented a solution for\nthe Write-All problem that for any constant \u01eb has work O(n + m2+\u01eb ). Their algorithm uses\na collection of q permutations with contention O(q log q) for a properly chosen constant q and\ndoes not rely on test-and-set operations. Although an efficient polynomial time construction of\npermutations with contention O(q polylog q) has been developed by Kowalski et al. [27], it is\nnot known to date how to construct permutations with contention O(q log q) in polynomial time.\nSubsequent to the conference version of this paper [25], Alistarh et al. [2] show that there exists a\ndeterministic algorithm for the Write-All problem with work O(n + m log5 n log2 max(n, m)),\nby derandomizing their randomized solution for the problem. Their solution is a breakthrough in\nterms of bridging the gap between the \u03a9 (n + m log m) lower bound for the Write-All problem\nand known deterministic solutions, but is so far existential. For a detailed overview of research\non the Write-All problem, we refer the reader to the books by Georgiou and Shvartsman [15, 16].\nWe note that the at-most-once problem becomes much simpler when shared-memory is supplemented by some type of read-modify-write operations. For example, one can associate a\ntest-and-set bit with each job, ensuring that the job is assigned to the only process that successfully sets the shared bit. An effectiveness optimal implementation can then be easily obtained\nfrom any Write-All solution. In this paper we deal only with the more challenging setting where\nalgorithms use atomic read/write registers.\nContributions: We present and analyze the algorithm KK\u03b2 that solves the at-most-once problem. The algorithm is parametrized by \u03b2 \u2265 m and has effectiveness n \u2212 \u03b2 \u2212 m + 2. If \u03b2 < m\nthe correctness of the algorithm is still guaranteed, but the termination of the algorithm cannot\nbe guaranteed. For \u03b2 = m the algorithm has optimal effectiveness of n \u2212 2m + 2 up to an\nadditive factor of m. Note that the upper bound for the effectiveness of any algorithm is n \u2212 f\n[26], where f \u2264 m \u2212 1 is the number of failures in the system. We further prove that for\n\u03b2 \u2265 3m2 the algorithm has work complexity O(nm log n log m). We use algorithm KK\u03b2 with\n\u03b2 = 3m2 , in order to construct an iterated version of our algorithm which for any constant \u01eb > 0,\n3+\u01eb\nhas effectiveness of n \u2212 O(m2 log n log m) and work complexity\nlog n). This is\npO(n + m\n3+\u01eb\nn/ log n). We note that our\nboth effectiveness-optimal and work-optimal for any m = O(\nsolutions are deterministic and assume worst-case behavior. In the probabilistic setting CensorHillel [22] and Kentros et al. [24] show that optimal effectiveness can be achieved with expected\nwork complexity O(nm2 log m) and O(n + m2+\u01eb log n), for any small constant \u01eb, respectively.\nWe then demonstrate how to use the iterated version of our algorithm in order to solve the\nWrite-All problem with work complexity O(n+m3+\u01eb log n) for any constant \u01eb > 0. Our solution\nimproves on the algorithm of Malewicz [36], which solves the Write-All problem for a non-trivial\nnumber of processes with optimal (linear) work complexity, in two ways. Firstpour solution is\nwork optimal for a wider\nrange of choices for m, namely for any m = O( 3+\u01eb n/ log n), cf.\np\nthe restriction m = O( 4 n/ log n) of Malewicz, [36]. Second our solution does not assume the\ntest-and-set primitive used by Malewicz and relies only on atomic read/write memory. There is\n4\n\n\falso a Write-All algorithm due to Kowalski and Shvartsman [28], which does not use test-and-set\noperations and\u221ais work optimal for a wider range of processors m than our algorithm, specifically\nfor m = O( 2+\u01eb n). However, their algorithm uses a collection of q permutations with contention\nO(q log q) and it is not known to date how to construct such permutations in polynomial time (see\nthe discussion in the related work section). Finally, subsequent to the conference version of this\npaper [25], Alistarh et al. [2] show that there exists a deterministic algorithm for the Write-All\nproblem with work O(n + m log5 n log2 max(n, m)). Their solution is so far existential, while\nours explicit.\nOutline: In Section 2 we formalize the model and introduce definitions and notations used in the\npaper. In Section 3 we present the algorithm KK\u03b2 . In Sections 4 and 5 we analyze correctness,\neffectiveness and work complexity of algorithm KK\u03b2 . In Section 6 we present and analyze the\niterative algorithm IterativeKK (\u01eb). In Section 7 we present and analyze the iterative algorithm\nWA IterativeKK (\u01eb) for the Write-All problem. Finally, we conclude with Section 8.\n2. Model, Definitions, and Efficiency\nWe define our model, the at-most-once problem, and measures of efficiency.\n2.1. Model and Adversary\nWe model a multi-processor as m asynchronous, crash-prone processes with unique identifiers from some set P. Shared memory is modeled as a collection of atomic read/write memory\ncells, where the number of bits in each cell is explicitly defined. We use the Input/Output Automata formalism [34, 35] to specify and reason about algorithms; specifically, we use the asynchronous shared memory automaton formalization [17, 35]. Each process p is defined in terms of\nits states statesp and its actions actsp , where each action is of the type input, output, or internal.\nA subset startp \u2286 statesp contains all the start states of p. Each shared variable x takes values\nfrom a set Vx , among which there is initx , the initial value of x.\nWe model an algorithm A as a composition of the automata for each process p. Automaton\nA consists of a set of states states(A), where each state s contains a state sp \u2208 statesp for\neach p, and a value v \u2208 Vx for each shared variable x. Start states start(A) is a subset of\nstates(A), where each state contains a startp for each p and an initx for each x. The actions of\nA, acts(A) consists of actions \u03c0 \u2208 actsp for each process p. A transition is the modification of\nthe state as a result of an action and is represented by a triple (s, \u03c0, s\u2032 ), where s, s\u2032 \u2208 states(A)\nand \u03c0 \u2208 acts(A). State s is called the enabling state of action \u03c0. The set of all transitions is\ndenoted by trans(A). Each action in acts(A) is performed by a process, thus for any transition\n(s, \u03c0, s\u2032 ), s and s\u2032 may differ only with respect to the state sp of process p that invoked \u03c0 and\npotentially the value of the shared variable that p interacts with during \u03c0. We also use triples\n({varss }, \u03c0, {varss\u2032 }), where varss and varss\u2032 are subsets of variables in s and s\u2032 respectively,\nas a shorthand to describe transitions without having to specify s and s\u2032 completely; here varss\nand varss\u2032 contain only the variables whose value changes as the result of \u03c0, plus possibly some\nother variables of interest.\nAn execution fragment of A is either a finite sequence, s0 ,\u03c01 ,s1 , . . .,\u03c0r ,sr , or an infinite\nsequence, s0 ,\u03c01 ,s1 , . . .,\u03c0r ,sr ,. . ., of alternating states and actions, where (sk , \u03c0k+1 , sk+1 ) \u2208\ntrans(A) for any k \u2265 0. If s0 \u2208 start(A), then the sequence is called an execution. The set\nof executions of A is execs(A). We say that execution \u03b1 is fair, if \u03b1 is finite and its last state\nis a state of A where no locally controlled action is enabled, or \u03b1 is infinite and every locally\n5\n\n\fcontrolled action \u03c0 \u2208 acts(A) is performed infinitely many times or there are infinitely many\nstates in \u03b1 where \u03c0 is disabled. The set of fair executions of A is fairexecs(A). An execution\nfragment \u03b1\u2032 extends a finite execution fragment \u03b1 of A, if \u03b1\u2032 begins with the last state of \u03b1. We\nlet \u03b1 * \u03b1\u2032 stand for the execution fragment resulting from concatenating \u03b1 and \u03b1\u2032 and removing\nthe (duplicated) first state of \u03b1\u2032 .\nFor two states s and s\u2032 of an execution fragment \u03b1, we say that state s precedes state s\u2032 and\nwe write s < s\u2032 if s appears before s\u2032 in \u03b1. Moreover we write s \u2264 s\u2032 if state s either precedes\nstate s\u2032 in \u03b1 or the states s and s\u2032 are the same state of \u03b1. We use the term precedes and the\nsymbols < and \u2264 in a same way for the actions of an execution fragment. We use the term\nprecedes and the symbol < if an action \u03c0 appears before a state s in an execution fragment \u03b1 or\nif a state s appears before an action \u03c0 in \u03b1. Finally for a set of states S of an execution fragment\n\u03b1, we define as smax = max S the state smax \u2208 S, s.t. \u2200s \u2208 S, s \u2264 smax in \u03b1.\nWe model process crashes by action stopp in acts(A) for each process p. If stopp appears\nin an execution \u03b1 then no actions \u03c0 \u2208 actsp appear in \u03b1 thereafter. We then say that process p\ncrashed. Actions stopp arrive from some unspecified external environment, called an adversary.\nIn this work we consider an omniscient, on-line adversary [23] that has complete knowledge of\nthe algorithm executed by the processes. The adversary controls asynchrony and crashes. We\nallow up to f < m crashes. We denote by fairexecs f (A) all fair executions of A with at most\nf crashes. Note that since the processes can only communicate through atomic read/write operations in the shared memory, all the asynchronous executions are linearizable. This means that\nconcurrent actions can be mapped to an equivalent sequence of state transitions, where only one\nprocess performs an action in each transition, and thus the model presented above is appropriate\nfor the analysis of a multi-process asynchronous atomic read/write shared memory system.\n2.2. At-Most-Once Problem, Effectiveness and Complexity\nLet A be an algorithm specified for m processes with ids from set P = [1 . . . m], and for\nn jobs with unique ids from set J = [1 . . . n]. We assume that there are at least as many jobs\nas there are processes, i.e., n \u2265 m. We model the performance of job j by process p by means\nof action dop,j . For a sequence c, we let len(c) denote its length, and we \u0001let c|\u03c0 denote the\nsequence of elements \u03c0 occurring in c. Then for an execution \u03b1, len \u03b1|dop,j is the number of\ntimes process p performs job j. Finally we denote by F\u03b1 = {p|stopp occurs in \u03b1} the set of\ncrashed processes in execution \u03b1. Now we define the number of jobs performed in an execution.\nNote here that we are borrowing most definitions from Kentros et al. [26].\nDefinition 2.1. For execution \u03b1 let J\u03b1 = {j \u2208 J |dop,j occurs in \u03b1 for some p \u2208 P}. The total\nnumber of jobs performed in \u03b1 is defined to be Do(\u03b1) = |J\u03b1 |.\nWe next define the at-most-once problem.\nDefinition 2.2. P\nAlgorithm A solves\n\u0001 the at-most-once problem if for each execution \u03b1 of A we\nhave \u2200j \u2208 J : p\u2208P len \u03b1|dop,j \u2264 1.\n\nDefinition 2.3. Let S be a set of elements with unique identifiers. We define as the rank of\nelement x \u2208 S and we write [x]S , the rank of x if we sort in ascending order the elements of S\naccording to their identifiers.\nMeasures of Efficiency\nWe analyze our algorithms in terms of two complexity measures: effectiveness and work.\nEffectiveness counts the number of jobs performed by an algorithm in the worst case.\n6\n\n\fDefinition 2.4. EA (n, m, f ) = min\u03b1\u2208fairexecs f (A) (Do(\u03b1)) is the effectiveness of algorithm A,\nwhere m is the number of processes, n is the number of jobs, and f is the number of crashes.\nA trivial algorithm can solve the at-most-once problem by splitting the n jobs in groups of\nn\nsize m\nand assigning one group to each process. Such a solution has effectiveness E(n, m, f ) =\nn\n(consider an execution where f processes fail at the beginning of the execution).\n(m \u2212 f ) * m\nWork complexity measures the total number of basic operations (comparisons, additions,\nmultiplications, shared memory reads and writes) performed by an algorithm. We assume that\neach internal or shared memory cell has size O(log n) bits and performing operations involving\na constant number of memory cell costs O(1). This is consistent with the way work complexity\nis measured in previous related work [23, 28, 36].\nDefinition 2.5. The work of algorithm A, denoted by WA , is the worst case total number of\nbasic operations performed by all the processes of algorithm A.\nFinally we repeat here as a theorem, Corollary 1 from Kentros et al. [26], that gives an upper\nbound on the effectiveness for any algorithm solving the at-most-once problem.\nTheorem 2.1. from Kentros et al. [26]\nFor all algorithms A that solve the at-most-once problem with m processes and n \u2265 m jobs in\nthe presence of f < m crashes it holds that EA (n, m, f ) \u2264 n \u2212 f .\n3. Algorithm KK\u03b2\nWe present algorithm KK\u03b2 , that solves the at-most-once problem. Parameter \u03b2 \u2208 N is the\ntermination parameter of the algorithm. Algorithm KK\u03b2 is defined for all \u03b2 \u2265 m. If \u03b2 = m,\nalgorithm KK\u03b2 has optimal up to an additive factor of m effectiveness. Note that although\n\u03b2 \u2265 m is not necessary in order to prove the correctness of the algorithm, if \u03b2 < m we cannot\nguarantee termination of algorithm KK\u03b2 .\nShared Variables:\nnext = {next1 , . . . , nextm }, nextq \u2208 {0, . . . , n} initially 0\ndone = {done1,1 , . . . , donem,n }, doneq,i \u2208 {0, . . . , n} initially 0\nSignature:\nInput:\nstopp , p \u2208 P\nOutput:\ndop,j , p \u2208 P, j \u2208 J\nState:\nSTATUS p\n\nInternal:\ncompNextp , p \u2208 P\ncheck p , p \u2208 P\n\nInternal Read:\ngatherTryp , p \u2208 P\ngatherDonep , p \u2208 P\n\nInternal Write:\nsetNextp , p \u2208 P\ndonep , p \u2208 P\n\n\u2208 {comp next, set next, gather try, gather done, check, do, done, end, stop},\n\ninitially STATUS p = comp next\nFREEp , DONEp , TRY p \u2286 J , initially FREEp = J and DONEp = TRY p = \u2205\nPOS p\n\n= {POS p (1) , . . . , POS p (m)}, where POS p (i) \u2208 {1, . . . , n}, initially POS p (i) = 1\n\nNEXT p \u2208 {1, . . . , n}, initially undefined\nTMP p \u2208 {0, . . . , n}, initially undefined\n\nQp\n\n\u2208 {1, . . . , m}, initially 1\n\nFigure 1: Algorithm KK\u03b2 : Shared Variables, Signature and States\n\nThe idea behind the algorithm KK\u03b2 (see Fig. 1, 2) is quite intuitive and is based on an algorithm for renaming processes presented by Attiya et al. [4]. Each process p, picks a job i to\nperform, announces (by writing in shared memory) that it is about to perform the job and then\n7\n\n\fchecks if it is safe to perform it (by reading the announcements other processes made in the\nshared memory, and the jobs other processes announced they have performed). If it is safe to\nperform the job i, process p will proceed with the dop,i action and then mark the job completed.\nIf it is not safe to perform i, p will release the job. In either case, p picks a new job to perform.\nIn order to pick a new job, p reads from the shared memory and gathers information on which\njobs are safe to perform, by reading the announcements that other processes made in the shared\nmemory about the jobs they are about to perform, and the jobs other processes announced they\nhave already performed. Assuming that those jobs are ordered, p splits the set of \"free\" jobs in\nm intervals and picks the first job of the interval with rank equal to p's rank. Note that since the\ninformation needed in order to decide whether it is safe to perform a specific job and in order to\npick the next job to perform is the same, these steps are combined in the algorithm. In Figure\n2, we use function rank(SET1 , SET2 , i), that returns the element of set SET1 \\ SET2 that has\nrank i. If SET1 and SET2 have O(n) elements and are stored in some tree structure like redblack tree or some variant of B-tree, the operation rank(SET1 , SET2 , i), costs O(|SET2 | log n)\nassuming that SET2 \u2286 SET1 .\nWe will prove that algorithm KK\u03b2 has effectiveness n \u2212 (\u03b2 + m \u2212 2). For \u03b2 = O(m) this\neffectiveness is asymptotically optimal for any m = o(n). Note that by Theorem 2.1 the upper\nbound on effectiveness of the at-most-once problem is n \u2212 f , where f is the number of failed\nprocesses in the system. Next we present algorithm KK\u03b2 in more detail.\nShared Variables. next is an array with m elements. In the cell nextq of the array process q\nannounces the job it is about to perform. From the structure of algorithm KK\u03b2 , only process q\nwrites in cell nextq . On the other hand any process may read cell nextq .\ndone is an m \u00d7 n matrix. In line q of the matrix, process q announces the jobs it has performed. Each cell of line q contains the identifier of exactly one job that has been performed by\nprocess q. Only process q writes in the cells of line q but any process may read them. Moreover,\nprocess q updates line q by adding entries at the end of it.\nInternal Variables of process p. The variable STATUSp records the status of process p and\ndefines its next action as follows: STATUSp = comp next - process p is ready to compute the\nnext job to perform (this is the initial status of p), STATUSp = set next - p computed the next job\nto perform and is ready to announce it by writing in the shared memory, STATUSp = gather try\n- p reads the array next in shared memory in order to compute the TRY p set, STATUSp =\ngather done - p reads the matrix done in shared memory in order to update the DONEp and\nFREEp sets, STATUSp = check - p has to check whether it is safe to perform its current job,\nSTATUSp = do - p can safely perform its current job, STATUSp = done - p performed its current\njob and needs to update the shared memory, STATUSp = end - p terminated, STATUSp = stop p crashed.\nFREEp , DONEp , TRYp \u2286 J are three sets that are used by process p in order to compute\nthe next job to perform and whether it is safe to perform it. We use some tree structure like redblack tree or some variant of B-tree [5, 19] for the sets FREEp , DONEp and TRYp , in order to\nbe able to add, remove and search elements in them with O(log n) work. FREEp , is initially set\nto J and contains an estimate of the jobs that are still available. DONEp is initially empty and\ncontains an estimate of the jobs that have been performed. No job is removed from DONEp or\nadded to FREEp during the execution of algorithm KK\u03b2 . TRYp is initially empty and contains\nan estimate of the jobs that other processes are about to perform. It holds that |TRY p | < m,\nsince there are m \u2212 1 processes apart from process p that may be attempting to perform a job.\nPOS p is an array of m elements. Position POS p (q) of the array contains a pointer in the line\n8\n\n\fTransitions of process p:\nInput stopp\nEffect:\nSTATUS p \u2190 stop\nInternal compNextp\nPrecondition:\nSTATUS p = comp next\nEffect:\nif |FREEp \\ TRY p | \u2265 \u03b2 then\n|FREEp |\u2212(m\u22121)\nTMP p \u2190\nm\nif TMP p \u2265 1 then\nTMP p \u2190 \u230a(p \u2212 1) * TMP p \u230b + 1\nNEXT p \u2190 rank (FREEp , TRY p , TMP p )\nelse\nNEXT p \u2190 rank (FREEp , TRY p , p)\nend\nQp \u2190 1\nTRY p \u2190 \u2205\nSTATUS p \u2190 set next\nelse\nSTATUS p \u2190 end\nend\nInternal Write setNextp\nPrecondition:\nSTATUS p = set next\nEffect:\nnextp \u2190 NEXT p\nSTATUS p \u2190 gather try\nInternal Read gatherTryp\nPrecondition:\nSTATUS p = gather try\nEffect:\nif Qp 6= p then\nTMP p \u2190 nextQp\nif TMP p > 0 then\nTRY p \u2190 TRYp \u222a {TMP p }\nend\nend\nif Qp + 1 \u2264 m then\nQp \u2190 Q p + 1\nelse\nQp \u2190 1\nSTATUS p \u2190 gather done\nend\n\nInternal Read gatherDonep\nPrecondition:\nSTATUS p = gather done\nEffect:\nif Qp 6= p then\nTMP p \u2190 doneQ ,POS Q\np( p )\n\u0001 p\nif POS p Q p \u2264 n AND TMP p > 0\nthen\nDONEp \u2190 DONEp \u222a {TMP p }\nFREEp \u2190\n\u0001 FREEp \\ {\u0001TMP p }\nPOS p Q p = POS p Q p + 1\nelse Qp \u2190 Qp + 1\nend\nelse Qp \u2190 Qp + 1\nend\nif Qp > m then\nQp \u2190 1\nSTATUS p \u2190 check\nend\nInternal checkp\nPrecondition:\nSTATUS p = check\nEffect:\nif NEXT p \u2208\n/ TRY p AND NEXT p \u2208\n/ DONEp\nthen STATUS p \u2190 do\nelse\nSTATUS p \u2190 comp next\nend\nOutput dop,j\nPrecondition:\nSTATUS p = do\nNEXT p = j\nEffect:\nSTATUS p \u2190 done\nInternal Write donep\nPrecondition:\nSTATUS p = done\nEffect:\ndonep,POSp (p) \u2190 NEXT p\nDONEp \u2190 DONEp \u222a { NEXT p }\nFREEp \u2190 FREEp \\ {NEXT p }\nPOS p (p) \u2190 POS p (p) + 1\nSTATUS p \u2190 comp next\n\nFigure 2: Algorithm KK\u03b2 : Transitions\n\nq of the shared matrix done. POSp (q) is the element of line q that process p will read from. In\nthe special case where q = p, POSp (p) is the element of line p that process p will write into\nafter performing a new job. The elements of the shared matrix done are read when process p is\nupdating the DONEp set.\nNEXTp contains the job process p is attempting to perform.\nTMP p is a temporary storage for values read from the shared memory.\nQ p \u2208 {1, . . . , m} is used as indexing for looping through process identifiers.\nActions of process p. We visit them one by one below.\ncompNextp : Process p computes the set FREEp \\ TRYp and if it has more or equal elements\nto \u03b2, were \u03b2 is the termination parameter of the algorithm, process p computes its next candidate\njob, by splitting the FREEp \\ TRY p set in m parts and picking the first element of the p-th\n9\n\n\fpart. In order to do that it uses the function rank(SET1 , SET2 , i), which returns the element\nof set SET1 \\ SET2 with rank i. Finally process p sets the TRYp set to the empty set, the Qp\ninternal variable to 1 and its status to set next in order to update the shared memory with its\nnew candidate job. If the FREEp \\ TRYp set has less than \u03b2 elements process p terminates.\nsetNextp : Process p announces its new candidate job by writing the contents of its NEXTp\ninternal variable in the p-th position of the next array. Remember that the next array is stored in\nshared memory. Process p changes its status to gather try, in order to start collecting the TRY p\nset from the next array.\ngatherTryp : With this action process p implements a loop, which reads from the shared\nmemory all the positions of the array next and updates the TRYp set. In each execution of the\naction, process p checks if Qp is equal to p. If it is not equal, p reads the Qp -th position of the\narray next, checks if the value read is greater than 0 and if it is, adds the value it read in the\nTRY p set. If Qp is equal to p, p just skips the step described above. Then p checks if the value\nof Qp + 1 is less than m + 1. If it is, then p increases Qp by 1 and leaves its status gather try,\notherwise p has finished updating the TRY p set and thus sets Qp to 1 and changes its status to\ngather done, in order to update the DONEp and FREEp sets from the contents of the done\nmatrix.\ngatherDonep : With this action process p implements a loop, which updates the DONEp and\nFREEp sets with values read from the matrix done, which is stored in shared memory. In each\nexecution of the action,\nprocess p checks if Qp is equal to p. If it is not equal, p uses the internal\n\u0001\nvariable POSp Qp , in order to read fresh values from the line\u0001Qp of the done matrix. In detail, p\nreads the shared variable doneQp ,POSp (Qp ) , checks if POSp Qp is less than n + 1 and if the value\nread is greater than 0. If both conditions hold, p adds the value\n\u0001 read at the DONEp set, removes\nthe value read from the FREEp set and increases POSp Q p by one. Otherwise, it means that\neither process Qp has terminated (by performing all the n jobs) or the line Qp does not contain\nany new completed jobs. In either case p increases the value of Qp by 1. The value of Qp is\nincreased by 1 also if Qp was equal to p. Finally p checks whether Qp is greater than m; if it is,\np has completed the loop and thus changes its status to check.\ncheckp : Process p checks if it is safe to perform its current job. This is done by checking if\nNEXT p belongs to the set TRYp or to the set DONEp . If it does not, then it is safe to perform the\njob NEXTp and p changes its status to do. Otherwise it is not safe, and thus p changes its status\nto comp next, in order to find a new job that may be safe to perform.\ndop,j : Process p performs job j. Note that NEXTp = j is part of the preconditions for the\naction to be enabled in a state. Then p changes its status to done.\ndonep : Process p writes in the donep,POS p (p) position of the shared memory the value of\nNEXT p , letting other processes know that it performed job NEXTp . Also p adds NEXT p to its\nDONEp set, removes NEXTp from its FREEp set, increases POSp (p) by 1 and changes its status\nto comp next.\nstopp : Process p crashes by setting its status to stop.\n4. Correctness and Effectiveness Analysis\nWe begin the analysis of algorithm KK\u03b2 , by showing in Lemma 4.1 that KK\u03b2 solves the\nat-most-once problem. That is, there exists no execution of KK\u03b2 in which 2 distinct actions\ndop,i and doq,i appear for some i \u2208 J and p, q \u2208 P. We continue the analysis by showing in\nTheorem 4.4 that algorithm KK\u03b2 has effectiveness EKK\u03b2 (n, m, f ) = n \u2212 (\u03b2 + m \u2212 2). This is\n10\n\n\fdone in two steps. First in Lemma 4.2, we show that algorithm KK\u03b2 cannot terminate its execution if less than n \u2212 (\u03b2 + m \u2212 1) jobs are performed. The effectiveness analysis is completed\nby showing in Lemma 4.3, that the algorithm is wait-free (it has no infinite fair executions). In\nTheorem 4.4 we combine the two lemmas in order to show that the effectiveness of algorithm\nKK\u03b2 is greater that or equal to n \u2212 (\u03b2 + m \u2212 2). Moreover, we show the existence of an adversarial strategy, that results in a terminating execution where n\u2212(\u03b2 + m \u2212 2) jobs are completed,\nshowing that the bound is tight.\nIn the analysis that follows, for a state s and a process p we denote by\ns.FREEp , s.DONEp , s.TRYp , the values of the internal variables FREE, DONE and TRY of\nprocess p in state s. Moreover with s.next, and s.done we denote the contents of the array next\nand the matrix done in state s. Remember that next and done, are stored in shared memory.\nLemma 4.1. There exists no execution \u03b1 of algorithm KK\u03b2 , such that \u2203i \u2208 J and \u2203p, q \u2208 P\nfor which dop,i , doq,i \u2208 \u03b1.\nProof. Let us for the sake of contradiction assume that there exists an execution \u03b1 \u2208\nexecs(KK\u03b2 ) and i \u2208 J and p, q \u2208 P such that dop,i , doq,i \u2208 \u03b1. We examine two cases.\n\u0010\n\u0011\n\u2032\n\u2032\n\u2032\nCase 1 p = q: Let states s1 , s1 , s2 , s2 \u2208 \u03b1, such that the transitions s1 , dop,i , s1 ,\n\u0011\n\u0010\n\u2032\n\u2032\ns2 , dop,i , s2 \u2208 \u03b1 and without loss of generality assume s1 \u2264 s2 in \u03b1. From Figure 2 we\n\u2032\n\n\u2032\n\nhave that s1 .NEXTp = i, s1 .STATUSp = done and s2 .\u0010NEXTp = i, s2\u0011\n.STATUSp = do. From al\u2032\n\ngorithm KK\u03b2 , state s2 must be preceded by transition s3 , checkp , s3 , such that s3 .NEXTp = i\n\u2032\n\n\u2032\n\n\u2032\n\nand s3 .NEXTp = i, s3 .STATUS\np = do,\n\u0011 where s1\u2032 precedes s3 in \u03b1. Finally s3 must be pre\u0010\n\u2032\nceded in \u03b1 by transition s4 , donep , s4 , where s1 precedes s4 , such that s4 .NEXTp = i and\n\u2032\n\n\u2032\n\ni \u2208 s4 .DONEp . Since s4 precedes s3 and during the execution of KK\u03b2 no elements are removed from DONEp , we have that i \u2208 s3 .DONEp . This is a contradiction, since the transition\n({NEXTp = i, i \u2208 DONEp } , checkp , {NEXTp = i, STATUSp = do}) \u2208\n/ trans(KK\u03b2 ).\n\u0011\n\u0010\n\u2032\nCase 2 p 6= q: Given transition s1 , dop,i , s1 in execution \u03b1, we deduce from Fig. 2 that\n\u0011\n\u0011 \u0010\n\u0011\n\u0010\n\u0010\n\u2032\n\u2032\n\u2032\nthere exist in \u03b1 transitions s2 , setNextp , s2 , s3 , gatherTryp , s3 , s4 , checkp , s4 , where\n\u2032\n\n\u2032\n\n\u2032\n\ns2 .nextp = s2 .NEXTp = i, s3 .nextp = s3 .NEXTp = i, s3 .Qp = q, s4 .NEXTp = i, s4 .NEXTp =\n\u2032\ni, s4 .STATUSp = do, such that s2 < s3 < s4 < s1 and there exists no action \u03c0 = compNextp in\n\u2032\nexecution \u03b1, such that s2 < \u03c0 < \u0010\ns1 .\n\u0011\n\u2032\nSimilarly for transition\nt1 , doq,i , t1\nthere exist in execution \u03b1 transitions\n\u0011 \u0010\n\u0011 \u0010\n\u0011\n\u0010\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\nt2 , setNextq , t2 , t3 , gatherTryq , t3 , t4 , checkq , t4 , where t2 .nextq = t2 .NEXTq = i,\n\u2032\n\n\u2032\n\nt3 .nextq = t3 .NEXTq = i, t3 .Qq = p, t4 .NEXTq = i, t4 .NEXTq = i, t4 .STATUSq = do, such\nthat t2 < t3 < t4 < t1 and there exists no action \u03c0 \u2032 = compNextq in execution \u03b1, such that\n\u2032\nt2 < \u03c0 < t1 .\nEither state s2 < t3 or t3 < s2 which implies t2 < s3 . We will show that if s2 < t3 then\ndoq,i cannot take place, leading to a contradiction. The case where t2 < s3 is symmetric and\nwill be omitted.\nLet us assume that s2 precedes t3 . We have two cases, either t3 .nextp = i or t3 .nextp 6= i.\n\u2032\nIn the first case i \u2208 t3 .TRY q . The only action in which entries are removed from the TRY q\n11\n\n\fset, is action compNextq , where the TRY q set is reset to \u2205. Thus i \u2208 t4 .TRYq , since \u2204 \u03c0 \u2032 =\n\u0011\n\u0010\n\u2032\n/\ncompNextq \u2208 \u03b1, such that t2 < \u03c0 \u2032 < t1 . This is a contradiction since t4 , checkq , t4 \u2208\n\u2032\n\ntrans(KK\u03b2 ), if i \u2208 t4 .TRY q\u0010, t4 .NEXTq = i and\n\u0011 t4 .STATUSq = do.\nIf t3 .nextp 6= i, since\n\n\u2032\n\n\u2032\n\n\u2208 \u03b1 and s2 < t3 there exists action \u03c01 =\n\ns2 , setNextp , s2\n\n\u2032\n\nsetNextp \u2208 \u03b1, such that s2 < \u03c01 < t3 . Moreover, there exists action \u03c02 = compNextp in\n\u2032\n\u2032\n\u03b1, such that s2 < \u03c02 < \u03c01 . Since \u2204 \u03c0 = compNextp \u2208 \u03b1, such that s2 < \u03c0 < s1 , it holds\n\u0011\n\u0010\n\u2032\n\u2032\nthat s1 < \u03c02 < \u03c01 < t3 . Furthermore, from Fig. 2 there exists transition s5 , donep , s5 in \u03b1\n\u2032\n\nand j \u2208 {1, . . . , n}, such that s5 .POSp (p) = j, s5 .donep,j = 0, s5 .NEXTp = i, s5 .donep,j = i\n\u2032\n\u2032\nand s1 < s5 < \u03c02 < t3 . It must be the case that i \u2208\n/ t2 .DON\nEq , since t2 .NEXT\n\u0010\n\u0011 q = i. From\n\u2032\n\nthat and from Fig. 2 we have that there exists transition t6 , gatherDoneq , t6 in \u03b1, such that\n\u2032\n\nt6 .Qq = p, t6 .POS q (p) = j and t3 < t6 < t4 . Since s5 < t3 and donep,j from algorithm\nKK\u03b2 cannot be changed again in execution \u03b1, we have that t6 .donep,j = i and as a result\n\u2032\ni \u2208 t6 .DONEq . Moreover, during the execution of algorithm KK\u03b2 , entries in set DONEq are\nonly added and\u0011never removed, thus we have that i \u2208 t4 .DON Eq . This is a contradiction since\n\u0010\n\u2032\n\n\u2032\n\n/ trans(KK\u03b2 ), if i \u2208 t4 .DONEq , t4 .NEXTq = i and t4 .STATUSq = do. This\nt4 , checkq , t4 \u2208\ncompletes the proof.\n\u0003\nNext we examine the effectiveness of the algorithm. First we show that algorithm KK\u03b2\ncannot terminate its execution if less than n \u2212 (\u03b2 + m \u2212 1) jobs are performed.\nLemma 4.2. For any \u03b2 \u2265 m, f \u2264 m \u2212 1 and for any finite execution \u03b1 \u2208 execs (KK\u03b2 )\nwith Do(\u03b1) \u2264 n \u2212 (\u03b2 + m \u2212 1), there exists a (non-empty) execution fragment \u03b1\u2032 such that\n\u03b1 * \u03b1\u2032 \u2208 execs (KK\u03b2 ).\n\nProof. From the algorithm KK\u03b2 , we have that for any process p and any state s \u2208 \u03b1,\n|s.FREEp | \u2265 n \u2212 Do(\u03b1) and |s.TRY p | \u2264 m \u2212 1. The first inequality holds since the s.FREEp\nset is estimated by p by examining the done matrix which is stored in shared memory. From\nalgorithm KK\u03b2 , a job j is only inserted in line q of the matrix done, if a doq,j action has already\nbeen performed by process q. The second inequality is obvious. Thus we have that \u2200p \u2208 P\nand \u2200s \u2208 \u03b1, |s.FREEp \\ s.TRYp | \u2265 n \u2212 (Do(\u03b1) + m \u2212 1). If Do(\u03b1) \u2264 n \u2212 (\u03b2 + m \u2212 1),\n\u2200p \u2208 P and \u2200s \u2208 \u03b1 we have that |s.FREEp \\ s.TRYp | \u2265 \u03b2. Since there can be f \u2264 m \u2212 1\nfailed processes in our system, at the final state s\u2032 of execution \u03b1 there exists at least one process\np \u2208 P that has not failed. This process has not terminated, since from Fig. 2 a process p can\nonly terminate if in the enabling state s of action compNextp , |s.FREEp \\ s.TRYp | < \u03b2. This\nprocess can continue executing steps and thus there exists a (non-empty) execution fragment \u03b1\u2032\nsuch that \u03b1 * \u03b1\u2032 \u2208 execs (KK\u03b2 ).\n\u0003\nSince no finite execution of algorithm KK\u03b2 can terminate if less than n \u2212 (\u03b2 + m \u2212 1) jobs\nare performed, Lemma 4.2 implies that if the algorithm KK\u03b2 has effectiveness less than or equal\nto n\u2212(\u03b2 +m\u22121), there must exist some infinite fair execution \u03b1 with Do(\u03b1) \u2264 n\u2212(\u03b2 +m\u22121).\nNext we prove that algorithm KK\u03b2 is wait-free (it has no infinite fair executions).\nLemma 4.3. For any \u03b2 \u2265 m, f \u2264 m\u22121 there exists no infinite fair execution \u03b1 \u2208 execs(KK\u03b2 ).\n12\n\n\fProof. We will prove this by contradiction. Let \u03b2 \u2265 m and \u03b1 \u2208 execs(KK\u03b2 ) an infinite fair\nexecution with f \u2264 m \u2212 1 failures, and let Do(\u03b1) be the jobs executed by execution \u03b1 according\nto Definition 2.1. Since \u03b1 \u2208 execs(KK\u03b2 ) and from Lemma 4.1 KK\u03b2 solves the at-most-once\nproblem, Do(\u03b1) is finite. Clearly there exists at least one process in execution \u03b1 that has not\ncrashed and does not terminate (some process must take steps in \u03b1 in order for it to be infinite).\nSince Do(\u03b1) and f are finite, there exists a state s0 in \u03b1 such that after s0 no process crashes,\nno process terminates, no do action takes place in \u03b1 and no process adds new entries in the done\nmatrix in shared memory. The later holds since the execution is infinite and fair, the Do(\u03b1) is also\nfinite, consequently any non failed process q that has not terminated will eventually update the\nq line of the done matrix to be in agreement with the doq,\u2217 actions it has performed. Moreover\nany process q that has terminated, has already updated the q line of done matrix with the latest\ndo action it performed, before it terminated, since in order to terminate it must have reached a\ncompNext action that has set its status to end.\nWe define the following sets of processes and jobs according to state s0 . J\u03b1 are jobs\nthat have been performed in \u03b1 according to Definition 2.1. P\u03b1 are processes that do not\ncrash and do not terminate in \u03b1. By the way we defined state s0 only processes in P\u03b1\ntake steps in \u03b1 after state s0 . STUCK\u03b1 = {i \u2208 J \\ J\u03b1 |\u2203 failed process p : s0 .nextp = i},\ni.e., STUCK\u03b1 expresses the set of jobs that are held by failed processes. DONE\u03b1 =\n{i \u2208 J\u03b1 |\u2203p \u2208 P and j \u2208 {1, . . . , n} : s0 .donep (j) = i}, i.e., DONE\u03b1 expresses the set of jobs\nthat have been performed before state s0 and the processes that performed them managed to update the shared memory. Finally we define POOL\u03b1 = J \\ (J\u03b1 \u222a STUCK\u03b1 ). After state s0 , all\nprocesses in P\u03b1 will keep executing. This means that whenever a process p \u2208 P\u03b1 takes action\ncompNextp in \u03b1, the first if statement is true. Specifically it holds that for \u2200p \u2208 P\u03b1 and for all\nthe enabling states s \u2265 s0 of actions compNextp in \u03b1, |FREEp \\ TRYp | \u2265 \u03b2.\nFrom Figure 2, we have that for any p \u2208 P\u03b1 , \u2203 sp \u2208 \u03b1 such that sp > s0 and for all states\ns \u2265 sp , s.DONEp = DONE\u03b1 , s.FREEp = J \\ DONE\u03b1 and s.FREEp \\ s.TRYp \u2286 POOL\u03b1 .\nLet s\u20320 = maxp\u2208P\u03b1 [sp ]. From the above we have: |J \\ DONE\u03b1 | \u2265 \u03b2 \u2265 m and |POOL\u03b1 | \u2265\n\u03b2 \u2265 m, since \u2200s\u2032 \u2265 s\u20320 we have that s\u2032 .FREEp = J \\ DONE\u03b1 and s\u2032 .FREEp \\ s\u2032 .TRYp \u2286\nPOOL\u03b1 and \u2200p \u2208 P\u03b1 and for all the enabling states s \u2265 s\u20320 of actions compNextp in \u03b1, we have\nthat |FREEp \\ TRYp | \u2265 \u03b2.\nLet p0 be the process with the smallest process identifier in P\u03b1 . We examine 2 cases according to the size of J \\ DONE\u03b1 .\nCase A |J \\ DONE\u03b1 | \u2265 2m \u2212 1: Let x0 \u2208 POOL\u03b1 be the job such that [x0 ]POOL\u03b1 =\nk\nj\n(p0 \u2212 1) * |J \\DONEm\u03b1 |\u2212(m\u22121) + 1. Such x0 exists since \u2200p \u2208 P\u03b1 and \u2200s \u2265 s\u20320 it holds\ns.FREEp \\s.TRYp \u2286 POOL\u03b1 , s.FREEp = J \\DONE\u03b1 from which we have that |POOL\u03b1 | \u2265\n|J \\ DONE\u03b1 | \u2212 |s.TRY p | \u2265 |J \\ DONE\u03b1 | \u2212 (m \u2212 1) \u2265 m.\nIt follows that any p \u2208 P\u03b1 that executes action compNextp after state s\u20320 , will have its NEXTp\nk\nj\nvariable pointing in a job x with [x]POOL\u03b1 \u2265 (p \u2212 1) * |J \\DONEm\u03b1 |\u2212(m\u22121) +1. Thus \u2200p \u2208 P\u03b1 ,\nk\nj\n\u2203 s\u2032p \u2265 s\u20320 in \u03b1 such that \u2200 states s \u2265 s\u2032p , [s.nextp ]POOL\u03b1 \u2265 (p \u2212 1) * |J \\DONEm\u03b1 |\u2212(m\u22121) + 1.\nLet s\u2032\u20320 = maxp\u2208P\u03b1 [s\u2032p ], we have 2 cases for p0 :\nCase A.1) After s\u2032\u20320 , process p0 executes action compNextp0 and the transition leads to state\nj\nk\ns1 > s\u2032\u20320 such that s1 .NEXTp0 = x0 . Since [x0 ]POOL\u03b1 = (p0 \u2212 1) * |J \\DONEm\u03b1 |\u2212(m\u22121) + 1\nand p0 = minp\u2208P\u03b1 [p], from the previous discussion we have that \u2200s \u2265 s1 and \u2200p \u2208 P \\ {p0 },\ns.nextp 6= x0 . Thus when p0 executes action checkp of Fig. 2 for the first time after state s1 , the\n13\n\n\fcondition will be true, so in some subsequent transition p0 will have to execute action dop0 ,x0 ,\nperforming job x0 , which is a contradiction, since after state s0 no jobs are executed.\nCase A.2) After s\u2032\u20320 , process p0 executes action compNextp0 and the transition leads\nin state s1 > s\u2032\u20320 such that s1 .NEXTp0j > x0 . Since p0 = minp\u2208P\nk \u03b1 [p], it holds that\n\n(p0 \u2212 1) * |J \\DONEm\u03b1 |\u2212(m\u22121) + 1, \u2204p \u2208 P such\n\u0011\n\u0010\n\u2032\nthat s1 .nextp = x. Let the transition s2 , compNextp0 , s2 \u2208 \u03b1, where s2 > s1 ,\nbe the first time that action compNextp0 is executed after state s1 . We have that \u2200x \u2208\nj\nk\nPOOL\u03b1 such that [x]POOL\u03b1 \u2264 (p0 \u2212 1) * |J \\DONEm\u03b1 |\u2212(m\u22121) + 1, x \u2208\n/ s2 .DONEp0 \u222a\n\u2200x \u2208 POOL\u03b1 such that [x]POOL\u03b1 \u2264\n\ns2 .TRY p0 , since fromj the discussion above we have\nk that \u2200s \u2265 s1 and \u2200p \u2208 P\u03b1 \\ {p0 },\n|J \\DONE\u03b1 |\u2212(m\u22121)\n[s.nextp ]POOL\u03b1 \u2265 (p \u2212 1) *\n+ 1. Thus [x0 ]s2 .FREEp \\s2 .TRYp =\nm\n0\n0\nk\nj\n\u2032\n|J \\DONE\u03b1 |\u2212(m\u22121)\n+ 1. As a result, s2 .NEXTp0 = x0 . With sim[x0 ]POOL\u03b1 = (p0 \u2212 1) *\nm\nilar arguments like in case A.1, we can see that job x0 will be performed by process p0 , which is\na contradiction, since after state s0 no jobs are executed.\nCase B |J \\ DONE\u03b1 | < 2m \u2212 1: Let x0 \u2208 POOL\u03b1 be the job such that [x0 ]POOL\u03b1 = p0 .\nSuch x0 exists since \u03b2 \u2265 m and POOL\u03b1 \u2265 \u03b2. It follows that any p \u2208 P\u03b1 that executes action\ncompNextp after state s\u20320 , will have its NEXTp variable pointing in a job x with [x]POOL\u03b1 \u2265 p.\nThus \u2200p \u2208 P\u03b1 , \u2203 s\u2032p \u2265 s\u20320 in \u03b1 such that \u2200 states s \u2265 s\u2032p , [s.nextp ]POOL\u03b1 \u2265 p. Let s\u2032\u20320 =\nmaxp\u2208P\u03b1 [s\u2032p ], we have 2 cases for p0 :\nCase B.1) After s\u2032\u20320 , process p0 executes action compNextp0 and the transition leads in state\ns1 > s\u2032\u20320 such that s1 .NEXTp0 = x0 . Since [x0 ]POOL\u03b1 = p0 and p0 = minp\u2208P\u03b1 [p], from the\nprevious discussion we have that \u2200s \u2265 s1 and \u2200p \u2208 P \\ {p0 }, s.nextp 6= x0 . Thus when p0\nexecutes action checkp of Fig. 2 for the first time after state s1 , the condition will be true, so in\nsome subsequent transition p0 will have to execute action dop0 ,x0 , performing job x0 , which is a\ncontradiction, since after state s0 no jobs are executed.\nCase B.2) After s\u2032\u20320 , process p0 executes action compNextp0 and the transition leads in state\ns1 > s\u2032\u20320 such that s1 .NEXTp0 > x0 . Since p0 = minp\u2208P\u03b1 [p], it holds \u0010that \u2200x \u2208 POOL\u03b1 such\n\u0011\n\u2032\n\nthat [x]POOL\u03b1 \u2264 p0 , \u2204p \u2208 P such that s1 .nextp = x. Let the transition s2 , compNextp0 , s2 \u2208\n\u03b1, where s2 > s1 , be the first time that action compNextp0 is executed after state s1 . We have\n/ s2 .DONEp0 \u222a s2 .TRYp0 , since from the\nthat \u2200x \u2208 POOL\u03b1 such that [x]POOL\u03b1 \u2264 p0 , x \u2208\ndiscussion above we have that \u2200s \u2265 s1 and \u2200p \u2208 P\u03b1 \\ {p0 }, [s.nextp ]POOL\u03b1 \u2265 p. Thus\n\u2032\n[x0 ]s2 .FREEp \\s2 .TRYp = [x0 ]POOL\u03b1 = p0 . As a result, s2 .NEXTp0 = x0 . With similar ar0\n0\nguments like in case B.1, we can see that job x0 will be performed by process p0 , which is a\ncontradiction, since after state s0 no jobs are executed.\n\u0003\nWe combine the last two lemmas in order to show the main result on the effectiveness of\nalgorithm KK\u03b2 .\n\nTheorem 4.4. For any \u03b2 \u2265 m, f \u2264 m \u2212 1 algorithm KK\u03b2 has effectiveness EKK\u03b2 (n, m, f ) =\nn \u2212 (\u03b2 + m \u2212 2).\nProof. From Lemma 4.2 we have that any finite execution \u03b1 \u2208 execs (KK\u03b2 ) with Do(\u03b1) \u2264\nn \u2212 (\u03b2 + m \u2212 1) can be extended, essentially proving that in such executions no process has\nterminated. Moreover from Lemma 4.3 we have that KK\u03b2 is wait free, and thus there exists no\ninfinite fair execution \u03b1 \u2208 execs (KK\u03b2 ), such that Do(\u03b1) \u2264 n \u2212 (\u03b2 + m \u2212 1). Since finite\n14\n\n\ffair executions are executions where all non-failed processes have terminated, from the above\nwe have that EKK\u03b2 (n, m, f ) \u2265 n \u2212 (\u03b2 + m \u2212 2).\nIf all processes but the process with id m fail in an execution \u03b1 in such a way that\nJ\u03b1 \u2229 STUCK\u03b1 = \u2205 and |STUCK\u03b1 | = m \u2212 1 (where STUCK\u03b1 is defined as in the proof\nof Lemma 4.3), it is easy to see that there exists an adversarial strategy, such that when\nprocess m terminates, \u03b2 + m \u2212 2 jobs have not been performed . Such an execution will\nbe a finite fair execution where n \u2212 (\u03b2 + m \u2212 2) jobs are performed. Thus we have that\nEKK\u03b2 (n, m, f ) = n \u2212 (\u03b2 + m \u2212 2).\n\u0003\n5. Work Complexity Analysis\nIn this section we are going to prove that for \u03b2 \u2265 3m2 algorithm KK\u03b2 has work complexity\nO(nm log n log m).\nThe main idea of the proof, is to demonstrate that under the assumption \u03b2 \u2265 3m2 , process\ncollisions on a job cannot accrue without making progress in the algorithm. In order to prove that,\nwe first demonstrate in Lemma 5.1 that if two different processes p, q set their NEXTp , NEXTq\ninternal variables to the same job i in some compNext actions, then the DONEp and DONEq sets\nof the processes, have at least |q \u2212 p| * m different elements, given that \u03b2 \u2265 3m2 . Next we prove\nin Lemma 5.4 that if two processes p, q collide three consecutive times, while trying to perform\nsome jobs, the size of the set DONEp \u222a DONEq that processes p and q know will increase by\nat least |q \u2212 p| * m elements. This essentially tells us that every three collisions between the\nsame two processes a significant number of jobs has been performed, and thus enough progress\nhas been made. In order to prove the above statement, we formally define what we mean by\ncollision in Definition 5.2, and tie such a collision with some specific state, the state the collision\nis detected, so that we have a fixed \"point of reference\" in the execution; and show that the order\ncollisions are detected in an execution, is consistent with the order the involved processes attempt\nto perform the respective jobs in Lemmas 5.2, 5.3. Finally we use Lemma 5.4,\nm to prove\nl in order\nn\ntimes in\nin Lemma 5.5, that a process p cannot collide with a process q more than 2 m*|q\u2212p|\nany execution.\nl This ismproven by contradiction, showing that if process p collides with process q\nn\nmore than 2 m*|q\u2212p|\ntimes, there exist states for which the set |DONEp \u222a DONEq | has more\nthan n elements which is impossible. Lemma 5.5 is used in order to prove the main result on\nthe work complexity of algorithm KK\u03b2 for \u03b2 \u2265 3m2 , Theorem 5.6. We obtain Theorem 5.6 by\ncounting the total number of collisions that can happen and the cost of each collision.\nWe start by defining the notion of immediate predecessor transition for a state s in an execution \u03b1. The immediate predecessor is the last transition of a specific action type that precedes\nstate s in the execution. This is particularly useful in uniquely identifying the transition with\naction compNextp in an execution, that last set a NEXTp internal variable to a specific value,\ngiven a state s of interest.\n\u0010\n\u0011\n\u2032\nDefinition 5.1. We say that transition s1 , \u03c01 , s1 is an immediate predecessor of state s2 in\n\u0011\n\u0010\n\u2032\n\u2032\nan execution \u03b1 \u2208 execs(KK\u03b2 ) and we write s1 , \u03c01 , s1 7\u2192 s2 , if s1 < s2 and in the execution\n\u2032\n\nfragment \u03b1\u2032 that begins with state s1 and ends with state s2 , there exists no action \u03c03 = \u03c01 .\n\nNext we define what a collision between two processes means. We say that process p collided\nwith process q in job i at state s, if process p attempted to preform job i, but was not able to,\n15\n\n\fbecause it detected in state s that either process q was trying to perform job i or process q has\nalready performed job i.\nDefinition 5.2. In an execution \u03b1 \u2208 execs(KK\u03b2 ), we\nwith process\n\u0011\n\u0010\n\u0010 say that process\u2032 \u0011p collided\n\u2032\nq in job i at state s, if (i) there exist in \u03b1 transitions s1 , compNextp , s1 , t1 , compNextq , t1\n\u0011\n\u0011\n\u0010\n\u0010\n\u2032\n\u2032\n\u2032\n\u2032\nand s2 , checkp , s2 , where s1 , compNextp , s1 7\u2192 s2 , t1 < s2 and s1 .NEXTp = t1 .NEXTq =\n\u2032\n\n\u2032\n\n\u2032\n\ns2 .NEXTp = i, s1 .STATUSp = t1 .STATUSq = set next, s2 .STATUS\n(ii) in\n\u0011\n\u0010 p = comp next,\n\u2032\n\u2032\n\u2032\nexecution fragment \u03b1 = s1 , . . . , s2 either there exists transition s, gatherTryp , s such that\n\u0011\n\u0010\n\u2032\ns.Qp = q, s.nextq = i, or transition s, gatherDonep , s and j \u2208 {1, . . . , n} such that s.Qp =\nq, s.POSp (q) = j, s.doneq,j = i and i \u2208\n/ s.TRY p .\nDefinition 5.3. In an execution \u03b1 \u2208 execs(KK\u03b2 ), we say that processes p, q collide in job i at\nstate s, if process p collided with process q or process q collided with process p in job i at state\ns, according to Definition 5.2.\nNext we show that if two processes p, q decide, with some compNext actions, to perform\nthe same job i, then their DONE sets at the enabling states of those compNext actions, differ in\nat-least |q \u2212 p| * m elements.\nLemma 5.1. If \u03b2 \u2265 3m2 and in an execution \u03b1 \u2208 execs(KK\u03b2 ) there exist states s1 , t1 and processes p, q \u2208 P with\u0011p < q such\nt1 .NEXTq = i \u2208 J , then there exist transitions\n\u0010 that s1 .NEXTp = \u0011\n\u0010\n\u2032\n\ns2 , compNextp , s2\n\n\u2032\n\n\u2032\n\n\u2032\n\n7\u2192 s1 , t2 , compNextq , t2\n\n\u2032\n\n\u2032\n\n7\u2192 t1 , where s2 .NEXTp = t2 .NEXTq = i,\n\ns2 .STATUSp = t2 .STATUSq = set next and s2 .DONEp \u2229 t2 .DONEq > (q \u2212 p) * m or\ns2 .DONEp \u2229 t2 .DONEq > (q \u2212 p) * m .\nProof.\u0010 We will prove this\nFrom algorithm\nKK\u03b2 there must exist transi\u0011\n\u0011 by contradiction.\n\u0010\n\u2032\n\u2032\n\u2032\ntions s2 , compNextp , s2 7\u2192 s1 and t2 , compNextq , t2 7\u2192 t1 , where s2 .NEXTp = i and\n\u2032\n\nt2 .NEXTq = i, if there exist s1 , t1 \u2208 \u03b1 and p, q \u2208 P with p < q such that s1 .NEXTp =\nt1 .NEXTq = i \u2208 J , since those are the transitions that set NEXTp and NEXTq to i. In\norder to get a contradiction we assume that s2 .DONEp \u2229 t2 .DONEq \u2264 (q \u2212 p) * m and\ns2 .DONEp \u2229 t2 .DONEq \u2264 (q \u2212 p) * m. We will prove that if this is the case, then\n\u2032\n\u2032\ns2 .NEXTp 6= t2 .NEXTq .\nLet A = J \\ s2 .DONEp = s2 .FREEp and B = J \\ t2 .DONEq = t2 .FREEq , thus from\nthe contradiction assumption we have that: A \u2229 B \u2264 (q \u2212 p) * m and A \u2229 B \u2264 (q \u2212 p) * m.\nIt could either be that |A| < |B| or |A| \u2265 |B|.\nCase 1 |A| < |B|: From the contradiction assumption we have that A \u2229 B \u2264 (q \u2212 p) * m.\nSince s2 .FREEp \\ s2 .TRYp can have up to m \u2212 1 fewer elements than A \u2013 the elements of set\ns2 .TRY p \u2013 and it can be the case that s2 .TRY p \u2229 t2 .TRYq = \u2205, we have:\n|t2 .FREEq \\ t2 .TRYq \u2229 s2 .FREEp \\ s2 .TRYp | \u2264 m(q \u2212 p) + m \u2212 1\n\n(1)\n\nMoreover, since s2 .FREEp \\ s2 .TRY p \u2286 A and |s2 .FREEp \\ s2 .TRYp | \u2265 \u03b2 \u2265 3m2 , |A| \u2265\n3m2 . Similarly |B| \u2265 3m2 . We have:\n(q \u2212 1)\n\n|B|\n|B|\n|A|\n|B|\n|B|\n= (p \u2212 1)\n+ (q \u2212 p)\n> (p \u2212 1)\n+ (q \u2212 p)\n\u21d2\nm\nm\nm\nm\nm\n16\n\n\f|B|\n|A|\n> (p \u2212 1)\n+ 3m(q \u2212 p) \u21d2\nm\nm\n|A|\n|B|\n> (p \u2212 1)\n+ (3m \u2212 1)(q \u2212 p) + (q \u2212 p) \u21d2\n\u21d2 (q \u2212 1)\nm\nm\n|B|\n|A|\n(q \u2212 p)(m \u2212 1)\n\u21d2 (q \u2212 1)\n> (p \u2212 1)\n+ (3m \u2212 1)(q \u2212 p) +\n\u21d2\nm\nm\nm\n\u0016\n\u0017\n\u0016\n\u0017\n|B| \u2212 (m \u2212 1)\n|A| \u2212 (m \u2212 1)\n(q \u2212 1)\n+ 1 \u2265 (p \u2212 1)\n+ 1 + (3m \u2212 1)(q \u2212 p)\nm\nm\n\u21d2 (q \u2212 1)\n\n\u2032\n\n(2)\n\n\u2032\n\nSince s2 .NEXTp = t2 .NEXTq = i, we have:\n[i]s2 .FREEp \\s2 .TRYp\n[i]t2 .FREEq \\t2 .TRYq\n\n\u0016\n\n\u0017\n|A| \u2212 (m \u2212 1)\n= (p \u2212 1)\n+1\nm\n\u0017\n\u0016\n|B| \u2212 (m \u2212 1)\n+1\n= (q \u2212 1)\nm\n\nEquation 2 becomes:\n[i]t2 .FREEq \\t2 .TRYq \u2265 [i]s2 .FREEp \\s2 .TRYp + (3m \u2212 1)(q \u2212 p)\nThus set t2 .FREEq \\ t2 .TRYq must have at least (3m \u2212 1)(q \u2212 p) more elements with rank less\nthat the rank of i, than set s2 .FREEp \\ s2 .TRY p does. This is a contradiction since from eq. 1\nwe have that:\n|t2 .FREEq \\ t2 .TRYq \u2229 s2 .FREEp \\ s2 .TRYp | \u2264 m(q \u2212 p) + m \u2212 1\nCase 2 |B| \u2264 |A|: We have that A \u2229 B \u2264 (q \u2212 p) * m and A \u2229 B \u2264 (q \u2212 p) * m from the\ncontradiction assumption. Since s2 .FREEp \\ s2 .TRYp can have up to m \u2212 1 less elements than\nA \u2013 the elements of set s2 .TRY p \u2013 and it can be the case that s2 .TRYp \u2229 t2 .TRY p = \u2205, we\nhave:\n|t2 .FREEq \\ t2 .TRYq \u2229 s2 .FREEp \\ s2 .TRYp | \u2264 m(q \u2212 p) + m \u2212 1\n(3)\nFrom the contradiction assumption and the case 2 assumption we have that |B| \u2264 |A| \u2264 |B| +\n(q \u2212 p) * m. Moreover |A| \u2265 \u03b2 \u2265 3m2 and |B| \u2265 \u03b2 \u2265 3m2 . We have:\n(q \u2212 1)\n\n|B| + (q \u2212 p)m\n|B| + (q \u2212 p)m\n|B| + (q \u2212 p)m\n= (p \u2212 1)\n+ (q \u2212 p)\n\u2265\nm\nm\nm\n\n|B| + (q \u2212 p)m\n|A|\n|A|\n+ (q \u2212 p)\n\u2265 (p \u2212 1)\n+ 3m (q \u2212 p) + (q \u2212 p)2 \u21d2\nm\nm\nm\n|B|\n|A|\n2\n\u21d2 (q \u2212 1)\n\u2265 (p \u2212 1)\n+ 3m (q \u2212 p) + (q \u2212 p) \u2212 (q \u2212 1)(q \u2212 p) \u21d2\nm\nm\n|A|\n|B|\n\u2265 (p \u2212 1)\n+ (3m \u2212 p + 1) (q \u2212 p) \u21d2\n\u21d2 (q \u2212 1)\nm\nm\n|A|\n|B|\n\u2265 (p \u2212 1)\n+ (2m + 2) (q \u2212 p) \u21d2\n\u21d2 (q \u2212 1)\nm\nm\n17\n\n\u2265 (p \u2212 1)\n\n\f|B|\n|A|\n(q \u2212 p) (m \u2212 1)\n\u2265 (p \u2212 1)\n+ (2m + 1) (q \u2212 p) +\n\u21d2\nm\nm\nm\n\u0016\n\u0017\n\u0016\n\u0017\n|B| \u2212 (m \u2212 1)\n|A| \u2212 (m \u2212 1)\n(q \u2212 1)\n+ 1 \u2265 (p \u2212 1)\n+ 1 + (2m + 1)(q \u2212 p)\nm\nm\n\u21d2 (q \u2212 1)\n\n\u2032\n\n(4)\n\n\u2032\n\nSince s2 .NEXTp = t2 .NEXTq = i, we have:\n[i]s2 .FREEp \\s2 .TRYp\n[i]t2 .FREEq \\t2 .TRYq\n\n\u0016\n\n\u0017\n|A| \u2212 (m \u2212 1)\n= (p \u2212 1)\n+1\nm\n\u0016\n\u0017\n|B| \u2212 (m \u2212 1)\n= (q \u2212 1)\n+1\nm\n\nEquation 4 becomes:\n[i]t2 .FREEq \\t2 .TRYq \u2265 [i]s2 .FREEp \\s2 .TRYp + (2m + 1)(q \u2212 p)\nThus set t2 .FREEq \\ t2 .TRYq must have at least (2m + 1)(q \u2212 p) more elements with rank less\nthat the rank of i, than set s2 .FREEp \\ s2 .TRYp . This is a contradiction since from eq. 3 we\nhave that:\n|t2 .FREEq \\ t2 .TRYq \u2229 s2 .FREEp \\ s2 .TRYp | \u2264 m(q \u2212 p) + m \u2212 1\n\u0003\nNext we show that if a process p detects consecutive collisions with process q, the processes\np, q attempted to perform the jobs associated with the collisions in the same order and the order\nprocess p detects the collisions according to Definition 5.2 is the same as the order processes p, q\nattempted to perform the jobs.\nIn the proofs that follow, for a state s in execution \u03b1 we define as s.DONE the following set:\ns.DONE = {i \u2208 J |\u2203p \u2208 P and j \u2208 {1, . . . , n} : s.donep (j) = i}.\nLemma 5.2. In an execution \u03b1 \u2208 execs(KK\u03b2 ) for any \u03b2 \u2265 m if there exist processes p, q,\njobs i1 , i2 \u2208 J and states s\u03031 < s\u03032 such that process p collided with process q in job\ni1 at state\ni2 at state \u0010s\u03032 according to Definition\n5.2, then\n\u0011\n\u0011\n\u0010 there exist tran\u0010 s\u03031 and in job\u2032 \u0011\n\u2032\n\u2032\nsitions s1 , compNextp , s1 7\u2192 s\u03031 , s2 , compNextp , s2 7\u2192 s\u03032 and t1 , compNextq , t1 ,\n\u0011\n\u0010\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\nt2 , compNextq , t2 where s1 .NEXTp = t1 .NEXTq = i1 , s2 .NEXTp = t2 .NEXTq = i2 ,\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\ns1 .STATUSp = s2 .STATUSp = t1 .STATUSq = t2 .STATUSq = set next such that:\ns1 < s2 and t1 < t2 .\n\n\u0011\n\u0010\n\u2032\nProof. From Definition 5.2 we have that there exist transitions s1 , compNextp , s1 ,\n\u0010\n\u0011\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\ns2 , compNextp , s2 with s1 .NEXTp = i1 , s2 .NEXTp = i2 , s1 .STATUSp = s2 .STATUSp =\nset next, and there exists no action \u03c01 = compNextp for which s1 < \u03c01 < s\u03031 or s2 < \u03c01 < s\u03032 .\nFrom the latter and the fact that s\u03031 < s\u03032 , it must be the case that s1 \u0010< s\u03031 < s2 < s\u0303\u00112 .\n\u2032\nFurthermore from Definition 5.2 we have that there exist transitions t1 , compNextq , t1 ,\n\u0010\n\u0011\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\nt2 , compNextq , t2 with t1 .NEXTq = i1 , t2 .NEXTq = i2 , t1 .STATUSq = t2 .STATUSq =\n18\n\n\f\u2032\n\n\u2032\n\nset next, such that t1 < s\u03031 and t2 < s\u03032 . We can pick those transitions in \u03b1 in such a way\n\u2032\nthat there exists no other transition between t1 and s\u03031 that sets NEXTq to i1 and similarly there\n\u2032\nexists no other transition between t2 and s\u03032 that sets NEXTq to i2 . We need to prove now that\nt1 < t2 . We will prove this by contradiction.\n\u2032\n\u2032\nLet t2 < t1 . Since t1 < s\u03031 , we have that t2 < t1 < t1 < s\u03031 < s2 < s\u03032 . Since from\nDefinition 5.2 either s\u03031 .nextq = i1 or there exists j \u2208 {1, . . . , n} such that s\u03031 .doneq,j = i1 , it\nmust be the case that s\u03032 .STATUSp = gather done, s\u03032 .Qp = q and there exists j \u2032 \u2208 {1, . . . , n}\nit must be that case that process \u0010\nq performed job\nsuch that s\u03032 .done\n\u0010 q,j \u2032 = i2 . Essentially,\n\u0011\n\u0011 i2\n\u2032\n\n\u2032\n\nafter transition t2 , compNextq , t2 . This means that there exists transition t3 , doneq , t3 and\n\u2032\n\n\u2032\n\n\u2032\n\nj \u2032 \u2208 {1, . . . , n} such that t3 .doneq,j \u2032 = i2 and t2 < t3 < t1 < t1 < s\u03031 < s2 < s\u03032 .\nIf s\u03031 .STATUSp = gather try then from algorithm KK\u03b2 we have that s\u03031 .DONE \u2286\ns2 .DONEp , since actions gatherTryp are followed by actions gatherDonep before any action\n\u0010 setNextp takes\u0011 place. As a result i2 \u2208 s2 .DONEp , which is a contradiction since\n\u2032\n\n\u2032\n\n\u2032\n\n/ trans(KK\u03b2 ) if i2 \u2208 s2 .DONEp and s2 .NEXTp = i2 , s2 .STATUSp =\ns2 , compNextp , s2 \u2208\nset next.\nIf s\u03031 .STATUSp = gather done then from algorithm KK\u03b2 we have that s\u03031 .Qp = q and there\n\u2032\nexists j \u2208 {1, . . . , n} such that s\u03031 .POSp (q) = j and s\u03031 .doneq,j = i1 . Since t2 < t3 < t1 <\n\u2032\nt1 < s\u03031 < s2 < s\u03032 it must be the case that j \u2032 < j and \u0010\nas a result i2 \u2208 s\u03031 .DONE\np . Clearly\n\u0011\n\u2032\n\n/ trans(KK\u03b2 )\ns\u03031 .DONEp \u2286 s2 .DONEp , which is a contradiction since s2 , compNextp , s2 \u2208\n\u2032\n\n\u2032\n\n\u0003\nif i2 \u2208 s2 .DONEp and s2 .NEXTp = i2 , s2 .STATUSp = set next.\nNext we show that if two consecutive collisions take place between processes p, q, and p detects the one collision and q the other, the processes p, q attempted to perform the jobs associated\nwith the collisions in the same order and the order in which the processes detect the collisions\naccording to Definition 5.2 is the same as the order the processes p, q attempted to perform the\njobs.\nLemma 5.3. In an execution \u03b1 \u2208 execs(KK\u03b2 ) for any \u03b2 \u2265 m if there exist processes p, q, jobs\ni1 , i2 \u2208 J and states s\u03031 < s\u03032 such that process p collided with process q in job i1 at state s\u03031 and\nprocess q collided\nwith process \u0011\np in job i2\u0010at state s\u03032 according\n\u0011\n\u0011 to Definition\n\u0010 5.2, then there exist\n\u0010\n\u2032\n\u2032\n\u2032\ntransitions s1 , compNextp , s1 7\u2192 s\u03031 , t2 , compNextq , t2 7\u2192 s\u03032 and s2 , compNextp , s2 ,\n\u0011\n\u0010\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\nt1 , compNextq , t1 , where s1 .NEXTp = t1 .NEXTq = i1 , s2 .NEXTp = t2 .NEXTq = i2 ,\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\ns1 .STATUSp = s2 .STATUSp = t1 .STATUSq = t2 .STATUSq = set next such that:\ns1 < s2 and t1 < t2 .\n\n\u0011\n\u0010\n\u2032\nProof. From Definition 5.2 we have that there exist transitions s1 , compNextp , s1 ,\n\u0010\n\u0011\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\ns2 , compNextp , s2 with s1 .NEXTp = i1 , s2 .NEXTp = i2 , s1 .STATUSp = s2 .STATUSp =\nset next, and there exists no action \u03c01 = compNextp for which s1 < \u03c01 < s\u03031 .\n\u0011\n\u0010\n\u2032\nFurthermore from Definition 5.2 we have that there exist transitions t1 , compNextq , t1 ,\n\u0010\n\u0011\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\nt2 , compNextq , t2 with t1 .NEXTq = i1 , t2 .NEXTq = i2 , t1 .STATUSq = t2 .STATUSq =\nset next, and there exists no action \u03c02 = compNextq for which t2 < \u03c02 < s\u03032 . From the\nlater and the fact that s\u03031 < s\u03032 , it must be the case that t1 < t2 < s\u03032 . We can pick the transitions\n19\n\n\fthat are enabled by states t1 and s2 in \u03b1 in such a way that there exists no other transition between\n\u2032\n\u2032\nt1 and s\u03031 that sets NEXTq to i1 and similarly there exists no other transition between s2 and s\u03032\nthat sets NEXTp to i2 . We need to prove now that s1 < s2 . We will prove this by contradiction.\nFrom algorithm\u0011 KK\u03b2 and Definition 5.2 there exist transitions\n\u0010 Let s2 < \u0011s1 .\n\u0010\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\ns3 , setNextp , s3 , and t3 , setNextq , t3 , where s3 .nextp = i2 , t3 .nextq = i1 and s2 <\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\ns3 < s1 , t1 < t3 < t2 . There are 2 cases, either s3 < t3 or t3 < s3 .\n\u0010\n\u0011\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\nCase 1 s3 < t3 : We have that s3 < t3 < t2 and t2 , compNextq , t2 , where t2 .NEXTq = i2 and\n\u2032\n\n/ t2 .TRYq \u222a t2 .DONEq . This is a contradiction\nt2 .STATUSq = set next which means that i2 \u2208\nsince the t2 .TRY q and t2 .DONEq are computed by actions gatherTryq and gatherDoneq that\n\u2032\nare preceded by state s3 . Either i2 \u2208 t2 .TRYq or a new action\n\u0010 setNextp\u2032 took\n\u0011 place before the\ngatherTryq actions. In the latter case, if there is a transition s4 , donep , s4 , where s4 .nextp =\ni2 , before the action setNextp , it must be the case that i2 \u2208 t2 .DONEq . If there exists no such\ntransition we have again a contradiction since we cannot have a collision in job i2 at state s\u03032 as\ndefined in Definition 5.2.\n\u0011\n\u0010\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\nCase 2 t3 < s3 : We have that t3 < s3 < s1 and s1 , compNextp , s1 , where s1 .NEXTp = i1\n\u2032\n\nand s1 .STATUSp = set next which means that i1 \u2208\n/ s1 .TRYp \u222a s1 .DONEp . This is a contradiction since the s1 .TRYp and s1 .DONEp sets are computed by gatherTryp and gatherDonep\n\u2032\nactions that are preceded by state t3 . Either i1 \u2208 s1 .TRY p or a new action\n\u0010 setNextq took\n\u0011 place\n\u2032\n\nbefore the gatherTryp actions. In the latter case, if there is a transition t4 , doneq , t4 , where\nt4 .nextq = i1 , before the action setNextq , it must be the case that i1 \u2208 s1 .DONEp . If there\nexists no such transition we have again a contradiction since we cannot have a collision in job i1\nat state s\u03031 as defined in Definition 5.2.\n\u0003\nNext we show that if 2 processes p, q \u2208 P collide three times, their DONE sets at the third\ncollision will contain at least m * (q \u2212 p) more jobs than they did at the first collision. This will\nallow us to find an upper bound on the collisions a process may participate in. It is possible\nthat both processes become aware of a collision or only one of them does while the other one\nsuccessfully completes the job.\nLemma 5.4. If \u03b2 \u2265 3m2 and in an execution \u03b1 \u2208 execs(KK\u03b2 ) there exist processes p 6= q,\njobs i1 , i2 , i3 \u2208 J and states s\u03031 < s\u03032 < s\u03033 such that process p, q collide in job i1 at state s\u03031 ,\nin job i2 at state s\u03032 and in job i3 at state s\u03033 according to Definition 5.3, then there exist states\ns1 < s3 and t1 < t3 such that:\ns1 .DONEp \u222a t1 .DONEq \u2286 s3 .DONEp \u2229 t3 .DONEq\n|s3 .DONEp \u222a t3 .DONEq | \u2212 |s1 .DONEp \u222a t1 .DONEq | \u2265 m * |q \u2212 p|\n\u0011\n\u0010\n\u2032\nProof. From Definitions 5.2, 5.3 we have that there exist transitions s1 , compNextp , s1 ,\n\u0011\n\u0011 \u0010\n\u0011\n\u0010\n\u0011 \u0010\n\u0010\n\u2032\n\u2032\n\u2032\n\u2032\ns2 , compNextp , s2 , s3 , compNextp , s3 and t1 , compNextq , t1 , t2 , compNextq , t2 ,\n\u0011\n\u0010\n\u2032\n\u2032\n\u2032\n\u2032\n\u2032\nt3 , compNextq , t3 , where s1 .NEXTp = t1 .NEXTq = i1 , s2 .NEXTp = t2 .NEXTq = i2 ,\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\ns3 .NEXTp = t3 .NEXTq = i3 , s1 .STATUSp = s2 .STATUSp = s3 .STATUSp = t1 .STATUSq =\n\u2032\n\u2032\nt2 .STATUSq = t3 .STATUSq = set next and s1 < s\u03031 , t1 < s\u03031 , s2 < s\u03032 , t2 < s\u03032 , and s3 < s\u03033 ,\n20\n\n\f\u0011\n\u0011 \u0010\n\u0010\n\u2032\n\u2032\nt3 < s\u03033 . We pick from \u03b1 the transitions s1 , compNextp , s1 , t1 , compNextq , t1 , in such\na way that there exists no other compNextp , compNextq between states s1 , s\u03031 respectively t1 ,\ns\u03031 that sets NEXTp respectively NEXTq to i1 . We can pick in a similar manner the transitions\nfor jobs i2 , i3 . From Lemmas 5.2, 5.3 and Definitions 5.2, 5.3 we have that s1 < s2 < s3 and\nt1 < t2 < t3 . We will first prove that:\ns1 .DONEp \u222a t1 .DONEq \u2286 s3 .DONEp \u2229 t3 .DONEq\n\u0010\n\u0011\n\u2032\nFrom algorithm KK\u03b2 we have that there exists in \u03b1 transitions s4 , setNextp , s4 ,\n\u0011\n\u0010\n\u2032\n\u2032\n\u2032\nt4 , setNextq , t4 with s4 .nextp = i2 , t4 .nextq = i2 and there exist no action \u03c01 =\n\u2032\n\n\u2032\n\ncompN extp , such that s2 < \u03c01 < s4 , and no action \u03c02 = compN extq , such that t2 < \u03c02 < t4 .\nWe need to prove that t1 < s4 and s1 < t4 .\nWe start by proving that t1 < s4 . In order to get a contradiction\n\u0011 s4 <\n\u0010 we assume that\n\u2032\nt1 . From algorithm KK\u03b2 we have that there exists in \u03b1 transition t5 , gatherTryq , t5 , with\n\u2032\n\nt5 .Qq = p, and there exists no action \u03c02 = compN extq , such that t5 < \u03c02 < t2 . We have that\n\u2032\ns4 < t1 < t5 < t2 and i2 \u2208\n/ t2 .TRYq \u222a t2 .DONEq . If t5 .nextp = i2 we have a contradiction\nsince i2 \u2208 s2 .TRY q . If t5 .nextq 6= i2 there exists an action\n\u0011 p in \u03b1, such that\n\u0010 \u03c03 = setNext\n\u2032\n\ns4 < \u03c03 < t5 . If this \u03c03 = setNextp is preceded by transition s5 , donep , s5 with s5 .NEXTp =\ni2 , we have a contradiction since i2 \u2208 t5 .DONE and t2 .DONEq is computed by gatherDoneq\nactions that are preceded by state t5 , which results in i2 \u2208 t2 .DONEq . If there exists no such\ntransition we have again a contradiction since we cannot have a collision in job i2 at state s\u03032 as\ndefined in Definition 5.2.\nThe case s1 < t4 is symmetric and can be proved with similar arguments.\nFrom the discussion above we have that t1 < s4 , thus t1 .DONEq \u2286 s4 .DONE. Moreover\ns3 .DONEp is computed by gatherDonep actions that are preceded by state s4 , from which we\nhave that t1 .DONEq \u2286 s3 .DONEp . Since s1 < s3 it holds that s1 .DONEp \u2286 s3 .DONEp , thus\nwe have that s1 .DONEp \u222a t1 .DONEq \u2286 s3 .DONEp . From s1 < t4 , with similar arguments as\nbefore, we can prove that s1 .DONEp \u222a t1 .DONEq \u2286 t3 .DONEq , which gives us that:\ns1 .DONEp \u222a t1 .DONEq \u2286 s3 .DONEp \u2229 t3 .DONEq\nNow it only remains to prove that:\n|s3 .DONEp \u222a t3 .DONEq | \u2212 |s1 .DONEp \u222a t1 .DONEq | > m * |q \u2212 p|\n\nIf p < q from Lemma 5.1 we have that s3 .DONEp \u2229 t3 .DONEq > (q \u2212 p)m or\ns3 .DONEp \u2229 t3 .DONEq > (q \u2212 p)m . Since s1 .DONEp \u222a t1 .DONEq \u2286 s3 .DONEp \u2229\nt3 .DONEq , we have that:\n|s3 .DONEp \u222a t3 .DONEq | \u2212 |s1 .DONEp \u222a t1 .DONEq | > (q \u2212 p) * m\nIf q < p with similar arguments we have that:\n|s3 .DONEp \u222a t3 .DONEq | \u2212 |s1 .DONEp \u222a t1 .DONEq | > (p \u2212 q) * m\nCombining the above we have:\n|s3 .DONEp \u222a t3 .DONEq | \u2212 |s1 .DONEp \u222a t1 .DONEq | > m * |q \u2212 p|\n21\n\n\fNext we prove that a process p cannot collide with a process q more than 2\nin any execution.\n\nl\n\nn\nm*|q\u2212p|\n\nm\n\n\u0003\ntimes\n\nLemma 5.5. If \u03b2 \u2265 3m2 therel exists no\nm execution \u03b1 \u2208 execs(KK\u03b2 ) at which process p collided\nwith process q in more than 2\n\nn\nm|q\u2212p|\n\nstates according to Definition 5.2.\n\nProof. Let execution\nl \u03b1 \u2208mexecs(KK\u03b2 ) be an execution at which\nl process\nm p collided with\nn\nn\nprocess q in at least 2 m|q\u2212p| + 1 states. Let us examine the first 2 m|q\u2212p| + 1 such states.\nLet those states be s\u03031 < s\u03032 < . . . < s\u03032\u2308 n \u2309 < s\u03032\u2308 n \u2309+1 . From Lemma 5.2 we have\nm|q\u2212p|\nm|q\u2212p|\nthat there exists states s1 < s2 < . . . < s2\u2308 n \u2309 < s2\u2308 n \u2309+1 that enable the compN extp\nm|q\u2212p|\nm|q\u2212p|\nactions and states t1 < t2 < . . . < t2\u2308 n \u2309 < t2\u2308 n \u2309+1 that enable the compN extq\nm|q\u2212p|\nm|q\u2212p|\nactions that lead to the collisions in states s\u03031 < s\u03032 < . . . < s\u03032\u2308 n \u2309 < s\u03032\u2308 n \u2309+1 . Then\nm|q\u2212p|\nm|q\u2212p|\nn\nl\nmo\nn\nfrom Lemma 5.4 we have that \u2200i \u2208 1, . . . , m|q\u2212p| :\n|s2i+1 .DONEp \u222a t2i+1 .DONEq | \u2212 |s2i\u22121 .DONEp \u222a t2i\u22121 .DONEq | > m|q \u2212 p|\n|s2i+1 .DONEp \u222a t2i+1 .DONEq | \u2212 |s1 .DONEp \u222a t1 .DONEq | > im|q \u2212 p|\n|s2i+1 .DONEp \u222a t2i+1 .DONEq | > im|q \u2212 p|\n\n(5)\n\nFrom eq. 5 we have that:\ns2\u2308\n\nn\nm|q\u2212p|\n\n\u2309+1 .DONEp \u222a t2\u2308\n\nn\nm|q\u2212p|\n\n\u2309+1 .DONEq > m|q \u2212 p|\n\nEquation 6 leads to a contradiction since s2\u2308\n\nand |J | = n.\n\nn\nm|q\u2212p|\n\n\u0018\n\n\u0019\nn\n\u2265n\nm|q \u2212 p|\n\n(6)\n\nn\n\u2309+1 .DONEp \u222a t2\u2308 m|q\u2212p|\n\u2309+1 .DONEq \u2286 J\n\n\u0003\nFinally we are ready to prove the main theorem on the work complexity of algorithm KK\u03b2\nfor \u03b2 \u2265 3m2 .\nTheorem 5.6. If \u03b2 \u2265 3m2 algorithm KK\u03b2 has work complexity WKK\u03b2 = O(nm log n log m).\nProof. We start with the observation\nthat in\n\u0011 any execution \u03b1 of algorithm KK\u03b2 , if there exists\n\u0010\n\u2032\nprocess p, job i, transition s1 , donep , s1 and j \u2208 {1, . . . , n} such that s1 .POSp (p) = j,\n\u0011\n\u0010\n\u2032\ns1 .NEXTp = i, for any process q 6= p there exists at most one transition t1 , gatherDoneq , t1\n\nin \u03b1, with t1 .Q q = p, t1 .POSq (p) = j and t1 \u2265 s1 . Such transition performs exactly one read\noperation from the shared memory, one insertion at the set DONEq and one removal from the\nset FREEq , thus such a transition costs O(log n) work. Clearly there exist at most m \u2212 1 such\ntransitions for each donep . From Lemma 4.1 for all processes there can be at most n actions\ndonep in any execution \u03b1 of algorithm KK\u03b2 . Each donep action performs one write operation\nin shared memory, one insertion at the set DONEp and one removal from the set FREEp , thus\nsuch an action has cost O(log n) work. Furthermore any donep is preceded by m \u2212 1 gatherTryp\nread actions that read the next array and each add at most one element to the set TRYp with\n22\n\n\fcost O(log n) and m \u2212 1 gatherDonep read actions that do not add elements in the DONEp set.\nNote that we have already counted the gatherDonep read actions that result in adding jobs at\nthe DONEp set. Finally any donep action is preceded by one compNextp action. This action is\ndominated by the cost of the rank(FREEp , TRYp , i) function. If the sets FREEp , TRY p are\nrepresented with some efficient tree structure like red-black tree or some variant of B-tree [5, 19]\nthat allows insertion, deletion and search of an element in O(log n), an invocation of function\nrank(FREEp , TRYp , i) costs O(m log n) work. That gives us a total of O(nm log n) work\nassociated with the donep actions.\nIf a process p collided with a process q in job i at state s, we have an extra compNextp action,\nm \u2212 1 extra gatherTryp read actions and insertions in the TRYp set and m \u2212 1 gatherDonep read\nactions that do not add elements in the DONEp set. Thus each collision costs O(m log n) work.\nSince \u03b2 \u2265 3m2 from Lemma 5.5 for twoldistinct mprocesses p, q we have that in any execution \u03b1\nn\nof algorithm KK\u03b2 there exist less than 2 m|q\u2212p|\ncollisions. For process p if we count all such\ncollisions with any other process q we get:\n\u0018\n\u0019\nX\nn\n2n X\n1\n2\n\u2264 2(m \u2212 1) +\n\u2264\nm|q \u2212 p|\nm\n|q \u2212 p|\nq\u2208P\u2212{p}\n\nq\u2208P\u2212{p}\n\n\u2308 m2 \u2309\n4n X 1\n4n\n\u2264 2(m \u2212 1) +\n\u2264 2(m \u2212 1) +\nlog m\nm i=1 i\nm\n\n(7)\n\nIf we count the total number of collisions for all the m processes we get that if \u03b2 \u2265 3m2 in any\nexecution of algorithm KK\u03b2 there can be at most 2m2 + 4n log m < 4(n + 1) log m collisions\n(since n > \u03b2). Thus collisions cost O(nm log n log m) work. Finally any process p that fails may\nadd in the work complexity less than O(m log n) work from its compNextp action and from reads\n(if the process fails without performing a donep action after its latest compNextp action). So for\nthe work complexity of algorithm KK\u03b2 if \u03b2 \u2265 3m2 we have that WKK\u03b2 = O(nm log n log m).\n\u0003\n6. An Asymptotically Work Optimal Algorithm\nWe demonstrate how to solve the at-most-once problem with effectiveness n \u2212\nO(m2 log n log m) and work complexity\u221aO(n + m(3+\u01eb) log n), for any constant \u01eb > 0, such that\n1/\u01eb is a positive integer, when m = O( 3 n), using algorithm KK\u03b2 with \u03b2 = 3m2 . Algorithm\nIterativeKK (\u01eb), presented in Fig. 3, performs iterative calls to a variation of algorithm KK\u03b2 ,\ncalled IterStepKK. IterativeKK (\u01eb) has 3 + 1/\u01eb distinct matrices done and vectors next in\nshared memory, with different granularities. One done matrix, stores the regular jobs performed,\nwhile the remaining 2+1/\u01eb matrices store super-jobs. Super-jobs are groups of consecutive jobs.\nFrom them, one stores super-jobs of size m log n log m, while the remaining 1 + 1/\u01eb matrices,\nstore super-jobs of size m1\u2212i\u01eb log n log1+i m for i \u2208 {1, . . . , 1/\u01eb}. The 3 + 1/\u01eb distinct vectors\nnext are used in a similar way as the matrices done.\nThe algorithm IterStepKK is different from KK\u03b2 in the following ways. First, all instances\nof IterStepKK work for \u03b2 = 3m2 . Moreover, IterStepKK has a termination flag in shared\nmemory. This termination flag is initially 0 and is set to 1 by any process that decides to terminate. In the execution of algorithm IterStepKK, a process p, that in an action compNextp\nhas |FREEp \\ TRYp | < 3m2 , sets the termination flag to 1, computes new sets FREEp and\n23\n\n\fIterativeKK (\u01eb) for process p:\n00 sizep,1 \u2190 1\n01 sizep,2 \u2190 m log n log m\n02 FREEp \u2190 map (J , sizep,1 , sizep,2 )\n03 FREEp \u2190 IterStepKK (FREEp , sizep,2 )\n04 for(i \u2190 1, i \u2264 1/\u01eb, i + +)\n05\n\nsizep,1 \u2190 sizep,2\n\n06\n\nsizep,2 \u2190 m1\u2212i\u01eb log n log1+i m\n\n07\n\nFREEp \u2190 map (FREEp , sizep,1 , sizep,2 )\n\n08\n\nFREEp \u2190 IterStepKK (FREEp , sizep,2 )\n\n09 endfor\n10 sizep,1 \u2190 sizep,2\n11 sizep,2 \u2190 1\n12 FREEp \u2190 map (FREEp , sizep,1 , sizep,2 )\n13 FREEp \u2190 IterStepKK (FREEp , sizep,2 )\n\nFigure 3: Algorithm IterativeKK (\u01eb): pseudocode\n\nTRY p , returns the set FREEp \\ TRY p and terminates. After a process p checks if it is safe to\nperform a job, the process also checks the termination flag and if the flag is 1, the process instead\nof performing the job, computes new sets FREEp and TRY p , returns the set FREEp \\ TRY p\nand terminates. Finally, algorithm IterStepKK takes as inputs the variable size and a set SET1 ,\nsuch that |SET1 | > 3m2 , and returns the set SET2 as output. SET1 contains super-jobs of size\nsize. In IterStepKK, with an action dop,j process p performs all the jobs of super-job j. A\nprocess p performs as many super-jobs as it can and returns in SET2 the super-jobs it can verify\nthat no process will perform.\nIn algorithm IterativeKK (\u01eb) we use also the function SET2 = map (SET1 , size1 , size2 ),\nthat takes the set of super-jobs SET1 , with super-jobs of size size1 and maps it to a set of superjobs SET2 with size size2 . A job i is always mapped to the same super-job of a specific size and\nthere is no intersection between the jobs in super-jobs of the same size.\n6.1. Analysis\nWe begin the analysis of algorithm IterativeKK (\u01eb) by showing in Theorem 6.3 that\nIterativeKK (\u01eb) solves the at-most-once problem. This is done by first showing in Lemma 6.1\nthat algorithm IterStepKK solves the at-most-once problem for the set of all super-jobs of a\nspecific size, and then by showing in Lemma 6.2 that there exist no performed super-jobs in any\noutput set SET2 . We complete the analysis with Theorem 6.4, where we show that algorithm\nIterativeKK (\u01eb) has effectiveness EIterativeKK(\u01eb) (n, m, f ) = n \u2212 O(m2 log n log m) and work\ncomplexity WIterativeKK(\u01eb) = O(n + m3+\u01eb log n).\nLet the set of all super-jobs of a specific size d be SuperSetd . All invocations of algorithm\nIterStepKK on sets SET1 \u2286 SuperSetd , use the matrix done and vector next that correspond\nto the super-jobs of size d. Moreover each process p invokes algorithm IterStepKK for a set\nSET1 \u2286 SuperSetd only once. We have the following lemma.\nLemma 6.1. Algorithm IterStepKK solves the at-most-once problem for the set SuperSetd .\nProof. As described above, algorithm IterStepKK is different from KK\u03b2 in the following ways:\n24\n\n\f\u2022 Process p, on algorithm IterStepKK, has an input set SET1 \u2286 SuperSetd of super-jobs\nof size d to be performed and outputs a set SET2 \u2282 SuperSetd of super-jobs, that have\nnot been performed. Process p initially sets its set FREEp , equal to SET1 and proceeds\nas it would do when executing KK\u03b2 , with the difference that an action dop,i results in\nperforming all the jobs under super-job i. Entries in the matrix done and vector next in\nshared memory correspond to the identifiers of super-jobs of set SuperSetd . Again after\nits initialization, entries are only removed from set FREEp .\nNote that the main difference caused by this modification, between algorithm IterStepKK\nand algorithm KK\u03b2 , is that jobs are replaced by super-jobs, and that the initial sets FREEp\nand FREEq of processes p, q could be set to different subsets of set SuperSetd . This\ndoes not affect the correctness of the algorithm, since in any state s of an execution \u03b1 of\nalgorithm KK\u03b2 , the sets FREEp and FREEq could be different subsets of the set of all\njobs J .\n\u2022 Algorithm IterStepKK has a termination flag in shared memory. The termination flag is\ninitially 0 and is set to 1 by any process that decides to terminate. As mentioned above,\nany process that discovers that |FREEp \\ TRY p | < 3m2 in an action compNextp , sets\nthe termination flag to 1, computes new sets FREEp and TRYp , returns the set FREEp \\\nTRYp and terminates. This modification only affects the sequence of actions during the\ntermination of a process p. Observe process p does not perform any super-jobs in that\ntermination sequence.\nAdditionally, after a process p checks if it is safe to perform a super-job, it also checks\nthe termination flag and if the flag is 1, the process instead of performing the super-job,\nenters the termination sequence, computing new sets FREEp and TRY p , returning the\nset FREEp \\ TRYp and terminating. A process p first checks if it is safe to perform a\nsuper-job according to algorithm KK\u03b2 and then checks the flag. Thus this modification\nonly affects the effectiveness, but not the correctness of the algorithm, since it could only\nresult in a super-job that was safe to perform not being performed.\n\u2022 Finally all instances of IterStepKK work for \u03b2 = 3m2 . This does not affect correctness,\nsince Lemma 4.1 holds for any \u03b2.\nIt is easy to see that none of the modifications described above affect the key arguments in\nthe proof of Lemma 4.1. Thus with similar arguments as in the proof of Lemma 4.1, we can show\nthat there exists no execution of algorithm IterStepKK, where two distinct actions \u03c0 = dop,i\nand \u03c0 \u2032 = doq,i take place for a super-job i \u2208 SuperSetd and processes p, q \u2208 P (p could be\nequal to q).\n\u0003\nNext we show that in the output sets of algorithm IterStepKK at a specific iteration (calls\nfor super-jobs of size d), no completed super-jobs are included. Combined with the previous\nlemma, this argument will help us establish that algorithm IterativeKK (\u01eb) solves that at-mostonce problem.\nLemma 6.2. There exists no execution \u03b1 of algorithm IterStepKK, such that there exists action\ndoq,i \u2208 \u03b1 for some process q and super-job i in the output set SET2 \u2282 SuperSetd of some\nprocess p (p could be equal to process q).\nProof. As described above, a process p before terminating algorithm IterStepKK, either sets the\nflag to 1 or observes that the flag is set to 1. The process p then computes new sets FREEp and\n25\n\n\fTRY p , returns the set FREEp \\ TRYp and terminates its execution of algorithm IterStepKK\nfor input set SET1 \u2286 SuperSetd and super-jobs of size d. Let state s be the state at which\nprocess p terminates, we have that SET2 = s.FREEp \\ s.TRYp . If p = q and there exists\naction \u03c0 = dop,i in execution \u03b1 of algorithm IterStepKK, for super-jobs i \u2208 SuperSetd , clearly\n\u03c0 < s, from which we have that i \u2208\n/ s.FREEp and thus i \u2208\n/ SET2 .\nIt is easy to see that if p 6= q and i \u2208 SET2 of process p, there exists no action \u03c0 = doq,i\nin execution \u03b1. If i \u2208 SET2 then i \u2208 s.FREEp and i \u2208\n/ s.TRYp . Moreover process p either\nset flag to 1 or observed that the flag was set, before computing sets s.FREEp and s.TRY p . If\nthere exists\n\u0010 \u03c0 = doq,i \u2208 \u03b1, \u0011for process q, it must be the case that after process q performed the\n\u2032\n\ntransition t, compNextq , t 7\u2192 \u03c0 (see Definition 5.1 of immediate predecessor), it read the flag\nand found it was equal to 0. This leads to a contradiction, since it must be the case that either\ni \u2208 s.TRY p or i \u2208\n/ s.FREEp .\n\u0003\nWe are ready now to show the correctness of algorithm IterativeKK (\u01eb).\nTheorem 6.3. Algorithm IterativeKK (\u01eb) solves the at-most-once problem.\n\nProof. From Lemma 6.1 we have that any super-job of a specific size d is performed at-mostonce (if performed at all) in the execution of algorithm IterStepKK for the super-jobs in the\nset SuperSetd . Moreover, from Lemma 6.2 we have that super-jobs in the output sets of an\nexecution of algorithm IterStepKK for super-jobs of size d, have not been performed. Function\nSET2 = map (SET1 , size1 , size2 ) maps the jobs in the super-jobs of set SET1 , to super-jobs\nin SET2 . A job i is always mapped to the same super-job of a specific size d and there is no\nintersection between the jobs of the super-jobs in set SuperSetd . It is easy to see that there exists\nno execution of algorithm IterativeKK (\u01eb), where a job i is performed more than once.\n\u0003\nWe complete the analysis of algorithm IterativeKK (\u01eb) with Theorem 6.4, which gives upper\nbounds for the effectiveness and work complexity of the algorithm.\nTheorem 6.4. Algorithm IterativeKK (\u01eb) has WIterativeKK(\u01eb) = O(n + m3+\u01eb log n) work complexity and effectiveness EIterativeKK(\u01eb) (n, m, f ) = n \u2212 O(m2 log n log m).\nProof.\nIn order to determine the effectiveness and work complexity of algorithm\nIterativeKK (\u01eb), we compute the jobs performed by and the work spent in each invocation of\nIterStepKK. Moreover we compute the work that the invocations to the function map () add.\nThe first invocation to function map () in line 02 can be completed by process p with work\nO( m log nn log m log n), since process p needs to construct a tree with m log nn log m elements. This\ncontributes for all processes O( lognm ) work. From Theorem 5.6 we have that IterStepKK in\n03 has total work O(n + m log nn log m m log n log m) = O(n), where the first n comes from do\nactions and the second term from the work complexity of Theorem 5.6. Note that we count\nO(1) work for each normal job executed by a do action on a super-job. That means that in\nthe invocation of IterStepKK in line 03, do actions cost m log n log m work. Moreover from\nTheorem 4.4 we have effectiveness m log nn log m \u2212 (3m2 + m \u2212 2) on the super-jobs of size\nm log n log m. From the super-jobs not completed, up to m \u2212 1 may be contained in the TRY p\nsets upon termination in line 03. Since those super-jobs are not added (and thus are ignored) in\nthe output FREEp set in line 03, up to (m \u2212 1)m log n log m jobs may not be performed by\nIterativeKK (\u01eb). The set FREEp returned by algorithm IterStepKK in line 03 has no more\nthan 3m2 + m \u2212 2 super-jobs of size m log n log m.\n26\n\n\fIn each repetition of the loop in lines 04 \u2212 09, the map () function in line 07 constructs a\nFREEp set with at most O(m2+\u01eb / log m) elements, which costs O(m2+\u01eb ) per process p for a\ntotal of O(m3+\u01eb ) work for all processes. Moreover each invocation of IterStepKK in line 08\ncosts O(3m3 log n log m + m3+\u01eb log m) < O(m3+\u01eb log n) work from Theorem 5.6, where the\nterm 3m3 log n log m is an upper bound on the work needed for the do actions on the super-jobs.\nFrom Theorem 4.4 we have that each output FREEp set in line 08 has at most 3m2 + m \u2212 2\nsuper-jobs. Moreover from each invocation of IterStepKK in line 08 at most m \u2212 1 super-jobs\nare lost in TRY sets. Those account for less than (m \u2212 1)m log n log m jobs in each iteration,\nsince the size of the super-jobs in the iterations of the loop in lines 04 \u2212 09 is strictly less than\nm log n log m.\nWhen we leave the loop in lines 04 \u2212 09, we have a FREEp set with at most 3m2 + m \u2212 2\nsuper-jobs of size log n log1+1/\u01eb m, which means that in line 12 function map () will return a\nset FREEp with less than (3m2 + m \u2212 2)(log n log1+1/\u01eb m) elements that correspond to jobs\nand not super-jobs. This costs for all processes a total of O(m3 log m log log n log log m) <\nO(m3+\u01eb log n) work, since \u01eb is a constant. Finally we have that IterStepKK in line 13 has from\nTheorem 5.6 work O(m3 log2 m log log n log log m) < O(m3+\u01eb log n) and from Theorem 4.4\neffectiveness (3m2 + m \u2212 2)(log n log1+1/\u01eb m) \u2212 (3m2 + m \u2212 2).\nIf we add up all the work, we have that WIterativeKK(\u01eb) = O(n + m3+\u01eb log n) since the loop\nin lines 04 \u2212 09 repeats 1 + 1/\u01eb times and \u01eb is a constant. Moreover for the effectiveness, we have\nthat less than or equal to (m \u2212 1)m log n log m jobs will be lost in the TRY set at line 03. After\nthat strictly less than (m \u2212 1)m log n log m jobs will be lost in the TRY sets of the iterations of\nthe loop in lines 04 \u2212 09 and fewer than 3m2 + m \u2212 2 jobs will be lost from the effectiveness\nof the last invocation of IterStepKK in line 13. Thus we have that EIterativeKK(\u01eb) (n, m, f ) =\nn \u2212 O(m2 log n log m).p\n\u0003\nFor any m = O( 3+\u01eb n/ log n), algorithm IterativeKK (\u01eb) is work optimal and asymptotically effectiveness optimal.\n7. An Asymptotically Optimal Algorithm for the Write-All Problem\nBased on IterativeKK (\u01eb) we construct algorithm WA IterativeKK (\u01eb) Fig. 4, that solves\nthe Write-All problem [23] with work complexity O(n + m(3+\u01eb) log n), for any constant \u01eb > 0,\nsuch that 1/\u01eb is a positive integer. From Kanellakis and Shvartsman [23] the Write-All problem\nfor the shared memory model, consists of: \"Using m processors write 1's to all locations of\nan array of size n.\" The problem assumes that all cells of the array are initialized to 0. Algorithm WA IterativeKK (\u01eb) is different from IterativeKK (\u01eb) in two ways. It uses a modified\nversion of IterStepKK, that instead of returning the FREEp \\ TRYp set upon termination returns the set FREEp instead. Let us name this modified version WA IterStepKK. Moreover in\nWA IterativeKK (\u01eb) after line 13, process p, instead of terminating, executes all jobs in the set\nFREEp . Note that since we are interested in the Write-All problem, when process p performs\na job i with action dop,i , process p just writes 1, in the i\u2212th position of the Write All array\nwa[1, . . . , n] in shared memory.\nTheorem 7.1. Algorithm WA IterativeKK (\u01eb) solves the Write-All problem with work complexity WWA IterativeKK(\u01eb) = O(n + m3+\u01eb log n).\nProof. We prove this with similar arguments as in the proof of Theorem 6.4. From Theorem\n4.4 after each invocation of WA IterStepKK the output set FREEp has less than 3m2 + m \u2212 1\n27\n\n\fWA IterativeKK (\u01eb) for process p:\n00 sizep,1 \u2190 1\n01 sizep,2 \u2190 m log n log m\n02 FREEp \u2190 map (J , sizep,1 , sizep,2 )\n03 FREEp \u2190 WA IterStepKK (FREEp , sizep,2 )\n04 for(i \u2190 1, i \u2264 1/\u01eb, i + +)\n05\n\nsizep,1 \u2190 sizep,2\n\n06\n\nsizep,2 \u2190 m1\u2212i\u01eb log n log1+i m\n\n07\n\nFREEp \u2190 map (FREEp , sizep,1 , sizep,2 )\n\n08\n\nFREEp \u2190 WA IterStepKK (FREEp , sizep,2 )\n\n09 endfor\n10 sizep,1 \u2190 sizep,2\n11 sizep,2 \u2190 1\n12 FREEp \u2190 map (FREEp , sizep,1 , sizep,2 )\n13 FREEp \u2190 WA IterStepKK (FREEp , sizep,2 )\n14 for(i \u2208 FREEp )\n15\n\ndop,i\n\n16 endfor\n\nFigure 4: Algorithm WA IterativeKK (\u01eb): pseudocode\n\nsuper-jobs. The difference is that now we do not leave jobs in the TRYp sets, since we are not\ninterested in maintaining the at-most-once property between successive invocations of algorithm\nWA IterStepKK. Since after each invocation of WA IterStepKK the output set FREEp has\nthe same upper bound on super-jobs as in IterativeKK (\u01eb), with similar arguments as in the\nproof of Theorem 6.4, we have that at line 13 the total work performed by all processes is\nO(n + m3+\u01eb log n). Moreover from Theorem 4.4 the output FREEp set in line p has less than\n3m2 + m \u2212 2 jobs. This gives us for all processes a total work of O(m3 ) for the loop in lines\n14 \u2212 16. After the loop in lines 14 \u2212 16 all jobs have been performed, since we left no TRY sets\nbehind, thus algorithm WA IterativeKK (\u01eb) solves the Write-All problem with work complexity\nWWA IterativeKK(\u01eb) = O(n + m3+\u01eb log n).\n\u0003\np\n3+\u01eb\nFor any m = O(\nn/ log n), algorithm WA IterativeKK (\u01eb) is work optimal.\n8. Conclusions\nWe devised and analyzed a deterministic algorithm for the at most once problem called\nKK\u03b2 . For \u03b2 = m algorithm KK\u03b2 has effectiveness n \u2212 2m + 2, which is asymptotically\noptimal for any m = o(n) and close by an additive factor of m to the effectiveness upper bound\nn \u2212 m + 1 on all possible algorithms. This is a significant improvement over the previous best\nknown deterministic algorithm [26], that achieves asymptotically optimal effectiveness\np only for\nm = O(1). With respect to work complexity, for any constant \u01eb and for m = O( 3+\u01eb n/ log n)\nwe demonstrate how to use KK\u03b2 with \u03b2 = 3m2 , in order to construct an iterated algorithm\nIterativeKK (\u01eb), that is work-optimal and asymptotically effectiveness-optimal. Finally we\nused algorithm IterativeKK (\u01eb) in order to solve the Write-All problem with workp\ncomplexity\nO(n + m(3+\u01eb) log n), for any constant \u01eb > 0, which is work optimal for m = O( 3+\u01eb n/ log n).\nOur solution improves on the algorithm of Malewicz [36] both in terms of the range of pro28\n\n\fcessors for which we achieve optimal work and on the fact that we do not assume test-andset primitives, but use only atomic read/write shared memory. The solution of Kowalski and\nShvartsman [28] is work optimal for a wider range of processors m than our algorithm, but their\nalgorithm uses a collection of q permutations with contention O(q log q). Although an efficient\npolynomial time construction of permutations with contention O(q polylog q) has been developed by Kowalski et al. [27], constructing permutations with contention O(q log q) in polynomial\ntime is still an open problem. Subsequent to the conference version of this paper [25], Alistarh\net al. [2] show that there exists a deterministic algorithm for the Write-All problem with work\nO(n + m log5 n log2 max(n, m)), by derandomizing their randomized solution for the problem.\nTheir solution is so far existential, while ours explicit.\nIn terms of open questions there still exists an effectiveness gap between the shown effectiveness of n \u2212 2m + 2 of algorithm KK\u03b2 and the known effectiveness bound of n \u2212 m + 1. It\nwould be interesting to see if this can be bridged for deterministic algorithms. Moreover, there is\na lack of an upper bound on work complexity, when the effectiveness of an algorithm approaches\nthe optimal. Finally it would be interesting to study the existence and efficiency of algorithms\nthat try to implement at-most-once semantics in systems with different means of communication,\nsuch as message-passing systems.\nReferences\n[1] Y. Afek, E. Weisberger, and H. Weisman. A completeness theorem for a class of synchronization objects. In Proc.\nof the 12th annual ACM Symp. on Principles of Distributed Computing(PODC '93), pages 159\u2013170. ACM, 1993.\n[2] D. Alistarh, M. Bender, S. Gilbert, and R. Guerraoui. How to allocate tasks asynchronously. In Foundations of\nComputer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pages 331 \u2013340, Oct. 2012.\n[3] R. J. Anderson and H. Woll. Algorithms for the certified write-all problem. SIAM J. Computing, 26(5):1277\u20131283,\n1997.\n[4] H. Attiya, A. Bar-Noy, D. Dolev, D. Peleg, and R. Reischuk. Renaming in an asynchronous environment. J. ACM,\n37(3):524\u2013548, 1990.\n[5] R. Bayer. Symmetric binary b-trees: Data structure and maintenance algorithms. Acta Informatica, 1:290\u2013306,\n1972.\n[6] A. D. Birrell and B. J. Nelson. Implementing remote procedure calls. ACM Trans. Comput. Syst., 2(1):39\u201359,\n1984.\n[7] D. Bokal, B. Bre\u0161ar, and J. Jerebic. A generalization of hungarian method and hall's theorem with applications in\nwireless sensor networks. Discrete Appl. Math., 160(4-5):460\u2013470, Mar. 2012.\n[8] S. Chaudhuri, B. A. Coan, and J. L. Welch. Using adaptive timeouts to achieve at-most-once message delivery.\nDistrib. Comput., 9(3):109\u2013117, 1995.\n[9] B. S. Chlebus and D. R. Kowalski. Cooperative asynchronous update of shared memory. In STOC, pages 733\u2013739,\n2005.\n[10] A. Czygrinow, M. Han\u0107kowiak, E. Szyma\u0144ska, and W. Wawrzyniak. Distributed 2-approximation algorithm for the\nsemi-matching problem. In Proceedings of the 26th international conference on Distributed Computing, DISC'12,\npages 210\u2013222, Berlin, Heidelberg, 2012. Springer-Verlag.\n[11] G. Di Crescenzo and A. Kiayias. Asynchronous perfectly secure communication over one-time pads. In Proc. of\n32nd International Colloquium on Automata, Languages and Programming(ICALP '05), pages 216\u2013227. Springer,\n2005.\n[12] A. Drucker, F. Kuhn, and R. Oshman. The communication complexity of distributed task allocation. In Proc. of\nthe 31st annual Symp. on Principles of Distributed Computing(PODC '12), pages 67\u201376. ACM, 2012.\n[13] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distributed consensus with one faulty process. J.\nACM, 32(2):374\u2013382, 1985.\n[14] M. Fitzi, J. B. Nielsen, and S. Wolf. How to share a key. In Allerton Conference on Communication, Control, and\nComputing 2007, 2007.\n[15] C. Georgiou and A. A. Shvartsman. Do-All Computing in Distributed Systems: Cooperation in the Presence of\nAdversity. Springer, 2008.\n[16] C. Georgiou and A. A. Shvartsman. Cooperative Task-Oriented Computing: Algorithms and Complexity. Synthesis\nLectures on Distributed Computing Theory. Morgan & Claypool Publishers, 2011.\n\n29\n\n\f[17] K. J. Goldman and N. A. Lynch. Modelling shared state in a shared action model. In Logic in Computer Science,\npages 450\u2013463, 1990.\n[18] J. Groote, W. Hesselink, S. Mauw, and R. Vermeulen. An algorithm for the asynchronous write-all problem based\non process collision. Distributed Computing, 14(2):75\u201381, 2001.\n[19] L. J. Guibas and R. Sedgewick. A dichromatic framework for balanced trees. In 19th Annual Symposium on\nFoundations of Computer Science(FOCS), pages 8\u201321, 1978.\n[20] N. J. A. Harvey, R. E. Ladner, L. Lov\u00e1sz, and T. Tamir. Semi-matchings for bipartite graphs and load balancing. J.\nAlgorithms, 59(1):53\u201378, Apr. 2006.\n[21] M. Herlihy. Wait-free synchronization. ACM Transactions on Programming Languages and Systems, 13:124\u2013149,\n1991.\n[22] K. C. Hillel. Multi-sided shared coins and randomized set-agreement. In Proc. of the 22nd ACM Symp. on Parallel\nAlgorithms and Architectures (SPAA'10), pages 60\u201368, 2010.\n[23] P. C. Kanellakis and A. A. Shvartsman. Fault-Tolerant Parallel Computaion. Kluwer Academic Publishers, 1997.\n[24] S. Kentros, C. Kari, and A. Kiayias. The strong at-most-once problem. In Proc. of 26th International Symp. on\nDistributed Computing(DISC'12), pages 390\u2013404, 2012.\n[25] S. Kentros and A. Kiayias. Solving the at-most-once problem with nearly optimal effectiveness. In ICDCN, pages\n122\u2013137, 2012.\n[26] S. Kentros, A. Kiayias, N. C. Nicolaou, and A. A. Shvartsman. At-most-once semantics in asynchronous shared\nmemory. In Proc. of 23rd International Symp. on Distributed Computing(DISC'09), pages 258\u2013273, 2009.\n[27] D. Kowalski, P. M. Musial, and A. A. Shvartsman. Explicit combinatorial structures for cooperative distributed\nalgorithms. In Proceedings of the 25th IEEE International Conference on Distributed Computing Systems, ICDCS\n'05, pages 49\u201358, Washington, DC, USA, 2005. IEEE Computer Society.\n[28] D. R. Kowalski and A. A. Shvartsman. Writing-all deterministically and optimally using a nontrivial number of\nasynchronous processors. ACM Transactions on Algorithms, 4(3), 2008.\n[29] L. Lamport. The part-time parliament. ACM Trans. Comput. Syst., 16(2):133\u2013169, 1998.\n[30] B. W. Lampson, N. A. Lynch, and J. F. S-Andersen. Correctness of at-most-once message delivery protocols. In\nProc. of the IFIP TC6/WG6.1 6th International Conference on Formal Description Techniques(FORTE '93), pages\n385\u2013400. North-Holland Publishing Co., 1994.\n[31] K.-J. Lin and J. D. Gannon. Atomic remote procedure call. IEEE Trans. Softw. Eng., 11(10):1126\u20131135, 1985.\n[32] B. Liskov. Distributed programming in argus. Commun. ACM, 31(3):300\u2013312, 1988.\n[33] B. Liskov, L. Shrira, and J. Wroclawski. Efficient at-most-once messages based on synchronized clocks. ACM\nTrans. Comput. Syst., 9(2):125\u2013142, 1991.\n[34] N. Lynch and M. Tuttle. An introduction to input/output automata. CWI-Quarterly, pages 219\u2013246, 1989.\n[35] N. A. Lynch. Distributed Algorithms. Morgan Kaufmann Publishers, 1996.\n[36] G. Malewicz. A work-optimal deterministic algorithm for the certified write-all problem with a nontrivial number\nof asynchronous processors. SIAM J. Comput., 34(4):993\u20131024, 2005.\n[37] A. Z. Spector. Performing remote operations efficiently on a local computer network. Commun. ACM, 25(4):246\u2013\n260, 1982.\n[38] R. W. Watson. The delta-t transport protocol: Features and experience. In Proc. of the 14th Conf. on Local\nComputer Networks, pages 399\u2013407, 1989.\n\n30\n\n\f"}