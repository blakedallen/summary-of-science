{"id": "http://arxiv.org/abs/1201.2918v2", "guidislink": true, "updated": "2012-11-10T03:38:18Z", "updated_parsed": [2012, 11, 10, 3, 38, 18, 5, 315, 0], "published": "2012-01-13T19:12:39Z", "published_parsed": [2012, 1, 13, 19, 12, 39, 4, 13, 0], "title": "Social Norm Design for Information Exchange Systems with Limited\n  Observations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.3184%2C1201.3395%2C1201.2284%2C1201.1233%2C1201.0131%2C1201.3102%2C1201.5804%2C1201.3772%2C1201.1100%2C1201.1848%2C1201.5255%2C1201.0289%2C1201.1409%2C1201.6066%2C1201.1915%2C1201.5511%2C1201.6325%2C1201.4317%2C1201.5798%2C1201.5613%2C1201.5514%2C1201.6427%2C1201.2666%2C1201.5695%2C1201.3806%2C1201.3351%2C1201.4888%2C1201.0334%2C1201.1629%2C1201.0015%2C1201.1882%2C1201.5639%2C1201.4337%2C1201.2645%2C1201.5506%2C1201.6230%2C1201.1062%2C1201.2658%2C1201.0339%2C1201.5829%2C1201.2285%2C1201.3716%2C1201.0022%2C1201.3798%2C1201.3490%2C1201.0908%2C1201.2918%2C1201.0993%2C1201.5515%2C1201.3972%2C1201.3274%2C1201.2303%2C1201.5342%2C1201.3556%2C1201.5032%2C1201.1699%2C1201.4672%2C1201.0440%2C1201.5699%2C1201.1459%2C1201.6112%2C1201.2107%2C1201.2484%2C1201.2024%2C1201.1237%2C1201.1235%2C1201.5458%2C1201.4543%2C1201.4901%2C1201.4172%2C1201.3236%2C1201.5709%2C1201.5421%2C1201.5146%2C1201.6474%2C1201.1172%2C1201.1550%2C1201.5307%2C1201.1871%2C1201.3440%2C1201.6222%2C1201.2859%2C1201.3901%2C1201.5556%2C1201.2849%2C1201.5416%2C1201.0537%2C1201.6318%2C1201.0817%2C1201.5993%2C1201.5498%2C1201.1136%2C1201.5051%2C1201.4517%2C1201.5918%2C1201.4299%2C1201.2746%2C1201.2565%2C1201.2442%2C1201.3289%2C1201.3860&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Social Norm Design for Information Exchange Systems with Limited\n  Observations"}, "summary": "Information exchange systems differ in many ways, but all share a common\nvulnerability to selfish behavior and free-riding. In this paper, we build\nincentives schemes based on social norms. Social norms prescribe a social\nstrategy for the users in the system to follow and deploy reputation schemes to\nreward or penalize users depending on their behaviors. Because users in these\nsystems often have only limited capability to observe the global system\ninformation, e.g. the reputation distribution of the users participating in the\nsystem, their beliefs about the reputation distribution are heterogeneous and\nbiased. Such belief heterogeneity causes a positive fraction of users to not\nfollow the social strategy. In such practical scenarios, the standard\nequilibrium analysis deployed in the economics literature is no longer directly\napplicable and hence, the system design needs to consider these differences. To\ninvestigate how the system designs need to change when the participating users\nhave only limited observations, we focus on a simple social norm with binary\nreputation labels but allow adjusting the punishment severity through\nrandomization. First, we model the belief heterogeneity using a suitable\nBayesian belief function. Next, we formalize the users' optimal decision\nproblems and derive in which scenarios they follow the prescribed social\nstrategy. With this result, we then study the system dynamics and formally\ndefine equilibrium in the sense that the system is stable when users\nstrategically optimize their decisions. By rigorously studying two specific\ncases where users' belief distribution is constant or is linearly influenced by\nthe true reputation distribution, we prove that the optimal reputation update\nrule is to choose the mildest possible punishment. This result is further\nconfirmed for higher order beliefs in simulations.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.3184%2C1201.3395%2C1201.2284%2C1201.1233%2C1201.0131%2C1201.3102%2C1201.5804%2C1201.3772%2C1201.1100%2C1201.1848%2C1201.5255%2C1201.0289%2C1201.1409%2C1201.6066%2C1201.1915%2C1201.5511%2C1201.6325%2C1201.4317%2C1201.5798%2C1201.5613%2C1201.5514%2C1201.6427%2C1201.2666%2C1201.5695%2C1201.3806%2C1201.3351%2C1201.4888%2C1201.0334%2C1201.1629%2C1201.0015%2C1201.1882%2C1201.5639%2C1201.4337%2C1201.2645%2C1201.5506%2C1201.6230%2C1201.1062%2C1201.2658%2C1201.0339%2C1201.5829%2C1201.2285%2C1201.3716%2C1201.0022%2C1201.3798%2C1201.3490%2C1201.0908%2C1201.2918%2C1201.0993%2C1201.5515%2C1201.3972%2C1201.3274%2C1201.2303%2C1201.5342%2C1201.3556%2C1201.5032%2C1201.1699%2C1201.4672%2C1201.0440%2C1201.5699%2C1201.1459%2C1201.6112%2C1201.2107%2C1201.2484%2C1201.2024%2C1201.1237%2C1201.1235%2C1201.5458%2C1201.4543%2C1201.4901%2C1201.4172%2C1201.3236%2C1201.5709%2C1201.5421%2C1201.5146%2C1201.6474%2C1201.1172%2C1201.1550%2C1201.5307%2C1201.1871%2C1201.3440%2C1201.6222%2C1201.2859%2C1201.3901%2C1201.5556%2C1201.2849%2C1201.5416%2C1201.0537%2C1201.6318%2C1201.0817%2C1201.5993%2C1201.5498%2C1201.1136%2C1201.5051%2C1201.4517%2C1201.5918%2C1201.4299%2C1201.2746%2C1201.2565%2C1201.2442%2C1201.3289%2C1201.3860&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Information exchange systems differ in many ways, but all share a common\nvulnerability to selfish behavior and free-riding. In this paper, we build\nincentives schemes based on social norms. Social norms prescribe a social\nstrategy for the users in the system to follow and deploy reputation schemes to\nreward or penalize users depending on their behaviors. Because users in these\nsystems often have only limited capability to observe the global system\ninformation, e.g. the reputation distribution of the users participating in the\nsystem, their beliefs about the reputation distribution are heterogeneous and\nbiased. Such belief heterogeneity causes a positive fraction of users to not\nfollow the social strategy. In such practical scenarios, the standard\nequilibrium analysis deployed in the economics literature is no longer directly\napplicable and hence, the system design needs to consider these differences. To\ninvestigate how the system designs need to change when the participating users\nhave only limited observations, we focus on a simple social norm with binary\nreputation labels but allow adjusting the punishment severity through\nrandomization. First, we model the belief heterogeneity using a suitable\nBayesian belief function. Next, we formalize the users' optimal decision\nproblems and derive in which scenarios they follow the prescribed social\nstrategy. With this result, we then study the system dynamics and formally\ndefine equilibrium in the sense that the system is stable when users\nstrategically optimize their decisions. By rigorously studying two specific\ncases where users' belief distribution is constant or is linearly influenced by\nthe true reputation distribution, we prove that the optimal reputation update\nrule is to choose the mildest possible punishment. This result is further\nconfirmed for higher order beliefs in simulations."}, "authors": ["Jie Xu", "Mihaela van der Schaar"], "author_detail": {"name": "Mihaela van der Schaar"}, "author": "Mihaela van der Schaar", "arxiv_comment": "24 pages, 9 figures", "links": [{"href": "http://arxiv.org/abs/1201.2918v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1201.2918v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.GT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.GT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "91A80", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1201.2918v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1201.2918v2", "journal_reference": null, "doi": null, "fulltext": "Social Norm Design for Information Exchange\nSystems with Limited Observations\nJie Xu and Mihaela van der Schaar1\nAbstract- Everyday people turn to the web to exchange services, data and ideas on websites such as BitTorrent, Yahoo\nAnswers, Yelp, Amazon Mechanical Turk and more. These information exchange systems differ in many ways, but all\nshare a common vulnerability to selfish behavior and free-riding. In this paper, we build incentives schemes based on\nsocial norms. Social norms prescribe a social strategy for the users in the system to follow and deploy reputation schemes\nto reward or penalize users depending on whether they follow or deviate from the prescribed strategy when selecting\nactions. Because users in these systems often have only limited capability to observe the global system information, e.g. the\nreputation distribution of the users participating in the system, their beliefs about the reputation distribution are\nheterogeneous and biased. Such belief heterogeneity causes a positive fraction of users to not follow the social strategy. In\nsuch practical scenarios, the standard equilibrium analysis deployed in the economics literature is no longer directly\napplicable and hence, the system design needs to consider these differences. To investigate how the system designs need to\nchange when the participating users have only limited observations, we focus on a simple social norm with binary\nreputation labels but allow adjusting the punishment severity through randomization. First, we model the belief\nheterogeneity using a suitable Bayesian belief function. Next, we formalize the users' optimal decision problems and\nderive in which scenarios they follow the prescribed social strategy. With this result, we then study the system dynamics\nand formally define equilibrium in the sense that the system is stable when users strategically optimize their decisions. By\nrigorously studying two specific cases where users' belief distribution is constant or is linearly influenced by the true\nreputation distribution, we prove that the optimal reputation update rule is to choose the mildest possible punishment.\nThis result is further confirmed for higher order beliefs in simulations. It is also shown that more observations do not\nnecessarily lead to a higher efficiency. In conclusion, our proposed design framework enables the development of optimal\nsocial norms for various deployment scenarios with limited observations.\nIndex Terms- Reputation, social norm, information exchange systems, limited observations, game theory.\n\nI. INTRODUCTION\nAs the web has evolved, it has become increasingly social. People turn to the web to\nexchange ideas, data and services, as evidenced by the popularity of sites like Wikipedia, BitTorrent, Yahoo Answers, Yelp, and online labor markets like Amazon Mechanical Turk (AMT).\nWhile these systems, which we refer to as information exchange systems, differ in many ways,\nthey share a common vulnerability to selfish behavior and free-riding. For example, a worker on\nAMT may attempt to complete jobs with as little effort as possible while still being paid; a user\nin a peer-to-peer system may wish to download files without using bandwidth to upload files for\nothers. In order for these sites to thrive, participants must be properly motivated to contribute.\nDistributed optimization techniques have been applied extensively in engineering to enable the\n\n1\n\nThe authors are with Department of Electrical Engineering, University of California, Los Angeles (UCLA).\n(Email: jiexu@ucla.edu, mihaela@ee.ucla.edu.)\n\n1\n\n\fefficient usage of resources by obedient or cooperative users. Only in recent years have\nengineers started to investigate incentive issues in systems formed by self-interested users. Many\nof the existing mechanisms to combat free-riding problems rely on game-theoretic approaches\nand can be classified as either pricing mechanisms or reciprocity mechanisms. Pricing\nmechanisms are appropriate in some settings, but do not make sense for applications like Yahoo\nAnswers, Wikipedia, or Yelp, where much of the appeal is that the information is free.\nUnder a reciprocity mechanism, a user is rewarded or punished based on its behavior in the\nsystem. Rewards and punishment are typically determined according to a differential service\nscheme [1], which might require, for example, that a user who contributed heavily to the system\nin the past should receive more resources than a user who contributed less [2]. This preferential\ntreatment provides an incentive for users to cooperate, and can be implemented using either\nvirtual currency or reputation. Under a virtual currency mechanism, users are incentivized to\ncontribute through a system of rewards based on virtual currency [3][4][5][6]. However, prior\nwork shows that even optimal designs based on virtual currency cannot achieve optimal\nperformance [7]. To measure good behavior, reciprocity mechanisms frequently associate a\nrating or reputation score with each other in the system. Depending on how a user's rating is\ngenerated, reciprocity-based protocols can be classified as direct reciprocity mechanisms [8], or\nindirect reciprocity mechanisms [9]. Direct reciprocity implies that the interaction between two\nusers is influenced only by their history of interactions with each other, and not by their\ninteractions with other users. Though easy to implement, direct reciprocity requires frequent\ninteractions between two users in order to establish accurate mutual ratings. This is restrictive in\nsystems characterized by high churn, asymmetry of interests, or infrequent interactions between\nany pair of users, such as most peer production systems, online labor markets, and review sites.\nProtocols that are based on indirect reciprocity typically assign to each user a global reputation\n[10] based on its past interactions with all other users in the system. A differential service\nscheme recommends actions (e.g., \"share a file with this user\" or \"do not share a file with this\nuser\") based only on the reputations of users, and not on their entire history of interactions. Much\n2\n\n\fof the existing work on reputation mechanisms is concerned with practical implementation\ndetails. Some focuses on effective information gathering techniques which differ in how the\nglobal reputation is calculated and propagated (e.g., through efficient information aggregation\n[11] or secure user identification [12]). Empirical studies have examined the impact of reputation\non a seller's prices and sales [13][14][15], motivation for participating in reputation-based\nmechanisms [16], among other things. There has also been some work analytically exploring the\nuse of reputation mechanisms to combat moral hazard in a repeated games setting [17][18][19],\nincluding some that does not require the presence of a trusted centralized system [20]. This work\ntypically considers one (or a few) long-lived seller with many short-lived buyers, which is not\nappropriate for information exchange systems where there are many interacting users playing the\nrole of buyer or seller or both, contributing and seeking information.\nTo rigorously capture the impact of various strategy and protocol design choices on information exchange systems, the authors of [21] propose a framework using social norms which\nwere originally designed to sustain cooperation in a community with a large population of individuals participating in anonymous random matching games [22][23][24]. In an incentive scheme based on a social norm, each individual is assigned a dynamic label indicating its reputation\nor status based on past behavior, and individuals with different labels are treated differently by\nothers in the system. Hence, a social norm can be adopted easily in social communities with an\ninfrastructure that collects, processes, and delivers information about individuals' behavior.\nWe build incentive schemes for information exchange systems based on social norms. In\ninformation exchange systems, users often have imperfect knowledge of the global information,\nin particular, the reputation distribution of the participating users. For example, users observe the\nreputations of a limited number of other users on the website and form (probably biased) beliefs\nof the reputation distribution. Moreover, users' beliefs are heterogeneous since the observations\nof various users are different. Such belief heterogeneity causes a positive fraction of users to not\nfollow the social strategy. In contrast, standard equilibrium analysis [21][23] requires that all\nusers follow the social strategy and is conducted under two assumptions: (1) users have\n3\n\n\fhomogenous and accurate knowledge about the reputation distribution; and (2) users believe that\nall users obey the social strategy. These assumptions [21][23] hold only if users have unlimited\nobservations and hence, they have perfect knowledge about the reputation distribution of the\nparticipating users. However, they do not hold in many practical systems where users only have\nlimited observations and the system dynamics does not evolve to an equilibrium where all users\nfollow the social strategy. Instead users have heterogeneous beliefs about the reputation\ndistribution and they tend to trust other users with high reputations and distrust those with low\nreputations. Therefore, users' limited observation capability leads to a different system design.\nThe main contributions of this paper are summarized as follows:\n\uf06c\n\nWe propose a simple class of social norms with binary reputations but allow adjusting the\npunishment severity through randomization. This class is simple and easy to implement\nwhile has also been shown to be close to the optimal strategy for the unlimited\nobservations case in [21]. (Note that this strategy includes the contagion strategy [23] as a\nspecial case.) Similar randomization approach is also used in [22].\n\n\uf06c\n\nWe model the users' heterogeneous beliefs of the reputation distribution due to limited\nobservations using a Bayesian belief model, which captures the feature that the\nobservation depends on the current true reputation distribution and that more observations\nlead to more accurate information about the reputation distribution.\n\n\uf06c\n\nWe prove that users follow the social strategy only if their beliefs about the reputation\ndistribution are above certain thresholds, i.e., they need to have sufficient \"trust\" in the\nsociety. Using this result, we can show that, in most interesting scenarios, the optimal\ndesign is to use the mildest possible punishment, thereby leading to a different social\nnorm design than in the unlimited observations cases.\n\nThe rest of this paper is organized as follows. Section II describes the basic setup, the social\nnorms and builds the belief model. Section III investigates users' decision problem. System\ndynamics and the equilibrium are then studied. In Section IV, the impact of punishment on the\nequilibrium performance is investigated. The optimal design is derived for two specific Bayesian\n4\n\n\fbelief functions. Simulations are conducted in Section V followed by conclusions in Section VI.\nII. SYSTEM MODEL\nA. Setup\nWe consider an information exchange system where users request and provide information or\nresources. We utilize the widely-used continuum model (mass 1), implicitly assuming that the\nuser population is large and static. The system is modeled as a discrete-time system where time\nis divided into periods. When a requester generates a task, it is posted on the website and a\nprovider is assigned to solve the task. We assume that there is no price associated with the task\n(as in Yelp, Yahoo Answers and etc.), the provider is the only strategic part that needs to decide\nwhether or not to exert effort to solve the task. Upon accepting, the provider incurs a cost c to\nfulfill the task while the requester receives a benefit b . We assume b \uf03e c \uf03e 0 to make providing\nthe service socially valuable and denote \uf067 \uf03d b / c as the benefit-to-cost ratio. This is a simple\ngift-giving game (see Fig. 1) in which the dominant strategy for the provider is not to provide\nservice. Incentives can be provided if the provider is long-lived in the system and will also\nbecome a requester in the future. We assume that users discount the future utility by a constant\nrate \uf062 \uf0ce \uf028 0,1\uf029 . For the accurate modeling for the real systems, we assume that in each period,\neach user requests a task to be solved and another user is randomly assigned to solve this task.\nThis random matching model is common in the economics literature [22][23][24]. Nevertheless,\nthe analysis could also apply to the scenarios where a fraction \uf06c \uf0ce \uf05b 0,1\uf05d of the population\ngenerates tasks in each period. The parameter \uf06c only indicates the request arrival rate of the\nsystem but does not change the result. For the considered case \uf06c \uf03d 1 , each user is a requester as\nwell as a provider who is assigned to another user's requested task.\nB. Punishment adjustable social norm\nA social norm \uf06b , which is designed by the protocol designer, is composed of a social strategy\n\n\uf073 , a reputation update rule \uf074 , and a reputation set \uf051 . Each user is tagged with a reputation \uf071\nrepresenting its social status. We consider only two available reputation labels for the users\n\uf051 \uf03d \uf07b0,1\uf07d with \uf071 \uf03d 1 indicating a good status and \uf071 \uf03d 0 indicating a bad status. Denote the\n\n5\n\n\fsocial strategy by the mapping \uf073 : \uf051 \uf0ae \uf041 , where \uf051 is the reputation set of the requester and\n\uf041 \uf03d \uf07b0,1\uf07d stands for the action set of the provider. The action a \uf03d 1 represents the case where\n\nthe provider offers the service while when a \uf03d 0 it does not provide service. Simply, the social\nstrategy is \uf073 (1) \uf03d 1, \uf073 (0) \uf03d 0 . The social strategy favors good users in such a way that the\nproviders are suggested to provide service only to good requesters but not to provide service to\nbad requesters. This strategy has the similar merit as the well-known Tit-for-Tat (TfT) strategy in\nrewarding for cooperative behaviors and punishing for non-cooperative behaviors. However, TfT\nstrategy requires direct reciprocity between interacting users while the social strategy that we use\nis applicable in systems where users have infrequent interactions and with indirect reciprocity.\nThe social norm also imposes a reputation update rule based on the action that the provider\ntakes. Intuitively, users who follow the social strategy should receive good reputations and those\nwho do not should receive bad reputations. Denote the reputation update rule by the mapping\n\n\uf074 : \uf051\uf0b4 \uf051 '\uf0b4 \uf041 \uf0ae [0,1] , where \uf071 \uf0ce \uf051 is the provider's reputation, \uf071 ' \uf0ce \uf051\uf0a2 is the requester's\nreputation and [0,1] indicates the probability that the provider has a good reputation in the next\nperiod. The update rule that we use is: \uf074 \uf028\uf071 , 0, a \uf029 \uf03d \uf071 , \uf022a and\n\uf0ec\uf0ef1,if a \uf03d \uf073 \uf0281 \uf029\n\uf0ef\uf0ee 0, if a \uf0b9 \uf073 \uf0281 \uf029\n\n\uf074 \uf0281,1, a \uf029 \uf03d \uf0ed\n\n\uf0ec\uf0ef\uf061 ,if a \uf03d \uf073 \uf0281 \uf029\n\uf0ef\uf0ee 0, if a \uf0b9 \uf073 \uf0281 \uf029\n\n\uf074 \uf028 0,1, a \uf029 \uf03d \uf0ed\n\n(1)\n\nEssentially, if the provider deviates from the prescribed social strategy when meeting a good\nrequester2, its reputation drops to 0; if the bad provider follows the prescribed social strategy, it\nrestores a good reputation with probability \uf061 \uf0ce \uf05b 0,1\uf05d . Hence, for a user to receive service when it\nbecomes a requester in the future, it needs to follow the social strategy as a provider in the\ncurrent period. The parameter \uf061 adjusts the severity of punishment of the social norm which\nneeds to be designed by the system designer. Such randomization can be easily implemented by\nthe central entity that maintains and processes users' reputations. A similar randomization\napproach is also used in [22] to adjust the punishment severity. For \uf061 \uf03d 1 , the punishment is the\nmildest, allowing the bad provider to repair its reputation after one-time cooperation with a good\n2\n\nWe will use in the remainder of this paper the term \"good/bad\" users to refer to the users with good/bad reputations.\n\n6\n\n\frequester; for \uf061 \uf03d 0 , the punishment is the harshest, preventing the bad provider from having a\ngood reputation again in the future no matter how it behaves; for \uf061 \uf0ce \uf028 0,1\uf029 , the expected time\nperiods for which the user remains in the bad reputation is at least 1 / \uf061 . Even though we focus on\na system using only binary reputation labels, randomization affects the punishment severity\nsimilarly as a system using multiple (more than 2) reputation labels. We portray the\naforementioned reputation update rule in Fig. 2.\nC. Belief heterogeneity\nIn this subsection, we model the users' belief heterogeneity due to their limited observations.\nBecause users are far-sighted, their decisions depend on how they evaluate the status of the\nsociety, i.e., the reputation distribution of the system. Since we are considering a binary\nreputation system, the reputation distribution can be fully described by the fraction of users with\ngood reputations, which we define as the social reputation \uf072s and use in the rest of this paper.\nIn each period, each user observes the system (e.g. observes the reputations of a number of\nother users) and form a belief of the social reputation. The beliefs are different for different users.\nAlso, note that a user's belief also varies across time because it makes different observations in\neach period. For a given social reputation \uf072s , we model users' beliefs of the social reputation as a\nprobability measure on the support \uf05b 0,1\uf05d . Specifically, conditional on the true social reputation,\nusers believe with probability f \uf028 \uf072 | \uf072 s \uf029 that the social reputation is \uf072 . We introduce the\nobservation granularity M to describe how much users are able to observe the system. The\ninterpretation of such granularity can be the number of other users' reputations that the strategic\nuser is able to observe by sampling the website or the number of past interactions that it is able to\nmemorize. The larger M is, the more accurate beliefs that the users have about the social\nreputation should be. In the following, we describe a belief function that satisfies this property\n(similar belief function is used in [25] to model users' posterior beliefs after observations):\nf (\uf072 | \uf072s ) \uf03d\n\n\uf0e6M \uf0f6 m\nM \uf02dm\nf m ,M \uf028 \uf072 \uf029 ,\n\uf0f7 \uf072 s \uf0281 \uf02d \uf072 s \uf029\nm\uf03d0\n\uf0f8\nM\n\n\uf0e5 \uf0e7\uf0e8 m\n\n(2)\n\nwith\n\n7\n\n\ff m ,M \uf028 \uf072 \uf029 \uf03d\n\n\uf047 \uf028M \uf02b 2\uf029\nM \uf02dm\n\uf072 m \uf0281 \uf02d \uf072 \uf029\n\uf047 \uf028 m \uf02b 1 \uf029 \uf047 \uf028 M \uf02d m \uf02b 1\uf029\n\n(3)\n\nand where \uf047 \uf028 *\uf029 denotes the gamma function. Essentially f m , M \uf02d m \uf028 \uf072 \uf029 is the beta distribution\nB \uf028 m \uf02b 1, M \uf02d m \uf02b 1\uf029 . In Bayesian statistics, the beta distribution can be seen as the posterior\n\nprobability of the parameter \uf072 of a binomial distribution after observing m successes and\nM \uf02d m failures. Hence, f m , M \uf02d m \uf028 \uf072 \uf029 can be interpreted as the user's belief of the social reputation\n\nafter observing m good users and M \uf02d m bad users. Furthermore, suppose that the matching\nprocess is uniformly random, the number of observations of good users also follows a binomial\nrandom distribution with parameter \uf072s . With the beta distribution, users' belief distribution is\ncontinuous and parameterized by M and \uf072s . Let us discuss the extreme cases:\n\uf06c M \uf03d 0 . f m , M \uf028 \uf072 \uf029 \uf03d f 0,0 \uf028 \uf072 \uf029 is constant and hence, f \uf028 \uf072 \uf029 is constant. It implies that users'\nbeliefs of the social reputation are uniformly random.\n\uf06c M \uf0ae \uf0a5 . In this case, f \uf028 \uf072 \uf029 \uf0ae I \uf028 \uf072 \uf02d \uf072 s \uf029 where I \uf028 *\uf029 is the indicate function. It implies that\nas the observation granularity becomes infinite, users have perfect knowledge of the social\nreputation. This result is obtained in the following proposition.\nProposition 1: For a given \uf072s , \uf022\uf064 , \uf022\uf0f2 , there exists M \uf028\uf064 \uf029 large such that \uf022M \uf0b3 M \uf028\uf064 \uf029 ,\n\uf072s \uf02b\uf064\n\n\uf0f2 f \uf028 \uf072 | \uf072 \uf029 d \uf072 \uf0b3 1\uf02d \uf0f2\ns\n\n(4)\n\n\uf072 \uf03d \uf072s \uf02d\uf064\n\nProof: Omitted due to space limitation. The proof can be found in [27]. \u25a0\nIII. SYSTEM DYNAMICS AND EQUILIBRIUM\nIn this section, we discuss the system dynamics and formally define the Bayesian-Nash\nequilibrium. For this, we first need to formalize the provider's decision problem and characterize\nthe social reputation distribution that arises in the steady-state in our model.\nA. User's decision problem\nWe begin by investigating a typical provider's decision problem assuming that it has a belief\n\n\uf072 . The provider's decision will be based on its own reputation \uf071 , the requester's reputation \uf071 \uf0a2\nand its belief \uf072 toward the social reputation. The provider chooses an action a \uf028\uf071 , \uf071 \uf0a2 | \uf072 \uf029 \uf0ce \uf041 to\n\n8\n\n\fmaximize its total expected discounted utility. Depending on which action the provider takes, the\nreputation transition follows the reputation update rule. The provider will follow the social\nstrategy if the long-run payoff is larger than the payoff by deviating and will deviate otherwise.\nFor the social strategy \uf073 , the stage payoff is \uf070 \uf028\uf073 | \uf071 , \uf071 \uf0a2, \uf072 \uf029 when the provider chooses an action\ndetermined by \uf073 and holds the belief \uf072 and the long-run payoff is given by\nV \uf028\uf073 | \uf071 , \uf071 \uf0a2, \uf072 \uf029 \uf03d \uf070 \uf028\uf073 | \uf071 , \uf071 \uf0a2, \uf072 \uf029\n\n\uf028\n\n\uf02b \uf062 \uf072V \uf028\uf073 | \uf074 \uf028\uf071 , \uf071 \uf0a2, \uf073 \uf029 ,1, \uf072 \uf029 \uf02b \uf0281 \uf02d \uf072 \uf029 V \uf028\uf073 | \uf074 \uf028\uf071 , \uf071 \uf0a2, \uf073 \uf029 , 0, \uf072 \uf029\n\n\uf029\n\n(5)\n\nAs in [26], we assume that the provider believes that bad users play defect and good users\nfollow the social strategy when calculating its payoff. For example, if the provider has a belief\n\n\uf072 \uf03d 0 , it believes that no other user will provide service to itself when it requests service in the\nfuture even if it has a good reputation. The next proposition shows that the provider needs to\nhave sufficient \"trust\" in the society in order for it to be willing to follow the social strategy.\nProposition 2: The optimal action a * \uf028\uf071 , \uf071 \uf0a2 | \uf072 \uf029 for the provider with belief \uf072 to follow the\nprescribed social strategy has a threshold property, i.e.,\n\uf0ec1 \uf03d \uf073 (1) , if \uf072 \uf0b3 \uf072 G\n;\na * \uf0281,1 | \uf072 \uf029 \uf03d \uf0ed\n\uf0ee 0, if \uf072 \uf03c \uf072 G\n\n\uf0ef\uf0ec1 \uf03d \uf073 \uf0281 \uf029 , if \uf072 \uf0b3 \uf072 B\na * \uf028 0,1 | \uf072 \uf029 \uf03d \uf0ed\n\uf0ef\uf0ee 0, if \uf072 \uf03c \uf072 B\n\n(6)\n\nand a * \uf028\uf071 , \uf071 \uf0a2 | \uf072 \uf029 \uf03d \uf073 \uf028\uf071 \uf0a2 \uf029 for all other cases where \uf072G , \uf072 B are threshold beliefs determined by\nthe system parameters \uf062 , b, c, \uf061 . Moreover, \uf072G \uf0a3 \uf072 B and equality holds only if a \uf03d 1 .\nProof: (1) First we consider the decision problem when the provider has a good reputation.\nObviously, if the requester's reputation is bad, it is optimal that the provider follows the social\nstrategy and plays defect (not provide). If the requester's reputation is good, the provider may\nhave incentives to deviate from the social strategy due to the instant cost. At the decision point,\nthe provider meets a good requester, the expected stage payoff by following the social strategy is\n\n\uf070 \uf028\uf073 \uf029 \uf03d \uf072 b \uf02d c and the stage payoff by deviating is \uf070 (\uf073 \uf0a2) \uf03d \uf072 b . Deviation causes its reputation to\ndrop to bad. The difference in payoffs occurs when it has not met with another good requester\nyet and hence, it does not have the opportunity to restore its reputation or it has met with another\ngood requester, provided the service but remains a bad reputation according to the punishment\n\n9\n\n\fprobability \uf061 . This makes it lose the instant payoff b at that time period as the requester because\nof the bad reputation. The utility loss X in the current period can be recursively calculated by\n(7)\nX \uf03d \uf0281 \uf02d \uf072 \uf029 \uf062 X \uf02b \uf072 \uf028 b \uf02b \uf0281 \uf02d \uf061 \uf029 \uf062 X \uf029\nHence,\nX\uf03d\n\n\uf072b\n\n1 \uf02d \uf062 \uf0281 \uf02d \uf072\uf061 \uf029\n\n(8)\n\nTo make following the social strategy incentive compatible, it should be larger than the cost c ,\n\uf062\uf072 b\n\uf0b3c\n(9)\n1 \uf02d \uf062 \uf0281 \uf02d \uf072\uf061 \uf029\nwhich then yields the condition on the user's belief\n1\uf02d \uf062\n\uf072\uf0b3\n\uf040 \uf072G\n\uf062 \uf028\uf067 \uf02d \uf061 \uf029\n\n(10)\n\nHence, the provider with a good reputation follows the social strategy only if \uf072 \uf0b3 \uf072G\n(2) Next we consider the incentives of the bad users. If the requester has a bad reputation, it is\nalso obvious that the provider will follow the social strategy and play defect. If the requester has\na good reputation, the decision depends on the provider's belief of the social reputation. At the\ndecision point, the expected payoff is \uf070 (\uf073 ) \uf03d \uf02dc if the provider follows the social strategy and is\n\n\uf070 (\uf073 \uf0a2) \uf03d 0 if it deviates. Following the social strategy increases its reputation to 1 with probability \uf061 . If the realization is that the typical user remains bad, the expected future utility is the\nsame as it deviates. Hence, we only need to consider the realization that the typical user restores\na good reputation. Then the analysis is similar to the first case. The loss in the future utility needs\nto satisfy the following to make incentive compatible for users to follow the social strategy,\n\uf061\uf062\uf072 b\n(11)\n\uf0b3c\n1 \uf02d \uf062 \uf0281 \uf02d \uf072\uf061 \uf029\nwhich yields,\n\uf072\uf0b3\n\n1\uf02d \uf062\n\uf040 \uf072B\n\uf062 \uf061 \uf028\uf067 \uf02d 1\uf029\n\n(12)\n\nThe provider with a bad reputation follows the social strategy only if \uf072 \uf0b3 \uf072 B . \u25a0\nThe above proposition is consistent with our intuitions: the user will only follow the social\nstrategy if it believes the society is in a sufficiently good status. However, it provides more\n10\n\n\finsights about users' behaviors: (1) Because both \uf072G , \uf072 B are strictly positive, there are always a\npositive fraction of users who deviate because of the heterogeneous beliefs; (2) Increasing the\npunishment (smaller \uf061 ) prevents fewer good users from deviating (as \uf072G becomes smaller)\nwhile it gives more bad users incentives to deviate (as \uf072 B becomes larger); (3) The incentive for\nbad users to cooperate is always no larger than that for good users since \uf072G \uf0a3 \uf072 B . The following\ncorollary is a direct result if both belief thresholds are larger than 1 and hence, no user cooperates.\nCorollary 1. No cooperation can be sustained if\n1\uf02d \uf062\n\uf062 \uf028\uf067 \uf02d \uf061\n\n\uf029\n\n(13)\n\n\uf03e 1.\n\nThe above condition highlights that when users are too impatient (small \uf062 ), the benefit-tocost ratio is too small (small \uf067 ) or the punishment is too mild (large \uf061 ), no cooperation can be\nsustained. However, when designing information exchange systems, we are interested in\nsustaining cooperation among the self-interested users and hence, we next derive conditions and\nthe associated system designs to achieve this.\nB. Dynamics and Equilibrium\nSuppose initially the social reputation is \uf072s . Limited observations induce heterogeneous beliefs\nf \uf028 \uf072 | \uf072 s \uf029 among users. Users optimize their strategies a* according to Proposition 2; these\n\n\uf028\n\n\uf029\n\nstrategies induce dynamics in the new social reputation \uf046 \uf072 s , a* . The equilibrium requires a\nconsistency check: the steady state social reputation remains invariant, i.e.\n\n\uf072 s \uf03d \uf046 \uf028 \uf072 s , a* \uf029\n\n(14)\n\nDefinition 1. (Bayesian-Nash equilibrium) Given \uf062 , \uf067 , \uf061 , M , let \uf072s be a social reputation,\nf \uf028 \uf072 | \uf072 s \uf029 be the induced belief distribution due to limited observations, and a* be the strategy\n\n\uf028\n\n\uf029\n\nfor the users. We say that \uf072 s , f , a* constitutes an equilibrium if\n1. Users adopt the optimal strategy \uf061 * to maximize their expected utilities (as in Proposition 2).\n\n\uf028\n\n\uf029\n\n2. The invariant property holds \uf072 s \uf03d \uf046 \uf072 s , a* .\nIt is worth noting that the users' optimal strategy does not rely on the current social reputation\n\n\uf072 s since the threshold beliefs are only functions of \uf062 , \uf067 , \uf061 but not \uf072s . However, because the\nbelief distribution is induced by \uf072s , the fraction of users who follow the social strategy is thus\n11\n\n\finfluenced by \uf072s , which in turn determines the social reputation in the next period. If we denote\n\n\uf044 \uf028 \uf072 s \uf029 \uf03d \uf046 \uf028 \uf072 s , a* \uf029 \uf02d \uf072 s as the change in the social reputation, we can calculate it as follows:\n\uf044 \uf028 \uf072 s \uf029 \uf03d \uf061 \uf0281 \uf02d \uf072 s \uf029 \uf072 s F \uf028 \uf072 \uf0b3 \uf072 B | \uf072 s \uf029 \uf02d \uf072 s2 F \uf028 \uf072 \uf0a3 \uf072 G | \uf072 s \uf029 ,\n\uf031\uf034\uf034\uf034\uf034\uf032\uf034\uf034\uf034\uf034\uf033 \uf031\uf034\uf034\n\uf034\uf032\uf034\uf034\uf034\n\uf033\nbad to good\n\n(15)\n\ngood to bad\n\nwith\nF \uf028 \uf072 \uf0b3 \uf072B | \uf072s \uf029 \uf03d\n\n1\n\n\uf0f2\n\nf \uf028 \uf072 | \uf072s \uf029 d \uf072 ,\n\nF \uf028 \uf072 \uf0a3 \uf072G | \uf072 s \uf029 \uf03d\n\n\uf072 \uf03d \uf072B\n\n\uf072G\n\n\uf0f2 f \uf028\uf072 | \uf072 \uf029d\uf072\ns\n\n(16)\n\n\uf072 \uf03d0\n\nThe first part in (15) is the fraction of users whose reputations change from bad to good and\nthe second part is the fraction of users whose reputations change from good to bad. To constitute\nan equilibrium, it is sufficient and necessary that \uf044 \uf028 \uf072 s \uf029 \uf03d 0 . However, we are more interested in\nwhether such an equilibrium is stable if there are some disturbances (e.g. small reputation update\nerrors). The following proposition provides the condition for the stable equilibrium.\nProposition 3: (stable equilibrium) The equilibrium with \uf072s is stable if and only if\nd \uf044 \uf028 \uf072s \uf029\n\uf044 \uf028 \uf072 s \uf029 \uf03d 0 and\n\uf03c0\nd \uf072s\n\n(17)\n\nProof: \uf044 \uf028 \uf072 s \uf029 \uf03d 0 is the condition for equilibrium. Because \uf044 \uf028 \uf072 s \uf029 is continuous in \uf072s , it is also\nsufficient and necessary that the first derivative is negative. \u25a0\nNow we study the conditions under which the stable equilibrium exists.\nProposition 4: Given \uf062 , \uf067 , \uf061 , M , the existence of the stable equilibrium depends on \uf072 B .\n1. If \uf072 B \uf03e 1 , \uf072 s \uf03d 0 is the unique stable equilibrium.\n2. If \uf072 B \uf0a3 1 , there exists at least one stable equilibrium \uf072 s \uf0ce \uf028 0,1\uf029 .\nProof: (1) If \uf072 B \uf03e 1 , F \uf028 \uf072 \uf0b3 \uf072 B | \uf072 s \uf029 \uf03d 0 for all \uf072s and hence, \uf044 \uf028 \uf072 s \uf029 \uf0a3 0 . Equality holds only for\n\n\uf072 s \uf03d 0 . It is also obvious that the first derivative at \uf072 s \uf03d 0 is negative, therefore it is the only\nstable equilibrium.\n(2) If \uf072 B \uf0a3 1 , we see that \uf044 \uf028 \uf072 s \uf03d 1\uf029 \uf03c 0, \uf044 \uf028 \uf072 s \uf0ae 0 \uf029 \uf03e 0 . Because \uf044 \uf028 *\uf029 is a continuous function\nin \uf072s , it is guaranteed that there exists at least one solution to \uf044 \uf028 \uf072 s \uf029 \uf03d 0 and the first derivative\nis negative. Moreover, notice that \uf072 s \uf03d 0 is not a stable equilibrium. \u25a0\nProposition 4 proves that neither full efficiency nor zero efficiency occurs in the stable\n\n12\n\n\fequilibrium in the limited observations case. As we will see later, the actual efficiency will\ndepend on the punishment severity which needs to be carefully designed by the system designer.\nBefore proceeding to that, we compare the achievable efficiency for limited observations case\nwith that for the unlimited observations case to illustrate the different design aspects.\nC. Unlimited observations\nIn this subsection, we investigate how the system evolves if users make unlimited\nobservations (i.e. f \uf028 \uf072 | \uf072 s \uf029 \uf03d I \uf028 \uf072 \uf02d \uf072 s \uf029 ) to illustrate why the system design should be different\nthan in the limited observations case. Suppose the system starts with an initial social reputation\n\n\uf072 s0 \uf0ce [0,1] , we are interested in which long-run state \uf072 st \uf0ae\uf0a5 that the system will be trapped in.\nProposition 5: With unlimited observations, the long-run system state is\n(1) If \uf072 s0 \uf0b3 \uf072 B , \uf072 st \uf0ae\uf0a5 \uf03d 1 ; (2) If \uf072 s0 \uf0a3 \uf072G , \uf072 st \uf0ae\uf0a5 \uf03d 0 ; (3) If \uf072G \uf03c \uf072 s0 \uf03c \uf072 B , \uf072 st \uf0ae\uf0a5 \uf03d \uf072 s0 .\nProof: Omitted due to space limitation. The proof can be found in [27]. \u25a0\nWe see that, in unlimited observations case, appropriately choosing the initial social reputation\ncan lead to full efficiency while starting from the wrong initial social reputation leads to zero\nefficiency regardless of the choice of \uf061 . This is quite different from the limited observations\ncase where the full efficiency can never be achieved while zero efficiency also does not occur in\na stable equilibrium. The achievable efficiency depends on the punishment severity of the social\nnorm and hence, this needs to be carefully designed as discussed in the next section.\nIV. OPTIMAL PUNISHMENT DESIGN\nThe minimum social reputation beliefs \uf072G , \uf072 B that sustain cooperation are determined by the\npunishment. The harsher the punishment is (smaller \uf061 ), fewer good providers deviate while\nalso fewer bad providers cooperate to restore their reputations. Hence, when designing the\npunishment, the tension between the incentives to the good and bad users needs to be considered.\nIn this section, we characterize the impact of punishment on the achievable system efficiency. In\nthis paper we are interested in maximizing the cooperation among the users and hence, we use\nthe social reputation, i.e. the fraction of good users in the system, as the efficiency metric.\n\n13\n\n\fThe objective of the system designer in our model is to choose the optimal punishment \uf061 ,\ngiven the network environment parameters \uf062 , b, c, M such that the social reputation is\nmaximized (hence the probability that users cooperate is also maximized which leads to the\nmaximized social welfare). Formally, the design problem is to solve\nmaximize \uf072 s\n\uf061\n\nsubject to\n\nd \uf044 \uf028 \uf072s \uf029\n\uf044 \uf028 \uf072 s \uf029 \uf03d 0,\n\uf03c0\n\n(18)\n\n\uf072s\n\nIn the following, we establish bounds on the achievable efficiency.\nProposition 6. Fix \uf062 , \uf067 , M and fix \uf061 , then the robust equilibrium \uf072 s* is bounded as follows\n\n\uf061 \uf0281 \uf02d \uf072 B \uf029\n\uf061 \uf0281 \uf02d \uf072 B \uf029\n\nM \uf02b1\n\nM \uf02b1\n\n\uf028\n\n\uf02b 1 \uf02d \uf0281 \uf02d \uf072 G \uf029\n\nM \uf02b1\n\n\uf029\n\n\uf0a3\uf072 \uf0a3\n*\ns\n\n\uf061 \uf0281 \uf02d \uf072 BM \uf02b1 \uf029\n\n\uf061 \uf0281 \uf02d \uf072 BM \uf02b1 \uf029 \uf02b \uf072 GM \uf02b1\n\n(19)\n\nwhere \uf072G , \uf072 B are determined in Proposition 2.\nProof: Omitted due to space limitation. The proof can be found in [27]. \u25a0\nCorollary 2. Fix \uf062 , \uf067 , M , for large \uf067 , the stable equilibrium \uf072 s* is bounded away from 1,\n\uf0e6 1\uf02d \uf062 \uf0f6\n\uf072 \uf0a3 1 \uf02d \uf0e7\uf0e7\n\uf0f7\uf0f7\n\uf0e8 \uf062 \uf028\uf067 \uf02d 1\uf029 \uf0f8\n*\ns\n\nM \uf02b1\n\n, \uf022 \uf061 \uf0ce \uf05b 0, 1\uf05d\n\n(20)\n\nProof: Simply combining Proposition 5 and the fact that the upper bound is increasing in \uf061\nwhen \uf067 is large yields the result. The right hand side is derived by choosing \uf061 \uf03d 1 .\u25a0\nThe above result shows that the upper bound depends on the granularity of observations. If\nthe system designer wants to achieve a higher efficiency, it is necessary that users are able to\nmake more observations to acquire more accurate reputation distribution information. (Though\nhaving more observations may not be the sufficient condition.) In some systems, the number of\nobservations can be designed by the designer. For example, the website designer may only allow\nusers to access the reputations of a limited number of other users due to privacy and security\nconcerns. Therefore the tradeoff between efficiency and privacy needs to be carefully considered.\nHowever, in this paper, we assume that the number of observations is exogenously determined.\nThe mildest punishment maximizes the efficiency upper bound for large \uf067 . In the following\nwe consider several specific cases of limited observations which induce different user belief\n\n14\n\n\fdistributions and show that the mildest punishment does indeed maximize the efficiency.\nA. Example 1: M \uf03d 0 (constant belief distribution)\nWe consider the simplest case M \uf03d 0 , i.e. users have no observation. In the belief model that\nwe use, M \uf03d 0 corresponds to the case that users have a (constant) uniform belief over all\npossible social reputations, namely f \uf028 \uf072 | \uf072 s \uf029 \uf03d 1 and\nF \uf028 \uf072 \uf0b3 \uf072 B | \uf072 s \uf029 \uf03d 1 \uf02d \uf072 B , F \uf028 \uf072 \uf0a3 \uf072G | \uf072 s \uf029 \uf03d \uf072G\n\n(21)\n\nFor this simple case, we are able to explicitly solve the unique stable equilibrium.\n\uf072 s* \uf03d\n\n\uf061 \uf0281 \uf02d \uf072 B \uf029\n\uf061 \uf0281 \uf02d \uf072 B \uf029 \uf02b \uf072 G\n\n1\uf02d \uf062\n\uf062 \uf028\uf067 \uf02d 1\uf029\n\uf03d\n1\uf02d \uf062\n1\uf02d \uf062\n\uf02b\n\uf061\uf02d\n\uf062 \uf028\uf067 \uf02d 1\uf029 \uf062 \uf028\uf067 \uf02d \uf061 \uf029\n\n\uf061\uf02d\n\n(22)\n\nIt is equivalent to consider the maximization problem,\n\uf0e6\n1\uf02d \uf062 \uf0f6\nm ax \uf0e7\uf0e7 \uf061 \uf02d\n\uf0f7 \uf028\uf067 \uf02d \uf061 \uf029\n\uf061\n\uf062 \uf028 \uf067 \uf02d 1 \uf029 \uf0f7\uf0f8\n\uf0e8\n\n(23)\n\nThe objective function is a quadratic function. The maximum is achieved at\n\uf0ec\uf0ef \uf067\uf062 \uf028 \uf067 \uf02d 1\uf029 \uf02b 1 \uf02d \uf062 \uf0fc\uf0ef\n, 1\uf0fd\n2 \uf062 \uf028 \uf067 \uf02d 1\uf029\n\uf0ef\uf0fe\n\uf0ee\uf0ef\n\n\uf061 * \uf03d min \uf0ed\n\n(24)\n\nIf the benefit-to-cost ratio \uf067 is large, the first term in (24) is also larger than 1. Such condition\ncan be easily satisfied (e.g. \uf067 \uf03e 2 ). Hence, in most scenarios, choosing the mildest punishment,\nnamely \uf061 \uf03d 1 , is optimal for the efficiency maximization.\nProposition 7: Fix \uf062 , \uf067 and for M \uf03d 0 , the optimal punishment rule is\n\uf0ec\uf0ef \uf067\uf062 \uf028 \uf067 \uf02d 1 \uf029 \uf02b 1 \uf02d \uf062 \uf0fc\uf0ef\n, 1\uf0fd\n2 \uf062 \uf028\uf067 \uf02d 1\uf029\n\uf0ef\uf0fe\n\uf0ee\uf0ef\n\n\uf061 * \uf03d m in \uf0ed\n\n(25)\n\nand the induced stable equilibrium is\n2\n\uf0ec\n1\uf0e6\n1\uf02d \uf062 \uf0f6\n\uf0ef\n\uf0e7\uf067 \uf02d\n\uf0f7\n4\uf0e8\n\uf062 \uf028\uf067 \uf02d 1 \uf029 \uf0f8\n\uf067\uf062 \uf028 \uf067 \uf02d 1 \uf029 \uf02b 1 \uf02d \uf062\n\uf0ef\n, if\n\uf03c1\n\uf0ef\n2\n2 \uf062 \uf028\uf067 \uf02d 1 \uf029\n\uf0f6\n\uf0ef1\uf0e6\n*\n1\n1\n\uf062\n\uf062\n\uf02d\n\uf02d\n\uf072s \uf03d \uf0ed \uf0e7\uf067 \uf02d\n\uf0f7 \uf02b\n\uf062 \uf028\uf067 \uf02d 1 \uf029 \uf0f8\n\uf062\n\uf0ef4\uf0e8\n\uf0ef\n\uf0ef1 \uf02d 1 \uf02d \uf062 , if \uf067\uf062 \uf028 \uf067 \uf02d 1 \uf029 \uf02b 1 \uf02d \uf062 \uf0b3 1\n\uf0ef\n2 \uf062 \uf028\uf067 \uf02d 1\uf029\n\uf062 \uf028\uf067 \uf02d 1 \uf029\n\uf0ee\n\n(26)\n\nProof: Simply solving (25) for \uf061 \uf0ce \uf05b 0,1\uf05d yields the result. \u25a0\nB. Example 2: M \uf03d 1 (linear belief distribution)\n\n15\n\n\fIn this subsection we consider the case M \uf03d 1 . For example, users observe the reputation of\none other user by sampling the system. It can also be interpreted as that users have linear belief\ndistribution regarding the true social reputation. The belief function thus is given by\nf \uf028 \uf072 | \uf072 s \uf029 \uf03d \uf072 s f1,1 \uf028 \uf072 \uf029 \uf02b \uf0281 \uf02d \uf072 s \uf029 f 0,1 \uf028 \uf072 \uf029 \uf03d 2 \uf0281 \uf02d \uf072 s + \uf028 2 \uf072 s \uf02d 1\uf029 \uf072 \uf029\n\n(27)\n\nNote f1,1 \uf028 \uf072 \uf029 \uf03d \uf072 , f 0,1 \uf028 \uf072 \uf029 \uf03d 1 \uf02d \uf072 . Hence, the cumulative belief functions are linear in \uf072s ,\n\nF \uf028 \uf072 \uf0b3 \uf072 B | \uf072 s \uf029 \uf03d \uf0281 \uf02d \uf072 B \uf029 \uf02b 2 \uf072 B \uf0281 \uf02d \uf072 B \uf029 \uf072 s\n\n(28)\n\nF \uf028 \uf072 \uf0a3 \uf072 G | \uf072 s \uf029 \uf03d \uf072 G \uf0281 \uf02d \uf072 G \uf029 \uf02d 2 \uf072 G \uf0281 \uf02d \uf072G \uf029 \uf072 s\n\n(29)\n\n2\n\nTo solve \uf044 \uf028 \uf072 s \uf029 \uf03d 0 , it is equivalent to solve \uf044 \uf028 \uf072 s \uf029 / \uf072 s \uf03d 0 for \uf072 s \uf0b9 0 . Let g \uf028 \uf072 s \uf029 \uf03d \uf044 \uf028 \uf072 s \uf029 / \uf072 s .\n\n\uf028\n\n\uf029\n\ng \uf028 \uf072 s \uf029 \uf03d \uf061 \uf0281 \uf02d \uf072 B \uf029 \uf02b 2 \uf072 B \uf0281 \uf02d \uf072 B \uf029 \uf072 s \uf0281 \uf02d \uf072 s \uf029 \uf02d \uf028 \uf072G \uf028 2 \uf02d \uf072G \uf029 \uf02d 2 \uf072G \uf0281 \uf02d \uf072G \uf029 \uf072 s \uf029 \uf072 s\n2\n\n(30)\n\nThe above function is a quadratic function regarding \uf072s . It is difficult to solve the stable\nequilibrium and even more difficult to analyze the impact of punishment directly. In the\nfollowing, we instead first establish tighter upper and lower bounds of the efficiency in the stable\nequilibrium than the general bounds given by (26) when \uf067 is large. Using the new bounds we\nare able to derive the optimal punishment based on which optimal social norms can be designed.\n\nProposition 8. Fix \uf062 , \uf067 and fix \uf061 , M \uf03d 1 , for large \uf067 , the stable equilibrium \uf072 s* is bounded by\n\uf061 \uf0281 \uf02d \uf072 B \uf029\n\uf061 \uf0281 \uf02d \uf072 B \uf029\n\uf0a3 \uf072 s* \uf0a3\n2\n\uf061 \uf0281 \uf02d \uf072 B \uf029\uf0281 \uf02d 3 \uf072 B \uf029 \uf02b \uf072 G \uf028 2 \uf02d \uf072 G \uf029\n\uf061 \uf0281 \uf02d \uf072 B \uf029 \uf02b \uf072 G2\n2\n\n2\n\n(31)\n\nProof: For large \uf067 , the belief thresholds \uf072G and \uf072 B are approximated by\n\uf072G \uf03d\n\n1\uf02d \uf062\n\n\uf062\uf067\n\n,\n\n\uf072B \uf03d\n\n1\uf02d \uf062\n\n(32)\n\n\uf061\uf062\uf067\n\n(1) We first establish the upper bound. Note that the quadratic coefficient of (30) is\n2\n\n\uf0e61\uf02d \uf062 \uf0f6 \uf0e6 1\n\uf0f6\n2 \uf028 \uf072 G \uf0281 \uf02d \uf072 G \uf029 \uf02d \uf061\uf072 B \uf0281 \uf02d \uf072 B \uf029 \uf029 \uf03d 2 \uf0e7\n\uf0f7 \uf0e7 \uf02d 1\uf0f7 \uf03e 0\n\uf0f8\n\uf0e8 \uf062\uf067 \uf0f8 \uf0e8 \uf061\n\n(33)\n\nHence, g \uf028 \uf072 s \uf029 is a convex quadratic function. Because there must be one and only one root that\nlies in \uf028 0,1\uf029 , it is upper bounded by\ng \uf0280 \uf029\n\uf061 \uf0281 \uf02d \uf072 B \uf029\n\uf072 \uf0a3\n\uf03d\ng \uf028 0 \uf029 \uf02d g \uf0281 \uf029 \uf061 \uf0281 \uf02d \uf072 B \uf0292 \uf02b \uf072 G2\n2\n\n*\ns\n\n(34)\n\nBecause \uf0281 \uf02d \uf072 B \uf029 \uf03c 1 \uf02d \uf072 B2 , this upper bound is tighter than the general upper bound.\n2\n\n(2) Next we establish the lower bound. Note the slope at \uf072 s \uf03d 0 of g \uf028 \uf072 s \uf029 is\n\n16\n\n\f\uf061 \uf0281 \uf02d \uf072 B \uf029\uf028 3 \uf072 B \uf02d 1\uf029 \uf02d \uf072G \uf028 2 \uf02d \uf072 G \uf029\n\n(35)\n\nBy the convexity, the root in \uf028 0,1\uf029 is lower bounded by\ng \uf0280 \uf029\n\uf061 \uf0281 \uf02d \uf072 B \uf029\n\uf072 \uf0b3\n\uf03d\n\uf061 \uf0281 \uf02d \uf072 B \uf029\uf0281 \uf02d 3 \uf072 B \uf029 \uf02b \uf072 G \uf028 2 \uf02d \uf072 G \uf029 \uf061 \uf0281 \uf02d \uf072 B \uf029\uf0281 \uf02d 3 \uf072 B \uf029 \uf02b \uf072 G \uf028 2 \uf02d \uf072 G \uf029\n2\n\n*\ns\n\n(36)\n\nBecause \uf0281 \uf02d 3\uf072 B \uf029 \uf03c \uf0281 \uf02d \uf072 B \uf029 , the lower bound is tighter than the general bound. \u25a0\nThe upper bound in the above proposition in fact has more implications for the optimal\npunishment design. For large \uf067 , in order to maximize the upper bound, it is equivalent to\nconsider the following maximization problem\nm ax \uf061 \uf0281 \uf02d \uf072 B \uf029\n\n2\n\n\uf061\n\nor\n\n\uf0e6\n1\uf02d \uf062 \uf0f6\nm ax \uf061 \uf0e7 1 \uf02d\n\uf061\n\uf061\uf062 \uf067 \uf0f7\uf0f8\n\uf0e8\n\n2\n\n(37)\n\nExpanding the objective function in (37), we get\n2\n\n2\n\n\uf0e6\n1\uf02d \uf062 \uf0f6\n1 \uf0e61\uf02d \uf062 \uf0f6\n1\uf02d \uf062\n\uf061 \uf0e71 \uf02d\n.\n\uf0f7 \uf03d\uf061 \uf02b \uf0e7\n\uf0f7 \uf02d2\n\uf061\uf062 \uf067 \uf0f8\n\uf061 \uf0e8 \uf062\uf067 \uf0f8\n\uf062\uf067\n\uf0e8\n\n(38)\n\nRemember that we need to ensure that \uf072 B \uf03c 1 since otherwise the only robust equilibrium is 0\naccording to Proposition 4 and hence, the feasible \uf061 needs to satisfy\n1\uf02d \uf062\n\uf061\uf0b3\n.\n\uf062\uf067\n\n(39)\n\nChoosing \uf061 \uf03d 1 maximizes (38) and hence, it maximizes the upper bound for all feasible \uf061 .\nNote that for \uf061 \uf03d 1 , the upper bound is indeed the actual efficiency because the upper and lower\nbounds are identical. Therefore, \uf061 \uf03d 1 maximizes the efficiency of the stable equilibrium. The\nfollowing proposition restates this result and also determines the social reputation in equilibrium.\n\nProposition 9. Fix \uf062 , \uf067 , M \uf03d 1 for large \uf067 , the stable equilibrium \uf072 s* is maximized by choosing\n\n\uf061 * \uf03d 1 , and the optimal solution is\n2\n\n\uf0e6\n1\uf02d \uf062 \uf0f6\n\uf0e71 \uf02d \uf062 \uf067 \uf0f7\n\uf0e8\n\uf0f8\n\uf072 s* \uf03d\n2\n2\n\uf0e6\n\uf0e61\uf02d \uf062 \uf0f6\n1\uf02d \uf062 \uf0f6\n\uf0e71 \uf02d \uf062 \uf067 \uf0f7 \uf02b \uf0e7 \uf062 \uf067 \uf0f7\n\uf0e8\n\uf0f8\n\uf0e8\n\uf0f8\n\n(40)\n\nNote that this stable equilibrium efficiency is close to 1 when \uf067 is large or \uf062 is close to 1. For\nthe higher order cases, i.e. values of M other than 0 and 1, it is rather difficult to derive any\nanalytical results. We will investigate this in the simulation section numerically. However, from\nthe analysis for the two specific examples, some design insights can still be drawn: when the\n17\n\n\fbenefit-to-cost ratio is large (1), it is optimal to choose \uf061 \uf03d 1 , which is the mildest punishment\npossible, and (2) a larger M leads to a higher social reputation for this choice of punishment.\nV. SIMULATIONS\nIn this section, we provide some simulation results. Fig. 3 illustrates the system evolution for\nvarious environments. It is shown that the system quickly converges to the stable state. In this set\nof simulations M \uf03d 0,1, 2 , the stable equilibrium is unique and hence, any initial state converges\nto the same stable equilibrium. However, there can also be multiple stable equilibria in which\ncase different initial states converge to different stable equilibria. As we show in Fig. 4 for\nM \uf03d 6 , there are two stable equilibria. If the system starts with a high initial state, it eventually\n\nhas a high social reputation while if the initial social reputation is low, the final social reputation\nis also low. Fig. 5 shows the case with perfect information, i.e. M \uf03d \uf0a5 . If the initial social\nreputation is higher than \uf072 B , no matter which \uf061 the system designer chooses, the system\nachieves full efficiency, i.e. all agents have good reputations; if the initial social reputation is\nlower than \uf072G , the system achieves zero efficiency, i.e. all agents have bad reputations; for the\ninitial social reputation that lie between [ \uf072G , \uf072 B ] , the system stays in the same state.\nFig. 6 illustrates the impact of M . As we see, more observations do not necessarily lead to\nbetter performance for a given punishment. In fact, the bounds established in (19) does not tell\nthe monotonicity regarding M . However, for a larger \uf061 , more observations do have a better\nperformance. Because \uf061 \uf03d 1 is often the optimal choice, basically M should be larger to achieve\na higher efficiency. In order to obtain the better performance, users need to have more accurate\ninformation of the reputation distribution. Fig. 7 further compares the simulated optimal\nequilibria with the bounds established by (20). The established bounds are close to the simulation\npoints and the performance becomes quite close to full efficiency as M increases.\nFig. 8 and Fig. 9 illustrate the impact of the benefit-to-cost ratio \uf067 and the discount factor \uf062 .\nFor a given punishment probability, the system performance improves with \uf067 . Moreover, for all\nsimulated values of \uf067 , choosing \uf061 \uf03d 1 generates the best performance. The discount factor \uf062\n\n18\n\n\fhas a similar impact as \uf067 : Larger \uf062 leads to a better performance and choosing \uf061 \uf03d 1 generates\nthe highest efficiency. Even though the fact that the mildest punishment is optimal may seem\ncounter-intuitive, this finding can be easily explained as follows. Punishment is often used to\nprevent users from misbehaving. When users are good, harsher punishments impose greater\nthreat on these users if they would deviate. Hence, it may seem that harsher punishments are\nneeded to obtain a better performance. However, this intuition is only valid when all users are on\nthe equilibrium path, i.e. they always follow the social strategy. For the limited observations\nscenario, there are always a positive fraction of users who deviate no matter what the punishment\nis. Once users are in the punishment phase, harsher punishment becomes a disincentive for them\nto restore their reputations. As we show that punishment has much greater impact on the belief\nthreshold for bad users, the system eventually will be in an equilibrium with a lower efficiency.\nVI. CONCLUSIONS\nIn this paper, we design the optimal social norm protocol for information exchange systems\nwhere users have heterogeneous beliefs due to limited observations of the system. First, a Bayesian belief model is proposed to model the belief heterogeneity. Second, the optimal provider\nstrategy is shown to have a threshold property: users cooperate only when they have sufficient\n\"trust\" in the system (i.e. believe that sufficient users are cooperating). Finally, the impact of the\npunishment severity on the stable equilibrium and the achievable system efficiency is rigorously\nstudied. When users can make unlimited observations, full or zero efficiency occurs in the stable\nequilibrium. However, in the more realistic limited observations scenario, full efficiency can\nnever be achieved and different punishment strategies lead to different stable equilibria having\ndifferent efficiencies. We show that choosing the mildest punishment is optimal for many\ninteresting scenarios and support this finding with both analytical and simulation results.\nREFERENCES\n[1] O. Loginova, H. Lu, and X. H. Wang, \"Incentive schemes in peer-to-peer networks,\" The B.E. Journal of\nTheoretical Economics, Oct. 2008.\n[2] K. Ranganathan, M. Ripeanu, A. Sarin, and I. Foster, \"Incentive mechanism for large collaborative resource\nsharing,\" In Proceedings of IEEE International Symposium on Cluster Computing and the Grid, 2004.\n\n19\n\n\f[3] P. Antoniadis, C. Courcoubetis, and B. Strulo, \"Comparing economics incentives in peer-to-peer networks,\"\nComputer Networks, 46(1):133-146, 2004.\n[4] E. J. Friedman, J. Y. Halpern, and I. A. Kash, \"Efficiency and nash equilibria in a scrip system for P2P\nnetworks,\" In Proceedings of the Seventh ACM Conference on Electronic Commerce, 2006.\n[5] I. A. Kash, E. J. Friedman, and J. Y. Halpern, \"Optimizing scrip systems: Efficiency, crashes, hoarders, and\naltruists,\" In Proceedings of the Eighth ACM Conference on Electronic Commerce, 2007.\n[6] R. Landa, D. Griffin, R. Clegg, E. Mykoniati, and M. Rio, \"A sybilproof indirect reciprocity mechanism for\npeer-to-peer networks,\" In Proceedings of INFOCOM, 2009.\n[7] J. Xu, M. van der Schaar, and W. Zame, \"Designing exchange for online communities,\" Technical report\navailable at http://arxiv.org/abs/1108.5871.\n[8] R. L. Trivers, \"The evolution of reciprocal altruism,\" Quarterly review of biology, 46(1):35-57, 1971.\n[9] R. D. Alexander, The Biology of Moral Systems, Aldine de Gruyter, New York, 1987.\n[10] H. Masum and Y. Zhang, \"Manifesto for the reputation society,\" First Monday, 9(7), 2004.\n[11] S. Kamvar, M. T. Schlosser, and H. G. Molina, \"The eigentrust algorithm for reputation management in P2P\nnetworks,\" In Proceedings of 12th International Conf. on World Wide Web, 2003.\n[12] A. Ravoaja and E. Anceaume, \"Storm: a secure overlay for P2P reputation management,\" In Proceedings of 1st\nInternational Conf. on Self-Adaptive and Self-Organizing Systems, 2007.\n[13] S. Ba and P. Pavlou, \"Evidence of the effect of trust building technology in electronic markets: price premiums\nand buyer behavior,\" MIS Quart, 26(3):243-268, 2002.\n[14] P. Resnick, R. Zeckhauser, \"Trust among strangers in internet transactions: empirical analysis of eBay's\nreputation system,\" Advances in Applied Microeconomics, 11, 2002.\n[15] P. Resnick, R. Zeckhauser, J. Swanson, and K. Lockwood, \"The value of reputation on eBay: a controlled\nexperiment,\" Exp Econ, 9:79-101, 2006.\n[16] C. Keser, \"Experimental games for the design of reputation management systems,\" IBM Systems Journal,\n42(3):498-506, 2003.\n[17] C. Dellarocas, \"Reputation mechanism design in online trading environments with pure moral hazard,\"\nInformation Systems Research, 16(2):209-230, 2005.\n[18] C. Dellarocas, \"How often should reputation mechanisms update a trader's reputation profile?\" Information\nSystems Research, 17(3):271-285, 2006.\n[19] M. Fan, Y. Tan, and A. B. Whinston, \"Evaluation and design of online cooperative feedback mechanism for\nreputation management,\" IEEE Transactions on Knowledge Data Engineering, 17(3):244-254, 2005.\n[20] G. Zacharia, A. Moukas, and P. Maes, \"Collaborative reputation mechanisms in electronic marketplaces,\"\nDecision Support Systems, 29(4):371-388, 2000.\n[21] Y. Zhang, J. Park, and M. van der Schaar, \"Social norms for networked communities,\" Technical report\navailable at http://arxiv.org/abs/1101.0272, 2011.\n[22] G. Ellison, \"Cooperation in the prisoner's dilemma with anonymous random matching,\" Review of Economic\nStudies, 61(3):567-588, 1994.\n[23] M. Kandori, \"Social norms and community enforcement,\" Review of Economic Studies, 59(1):63-80, 1992.\n[24] M. Okuno-Fujiwara and A. Postlewaite. \"Social norms and random matching games,\" Games and Economic\nBehaviors, 9(1):79-109, 1995.\n[25] K. Iyer, R. Johari, M. Sundararajan, \"Mean field equilibria of dynamic auctions with learning,\" In Proceedings\nof the 12th ACM conference on Electronic Commerce, 2011.\n[26] Y. Zhang and M. van der Schaar, \"Influencing the long-term evolution of online communities using social\nnroms,\" Allerton Conference, 2011.\n[27] Omitted proofs available at http://ee.ucla.edu/~jiexu/documents/appendix_jsac_111215.pdf.\n\n20\n\n\fProvider\nProvide service\n\nRequester\n\nNot provide service\n\nb, -c\n\n0,0\n\nFigure 1. The utility matrix of the gift-giving game.\n\" D \" \uf0281\uf029\n\n\" C \" \uf0281\uf029\n\n\" C \" \uf0281 \uf02d \uf061 \uf029\n\n0\n\n1\n\n\" D \" \uf0281\uf029\n\n\" C \" \uf028\uf061 \uf029\n\nFigure 2. Reputation updating rule.\n\n1\n0.9\n\n0.8\n\nsocial reputation \uf072\n\ns\n\n0.7\n\n0.6\n0.5\n\n0.4\nM=0\nM=1\nM=2\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n120\ntime slot\n\n140\n\n160\n\n180\n\n200\n\nFigure 3. Stable state of the system for \uf062 \uf03d 0.25, \uf067 \uf03d 10 and \uf061 \uf03d 0.9 .\n\n21\n\n\f1\n0.9\ninitial social reputation is 0.3\ninitial social reputation is 0.7\n\n0.8\n\nsocial reputation \uf072s\n\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n120\ntime slot\n\n140\n\n160\n\n180\n\n200\n\nFigure 4. Stable state of the system for \uf062 \uf03d 0.25, \uf067 \uf03d 8, M \uf03d 6 and \uf061 \uf03d 0.9 .\n\n1\n0.9\ninitial social reputation is 0.3 (< \uf072G)\n\n0.8\n\ninitial social reputation is 0.7 (> \uf072B)\n\nsocial reputation \uf072s\n\n0.7\n\ninitial social reputation is 0.45 (> \uf072G, < \uf072B)\n\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n120\ntime slot\n\n140\n\n160\n\n180\n\n200\n\nFigure 5. Stable state of the system for \uf062 \uf03d 0.25, \uf067 \uf03d 8, M \uf03d \uf0a5 and \uf061 \uf03d 0.9 .\n\n22\n\n\f1\n0.9\n0.8\n\nsocial reputation \uf072\n\ns\n\n0.7\n0.6\n0.5\n0.4\n0.3\nM=0\nM=1\nM=2\nM=3\n\n0.2\n0.1\n0\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n0.7\npunishment \uf061\n\n0.8\n\n0.9\n\n1\n\nFigure 6. Impact of the observation granularity M (fix \uf062 \uf03d 0.5, \uf067 \uf03d 5 ).\n\n1\n\nsocial reputation \uf072\n\ns\n\n0.95\n\n0.9\n\n0.85\n\n0.8\n\n0.75\n\nsimulation\nbound\n\n0\n\n1\n\n2\n3\nobservation granularity M\n\n4\n\n5\n\nFigure 7. Bounds on the optimal social reputation for various observation granularities.\n( \uf067 \uf03d 6, \uf062 \uf03d 0.5 )\n\n23\n\n\f1\n0.9\n0.8\n\nsocial reputation \uf072\n\ns\n\n0.7\n0.6\n0.5\n\n\uf061\n\uf061\n\uf061\n\uf061\n\uf061\n\n0.4\n0.3\n0.2\n\n= 0.2\n= 0.4\n= 0.6\n= 0.8\n=1\n\n0.1\n0\n\n3\n\n3.5\n\n4\n\n4.5\n\n5\n5.5\n6\nbenefit-to-cost ratio \uf067\n\n6.5\n\n7\n\n7.5\n\n8\n\nFigure 8. Impact of the benefit-to-cost ratio \uf067 (fix \uf062 \uf03d 0.5, M \uf03d 1 ).\n\n1\n\n\uf061\n\uf061\n\uf061\n\uf061\n\uf061\n\n0.9\n0.8\n\nsocial reputation \uf072s\n\n0.7\n\n= 0.2\n= 0.4\n= 0.6\n= 0.8\n=1\n\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n0.6\ndiscount factor \uf062\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 9. Impact of the discount factor \uf062 (fix \uf067 \uf03d 4, M \uf03d 1 ).\n\n24\n\n\f"}