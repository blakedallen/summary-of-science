{"id": "http://arxiv.org/abs/0812.1869v1", "guidislink": true, "updated": "2008-12-10T09:00:40Z", "updated_parsed": [2008, 12, 10, 9, 0, 40, 2, 345, 0], "published": "2008-12-10T09:00:40Z", "published_parsed": [2008, 12, 10, 9, 0, 40, 2, 345, 0], "title": "Convex Sparse Matrix Factorizations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0812.0016%2C0812.3996%2C0812.0140%2C0812.1908%2C0812.5008%2C0812.2610%2C0812.1114%2C0812.0962%2C0812.4756%2C0812.3547%2C0812.2972%2C0812.2136%2C0812.0880%2C0812.4308%2C0812.2399%2C0812.3057%2C0812.4657%2C0812.2225%2C0812.1034%2C0812.2738%2C0812.1862%2C0812.4995%2C0812.4909%2C0812.0316%2C0812.3204%2C0812.3828%2C0812.2895%2C0812.1372%2C0812.0119%2C0812.5021%2C0812.2810%2C0812.1039%2C0812.1763%2C0812.2057%2C0812.0593%2C0812.4185%2C0812.0827%2C0812.2503%2C0812.0615%2C0812.0092%2C0812.1606%2C0812.0589%2C0812.3007%2C0812.3569%2C0812.3937%2C0812.1460%2C0812.0372%2C0812.3664%2C0812.3312%2C0812.0186%2C0812.0946%2C0812.3267%2C0812.3327%2C0812.4660%2C0812.3923%2C0812.1298%2C0812.3914%2C0812.3073%2C0812.4689%2C0812.1808%2C0812.3333%2C0812.1542%2C0812.1210%2C0812.0576%2C0812.3769%2C0812.3500%2C0812.4726%2C0812.3744%2C0812.3362%2C0812.4584%2C0812.0524%2C0812.1062%2C0812.0463%2C0812.5038%2C0812.4832%2C0812.1106%2C0812.0700%2C0812.0823%2C0812.0286%2C0812.3104%2C0812.0034%2C0812.0553%2C0812.2922%2C0812.0805%2C0812.1756%2C0812.4424%2C0812.2244%2C0812.1018%2C0812.1013%2C0812.1869%2C0812.0027%2C0812.4950%2C0812.0442%2C0812.0250%2C0812.0837%2C0812.2260%2C0812.4481%2C0812.1612%2C0812.0295%2C0812.4675%2C0812.0506&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Convex Sparse Matrix Factorizations"}, "summary": "We present a convex formulation of dictionary learning for sparse signal\ndecomposition. Convexity is obtained by replacing the usual explicit upper\nbound on the dictionary size by a convex rank-reducing term similar to the\ntrace norm. In particular, our formulation introduces an explicit trade-off\nbetween size and sparsity of the decomposition of rectangular matrices. Using a\nlarge set of synthetic examples, we compare the estimation abilities of the\nconvex and non-convex approaches, showing that while the convex formulation has\na single local minimum, this may lead in some cases to performance which is\ninferior to the local minima of the non-convex formulation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0812.0016%2C0812.3996%2C0812.0140%2C0812.1908%2C0812.5008%2C0812.2610%2C0812.1114%2C0812.0962%2C0812.4756%2C0812.3547%2C0812.2972%2C0812.2136%2C0812.0880%2C0812.4308%2C0812.2399%2C0812.3057%2C0812.4657%2C0812.2225%2C0812.1034%2C0812.2738%2C0812.1862%2C0812.4995%2C0812.4909%2C0812.0316%2C0812.3204%2C0812.3828%2C0812.2895%2C0812.1372%2C0812.0119%2C0812.5021%2C0812.2810%2C0812.1039%2C0812.1763%2C0812.2057%2C0812.0593%2C0812.4185%2C0812.0827%2C0812.2503%2C0812.0615%2C0812.0092%2C0812.1606%2C0812.0589%2C0812.3007%2C0812.3569%2C0812.3937%2C0812.1460%2C0812.0372%2C0812.3664%2C0812.3312%2C0812.0186%2C0812.0946%2C0812.3267%2C0812.3327%2C0812.4660%2C0812.3923%2C0812.1298%2C0812.3914%2C0812.3073%2C0812.4689%2C0812.1808%2C0812.3333%2C0812.1542%2C0812.1210%2C0812.0576%2C0812.3769%2C0812.3500%2C0812.4726%2C0812.3744%2C0812.3362%2C0812.4584%2C0812.0524%2C0812.1062%2C0812.0463%2C0812.5038%2C0812.4832%2C0812.1106%2C0812.0700%2C0812.0823%2C0812.0286%2C0812.3104%2C0812.0034%2C0812.0553%2C0812.2922%2C0812.0805%2C0812.1756%2C0812.4424%2C0812.2244%2C0812.1018%2C0812.1013%2C0812.1869%2C0812.0027%2C0812.4950%2C0812.0442%2C0812.0250%2C0812.0837%2C0812.2260%2C0812.4481%2C0812.1612%2C0812.0295%2C0812.4675%2C0812.0506&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present a convex formulation of dictionary learning for sparse signal\ndecomposition. Convexity is obtained by replacing the usual explicit upper\nbound on the dictionary size by a convex rank-reducing term similar to the\ntrace norm. In particular, our formulation introduces an explicit trade-off\nbetween size and sparsity of the decomposition of rectangular matrices. Using a\nlarge set of synthetic examples, we compare the estimation abilities of the\nconvex and non-convex approaches, showing that while the convex formulation has\na single local minimum, this may lead in some cases to performance which is\ninferior to the local minima of the non-convex formulation."}, "authors": ["Francis Bach", "Julien Mairal", "Jean Ponce"], "author_detail": {"name": "Jean Ponce"}, "author": "Jean Ponce", "links": [{"href": "http://arxiv.org/abs/0812.1869v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0812.1869v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0812.1869v1", "affiliation": "INRIA Rocquencourt", "arxiv_url": "http://arxiv.org/abs/0812.1869v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:0812.1869v1 [cs.LG] 10 Dec 2008\n\nConvex Sparse Matrix Factorizations\nFrancis Bach, Julien Mairal, Jean Ponce\nWillow Project-team\nLaboratoire d'Informatique de l'Ecole Normale Sup\u00e9rieure\n(CNRS/ENS/INRIA UMR 8548)\n45, rue dUlm, 75230 Paris, France\nDecember 11, 2008\nAbstract\nWe present a convex formulation of dictionary learning for sparse signal decomposition.\nConvexity is obtained by replacing the usual explicit upper bound on the dictionary size by a\nconvex rank-reducing term similar to the trace norm. In particular, our formulation introduces an\nexplicit trade-off between size and sparsity of the decomposition of rectangular matrices. Using\na large set of synthetic examples, we compare the estimation abilities of the convex and nonconvex approaches, showing that while the convex formulation has a single local minimum, this\nmay lead in some cases to performance which is inferior to the local minima of the non-convex\nformulation.\n\n1 Introduction\nSparse decompositions have become prominent tools in signal processing [1], image processing [2], machine learning, and statistics [3]. Many relaxations and approximations of the associated minimum cardinality problems are now available, based on greedy approaches [4] or\nconvex relaxations through the l1 -norm [1, 3]. Active areas of research are the design of efficient algorithms to solve the optimization problems associated with the convex non differentiable norms (see, e.g., [5]), the theoretical study of the sparsifying effect of these norms [6, 7],\nand the learning of the dictionary directly from data (see, e.g., [8, 2]).\nIn this paper, we focus on the third problem-namely, we assume that we are given a matrix\nY \u2208 RN \u00d7P and we look for factorizations of the form X = U V \u22a4 , where U \u2208 RN \u00d7M and\nV \u2208 RP \u00d7M , that are close to Y and such that the matrix U is sparse. This corresponds to\ndecomposing N vectors in RP (the rows of Y ) over a dictionary of size M . The columns of\nV are the dictionary elements (of dimension P ), while the rows of U are the decomposition\ncoefficients of each data point. Learning sparse dictionaries from data has shown great promise\nin signal processing tasks, such as image or speech processing [2], and core machine learning\ntasks such as clustering may be seen as special cases of this framework [9].\nVarious approaches have been designed for sparse dictionary learning. Most of them consider a specific loss between entries of X and Y , and directly optimize over U and V , with\n\n1\n\n\fadditional constraints on U and V [10, 11]: dictionary elements, i.e., columns of V , may or\nmay not be constrained to unit norms, while a penalization on the rows of U is added to impose\nsparsity. Various forms of jointly non-convex alternating optimization frameworks may then be\nused [10, 11, 2]. The main goal of this paper is to study the possibility and efficiency of convexifying these non-convex approaches. As with all convexifications, this leads to the absence\nof non-global local minima, and should allow simpler analysis. However, does it really work in\nthe dictionary learning context? That is, does convexity lead to better decompositions?\nWhile in the context of sparse decomposition with fixed dictionaries, convexification has led\nto both theoretical and practical improvements [6, 3, 7], we report both positive and negative\nresults in the context of dictionary learning. That is, convexification sometimes helps and sometimes does not. In particular, in high-sparsity and low-dictionary-size situations, the non-convex\nfomulation outperforms the convex one, while in other situations, the convex formulation does\nperform better (see Section 5 for more details).\nThe paper is organized as follows: we show in Section 2 that if the size of the dictionary is\nnot bounded, then dictionary learning may be naturally cast as a convex optimization problem;\nmoreover, in Section 3, we show that in many cases of interest, this problem may be solved\nin closed form, shedding some light on what is exactly achieved and not achieved by these\nformulations. Finally, in Section 4, we propose a mixed l1 -l2 formulation that leads to both\nlow-rank and sparse solutions in a joint convex framework. In Section 5, we present simulations\non a large set of synthetic examples.\nNotations Given a rectangular matrix X \u2208 RN \u00d7P and n \u2208 {1, . . . , N }, p \u2208 {1, . . . , P },\nwe denote by X(n, p) or Xnp its element indexed by the pair (n, p), by X(:, p) \u2208 RN its p-th\ncolumn and by X(n, :) \u2208 RP its n-th row. Moreover, given a vector x \u2208 RN , we denote by kxkq\nP\nq 1/q and kxk\nits lq -norm, i.e., for q \u2208 [1, \u221e), kxkq = ( N\n\u221e = maxn\u2208{1,...,N } |xn |. We\nn=1 |xn | )\nN\n\u00d7P\nalso write a matrix U \u2208 R\nas U = [u1 , . . . , uM ], where each um \u2208 RN .\n\n2 Decomposition norms\nWe consider a loss l : R \u00d7 R \u2192 R which is convex with respect to the second variable. We\nassumePin this\nthat all entries of Y are observed and the risk of the estimate X is equal\nPpaper\nP\nl(Y\nto N1P N\nnp , Xnp ). Note that our framework extends in a straightforward way to\nn=1\np=1\nmatrix completion settings by summing only over observed entries [12].\nWe consider factorizations of the form X = U V \u22a4 ; in order to constrain U and V , we\nconsider the following optimization problem:\nmin\n\nU \u2208RN\u00d7M ,V \u2208RP \u00d7M\n\nN P\nM\n\u03bbX\n1 XX\n\u22a4\n(kum k2C + kvm k2R ),\nl(Ynp , (U V )np ) +\nNP\n2\nn=1 p=1\n\n(1)\n\nm=1\n\nwhere k * kC and k * kR are any norms on RN and RP (on the column space and row space\nof the original matrix X). This corresponds to penalizing each column of U and V . In this\npaper, instead of considering U and V separately, we consider the matrix X and the set of its\ndecompositions on the form X = U V \u22a4 , and in particular, the one with minimum sum of norms\nkum k2C , kvm k2R , m \u2208 {1, . . . , M }. That is, for X \u2208 RN \u00d7P , we consider\nM\nfD\n(X)\n\n=\n\nmin\n\n(U,V )\u2208RN\u00d7M \u00d7RP \u00d7M , X=U V \u22a4\n\n2\n\nM\n1X\n(kum k2C + kvm k2R ).\n2 m=1\n\n(2)\n\n\fM (X) = +\u221e. Note that the minimum\nIf M is strictly smaller than the rank of X, then we let fD\nis always attained if M is larger than or equal to the rank of X. Given X, each pair (um , vm )\nis defined up to a scaling factor, i.e., (um , vm ) may be replaced by (um sm , vm s\u22121\nm ); optimizing\nwith respect to sm leads to the following equivalent formulation:\n\nM\nfD\n(X)\n\n=\n\nmin\n\n(U,V )\u2208RN\u00d7M \u00d7RP \u00d7M , X=U V \u22a4\n\nM\nX\n\nkum kC kvm kR .\n\n(3)\n\nm=1\n\nMoreover, we may derive another equivalent formulation by constraining the norms of the\ncolumns of V to one, i.e.,\nM\n(X)\nfD\n\n=\n\nmin\n\n(U,V )\u2208RN\u00d7M \u00d7RP \u00d7M , X=U V \u22a4 , \u2200m,kvm kR =1\n\nM\nX\n\nkum kC .\n\n(4)\n\nm=1\n\nThis implies that constraining dictionary elements to be of unit norm, which is a common assumption in this context [11, 2], is equivalent to penalizing the norms of the decomposition\ncoefficients instead of the squared norms.\nOur optimization problem in Eq. (1) may now be equivalently written as\nmin\n\nX\u2208RN\u00d7P\n\nN P\n1 XX\nM\nl(Ynp , Xnp ) + \u03bbfD\n(X).\nNP\n\n(5)\n\nn=1 p=1\n\nM (X) in Eqs. (2)-(4). The next proposition shows that if\nwith any of the three formulations of fD\nthe size M of the dictionary is allowed to grow, then we obtain a norm on rectangular matrices,\nwhich we refer to as a decomposition norm. In particular, this shows that if M is large enough\nthe problem in Eq. (5) is a convex optimization problem.\n\u221e (X) = lim\nM\n\u221e\nProposition 1 For all X \u2208 RN \u00d7P , the limit fD\nM \u2192\u221e fD (X) exists and fD (*) is a\nnorm on rectangular matrices.\nM (X) is nonnegative and clearly nonincreasing with M , it has a nonProof Since given X, fD\nnegative limit when M tends to infinity. The only non trivial part is the triangular inequal\u221e (X + X ) 6 f \u221e (X ) + f \u221e (X ). Let \u03b5 > 0 and let (U , V ) and (U , V ) be\nity, i.e., fD\n1\n2\n1\n2\n1 1\n2 2\nD\nD\nPM1\n\u221e (X ) >\nthe two \u03b5-optimal decompositions, i.e., such that fD\nku\nk\nkv\nk\n1\n1m C 1m R \u2212 \u03b5 and\nm=1\nPM2\n\u221e\nfD (X2 ) >\nm=1 ku2m kC kv2m kR \u2212 \u03b5. Without loss of generality, we may asssume that\nM1 = M2 = MP. We consider U = [U1 U2 ], V = [V1 V2 ], we have X = X1 + X2 = U V \u22a4\nM\n\u221e (X) 6\n\u221e\n\u221e\nand fD\nm=1 (ku1m kC kv1m kR + ku2m kC kv2m kR ) 6 fD (X1 ) + fD (X2 ) + 2\u03b5. We\nobtain the triangular inequality by letting \u03b5 tend to zero.\n\nFollowing the last proposition, we now let M tend to infinity; that is, if we denote kXkD =\n\u221e (X), we consider the following rank-unconstrained and convex problem:\nfD\nmin\n\nX\u2208RN\u00d7P\n\nN P\n1 XX\nl(Ynp , Xnp ) + kXkD .\nNP\n\n(6)\n\nn=1 p=1\n\nHowever, there are three potentially major caveats that should be kept in mind:\nConvexity and polynomial time Even though the norm k * kD leads to a convex function, computing or approximating it may take exponential time-in general, it is not because a\n\n3\n\n\fproblem is convex that it can be solved in polynomial time. In some cases, however, it may be\ncomputed in closed form, as presented in Section 3, while in other cases, an efficiently computable convex lower-bound is available (see Section 4).\nRank and dictionary size The dictionary size M must be allowed to grow to obtain\n\u221e (X) = f M (X).\nconvexity and there is no reason, in general, to have a finite M such that fD\nD\nIn some cases presented in Section 3, the optimal M is finite, but we conjecture that in general\nthe required M may be unbounded. Moreover, in non sparse situations, the rank of X and the\ndictionary size M are usually equal, i.e., the matrices U and V have full rank. However, in\nsparse decompositions, M may be larger than the rank of X, and sometimes even larger than\nthe underlying data dimension P (the corresponding dictionaries are said the be overcomplete).\nLocal minima The minimization problem in Eq. (1), with respect to U and V , even with\nM very large, may still have multiple local minima, as opposed to the one in X, i.e., in Eq. (6),\nwhich has a single local minimum. The main reason is that the optimization problem defining\n(U, V ) from X, i.e., Eq. (3), may itself have multiple local minima. In particular, it is to be\nconstrasted to the optimization problem\nmin\n\nU \u2208RN\u00d7M ,V N\u00d7M\n\nN P\n1 XX\nl(Ynp , (U V \u22a4 )np ) + \u03bbkU V \u22a4 kD ,\nNP\n\n(7)\n\nn=1 p=1\n\nwhich will turn out to have no local minima if M is large enough (see Section 4.3 for more\ndetails).\nBefore looking at special cases, we compute the dual norm of k * kD (see, e.g., [13] for the\ndefinition and properties of dual norms), which will be used later.\nProposition 2 (Dual norm) The dual norm kY k\u2217D , defined as\nkY k\u2217D =\n\nsup tr X \u22a4 Y,\nkXkD 61\n\nis equal to kY k\u2217D = supkukC 61, kvkR 61 v \u22a4 Y \u22a4 u.\nProof We have, by convex duality (see, e.g., [13]),\nkY k\u2217D =\n=\n\nsup tr X \u22a4 Y = inf sup tr X \u22a4 Y \u2212 \u03bbkXkD + \u03bb\n\u03bb>0 X\n\nkXkD 61\n\ninf lim\n\n\u03bb>0 M \u2192\u221e\n\nLet a = supkukC 61, kvkR 61\n\nM\nX\n\nm=1\nv \u22a4 Y \u22a4 u.\n\n\u22a4 \u22a4\nY um \u2212 \u03bbkum kC kvm kR ) + \u03bb\n( sup vm\num ,vm\n\nIf \u03bb < a,\n\n\u22a4 \u22a4\nsup vm\nY um \u2212 \u03bbkum kC kvm kR = +\u221e,\n\num ,vm\n\nwhile if \u03bb > a, then\n\u22a4 \u22a4\nY um \u2212 \u03bbkum kC kvm kR = 0.\nsup vm\n\num ,vm\n\nThe result follows.\n\n4\n\n\f3 Closed-form decomposition norms\nWe now consider important special cases, where the decomposition norms can be expressed\nin closed form. For these norms, with the square loss, the convex optimization problems may\nalso be solved in closed form. Essentially, in this section, we show that in simple situations\ninvolving sparsity (in particular when one of the two norms k * kC or k * kR is the l1 -norm),\nletting the dictionary size M go to infinity often leads to trivial dictionary solutions, namely\na copy of some of the rows of Y . This shows the importance of constraining not only the l1 norms, but also the l2 -norms, of the sparse vectors um , m \u2208 {1, . . . , M }, and leads to the joint\nlow-rank/high-sparsity solution presented in Section 4.\n\n3.1\n\nTrace norm: k * kC = k * k2 and k * kR = k * k2\n\nWhen we constrain both the l2 -norms of um and of vm , it is well-known, that k * kD is the sum\nof the singular values of X, also known as the trace norm [12]. In this case we only need M 6\nmin{N, P } dictionary elements, but this number will turn out in general to be a lot smaller-\nsee in particular[14] for rank consistency results related to the trace norm. Moreover, with the\nPmin{N,P }\nsquare loss, the solution of the optimization problem in Eq. (5) is X = m=1\nmax{\u03c3m \u2212\nPmin{N,P }\n\u22a4\n\u22a4\n\u03bbN P, 0}um vm , where Y = m=1\n\u03c3m um vm is the singular value decomposition of Y .\nThresholding of singular values, as well as its interpretation as trace norm minimization is wellknown and well-studied. However, sparse decompositions (as opposed to simply low-rank decompositions) have shown to lead to better decompositions in many domains such as image\nprocessing (see, e.g., [8]).\n\n3.2\n\nSum of norms of rows: k * kC = k * k1\n\nWhen we use the l1 -norm for kum kC , whatever the norm on vm , we have:\nkY k\u2217D =\n=\n\nv \u22a4 Y \u22a4 u = sup\n\nsup\nkuk1 61, kvkR 61\n\nmax\n\nn\u2208{1,...,N }\n\nsup v \u22a4 Y \u22a4 u = sup kY vk\u221e\n\nkvkR 61 kuk1 61\n\nmax kY (n, :)vkR =\nv\n\nmax\n\nn\u2208{1,...,N }\n\nkY\n\nkvkR 61\n\u22a4 \u2217\n(n, :) kR ,\n\nwhich implies immediately that\nkXkD = sup tr X \u22a4 Y =\nkY k\u2217D 61\n\nN\nX\n\nn=1 kY\n\ntr X(n, :)Y (n, :)\u22a4 =\n\nsup\n(n,:)\u22a4 k\u2217R 61\n\nN\nX\n\nkX(n, :)\u22a4 kR .\n\nn=1\n\nThat is, the decomposition norm\nthe sum of the norms of the rows. Moreover, an optiPN is simply\n\u22a4\nmal decomposition is X = n=1 \u03b4n \u03b4n X, where \u03b4n \u2208 RN is a vector with all null components\nexcept at n, where it is equal to one. In this case, each row of X is a dictionary element and the\ndecomposition is indeed extremely sparse (only one non zero coefficient).\nIn particular, when k*kR = k*k2 , we obtain the sum of the l2 -norms of the rows, which leads\nto a closed form solution to Eq. (6) as X(n, :) = max{kY (n, :)\u22a4 k2 \u2212 \u03bbN P, 0}Y (n, :)/kY (n, :\n)\u22a4 k2 for all n \u2208 {1, . . . , N }. Also, when k * kR = k * k1 , we obtain the sum of the l1 -norms\nof the rows, i.e, the l1 -norm of all entries of the matrix, which leads to decoupled equations for\neach entry and closed form solution X(n, p) = max{|Y (n, p)| \u2212 \u03bbN P, 0}Y (n, p)/|Y (n, p)|.\n\n5\n\n\fThese examples show that with the l1 -norm on the decomposition coefficients, these simple\ndecomposition norms do not lead to solutions with small dictionary sizes. This suggests to\nconsider a larger set of norms which leads to low-rank/small-dictionary and sparse solutions.\nHowever, those two extreme cases still have a utility as they lead to good search ranges for the\nregularization parameter \u03bb for the mized norms presented in the next section.\n\n4\n\nSparse decomposition norms\n\nWe now assume that we have k * kR = k * k2 , i.e, we use the l2 -norm on the dictionary elements.\nIn this situation, when k * kC = k * k1 , as shown in Section 3.2, the solution corresponds to a very\nsparse but large (i.e., large dictionary size M ) matrix U ; on the contrary, when k * kC = k * k2 ,\nas shown in Section 3.1, we get a small but non sparse matrix U . It is thus natural to combine\nthe two norms on the decomposition coefficients. The main result of this section is that the way\nwe combine them is mostly irrelevant and we can choose the combination which is the easiest\nto optimize.\nProposition 3 If the loss l is differentiable, then for any function f : R+ \u00d7 R+ \u2192 R+ , such\nthat k * kC = f (k * k1 , k * k2 ) is a norm, and which is increasing with respect to both variables,\nthe solution of Eq. (6) for k * kC = f (k * k1 , k * k2 ) is the solution of Eq. (6) for k * kC =\n[(1 \u2212 \u03bd)k * k21 + \u03bdk * k22 ]1/2 , for a certain \u03bd and a potentially different regularization parameter\n\u03bb.\nP\nPP\n\u2217\nProof If we denote L(X) = N1P N\nn=1\np=1 l(Ynp , Xnp ) and L its Fenchel conjugate [13],\nthen the dual problem of Eq. (6) is the problem of maximizing \u2212L\u2217 (Y ) such that kY k\u2217D 6 \u03bb.\nSince the loss L is differentiable, the primal solution X is entirely characterized by the dual\nsolution Y . The optimality condition for the dual problem is exactly that the gradient of L\u2217\nis equal to uv \u22a4 , where (u, v) is one of the maximizers in the definition of the dual norm, i.e.,\nin supf (kuk1 ,kuk2 )61, kvk2 61 v \u22a4 Y \u22a4 u. In this case, we have v in closed form, and u is the maximizer of supf (kuk1 ,kuk2 )61 u\u22a4 Y Y \u22a4 u. With our assumptions on f , these maximizers are the\nsame as the ones subject to kuk1 6 \u03b11 and kuk2 6 \u03b12 for certain \u03b11 , \u03b12 \u2208 R+ . The optimality\ncondition is thus independent of f . We then select the function f (a, b) = [(1 \u2212 \u03bd)a2 + \u03bdb2 ]1/2\nwhich is practical as it leads to simple lower bounds (see below).\nWe thus now consider the norm defined as kuk2C = (1 \u2212 \u03bd)kuk21 + \u03bdkuk22 . We denote by F\nP\nthe convex function defined on symmetric matrices as F (A) = (1 \u2212 \u03bd) N\ni,j=1 |Aij | + \u03bd tr A,\n\u22a4\n2\n2\n2\nfor which we have F (uu ) = (1 \u2212 \u03bd)kuk1 + \u03bdkuk2 = kukC .\nM (X) in Eq. (2), we can optimize with respect to V in closed form,\nIn the definition of fD\ni.e.,\nM\n1 X\n1\nmin\nkvm k22 = tr X \u22a4 (U U \u22a4 )\u22121 X\nP\n\u00d7M\n\u22a4\n2\nV \u2208R\n, X=U V 2\nm=1\nis attained at V = X \u22a4 (U U \u22a4 )\u22121 U (the value is infinite if the span of the columns of U is not\nincluded in the span of the columns of X). Thus the norm is equal to\nkXkD = lim\n\nmin\n\nM \u2192\u221e U \u2208RN\u00d7M\n\nM\n1\n1 X\nF (um u\u22a4\ntr X \u22a4 (U U \u22a4 )\u22121 X.\nm) +\n2\n2\nm=1\n\n6\n\n(8)\n\n\fThough kXkD is a convex function of X, we currently don't\ntime algorithm\nP have a polynomialP\n\u22a4\n)\n>\nF\n(\nto compute it, but, since F is convex and homogeneous, m>0 F (um u\u22a4\nm\nm>0 um um ).\nThis leads to the following lower-bounding convex optimization problem in the positive semidefinite matrix A = U U \u22a4 :\nkXkD >\n\nmin\n\nA\u2208RN\u00d7N ,\n\n1\n1\nF (A) + tr X \u22a4 A\u22121 X.\n2\n2\nA<0\n\n(9)\n\nThis problem can now be solved in polynomial time [13]. This computable lower bound in\nEq. (9) may serve two purposes: (a) it provides a good initialization to gradient descent or path\nfollowing rounding techniques presented in Section 4.1; (b) the convex lower bound provides\nsufficient conditions for approximate global optimality of the non convex problems [13].\n\n4.1 Recovering the dictionary and/or the decomposition\nGiven a solution or approximate solution X to our problem, one may want to recover dictionary elements U and/or the decomposition V for further analysis. Note that (a) having one of\nthem automatically gives the other one and (b) in some situations, e.g., denoising of Y through\nestimating X, the matrices U and V are not explicitly needed.\nWe propose to iteratively minimize with respect to U (by gradient descent) the following\nfunction, which is a convex combination of the true function in Eq. (8) and its upper bound in\nEq. (9):\n1\n\u03b7 X\n1\u2212\u03b7\nF (um u\u22a4\nF (U U \u22a4 ) +\ntr X \u22a4 (U U \u22a4 )\u22121 X.\nm) +\n2\n2\n2\nm>0\n\nWhen \u03b7 = 0 this is exactly our convex lower bound applied defined in Eq. (9), for which\nthere are no local minima in U , although it is not a convex function of U (see Section 4.3 for\nmore details), while at \u03b7 = 1, we get a non-convex function of U , with potentially multiple\nlocal minima. This path following strategy has shown to lead to good local minima in other\nsettings [15].\nMoreover, this procedure may be seen as the classical rounding operation that follows a\nconvex relaxation-the only difference here is that we relax a hard convex problem into a simple\nconvex problem. Finally, the same technique can be applied when minimizing the regularized\nestimation problem in Eq. (6), and, as shown in Section 5, rounding leads to better performance.\n\n4.2 Optimization with square loss\nIn our simulations, we will focus on the square loss as it leads to simpler optimization, but our\ndecomposition norm framework could be applied to other losses. With the square loss, we can\noptimize directly with respect to V (in the same way theat we could earlier for computing the\nnorm itself); we temporarily assume that U \u2208 RN \u00d7M is known; we have:\n=\n\nmin\n\n\u03bb\n1\nkY \u2212 U V \u22a4 k2F + kV k2F\n2N P\n2\ni\nh\ntr Y \u22a4 I \u2212 U (U \u22a4 U + \u03bbN P I)\u22121 U \u22a4 Y\n\nV \u2208RP \u00d7M\n\n=\n\n1\n2N P\n\n=\n\n1\ntr Y \u22a4 (U U \u22a4 /\u03bbN P + I)\u22121 Y,\n2N P\n\n7\n\n\fwith a minimum attained at V = Y \u22a4 U (U \u22a4 U + \u03bbN P I)\u22121 = Y \u22a4 (U U \u22a4 + \u03bbN P I)\u22121 U . The\nminimum is a convex function of U U \u22a4 \u2208 RN \u00d7N and we now have a convex optimization\nproblem over positive semi-definite matrices, which is equivalent to Eq. (6):\nX\n1\n\u03bb\ntr Y \u22a4 (A/\u03bbN P + I)\u22121 Y +\nmin\nF (um u\u22a4\nP\nm ).\n2 A= m>0 um u\u22a4\nA\u2208RN\u00d7N , A<0 2N P\nm\nmin\n\n(10)\n\nm>0\n\nIt can be lower bounded by the following still convex, but now solvable in polynomial time,\nproblem:\n\u03bb\n1\ntr Y \u22a4 (A/\u03bb + I)\u22121 Y + F (A).\n(11)\nmin\n2\nA\u2208RN\u00d7N , A<0 2\nThis fully convex approach will be solved within a globally optimal low-rank optimization\nframework (presented in the next section). Then, rounding operations similar to Section 4.1\nmay be used to improve the solution-note that this rounding technique takes Y into account\nand it thus preferable to the direct application of Section 4.1.\n\n4.3 Low rank optimization over positive definite matrices\nP\n2\n2 1/2 + \u03bd tr A as an approximation\nWe first smooth the problem by using (1 \u2212 \u03bd) N\ni,j=1 (Aij + \u03b5 )\nPN\nof F (A), and (1 \u2212 \u03bd)( i=1 (u2i + \u03b52 )1/2 )2 + \u03bdkuk22 as an approximation of F (uu\u22a4 ).\nFollowing [16], since we expect low-rank solutions, we can optimize over low-rank matrices. Indeed, [16] shows that if G is a convex function over positive semidefinite symmetric\nmatrices of size N , with a rank deficient global minimizer (i.e., of rank r < N ), then the\nfunction U 7\u2192 G(U U \u22a4 ) defined over matrices U \u2208 RN \u00d7M has no local minima as soon as\nM > r. The following novel proposition goes a step further for twice differentiable functions\nby showing that there is no need to know r in advance:\nProposition 4 Let G be a twice differentiable convex function over positive semidefinite symmetric matrices of size N , with compact level sets. If the function H : U 7\u2192 G(U U \u22a4 ) defined\nover matrices U \u2208 RN \u00d7M has a local minimum at a rank-deficient matrix U , then U U \u22a4 is a\nglobal minimum of G.\nProof Let N = U U \u22a4 . The gradient of H is equal to \u2207H(U ) = 2\u2207G(U U \u22a4 )U and the\nHessian of H is such that \u22072 H(U )(V, V ) = 2 tr \u2207G(U U \u22a4 )V V \u22a4 + \u22072 G(U U \u22a4 )(U V \u22a4 +\nV U \u22a4 , U V \u22a4 + V U \u22a4 ). Since we have a local mimimum, \u2207H(U ) = 0 which implies that\ntr \u2207G(N )N = tr \u2207H(U )U \u22a4 = 0. Moreover, by invariance by post-multiplying U by an\northogonal matrix, without loss of generality, we may consider that the last column of U is zero.\nWe now consider all directions V \u2208 RN \u00d7M with first M \u2212 1 columns equal to zero and last\ncolumn being equal to a given v \u2208 RN . The second order Taylor expansion of H(U + tV ) is\nH(U + tV ) = H(U ) + t2 tr \u2207G(N )V V \u22a4\nt2\n= + \u22072 G(N )(U V \u22a4 + V U \u22a4 , U V \u22a4 + V U \u22a4 ) + O(t3 )\n2\n= H(U ) + t2 v \u22a4 \u2207G(N )v + O(t3 ).\nSince we have a local minima, we must have v \u22a4 \u2207G(N )v > 0. Since v is arbitrary, this implies\nthat \u2207G(N ) < 0. Together with the convexity of G and tr \u2207G(N )N = 0, this implies that we\nhave a global minimum of G [13].\n\n8\n\n\fThe last proposition suggests to try a small M , and to check that a local minimum that we can\nobtain with descent algorithms is indeed rank-deficient. If it is, we have a solution; if not, we\nsimply increase M and start again until M turns out to be greater than r.\nNote that minimizing our convex lower bound in Eq. (7) by any descent algorithm in (U, V )\nis different than solving directly Eq. (1): in the first situation, there are no (non-global) local\nminima, whereas there may be some in the second situation. In practice, we use a quasi-Newton\nalgorithm which has complexity O(N 2 ) to reach a stationary point, but requires to compute the\nHessian of size N M \u00d7 N M to check and potentially escape local minima.\n\n4.4 Links with sparse principal component analysis\nIf we now consider that we want sparse dictionary elements instead of sparse decompositions,\nwe exactly obtain the problem of sparse PCA [17, 18], where one wishes to decompose a data\nmatrix Y into X = U V \u22a4 where the dictionary elements are sparse, and thus easier to interpret.\nNote that in our situation, we have seen that with k * kR = k * k2 , the problem in Eq. (1) is\nequivalent to Eq. (10) and indeed only depends on the covariance matrix P1 Y Y \u22a4 .\nThis approach to sparse PCA is similar to the non convex formulations of [18] and is to be\ncontrasted with the convex formulation of [17] as we aim at directly obtaining a full decomposition of Y with an implicit trade-off between dictionary size (here the number of principal\ncomponents) and sparsity of such components. Most works consider one unique component,\neven though the underlying data have many more underlying dimensions, and deal with multiple components by iteratively solving a reduced problem. In the non-sparse case, the two\napproaches are equivalent, but they are not here. By varying \u03bb and \u03bd, we obtain a set of solutions with varying ranks and sparsities. We are currently comparing the approach of [18], which\nconstrains the rank of the decomposition to ours, where the rank is penalized implicitly.\n\n5 Simulations\nWe have performed extensive simulations on synthetic examples to compare the various formulations. Because of identifiability problems which are the subject of ongoing work, it is not\nappropriate to compare decomposition coefficients and/or dictionary elements; we rather consider a denoising experiment. Namely, we have generated matrices Y0 = U V \u22a4 as follows:\nselect M unit norm dictionary elements v1 , . . . , vM in RP uniformly and independently at random, for each n \u2208 {1, . . . , N }, select S indices in {1, . . . , M } uniformly at random and form\nthe n-th row of U \u2208 RN \u00d7M with zeroes except for random normally distributed elements at the\nS selected indices. Construct Y = Y0 + (tr Y0 Y0\u22a4 )1/2 \u03c3\u03b5/(N P )1/2 , where \u03b5 has independent\nstandard normally distributed elements and \u03c3 (held fixed at 0.6). The goal is to estimate Y0\nfrom Y , and we compare the three following formulations on this task: (a) the convex minimization of Eq. (11) through techniques presented in Section 4.3 with varying \u03bd and \u03bb, denoted\nas C ONV, (b) the rounding of the previous solution using techniques described in Section 4.1,\ndenoted as C ONV-R, and (c) the low-rank constrained problem in Eq. (1) with k * kC = k * k1\nand k * kR = k * k2 with varying \u03bb and M , denoted as N O C ONV, and which is the standard\nmethod in sparse dictionary learning [8, 2, 11].\nFor the three methods and for each replication, we select the two regularization parameters\nthat lead to the minimum value kX \u2212 Y0 k2 , and compute the relative improvement on using the\nsingular value decomposition (SVD) of Y . If the value is negative, denoising is better than with\n\n9\n\n\f#\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\nP\n10\n20\n10\n20\n10\n20\n10\n20\n10\n20\n10\n20\n10\n20\n10\n20\n10\n20\n\nM S\n10 2\n10 2\n20 2\n20 2\n40 2\n40 2\n10 4\n10 4\n20 4\n20 4\n40 4\n40 4\n10 8\n10 8\n20 8\n20 8\n40 8\n40 8\n\nN O C ONV\n-16.4\u00b15.7\n-40.8\u00b14.2\n-8.6\u00b13.6\n-24.9\u00b13.3\n-6.6\u00b12.8\n-13.2\u00b12.6\n1.7\u00b13.9\n-16.7\u00b15.9\n2.2\u00b12.4\n-1.2\u00b12.5\n3.5\u00b13.0\n3.7\u00b12.3\n9.6\u00b13.4\n-1.6\u00b13.7\n9.6\u00b12.4\n11.3\u00b11.8\n8.8\u00b13.0\n10.9\u00b11.1\n\nN = 100\nC ONV-R\n-9.0\u00b11.9\n-11.6\u00b12.6\n-9.0\u00b11.8\n-13.0\u00b10.7\n-8.9\u00b11.5\n-12.3\u00b11.4\n-1.5\u00b10.5\n-1.4\u00b10.8\n-2.5\u00b10.9\n-3.1\u00b11.1\n-3.3\u00b11.3\n-3.9\u00b10.6\n-0.1\u00b10.1\n0.0\u00b10.0\n-0.4\u00b10.4\n-0.2\u00b10.2\n-0.8\u00b10.7\n-0.9\u00b10.6\n\nC ONV\n-6.5\u00b12.3\n-5.6\u00b13.2\n-8.4\u00b11.9\n-10.4\u00b11.1\n-9.0\u00b11.4\n-11.5\u00b11.3\n-0.2\u00b10.2\n-0.0\u00b10.0\n-1.7\u00b10.8\n-0.9\u00b10.9\n-3.3\u00b11.5\n-3.6\u00b10.8\n0.0\u00b10.0\n0.0\u00b10.0\n-0.2\u00b10.3\n-0.0\u00b10.0\n-0.7\u00b10.7\n-0.6\u00b10.5\n\nN O C ONV\n-19.8\u00b12.3\n-45.5\u00b12.0\n-15.0\u00b12.7\n-40.9\u00b12.2\n-7.6\u00b12.6\n-25.4\u00b13.0\n-1.9\u00b12.5\n-27.1\u00b11.8\n2.0\u00b12.9\n-12.1\u00b13.0\n2.6\u00b10.9\n-1.7\u00b11.7\n7.2\u00b13.0\n-4.8\u00b12.3\n9.4\u00b11.5\n7.0\u00b12.5\n7.2\u00b11.3\n9.4\u00b11.0\n\nN = 200\nC ONV-R\n-10.2\u00b11.6\n-16.4\u00b11.4\n-11.5\u00b11.5\n-18.9\u00b10.8\n-10.1\u00b11.6\n-16.7\u00b11.3\n-1.7\u00b10.6\n-3.0\u00b10.7\n-2.5\u00b10.8\n-5.5\u00b11.0\n-3.3\u00b10.5\n-6.3\u00b10.9\n-0.1\u00b10.1\n0.0\u00b10.0\n-0.4\u00b10.4\n-0.4\u00b10.3\n-0.7\u00b10.4\n-1.0\u00b10.4\n\nC ONV\n-7.1\u00b12.0\n-7.0\u00b11.3\n-10.5\u00b11.5\n-14.8\u00b10.7\n-9.9\u00b11.6\n-15.6\u00b11.4\n-0.1\u00b10.1\n0.0\u00b10.0\n-1.2\u00b11.0\n-1.6\u00b11.0\n-3.3\u00b10.5\n-5.3\u00b10.8\n0.0\u00b10.0\n0.0\u00b10.0\n-0.2\u00b10.2\n-0.0\u00b10.0\n-0.5\u00b10.5\n-0.4\u00b10.4\n\nTable 1: Percentage of improvement in mean squared error, with respect to spectral denoising, for\nvarious parameters of the data generating process. See text for details.\n\n10\n\n\fthe SVD (the more negative, the better). In Table 1, we present averages over 10 replications\nfor various values of N , P , M , and S.\nFirst, in these simulations where the decomposition coefficients are known to be sparse,\npenalizing by l1 -norms indeed improves performance on spectral denoising for all methods.\nSecond, as expected, the rounded formulation (C ONV-R) does perform better than the nonrounded one (C ONV), i.e., our rounding procedure allows to find \"good\" local minima of the\nnon-convex problem in Eq. (1).\nMoreover, in high-sparsity situations (S = 2, lines 1 to 6 of Table 1), we see that the rankconstrained formulation N O C ONV outperforms the convex formulations, sometimes by a wide\nmargin (e.g., lines 1 and 2). This is not the case when the ratio M/P becomes larger than 2\n(lines 3 and 5). In the medium-sparsity situation (S = 4, lines 7 to 12), we observe the same\nphenomenon, but the non-convex approach is better only when the ratio M/P is smaller than or\nequal to one. Finally, in low-sparsity situations (S = 8, lines 13 to 18), imposing sparsity does\nnot improve performance much and the local minima of the non-convex approach N O C ONV\nreally hurt performance. Thus, from Table 1, we can see that with high sparsity (small S) and\nsmall relative dictionary size of the original non noisy data (i.e., low ratio M/P ), the non convex\napproach performs better. We are currently investigating theoretical arguments to support these\nempirical findings.\n\n6 Conclusion\nIn this paper, we have investigated the possibility of convexifying the sparse dictionary learning problem. We have reached both positive and negative conclusions: indeed, it is possible to\nconvexify the problem by letting the dictionary size explicitly grow with proper regularization\nto ensure low rank solutions; however, it only leads to better predictive performance for problems which are not too sparse and with large enough dictionaries. In the high-sparsity/smalldictionary cases, the non convex problem is empirically simple enough to solve so that our\nconvexification leads to no gain.\nWe are currently investigating more refined convexifications and extensions to nonnegative\nvariants [9], applications of our new decomposition norms to clustering [9], the possibility of\nobtaining consistency theorems similar to [14] for the convex formulation, and the application\nto the image denoising problem [2].\n\nReferences\n[1] Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition\nby basis pursuit. SIAM Journal on Scientific Computing, 20(1):33\u201361, 1999.\n[2] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over\nlearned dictionaries. IEEE Trans. Image Proc., 15(12):3736\u20133745, 2006.\n[3] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of The Royal\nStatistical Society Series B, 58(1):267\u2013288, 1996.\n[4] S. Mallat and Z. Zhang. Matching pursuit with time-frequency dictionaries. IEEE Trans\nSig. Proc., 41:3397\u20133415, 1993.\n\n11\n\n\f[5] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Ann. Stat.,\n32:407, 2004.\n[6] E. J. Cand\u00e8s and T. Tao. Decoding by linear programming. IEEE Trans. Information\nTheory, 51(12):4203\u20134215, 2005.\n[7] P. Zhao and B. Yu. On model selection consistency of Lasso. J. Mach. Learn. Res.,\n7:2541\u20132563, 2006.\n[8] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy\nemployed by V1? Vision Research, 37:3311\u20133325, 1997.\n[9] C. Ding, T. Li, and M. I. Jordan. Convex and semi-nonnegative matrix factorizations.\nTechnical Report 60428, Lawrence Berkeley Nat. Lab., 2006.\n[10] M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Trans. Sig. Proc., 54(11):4311\u20134322,\n2006.\n[11] H. Lee, Al. Battle, R. Raina, and A. Y. Ng. Efficient sparse coding algorithms. In NIPS,\n2007.\n[12] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In\nNIPS, 2005.\n[13] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2003.\n[14] F. R. Bach. Consistency of trace norm minimization. Technical Report 00179522, HAL,\n2008.\n[15] A. Blake and A. Zisserman. Visual Reconstruction. MIT Press, 1987.\n[16] S. A. Burer and R. D. C. Monteiro. Local minima and convergence in low-rank semidefinite programming. Math. Prog., 103:427\u2013444, 2005.\n[17] A. D'aspremont, El L. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation\nfor sparse PCA using semidefinite programming. SIAM Review, 49(3):434\u201348, 2007.\n[18] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. J. Comput.\nGraph. Statist., 15:265\u2013286, 2006.\n\n12\n\n\f"}