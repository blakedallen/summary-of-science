{"id": "http://arxiv.org/abs/1110.6916v1", "guidislink": true, "updated": "2011-10-31T19:46:51Z", "updated_parsed": [2011, 10, 31, 19, 46, 51, 0, 304, 0], "published": "2011-10-31T19:46:51Z", "published_parsed": [2011, 10, 31, 19, 46, 51, 0, 304, 0], "title": "Multi-Terminal Source Coding With Action Dependent Side Information", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.6517%2C1110.2347%2C1110.5115%2C1110.6154%2C1110.5388%2C1110.5566%2C1110.4041%2C1110.0032%2C1110.4682%2C1110.3288%2C1110.1827%2C1110.0906%2C1110.3047%2C1110.6916%2C1110.3596%2C1110.1129%2C1110.3701%2C1110.1232%2C1110.3811%2C1110.3231%2C1110.4301%2C1110.2839%2C1110.4167%2C1110.3877%2C1110.2528%2C1110.4512%2C1110.3657%2C1110.3867%2C1110.2742%2C1110.1338%2C1110.3914%2C1110.0837%2C1110.5890%2C1110.1928%2C1110.0427%2C1110.0171%2C1110.4209%2C1110.5124%2C1110.3002%2C1110.4054%2C1110.2718%2C1110.3020%2C1110.1346%2C1110.6806%2C1110.0533%2C1110.2107%2C1110.3062%2C1110.4358%2C1110.4553%2C1110.2860%2C1110.5445%2C1110.6422%2C1110.3809%2C1110.0908%2C1110.4673%2C1110.0103%2C1110.3968%2C1110.2328%2C1110.4159%2C1110.4563%2C1110.6180%2C1110.0088%2C1110.1446%2C1110.5569%2C1110.5061%2C1110.3059%2C1110.4739%2C1110.5150%2C1110.5744%2C1110.5161%2C1110.2357%2C1110.5205%2C1110.1181%2C1110.2295%2C1110.2667%2C1110.5919%2C1110.6175%2C1110.3887%2C1110.0012%2C1110.5845%2C1110.4693%2C1110.2861%2C1110.6078%2C1110.6325%2C1110.6195%2C1110.0877%2C1110.0202%2C1110.5020%2C1110.0278%2C1110.0281%2C1110.2634%2C1110.6002%2C1110.4938%2C1110.5896%2C1110.4904%2C1110.1633%2C1110.4509%2C1110.2721%2C1110.0534%2C1110.6483%2C1110.3796&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Multi-Terminal Source Coding With Action Dependent Side Information"}, "summary": "We consider multi-terminal source coding with a single encoder and multiple\ndecoders where either the encoder or the decoders can take cost constrained\nactions which affect the quality of the side information present at the\ndecoders. For the scenario where decoders take actions, we characterize the\nrate-cost trade-off region for lossless source coding, and give an\nachievability scheme for lossy source coding for two decoders which is optimum\nfor a variety of special cases of interest. For the case where the encoder\ntakes actions, we characterize the rate-cost trade-off for a class of lossless\nsource coding scenarios with multiple decoders. Finally, we also consider\nextensions to other multi-terminal source coding settings with actions, and\ncharacterize the rate -distortion-cost tradeoff for a case of successive\nrefinement with actions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.6517%2C1110.2347%2C1110.5115%2C1110.6154%2C1110.5388%2C1110.5566%2C1110.4041%2C1110.0032%2C1110.4682%2C1110.3288%2C1110.1827%2C1110.0906%2C1110.3047%2C1110.6916%2C1110.3596%2C1110.1129%2C1110.3701%2C1110.1232%2C1110.3811%2C1110.3231%2C1110.4301%2C1110.2839%2C1110.4167%2C1110.3877%2C1110.2528%2C1110.4512%2C1110.3657%2C1110.3867%2C1110.2742%2C1110.1338%2C1110.3914%2C1110.0837%2C1110.5890%2C1110.1928%2C1110.0427%2C1110.0171%2C1110.4209%2C1110.5124%2C1110.3002%2C1110.4054%2C1110.2718%2C1110.3020%2C1110.1346%2C1110.6806%2C1110.0533%2C1110.2107%2C1110.3062%2C1110.4358%2C1110.4553%2C1110.2860%2C1110.5445%2C1110.6422%2C1110.3809%2C1110.0908%2C1110.4673%2C1110.0103%2C1110.3968%2C1110.2328%2C1110.4159%2C1110.4563%2C1110.6180%2C1110.0088%2C1110.1446%2C1110.5569%2C1110.5061%2C1110.3059%2C1110.4739%2C1110.5150%2C1110.5744%2C1110.5161%2C1110.2357%2C1110.5205%2C1110.1181%2C1110.2295%2C1110.2667%2C1110.5919%2C1110.6175%2C1110.3887%2C1110.0012%2C1110.5845%2C1110.4693%2C1110.2861%2C1110.6078%2C1110.6325%2C1110.6195%2C1110.0877%2C1110.0202%2C1110.5020%2C1110.0278%2C1110.0281%2C1110.2634%2C1110.6002%2C1110.4938%2C1110.5896%2C1110.4904%2C1110.1633%2C1110.4509%2C1110.2721%2C1110.0534%2C1110.6483%2C1110.3796&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider multi-terminal source coding with a single encoder and multiple\ndecoders where either the encoder or the decoders can take cost constrained\nactions which affect the quality of the side information present at the\ndecoders. For the scenario where decoders take actions, we characterize the\nrate-cost trade-off region for lossless source coding, and give an\nachievability scheme for lossy source coding for two decoders which is optimum\nfor a variety of special cases of interest. For the case where the encoder\ntakes actions, we characterize the rate-cost trade-off for a class of lossless\nsource coding scenarios with multiple decoders. Finally, we also consider\nextensions to other multi-terminal source coding settings with actions, and\ncharacterize the rate -distortion-cost tradeoff for a case of successive\nrefinement with actions."}, "authors": ["Yeow-Khiang Chia", "Himanshu Asnani", "Tsachy Weissman"], "author_detail": {"name": "Tsachy Weissman"}, "author": "Tsachy Weissman", "arxiv_comment": "23 pages", "links": [{"href": "http://arxiv.org/abs/1110.6916v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1110.6916v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1110.6916v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1110.6916v1", "journal_reference": null, "doi": null, "fulltext": "Multi-Terminal Source Coding With Action\nDependent Side Information\n\n1\n\nYeow-Khiang Chia, Himanshu Asnani and Tsachy Weissman\nDepartment of Electrical Engineering, Stanford University\nEmail: ykchia@stanford.edu, asnani@stanford.edu, tsachy@stanford.edu\n\narXiv:1110.6916v1 [cs.IT] 31 Oct 2011\n\nAbstract\nWe consider multi-terminal source coding with a single encoder and multiple decoders where either the encoder\nor the decoders can take cost constrained actions which affect the quality of the side information present at the\ndecoders. For the scenario where decoders take actions, we characterize the rate-cost trade-off region for lossless\nsource coding, and give an achievability scheme for lossy source coding for two decoders which is optimum for\na variety of special cases of interest. For the case where the encoder takes actions, we characterize the rate-cost\ntrade-off for a class of lossless source coding scenarios with multiple decoders. Finally, we also consider extensions\nto other multi-terminal source coding settings with actions, and characterize the rate -distortion-cost tradeoff for a\ncase of successive refinement with actions.\n\nI. I NTRODUCTION\nThe problem of source coding with decoder side information (S.I.) was introduced in [1]. S.I. acts as an important\nresource in rate distortion problems, where it can significantly reduce the compression rate required. In classical\nshannon theory and in work building on [1], S.I. is assumed to be either always present or absent. However, in\npractical systems as we know, acquisition of S.I. is costly, the encoder or decoder has to expend resources to aquire\nside information. With this motivation, the framework for the problem of source coding with action-dependent side\ninformation (S.I.) was introduced in [2], where the authors considered the cases where the encoder or decoder are\nallowed to take actions (with cost constraints) that affect the quality or availability of the side information present\nat the decoders, and in some settings, the encoder. As noted in [2], one motivation for this setup is the case where\nthe side information is obtained via a sensor through a sequence of noisy measurements of the source sequence.\nThe sensor may have limited resources, such as acquisition time or power, in obtaining the side information. This\nis therefore modeled by the cost constraint on the action sequence to be taken at the decoder. Additional motivation\nfor considering this framework is given in [2]. We also refer readers to recent work in [3], [4] for related Shannon\ntheoretic scenarios invoking the action framework.\nIn this paper, we extend the source coding with action framework to the case where there are multiple decoders,\nwhich can take actions that affect the quality or availability of S.I. at each decoder, or where the encoder takes\nactions that affect the quality or availability of S.I. at the decoders. As a motivation for this framework, consider the\nfollowing problem: An encoder observes an i.i.d source sequence X n which it wishes to describe to two decoders\nvia a common rate limited link of rate R. The decoders, in addition to observing the output of the common rate\nlimited link, also have access to a common sensor which gives side information Y that is correlated with X.\nHowever, because of contention or resource constraints, when decoder 1 observes the side information, decoder\n2 cannot access the side information and vice versa. This problem is depicted in Figure 1. Even in the absence\nof cost constraints on the cost of switching to 1 or 2, this problem is interesting and non-trivial. How should the\ndecoders share the side information and what is the optimum sequence of actions be conveyed and then taken by\nthe decoder?\nBy posing the above problem in the framework of source coding with action dependent side information, we\nsolve it for the (near) lossless source coding case, a special case of lossy source coding with switching dependent\nside information, and give interpretations of the standard random binning and coding arguments when specialized\nto this switching problem. As one example for the implications of our findings, when Y = X, we show that the\noptimum rate required for lossless source coding in the above problem is H(X)/2 - clearly a lower bound on\nthe required rate, but that it suffices for perfect reconstruction of the source simultaneously at both decoders is,\nat first glance, surprising. We devote a significant portion of this paper to the setting where the side information\n\n\fPSfrag replacements\n\nXn\n\nDec 1\n\nXn\n\nM \u2208 [1 : 2\n\nnR\n\n1\n\n]\n\nEnc.\n\n2\n\nAi (M )\nYi\nAi (M )\n\nDec 2\n\nXn\n\nFig. 1: Lossless source coding with switching dependent side information. When the switch is at position 1, decoder\n1 observes the side information. When the switch is at position 2, decoder 2 observes the side information.\nat the decoders is obtained through a switch that determines which of the two decoders gets to observe the side\ninformation, and obtain a complete characterization of the fundamental performance limits in various scenarios\ninvolving such switching. The achieving schemes in these scenarios are interesting in their own right, and also\nprovide insight into more general cases.\nThe rest of the paper is organized as follows. In section II, we provide formal definitions and problem formulations\nfor the cases considered. In section III, we first consider the setting of lossless source coding with decoders taking\nactions with cost constraints and give the optimum rate-cost trade-off region for this setting. Next, we consider the\nsetting of lossy source coding decoders taking actions with cost constraints and give a general achievability scheme\nfor this setup. We then specialize our achievability scheme to obtain the optimum rate-distortion and cost trade-off\nregion for a number of special cases. In section V, we consider the setting where actions are taken by the encoder.\nThe rate-cost-distortion tradeoff setting is open even for the single decoder case. Hence, we only consider a special\ncase of lossless source coding for which we can characterize the rate-cost tradeoff. In section VI, we extend our\nsetup to two other multiple users settings, including the case of successive refinement with actions. The paper is\nconcluded in section VII.\nII. P ROBLEM D EFINITION\nIn this section, we give formal definitions for, and focus on, the case where there are two decoders. Generalization\nof the definitions to K decoders is straightforward, and, as we indicate in subsequent sections, some of our results\nhold in the K decoders setting. We follow the notation of [5]. We use A to denote the action random variable. The\ndistortion\nmeasure between sequences is defined in the usual way. Let d : X \u00d7 X\u0302 \u2192 [0,P\n\u221e). Then, d(xn , x\u0302n ) :=\nn\n1 Pn\n1\nn\nd(x\n,\nx\u0302\n).\nThe\ncost\nconstraint\nis\nalso\ndefined\nin\nthe\nusual\nfashion:\nlet\n\u039b(A\n)\n:=\ni\ni\ni=1\ni=1 \u039b(Ai ). Throughout\nn\nn\nQ\nn\nthis paper, sources (X n , Y n ) are specified by the joint distribution p(xn , y n ) = i=1 pX,Y (xi , yi ) (i.i.d.). The\ndecoders obtain side informationQthrough a discrete memoryless action channel PY1 ,Y2 |X,A specified by conditional\nn\ndistribution p(y1n , y2n |xn , an ) = i=1 pY1 ,Y2 |X,A (y1i , y2i |xi , ai ), with decoder j obtaining side information Yjn for\nj \u2208 {1, 2}. Extensions to more than two sources or more than two channel outputs for multiple decoders are\nstraightforward.\nA. Source coding with actions taken at the decoders\nThis setting for two decoders is shown in figure 2. A (n, 2nR ) code for the above setting consists of one encoder\nf : X n \u2192 M \u2208 [1 : 2nR ],\none joint action encoder at all decoders\nfA\u2212Dec. : M \u2208 [1 : 2nR ] \u2192 An ,\nand two decoders\ng1 : Y1n \u00d7 [1 : 2nR ] \u2192 X\u02c61n ,\ng2 : Y n \u00d7 [1 : 2nR ] \u2192 X\u02c6n ,\n2\n\n2\n\n2\n\n\fPSfrag replacements\n\nX\u03021n\n\nDec 1\nAn (M )\nXn\n\nY1n\n\nM \u2208 [1 : 2nR ]\nEnc.\n\nPY1 ,Y2 |X,A\nY2n\n\nAn (M )\nDec 2\n\nX\u03022n\n\nFig. 2: Lossy source coding with actions at the decoders.\nPSfrag replacements\nDec 1\n\nM \u2208 [1 : 2nR ]\nXn\n\nX\u03021n\n\nY1n\nPY1 ,Y2 |X,A\n\nEnc.\nAn (X n )\n\nY2n\nDec 2\n\nX\u03022n\n\nFig. 3: Lossy source coding with actions at the encoder.\nGiven a distortion-cost tuple (D1 , D2 , C), a rate R is said to be achievable if, for any \u01eb > 0 and n sufficiently\nlarge, there exists (n, 2nR ) code such that\n\" n\n#\n1X\nE\ndj (Xi , X\u0302j,i ) \u2264 Dj + \u01eb, j=1,2,\nn i=1\n\" n\n#\n1X\nE\n\u039b(Ai ) \u2264 C + \u01eb.\nn i=1\n\nThe rate-distortion-cost region, R(D1 , D2 , C), is defined as the infimum of all achievable rates.\nCausal reconstruction with action dependent side information: Some results in this paper involves the case of\ncausal reconstruction. In the case of causal reconstruction, the decoder reconstructs X\u0302i based only on the received\nmessage M and the side information up to time i. That is,\ngj,i : Yji \u00d7 [1 : 2nR ] \u2192 X\u0302j,i ,\n\nfor j \u2208 {1, 2} and i \u2208 [1 : n].\nRemark 2.1: The case of the decoders taking separate actions A1 and A2 respectively is a special case of our\nsetup since we can write A := (A1 , A2 ).\nRemark 2.2: For the reconstruction mappings, we excluded the action sequence as an input since An is a function\nof the other input M . In our (information) rate expressions, we will see the appearance of A in the expressions.\nAs we will see in the next subsection, an advantage of this definition is that it carries over to the case when the\nencoder takes actions rather than the decoders.\n\n3\n\n\fB. Source coding with action taken at the encoder\nThis setting is shown in figure 3. As the definitions and problem statement for this case are similar to the first\nsetting, we will only mention the differences between the two settings. The main difference is that the encoder\ntakes actions rather than the decoders. Therefore, in the definition of a code, we replace the case of a joint action\nencoder at the decoders with the encoder taking actions given by the function\nfA\u2212Enc : X n \u2192 An .\nAs in the setting of actions taken at the decoder, here too we assume that the side information observed by the\ndecoders is not available at the encoder. In subsequent sections we also describe the results pertaining to the case\nwhere side information is available at the encoder.\nRemark 2.3: Lossless source coding - Some of our results concern the case of lossless source coding. In the\ncase of lossless source coding, the definitions are similar, except that the distortion constraints D1 , D2 are replaced\nby the block probability of error constraint: P({X\u03021n 6= X n } \u222a {X\u03022n 6= X n }) \u2264 \u01eb.\nIII. L OSSLESS SOURCE\n\nCODING WITH ACTIONS AT THE DECODERS\n\nIn this section and the next, we consider the case of source coding with actions taken at the decoders. We first\npresent results for the lossless source coding setting. While the lossless case can be taken to be a special case of\nlossy source coding, we present them separately, as we are able to obtain stronger results for more general scenarios\nin the lossless setting, and give several interesting examples that arise from this setup. The case of lossy source\ncoding for two decoders is presented in section IV.\nFor the lossless case, we first state the result for the general case of K decoders. Our result is stated in Theorem 1.\nTheorem 1: Let the action channel be given by the conditional distribution PY1 ,Y2 ,...,YK |X,A with decoder j\nobserving the side information Yj . Then, the minimum rate required for lossless source coding with actions taken\nat the decoders and cost constraint C is given by\nR = min max {H(X|Yj , A)} + I(X; A),\nj\u2208[1:K]\n\nwhere min is taken over the distributions p(x)p(a|x)p(y1 , y2 , . . . , yK |x, a) such that E \u039b(A) \u2264 C.\nAchievability\nAs the achievability techniques used are fairly standard (cf. [5]), we give only a sketch of achievability.\nCodebook Generation:\nQ\n\u2022 Generate 2n(I(X;A)+\u01eb) An sequences according to ni=1 p(ai ).\n\u2022 Bin the set of all X n sequences into 2n(maxj\u2208[1:K] {H(X|Yj ,A)}+\u01eb) bins, B(mb ), mb \u2208 [1 : 2n(maxj\u2208[1:K] {H(X|Yj ,A)}+\u01eb) ].\nEncoding:\n\u2022 Given a source sequence xn , the encoder looks for an index MA \u2208 [1 : 2n(I(X;A)+\u01eb) ] such that (xn , an (MA )) \u2208\n(n)\nT\u01eb . If there is none, it outputs an uniform random index from [1 : 2n(I(X;A)+\u01eb) ]. If there is more than one\nsuch index, it selects an index uniformly at random from the set of feasible indices. From the covering lemma\n[5, Chapter 3], the probability of error for this step goes to 0 as n \u2192 \u221e since there are 2n(I(X;A)+\u01eb) An\nsequences.\n\u2022 The encoder also looks the index mb \u2208 [1 : 2n(maxj\u2208[1:K] {H(X|Yj ,A)}+\u01eb) ] such that xn \u2208 B(mb ).\n\u2022 It then sends the indices mb and MA to the decoders via the common link. This step requires a rate of\nR = maxj\u2208[1:K] {H(X|Yj , A)} + I(X; A) + 2\u01eb.\nDecoding:\n\u2022 The decoders take the joint action an (MA ) and obtain their side informations Yj for j \u2208 [1 : K].\n(n)\n\u2022 Decoder j then looks for the unique X n sequence in bin B(mb ) such that (X n , Yjn , an (MA )) \u2208 T\u01eb .\nAn error is declared if there is none more than one xn sequence satisfying the decoding condition. The\nprobability of error for this step goes to 0 as n \u2192 \u221e from the strong law of large numbers and the fact that\n|B| > 2n(maxj\u2208[1:K] {H(X|Yj ,A))} .\n\n4\n\n\fConverse\nGiven a (n, 2nR , C) code, consider the rate constraint for decoder j. We have\nnR \u2265 H(M )\n= I(M ; X n )\n(a)\n\n= I(An ; X n ) + I(M ; X n |An )\n\n(b)\n\n\u2265 I(An ; X n ) + H(M |An , Yjn ) \u2212 H(M |An , X n , Yjn )\n\n= H(X n ) \u2212 H(X n |An ) + I(M ; X n |An , Yjn )\n(c)\n\n\u2265 H(X n ) \u2212 H(X n |An ) + H(X n |An , Yjn ) \u2212 n\u01ebn\n\n= H(X n ) \u2212 H(X n |An ) + H(X n |An )\n+ H(Yjn |X n , An ) \u2212 H(Yjn |An ) \u2212 n\u01ebn\nn\n(d) X\nH(Xi ) + H(Yji |Xi , Ai ) \u2212 H(Yji |Ai ) \u2212 n\u01ebn .\n\u2265\ni=1\n\n(a) follows from An being a function of M ; (b) follows from the Markov chain M \u2192 (X n , An ) \u2192 Yjn ; (c)\nfollows from the assumption of lossless source coding; (d) follows from conditioning reduces entropy and the fact\nthat the action channel is a discrete memoryless channel (DMC). Define Q as the standard time sharing random\nvariable. Observe that H(XQ |Q) = H(XQ ) = H(X), H(YjQ |AQ , XQ , Q) = H(YjQ |AQ , XQ ) = H(Yj |A, X)\nand H(YjQ |AQ , Q) \u2264 H(Yj |A). Hence, we can write the lower bound as\nnR \u2265 n(H(X) + H(Yj |X, A) \u2212 H(Yj |A) \u2212 \u01ebn )\n= n(I(X; A) + H(X|Yj , A) \u2212 \u01ebn ).\nTaking the intersection of all lower bounds for all K decoders then\nus the rate expression given in the Theorem.\nPgive\nn\nFinally, the cost constraint on the action follows from C \u2265 E n1 i=1 \u039b(Ai ) = E \u039b(A).\nWe now specialize the result in Theorem 1 to the case of source coding with switching dependent side information\nmentioned in the introduction. We consider the more general setting involving K decoders.\nCorollary 1: Source coding with switching dependent side information and no cost constraints. Let (X, Y ) be\njointly distributed according to p(x, y). Let A = [1 : K] and PY1 ,Y2 ,...,YK |X,A be defined by Yj = Y when A = j\nand e otherwise for j \u2208 [1 : K]. Let \u039b(A) := 0 for all a \u2208 A. Then, the minimum rate is given by\nH(X|Y ) +\n\nK \u22121\nI(X; Y ).\nK\n\nProof:\nProof of Corollary 1 amounts to an explicit characterization of the distribution of p(a|x) in Theorem 1. For each\nj \u2208 [1 : K], we have, from Theorem 1,\nR \u2265 H(X|Yj , A) + I(X; A)\n= H(X|Y ) + I(X; Y ) \u2212 I(X; Yj |A).\n\n(1)\n\nConsider now the sum\nK\nX\n\n(a)\n\nI(X; Yj |A) =\n\nj=1\n\nX\n\np(a)I(X; Y |A = a)\n\na\u2208A\n\n= H(Y |A) \u2212 H(Y |X, A)\n(b)\n\n\u2264 H(Y ) \u2212 H(Y |X)\n= I(X; Y ).\n\n5\n\n(2)\n\n\f(a) follows from the fact that Yj = e for a 6= j and Yj = Y for a = j. (b) follows from the Markov Chain\nA\u2212X \u2212Y.\nNext, summing over the K lower bounds in (1), we obtain\nR\u2265\n\nK\nX\n1\nI(X; Yj |A))\n(KH(X|Y ) + KI(X; Y ) \u2212\nK\nj=1\n\n\u2265 H(X|Y ) + I(X; Y ) \u2212\n\n1\nI(X; Y )\nK\n\nK \u22121\nI(X; Y ),\nK\nwhere we used inequality (2) in the second last step. Finally, noting that this lower bound on the achievable rate\ncan be obtained from Theorem 1 by setting A\u22a5X and p(a = j) = 1/K completes the proof of Corollary 1.\nRemark 3.1: The action can be set to a fixed sequence independent of the source sequence. This is perhaps not\nsurprising since there is no cost on the actions.\nRemark 3.2: For K = 2 and X = Y , which is the example given in the introduction, we have R = H(X)/2.\nRemark 3.3: For this class of channels, the achievability scheme in Theorem 1 has a simple and interesting\n\"modulo-sum\" interpretation. We present a sketch of an alternative scheme for this class of switching channels for\nK = 2. It is straightforward to extend the achievability scheme given below to K decoders.\nAlternative achievability scheme\nn/2\nn\nSplit the X n sequence into 2 equal parts; X1 and Xn/2+1\nand select the fixed action sequence of letting\n= H(X|Y ) +\n\nn/2\n\nn\ndecoder 1 observe Y1\nand decoder 2 observe Yn/2+1\n. Separately compress each part using standard random\nbinning with side information to obtain M1 \u2208 [1 : 2n(H(X|Y )/2+\u01eb) ] and M2 \u2208 [1 : 2n(H(X|Y )/2+\u01eb) ] corresponding\nto the first and second half respectively. Within each bin, with high probability, there are only 2nI(X;Y )/2 typical\nX n/2 sequences and we represent each of them with an index Mj1 \u2208 [1 : 2n(I(X;Y )/2+\u01eb) ], where j \u2208 {1, 2}. Send\nout the indexes M1 and M2 , which requires a rate of H(X|Y ) + 2\u01eb. Next, send out the index M11 \u2295 M21 which\nn/2\nn/2\nrequires a rate of I(X; Y )/2 + \u01eb. From M1 and side information Y1 , decoder 1 can recover X1 with high\nprobability. Therefore, it can recover M11 with high probability. Hence, it can recover M21 from M11 \u2295 M21 and\ntherefore, recover the X nn +1 sequence. The same analysis holds for decoder 2 with the indices interchanged.\n2\nCorollary 2 gives the characterization of the achievable rate for a general switching dependent side information\nsetup with cost constraint on the actions for two decoders.\nCorollary 2: General switching depedent side information for 2 decoders. Define the action channel as follows:\nA \u2208 {0, 1, 2, 3}; A = 0, Y1 = e, Y2 = e; A = 1, Y1 = Y, Y2 = e; A = 2, Y1 = e, Y2 = Y ; and A = 3, Y1 = Y, Y2 =\nY . Let \u039b(A = j) = Cj for j \u2208 [0 : 3]. Then, the optimum rate-cost trade-off for this class of channel is given by\n\nR \u2265 I(X; A) + max {H(X|Y1 , A), H(X|Y2 , A)}\n= I(X; A) + p0 H(X|A = 0) +\n\n3\nX\n\npj H(X|Y, A = j)\n\nj=1\n\n+ max{p1 I(X; Y |A = 1), p2 I(X; Y |A = 2)},\nP\nfor some p(a|x), where P{A = j} = pj , satisfying 3j=0 pj Cj \u2264 C.\nRemark 3.4: This setup again has a \"modulo-sum interpretation\" for the term max{p1 I(X; Y |A = 1), p2 I(X; Y |A =\n2)} and the rate can also be achieved by extending the achievability scheme described in Corollary 1. The\nscheme involves partitioning the X n sequence according to the value of Ai for i \u2208 [1 : n]. Following the\nscheme in Corollary 1, Q\nwe let Mj \u2208 [1 : 2n(pj H(X|Y,A=j)+\u01eb) ] for j \u2208 [0 : 3]. We first generate a set of An\nn\ncodewords according to i=1 p(ai ). Next, for each An codeword, define Anj to be {Ai : Ai = j}. Similarly, let\nnj\nX := {Xi : Ai = j, i \u2208 [1 : n]} be the set of possible X sequences corresponding to Anj . We bin the set of\nall X nj sequences to 2n(pj H(X|Y,A=j)+\u01eb) bins, Bj (Mj ). For j \u2208 {1, 2}, further bin the set of xnj sequences into\n2n(pj I(X;Y |A=j)+\u01eb) bins, Bj1 (Mj1 ), Mj1 \u2208 [1 : 2n(pj I(X;Y |A=j)+\u01eb) ].\nFor encoding, given an xn sequence, the encoder first finds an An sequence that is jointly typical with xn .\nIt sends out the index corresponding to the An sequence found. Next, it splits the xn sequence into four partial\n6\n\n\fsequences, xnj , for j \u2208 [0 : 3], where xnj is the set of xi corresponding to Ai = j. It then finds the corresponding\nbin indices such that xnj \u2208 Bj (Ij ) for j \u2208 [0 : 3]. It then sends out the indices M0 , M1 , M2 , M3 and M11 \u2295 M21 .\nFor decoding, we mention only the scheme employed by the first decoder, since the scheme is the same in for\ndecoder 2. From the properties of jointly typical sequences and standard analysis for Slepian-Wolf lossless source\ncoding [6], it is not difficult to see that decoder 1 can recover xn0 , xn1 , xn3 with high probability. Recovery of xn1\nalso allows decoder 1 to recover the index M11 and hence, M21 from M11 \u2295 M21 . Noting that the rate of M21 and\nM2 sums up to p2 H(X|A = 2) + 2\u01eb, it is then easy to see that decoder 1 can recover xn2 with high probability.\nIn corollary 1, we showed that, for the case of switching dependent side information, the action sequence is\nindependent of the source X n when cost constraint on the actions is absent. A natural question to ask is whether\nthe action is still independent of X n when a cost constraint on the actions is present? The following example shows\nthat the optimum action sequence is in general dependent on X n .\nExample 1: Action is dependent on source statistics when cost constraint is present. Let K = 2 and (X, Y ) be\ndistributed according to an S channel, with X \u223c Bern(1/2), P(Y = 1|X = 1) = 1 and P(Y = 0|X = 0) = 0.2.\nLet A \u2208 {1, 2} with Y1 = Y if A = 1 and Y2 = Y if A = 2. Let P(A = 1) = p1 , P(X = 0|A = 1) = 1/2 + \u03b41\nand P(X = 0|A = 2) = 1/2 \u2212 \u03b42 . Figure 4 shows the probability distributions between the random variables.\nPSfrag replacements\n\n1\n2\n\n1\n\n0\n\n+ \u03b41\n\n0.2\n\n0\n\n1\n\n1\n\nX\n\nY\n\n2\n1\n2\n\n+ \u03b42\n\nA\n\nFig. 4: Probability distributions for random variables used in example 1\nSince X \u223c Bern(1/2), d1 and d2 are related by \u03b42 = p1 \u03b41 /(1\u2212p1). Therefore, we set \u03b41 = \u03b4 and \u03ba = p1 /(1\u2212p1 )\nfor this example.\nNow, let \u039b(A = 1) = 1 and \u039b(A = 2) = 0 and C = 0.4. The optimum rate-cost tradeoff in this case may be\nobtained from Corollary 2 by setting C0 = C3 = \u221e, C1 = 1 and C2 = 0, giving us\nR = I(X; A) + p1 H(X|Y, A = 1) + (1 \u2212 p1 )H(X|Y, A = 2)\n+ max{p1 I(X; Y |A = 1), (1 \u2212 p1 )I(X; Y |A = 2)},\nfor some p(a|x), where P{A = 1} = p1 , satisfying p1 \u2264 0.4. The problem of finding the optimum action sequence\nto take then reduces (after some straightforward algebra) to the following optimization problem:\nmin 1 \u2212 p1 H2 (0.5 \u2212 \u03b4) \u2212 (1 \u2212 p1 )H2 (0.5 \u2212 \u03ba\u03b4)\np1 ,\u03b4\n\n+ p1 H(X|Y, A = 1) + (1 \u2212 p1 )H(X|Y, A = 2)\n+ max{p1 I(X; Y |A = 1), (1 \u2212 p1 )I(X; Y |A = 2)},\nsubject to\n0 \u2264 p1 \u2264 0.4,\n\u2212 0.5 \u2264 \u03b4 \u2264 0.5,\n\n7\n\n\fwhere\nH(X|A = 1) = p1 H2 (0.5 \u2212 \u03b4),\nH(X|A = 2) = (1 \u2212 p1 )H2 (0.5 \u2212 \u03ba\u03b4),\n\u0012\n\n\u0013\n0.5 \u2212 \u03b4\n,\n(0.5 + \u03b4)(0.8) + (0.5 \u2212 \u03b4)\n\u0013\n\u0012\n0.5 + \u03ba\u03b4\n,\nH(X|Y, A = 2) = ((0.5 \u2212 \u03ba\u03b4)(0.8) + (0.5 + \u03ba\u03b4))H2\n(0.5 \u2212 \u03ba\u03b4)(0.8) + (0.5 + \u03ba\u03b4)\nH(X|Y, A = 1) = ((0.5 + \u03b4)(0.8) + 0.5 \u2212 \u03b4)H2\n\nand H2 (.) is the binary entropy function.\nWhile exact solution to this (non-convex) optimization problem involves searching over p1 and \u03b4, it is easy to see\nthat if A is restricted to be independent of X, which corresponds to restricting \u03b4 to be equal to 0, then the optimum\nsolution for p1 is 0.4. Under p1 = 0.4 and \u03b4 = 0, we obtain RA\u22a5X = 0.9568. In contrast, setting p1 = 0.4 and\n\u03b4 = \u22120.05, we obtain R = 0.9554, which shows that the optimum action sequence is in general dependent on the\nsource X when cost constraints are present.\nAn explanation for this observation is as follows. The cost constraint forces decoder 1 to see less of the side\ninformation Y than decoder 2. It may therefore make sense to bias the distribution X|A = 1 so that Y conveys more\ninformation about the source sequence X, even at the expense of describing the action sequence to the decoders.\nRoughly speaking, the amount of information conveyed about X by Y may be measured by I(X; Y ). Note that\nunder \u03b4 = 0, I(X; Y |A = 1) = 0.108, whereas under \u03b4 = \u22120.05, I(X; Y |A = 1) = 0.1116. A plot of the optimum\nrate versus cost tradeoff obtained by searching over a grid of p1 and \u03b4 is shown in Figure 5. The figure also shows\nthe rate obtained if actions were forced to be independent of the source sequence.\nIV. L OSSY SOURCE\n\nCODING WITH ACTION AT THE DECODERS\n\nIn this section, we first consider the case when causal reconstruction is required, and give the general ratedistortion-cost region for K decoders. Next, we consider the case of lossy noncausal reconstruction for two decoders\nand give a general achievability scheme for this case. We then show that our achievability scheme is optimum for\nseveral special cases. Finally, we discuss some connections between our setting and the complementary delivery\nsetting introduced in [7].\nA. Causal reconstruction for K decoders\nTheorem 2: Causal lossy reconstruction for K decoders\nWhen the decoders are restricted to causal reconstruction [8], R(D1 , D2 , . . . , DK , C) is given by\nR = I(U ; X)\nfor some p(u|x), A = f (U ) and reconstruction functions x\u0302j for j \u2208 [1 : K] such that\nE dj (X, x\u0302j (U, Yj )) \u2264 Dj for j \u2208 [1 : K]\nE \u039b(A) \u2264 C.\nThe cardinality of U is upper bounded by |U| \u2264 |X ||A| + K.\nRemark 4.1: Theorem 2 generalizes the corresponding result for one decoder in [2, Theorem 3].\nProof: As the achievability scheme is a straightforward extension of the scheme in [2, Theorem 3], we will\nomit the proof of achievability here. For the converse, given a code that satisfies the cost and distortion constraints,\n\n8\n\n\f1\nOptimum\nAction independent\n0.99\n\nRate\n\n0.98\n\n0.97\n\n0.96\n\n0.95\n\n0.94\n\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n0.25\n0.3\nCost constraint\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\nFig. 5: Rate versus cost constraint for the example 1. It is easy to show operationally that the optimum rate versus\ncost curve is convex in the cost constraint. When the cost constraint approaches zero, the rate approaches 1, since\nthis case corresponds to decoder 1 not seeing any of the side information. When the cost constraint approaches\n0.5, the rate approaches the minimum rate without cost constraint. The red dashed line shows the rate that would\nbe obtained if actions were forced to be independent of the source. As can be seen on graph, forcing actions to\nbe independent of the source is in general not optimum when cost constraint is present. The optimum rate versus\ncost constraint plot appears to be linear over a range of cost constraints. It can be shown that if the cost constraint\nis below a threshold, then the optimum rate is a linear function of the cost constraint. However, the plot obtained\nvia numerical simulation appears to be linear in the cost constraint over a wider range than what we obtained by\nanalysis. Performing a more refined analysis to obtain a cost constraint threshold that matches the cost threshold\nobtained by simulation appears to be difficult, due to the nature of the optimization problem that is involved.\n\nwe have\nnR \u2265 H(M )\n= I(X n ; M )\nn\n(a) X\n(H(Xi ) \u2212 H(Xi |M, X i\u22121 ))\n=\ni=1\n\nn\n(b) X\n(H(Xi ) \u2212 H(Xi |M, X i\u22121 , Ai\u22121 ))\n=\ni=1\n\nn\nX\n(H(Xi ) \u2212 H(Xi |M, X i\u22121 , Ai\u22121 , Y1i\u22121 , . . . , YKi\u22121 ))\n=\n\n(c)\n\ni=1\n\n\u2265\n\nn\nX\n\n(H(Xi ) \u2212 H(Xi |Ui )),\n\ni=1\n\nwhere (a) follows from the fact that X n is a memoryless source; (b) follows from the fact that Ai\u22121 is a function\nof M ; (c) follows from the fact that the action channel p(y1 , y2 , . . . , yk |x, a) is a memoryless channel; and the last\nstep follows from defining Ui = (M, Y1i\u22121 , . . . , YKi\u22121 ). Finally, defining Q to be a random variable uniform over\n9\n\n\f[1 : n], independent of all other random variables, U = (UQ , Q), X = XQ , A = AQ and Yj = YjQ for j \u2208 [1 : K]\nthen gives the required lower bound on the minimum rate required. Further, we have A = f (U ). It remains to\nverify that the cost and distortion constraints are satisfied. Verification of the cost constraint is straightforward. For\nthe distortion constraint, we have for j \u2208 [1 : K]\nn\n\nE\n\n1X\ndj (Xi , x\u0302ji (M, Yji )) \u2265 E dj (X, x\u0302\u2032j (U, Yj )),\nn i=1\n\nwhere we define x\u0302\u2032j (U, Yj ) := x\u0302jQ (M, Yji ). This shows that the definition of the auxiliary random variable U\nsatisfies the distortion constraints. Finally, the cardinality of U can be upper bounded by using the support lemma\n[9]. We require |X ||A| \u2212 1 letters to preserve PX,A , which also preserves the cost constraint. In addition, we require\nK + 1 letters to preserve the rate and K distortion constraints.\nWe now turn to the case of noncausal reconstruction. For this setting, we give results only for the case of two\ndecoders.\nB. Noncausal reconstruction for two decoders\nWe first give a general achievability scheme for this setting.\nTheorem 3: An achievable scheme for the lossy source coding with actions at the decoders is given by\nR \u2265 I(X; A) + max {I(X; U |A, Y1 ), I(X; U |A, Y2 )}\n+ I(X; V1 |U, A, Y1 ) + I(X; V2 |U, A, Y2 )\nfor some p(x)p(a|x)p(u|a, x)p(v1 |u, a, x)p(v2 |u, a, x)p(y1 , y2 |x, a) and reconstruction functions x\u03021 and x\u03022 satisfying\nE dj (X, x\u0302j (U, Vj , A, Yj )) \u2264 Dj for j = 1, 2,\nE \u039b(A) \u2264 C.\nWe provide a sketch of achievability in Appendix A since the techniques used are fairly straightforward. As an\noverview, the encoder first tells the decoders the action sequence to take. It then sends a common description of\nX n , U n , to both decoders. Based on the action sequence An and the common description U n , the encoder sends\nV1n and V2n to decoders 1 and 2 respectively. We do not require decoder 1 to decode V2n , or for decoder 2 to\ndecode V1n .\nTheorem 3 is optimum for the following special cases.\nProposition 1: Heegard-Berger-Kaspi [10], [11] Extension. Suppose the following Markov chain holds: (X, A)\u2212\n(A, Y1 ) \u2212 (A, Y2 ). Then, the rate-distortion-cost trade-off region is given by\nR \u2265 I(X; A) + I(X; U |A, Y2 )\n+ I(X; V1 |U, A, Y1 )\nfor some p(x)p(a|x)p(u, v1 |x, a)p(y1 |x, a)p(y2 |y1 , a) satisfying\nE d1 (X, X\u03021 (U, V1 , A, Y1 )) \u2264 D1 ,\nE d2 (X, X\u03022 (U, A, Y2 )) \u2264 D2 ,\nE \u039b(A) \u2264 C.\nThe cardinality of the auxiliary random variables is upper bounded by |U| \u2264 |X ||A| + 2 and |V1 | \u2264 |U|(|X ||A| + 1).\nThe achievability for this proposition follows from Theorem 3 by setting V2 = \u2205 and noting that since (X, A) \u2212\n(A, Y1 ) \u2212 (A, Y2 ), the terms in the max{.} function simplifies to I(X; U |A, Y2 ). We give a proof of converse as\nfollows.\nConverse: Given a code that satisfies the constraints,\nnR \u2265 H(M )\n= H(M, An )\n10\n\n\f= H(An ) + H(M |An )\n\u2265 H(An ) \u2212 H(An |X n ) + H(M |An , Y2n ) \u2212 H(M |Y2n , An , X n )\n= I(X n ; An ) + I(X n ; M |An , Y2n )\n= I(X n ; An ) + I(X n ; M, Y1n |An , Y2n ) \u2212 I(X n ; Y1n |M, An , Y2n )\n= I(X n ; An ) + H(X n |An , Y2n ) \u2212 H(X n |M, Y1n , An , Y2n ) \u2212 I(X n ; Y1n |M, An , Y2n )\nn\nX\n(H(Xi |M, Y1n , An , Y2n , X i\u22121 ) + I(X n ; Y1i |M, An , Y2n , Y1i\u22121 ))\n= I(X n ; An ) + H(X n |An , Y2n ) \u2212\ni=1\n\nn\nX\n(H(Xi |M, Y1n , An , Y2n ) + I(X n ; Y1i |M, An , Y2n , Y1i\u22121 ))\n\u2265 I(X n ; An ) + H(X n |An , Y2n ) \u2212\n(a)\n\n= I(X n ; An ) + H(X n |An , Y2n ) \u2212\n\ni=1\nn\nX\n\n(H(Xi |M, Y1n , An , Y2n ) + I(Xi ; Y1i |M, An , Y2n , Y1i\u22121 ))\n\ni=1\n\n= I(X n ; An ) + H(X n |An , Y2n ) \u2212\n\nn\nX\n\nH(Xi |M, Y1i\u22121 , An , Y2n )\n\ni=1\n\n+\n\nn\nX\n\n(I(Xi ; Y1in |M, An , Y2n , Y1i\u22121 ) \u2212 I(Xi ; Y1i |M, An , Y2n , Y1i\u22121 ))\n\ni=1\n\n= I(X n ; An ) + H(X n |An , Y2n ) \u2212\n\nn\nX\n\nH(Xi |M, Y1i\u22121 , An , Y2n )\n\ni=1\n\n+\n\nn\nX\n\nn\n(I(Xi ; Y1,i+1\n|M, An , Y2n , Y1i ),\n\ni=1\n\nn\n\nn\n\nn\n\nn\n\n= I(X ; A ) + H(X |A\n\n, Y2n )\n\n\u2212\n\nn\nX\n\nH(Xi |M, Y1i\u22121 , An , Y2n )\n\ni=1\n\n+\n\nn\nX\n\nn\\i\n\nn\n(I(Xi ; Y1,i+1\n|M, An , Y2\n\n, Y1i , Y1i\u22121 ),\n\ni=1\n\nwhere (a) follows from the fact that X n\\i \u2212 (M, An , Y2n , Y1i\u22121 , Xi )\u2212 Y1i and the last step follows from the Markov\nChain assumption Xi \u2212 (Ai , Y1i ) \u2212 (Ai , Y2i ). Consider now,\nI(X n ; An ) + H(X n |An , Y2n ) = I(X n ; An ) + H(X n , Y2n |An ) \u2212 H(Y2n |An )\n= H(X n ) + H(Y2n |An , X n ) \u2212 H(Y2n |An )\nn\nX\n(H(Xi ) + H(Y2i |Xi , Ai ) \u2212 H(Y2i |Ai )).\n\u2265\ni=1\n\nHence,\nnR \u2265\n\nn\nX\n\n(H(Xi ) + H(Y2i |Xi , Ai ) \u2212 H(Y2i |Ai )) \u2212\n\ni=1\nn\nX\n\n+\n\nn\nX\n\nH(Xi |M, Y1i\u22121 , An , Y2n )\n\ni=1\n\nn\n(I(Xi ; Y1,i+1\n|M, An , Y2n\u0131 , Y1i , Y1i\u22121 ).\n\ni=1\n\nDefine now Q to be a random variable uniform over [1 : n], independent of all other random variables; X = XQ ,\nn\\i\nn\nY1 = Y1Q , Y2 = Y2Q , A = AQ , Ui = (M, Y1i\u22121 , An\\i , Y2 ), Vi = Y1,i+1\n, U = (UQ , Q) and V = VQ . Then, we\n\n11\n\n\fhave\nR \u2265 H(X) + H(Y2 |X, A) \u2212 H(Y2 |A, Q) \u2212 H(X|A, Y2 , U )\n+ I(X; V |A, Y1 , U )\n\u2265 H(X) + H(Y2 |X, A) \u2212 H(Y2 |A) \u2212 H(X|A, Y2 , U )\n+ I(X; V |A, Y1 , U )\n= I(X; A) + I(X; U |A, Y2 ) + I(X; V |A, Y1 , U ).\nIt remains to verify that the definitions of U , V and A satisfy the distortion and cost constraints, which is\nstraightforward. Prove of the cardinality bounds follows from standard techniques.\nThe next proposition extends our results for the case of switching dependent side information to the a class of\nlossy source coding with switching dependent side information.\nProposition 2: Special case of switching dependent side information. Let Y1 = X, Y2 = Y if A = 1 and\nY1 = Y, Y2 = X if A = 2 and for all x, there exists x\u03021 and x\u03022 such that d1 (x, x\u03021 ) = 0 and d2 (x, x\u03022 ) = 0. Then,\nthe rate-distortion-cost trade-off region is given by\nR \u2265 I(X; A) + max{P(A = 2)I(X; U1 |A = 2, Y ),\nP(A = 1)I(X; U2 |A = 1, Y )}\nfor some p(x, y)p(a|x)p(u1 |x, a = 2)p(u2 |x, a = 1) satisfying\nP(A = 2) E d1 (X, X\u03021 (Y, U1 )|A = 2) \u2264 D1 ,\nP(A = 1) E d2 (X, X\u03022 (Y, U2 )|A = 1) \u2264 D2 ,\nE \u039b(A) \u2264 C.\nThe cardinality of the auxiliary random variables is upper bounded by |U1 | \u2264 |X | + 1 and |U2 | \u2264 |X | + 1.\nAchievability follows from Theorem 3 by setting V1 = V2 = \u2205 and U = U2 if A = 1 and U = U1 if A = 2. We\ngive the proof of converse as follows.\nConverse: Given a code that satisfies the cost and distortion constraints, consider the rate required for decoder\n1. We have\nnR \u2265 H(M )\n= H(M, An )\n= H(An ) + H(M |An )\n\u2265 H(An ) \u2212 H(An |X n ) + H(M |An , Y1n ) \u2212 H(M |Y1n , An , X n )\n= I(X n ; An ) + I(X n ; M |An , Y1n )\n= I(X n ; An ) + H(X n , Y1n |An ) \u2212 H(Y1n |An ) \u2212 H(X n |M, An , Y1n )\n= H(X n ) + H(Y1n |An , X n ) \u2212 H(Y1n |An ) \u2212 H(X n |M, An , Y1n )\nn\nn\nX\nX\nH(Xi |M, An , Y1n ).\n(H(Xi ) + H(Y1i |Xi , Ai ) \u2212 H(Y1i |Ai )) \u2212\n\u2265\ni=1\n\ni=1\n\nAs before, we define Q to be an uniform random variable over [1 : n], independent of all other random variables.\nWe then have\nR \u2265 H(XQ |Q) + H(Y1Q |XQ , AQ , Q) \u2212 H(YQ |AQ , Q) \u2212 H(XQ |M, An , Y1n , Q)\n(a)\n\n\u2265 H(X) + H(Y1 |X, A) \u2212 H(Y1 |A) \u2212 H(X|M, An , Y1n )\n\n(b)\n\n= I(X; A) + I(X; U1 |Y1 , A).\n\n(a) follows from the discrete memoryless nature of the action channel and the fact that conditioning reduces entropy;\nn\\i\n(b) follows from defining U1i = (M, An\\i , Y1 ) and U1 = (U1Q , Q). Expanding the second term in terms of A\n\n12\n\n\fand using the observation that Y1 = X when A = 1 and Y1 = Y when A = 2, we obtain\nR \u2265 I(X; A) + P(A = 2)I(X; U1 |Y, A = 2).\nn\\i\n\nFor decoder 2, the same steps with side information Y2 instead of Y1 and defining U2i = (M, An\\i , Y2\n(U2Q , Q) yield\n\n), U2 =\n\nR \u2265 I(X; A) + P(A = 1)I(X; U2 |Y, A = 1).\nTaking the maximum over two lower bounds yield\nR \u2265 I(X; A) + max{P(A = 2)I(X; U1 |Y, A = 2), P(A = 1)I(X; U2 |Y, A = 1)}\nfor some p(a|x)p(u1 , u2 |x, a). Verifying the cost constraint is straightforward. As for the distortion constraint, we\nhave for the decoder 1\n1\nE d1 (X n , x\u0302n1 (M, An , Y1n )) = E d1 (X, x\u03021 (U1 , A, Y1 ))\nn\n= P(A = 2) E(d1 (X, x\u03021 (U1 , Y ))|A = 2).\nThe same arguments hold for decoder 2. It remains to show that the probability distribution can be restricted to the\nform p(a|x)p(u1 |a, x)p(u2 |a, x). Observe that P(A = 2) E(d1 (X, x\u03021 (U1 , Y ))|A = 2) and P(A = 2)I(X; U1 |Y, A =\n2) depends on the joint distribution only through the marginal p(a, u1 |x) and P(A = 1) E(d2 (X, x\u03022 (U2 , Y ))|A = 1)\nand P(A = 1)I(X; U2 |Y, A = 1) depends on the joint distribution only through the marginal p(a, u2 |x). Hence,\nrestricting the joint distribution to the form p(a|x)p(u1 |a, x)p(u2 |a, x) does not affect the rate, cost or distortion\nconstraints. It remains to bound the cardinality of the auxiliary random variables used, which follows from standard\ntechniques. This completes the proof of converse.\nRemark 4.2: The condition on the distortion constraints is simply to remove distortion offsets. It can be removed\nin a fairly straightforward manner.\nRemark 4.3: As with the lossless source coding with switching dependent side information case, a modulo sum\ninterpretation for the terms in the max expression is possible. When A = 1, the encoder codes for decoder 2,\nresulting, after binning, in an index I2 for the codeword U2n ; and when A = 2, the encoder codes for decoder 1,\nresulting, after binning, in an index I1 for the codeword U1n . The encoder sends out the modulo sum of the indices\nof the two codewords (I1 \u2295 I2 ) along with the index of the action codeword. Decoder 1 has the Xi sequence when\nA = 2 and hence, it has the index I2 . Therefore, it can recover it's desired index I1 from I1 \u2295 I2 . A similar analysis\nholds for decoder 2.\nExample 2: Binary source with Hamming distortion and no cost constraint. Let Y = \u2205 and X \u223c Bern(1/2).\nAssume no cost on the actions taken: \u039b(A = 1) = \u039b(A = 2) = 0 and let the distortion measure be Hamming.\nThen, the rate distortion trade-off evaluates to\nR = min max {\u03b1 (1 \u2212 H2 (D1 /\u03b1)) 1 (D1 /\u03b1 \u2264 1/2) ,\n\u03b1\n\n(1 \u2212 \u03b1) (1 \u2212 H2 (D2 /(1 \u2212 \u03b1))) 1 (D2 /(1 \u2212 \u03b1) \u2264 1/2)} ,\nwhere 1(x) denotes the indicator function. As a check, note that if D1 , D2 \u2192 0, then the rate obtained is 1/2, which\nagrees with the rate obtained in Corollary 1 for the lossless case. The result follows from explicitly evaluating the\nresult in Proposition 2. Let P(A = 2) = \u03b1. From Proposition 2, we have\nR \u2265 I(X; A) + P(A = 2)I(X; U1 |Y, A = 2)\n= 1 \u2212 (1 \u2212 \u03b1)H(X|A = 1) \u2212 \u03b1H(X|A = 2) + \u03b1H(X|A = 2) \u2212 \u03b1H(X|U1 , A = 2)\n\u2265 \u03b1 \u2212 \u03b1H(X|U1 , A = 2)\n\u2265 \u03b1(1 \u2212 H(X \u2295 X\u03021 |U1 , A = 2))\n\u0013\u0013 \u0012\n\u0013\n\u0012\n\u0012\nD1\nD1\n1\n\u2264 1/2 .\n\u2265 \u03b1 1 \u2212 H2\n\u03b1\n\u03b1\nThe last step follows from the observations that (i) if D1 /\u03b1 > 1/2, then we lower bound R by 0; and (ii) if\nD1 /\u03b1 \u2264 1/2, then from the distortion constraint \u03b1 E d(X, X\u03021 |A = 2) \u2264 D1 , H(X \u2295 X\u03021 |A = 2) \u2264 H2 (D1 /\u03b1).\n13\n\n\fThe other bound is derived in the same manner. The fact that this rate can be attained is straightforward, since we\ncan choose U1 = X\u03021 when A = 2 and U2 = X\u03022 when A = 1. In this example, the action sequence is independent of\nthe source, but unlike the case of lossless source coding, P(A = 1) is not in general equal to P(A = 2). It depends\non the distortion constraints for the individual decoders. A surface plot of the rate versus distortion constraints for\nthe two decoders is shown in Figure 6.\n\n0.5\n\nR(D1,D2)\n\n0.4\n0.3\n0.2\n0.1\n0\n0\n0.1\n0.5\n\n0.2\n\n0.4\n\n0.3\n\n0.3\n0.2\n\n0.4\n0.5\n\n0.1\n0\nD2\n\nD1\n\nFig. 6: Plot of rate versus distortions. The figure above plots the rate distortion surface R(D1 , D2 ) for the Example\n2. There is no side information, i.e., Y = \u2205 and X \u223c Bern(1/2). Assume no cost on the actions taken: \u039b(A =\n1) = \u039b(A = 2) = 0 and let the distortion measure be Hamming. Note that if any of D1 , D2 \u2192 0.5, R approaches\n0, also if D1 = D2 = 0, rate is 0.5\n\nC. Connections with Complementary Delivery\nIn the prequel, we consider several cases for switching dependent side information in which the achievability\nscheme has a simple \"modulo sum\" interpretation for the terms in the max function. This interpretation is not\nunique to our setup and in this subsection, we consider the complementary delivery setting [7] in which this\ninterpretation also arises. Formally, the complementary delivery problem is a special case of our setting and is\nobtained by letting A = \u2205, X = (X\u0303, \u1ef8 ), P(Y1 , Y2 |X) = 1Y1 =X\u0303,Y2 =\u1ef8 , \u039b(A) = 0, d1 (X, X\u03021 ) = d\u20321 (\u1ef8 , X\u03021 ) and\nd2 (X, X\u03022 ) = d\u20322 (X\u0303, X\u03022 ). For this subsection, for notational convenience, we will use X in place of X\u0303, Y in place\nof \u1ef8 , \u0176 in place of X\u03021 and X\u0302 in place of X\u03022 . This setting is shown in Figure 7. In [7], the following achievable\nrate was established\nR(D1 , D2 ) \u2265 max{I(U ; Y |X), I(U ; X|Y )},\n\n(3)\n\nfor some p(u|x, y) satisfying E d1 (Y, \u0176 (U, X)) \u2264 D1 and E d2 (X, X\u0302(U, Y )) \u2264 D2 .\nOur achievability scheme in Theorem 3 generalizes this scheme when specialized to the complementary delivery\nsetting, but we do not yet know if our achievable rate can be strictly smaller for the same distortions. However,\nby taking a modulo sum interpretation for the terms in the max{.} function in (3), as we have done for several\nexamples in this paper, we are able to give simple proofs and explicit characterization for two canonical cases:\nthe Quadratic Gaussian and the doubly symmetric binary Hamming distortion complementary delivery problems.\nWhile characterizations for these two settings also appear independently in [12], our approach in characterizing\nthese settings is different from that in [12], and we believe would be of interest to readers. Furthermore, by taking\nthe \"modulo sum\" interpretation, we establish the following, which may be a useful observation in practice: \"For\n14\n\n\fXn\n\nPSfrag replacements\n(X n , Y n )\n\nEncoder\n\nDecoder 1\n\n\u0176 n\n\nDecoder 2\n\nX\u0302 n\n\nR\n\nYn\n\nFig. 7: Complementary Delivery setting\n\nthe Quadratic Gaussian complementary delivery problem, if one has a good code (in the sense of achieving the\noptimum rate distortion tradeoff) for the point to point Wyner-Ziv [1] Quadratic Gaussian setup, then a simple\nmodification exists to turn the code into a good code for the Quadratic Gaussian complementary delivery problem.\"\nA similar observation holds for the doubly symmetric binary Hamming distortion case. We first consider the\nQuadratic Gaussian case.\nProposition 3: Quadratic Gaussian complementary delivery. Let Y = X +Z, where Z \u223c N (0, N ) is independent\nof X \u223c N (0, P ), and the distortion measures be mean square distortion. Let P \u2032 = P N/(P +N ). The rate distortion\nregion for the non-trivial constraints of D2 \u2264 P \u2032 and D1 \u2264 N is given by\n\u001a\n\u0013\n\u0012\n\u0012 \u2032 \u0013\u001b\n1\n1\nN\nP\nR(D1 , D2 ) = max\n, log\n.\nlog\n2\nD1\n2\nD2\nProof:\nConverse\nThe converse follows from straightforward cutset bound arguments. The reader may notice that the expression\ngiven above is the maximum of the Quadratic Gaussian Wyner-Ziv [1] rate to decoder 1 and the Quadratic Gaussian\nWyner-Ziv rate to decoder 2, or equivalently the maximum of the two cutset bounds. Clearly, this rate is the lowest\npossible for the given distortions.\nAchievability\nWe now show that it is also achievable using a modulo sum interpretation for (3). Consider first encoding\nfor decoder 1. From the Quadratic Gaussian Wyner-Ziv result, we know that side information at the encoder is\nredundant. Therefore, without loss of optimality, the encoder can code for decoder 1 using only Y n , resulting in\nthe codeword UYn and the corresponding index IY after binning. Similarly, for decoder 2, the encoder can code for\nn\ndecoder 2 using X n only, resulting in the codeword UX\nand index IX after binning. The encoder then sends out the\nn\nindex IX \u2295 IY . Since decoder 1 has the X sequence as side information, it knows the index IX and can therefore\nrecover IY from IX \u2295 IY . The same decoding scheme works as well for decoder 2. Therefore, we have shown the\nachievability of the given rate expression. We note further that this scheme corresponds to setting U = (UX , YY )\nsuch that UX \u2212 X \u2212 Y \u2212 UY in rate expression (3).\nRemark 4.4: As shown in our proof of achievability, if we have a good practical code for the Wyner-Ziv Quadratic\nGaussian problem, then we also have a good practical code for the complementary delivery problem setting. We\nfirst develop two point to point codes: one for the Wyner-Ziv Quadratic Gaussian case with X as the source and\nY as the side information, and another for the case where Y is the source and X is the side information. A good\ncode for the complementary delivery setting is then obtained by taking the modulo sum of the indices produced by\nthese two point to point codes.\n15\n\n\fWe now turn to the doubly symmetric binary sources with Hamming distortion case. Here, the achievability\nscheme involves taking the modulo sum of the sources X n and Y n .\nProposition 4: Doubly symmetric binary source with Hamming distortion. Let X \u223c Bern(1/2), Y \u223c Bern(1/2),\nX \u2295 Y \u223c Bern(p) and both distortion measures be Hamming distortion. Assume, without loss of generality, that\nD1 , D2 \u2264 p. Then,\nR(D1 , D2 ) = max{H(p) \u2212 H(D1 ), H(p) \u2212 H(D2 )}.\nProof: The converse again follows from straightforward cutset bounds by considering decoders 1 and 2\nindividually. For the achievability scheme, let Z = X \u2295 Y and assume that D1 \u2264 D2 . Since Z is i.i.d. Bern(p),\nusing a point to point code for Z at distortion D1 , we obtain a rate of H(p) \u2212 H(D1 ). Denote the reconstruction\nfor Z at time i by \u1e90i . Decoder 1 reconstructs Yi by \u0176i = Xi \u2295 \u1e90i for i \u2208 [1 : n]. Similarly, decoder 2\nreconstructs X by X\u0302i = Yi \u2295 \u1e90i for i \u2208 [1 : n]. To verify that the distortion constraint holds, note that\nd1 (Yi , Xi \u2295 \u1e90i ) = Yi \u2295 Xi \u2295 \u1e90i = Zi \u2295 \u1e90i . Since \u1e90 is a code that achieves distortion D1 , \u0176 satisfies the\ndistortion constraint for decoder 1. The same analysis holds for decoder 2.\nRemark 4.5: In this case, we only need a good code for the standard point to point rate distortion problem for a\nbinary source. A good rate distortion code for a binary source is also a good code for the doubly symmetric binary\nsource with Hamming distortion complementary delivery problem.\nRemark 4.6: In our scheme, the reconstruction symbols at time i depend only on the received message and the side\ninformation at the decoder at time i. Therefore, for this case, the rate distortion region for causal reconstruction [8]\nis the same as the rate distortion region for noncausal reconstruction.\nV. ACTIONS\n\nTAKEN AT THE ENCODER\n\nWe now turn to the case where the encoder takes action (figure 3) instead of the decoders. When the actions are\ntaken at the encoder, the general rate-cost-distortion tradeoff region is open even for the case of a single decoder.\nSpecial cases which have been characterized includes the lossless case [2]. In this section, we consider a special\ncase of lossless source coding with K decoders in which we can characterize the rate-cost tradeoff region.\nTheorem 4: Special case of lossless source coding with actions taken at the encoder. Let the action channel be\ngiven by the conditional distribution PY1 ,Y2 ,...,YK |X,A . Assume further that A = f1 (Y1 ) = f2 (Y2 ) = . . . , fK (YK ).\nThen, the minimum rate required for lossless source coding with actions taken at the encoder and cost constraint\nC is given by\nR = min[ max {H(X|Yj , A)} \u2212 H(A|X)]+ ,\nj\u2208[1:K]\n\nwhere minimization is over the joint distribution p(x)p(a|x)p(y1 , y2 , . . . , yK |x, a) such that E \u039b(A) \u2264 C.\nProof:\nConverse The proof of converse is a straightforward extension from the single decoder case given in [2]. We give\nthe proof here for completeness. Consider the rate required for decoder j.\nnR \u2265 H(M )\n\u2265 H(M, X n |Yjn ) \u2212 H(X n |M, Yjn )\n\u2265 H(M, X n |Yjn ) \u2212 n\u01ebn\n(a)\n\n= H(X n |Yjn ) \u2212 n\u01ebn\n\n(b)\n\n= H(X n ) + H(Yjn |X n , An ) \u2212 H(Yjn ) \u2212 n\u01ebn\nn\nX\n(H(Xi ) + H(Yji |Xi , Ai ) \u2212 H(Yj,i )) \u2212 n\u01ebn ,\n\u2265\ni=1\n\nwhere (a) follows from the fact that M is a function of X n and (b) follows from An being a function of X n .\nThe last step follows from X n being a discrete memoryless source; the action channel being memoryless and\n\n16\n\n\fconditioning reduces entropy. As before, we define Q to be an uniform random variable over [1 : n] independent\nof all other random variables to obtain\nR \u2265 H(X) + H(Yj |X, A) \u2212 H(Yj ) \u2212 \u01ebn\n= H(X) + H(Yj , X|A) \u2212 H(X|A) \u2212 H(Yj ) \u2212 \u01ebn\n= H(X|A, Yj ) + I(X; A) \u2212 I(Yj ; A) \u2212 \u01ebn\n= H(X|A, Yj ) \u2212 H(A|X) \u2212 \u01ebn .\nThe last step follows from the fact that A = f j(Yj ). Combining the lower bounds over K decoders then give us\nthe achievable rate stated in the Theorem.\nAchievability We give a sketch of achievability since the techniques used are relatively straightforward. Assume\nfirst that R > 0. We first bin the set of X n sequences to 2n(maxj\u2208[1:K] H(Yj |X,A)+\u01eb) , B(MX ), MX \u2208 [1 :\n2n(maxj\u2208[1:K]H(Yj |X,A)+\u01eb) ]. Given an xn sequence, we first find the bin index mx such that xn \u2208 B(mx ). We then\nsplit mx into two sub-messages: mxr \u2208 [1 : 2maxj\u2208[1:K] {H(X|Yj ,A)}\u2212H(A|X)+2\u01eb ] and mxa \u2208 [1 : 2n(H(A|X)\u2212\u01eb) ].\nmxr is transmitted over the noiseless link, giving us the rate stated in the Theorem. As for mxa , we will send\nthe message through the action channel by treating the action channel as a channel with i.i.d. state X noncausally\nknown at the transmitter (A). We can therefore use Gel'fand Pinsker coding [13] for this channel.\nEach decoder first decodes mxa from their side information Yj . From the condition that A = fj (Yj ) for all\nj, we have H(A|X) \u2212 \u01eb = I(Yj ; A) \u2212 I(X; A) \u2212 \u01eb. From analysis of Gel'fand-Pinsker coding, since |Mxa | =\nI(Yj ; A) \u2212 I(X; A) \u2212 \u01eb, the probability of error in decoding mxa goes to zero as n \u2192 \u221e. The decoder then\nreconstructs mx from mxr and mxa . It then finds the unique x\u0302n \u2208 B(mx ) that is jointly typical with Yjn and\nAn . Note that due to Gel'fand-Pinsker coding, the true xn sequence is jointly typical with Yjn and An with\nhigh probability. Therefore, the probability of error in this decoding step goes to zero as n \u2192 \u221e since we have\n2n(maxj\u2208[1:K] H(Yj |X,A)+\u01eb) bins.\nFor the case where R = 0, we send the entire message through the action channel.\nExample 3: Consider the case of K = 2 with switching dependent side information: A = {1, 2} and (X, Y ) \u223c\np(x, y) with PY1 ,Y2 |X,A specified by Y1 = Y, Y2 = e when A = 1 and Y1 = e, Y2 = Y when A = 2. Note\nthat A is a function of Y1 , and also of Y2 . It therefore satisfies the condition in Theorem 4. Let P(A = 1) = \u03b1,\n\u039b(A = 1) = C1 and \u039b(A = 2) = C2 . The rate-cost tradeoff is characterized by\nR = max{\u03b1H(X|A = 1, Y ) + (1 \u2212 \u03b1)H(X|A = 1), (1 \u2212 \u03b1)H(X|A = 2, Y ) + \u03b1H(X|A = 2)}\n+ H(X) \u2212 H2 (\u03b1) \u2212 \u03b1H(X|A = 1) \u2212 (1 \u2212 \u03b1)H(X|A = 2)\nfor some p(a|x) satisfying \u03b1C1 + (1 \u2212 \u03b1)C2 \u2264 C.\nVI. OTHER SETTINGS\nIn this section, we consider other settings involving multi-terminal source coding with action dependent side\ninformation. The first setting that we consider in this section generalizes [2, Theorem 7] to the case where there\nis a rate-limited link from the source encoder to the action encoder. The second setting we consider is a case of\nsuccessive refinement with actions.\nA. Single decoder with Markov Form X-A-Y and rate limited link to action encoder\nIn this subsection, we consider the setting illustrated in Figure 8. Here, we have a single decoder with actions\ntaken at an action encoder. The source encoder have access to source X n and sends out two indices M \u2208 [1 : 2nR ]\nand MA \u2208 [1 : 2nRA ]. The action encoder is a function f : MA \u2192 An . In addition, we have the Markov relation\nX \u2212 A \u2212 Y . That is, the side information Y is dictated only by the action A taken. The other definitions remain\nthe same and we omit them here.\nProposition 5: R(D, C) for the setting shown in figure 8 is given by\nR(D, C) = min max{I(X; X\u0302) \u2212 RA , I(X; X\u0302) \u2212 I(A; Y )},\nwhere the minimization is over p(x)p(a)p(y|a)p(x\u0302|x) satisfying the cost and distortion constraints E d(X, X\u0302) \u2264 D\nand E \u039b(A) \u2264 C.\n17\n\n\fXn\nPSfrag replacements\n\nR\nSource Encoder\n\nDecoder\n\nX\u0302 n\n\nYn\n\nRA\nAn\n\nAction Encoder\n\nPY |A\n\nFig. 8: Lossy source coding with rate limited link to action encoder\nRemark 6.1: If we set RA = \u221e in Proposition 5, then we recover the result in [2, Theorem 7]. Essentially, the\nsource encoder tries to send as much information as possible through the rate limited action link until the link\nsaturates.\nProof:\nAchievability: The achievability is straightforward. Using standard rate distortion coding, we cover X n with\n2n(I(X;X\u0302)+\u01eb) X\u0302 n codewords. Given a source sequence xn , we find an X\u0302 n that is jointly typical with xn . We then\nsplit the index MX corresponding to the chosen X\u0302 n codeword into two parts: MA \u2208 [1 : 2n(min{RA ,I(A;Y )}+\u01eb) ]\nand M \u2208 [1 : 2nR ]. The action encoder takes the index and transmit it through the action channel. Since the rate\nof MA is less than I(A; Y ) \u2212 \u01eb, the decoder can decode MA with high probability of success. It then combines\nMA with M to obtain the index of the reconstruction codeword X\u0302 n .\nConverse Given a code that satisfy the distortion and cost constraints, we have\nnR \u2265 H(M )\n= I(X n ; M )\n\u2265 I(X n ; M ) \u2212 I(X n ; Y n )\n= I(X n ; X\u0302 n ) \u2212 I(X n , MA ; Y n )\nn\n(a) X\nI(Xi ; X\u0302i ) \u2212 I(X n , MA , An ; Y n )\n\u2265\ni=1\n\n(b)\n\n\u2265\n\nn\nX\n\nI(Xi ; X\u0302i ) \u2212 I(MA , An ; Y n ).\n\ni=1\n\nn\n\n(a) follows from the fact that A is a functionP\nof MA . (b) follows from the Markov chain X \u2212 A \u2212 Y . Now, it\nn\nis easy to see that I(MA , An ; Yi ) \u2264 min{nRA , i=1 I(Ai ; Yi )}. The bound on the rate is then single letterized in\nthe usual manner, giving us\nR(D, C) = min max{I(X; X\u0302) \u2212 RA , I(X; X\u0302) \u2212 I(A; Y )},\nfor some p(a, x\u0302|x) satisfying the distortion and cost constraints. Finally, we note that p(a, x\u0302|x) can be restricted to\nthe form p(a)p(x\u0302|x). To see this, note that none of the terms depend on the joint p(a, x\u0302|x). Furthermore, due to the\nMarkov conditon X \u2212 A \u2212 Y , it suffices to consider A independent of X, giving us the p.m.f in the Proposition.\nB. Successive refinement with actions\nThe next setup that we consider is a case of successive refinement [14], [15] with actions taken at the \"more\ncapable\" decoder. The setting is shown in Figure 9.\n\n18\n\n\fXn\n\nR1\nSource Encoder\n\nX\u03021n , D1\n\nDecoder 1\n\nPSfrag replacements\n\nAction Encoder\n\nX\u03022n , D2\n\nDecoder 2\n\nR2\n\nYn\n\nAn\n\nPY |X,A\n\nFig. 9: Successive refinement with actions\nProposition 6: Successive refinement with actions taken at the more capable decoder For the setting shown in\nfigure 9, the rate distortion cost tradeoff region is given by\nR1 \u2265 I(X; X\u03021 ),\nR1 + R2 \u2265 I(X; X\u03021 , A) + I(X; U |X\u03021 , Y, A)\nfor some p(x\u03021 , a, u|x) satisfying\nE d1 (X, X\u03021 ) \u2264 D1 ,\nE d1 (X, X\u03022 (U, Y, A)) \u2264 D2 ,\nE \u039b(A) \u2264 C.\nThe cardinality of the auxiliary U may be upper bounded by |U| \u2264 |X ||X\u02c61 ||A| + 1.\nIf we restrict R2 = 0, then Proposition 6 gives the rate-distortion-cost tradeoff region for a special case of\nProposition 1. That is, the case when Y2 = \u2205 and actions are taken only at decoder 1.\nProof:\nAchievability: We give the case where R1 = I(X; X\u03021 ) + \u01eb and R2 = I(X; A|X\u03021 ) + I(X; U |X\u03021 , Y, A) + 3\u01eb. The\ngeneral region stated in the Proposition can then be obtained by rate splitting of R2 .\nCodebook generation\nQn\n\u2022 Generate 2nR1 X\u03021n (m1 ) sequences according to i=1 p(x\u03021i ), m1 \u2208 [1 : 2nR ].\nQn\n\u2022 For each X\u03021n (m1 ) sequence, generate 2n(I(X;A|X\u03021 )+\u01eb) An (m1 , m21 ), sequences according to i=1 p(ai |x\u03021i ).\n\u2022 For each X\u03021n Q\n(m1 ) and An (m1 , m2 ) sequence pair, generate 2n(I(X;U|X\u03021 ,A)+\u01eb) U n (m1 , m21 , l22 ), sequences\nn\naccording to i=1 p(ui |x\u03021i , ai ).\n\u2022 Partition the set of l22 indices into 2I(X;U|X\u03021 ,Y,A)+2\u01eb bins, B(m1 , m21 , m22 ), m22 \u2208 [1 : 2n(I(X;U|X\u03021 ,Y,A)+2\u01eb) ].\nEncoding\n(n)\n\n\u2022 Given a sequence xn , the encoder first looks for an x\u0302n1 (m1 ) sequence such that (xn , x\u0302n1 ) \u2208 T\u01eb\nsucceeds with high probability since R1 = I(X; X\u03021 ) + \u01eb.\n\n19\n\n. This step\n\n\f\u2022 Next, the encoder looks for an An (m1 , m21 ) sequence such that (xn , an , x\u0302n1 ) \u2208 aep. This step succeeds with\nhigh probability since we have 2n(I(X;A|X\u03021 )+\u01eb) An sequences.\n\u2022 The encoder then looks for an U n (m1 , m21 , l22 ) sequence such that (xn , an , x\u0302n1 , un ) \u2208 aep. This step succeeds\nwith high probability since we have 2n(I(X;U|X\u03021 ,A)+\u01eb) U n sequences.\n\u2022 It then finds the bin index such that l22 \u2208 B(m1 , m21 , m22 ).\n\u2022 The encoder sends out the indices m1 over the link R1 and m21 and m22 over the link R2 , giving us the\nstated rates.\nDecoding and reconstruction\n\u2022 Since decoder 1 has index m1 , it reconstructs xn using x\u03021 (m1 )n . Since (xn , x\u0302n1 ) are jointly typical with high\nprobability, the expected distortion satisfies the D1 distortion constraint to within \u01eb.\n\u2022 For decoder 2, from m1 and m21 , it recovers the action sequence an (m1 , m21 ). It then takes the action\nan (m1 , m21 ) to obtain it's side information Y n . With the side information, it recovers the un sequence by\n(n)\nlooking for the unique \u02c6\nl22 \u2208 B(m1 , m21 , m22 ) such that (un (m1 , m21 , l\u030222 ), x\u0302n1 , an , Y n ) \u2208 T\u01eb . Since there\n(n)\nare only 2n(I(U;Y |X\u03021 ,A)\u2212\u01eb) U n sequences in the bin and (un (m1 , m21 , l22 ), x\u0302n1 , an , Y n ) \u2208 T\u01eb\nwith high\nprobability from the fact that Y is generated i.i.d. according to p(y|ai , xi ), the probability of error goes to\nzero as n \u2192 \u221e. Decoder 2 then reconstructs xn using x\u03022i (ai , ui , yi ) for i \u2208 [1 : n].\nConverse: We consider only the lower bound for R1 + R2 . The lower bound for R1 is straightforward. Given a\ncode which satisfies the distortion and cost constraints, we have\nn(R1 + R2 ) \u2265 H(M1 , M2 )\n= H(M1 , M2 , An , X\u03021n )\n= H(An , X\u03021n ) + H(M1 , M2 |An , X\u03021n )\n\u2265 I(X n ; An , X\u03021n ) + H(M1 , M2 |An , X\u03021n , Y n ) \u2212 H(M1 , M2 |Y n , An , X\u03021n , X n )\n= I(X n ; An , X\u03021n ) + I(X n ; M1 , M2 |An , X\u03021n , Y n )\n= I(X n ; An , X\u03021n ) + H(X n |An , X\u03021n , Y n ) \u2212 H(X n |An , X\u03021n , Y n , M1 , M2 )\n= I(X n ; An , X\u03021n ) + H(X n , Y n |An , X\u03021n ) \u2212 H(Y n |X\u03021n , An ) \u2212 H(X n |An , X\u03021n , Y n , M1 , M2 )\n= H(X n ) \u2212 H(X n |An , X\u03021n ) + H(X n , Y n |An , X\u03021n ) \u2212 H(Y n |X\u03021n , An ) \u2212 H(X n |An , X\u03021n , Y n , M1 , M2 )\n= H(X n ) + H(Y n |X n , An , X\u03021n ) \u2212 H(Y n |X\u03021n , An ) \u2212 H(X n |An , X\u03021n , Y n , M1 , M2 )\nn\nX\n(H(Xi ) + H(Yi |X n , An , X\u03021n , Y i\u22121 ) \u2212 H(Yi |X\u03021n , An , Y i\u22121 ) \u2212 H(Xi |An , X\u03021n , Y n , M1 , M2 ))\n\u2265\n(a)\n\n\u2265\n\ni=1\nn\nX\n\n(H(Xi ) + H(Yi |Xi , Ai , X\u03021i ) \u2212 H(Yi |X\u03021i , Ai ) \u2212 H(Xi |An , X\u03021n , Y n , M1 , M2 ))\n\ni=1\n\n=\n\nn\nX\n\n(H(Xi ) + H(Yi |Xi , Ai , X\u03021i ) \u2212 H(Yi |X\u03021i , Ai ) \u2212 H(Xi |An , X\u03021n , Y n , M1 , M2 ))\n\ni=1\n\n\u2265\n\nn\nX\n\n(H(Xi ) + H(Yi |Xi , Ai , X\u03021i ) \u2212 H(Yi |X\u03021i , Ai ) \u2212 H(Xi |Ui , Ai , Yi , X\u03021i ))\n\ni=1\n\nn\\i\n\n(a) follows from the Markov Chain (X n\\i , An\\i , X\u03021 , Y i\u22121 ) \u2212 (X\u0302i , Xi , Ai ) \u2212 Yi and the last step follows from\ndefining Ui = (M1 , M2 , Y n\\i , An\\i ). The proof is then completed in the usual manner by defining the time sharing\nuniform random variable Q and U = (UQ , Q), giving us\nR1 + R2 \u2265 H(X) + H(Y |X, A, X\u03021 ) \u2212 H(Y |X\u03021 , A) \u2212 H(X|U, A, Y, X\u03021 )\n= I(X; X\u03021 , A) + I(X; U |X\u03021 , Y, A).\n\n20\n\n\fThe fact that X\u03022 is a function of U , Y and A, which is straightforward. Finally, the cardinality bound on U may be\nobtained from standard techniques. Note that we need |X\u02c61 ||X ||A| \u2212 1 letters to preserve p(u, a, x) and two more\nto preserve the rate and distortion constraints.\nRemark 6.2: An interesting question to explore characterizing the more general case when degraded side information\nis also available at decoder 1. That is, we have the side informations Y1 at decoder 1 and Y2 at decoder 2 are\ngenerated by a discrete memoryless channel PY1 ,Y2 |X,A such that (X, A)\u2212 (Y2 , A)\u2212 (Y1 , A). This generalized setup\nwould allow us to generalize Proposition 1 entirely and also leads to a generalization of successive refinement for\nthe Wyner-Ziv problem in [16] to the action setting.\nVII. C ONCLUSION\nIn this paper, we considered an important class of multi-terminal source coding problems, where the encoder\nsends the description of the source to the decoders, which then take cost-constrained actions that affect the quality\nor availability of side information. We computed the optimum rate region for lossless compression, while for the\nlossy case we provide a general achievability scheme that is shown to be optimal for a number of special cases,\none of them being the generalization of Heegard-Berger-Kaspi setting. (cf. [10], [11]). In all these cases in addition\nto a standard achievability argument, we also provided a simple scheme which has a modulo sum interpretation.\nThe problem where the encoder takes actions rather than the decoders, was also considered. Finally, we extended\nthe scope to additional multi-terminal source coding problems such as successive refinement with actions.\nACKNOWLEDGMENT\nWe thank Professor Haim Permuter and Professor Yossef Steinberg for helpful discussions. The authors' research\nwas partially supported by NSF grant CCF-0729195 and the Center for Science of Information (CSoI), an NSF\nScience and Technology Center, under grant agreement CCF-0939370; the Scott A. and Geraldine D. Macomber\nStanford Graduate Fellowship and the Office of Technology Licensing Stanford Graduate Fellowship.\nR EFERENCES\n[1] A. D. Wyner and J. Ziv, \"The rate-distortion function for source coding with side information at the decoder,\" IEEE Trans. Inf. Theory,\nvol. 22, no. 1, pp. 1\u201310, 1976.\n[2] H. Permuter and T. Weissman, \"Source coding with a side information \"vending machine\",\" IEEE Trans. Inf. Theory, vol. 57, no. 7, pp.\n4530\u20134544, July 2011.\n[3] H. Asnani, H. H. Permuter, and T. Weissman, \"Probing capacity,\" CoRR, vol. abs/1010.1309, 2010.\n[4] K. Kittichokechai, T. J. Oechtering, M. Skoglund, and R. Thobaben, \"Source and channel coding with action-dependent partially known\ntwo-sided state information,\" in Proc. IEEE International Symposium on Information Theory, 2010, pp. 629\u2013633\u2013194.\n[5] A. El Gamal and Y. H. Kim, \"Lectures on network information theory,\" 2010, available online at ArXiv.\n[6] T. M. cover, \"A proof of the data compression theorem of slepian and wolf for ergodic sources,\" IEEE Trans. Inf. Theory, vol. 21, no. 2,\npp. 226\u2013228, 1975.\n[7] A. Kimura and T. Uyematsu, \"Multiterminal source coding with complementary delivery,\" in Proc. International Symposium on Information\nTheory and its Applications (ISITA), 2006, pp. 189\u2013194.\n[8] T. Weissman and A. El Gamal, \"Source coding with limited-look-ahead side information at the decoder,\" IEEE Trans. Inf. Theory, vol. 52,\nno. 12, pp. 5218\u20135239, 2006.\n[9] H. G. Eggleston, Convexity. Cambridge: Cambridge University Press, 1958.\n[10] C. Heegard and T. Berger, \"Rate distortion when side information may be absent,\" IEEE Trans. Inf. Theory, vol. 31, no. 6, pp. 727\u2013734,\n1985.\n[11] A. H. Kaspi, \"Rate-distortion function when side-information may be present at the decoder,,\" IEEE Trans. Inf. Theory, vol. 40, no. 6, pp.\n2031\u20132034, 1994.\n[12] R. Timo, A. Grant, and G. Kramer, \"Rate-distortion functions for source coding with complementary side information,\" in Proc. IEEE\nInternational Symposium on Information Theory, St. Petersburg, Russia, 2011, pp. 2934\u20132938.\n[13] S. I. Gel'fand and M. S. Pinsker, \"Coding for channels with random parameters,\" Probl. Control and Information Theory, vol. 9, pp.\n19\u201331, 1980.\n[14] W. H. R. Equitz and T. M. Cover, \"Successive refinement of information,\" IEEE Trans. Inf. Theory, vol. 37, no. 2, pp. 269\u2013275, 1991.\n[15] --, \"Successive refinement of information: Characterization of the achievable rates,,\" IEEE Trans. Inf. Theory, vol. 40, no. 1, pp.\n253\u2013259, 1994.\n[16] Y. Steinberg and N. Merhav, \"On successive refinement for the wyner-ziv problem,\" IEEE Trans. Inf. Theory, vol. 50, no. 8, pp. 1636\u20131654,\n2004.\n\n21\n\n\fA PPENDIX A\nACHIEVABILITY S KETCH FOR T HEOREM 3\nCodebook generation\nQn\n\u2022 Generate 2n(I(X;A)+\u01eb) An (la ), la \u2208 [1 : 2n(I(X;A)+\u01eb) ], sequences according to i=1 p(ai ).\n\u2022 Q\nFor each An sequence, generate 2n(I(U;X|A)+\u01eb) U n (la , l0 ), l0 \u2208 [1 : 2n(I(U;X|A)+\u01eb) ], sequences according to\nn\ni=1 pU|A (ui |ai ).\n\u2022 Partition the set of indices corresponding to the U n codewords uniformly to 2n(max{I(X;U|A,Y1 ),I(X;U|A,Y2 )}+2\u01eb)\nbins, BU (la , m0 ), m0 \u2208 [1 : 2n(max{I(X;U|A,Y1 ),I(X;U|A,Y2 )}+2\u01eb) ].\n\u2022 For each pair of An andQU n sequences, generate 2n(I(V1 ;X|A,U)+\u01eb) V1n (la , l0 , l1 ), l1 \u2208 [1 : 2n(I(V1 ;X|A,U)+\u01eb) ],\nsequences according to ni=1 pV1 |A,U (v1i |ai , ui ).\n\u2022 Partition the set of indices corresponding to the V1n codewords uniformly to 2n(I(X;V1 |U,A,Y1 )+2\u01eb) bins,\nBV1 (la , l0 , m1 ), m1 \u2208 [1 : 2n(I(X;V1 |U,A,Y1 )+2\u01eb) ].\n\u2022 For each pair of An andQU n sequences, generate 2n(I(V2 ;X|A,U)+\u01eb) V2n (la , l0 , l2 ), l1 \u2208 [1 : 2n(I(V2 ;X|A,U)+\u01eb) ],\nsequences according to ni=1 pV2 |A,U (v2i |ai , ui ).\n\u2022 Partition the set of indices corresponding to the V2n codewords uniformly to 2n(I(X;V2 |U,A,Y2 )+2\u01eb) bins,\nBV2 (la , l0 , m2 ), m2 \u2208 [1 : 2n(I(X;V2 |U,A,Y2 )+2\u01eb) ].\nEncoding\n(n)\n\n\u2022 Given an xn sequence, the encoder first looks for an an (la ) sequence such that (xn , an ) \u2208 T\u01eb . If there is\nnone, it outputs and index chosen uniformly at random from the set of possible la indices. If there is more\nthan one, it outputs an index chosen uniformly at random from the set of feasible indices. Since there are\n2n(I(X;A)+\u01eb) such sequences, the probability of error \u2192 0 as n \u2192 \u221e.\n\u2022 The encoder then looks for a un (la , l0 ) sequence that is jointly typical with (an (la ), xn ). If there is none, it\noutputs and index chosen uniformly at random from the set of possible l0 indices. If there is more than one, it\noutputs an index chosen uniformly at random from the set of feasible indices. Since there are 2n(I(U;X|A)+\u01eb)\nsuch sequences, the probability of error \u2192 0 as n \u2192 \u221e.\n\u2022 Next, the encoder looks for a v1n (la , l0 , l1 ) sequence that is jointly typical with (an (la ), un (l0 ), xn ). If there\nis none, it outputs and index chosen uniformly at random from the set of possible l0 indices. If there is more\nthan one, it outputs an index chosen uniformly at random from the set of feasible indices. Since there are\n2n(I(V1 ;X|A,U)+\u01eb) such sequences, the probability of error \u2192 0 as n \u2192 \u221e.\n\u2022 Next, the encoder looks for a v2n (la , l0 , l2 ) sequence that is jointly typical with (an (la ), un (l0 ), xn ). If there\nis none, it outputs and index chosen uniformly at random from the set of possible l0 indices. If there is more\nthan one, it outputs an index chosen uniformly at random from the set of feasible indices. Since there are\n2n(I(V2 ;X|A,U)+\u01eb) such sequences, the probability of error \u2192 0 as n \u2192 \u221e.\n\u2022 The encoder then sends out the indices la , m0 , m1 and m2 such that l0 \u2208 BU (la , m0 ), l1 \u2208 BV1 (la , l0 , m1 )\nand l2 \u2208 BV2 (la , l0 , m2 ).\nDecoding and reconstruction\nDecoder 1:\n\u2022 Decoder 1 first takes the action sequence an (la ) to obtain the side information Y1n . We note that if\n(n)\n(n)\nn\n(an (la ), xn , un (la , l0 ), v1n (la , l0 , l1 )) \u2208 T\u01eb , then P{(an (la ), xn , un (la , l0 ), v1n (la , l0 , lQ\n}\u21921\n1 ), Y1 ) \u2208 T\u01eb\nn\nn\nas n \u2192 \u221e by the conditional typicality lemma [5, Chapter 2] and the fact that Y1 \u223c i=1 p(y1i |xi , ai ).\n\u2022 Decoder 1 then decodes U n . it does this by finding the unique \u02c6l0 such that un (la , l\u03020 ) \u2208 BU (la , m0 ). If there\nis none or more than one such l\u03020 , an error is declared. Following standard analysis for the Wyner-Ziv setup\n(see for e.g. [5, Chapter 12]), the probability of error goes to zero as n \u2192 \u221e since there are less than or equal\nto 2n(I(U;Y1 |A)\u2212\u01eb) U n sequences within each bin.\n\u2022 Similarly, decoder 1 decodes V1n . It does this by finding the unique \u02c6l1 such that v1n (la , l\u03020 , l\u03021 ) \u2208 BV1 (la , l\u03020 , m1 ).\nIf there is none or more than one such l\u03021 , an error is declared. As with the previous step, the probability of\nerror goes to zero as n \u2192 \u221e since there are only 2n(I(V1 ;Y1 |A,U)\u2212\u01eb) V1n sequences within each bin.\n\u2022 Decoder 1 then reconstructs xn as x\u03021i (ai (la ), ui (la , l\u03020 ), v1i (la , l\u03020 , l\u03021 ), y1i ) for i \u2208 [1 : n].\n22\n\n\fDecoder 2: As the decoding steps for decoder 2 are similar to that for 1, we will only mention the differences\nhere. That is, decoder 2 uses side information Y2n instead of Y1n to perform the decoding operations and instead\nof decoding V1n , decoder 2 decodes V2n .\n\u2022 Decoder 2 decodes V2n . It does this by finding the unique l\u03022 such that v2n (la , l\u03020 , l\u03022 ) \u2208 BV2 (la , l\u03020 , m2 ). If there\nis none or more than one such l\u03022 , an error is declared. As with the previous step, the probability of error goes\nto zero as n \u2192 \u221e since there are only 2n(I(V2 ;Y2 |A,U)\u2212\u01eb) V2n sequences within each bin.\n\u2022 Decoder 1 then reconstructs xn as x\u03022i (ai (la ), ui (la , l\u03020 ), v2i (la , l\u03020 , l\u03022 ), y2i ) for i \u2208 [1 : n].\nDistortion and cost constraints\n\u2022 For the cost constraint, since the chosen An sequence is typical with high probability, E \u039b(An ) \u2264 C + \u01eb by\nthe typical average lemma [5, Chapter 2].\n\u2022 For the distortion constraints, since the probability of \"error\" goes to zero as n \u2192 \u221e and we are dealing only\nwith finite cardinality random variables, following the analysis in [5, Chapter 3], we have\n1\nE d1 (X n , X\u03021n ) \u2264 D1 + \u01eb,\nn\n1\nE d2 (X n , X\u03022n ) \u2264 D2 + \u01eb.\nn\n\n23\n\n\f"}