{"id": "http://arxiv.org/abs/1201.0472v4", "guidislink": true, "updated": "2013-01-21T00:11:58Z", "updated_parsed": [2013, 1, 21, 0, 11, 58, 0, 21, 0], "published": "2012-01-02T14:28:37Z", "published_parsed": [2012, 1, 2, 14, 28, 37, 0, 2, 0], "title": "Holonomic gradient method for the distribution function of the largest\n  root of a Wishart matrix", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.6291%2C1203.5458%2C1203.6445%2C1203.4408%2C1203.5431%2C1203.4515%2C1203.5847%2C1203.6143%2C1203.4171%2C1203.0639%2C1203.6485%2C1203.6862%2C1203.2471%2C1203.1906%2C1203.2029%2C1203.0620%2C1203.2070%2C1203.6159%2C1203.0377%2C1203.2499%2C1203.0355%2C1203.6051%2C1203.0008%2C1203.0127%2C1203.0086%2C1203.2764%2C1203.4428%2C1203.0199%2C1203.6390%2C1203.1865%2C1203.4941%2C1203.3802%2C1203.6671%2C1203.6611%2C1203.3279%2C1203.6392%2C1203.1970%2C1203.1156%2C1203.2200%2C1203.3871%2C1203.1063%2C1203.3786%2C1203.3503%2C1203.2953%2C1203.2820%2C1203.6462%2C1203.4650%2C1203.0322%2C1203.5821%2C1203.1962%2C1203.6633%2C1203.1583%2C1203.0585%2C1203.1849%2C1203.2806%2C1203.0737%2C1203.6396%2C1203.1675%2C1203.5877%2C1203.5074%2C1203.3839%2C1201.3841%2C1201.4238%2C1201.3966%2C1201.2661%2C1201.6546%2C1201.5594%2C1201.3615%2C1201.0472%2C1201.5362%2C1201.5793%2C1201.0390%2C1201.1990%2C1201.0689%2C1201.6269%2C1201.5167%2C1201.1801%2C1201.5315%2C1201.4539%2C1201.1721%2C1201.3070%2C1201.4120%2C1201.5719%2C1201.1865%2C1201.6568%2C1201.3403%2C1201.0968%2C1201.5104%2C1201.6018%2C1201.0535%2C1201.5233%2C1201.0670%2C1201.5577%2C1201.4759%2C1201.3339%2C1201.1532%2C1201.4825%2C1201.1437%2C1201.5446%2C1201.4768%2C1201.3057&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Holonomic gradient method for the distribution function of the largest\n  root of a Wishart matrix"}, "summary": "We apply the holonomic gradient method introduced by Nakayama et al.(2011) to\nthe evaluation of the exact distribution function of the largest root of a\nWishart matrix, which involves a hypergeometric function 1F1 of a matrix\nargument. Numerical evaluation of the hypergeometric function has been one of\nthe longstanding problems in multivariate distribution theory. The holonomic\ngradient method offers a totally new approach, which is complementary to the\ninfinite series expansion around the origin in terms of zonal polynomials. It\nallows us to move away from the origin by the use of partial differential\nequations satisfied by the hypergeometric function. From numerical viewpoint we\nshow that the method works well up to dimension 10. From theoretical viewpoint\nthe method offers many challenging problems both to statistics and D-module\ntheory.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.6291%2C1203.5458%2C1203.6445%2C1203.4408%2C1203.5431%2C1203.4515%2C1203.5847%2C1203.6143%2C1203.4171%2C1203.0639%2C1203.6485%2C1203.6862%2C1203.2471%2C1203.1906%2C1203.2029%2C1203.0620%2C1203.2070%2C1203.6159%2C1203.0377%2C1203.2499%2C1203.0355%2C1203.6051%2C1203.0008%2C1203.0127%2C1203.0086%2C1203.2764%2C1203.4428%2C1203.0199%2C1203.6390%2C1203.1865%2C1203.4941%2C1203.3802%2C1203.6671%2C1203.6611%2C1203.3279%2C1203.6392%2C1203.1970%2C1203.1156%2C1203.2200%2C1203.3871%2C1203.1063%2C1203.3786%2C1203.3503%2C1203.2953%2C1203.2820%2C1203.6462%2C1203.4650%2C1203.0322%2C1203.5821%2C1203.1962%2C1203.6633%2C1203.1583%2C1203.0585%2C1203.1849%2C1203.2806%2C1203.0737%2C1203.6396%2C1203.1675%2C1203.5877%2C1203.5074%2C1203.3839%2C1201.3841%2C1201.4238%2C1201.3966%2C1201.2661%2C1201.6546%2C1201.5594%2C1201.3615%2C1201.0472%2C1201.5362%2C1201.5793%2C1201.0390%2C1201.1990%2C1201.0689%2C1201.6269%2C1201.5167%2C1201.1801%2C1201.5315%2C1201.4539%2C1201.1721%2C1201.3070%2C1201.4120%2C1201.5719%2C1201.1865%2C1201.6568%2C1201.3403%2C1201.0968%2C1201.5104%2C1201.6018%2C1201.0535%2C1201.5233%2C1201.0670%2C1201.5577%2C1201.4759%2C1201.3339%2C1201.1532%2C1201.4825%2C1201.1437%2C1201.5446%2C1201.4768%2C1201.3057&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We apply the holonomic gradient method introduced by Nakayama et al.(2011) to\nthe evaluation of the exact distribution function of the largest root of a\nWishart matrix, which involves a hypergeometric function 1F1 of a matrix\nargument. Numerical evaluation of the hypergeometric function has been one of\nthe longstanding problems in multivariate distribution theory. The holonomic\ngradient method offers a totally new approach, which is complementary to the\ninfinite series expansion around the origin in terms of zonal polynomials. It\nallows us to move away from the origin by the use of partial differential\nequations satisfied by the hypergeometric function. From numerical viewpoint we\nshow that the method works well up to dimension 10. From theoretical viewpoint\nthe method offers many challenging problems both to statistics and D-module\ntheory."}, "authors": ["Hiroki Hashiguchi", "Yasuhide Numata", "Nobuki Takayama", "Akimichi Takemura"], "author_detail": {"name": "Akimichi Takemura"}, "author": "Akimichi Takemura", "links": [{"href": "http://arxiv.org/abs/1201.0472v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1201.0472v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62H10, 13N10", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1201.0472v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1201.0472v4", "arxiv_comment": null, "journal_reference": "Journal of Multivariate Analysis 117 (2031) 296-312", "doi": null, "fulltext": "arXiv:1201.0472v4 [math.ST] 21 Jan 2013\n\nHolonomic gradient method for the distribution\nfunction of the largest root of a Wishart matrix\nHiroki Hashiguchi\u2217, Yasuhide Numata\u2020\u2021, Nobuki Takayama\u00a7\u2021\nand Akimichi Takemura\u2020\u2021\nJanuary 2012\n\nAbstract\nWe apply the holonomic gradient method introduced by Nakayama et al. [23]\nto the evaluation of the exact distribution function of the largest root of a Wishart\nmatrix, which involves a hypergeometric function 1F1 of a matrix argument. Numerical evaluation of the hypergeometric function has been one of the longstanding\nproblems in multivariate distribution theory. The holonomic gradient method offers\na totally new approach, which is complementary to the infinite series expansion\naround the origin in terms of zonal polynomials. It allows us to move away from\nthe origin by the use of partial differential equations satisfied by the hypergeometric function. From numerical viewpoint we show that the method works well up\nto dimension 10. From theoretical viewpoint the method offers many challenging\nproblems both to statistics and D-module theory.\n\nKeywords and phrases: D-modules, Gr\u00f6bner basis, hypergeometric function of a matrix\nargument, zonal polynomial\n\n1\n\nIntroduction\n\nFor multivariate distribution theory in statistics, the theory of zonal polynomials and\nhypergeometric functions of matrix arguments, introduced by A.T. James and other authors, was a very important development in the 1950's. They allowed explicit expressions\nof density functions and cumulative distribution functions of basic test statistics under\nnon-null cases. Zonal polynomials are based on the representation theory of real general\nlinear group and they possess many interesting combinatorial properties. Properties and\n\u2217\n\nGraduate School of Science and Engineering, Saitama University\nDepartment of Mathematical Informatics, Graduate School of Information Science and Technology,\nUniversity of Tokyo\n\u2021\nJST CREST\n\u00a7\nDepartment of Mathematics, Kobe University\n\u2020\n\n1\n\n\fapplications of zonal polynomials and hypergeometric functions of matrix arguments are\nsurveyed in Gross and Richards [5] and Richards [25]. Zonal polynomials are special cases\nof Jack polynomials, whose properties have been intensively studied by many mathematicians. See for example Chapters VI and VII of Macdonald [18] and Stanley [29]. Jack\npolynomials are further generalized to Macdonald polynomials (see, e.g., Kuznetsov and\nSahi [17]).\nZonal polynomials and hypergeometric functions of matrix arguments are important\nand difficult to compute in non-null cases rather than the null case, where the covariance\nmatrix is a multiple of the identity matrix. In the null case there are several approaches\nto obtain the distribution function or moments. Recent representative approach is to use\nthe random matrix theory (RMT) and the landmark study on the connection between\nRMT and multivariate analysis was conducted by Johnstone [12, 13]. Butler and Paige\n[2] proposed a method to compute the exact null distributions based on their Pfaffian\nrepresentation given by Gupta and Richards [6].\nDespite the above nice mathematical properties of zonal polynomials and hypergeometric functions of matrix arguments, from practical viewpoint they were not really\nuseful for computations. Coefficients of zonal polynomials can be computed only through\nnontrivial combinatorial recursions. Although very ingenious recursion algorithms have\nbeen recently developed (Koev and Edelman [14]), computing zonal polynomials of large\ndegrees remains to be a difficult problem because of inherent combinatorial complexities.\nAlso, the convergence of infinite series expansion of hypergeometric functions of a matrix\nargument in terms of zonal polynomials was found to be slow (Muirhead [21], Hashiguchi\nand Niki [7]). Since the expansion of the hypergeometric function in terms of zonal polynomials is the expansion at the origin, the convergence for large values of the argument\nis necessarily slow.\nThe holonomic gradient method allows us to move away from the origin by the use\nof partial differential equations. Thus our approach provides a promising new method\nfor attacking a longstanding problem in multivariate statistics. Our holonomic gradient\nmethod is, in spirit, on the track of the holonomic systems approach to combinatorial\nidentities by Zeilberger [36]. Note that the series expansion and our holonomic gradient method are in fact complementary methods, because our method needs the series\nexpansion for obtaining initial values for the partial differential equations.\nThe main purpose of this paper is to verify the performance of holonomic gradient\nmethod for 1F1 . We found that a straightforward implementation of the holonomic gradient method works well for dimensions up to 10.\nButler and Wood [3] showed that the Laplace method gives a very good approximation\nto 1F1 even for a high dimension, e.g., m = 32. However the Laplace method needs a\npeaked density function, which corresponds to a large degrees of freedom. Our method is\nan exact method, where the errors only come from discretization in numerically solving\ndifferential equations and the accuracies in the initial values. Hence our method works\neven for small degrees of freedom.\nThe organization of this paper is as follows. In Section 2 we summarize preliminary\nfacts on the exact distribution of the largest root of a Wishart matrix. In particular we\n2\n\n\fstate the partial differential equation for 1F1 by Muirhead [20]. In Section 3, for expository\npurposes, we fully describe our holonomic gradient method for dimension two. In Section\n4 we derive properties of Pfaffian system for general dimensions. The Pfaffian system is\na system of partial differential equations and is called an integrable connection in some\nliteratures. Results of symbolic computations are presented in Section 5 and results of\nnumerical experiments are presented in Section 6. We end the paper with discussion of\nopen problems in Section 7.\n\n2\n\nPreliminaries\n\nLet \u03ba = (k1 , . . . , kl ) \u22a2 k be a partition of a non-negative integer k and define the Pochhammer symbol (a)\u03ba by\n(a)\u03ba =\n\nl \u0012\nY\ni=1\n\ni\u22121\na\u2212\n2\n\n\u0013\n\n,\n\n(a)ki =\n\nki\n\nki\nY\n\n(a + j \u2212 1) ((a)0 = 1).\n\nj=1\n\nLet C\u03ba (Y ) denote the (\"C-normalization\" of) zonal polynomial indexed by \u03ba of an m \u00d7 m\nsymmetric matrix Y . It is a homogeneous symmetric\npolynomial of degree k in the charP\nacteristic roots y1 , . . . , ym of Y , satisfying \u03ba\u22a2k C\u03ba (Y ) = (tr Y )k . For zonal polynomials\nin statistics see, e.g., James [10], Muirhead [22], Takemura [34] and Mathai et al. [19]. A\nhypergeometric function of a matrix argument is defined (Constantine [4]) as\npFq (a1 , . . . , ap ; c1 , . . . , cq ; Y\n\n)=\n\n\u221e X\nX\n(a1 )\u03ba . . . (ap )\u03ba C\u03ba (Y )\nk=0 \u03ba\u22a2k\n\n(c1 )\u03ba . . . (cq )\u03ba\n\nk!\n\n.\n\n(1)\n\nIn this paper we study holonomic gradient method for 1F1 (a; c; Y ). Let Im denote the\nm \u00d7 m identity matrix and let |X| denote the determinant of X. For Ra > (m + 1)/2,\nR(b \u2212 a) > (m + 1)/2, 1F1 (a; c; Y ) has the following integral representation\nZ\n\u0393m (b)\nexp(tr XY )|X|a\u2212(m+1)/2 |Im \u2212 X|c\u2212a\u2212(m+1)/2 dX,\n1F1 (a; c; Y ) =\n\u0393m (a)\u0393m (c \u2212 a) 0<X<Im\n(2)\nQ\nwhere 0 < X < Im means that X and Im \u2212 X are positive definite, dX = i\u2264j dxij is the\nLebesgue measure of the upper triangular entries of X, and\n\u0013\n\u0012\nm\nY\n1\ni\u22121\nm(m\u22121)\n.\n\u0393m (a) = \u03c0 4\n\u0393 a\u2212\n2\ni=1\nThe hypergeometric function 1F1 satisfies the the following Kummer relation (see (2.8) of\nHerz [8], (51) of James [10]):\nexp(\u2212 tr Y )1F1 (a; c; Y ) = 1F1 (c \u2212 a, c; \u2212Y ).\nNote that (2) implies that 1F1 is an entire function in Y .\n3\n\n(3)\n\n\fThe cumulative distribution function of the largest root l1 of the m \u00d7 m Wishart\nmatrix W with n degrees of freedom and the covariance matrix \u03a3 is written as follows\n\u0013\n\u0012\n\u0010 x\n\u0011 1\nm + 1 n + m + 1 x \u22121\n\u22121\nnm\n2\nPr[l1 < x] = C exp \u2212 tr \u03a3\nx\n,\n(4)\n;\n; \u03a3\n1F1\n2\n2\n2\n2\nwhere\nC=\n\nm+1\n2\n1\n1\nnm\nn\n2\n2\n2\n(det \u03a3) \u0393m\n\n\u0393m\n\n\u0001\n\nn+m+1\n2\n\n\u0001.\n\nThis follows from the results in Section 9 of Constantine [4] and the Kummer relation (3).\nSee also Sugiyama [31].\nThe following partial differential equations for 1F1 (a; b; Y ) were derived by Muirhead\n[20].\nTheorem 1 (Theorem 5.1 of Muirhead [20], Theorem 7.5.6 of Muirhead [22]). The hypergeometric function F = 1F1 (a; c; Y ) of a matrix argument Y = diag(y1 , . . . , ym ) is the\nunique solution of the following set of m partial differential equations\n\"\n(\n)\n#\nm\nm\nX\nX\nm\n\u2212\n1\ny\ny\n1\n1\ni\nj\nyi \u2202i2 + c \u2212\n\u2202i \u2212\n\u2202j \u2212 a F = 0,\n(5)\n\u2212 yi +\n2\n2 j=1,j6=i yi \u2212 yj\n2 j=1,j6=i yi \u2212 yj\n\n(i = 1, . . . , m),\n\nsubject to the conditions that F is symmetric in y1 , . . . , ym and F is analytic at Y = 0,\nF (0) = 1.\nThe partial differential equation (5) has singularities along yi = 0 and yj = yi , j 6= i.\nHowever since F is an entire function,QF is determined\nby the partial differential equations\nQ\non the open region X = {y \u2208 Cm | m\ny\n(y\n\u2212\nyj ) 6= 0}. In this paper we call X\ni=1 i\ni6=j i\nthe non-diagonal region. Using\nyj\nyi\n=1+\nyi \u2212 yj\nyi \u2212 yj\n\nwe can rewrite (5) as gi F = 0, i = 1, . . . , m, where\ngi = yi \u2202i2 + (c \u2212 yi )\u2202i +\n\nm\nyj\n1 X\n(\u2202i \u2212 \u2202j ) \u2212 a\n2 j=1,j6=i yi \u2212 yj\n\n(6)\n\nis a differential operator annihilating F . In our holonomic gradient method we make a\ndirect use of the partial differential equations for numerical evaluation of 1F1 .\n\n4\n\n\f3\n\nHolonomic gradient method for dimension two\n\nIn this section we illustrate the holonomic gradient method for the case of m = 2. Although our purpose is to implement an algorithm of our method for a larger dimension,\nfor clarity it is best to do \"by hand\" calculation for the case of m = 2. As in the previous\nsection we simply write F (Y ) = 1F1 (a; c; Y ).\nIn Nakayama et al. [23] the holonomic gradient method was used to obtain the maximum likelihood estimate. The reciprocal of the likelihood function was minimized and\nthe method was called the holonomic gradient descent. For the application of this paper\nwe simply use the holonomic gradient method for evaluating F . Hence we omit the term\n\"descent\". Also, for minimization, at each step of the iteration, a direction for increments\nwas chosen to decrease the value of the function. In our application, starting from the\norigin Y = 0, we can choose arbitrary path to the target value Y where we want to\nevaluate F (Y ).\nAnother minor difference of the expository explanation in this section from Nakayama\net al. [23] and Sei et al. [28] is that we use the simple forward Euler method (e.g., Section\n3.1 of Ascher and Petzold [1]) for updating partial derivatives of F . In Nakayama et al.\n[23], once an updating direction is chosen at each step of the iteration, the 4-th order\nRunge-Kutta method was used. The simple Euler method is used only for the purpose of\nexposition. It is easier to explain the basic idea of the holonomic gradient method with the\nsimple Euler method. In our actual implementation in Section 6 we use the Runge-Kutta\nmethod for numerically solving the differential equation.\nWe will reduce our problem to a traditional problem of numerical analysis of an ordinary differential equation (ODE). For the reduction we utilize the notion of holonomic\ndifferential equations and the gradients of their solutions. It is why we call our method\nholonomic gradient method.\nIn the following we discuss the case of y1 6= y2 and y1 = y2 separately.\n\n3.1\n\nHolonomic gradient method for non-diagonal region\n\nIn this subsection we assume y1 6= y2 . Two partial differential equations in (6) are written\nas\nh\ni\n1 y2\ny1 \u220212 + (c \u2212 y1 )\u22021 +\n(\u22021 \u2212 \u22022 ) \u2212 a F = 0,\n(7)\n2 y1 \u2212 y2\nh\ni\n1 y1\ny2 \u220222 + (c \u2212 y2 )\u22022 +\n(\u22022 \u2212 \u22021 ) \u2212 a F = 0.\n(8)\n2 y2 \u2212 y1\nSuppose that we want to evaluate a higher derivative \u22021n1 \u22022n2 F = \u22022n2 \u22021n1 F of F . Let\nn2 \u2265 2. Then by (8)\nc\n1\na\u0001\ny1\n\u22021n1 \u22022n2 F = \u22021n1 \u22022n2 \u22122 \u2212 \u22022 + \u22022 \u2212\nF.\n(9)\n(\u22022 \u2212 \u22021 ) +\ny2\n2 y2 (y2 \u2212 y1 )\ny2\nNoting\ny1\n1\ny1 (2y2 \u2212 y1 )\n1\n=\u2212 2\n,\n\u22022 = \u2212 2 , \u22022\ny2\ny2\ny2 (y2 \u2212 y1 )\ny2 (y2 \u2212 y1 )2\n5\n\n\ffor n2 > 2, the right-hand side of (9) is further written as\n\u0010c\nc \u2212 y2 2 1 y1 (2y2 \u2212 y1 )\n\u22021n1 \u22022n2 \u22123 2 \u22022 \u2212\n\u2202 +\n(\u22022 \u2212 \u22021 )\ny2\ny2 2 2 y22 (y2 \u2212 y1 )2\na \u0011\ny1\na\n1\n(\u220222 \u2212 \u22021 \u22022 ) \u2212 2 + \u22022 F.\n\u2212\n2 y2 (y2 \u2212 y1 )\ny2 y2\n\n(10)\n\nAlthough the result is somewhat complicated, the important fact is that the total degree\nof differentiation n1 + n2 on the left-hand side of (9) is decreased by one to n1 + n2 \u2212 1\nin (10). As long as the degree of \u22021 or \u22022 is more than one, then we can recursively apply\n(7) or (8) to decrease the total degree of differentiation. It follows that for each n1 , n2 ,\n(n ,n )\n(n ,n )\n(n ,n )\n(n ,n )\nthere exist rational functions h001 2 , h101 2 , h011 2 , h111 2 in (y1 , y2) such that\n(n ,n2 )\n\n\u22021n1 \u22022n2 F = h001\n\n(n ,n2 )\n\nF + h101\n\n(n ,n2 )\n\n\u22021 F + h011\n\n(n ,n2 )\n\n\u22022 F + h111\n\n\u22021 \u22022 F.\n\n(11)\n\nIn this notation (7) is written as\n\u220212 F =\n\na\nc \u2212 y1 1\n1\ny2\ny2\nF \u2212(\n+\n)\u22021 F +\n\u22022 F\ny1\ny1\n2 y1 (y1 \u2212 y2 )\n2 y1 (y1 \u2212 y2 )\n(2,0)\n\n(2,0)\n\n(2,0)\n\n= h00 F + h10 \u22021 F + h01 \u22022 F\n\n(2,0)\n\n(h11\n\n\u2261 0).\n\n(12)\n\nFor a general dimension, (11) corresponds to the reduction by a Gr\u00f6bner basis as discussed\nin Section 4.\nFor us the important case is n1 = 1, n2 = 2. Since\n1\ny1\n1\u0001\n1\n=\n\u22021\n= \u22021\n\u2212\ny2 (y2 \u2212 y1 )\ny2 \u2212 y1 y2\n(y2 \u2212 y1 )2\n\nwe have\n\nc \u2212 y2\n\u22022 \u2212\ny2\nc \u2212 y2\n= \u2212\n\u22021 \u22022 \u2212\ny2\n\n\u22021 \u220222 F = \u22021 \u2212\n\n1\na\u0001\ny1\nF\n(\u22022 \u2212 \u22021 ) +\n2 y2 (y2 \u2212 y1 )\ny2\n1\n1\na \u0001\n1\ny1\n2\n(\u2202\n\u2212\n\u2202\n)\n\u2212\n(\u2202\n\u2202\n\u2212\n\u2202\n)\n+\n\u22021 F.\n2\n1\n1\n2\n1\n2 (y2 \u2212 y1 )2\n2 y2 (y2 \u2212 y1 )\ny2\n\nThere is a term y1 \u220212 on the right-hand side, into which we further substitute (7). Then\n(11) for \u22021 \u220222 F is written as\n\u0010 c\u2212y\n1\n1\na\n1\ny1\n2\n\u22021 \u22022 \u2212\n(\u22022 \u2212 \u22021 ) \u2212\n\u22021 \u22022 + \u22021\n\u22021 \u220222 F = \u2212\n2\ny2\n2 (y2 \u2212 y1 )\n2 y2 (y2 \u2212 y1 )\ny2\n\u0011\n\u0001\n1 y2\n1\n(c \u2212 y1 )\u22021 +\n(\u22021 \u2212 \u22022 ) \u2212 a F\n\u2212\n2y2 (y2 \u2212 y1 )\n2 y1 \u2212 y2\n\u00103\na\na\nc \u2212 y1 \u0011\n1\n=\n\u22021 F\nF+\n+\n\u2212\n2y2 (y2 \u2212 y1 )\n4 (y2 \u2212 y1 )2 y2 2y2(y2 \u2212 y1 )\n\u0011\n\u0010c \u2212 y\n1\n1\ny1\n3\n2\n\u22021 \u22022 F\n\u2202\nF\n\u2212\n+\n\u2212\n2\n4 (y2 \u2212 y1 )2\ny2\n2 y2 (y2 \u2212 y1 )\n(1,2)\n\n(1,2)\n\n(1,2)\n\n(1,2)\n\n= h00 F + h10 \u22021 F + h01 \u22022 F + h11 \u22021 \u22022 F.\n6\n\n(13)\n\n\fSince F is a symmetric function in y1 and y2 , \u220212 \u22022 F is obtained by permuting y1 and y2 .\nLet\n\uf8f6\n\uf8eb\nF\n\uf8ec \u22021 F \uf8f7\n\uf8f7\nF~ = \uf8ec\n\uf8ed \u22022 F \uf8f8\n\u22021 \u22022 F\n\ndenote the vector consisting of F and its square-free mixed derivatives. Differentiate the\ncomponents of F~ by y1 and denote \u22021 F~ = (\u22021 F, \u220212 F, \u22021 \u22022 F, \u220212 \u22022 F )t . Similarly define \u22022 F~ .\nThen by (12) and (13), \u2202i F~ , i = 1, 2, are written as \u2202i F~ = Pi (Y )F~ , where P1 and P2 are\nthe following 4 \u00d7 4 matrices with rational function entries\n\uf8f6\n\uf8f6\n\uf8eb\n\uf8eb\n0\n0\n1\n0\n0\n1\n0\n0\n(2,0)\n(2,0)\n\uf8ec 0\n\uf8ech(2,0)\n0\n0\n1 \uf8f7\nh10\nh01\n0 \uf8f7\n00\n\uf8f7.\n\uf8f7 , P2 (Y ) = \uf8ec (0,2) (0,2) (0,2)\n\uf8ec\nP1 (Y ) = \uf8ed\n\uf8edh00\nh10\nh01\n0 \uf8f8\n0\n0\n0\n1 \uf8f8\n(1,2)\n(1,2)\n(1,2)\n(1,2)\n(2,1)\n(2,1)\n(2,1)\n(2,1)\nh00\nh10\nh01\nh11\nh00\nh10\nh01\nh11\n\nThe matrices P1 , P2 are called coefficient matrices of a Pfaffian system (an integrable\nconnection) for F (Nakayama et al. [23]). Note that P2 is obtained from P1 by permutation\nof y1 and y2 . If we know the values of the components of F~ at Y = (y1 , y2 ), y1 6= y2 , then\nvalues at a nearby point Y + \u2206Y = (y1 + \u2206y1 , y2 + \u2206y2 ) can be approximated by the\nsimple Euler method (i.e. linear approximation) as\n.\nF~ (Y + \u2206Y ) = F~ (Y ) + \u2206y1 \u22021 F~ (Y ) + \u2206y2 \u22022 F~ (Y )\n= F~ (Y ) + \u2206y1 P1 (Y )F~ (Y ) + \u2206y2 P2 (Y )F~ (Y ).\n\n(14)\n\nNow suppose that we want to evaluate F (y1, y2 ) at a particular point (y1 , y2 ) with\n(0) (0)\n(0)\n(0)\ny1 6= y2 . If we know F~ (Y0 ) at some point Y0 = (y1 , y2 ), y1 6= y2 , close to the origin,\n(l) (l)\nthen we can choose an appropriate sequence of points Y (l) = (y1 , y2 ), l = 0, . . . , L, such\n(L) (L)\nthat (y1 , y2 ) = (y1 , y2 ). Along the sequence we can use (14) to update F~ (Y (l) ) and\nfinally the first element of F~ (Y (L) ) gives F (y1 , y2 ).\nTherefore it remains to consider how to obtain the initial values. Close to the origin\nwe can use the definition (1) of 1F1 . If Y is very close to zero, then we only need zonal\npolynomials of low orders, whose explicit forms are known. Zonal polynomials up to the\nthird order are as follows; C(1) (Y ) = M(1) (Y ),\n\uf8f6\n\uf8eb\n2 \u0012\n\u0013\n\u0012\n\u0013\n1\nC(2) (Y )\n\uf8ec 3 \uf8f7 M(2) (Y )\n,\n=\uf8ed\n4 \uf8f8 M(1,1) (Y )\nC(1,1) (Y )\n0\n3\n\n\uf8eb\n3\n1\nC(3) (Y )\n\uf8ec\n5\n\uf8ed C(2,1) (Y ) \uf8f8 = \uf8ec\n12\n\uf8ec\n\uf8ed0\nC(1,1,1) (Y )\n5\n0 0\n\uf8eb\n\n\uf8f6\n\n7\n\n2 \uf8f6\uf8eb\n\uf8f6\nM(3) (Y )\n5\uf8f7\n18 \uf8f7\n\uf8f7 \uf8ed M(2,1) (Y ) \uf8f8 ,\n\uf8f8\nM(1,1,1) (Y )\n5\n2\n(15)\n\n\fwhere M\u03ba (Y ) is the monomial symmetric polynomial associated with a partition \u03ba. Since\nF (y1 , y2) can be expanded as\n\u0012\n\u0013\n(a)(1)\n(a)(1,1)\n1 (a)(2)\nF (y1 , y2) = 1 +\nC(1) (Y ) +\nC(2) (Y ) +\nC(1,1) (Y ) + * * *\n(c)(1)\n2! (c)(2)\n(c)(1,1)\n\u0013\n\u0012\n(a)(2)\n2(a)(1,1)\n(a)(2)\n(a)(1)\nM(1,1) (Y ) + * * * ,\nM(1) (Y ) +\nM(2) (Y ) +\n+\n=1+\n(c)(1)\n2(c)(2)\n3(c)(2)\n3(c)(1,1)\n(16)\nfor an example, \u22021 \u22022 F (0, 0) is obtained as\n2a(a \u2212 12 )\n(a)2\n+\n\u22021 \u22022 F (0, 0) =\n.\n3(c)2\n3c(c \u2212 12 )\nIn a similar manner, we have\n(a)2\na\n,\n\u22021 F (0, 0) = \u22022 F (0, 0) = , \u220212 F (0, 0) = \u220222 F (0, 0) =\nc\n(c)2\n4(a)2 (a \u2212 21 )\n(a)3\n.\n\u220212 \u22022 F (0, 0) = \u220222 \u22021 F (0, 0) =\n+\n5(c)3\n5(c)2 (c \u2212 21 )\n\n(17)\n\nThese formulae can be obtained by a symbolic mathematics software, such as the routines\nfor Jack polynomials in sage mathematics software system (Stein et al. [30]).\n(0) (0)\nIn order to obtain the initial value F~ (Y0 ) at Y0 = (y1 , y2 ) close to the origin, we can\nuse the approximation\n(0)\n(0) .\n(0)\n(0)\nF~ (y1 , y2 ) = F~ (0, 0) + y1 \u22021 F~ (0, 0) + y2 \u22022 F~ (0, 0).\n\n(18)\n\nWe code the above procedure using deSolve package in the data analysis system R. We\nshow a simple source program in Appendix B. In addition, since the zonal polynomials\nare easy to evaluate for m = 2, we also evaluate the series expansion of 1F1 up to k = 150.\nAs an example, we compute percentage points by two methods for the case of n = 3,\n\u03a3 = diag(1/2, 1/4). The following percentage points for l1 agree in two methods to 6\ndigits.\n50%\n90%\n95%\n99%\n1.63785 3.54999 4.31600 6.05836\nButler and Wood [3] proposed the Laplace approximation for 1F1 and Koev and\nEdelman [14] proposed efficient algorithms for computing the truncation of 1F1 . For\nm = 2, n = 30 and \u03a3 = diag(1/2, 1/4), Figure 1 shows an illustrative example; the\nLaplace approximation fails to give the upper probability and the approximation by the\ntruncation rapidly converges to zero with partitions of degrees which are not sufficiently\nlarge. The distribution function by the holonomic gradient method is stable and accurate\neven when x is large.\n8\n\n\fLaplace approx.\n1.2\n\nHGM\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nTruncation of 1F1 up to k = 50\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\nFigure 1: m = 2, n = 30, \u03a3 = diag(1/2, 1/4)\n\n3.2\n\nHolonomic gradient method for the diagonal line\n\nIn the previous subsection we assumed y1 6= y2 to avoid singularity of the differential\nequations. However 1F1 itself does not have singularities. Hence we should be able to\nderive some differential equation even for y = y1 = y2 .\nIn (7) and (8) we can perform the limiting operation y1 \u2192 y2 = y using the l'H\u00f4pital\nrule. Since F is a symmetric function, at (y, y) we have\n\u22021 F (y, y) = \u22022 F (y, y).\nAlso \u220212 F (y, y) = \u220222 F (y, y). Hence by the l'H\u00f4pital rule, in (7) we have\n\u22021 F \u2212 \u22022 F\n= \u220222 F \u2212 \u22021 \u22022 F = \u220212 F \u2212 \u22021 \u22022 F.\ny1 \u2192y2 =y\ny1 \u2212 y2\nlim\n\nThen (7) for at (y, y) is written as\n\u0014\n\u0015\nh\ni\ny 2\ny\n3 2\n2\n0 = y\u22021 + (c \u2212 y)\u22021 + (\u22021 \u2212 \u22021 \u22022 ) \u2212 a F = y\u22021 + (c \u2212 y)\u22021 \u2212 \u22021 \u22022 \u2212 a F. (19)\n2\n2\n2\nBased on this we derive an ODE for f (y) = F (y, y). Firstly,\nf \u2032 (y) = 2\u22021 F or \u22021 F = f \u2032 (y)/2.\nSecondly,\nf \u2032\u2032 (y) = 2\u220212 F + 2\u22021 \u22022 F.\nFrom (19)\n\n1\n3 2\ny\u22021 F = y\u22021 \u22022 F \u2212 (c \u2212 y)\u22021 F + aF,\n2\n2\n9\n\n\fand\n3 \u2032\u2032\n1\n3\nyf (y) = y\u22021 \u22022 F \u2212 (c \u2212 y)\u22021 F + aF + y\u22021 \u22022 F\n4\n2\n2\n= 2y\u22021 \u22022 F \u2212 (c \u2212 y)\u22021 F + aF\nc\u2212y \u2032\n= 2y\u22021 \u22022 F \u2212\nf (y) + af (y)\n2\nor\n\n3\nc\u2212y \u2032\na\n\u22021 \u22022 F (y, y) = f \u2032\u2032 (y) +\nf (y) \u2212 f (y).\n8\n4y\n2y\n\n(20)\n\nf \u2032\u2032\u2032 (y) = 2\u220213 F + 6\u220212 \u22022 F.\n\n(21)\n\nThirdly,\nIn order to get another relation for \u220213 F and \u220212 \u22022 F , we differentiate (7) by y2 . Then by\ny1\ny1\ny2\n= \u22022 (\n\u2212 1) =\n,\n(22)\n\u22022\ny1 \u2212 y2\ny1 \u2212 y2\n(y1 \u2212 y2 )2\nwe obtain the following differential operator annihilating F :\ny1 \u220212 \u22022 + (c \u2212 y1 )\u22021 \u22022 +\n\n1\ny1\n1 y2\n(\u22021 \u2212 \u22022 ) +\n(\u22021 \u22022 \u2212 \u220222 ) \u2212 a\u22022 .\n2\n2 (y1 \u2212 y2 )\n2 y1 \u2212 y2\n\n(23)\n\nNoting y2 /(y1 \u2212 y2 ) = y1 /(y1 \u2212 y2 ) \u2212 1 this can be further written as\ny1 (\u22021 \u2212 \u22022 ) + (y1 \u2212 y2 )(\u22021 \u22022 \u2212 \u220222 ) 1\n\u2212 (\u22021 \u22022 \u2212 \u220222 ) \u2212 a\u22022\n2\n(y1 \u2212 y2 )2\n2\ny1 (\u22021 \u2212 \u22022 ) + (y1 \u2212 y2 )(\u22021 \u22022 \u2212 \u220222 ) 1\n= y1 \u220212 \u22022 + (c \u2212 1 \u2212 y1 )\u22021 \u22022 +\n+ (\u22021 \u22022 + \u220222 ) \u2212 a\u22022 .\n2\n(y1 \u2212 y2 )2\n2\n\ny1 \u220212 \u22022 + (c \u2212 y1 )\u22021 \u22022 +\n\nWe now apply the l'H\u00f4pital rule to\n(\u22021 \u2212 \u22022 ) + (y1 \u2212 y2 )(\u22021 \u22022 \u2212 \u220222 )\n.\n(y1 \u2212 y2 )2\nWe again let y1 \u2192 y2 = y. The second derivative of the denominator with respect to y1\ngives 2. Now\n\u0001\n\u220212 (\u22021 \u2212 \u22022 ) + (y1 \u2212 y2 )(\u22021 \u22022 \u2212 \u220222 )\n\u0001\n= \u22021 (\u220212 \u2212 \u22021 \u22022 ) + (\u22021 \u22022 \u2212 \u220222 ) + (y1 \u2212 y2 )(\u220212 \u22022 \u2212 \u22021 \u220222 )\n\u0001\n= \u22021 (\u220212 \u2212 \u220222 ) + (y1 \u2212 y2 )(\u220212 \u22022 \u2212 \u22021 \u220222 )\n= (\u220213 \u2212 \u22021 \u220222 ) + (\u220212 \u22022 \u2212 \u22021 \u220222 ) + (y1 \u2212 y2 )(\u220213 \u22022 \u2212 \u220212 \u220222 ).\nEvaluating the right-hand side at y = y1 = y2 and noting that \u220212 \u22022 F = \u22021 \u220222 F at (y, y),\nwe just have \u220213 \u2212 \u220212 \u22022 . Hence (23) at (y, y) reduces to\n1\ny\ny\u220212 \u22022 + (c \u2212 1 \u2212 y)\u22021 \u22022 + (\u220213 \u2212 \u220212 \u22022 ) + (2\u220212 + 2\u22021 \u22022 ) \u2212 a\u22021\n4\n4\n1\ny\n= (2\u220213 + 6\u220212 \u22022 ) + (c \u2212 1 \u2212 y)\u22021 \u22022 + (2\u220212 + 2\u22021 \u22022 ) \u2212 a\u22021 ,\n8\n4\n10\n\n\fwhere we used \u22021 F = \u22022 F at (y, y). Comparing the right-hand side with (21) and by (20)\nwe obtain\n\u0001 1\ny \u2032\u2032\u2032\nc\u2212y \u2032\na\na\n3\nf (y) + (c \u2212 1 \u2212 y) f \u2032\u2032 (y) +\nf (y) \u2212 f (y) + f \u2032\u2032 (y) \u2212 f \u2032 (y) = 0.\n8\n8\n4y\n2y\n4\n2\n\n(24)\n\nThis equation can be written as\n\nf \u2032\u2032\u2032 (y) = h2 (y)f \u2032\u2032(y) + h1 (y)f \u2032(y) + h0 (y)f (y),\nwhere\nh2 (y) = \u2212\n\n4a 2(c \u2212 y)(c \u2212 1 \u2212 y)\n4a(c \u2212 1 \u2212 y)\n3(c \u2212 1 \u2212 y) 2\n\u2212 , h1 (y) =\n\u2212\n, h0 (y) =\n2\ny\ny\ny\ny\ny2\n\nare rational functions in y. The coefficient matrix for the Pfaffian system for a onedimensional ODE is simply the companion matrix\n\uf8eb\n\uf8f6\n0\n1\n0\nP =\uf8ed 0\n0\n1 \uf8f8.\nh0 (y) h1 (y) h2 (y)\nNote that the values of f , f \u2032 , f \u2032\u2032 and f \u2032\u2032\u2032 at the origin are given by\n\n2a\n,\nc\n8(a)2 4a(a \u2212 21 )\n\u2032\u2032\n2\n,\nf (0) = 2\u22021 F (0, 0) + 2\u22021 \u22022 F (0, 0) =\n+\n3(c)2\n3c(c \u2212 21 )\n\u0010 (a)\n4(a)2 (a \u2212 12 ) \u0011\n(a)3\n3\n\u2032\u2032\u2032\n3\n2\nf (0) = 2\u22021 F (0, 0) + 6\u22021 \u22022 F (0, 0) = 2\n+6\n+\n.\n(c)3\n5(c)3\n5(c)2 (c \u2212 12 )\nf (0) = F (0, 0) = 1,\n\nf \u2032 (0) = 2\u22021 F (0, 0) =\n\nAs seen above, the computation using the l'H\u00f4pital rule is already tedious for m = 2.\nActually the computation can be automated by the restriction algorithm for holonomic\nideals. This will be explained in Section 5.2.\n\n4\n\nProperties of the Pfaffian system (integrable connection) for a general dimension\n\nWe now consider our problem for a general dimension. We fully utilize Gr\u00f6bner basis\ntheory for the ring of differential operators. In this section we only consider the nondiagonal region X . Let K = C(y1 , . . . , ym ) be the field of rational functions in y1 , . . . , ym\nwith complex coefficients. Further let\nR = Kh\u22021 , . . . , \u2202m i = C(y1 , . . . , ym )h\u22021 . . . , \u2202m i\n11\n\n\fbe the ring of differential operators with rational function coefficients (see Appendix of\nNakayama et al. [23]). Let I denote the left ideal of R generated by g1 , . . . , gm :\nI = hg1, . . . , gm i,\n\n(25)\n\nwhere gi is given in (6).\nWe now prove the following lemma concerning the commutators of g1 , . . . , gm .\nLemma 1. For 1 \u2264 i 6= j \u2264 m,\n[gi , gj ] = \u2212\n\n1 yi + yj\n(gi \u2212 gj ).\n2 (yi \u2212 yj )2\n\n(26)\n\nA similar result for 2F1 is given in Lemma 9.9 of Ibukiyama et al. [9]. Although\nthey claim that their Lemma 9.9 follows from a straightforward computation, in fact the\ncomputation for checking (26) is tedious even for m = 2. However for m = 2, (26) can be\nverified by some software systems (e.g., RisaAsir developing team [26]), which can handle\nrings of differential operators. The following program in Risa/Asir\nimport(\"names.rr\"); import(\"yang.rr\");\nyang.define_ring([\"partial\",[y1,y2]]);\nG1=y1*dy1^2+(c-y1)*dy1+(1/2)*(y2/(y1-y2))*(dy1-dy2)-a;\nG2=base_replace(G1,[[y1,y2],[y2,y1],[dy1,dy2],[dy2,dy1]]);\nG=yang.mul(G1,G2)-yang.mul(G2,G1)+(1/2)*(y1+y2)/(y1-y2)^2*(G1-G2);\nprintf(\"G=%a\\n\",G);\n\noutputs the result G=0. Therefore in the following proof, assuming that (26) holds for\nm = 2, we show that it holds for m > 2.\nProof. By symmetry we only need to prove the case i = 1, j = 2. Define g\u03031 , g\u03032\n1 y2\n(\u22021 \u2212 \u22022 ) \u2212 a,\n2 y1 \u2212 y2\n1 y1\n(\u22022 \u2212 \u22021 ) \u2212 a.\ng\u03032 = y2 \u220222 + (c \u2212 y2 )\u22022 +\n2 y2 \u2212 y1\n\ng\u03031 = y1 \u220212 + (c \u2212 y1 )\u22021 +\n\nThen\n\nm\n\ngi = g\u0303i + hi ,\nWe already know\n\n1 X yk\n(\u2202i \u2212 \u2202k ),\nhi =\n2 k=3 yi \u2212 yk\n\n[g\u03031 , g\u03032 ] = \u2212\n\ni = 1, 2.\n\n1 y1 + y2\n(g\u03031 \u2212 g\u03032 ).\n2 (y1 \u2212 y2 )2\n\nThen\n[g1 , g2 ] = [g\u03031 + h1 , g\u03032 + h2 ] = [g\u03031 , g\u03032 ] + [h1 , g\u03032 ] + [g\u03031 , h2 ] + [h1 , h2 ].\nTherefore it suffices to show\n[h1 , g\u03032 ] + [g\u03031 , h2 ] + [h1 , h2 ] = \u2212\n12\n\n1 y1 + y2\n(h1 \u2212 h2 ).\n2 (y1 \u2212 y2 )2\n\n\fIn considering commutators, we only need to look at terms, where a differential operator actually differentiate rational functions in y1 , . . . , ym . For example consider h1 g\u03032 in\n[h1 , g\u03032 ]. In h1 g\u03032 the only relevant term is \u22021 in h1 differentiating y1 /(y1 \u2212 y2 ) in g\u03032 . Noting\n\u22021\n\n\u0001\ny1\ny2\ny2\n= \u22021\n\u22121 =\n,\ny2 \u2212 y1\ny2 \u2212 y1\n(y2 \u2212 y1 )2\n\nin h1 g\u03032 the relevant terms are\n\nm\nm\nX\nX\n\u0001\nyk\nyk\n1\n1\ny2\ny2\n(\u2202\n\u2212\n\u2202\n)\n=\n(\u2202\n\u2212\n\u2202\n)\n\u2212\n(\u2202\n\u2212\n\u2202\n)\n.\n2\n1\n2\nk\n1\nk\n4 (y2 \u2212 y1 )2 k=3 y1 \u2212 yk\n4 (y2 \u2212 y1 )2 k=3 y1 \u2212 yk\n\nIn g\u03032 h1 we need to look at \u22021 in g\u03032 differentiating yk /(y1 \u2212 yk ). Hence we have\nm\n1 y1 X\nyk\n(\u22021 \u2212 \u2202k ).\n4 y2 \u2212 y1 k=3 (y1 \u2212 yk )2\n\nSimilarly in [g\u03031 , h2 ] = \u2212[h2 , g\u03031 ] the relevant terms are\nm\nm\nX\n\u0001 1 y2 X\nyk\nyk\ny1\n1\n(\u22021 \u2212 \u2202k ) \u2212 (\u22022 \u2212 \u2202k ) +\n(\u22022 \u2212 \u2202k ).\n\u2212\n2\n4 (y1 \u2212 y2 ) k=3 y2 \u2212 yk\n4 y1 \u2212 y2 k=3 (y2 \u2212 yk )2\n\nFinally in [h1 , h2 ] we look at \u2202k differentiating yk /(yi \u2212 yk ). Then the relevant terms\nare\n\nm\n\nm\n\n1 X yk\n1 X yk\ny2\ny1\n\u2212\n(\u2202\n\u2212\n\u2202\n)\n+\n(\u22021 \u2212 \u2202k ).\n2\nk\n2\n4 k=3 y1 \u2212 yk (y2 \u2212 yk )\n4 k=3 y2 \u2212 yk (y1 \u2212 yk )2\n\nThen the coefficient for \u2212(\u22021 \u2212 \u2202k )/4 is\n\ny2\ny1\nyk\ny1\nyk\ny1\nyk\nyk\n+\n+\n\u2212\n2\n2\n2\n(y2 \u2212 y1 ) y1 \u2212 yk y2 \u2212 y1 (y1 \u2212 yk )\n(y1 \u2212 y2 ) y2 \u2212 yk y2 \u2212 yk (y1 \u2212 yk )2\nyk\ny1 + y2\n,\n=\n2\n(y2 \u2212 y1 ) y1 \u2212 yk\nwhich coincides with the coefficient of \u2212(\u22021 \u2212 \u2202k )/4 in\n\u2212\n\n1 y1 + y2\nh1 .\n2 (y1 \u2212 y2 )2\n\nSimilarly the coefficients of (\u22022 \u2212 \u2202k ) coincide on both sides.\nWe now consider the graded lexicographic term order \u227b. The initial term of gi (without\nthe coefficient yi ) is given as\nin\u227b gi = \u2202i2 .\nWe now prove the following theorem.\n13\n\n\fTheorem 2. For the term order \u227b, {g1 , . . . , gm } is a Gr\u00f6bner basis of I in R and the ini2\ntial ideal is given by h\u220212 , . . . , \u2202m\ni. I is zero-dimensional and the set of standard monomials\nis given by the set of square-free mixed derivatives\n{\u2202i1 \u2202i2 . . . \u2202ik | 1 \u2264 i1 < * * * < ik \u2264 m, k \u2264 m},\nwhich has the cardinality 2m .\nProof. By Lemma 1 and the Buchberger's criterion for the ring R (cf. Theorem 1.1.10\nof Saito et al. [27]), gi , i = 1, . . . , m, form a Gr\u00f6bner basis and the initial ideal is given\n2\n2\nby h\u220212 , . . . , \u2202m\ni. Let J = h\u22021 , . . . , \u2202m i. Then J m+1 \u2282 h\u220212 , . . . , \u2202m\ni. Hence I is a zerodimensional ideal. Furthermore this shows that the set of standard monomials is given\nby the set of square-free mixed derivatives.\nIt follows from Theorem 2 that there exists a Pfaffian system and 2m \u00d72m matrices (as\nPi (Y ) for m = 2 in the expository section 3) are obtained by the normal form algorithm\nin the ring of differential operators R. The matrices are used to numerically solve the\nassociated ODE. However, the derivation of the matrices on computer is heavy and the\nobtained matrices are not in a relevant form for an efficient numerical evaluation. Then,\nwe do it by hand in the sequel.\nnm\nConsider a higher order derivative \u22021n1 . . . \u2202m\nF of F = 1F1 (a; c; y1, . . . , ym ). If total\ndegree of differentiation n = n1 + * * * + nm is greater than or equal to m + 1, then for\nsome i we have ni \u2265 2. Then as in the previous section we can use gi F = 0 to decrease\nthe total degree of differentiation. Therefore as in (11), for each n1 , . . . , nm , there exist\n(n1 ,...,nm )\n, ij = 0, 1, j = 1, . . . , m, such that\n2m rational functions hi1 ,...,i\nm\n\u22021n1\n\nnm\nF\n. . . \u2202m\n\n=\n\n1\nX\n\n***\n\ni1 =0\n\n1\nX\n\n(n ,...,n )\n\nm\n1\nim\nF.\n\u22021i1 . . . \u2202m\nhi1 ,...,i\nm\n\n(27)\n\nim =0\n\nIn the holonomic gradient method, as in the case of m = 2 in (14), we only need\nand at most one of n1 , . . . , nm is two, such as\nvector of square-free mixed derivatives of F by\nF~ = (F, \u22021 F, \u22022 F, \u22021 \u22022 F, . . . , \u22021 . . . \u2202m F )t . In F~ the elements are lexicographically ordered,\nfor convenience in programming. \u2202i F~ is written as\n(n1 ,...,nm )\nwhere 0 \u2264 n1 , . . . , nm \u2264 2\nhi1 ,...,i\nm\n(2,1,...,1,0,...,0)\n. Define a 2m -dimensional\nhi1 ,...,im\n\n\u2202i F~ = Pi (y)F~ ,\n\ni = 1, . . . , m,\n\nwhere Pi (y), i = 1, . . . , m, in the Pfaffian system are 2m \u00d7 2m matrices consisting of\n(n1 ,...,nm )\n's.\nhi1 ,...,i\nm\n(n1 ,...,nm )\n, where 0 \u2264 n1 , . . . , nm \u2264 2 and at most one of\nWe now study the form of hi1 ,...,i\nm\nn1 , . . . , nm is two. Denote [m] = {1, . . . , m}. For a subset J \u2282 [m] denote\nY\n\u2202J =\n\u2202j .\nj\u2208J\n\n14\n\n\fChoose i \u2208 [m] and J \u2282 [m] such that i 6\u2208 J. Write I = J \u222a {i}. \u2202J gi F = \u2202J 0 = 0, where\ngi is in (6). Since i 6\u2208 J, we can write \u2202J gi as\nyi \u2202i2 \u2202J + (c \u2212 yi )\u2202I +\nFor k 6\u2208 J\n\u2202J\n\n\u0001\n1X\nyk\n\u2202J\n(\u2202i \u2212 \u2202k ) \u2212 a\u2202J .\n2 k6=i\nyi \u2212 yk\n\n\u0001\nyk\nyk\n(\u2202i \u2212 \u2202k ) =\n(\u2202I \u2212 \u2202J\u222a{k} ).\nyi \u2212 yk\nyi \u2212 yk\n\nOn the other hand for k \u2208 J, by (22)\n\u2202J\n\n\u0001\nyk\nyi\nyk\n(\u2202i \u2212 \u2202k ) =\n(\u2202I \u2212 \u2202J \u2202k ) +\n(\u2202{i}\u222aJ\\{k} \u2212 \u2202J ).\nyi \u2212 yk\nyi \u2212 yk\n(yi \u2212 yk )2\n\nHere \u2202J \u2202k is not square-free and in fact\n\n\u2202J \u2202k = \u2202k2 \u2202J\\{k} ,\nwhich causes recursive application of (6). In \u2202J gi we now separate square-free terms and\ndefine\nh\n1 X yk\nr(i, J; y) = \u2212 (c \u2212 yi )\u2202I \u2212 a\u2202J +\n(\u2202I \u2212 \u2202J \u2202k )\n2\nyi \u2212 yk\nk6\u2208I\ni\n1 X yk\nyi\n1X\n+\n\u2202I +\n(\u2202i \u2202J\\{k} \u2212 \u2202J ) ,\n2 k\u2208J yi \u2212 yk\n2 k\u2208J (yi \u2212 yk )2\nwhere for J = \u2205, reflecting the original gi , we define\ni\nh\n1 X yk\n(\u2202i \u2212 \u2202k ) .\nr(i, \u2205; y) = \u2212 (c \u2212 yi )\u2202i \u2212 a +\n2 k6=i yi \u2212 yk\nThen \u2202i2 \u2202J F is expanded as\nyi \u2202i2 \u2202J F = r(i, J; y)F +\n\n1X 1\n(yk \u2202k2 \u2202J\\{k} )F.\n2\nyi \u2212 yk\n\n(28)\n\nk\u2208J\n\nThe use of this recursive expression yields an efficient numerical evaluation of the matrices\nof the Pfaffian system. We keep numerical values of \u2202k2 \u2202J\\{k} F in a table and use them to\nevaluate \u2202i2 \u2202J F and keep it in the table, again.\nWe can also apply the recursion to the last term on the right-hand side. The resulting\n\n15\n\n\fexpression for yi \u2202i2 \u2202J F is given as\nyi \u2202i2 \u2202J F = r(i, J; y)F +\n\n1\n1X\nr(k1 , J \\ {k}; y)F\n2 k \u2208J yi \u2212 yk1\n1\n\n1\n+\n4\n+\n\n+\n\n1\n8\n\n1\nr(k2 , J \\ {k1 , k2 }; y)F\n(yi \u2212 yk1 )(yk1 \u2212 yk2 )\n\nX\n\nk1 ,k2 \u2208J\nk1 ,k2 :distinct\n\nX\n\nk1 ,k2 ,k3 \u2208J\nk1 ,k2 ,k3 :distinct\n\n1\n2|J|\n\n1\nr(k3 , J \\ {k1 , k2 , k3 }; y)F + . . .\n(yi \u2212 yk1 )(yk1 \u2212 yk2 )(yk2 \u2212 yk3 )\n1\nr(k|J|, \u2205; y)F.\n(yi \u2212 yk1 )(yk1 \u2212 yk2 ) . . . (yk|J |\u22121 \u2212 yk|J | )\n\nX\n\nk1 ,...,k|J | \u2208J\n\n(29)\n\nk1 ,...,k|J | :distinct\n\nNow in (4) we write \u03a3\u22121 /2 = \u03b2 = (\u03b21 , . . . , \u03b2m ), where \u03b21 , . . . , \u03b2m are distinct, and\n~ in a scalar x by\ndefine a 2m -dimensional vector valued function G\n~\nG(x)\n= exp(\u2212x\n\nm\nX\n\n\u03b2i )xmn/2 F~ (\u03b2x).\n\ni=1\n\n~ satisfies the ODE\nThen G\n~\ndG\n=\ndx\n\n\u2212(\n\nm\nX\n\n\u03b2i )I2m\n\ni=1\n\n!\nm\nX\nmn\n~\n+\nPi (\u03b2x)\u03b2i G,\nI2m +\n2x\ni=1\n\n(30)\n\n~ We\nwhere I2m is the 2m \u00d7 2m identity matrix. We denote the right-hand side as P\u03b2 G.\nnow prove the following theorem, which is important for guaranteeing stability of ODE\nat x = +\u221e.\nTheorem 3. As x \u2192 \u221e\nP\u03b2 = A0 + O(1/x),\nwhere A0 only depends on \u03b2 and the 2m eigenvalues of A0 are given as \u2212e1 \u03b21 \u2212* * *\u2212em \u03b2m ,\nwhere (e1 , . . . , em ) \u2208 {0, 1}m.\nProof. Note that y1 = \u03b21 x, . . . , ym = \u03b2m x = O(x). Divide (29) by yi = \u03b2i x. Then on the\nright-hand side of (29), the only constant order term is \u2202I in r(i, J; y). Now\nm\n\nX\nd\n\u2202I F (\u03b2x) =\n\u03b2i \u2202i \u2202I F (\u03b2x)\ndx\ni=1\nX\nX\n\u03b2i \u2202I\u222a{i} F (\u03b2x)\n=\n\u03b2i \u2202i2 \u2202I\\{i} F (\u03b2x) +\ni\u2208I\n\n=\n\nX\ni\u2208I\n\ni6\u2208I\n\n\u0001\n\n\u03b2i \u2202I F (\u03b2x) + O(1/x) +\n\nX\ni6\u2208I\n\n16\n\n\u03b2i \u2202I\u222a{i} F (\u03b2x).\n\n\fThis implies that the I-th diagonal element of A0 is given\nm\nX\nX\nX\n\u2212\n\u03b2i +\n\u03b2i = \u2212\n\u03b2i .\ni=1\n\ni\u2208I\n\ni6\u2208I\n\nFurthermore the (I, I \u222a{i})-element of A0 is \u03b2i . Other elements\nof A0 are zeros. Hence A0\nP\nis an upper triangular matrix with diagonal elements \u2212 i6\u2208I \u03b2i , I \u2282 [m]. The theorem\nholds because the diagonal elements of an upper triangular matrix are its eigenvalues.\n\n5\n\nSome results of symbolic computation\n\nIn this section we present some results on symbolic computation for the initial values (cf.\n(17)) and the restriction for diagonal regions (cf. Section 3.2). We omit writing down the\nfully expanded form of (29), since the recursive formula (28) can be directly used in our\nimplementation of holonomic gradient method.\n\n5.1\n\nInitial values\n\nInitial values for our holonomic gradient method can be obtained by expressing 1F1 in\nterms of monomial symmetric polynomials as in (16). We denote the relation between\nthe zonal polynomials and the monomial symmetric polynomials in (15) as\nX\nc\u03ba,\u03bb M\u03bb (Y ),\nC\u03ba (Y ) =\n\u03bbE\u03ba\n\nP\nP\nwhere \u03bb E \u03ba means that \u03bb is dominated by \u03ba, i.e. si=1 \u03bbi \u2264 si=1 \u03bai for all s. Then 1F1\nis expressed as\n\u221e X\nX (a)\u03ba c\u03ba,\u03bb\nX\nq\u03bb (a, c)M\u03bb (Y ), q\u03bb (a, c) =\nF\n(a;\nc;\nY\n)\n=\n.\n1 1\n(c)\n\u03ba k!\n\u03ba\u22a2k,\u03baD\u03bb\nk=0 \u03bb\u22a2k\n\nA recurrence relation for c\u03ba,\u03bb 's is given by James [11] (see also (14) in Section 7.2.1\nof Muirhead [22] and Section 4.5.4 of Takemura [34]), which can be used to compute\nq\u03ba (a, c). However James' recurrence relation works for each C\u03ba separately. Recently Koev\nand Edelman [14] gave a much improved algorithm based on recursive relations among the\nvalues of zonal polynomials for m variables and m \u2212 1 variables. For our implementation\nof holonomic gradient method, we adapted Koev-Edelman's recurrence relation also for\nderivatives of 1F1 to evaluate the initial values.\nClose to the origin, we can use rough initial values given by the linear approximation\nas in (18). Then we only need \u03ba = (k1 , . . . , kl ) such that k1 = * * * = kl = 1 or k1 = 2,\nk1 = * * * = kl = 1. Some q\u03bb (a, c)'s for small \u03bb's are as follows.\n2(a)(1,1)\n2(a)(2,1)\na\n(a)2\n(a)3\n(a)2\nq\u2205 = 1, q(1) = , q(2) =\n, q(1,1) =\n+\n, q(2,1) =\n+\n,\nc\n2(c)2\n3(c)2 3(c)(1,1)\n10(c)3 5(c)(2,1)\n3(a)(2,1)\n(a)(1,1,1)\n4(a)(2,2)\n11(a)(3,1) 2(a)(2,1,1)\n(a)4\n(a)3\n+\n+\n, q(2,1,1) =\n+\n+\n+\n,\nq(1,1,1) =\n15(c)3 5(c)(2,1)\n3(c)(1,1,1)\n70(c)4 45(c)(2,2)\n63(c)(3,1)\n9(c)(2,1,1)\n17\n\n\fwhere \u2205 stands for the unique partition of zero. Write (1k ) = (1, . . . , 1), (2, 1k\u22122) =\n(2, 1, . . . , 1), which are partitions of k. Given the above quantities, the linear approximation of \u22021 . . . \u2202l F (Y ), 0 \u2264 l \u2264 m, for Y = (y1 , . . . , ym ) close to the origin is expressed\nas\n.\n\u22021 . . . \u2202l F (Y ) = q(1l ) (a, c) + 2q(2,1l\u22121 ) (a, c)(y1 + * * * + yl ) + q(1l+1 ) (a, c)(yl+1 + * * * + ym), (31)\nwhere for l = 0 the second term on the right-hand side is zero and for l = m the third\nterm is zero. We found that initial values by (31) are practical enough for m \u2264 5.\nIn fact, by Lemma 1 in Section 4.5.2 of [34] and by Proposition 7.3 of [29], q(1k ) (a, c)\nand q(2,1k\u22122 ) (a, c) are explicitly written as follows:\nQ\nX 1\u2264i<j\u2264l(\u03ba)(2ki \u2212 2kj \u2212 i + j) (a)\u03ba\nq(1k ) (a, c) = 2k k!\n,\nQl(\u03ba)\n(c)\n\u03ba\n(2k\n+\nl(\u03ba)\n\u2212\ni)!\ni\ni=1\n\u03ba\u22a2k\nQ\nl(\u03ba)\nX 1\u2264i<j\u2264l(\u03ba) (2ki \u2212 2kj \u2212 i + j) \u0012k \u0013 X\n\u0001 (a)\u03ba\nk\nki (ki \u2212 i)\n+\n,\nq(2,1k\u22122 ) (a, c) = 2 (k \u2212 2)!\nQl(\u03ba)\n2\n(c)\u03ba\ni=1\ni=1 (2ki + l(\u03ba) \u2212 i)!\n\u03ba\u22a2k\n\nwhere l(\u03ba) is the length (number of non-zero parts) of \u03ba = (k1 , . . . , kl(\u03ba) ).\nFor larger values of m we need higher order terms for initial values. For two partitions\n\u03bc, \u03bb, we write \u03bc \u2282 \u03bb to denote \u03bci \u2264 \u03bbi for all i. For two partitions \u03ba, \u03bd, we denote by\n\u03ba \u228e \u03bd the concatenation of \u03ba and \u03bd obtained from (\u03ba1 , \u03bd1 , \u03ba2 , \u03bd2 , . . .) by sorting. Consider\na rectangular partition \u03c4 = (t, . . . , t) = (tl ) \u22a2 tl. For \u03c4 = (tl ) and \u03bb \u2283 \u03c4 we define\nI(\u03bb, \u03c4 ) = {(\u03ba, \u03bd) | \u03ba \u228e \u03bd = \u03bb, \u03c4 \u2282 \u03ba, \u03bal+1 = 0}.\n\n\u03bcm\nConsider \u2202 \u03bc M\u03bb (Y ) = \u22021\u03bc1 \u22022\u03bc2 * * * \u2202m\nM\u03bb (Y ). Note that \u2202 \u03bc M\u03bb (Y ) = 0, if \u03bc 6\u2282 \u03bb. For a\nl\nrectangular \u03c4 = (t ) we can calculate \u2202 \u03c4 M\u03bb (Y ) by the following lemma.\n\nLemma 2. For \u03c4 = (tl ) \u2282 \u03bb\n\u2202 \u03c4 M\u03bb (Y ) =\n\nX\n\n(\u03ba,\u03bd)\u2208I(\u03bb,\u03c4 )\n\nwhere \u03b3! =\n\nQ\n\ni (\u03b3i !)\n\n\u03ba!\nM\u03ba\u2212\u03c4 (y1 , . . . , yl )M\u03bd (yl+1, . . . , ym ),\n(\u03ba \u2212 \u03c4 )!\n\nfor a partition \u03b3, and \u03ba \u2212 \u03c4 = (\u03ba1 \u2212 t, . . . , \u03bal \u2212 t).\n\nProof is straightforward and omitted. Using this lemma we have the following proposition.\nProposition 1. For a rectangular partition \u03c4 = (tl ),\n\u03c4\n\n\u2202 1F1 (a; c; Y ) =\n\n\u221e X\nX\nk=tl \u03bb\u22a2k,\n\u03c4 \u2282\u03bb\n\nq\u03bb (a, c)\n\nX\n\n(\u03b3,\u03bd)\u2208I(\u03bb,\u03c4 )\n\n\u03b3!\nM\u03b3\u2212\u03c4 (y1 , . . . , yl )M\u03bd (yl+1 , . . . , ym ).\n(\u03b3 \u2212 \u03c4 )!\n(32)\n18\n\n\fFor our initial values we only need to consider \u03c4 = (1l ). We obtain (31) if we only look\nat linear terms on the right-hand side of (32). Note that since 1F1 (a; c; Y ) is a symmetric\nfunction in y1 , . . . , ym , other derivatives are obtained by permutation of y1 , . . . , ym.\nAlthough (32) only gives derivative with respect to a rectangular partition \u03c4 = (tl ), we\ncan obtain other derivatives \u22021\u03bc1 . . . \u2202l\u03bch 1F1 (a; c; Y ), \u03bc1 \u2265 * * * \u2265 \u03bch , by repeated application\nof (32) for different values of l's.\n\n5.2\n\nRestriction to diagonal regions\n\nAs mentioned at the end of Section 3.2, the tedious operation involving the l'H\u00f4pital rule\nfor the diagonal region can be performed by the restriction algorithm for holonomic ideals.\nThe following program in Risa/Asir for m = 2\nimport(\"names.rr\"); import(\"nk_restriction.rr\");\nG1=y1*dy1^2 + (c-y1)*dy1+(1/2)*(y2/(y1-y2))*(dy1-dy2)-a; G1=red((y1-y2)*G1);\nG2=base_replace(G1,[[y1,y2],[y2,y1],[dy1,dy2],[dy2,dy1]]);\nF=base_replace([G1,G2],[[y1,y],[y2,y+z2],[dy1,dy-dz2],[dy2,dz2]]);\nA=nk_restriction.restriction_ideal(F,[z2,y],[dz2,dy],[1,0] | param=[a,c]);\n\nproduces the output\n-y^2*dy^3+(3*y^2+(-3*c+1)*y)*dy^2+(-2*y^2+(4*a+4*c-2)*y-2*c^2+2*c)*dy-4*a*y+(4*c-4)*a\n\nThis is the same as (24). Adapting the above program for m = 3, we obtain\ny 3 f \u2032\u2032\u2032\u2032 (y) + (\u22126y 3 + (6c \u2212 4)y 2)f \u2032\u2032\u2032 (y)\n+ (11y 3 + (\u221210a \u2212 22c + 18)y 2 + (11c2 \u2212 17c + 4)y)f \u2032\u2032(y)\n+ (\u22126y 3 + (30a + 18c \u2212 18)y 2 + ((\u221230c + 34)a \u2212 18c2 + 34c \u2212 12)y\n+ 6c3 \u2212 16c2 + 10c)f \u2032(y)\n+ (\u221218ay 2 + (9a2 + (36c \u2212 51)a)y + (\u221218c2 + 48c \u2212 30)a)f (y) = 0.\nFor m = 4, we found that the computation by Risa/Asir takes too much time and\nmemory.\nQ\nWe conjecture that the ideal I generated by j6=i (yi \u2212 yj )gi , i = 1, . . . , m in the\nWeyl algebra Dm is an holonomic ideal. In fact, the conjecture can be checked for small\ndimensions m on a computer. See the Appendix A. If I is a holonomic ideal, then\nJ = (I + (y1 \u2212 y2 )Dm + (y1 \u2212 y3 )Dm + * * * + (y1 \u2212 ym )Dm ) \u2229 Chy1 , \u2202y1 i\nis not 0 and is an holonomic ideal in D1 by the theorem of Bernstein (see, e.g., the\nChapter 5 of [27]). The generators of J is ordinary differential equations for the function\nrestricted to the diagonal y1 = * * * = ym . Thus, the holonomicity implies the existence\nof the diagonal ordinary differential equation. The ideal J can be obtained by Oaku's\nalgorithm ([24]) based on Gr\u00f6bner bases and the Risa/Asir package nk restriction\nuses this algorithm.\n\n19\n\n\f6\n\nNumerical experiments\n\nWe implemented the holonomic gradient method in a straightforward manner. Our source\nprograms in the language C are available from\nhttp://www.math.kobe-u.ac.jp/OpenXM/Math/1F1.\nThe updating step of the holonomic gradient method was implemented using the\nrecursive relation (28) for a general dimension. For initial values we adapted Koev and\nEdelman [14] for derivatives of 1F1 as discussed in Section 5.1.\nThe accuracy of the holonomic gradient method can be simply checked by looking at\nthe numerical convergence Pr[l1 < x] \u2192 1 as x \u2192 \u221e. This is because the initial values\nare evaluated at small x > 0 and Pr[l1 < x] at large x is obtained after many updating\nsteps. This is another advantage of our method.\nAlso we can use the following simple bounds for the upper tail probability for the\npurpose of checking. Let Pr[l1 < x|\u03a3] denote the probability under the covariance matrix\n2\n2\n\u03a3. Consider \u03a3 = diag(\u03c312 , . . . , \u03c3m\n), \u03c312 \u2265 * * * \u2265 \u03c3m\n. Then by standard stochastic ordering\nconsideration, we have\n2\nPr[l1 < x| diag(\u03c312 , . . . , \u03c312 )] \u2264 Pr[l1 < x| diag(\u03c312 , . . . , \u03c3m\n)]\n2\n\u2264 Pr[l1 < x| diag(\u03c31 , 0, . . . , 0)].\n\n(33)\n\nThe upper bound coincides with the cumulative probability of chi-square distribution\nwith n degrees of freedom (cf., [32],[35]). Accurate approximation for the lower bound\nPr[l1 < x|\u03c312 Im ] is given by the tube method ([15], [16]).\nWe first consider the case m = 5, n = 7, \u03a3\u22121 /2 = \u03b2 = (1, 2, 3, 4, 5). For x = 20,\nthe two bounds in (33) are given as 0.9996034 and 0.9999987. With the initial value of\nx0 = 0.01 and step size 0.0001 we obtained\nPr[l1 < 20] = 0.999972.\nThe cumulative distribution function for this case is plotted on the left part of Figure 2.\nNext we consider the case m = 10, n = 12, \u03b2 = (1, 2, . . . , 10). For x = 30, the bounds\nare (0.99866943, 0.99999998). For generation of initial values it takes about 20 seconds\nfor approximating 1F1 and its derivatives up to the degree 20 with an Intel Core i7 CPU.\nWith the initial value of x0 = 0.2 and step size 0.001, we obtain\nPr[l1 < 30] = 0.999545\nin about 75 seconds. The cumulative distribution function for this case is plotted on the\nright part of Figure 2. We see that enough accuracy is obtained even for m = 10 within\npractical amount of time.\nThe Laplace approximation fails to give a probability for the above two cases too as\nin m = 2 (see Section 3.1); it exceeds one.\n~ (30) is\nThe complexity of numerically solving the ODE for G\nO(m2m ) \u00d7 (steps of the Runge-Kutta method with a prescribed precision).\n~\nIn fact, since the matrix Pi (\u03b2x) has sparsity, each vector Pi (\u03b2x)G(x),\nwhich has 2m\n~\nelements, can be evaluated in O(2m ) steps at x from the values of G(x)\nby utilizing (28).\n20\n\n\f1.2\n\n1.2\nby hg\n\nby hg\n\n1\n\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n\n0\n0\n\n5\n\n10\n\n15\n\n20\n\n0\n\n5\n\n10\n\n15\n\n20\n\nFigure 2: Cumulative distributions for m = 5 and m = 10\n\n7\n\nDiscussion of open problems\n\nThe holonomic gradient method ([23]) gives a general algorithm for obtaining the partial\ndifferential equations satisfied by parametrized definite integrals such as the normalizing\nconstant of a family of probability distributions. In fact, in Nakayama et al. [23] and\nSei et al. [28] we used the holonomic gradient method for deriving the partial differential\nequations of the normalizing constants and for maximum likelihood estimation for distributions in directional statistics. For the case of 1F1 , the partial differential equations\nwere already derived by Muirhead [20] more than 40 years ago. Our use of those partial\ndifferential equations for numerical evaluation of 1F1 is very straightforward as discussed\nin Section 3 for the two dimensional case. Yet, from the viewpoint of holonomic functions, the partial differential equations of Muirhead [20] present many interesting open\nproblems.\nOne important question is to obtain the ordinary and partial differential equations\nfor the diagonal case as discussed in Section 3.2 for the case of m = 2. For a general\ndimension m > 2, it is desirable to be able to handle various patterns of diagonalization,\nsuch as the two-block diagonalization y1 = * * * = yl > yl+1 = * * * = ym . A direct \"by\nhand\" calculation using the l'H\u00f4pital rule becomes quickly infeasible when we increase m.\nAlso the use of the restriction algorithm for holonomic ideals is limited by computational\ncomplexity. It is in fact a very heavy algorithm. Currently the nk_restriction routine of\nRisa/Asir in Section 5.2 takes too much time for m \u2265 4. One possibility is to follow the\napproach in Muirhead [20] and Sugiyama et al. [33], where differentiation with respect to\nelementary symmetric functions of the roots of Y are\nQ considered. As discussed in Section\n5.2, we conjecture that the ideal I generated by j6=i (yi \u2212 yj )gi , i = 1, . . . , m in the\nWeyl algebra Dm is an holonomic ideal. Holonomicity guarantees the existence of partial\ndifferential equations for diagonal regions.\nAnother question is to consider the asymptotics for Pr[l1 \u2265 x] = 1 \u2212 Pr[l1 < x] as\nx \u2192 \u221e. As mentioned in the previous section, this tail probability can be approximated\nby the tube method ([15], [16]). One theoretical problem in applying the tube method\n21\n\n\fis that only the approximation for the tail probability itself has been justified and the\njustification of its derivatives has to be proved. However it is obvious that the current\napproach of taking the initial values close to the origin causes difficulty in precision for\nthe extreme upper tail probability, in the case we want to evaluate the small probability\nPr[l1 \u2265 x]. Hence it is desirable to be able to use tube formula approximation as the\ninitial values at x = \u221e.\nFrom computational viewpoint, our holonomic gradient method has exponential complexity in the dimension m. We need to keep the 2m -dimensional numerical vector F~\nin memory at each step of the iteration. For m = 20, the dimension of the vector is\nabout one million. Hence we do not expect that the current implementation of the holonomic gradient method works for m = 20. It might be possible to improve our current\nimplementation by fully exploiting the fact that 1F1 is a symmetric function in Y .\n\nA\n\nHolonomicity for dimension two\n\nIn the theory of holonomic functions, the holonomicity of the left ideal generated by the\nset of partial differential operators is an important question. In fact, the existence of the\nordinary differential equation with polynomial coefficients for the function restricted to\nthe diagonal region follows from the holonomicity. Holonomicity of the ideal generated by\ng1 , g2 in the two-dimensional case can be verified by Gr\u00f6bner basis computation. Here we\npresent this result. As to a general introduction to holonomic ideals and Gr\u00f6bner bases,\nwe refer to the Chapter 1 of Saito et al. [27].\nNote that the holonomicity on the non-diagonal region X follows from Theorem 2\nbecause the zero set of yi \u03bei2 = 0, i = 1, . . . , m contains the characteristic variety on X .\nThe holonomicity on X can also be proved by an analogous method with Ibukiyama et al.\n[9].\nP\nP\nLet D2 be the second Weyl algebra. For P = dk=0 \u03b11 +\u03b12 =k f\u03b11 ,\u03b12 (y1 , y2)\u22021\u03b11 \u22022\u03b12 \u2208\nD2 , we define in(P ) by\nX\nin(P ) =\nf\u03b11 ,\u03b12 (y1 , y2 )\u03be1\u03b11 \u03be2\u03b12 \u2208 C[y1 , y2 , \u03be1 , \u03be2],\n\u03b11 +\u03b12 =d\n\nwhere we assume that f\u03b11 ,\u03b12 (y1 , y2 ) \u2208 C[y1 , y2 ] and that f\u03b11 ,\u03b12 (y1 , y2 ) 6= 0 for some \u03b11 , \u03b12\nwith \u03b11 + \u03b12 = d. For a left ideal I of D2 , the characteristic variety ch(I) is defined by\nch(I) = {(y1 , y2 , \u03be1 , \u03be2) \u2208 C2*2 | \u2200P \u2208 I, in(P )(y1, y2 , \u03be1, \u03be2 ) = 0}.\nIt is a basic fact that the dimension of the characteristic variety ch(I) of the proper left\nideal I of D2 is greater than or equal to 2. A left ideal I of D2 is called holonomic if the\ndimension of the characteristic variety ch(I) equals 2.\nLet P1 = (y1 \u2212 y2 )g1 , P2 = (y2 \u2212 y1 )g2 and let I be the ideal of D2 generated by P1 and\nP2 . We will show that I is holonomic. Let S = y2 \u220222 P1 + y1 \u220212 P2 + c(\u22022 P1 + \u22021 P2 ) \u2208 I. By\n\n22\n\n\fdirect calculation we have\ny12\ny2\ny2\ny2\n\u2212 2y1 y2 )\u220212 \u22022 + (\u2212y12 y2 + y1 y22 \u2212 2y1 y2 + 2 )\u22021 \u220222 \u2212 1 \u220213 \u2212 2 \u220223\n2\n2\n2\n2\n3cy\n3cy\n2\n1\n\u2212 y1 )\u220212 + (\u2212y2 + 2ay2 \u2212 ay1 y2 + ay22 \u2212\n)\u220222\n+ (ay12 \u2212 ay1 y2 \u2212\n2\n2\n3cy1 3cy2\n\u2212\n+ y1 + y2 )\u22021 \u22022\n+ (\u2212cy12 + 2cy1 y2 \u2212 cy22 + 4y1 y2 \u2212\n2\n2\n+ (acy1 \u2212 acy2 + 2ay1 + cy1 \u2212 c2 )\u22021 + (\u2212c2 \u2212 acy1 + acy2 + cy2 )\u22022 + 2ac.\n\nS =(y12y2 \u2212 y1 y22 +\n\nHence\nin(S)\n\n=(y12y2\n\n\u2212\n\ny1 y22\n\ny12\ny22\ny12 3 y22 3\n2\n2\n2\n2\n+\n\u2212 2y1 y2 )\u03be1 \u03be2 + (\u2212y1 y2 + y1 y2 \u2212 2y1 y2 + )\u03be1 \u03be2 \u2212 \u03be1 \u2212 \u03be2 .\n2\n2\n2\n2\n\nConsider the ideal J of C[y1 , y2, \u03be1 , \u03be2 ] generated by in(P1 ), in(P2 ) and in(S). The following\nis a Gr\u00f6bner base of J with respect to the graded reverse lexicographic order with y1 >\ny2 > \u03be1 > \u03be2 :\n{y22 \u03be13 \u03be22 + 3y22\u03be12 \u03be23 + 3y22\u03be1 \u03be24 + y22 \u03be25 , y1y2 \u03be13 + 3y1 y2 \u03be12 \u03be2 + 3y22 \u03be1 \u03be22 + y22\u03be23 ,\ny12 \u03be12 \u2212 y1 y2 \u03be12, y1 y2 \u03be22 \u2212 y22\u03be22 }.\nThus the Krull dimension of J is 2. Since {P1 , P2 , S} \u2282 I, the characteristic variety ch(I)\nof I is contained in the zero set of J. This implies that the dimension of ch(I) does not\nexceed 2 and hence the dimension of ch(I) is equal to 2. Therefore I is holonomic for\nm = 2.\n\nB\n\nR source program for dimension two\n\nThe following program in data analysis system R implements holonomic gradient method\nfor m = 2 of Section 3.1 based on deSolve add-on package for R.\nlibrary(deSolve)\nm <- 2\n# dimension\nn <- 3\n# degrees of freedom\nx <- 4.31600 # specify x.\nWe evaluate Pr( l1 < x )\nb1 <- 1; b2 <- 2 # (b1,b2) = (1/2) diag(Sigma^{-1})\na <- (m+1)/2; c <- (n+m+1)/2; totalsteps <- 10000; stepsize <- x/totalsteps\n# h's\nh2000\nh2010\nh2001\nh1200\nh1210\nh1201\nh1211\n\n<<<<<<<-\n\nfunction(y1,y2)\nfunction(y1,y2)\nfunction(y1,y2)\nfunction(y1,y2)\nfunction(y1,y2)\nfunction(y1,y2)\nfunction(y1,y2)\n\na/y1\n-(c-y1)/y1 - y2/(2*y1*(y1-y2))\ny2/(2*y1*(y1-y2))\na/(2*y2*(y2-y1))\n3/(4*(y2-y1)^2) + a/y2 - (c-y1)/(2*y2*(y2-y1))\n-3/(4*(y2-y1)^2)\n-(c-y2)/y2 - y1/(2*y2*(y2-y1))\n\n#initial values\nx1 <- b1*stepsize; x2 <- b2*stepsize\nfi <- c(a/c, (a*(a+1))/(c*(c+1)), (a*(a+1))/(3*c*(c+1)) + (2*a*(a-1/2))/(3*c*(c-1/2)),\n\n23\n\n\f(a*(a+1)*(a+2))/(5*c*(c+1)*(c+2)) + (4*a*(a+1)*(a-1/2))/(5*c*(c+1)*(c-2/1)))\nfi <- c(1+(x1+x2)*fi[1], fi[1]+x1*fi[2]+x2*fi[3], fi[1]+x2*fi[2]+x1*fi[3], fi[3]+(x1+x2)*fi[4])\n# gradient\nf11m2 <- function(y,fv,parm){y1 <- y*b1; y2 <- y*b2;\nlist(c(\nb1*fv[2] + b2*fv[3],\nb1*(fv[1]*h2000(y1,y2)+fv[2]*h2010(y1,y2)+fv[3]*h2001(y1,y2)) + b2*fv[4],\nb2*(fv[1]*h2000(y2,y1)+fv[2]*h2001(y2,y1)+fv[3]*h2010(y2,y1)) + b1*fv[4],\nb1*(fv[1]*h1200(y2,y1)+fv[2]*h1201(y2,y1)+fv[3]*h1210(y2,y1)+fv[4]*h1211(y2,y1))\n+b2*(fv[1]*h1200(y1,y2)+fv[2]*h1210(y1,y2)+fv[3]*h1201(y1,y2)+fv[4]*h1211(y1,y2)))\n)}\noutput <- ode(fi,func=f11m2,(1:totalsteps)*x/totalsteps)\nprob0 <- ((b1*b2)^(n/2)*gamma(a)*gamma(a-1/2))/(gamma(c)*gamma(c-1/2)) * x^(n*m/2) * exp(-x*(b1+b2))\ncat(\"x=\",x, \"prob=\", output[totalsteps,2]*prob0,\"\\n\")\n\nReferences\n[1] Uri M. Ascher and Linda R. Petzold. Computer Methods for Ordinary Differential\nEquations and Differential-Algebraic Equations. Society for Industrial and Applied\nMathematics (SIAM), Philadelphia, PA, 1998. ISBN 0-89871-412-5.\n[2] Ronald Butler and Robert Paige. Exact distributional computations for Roy's statistic and the largest eigenvalue of a Wishart distribution. Statistics and Computing,\n21:147\u2013157, 2011. ISSN 0960-3174.\n[3] Ronald W. Butler and Andrew T. A. Wood. Laplace approximations for hypergeometric functions with matrix argument. Ann. Statist., 30(4):1155\u20131177, 2002. ISSN\n0090-5364.\n[4] A. G. Constantine. Some non-central distribution problems in multivariate analysis.\nAnn. Math. Statist., 34:1270\u20131285, 1963. ISSN 0003-4851.\n[5] Kenneth I. Gross and Donald St. P. Richards. Special functions of matrix argument.\nI. Algebraic induction, zonal polynomials, and hypergeometric functions. Trans.\nAmer. Math. Soc., 301(2):781\u2013811, 1987. ISSN 0002-9947.\n[6] R. D. Gupta and D. St. P. Richards. Hypergeometric functions of scalar matrix\nargument are expressible in terms of classical hypergeometric functions. SIAM J.\nMath. Anal., 16:852\u2013858, 1985. ISSN 1095-7154.\n[7] Hiroki Hashiguchi and Naoto Niki. Numerical computation on distributions of the\nlargest and the smallest latent roots of the Wishart matrix. J. Japanese Soc. Comput.\nStatist., 19(1):45\u201356, 2006. ISSN 0915-2350.\n[8] Carl S. Herz. Bessel functions of matrix argument. Ann. of Math. (2), 61:474\u2013523,\n1955. ISSN 0003-486X.\n\n24\n\n\f[9] Tomoyoshi Ibukiyama, Takako Kuzumaki, and Hiroyuki Ochiai. Holonomic systems\nof Gegenbauer type polynomials of matrix arguments related with Siegel modular\nforms. Journal of the Mathematical Society of Japan. In Press.\n[10] Alan T. James. Distributions of matrix variates and latent roots derived from normal\nsamples. Ann. Math. Statist., 35:475\u2013501, 1964. ISSN 0003-4851.\n[11] Alan T. James. Calculation of zonal polynomial coefficients by use of the LaplaceBeltrami operator. Ann. Math. Statist, 39:1711\u20131718, 1968. ISSN 0003-4851.\n[12] I.M. Johnstone. On the distribution of the largest eigenvalue in principal components\nanalysis. The Annals of statistics, 29(2):295\u2013327, 2001.\n[13] I.M. Johnstone. Multivariate analysis and jacobi ensembles: Largest eigenvalue,\ntracy\u2013widom limits and rates of convergence. The Annals of statistics, 36(6):2638\u2013\n2716, 2008.\n[14] Plamen Koev and Alan Edelman. The efficient evaluation of the hypergeometric\nfunction of a matrix argument. Math. Comp., 75(254):833\u2013846, 2006. ISSN 00255718.\n[15] Satoshi Kuriki and Akimichi Takemura. Tail probabilities of the maxima of multilinear forms and their applications. Ann. Statist., 29(2):328\u2013371, 2001. ISSN 0090-5364.\n[16] Satoshi Kuriki and Akimichi Takemura. Euler characteristic heuristic for approximating the distribution of the largest eigenvalue of an orthogonally invariant random\nmatrix. J. Statist. Plann. Inference, 138(11):3357\u20133378, 2008. ISSN 0378-3758.\n[17] Vadim B. Kuznetsov and Siddhartha Sahi, editors. Jack, Hall-Littlewood and Macdonald Polynomials, volume 417 of Contemporary Mathematics, Providence, RI, 2006.\nAmerican Mathematical Society. ISBN 0-8218-3683-8.\n[18] I. G. Macdonald. Symmetric Functions and Hall Polynomials. Oxford Mathematical Monographs. The Clarendon Press Oxford University Press, New York, second\nedition, 1995. ISBN 0-19-853489-2. With contributions by A. Zelevinsky, Oxford\nScience Publications.\n[19] A. M. Mathai, Serge B. Provost, and Takesi Hayakawa. Bilinear Forms and Zonal\nPolynomials, volume 102 of Lecture Notes in Statistics. Springer-Verlag, New York,\n1995. ISBN 0-387-94522-9.\n[20] R. J. Muirhead. Systems of partial differential equations for hypergeometric functions\nof matrix argument. Ann. Math. Statist., 41:991\u20131001, 1970. ISSN 0003-4851.\n[21] Robb J. Muirhead. Latent roots and matrix variates: a review of some asymptotic\nresults. Ann. Statist., 6(1):5\u201333, 1978. ISSN 0090-5364.\n\n25\n\n\f[22] Robb J. Muirhead. Aspects of Multivariate Statistical Theory. John Wiley & Sons\nInc., New York, 1982. ISBN 0-471-09442-0. Wiley Series in Probability and Mathematical Statistics.\n[23] Hiromasa Nakayama, Kenta Nishiyama, Masayuki Noro, Katsuyoshi Ohara,\nTomonari Sei, Nobuki Takayama, and Akimichi Takemura. Holonomic gradient\ndescent and its application to the Fisher-Bingham integral. Advances in Applied\nMathematics, 47:639\u2013658, 2011. ISSN 0196-8858.\n[24] Toshinori Oaku. Algorithms for b-functions, restrictions, and algebraic local cohomology groups of D-modules. Adv. in Appl. Math., 19(1):61\u2013105, 1997. ISSN 0196-8858.\n[25] D. St. P. Richards. Functions of matrix argument. In NIST Handbook of Mathematical\nFunctions, pages 767\u2013774. U.S. Dept. Commerce, Washington, DC, 2010.\n[26] RisaAsir developing team. Risa/asir, a computer algebra system.\nhttp://www.math.kobe-u.ac.jp/Asir/.\n\nAvailable at\n\n[27] Mutsumi Saito, Bernd Sturmfels, and Nobuki Takayama. Gr\u00f6bner Deformations of\nHypergeometric Differential Equations, volume 6 of Algorithms and Computation in\nMathematics. Springer-Verlag, Berlin, 2000. ISBN 3-540-66065-8.\n[28] Tomonari Sei, Hiroki Shibata, Akimichi Takemura, Katsuyoshi Ohara, and Nobuki\nTakayama. Properties and applications of Fisher distribution on the rotation group,\n2011. arXiv:1110.0721v1.\n[29] Richard P. Stanley. Some combinatorial properties of Jack symmetric functions. Adv.\nMath., 77(1):76\u2013115, 1989. ISSN 0001-8708.\n[30] W. A. Stein et al. Sage Mathematics Software (Version 4.7.2). The Sage Development\nTeam, 2011. http://www.sagemath.org.\n[31] Takakazu Sugiyama. On the distribution of the largest latent root of the covariance\nmatrix. Ann. Math. Statist, 38:1148\u20131151, 1967. ISSN 0003-4851.\n[32] Takakazu Sugiyama. Approximation for the distribution of the largest latent root of\na Wishart matrix. Austral. J. Statist., 14:17\u201324, 1972. ISSN 0004-9581.\n[33] Takakazu Sugiyama, Yuuichi Takeda, and Masafumi Fukuda. On the numerical\ncomputation of confluent hypergeometric function with zonal polynomials of order\n3. J. Japanese Soc. Computat. Statist., 11(1):1\u20138, 1998. ISSN 0915-2350.\n[34] Akimichi Takemura. Zonal Polynomials. Institute of Mathematical Statistics Lecture\nNotes-Monograph Series, 4. Institute of Mathematical Statistics, Hayward, CA,\n1984. ISBN 0-940600-05-6.\n\n26\n\n\f[35] Akimichi Takemura and Yo Sheena. Distribution of eigenvalues and eigenvectors\nof Wishart matrix when the population eigenvalues are infinitely dispersed and its\napplication to minimax estimation of covariance matrix. J. Multivariate Anal., 94\n(2):271\u2013299, 2005. ISSN 0047-259X.\n[36] Doron Zeilberger. A holonomic systems approach to special functions identities. J.\nComput. Appl. Math., 32(3):321\u2013368, 1990. ISSN 0377-0427.\n\n27\n\n\f"}