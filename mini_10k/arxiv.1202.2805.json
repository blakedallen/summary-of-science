{"id": "http://arxiv.org/abs/1202.2805v2", "guidislink": true, "updated": "2013-04-22T13:23:44Z", "updated_parsed": [2013, 4, 22, 13, 23, 44, 0, 112, 0], "published": "2012-02-13T18:01:59Z", "published_parsed": [2012, 2, 13, 18, 1, 59, 0, 44, 0], "title": "D-ADMM: A Communication-Efficient Distributed Algorithm For Separable\n  Optimization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.0364%2C1202.5682%2C1202.0456%2C1202.0546%2C1202.0837%2C1202.6578%2C1202.5557%2C1202.1726%2C1202.3831%2C1202.4707%2C1202.1317%2C1202.2805%2C1202.4985%2C1202.3019%2C1202.1732%2C1202.1770%2C1202.2464%2C1202.3326%2C1202.2281%2C1202.4305%2C1202.6277%2C1202.0513%2C1202.0312%2C1202.2466%2C1202.4743%2C1202.6563%2C1202.2223%2C1202.4038%2C1202.0109%2C1202.3595%2C1202.2652%2C1202.2087%2C1202.5080%2C1202.4807%2C1202.2188%2C1202.0644%2C1202.4143%2C1202.3104%2C1202.2419%2C1202.0732%2C1202.4606%2C1202.4064%2C1202.3187%2C1202.1782%2C1202.0219%2C1202.1172%2C1202.4981%2C1202.6057%2C1202.5071%2C1202.3793%2C1202.3505%2C1202.0813%2C1202.4750%2C1202.5738%2C1202.2638%2C1202.0009%2C1202.4419%2C1202.4942%2C1202.3383%2C1202.4381%2C1202.5857%2C1202.4116%2C1202.5832%2C1202.2835%2C1202.0555%2C1202.1086%2C1202.4837%2C1202.3059%2C1202.4412%2C1202.1962%2C1202.2278%2C1202.2687%2C1202.4259%2C1202.3811%2C1202.5171%2C1202.2345%2C1202.5229%2C1202.5509%2C1202.5151%2C1202.1464%2C1202.0430%2C1202.3118%2C1202.1436%2C1202.3572%2C1202.2073%2C1202.3172%2C1202.6157%2C1202.3659%2C1202.0270%2C1202.1663%2C1202.2258%2C1202.3614%2C1202.1700%2C1202.0512%2C1202.3447%2C1202.4080%2C1202.3530%2C1202.4790%2C1202.2426%2C1202.5784%2C1202.1980&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "D-ADMM: A Communication-Efficient Distributed Algorithm For Separable\n  Optimization"}, "summary": "We propose a distributed algorithm, named Distributed Alternating Direction\nMethod of Multipliers (D-ADMM), for solving separable optimization problems in\nnetworks of interconnected nodes or agents. In a separable optimization problem\nthere is a private cost function and a private constraint set at each node. The\ngoal is to minimize the sum of all the cost functions, constraining the\nsolution to be in the intersection of all the constraint sets. D-ADMM is proven\nto converge when the network is bipartite or when all the functions are\nstrongly convex, although in practice, convergence is observed even when these\nconditions are not met. We use D-ADMM to solve the following problems from\nsignal processing and control: average consensus, compressed sensing, and\nsupport vector machines. Our simulations show that D-ADMM requires less\ncommunications than state-of-the-art algorithms to achieve a given accuracy\nlevel. Algorithms with low communication requirements are important, for\nexample, in sensor networks, where sensors are typically battery-operated and\ncommunicating is the most energy consuming operation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.0364%2C1202.5682%2C1202.0456%2C1202.0546%2C1202.0837%2C1202.6578%2C1202.5557%2C1202.1726%2C1202.3831%2C1202.4707%2C1202.1317%2C1202.2805%2C1202.4985%2C1202.3019%2C1202.1732%2C1202.1770%2C1202.2464%2C1202.3326%2C1202.2281%2C1202.4305%2C1202.6277%2C1202.0513%2C1202.0312%2C1202.2466%2C1202.4743%2C1202.6563%2C1202.2223%2C1202.4038%2C1202.0109%2C1202.3595%2C1202.2652%2C1202.2087%2C1202.5080%2C1202.4807%2C1202.2188%2C1202.0644%2C1202.4143%2C1202.3104%2C1202.2419%2C1202.0732%2C1202.4606%2C1202.4064%2C1202.3187%2C1202.1782%2C1202.0219%2C1202.1172%2C1202.4981%2C1202.6057%2C1202.5071%2C1202.3793%2C1202.3505%2C1202.0813%2C1202.4750%2C1202.5738%2C1202.2638%2C1202.0009%2C1202.4419%2C1202.4942%2C1202.3383%2C1202.4381%2C1202.5857%2C1202.4116%2C1202.5832%2C1202.2835%2C1202.0555%2C1202.1086%2C1202.4837%2C1202.3059%2C1202.4412%2C1202.1962%2C1202.2278%2C1202.2687%2C1202.4259%2C1202.3811%2C1202.5171%2C1202.2345%2C1202.5229%2C1202.5509%2C1202.5151%2C1202.1464%2C1202.0430%2C1202.3118%2C1202.1436%2C1202.3572%2C1202.2073%2C1202.3172%2C1202.6157%2C1202.3659%2C1202.0270%2C1202.1663%2C1202.2258%2C1202.3614%2C1202.1700%2C1202.0512%2C1202.3447%2C1202.4080%2C1202.3530%2C1202.4790%2C1202.2426%2C1202.5784%2C1202.1980&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose a distributed algorithm, named Distributed Alternating Direction\nMethod of Multipliers (D-ADMM), for solving separable optimization problems in\nnetworks of interconnected nodes or agents. In a separable optimization problem\nthere is a private cost function and a private constraint set at each node. The\ngoal is to minimize the sum of all the cost functions, constraining the\nsolution to be in the intersection of all the constraint sets. D-ADMM is proven\nto converge when the network is bipartite or when all the functions are\nstrongly convex, although in practice, convergence is observed even when these\nconditions are not met. We use D-ADMM to solve the following problems from\nsignal processing and control: average consensus, compressed sensing, and\nsupport vector machines. Our simulations show that D-ADMM requires less\ncommunications than state-of-the-art algorithms to achieve a given accuracy\nlevel. Algorithms with low communication requirements are important, for\nexample, in sensor networks, where sensors are typically battery-operated and\ncommunicating is the most energy consuming operation."}, "authors": ["Jo\u00e3o F. C. Mota", "Jo\u00e3o M. F. Xavier", "Pedro M. Q. Aguiar", "Markus P\u00fcschel"], "author_detail": {"name": "Markus P\u00fcschel"}, "author": "Markus P\u00fcschel", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TSP.2013.2254478", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1202.2805v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.2805v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "To appear in IEEE Transactions on Signal Processing", "arxiv_primary_category": {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.2805v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.2805v2", "journal_reference": "IEEE Transactions on Signal Processing, Vol. 61, No. 10, pp.\n  2718-1723, 2013", "doi": "10.1109/TSP.2013.2254478", "fulltext": "1\n\nD-ADMM: A Communication-Efficient Distributed Algorithm\nFor Separable Optimization\nJo\u00e3o F. C. Mota, Jo\u00e3o M. F. Xavier, Pedro M. Q. Aguiar, and Markus P\u00fcschel\n\narXiv:1202.2805v2 [math.OC] 22 Apr 2013\n\nf10 , X10\nAbstract-We propose a distributed algorithm, named Distributed\nAlternating Direction Method of Multipliers (D-ADMM), for solving\nseparable optimization problems in networks of interconnected nodes\nor agents. In a separable optimization problem there is a private cost\nfunction and a private constraint set at each node. The goal is to minimize\nthe sum of all the cost functions, constraining the solution to be in the\nintersection of all the constraint sets. D-ADMM is proven to converge\nwhen the network is bipartite or when all the functions are strongly\nconvex, although in practice, convergence is observed even when these\nconditions are not met. We use D-ADMM to solve the following problems\nfrom signal processing and control: average consensus, compressed\nsensing, and support vector machines. Our simulations show that DADMM requires less communications than state-of-the-art algorithms\nto achieve a given accuracy level. Algorithms with low communication\nrequirements are important, for example, in sensor networks, where\nsensors are typically battery-operated and communicating is the most\nenergy consuming operation.\nIndex Terms-Distributed algorithms, alternating direction method of\nmultipliers, sensor networks.\n\nI. I NTRODUCTION\nIn this paper, we propose a distributed algorithm for solving\nseparable optimization problems:\nminimize\n\nf1 (x) + f2 (x) + * * * + fP (x)\n\nsubject to\n\nx \u2208 X1 \u2229 X2 \u2229 * * * \u2229 XP ,\n\nx\n\n(1)\n\nwhere x \u2208 Rn is the global optimization variable, and x\u22c6 will denote\nany solution of (1). As illustrated in Fig. 1, we associate a network\nof P nodes with problem (1), where only node p has access to its\nprivate cost function fp and to its private set Xp . Each node can only\ncommunicate with its neighbors, but all of them have to solve (1) in\na cooperative way. We call any method that solves (1) without using\na central node and without aggregating data at any specific location\na distributed algorithm.\nContributions. The goal of this paper is twofold: to show that the\nrecent distributed algorithm proposed in [1] for a specific problem\ncalled Basis Pursuit can be generalized to solve the class (1); and\nto show that, for many problems of interest, the resulting algorithm\nrequires usually significant less communications than prior distributed\nalgorithms to achieve a given solution accuracy. This also includes\nalgorithms that were specifically designed for a particular problem\nand are not applicable to the entire problem class (1). Algorithms\nwith low communication cost are relevant, for example, in sensor\nnetworks where communication is often the most energy-consuming\ntask and the nodes rely on batteries [2], [3].\nJo\u00e3o F. C. Mota, Jo\u00e3o, M. F. Xavier, and Pedro M. Q. Aguiar are with\nInstituto de Sistemas e Rob\u00f3tica (ISR), Instituto Superior T\u00e9cnico (IST),\nTechnical University of Lisbon, Portugal.\nMarkus P\u00fcschel is with the Department of Computer Science at ETH\nZurich, Switzerland.\nJo\u00e3o F. C. Mota is also with the Department of Electrical and Computer\nEngineering at Carnegie Mellon University, USA.\nThis work was supported by the following grants from Funda\u00e7\u00e3o\npara a Ci\u00eancia e Tecnologia (FCT): CMU-PT/SIA/0026/2009, PEstOE/EEI/LA0009/2011, and SFRH/BD/33520/2008 (through the Carnegie\nMellon/Portugal Program managed by ICTI).\n\nf1 , X1\n\n10\n1\n\nf7 , X7\n\nf6 , X6\n7\n6\n\nf2 , X2\n2\n\n5\n4\n\nf5 , X5\n\nf4 , X4\n\n9\n8\n3\n\nf3 , X3\n\nf9 , X9\n\nf8 , X8\n\nFigure 1. Network with P = 10 nodes. Node p only knows fp and Xp ,\nbut cooperates with its neighbors in order to solve (1).\n\nFormal problem statement. Given a network with P nodes, we\nassociate each fp and Xp in (1) with the pth node of the network.\nWe make the following assumptions:\nAssumptions.\n1) Each fp : Rn \u2212\n\u2192 R is a convex function over Rn , and each\nset Xp is closed and convex.\n2) Problem (1) is solvable.\n3) The network is connected and it does not vary with time.\n4) A coloring scheme of the network is available.\nAssumption 2) implies that (1) has at least one solution x\u22c6 . In\nAssumption 3), a network is connected if there is a path between\nevery pair of nodes. Finally, in Assumption 4), a coloring scheme\nis an assignment of numbers to the nodes of the network such that\nno adjacent nodes have the same number. These numbers are usually\ncalled colors, and they will be used to set up our distributed algorithm.\nNote that, in wireless scenarios, coloring schemes are often used in\nMedia Access Control (MAC) protocols to determine the nodes' order\nof communication.\nUnder the previous assumptions, we solve the following problem:\ngiven a network, design a distributed algorithm that solves (1). By\n\"distributed\" we mean there is no notion of a central or special node\nand each node communicates only with its neighbors; also, only\nnode p has access to fp or Xp at any time during or before the\nalgorithm.\nOur solution for this problem relies on the Alternating Direction\nMethod of Multipliers (ADMM), which has become very popular in\nrecent years; see [4] for a survey. Specifically, we use an extended\nversion of ADMM, whose proof of convergence was recently established in [5]. This result will also guarantee the convergence of our\nalgorithm for some problems of interest.\nRelated work. Gradient and subgradient methods, including incremental versions, are long known to yield distributed algorithms (in\nthe sense defined before); see, e.g., [6], [7], [8]. Advantages of these\nmethods are computational simplicity at each node and theoretical\nrobustness guarantees. However, they generally require too many\niterations (and hence communications) to converge.\nAugmented Lagrangian methods have also been used for dis-\n\n\f2\n\ntributed optimization, e.g., [9], [10], [11]. They consist of two loops:\nan outer loop updating the dual variables, and an inner loop updating\nthe primal variables. The most common outer loop algorithm is the\ngradient method, yielding the method of multipliers. For the inner\nloop, common choices are Gauss-Seidel and Jacobi methods.\nThe Alternating Direction Method of Multipliers (ADMM) [4] is\nan augmented Lagrangian-based algorithm that consists of only one\nloop. ADMM is not directly applicable to (1): one has to reformulate\nthat problem first. Possible reformulations were addressed in [12]\nand [13], yielding algorithms that require two and one communication\nsteps per ADMM iteration, respectively. Other work that explores\nthese algorithms for particular instances of (1) include [14], [15],\n[16], [17], [18]. The algorithm we propose is also based on ADMM\n(on an extended version), but applied to a different reformulation\nof (1). Our simulations show that the proposed algorithm requires\nless communications than any of the previous approaches.\nAll the above algorithms solve (1) in a distributed way. There are,\nhowever, other algorithms that solve (1), but are not distributed in\nour sense. For example, the algorithm in [4, \u00a77.2] solves (1), but it\nrequires an all-to-all communication in each iteration; this can only\nbe accomplished in networks that are fully connected or that have a\ncentral node. In contrast, our algorithm and the ones described above\nare distributed and can run on any connected network topology.\nII. A LGORITHM D ERIVATION\nTo derive the algorithm, we reformulate (1) to make ADMM\napplicable. As mentioned before, several reformulations are possible:\nours takes advantage of node coloring. First we introduce some\nnotation.\nNetwork notation. Networks are represented as undirected\ngraphs G = (V, E ), where V = {1, 2, . . . , P } is the set of nodes\nand E \u2286 V \u00d7 V is the set of edges. The cardinality of these sets is\nrepresented respectively by P and E. An edge is represented by (i, j),\nwith i < j, and (i, j) \u2208 E means that nodes i and j can exchange\ndata with each other. We define the neighborhood Np of a node p as\nthe set of nodes connected to node p, but excluding it; the cardinality\nof this set, Dp := |Np |, is the degree of node p.\nColoring. We assume the network is given together with a coloring\nscheme of C colors. The set of nodes that have color c will be denoted\nwith Cc , for c = 1, . . . , C, and its cardinality with Cc = |Cc |. Note\nthat {Cc }C\nc=1 partitions V.\nProblem manipulations. Without loss of generality, assume the\nnodes are ordered such that the first C1 nodes have color 1, the\nnext C2 nodes have color 2, and so on, i.e., C1 = {1, 2, . . . , C1 },\nC2 = {C1 + 1, C1 + 2, . . . , C1 + C2 }, . . . . We decouple problem (1)\nby assigning copies of the global variable x to each node and then\nconstrain all copies to be equal. Let xp \u2208 Rn denote the copy held\nby node p. As in [13], we constrain all copies to be equal in an\nedge-based way, and rewrite (1) as\n\nith and jth entry, respectively; the remaining entries are zeros.\nassumption\ninduces a natural partition of B as\n\u0002Our\u22a4 numbering\n\u0003\n\u22a4 \u22a4\n, where the columns of Bc\u22a4 are associated to\nB1 B2\u22a4 * * * BC\nthe nodes with color c. We partition x\u0304 similarly: x\u0304 = (x\u03041 , . . . , x\u0304C ),\nwhere x\u0304c \u2208 (Rn )Cc collects the copies of all nodes with color c.\nThis enables rewriting (2) as\nPC P\nminimize\nc=1\np\u2208Cc fp (xp )\nx\u03041 ,...,x\u0304C\n(3)\nsubject to x\u0304c \u2208 X\u0304c , c = 1, . . . , C\nPC\n\u22a4\n(B\n\u2297\nI\n)x\u0304\n=\n0\n,\nn\nc\nc\nc=1\nQ\nwhere X\u0304c :=\np\u2208Cc Xp . Problem (3) can be solved with the\nExtended ADMM, explained next.\nExtended ADMM. The Extended ADMM is a natural generalization of the Alternating Direction Method of Multipliers (ADMM) [5].\nGiven C functions gc , C sets Xc , and C matrices Ac , all with the\nsame number of rows, the extended ADMM solves\nPC\nminimize\nc=1 gc (xc )\nx1 ,...,xC\n(4)\nsubject to xc \u2208 Xc , c = 1, . . . , C\nPC\nc=1 Ac xc = 0 ,\nwhere x := (x1 , . . . , xC ) is the optimization variable. The extended\nADMM consists of iterating on k:\nxk+1\n= arg min L\u03c1 (x1 , xk2 , . . . , xkP ; \u03bbk )\n1\n\n(5)\n\nxk+1\n= arg min L\u03c1 (xk+1\n, x2 , xk3 , . . . , xkC ; \u03bbk )\n2\n1\n\n(6)\n\nx1 \u2208X1\n\nx2 \u2208X2\n\n..\n.\nk\nxk+1\n= arg min L\u03c1 (xk+1\n, xk+1\n, . . . , xk+1\n1\n2\nC\nC\u22121 , xC ; \u03bb )\n\n(7)\n\nxC \u2208XC\n\n\u03bbk+1 = \u03bbk + \u03c1\n\nC\nX\n\nAc xk+1\n,\nc\n\n(8)\n\nc=1\n\n\u0001 \u03c1 PC\nP\n2\n\u22a4\nwhere L\u03c1 (x; \u03bb) = C\nis\nc=1 gc (xc ) + \u03bb Ac xc + 2\nc=1 Ac xc\nthe augmented Lagrangian of (4), \u03bb is the dual variable, and \u03c1 is\na positive parameter. When C = 2, (5)-(8) becomes the ordinary\nADMM and it converges under very mild assumptions. When C > 2,\nthere is only a known proof of convergence when all the functions gc\nare strongly convex [5]. In particular, the following theorem holds.\nTheorem 1 ([19], [5]). Let gc : Rnc \u2212\n\u2192 R be a convex function\nover Rnc , Xc \u2286 Rnc a closed convex set, and Ac an m \u00d7 nc matrix,\nfor c = 1, . . . , C. Assume (4) is solvable and that either\n1) C = 2 and each Ac has full column-rank,\n2) or C \u2265 2 and each fc is strongly convex.\n\n(2)\n\nThen, the sequence {(xk1 , . . . , xkC , \u03bbk )} generated by (5)-(8) converges to (x\u22c61 , . . . , x\u22c6C , \u03bb\u22c6 ), where (x\u22c61 , . . . , x\u22c6C ) solves (4) and \u03bb\u22c6\nsolves the dual problem of (4): max\u03bb G1 (\u03bb) + * * * + GC (\u03bb),\nwhere Gc (\u03bb) = inf xc \u2208Xc (gc (xc ) + \u03bb\u22a4 Ac xc ), for c = 1, . . . , C.\n\nwhere x\u0304 = (x1 , . . . , xP ) \u2208 (Rn )P is the optimization variable.\nProblem (2) is no longer coupled by a global variable, as (1), but\ninstead by the new equations xi = xj , for all the pairs (i, j) \u2208 E .\nThese equations enforce all copies to be equal since the network is\nconnected (cf. Assumption 3)). Note that these constraints can be\nwritten more compactly as (B \u22a4 \u2297 In )x\u0304 = 0, where B \u2208 RP \u00d7E\nis the node arc-incidence matrix of the graph, In is the identity\nmatrix in Rn , and \u2297 is the Kronecker product. Each column of B\nis associated with an edge (i, j) \u2208 E and has 1 and \u22121 in the\n\nA proof for case 1) can be found in [19], which generalizes the\nproofs of [9], [4]. A proof for case 2) can be found in [5]. It is\nbelieved that (5)-(8) still converges when C > 2 and each Ac has full\ncolumn-rank, i.e., that the generalization of Theorem 1 under case 1)\nstill holds [1], [5], [20]. Recently, [21] proved that if we replace \u03c1\nin (8) by a small constant, the resulting algorithm converges linearly.\nApplying the extended ADMM. We now apply the extended\nADMM to problem (3), which has the format of (4). We start\nby showing that the cth optimization problem in (5)-(7) yields Cc\noptimization problems that can be solved in parallel. For example,\n\nminimize\n\nf1 (x1 ) + f2 (x2 ) + * * * + fP (xP )\n\nsubject to\n\nxp \u2208 Xp , p = 1, . . . , P\nxi = xj , (i, j) \u2208 E ,\n\nx\u0304=(x1 ,...,xP )\n\n\f3\n\nx\u03041 = (x1 , . . . , xC1 ) is updated as\nx\u0304k+1\n= arg min\n1\nx\u03041 \u2208X\u03041\n\nX\n\n\u22a4\n\nfp (xp ) + \u03bbk A1 x\u03041 +\n\np\u2208C1\n\nC\nX\n\n\u03c1\nAc x\u0304kc\nA1 x\u03041 +\n2\nc=2\n\n2\n\n,\n\n(9)\n\nwhere A1 = B1\u22a4 \u2297 In . The last term in (9) can be written as\nC\nC\nX\n\u03c1 \u22a4 \u22a4\n\u03c1 X\nk\nAc x\u0304kc\nx\u03041 A1 A1 x\u03041 + \u03c1 x\u0304\u22a4\nA\u22a4\n1\n1 Ac x\u0304c +\n2\n2\nc=2\nc=2\n\n2\n\n.\n\n(10)\n\n\u22a4\n\u22a4\nIn the first term, A\u22a4\n1 A1 = B1 B1 \u2297 In , where B1 B1 is a diagonal\nblock of the graph Laplacian. Since the nodes with color 1 are not\nneighbors between themselves, B1 B1\u22a4 will be a diagonal matrix,\nwith the degrees of\nPthe respective 2nodes in the diagonal. This means\n\u22a4\nx\u0304\u22a4\n1 A1 A1 x\u03041 =\np\u2208C1 Dp kxp k . Similarly, in the second term,\n\u22a4\n\u22a4\nA\u22a4\nA\n=\nB\nB\n\u2297\nI\nc\n1 c\nn , where B1 Bc corresponds to an off-diagonal\n1\nblock of the Laplacian matrix. For i 6= j, the ijth entry of the Laplacian matrix contains \u22121P\nif nodes i and j are P\nneighbors,\nP and 0\u22a4othC\nk\n\u22a4\nk\nerwise. This implies x\u0304\u22a4\n1\nj\u2208Np xp xj .\nc=2 A1 Ac x\u0304c = \u2212\np\u2208C1\nFinally, the last term of (10) does not depend on x\u03041 and can be\nignored from the optimization problem. Thus, (9) simplifies to\n\u0010\nX k \u0011\u22a4\nX\n\u03c1Dp\nfp (xp ) + \u03b3pk \u2212 \u03c1\nxj xp +\nx\u0304k+1\n= arg min\nkxp k2 ,\n1\n2\nx\u03041 \u2208X\u03041 p\u2208C\nj\u2208Np\n1\n(11)\nP\nk\nwhere \u03b3pk :=\nj\u2208Np \u03bbpj comes from the second term in (9):\nP\nP\nk \u22a4\n((B1 \u2297 In )\u03bbk )\u22a4 x\u03041 =\nj\u2208Np \u03bbpj xp . We decomposed \u03bb\np\u2208C1\nedge-wise: \u03bb = (. . . , \u03bbij , . . .), where \u03bbij is defined for i < j and\nassociated to the constraint xi = xj in (2). It is now clear that (11)\ndecomposes into C1 problems that can be solved in parallel. For\nthe other colors, we can apply a similar reasoning, but we must be\ncareful defining \u03b3pk , due\nnumbering. Its general\nP to the nodes' relative\nk\ndefinition is \u03b3pk :=\nsign(j\n\u2212\np)\u03bb\n,\nwhere sign(a) = 1,\npj\nj\u2208Np\nif a \u2265 0, and sign(a) = \u22121, otherwise. Note that we extended the\ndefinition of \u03bbij for i > j such that \u03bbij := \u03bbji . Algorithm 1 shows\nthe resulting algorithm, named Distributed-ADMM, or D-ADMM.\n\nAlgorithm 1 is asynchronous in the sense that nodes operate in\na color-based order, with nodes with the same color operating in\nparallel. Since nodes with the same color are not neighbors, we would\napparently need some kind of coordination to execute the algorithm.\nActually, such coordination is not needed provided each node knows\nits own color and the colors of its neighbors. In fact, as soon as node p\nhas received xk+1\nfrom all its neighbors with lower colors, node p\nj\ncan \"work,\" since step 4 (and subsequently step 5) can be performed.\nIn conclusion, knowing its own and its neighbors' colors provides an\nautomatic coordination mechanism. Regarding the convergence of DADMM, we have:\nCorollary 1. Let Assumptions 1) - 4) hold. Then, Algorithm 1 produces a sequence (xk1 , . . . , xkP ) convergent to (x\u22c6 , . . . , x\u22c6 ), where x\u22c6\nsolves (1), whenever 1) the network is bipartite, or 2) each fp is\nstrongly convex.\nProof: The proof is based on showing that the conditions of\nTheorem 1 are satisfied. First, note that Assumptions 1) and 2)\nand the equivalence between (1)\nP and (3) imply that problem (3)\nn\nis solvable, that each function\np\u2208Cc fp (xp ) is convex over R ,\nand that each set X\u0304c is closed and convex. Now, we will see that\nAssumption 3) implies that each Bc\u22a4 \u2297 In has full column-rank.\nNote that it is sufficient to prove that Bc\u22a4 has full column-rank. If,\non the other hand, we prove that Bc Bc\u22a4 has full rank, then the result\nfollows because rank(Bc Bc\u22a4 ) = rank(Bc\u22a4 ). Note that Bc Bc\u22a4 is a\ndiagonal matrix, where the diagonal contains the degrees of the nodes\nbelonging to the subnetwork composed by the nodes in Cc . Since no\nnode has degree 0 (cf. Assumption 3)), Bc Bc\u22a4 has full rank.\nFinally, note that a bipartite network can be colored with just two\ncolors. In that case, condition 1) of Theorem 1 is satisfied together\nwith the remaining conditions, which ensures the convergence of\nAlgorithm 1. When the network is non-bipartite and each fp is\nstrongly convex, we are in case 2) of Theorem 1, which again ensures\nthe convergence of Algorithm 1.\nIII. A PPLICATIONS\n\nAlgorithm 1 D-ADMM\nInitialization: for all p \u2208 V , set \u03b3p1 = x1p = 0 and k = 1\n1: repeat\n2:\nfor c = 1, . . . , C do\n3:\nfor all p \u2208 Cc [in parallel] do\nX\nX\nxk+1\n\u2212\u03c1\nj\n\nvpk = \u03b3pk \u2212 \u03c1\n\nj\u2208Np\nj<p\n\nand find\n\n4:\n\nxk+1\np\n\n= argmin\n\ns.t.\n5:\n6:\n7:\n8:\n\nxkj\n\nj\u2208Np\nj>p\n\n\u22a4\n\nfp (xp ) + vpk xp +\nx p \u2208 Xp\n\nDp \u03c1\nkxp k2\n2\n\nxk+1\np\n\nSend\nto Np\nend for\nend for\nfor all p \u2208 V [in parallel] do\nP\nk+1\nk+1\nk\n\u03b3p\n\n= \u03b3p + \u03c1\n\nj\u2208Np (xp\n\n\u2212 xk+1\n)\nj\n\n9:\nend for\n10:\nk \u2190k+1\n11: until some stopping criterion is met\n\nIn Algorithm 1, the edge-wise dual variables \u03bbij were totally\nreplaced by the node-wise dual variables \u03b3p . This is because the\nproblem in step 4 depends only on \u03b3pk and not on the individual\n\u03bbkij 's. The update for \u03b3p in step 8 stems from replacing \u03bbk+1\n=\nij\nk+1\nk+1\n\u03bbkij + sign(j \u2212 i)(xk+1\n\u2212\nx\n)\nin\nthe\ndefinition\nof\n\u03b3\n.\np\ni\nj\n\nWe will now see how some important optimization problems can\nbe recast as (1). These reformulations, except the one for LASSO,\nare not new: see [13], [14], [16], [15]. Therefore, we refer to these\nreferences for the details of solving the optimization problem in step 4\nof Algorithm 1. We note that in all the problems, except in consensus,\nnone of the functions fp is strongly convex. Therefore, D-ADMM\nis only guaranteed to converge under condition 1) of Corollary 1.\nNevertheless, in section IV, we will see that in practice D-ADMM,\nnot only converges for all these problems, but also outperforms\nprevious work in terms of the number of communications, including\nthe ADMM-based algorithms [13], [12], [16].\nConsensus. Consensus is a fundamental problem in networks [18],\n[22]. Given a network with P nodes, node p generates aP\nnumber,\nsay \u03b8p , and the goal is to compute the average \u03b8\u22c6 = (1/P ) P\np=1 \u03b8p\nat every node. Consensus can be cast as [7], [18]:\nminimize\nx\n\nP\n1X\n(x \u2212 \u03b8p )2 ,\n2 p=1\n\nwhich is clearly an unconstrained version of (1), with fp (x) =\n(1/2)(x \u2212 \u03b8p )2 ; thus, it can be solved with D-ADMM. In this case,\nthe problem of step 4 of Algorithm 1 has a closed-form solution:\nxk+1\n= (\u03b8p \u2212 vpk )/(1 + Dp \u03c1).\np\nSparse solutions of linear systems. Finding sparse solutions of\nlinear systems is important in many areas, including statistics, compressed sensing, and cognitive radio [23], [14]. A common approach\n\n\f4\n\nRow Partition\n\n\"\n\nWe now introduce a variable y \u2208 Rm in (14) and rewrite it as:\nPP\n2\n\u03b4\nminimize\np=1 (kxp k1 + 2 kxp k )\n\nColumn Partition\n\n\" \"\n\nA1\n...\nAP\n\nA1\n\nA2\n\n***\n\nAP\n\n\"\n\nx,y\n\nsubject to\n\nFigure 2. Row partition and column partition of A into P blocks. A block\nin the row (resp. column) partition is a set of rows (resp. columns).\n\nto tackle this problem is by solving LASSO [23] or BPDN [24],\nrespectively,\nLASSO:\nBPDN:\n\nminimize\n\nkxk1\n\nsubject to\n\nkAx \u2212 bk \u2264 \u03c3 ,\n\nminimize\n\nkAx \u2212 bk2 + \u03b2kxk1 ,\n\nx\n\nx\n\nminimize\n\nkxk1 + 2\u03b4 kxk2\n\nsubject to\n\nkAx \u2212 bk \u2264 \u03c3 ,\n\nx\n\n(14)\n\nwhere \u03b4 > 0 is small enough. This regularization is inspired by [25],\nwhich establishes exact regularization conditions. By exact, we mean\nthere exists \u03b4\u0304 > 0 such that the solution of (14) is always a LASSO\nsolution, for \u03b4 \u2264 \u03b4\u0304. One of these conditions is that the objective is\nlinear and the constraint set is the intersection of a linear system with\na closed polyhedral cone. Although LASSO can be recast as\nminimize\n\n1\u22a4\nnt\n\nsubject to\n\nkuk \u2264 v\nu = Ax \u2212 b , v = \u03c3\nx \u2264 t , \u2212x \u2264 t ,\n\nx,t,u,v\n\nP \u0010\nX\n\nminimize\nx\n\nwhere the matrix A \u2208 Rm\u00d7n , the vector b \u2208 Rm , and the\nparameters \u03c3, \u03b2 > 0 are given. LASSO first appeared in [23] to\ndenote a related problem, although the problem in (12) is known\nby the same name. We solve LASSO and BPDN in two different\nscenarios, visualized in Fig. 2: row partition (resp. column partition),\nwhere each node stores a block of rows (resp. columns) of A. While\nin the row partition vector b is partitioned similarly to A, in the\ncolumn partition we assume all nodes know the full vector b.\nWe propose solving LASSO with a column partition and BPDN\nwith a row partition. The reverse cases, i.e., LASSO with a row\npartition and BPDN with a column partition, cannot be trivially recast\nas (1). However, in our previous work [1], we solved Basis Pursuit\n(i.e., LASSO with \u03c3 = 0) for both the row and the column partition.\nLASSO: column partition. Assume A is partitioned by columns,\nand the pth block is only known at node p. Also, assume vector b,\nparameter \u03c3, and the number of nodes P are available at all nodes.\nLASSO in this scenario cannot be directly recast as (1): we will have\nto do it through duality. However, only solving the ordinary dual of\nLASSO will not allow us to recover a primal solution afterwards,\nsince its objective is not strictly convex. We thus start by regularizing\nLASSO, making it strictly convex:\n\n(15)\n\nwhere 1n \u2208 Rn is the vector of ones, the closed convex cone\n{(u, v) : kuk \u2264 v} is not polyhedral; thus, there is not a proof of\nexact regularization for (15). However, experimental results in [25]\nsuggest that exact regularization might occur for non-polyhedral\ncones. In the simulations discussed in the next section, we solved (14)\nalways with \u03b4 = 10\u22122 and the corresponding solutions never differed\nmore than 0.5% from the \"true\" solution of LASSO.\n\n(16)\n\nIf we only dualize\nthe dual problem\nPPthe last constraint of (16), we get\n\u22a4\n1\nof minimizing\np=1 gp (\u03bb), where gp (\u03bb) := P (b \u03bb + \u03c3k\u03bbk) \u2212\n2\n\u22a4\n\u03b4\nkx\nk\n)\nis\nthe\nfunction\nassociated to\ninf xp (kxp k1 + (A\u22a4\n\u03bb)\nx\n+\np\np\np\n2\nnode p. This problem is clearly an unconstrained version of (1).\nBPDN: row partition. In BPDN, A and b are partitioned by rows,\nwith the blocks Ap and bp stored at node p. In this scenario, BPDN\ncan be readily rewritten as\n\n(12)\n(13)\n\nkyk \u2264 \u03c3\nP\ny= P\np=1 Ap xp \u2212 b .\n\nkAp x \u2212 bp k2 +\n\np=1\n\n\u0011\n\u03b2\nkxk1 ,\nP\n\n(17)\n\nwhich is an unconstrained version of (1): just make fp (x) := kAp x\u2212\nbp k2 + P\u03b2 kxk1 .\nDistributed support vector machines. A Support Vector Machine\nis an optimization problem that arises in machine learning in the\ncontext of classification and regression [26, Ch.7]. While there are\nseveral possible formulations for an SVM, here we solve [26, \u00a77.1.1]\n+ \u03b2 1\u22a4\nK\u03be\n\nminimize\n\n1\nksk2\n2\n\nsubject to\n\nyk (s\u22a4 xk \u2212 r) \u2265 1 \u2212 \u03bek ,\n\u03be \u2265 0,\n\ns,r,\u03be\n\nk = 1, . . . , K\n\n(18)\n\nwhere the parameter \u03b2 > 0 and the pairs (xk , yk ), k = 1, . . . , K,\nare given. Each point xk belongs to one of two classes: yk = 1\nor yk = \u22121. The goal in solving (18) is to find an hyperplane {x \u2208\nRn : s\u22a4 x = r} that best separates the two classes. The optimization\nvariables in (18) are s \u2208 Rn , the vector orthogonal to the hyperplane,\nr \u2208 R, the hyperplane offset, and \u03be \u2208 RK , the vector of slack\nvariables. We assume that K is divisible by the number of nodes P ,\nand that each node knows m := K/P pairs of points (xk , yk ).\nThe resulting problem can be formulated as an unconstrained version\nof (1) by setting\nfp (s, r) = inf\n\u03be\u0304p\n\ns.t.\n\n1\n2P\n\n \u0304\nksk2 + \u03b2 1\u22a4\nm \u03bep\n\nYp (Xp s \u2212 r1m ) \u2265 1m \u2212 \u03be \u0304p\n\u03be \u0304p \u2265 0 ,\n\nwhere Yp is a diagonal matrix with the yk 's corresponding to\nnode p in the diagonal, and Xp is an m \u00d7 n matrix with each row\ncontaining x\u22a4\nk . Note that the size of the variable to be transmitted\namong the nodes is n + 1, corresponding to the size of the global\nvariable (s, r): the variables \u03be\u0304p are internal to each node.\nIV. S IMULATION R ESULTS\nThis section shows simulation results of D-ADMM and related\nalgorithms solving the problems presented in the previous section.\nWe focus on the ADMM-based algorithms [12] and [13], since these\nare the best among the distributed algorithms for (1). We start by\ndiscussing the performance measure.\nPerformance measure: communication steps. We say that a\nCommunication Step (CS) has occurred when all the nodes have\ntransmitted a vector of size n to its neighbors. All the algorithms we\nconsider here, including D-ADMM, consist of one iterative loop. One\niteration of D-ADMM, as well as of [13], corresponds to one CS; one\niteration of [12] corresponds to two CSs, since each node transmits\ntwo vectors of size n per iteration. The number of CSs is proportional\nto the number of total communications. Thus, in a wireless scenario,\nthe smaller the number of CSs, the lower the energy consumption.\n\n\f5\n\nCommunication steps\n\nCommunication steps\n\nCommunication steps\n\nCommunication steps\n\n104\n\n104\n\n104\n\n104\n\n3\n\n3\n\n10\n\n3\n\n10\n\n2\n\n102\n\n101\n\n101\n\n[16]\n10\n\n10\n\n[13]\n[12]\n\n10\n\n2\n\n[12]\n\n[13]\n\n[13]\n10\n\n2\n\nD-ADMM\n\n[22]\nD-ADMM\n\n101\n\n101\n\n100\n\n100\n1\n\n2\n3\n4\nNetwork number\n\n5\n\n2\n3\n4\nNetwork number\n\n5\n\n(b) LASSO\n\n(a) Consensus\nFigure 3.\n\n[12]\n\nD-ADMM\n\n100\n1\n\nD-ADMM [13]\n103\n\n100\n1\n\n2\n3\n4\nNetwork number\n\n5\n\n1\n\n2\n3\n4\nNetwork number\n\n(c) BPDN\n\n5\n\n(d) SVM\n\nResults of the simulations for (a) consensus, (b) LASSO, (c) BPDN, and (d) SVM.\nTable I\nN ETWORK MODELS\nNetwork\n1\n2\n3\n4\n5\n\nModel (parameters)\nErd\u0151s-R\u00e9nyi (0.12)\nWatts-Strogatz (4, 0.4)\nBarabasi (2)\nGeometric (0.23)\nLattice (5 \u00d7 10)\n\n# Colors\n5\n4\n3\n10\n2\n\nNote, however, that the CS measure does not take into account the\ncomputational complexity at each node. (Actually, D-ADMM, and\nthe algorithms in [12], [13] have similar computational complexities.)\nAlso, this measure is not necessarily related with execution time. In\nfact, while D-ADMM requires less CSs than competing algorithms\n(as we will see), it may be slower than some of them. The reason is\nbecause D-ADMM is asynchronous, while all the other algorithms\nare synchronous. Scenarios allowing synchronous transmissions are,\nhowever, limited to very controlled environments, such as computer\nclusters or super-computers, where approaches like [4, \u00a77.2] would\nprobably be more appropriate than distributed algorithms. On the\nother hand, in wireless networks, the single fact that one node cannot\ntransmit and receive messages at the same time forces synchronous\nalgorithms to operate asynchronously.\nExperimental setup. We generated 5 networks with P = 50\nnodes according to the models of Table I; see [1] for a description\nof these models. Table I also gives the number of colors for each\nnetwork. The results of our simulations are in Fig. 3, where each\nplot depicts the number of CSs as a function of the network. Having\ncomputed the solution x\u22c6 of (1) beforehand and in a centralized way,\neach algorithm stopped whenever kxk \u2212 x\u22c6 k/kx\u22c6 k \u2264 \u01eb, or when a\nmaximum number of M CSs were reached. In the case of consensus,\nBPDN, and SVM, xk denotes the estimate of x\u22c6 at an arbitrary\nnode; in the case of LASSO, it represents the global estimate of the\nnetwork, since each node only estimates some components of x\u22c6 .\nWe used the following values for the pair (\u01eb, M ): (10\u22124 , 103 ) for\nconsensus, (5 \u00d7 10\u22123 , 103 ) for LASSO, (10\u22124 , 2 \u00d7 103 ) for BPDN,\nand (10\u22123 , 103 ) for SVM. Since the problem in step 4 of Algorithm 1\ndoes not have a closed-form solution for all the applications we\nconsider, except for consensus, it has to be solved iteratively in each\nof the algorithms we compare. To make a fair comparison in terms\nof CSs, we thus use the same solver in all the algorithms, i.e., the\nproblem in step 4 of Algorithm 1 is solved with the same precision\nin all the algorithms we compare.\n\nIt is known that the parameter \u03c1 affects strongly the performance\nof ADMM-based algorithms. Hence, to make a fair comparison, we\nran each (ADMM-based) algorithm for several values of \u03c1 and chose\nalways the best result, i.e., the smaller number of CSs. The values\nfor \u03c1 were taken from the set {10\u22124 , 10\u22123 10\u22122 10\u22121 , 1, 10, 102 }.\nConsensus. For the consensus problem, we generated each \u03b8p\ni.i.d.\nrandomly from a Gaussian distribution: \u03b8p \u223c N (10, 104 ). Fig. 3(a)\nshows the results for D-ADMM, the ADMM-based algorithms [12],\n[13], and the algorithm [22], which is considered to be the fastest\nconsensus algorithm [18]. Note that [22] was designed for consensus\nonly and cannot be easily generalized to solve (1). Fig. 3(a) shows\nthat D-ADMM has a performance very similar to that of [22].\nLASSO and BPDN. The matrix A for the problems LASSO and\nBPDN was taken from problem 902 of the Sparco toolbox [27]. The\nvector b was generated as b = As + n, where s is a sparse vector\nand n is Gaussian noise. We chose \u03c3 = 0.1 and \u03b2 = 0.3 for the\nnoise parameters, and \u03b4 = 10\u22122 for the approximation parameter in\nLASSO. The results of these experiments for LASSO and BPDN are\nshown, respectively, in Figs. 3(b) and 3(c). Additionally, we show\nthe performance of Algorithm 3 of [16], which is an ADMM-based\nalgorithm specifically designed to solve BPDN. This algorithm has\nthe advantage of requiring much simpler computations at each node,\nbut in our simulations it achieved the maximum number of CSs in\nall but the last two networks. In both LASSO and BPDN, D-ADMM\nwas always the algorithm requiring fewer CSs to converge.\nSVM. For the SVM problem (18), we used data from [28], namely\ntwo overlapping sets of points from the Iris dataset. The parameter \u03b2\nwas always set to 1. In this case, the algorithm from [12] achieved\nalways the maximum number of CSs and thus is not represented in\nFig. 3(d), which shows the simulation results. Again, we see that DADMM was the algorithm requiring the smallest number of CSs to\nconverge.\nV. C ONCLUSIONS\nWe proposed an algorithm for solving separable problems in\nnetworks, in a distributed way. Each node has a private cost and a\nprivate constraint set, but all nodes cooperate to solve the optimization\nproblem that minimizes the sum of all costs and that has the\nintersection of all sets as a constraint. Our algorithm hinges on a\ncoloring scheme of the network, according to which the nodes operate\nasynchronously. This results in an algorithm with fewer communication requirements than previous algorithms, as shown experimentally\nfor several problems. Although we proved the convergence of the\nalgorithm, it still remains an open question to explain theoretically\nwhy the algorithm is more efficient than previous algorithms.\n\n\f6\n\nR EFERENCES\n[1] J. Mota, J. Xavier, P. Aguiar, and M. P\u00fcschel, \"Distributed basis pursuit,\"\nIEEE Trans. Sig. Proc., vol. 60, no. 4, 2012.\n[2] I. Akyildiz, Y. Sankarasubramaniam, and E. Cayirci, \"Wireless sensor\nnetworks: a survey,\" Comp. Netw., vol. 38, 2002.\n[3] P. Fischione, P. Park, and K. Johansson, Wireless Network Based Control,\nchapter Design Principles of Wireless Sensor Network Protocols for\nControl Applications, Springer, 2011.\n[4] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, \"Distributed\noptimization and statistical learning via the alternating method of multipliers,\" Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1\u2013122, 2011.\n[5] D. Han and X. Yuan, \"A note on the alternating direction method of\nmultipliers,\" J. Optim. Theory Appl., 2012.\n[6] J. Tsitsiklis, D. Bertsekas, and M. Athans, \"Distributed asynchronous\ndeterministic and stochastic gradient optimization algorithms,\" IEEE\nTrans. Aut. Contr., vol. AC-31, no. 9, 1986.\n[7] M. Rabbat and R. Nowak, \"Distributed optimization in sensor networks,\"\nin Proc. IPSN'04, 2004, pp. 20\u201327.\n[8] P Palomar and Y. Eldar, Convex Optimization in Signal Processing and\nCommunications, Cambridge Univ. Press, 2010.\n[9] D. Bertsekas and J. Tsitsiklis, Parallel and Distributed Computation:\nNumerical Methods, Athena Scientific, 1997.\n[10] D. Jakoveti\u0107, J. Xavier, and J. Moura, \"Cooperative convex optimization\nin networked systems: Augmented lagrangian algorithms with directed\ngossip communication,\" IEEE Trans. Sig. Proc., vol. 59, no. 8, 2011.\n[11] M. Rabbat, R. Nowak, and J. Bucklew, \"Generalized consensus algorithms in networked systems with erasure links,\" in IEEE Workshop Sig.\nProc. Advances Wirel. Comm., 2005.\n[12] I. Schizas, A. Ribeiro, and G. Giannakis, \"Consensus in ad hoc wsns\nwith noisy links - Part I: Distributed estimation of deterministic signals,\"\nIEEE Trans. Sig. Proc., vol. 56, no. 1, pp. 350\u2013364, 2008.\n[13] H. Zhu, G. Giannakis, and A. Cano, \"Distributed in-network channel\ndecoding,\" IEEE Trans. Sig. Proc., vol. 57, no. 10, 2009.\n[14] J. Bazerque and G. Giannakis, \"Distributed spectrum sensing for\ncognitive radio networks by exploiting sparsity,\" IEEE Trans. Sig. Proc.,\nvol. 58, no. 3, 2010.\n[15] P. Forero, A. Cano, and G. Giannakis, \"Consensus-based distributed\nsupport vector machines,\" J.M.L.R., vol. 11, 2010.\n[16] G. Mateos, J. Bazerque, and G. Giannakis, \"Distributed sparse linear\nregression,\" IEEE Trans. Sig. Proc., vol. 58, no. 10, 2010.\n[17] J. Bazerque, G. Mateos, and G. Giannakis, \"Group-Lasso on splines for\nspectrum cartography,\" IEEE Trans. Sig. Proc., vol. 59, no. 10, 2011.\n[18] T. Erseghe, D. Zennaro, E. Dall'Anese, and L. Vangelista, \"Fast\nconsensus by the alternating direction multipliers method,\" IEEE Trans.\nSig. Proc., vol. 59, no. 11, 2011.\n[19] J. Mota, J. Xavier, P. Aguiar, and M. P\u00fcschel, \"A proof of convergence\nfor the alternating direction method of multipliers applied to polyhedralconstrained functions,\" http://arxiv.org/abs/1112.2295 , 2011.\n[20] B. He, M. Tao, and X. Yuan, \"Alternating direction method with\nGaussian back substitution for separable convex programmming,\" SIAM\nJ. Optim., vol. 22, no. 2, 2012.\n[21] Z. Luo, \"On the linear convergence of the alternating direction method\nof multipliers,\" arxiv.org/abs/1208.3922, 2012.\n[22] B. Oreshkin, M. Coates, and M. Rabbat, \"Optimization and analysis of\ndistributed averaging with short node memory,\" IEEE Trans. Sig. Proc.,\nvol. 58, no. 5, 2010.\n[23] R. Tibshirani, \"Regression shrinkage and selection via the lasso,\" J. R.\nStatist. Soc. B, vol. 58, no. 1, 1996.\n[24] S. Chen, D. Donoho, and M. Saunders, \"Atomic decomposition by basis\npursuit,\" SIAM J. Sc. Cp., vol. 20, no. 1, 1998.\n[25] M. Friedlander and P. Tseng, \"Exact regulatization of convex programs,\"\nSIAM J. Optim., vol. 18, no. 4, 2007.\n[26] C. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.\n[27] E. Berg, M. Friedlander, G. Hennenfent, F. Herrmann, R. Saab, and\n\u00d6. Yilmaz, \"Sparco: a testing framework for sparse reconstruction,\"\nTech. Rep., Dept. Computer Science, University of British Columbia,\nVancouver, 2007.\n[28] A. Frank and A. Asuncion, UCI Machine Learning Repository, Univ.\nCalif., 2010.\n\nJo\u00e3o Mota received a M.S. degree in Electrical\nand Computer Engineering from Instituto Superior\nT\u00e9cnico, Technical University of Lisbon, Lisbon,\nPortugal, in 2008. He is currently working towards\nhis Ph.D. degree in Electrical and Computer Engineering, within a joint program between Carnegie\nMellon University, Pittsburgh, PA, and Instituto Superior T\u00e9cnico, Lisbon, Portugal. His research interests include distributed optimization and control,\nand sensor networks.\n\nJo\u00e3o Xavier (S'97\u2013M'03) received the Ph.D. degree in Electrical and Computer Engineering from\nInstituto Superior Tecnico (IST), Lisbon, Portugal,\nin 2002. Currently, he is an Assistant Professor in\nthe Department of Electrical and Computer Engineering, IST. He is also a Researcher at the Institute\nof Systems and Robotics (ISR), Lisbon, Portugal.\nHis current research interests are in the area of\noptimization, sensor networks and signal processing\non manifolds.\n\nPedro M. Q. Aguiar (S'95\u2013M'00\u2013SM'08) received\nthe Ph.D. degree in electrical and computer engineering from the Instituto Superior T\u00e9cnico, Technical\nUniversity of Lisbon, Lisbon, Portugal, in 2000.\nHe is currently an Assistant Professor with the\nInstituto Superior T\u00e9cnico, Technical University of\nLisbon, Lisbon, Portugal. He is also affiliated with\nthe Institute for Systems and Robotics, Lisbon, Portugal, and has been Visiting Scholar with CarnegieMellon University, Pittsburgh, PA, and a Consultant\nwith Xerox Palo Alto Research Center, Palo Alto,\nCA. His main research interests are in image analysis and computer vision.\n\nMarkus P\u00fcschel (M'99\u2013SM'05) is a Professor of\nComputer Science at ETH Zurich, Switzerland. Before, he was a Professor of Electrical and Computer Engineering at Carnegie Mellon University,\nwhere he still has an adjunct status. He received his\nDiploma (M.Sc.) in Mathematics and his Doctorate\n(Ph.D.) in Computer Science, in 1995 and 1998,\nrespectively, both from the University of Karlsruhe,\nGermany. From 1998-1999 he was a Postdoctoral\nResearcher at Mathematics and Computer Science,\nDrexel University. From 2000-2010 he was with\nCarnegie Mellon University, and since 2010 he has been with ETH Zurich.\nHe was an Associate Editor for the IEEE Transactions on Signal Processing,\nthe IEEE Signal Processing Letters, was a Guest Editor of the Proceedings\nof the IEEE and the Journal of Symbolic Computation, and served on\nvarious program committees of conferences in computing, compilers, and\nprogramming languages. He is a recipient of the Outstanding Research Award\nof the College of Engineering at Carnegie Mellon and the Eta Kappa Nu\nAward for Outstanding Teaching. He also holds the title of Privatdozent\nat the University of Technology, Vienna, Austria. In 2009 he cofounded\nSpiralGen, Inc.\n\n\f"}