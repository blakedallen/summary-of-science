{"id": "http://arxiv.org/abs/1205.0925v1", "guidislink": true, "updated": "2012-05-04T11:34:05Z", "updated_parsed": [2012, 5, 4, 11, 34, 5, 4, 125, 0], "published": "2012-05-04T11:34:05Z", "published_parsed": [2012, 5, 4, 11, 34, 5, 4, 125, 0], "title": "Controlled stochastic networks in heavy traffic: Convergence of value\n  functions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1205.1464%2C1205.3025%2C1205.4082%2C1205.1495%2C1205.0267%2C1205.0797%2C1205.4203%2C1205.2054%2C1205.1136%2C1205.0020%2C1205.2183%2C1205.3785%2C1205.1506%2C1205.0892%2C1205.3023%2C1205.1226%2C1205.1362%2C1205.0589%2C1205.2833%2C1205.0515%2C1205.2843%2C1205.4488%2C1205.0501%2C1205.1954%2C1205.1688%2C1205.2231%2C1205.4269%2C1205.3523%2C1205.2767%2C1205.2139%2C1205.0392%2C1205.3434%2C1205.1726%2C1205.3102%2C1205.2854%2C1205.0841%2C1205.1878%2C1205.2803%2C1205.1107%2C1205.1958%2C1205.1698%2C1205.0766%2C1205.3841%2C1205.0133%2C1205.1148%2C1205.2459%2C1205.0003%2C1205.2850%2C1205.4044%2C1205.3737%2C1205.2816%2C1205.3560%2C1205.3553%2C1205.1311%2C1205.2354%2C1205.1135%2C1205.0452%2C1205.3492%2C1205.2111%2C1205.1390%2C1205.3787%2C1205.1661%2C1205.0007%2C1205.0780%2C1205.2710%2C1205.0667%2C1205.0512%2C1205.2041%2C1205.2531%2C1205.1935%2C1205.3772%2C1205.0034%2C1205.3716%2C1205.3801%2C1205.0430%2C1205.1995%2C1205.0001%2C1205.0925%2C1205.2434%2C1205.0166%2C1205.2076%2C1205.4307%2C1205.3439%2C1205.3637%2C1205.0455%2C1205.1209%2C1205.0119%2C1205.2909%2C1205.1929%2C1205.2342%2C1205.0565%2C1205.1210%2C1205.0367%2C1205.2589%2C1205.1453%2C1205.1121%2C1205.2057%2C1205.4317%2C1205.2322%2C1205.3244%2C1205.0377&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Controlled stochastic networks in heavy traffic: Convergence of value\n  functions"}, "summary": "Scheduling control problems for a family of unitary networks under heavy\ntraffic with general interarrival and service times, probabilistic routing and\nan infinite horizon discounted linear holding cost are studied. Diffusion\ncontrol problems, that have been proposed as approximate models for the study\nof these critically loaded controlled stochastic networks, can be regarded as\nformal scaling limits of such stochastic systems. However, to date, a rigorous\nlimit theory that justifies the use of such approximations for a general family\nof controlled networks has been lacking. It is shown that, under broad\nconditions, the value function of the suitably scaled network control problem\nconverges to that of the associated diffusion control problem. This scaling\nlimit result, in addition to giving a precise mathematical basis for the above\napproximation approach, suggests a general strategy for constructing near\noptimal controls for the physical stochastic networks by solving the associated\ndiffusion control problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1205.1464%2C1205.3025%2C1205.4082%2C1205.1495%2C1205.0267%2C1205.0797%2C1205.4203%2C1205.2054%2C1205.1136%2C1205.0020%2C1205.2183%2C1205.3785%2C1205.1506%2C1205.0892%2C1205.3023%2C1205.1226%2C1205.1362%2C1205.0589%2C1205.2833%2C1205.0515%2C1205.2843%2C1205.4488%2C1205.0501%2C1205.1954%2C1205.1688%2C1205.2231%2C1205.4269%2C1205.3523%2C1205.2767%2C1205.2139%2C1205.0392%2C1205.3434%2C1205.1726%2C1205.3102%2C1205.2854%2C1205.0841%2C1205.1878%2C1205.2803%2C1205.1107%2C1205.1958%2C1205.1698%2C1205.0766%2C1205.3841%2C1205.0133%2C1205.1148%2C1205.2459%2C1205.0003%2C1205.2850%2C1205.4044%2C1205.3737%2C1205.2816%2C1205.3560%2C1205.3553%2C1205.1311%2C1205.2354%2C1205.1135%2C1205.0452%2C1205.3492%2C1205.2111%2C1205.1390%2C1205.3787%2C1205.1661%2C1205.0007%2C1205.0780%2C1205.2710%2C1205.0667%2C1205.0512%2C1205.2041%2C1205.2531%2C1205.1935%2C1205.3772%2C1205.0034%2C1205.3716%2C1205.3801%2C1205.0430%2C1205.1995%2C1205.0001%2C1205.0925%2C1205.2434%2C1205.0166%2C1205.2076%2C1205.4307%2C1205.3439%2C1205.3637%2C1205.0455%2C1205.1209%2C1205.0119%2C1205.2909%2C1205.1929%2C1205.2342%2C1205.0565%2C1205.1210%2C1205.0367%2C1205.2589%2C1205.1453%2C1205.1121%2C1205.2057%2C1205.4317%2C1205.2322%2C1205.3244%2C1205.0377&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Scheduling control problems for a family of unitary networks under heavy\ntraffic with general interarrival and service times, probabilistic routing and\nan infinite horizon discounted linear holding cost are studied. Diffusion\ncontrol problems, that have been proposed as approximate models for the study\nof these critically loaded controlled stochastic networks, can be regarded as\nformal scaling limits of such stochastic systems. However, to date, a rigorous\nlimit theory that justifies the use of such approximations for a general family\nof controlled networks has been lacking. It is shown that, under broad\nconditions, the value function of the suitably scaled network control problem\nconverges to that of the associated diffusion control problem. This scaling\nlimit result, in addition to giving a precise mathematical basis for the above\napproximation approach, suggests a general strategy for constructing near\noptimal controls for the physical stochastic networks by solving the associated\ndiffusion control problem."}, "authors": ["Amarjit Budhiraja", "Arka P. Ghosh"], "author_detail": {"name": "Arka P. Ghosh"}, "author": "Arka P. Ghosh", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/11-AAP784", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1205.0925v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1205.0925v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/11-AAP784 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1205.0925v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1205.0925v1", "journal_reference": "Annals of Applied Probability 2012, Vol. 22, No. 2, 734-791", "doi": "10.1214/11-AAP784", "fulltext": "arXiv:1205.0925v1 [math.PR] 4 May 2012\n\nThe Annals of Applied Probability\n2012, Vol. 22, No. 2, 734\u2013791\nDOI: 10.1214/11-AAP784\nc Institute of Mathematical Statistics, 2012\n\nCONTROLLED STOCHASTIC NETWORKS IN HEAVY TRAFFIC:\nCONVERGENCE OF VALUE FUNCTIONS\nBy Amarjit Budhiraja1 and Arka P. Ghosh2\nUniversity of North Carolina and Iowa State University\nScheduling control problems for a family of unitary networks under heavy traffic with general interarrival and service times, probabilistic routing and an infinite horizon discounted linear holding cost\nare studied. Diffusion control problems, that have been proposed as\napproximate models for the study of these critically loaded controlled\nstochastic networks, can be regarded as formal scaling limits of such\nstochastic systems. However, to date, a rigorous limit theory that justifies the use of such approximations for a general family of controlled\nnetworks has been lacking. It is shown that, under broad conditions,\nthe value function of the suitably scaled network control problem converges to that of the associated diffusion control problem. This scaling\nlimit result, in addition to giving a precise mathematical basis for the\nabove approximation approach, suggests a general strategy for constructing near optimal controls for the physical stochastic networks\nby solving the associated diffusion control problem.\n\n1. Introduction. As an approximation to control problems for criticallyloaded stochastic networks, Harrison (in [16], see also [18, 20]) has formulated a stochastic control problem in which the state process is driven by\na multidimensional Brownian motion along with an additive control that\nsatisfies certain feasibility and nonnegativity constraints. This control problem, that is, usually referred to as the Brownian Control Problem (BCP)\nhas been one of the key developments in the heavy traffic theory of controlled stochastic processing networks (SPN). BCPs can be regarded as forReceived March 2010; revised December 2010.\nSupported in part by the NSF (DMS-10-04418), Army Research Office (W911NF-01-0080, W911NF-10-1-0158) and the US-Israel Binational Science Foundation (2008466).\n2\nSupported in part by NSF Grant DMS-06-08634.\nAMS 2000 subject classifications. Primary 60K25, 68M20, 90B22, 90B36; secondary\n60J70.\nKey words and phrases. Heavy traffic, stochastic control, scaling limits, diffusion approximations, unitary networks, controlled stochastic processing networks, asymptotic\noptimality, singular control with state constraints, Brownian control problem (BCP).\n1\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Applied Probability,\n2012, Vol. 22, No. 2, 734\u2013791. This reprint differs from the original in pagination\nand typographic detail.\n1\n\n\f2\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nmal scaling limits for a broad range of scheduling and sequencing control\nproblems for multiclass queuing networks. Finding optimal (or even nearoptimal) control policies for such networks-which may have quite general\nnon-Markovian primitives, multiple server capabilities and rather complex\nrouting geometry-is in general prohibitive. In that regard, BCPs that provide significantly more tractable approximate models are very useful. In this\ndiffusion approximation approach to policy synthesis, one first finds an optimal (or near-optimal) control for the BCP which is then suitably interpreted\nto construct a scheduling policy for the underlying physical network. In recent years there have been many works [1, 3, 4, 8, 11, 12, 26, 28] that consider\nspecific network models for which the associated BCP is explicitly solvable\n(i.e., an optimal control process can be written as a known function of the\ndriving Brownian motions) and, by suitably adapting the solution to the underlying network, construct control policies that are asymptotically (in the\nheavy traffic limit) optimal. The paper [25] also carries out a similar program for the crisscross network where the state\u2013space is three dimensional,\nalthough an explicit solution for the BCP here is not available.\nAlthough now there are several papers which establish a rigorous connection between a network control problem and its associated BCP by exploiting\nthe explicit form of the solution of the latter, a systematic theory which justifies the use of BCPs as approximate models has been missing. In a recent\nwork [9] it was shown that for a large family of Unitary Networks (following terminology of [7], these are networks with a structure as described in\nSection 2), with general interarrival and service times, probabilistic routing\nand an infinite horizon discounted linear holding cost, the cost associated\nwith any admissible control policy for the network is asymptotically, in the\nheavy traffic limit, bounded below by the value function of the BCP. This\ninequality, which provides a useful bound on the best achievable asymptotic\nperformance for an admissible control policy, was a key step in developing\na rigorous general theory relating BCPs with SPN in heavy traffic.\nThe current paper is devoted to the proof of the reverse inequality. The\nnetwork model is required to satisfy assumptions made in [9] (these are summarized above Theorem 2.10). In addition, we impose a nondegeneracy condition (Assumption 2.12), a condition on the underlying renewal processes\nregarding probabilities of deviations from the mean (Assumption 2.13) and\nregularity of a certain Skorohod map (Assumption 2.15) (see next paragraph for a discussion of these conditions). Under these assumptions we\nprove that the value function of the BCP is bounded below by the heavy\ntraffic limit (limsup) of the value functions of the network control problem\n(Theorem 2.16). Combining this with the result obtained in [9] (see Theorem 2.10), we obtain the main result of the paper (Theorem 2.18). This\ntheorem says that, under broad conditions, the value function of the network control problem converges to that of the BCP. This result provides,\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n3\n\nunder general conditions, a rigorous basis for regarding BCPs as approximate models for critically loaded stochastic networks.\nConditions imposed in this paper allow for a wide range of SPN models.\nSome such models, whose description is taken from [7], are discussed in detail\nin Examples 1(a)\u2013(c). We note that our approach does not require the BCP\nto be explicitly solvable and the result covers many settings where explicit\nsolutions are unavailable. Most of the conditions that we impose are quite\nstandard and we only comment here on three of them: Assumptions 2.5,\n2.6 and 2.15. Assumption 2.5 says that each buffer is processed by at least\none basic activity (see Remark 2.4). This condition, which was introduced\nin [7], is fundamental for our analysis. In fact, [7] has shown that without\nthis assumption even the existence of a nonnegative workload matrix may\nfail. Assumption 2.6 is a natural condition on the geometry of the underlying\nnetwork. Roughly speaking, it says that a nonzero control action leads to\na nonzero state displacement. Assumption 2.15 is the third key requirement\nin this work. It says that the Skorohod problem associated with a certain\nreflection matrix D [see equation (2.43) for the definition of D] is well posed\nand the associated Skorohod map is Lipschitz continuous. As Example 1\ndiscusses, this condition holds for a broad family of networks (including all\nmulticlass open queuing networks, as well as a large family of parallel server\nnetworks and job-shop networks).\nThe papers [1, 3, 4, 8, 12, 28] noted earlier, that treat the setting of explicitly solvable BCP, do much more than establish convergence of value\nfunctions. In particular, these works give an explicit implementable control policy for the underlying network that is asymptotically optimal in the\nheavy traffic limit. In the generality treated in the current work, giving explicit recipes (e.g., threshold type policies) is unfeasible, however, the policy\nsequence constructed in Section 4.1 suggests a general approach for building\nnear asymptotically optimal policies for the network given a near optimal\ncontrol for the BCP. Obtaining near optimal controls for the BCP in general\nrequires numerical approaches (see, e.g., [23, 24, 27]), discussion of which is\nbeyond the scope of the current work.\nWe now briefly describe some of the ideas in the proof of the main result-\nTheorem 2.16. We begin by choosing, for an arbitrary \u03b5 > 0, a suitable \u03b5optimal control \u1ef8 for the BCP and then, using \u1ef8 , construct a sequence\nof control policies {T r }r\u22651 for the network model such that the (suitably\nscaled) cost associated with T r converges to that associated with \u1ef8 , as\nr \u2192 \u221e. This yields the desired reverse inequality. One of the key difficulties\nis in the translation of a given control for the BCP to that for the physical\nnetwork. Indeed, a (near) optimal control for the BCP can be a very general adapted process with RCLL paths. Without additional information on\nsuch a stochastic process, it is not at all clear how one adapts and applies\n\n\f4\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nit to a given network model. A control policy for the network needs to specify how each server distributes its effort among various job classes at any\ngiven time instant. By a series of approximations we show that one can find\na rather simple \u03b5-optimal control for the BCP, that is, easy to interpret and\nimplement on a network control problem. As a first step, using PDE characterization results for general singular control problems with state constraints\nfrom [2] (these, in particular, make use of the nondegeneracy assumption-\nAssumption 2.12), one can argue that a near-optimal control can be taken to\nbe adapted to the driving Brownian motion and be further assumed to have\nmoments that are subexponential in the time variable (see Lemma 3.10).\nUsing results from [10], one can perturb this control so that it has continuous sample paths without significantly affecting the cost. Next, using ideas\ndeveloped by Kushner and Martins [25] in the context of a two-dimensional\nBCP, one can further approximate such a control by a process with a fixed\n(nonrandom) finite number of jumps that take values in a finite set. Two\nmain requirements (in addition to the usual adaptedness condition) for such\na process to be an admissible control of a BCP (see Definition 2.9) are the\nnonnegativity constraints (2.39) and state constraints (2.38). It is relatively\neasy to construct a pure jump process that satisfies the first requirement\nof admissibility, namely, the nonnegativity constraints, however, the nondegenerate Brownian motion in the dynamics rules out the satisfaction of the\nsecond requirement, that is, state constraints, without additional modifications. This is where the regularity assumption on a certain Skorohod map\n(Assumption 2.15) is used. The pure jump control is modified in a manner\nsuch that in between successive jumps one uses the Skorohod map to employ\nminimal control needed in order to respect state constraints. Regularity of\nthe Skorohod problem ensures that this modification does not change the\nassociated cost much. The Skorohod map also plays a key role in the weak\nconvergence arguments used to prove convergence of costs. The above construction is the essential content of Theorem 3.5. The \u03b5-optimal control that\nwe use for the construction of the policy sequence requires two additional\nmodifications [see part (iii) of Theorem 3.5 and below (3.14)] which facilitate adapting such a control for the underlying physical network and in some\nweak convergence proofs, but we leave that discussion for later in Section 3\n(see Remark 3.6 and above Theorem 3.8).\nUsing a near-optimal control \u1ef8 of the form given in Section 3 (cf. Theorem 3.8), we then proceed to construct a sequence of policies {T r } for the\nunderlying network. The key relation that enables translation of \u1ef8 into {T r }\nis (2.16) using which one can loosely interpret \u1ef8 (t) as the asymptotic deviation, with suitable scaling, of T r (t) from the nominal allocation x\u2217 t (see\nDefinition 2.2 for the definition of nominal allocation vector). Recall that \u1ef8\nis constructed by modifying, through a Skorohod constraining mechanism,\na pure jump process (say, \u1ef80 ). In particular, \u1ef8 has sample paths that are,\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n5\n\nin general, discontinuous. On the other hand, note that an admissible policy T r is required to be a Lipschitz function (see Remark 2.8). This suggests\nthe following construction for T r . Over time periods (say, \u2206t) of constancy\nof \u1ef80 one should use the nominal allocation (i.e., x\u2217 \u2206t), while jump-instants\nshould be stretched into periods of length of order r (note that in the scaled\nnetwork, time is accelerated by a factor of r 2 and so such periods translate\nto intervals of length 1/r in the scaled evolution and thus are negligible)\nover which a nontrivial control action is employed as dictated by the jump\nvector (see Figure 4 for a more complete description). This is analogous to\nthe idea of a discrete review policy proposed by Harrison [17] (see also [1]\nand references therein). There are some obvious difficulties with the above\nprescription, for example, a nominal allocation corresponds to the average\nbehavior of the system and for a given realization is feasible only when the\nbuffers are nonempty. Thus, one needs to modify the above construction\nto incorporate idleness, that is, caused due to empty buffers. The effect of\nsuch a modification is, of course, very similar to that of a Skorohod constraining mechanism and it is tempting to hope that the deviation process\ncorresponding to this modified policy converges to \u1ef8 (in an appropriate\nsense), as r \u2192 \u221e. However, without further modifications, it is not obvious\nthat the reflection terms that are produced from the idling periods under\nthis policy are asymptotically consistent with those obtained from the Skorohod constraining mechanism applied to (the state process corresponding\nto) \u1ef80 . The additional modification [see (4.8)] that we make roughly says\nthat jobs are processed from a given buffer over a small interval \u2206, only\nif at the beginning of this interval there are a \"sufficient\" number of jobs\nin the buffer. This idea of safety stocks is not new and has been used in\nprevious works (see, e.g., [1, 3, 4, 8, 27]). The modification, of course, introduces a somewhat nonintuitive idleness even when there are jobs that\nrequire processing. However, the analysis of Section 4 shows that this idleness does not significantly affect the asymptotic cost. The above very rough\nsketch of construction of T r is made precise in Section 4.1.\nThe rest of the paper is devoted to showing that the cost associated\nwith T r converges to that associated with \u1ef8 . It is unreasonable to expect\nconvergence of controls (e.g., with the usual Skorohod topology)-in particular, note that T r has Lipschitz paths for every r while \u1ef8 is a (modification\nof) a pure jump process \u2013 however, one finds that the convergence of costs\nholds. This convergence proof, and the related weak convergence analysis,\nis carried out in Sections 4.2 and 4.3.\nThe paper is organized as follows. Section 2 describes the network structure, all the associated stochastic processes and the heavy-traffic assumptions as well as the other assumptions of the paper. The section also presents\nthe SPN control problem, that is, considered here, along with the main result of the paper (Theorem 2.18). Section 3 constructs (see Theorem 3.8)\n\n\f6\n\nA. BUDHIRAJA AND A. P. GHOSH\n\na near-optimal control policy for the BCP which can be suitably adapted to\nthe network control problem. In Section 4 the near-optimal control policy\nfrom Section 3 is used to obtain a sequence of admissible control policies for\nthe scaled SPN. The main result of the section is Theorem 4.5, which establishes weak convergence of various scaled processes. Convergence of costs\n(i.e., Theorem 2.17) is an immediate consequence of this weak convergence\nresult. Theorem 2.18 then follows on combining Theorem 2.17 with results\nof [9] (stated as Theorem 2.10 in the current work). Finally, the Appendix\ncollects proofs of some auxiliary results.\nThe following notation will be used. The space of reals (nonnegative reals),\npositive (nonnegative) integers will be denoted by R (R+ ), N (N0 ), respectively. For m \u2265 1 and \u03b8 \u2208 (0, \u221e), C m [C\u03b8m ] will denote the space of continuous\nfunctions from [0, \u221e) (resp. [0, \u03b8]) to Rm with the topology of uniform convergence on compacts (resp. uniform convergence). Also, D m [D\u03b8m ] will denote\nthe space of right continuous functions with left limits, from [0, \u221e) (resp.\n[0, \u03b8]) to Rm with the usual Skorohod topology. For y \u2208 D m and t, \u03b4 > 0,\nwe write sup0\u2264s\u2264t |y(s)| = |y|\u221e,t and sup0\u2264s1 \u2264s2 \u2264t,|s1 \u2212s2 |\u2264\u03b4 |y(s1 ) \u2212 y(s2 )| =\nP\n2\n\u031fyt (\u03b4), where for z = (z1 , . . . , zm )\u2032 \u2208 Rm , |z|2 = m\ni=1 |zi | . All vector inequalities are to be interpreted component-wise. We will call a function f \u2208 D m\nnonnegative if f (t) \u2265 0 for all t \u2208 R+ . A function f \u2208 D m is called nondecreasing if it is nondecreasing in each component. All (stochastic) processes\nin this work will have sample paths that are right continuous and have\nleft limits, and thus can be regarded as D m -valued random variables with\na suitable m. For a Polish space E, B(E) will denote the corresponding Borel\nsigma-field. Weak convergence of (E, B(E)) valued random variables Zn to Z\nwill be denoted as Zn \u21d2 Z. Sequence of processes {Zn } is tight if and only if\nthe measures induced by Zn 's on (D m , B(D m )) form a tight sequence. A sequence of processes with paths in D m (m \u2265 1) is called C-tight if it is tight\nin D m and any weak limit point of the sequence has paths in C m almost\nsurely (a.s.). For processes {Zn }, Z defined on a common probability space,\nwe say that Zn converge to Z, uniformly on compact time intervals (u.o.c.),\nin probability (a.s.) if for all t > 0, sup0\u2264s\u2264t |Zn (s) \u2212 Z(s)| converges to zero\nin probability (resp. a.s.). To ease the notational burden, standard notation\n(that follow [6, 7]) for different processes are used (e.g., Q for queue-length, I\nfor idle time, W for workload process etc.). We also use standard notation,\nfor example, W\u0304 , \u0174 , to denote fluid scaled, respectively, diffusion scaled, versions of various processes of interest [see (2.21) and (2.22)]. All vectors will\nbe column vectors. An m-dimensional vector with all entries 1 will be denoted by 1m . For a vector a, diag(a) will denote the diagonal matrix such\nthat the vector of its diagonal entries is a. M \u2032 will denote the transpose\nof a matrix M . Also, Ci , i = 0, 1, 2, . . . , will denote generic constants whose\nvalues may change from one proof to the next.\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n7\n\n2. Multiclass queueing networks and the control problem. Let (\u03a9, F, P)\nbe a probability space. All the random variables associated with the network\nmodel described below are assumed to be defined on this probability space.\nThe expectation operation under P will be denoted by E.\nNetwork structure. We begin by introducing the family of stochastic processing network models that will be considered in this work. We closely follow\nthe terminology and notation used in [4, 6, 7, 16, 18, 20]. The network has I\ninfinite capacity buffers (to store I many different classes of jobs) and K\nnonidentical servers for processing jobs. Arrivals of jobs, given in terms of\nsuitable renewal processes, can be from outside the system and/or from the\ninternal rerouting of jobs that have already been processed by some server.\nSeveral different servers may process jobs from a particular buffer. Service\nfrom a given buffer i by a given server k is called an activity. Once a job\nstarts being processed by an activity, it must complete its service with that\nactivity, even if its service is interrupted for some time (e.g., for preemption by a job from another buffer). When service of a partially completed\njob is resumed, it is resumed from the point of preemption-that is, the\njob needs only the remaining service time from the server to get completed\n(preemptive-resume policy). Also, an activity must complete service of any\njob that it started before starting another job from the same buffer. An activity always selects the oldest job in the buffer that has not yet been served,\nwhen starting a new service [i.e., First In First Out (FIFO) within class].\nThere are J activities [at most one activity for a server-buffer pair (i, k), so\nthat J \u2264 I * K]. Here the integers I, J, K are strictly positive. Figure 1 gives\na schematic for such a model.\nLet I = {1, . . . , I}, J = {1, . . . , J} and K = {1, . . . , K}. The correspondence\nbetween the activities and buffers, and activities and servers are described\nby two matrices C and A respectively. C is an I \u00d7 J matrix with Cij = 1 if the\njth activity processes jobs from buffer i, and Cij = 0 otherwise. The matrix A\nis K \u00d7 J with Akj = 1 if the kth server is associated with the jth activity,\nand Akj = 0 otherwise. Each activity associates one buffer and one server,\nand so each column of C has exactly one 1 (and similarly, every column\nof A has exactly one 1). We will further assume that each row of C (and A)\nhas at least one 1, that is, each buffer is processed by (server is processing,\nresp.) at least one activity. For j \u2208 J, let \u03c3(j) \u2261 (\u03c31 (j), \u03c32 (j)) = (i, k), if\nactivity j corresponds to the kth server processing class i jobs. Let, for\n.\n.\nk \u2208 K, J(k) = {j \u2208 J : \u03c32 (j) = k} and I(k) = {\u03c31 (j) : j \u2208 J(k)}. Thus, for the\nkth server, J(k) denotes the set of activities that the server can perform,\nand I(k) represents the corresponding buffers from which the jobs can be\nprocessed.\n\n\f8\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nFig. 1. A network with I buffers, J activities, K servers and probabilistic routing (given\nby the matrix P ).\n\nStochastic primitives. We are interested in the study of networks that\nare nearly critically loaded. Mathematically, this is modeled by considering\na sequence of networks {N r } that \"approach heavy traffic,\" as r \u2192 \u221e, in\nthe sense of Definition 2.2 below. Each network in the sequence has identical\nstructure, except for the rate parameters that may depend on r. Here r \u2208\nS \u2286 R+ , where S is a countable set: {r1 , r2 , . . .} with 1 \u2264 r1 < r2 < * * * and\nrn \u2192 \u221e, as n \u2192 \u221e. One thinks of the physical network of interest as the\nrth network embedded in this sequence, for a fixed large value of r. For\nnotational simplicity, throughout the paper, we will write the limit along\nthe sequence rn as n \u2192 \u221e simply as \"r \u2192 \u221e.\" Also, r will always be taken\nto be an element of S and, thus, hereafter the qualifier r \u2208 S will not be\nstated explicitly.\nThe rth network N r is described as follows. If the ith class (i \u2208 I) has\nexogenous job arrivals, the interarrival times of such jobs are given by a sequence of nonnegative random variables {uri (n) : n \u2265 1} that are i.i.d with\nmean and standard deviation 1/\u03b1ri , \u03c3iu,r \u2208 (0, \u221e) respectively. Let, by relabeling if needed, the buffers with exogenous arrivals correspond to i \u2208\n{1, . . . , I\u2032 } := I\u2032 , where I\u2032 \u2264 I. We set \u03b1ri , \u03c3iu,r = 0 and uri (n) = \u221e, n \u2265 1, for\ni \u2208 I \\ I\u2032 . Service times for the jth type of activity (for j \u2208 J) are given by\na sequence of nonnegative random variables {vjr (n) : n \u2265 1} that are i.i.d.\nwith mean and standard deviation 1/\u03b2jr , \u03c3jv,r \u2208 (0, \u221e) respectively. We will\nassume that the above random variables are in fact strictly positive, that is,\n(2.1)\n\nfor all i \u2208 I, j \u2208 J,\n\nP(uri (1) > 0) = P(vjr (1) > 0) = 1.\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n9\n\nWe will further impose the following uniform integrability condition:\n(2.2)\n\nthe collection {(uri (1))2 , (vjr (1))2 ; r \u2265 1, j \u2208 J, i \u2208 I\u2032 }\nis uniformly integrable.\n\nRerouting of jobs completed by the jth activity is specified by a sequence\nj,r\n\u2032\nj,r\nof (I + 1)-dimensional vector {(\u03c6j,r\n0 (n), \u03c6 (n)) , n \u2265 1}, where \u03c6 (n) =\nj,r\nj,r\n(\u03c6i (n) : i \u2208 I). For each j \u2208 J and i \u2208 I \u222a {0}, \u03c6i (n) = 1 if the nth completed job by activity j gets rerouted to buffer i, and takes the value zero\notherwise, where i = 0 represents jobs leaving the system. It is assumed that\nj,r\nfor each fixed r, {(\u03c6j,r\n0 (n), \u03c6 (n)), n \u2265 1}, j \u2208 J, are (mutually) independent\nsequences of i.i.d Multinomial(I+1) (1, (pj0 , pj )), where pj = (pji : i = 1, . . . , I).\nP\nPI\nj\nThat, in particular, means, for j \u2208 J, n \u2265 1, Ii=0 \u03c6j,r\ni (n) =\ni=0 pi = 1. Furthermore, for fixed j \u2208 J, i1 , i2 \u2208 I,\n\n(2.3)\n\n\u03c6\n\nj,r\nj j\nj\nj\nCov(\u03c6j,r\ni1 (n), \u03c6i2 (n)) = \u03c3i1 i2 = \u2212pi1 pi2 + pi1 \u03b4i1 ,i2 ,\n\nwhere \u03b4i1 ,i2 is 1 if i1 = i2 and 0 otherwise. We also assume that, for each r,\nthe random variables\nj,r\n{uri (n), vjr (n), \u03c6j,r\n0 (n), \u03c6 (n), n \u2265 1, i \u2208 I, j \u2208 J}\n(2.4)\nare mutually independent.\nNext we introduce the primitive renewal processes, (E r , S r ), that describe\nthe state dynamics. The process (E1r , . . . , EIr\u2032 ) is the I\u2032 -dimensional exogenous arrival process, that is, for each i \u2208 I\u2032 , Eir (t) is a renewal process which\ndenotes the number of jobs that have arrived to buffer i from outside the\nsystem over the interval [0, t]. For class i to which there are no exogenous\narrivals (i.e., i \u2208 I \\ I\u2032 ), we set Eir (t) = 0 for all t \u2265 0. We will denote the process (E1r , . . . , EIr )\u2032 by E r . For each activity j \u2208 J, Sjr (t) denotes the number\nof complete jobs that could be processed by activity j in [0, t] if the associated server worked continuously and exclusively on jobs from the associated\nbuffer in [0, t] and the buffer had an infinite reservoir of jobs. The vector\n(S1r , . . . , SJr )\u2032 is denoted by S r . More precisely, for i \u2208 I, j \u2208 J, m \u2265 1, let\nm\n\n(2.5)\n\n. X r\nui (n),\n\u03beir (m) =\nn=1\n\nm\n\n. X r\nvj (n).\n\u03b7jr (m) =\nn=1\n\nWe set \u03beir (0) = 0, \u03b7jr (0) = 0. Then Eir , Sjr are renewal processes given as\nfollows. For t \u2265 0,\n(2.6)\n\nEir (t) = max{m \u2265 0 : \u03beir (m) \u2264 t},\n\nSjr (t) = max{m \u2265 1 : \u03b7jr (m) \u2264 t}.\n\nFinally, we introduce the routing sequences. Let \u03a6j,r\ni (n) denote the number\nof jobs that are routed to the ith buffer, among the first n jobs completed\nby activity j. Thus, for i \u2208 I, j \u2208 J,\nn\nX\n(2.7)\nn = 1, 2, . . . .\n\u03c6j,r\n(n)\n=\n\u03a6j,r\ni (m),\ni\nm=1\n\n\f10\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nj,r\n\u2032\nWe will denote the I-dimensional sequence {(\u03a6j,r\n1 (n), . . . , \u03a6I (n)) } corresponding to routing of jobs completed by the jth activity by {\u03a6j,r (n)}.\nAlso, \u03a6r (n) will denote the I \u00d7 J matrix (\u03a61,r (n), \u03a62,r (n), . . . , \u03a6J,r (n)).\n\nControl. A Scheduling policy or control for the rth SPN is specified by\na nonnegative, nondecreasing J-dimensional process T r = {(T1r (t), . . . , TJr (t))\u2032 ,\nt \u2265 0}. For any j \u2208 J, t \u2265 0, Tjr (t) represents the cumulative amount of time\nspent on the jth activity up to time t. For a control T r to be admissible, it\nmust satisfy additional properties which are specified below in Definition 2.7.\nState processes. For a given scheduling policy T r , the state processes of\nthe network are the associated I-dimensional queue length process Qr and\nthe K-dimensional idle time process I r . For each t \u2265 0, i \u2208 I, Qri (t) represents\nthe queue-length at the ith buffer at time t (including the jobs that are in\nservice at that time), and for k = 1, . . . , K, Ikr (t) is the total amount of\ntime the kth server has idled up to time t. Let q r = Qr (0) \u2208 NI be the Idimensional vector of queue-lengths at time 0. Note that, for j \u2208 J, t \u2265 0,\nSjr (Tjr (t)) is the total number of services completed by the jth activity up\nto time t. The total number of completed jobs (by activity j) up to time t\nr\nr\nthat get rerouted to buffer i equals \u03a6j,r\ni (Sj (Tj (t))). Recalling the definition\nof matrices C and A, the state of the system at time t \u2265 0 can be described\nby the following equations:\n(2.8) Qri (t) = q r + Eir (t) \u2212\n(2.9)\n\nIkr (t)\n\n=t\u2212\n\nJ\nX\nj=1\n\nJ\nX\n\nCij Sjr (Tjr (t)) +\n\nj=1\n\nj=1\n\nAkj Tjr (t),\n\nJ\nX\n\nr\nr\n\u03a6j,r\ni (Sj (Tj (t))),\n\ni \u2208 I,\n\nk \u2208 K.\n\nHeavy traffic. We now describe the main heavy traffic assumption[18,\n20]. We begin with a condition on the convergence of various parameters in\nthe sequence of networks {N r }.\nAssumption 2.1. There are q, \u03b1, \u03c3 u \u2208 RI+ , \u03b2, \u03c3 v \u2208 RJ+ , \u03b81 \u2208 RI , \u03b82 \u2208 RJ\nsuch that \u03b2 > 0, \u03c3 v > 0, \u03b1i , \u03c3iu = 0 if and only if i \u2208 I \\ I\u2032 , and, as r \u2192 \u221e,\n.\n.\n\u03b81r = r(\u03b1r \u2212 \u03b1) \u2192 \u03b81 ,\n\u03b82r = r(\u03b2 r \u2212 \u03b2) \u2192 \u03b82 ,\n(2.10)\n. qr\n\u2192 q.\n\u03c3 u,r \u2192 \u03c3 u ,\n\u03c3 v,r \u2192 \u03c3 v ,\nq\u0302 r =\nr\nThe definition of heavy traffic, for the sequence {N r }, as introduced in [20]\n(also see [6, 7, 18]), is as follows.\n\n\f11\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nDefinition 2.2 [Heavy traffic]. Define I \u00d7 J matrices P \u2032 , R, such that\n.\nPij\u2032 = pji , for i \u2208 I, j \u2208 J, and\n.\n(2.11)\nR = (C \u2212 P \u2032 ) diag(\u03b2).\n\nWe say that the sequence {N r } approaches heavy traffic as r \u2192 \u221e if, in\naddition to Assumption 2.1, the following two conditions hold:\n\n(i) There is a unique optimal solution (x\u2217 , \u03c1\u2217 ) to the following linear\nprogram (LP):\n(2.12)\n\nminimize \u03c1 such that Rx = \u03b1 and Ax \u2264 \u03c11K\n\n(ii) The pair\n\n(x\u2217 , \u03c1\u2217 )\n\n(2.13)\nAssumption 2.3.\ntraffic as r \u2192 \u221e.\n\nsatisfies\n\nfor all x \u2265 0.\n\n\u03c1\u2217 = 1 and Ax\u2217 = 1K .\nThe sequence of networks {N r } approaches heavy\n\nRemark 2.4. From Assumption 2.3, x\u2217 given in (i) of Definition 2.2 is\nthe unique J-dimensional nonnegative vector satisfying\n(2.14)\n\nRx\u2217 = \u03b1,\n\nAx\u2217 = 1K .\n\nFollowing [20], assume without loss of generality (by relabeling activities, if\nnecessary), that the first B components of x\u2217 are strictly positive (corresponding activities are referred to as basic) and the rest are zero (nonbasic\nactivities). For later use, we partition the following matrices and vectors in\nterms of basic and nonbasic components:\n\u0014 r\u0015\n\u0014 \u2217\u0015\nT\nxb\nr\n\u2217\n(2.15)\n, T = br , A = [B : N ], R = [H : M ],\nx =\nTn\n0\nwhere T r is some control policy, 0 is a (J \u2212 B)-dimensional vector of zeros, B, N, H, M are K \u00d7 B, K \u00d7 (J \u2212 B), I \u00d7 B and I \u00d7 (J \u2212 B) matrices,\nrespectively.\n\nThe following assumption (see [7]) says that for each buffer there is an\nassociated basic activity.\nx\u2217j\n\nAssumption 2.5.\n> 0.\n\nFor every i \u2208 I, there is a j \u2208 J such that Rij > 0 and\n\nOther processes. Components of the vector x\u2217 defined above can be interpreted as the nominal allocation rates for the J activities. Given a control\npolicy T r , define the deviation process Y r as the difference between T r and\nthe nominal allocation:\n.\n(2.16)\nY r (t) = x\u2217 t \u2212 T r (t),\nt \u2265 0.\n\n\f12\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nIt follows from (2.9) and (2.14) that the idle-time process I r has the following\nrepresentation:\nI r (t) = AY r (t),\n\nt \u2265 0.\n.\nLet N = K + J \u2212 B. Next we define a N \u00d7 J matrix K and N-dimensional\nprocess U r as follows:\n\u0014\n\u0015\n. B N\n.\nK=\n(2.17)\n,\nU r (t) = KY r (t),\nt \u2265 0,\n0 \u2212I\n\nwhere I denotes a (J \u2212 B) \u00d7 (J \u2212 B) identity matrix. Note that, with Tnr as\nin (2.15),\n\u0014 r \u0015\nI (t)\nr\nU (t) =\n(2.18)\n,\nt \u2265 0.\nTnr (t)\n\nFinally, we introduce the workload process W r which is defined as a certain\nlinear transformation of the queue-length process and is of dimension no\ngreater than of the latter. More precisely, W r is an L-dimensional process\n(L = I + K \u2212 B, see [7]) defined as\n\n(2.19)\n\nW r (t) = \u039bQr (t),\n\nt \u2265 0,\n\nwhere \u039b is a L \u00d7 I-dimensional matrix with rank L and nonnegative entries,\ncalled the workload matrix. We will not give a complete description of \u039b\nsince that requires additional notation; and we refer the reader to [7, 18] for\ndetails. The key fact that will be used in our analysis is that there is a L \u00d7 N\nmatrix G with nonnegative entries (see (3.11) and (3.12) in [18]) such that\n(2.20)\n\n\u039bR = GK.\n\nWe will impose the following additional assumption on G which says that\neach of its columns has at least one strictly positive entry. The assumption\nis needed in the proof of Lemma 3.10 [see (3.36)].\nAssumption 2.6.\n|Gu| \u2265 c|u|.\n\nThere exists a c > 0 such that for every u \u2208 RN\n+,\n\nRescaled processes. We now introduce two types of scalings. The first\nis the so-called fluid scaling, corresponding to a law of large numbers, and\nthe second is the standard diffusion scaling, corresponding to a central limit\ntheorem.\nFluid Scaled Process: This is obtained from the original process by accelerating time by a factor of r 2 and scaling down space by the same factor.\nThe following fluid scaled processes will play a role in our analysis. For t \u2265 0,\n.\n.\n\u0112 r (t) = r \u22122 E r (r 2 t),\nS\u0304 r (t) = r \u22122 S r (r 2 t),\n.\n.\nr\n(2.21)\n\u03a6\u0304 (t) = r \u22122 \u03a6r (\u230ar 2 t\u230b),\nT\u0304 r (t) = r \u22122 T r (r 2 t),\n.\n.\nI \u0304r (t) = r \u22122 I r (r 2 t),\nQ\u0304r (t) = r \u22122 Qr (r 2 t).\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n13\n\nHere for x \u2208 R+ , \u230ax\u230b denotes its integer part, that is, the greatest integer\nbounded by x.\nDiffusion Scaled Process: This is obtained from the original process by\naccelerating time by a factor of r 2 and, after appropriate centering, scaling\ndown space by r. Some diffusion scaled processes that will be used are as\nfollows. For t \u2265 0,\n\n(2.22)\n\n. (S r (r 2 t) \u2212 \u03b2 r r 2 t)\n. (E r (r 2 t) \u2212 \u03b1r r 2 t)\n,\n\u015c r (t) =\n,\n\u00ca r (t) =\nr\nr\nr\nr\n. (\u03a6 (\u230ar 2 t\u230b]) \u2212 \u230ar 2 t\u230bP \u2032 )\n\u03a6\u0302 (t) =\n,\nr\n.\n.\n\u00db r (t) = r \u22121 U r (r 2 t),\nQ\u0302r (t) = r \u22121 Qr (r 2 t),\n.\n.\n\u0174 r (t) = r \u22121 W r (r 2 t),\n\u0176 r (t) = r \u22121 Y r (r 2 t).\n\nThe processes U r , Qr , W r are not centered, as one finds (see Lemma 3.3\nof [9]) that, with any reasonable control policy, their fluid scaled versions\nconverge to zero as r \u2192 \u221e. Define for t \u2265 0,\nJ\n\n(2.23)\n\nX\n.\n(Cij \u2212 pji )\u015cjr (T\u0304jr (t))\nX\u0302ir (t) = \u00cair (t) \u2212\nj=1\n\n\u2212\n\nJ\nX\n\nj,r\n\n\u03a6\u0302i (S\u0304jr (T\u0304jr (t))),\n\nj=1\n\ni \u2208 I.\n\nRecall \u03b8ir and q\u0302 r from Assumption 2.1. Using (2.8), (2.9), (2.14) and (2.17),\none has the following relationships between the various scaled quantities\ndefined above. For all t \u2265 0,\nQ\u0302r (t) = \u03b6\u0302 r (t) + R\u0176 r (t),\n\n\u00db r (t) = K \u0176 r (t),\n\nwhere\n(2.24)\n\n\u03b6\u0302 r (t) = q\u0302 r + X\u0302 r (t) + [\u03b81r t \u2212 (C \u2212 P \u2032 ) diag(\u03b82r )T\u0304 r (t)].\n\nAlso, using (2.19), (2.20) and (2.24), for all t \u2265 0,\n(2.25)\n\n\u0174 r (t) = \u039bq\u0302 r + \u039bX\u0302 r (t) + \u039b[\u03b81r t \u2212 (C \u2212 P \u2032 ) diag(\u03b82r )T\u0304 r (t)] + G\u00db r (t).\n\nAdmissibility of control policies. The definition of admissible policies\n(Definition 2.7), given below, incorporates appropriate nonanticipativity requirements and ensures feasibility by requiring that the associated queuelength and idle-time processes (Qr , I r ) are nonnegative.\nFor m = (m1 , . . . , mI ) \u2208 NI , n = (n1 , . . . , nJ ) \u2208 NJ we define the multiparameter filtration generated by interarrival and service times and routing\n\n\f14\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nvariables as\n(2.26)\n\nF\u0304 r ((m, n))\n\u2032\n\u2032\n\u2032\n= \u03c3{uri (m\u2032i ), vjr (n\u2032j ), \u03c6j,r\ni (nj ) : mi \u2264 mi , nj \u2264 nj ; i \u2208 I, j \u2208 J}.\n\nThen {F\u0304 r ((m, n)) : m \u2208 NI , n \u2208 NJ } is a multiparameter filtration with the\nfollowing (partial) ordering:\n(m1 , n1 ) \u2264 (m2 , n2 ) if and only if\n\nm1i \u2264 m2i , n1j \u2264 n2j ; i \u2208 I, j \u2208 J.\n\nWe refer the reader to Section 2.8 of [15] for basic definitions and properties\nof multiparameter filtrations, stopping times and martingales. Let\n_\n.\n(2.27)\nF\u0304 r ((m, n)).\nF\u0304 r =\n(m,n)\u2208NI+J\n\nFor all (m, n) \u2208 {0, 1}I+J , we define F\u0304 r ((m, n)) = F\u0304 r ((1, 1)) where 1 denotes\nthe vector of 1's. It will be convenient to allow for extra randomness, than\nthat captured by F\u0304 r , in formulating the class of admissible policies. Let G be\n.\na \u03c3-field independent of F\u0304 r . For m \u2208 NI , n \u2208 NJ , let F r ((m, n)) \u2261 FGr (m, n) =\nF\u0304 r ((m, n)) \u2228 G.\nDefinition 2.7. For a fixed r and q r \u2208 RI+ , a scheduling policy T r =\n{(T1r (t), . . . , TJr (t)) : t \u2265 0} is called admissible for N r with initial condition q r\nif for some G independent of F\u0304 r , the following conditions hold:\n\n(i) Tjr is nondecreasing, nonnegative and satisfies Tjr (0) = 0 for j \u2208 J.\n(ii) Ikr defined by (2.9) is nondecreasing, nonnegative and satisfies Ikr (0) =\n0 for k = 1, . . . , K.\n(iii) Qri defined in (2.8) is nonnegative for i \u2208 I.\n(iv) Define for each r, t \u2265 0,\n(2.28)\n\n\u03c30r (t) = (\u03c30r,E (t), \u03c30r,S (t))\n.\n= (Eir (r 2 t) + 1 : i \u2208 I; Sjr (Tjr (r 2 t)) + 1 : j \u2208 J).\n\nThen, for each t \u2265 0,\n(2.29)\n\n\u03c30r (t) is a {F r ((m, n)) : m \u2208 NI , n \u2208 NJ } stopping time.\n\nDefine the filtration {F1r (t) : t \u2265 0} as\n.\nF1r (t) =F r (\u03c30r (t))\n(2.30)\n=\u03c3{A \u2208 F r : A \u2229 {\u03c30r (t) \u2264 (m, n)} \u2208 F r ((m, n)), m \u2208 NI , n \u2208 NJ }.\nThen\n(2.31)\n\n\u00db r is {F1r (t)}-adapted.\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n15\n\nDenote by Ar (q r ) the collection of all admissible policies for N r with initial\ncondition q r .\nRemark 2.8. (i) and (ii) in Definition 2.7 imply, in view of (2.9) and\nproperties of the matrix A, that\n(2.32)\n\n0 \u2264 Tjr (t) \u2212 Tjr (s) \u2264 t \u2212 s,\n\nj \u2208 J for all 0 \u2264 s \u2264 t < \u221e.\n\nIn particular, Tjr is a process with Lipschitz continuous paths. Condition (iv)\nin Definition 2.7 can be interpreted as a nonanticipativity condition. Proposition 2.8 and Theorem 5.4 of [9] give general sufficient conditions under\nwhich this property holds (see also Proposition 4.1 of the current work).\nCost function. For the network N r , we consider an expected infinite horizon discounted (linear) holding cost associated with a scheduling policy T r\nand initial queue length vector q r :\n\u0013\n\u0013\n\u0012Z \u221e\n\u0012Z \u221e\n\u2212\u03b3t\nr\n\u2212\u03b3t\nr\nr r\nr .\n(2.33) J (q , T ) = E\ne p * d\u00db (t) .\ne h * Q\u0302 (t) dt + E\n0\n\n0\n\nHere, \u03b3 \u2208 (0, \u221e) is the \"discount factor\" and h, an I-dimensional vector\nwith each component hi \u2208 (0, \u221e), i \u2208 I, is the vector of \"holding costs\" for\nthe I buffers. In the second term, p \u2265 0 is an N-dimensional vector. The\nfirst K block of U corresponds to the idleness process I, and, thus, the\nsecond term in the cost, in particular, captures the idleness cost. The last\nJ \u2212 B components of U correspond to the time spent on nonbasic activities.\nThus, this formulation of the cost allows, in addition to the idleness cost,\nthe user to put a penalty for using nonbasic activities.\nThe formulation of the cost function considered in our work goes back to\nthe original work of Harrison et al. [16, 20].\nThe scheduling control problem for N r is to find an admissible control\npolicy T r that minimizes the cost J r . The value function V r for this control\nproblem is defined as\n.\n(2.34)\nV r (q r ) = inf\nJ r (q r , T r ),\nq r \u2208 NI0 .\nT r \u2208Ar (q r )\n\nBrownian control problem. The goal of this work is to characterize the\nlimit of value functions V r as r \u2192 \u221e, as the value function of a suitable\ndiffusion control problem. In order to see the form of the diffusion control\nproblem, we will like to send r \u2192 \u221e in (2.24). Using the functional central\nlimit theorem for renewal processes, it is easily seen that, for all reasonable\ncontrol policies (see again Lemma 3.3 of [9]), when q\u0302 r converges to some\nq \u2208 RI+ , \u03b6\u0302 r defined in (2.24) converges weakly to\n(2.35)\n\n\u03b6\u0303 = q + X\u0303 + \u03b8i ,\n\n\f16\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nwhere\n(2.36)\n\n.\n\u03b8 = \u03b81 \u2212 (C \u2212 P \u2032 ) diag(\u03b82 )x\u2217 .\n\nHere i (s) = s, s \u2265 0 is the identity map and X\u0303 is a Brownian motion with\ndrift 0 and covariance matrix\nJ\n\n(2.37)\n\nX\nj\n.\n\u03b2j x\u2217j \u03a3\u03c6 ,\n\u03a3 = \u03a3u + (C \u2212 P \u2032 )\u03a3v diag(x\u2217 )(C \u2212 P \u2032 )\u2032 +\nj=1\n\nwhere \u03a3u is a I \u00d7 I diagonal matrix with diagonal entries (\u03c3iu )2 , i \u2208 I, \u03a3v\nj\nis a J \u00d7 J diagonal matrix with diagonal entries (\u03c3jv )2 , j \u2208 J and \u03a3\u03c6 s are\nj\n\nI \u00d7 I matrices with entries \u03c3i\u03c61 i2 , i1 , i2 \u2208 I [see (2.3)]. Although the process \u0176 r\nin (2.24), for a general policy sequence {T r }, need not converge, upon formally taking limit as r \u2192 \u221e, one is led to the following diffusion control\nproblem.\n\nDefinition 2.9 [Brownian Control Problem (BCP)]. A J-dimensional\nadapted process \u1ef8 , defined on some filtered probability space (\u03a9\u0303, F\u0303, P\u0303, {F\u0303 (t)})\nwhich supports an I-dimensional {F\u0303 (t)}-Brownian motion X\u0303 with drift 0\nand covariance matrix \u03a3 given by (2.37), is called an admissible control\nfor the Brownian control problem with the initial condition q \u2208 RI+ iff the\nfollowing two properties hold P\u0303-a.s.:\n.\n(2.38)\nQ\u0303(t) = \u03b6\u0303(t) + R\u1ef8 (t) \u2265 0\nwhere \u03b6\u0303(t) = q + X\u0303(t) + \u03b8t, t \u2265 0,\n.\n(2.39)\n\u0168 = K \u1ef8 is nondecreasing and \u0168 (0) \u2265 0,\nwhere \u03b6\u0303 and \u03b8 are as in (2.35) and (2.36) respectively. We refer to \u03a6 = (\u03a9\u0303, F\u0303,\nP\u0303, {F\u0303 (t)}, X\u0303) as a system. We denote the class of all such admissible controls\nby \u00c3(q). The Brownian control problem is to\n#\n\"Z\nZ\n\u221e\n.\n \u0303 \u1ef8 ) = \u1ebc\n(2.40)\ne\u2212\u03b3t p * d\u0168 (t) ,\ninfimize J(q,\ne\u2212\u03b3t h * Q\u0303(t) dt +\n[0,\u221e)\n\n0\n\nover all admissible controls \u1ef8 \u2208 \u00c3(q). Define the value function\n(2.41)\nJ \u0303\u2217 (q) = inf J \u0303(q, \u1ef8 ).\n\u1ef8 \u2208\u00c3(q)\n\nRecall our standing assumptions (2.1), (2.2), (2.4), Assumptions 2.1, 2.3,\n2.5 and 2.6. The following is the main result of [9].\nTheorem 2.10 (Budhiraja and Ghosh [9], Theorem 3.1, Corollary 3.2).\nFix q \u2208 RI+ and for r > 0, q r \u2208 NI such that q\u0302 r \u2192 q as r \u2192 \u221e. Then\nlim inf V r (q r ) \u2265 J \u0303\u2217 (q).\nr\u2192\u221e\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n17\n\nRemark 2.11. The proof in [9] is presented for the case where in the definition of V r (q r ) [see (2.34)], Ar (q r ) is replaced by the smaller family \u0100r (q r )\nwhich consists of all T r \u2208 Ar (q r ) that satisfy (iv) of Definition 2.7 with\nF r ((m, n)) replaced by F\u0304 r ((m, n)). Proof for the slightly more general setting considered in the current paper requires only minor modifications and,\nthus, we omit the details.\nFor the main result of this work, we will need additional assumptions.\nAssumption 2.12.\n\nThe matrix \u03a3 is positive definite.\n\nWe will make the following assumption on the probabilities of deviations\nfrom the mean for the underlying renewal processes. Similar conditions have\nbeen used in previous works on construction of asymptotically optimal control policies [1, 3, 4, 8, 12].\nAssumption 2.13. There exists m > 2 and, for each \u03b4 > 0, some \u03c2(\u03b4) \u2208\n(0, \u221e) such that, for j \u2208 J, i \u2208 I, r \u2265 1, t \u2208 (1, \u221e),\n\u03c2(\u03b4)\n,\ntm\n\u03c2(\u03b4)\nP(|Eir (t) \u2212 \u03b1ri t| \u2265 \u03b4t) \u2264 m ,\nt\n\u03c2(\u03b4)\nj r\nr\nP(|\u03a6j,r\ni (Sj (t)) \u2212 pi \u03b2j t| \u2265 \u03b4t) \u2264 m .\nt\nP(|Sjr (t) \u2212 \u03b2jr t| \u2265 \u03b4t) \u2264\n\nThe third inequality above is a consequence of the first two, but we note\nit explicitly here for future use. The assumption is clearly satisfied when Ei\nand Sj are Poisson processes. For general renewal processes, such inequalities\nhold under suitable moment conditions on the interarrival and service time\ndistributions. Indeed, if for some m > 1\ni\nh\nE sup[uri (1)]2m < \u221e,\n\n(2.42)\n\nr\n\ni\nE sup[vjr (1)]2m < \u221e\nh\n\nr\n\nfor all i \u2208 I, j \u2208 J,\n\nthen, from Theorem 4 of [21], Assumption 2.13 is satisfied.\nWe now introduce an assumption on the regularity properties of a certain Skorohod map. This map plays a crucial role in our analysis; see proofs\nof Theorems 3.5 and 4.5 [see in particular, (3.13), (3.39), discussion be.\nI =\n{x \u2208 D I : x(0) \u2265 0} and\nlow (3.45) and the proof of Theorem 3.8]. Let D+\n(2.43)\n\nD = (C \u2212 P \u2032 ) diag(\u03b2) diag(x\u2217 )C \u2032 .\n\n\f18\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nI , we say (z, y) \u2208 D I \u00d7 D I solve the\nDefinition 2.14. Given x \u2208 D+\nSkorohod Problem (SP) for (x, D) if: (i) z(0) = x(0), (ii) z = x + Dy, (iii) y is\nnondecreasing\nand y(0) \u2265 0, (iv) z(t) \u2265 0 for all t \u2265 0, and\nR\n(v) [0,\u221e) 1{xi (t)>0} dyi (t) = 0 for all i \u2208 I.\n\nI such that there is a unique solution\nDenoting by D0 the set of x \u2208 D+\nto the SP for (x, D), we define maps \u0393 : D0 \u2192 D I , \u0393\u0302 : D0 \u2192 D I as \u0393(x) = z,\n\u0393\u0302(x) = y if (z, y) solve the SP for (x, D).\nI and the maps \u0393, \u0393\u0302 are Lipschitz, namely,\nAssumption 2.15. D0 = D+\nI,\nthere exists L \u2208 (0, \u221e) such that for all x1 , x2 \u2208 D+\n\nsup {|\u0393(x1 )(t) \u2212 \u0393(x2 )(t)| + |\u0393\u0302(x1 )(t) \u2212 \u0393\u0302(x2 )(t)|} \u2264 L sup |x1 (t) \u2212 x2 (t)|.\n\n0\u2264t<\u221e\n\n0\u2264t<\u221e\n\nWe refer the reader to [13, 14] and [19] for sufficient conditions under which\nthe above regularity property of the Skorohod map holds. See also Example 1\nbelow. For later use we introduce the notation \u0393\u0304(x) = diag(x\u2217 )C \u2032 \u0393\u0302(x) for\nI . Since A has nonnegative entries and x\u2217 = 0 for j = B + 1, . . . , J, we\nx \u2208 D+\nj\nsee from the definition of K [see (2.17)] that\n(2.44)\n\nif y = \u0393\u0304(x), then Ky \u2265 0.\n\nFor rest of the paper, in addition to the assumptions listed above Theorem 2.10, Assumptions 2.12, 2.13 and 2.15 will be in force. The main result\nof the paper is the following.\nTheorem 2.16.\nas r \u2192 \u221e. Then\n\nFix q \u2208 RI+ . Let for r > 0, q r \u2208 NI0 be such that q\u0302 r \u2192 q\nlim sup V r (q r ) \u2264 J \u0303\u2217 (q).\nr\u2192\u221e\n\nThe theorem is an immediate consequence of Theorem 2.17 below which\nis proved in Section 4. For \u03b5 > 0, we say Y \u2208 \u00c3(q) is \u03b5-optimal for the BCP\nwith initial value q if\n \u0303 Y ) \u2264 J \u0303\u2217 (q) + \u03b5.\nJ(q,\nWhen clear from the context, we will omit the phrase \"for the BCP with\ninitial value q\" and merely say that Y is \u03b5-optimal.\nTheorem 2.17. Fix q \u2208 RI+ . For r > 0, let q r \u2208 NI0 be such that q\u0302 r \u2192 q\nas r \u2192 \u221e. For every \u03b5 > 0, there exists \u1ef8 \u2208 \u00c3(q), which is \u03b5-optimal, and\na sequence T r \u2208 Ar (q r ), r \u2265 1 such that\n \u0303 \u1ef8 )\nJ r (q r , T r ) \u2192 J(q,\nas r \u2192 \u221e.\nCombining Theorems 2.10 and 2.16, the following is immediate.\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n19\n\nTheorem 2.18. Fix q \u2208 RI+ . For r > 0, let q r \u2208 NI0 be such that q\u0302 r \u2192 q\nas r \u2192 \u221e. Then, as r \u2192 \u221e, V r (q r ) \u2192 J \u0303\u2217 (q).\nAssumptions made in this work can loosely be divided into two categories:\nAssumptions on the underlying stochastic primitives, which include, in particular, the heavy traffic conditions (Assumptions 2.1, 2.3, 2.12 and 2.13),\nand assumptions made on the network structure (Assumptions 2.5, 2.6\nand 2.15). Below we discuss the validity of these structural assumptions\nfor some basic families of SPN models.\nExample 1. The following examples have been described in detail in [7].\nWe will assume here, without loss of generality, that \u03b2j > 0 for all j \u2208 J (an\nactivity j for which \u03b2j = 0 can simply be deleted from the network description). Furthermore, for all three settings considered below, Assumption 2.5\ncan be made without loss of generality, since otherwise one can consider\na reduced system obtained by omitting the buffers that are not processed\nby any basic activity. Assumption 2.6 states that the matrix G can be chosen\nin a manner such that it has no columns that are identically zero. Roughly\nspeaking, it says that a nonzero control action leads to a nonzero state displacement. Although this appears to be a very natural geometric condition\nand is trivially satisfied for networks in part (a) below, it is not clear that it\nholds always for examples in parts (b) and (c) below. We will assume this\ncondition to hold without further comment.\nThus, in discussion below, we will focus only on Assumption 2.15.\n(a) Open multiclass queueing networks: These correspond to a setting\nwhere each buffer is processed by exactly one activity and, consequently,\nthere is a one-to-one correspondence between activities and buffers, that is,\nJ = I (see left figure in Figure 2 for an example). For such networks, R is\nan I \u00d7 I-matrix of the form R = (I \u2212 P \u2032 ) diag(\u03b2) where P is a nonnegative\n\nFig. 2.\n\nOpen multiclass network (left) and parallel-server system (right).\n\n\f20\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nFig. 3.\n\nA Job-shop network. Here p61 = p51 , p62 = p52 , p33 = p43 .\n\nmatrix with spectral radius less than 1. In particular, R is nonsingular, K is\na K \u00d7 J matrix with full row rank and one can take \u039b = KR\u22121 and G = I.\nHere D = (I \u2212 P \u2032 ) diag(\u03b2) diag(x\u2217 ) and from [19] it is known that for such D\nAssumption 2.15 is satisfied.\n(b) Parallel server networks: For such SPN, a buffer can be served by\nmore than one activity, however, each job gets processed exactly once before\nleaving the system (i.e., there is no rerouting). See right figure in Figure 2\nfor an example. In particular, P = 0 and, hence, RP\n= C diag(\u03b2). In this case,\nD = C diag(\u03b2) diag(x\u2217 )C \u2032 \u2261 diag(\u03b3 \u2217 ), where \u03b3i\u2217 = j : \u03c31 (j)=i \u03b2j x\u2217j , for i \u2208 I.\nFrom Assumption 2.5 (which, as was noted above, can be made without\nloss of generality) we have that \u03b3 \u2217 > 0 and, thus, D is a diagonal matrix\nwith strictly positive diagonal entries. Assumption 2.15 is clearly satisfied\nfor such matrices.\n(c) Job-shop networks: This subclass of networks combines features of\nboth (a) and (b): A buffer can be processed by more than one activity and\njobs, once served, can get rerouted to another buffer for additional processing. See Figure 3 for an example. Following specific examples considered\nin [22] (see also [7]), we define job-shop networks as those which satisfy the\nfollowing property: If for some j, j \u2032 \u2208 J, and i \u2208 I, \u03c31 (j) = \u03c31 (j \u2032 ) = i, then\npji\u2032 = pj \u2032 i\u2032 for all i\u2032 \u2208 I. Namely, jobs corresponding to any two activities that\nprocess the same buffer i have an identical (probabilistic) routing structure,\nfollowing their completion by the respective servers. It is easily checked that\nin this case D = (I \u2212 P \u0303\u2032 ) diag(\u03b3 \u2217 ), where \u03b3 \u2217 is as introduced in (b) and P\u0303\nis an I \u00d7 I-dimensional matrix with entries p\u0303i,i\u2032 = pj,i\u2032 where j \u2208 J is such\nthat \u03c31 (j) = i. Under the condition that P\u0303 has spectral radius less than 1,\nit follows from [19] that Assumption 2.15 is satisfied.\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n21\n\n3. Near-optimal controls for BCP. The rest of the paper is devoted to\nthe proof of Theorem 2.17. Toward that goal, in this section we construct\nnear-optimal controls for the BCP with certain desirable features. This construction is achieved in Theorem 3.8, which is the main result of this section.\nSince an admissible control is not required to be of bounded variation, the\nBCP is a somewhat nonstandard diffusion control problem and is difficult\nto analyze directly. However, as shown in [20], under assumptions made in\nthis paper, one can replace this control problem by an equivalent problem of\nSingular Control with State Constraints (SCSC). This control problem, also\nreferred to as the Equivalent Workload Formulation (EWF) of the BCP, is\ngiven below. We begin by introducing the cost function, that is, optimized\nin this equivalent control problem.\nEffective cost function: Recall the definition of the workload matrix \u039b\n.\nintroduced in (2.19). Let W = {\u039bz : z \u2208 RI+ }. For each w \u2208 W, define\n.\n(3.1)\n\u0125(w) = inf{h * q : \u039bq = w, q \u2265 0}.\n\nSince h > 0, the infimum is attained for all w \u2208 W. It is well known (see\nTheorem 2 of [5]) that one can take a continuous selection of the minimizer\nin the above linear program. That is, there is a continuous map q\u0303 \u2217 : W \u2192 RI+\nsuch that\n(3.2)\n\nq\u0303 \u2217 (w) \u2208 argmin{h * q : \u039bq = w, q \u2265 0}.\nq\n\nThus, in particular, \u0125 is continuous. One can check that \u0125 satisfies linear\nlower and upper bounds. In order to see this, define\n\u0012\n\u0013\nw\nq \u2217 (w) = q\u0303 \u2217 (w)1{|w|\u22641} + |w|q\u0303 \u2217\n(3.3)\n1\n.\n|w| {|w|>1}\n\nThen (3.2) holds with q\u0303 \u2217 replaced by q \u2217 . Since \u0125(w) = h * q \u2217 (w) and h > 0,\nwe have from the above display that\n\n(3.4)\n\nb1 |w| \u2212 b2 \u2264 |\u0125(w)| \u2264 b3 (1 + |w|),\n\nw\u2208W\n\nfor some b1 , b2 , b3 \u2208 (0, \u221e). Also, uniform continuity of q \u2217 on {w \u2208 W : |w| \u2264 1}\nshows that\n(3.5)\n\n|\u0125(w1 ) \u2212 \u0125(w2 )| \u2264 m\u0302(\u03b4)(1 + |w1 | + |w2 |),\nw1 , w2 \u2208 W, |w1 \u2212 w2 | \u2264 \u03b4.\n\nHere m\u0302 is a modulus, that is, a nondecreasing function from [0, \u221e) \u2192 [0, \u221e)\nsatisfying m\u0302(0+) = 0. Inequalities (3.4) and (3.5) will be used in order to\nappeal to some results from [2, 10] (see Remark 3.4 below). Define\n.\n(3.6)\nK = {u \u2208 RN |u = Ky, y \u2208 RJ }.\nThe Equivalent Workload Formulation (EWF) and the associated control\nproblem are defined as follows.\n\n\f22\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nDefinition 3.1 [Equivalent Workload Formulation (EWF)]. An N-dimensional adapted process \u0168 , defined on some filtered probability space (\u03a9\u0303,\nF\u0303 , P\u0303, {F\u0303 (t)}) which supports an I-dimensional {F(t)}-Brownian motion X\u0303\nwith drift 0 and covariance matrix \u03a3 defined in (2.37), is called an admissible control for the EWF with initial condition w \u2208 W iff the following two\nproperties hold P\u0303-a.s.:\n(3.7)\n\n\u0168 is nondecreasing,\n\u0168 (0) \u2265 0,\n\u0168 (t) \u2208 K\nfor all t \u2265 0,\n.\nW\u0303 (t) = w + \u039b\u03b8t + \u039bX\u0303(t) + G\u0168 (t) \u2208 W\nfor all t \u2265 0,\n\nwhere \u03b8 is as in (2.36). We denote the class of all such admissible controls\nby \u00c30 (w). The control problem for the EWF is to\nZ\nZ \u221e\n.\n\u2212\u03b3t\n \u0303\n(3.8) infimize J0 (w, \u0168 ) = \u1ebc\ne\u2212\u03b3t p * d\u0168 (t),\ne \u0125(W\u0303 (t)) dt + \u1ebc\n[0,\u221e)\n\n0\n\nover all admissible controls \u0168 \u2208 \u00c30 (w). Define the value function\n(3.9)\nJ \u03030\u2217 (w) = inf J \u03030 (w, \u0168 ).\n\u0168 \u2208\u00c30 (w)\n\nFrom Theorem 2 of [20] it follows that for all w \u2208 W, q \u2208 RI+ satisfying\nw = \u039bq,\n(3.10)\nJ \u0303\u2217 (q) = J \u03030\u2217 (w).\nThe following lemma will be used in order to appeal to some results from [2,\n10]. The proof is based on arguments in [7]. Let K+ = K \u2229 RN\n+.\nLemma 3.2.\nGK+ 6= \u2205.\n\nThe cones K+ and GK+ have nonempty interiors and W o \u2229\n\nProof. From [7] (see above Corollary 7.4 therein) it follows that H\nhas full row rank and so there is a B \u00d7 I matrix H \u2020 such that HH \u2020 = I.\nLet xb = H \u2020 1I and let \u03b50 \u2208 (0, \u221e) be sufficiently small such that for all\n\u03b5 \u2208 (0, \u03b50 ], x\u03b5b = x\u2217b + \u03b5xb > 0. Let \u03b10 = \u03b1 + \u03b50 1I and x0 = [x\u03b5b 0 , 0]\u2032 \u2208 RJ . Then\nRx0 = \u03b10 . We will now argue that \u03b8 = \u039b\u03b10 \u2208 W o \u2229 (GK+ )o . Since the rows\nof \u039b are linearly independent, we can find an I \u00d7 L matrix \u039b\u2020 such that\n\u039b\u039b\u2020 = I. Fix \u03b4 = 2|\u039b\u03b5 \u2020 | . Then, whenever \u03b8\u0303 \u2208 RL , |\u03b8\u0303| < \u03b4, we have \u03b10 + \u039b\u2020 \u03b8\u0303 \u2208\n\nRI+ and so \u03b8 + \u03b8\u0303 = \u039b(\u03b10 + \u039b\u2020 \u03b8\u0303) \u2208 W. This shows that \u03b8 \u2208 W o . Next note\nthat \u03b8 = \u039b\u03b10 = \u039bRx0 = GKx0 . Since Kx0 = Bx\u03b5b 0 and x\u03b5b 0 > 0, we have that\nKx0 \u2208 K+ and so \u03b8 = GKx0 \u2208 GK+ . Since x\u03b5b 0 > 0, we can find \u03b51 \u2208 (0, \u221e)\nsuch that whenever x\u0303b \u2208 RB is such that |x\u0303b | \u2264 \u03b51 , x\u03b5b 0 + x\u0303b > 0. Now fix\nL with |\u03b8\u0303| \u2264 \u03b4 and x\u0303 = [H \u2020 \u039b\u2020 \u03b8\u0303, 0]\u2032 \u2208 RJ ,\n1\n\u03b41 = |H \u2020\u03b5||\u039b\n1\n\u2020 | . Then, for any \u03b8\u0303 \u2208 R\n\u03b8 + \u03b8\u0303 = \u039b(\u03b10 + \u039b\u2020 \u03b8\u0303) = \u039b(Rx0 + HH \u2020 \u039b\u2020 \u03b8\u0303) = \u039b(Rx0 + Rx\u0303) = G(K(x0 + x\u0303)).\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n23\n\nSince |H \u2020 \u039b\u2020 \u03b8\u0303| \u2264 \u03b51 , we have (x0 + x\u0303)j > 0 for all j = 1, . . . , B. Also, (x0 +\nx\u0303)j = 0 for all j = B + 1, . . . , J. Thus, K(x0 + x\u0303) \u2208 K+ and, therefore, \u03b8 + \u03b8\u0303 \u2208\nGK+ . It follows that \u03b8 \u2208 (GK+ )o .\no 6= \u2205. Since Ax\u2217 = 1 , every row of B must conFinally, we show that K+\nK\ntain at least one strictly positive entry. Thus, Bx\u03b5b 0 > 0. Choose \u03b52 \u2208 (0, \u221e)\nsufficiently small such that Bx\u03b5b 0 \u2212 \u03b52 N 1J\u2212B > 0. Let x\u0304 = [x\u03b5b 0 , \u2212\u03b52 1J\u2212B ]\u2032 \u2208\nRJ . We now argue that \u016b = K x\u0304 \u2208 (K+ )o . Note that by construction \u016b > 0.\nThus, we can find \u03b53 > 0 such that \u016b + \u0169 \u2265 0 whenever \u0169 \u2208 RN satisfies\n|\u0169| \u2264 \u03b53 . Also, since K has full row rank (see Corollary 6.2 of [7]), we can\nfind a J \u00d7 N matrix K \u2020 such that KK \u2020 = I. Thus, \u016b + \u0169 = K(x\u0304 + K \u2020 \u0169) and,\nconsequently, \u016b + \u0169 \u2208 K+ . The result follows. \u0003\nNote that the vector x0 constructed in the proof of the lemma above has\nthe property that Rx0 > 0 and Kx0 \u2265 0. Thus, we have shown the following:\nCorollary 3.3.\n\nThe set T = {y \u2208 RJ : Ky \u2265 0, Ry > 0} is nonempty.\n\nThe above result will be used in the construction of a suitable near optimal\ncontrol policy for the BCP [see below (3.14)].\nRemark 3.4. We will make use of some results from [2] and [10] that\nconcern a general family of singular control problems with state constraints.\nWe note below some properties of the model studied in the current paper\nthat ensure that the assumptions of [2] and [10] are satisfied:\n(a) G has full row rank. This follows from the observation that K, \u039b\nand R have full row ranks and, therefore,\nrank(G) = rank(GK) = rank(\u039bR) = rank(\u039b) = L.\n(b) W o \u2229 (GK+ )o 6= \u2205 and K+ has a nonempty interior (see Lemma 3.2).\n(c) (Gu) * 1L \u2265 |Gu|, u * 1N \u2265 |u| for all u \u2208 K+ and w * 1L \u2265 |w| for all\nw \u2208 W. This is an immediate consequence of the fact that the entries of G\nand \u039b are nonnegative [see above (2.20)].\n(d) Since \u039b has full row rank and, by Assumption 2.12, \u03a3 is positive\ndefinite, we have that \u039b\u03a3\u039b\u2032 is positive definite.\nThe above properties along with Assumption 2.6, (3.4) and (3.5) ensure that\nAssumptions of [2] and [10] are satisfied in our setting. In particular, Assumption (2.1)\u2013(2.2) and (2.8)\u2013(2.10) of [2] hold in view of properties (b),\n(c) and (d) and equations (3.4) and (3.5). Similarly, Assumptions (1), (5)\nand 2.2 of [10] hold in our setting [from property (c), (3.4) and Assumption 2.6, resp.]. Henceforth, when appealing to results from [2] and [10], we\nwill not make an explicit reference to these conditions.\n\n\f24\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nRecall \u03b6\u0303(t) and the map \u0393\u0304 introduced above (2.35) and (2.44), respectively. The following is a key step in the construction of a near-optimal\ncontrol with desirable properties.\nTheorem 3.5. Fix q \u2208 RI+ . For each \u03b5 \u2208 (0, \u221e), there exists \u1ef8 (1) \u2208 \u00c3(q),\ngiven on some system \u03a6, that is \u03b5-optimal and has the following properties:\n(1)\n\n\u1ef8 (1) = \u1ef80\n\n(3.11)\n\n(1)\n\n+ \u0393\u0304(\u03b6\u0303 + R\u1ef80 ),\n\n(1)\n\nwhere \u1ef80 is an adapted process with sample paths in D J satisfying the following: For some T, \u03b7, M \u2208 (0, \u221e), p0 , j0 \u2208 N, with \u03b8 = T /p0 and \u03ba = \u03b8/j0 ,\n(1)\n\n(1)\n\n(1)\n\n(i) \u1ef80 (t) = \u1ef80 (n\u03b8) for t \u2208 [n\u03b8, (n+1)\u03b8), n = 0, 1, . . . , p0 \u22121 and \u1ef80 (t) =\n(1)\n\u1ef80 (p0 \u03b8) for t \u2265 T = p0 \u03b8.\n\u03b7\n= {b\u03b7 : b \u2208 ZJ , |b|\u03b7 \u2264 M, Kb \u2265 0},\n(ii) Letting SM\n. (1)\n(1)\n(1)\n\u03b7\n\u2202 \u1ef80 (n) = \u1ef80 (n\u03b8) \u2212 \u1ef80 ((n \u2212 1)\u03b8) \u2208 SM\n,\n. (1)\n(1)\nfor n = 1, . . . , p0 and \u2202 \u1ef80 (0) = \u1ef80 (0) = 0.\n(iii) There is an i.i.d sequence of Uniform (over [0, 1]) random variables {\u0168n },\nthat is, independent of \u03b6\u0303, and for each n = 1, . . . , p0 a measurable map\n\u03b7\n\u031fn : RInj0 \u00d7 [0, 1] \u2192 SM\n, n = 1, . . . , p0 , such that the map x 7\u2192 \u031fn (x, t)\nis continuous, for a.e. t in [0, 1], and\n(3.12)\n\n(1)\n\n\u2202 \u1ef80 (n) = \u031fn (X \u03ba (n), \u0168n ),\n\nwhere X \u03ba (n) = {X\u0303(l\u03ba) : l = 1, . . . , nj0 }, n = 0, 1, . . . , p0 .\nProof of Theorem 3.5 is given in Section 3.1.\nRemark 3.6. The above theorem provides an \u03b5-optimal control \u1ef8 (1) ,\n(1)\nwhich is the \"constrained\"-version of a piecewise constant process \u1ef80 . The\n(1)\nvalue of \u1ef80 changes only at time-points that are integer multiples of \u03b8 and\nis constant for t > T = p0 \u03b8. Also, the changes in (the value of) the process\noccur in jumps with sizes that are integer multiples of some \u03b7 > 0 and are\nbounded by M . The third property in the theorem plays an important role in\nthe weak convergence proof [Theorem 4.5, see, e.g., (4.56)] and says that the\njump-sizes of this piecewise constant process are determined by the Brownian motion X\u0303 sampled at discrete instants {\u03ba, 2\u03ba, . . .} and the independent\nrandom variable \u0168n ; furthermore, the dependence on X\u0303 is continuous. The\ncontinuous dependence is ensured using a mollification argument [see below (3.46)] that has previously been used in [25].\nThe following lemma is a straightforward consequence of the Lipschitz\nproperty of the Skorohod map, the linearity of the cost and the state dynamics. Proof is given in the Appendix.\n\n\f25\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nLemma 3.7. There is a c1 \u2208 (0, \u221e) such that, if q \u2208 RI+ , T \u2208 (0, \u221e)\nand \u1ef8 1 , \u1ef8 2 \u2208 \u00c3(q) defined on a common filtered probability space are such\nthat\n\u1ef8 i (T + *) \u2212 \u1ef8 i (T ) = \u0393\u0304(Q\u0303i (T ) + \u03b6\u0303(T + *) \u2212 \u03b6\u0303(T )),\n\ni = 1, 2,\n\nwhere Q\u0303i is defined by the right-hand side of (2.38) by replacing \u1ef8 there\nby \u1ef8 i , then\n \u0303 Y 1 ) \u2212 J(q,\n \u0303 Y 2 )| \u2264 c1 E|Y 1 \u2212 Y 2 |\u221e,T .\n|J(q,\nDefine \u03b8 : RI+ \u00d7 RJ \u2192 RJ as\n\u03b8(q0 , y) = y + \u0393\u0304(q0 + Ryi )(1),\n\nq0 \u2208 RI+ , y \u2208 RJ .\n\nNote that \u03b8 is a Lipschitz map: that is, for some \u03b8lip \u2208 (0, \u221e), we have for\n(q0 , y), (q\u03030 , \u1ef9) \u2208 RI+ \u00d7 RJ ,\n|\u03b8(q0 , y) \u2212 \u03b8(q\u03030 , \u1ef9)| \u2264 \u03b8lip (|q0 \u2212 q\u03030 | + |y \u2212 \u1ef9|).\n\n(3.13)\n\nAlso, since \u03b8(q0 , 0) = 0, we have for (q0 , y) \u2208 RI+ \u00d7 RJ ,\n|\u03b8(q0 , y)| \u2264 \u03b8lip |y|.\n\n(3.14)\n\nWe now present the near-optimal control that will be used in the proof of\nTheorem 2.17. Recall the set T introduced in Corollary 3.3. Fix \u03b5 > 0, and\na unit vector y \u2217 \u2208 T and define c2 \u2208 [1, \u221e) as\n(1)\n\nc2 = max{2c1 (p0 + 1)(1 + L| diag(x\u2217 )C \u2032 |)|Ry \u2217 |, 1}.\n\nLet \u1ef80 , \u1ef8 (1) be as in Theorem 3.5 with \u03b5 = \u03b5/2. Let \u03b50 = \u03b5/c2 and define\n\u03b8\u03b50 (x, y) = \u03b8(x, y) + \u03b50 y \u2217 . Define control process \u1ef8 \u2208 \u00c3(q) with the corresponding state process Q\u0303 [defined by the right-hand side of (2.38)] by the\nfollowing equations. For n = 0, 1, . . . , p0 ,\n(3.15)\n\n(1)\n\n\u1ef8 (n\u03b8) \u2212 \u1ef8 (n\u03b8\u2212) = \u03b8\u03b50 (Q\u0303(n\u03b8\u2212), \u2202 \u1ef80 (n)),\n\nand\n(3.16) \u1ef8 (t + n\u03b8) \u2212 \u1ef8 (n\u03b8) = \u0393\u0304(Q\u0303(n\u03b8) + \u03b6\u0303(* + n\u03b8) \u2212 \u03b6\u0303(n\u03b8))(t),\n\nt \u2208 [0, \u03b8),\n\nwith the conventions that for n = p0 , [0, \u03b8) is replaced by [0, \u221e) and for\nn = 0, Q\u0303(n\u03b8\u2212) = q. The control \u1ef8 evolves in a similar manner to \u1ef8 (1) at all\ntime points excepting n\u03b8, n = 0, 1, . . . , p0 . Since y \u2217 \u2208 T, for every q0 \u2208 RI+\nand y \u2208 RJ , q0 + R\u03b8\u03b50 (q0 , y) > 0. This, along with the definition of the\nmap \u0393\u0304 (see below Assumption 2.15), ensures that Q\u0303 is nonnegative over\nthe time intervals (n\u03b8, (n + 1)\u03b8) and Q\u0303(n\u03b8) > 0, for n = 0, 1, . . . , p0 . Furthermore, (2.44) and the property Ky \u2217 > 0 (see definition of T in Corollary 3.3)\nensure that K \u1ef8 is nondecreasing and nonnegative. Thus, the process de-\n\n\f26\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nfined by relations (3.15) and (3.16) is indeed an element of \u00c3(q). The strict\npositivity of Q\u0303 at time instants n\u03b8, n \u2264 p0 will be exploited in the weak convergence analysis of Section 4 [see equation (4.61) and also below (4.44)].\nTheorem 3.8. The process \u1ef8 defined above is \u03b5-optimal for the BCP\nwith initial value q.\nProof. Since \u1ef8 (1) is \u03b5/2 optimal, in view of Lemma 3.7, it suffices to\nshow that\n\u03b5\n(3.17)\n.\n|\u1ef8 (1) \u2212 \u1ef8 |\u221e,T \u2264\n2c1\nFor this we will introduce a collection of J-dimensional processes \u1ef8(n) , n =\n0, 1, . . . , p0 + 1, such that \u1ef8(0) = \u1ef8 (1) and \u1ef8(p0 +1) = \u1ef8 (1) . These processes are\nonly used in the current proof and do not appear elsewhere in this work.\nDefine, recursively, for n = 0, 1, . . . , (p0 + 1), processes \u1ef8(n) with corresponding state processes Q\u0303(n) , as follows:\n(Q\u0303(0) , \u1ef8(0) ) = (Q\u0303(1) , \u1ef8 (1) ),\nand for n \u2265 0, t \u2265 0,\n\nQ\u0303(n+1) (t) = Q\u0303(n) (t)1[0,n\u03b8) (t) + \u0393(H\u0303 (n) )(t \u2212 n\u03b8)1[n\u03b8,\u221e)(t),\n\u1ef8(n+1) (t) = \u1ef8(n) (t)1[0,n\u03b8) (t) + [\u1ef8(n) (n\u03b8\u2212) + \u03b8\u03b50 (Q\u0303(n) (n\u03b8\u2212), \u2202 \u1ef80 (n))\nwhere for all t \u2265 0,\n\n+ \u0393\u0304(H\u0303 (n) )(t \u2212 n\u03b8)]1[n\u03b8,\u221e) (t),\n\nH\u0303 (n) (t) = Q\u0303(n) (n\u03b8\u2212) + R\u03b8\u03b50 (Q\u0303(n) (n\u03b8\u2212), \u2202 \u1ef80 (n)) + \u03b6\u0303(t + n\u03b8) \u2212 \u03b6\u0303(n\u03b8)\n+ R[\u1ef8 (t + n\u03b8) \u2212 \u1ef8 (n\u03b8)].\n\nNote that for t \u2208 [n\u03b8, \u221e),\n\n\u1ef8(n) (t) = \u1ef8(n) (n\u03b8\u2212) + \u03b8(Q\u0303(n) (n\u03b8\u2212), \u2202 \u1ef80 (n)) + \u0393\u0304(H (n) )(t \u2212 n\u03b8),\n\nwhere for all t \u2265 0,\n\nH (n) (t) = Q\u0303(n) (n\u03b8\u2212) + R\u03b8(Q\u0303(n) (n\u03b8\u2212), \u2202 \u1ef80 (n)) + \u03b6\u0303(t + n\u03b8) \u2212 \u03b6\u0303(n\u03b8)\n+ R[\u1ef8 (t + n\u03b8) \u2212 \u1ef8 (n\u03b8)].\n\nUsing the Lipschitz property of \u0393\u0304, it follows that, for t \u2265 0,\n\n|\u1ef8(n+1) (t) \u2212 \u1ef8(n) (t)| \u2264 \u03b50 (1 + L| diag(x\u2217 )C \u2032 |)|Ry \u2217 |.\n\nThus, for t \u2265 0,\n\n|\u1ef8(p0 +1) (t) \u2212 \u1ef8(0) (t)| \u2264 (p0 + 1)\u03b50 (1 + L| diag(x\u2217 )C \u2032 |)|Ry \u2217 | \u2264\n\n\u03b5\n.\n2c1\n\nThe result follows on noting that \u1ef8(0) = \u1ef8 (1) and \u1ef8(p0 +1) = \u1ef8 . \u0003\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n27\n\n3.1. Proof of Theorem 3.5. Throughout this section we fix q \u2208 RI+ and\n\u03b5 \u2208 (0, \u221e). We begin with some preparatory results. Let \u00c30 be the class of all\nJ-dimensional adapted processes Y given on some filtered probability space\nsuch that U = KY is nondecreasing, U (0) \u2265 0 and q + RY (0) \u2265 0. Note that\n\u00c3(q) \u2282 \u00c30 . Also, a given \u1ef8 \u2208 \u00c30 is in \u00c3(q) if and only if (2.38) is satisfied.\nGiven an adapted process Y0 , on some system \u03a6, with sample paths in D J ,\nand satisfying q + RY (0) \u2265 0, we will denote the process Y , defined by\n(3.18)\n\nY = Y0 + \u0393\u0304(\u03b6\u0303 + RY0 ),\n\nas \u03a5(Y0 ). We claim that\n(3.19)\n\nif Y0 \u2208 \u00c30 , then \u1ef8 = \u03a5(\u1ef80 ) \u2208 \u00c3(q).\n\nIndeed, Q\u0303 = \u03b6\u0303 + R\u1ef8 = \u0393(\u03b6\u0303 + R\u1ef80 ) \u2265 0. Also, K \u1ef8 = K \u1ef80 + K \u0393\u0304(\u03b6\u0303 + R\u1ef80 ).\nI,\nSince Y0 \u2208 \u00c30 , KY0 is nondecreasing and KY0 (0) \u2265 0. Also, for x \u2208 D+\n\u2217\n\u2032\nK \u0393\u0304(x) = K diag(x )C \u0393\u0302(x), which is a nonnegative and nondecreasing function since \u0393\u0302(x) has these properties and the matrix\n\u0014\n\u0015\nBx\u2217n 0\nK diag(x\u2217 )C \u2032 =\nC\u2032\n0\n0\nhas nonnegative entries. Combining these observations, we see that the process \u1ef8 = \u03a5(\u1ef80 ) satisfies (2.38) and (2.39). The claim follows.\nNext, from the Lipschitz property of \u0393 it follows that there is a L\u0304 \u2208 (1, \u221e)\n(i)\nsuch that, if \u1ef80 \u2208 \u00c30 , i = 1, 2, then for all T > 0,\n(3.20)\n\n(1)\n\n(2)\n\n(1)\n\n|\u03a5(\u1ef80 ) \u2212 \u03a5(\u1ef80 )|\u221e,T \u2264 L\u0304|\u1ef80\n\n(2)\n\n\u2212 \u1ef80 |\u221e,T .\n\nIn what follows, we will denote \u03c3{X\u0303s : 0 \u2264 s \u2264 t} by FtX\u0303 .\nTheorem 3.9. Let Y \u2208 \u00c3(q) be a {FtX\u0303 }-adapted process with a.s. continuous paths. Suppose further that for some m > 0,\n(3.21)\n\nE[|Y |m\n\u221e,t ] < \u221e\n\nfor all t > 0.\n\nThen for any \u03b51 , T \u2208 (0, \u221e), there are \u03b7, M \u2208 (0, \u221e), p0 \u2208 N and a \u1ef8 (1) \u2208\n(1)\n(1)\n\u00c3(q) such that \u1ef8 (1) = \u03a5(\u1ef80 ) for some \u1ef80 \u2208 \u00c30 , that is, {FtX\u0303 }-adapted\nand satisfies (i) and (ii) of Theorem 3.5 with \u03b8 = T /p0 and\n(3.22)\n\n(1)\n\nE[|Y \u2212 \u1ef80 |m\n\u221e,T ] < \u03b51 .\n\nProof. The construction of \u1ef8 (1) proceeds by defining, successively, simpler approximations of Y , denoted as Y (1) , Y (2) , Y (3) , Y (4) . The process Y (1)\nis given in terms of a sequence {Yn } of J-dimensional processes, whereas\nthe processes Y (3) and Y (4) are given in terms of one parameter families\n\n\f28\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nof J-dimensional processes {Y (\u03b8, *), \u03b8 > 0}, {Y \u2217 (M, *), M > 0}, respectively.\nAll the processes Y (i) , i = 1, 2, 3, 4, and {Yn }, {Y (\u03b8, *)}, {Y \u2217 (M, *)} are only\nused in this proof and do not appear elsewhere\nin the paper.\nRt\nFix \u03b51 , T \u2208 (0, \u221e). Define Yn (t) = n (t\u22121/n)+ Y (s) ds, n \u2265 1, t \u2265 0. Note\nthat for all t, t\u2032 \u2208 [0, T ],\n|Yn (t) \u2212 Yn (t\u2032 )| \u2264 2n|t \u2212 t\u2032 ||Y |\u221e,T .\n\nHence, by (3.21), we have, for each n,\n\u0014\nYn (t) \u2212 Yn (t\u2032 )\n(3.23)\nE sup\nt \u2212 t\u2032\nt,t\u2032 \u2208[0,T ]\n\nm\u0015\n\n< \u221e.\n\nNote that for t \u2208 [1/n, T ], |Yn (t) \u2212 Y (t)| \u2264 \u031fYT (1/n) and for t \u2208 [0, 1/n],\n|Yn (t) \u2212 Y (t)| \u2264 2|Y |\u221e,1/n . Since Y is continuous and Y (0) = 0, \u031fYT (1/n) +\n2|Y |\u221e,1/n \u2192 0 a.s. Combining this with (3.21) and the estimate |Yn \u2212Y |\u221e,T \u2264\n.\n2|Y |\u221e,T , we now have that, for some n0 \u2208 N, Y (1) = Yn0 satisfies\nE|Y (1) \u2212 Y |m\n\u221e,T \u2264 \u03b5\u03030 ,\n\n(3.24)\nwith \u03b5\u03030 = \u03b51 /4. Also,\n\u0014\n(3.25)\nE sup\n\nt,t\u2032 \u2208[0,T ]\n\nY 1 (t) \u2212 Y 1 (t\u2032 )\nt \u2212 t\u2032\n\nm\u0015\n\n.\n= C1 < \u221e.\n\nNote that\nY (1) (0) = 0\n\n(3.26)\n\nand Y (1) \u2208 \u00c30 .\n\nGiven p0 \u2208 N and \u03b8 = T /p0 , define\n\u0012\u0016 \u0017 \u0013\nt\n1\n\u03b8 1[0,T ) (t) + Y 1 (T )1[T,\u221e) (t).\nY (\u03b8, t) = Y\n\u03b8\nFix p0 large enough so that \u03b8 < ( C\u03b5\u030301 )1/m and set Y (2) (t) = Y (\u03b8, t). Then,\nfrom (3.25) we have\n\n(3.27)\n\nE[|Y (2) (t) \u2212 Y (1) (t)|m\n\u221e,T ]\nn\nh\nsup\n=E\nmax\n\nn=0,...,p0 \u22121 t\u2208[n\u03b8,(n+1)\u03b8)\n\nm\n\n\u2264\u03b8 E\n\n\u0014\n\nsup\n\nn=0,1,...,p0 \u22121\n\n\u2264 \u03b8 m C1 < \u03b5\u03030 .\n\n|Y (1) (n\u03b8) \u2212 Y (1) (t)|m\n\n|Y (1) (t) \u2212 Y (1) (t\u2032 )|\nsup\n|t \u2212 t\u2032 |\nt,t\u2032 \u2208[n\u03b8,(n+1)\u03b8)\n\n\u001a\n\nFrom (3.26) we have\n(3.28)\n\noi\n\nY (2) (0) = 0\n\nand Y (2) \u2208 \u00c30 .\n\n\u001bm \u0015\n\n\f29\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nFor x \u2208 R, let \u2308x\u2309 denote the smallest integer upper bound for x. For x \u2208 RJ ,\n\nlet \u2308x\u2309 = (\u2308x1 \u2309, . . . , \u2308xJ \u2309)\u2032 . Fix \u03b7 \u2264\ndefine for t \u2265 0\nY (3) (t) =\n\n1/m\n\n\u03b5\u03030\n\u221a\np0 J\n\n\u230at\u2227T /\u03b8\u230b\n\nX\n\nn=0\n\nwhere for y\n\n\u2208 DJ ,\n\nand, with convention Y (2) (\u2212\u03b8) = 0,\n\n\u2308\u2202Y (2) (n)/\u03b7\u2309\u03b7,\n\n\u2202y(n) denotes y(n\u03b8) \u2212 y((n \u2212 1)\u03b8). Note that for t \u2264 T ,\nY\n\n(2)\n\n(t) =\n\n\u230at\u2227T /\u03b8\u230b\n\nX\n\n\u2202Y (2) (n).\n\nn=0\n\n\u221a\nObserving that for x \u2208 RJ , |x \u2212 \u230ax/\u03b7\u230b\u03b7| \u2264 \u03b7 J and recalling that T = p0 \u03b8,\nwe have that\n\u221a m\n(3.29)\nE[|Y (3) \u2212 Y (2) |m\n\u221e,T ] \u2264 (p0 \u03b7 J) \u2264 \u03b5\u03030 .\nNote that if y \u2208 RJ satisfies Ky \u2265 0, then yj \u2264 0 for all j = B + 1, . . . , J\nand, consequently, for such j, \u2308yj \u2309 \u2264 0. Combining this with the fact that A\nhas nonnegative entries, we see that K\u2308y\u2309 \u2265 0. From this observation, along\nwith (3.28), we have\nY (3) (0) = 0\n\n(3.30)\n\nand Y (3) \u2208 \u00c30 .\n\nThe process Y (3) constructed above is constant on [n\u03b8, (n + 1)\u03b8) and the\njumps \u2202Y (3) (n) take value in the lattice {k\u03b7 : k \u2208 Z}, for n = 0, . . . , p0 . Also,\nY (3) (t) = Y (3) (\u03b8p0 ) = Y (3) (T ) for t \u2265 T .\nFor fixed M \u2208 (0, \u221e), define\n\u2217\n\nY (M, t) =\n\n\u230at/\u03b8\u230b\n\nX\n\nn=0\n\n\u2202Y (3) (n)I{|\u2202Y (3) (n)|\u2264M } ,\n\nt \u2265 0.\n\nThen there exists C2 \u2208 (0, \u221e) such that, for all M > 0,\n(3.31)\n\n\u2217\n\nE[|Y (M, *) \u2212 Y\n\n(3) m\n|\u221e,T ] \u2264 C2\n\np0\nX\n\nE[|\u2202Y (3) (n)|m I(|\u2202Y (3) (n)|>M ) ].\n\nn=0\n\nAlso, for some C3 \u2208 (0, \u221e), we have from (3.21), (3.24), (3.27) and (3.29)\nthat, for n = 0, 1, . . . , p0 ,\nE[|\u2202Y (3) (n)|m ] \u2264 C3 (E[|Y |m\n\u221e,T ] + 1) < \u221e.\nFix M > 0 such that the right-hand side of (3.31) is bounded by \u03b5\u03030 . Setting\nY (4) = Y \u2217 (M, *), we now have that\n(3.32)\n\nE[|Y (4) \u2212 Y (3) |m\n\u221e,T ] \u2264 \u03b5\u03030 .\n\n\f30\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nAlso,\nY (4) (0) = 0\n\n(3.33)\n\nand Y (4) \u2208 \u00c30 .\n(1)\n\nCombining (3.21), (3.24), (3.27), (3.29) and (3.32), we now have that \u1ef80 =\nY (4) satisfies (3.22) as well as (i) and (ii) of Theorem 3.5. This completes\nthe proof. \u0003\nLemma 3.10. For each \u03b51 > 0 there exists an \u03b51 -optimal Y \u2208 \u00c3(q),\nwhich is {FtX\u0303 }-adapted, continuous a.s., and satisfies\n(3.34)\n\nlim sup e\u2212\u03b3T E|Y |m\n\u221e,T = 0\n\nfor every m > 0.\n\nT \u2192\u221e\n\nProof. Fix \u03b51 > 0 and let w = \u039bq. Applying Theorem 2.1(iv) of [2], we\nhave that J \u03030\u2217 (w) = inf J \u03030 (w, U ), where the infimum is taken over all {FtX\u0303 }adapted controls U . Hence, using (3.10), we conclude that there is an Ndimensional {FtX\u0303 }-adapted process U for which (3.7) holds and\nJ \u0303\u2217 (q) = J \u03030\u2217 (w) \u2265 J \u03030 (w, U ) \u2212 \u03b51 .\n\n(3.35)\n\nFrom Lemma 4.7 of [2] and following the construction of Proposition 3.3\nof [10] [cf. (12) and (14) of that paper], we can assume without loss of\ngenerality that U has continuous sample paths and for all m > 0,\nlim sup e\u2212\u03b3T E|GU |m\n\u221e,T = 0.\nT \u2192\u221e\n\nHence, using properties of the G matrix (see Assumption 2.6 ), we have that\n\n(3.36)\n\nlim sup e\u2212\u03b3T E|U (T )|m \u2264 c\u2212m lim sup e\u2212\u03b3T E|GU |m\n\u221e,T = 0\nT \u2192\u221e\n\nT \u2192\u221e\n\nfor all m > 0.\nWe will now use a construction given in the proof of Theorem 1 of [20]. This\nconstruction shows that there is a J \u00d7 N matrix F1 and a J \u00d7 I matrix F2\nsuch that letting\n(3.37)\n\nY (t) = F1 U (t) + F2 (q \u2217 (W\u0303 (t)) \u2212 q \u2212 X\u0303(t)),\n\nt > 0,\n\n \u0303 Y ). We refer the reader to equawe have that Y \u2208 \u00c3(q) and J \u03030 (w, U ) = J(q,\ntions (35) and (36) of [20] for definitions and constructions of these matrices.\nFrom (3.35) we now have that Y is an \u03b51 -optimal control, has continuous\nsample paths a.s. and is {FtX }-adapted. Finally from (3.37), we have that\nfor some C2 \u2208 (0, \u221e),\nm\nm\n\u2217\nm\nE|Y |m\n\u221e,T \u2264 C2 (1 + E|U (T )| + T + E|q (W\u0303 )|\u221e,T ).\n\n\f31\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nBy combining (3.3) and (3.7), the fourth term on the right-hand side can\nbe bounded above by C3 (1 + T m + E|U (T )|m ) for some C3 > 0. The result\nthen follows on using (3.36). \u0003\nThe following construction will be used in the proof of Theorem 3.5.\n \u0303 Y ) < \u221e and (3.34) holds.\nLemma 3.11. Fix Y \u2208 \u00c3(q) such that J(q,\nT\nT\nFor T > 0, let \u1ef80 (t) = Y (t \u2227 T ), t > 0, and Y = \u03a5(\u1ef80T ). Then given \u03b51 > 0,\n \u0303 Y T )| < \u03b51 and (3.34) holds\nthere exists T \u2208 (0, \u221e) such that |J \u0303(q, Y ) \u2212 J(q,\nwith Y replaced by Y T .\nProof. Since J \u0303(q, Y ) < \u221e, we have that\nZ \u221e\nZ\n.\n\u2212\u03b3t\ne\u2212\u03b3t p * dU (t) \u2192 0\nL(T, Y ) = E\ne h * Q\u0303(t) dt + E\n(T,\u221e)\n\nT\n\nas T \u2192 \u221e,\n\nwhere Q\u0303 is the state process corresponding to Y and U = KY . Choose T1\nlarge enough so that\n(3.38)\n\nL(T, Y ) < \u03b51 /2\n\nfor T \u2265 T1 .\n\nUsing the Lipschitz property of \u0393 and \u0393\u0302 (see Assumption 2.15), we can find\nC1 \u2208 (0, \u221e) such that, for all T > 0,\n|Q\u0303T (t) \u2212 Q\u0303(T )| + |Y T (t) \u2212 Y (T )|\n\n(3.39)\n\n\u2264 C1 sup |\u03b6\u0303(s) \u2212 \u03b6\u0303(T )|,\nT \u2264s\u2264t\n\nt \u2265 T,\n\nwhere Q\u0303T is the state process corresponding to Y T . Thus, for some C2 \u2208\n(0, \u221e),\nZ \u221e\nZ \u221e\nh\u0304\ne\u2212\u03b3t h * Q\u0303T (t) dt \u2264 e\u2212\u03b3T E|Q\u0303(T )| + C2\nE\ne\u2212\u03b3t (1 + t) dt,\n\u03b3\nT\nT\n\nwhere h\u0304 = maxi\u2208I hi . Using (3.34), we can now choose T2 large enough so\nthat\nZ \u221e\n(3.40)\ne\u2212\u03b3t h * Q\u0303T (t) dt \u2264 \u03b51 /4\nfor all T > T2 .\nE\nT\n\nNext, letting\n\nUT\n\n= KY T , we have from (3.39) that for some C3 \u2208 (0, \u221e),\n\nE|U T (t) \u2212 U (T )| \u2264 C3 (1 + (t \u2212 T ))\n\nfor 0 < T < t.\n\nIntegration by parts now yields that for some T3 > 0,\nZ\n(3.41)\ne\u2212\u03b3T p * dU T (t) \u2264 \u03b51 /4\nfor T \u2265 T3 .\nE\n(T,\u221e)\n\nCombining the estimates in (3.38), (3.40) and (3.41), we now have that for\nall T \u2265 max{T1 , T2 , T3 },\n \u0303 Y ) \u2212 J(q,\n \u0303 Y T )| \u2264 L(T, Y ) + L(T, Y T ) \u2264 \u03b51 /2 + \u03b51 /4 + \u03b51 /4 = \u03b51 .\n|J(q,\n\n\f32\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nFinally, the fact that (3.34) holds with Y replaced by Y T is an immediate\nconsequence of (3.39). \u0003\nWe can now complete the proof of Theorem 3.5.\nProof of Theorem 3.5. Using Lemma 3.10, one can find Y \u2208 A(q)\nwhich is \u03b5/5-optimal, has continuous paths a.s., is {FtX\u0303 }-adapted and satisfies (3.34). Using Lemma 3.11, we can find T \u2208 (0, \u221e) such that Y T =\n\u03a5(Y (* \u2227 T )) is 2\u03b5/5-optimal and (3.34) holds with Y replaced by Y T . We\nwill apply Theorem 3.9, with m = 1, Y replaced with Y T , \u03b51 replaced\nby \u03b5/(5c1 L\u0304) and denote the corresponding processes obtained from The(1)\norem 3.9, once again by \u1ef8 (1) and \u1ef80 . In particular, \u1ef8 (1) \u2208 A(q) is such\n(1)\nthat \u1ef8 (1) = \u03a5(\u1ef80 ), where \u1ef80 is {FtX\u0303 }-adapted, satisfies (i) and (ii) of Theorem 3.5, for some \u03b7, M, \u03b8 \u2208 (0, \u221e) and p0 \u2208 N, and (3.22) holds with m = 1,\nY replaced by Y T and where L\u0304 is as in (3.20). Then\n\u03b5\n(1)\nE[|Y T \u2212 \u1ef8 (1) |\u221e,T ] \u2264 L\u0304E[|Y T \u2212 \u1ef80 |\u221e,T ] \u2264\n.\n5c1\nThus, from Lemma 3.7, \u1ef8 (1) is 3\u03b5/5-optimal.\n(1)\nProcesses \u1ef80 , \u1ef8 (1) as in the statement of Theorem 3.5 will be constructed\n(1)\nby modifying the processes \u1ef80 , \u1ef8 (1) above (but denoted once more by the\n(\u03ba)\nsame symbols), by constructing successive approximations (Y0 , Y (\u03ba) ) and\n(\u03b3)\n(\u1ef80 , Y (\u03b3) ). These approximations are only used in the current proof and\ndo not appear elsewhere in the paper.\n(n)\nConsider \u03ba > 0 such that \u03b8/\u03ba \u2208 N. Let G\u03ba = \u03c3{X \u03ba (n)}, G (n) = \u03c3{X\u0303(s) :\n(n)\n(1)\ns \u2264 n\u03b8}, G = \u03c3{X\u0303(s) : s \u2265 0}, n \u2208 N, \u03ba > 0. Since G\u03ba \u2191 G (n) as \u03ba \u2193 0 and \u1ef80\n\u03b7\nis {FtX\u0303 }-adapted, we have for each fixed \u03c2 \u2208 SM\n,\n(1)\n\n(3.42)\n\n(1)\n\nP[\u2202 \u1ef80 (n) = \u03c2|G\u03ba(n) ] \u2192 P[\u2202 \u1ef80 (n) = \u03c2|G (n) ]\n= 1{\u2202 \u1ef8 (1) (n)=\u03c2}\n0\n\na.e., as \u03ba \u2193 0.\n\n\u03b7\nNote that for fixed \u03c2 \u2208 SM\nand n = 1, . . . , p0 ,\n(1)\n\nP[\u2202 \u1ef80 (n) = \u03c2|G\u03ba(n) ] = p\u03ban,\u03c2 (X \u03ba (n)),\nfor some measurable map p\u03ban,\u03c2 : Rnj0I \u2192 [0, 1] satisfying\nX\n(3.43)\np\u03ban,\u03c2 (x) = 1.\nfor all x \u2208 RInj0\n\u03b7\n\u03c2\u2208SM\n\nUsing Lemma A.1 in the Appendix, we can construct, by suitably aug(\u03ba)\nmenting the filtered probability space, an adapted process Y0 such that\n\n\f33\n\nCONVERGENCE OF VALUE FUNCTIONS\n\n. (\u03ba)\n(\u03ba)\n(\u03ba)\n(\u03ba)\n(\u03ba)\n(\u03ba)\nY0 (0) = 0, Y0 (t) = Y0 (\u230at/\u03b8\u230b\u03b8) and \u2202Y0 (n) = Y0 (n\u03b8) \u2212 Y0 ((n \u2212\n1)\u03b8) satisfies\n(\u03ba)\n\nP[\u2202Y0 (n) = \u03c2|G \u2228 Y0n\u22121 ] = p\u03ban,\u03c2 (X \u03ba (n)),\n\n(3.44)\n\n(\u03ba)\n\n\u03b7\n1 \u2264 n \u2264 p0 , \u03c2 \u2208 SM\n,\n\nwhere Y0n = \u03c3{\u2202Y0 (j), j \u2264 n}. Note that if f is a real bounded continuous\nmap on C I , then by successive conditioning and (3.42), as \u03ba \u2193 0,\n!\n!\np0\np0\nY\nY\n\u03ba\n\u03ba\npn,\u03c2n (X (n))\n1{\u2202Y (\u03ba) (n)=\u03c2 } = E f (X\u0303)\nE f (X\u0303)\nn\n\n0\n\nn=1\n\n\u2192 E f (X\u0303)\nfor any fixed (\u03c21 , . . . , \u03c2p0 ). Thus, in particular,\n(\u03ba)\n\n(1)\n\n(X\u0303, Y0 ) \u21d2 (X\u0303, \u1ef80 )\n\n(3.45)\n\n(\u03ba)\n\nn=1\np0\nY\n\nn=1\n\n1{\u2202 \u1ef8 (1) (n)=\u03c2\n0\n\nn}\n\n!\n\n,\n\nas \u03ba \u2193 0.\n\nDefine Y (\u03ba) = \u03a5(Y0 ). Then Y (\u03ba) \u2208 A(q) and satisfies (i) and (ii) of Theorem 3.5. Also, (3.45) along with the Lipschitz property of the Skorohod\n \u0303 \u1ef8 (1) ) as \u03ba \u2193 0. Fix \u03ba sufficiently small so\nmap yields that J \u0303(q, Y (\u03ba) ) \u2192 J(q,\n(\u03ba)\nthat Y (\u03ba) is 4\u03b5/5-optimal and, suppressing \u03ba, denote Y0 by \u1ef80 , Y (\u03ba) by \u1ef8\nand the generating kernels by pn,\u03c2 .\nFinally, in order to ensure property (iii) in the theorem, we mollify the\nkernels pn,\u03c2 as follows. For \u03b3 > 0, and n = 1, . . . , p0 , define\nZ\nnj0\nY\n.\n\u03b7\n\u03b3\n(3.46) p\u0302n,\u03c2 (x) =\nx \u2208 Rnj0 I , \u03c2 \u2208 SM\n,\npn,\u03c2 (x + z) (\u03c6\u03b3 (zj ) dzj ),\nRnj0 I\n\nj=1\n\n\u03c6\u03b3\n\nwhere\nis the density function of an I-dimensional Normal random variable\nwith mean 0 and variance \u03b3I. Note that the map x 7\u2192 p\u0302\u03b3n,\u03c2 (x) is continuous\nfor every \u03b3, n, \u03c2 and (3.43) is satisfied with p\u03ban,\u03c2 (x) replaced with p\u0302\u03b3n,\u03c2 (x).\nFrom continuity of the maps p\u0302\u03b3n,\u03c2 , we can find measurable maps (suppressing\n\u03b7\ndependence on \u03b3 in notation) \u031fn : RInj0 \u00d7 [0, 1] \u2192 SM\n, n = 1, . . . , p0 , such\nInj\nthat \u031fn (*, t) is continuous, at every x \u2208 R 0 , for a.e. t in [0, 1], and if U is\na Uniform random variable on [0, 1], then\nP(\u031fn (x, U ) = \u03c2) = p\u0302\u03b3n,\u03c2 (x),\n\n\u03b7\nx \u2208 RInj0 , \u03c2 \u2208 SM\n.\n\nLet {\u0168n } be an i.i.d sequence of Uniform random variables, that is, inde(\u03b3)\n(\u03b3)\npendent of X\u0303. Now construct \u1ef80 such that \u1ef80 (0) = 0,\n(\u03b3)\n\n(\u03b3)\n\n(\u03b3)\n\n\u2202 \u1ef80 (n) = \u1ef80 (n\u03b8) \u2212 \u1ef80 ((n \u2212 1)\u03b8) = \u031fn (X \u03ba (n), \u0168n ),\n(\u03b3)\n\n(\u03b3)\n\nn\u22651\n\nand \u1ef80 (t) = \u1ef80 (\u03b8\u230at/\u03b8\u230b). Note that\n(\u03b3)\n\nP[\u2202 \u1ef80 (n) = \u03c2|G \u2228 Y0n\u22121 ] = p\u0302\u03b3n,\u03c2 (X \u03ba (n)),\n\n\u03b7\nn \u2265 1, \u03c2 \u2208 SM\n.\n\n\f34\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nSince p\u0302\u03b3n,\u03c2 \u2192 pn,\u03c2 pointwise, as \u03b3 \u2192 0, we have for every real bounded map f\non C I ,\n!\n!\np0\np0\nY\nY\np\u0302\u03b3n,\u03c2n (X \u03ba (n))\n1{\u2202 \u1ef8 (\u03b3) (n)=\u03c2 } = E f (X\u0303)\nE f (X\u0303)\nn=1\n\n0\n\nn\n\nn=1\n\n\u2192 E f (X\u0303)\n= E f (X\u0303)\n\np0\nY\n\nn=1\np0\nY\n\nn=1\n(\u03b3)\n\n!\n\npn,\u03c2n (X \u03ba (n))\n\n!\n\n1{\u2202 \u1ef80 (n)=\u03c2n } ,\n\nfor every (\u03c21 , . . . , \u03c2p0 ), as \u03b3 \u2192 0. Thus, (X\u0303, \u1ef80 ) \u21d2 (X\u0303, \u1ef80 ), as \u03b3 \u2192 0. Let\n(\u03b3)\n\u1ef8 (\u03b3) = \u03a5(\u1ef80 ). Then \u1ef8 (\u03b3) \u2208 A(q) and the above weak convergence and, once\n \u0303 \u1ef8 (\u03b3) ) \u2192\nmore, the Lipschitz property of the Skorohod map yields that J(q,\n \u0303\nJ(q, \u1ef8 ) as \u03b3 \u2193 0. Recall that \u1ef8 is 4\u03b5/5-optimal. We now choose \u03b3 sufficiently\n.\n(\u03b3)\n(1) .\nsmall so that \u1ef8 (\u03b3) is \u03b5-optimal. By construction, \u1ef80 = \u1ef80 and \u1ef8 (1) = \u1ef8 (\u03b3)\nsatisfy all the properties stated in the theorem. \u0003\n4. Asymptotically near-optimal controls for SPN. The goal of this section is to prove Theorem 2.17. Fix q \u2208 RI+ and \u03b5 \u2208 (0, 1). Let \u1ef8 \u2208 \u00c3(q) be\nthe \u03b5-optimal control introduced above Theorem 3.8. Fix q r \u2208 NI0 , r > 0\nsuch that q\u0302 r \u2192 q, as r \u2192 \u221e. Section 4.1 below gives the construction of\nthe sequence of policies {T r }, T r \u2208 Ar (q r ), such that J r (q r , T r ) \u2192 J \u0303(q, \u1ef8 ),\nyielding the proof of Theorem 2.17. The latter convergence of costs is proved\nin Section 4.2. The main ingredient in this proof is Theorem 4.5 whose proof\nis given in Section 4.3. For the rest of this section \u1ef8 as in Theorem 3.8 and\nparameters T, \u03b7, M, p0 , j0 , \u03b8, \u03ba that specify \u1ef8 shall be fixed. In addition, let\n\u03c1 \u2208 (0, \u221e) and r0 \u2265 1 be such that\ni\nh\n\u2217\n(4.1)\n> M (\u03b8lip + 1) and r0 \u03b8 > \u03c1,\n\u03c1 min\nx\nj\n\u2217\nxj 6=0\n\nwhere \u03b8lip is as in (3.13).\n\n4.1. Construction of the policy sequence. We will only specify a T r \u2208Ar (q r )\nfor r \u2265 r0 and so henceforth, without loss of generality, we assume r > r0 .\nIn this section, since r \u2265 r0 will be fixed, the superscript r will frequently\nbe suppressed from the notation. The following additional notation will be\nused. For n \u2264 p0 , define a(n) = nr 2 \u03b8, b(n) = nr 2 \u03b8 + r\u03c1, I(n) = [a(n), a(n +\n1)), I1 (n) = [a(n), b(n)) and I2 (n) = [b(n), a(n+1)), where we set a(p0 +1) =\n\u221e. Note I(n) = I1 (n) \u222a I2 (n), n \u2264 p0 .\nRecall m introduced in Assumption 2.13. Fix k \u2208 (0, 1) such that\n.\n(4.2)\nk(1 + m) \u2212 2 = \u03c5 > 1.\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n35\n\nFig. 4. The figure above shows the behavior, under T (1) , of the kth server, that is, responsible for the nk activities (j1 , . . . , jnk ). The actual policy T is given as a certain\nmodification of T (1) that ensures feasibility and a strict positivity property.\n\nFix d1 \u2208 (0, \u221e) such that d1 > supr {\u03b2jr : j \u2208 J} + 1. Define\n(4.3)\n\n.\n\u2206r = r k ,\n\n.\n\u0398r (s) = d1 r k 1(\u222an\u2264p0 I2 (n)) (s)\n\nAlso define pr : [0, \u221e) \u2192 [0, \u221e) as\n\uf8f1\n\uf8f2 s,\n\u0017\n\u0016\n.\npr (s) =\ns \u2212 b(n)\n\uf8f3 b(n) +\n\u2206,\n\u2206\n\nfor s \u2265 0.\n\nif s \u2208 I1 (n), n = 0, 1, . . . , p0 ,\nif s \u2208 I2 (n), n = 0, 1, . . . , p0 .\n\nThus, if s \u2208 I2 (n) for some n, \u0398r (s) = d1 \u2206r and pr (s) equals the left end\npoint of the \u2206-subinterval in which s falls. Otherwise, if s \u2208 I1 (n) for some n,\n\u0398r (s) = 0 and pr (s) = s.\nRecall the probability space (\u03a9, F, P) introduced in Section 2 which supports all the random variables and stochastic processes introduced therein.\nLet {Ui : i \u2208 N} be a sequence of Uniform random variables on [0, 1] on this\nprobability space (constructed by augmenting the space if needed), independent of the \u03c3-field F\u0304 r defined in (2.27). This sequence will be used in the\nconstruction of the control policy.\nThe policy T \u2261 T r is constructed recursively over time intervals I(n), n =\n0, . . . , p0 , as follows. We will describe the effect of the policy on the kth\nserver (for each k \u2208 K) at every time-instant s \u2265 0 (see Figure 4). Recall\nthe set J(k) introduced in Section 2. Let for k \u2208 K, j1 < j2 < * * * < jnk be\nthe ordered elements of J(k) (the set of activities that the kth server can\nperform).\n\n\f36\n\nA. BUDHIRAJA AND A. P. GHOSH\n2\n\nStep 1 : [T r (s) when s \u2208 I(0)]. Let m0 = \u230a r\u2206\u03b8 \u230b. Recall that \u1ef8 (0) = \u03b50 y \u2217 .\nWrite \u03bd 0 = \u03b50 y \u2217 . For k \u2208 K, let\n\u0012\n\u0013\n\u03bdj0l\n.\n1,k,0 .\n\u2217\n\u2207l\n= xj l \u2212\n(4.4)\n\u2206, \u2207l2,k,0 = x\u2217jl \u2206, l = 1, . . . , nk .\n\u03c1\n\nNote that, by our choice of \u03c1, \u2207l1,k,0 > 0 if jl is a basic activity. Also, if jl is\nnonbasic, then x\u2217jl = 0, but since K\u03bd 0 \u2265 0, we have \u03bdj0l \u2264 0. Thus, \u2207l1,k,0 \u2265\n0 for every l, k. Also, from Assumption 2.3 [see (2.14)] and recalling that\nA\u03bd 0 \u2265 0, we get\n\u0012\n\u0013\nnk\nnk\nX\nX\n1\n1,k,0\n0\n\u2207l\n= \u2206 1 \u2212 (A\u03bd )k \u2264 \u2206,\n(4.5)\n\u2207l2,k,0 = \u2206, k \u2208 K.\n\u03c1\nl=1\n\nl=1\n\n.\nr . r 2 \u03b8\u2212r\u03c1\nLet m1 \u2261 mr1 = \u230a r\u03c1\n\u2206 \u230b and m2 \u2261 m2 = \u230a \u2206 \u230b. Define, for each k \u2208 K, and\nl = 1, . . . , nk ,\n\uf8f1\nm\n1 \u22121\n\uf8f4\nX\n\uf8f4\n\uf8f4\n\uf8f4\n1E m,0 (s),\ns \u2208 I1 (0),\n\uf8f4\n\uf8f2\nl\n(1)\nm=0\n\u1e6ajl (s) = m \u22121\n2\n\uf8f4\nX\n\uf8f4\n\uf8f4\n\uf8f4\n1\u00ca m,0 (s),\ns \u2208 I2 (0),\n\uf8f4\n\uf8f3\nl\nm=0\n\nwhere for m\u0303i = 0, . . . , mi \u2212 1, i = 1, 2, l = 1, . . . , nk ,\n!\n\"\nl\nl\u22121\nX\nX\n1,k,0\n1,k,0\nm\u03031 ,0\n(4.6)\n,\n\u2207i\n\u2207i , a(0) + m\u03031 \u2206 +\nEl\n= a(0) + m\u03031 \u2206 +\n\n(4.7)\n\n\"\n\n\u00calm\u03032 ,0 = b(0) + m\u03032 \u2206 +\n\u2207i,k,0\n0\n\ni=0\n\ni=0\n\nl\u22121\nX\n\nl\nX\n\ni=0\n\n\u2207i2,k,0 , b(0) + m\u03032 \u2206 +\n\ni=0\n\n\u2207i2,k,0\n\n!\n\nand, by convention,\n= 0, i = 1, 2. Since {J(k), k \u2208 K} gives a partition\nof J, the above defines the J-dimensional process \u1e6a (1) (s) for s \u2208 [a(0), a(1)).\nWe set\nZ s\n\u1e6a (1) (s) ds,\ns \u2208 I(0).\nT (1) (s) =\n0\n\nNext, define V(s) \u2261 V r (s) = (Qr (s), X r (s), T r (s)) for s \u2208 I(0) by the system\nof equations below:\nQr (s) = q r + X r (s) + r(\u03b81r s \u2212 (C \u2212 P \u2032 ) diag(r 2 \u03b82r )T\u0304 r (s/r 2 ))\n(4.8)\n\n+ R(x\u2217 s \u2212 T r (s)),\n\nX r (s) = r X\u0302 r (s/r 2 ),\nX\u0302 r defined by (2.23),\nZ\nr,(1)\n1{Qr\u03c3 (j) (u\u2212)>0} 1{Qr\u03c3 (j) (pr (u))>\u0398r (u)} dTj (u),\nTjr (s) =\n[0,s]\n\n1\n\n1\n\nj \u2208 J,\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n37\n\nwhere (as introduced in Section 2), \u03c31 (j) denotes the index of the buffer that\nthe jth activity is associated with. The above construction can be interpreted\nas follows. The policy T \"attempts\" to implement T (1) over I1 (0), unless\nthe corresponding buffer is empty (in which case, it idles). Over I2 (0), T has\na similar behavior, but, in addition, it idles when the corresponding buffer\ndoes not have at least \u0398-many jobs at the beginning of the \u2206-subintervals.\nProcessing jobs only when there is a \"safety-stock\" at each buffer at the\nbeginning of the \u2206-subinterval ensures that with \"high probability\" either\nall the activities associated with the given buffer receive nominal time effort\nover the interval or all of them receive zero effort. This, in particular, makes\nsure that the idling processes associated with the policy are consistent with\nthe reflection terms for the Skorohod map associated with the constraint matrix D [see (4.21)]. This property will be exploited in the weak convergence\narguments of Section 4.2 [see, e.g., arguments below (4.45)].\nLet I r be defined by (2.9). By construction, T r (0) = 0, I r (0) = 0 and\n(4.9)\n\nQr (s) \u2265 0,\n\nT r (s) \u2265 0,\n\nI r (s) \u2265 0\n\nfor s \u2208 I(0) and T r , I r are nondecreasing on I(0).\n\nThis completes the construction of the policy and the associated processes\non I(0).\nStep 2 : [T r (s) for s \u2208 I(n), for 1 \u2264 n \u2264 p0 ]. Suppose now that the process V(s) has been defined for s \u2208 [a(0), a(n)), where 1 \u2264 n \u2264 p0 . We now describe the construction over the interval I(n) = [a(n), a(n+1)). Let \u03c7\u0302\u03ba,r (n) =\n{X\u0302 r (l\u03ba) : l = 1, . . . , nj0 }. Recall that nj0 \u03ba = n\u03b8. Define\n.\n.\n(4.10)\n\u03bd\u0304 r,n = \u031fn (\u03c7\u0302\u03ba,r (n), Un ),\n\u03bd r,n = \u03b8\u03b50 (Q\u0302r (n\u03b8\u2212), \u03bd\u0304 r,n),\n\nwhere, as in Section 2, Q\u0302r (t) = Qr (r 2 t)/r, t \u2265 0. We will suppress \u03ba and r\nfrom the notation and write (\u03c7\u0302\u03ba,r (n), \u03bd\u0304 r,n , \u03bd r,n ) \u2261 (\u03c7\u0302(n), \u03bd\u0304 n , \u03bd n ). For each\nk \u2208 K, define\n\u0012\n\u0013\n\u03bdjnl\n.\n1,k,n .\n\u2217\n\u2207l\n= xj l \u2212\n(4.11)\n\u2206, \u22072,k,n\n= x\u2217jl \u2206, l = 1, . . . , nk .\nl\n\u03c1\n\nAs before, (4.5) (with \u2207j,k,0\nreplaced by \u2207j,k,n\n) is satisfied. Define, for each\nl\nl\nk \u2208 K, and l = 1, . . . , nk ,\n\uf8f1\nm\n1 \u22121\n\uf8f4\nX\n\uf8f4\n\uf8f4\n\uf8f4\n1E m,n (s),\ns \u2208 I1 (n),\n\uf8f4\nl\n\uf8f2\nm=0\n1\n(4.12)\n\u1e6ajl (s) = m \u22121\n2\n\uf8f4\n\uf8f4\n\uf8f4 X 1 m,n (s),\n\uf8f4\ns \u2208 I2 (n),\n\uf8f4\n\u00cal\n\uf8f3\nm=0\n\nwhere for m\u0303i = 0, . . . , mi \u2212 1, i = 1, 2, l = 1, . . . , nk , Elm\u03031 ,n , \u00calm\u03032 ,n are defined\nby the right-hand side of (4.6) and (4.7) respectively, with (a(0), b(0), \u2207i1,k,0 ,\n\n\f38\n\nA. BUDHIRAJA AND A. P. GHOSH\n\n\u2207i2,k,0 ) replaced by (a(n), b(n), \u22071,k,n\n, \u22072,k,n\n) We set\ni\ni\nZ s\n\u1e6a 1 (s) ds,\ns \u2208 I(n)\nT 1 (s) = T (a(n)) +\na(n)\n\nand define V(s) for s \u2208 I(n) by the system of equations in (4.8).\nThe above recursive procedure gives a construction for the process V r (s) =\n(Qr (s), X r (s), T r (s)) for all s \u2208 [0, \u221e).\nThe policy constructed above clearly satisfies parts (i), (ii) and (iii) of\nDefinition 2.7. In fact, it also satisfies part (iv) of the definition and, consequently, we have the following result. The proof is given in the Appendix.\nProposition 4.1.\n\nFor all r \u2265 r0 , T r \u2208 Ar (q r ).\n\n4.2. Convergence of costs. In this section we prove Theorem 2.17 by\nshowing that, with {T r } as in Section 4.1 and \u1ef8 as introduced above The \u0303 \u1ef8 ), as r \u2192 \u221e. We begin with an elementary\norem 3.8, J r (q r , T r ) \u2192 J(q,\nlemma.\nLemma 4.2. Let g n \u2208 D I , n \u2265 1, g \u2208 C I be such that g n \u2192 g u.o.c. as\nn \u2192 \u221e. Let \u03b5n \u2208 (0, \u221e), n \u2265 1, be such that \u03b5n \u2192 0 as n \u2192 \u221e. Let B n , B\nbe I \u00d7 I matrices such that B n \u2192 B as n \u2192 \u221e. Suppose f n , hn \u2208 D I and\n\u03b3 n \u2208 D 1 satisfy for all t \u2265 0:\n(i) f n (t) \u2265 0, \u03b3 n (t) \u2265 0,\nR\n(ii) f n (t) = gn (t) + B n hn (t), hni (t) = [0,t] 1{fin (\u03b3 n (s))\u2264\u03b5n } ds, i \u2208 I and\n(iii) |\u03b3 n (t) \u2212 t| \u2264 \u03b5n .\n\nThen (f n , gn , hn , \u03b3 n ) is precompact in D 3I+1 and any limit\nR point (f, g, h, \u03b3)\nsatisfies for all t \u2265 0, \u03b3(t) = t; f (t) = g(t) + Bh(t); h(t) = [0,t] h\u0303(s) ds, where\nh\u0303 : [0, \u221e) \u2192 [0, 1]I is a measurable map such that\nZ\n1{fi (s)>0} h\u0303i (s) ds = 0\nfor all t \u2265 0.\n[0,t]\n\nThe proof of the above lemma is given in the Appendix. Recall the definitions of various scaled processes given in (2.21)\u2013(2.25), and that i (t) = t\nfor t \u2265 0. In addition, we define T\u0304 r,1 (t) = T r,1 (r 2 t)/r 2 , T\u0302 r,1 (t) = T r,1 (r 2 t)/r,\nr > 0, t \u2265 0.\nProposition 4.3.\n\nAs r \u2192 \u221e, T\u0304 r \u2192 x\u2217 i , u.o.c. in probability.\n\nProof. From the definition of T r,1 [see (4.12)], and the observation that\nthe interval E\u02c6lm,n has length x\u2217jl \u2206 [see (4.5) and (4.7)], we have that over\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n39\n\neach interval [m\u2206, (m + 1)\u2206), that is, contained in I2 (n) for some n \u2264 p0 ,\nthe Lebesgue measure of the time instants s such that \u1e6aj1 (s) = 1 equals x\u2217j \u2206,\nj \u2208 J. This is equivalent to the statement that\nZ\nd(Tjr,1 (s) \u2212 x\u2217j s) = 0\n[m\u2206,(m+1)\u2206)\n\n(4.13)\n\nwhenever [m\u2206, (m + 1)\u2206) \u2282 I2 (n) for some n \u2264 p0 .\n\nAlso, noting that for j \u2208 J, 0 \u2264 \u1e6ajr,1 \u2264 1 and x\u2217j \u2264 1, we have that for some\nC1 > 0,\nZ\nr,(1)\nd(Tj (s) + x\u2217j s) < C1 \u2206\n[m\u2206,(m+1)\u2206)\n\n(4.14)\n\nwhenever [m\u2206, (m + 1)\u2206) \u2282 I(n) for some n \u2264 p0 .\n\nFix j \u2208 J and t > 0 such that r 2 t \u2208 [a(n), a(n + 1)) for some n \u2264 p0 . Then\nr 2 |x\u2217j t \u2212 T\u0304jr,1 (t)|\n\u2264\n\nn X\n2\nX\n\nX\n\nZ\n\nn0 =0 l=1 m : [m\u2206,(m+1)\u2206)\u2208Il (n0 ) [m\u2206\u2227t,(m+1)\u2206\u2227t)\n\nd(Tjr,1 (s) \u2212 x\u2217j s).\n\nUsing (4.13) and (4.14) and the fact that the number of \u2206 intervals in I1 (n)\nis bounded by r\u03c1/\u2206, we see that\n\u0012\n\u0013\nr\u03c1(p0 + 1)\nr,1\n2 \u2217\nr |xj t \u2212 T\u0304j (t)| \u2264\n+ 1 C1 \u2206.\n\u2206\nThus, for some \u033a \u2208 (0, \u221e),\n\n(4.15)\n\nsup |x\u2217 t \u2212 T\u0304 r,1 (t)| \u2264 \u033a/r\n\n0\u2264t<\u221e\n\nfor all r \u2265 r0 .\n\nAlso, since T\u0304jr (t) \u2264 t for all t \u2265 0, j \u2208 J, we get from (2.23) and standard\n.\nestimates for renewal processes (see, e.g., Lemma 3.5 of [9]) that X\u0304 r = X\u0302 r /r\nconverges to 0 u.o.c. in probability, as r \u2192 \u221e. Combining this with Assumption 2.3 and (4.15), we have\n.\n(4.16)\n\u03b6\u0304 r = \u03b6\u0302 r /r converges to 0, u.o.c., in probability.\nNext, define p\u0304r (s) = pr (r 2 s)/r 2 , \u0398\u0304r (s) = \u0398r (r 2 s)/r 2 and\nSjr (t) = {s \u2208 [0, t] : Q\u0304r\u03c31 (j) (s) = 0, or Q\u0304r\u03c31 (j) (p\u0304r (s)) \u2264 \u0398\u0304r (s)}\n(4.17)\n\n= {s \u2208 [0, t] : Q\u0304r\u03c31 (j) (p\u0304r (s)) \u2264 \u0398\u0304r (s)}\n\u222a {s \u2208 [0, t] : Q\u0304r\u03c31 (j) (s) = 0, Q\u0304r\u03c31 (j) (p\u0304r (s)) > \u0398\u0304r (s)}\n\n.\n= Sjr,1 (t) \u222a Sjr,2 (t).\n\n\f40\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nWe will fix t > 0 for the rest of the proof and suppress t from the notation\nwhen writing Sjr,i (t), unless there is scope for confusion. Using the above\ndisplay and (4.8), we have that\nZ\nr,1\nr\n(4.18)\n1Sjr (s) dT\u0304jr,1 (s),\nj \u2208 J, t \u2265 0.\nT\u0304j (t) \u2212 T\u0304j (t) =\n[0,t]\n\nUsing the fact that Q\u0304r (*) =\n\nQ\u0302r (*)\nr\n\nalong with (2.24) and (4.18), we can write\n\nQ\u0304r (t) = \u03b6\u0304 r (t) + RH\u0304 r (t) + RL\u0304r (t),\n\n(4.19)\nwhere for j \u2208 J,\n\nH\u0304jr (t)\n\n= (x\u2217j t \u2212 T\u0304 r,1 (t)) +\n\n(4.20)\n+\n\nZ\n\n[0,t]\n\nZ\n\n[0,t]\n\nt \u2265 0,\n\n1S r,1 (s) d(T\u0304jr,1 (s) \u2212 x\u2217j s)\nj\n\n1S r,2 (s) dT\u0304jr,1 (s)\nj\n\nand\nL\u0304rj (t) = x\u2217j\n\nZ\n\n[0,t]\n\n1S r,1 (s) ds = (diag(x\u2217 )C \u2032 l\u0304r )j (t),\nj\n\nwhere\nl\u0304ri (t) =\n\nZ\n\n[0,t]\n\n1{Q\u0304r (p\u0304r (s))\u2264\u0398\u0304r (s)} ds,\ni\n\nt \u2265 0, i \u2208 I.\n\nSince D = R diag(x\u2217 )C \u2032 [see (2.11) and (2.43)], we have\nQ\u0304r = \u03b6\u0304 r + RH\u0304 r + D l\u0304r .\n\n(4.21)\n2\n\nr t\nNext, letting m\u0302r0 = \u230a \u2206\nr \u230b + 1, we have from the choice of d1 [see above (4.3)]\nthat\n\nP(Sjr,2 6= \u2205)\nr\n\n\u2264\n\nm\u03020\nX\nX\n\nP(Qri (k\u2206r ) > d1 r k , Qri (u) = 0\n\nk=0 i\u2208I\n\nfor some u \u2208 [k\u2206r , (k + 1)\u2206r ))\nr\n\n(4.22)\n\n\u2264I\n\nm\u03020\nX\nX\nk=0 j\u2208J\n\n\u2264 m\u0302r0 IJ\n\nP(Sjr (Tjr (k\u2206r ) + \u2206r ) \u2212 Sjr (Tjr (k\u2206r )) \u2265 d1 r k )\n\n\u03c2(1)\nr km\n\n\u2264 C2 (t + 1)r 2\u2212k(1+m) ,\n\n\f41\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nfor some C2 \u2208 (0, \u221e), where the next to last inequality makes use of Assumption 2.13. Recalling [from (4.2)] that \u03c5 = k(1 + m) \u2212 2 > 1, we get that\nP(Sjr,2 6= \u2205) \u2264 C2 (t + 1)r \u2212\u03c5 \u2192 0\n\n(4.23)\n\nas r \u2192 \u221e.\n\nWe note that the above convergence only requires that \u03c5 > 0. The property\n\u03c5 > 1 will, however, be needed in the proof of Proposition 4.4 [see (4.28)].\nNext, using (4.13) and (4.14), we have for some \u033a1 \u2208 (0, \u221e),\nZ\n\u033a1\n(4.24)\n\u21920\nas r \u2192 \u221e,\n1S r,1 d(T\u0304jr,1 (s) \u2212 x\u2217j s) \u2264\nsup\nj\nr\n0\u2264u<\u221e [0,u]\n\nfor all j \u2208 J. The above inequality follows from the fact that the integral can\nbe written as the sum of integrals over \u2206-subintervals: When the subinterval\nis within some I2 (n), the integral is zero [using the definition of pr (s) in\nsuch intervals and (4.13)] and when the subinterval is within some I1 (n)\n[the number of such intervals is (p0 + 1)mr1 which can be bounded by C3 r\u03c1\n\u2206\nfor some C3 ], the integral is bounded by C1 \u2206/r 2 from (4.14).\nNow, combining (4.15), (4.23) and (4.24), we have that, for each j \u2208 J,\nH\u0304jr \u2192 0, u.o.c. in probability, as r \u2192 \u221e. From (4.21), (4.16), Lemma 4.2\nand unique solvability of the Skorohod problem for (0, D), we now have that\n(Q\u0304r , l\u0304r ) \u2192 (0, 0), u.o.c. in probability, as r \u2192 \u221e. Thus, L\u0304r converges to 0 as\nwell. The result now follows on noting that T\u0304 r = x\u2217 i \u2212 H\u0304 r \u2212 L\u0304r . \u0003\n\nThe following proposition gives a key estimate in the proof of Theorem 2.17.\nProposition 4.4.\nE(|Q\u0302r |\u1fe1\u221e,t\n\n+ |\u03b6\u0302 r |\u1fe1\u221e,t\n\nFor some c2 \u2208 (0, \u221e) and \u1fe1 \u2208 (1, \u221e),\n+ |\u0176 r |\u1fe1\u221e,t ) \u2264 c2 (1 + t3 )\n\nfor all r \u2265 r0 , t \u2265 0.\n\nProof. Using standard moment estimates for renewal processes (cf.\nLemma 3.5 of [9]), one can find C1 \u2208 (0, \u221e) such that\n(4.25)\n\nE|\u03b6\u0302 r |2\u221e,t \u2264 C1 (1 + t2 )\n\nFrom (4.19) and (4.21), we have\n\nfor all r \u2265 r0 , t \u2265 0.\n\nQ\u0302r (t) = \u03b6\u0302 r (t) + R\u0124 r (t) + RL\u0302r (t) = \u03b6\u0302 r (t) + R\u0124 r (t) + D l\u0302r (t),\n\nt \u2265 0,\n\nwhere \u0124 r = r H\u0304 r , L\u0302r = r L\u0304r and l\u0302r = r l\u0304r . We rewrite the above display as\nQ\u0302r (p\u0304r (t)) = [Q\u0302r (p\u0304r (t)) \u2212 Q\u0302r (t)] + \u03b6\u0302 r (t) + R\u0124 r (t) + D l\u0302r (t),\n\nFrom Theorem 5.1 of [29], for some C2 \u2208 (0, \u221e),\n(4.26)\n\nt \u2265 0.\n\n|l\u0302r |\u221e,t + |Q\u0302r |\u221e,t\n\u0013\n\u0012\nd1 r k\nr\nr\nr\nr r\nr\n.\n\u2264 C2 q\u0302 + |\u03b6\u0302 |\u221e,t + |\u0124 |\u221e,t + |Q\u0302 (p\u0304 (*)) \u2212 Q\u0302 |\u221e,t +\nr\n\n\f42\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nAlso, from (4.20), (4.15) and (4.24), for all t \u2265 0 and r \u2265 r0 ,\nZ\nr\n1S r,2 (s) dT\u0304jr,1 (s) \u2264 \u033a + \u033a1 .\n\u0124j (t) \u2212 r\n(4.27)\n[0,t]\n\nj\n\nNext, using (4.23), we get, for some C3 \u2208 (0, \u221e),\n\u0013\u03c5\u22272\n\u0012\nZ\nr,1\n\u2264 r \u03c5 C3 (t3 + 1)r \u2212\u03c5\nE r sup\n1S r,2 (s) dT\u0304j (s)\n(4.28)\n\n0\u2264u\u2264t [0,u]\n\nj\n\n\u2264 C3 (t3 + 1).\n\nFinally, for some C4 \u2208 (1, \u221e), for all a \u2265 1,\n(4.29)\n\nP(|Q\u0302r (p\u0304r (*)) \u2212 Q\u0302r (*)|\u221e,t \u2265 a)\n\u0012\n\u0012\n\u0013 X \u0012\n\u0013\u0013\nr2t X\nar\nar\nr\nr\nr\nr\n\u2264 r\nP Ai (\u2206 ) \u2265\n+\nP Sj (\u2206 ) \u2265\n.\n\u2206\nC4\nC4\ni\u2208I\n\nj\u2208J\n\nUsing moment estimates for renewal process once more (Lemma 3.5 of [9]),\nwe can find C5 \u2208 (0, \u221e) such that\nP(|Q\u0302r (p\u0304r (*)) \u2212 Q\u0302r (*)|\u221e,t \u2265 a) \u2264\n\u2264\n\nC5 \u2206 r\nr2t\n\u2206r (ar \u2212 C5 \u2206r )2\n\nC5 r 2 t\n1\n.\na2 (r \u2212 C5 \u2206r /a)2\n\nThus, there is an r1 \u2208 (r0 , \u221e) and C6 \u2208 (0, \u221e) such that, for all r \u2265 r1 ,\n\n1\nC6 (t + 1).\na2\nThis shows that for some \u03c51 \u2208 (1, \u221e) and C7 \u2208 (0, \u221e),\n(4.30)\n\n(4.31)\n\nP(|Q\u0302r (p\u0304r (*)) \u2212 Q\u0302r (*)|\u221e,t \u2265 a) \u2264\n\n1\nE|Q\u0302r (p\u0304r (*)) \u2212 Q\u0302r (*)|\u03c5\u221e,t\n\u2264 C7 (1 + t),\n\nt \u2265 0.\n\nThe result now follows on using (4.25), (4.27), (4.28) and (4.31) in (4.26)\nand observing that \u0176 r = \u0124 r + diag(x\u2217 )C \u2032 l\u0302r . \u0003\nIn preparation for the proof of Theorem 2.17, we introduce the following\nnotation. For n = 0, . . . , p0 \u2212 1, we define processes q r,n , z r,n with paths in D\u03b8I\nand C\u03b8J , respectively, as\n(\n(Q\u0302r (n\u03b8 + \u03c1/r), 0),\nt \u2208 [0, \u03c1/r),\n(q r,n (t), z r,n (t)) =\nr\nr\nr\n(Q\u0302 (t + n\u03b8), \u0176 (t + n\u03b8) \u2212 \u0176 (n\u03b8 + \u03c1/r)),\nt \u2208 [\u03c1/r, \u03b8].\nWe denote by q r,p0 , z r,p0 the processes with paths in D I and C J , respectively,\ndefined by the right-hand side in the display above, by replacing n by p0\n\n\f43\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nand [\u03c1/r, \u03b8] with [\u03c1/r, \u221e). Then\n.\nq r = (q r,\u2217 , q r,p0 ) \u2208 D\u03b8p0 I \u00d7 D I\nq r,\u2217\n\na.s.,\n\n= (q r,0 , . . . , q r,p0 \u22121 ).\n\nwhere\nSimilarly, define the process z r with paths in\np0 J\nC\u03b8 \u00d7 C J . Recall \u03bd\u0304 r,n , \u03bd r,n introduced in (4.10). Denote \u03bd\u0304 r = (\u03bd\u0304 r,0 , \u03bd\u0304 r,1 , . . . ,\n\u03bd\u0304 r,p0 ) and \u03bd r = (\u03bd r,0 , \u03bd r,1 , . . . , \u03bd r,p0 ), where we set \u03bd\u0304 r,0 = 0 and \u03bd r,0 = \u03b50 y \u2217 .\nNext, for n = 0, . . . , p0 \u2212 1, define processes q (n) , z (n) with paths in C\u03b8I\nand C\u03b8J , respectively, as\n\u001a\n(Q\u0303(t + n\u03b8), \u1ef8 (t + n\u03b8) \u2212 \u1ef8 (n\u03b8)),\nt \u2208 [0, \u03b8),\n(q (n) (t), z (n) (t)) =\n(Q\u0303((n + 1)\u03b8\u2212), \u1ef8 ((n + 1)\u03b8\u2212) \u2212 \u1ef8 (n\u03b8)),\nt = \u03b8.\n\nAlso, define q (p0 ) , z (p0 ) by the first line of the above display by replacing \u03b8 by\n.\n\u221e. Then q = (q \u2217 , q (p0 ) ) \u2208 C\u03b8p0 I \u00d7 C I , a.s., where q \u2217 = (q (0) , . . . , q (p0 \u22121) ). Similarly, define the process z with paths in C\u03b8p0 J \u00d7 C J . Also, let for n = 1, . . . , p0 ,\n(1)\n(1)\n(1)\n(1)\n\u03bd\u0304 (n) = \u2202 \u1ef80 (n) = \u1ef80 (n\u03b8) \u2212 \u1ef80 ((n \u2212 1)\u03b8), where \u1ef80 is as above (3.15),\nand \u03bd (n) = \u1ef8 (n\u03b8) \u2212 \u1ef8 (n\u03b8\u2212). Then\n.\n.\n(4.32)\n\u03bd (n) = \u03b8\u03b50 (Q\u0303(n\u03b8\u2212), \u03bd\u0304 (n)).\n\u03bd\u0304 (n) = \u031fn (\u03c7\u03ba (n), \u0168n ),\nDefine \u03bd\u0304 = (\u03bd\u0304 (0) , \u03bd\u0304 (1) , . . . , \u03bd\u0304 (p0 ) ) and \u03bd = (\u03bd (0) , \u03bd (1) , . . . , \u03bd (p0 ) ), where \u03bd\u0304 (0) =\n\u03b7 \u2297(p0 +1)\n0 and \u03bd (0) = \u03b50 y \u2217 . Then \u03bd\u0304 r , \u03bd\u0304 \u2208 (SM\n)\nand \u03bd r , \u03bd \u2208 RJ(p0 +1) . Next, let\n(n)\n\n\u03bd0r,n = \u0176 r (n\u03b8 + \u03c1/r),\n\u03bd0 = \u1ef8 (n\u03b8),\nn = 0, 1, . . . , p0 .\n.\n. (0)\n(p )\nThen \u03bd0r = (\u03bd0r,0 , . . . , \u03bd0r,p0 ); \u03bd0 = (\u03bd0 , . . . , \u03bd0 0 ) \u2208 RJ(p0 +1) . Let\n\n\u03b7 \u2297(p0 +1)\n\u039e = D I \u00d7 (SM\n)\n\u00d7 (RJ(p0 +1) ) \u00d7 (RJ(p0 +1) ) \u00d7 (D\u03b8p0 I \u00d7 D I ) \u00d7 (C\u03b8p0 J \u00d7 C J ).\n.\nNote that J r = (\u03b6\u0302 r , \u03bd\u0304 r , \u03bd r , \u03bd0r , q r , z r ), r \u2265 1 and J = (\u03b6\u0303, \u03bd\u0304, \u03bd, \u03bd0 , q, z) are \u039evalued random variables. The following is the main step in the proof of\nTheorem 2.17.\n\nTheorem 4.5.\n\nAs r \u2192 \u221e, J r \u21d2 J .\n\nProof of the above theorem is given in the next subsection. Using Theorem 4.5, the proof of Theorem 2.17 is now completed as follows.\nProof of Theorem 2.17.\np0 \u0014 Z\nX\nr r\nr\nJ (q , T ) =\nE\n\n[br (n)/r 2 ,ar (n+1)/r 2 )\n\nn=0\n\n(4.33)\n\nFrom proposition and integration by parts,\n\n+E\n\nZ\n\n[ar (n)/r 2 ,br (n)/r 2 )\n\n=\n\nZ\np0\nX\nE\n\nn=0\n\n[br (n)/r 2 ,ar (n+1)/r 2 )\n\ne\u2212\u03b3t (h * Q\u0302r (t) + \u03b3p * \u00db r (t)) dt\ne\u2212\u03b3t (h * Q\u0302r (t) + \u03b3p * \u00db r (t)) dt\n\n\u0015\n\ne\u2212\u03b3t (h * Q\u0302r (t) + \u03b3p * \u00db r (t)) dt + \u03b5r ,\n\n\f44\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nwhere, using Proposition 4.4 and the observation that ar (n + 1)/r 2 \u2212 br (n)/\nr 2 \u2264 \u03c1/r \u2192 0, we have that \u03b5r \u2192 0 as r \u2192 \u221e. From Theorem 4.5, as r \u2192 \u221e,\nq r \u21d2 q. Combining this with Proposition 4.4, we get for every n = 0, . . . , p0 ,\nZ\ne\u2212\u03b3t h * Q\u0302r (t) dt\nlim E\nr\u2192\u221e\n\n[br (n)/r 2 ,ar (n+1)/r 2 )\n\nr\u2192\u221e\n\nZ\n\n= lim E\n\nZ\n\n= lim E\n(4.34)\n\n[n\u03b8+\u03c1/r,(n+1)\u03b8)\n\nr\u2192\u221e\n\n[n\u03b8,(n+1)\u03b8)\n\ne\u2212\u03b3t h * Q\u0302r (t) dt\n\ne\u2212\u03b3t h * q r,n (t \u2212 n\u03b8) dt\n\n=E\n\nZ\n\ne\u2212\u03b3t h * q (n) (t \u2212 n\u03b8) dt\n\n=E\n\nZ\n\ne\u2212\u03b3t h * Q\u0303(t) dt,\n\n[n\u03b8,(n+1)\u03b8)\n\n[n\u03b8,(n+1)\u03b8)\n\nwhere, by convention, [n\u03b8, (n + 1)\u03b8) = [p0 \u03b8, \u221e) when n = p0 . Next, for t \u2208\n[n\u03b8 + \u03c1/r, (n + 1)\u03b8), n = 0, . . . , p0 ,\n(4.35)\n\n\u03b3p * \u00db r (t) = \u03b3p * K(\u0176 r (t) \u2212 \u0176 r (n\u03b8 + \u03c1/r)) + \u03b3p * K \u0176 r (n\u03b8 + \u03c1/r)\n= \u03b3p * Kz r,n (t \u2212 n\u03b8) + \u03b3p * K\u03bd0r,n .\n\nFrom Theorem 4.5, as r \u2192 \u221e,\n\n(n)\n\n\u03b3p * Kz r,n (*) + \u03b3p * K\u03bd0r,n \u21d2 \u03b3p * Kz (n) (*) + \u03b3p * K\u03bd0\n\nin C\u03b81 . Combining this with (4.35) and Proposition 4.4 we now get similarly\nto (4.34), for n = 0, . . . , p0 ,\nZ\n\u03b3e\u2212\u03b3t p * \u00db r (t) dt\nlim E\nr\u2192\u221e\n\n[br (n)/r 2 ,ar (n+1)/r 2 )\n\n= lim E\nr\u2192\u221e\n\n=E\n\nZ\n\nZ\n\n[n\u03b8,(n+1)\u03b8)\n\n\u03b3e\u2212\u03b3t p * K(z r,n (t \u2212 n\u03b8) + \u03bd0r,n ) dt\n(n)\n\n[n\u03b8,(n+1)\u03b8)\n\n\u03b3e\u2212\u03b3t p * K(z (n) (t \u2212 n\u03b8) + \u03bd0 ) dt.\n\nNote that for t \u2208 [n\u03b8, (n + 1)\u03b8),\n(n)\n\nz (n) (t \u2212 n\u03b8) + \u03bd0 = \u1ef8 (n\u03b8) + \u1ef8 (t) \u2212 \u1ef8 (n\u03b8) = \u1ef8 (t).\n\nThus, the expression on the right-hand side of the above display equals\nZ\n\u03b3e\u2212\u03b3t p * \u0168 (t) dt.\nE\n[n\u03b8,(n+1)\u03b8)\n\nThe result now follows on using this observation along with (4.34) in (4.33).\n\u0003\n\n\f45\n\nCONVERGENCE OF VALUE FUNCTIONS\n\n4.3. Proof of Theorem 4.5. For j \u2208 J, n = 0, 1, . . . , p0 , and \u03c9 \u2208 \u03a9, define\n\n(4.36)\n\n\u0160jr,n (\u03c9) = {s \u2208 [0, \u03c1] : Qr\u03c31 (j) (nr 2 \u03b8 + rs, \u03c9) = 0}.\n\nFrom the definition of \u03b8\u03b50 , it follows that for some \u03b30 > 0,\ninf\n\nmin\n\n(R\u03bd r,n )j \u2265 \u03b30\n\nr\u2265r0 n=0,...,p0 ;j\u2208J\n\na.e.\n\nAs a consequence of this observation, we have the following result. The proof\nis given in Section 4.4.\nProposition 4.6.\nwe have\n\nFor some {\u03c1r } \u2282 [0, \u03c1] such that \u03c1r \u2192 0 as r \u2192 \u221e,\n\nP(\u03a8rn ) \u2192 1\nas r \u2192 \u221e, for all n = 0, 1, . . . , p0 ,\nS\nr,n\nr\nwhere \u03a8n = {\u03c9 \u2208 \u03a9 : ( j\u2208J \u0160j (\u03c9)) \u2229 [\u03c1r , \u03c1] = \u2205}.\n(4.37)\n\nFor n = 0, 1, . . . , p0 , let \u03bd\u0304 r [n] = (\u03bd\u0304 r,0 , . . . , \u03bd\u0304 r,n ). We define \u03bd r [n], \u03bd0r [n], q r [n],\nz r [n] and their limiting analogues \u03bd\u0304[n], \u03bd[n], \u03bd0 [n], q[n], z[n] in a similar fashion. Set\nJ r [n] = (\u03b6\u0302 r , \u03bd\u0304 r [n], \u03bd r [n], \u03bd0r [n], q r [n], z r [n]),\nJ [n] = (\u03b6\u0302, \u03bd\u0304[n], \u03bd[n], \u03bd0 [n], q[n], z[n]).\n\nThen J r [n], J [n] are \u039e[n]-valued random variables, with\n\n(n+1)I\n\n\u03b7 \u2297(n+1)\n\u039e[n] = D I \u00d7 (SM\n)\n\u00d7 (RJ(n+1) ) \u00d7 (RJ(n+1) ) \u00d7 D\u03b8\n\n(n+1)J\n\n\u00d7 C\u03b8\n\n,\n\nwhere we follow the usual convention for n = p0 . To prove the theorem, we\nneed to show that J r [p0 ] \u21d2 J [p0 ]. In the lemma below we will in fact show,\nrecursively in n, that J r [n] \u21d2 J [n] as r \u2192 \u221e, for each n = 0, 1, . . . , p0 , which\nwill complete the proof of Theorem 4.5.\nLemma 4.7.\n\nFor each n = 0, 1, . . . , p0 , J r [n] \u21d2 J [n], as r \u2192 \u221e.\n\nProof. The proof will follow the following two steps:\n(i) As r \u2192 \u221e, J r [0] \u21d2 J [0].\n(ii) Suppose that J r [k] \u21d2 J [k] as r \u2192 \u221e for k = 0, 1, . . . , n, for some\nn < p0 . Then, as r \u2192 \u221e, J r [n + 1] \u21d2 J [n + 1].\n\nConsider (i). Define scaled processes Q\u030cr (t) = Qr (rt)/r, Y\u030c r (t) = Y r (rt)/r.\nProcesses X\u030c r , \u0164 r , \u0164 r,1 , \u03b6\u030c r are defined similarly. By the functional central\nlimit theorem for renewal processes and Proposition 4.3, it follows that (cf.\nLemma 3.3 of [9])\n(4.38)\n\n\u03b6\u0302 r \u21d2 \u03b6\u0303.\n\nAlso, convergence of (\u03bd\u0304 r [0], \u03bd r [0]) follows trivially since \u03bd\u0304 r [0] = \u03bd\u0304[0] = 0 and\n\u03bd r [0] = \u03bd[0] = \u03b50 y \u2217 . Next, consider \u03bd0r [0] = \u0176 r (\u03c1/r). From the definition of\n\n\f46\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nthe scaled processes defined above (4.36), we have that\n1\n(4.39)\n\u03b6\u030c r (t) = q\u030c r + X\u030c r (t) + [\u03b81r t \u2212 (C \u2212 P \u2032 ) diag(\u03b82r )\u0164 r (t)],\nr\nand\n(4.40)\n\nQ\u030cr (t) = \u03b6\u030c r (t) + RY\u030c r (t),\n\nt \u2208 [0, \u03c1].\n\nAlso, observe that Y\u030c r can be written as\n\u0012\u0012\n\u0013\n\u0013\n\u03b50 y \u2217\n1\nr\n\u2217\nr,1\n(4.41)\nY\u030c (t) =\nx \u2212\nt \u2212 \u0164 (t) + \u03b50 y \u2217 t + \u0147 r (t),\n\u03c1\n\u03c1\nwhere, with \u0160jr,0 defined in (4.36),\nZ\nr\n(4.42)\n1\u0160 r,0 (s) d\u0164jr,1 (s).\n\u0147j (t) =\n[0,t]\n\nj\n\nNext, note that, for a suitable C1 \u2208 (0, \u221e),\n\u0012\n\u0013\n\u03b50 y \u2217\n\u2206r\nr,1\n\u2217\n\u0164 \u2212 x \u2212\n(4.43)\n.\ni\n\u2264 C1\n\u03c1\nr\n\u221e,\u03c1\n\nAlso, |\u0147 r |\u221e,\u03c1 \u2264 \u03c1r + \u03c11(\u03a8rn )c . Thus, from Proposition 4.6, |\u0147 r |\u221e,\u03c1 converges\nto 0 in probability as r \u2192 \u221e, which shows that\n\u0013\n\u0012\n\u03b50 \u2217\n\u03b50 \u2217\nr\n\u2217\n= x \u2212 y i \u2212 \u0164 r\nY\u030c \u2212 y\n\u21920\n\u03c1\n\u03c1\n\u221e,\u03c1\n\u221e,\u03c1\n(4.44)\nin probability, as r \u2192 \u221e.\n\nThe above convergence is the key reason for introducing the modification\nof \u1ef8 (1) , through the vector y \u2217 , described above Theorem 3.8.\nNext, standard moment bounds for renewal processes (see, e.g., Lemma 3.5\nof [9]) yield that |X\u030c r |\u221e,\u03c1 converges to zero in probability as r \u2192 \u221e. Combining these observations, we get from (4.39) and (4.40) that (Q\u030cr , Y\u030c r ) converge,\n\u2217\nuniformly over [0, \u03c1], in probability, to (q + \u03b5\u03c10 Ry \u2217 i , \u03b50\u03c1y i ). In particular, this\nshows that\n(4.45)\n\n(q r,0 (0), \u03bd0r,0 ) = (Q\u030cr (\u03c1), Y\u030c r (\u03c1))\n\u21d2\n\n(0)\n\n(q + \u03b50 Ry \u2217 , \u03b50 y \u2217 ) = (q (0) (0), \u03bd0 ).\n\nFinally, we prove the convergence of (q r,0 , z r,0 ) to (q (0) , z (0) ). We will apply\nTheorem 4.1 of [29]. Note that\n(4.46)\n\nq r,0 (t) = q r,0 (0) + wr,0 (t) + Rz r,0 (t),\n\nt \u2208 [0, \u03b8],\n\nwhere wr,0 is a D\u03b8I -valued random variable defined as wr,0 (t) = (\u03b6\u0302 r (t) \u2212\n\u03b6\u0302 r (\u03c1/r))1[\u03c1/r,\u221e) (t). From (4.45) and (4.38)\n(4.47)\n\nq r,0 (0) \u2192 q (0) (0)\n\nand\n\nwr,0 \u21d2 \u03b6\u0303\n\nas r \u2192 \u221e.\n\n\f47\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nNext, for t \u2208 [\u03c1/r, \u03b8), write\n\nz r,0 (t) = \u0124 r,0 (t) + L\u0302r,0 (t),\n\n(4.48)\nwhere for j \u2208 J,\nZ\n\u0124jr,0 (t) = r\n\n[\u03c1/r,t]\n\n\u2217\nL\u0302r,0\nj (t) = rxj\n\n(1 \u2212 1S r,1 (s)) d(x\u2217j s \u2212 T\u0304jr,1 (s)) + r\nj\n\nZ\n\n[\u03c1/r,t]\n\nZ\n\n[\u03c1/r,t]\n\n1S r,2 (s) dT\u0304jr,1 (s),\nj\n\n1S r,1 (s) ds,\nj\n\nwith Sjr,i defined in (4.17). Using calculations similar to those in the proof\nof Proposition 4.3 [see (4.22)], we get\nZ\n1S r,2 (s)T\u0304jr,1 (s) \u2192 0\nsup r\n(4.49)\nin probability, as r \u2192 \u221e,\n\u03c1/r\u2264t\u2264\u03b8\n\n[\u03c1/r,t]\n\nj\n\nfor all j \u2208 J.\nAlso, from (4.13) it follows that\nZ\n\u2206r\n(1 \u2212 1S r,1 (s)) d(x\u2217j s \u2212 T\u0304jr,1 (s)) \u2264\nsup r\n.\nj\nr\n[\u03c1/r,t]\n\u03c1/r\u2264t\u2264\u03b8\nCombining the above estimates,\n(4.50)\n\nsup |\u0124jr,0 (t)| \u2192 0\n\n\u03c1/r\u2264t\u2264\u03b8\n\nin probability, as r \u2192 \u221e.\n\nAlso, L\u0302r,0 (t) = (diag(x\u2217 ))C \u2032 l\u0302r,0 (t), where for i \u2208 I and t \u2208 [\u03c1/r, \u03b8],\nZ\nr,0\n1{Q\u0302r (p\u0304r (s))\u2264r\u0398\u0304r (s)} ds\nl\u0302i (t) = r\ni\n\n[\u03c1/r,t]\n\n(4.51)\n\n=r\n\nZ\n\n[\u03c1/r,t]\n\n1{q r,0 (p\u0304r (s))\u2264r\u0398\u0304r (s)} ds.\ni\n\nRecall that z r,0 (t) = 0 for t \u2208 [0, \u03c1/r]. Hence, setting \u0124 r,0 (t) = l\u0302r,0 (t) = 0 for\nt \u2208 [0, \u03c1/r], we have from (4.46) and (4.48)\n(4.52)\n\nq r,0 (t) = q r,0 (0) + wr,0 (t) + R\u0124 r,0 (t) + D l\u0302r,0 (t),\n\nFrom (4.47) and (4.50) we now have that, as r \u2192 \u221e,\n(4.53)\n\nq r,0 (0) + wr,0 + R\u0124 r,0 \u21d2 q (0) + \u03b6\u0303\n\nt \u2208 [0, \u03b8].\n\nin D\u03b8I . Using the definition of pr , Assumption 2.13 and elementary properties\nof renewal processes [see similar arguments in (4.22) and (4.29)], we have\nthat for some C2 , as r \u2192 \u221e,\n\u0010\n\u0011\nr2\u03b8\nr2\u03b8 1\n=\nC\n\u2192 0.\nP sup |Q\u0302r (p\u0304r (s)) \u2212 Q\u0302r (s)| > \u03b5 \u2264 C2 r\n2\n\u2206 (\u2206r )m\nr k(m+1)\ns\u2208[\u03c1/r,\u03b8]\n\n\f48\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nThis shows that\n(4.54)\n\nsup |q r,0 (s) \u2212 q r,0 (p\u0304r (s))| \u2192 0\n\ns\u2208[0,\u03b8]\n\nin probability, as r \u2192 \u221e.\n\nUsing Theorem 4.1 of [29] along with (4.51), (4.52), (4.53) and (4.54), we\nnow have that\n(\u03b6\u0302 r , q r,0 , l\u0302r,0 ) \u21d2 (\u03b6\u0303, \u0393(q (0) (0) + \u03b6\u0303), \u0393\u0302(q (0) (0) + \u03b6\u0303))\n\nas D I \u00d7 D\u03b8I \u00d7 D\u03b8I valued random variables. Since\n\u0393(q (0) (0) + \u03b6\u0303) = q (0)\n\nand\n\nas r \u2192 \u221e,\n\ndiag(x\u2217 )C \u2032 \u0393\u0302(q (0) (0) + \u03b6\u0303) = z (0) ,\n\nwe get from the above display that (q r,0 , z r,0 ) \u21d2 (q (0) , z (0) ) as r \u2192 \u221e. Combining this with (4.45) and observations below (4.38), we have J r [0] \u21d2 J [0],\nwhich completes the proof of (i).\nWe now prove (ii). We can write\nJ r [n + 1] = (J r [n], (\u03bd\u0304 r,n+1 , \u03bd r,n+1 , \u03bd0r,n+1 , q r,n+1 , z r,n+1 ))\n(n+1)\n\nJ [n + 1] = (J r [n], (\u03bd\u0304 (n+1) , \u03bd (n+1) , \u03bd0\n\n, q (n+1) , z (n+1) )).\n\nBy assumption,\n\nJ r [n] \u21d2 J [n]\n\n(4.55)\n\nand, thus, in particular, (4.38) holds. This shows that \u03c7\u0302\u03ba,r (n+1) \u21d2 \u03c7\u03ba (n+1)\nand as a consequence, using continuity properties of \u031fn+1 ,\n.\n\u03bd\u0304 r,n+1 = \u031fn+1 (\u03c7\u0302\u03ba,r (n + 1), Un+1 ) \u21d2 \u031fn+1 (\u03c7\u03ba (n + 1), \u0168n+1 )\n(4.56)\n= \u03bd\u0304 (n+1) .\nIn fact, this shows the joint convergence: (J r [n], \u03bd\u0304 r,n+1 ) \u21d2 (J [n], \u03bd\u0304 n+1 ). In\nparticular, we have\n(\u03bd\u0304 r,n+1 , Q\u0302r ((n + 1)\u03b8)) = (\u03bd\u0304 r,n+1 , q r,n (\u03b8))\n\u21d2\n\n(\u03bd\u0304 (n+1) , q (n) (\u03b8)) = (\u03bd\u0304 (n+1) , Q\u0303((n + 1)\u03b8\u2212)).\n\nFor the remaining proof, to keep the presentation simple, we will not explicitly note the joint convergence of all the processes being considered. From\ncontinuity of the map \u03b8\u03b50 , we now have that\n\u03bd r,n+1 = \u03b8\u03b50 (Q\u0302r ((n + 1)\u03b8), \u03bd\u0304 r,n+1 )\n\n\u21d2\n\n\u03b8\u03b50 (Q\u0303((n + 1)\u03b8\u2212), \u03bd\u0304 (n+1) ) = \u03bd (n+1) .\n(n+1)\n\nNext, we consider the weak convergence of \u03bd0r,n+1 to \u03bd0\n. The proof is\nsimilar to the case n + 1 = 0 treated in the first part of the lemma [cf.\nbelow (4.38)] and so only a sketch will be provided. Note that\n\u03bd0r,n+1 = \u03bd0r,n + z r,n (\u03b8) + (\u0176 r ((n + 1)\u03b8 + \u03c1/r) \u2212 \u0176 r ((n + 1)\u03b8))\n\nq r,n+1 (0) = q r,n (\u03b8) + (Q\u0302r ((n + 1)\u03b8 + \u03c1/r) \u2212 Q\u0302r ((n + 1)\u03b8)).\n\n\f49\n\nCONVERGENCE OF VALUE FUNCTIONS\n(n)\n\nWeak convergence of (q r,n (\u03b8), \u03bd0r,n + z r,n (\u03b8)) to (q (n) (\u03b8), \u03bd0 + z (n) (\u03b8)) is\na consequence of (4.55). Next, abusing notation introduced above (4.38),\ndefine for t \u2208 [0, \u03c1],\nQ\u030cr (t) = r \u22121 Qr (r 2 \u03b8(n + 1) + rt),\n\nq\u030c r = Q\u030cr (0),\n\nY\u030c r (t) = r \u22121 (Y r (r 2 \u03b8(n + 1) + rt) \u2212 Y r (r 2 \u03b8(n + 1))).\n\nProcesses X\u030c r , \u0164 r , \u0164 r,1 , \u03b6\u030c r are defined similarly to Y\u030c r . Then, equations (4.39)\nand (4.40) are satisfied with these new definitions. Hence, using arguments\nsimilar to the ones used in the proof of (4.45) (in particular, making use of\nProposition 4.6), we have that (Y\u030c r , Q\u030cr ) converges in distribution to\n\u0012 (n+1)\n\u0013\n\u03bd\n1 (n+1)\ni , Q\u0303((n + 1)\u03b8\u2212) + R\u03bd\ni ,\n\u03c1\n\u03c1\nas r \u2192 \u221e. Combining the above observations, we have, as r \u2192 \u221e,\n\n(4.57)\n\n(q r,n+1 (0), \u03bd0r,n+1 ) = (Q\u030cr (\u03c1), Y\u030c r (\u03c1))\n(n)\n\n(n+1)\n\n(Q\u0303((n + 1)\u03b8), \u03bd0 + z (n) (\u03b8) + \u03bd (n+1) ) = (q (n+1) (0), \u03bd0\n\n\u21d2\n\nFinally, we consider weak convergence of\nSimilar to (4.46), we have\n\n(q r,n+1 , z r,n+1 )\n\nq r,n+1 (t) = q r,n+1 (0) + wr,n+1 (t) + Rz r,n+1 (t),\nwhere\n\nwr,n+1\n\nis a\n\nD\u03b8I -valued\nr\n\nrandom variable defined as\n\nto\n\n).\n\n(q (n+1) , z (n+1) ).\n\nt \u2208 [0, \u03b8],\n\nwr,n+1 (t) = (\u03b6\u0302 (t + (n + 1)\u03b8) \u2212 \u03b6\u0302 r ((n + 1)\u03b8 + \u03c1/r))1[\u03c1/r,\u221e) (t).\n\nUsing (4.57) and (4.38), as r \u2192 \u221e,\n(4.58)\n\nq r,n+1 (0) + wr,n+1 \u21d2 q (n+1) (0) + \u03b6\u0303((n + 1)\u03b8 + *) \u2212 \u03b6\u0303((n + 1)\u03b8).\n\nWeak convergence of (q r,n+1 , z r,n+1 ) to (q (n+1) , z (n+1) ) now follows exactly\nas below (4.47). Combining the above weak convergence properties, we now\nhave J r [n + 1] \u21d2 J [n + 1] and the result follows. \u0003\n4.4. Proof of Proposition 4.6. We will only consider the case n = 0. The\nr\u03c1\ngeneral case is treated similarly. Let Mr = \u230a \u2206\nr \u230b. From Assumption 2.13, for\neach \u03b4 > 0, one can find C1 (\u03b4) such that, for i \u2208 I, j \u2208 J, r \u2265 1 and k \u2264 Mr ,\nC1 (\u03b4)\n,\nr km\nC1 (\u03b4)\nP(|Sjr (Tjr,1 ((k + 1)\u2206r )) \u2212 Sjr (Tjr,1 (k\u2206r )) \u2212 \u03b2jr \u03c4jr,k | \u2265 \u03b4\u2206r ) \u2264 km ,\nr\nP(|Eir ((k + 1)\u2206r ) \u2212 Eir (k\u2206r ) \u2212 \u03b1ri \u2206r | \u2265 \u03b4\u2206r ) \u2264\n\n(4.59)\n\nr,1\nj,r\nr,1\nj r r,k\nr\nr\nr\nr\nr\nP(|\u03a6j,r\ni (Sj (Tj ((k + 1)\u2206 ))) \u2212 \u03a6i (Sj (Tj (k\u2206 ))) \u2212 pi \u03b2j \u03c4j | \u2265 \u03b4\u2206 )\n\n\u2264\n\nC1 (\u03b4)\n,\nr km\n\n\f50\n\nA. BUDHIRAJA AND A. P. GHOSH\n\u03b50 y \u2217\n\nwhere \u03c4jr,k = Tjr,1 ((k + 1)\u2206r ) \u2212 Tjr,1 (k\u2206r ) = (x\u2217j \u2212 \u03c1 j )\u2206r . Denote the union,\nover all i, j, of events on the left-hand side of the three displays in (4.59),\nby Hkr . Then, the above estimates, along with (4.2), yield\n!\nM\n[r\nP\n(4.60)\nas r \u2192 \u221e.\nHkr \u2192 0\nk=0\n\nDefine for k = 0, 1, . . . , Mr ,\n.\nQr0 ((k + 1)\u2206r ) = Qr (k\u2206r ) + E r ((k + 1)\u2206r ) \u2212 E r ((k\u2206r ))\n\n\u2212 C(S r (T r,1 ((k + 1)\u2206r )) \u2212 S r (T r,1 (k\u2206r )))\n\n+ \u03a6r (S r (T r,1 ((k + 1)\u2206r ))) \u2212 \u03a6r (S r (T r,1 (k\u2206r ))).\n\nNote that, on the set (Hkr )c , we have for some C2 > 0,\nQr0 ((k + 1)\u2206r )\n\n\u0013\n\u0012\n\u03b50 y \u2217\n\u2217\n\u2206r \u2212 C2 \u03b4\u2206r 1I .\n\u2265 Q (k\u2206 ) + \u03b1 \u2206 \u2212 (C \u2212 P ) diag(\u03b2 ) x \u2212\n\u03c1\nr\n\nr\n\nr\n\nr\n\n\u2032\n\nr\n\nUsing Assumption 2.1, we now have that for some C3 , on the set (Hkr )c , for\nall r \u2265 r0 ,\n\u0012\n\u0013\n\u2206r\n\u03b50 \u2217 r\nr\nr\nr\nr\nr\n1I .\nQ0 ((k + 1)\u2206 ) \u2265 Q (k\u2206 ) + Ry \u2206 \u2212 C2 \u03b4\u2206 + C3\n\u03c1\nr\nRecall that Ry \u2217 > \u03b30 1I . Fix \u03b4 small enough so that for some \u03b51 > 0 and\nr1 > r0 ,\n\u0012\n\u0013\n\u03b50\nC3\n\u03b30 \u2212 C2 \u03b4 +\n(4.61)\n\u2265 \u03b51\nfor all r \u2265 r1 .\n\u03c1\nr\nThen, for every k = 0, 1, . . . , Mr ,\n(4.62)\n\non the set (Hkr )c ,\n\nQr ((k + 1)\u2206r ) \u2265 Qr0 ((k + 1)\u2206r )\n\n\u2265 Qr (k\u2206r ) + \u03b51 1I \u2206r ,\n\nr \u2265 r1 .\n\nRecall d1 introduced above (4.3). Let m0 be large enough so that \u03b51 m0 > d1 .\nThen using (4.62), we get that\n(4.63)\n\non the set\n\nM\n\\r\n\n(Hkr )c ,\n\nk=0\n\nQr (k\u2206r ) \u2265 d1 1I \u2206r ,\n\nNext, let\no\nn\nFkr = \u03c9 : inf Qri (k\u2206r , \u03c9) \u2265 d1 \u2206r\n\nfor all k = m0 , . . . , Mr , r \u2265 r1 .\n\ni\u2208I\n\n(4.64)\n\n\u2229 {\u03c9 : Qri (t, \u03c9) = 0 for some i \u2208 I, t \u2208 [k\u2206r , (k + 1)\u2206r ]}\n\n.\n= Grk \u2229 Bkr .\n\n\f51\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nUsing estimates below (4.21), we see that\n!\nM\n[r\nP\n(4.65)\nas r \u2192 \u221e.\nFkr \u2192 0\nk=0\n\nAlso, from (4.63),\n\nlim inf P(Grm0 )\nr\u2192\u221e\n\n\u2265 lim inf P\n\nGrm0\n\n= lim inf P\n\nM\n\\r\n\nr\u2192\u221e\n\n(4.66)\nr\u2192\u221e\n\nNext, for r \u2265 r1 ,\n(4.67)\n\nP\n\nMr\n[\n\nk=m0\n\nAlso,\nP\n\nM\n[r\n\nk=m0\n\n(4.68)\n\n(Bkr\n\nBkr\n\n!\n\n\u2264P\n\n!\n\n\u2229 Grm0 )\n\nM\n[r\n\nk=m0\n\n=P\n\n\"\n\n+P\n\n\u2264P\n\n!\n\n(Hkr )c\nk=0\n\n\u0015!\n\n= 1.\n\n(Bkr \u2229 Grm0 ) + P((Grm0 )c ).\n\n(Bkr\n\nk=m0\n\n\"\n\nk=0\n\n(Hkr )c\nk=0\n\n!\n\nM\n[r\n\nM\n[r\n\n\u2229\n\n\u0014M\n\\r\n\nM\n[r\n\n#\n\n\u2229 Grm0 )\n\n!\n\n#\n\n(Bkr\n\n\u2229 Grm0 )\n\n+P\n\nM\n[r\n\nk=m0\n\nHkr\n\n\u2229\n\n\"M\n[r\n\nk=m0\n\nHkr\n\nk=0\n\n\u2229\n\n(Bkr\n\n\"M\n\\r\n\n#!\n\n(Hkr )c\nk=0\n!\n\n\u2229 Grk )\n\n#!\n\n,\n\nT r\nr c\nwhere the last inequality is a consequence of the fact that on M\nk=0 (Hk ) ,\nr\nr\nGk \u2286 Gk+1 for k \u2265 m0 . From (4.60) and (4.65) the above expression is seen\nto approach zero as r \u2192 \u221e. Using this observation and (4.66) in (4.67), we\nS r\nr\nnow see that P( M\nk=m0 Bk ) \u2192 0 as r \u2192 \u221e. Finally, recalling the definition\nr\nof Bk , we have\n!\nr\nM\n[\nP(Qri (s) = 0, for some i \u2208 I and s \u2208 [m0 \u2206r , r\u03c1]) = P\nBkr .\nk=m0\n\nThe proposition now follows on setting \u03c1r =\n\nm0 \u2206r\nr .\n\nAPPENDIX\nLemma A.1. Let {\u1ef8n }n\u22651 be a sequence of random variables, with values\nin a finite set S, given on a probability space (\u03a9, F, P). Let G be a sub-\u03c3 field\nof F . Suppose that {Gn }n\u22651 is a sequence of sub-\u03c3 fields of G and {Xn }n\u22651\na sequence of {Gn }-adapted, Rd -valued random variables such that\nP(\u1ef8n = \u03b6|Gn ) = pn,\u03b6 (Xn ),\n\nn \u2265 1, \u03b6 \u2208 S,\n\n\f52\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nwhere pn,\u03b6 : Rd \u2192 [0, 1] are measurable maps such that \u03a3\u03b6\u2208S pn,\u03b6 (x) = 1 for all\nx \u2208 Rd , n \u2265 1. Then there is a sequence of S-valued random variables {Yn }\ndefined on an augmentation of (\u03a9, F, P) such that\nP(Yn = \u03b6|G \u2228 Y0n\u22121 ) = pn,\u03b6 (Xn ),\n\nwhere Y0n\u22121 = \u03c3{Y1 , . . . , Yn\u22121 }.\n\nn \u2265 1, \u03b6 \u2208 S,\n\nProof. By suitably augmenting the space, we can assume that the\nprobability space (\u03a9, F, P) supports an i.i.d. sequence {Un }n\u22651 of Uniform\n[0, 1] random variables, independent of G. Let, for n \u2265 1, \u03b6 \u2208 S, an,\u03b6 , bn,\u03b6 : Rd \u2192\n[0, 1] be measurable maps, such that for all x \u2208 Rd :\n(i) bn,\u03b6 (x) \u2212 an,\u03b6 (x) = pn,\u03b6 (x), n \u2265 1, \u03b6 \u2208 S.\n(ii) S\n[an,\u03b6 (x), bn,\u03b6 (x)) \u2229 [an,\u03b6 \u2032 (x), bn,\u03b6 \u2032 (x)) = \u2205, \u03b6, \u03b6 \u2032 \u2208 S, \u03b6 6= \u03b6 \u2032 , n \u2265 1.\n(iii) \u03b6\u2208S [an,\u03b6 (x), bn,\u03b6 (x)) = [0, 1). The result follows on defining\nX\nYn =\n\u03b61[an,\u03b6 (Xn ),bn,\u03b6 (Xn )) (Un ),\nn \u2265 1.\n\u03b6\u2208S\n\n\u0003\n\nA.1. Proof of Lemma 3.7. From (2.38) we have that for some C1 \u2208\n(0, \u221e),\n|Q\u03031 \u2212 Q\u03032 |\u221e,T \u2264 C1 |\u1ef8 1 \u2212 \u1ef8 2 |\u221e,T .\n\n(A.1)\n\nThus, for some C2 \u2208 (0, \u221e),\nZ\nZ T\n\u2212\u03b3t\n1\ne h * Q\u0303 (t) dt \u2212 \u1ebc\n\u1ebc\n(A.2)\n\nT\n\n0\n\n0\n\ne\u2212\u03b3t h * Q\u03032 (t) dt \u2264 C2 \u1ebc|\u1ef8 1 \u2212 \u1ef8 2 |\u221e,T .\n\nNext, for t \u2265 0 and i = 1, 2,\n\nQ\u0303i (t + T ) = \u0393(Q\u0303i (T ) + \u03b6\u0303(T + *) \u2212 \u03b6\u0303(T )).\n\n(A.3)\n\nUsing Assumption 2.15 and (A.1), we now have that for all t \u2265 0,\n\n(A.4)\n\n|Q\u03031 (t + T ) \u2212 Q\u03032 (t + T )| \u2264 L|Q\u03031 (T ) \u2212 Q\u03032 (T )| \u2264 LC1 |\u1ef8 1 \u2212 \u1ef8 2 |\u221e,T .\n\nThis shows that, for some C3 \u2208 (0, \u221e),\nZ \u221e\nZ \u221e\n\u2212\u03b3t\n1\n(A.5) \u1ebc\ne\u2212\u03b3t h * Q\u03032 (t) dt \u2264 C3 \u1ebc|\u1ef8 1 \u2212 \u1ef8 2 |\u221e,T .\ne h * Q\u0303 (t) dt \u2212 \u1ebc\nT\n\nT\n\nNext, for some C4 \u2208 (0, \u221e),\nZ\nZ\n\u2212\u03b3t\n1\ne p * d\u0168 (t) \u2212\n\n[0,T ]\n\n[0,T ]\n\n(A.6)\n\ne\u2212\u03b3t p * d\u0168 2 (t)\n\n\u0014\n\u2264 |p| |\u0168 1 (0) \u2212 \u0168 2 (0)| + e\u2212\u03b3T |\u0168 1 (T ) \u2212 \u0168 2 (T )|\n\u0015\nZ\n1\n2\n|\u0168 (t) \u2212 \u0168 (t)| dt\n+\u03b3\n[0,T ]\n\n1\n\n2\n\n\u2264 C4 |\u1ef8 \u2212 \u1ef8 |\u221e,T .\n\n\f53\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nNext, note that for S > T , i = 1, 2,\nZ\ne\u2212\u03b3t p * d\u0168 i (t)\n(A.7)\n\n(T,S]\n\n=\u03b3\n\nZ\n\nS\n\ne\u2212\u03b3t p * [\u0168 i (t) \u2212 \u0168 i (T )] dt + e\u2212\u03b3S p * [\u0168 i (S) \u2212 \u0168 i (T )].\n\nT\n\nAlso, using Assumption 2.15 and (2.39), for some C5 \u2208 (0, \u221e),\n|\u0168 i (t) \u2212 \u0168 i (T )| \u2264 C5 |\u03b6\u0303(T + *) \u2212 \u03b6\u0303(T )|\u221e,t\u2212T ,\n\nt \u2265 T,\n\nwhich shows that, for i = 1, 2,\n\u1ebc|e\u2212\u03b3S p * [\u0168 i (S) \u2212 \u0168 i (T )]| \u2192 0\n\nas S \u2192 \u221e.\n\nCombining this observation with (A.7), we now have, on sending S \u2192 \u221e,\nthat for i = 1, 2,\nZ \u221e\nZ\ne\u2212\u03b3t p * [\u0168 i (t) \u2212 \u0168 i (T )] dt.\ne\u2212\u03b3t p * d\u0168 i (t) = \u03b3 \u1ebc\n\u1ebc\nT\n\n(T,\u221e)\n\nThus, for some C6 \u2208 (0, \u221e),\nZ\nZ\n\u2212\u03b3t\n1\n\u1ebc\ne p * d\u0168 (t) \u2212\n(T,\u221e)\n\n\u2264 \u03b3|p|\n\nZ\n\n\u2264 C6 \u1ebc\n\n(T,\u221e)\n\nZ\n\n(T,\u221e)\n\ne\u2212\u03b3t p * d\u0168 2 (t)\n\ne\u2212\u03b3t |[\u0168 1 (t) \u2212 \u0168 1 (T )] \u2212 [\u0168 2 (t) \u2212 \u0168 2 (T )]| dt\n\n(T,\u221e)\n\ne\u2212\u03b3t |Q\u03031 (T ) \u2212 Q\u03032 (T )| dt,\n\nwhere the last equality follows on using Assumption 2.15 and (A.3). Combining this with (A.1), we now have that\nZ\nZ\ne\u2212\u03b3t p * d\u0168 2 (t) \u2264 C7 \u1ebc|\u1ef8 1 \u2212 \u1ef8 2 |\u221e,T .\n\u1ebc\ne\u2212\u03b3t p * d\u0168 1 (t) \u2212\n(T,\u221e)\n\n(T,\u221e)\n\nThe result now follows on combining the above estimate with (A.2), (A.5)\nand (A.6).\nA.2. Proof of Proposition 4.1. It is immediate from the construction\nthat T r satisfies (i)\u2013(iii) of Definition 2.7. We now verify that, with G =\n\u03c3{Ui , i \u2265 1},\n(A.8)\n\nT r satisfies (iv).\n\nThe proof of (A.8) is similar to that of Theorem 5.4 in [9], which shows that\nif a policy satisfies certain natural conditions (see Assumptions 5.1, 5.2, 5.3\n\n\f54\n\nA. BUDHIRAJA AND A. P. GHOSH\n\ntherein), then it is admissible (in the sense of Definition 2.7 of the current\npaper). The policy T r constructed in Section 4.1 does not exactly satisfy\nconditions in Section 5 of [9], but it has similar properties. Since most of the\narguments are similar to [9], we only provide a sketch, emphasizing only the\nchanges that are needed. For the convenience of the reader, we use similar\nnotation as in [9]. Also, we suppress the superscript r from the notation.\nRecall from (4.8) that\nZ\n(1)\n(A.9)\n1{Q\u03c3 (j) (u)>0} 1{Q\u03c3 (j) (p(u))>\u0398(u)} \u1e6aj (u) du,\nTj (t) =\n[0,t]\n\n1\n\n1\n\nj \u2208 J, t \u2265 0.\n\nIn particular, Assumption 5.1 of [9] is satisfied. In view of (2.1), the integrand\n(1)\nabove has countably many points (a.s.) where the value of \u1e6aj changes\nfrom 0 to 1 (or vice versa). Denote these points by {\u1fe91l }l\u2208N . Set \u1fe910 = 0. We\nrefer to these points as the \"break-points\" of T . Break-points are boundaries\nof the intervals of the form {[nr 2 \u03b8, (n + 1)r 2 \u03b8)}n or those of subintervals of\nlength \u22071,k,n\nor \u22072,k,n\nfor some k, i, n [see (4.11), see also (4.4) for \u22071,k\ni\ni\ni\n2,k\nor \u2207i ] that are used to define the policy T . Next, define {\u1fe90l }l\u2208N0 as the\ncountable set of (random) \"event-points\" as defined in [9] (denoted there as\n{\u03a5l }l\u2208N0 ). These are the points where either an arrival of a job or service\ncompletion of a job takes place anywhere in the network. Combining the\nevent-points and the break-points, we get the set of \"change-points\" of the\npolicy T denoted by {\u03a5l }:\n{\u03a5l } = {\u1fe90l } \u222a {\u1fe91l }.\n\nWe will assume that the sequence {\u03a5l } [resp. {\u1fe90l }, {\u1fe91l }] is indexed such\nthat \u03a5l [resp. \u1fe90l , \u1fe91l ] is a strictly increasing sequence in l.\nAs noted earlier, [9] uses the notation {\u03a5l }, instead of {\u1fe90l }, for event\npoints. We have made this change of notation since {\u03a5l } here plays an\nidentical role as that of event-points in the proof of [9]. In particular, it is\neasily seen that Assumption 5.2 of [9] holds with this new definition of {\u03a5l }.\nWe will next verify Assumption 5.3 (a nonanticipativity condition) of [9] in\nLemma A.2 below.\n.\nFor i \u2208 I and l \u2208 N0 , let uli = \u03bei (Ei (\u03a5l ) + 1) \u2212 \u03a5l . Thus, uli is the residual\n(exogenous) arrival time at the ith buffer at time \u03a5l , unless an arrival of the\nith class occurred at time \u03a5l , in which case it equals uli = ui (Ei (\u03a5l ) + 1).\n.\nSimilarly, for j \u2208 J, l \u2208 N0 , define vjl = \u03b7j (Sj (\u03a5l ) + 1) \u2212 \u03a5l . Next, write\nRt\nT (t) = 0 \u1e6a (s) ds, where \u1e6a is right continuous. For i \u2208 I, set Qi,0 = 0, and\n.\n.\nfor l \u2265 1, Qi,l = Qi (\u03a5l ). Also, for j \u2208 J, and l \u2265 0, let \u1e6ajl = \u1e6aj (\u03a5l ). Let\n.\n\u1e6aj\u22121 = 0. Finally, define for l \u2265 0,\n\u2032\n\u2032\n\u2032\n\u2032\n.\n(A.10) \u03c7l = {(\u03a5l\u2032 , uli , vjl , Qli , \u1e6ajl \u22121 : i \u2208 I, j \u2208 J, {Ui : i \u2208 N}) : l\u2032 = 0, . . . , l}.\n\n\f55\n\nCONVERGENCE OF VALUE FUNCTIONS\n\nThe definition of \u03c7l above is similar to that in [9], with the exception of the\nsequence {Ui : i \u2208 N}. This enlargement of the collection \u03c7l is needed due to\nthe randomization step, involving the sequence {Ui }, in the construction of\nthe policy [see (4.10)]. In [9], part (iv) of the admissibility requirement (for\nthe smaller class of policies considered there) was in fact shown with respect\nto a smaller filtration, namely, F\u0304 r ((m, n)). Here, using the above enlargement, we will show that part (iv) holds (for the policy in Section 4.1) with\nF r ((m, n)) = F\u0304 r ((m, n)) \u2228 {Ui }. In Lemma A.2 below, we prove that \u1e6a (\u03a5l )\nis a measurable function of \u03c7l , for all l \u2208 N0 . This shows that T satisfies\nAssumptions 5.1\u20135.3 of [9] with the modified definition of \u03a5l and \u03c7l . Now\npart (iv) of the admissibility requirement [i.e., (A.8)] follows exactly as the\nproof of Theorem 5.4 of [9]. This completes the proof of the proposition.\nLemma A.2.\n\n\u1e6a (\u03a5l ) is a measurable function of \u03c7l , for all l \u2208 N0 .\n\nProof. Let for m \u2208 N0 , Lm = (\u1fe91m+1 \u2212 \u1fe91m ) denote the length of the\nmth break-point interval. Define \u03ba0 = 0 and \u03bal = max{m \u2265 0 : \u1fe91m \u2264 \u03a5l } for\nl \u2208 N0 . Hence, \u03bal denotes the number of break-points that preceded the\nlth change-point, and \u1fe91\u03bal is the \"last\" break-point before the lth changepoint \u03a5l (note that \u03bal \u2264 l) for all l \u2208 N0 . Also, define for l \u2208 N0 , \u2206l =\n(\u1fe91\u03bal + L\u03bal \u2212 \u03a5l ) as the \"residual\" time for the next break-point after \u03a5l . In\nparticular, \u2206l = 0 implies that \u03a5l itself is a break-point. By definition of T\n[see (4.10)], it follows that \u03bal , \u1fe91\u03bal , L\u03bal , and, hence, \u2206l are all measurable\nfunctions of \u03c7l for l \u2208 N0 . Summarizing this, we get\n(A.11)\n\n\u03bal , \u2206l , \u1fe91\u03bal are measurable functions of \u03c7l\n\nfor l \u2208 N0 .\n\nUsing notation from [9], let Ji = {j \u2208 J : \u03c31 (j) = i} be the set of all activities\nthat are associated with the buffer i and, for a \u2208 {0, 1}J , Ji (a) be as defined\nby equation (5.2) of [9]. Then Ji (\u1e6a (t)) denotes all activities in Ji that are\nactive at time t \u2265 0, under T . Clearly, for l \u2208 N0 ,\n(A.12)\n\n\u03a5l = \u03a5l\u22121 + min min{\u2206l\u22121 , uil\u22121 , vjl\u22121 : j \u2208 Ji (\u1e6a l\u22121 )}.\ni\u2208I\n\nFor i \u2208 I, let Iil be the indicator function of the event that at the changepoint \u03a5l an arrival or service completion occurs at buffer i. More precisely,\nfor i \u2208 I and l \u2265 0,\n\uf8f1\n\uf8f4\nif min{uil\u22121 , vjl\u22121 : j \u2208 Ji (\u1e6a l\u22121 )}\n\uf8f2 1,\nIil =\n(A.13)\n= min\nmin{\u2206l\u22121 , uil\u22121\n, vjl\u22121 : j \u2208 Ji\u2032 (\u1e6a l\u22121 )},\n\u2032\n\uf8f4\ni\u2032 \u2208I\n\uf8f3\n0,\notherwise.\nFrom (A.11) and (A.10), it follows that\n(A.14)\n\nboth Iil , \u03a5l are measurable functions of \u03c7l ,\n\nl \u2208 N0 .\n\n\f56\n\nA. BUDHIRAJA AND A. P. GHOSH\n\nUsing (4.10) and the construction below it, along with (A.14), it is easily\nchecked that \u1e6a (1) (\u03a5l ) is a measurable function of \u03c7l . Next, for j \u2208 J,\n(A.15)\n\n\u1e6aj (\u03a5l ) = 1{Q\u03c3\n\n1 (j)\n\n(1)\n(\u03a5l )>0} 1{Q\u03c31 (j) (p(\u03a5l ))>\u0398(\u03a5l )} \u1e6aj (\u03a5l ).\n\nFrom (A.14) and (A.10), \u0398(\u03a5l ) and Q\u03c31 (j) (\u03a5l ) are \u03c7l measurable, thus, so is\n1\nthe first indicator in the above display.\nS Also, since p(\u03a5l ) is either \u03a5l or \u1fe9\u03bal -\ndepending on whether \u03a5l is in n\u2264p0 I2 (n) or not-and both Q\u03c31 (j) (\u03a5l )\nand Q\u03c31 (j) (\u1fe91\u03bal ) are \u03c7l measurable, we see that the second indicator in (A.15)\nis \u03c7l measurable as well. The lemma follows on combining the above observations. \u0003\nA.3. Proof of Lemma 4.2. Since hn is equicontinuous, pre-compactness\nof (f n , gn , hn , \u03b3 n ) is immediate. Suppose now that (f n , gn , hn , \u03b3 n ) converges\n(in D 3I+1 ), along some subsequence, to (f, g, h, \u03b3). Then \u03b3(t) = t for t \u2265 0\nand f, g, h \u2208 C I . Also, for suitable measurable maps h\u0303i : [0, \u221e) \u2192 [0, 1], i \u2208 I,\nZ\nh\u0303i (s) ds,\ni \u2208 I, t \u2265 0.\nhi (t) =\n[0,t]\n\nIf \u03c8 : [0, \u221e) \u2192 R is a continuous map with compact support, then, along the\nabove subsequence,\nZ\nZ\nn\nn\n\u03c8(fi (s))h\u0303i (s) ds,\nt \u2265 0, i \u2208 I.\n\u03c8(fi (s))1{fi (\u03b3 n (s))\u2264\u03b5n } ds \u2192\n[0,t]\n\n[0,t]\n\nSuppose now that supp(\u03c8) \u2282 (\u03b4, \u221e) for some \u03b4 > 0. RThen the left-hand side\nof the above display converges to 0 and so for all t, i, [0,t] \u03c8(fi (s))h\u0303i (s) ds = 0\nfor such \u03c8. Since \u03b4 > 0 is arbitrary, we get\nZ\n1{fi (s)=0} h\u0303i (s) ds = hi (t).\n[0,t]\n\nThe result follows.\nAcknowledgment. We thank an anonymous referee for pointing us to the\npaper [21].\nREFERENCES\n[1] Ata, B. and Kumar, S. (2005). Heavy traffic analysis of open processing networks\nwith complete resource pooling: Asymptotic optimality of discrete review policies. Ann. Appl. Probab. 15 331\u2013391. MR2115046\n[2] Atar, R. and Budhiraja, A. (2006). Singular control with state constraints on\nunbounded domain. Ann. Probab. 34 1864\u20131909. MR2271486\n[3] Bell, S. L. and Williams, R. J. (2001). Dynamic scheduling of a system with two\nparallel servers in heavy traffic with resource pooling: Asymptotic optimality of\na threshold policy. Ann. Appl. Probab. 11 608\u2013649. MR1865018\n\n\fCONVERGENCE OF VALUE FUNCTIONS\n\n57\n\n[4] Bell, S. L. and Williams, R. J. (2005). Dynamic scheduling of a parallel server\nsystem in heavy traffic with complete resource pooling: Asymptotic optimality\nof a threshold policy. Electron. J. Probab. 10 1044\u20131115. MR2164040\n[5] B\u00f6hm, V. (1975). On the continuity of the optimal policy set for linear programs.\nSIAM J. Appl. Math. 28 303\u2013306. MR0371390\n[6] Bramson, M. and Williams, R. J. (2000). On dynamic scheduling of stochastic\nnetworks in heavy traffic and some new results for the workload process. In\nProceedings of the 39th IEEE Conference on Decision and Control 516\u2013521.\nIEEE, Piscataway, NJ.\n[7] Bramson, M. and Williams, R. J. (2003). Two workload properties for Brownian\nnetworks. Queueing Syst. 45 191\u2013221. MR2024178\n[8] Budhiraja, A. and Ghosh, A. P. (2005). A large deviations approach to asymptotically optimal control of crisscross network in heavy traffic. Ann. Appl. Probab.\n15 1887\u20131935. MR2152248\n[9] Budhiraja, A. and Ghosh, A. P. (2006). Diffusion approximations for controlled\nstochastic networks: An asymptotic bound for the value function. Ann. Appl.\nProbab. 16 1962\u20132006. MR2288710\n[10] Budhiraja, A. and Ross, K. (2006). Existence of optimal controls for singular\ncontrol problems with state constraints. Ann. Appl. Probab. 16 2235\u20132255.\nMR2288720\n[11] Chen, H. and Yao, D. D. (2001). Fundamentals of Queueing Networks: Performance, Asymptotics, and Optimization. Applications of Mathematics (New\nYork) 46. Springer, New York. MR1835969\n[12] Dai, J. G. and Lin, W. (2008). Asymptotic optimality of maximum pressure policies\nin stochastic processing networks. Ann. Appl. Probab. 18 2239\u20132299. MR2473656\n[13] Dupuis, P. and Ishii, H. (1991). On Lipschitz continuity of the solution mapping\nto the Skorokhod problem, with applications. Stochastics Stochastics Rep. 35\n31\u201362. MR1110990\n[14] Dupuis, P. and Ramanan, K. (1999). Convex duality and the Skorokhod problem.\nI, II. Probab. Theory Related Fields 115 153\u2013195, 197\u2013236. MR1720348\n[15] Ethier, S. N. and Kurtz, T. G. (1986). Markov Processes: Characterization and\nConvergence. Wiley, New York. MR0838085\n[16] Harrison, J. M. (1988). Brownian models of queueing networks with heterogeneous customer populations. In Stochastic Differential Systems, Stochastic Control Theory and Applications (Minneapolis, Minn., 1986). IMA Vol. Math. Appl.\n10 147\u2013186. Springer, New York. MR0934722\n[17] Harrison, J. M. (1996). The bigstep approach to flow management in stochastic processing networks. In Stochastic Networks: Theory and Applications (F.\nP. Kelly, S. Zachary and I. Ziedins, eds.) 57\u201390. Oxford Univ. Press, Oxford.\n[18] Harrison, J. M. (2000). Brownian models of open processing networks: Canonical\nrepresentation of workload. Ann. Appl. Probab. 10 75\u2013103. MR1765204\n[19] Harrison, J. M. and Reiman, M. I. (1981). Reflected Brownian motion on an\northant. Ann. Probab. 9 302\u2013308. MR0606992\n[20] Harrison, J. M. and Van Mieghem, J. A. (1997). Dynamic control of Brownian\nnetworks: State space collapse and equivalent workload formulations. Ann. Appl.\nProbab. 7 747\u2013771. MR1459269\n[21] Krichagina, E. V. and Taksar, M. I. (1992). Diffusion approximation for GI/G/1\ncontrolled queues. Queueing Syst. 12 333\u2013367. MR1200872\n\n\f58\n\nA. BUDHIRAJA AND A. P. GHOSH\n\n[22] Kumar, S. (1999). Scheduling open queueing networks with sufficiently flexible resources. In Proceedings of the 37th Allerton Conference. Univ. Illinois.\n[23] Kushner, H. J. (2001). Heavy Traffic Analysis of Controlled Queueing and Communication Networks. Applications of Mathematics (New York) 47. Springer, New\nYork. MR1834938\n[24] Kushner, H. J. and Dupuis, P. G. (1992). Numerical Methods for Stochastic Control\nProblems in Continuous Time. Applications of Mathematics (New York) 24.\nSpringer, New York. MR1217486\n[25] Kushner, H. J. and Martins, L. F. (1996). Heavy traffic analysis of a controlled\nmulticlass queueing network via weak convergence methods. SIAM J. Control\nOptim. 34 1781\u20131797. MR1404856\n[26] Meyn, S. P. (2003). Sequencing and routing in multiclass queueing networks.\nII. Workload relaxations. SIAM J. Control Optim. 42 178\u2013217 (electronic).\nMR1982741\n[27] Meyn, S. P. (2007). Control Techniques for Complex Networks. Cambridge Univ.\nPress, Cambridge.\n[28] Ward, A. R. and Kumar, S. (2008). Asymptotically optimal admission control of\na queue with impatient customers. Math. Oper. Res. 33 167\u2013202. MR2393546\n[29] Williams, R. J. (1998). An invariance principle for semimartingale reflecting Brownian motions in an orthant. Queueing Syst. 30 5\u201325. MR1663755\nDepartment of Statistics\nand Operations Research\nUniversity of North Carolina\nChapel Hill, North Carolina 27599-3260\nUSA\nE-mail: budhiraj@email.unc.edu\n\nDepartment of Statistics\n3216 Snedecor Hall\nIowa State University\nAmes, Iowa 50011-1210\nUSA\nE-mail: apghosh@iastate.edu\n\n\f"}