{"id": "http://arxiv.org/abs/physics/0111020v1", "guidislink": true, "updated": "2001-11-06T13:20:24Z", "updated_parsed": [2001, 11, 6, 13, 20, 24, 1, 310, 0], "published": "2001-11-06T13:20:24Z", "published_parsed": [2001, 11, 6, 13, 20, 24, 1, 310, 0], "title": "Model selection for inverse problems: Best choice of basis functions and\n  model order selection", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0603210%2Cphysics%2F0603181%2Cphysics%2F0603117%2Cphysics%2F0603150%2Cphysics%2F0603206%2Cphysics%2F0603074%2Cphysics%2F0603078%2Cphysics%2F0603143%2Cphysics%2F0603153%2Cphysics%2F0603100%2Cphysics%2F0603032%2Cphysics%2F0603177%2Cphysics%2F0603263%2Cphysics%2F0603187%2Cphysics%2F0603120%2Cphysics%2F0603076%2Cphysics%2F0603063%2Cphysics%2F0603112%2Cphysics%2F0603106%2Cphysics%2F0603190%2Cphysics%2F0603202%2Cphysics%2F0603069%2Cphysics%2F0603235%2Cphysics%2F0603020%2Cphysics%2F0603089%2Cphysics%2F0603037%2Cphysics%2F0603073%2Cphysics%2F0603064%2Cphysics%2F0603058%2Cphysics%2F0603170%2Cphysics%2F0603070%2Cphysics%2F0603273%2Cphysics%2F0603108%2Cphysics%2F0603133%2Cphysics%2F0603033%2Cphysics%2F0603199%2Cphysics%2F0603255%2Cphysics%2F0603030%2Cphysics%2F0603264%2Cphysics%2F0603179%2Cphysics%2F0603252%2Cphysics%2F0603123%2Cphysics%2F0603015%2Cphysics%2F0603071%2Cphysics%2F0603223%2Cphysics%2F0603079%2Cphysics%2F0111123%2Cphysics%2F0111016%2Cphysics%2F0111163%2Cphysics%2F0111155%2Cphysics%2F0111030%2Cphysics%2F0111213%2Cphysics%2F0111204%2Cphysics%2F0111170%2Cphysics%2F0111195%2Cphysics%2F0111029%2Cphysics%2F0111143%2Cphysics%2F0111130%2Cphysics%2F0111147%2Cphysics%2F0111162%2Cphysics%2F0111205%2Cphysics%2F0111128%2Cphysics%2F0111165%2Cphysics%2F0111091%2Cphysics%2F0111013%2Cphysics%2F0111028%2Cphysics%2F0111157%2Cphysics%2F0111158%2Cphysics%2F0111137%2Cphysics%2F0111190%2Cphysics%2F0111166%2Cphysics%2F0111181%2Cphysics%2F0111102%2Cphysics%2F0111129%2Cphysics%2F0111173%2Cphysics%2F0111020%2Cphysics%2F0111144%2Cphysics%2F0111167%2Cphysics%2F0111087%2Cphysics%2F0111192%2Cphysics%2F0111040%2Cphysics%2F0111097%2Cphysics%2F0111059%2Cphysics%2F0111001%2Cphysics%2F0111164%2Cphysics%2F0111009%2Cphysics%2F0111039%2Cphysics%2F0111109%2Cphysics%2F0111196%2Cphysics%2F0111050%2Cphysics%2F0111027%2Cphysics%2F0111107%2Cphysics%2F0111042%2Cphysics%2F0111015%2Cphysics%2F0111108%2Cphysics%2F0111018%2Cphysics%2F0111212%2Cphysics%2F0111048%2Cphysics%2F0111082%2Cphysics%2F0111104%2Cphysics%2F0111136&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Model selection for inverse problems: Best choice of basis functions and\n  model order selection"}, "summary": "A complete solution for an inverse problem needs five main steps: choice of\nbasis functions for discretization, determination of the order of the model,\nestimation of the hyperparameters, estimation of the solution, and finally,\ncharacterization of the proposed solution. Many works have been done for the\nthree last steps. The first two have been neglected for a while, in part due to\nthe complexity of the problem. However, in many inverse problems, particularly\nwhen the number of data is very low, a good choice of the basis functions and a\ngood selection of the order become primary. In this paper, we first propose a\ncomplete solution within a Bayesian framework. Then, we apply the proposed\nmethod to an inverse elastic electron scattering problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0603210%2Cphysics%2F0603181%2Cphysics%2F0603117%2Cphysics%2F0603150%2Cphysics%2F0603206%2Cphysics%2F0603074%2Cphysics%2F0603078%2Cphysics%2F0603143%2Cphysics%2F0603153%2Cphysics%2F0603100%2Cphysics%2F0603032%2Cphysics%2F0603177%2Cphysics%2F0603263%2Cphysics%2F0603187%2Cphysics%2F0603120%2Cphysics%2F0603076%2Cphysics%2F0603063%2Cphysics%2F0603112%2Cphysics%2F0603106%2Cphysics%2F0603190%2Cphysics%2F0603202%2Cphysics%2F0603069%2Cphysics%2F0603235%2Cphysics%2F0603020%2Cphysics%2F0603089%2Cphysics%2F0603037%2Cphysics%2F0603073%2Cphysics%2F0603064%2Cphysics%2F0603058%2Cphysics%2F0603170%2Cphysics%2F0603070%2Cphysics%2F0603273%2Cphysics%2F0603108%2Cphysics%2F0603133%2Cphysics%2F0603033%2Cphysics%2F0603199%2Cphysics%2F0603255%2Cphysics%2F0603030%2Cphysics%2F0603264%2Cphysics%2F0603179%2Cphysics%2F0603252%2Cphysics%2F0603123%2Cphysics%2F0603015%2Cphysics%2F0603071%2Cphysics%2F0603223%2Cphysics%2F0603079%2Cphysics%2F0111123%2Cphysics%2F0111016%2Cphysics%2F0111163%2Cphysics%2F0111155%2Cphysics%2F0111030%2Cphysics%2F0111213%2Cphysics%2F0111204%2Cphysics%2F0111170%2Cphysics%2F0111195%2Cphysics%2F0111029%2Cphysics%2F0111143%2Cphysics%2F0111130%2Cphysics%2F0111147%2Cphysics%2F0111162%2Cphysics%2F0111205%2Cphysics%2F0111128%2Cphysics%2F0111165%2Cphysics%2F0111091%2Cphysics%2F0111013%2Cphysics%2F0111028%2Cphysics%2F0111157%2Cphysics%2F0111158%2Cphysics%2F0111137%2Cphysics%2F0111190%2Cphysics%2F0111166%2Cphysics%2F0111181%2Cphysics%2F0111102%2Cphysics%2F0111129%2Cphysics%2F0111173%2Cphysics%2F0111020%2Cphysics%2F0111144%2Cphysics%2F0111167%2Cphysics%2F0111087%2Cphysics%2F0111192%2Cphysics%2F0111040%2Cphysics%2F0111097%2Cphysics%2F0111059%2Cphysics%2F0111001%2Cphysics%2F0111164%2Cphysics%2F0111009%2Cphysics%2F0111039%2Cphysics%2F0111109%2Cphysics%2F0111196%2Cphysics%2F0111050%2Cphysics%2F0111027%2Cphysics%2F0111107%2Cphysics%2F0111042%2Cphysics%2F0111015%2Cphysics%2F0111108%2Cphysics%2F0111018%2Cphysics%2F0111212%2Cphysics%2F0111048%2Cphysics%2F0111082%2Cphysics%2F0111104%2Cphysics%2F0111136&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A complete solution for an inverse problem needs five main steps: choice of\nbasis functions for discretization, determination of the order of the model,\nestimation of the hyperparameters, estimation of the solution, and finally,\ncharacterization of the proposed solution. Many works have been done for the\nthree last steps. The first two have been neglected for a while, in part due to\nthe complexity of the problem. However, in many inverse problems, particularly\nwhen the number of data is very low, a good choice of the basis functions and a\ngood selection of the order become primary. In this paper, we first propose a\ncomplete solution within a Bayesian framework. Then, we apply the proposed\nmethod to an inverse elastic electron scattering problem."}, "authors": ["A. Mohammad-Djafari"], "author_detail": {"name": "A. Mohammad-Djafari"}, "author": "A. Mohammad-Djafari", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1063/1.1381850", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/physics/0111020v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/physics/0111020v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Presented at MaxEnt00. Appeared in Bayesian Inference and Maximum\n  Entropy Methods, Ali Mohammad-Djafari(Ed.), AIP Proceedings\n  (http://proceedings.aip.org/proceedings/confproceed/567.jsp)", "arxiv_primary_category": {"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/physics/0111020v1", "affiliation": "Laboratoire des Signaux et Syst\u00e8mes CNRS-UPS-SUPELEC, Gif-sur-Yvette, France", "arxiv_url": "http://arxiv.org/abs/physics/0111020v1", "journal_reference": null, "doi": "10.1063/1.1381850", "fulltext": "arXiv:physics/0111020v1 [physics.data-an] 6 Nov 2001\n\nModel selection for inverse problems: Best choice\nof basis functions and model order selection\nA. Mohammad-Djafari\nLaboratoire des Signaux et Syst\u00e8mes\nSup\u00e9lec, Plateau de Moulon, 91192 Gif\u2013sur\u2013Yvette Cedex, France\nAbstract. A complete solution for an inverse problem needs five main steps: choice of basis functions for discretization, determination of the order of the model, estimation of the hyperparameters,\nestimation of the solution, and finally, characterization of the proposed solution. Many works have\nbeen done for the three last steps. The first two have been neglected for a while, in part due to the\ncomplexity of the problem. However, in many inverse problems, particularly when the number of\ndata is very low, a good choice of the basis functions and a good selection of the order become\nprimary. In this paper, we first propose a complete solution within a Bayesian framework. Then, we\napply the proposed method to an inverse elastic electron scattering problem.\n\nINTRODUCTION\nIn a very general linear inverse problem, the relation between the data y = [y1 , * * * , ym ]t\nand the unknown function f (.) is\nyi =\n\nZZ\n\nhi (r) f (r) dr,\n\ni = 1, * * * , m,\n\n(1)\n\nwhere hi (r) is the system response for the data yi . We assume here that the hi (r) are\nknown perfectly. The first step for any numerical processing is the choice of a basis\nfunction bj (r) and an order k, in such a way to be able to write\nf (r) =\n\nk\nX\n\nxj bj (r).\n\n(2)\n\nj=1\n\nThis leads to\ny = Ax + \u01eb\n\n(3)\n\nwith y = [y1 , * * * , ym ]t , x = [x1 , * * * , xk ]t and\nAi,j =\n\nZZ\n\nhi (r) bj (r) dr,\n\ni = 1, * * * , m, j = 1, * * * , k,\n\n(4)\n\nwhere \u01eb = [\u01eb1 , * * * , \u01ebm ]t represents the errors (both the measurement noise and the modeling and the approximation related to the numerical computation of matrix elements\nAi,j ). Even when the choice of the basis functions bi (r) and the model order k is fixed,\nobtaining a good estimate for x needs other assumptions about the noise \u01eb and about\n\n\fx itself. The Bayesian approach provides a coherent and complete framework to handle\nthe random nature of \u01eb and the a priori incomplete knowledge of x.\nThe first step in a Bayesian approach is to assign the prior probability laws\np(y | x, \u03c6, k, l) = p\u01eb (y \u2212 Ax | \u03c6, k, l), p(x | \u03c8, k, l), p(\u03c6 | k, l) and p(\u03c8 | k, l), where\np\u01eb (y \u2212 Ax|\u03c6, k, l) is the probability law of the noise, and (\u03c6, \u03c8) the hyperparameters of\nthe problem. Note that x represents the unknown parameters, k = dim(x) is the order of\nthe model, m = dim(y) is the number of the data and l is an index to a particular choice\nof basis functions. Note that the elements of the matrix A depend on the choice of the\nbasis functions. However, to simplify the notations, we do not write this dependence\nexplicitly. We assume that we have to select one set l of basis functions among a finite\nset (indexed by [1 : lmax ]) of them. Thus, for a given l \u2208 [1, lmax ] and a given model\norder k \u2208 [1, kmax ], and using the mentioned prior laws, we define the joint probability\nlaw\np(y, x, \u03c6, \u03c8 | k, l) = p(y | x, \u03c6, k, l) p(x | \u03c8, k, l) p(\u03c6 | k, l) p(\u03c8 | k, l).\n(5)\nFrom this probability law, we obtain, either by integration or by summation, any\nmarginal law, and any a posteriori probability law using the Bayes rule.\nWhat we propose in this paper is to consider the following problems:\n\u2022\n\nParameter estimation:\no\n\nn\n\nb \u03c8,\nb k,\nb l)\nb ,\nb = arg max p(x | y, \u03c6,\nx\nx\n\n(6)\n\np(x | y, \u03c6, \u03c8, k, l) = p(y, x | \u03c6, \u03c8, k, l) / p(y | \u03c6, \u03c8, k, l),\n\n(7)\n\np(y, x | \u03c6, \u03c8, k, l) = p(y | x, \u03c6, k, l) p(x | \u03c8, k, l)\n\n(8)\n\nwhere\n\nand\np(y | \u03c6, \u03c8, k, l) =\n\u2022\n\nHyperparameter estimation:\n\nZZ\n\np(y, x | \u03c6, \u03c8, k, l) dx.\nn\n\no\n\n(9)\n\nb \u03c8)\nb = arg max p(\u03c6, \u03c8 | y, k,\nb l)\nb ,\n(\u03c6,\n(\u03c6,\u03c8 )\n\n(10)\n\np(\u03c6, \u03c8 | y, k, l) = p(y, \u03c6, \u03c8 | k, l) / p(y | k, l)\n\n(11)\n\nwhere\nand\np(y | k, l) =\n\u2022\n\nModel order selection:\n\nZZ ZZ\n\np(y, \u03c6, \u03c8 | k, l) d\u03c6 d\u03c8.\no\n\nb ,\nkb = arg max p(k | y, l)\n\n(13)\n\np(k | y, l) = p(y | k, l) p(k)/ p(y | l)\n\n(14)\n\nk\n\nwhere\n\nn\n\n(12)\n\nand\np(y | l) =\n\nkX\nmax\nk=1\n\np(y | k, l) p(k).\n\n(15)\n\n\f\u2022\n\nBasis function selection:\n\nlb = arg max {p(l | y)} ,\n\n(16)\n\np(l | y) = p(y | l) p(l)/ p(y)\n\n(17)\n\nl\n\nwhere\nand\n\np(y) =\n\nlX\nmax\n\np(y | l) p(l).\n\n(18)\n\nl=1\n\n\u2022\n\nJoint parameter, hyperparameter, model order and basis function estimation:\nb \u03c8,\nb k,\nb l)\nb = arg max {p(y, x, \u03c6, \u03c8 | k, l) p(k) p(l)} .\nb \u03c6,\n(x,\n(x,\u03c6,\u03c8 ,k,l)\n\n(19)\n\nAs it can be easily seen, the first problem is, in general, a well posed problem and the\nsolution can be computed, either analytically or numerically. The others (except the last)\nneed integrations. These integrals can be done analytically only in the case of Gaussian\nlaws. In other cases, one can either use a numerical integration (either deterministic or\nstochastic) or to resort to approximations such as the Laplace method which allows us\nto obtain a closed-form expression for the optimality criterion.\nHere, we consider these problems for the particular case of Gaussian prior laws:\n!\n\n\u0014\n\n1\n1\np(y | x, \u03c6, k, l) = N Ax, I = (2\u03c0/\u03c6)\u2212m/2 exp \u2212 \u03c6 ky \u2212 Axk2\n\u03c6\n2\n!\n\u0015\n\u0014\n1\n1\n\u2212k/2\n2\nexp \u2212 \u03c8 kxk ,\np(x | \u03c8, k, l) = N 0, I = (2\u03c0/\u03c8)\n\u03c8\n2\nwhere\n\n1\n\u03c6\n\nand\n\n1\n\u03c8\n\n\u0015\n\n(20)\n(21)\n\nare respectively the variance of the noise and the parameters.\n\nPARAMETER ESTIMATION\nFirst note that in this special case we have\n\u2212m/2\n\np(y, x | \u03c6, \u03c8, k, l) = (2\u03c0/\u03c6)\n\n\u2212k/2\n\n(2\u03c0/\u03c8)\n\n\u0014\n\n\u0015\n\n1\n1\nexp \u2212 \u03c6 ky \u2212 Axk2 \u2212 \u03c8 kxk2 . (22)\n2\n2\n\nIntegration with respect to x can be done analytically and we have:\np(y | \u03c6, \u03c8, k, l) =\n\nZZ\n\np(y, x | \u03c6, \u03c8, k, l) dx = N (0, P y ) ,\n\n(23)\n\nwith\n\n1\n1\n1\n\u03c8\nAAt + I = (AAt + \u03bbI) and \u03bb = .\n\u03c8\n\u03c6\n\u03c8\n\u03c6\nIt is then easy to see that the a posteriori law of x is also Gaussian:\nPy =\n\n\u0011\n\u0010\nc with P\nc = 1 (At A + \u03bbI)\u22121 and x\nc At y.\nb P\nb = \u03c6P\np(x | y, \u03c6, \u03c8, k, l) = N x,\n\u03c6\n\n(24)\n\n(25)\n\n\fThus the parameter estimation in this case is straightforward:\n\nwith\n\nb = arg max {p(x | y, \u03c6, \u03c8, k, l)} = arg min {J1 (x)} ,\nx\nx\nx\n\n(26)\n\nJ1 (x) = ky \u2212 Axk2 + \u03bbkxk2 ,\n\n(27)\n\nwhich is a quadratic function of x. The solution is then a linear function of the data y\nand is given by\nb = K(\u03bb) y\nx\n\nwith K(\u03bb) = (At A + \u03bbI)\u22121 At .\n\n(28)\n\nHYPERPARAMETER ESTIMATION\nFor the hyperparameter estimation problem we note that:\np(\u03c6 | k, l) p(\u03c8 | k, l)\np(y | \u03c6, \u03c8, k, l)\np(y | k, l)\n\u0014\n\u0015\n1\np(\u03c6 | k, l) p(\u03c8 | k, l)\n(2\u03c0)\u2212m/2 |P y |\u22121/2 exp \u2212 y t P \u22121\ny\n. (29)\n=\ny\np(y | k, l)\n2\n\np(\u03c6, \u03c8 | y, k, l) =\n\nThus, the hyperparameter estimation problem becomes:\n(\u03c6,\u03c8)\n\nwhere\n\nn\n\no\n\nb \u03c8)\nb = arg max p(\u03c6, \u03c8 | y, k,\nb l)\nb = arg min {J (\u03c6, \u03c8)}\n(\u03c6,\n2\n\n(30)\n\n(\u03c6,\u03c8)\n\n1\n1\nln |P y | + y t P \u22121\n(31)\ny y.\n2\n2\nUnfortunately, in general, there is not an analytical expression for the solution, but this\noptimization can be done numerically. Many works have been investigated to perform\nthis optimization appropriately for particular choices of p(\u03c6 | k, l) and p(\u03c8 | k, l). Among\nthe others, we may note the choice of improper prior laws such as Jeffreys' prior\n1\np(\u03c6 | k, l) \u221d \u03c61 and p(\u03c8 | k, l) \u221d \u03c81 or proper uniform prior laws p(\u03c6 | k, l) = \u03c6max \u2212\u03c6\nmin\n1\nor\nstill\nthe\nproper\nGamma\nprior\nlaws.\nand p(\u03c8 | k, l) = \u03c8max \u2212\u03c8\nmin\nOne main issue with improper prior laws is the existence of the solution, because\np(\u03c6, \u03c8 | y, k, l) may not even have a maximum or its maximum can be located at the\nborder of the domain of variation of (\u03c6, \u03c8). Here, we propose to use the following proper\nGamma priors :\nJ2 (\u03c6, \u03c8) = \u2212 ln p(\u03c6 | k, l) \u2212 ln p(\u03c8 | k, l) +\n\np(\u03c6) = G(\u03b11 , \u03b21 ) \u221d \u03c6(\u03b11 \u22121) exp [\u2212\u03b21 \u03c6] \u2212\u2192 E {\u03c6} = \u03b11 /\u03b21\np(\u03c8) = G(\u03b12 , \u03b2) \u221d \u03c8 (\u03b12 \u22121) exp [\u2212\u03b22 \u03c8] \u2212\u2192 E {\u03c8} = \u03b12 /\u03b22 .\n\n(32)\n(33)\n\nWith these priors, we have\nJ2 (\u03c6, \u03c8) = (1 \u2212 \u03b11 ) ln \u03c6 + (1 \u2212 \u03b12 ) ln \u03c8 + \u03b21 \u03c6 + \u03b22 \u03c8 +\n\n1\n1\nln |P y | + y t P \u22121\ny y.\n2\n2\n\n(34)\n\n\fThe second main issue is the numerical optimization. Many works have been done\non this subject. Among the others we can mention those who try to integrate out one\nof the two parameters directly or after some transformation. For example transforming\n(\u03c6, \u03c8) \u2212\u2192 (\u03c6, \u03bb) and using the identities\n\nand\n\nwe have\nand\n\nAAt + \u03bbI = \u03bbm\u2212k At A + \u03bbI\n\n(35)\n\n1\n(AAt + \u03bbI)\u22121 = (I \u2212 AK(\u03bb)),\n\u03bb\n\n(36)\n\nln |P y | = \u2212m ln \u03c6 \u2212 k ln \u03bb + ln At A + \u03bbI\n\n(37)\n\nt\nt\nt\nb\nb\ny t P \u22121\ny y = \u03c6 y (I \u2212 AK(\u03bb))y = \u03c6 y (y \u2212 Ax) = \u03c6 y (y \u2212 y ).\n\n(38)\n\nThen, we obtain\n\nJ2 (\u03c6, \u03c8) = (1 \u2212 \u03b11 \u2212 m\u2212k\n) ln \u03c6 + (1 \u2212 \u03b12 \u2212 k2 ) ln \u03c8 + \u03b21 \u03c6 + \u03b22\u03c8\n2\nb ).\n+ 21 ln At A + \u03c8\u03c6 I + \u03c62 y t (y \u2212 y\n\nor\n\nJ2 (\u03c6, \u03bb) = (2 \u2212 \u03b11 \u2212 \u03b12 \u2212 m2 ) ln \u03c6 + (1 \u2212 \u03b12 \u2212 k2 ) ln \u03bb + \u03b21 \u03c6 + \u03b22 \u03c6\u03bb\nb ).\n+ 12 ln At A + \u03bbI + \u03c62 y t (y \u2212 y\n\n(39)\n\n(40)\n\nFor fixed \u03bb, equating to zero the derivative of this expression with respect to \u03c6 gives an\nexplicit solution which is\n\u0014\n\n\u0015\n\n\u2202J2 (\u03c6, \u03bb)\nm\n1\nb .\n= 0 \u2212\u2192 \u03c6 = ( + \u03b11 + \u03b12 \u2212 2) / \u03b21 + \u03bb\u03b22 + y t (y \u2212 y)\n\u2202\u03c6\n2\n2\n\n(41)\n\nPutting this expression into J2 we obtain a criterion depending only on \u03bb which can be\noptimized numerically. In addition, it is possible to integrate out \u03c6 to obtain p(\u03bb|y, k, l),\nbut the expression is too complex to write.\n\nJOINT ESTIMATION\nOne may try to estimate all the unknowns simultaneously by\nb \u03c8,\nb k,\nb l)\nb = arg max {p(x, \u03c6, \u03c8, k, l|y} = arg min {J (x, \u03c6, \u03c8, k, l)} ,\nb \u03c6,\n(x,\n3\n(x,\u03c6,\u03c8,k,l)\n(x,\u03c6,\u03c8,k,l)\n\n(42)\n\nwhere\n\nJ3 (x, \u03c6, \u03c8, k, l) = \u2212 ln p(k) \u2212 ln p(l) \u2212 ( m2\u0010+ \u03b11 \u2212 1) ln \u03c6\n\u0011\n\u0010\n\u0011\n\u2212( k2 + \u03b12 \u2212 1) ln \u03c8 + \u03c6 \u03b21 + 21 ky \u2212 Axk2 + \u03c8 \u03b22 + 12 kxk2 .\n(43)\n\n\fThe main advantage of this criterion is that we obtain explicit solutions for x, \u03c6 and \u03c8\nby equating to zero the derivatives of J3 (x, \u03c6, \u03c8, k, l) with respect to them:\n\uf8f1\nb = (At A + \u03bbI)\u22121 At y,\nx\nwith \u0011 \u03bb = \u03c6/\u03c8;\n\uf8f4\n\uf8f4\n\u0010\n\uf8f2\nm\n1\nb\nb 2 ;\n\u03c6 = ( 2 + \u03b11 \u2212 1)/ \u03b21 + 2 ky \u2212 Axk\n\u0011\n\u0010\n\uf8f4\n\uf8f4\n\uf8f3 \u03c8\nb = ( k + \u03b1 \u2212 1)/ \u03b2 + 1 kxk\nb 2 .\n2\n2\n2\n\n(44)\n\n2\n\nWe cannot obtain closed form expressions for kb and lb which depend on the particular\nchoice for p(k) and p(l). These relations suggest an iterative algorithm such as:\nJoint MAP estimation algorithm 1\nfor l = 1 : lmax\nfor k = 1 : kmax\ncompute the elements of the matrix A;\nb=\u03bb ;\ninitialize \u03bb\n0\nrepeat until convergency:\nt\nb \u22121 t y;\nb\n\uf8f1x = (A A + \u03bbI) A\n\u0011\n\u0010\nb = ( m + \u03b1 \u2212 1)/ \u03b2 + 1 ky \u2212 Axk\n\uf8f2 \u03c6\nb 2 ;\n1\n1\n2\n2\n\u0011\n\u0010\nb = ( k + \u03b1 \u2212 1)/ \u03b2 + 1 kxk\n\uf8f3 \u03c8\nb 2\n2\n\n2\n\n2\n\n2\n\nb = \u03c6/\nb \u03c8\nb\n\u2212\u2192 \u03bb\n\nend\nb \u03c8,\nb k, l);\nb \u03c6,\ncompute J(k, l) = J3 (x,\nend\nend\nchoose the best model and the best order by\nb k)\nb = arg min\n(l,\n(k,l) {J(k, l)}\n\nNote however that, for fixed x, \u03c6 and \u03c8, the criteria J3 in (43) or J5 in (47) are mainly\nlinear functions of k if we choose a uniform law for p(k). This means that we may not\nhave a minimum for these criteria as a function of k. The choice of the prior p(k) is then\nimportant. One possible choice is the following:\np(k) =\n\n(\n\n2(kmax \u2212k)\nkmax (kmax \u22121)\n\n0\n\n1 \u2264 k < kmax\nk > kmax\n\n(45)\n\nwhich is a decreasing function of k in the range k \u2208 [1, kmax ] and zero elsewhere. This\nchoice may insure the existence of a minimum if kmax is chosen appropriately. For p(l)\nwe propose to choose a uniform law, because we do not want to give any favor to any\nmodel.\nb into J3 to obtain\nAnother algorithm can be obtained if we replace the expression of x\na criterion depending only on (\u03c6, \u03c8):\nk\nJ4 (\u03c6, \u03c8, k, l) = \u2212 ln\u0010p(k) \u2212 ln p(l) \u2212 ( m2 \u0011+ \u03b11 \u2212\n\u0010 1) ln \u03c6 \u2212 ( 2 + \u03b1\n\u00112 \u2212 1) ln \u03c8\n1\n1\n2\n2\nb\nb\n+ \u03c8 \u03b22 + 2 kx(\u03bb)k\n+\u03c6 \u03b21 + 2 ky \u2212 y(\u03bb)k\n\n(46)\n\n\for on (\u03c6, \u03bb):\n+ \u03b11 +\u0010\u03b12 \u2212 2) ln \u03c6 \u2212 ( \u0011k2 + \u03b12 \u2212 1) ln \u03bb\nJ5 (\u03c6, \u03bb, k, l) = \u2212 ln\u0010p(k) \u2212 ln p(l) \u2212 ( m+k\n2\u0011\n2\n2\nb\nb\n+\u03c6 \u03b21 + 12 ky \u2212 y(\u03bb)k\n+ (\u03bb\u03c6) \u03b22 + 21 kx(\u03bb)k\n(47)\nand then optimize it with respect to them. In the second case, we can again obtain first\n\u03c6 and put its expression\n!\n\n\u0014\n\nm+k\n1\n1\n2\n2\nb\nb\n\u03c6b =\n+ \u03b11 + \u03b12 \u2212 2 / (\u03b21 + ky \u2212 y(\u03bb)k\n) + \u03bb(\u03b22 + kx(\u03bb)k\n)\n2\n2\n2\n\n\u0015\n\n(48)\n\nin the criterion to obtain another criterion depending only on \u03bb and optimize it numerically. This gives the following algorithm:\nJoint MAP estimation algorithm 2\nfor l = 1 : lmax\nfor k = 1 : kmax\ncompute the elements of the matrix A;\nfor \u03bb \u2208 10[\u22128:1:4]\nb = (At A + \u03bbI)\u22121 At y and y\nb = Ax\nb\ncompute x\nb\ncompute \u03c6 using (eq. 48)\nb \u03bb, k, l) (eq. 47)\ncompute J(\u03bb) = J5 (\u03c6,\nend\nb = arg min {J(\u03bb)}\nchoose \u03bb\n\u03bb\nb \u22121 At y;\nb = (At A + \u03bbI)\ncompute x\ncompute \u03c6b using (eq. 48);\nb \u03bb,\nb k, l) (eq. 47)\ncompute J(k, l) = J5 (\u03c6,\nend\nend\nchoose the best model and the best order by\nb k)\nb = arg min\n(l,\n(k,l) {J(k, l)}\n\nMODEL ORDER SELECTION\nThe model order selection\nkb = arg max {p(k | y, l)} = arg min {J6 (k)} ,\n\n(49)\n\nJ6 (k) = \u2212 ln p(k) \u2212 ln p(y | k, l),\n\n(50)\n\nk\n\nwith\n\nk\n\nneeds one more integration\np(y | k, l) =\n\nZZ\n\np(y, \u03c6, \u03c8 | k, l) d\u03c6 d\u03c8.\n\n(51)\n\n\for\np(y | k, l) =\n\nZZ\n\np(y, \u03c6, \u03bb|k, l) d\u03c6 d\u03bb,\n\n(52)\n\nwhere p(y, \u03c6, \u03bb|k, l) \u221d exp [\u2212J2 (\u03c6, \u03bb)] given by (40). As we mentioned in the preceeding\nsection, these integrations can only be down numerically. A good approximation can be\nobtained using the following:\np(y | k, l) =\n\nZ Z\n\np(y, \u03c6, \u03c8 | k, l) d\u03c6 d\u03c8 \u2243\n\nXX\ni\n\np(y|\u03c6j , \u03c8i , k, l),\n\n(53)\n\nj\n\nwhere {\u03c6j } and {\u03c8i } are samples generated using the prior laws p(\u03c6) and p(\u03c8).\n\nBEST BASIS OR MODEL SELECTION\nThe model selection\nb\nl = arg max {p(l | y)} = arg min {J\nl\n\nwith\n\nl\n\n7 (l)}\n\nJ7 (l) = \u2212 ln p(l) \u2212 ln p(y | l)\n\n(54)\n\n(55)\n\ndoes not need any more integration, but only one summation. Choosing p(l) uniform\nand making the same previous approximations we have\nJ7 (l) = \u2212 ln\n\nkX\nmax\nk=1\n\np(y | k, l) p(k).\n\n(56)\n\n\fPROPOSED ALGORITHMS\nBased on equations (55), (53), (50), (39) and (40), we propose the following algorithm:\nMarginal MAP estimation algorithm 2\nGenerate a set of samples {\u03c6j } drawn from p(\u03c6)\nGenerate a set of samples {\u03c8i } drawn from p(\u03c8)\nfor l = 1 : lmax\nfor k = 1 : kmax\ncompute the elements of the matrix A;\nfor \u03c6 \u2208 {\u03c6j }\nfor \u03c8 \u2208 {\u03c8i }\nb = (At A + \u03bbI)\u22121 At y and y\nb = Ax\nb\ncompute \u03bb = \u03c6/\u03c8, x\ncompute p\u03c8 (i, j, k, l) = exp [\u2212J2 (\u03c6j , \u03c8i )] (eq. 39)\nend\nP\nnormalize p\u03c8 (i, j, k, l)P\n= p\u03c8 (i, j, k, l) / i p\u03c8 (i, j, k, l)\ncompute p\u03c6 (j, k, l) = i p\u03c8 (i, j, k, l)\nend\nP\nnormalize p\u03c6 (j, k, l)P= p\u03c6 (j, k, l) / j p\u03c6 (j, k, l)\ncompute pk (k, l) = j p\u03c6 (j, k, l)\nend\nP\nnormalize pk (k, l)P= pk (k, l) / k pk (k, l)\ncompute pl (l) = k pk (k, l)\nend\nP\nnormalize pl (l) = pl (l) / l p(l)\nchoose the best model by lb = arg maxl {pl (l)}n\no\nb\nchoose the best model order by kb = arg maxk pk (k, l)\nn\n\nb k)\nb\nchoose the best value for \u03c6 = \u03c6bj with bj = arg maxj p\u03c6 (j, l,\nn\n\no\n\nb k)\nb\nchoose the best value for \u03c8 = \u03c8bi with bi = arg maxi p\u03c8 (i, bj, l,\nb = \u03c6/\nb \u03c8\nb\ncompute \u03bb\ncompute the elements of the matrix A for l = bl and k = kb\nb \u22121 At y.\nb = (At A + \u03bbI)\ncompute x\n\no\n\nAPPLICATION: ELECTRON SCATTERING DATA INVERSION\nElastic electron scattering provides a means of determining the charge density of a nucleus, \u03c1(r), from the experimentally determined charge form factor, F (q). The connection between the charge density and the cross section is well understood and in plane\nwave Born approximation F (q) is just the Fourier transform of \u03c1(r), which for the case\n\n\fof even-even nuclei, which we shall consider, is simply given by\nF (q) = 4\u03c0\n\nZ\n\n\u221e\n\n0\n\nr 2 J0 (qr)\u03c1(r) dr,\n\n(57)\n\nwhere J0 is the spherical Bessel function of zero order and q is the absolute value of the\nthree momentum transfer.\nWe applied the proposed method with the following usual discretization procedure:\n\u03c1(r) =\n\n( P\nk\n\nwhich results in\nF (q) = 4\u03c0\n\nj=1 xj bj (r)\n\n0\n\nk\nX\n\nxj\n\nj=1\n\nZ\n\nRc\n0\n\nr \u2264 Rc\nr > Rc\n\nr 2 J0 (qr) bj (r) dr\n\n(58)\n\n(59)\n\nand\ny = Ax + \u01eb,\n\n(60)\n\nwhere x is a vector containing the coefficients {xj , j = 1, * * * , k}, y is a vector containing\nthe form factor data {F (qi ), i = 1, * * * , m} and A an (m \u00d7 k) matrix containing the\ncoefficients Ai,j given by\nAi,j = 4\u03c0\n\nZ\n\nRc\n\n0\n\nr 2 J0 (qi r) bj (r) dr.\n\n(61)\n\nTo compute Ai,j we define a discretization step \u2206r = Rc /N, a vector r = {rn =\n(n \u2212 1)\u2206r, n = 1, * * * , N}, a (N \u00d7 k) matrix B with elements Bn,j = bj (rn ), a (m \u00d7 N)\nmatrix C with elements Ci,n = (4\u03c0\u2206r)rn2 J0 (qi rn ) such that we have A = CB. Note\nalso that when the vector x is determined, we can compute \u03c1 = {\u03c1(rn ), n = 1, * * * , N}\nby \u03c1 = Bx.\nTo test the proposed methods, we used the following simulation procedure:\nSelect a model type l and an order k and generate the matrices B, C and A, and\nfor a random set of parameters x generate the data y = Ax.\n\u2022 Add some noise \u01eb on y to obtain y = A x + \u01eb.\nb k,\nb x,\nb y\nb = Ax\nb and \u03c1\nb = Bx\nb and compare them with l, k,\n\u2022 Compute the estimates l,\nx, y = Ax and \u03c1 = Bx.\n\u2022\n\nWe chose the following basis functions:\n\nl = 1 : bj (r) = J0 (qj r)- This is a natural choice due to the integral kernel and\nthe orthogonality property of the Bessel functions.\n\u2022 l=2:\nbj (r) = sinc(qj r)- This choice is also natural due to the orthogonality\nand the limited support hypothesis for the function \u03c1(r).\n\u2022\n\nh\n\ni\n\n\u2022\n\nl = 3 : bj (r) = exp \u2212 21 (qj r)2 - This choice can account for the positivity of\nthe function \u03c1(r) if the {xj } are constrained to be positive.\n\n\u2022\n\nl = 4 : bj (r) = exp \u2212 12 (qj r)2 J0 (qj r)- This choice combines the first and the\nthird properties.\n\nh\n\ni\n\n\fl=5:\none.\n\u2022 l=6:\none.\n\nbj (r) = 1/(cosh(qj r))- This choice has the same properties as the third\n\n\u2022\n\nbj (r) = 1/(1 + (qj r)2 )- This choice has the same properties as the third\n\nIn all these experiments we chose k = 6, m = 20, N = 100, Rc = 8 and qi = i\u03c0/Rc .\nThe following figures show typical solutions. Figures 1 and 2 show the details of the\nprocedure for the case l = 1. Figures 3, 4 and 5 show the results for the cases l = 1 to\nl = 6.\n1\n\nbj (r)\nj = 1, . . . , k = 6\n\nj\n\nb (r)\n\n0.5\n\n0\n\n\u22120.5\n\n0\n\n1\n\n2\n\n3\n\n4\nr\n\n5\n\n6\n\n7\n\n8\n\n1\n\n\u03c1(r) =\n\n0.8\n\nk\nX\n\nxj bj (r)\n\nj=1\n\nwith\nx1 = x2 = * * * = x6 = 1\n\nrho(r)\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n\u22120.2\n\n0\n\n1\n\n2\n\n3\n\n4\nr\n\n5\n\n6\n\n7\n\n8\n\n5\n\n4\n\n3\n\nlog(abs(F(q)))\n\n2\n\nF (qi ) =\n\n1\n\nZ\n\n0\n\nk\nX\n\nAij xj\n\nj=1\n2\n\nAij = r J0 (qi r) bj (r) dr\ni = 1, . . . , m = 14\nj = 1, . . . , k = 6\n\n\u22121\n\n\u22122\n\n\u22123\n\n\u22124\n\n\u22125\n\n0\n\n1\n\n2\n\n3\n\n4\nq\n\n5\n\nFig. 1: a) basis functions bj (r), b) \u03c1(r),\n\n6\n\n7\n\n8\n\nc) data F (qi ) in a logarithmic scale.\n\n\f0.1\n\n0.05\n\n0\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\np(k, l)\n\nmodel order\n\nmodel number\n0.8\n\n0.1\n0.08\n\n0.6\n\n0.06\n0.4\n0.04\n0.2\n\nb\np(l) and p(k|l)\n\n0.02\n\n0\n\n1\n\n2\n\n0\n\n3\n4\n5\n6\nmodel number\n\n1 2 3 4 5 6 7 8 9101112131415\nmodel order\n\n0.9\n\n0.8\n\nb\n\u03c1(r)\n=\n\n0.7\n\n0.6\n\nand\n\n0.5\n\n0.4\n\n\u03c1(r) =\n\n0.3\n\nk\nX\n\nj=1\nk\nX\n\nxbj bj (r)\nxj bj (r)\n\nj=1\n\n0.2\n\n0.1\n\n0\n\n\u22120.1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n4\n\n3\n\n2\n\nj=1\n\nF (qi ) =\n\nk\nX\n\nand\n\n1\n\n0\n\nk\nX\n\nFb (qi ) =\n\nAij xbj\nAij xj\n\nj=1\n\n\u22121\n\n\u22122\n\n\u22123\n\n\u22124\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nb\nb\nFig. 2: a) p(k, l|y), p(l|y) and p(k|y, l),\nb) original \u03c1(r) and estimated \u03c1(r),\nb\nc) original F (qi ) and estimated F (qi ).\n\n\f1\n\n1\n\n0.8\n\n0.6\n0.5\n\nbj (r)\n\nbj (r)\n\n0.4\n\n0.2\n0\n0\n\n\u22120.2\n\n\u22120.5\n\n0\n\n1\n\n2\n\n3\n\n4\nr\n\n5\n\n6\n\n7\n\n\u22120.4\n\n8\n\n0\n\n1\n\n2\n\n3\n\n4\nr\n\n5\n\n0.45\n\n0.16\n\n0.4\n\n0.18\n\n0.4\n\n0.14\n\n0.35\n\n0.16\n\n0.12\n\n0.3\n\n0.1\n\n0.25\n\n0.35\n\n6\n\n7\n\n8\n\n0.14\n\n0.3\n\n0.12\n\n0.08\n\nP(k)\n\nP(l)\n\nP(l)\n\n0.1\n\nP(k)\n\n0.25\n0.2\n\n0.2\n\n0.08\n0.06\n\n0.15\n\n0.04\n\n0.1\n\n0.05\n\n0.02\n\n0.05\n\n0\n\n0\n\n0\n\n0.15\n\n0.06\n\n0.1\n\n0.04\n\n1\n\n2\n\n3\n4\n5\nModel number\n\n6\n\n1 2 3 4 5 6 7 8 9 101112131415\nModel order\n\n0.9\n\n0.9\n\n0.8\n\n0.8\n\n0.7\n\n0.02\n\n1\n\n2\n\n3\n4\n5\nModel number\n\n0\n\n6\n\n1 2 3 4 5 6 7 8 9 101112131415\nModel order\n\n0.7\n\n0.6\n0.6\n0.5\n0.5\n0.4\n0.4\n0.3\n0.3\n0.2\n0.2\n\n0.1\n\n0.1\n\n0\n\n\u22120.1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n4\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n5\n\n4\n\n3\n\n3\n2\n2\n1\n\n1\n\n0\n\n0\n\n\u22121\n\n\u22121\n\n\u22122\n\u22122\n\u22123\n\u22123\n\n\u22124\n\n\u22124\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n\u22125\n\n0\n\n1\n\n2\n\n3\n\n4\n\nFig. 3:\nLeft: l = 1\nRight: l = 2\nb c) \u03c1(r) and \u03c1(r),\nb\na) basis functions bj (r), b) p(k|y) and p(k|y, l),\nb\nd) F (qi ) and F (qi ).\n\n5\n\n6\n\n7\n\n\f1\n\n1\n\n0.9\n0.8\n0.8\n\n0.7\n0.6\n\nbj (r)\n\n0.5\n\nj\n\nb (r)\n\n0.6\n\n0.4\n\n0.4\n0.2\n0.3\n\n0.2\n0\n0.1\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\nr\n\n5\n\n0.5\n\n6\n\n7\n\n\u22120.2\n\n8\n\n0.35\n\n0\n\n1\n\n2\n\n3\n\n4\nr\n\n1\n\n0.45\n\n5\n\n6\n\n7\n\n8\n\n0.12\n\n0.9\n0.3\n\n0.1\n\n0.4\n\n0.8\n0.25\n\n0.35\n\n0.7\n0.08\n\n0.3\n\n0.6\n\nP(k)\n\n0.25\n\nP(l)\n\nP(k)\n\nP(l)\n\n0.2\n0.5\n\n0.06\n\n0.15\n0.2\n\n0.4\n0.04\n\n0.15\n\n0.3\n\n0.1\n\n0.1\n\n0.2\n0.02\n\n0.05\n0.05\n\n0\n\n0.1\n\n1\n\n2\n\n3\n4\n5\nModel number\n\n0\n\n6\n\n0\n\n1 2 3 4 5 6 7 8 9 101112131415\nModel order\n\n0.9\n\n1\n\n0.8\n\n0.9\n\n1\n\n2\n\n3\n4\n5\nModel number\n\n0\n\n6\n\n1 2 3 4 5 6 7 8 9 101112131415\nModel order\n\n0.8\n\n0.7\n\n0.7\n0.6\n0.6\n0.5\n0.5\n0.4\n0.4\n0.3\n0.3\n0.2\n\n0.2\n\n0.1\n\n0\n\n0.1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n0\n\n4\n\n4\n\n3\n\n3\n\n2\n\n2\n\n1\n\n1\n\n0\n\n0\n\n\u22121\n\n\u22121\n\n\u22122\n\n\u22122\n\n\u22123\n\n\u22123\n\n\u22124\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n\u22124\n\n0\n\n0\n\n1\n\n2\n\n1\n\n3\n\n2\n\n4\n\n3\n\n5\n\n4\n\nFig. 4:\nLeft: l = 3\nRight: l = 4\nb c) \u03c1(r) and \u03c1(r),\nb\na) basis functions bj (r), b) p(k|y) and p(k|y, l),\nb\nd) F (qi ) and F (qi ).\n\n6\n\n5\n\n7\n\n6\n\n8\n\n7\n\n\f0.8\n\n0.8\n\n0.7\n\n0.7\n\n0.6\n\n0.6\n\nb (r)\n\n1\n\n0.9\n\n0.5\n\nj\n\n0.5\n\nj\n\nb (r)\n\n1\n\n0.9\n\n0.4\n\n0.4\n\n0.3\n\n0.3\n\n0.2\n\n0.2\n\n0.1\n\n0.1\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\nr\n\n5\n\n0.35\n\n6\n\n7\n\n0\n\n8\n\n0.25\n\n0\n\n1\n\n2\n\n3\n\n4\nr\n\n5\n\n0.35\n\n0.3\n\n6\n\n7\n\n8\n\n0.8\n\n0.7\n\n0.3\n0.2\n\n0.6\n0.25\n\n0.25\n0.5\n\n0.15\n\nP(l)\n\nP(l)\n\nP(k)\n\n0.2\n\nP(k)\n\n0.2\n\n0.15\n\n0.4\n\n0.15\n0.1\n\n0.3\n\n0.1\n\n0.1\n0.2\n0.05\n\n0.05\n\n0\n\n0.05\n\n1\n\n2\n\n3\n4\n5\nModel number\n\n0\n\n6\n\n0\n\n1 2 3 4 5 6 7 8 9 101112131415\nModel order\n\n0.9\n\n1\n\n0.8\n\n0.9\n\n0.1\n\n1\n\n2\n\n3\n4\n5\nModel number\n\n0\n\n6\n\n1 2 3 4 5 6 7 8 9 101112131415\nModel order\n\n0.8\n\n0.7\n\n0.7\n0.6\n0.6\n0.5\n0.5\n0.4\n0.4\n0.3\n0.3\n0.2\n\n0.2\n\n0.1\n\n0\n\n0.1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n5\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n5\n\n4\n4\n3\n3\n\n2\n\n1\n2\n0\n1\n\u22121\n\n\u22122\n\n0\n\n\u22123\n\u22121\n\u22124\n\n\u22125\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n\u22122\n\n0\n\n1\n\n2\n\n3\n\n4\n\nFig. 5:\nLeft: l = 5\nRight: l = 6\nb c) \u03c1(r) and \u03c1(r),\nb\na) basis functions bj (r), b) p(k|y) and p(k|y, l),\nb\nd) F (qi ) and F (qi ).\n\n5\n\n6\n\n7\n\n\fNote that in these tests, we know perfectly the model and generated the data according\nto our hypothesis. To test the method in a more realistic case, we choose a model for\nwhich we can have an exact analytic expression for the integrals. For example, if we\nchoose a symmetric Fermi distribution [4]\n\u03c1(r) = \u03b1\n\ncosh(R/d)\n,\ncosh(R/d) + cosh(r/d)\n\n(62)\n\nan analytical expression for the corresponding charge form factor can easily be obtained [5]:\n#\n\n\"\n\n4\u03c0 2 \u03b1d cosh(R/d) R cos(qR) \u03c0d sin(qR) cosh(\u03c0qd)\n.\n\u2212\nF (q) = \u2212\nq sinh(R/d) sinh(\u03c0qd)\nsinh2 (\u03c0qd)\n\n(63)\n\nOnly two of the parameters \u03b1, R and d are independent since the charge density must\nfulfill the normalization condition\n4\u03c0\n\nZ\n\nr 2 \u03c1(r) dr = Z.\n\n(64)\n\nFigure 6 shows the theoretical charge density \u03c1(r) of 12 C (Z=6) obtained from (62)\nfor r \u2208 [0, 8] fm with R = 1.1 A and d = 0.626 fm and the theoretical charge form factor\nF (q) obtained by (63) for q \u2208 [0, 8] fm\u22121 and the 15 simulated data:\nq = [0.001, .5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0] fm\u22121\nwhich are used as inputs to the inversion method.\nCharge density\n\n4\n\n1\n\n2\n0.8\n\n0\n\n\u22122\n\nForm factor\n\nCharge density\n\n0.6\n\n0.4\n\n\u22124\n\n\u22126\n\n\u22128\n\n0.2\n\n\u221210\n0\n\n\u221212\n\n\u22120.2\n\n0\n\n1\n\n2\n\n3\n\n4\nRadius\n\n5\n\n6\n\n7\n\n8\n\n\u221214\n\n0\n\n1\n\n2\n\n3\n\n4\n5\nMomentum transfer\n\n6\n\n7\n\n8\n\nFig. 6: Theoretical charge density \u03c1(r), charge form factor log |F (q)| and the data\n[stars] used for numerical experiments [right].\n\n\fFirst note that, even with the exact data, there are an infinite number of solutions\nwhich fits exactly the data. The following figure shows a few sets of these solutions.\n\n0.06\n0.04\n0.02\n0\n1\n\n2\n\n3\n\n4\n\n5\n\n6\nmodel order\n\nmodel number\n0.4\n\n0.06\n0.05\n\n0.3\n0.04\n0.2\n\n0.03\n0.02\n\n0.1\n0.01\n0\n\n1\n\n2\n\n0\n\n3\n4\n5\n6\nmodel number\n\n0.8\n\n4\n\n0.7\n\n2\n\n0.6\n\n0\n\n0.5\n\n\u22122\n\n0.4\n\n\u22124\n\n0.3\n\n\u22126\n\n0.2\n\n\u22128\n\n0.1\n\n\u221210\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n\u221212\n\n0\n\n1 2 3 4 5 6 7 8 9101112131415\nmodel order\n\n1\n\n2\n\n3\n\n4\n\nFig. 7: a) p(k, l|y)\nb\nb) p(k|y)\nc) p(k|y, l)\nb\nd) \u03c1(r) and \u03c1(r)\ne) F (qi ) and Fb (qi ).\n\n5\n\n6\n\n7\n\n\fCONCLUSIONS\nWe discussed the different steps for a complete resolution of an inverse problem and\nfocused on the choice of a basis function selection and the order of the model. An\nalgorithm based on Bayesian estimation is proposed and tested on simulated data.\n\nREFERENCES\n1.\n2.\n3.\n4.\n5.\n6.\n7.\n8.\n9.\n10.\n11.\n12.\n13.\n14.\n\nJ. L. Friar and J. W. Negele, Nucl. Phys. A 212, 93 (1973).\nB. Dreher et al., Nucl. Phys. A 235, 219 (1974).\nJ. Heisenberg and H. P. Blok, Ann. Rev. Nucl. Part. Sc. 33, 569 (1983).\nM. E. Grypeos, G. A. Lalazissis, S. E. Massen, and C. P. Panos, J. Phys. G 17, 1093 (1991).\nR. E. Kozak, Am. J. Phys. 59, 74 (1991).\nJ. Baker-Jarvis, J. Math. Phys. 30, 302 (1989).\nJ. Baker-Jarvis, M. Racine, and J. Alameddine, J. Math. Phys. 30, 1459 (1989).\nN. Canosa, H. G. Miller, A. Plastino and R. Rossignoli, Physica A220, 611 (1995).\nBuck and Macaulay, \"Linear inversion by the method of maximum entropy,\" in Maximum Entropy\nand Bayesian Methods 89, (J. Skilling, ed.), Kluwer Academic Publishers, 1990.\nA. Mohammad-Djafari, \"A full Bayesian approach for inverse problems,\" in Maximum Entropy and\nBayesian Methods 95, (K. Hanson and R. Silver, ed.), Kluwer Academic Publishers, 1996.\nD.J.C. MacKay, \"Hyperparameters: Optimize or integrate out?\" in Maximum Entropy and Bayesian\nMethods 93, (G. Heidbreder, ed.), pp. 43\u201359, Kluwer Academic Publishers, 1996.\nV.A. Macaulay and B. Buck, \"A fresh look at model selection in inverse scattering,\" in Maximum\nEntropy and Bayesian Methods 94, (J. Skilling and S. Sibisi ed.), Kluwer Academic Publishers,\n1996.\nA. Mohammad-Djafari and J. Idier, \"A scale invariant Bayesian method to solve linear inverse\nproblems\", pp. 121\u2013134. in Maximum Entropy and Bayesian Methods 94, (G. Heidbreder, ed.),\nKluwer Academic Publishers, 1996.\nA. Mohammad-Djafari and J. Idier, \"Maximum entropy prior laws of images and estimation of their\nparameters,\" pp. 285\u2013293. in Maximum Entropy and Bayesian Methods 90, (T. Grandy, ed.), Kluwer\nAcademic Publishers, 1991.\n\n\f"}