{"id": "http://arxiv.org/abs/cs/0405009v1", "guidislink": true, "updated": "2004-05-04T23:48:39Z", "updated_parsed": [2004, 5, 4, 23, 48, 39, 1, 125, 0], "published": "2004-05-04T23:48:39Z", "published_parsed": [2004, 5, 4, 23, 48, 39, 1, 125, 0], "title": "Intelligent Systems: Architectures and Perspectives", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0312053%2Ccs%2F0312046%2Ccs%2F0312037%2Ccs%2F0312049%2Ccs%2F0312044%2Ccs%2F0312020%2Ccs%2F0312052%2Ccs%2F0312054%2Ccs%2F0312004%2Ccs%2F0312015%2Ccs%2F0312029%2Ccs%2F0312028%2Ccs%2F0312006%2Ccs%2F0312012%2Ccs%2F0312027%2Ccs%2F0312030%2Ccs%2F0312002%2Ccs%2F0312055%2Ccs%2F0312010%2Ccs%2F0312039%2Ccs%2F0312051%2Ccs%2F0312025%2Ccs%2F0312001%2Ccs%2F0312034%2Ccs%2F0312058%2Ccs%2F0312045%2Ccs%2F0312038%2Ccs%2F0312059%2Ccs%2F0312041%2Ccs%2F0312016%2Ccs%2F0312040%2Ccs%2F0312047%2Ccs%2F0312023%2Ccs%2F0312003%2Ccs%2F0312056%2Ccs%2F0312043%2Ccs%2F0312031%2Ccs%2F0312026%2Ccs%2F0312022%2Ccs%2F0312035%2Ccs%2F0312014%2Ccs%2F0312008%2Ccs%2F0312048%2Ccs%2F0312024%2Ccs%2F0312036%2Ccs%2F0312007%2Ccs%2F0405095%2Ccs%2F0405111%2Ccs%2F0405040%2Ccs%2F0405107%2Ccs%2F0405080%2Ccs%2F0405027%2Ccs%2F0405059%2Ccs%2F0405054%2Ccs%2F0405091%2Ccs%2F0405061%2Ccs%2F0405067%2Ccs%2F0405009%2Ccs%2F0405106%2Ccs%2F0405088%2Ccs%2F0405078%2Ccs%2F0405029%2Ccs%2F0405103%2Ccs%2F0405046%2Ccs%2F0405113%2Ccs%2F0405074%2Ccs%2F0405102%2Ccs%2F0405042%2Ccs%2F0405015%2Ccs%2F0405011%2Ccs%2F0405062%2Ccs%2F0405104%2Ccs%2F0405033%2Ccs%2F0405093%2Ccs%2F0405063%2Ccs%2F0405004%2Ccs%2F0405021%2Ccs%2F0405097%2Ccs%2F0405108%2Ccs%2F0405076%2Ccs%2F0405041%2Ccs%2F0405064%2Ccs%2F0405044%2Ccs%2F0405037%2Ccs%2F0405028%2Ccs%2F0405079%2Ccs%2F0405001%2Ccs%2F0405023%2Ccs%2F0405081%2Ccs%2F0405060%2Ccs%2F0405101%2Ccs%2F0405047%2Ccs%2F0405099%2Ccs%2F0405056%2Ccs%2F0405018%2Ccs%2F0405084%2Ccs%2F0405036%2Ccs%2F0405072%2Ccs%2F0405017%2Ccs%2F0405069%2Ccs%2F0405100&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Intelligent Systems: Architectures and Perspectives"}, "summary": "The integration of different learning and adaptation techniques to overcome\nindividual limitations and to achieve synergetic effects through the\nhybridization or fusion of these techniques has, in recent years, contributed\nto a large number of new intelligent system designs. Computational intelligence\nis an innovative framework for constructing intelligent hybrid architectures\ninvolving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic\nReasoning (PR) and derivative free optimization techniques such as Evolutionary\nComputation (EC). Most of these hybridization approaches, however, follow an ad\nhoc design methodology, justified by success in certain application domains.\nDue to the lack of a common framework it often remains difficult to compare the\nvarious hybrid systems conceptually and to evaluate their performance\ncomparatively. This chapter introduces the different generic architectures for\nintegrating intelligent systems. The designing aspects and perspectives of\ndifferent hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC\nsystems are presented. Some conclusions are also provided towards the end.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0312053%2Ccs%2F0312046%2Ccs%2F0312037%2Ccs%2F0312049%2Ccs%2F0312044%2Ccs%2F0312020%2Ccs%2F0312052%2Ccs%2F0312054%2Ccs%2F0312004%2Ccs%2F0312015%2Ccs%2F0312029%2Ccs%2F0312028%2Ccs%2F0312006%2Ccs%2F0312012%2Ccs%2F0312027%2Ccs%2F0312030%2Ccs%2F0312002%2Ccs%2F0312055%2Ccs%2F0312010%2Ccs%2F0312039%2Ccs%2F0312051%2Ccs%2F0312025%2Ccs%2F0312001%2Ccs%2F0312034%2Ccs%2F0312058%2Ccs%2F0312045%2Ccs%2F0312038%2Ccs%2F0312059%2Ccs%2F0312041%2Ccs%2F0312016%2Ccs%2F0312040%2Ccs%2F0312047%2Ccs%2F0312023%2Ccs%2F0312003%2Ccs%2F0312056%2Ccs%2F0312043%2Ccs%2F0312031%2Ccs%2F0312026%2Ccs%2F0312022%2Ccs%2F0312035%2Ccs%2F0312014%2Ccs%2F0312008%2Ccs%2F0312048%2Ccs%2F0312024%2Ccs%2F0312036%2Ccs%2F0312007%2Ccs%2F0405095%2Ccs%2F0405111%2Ccs%2F0405040%2Ccs%2F0405107%2Ccs%2F0405080%2Ccs%2F0405027%2Ccs%2F0405059%2Ccs%2F0405054%2Ccs%2F0405091%2Ccs%2F0405061%2Ccs%2F0405067%2Ccs%2F0405009%2Ccs%2F0405106%2Ccs%2F0405088%2Ccs%2F0405078%2Ccs%2F0405029%2Ccs%2F0405103%2Ccs%2F0405046%2Ccs%2F0405113%2Ccs%2F0405074%2Ccs%2F0405102%2Ccs%2F0405042%2Ccs%2F0405015%2Ccs%2F0405011%2Ccs%2F0405062%2Ccs%2F0405104%2Ccs%2F0405033%2Ccs%2F0405093%2Ccs%2F0405063%2Ccs%2F0405004%2Ccs%2F0405021%2Ccs%2F0405097%2Ccs%2F0405108%2Ccs%2F0405076%2Ccs%2F0405041%2Ccs%2F0405064%2Ccs%2F0405044%2Ccs%2F0405037%2Ccs%2F0405028%2Ccs%2F0405079%2Ccs%2F0405001%2Ccs%2F0405023%2Ccs%2F0405081%2Ccs%2F0405060%2Ccs%2F0405101%2Ccs%2F0405047%2Ccs%2F0405099%2Ccs%2F0405056%2Ccs%2F0405018%2Ccs%2F0405084%2Ccs%2F0405036%2Ccs%2F0405072%2Ccs%2F0405017%2Ccs%2F0405069%2Ccs%2F0405100&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The integration of different learning and adaptation techniques to overcome\nindividual limitations and to achieve synergetic effects through the\nhybridization or fusion of these techniques has, in recent years, contributed\nto a large number of new intelligent system designs. Computational intelligence\nis an innovative framework for constructing intelligent hybrid architectures\ninvolving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic\nReasoning (PR) and derivative free optimization techniques such as Evolutionary\nComputation (EC). Most of these hybridization approaches, however, follow an ad\nhoc design methodology, justified by success in certain application domains.\nDue to the lack of a common framework it often remains difficult to compare the\nvarious hybrid systems conceptually and to evaluate their performance\ncomparatively. This chapter introduces the different generic architectures for\nintegrating intelligent systems. The designing aspects and perspectives of\ndifferent hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC\nsystems are presented. Some conclusions are also provided towards the end."}, "authors": ["Ajith Abraham"], "author_detail": {"name": "Ajith Abraham"}, "author": "Ajith Abraham", "links": [{"href": "http://arxiv.org/abs/cs/0405009v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0405009v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.0", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0405009v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0405009v1", "arxiv_comment": null, "journal_reference": "Recent Advances in Intelligent Paradigms and Applications, Abraham\n  A., Jain L. and Kacprzyk J. (Eds.), Studies in Fuzziness and Soft Computing,\n  Springer Verlag Germany, ISBN 3790815381, Chapter 1, pp. 1-35, 2002", "doi": null, "fulltext": "Intelligent Systems: Architectures and\nPerspectives\nAjith Abraham\nFaculty of Information Technology, School of Business Systems\nMonash University (Clayton Campus), Victoria 3168, Australia\nEmail: ajith.abraham@ieee.org, URL: http://ajith.softcomputing.net\n\nAbstract: The integration of different learning and adaptation techniques to\novercome individual limitations and to achieve synergetic effects through the\nhybridization or fusion of these techniques has, in recent years, contributed to a\nlarge number of new intelligent system designs. Computational intelligence is an\ninnovative framework for constructing intelligent hybrid architectures involving\nNeural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic Reasoning\n(PR) and derivative free optimization techniques such as Evolutionary\nComputation (EC). Most of these hybridization approaches, however, follow an ad\nhoc design methodology, justified by success in certain application domains. Due\nto the lack of a common framework it often remains difficult to compare the\nvarious hybrid systems conceptually and to evaluate their performance\ncomparatively. This chapter introduces the different generic architectures for\nintegrating intelligent systems. The designing aspects and perspectives of different\nhybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC\nsystems are presented. Some conclusions are also provided towards the end.\nKeywords: computational intelligence, hybrid systems, neural network, fuzzy\nsystem, evolutionary computation\n\n1. Introduction\nIn recent years, several adaptive hybrid soft computing [108] frameworks have\nbeen developed for model expertise, decision support, image and video\nsegmentation techniques, process control, mechatronics, robotics and complicated\nautomation tasks. Many of these approaches use a combination of different\nknowledge representation schemes, decision making models and learning\nstrategies to solve a computational task. This integration aims at overcoming the\nlimitations of individual techniques through hybridization or the fusion of various\n\n\ftechniques. These ideas have led to the emergence of several different kinds of\nintelligent system architectures [14][51-53][58][66][69][92].\nIt is well known that intelligent systems, which can provide human-like expertise\nsuch as domain knowledge, uncertain reasoning, and adaptation to a noisy and\ntime-varying environment, are important in tackling practical computing\nproblems. In contrast with conventional artificial intelligence techniques which\nonly deal with precision, certainty and rigor, the guiding principle of soft\ncomputing is to exploit the tolerance for imprecision, uncertainty, low solution\ncost, robustness, partial truth to achieve tractability, and better rapport with reality\n[108]. In general hybrid soft computing consists of 4 essential paradigms: NN,\nFIS, EC and PR. Nevertheless, developing intelligent systems by hybridization is\nan open-ended rather than a conservative concept. That is, it is evolving those\nrelevant techniques together with the important advances in other new computing\nmethods [35][96]. Table 1 lists the three principal ingredients together with their\nadvantages [12][42].\nTable 1. Comparison of different intelligent systems with classical approaches\u2020.\n\nMathematical model\nLearning ability\nKnowledge representation\nExpert knowledge\nNonlinearity\nOptimization ability\nFault tolerance\nUncertainty tolerance\nReal time operation\n\nFIS\n\nNN\n\nEC\n\nSG\nB\nG\nG\nG\nB\nG\nG\nG\n\nB\nG\nB\nB\nG\nSG\nG\nG\nSG\n\nB\nSG\nSB\nB\nG\nG\nG\nG\nSB\n\nSymbolic\nAI\nSB\nB\nG\nG\nSB\nB\nB\nB\nB\n\n\u2020\n\nFuzzy terms used for grading are good (G), slightly good (SG), slightly bad (SB)\nand bad (B).\nTo achieve a highly intelligent system, a synthesis of various techniques is\nrequired. Figure 1 shows the synthesis of NN, FIS and EC and their mutual\ninteractions leading to different architectures. Each technique plays a very\nimportant role in the development of different hybrid soft computing architectures.\nExperience has shown that it is crucial, in the design of hybrid systems, to focus\nprimarily on the integration and interaction of different techniques rather than to\nmerge different methods to create ever-new techniques. Techniques already well\nunderstood should be applied to solve specific domain problems within the\nsystem. Their weaknesses must be addressed by combining them with\ncomplementary methods.\nNeural networks offer a highly structured architecture with learning and\ngeneralization capabilities, which attempts to mimic the neurological mechanisms\n\n\fof the brain. NN stores knowledge in a distributive manner within its weights\nwhich have been determined by learning from known samples. The generalization\nability of new inputs is then based on the inherent algebraic structure of the NN.\nHowever it is very hard to incorporate human a priori knowledge into a NN\nmainly because the connectionist paradigm gains most of its strength from a\ndistributed knowledge representation.\nNN\n\nNN - FIS\n\nNN - EC\n\nANN - FIS - EC\n\nFIS\n\nEC\n\nFIS - EC\n\nFigure 1. General framework for hybrid soft computing architectures\nBy contrast, fuzzy inference systems [106-107] exhibit complementary\ncharacteristics, offering a very powerful framework for approximate reasoning\nwhich attempts to model the human reasoning process at a cognitive level [61].\nFIS acquires knowledge from domain experts which is encoded within the\nalgorithm in terms of the set of if-then rules. FIS employ this rule-based approach\nand interpolative reasoning to respond to new inputs [30]. The incorporation and\ninterpretation of knowledge is straightforward, whereas learning and adaptation\nconstitute major problems.\nProbabilistic reasoning such as Bayesian belief networks [20] and the DempsterShafer theory of belief [36] [86], gives us a mechanism for evaluating the outcome\nof systems affected by randomness or other types of probabilistic uncertainty. An\nimportant advantage of probabilistic reasoning is its ability to update previous\noutcome estimates by conditioning them with newly available evidence [57].\nGlobal optimization involves finding the absolutely best set of parameters to\noptimize an objective function. In general, it may be possible to have solutions\nthat are locally but not globally optimal. Consequently, global optimization\nproblems are typically quite difficult to solve exactly: in the context of\ncombinatorial problems, they are often NP-hard. Evolutionary Computation works\nby simulating evolution on a computer by iterative generation and alteration\nprocesses operating on a set of candidate solutions that form a population. The\n\n\fentire population evolves towards better candidate solutions via the selection\noperation and genetic operators such as crossover and mutation. The selection\noperator decides which candidate solutions move on into the next generation and\nthus limits the search space [40].\nSection 2 presents the various techniques to forumlate hybrid intelligent\narchitectures followed by optimization of neural network using evolutionary\ncomputation and local search techniques in Section 3. Adaptation issues of fuzzy\ninference systems are discussed in Section 4 followed by evolutionary fuzzy\nsystems and cooperative neuro-fuzzy systems in Section 5 and 6 respectively.\nIntegrated neuro-fuzzy systems are presented in Section 7. In Section 8, a\nframework for an integrated neuro-fuzzy-evolutionary system is presented.\nOptimization of evolutionary algorithms using soft computing techniques is\npresented in Section 9 and finally interactions between soft computing technology\nand probabilistic reasoning techniques are given in Section 10. Some conclusions\nare also presented.\n\n2. Models Of Hybrid Soft Computing Architectures\nWe broadly classify the various hybrid intelligent architectures into 4 different\ncategories based on the system's overall architecture: (1) Stand-alone (2)\nTransformational (3) Hierarchical hybrid and (4) Integrated hybrid. The following\nsections discuss each of these strategies, the expected uses of the model and some\nbenefits and limitations of the approach.\n\n2.1 Stand Alone Intelligent System\nStand-alone models consist of independent software components which do not\ninteract in any way. Developing stand-alone systems can have several purposes:\nfirst, they provide a direct means of comparing the problem solving capabilities of\ndifferent techniques with reference to a certain application [13]. Running different\ntechniques in a parallel environment permits a loose approximation of integration.\nStand-alone models are often used to develop a quick initial prototype, while a\nmore time-consuming application is developed. Figure 2 displays a stand-alone\nsystem where a neural network and a fuzzy system are used separately.\n\nNeural network\n\nFuzzy system\n\nFigure 2. Stand\u2013alone system\nSome of the benefits are simplicity and ease of development by using\ncommercially available software packages . On the other hand, stand-alone\n\n\ftechniques are not transferable: neither can support the weakness of the other\ntechnique.\n\n2.2 Transformational Hybrid Intelligent System\nIn a transformational hybrid model, the system begins as one type and ends up as\nthe other. Determining which technique is used for development and which is used\nfor delivery is based on the desirable features that the technique offers. Figure 3\nshows the interaction between a neural network and an expert system in a\ntransformational hybrid model [69]. Obviously, either the expert system is\nincapable of adequately solving the problem, or the speed, adaptability, and\nrobustness of neural network is required. Knowledge from the expert system is\nused to determine the initial conditions and the training set for the artificial neural\nnetwork.\n\nNeural network\n\nExpert system\n\nFigure 3. Transformational hybrid architecture\nTransformational hybrid models are often quick to develop and ultimately require\nmaintenance on only one system. They can be developed to suit the environment\nand offer many operational benefits. Unfortunately, transformational models are\nsignificantly limited: most are just application-oriented. For a different\napplication, a totally new development effort might be required such as a fully\nautomated means of transforming an expert system to a neural network and vice\nversa.\nNeural network\n\nEvolutionary\nalgorithm\n\nFuzzy system\n\nFigure 4. Hierarchical hybrid architectures\n\n2.3 Hierarchical Hybrid Intelligent System\nThis architecture is built in a hierarchical fashion, associating a different\nfunctionality with each layer. The overall functioning of the model depends on the\ncorrect functioning of all the layers. Figure 4 demonstrates a hierarchical hybrid\n\n\farchitecture involving a neural network, an evolutionary algorithm and a fuzzy\nsystem. The neural network uses an evolutionary algorithm to optimize its\nperformance and the network output acts as a pre-processor to a fuzzy system,\nwhich then produces the final output. Poor performance in one of the layers\ndirectly affects the final output.\n\n2.4 Integrated Intelligent System\nFused architectures are the first true form of integrated intelligent systems. They\ninclude systems which combine different techniques into one single computational\nmodel. They share data structures and knowledge representations. Another\napproach is to put the various techniques side-by-side and focus on their\ninteraction in a problem-solving task. This method can allow for integrating\nalternative techniques and exploiting their mutuality. Furthermore, the conceptual\nview of the agent allows one to abstract from the individual techniques and focus\non the global system behavior, as well as to study the individual contribution of\neach component [51].\nThe benefits of integrated models include robustness, improved performance and\nincreased problem-solving capabilities. Finally, fully integrated models can\nprovide a full range of capabilities such as adaptation, generalization, noise\ntolerance and justification. Fused systems have limitations caused by the increased\ncomplexity of the inter-module interactions and specifying, designing, and\nbuilding fully integrated models is complex. In this chapter, discussions is limited\nto different integrated intelligent systems involving neural networks, fuzzy\ninference systems, evolutionary algorithms and probabilistic reasoning techniques.\n\n3. Neural Networks and Evolutionary Algorithms\nEven though artificial neural networks are capable of performing a wide variety of\ntasks, in practice, they sometimes deliver only marginal performance.\nInappropriate topology selection and learning algorithms are frequently blamed.\nThere is little reason to expect to find a uniformly best algorithm for selecting the\nweights in a feedforward artificial neural network [97]. It is an NP-complete\nproblem to find a set of weights for a given neural network and a set of training\nexamples to classify even two-thirds of them correctly. In general, claims in the\nliterature on training algorithms that one being proposed is substantially better\nthan most others should be treated with scepticism. Such claims are often\ndefended through simulations based on applications in which the proposed\nalgorithm performed better than some familiar alternative.\nThe artificial neural network (ANN) methodology enables the design of useful\nnonlinear systems accepting large numbers of inputs, with the design based solely\non instances of input-output relationships. For a training set T, consisting of n\nargument value pairs, and given a d-dimensional argument x, an associated target\nvalue t will be approximated by the neural network output. The function\napproximation could be represented as:\n\n\fT = {( xi , t i ), i = 1 : n }\n\n(1)\n\nIn most applications, the training set T is considered to be noisy and while the goal\nis not to reproduce it exactly the intention is to construct a network function that\ngeneralizes well to new function values. An attempt will be made to address the\nproblem of selecting the weights to learn the training set. The notion of closeness\non the training set T is typically formalized through an error function of the form:\nn\n\n\u03c8 T = \u2211 yi \u2212 ti\n\n2\n\n(2)\n\ni =1\n\nwhere yi is the network output. A long recognized bane of analysis of the error\nsurface and the performance of training algorithms is the presence of multiple\nstationary points, including multiple minima. Empirical results with practical\nproblems and training algorithms show that different initialization yields different\nnetworks [5][9]. Hence the issue of many minima is a real one. According to Auer\net al [17], a single node network with n training pairs and Rd inputs, could end up\nn\nhaving ( ) d local minima. Hence, not only do multiple minima exist, but also,\nd\nthere may be huge numbers of them.\nDifferent learning algorithms have staunch proponents who can always construct\ninstances in which their algorithm performs better than most others. In practice,\noptimization algorithms that are used to minimize \u03a8T (w) can be classified into\nfour categories. The first three methods, gradient descent, conjugate gradients and\nquasi-Newton, are general optimization methods whose operation can be\nunderstood in the context of minimization of a quadratic error function\n[25][38[73]. Although the error surface is not quadratic, for differentiable node\nfunctions, it will be in a sufficiently small neighborhood of a local minimum. Such\nan analysis provides information about the behavior of the training algorithm over\nthe span of a few iterations and also as it approaches its goal. The fourth method,\nthat of Levenberg and Marquardt [31], is specifically adapted to minimization of\nan error function that arises from a squared error criterion of the form assumed.\nBackpropagation calculation of the gradient can be adapted easily to provide the\ninformation about the Jacobian matrix J needed for this method. A common\nfeature of these training algorithms is the requirement of repeated efficient\ncalculation of gradients [56].\nMany of the conventional ANNs now being designed are statistically quite\naccurate but still leave a bad taste with users who expect computers to solve their\nproblems accurately. The important drawback is that the designer has to specify\nthe number of neurons, their distribution over several layers and the\ninterconnection between them. Several methods have been proposed to\nautomatically construct ANNs for reduction in network complexity that is to\ndetermine the appropriate number of hidden units, layers and learning rules [82].\nTopological optimization algorithms such as Extentron [18], Upstart [41], Tiling\n\n\f[70], Pruning [88] and Cascade Correlation [37] have their own limitations\n[5][104].\nEvolutionary design of neural networks eliminates the tedious trial and error work\nof manually finding an optimal network [5][15][19][39][94-95][103]. The\nadvantage of automatic design over manual design becomes clearer as the\ncomplexity of ANN increases. Evolutionary Artificial Neural Networks (EANN)\nprovide a general framework for investigating various aspects of simulated\nevolution and learning. In EANN's, evolution can be introduced at various levels.\nAt the lowest, it can be introduced into weight training, where ANN weights are\nevolved. At the next level, it can be introduced into neural network architecture\nadaptation, where the architecture (number of hidden layers, the number of hidden\nneurons and node transfer functions) is evolved. At the highest level, it can be\nintroduced into the learning mechanism.\n\n3.1 Meta Learning Evolutionary Artificial Neural Networks\nOne major problem with evolutionary algorithms is their inefficiency in fine\ntuning local search, although they are good at global searches [7]. The efficiency\nof evolutionary training can be improved significantly by incorporating a local\nsearch procedure into the evolution. Evolutionary algorithms are used first to\nlocate a good region in the space and then a local search procedure is used to find\na near optimal solution in this region. It is interesting to think of finding good\ninitial weights as locating a good region in the space. Defining that the basin of\nattraction of a local minimum is composed of all the points, sets of weights in this\ncase, which can converge to the local minimum through a local search algorithm,\nthen a global minimum can easily be found by the local search algorithm if the\nevolutionary algorithm can locate any point, that is, a set of initial weights, in the\nbasin of attraction of the global minimum. In Figure 5, G1 and G2 could be\nconsidered to be the initial weights as located by the evolutionary search, and WA\nand WB the corresponding final weights fine-tuned by the meta-learning technique.\nFigure 6 illustrates the architecture of the Meta Learning Evolutionary Artificial\nNeural Network (MLEANN) and the general interaction mechanism with the\nlearning mechanism evolving at the highest level on the slowest time scale [5]. All\nthe randomly generated architectures of the initial population are trained by\ndifferent learning algorithms (backpropagation - BP, scaled conjugate gradient SCG, quasi-Newton algorithm - QNA and Levenberg Marquardt - LM) and\nevolved in a parallel environment. Parameters controlling the performance of the\nlearning algorithm will be adapted (for example, the learning rate and the\nmomentum for BP) according to the problem. The Architecture of the\nchromosome is presented in Figure 7. Figure 8 depicts the MLEANN algorithm.\n\n\fTraining error\n\nG2\nG1\n\nA\nWG1\n\nWA\n\nB\nWB\n\nWG2\n\nFigure 5. Fine tuning of weights using meta-learning\n\nEvolutionary search of learning algorithms and its parameters\nBackpropagation\n\nScaled Conjugate\nGradient\n\nQuasi- Newton\n\nLevenberg\nMarquardt\n\nEvolutionary search of architectures and node transfer functions\n\nEvolutionary search of connection weights\n\nFigure 6. Interaction of various evolutionary search mechanisms\nFrom the point of view of engineering, the decision about the level of evolution\ndepends on what kind of prior knowledge is available. If there is more prior\nknowledge about EANN's architectures than that about their learning rules or a\nparticular class of architectures is pursued, it is better to implement the evolution\nof architectures at the highest level because such knowledge can be used to reduce\nthe search space and the lower levels of evolution of learning algorithms can be\nmore biased towards this kind of architecture. On the other hand, the evolution of\nlearning algorithms should be at the highest level if there is more prior knowledge\navailable or a special interest in certain types of learning algorithm. Connection\nweights may be represented as binary strings represented by a certain length. The\nwhole network is encoded by concatenation of all the connection weights of the\nnetwork in the chromosome. A heuristic concerning the order of the concatenation\nis to put connection weights of the same node together.\n\n\fEvolutionary architecture adaptation can be achieved by constructive [18][41] and\ndestructive [88] algorithms. The former, which add complexity to the network\nstarting from a very simple architecture until the entire network is able to learn the\ntask. The latter start with large architectures and remove nodes and\ninterconnections until the ANN is no longer able to perform its task. Then the last\nremoval is undone. Direct encoding of the architecture makes the mapping simple\nbut often suffers problems like scalability and implementation of crossover\noperators. For an optimal network, the required node transfer function (such as\nGaussian, sigmoidal) could be formulated as a global search problem, which is\nevolved simultaneously with the search for architectures. For the neural network\nto be fully optimal, the learning algorithms have to be adapted dynamically\naccording to the architecture and the given problem. Deciding the learning rate\nand momentum can be considered as the first attempt at adaptation of the local\nsearch technique (learning algorithm). The best learning algorithm will again be\ndecided by the evolutionary search mechanism. Genotypes of the learning\nparameters of the different learning algorithms can be encoded as real-valued\ncoefficients [15].\nIn Figure 7, for every learning algorithm parameter (LR2), there is the evolution of\narchitectures (AR1, AR2.....AR7....) that proceeds on a faster time scale in an\nenvironment decided by the learning algorithm. For each architecture (AR3), the\nevolution of connection weights (WT1, WT2.....WT5....) proceeds at a faster time\nscale in an environment decided by the problem, the learning algorithm and the\narchitecture.\nLR1\n\nLR2\n\nLR3\n\nLR4\n\nLR5\n\nLR6\n\nLR7\n\nLR8\n\nLR9\n\nparameters of learning\nalgorithm\nAR1\n\nAR2\n\nAR3\n\nAR4\n\nAR5\n\nAR6\n\nAR7\n\nneural network\narchitectures\nWT1\n\nWT2\n\nWT3\n\nWT4\n\nWT5\n\ninitial weights\n\nFigure 7. MLEANN chromosome architecture\nThe MLEANN approach has been applied for modelling three benchmark chaotic\ntime series and the empirical results on test data sets clearly demonstrate the\nimportance and efficacy of the meta learning approach for designing evolutionary\nneural networks [5][7]. Test results also demonstrate that MLEANN could\noutperform a Takagi-Sugeno [90] and Mamdani [68] fuzzy inference system\nwhich is learned using neural network learning methods.\n\n\f1. Set t=0 and randomly generate an initial population of neural\nnetworks with architectures, node transfer functions and connection\nweights assigned at random.\n2. In a parallel mode, evaluate fitness of each ANN using\nBP/SCG/QNA and LM\n3. Based on fitness value, select parents for reproduction\n4. Apply mutation to the parents and produce offspring (s) for next\ngeneration. Refill the population back to the defined size.\n5. Repeat step 2\n6. STOP when the required solution is found or number of iterations\nhas reached the required limit.\nFigure 8. MLEANN algorithm\nThe MLEANN approach was compared with the Cutting Angle Method (CAM)\nwhich is a deterministic global optimization technique [21]. This technique is\nbased on theoretical results in abstract convexity [16]. It systematically explores\nthe whole domain by calculating the values of the objective function f(x) at certain\npoints which are selected in such a way that the algorithm does not return to\nunpromising regions where function values are high. The new point is chosen\nwhere the objective function can potentially take the lowest value. The function is\nassumed to be Lipschitz, and the value of the potential minima is calculated based\non both the distance to the neighbouring points and the function values at these\npoints. This process can be seen as constructing the piecewise linear lower\napproximation of the objective function f(x). With the addition of new points, the\napproximation hk(x) becomes closer to the objective function, and the global\nminimum of the approximating function x* converges to the global minimum of\nthe objective function. The lower approximation, the auxiliary function hk(x), is\ncalled the saw-tooth cover of f. The MLEANN approach performed marginally\nbetter in terms of the lowest error on test sets. However CAM performed much\nfaster when compared to the population-based MLEANN approach.\nSelection of the architecture of a network (the number of layers, hidden neurons,\nactivation functions and connection weights) and the correct learning algorithm is\na tedious task for designing an optimal artificial neural network. Moreover, for\ncritical applications and hardware implementations optimal design often becomes\na necessity. Empirical results are promising and similar approach could be used\nfor optimizing recurrent neural networks and other connectionist models. For the\nevolutionary search of architectures, it will be interesting to model as co-evolving\n[34] sub-networks instead of evolving the whole network. Further, it will be\nworthwhile to explore the whole population information of the final generation for\ndeciding the best solution [103]. A fixed chromosome structure (direct encoding\n\n\ftechnique) was used to represent the connection weights, architecture, learning\nalgorithms and its parameters. As the size of the network increases, the\nchromosome size grows. Moreover, implementation of crossover operator is often\ndifficult due to production of non-functional offsprings. Parameterized encoding\novercomes the problems with direct encoding but the search of architectures is\nrestricted to layers. In the grammatical encoding a re-written grammar is encoded.\nThe success will depend on the coding of grammar (rules). Cellular configuration\nmight be helpful to explore the architecture of neural networks more efficiently.\nGutierrez et al [45] has shown that their cellular automata technique performed\nbetter than direct coding.\nAdaptation of fuzzy\ninference system\n\nPerformance\nmeasure\n\nmembership functions\n+\n\nif-then rules\n-\n\nProcess\n\nfuzzy operators\nKnowledge base\nFuzzy Inference System\n\nFigure 9. Architecture of adaptive fuzzy inference systems\n\n4. Adaptation of Fuzzy Inference Systems\nA conventional fuzzy controller makes use of a model of the expert who is in a\nposition to specify the most important properties of the process. Expert knowledge\nis often the main source for designing Fuzzy Inference Systems (FIS) [81]. Figure\n9 shows the architecture of the fuzzy inference system controlling a process.\nAccording to the performance measure of the problem environment, the\nmembership functions, the knowledge base and the inference mechanism are to be\nadapted. Several research works continue to explore the adaptation of fuzzy\ninference systems [32][49][66-67][84][99]. These include the adaptation of\nmembership functions, rule bases and the aggregation operators. They include but\nare not limited to:\n\u2022\n\nThe self-organizing process controller by Procyk et al [83] which considered\nthe issue of rule generation and adaptation.\n\n\f\u2022\n\nThe gradient descent and its variants which have been applied to fine-tune\nthe parameters of the input and output membership functions [100].\n\n\u2022\n\nPruning the quantity and adapting the shape of input/output membership\nfunctions [101].\n\n\u2022\n\nTools to identify the structure of fuzzy models [89].\n\n\u2022\n\nFuzzy discretization and clustering techniques [105].\n\n\u2022\n\nIn most cases the inference of the fuzzy rules is carried out using the 'min'\nand 'max' operators for fuzzy intersection and union. If the T-norm and Tconorm operators are parameterized then the gradient descent technique\ncould be used in a supervised learning environment to fine-tune the fuzzy\noperators.\n\nThe antecedent of the fuzzy rule defines a local fuzzy region, while the consequent\ndescribes the behavior within the region via various constituents. The consequent\nconstituent can be a membership function (Mamdani model) [68] or a linear\nequation (first order Takagi-Sugeno model) [90]. An easiest way to formulate the\ninitial rule base is the grid partition method, as shown in Figure 10 where the input\nspace is divided into multi-dimensional partitions and then assign actions to each\nof the partitions. The consequent parts of the rule represent the actions associated\nwith each partition. It is evident that the MFs and the number of rules are tightly\nrelated to the partitioning and it encounters problems when we have a moderately\nlarge number of input variables (curse of dimensionality). Tree and scatter\npartition relieves the problem of exponential increase in the number of rules but\northogonality is often a major problem associated with these partitioning\ntechniques [54].\n\nFigure 10. Grid partition: A simple if-then rule will appear as \"If input-1 is\nmedium and input 2 is large then rule R8 is fired\".\n\n\f5. Evolutionary Fuzzy Systems\nAdaptation of fuzzy inference systems using evolutionary computation techniques\nhas been widely explored [11][32][76][79][85]. The evolutionary search of\nmembership functions, rule base, fuzzy operators progress on different time scales\nto adapt the fuzzy inference system according to the problem environment. Figure\n11 illustrates the general interaction mechanism with the evolutionary search of a\nfuzzy inference system (Mamdani, Takagi -Sugeno etc) evolving at the highest\nlevel on the slowest time scale. For each evolutionary search of fuzzy operators\n(for example, best combination of T-norm, T-conorm and defuzzification\nstrategy), the search for the fuzzy rule base progresses at a faster time scale in an\nenvironment decided by the fuzzy inference system and the problem. In a similar\nmanner, the evolutionary search of membership functions proceeds at a faster time\nscale (for every rule base) in the environment decided by the fuzzy inference\nsystem, fuzzy operators and the problem. The chromosome architecture is\ndepicted in Figure 12.\nSlow\n\nEvolutionary search of fuzzy inference system\nEvolutionary search of fuzzy operators and defuzzification strategies\nEvolutionary search of fuzzy rules\nEvolutionary search of membership functions\n\nFast\n\nFigure 11. Interaction of the different evolutionary search mechanisms in the\nadaptation of fuzzy inference system\nFIS1\n\nFIS2\n\nFIS3\n\nFIS4\n\nFIS5\n\nFIS6\n\nFIS7\n\nFuzzy inference system\n\nOP1\n\nOP2\n\nOP3\n\nOP4\n\nOP5\n\nOP6\n\nFuzzy operators\n\nRule1\n\nRule2\n\nRule3\n\nRule4\n\nRule5\n\nFuzzy rules\nMF1\n\nMF2\n\nMF3\n\nMF4\n\nFuzzy membership functions\n\nFigure 12. Chromosome representation of the adaptive fuzzy inference system\n\n\fThe automatic adaptation of membership functions is popularly known as selftuning. The genome encodes parameters of trapezoidal, triangle, logistic,\nhyperbolic-tangent, Gaussian membership functions and so on [27].\nThe evolutionary search of fuzzy rules can be carried out using three approaches\n[32]. In the first (Michigan approach), the fuzzy knowledge base is adapted as a\nresult of the antagonistic roles of competition and cooperation of fuzzy rules. Each\ngenotype represents a single fuzzy rule and the entire population represents a\nsolution. A classifier rule triggers whenever its condition part matches the current\ninput, in which case, the proposed action is sent to the process to be controlled.\nThe global search algorithm generates new classifier rules based on the rule\nstrengths acquired during the entire process. The fuzzy behavior is created by an\nactivation sequence of mutually-collaborating fuzzy rules. The entire knowledge\nbase is built up by a cooperation of competing multiple fuzzy rules.\nThe second method (Pittsburgh approach) evolves a population of knowledge\nbases rather than individual fuzzy rules. Genetic operators serve to provide a new\ncombination of rules and new rules. In some cases, variable length rule bases are\nused employing modified genetic operators for dealing with these variable length\nand position independent genomes. The disadvantage is the increased complexity\nof the search space and the additional computational burden, especially for online\nlearning.\nThe third method (iterative rule learning approach) is similar to the first, with each\nchromosome representing a single rule, but contrary to the Michigan approach,\nonly the best individual is considered to form part of the solution, the remaining\nchromosomes in the population are discarded. The evolutionary learning process\nbuilds up the complete rule base through an iterative learning process [44].\n\n6. Cooperative Neuro-Fuzzy Systems\nHayashi et al [47] showed that a feedforward neural network could approximate\nany fuzzy-rule-based system and any feedforward neural network may be\napproximated by a rule-based fuzzy inference system [64]. A fusion of artificial\nneural networks and fuzzy inference systems has attracted growing interest\namoung researchers in various scientific and engineering areas due to the growing\nneed for adaptive intelligent systems to solve real world problems\n[2][4][6][8][10][33][43][46][52-54][59][62][66][78][98]. The advantages of a\ncombination of neural networks and fuzzy inference systems are obvious [2829][71]. An analysis reveals that the drawbacks pertaining to these approaches\nseem complementary and therefore, it is natural to consider building an integrated\nsystem combining the concepts. While the learning capability is an advantage\nfrom the viewpoint of a fuzzy inference system, the automatic formation of a\nlinguistic rule base is an advantage from the viewpoint of neural networks. Neural\nnetwork learning techniques could be used to learn the fuzzy inference system in a\ncooperative and an integrated environment. In this Section, three different types of\ncooperative neuro-fuzzy models are presented, namely fuzzy associative\n\n\fmemories, fuzzy rule extraction using self-organizing maps and systems capable\nof learning fuzzy set parameters. Integrated neuro-fuzzy systems are presented in\nSection 7.\nAt the simplest level, a cooperative model can be thought of as a preprocessor\nwherein the ANN learning mechanism determines the fuzzy inference system\nmembership functions or fuzzy rules from the training data. Once the FIS\nparameters are determined, ANN goes to the background.\nKosko's fuzzy associative memories [62], Pedryz's (et al) fuzzy rule extraction\nusing self organizing maps [80] and Nomura's. (et al) systems capable of learning\nof fuzzy set parameters [75] are some good examples of cooperative neuro-fuzzy\nsystems.\n\n6.1 Fuzzy Associative memories\nKosko interprets a fuzzy rule as an association between antecedent and consequent\nparts [62]. If a fuzzy set is seen as a point in the unit hypercube and rules are\nassociations, then it is possible to use neural associative memories to store fuzzy\nrules. A neural associative memory can be represented by its connection matrix.\nAssociative recall is equivalent to multiplying a key factor with this matrix. The\nweights store the correlations between the features of the key, k, and the\ninformation part, i. Due to the restricted capacity of associative memories and\nbecause the combination of multiple connection matrices into a single matrix is\nnot recommended due to severe loss of information, it is necessary to store each\nfuzzy rule in a single FAM. Rules with n conjunctively combined variables in\ntheir antecedents can be represented by n FAMs, where each stores a single rule.\nThe FAMs are completed by aggregating all the individual outputs (maximum\noperator in the case of Mamdani fuzzy system) and a defuzzification component.\nLearning can be incorporated in FAM as learning the weights associated with\nFAMs output or to create FAMs completely by learning. A neural networklearning algorithm determines the rule weights for the fuzzy rules. Such factors\nare often interpreted as the influence of a rule and are multiplied with the rule\noutputs. Rule weights can be replaced equivalently by modifying the membership\nfunctions. However, this could result in a misinterpretation of fuzzy sets and\nidentical linguistic values might be represented differently in different rules.\nKosko suggests a form of adaptive vector quantization technique to learn the\nFAMs. This approach is called differential competitive learning and is very similar\nto the learning in self-organizing maps.\nFigure 13 depicts a cooperative neuro-fuzzy model where the neural network\nlearning mechanism is used to determine the fuzzy rules, parameters of fuzzy sets,\nrule weights and so on. Kosko's adaptive FAM is a cooperative neuro-fuzzy model\nbecause it uses a learning technique to determine the rules and its weights. Its\nmain disadvantage is the weighting of rules. Just because certain rules, do not\nhave much influence does not mean that they are totally unimportant. Hence, the\n\n\freliability of FAMs for certain applications is questionable. But because of their\nimplementation simplicity, they are used in many applications.\nFuzzy rules\nFuzzy Inference system\n\nNeural Network\n\nOutput\n\nData\n\nFuzzy sets\n\nFigure 13. Cooperative neuro-fuzzy model\n\n6.2 Fuzzy Rule Extraction Using Self Organizing Maps\nPedryz et al [80] used self-organizing maps with a planar competition layer to\ncluster training data, and they provide means to interpret the learning results. The\nlearning results show whether two input vectors are similar to each other or belong\nto the same class. However, in the case of high-dimensional input vectors, the\nstructure of the learning problem can rarely be detected in the two dimensional\nmap. Pedryz et al provides a procedure for interpreting the learning results using\nlinguistic variables.\nAfter the learning process, the weight matrix W represents the weight of each\nfeature of the input patterns to the output. Such a matrix defines a map for a single\nfeature only. For each feature of the input patterns, fuzzy sets are specified by a\nlinguistic description, B (one fuzzy set for each variable). They are applied to the\nweight matrix, W, to obtain a number of transformed matrices. Each combination\nof linguistic terms is a possible description of a pattern subset or cluster. To check\na linguistic description, B, for validity, the transformed maps are intersected and a\nmatrix D is obtained. Matrix D determines the compatibility of the learning result\nwith the linguistic description B. D(B) is a fuzzy relation, and d (B) is interpreted as\nthe degree of support of B. By describing D(B) by its \u03b1-cuts, D B subsets of output\n\n\u03b1\n\nnodes, whose degree of membership is at least \u03b1, so that the confidence of all\npatterns, X\u03b1 , belong to the class described by B vanishes with decreasing \u03b1. Each\nB is a valid description of a cluster if D(B) has a non-empty \u03b1-cut D B . If the\n\n\u03b1\n\nfeatures are separated into input and output according to the application\nconsidered, then each B represents a linguistic rule, and by examining each\ncombination of linguistic values, a complete fuzzy rule base can be created. This\nmethod also shows which patterns belong to a fuzzy rule, because they are not\ncontained in any subset, X\u03b1. An important advantage compared to FAMs is that the\n\n\frules are not weighted. The problem is with the determination of the number of\noutput neurons and the \u03b1 values for each learning problem. Compared to FAM,\nsince the form of the membership function determines a crucial role in the\nperformance, the data could be better exploited. Since Kosko's learning procedure\ndoes not take into account the neighborhood relation between the output neurons,\nperfect topological mapping from the input patterns to the output patterns might\nnot be obtained. Thus the FAM learning procedure is more dependent on the\nsequence of the training data than the Pedryz et al procedure.\nPedryz et al initially determine the structure of the feature space and then the\nlinguistic descriptions best matching the learning results, by using the available\nfuzzy partitions obtained. If a large number of patterns fit none of the descriptions,\nthis may be due to an insufficient choice of membership functions and they can be\ndetermined anew. Hence, for learning the fuzzy rules, this approach is preferable\ncompared to FAM [23]. Performance of this method still depends on the learning\nrate and the neighborhood size for weight modification, which is problemdependant and could be determined heuristically. Fuzzy C-means algorithm also\nhas been explored to determine the learning rate and neighborhood size [23][50].\n\n6.3 Systems Capable of Learning Fuzzy Set Parameters\nNomura et al [75] proposed a supervised learning technique to fine-tune the fuzzy\nsets of an existing Sugeno type fuzzy system. Parameterized triangular\nmembership functions were used for the antecedent part of the fuzzy rules. The\nlearning algorithm is a gradient descent procedure that uses an error measure, E,\n(difference between the actual and target outputs) to fine-tune the parameters of\nthe MF. Because the underlying fuzzy system uses neither a defuzzification\nprocedure nor a non-differentiable t-norm to determine the fulfilment of rules, the\ncalculation of the modifications of the MF parameters is trivial. The procedure is\nvery similar to the delta rule for multilayer perceptrons. The learning takes place\nin an offline mode. For the input vector, the resulting error, E, is calculated and,\nbased on that, the consequent parts (a real value) are updated. Then the same\npatterns are propagated again and only the parameters of the MFs are updated.\nThis is done to take the changes in the consequents into account when the\nantecedents are modified. A severe drawback of this approach is that the\nrepresentation of the linguistic values of the input variables depends on the rules\nthey appear in. Initially, identical linguistic terms are represented by identical\nmembership functions. During the learning process, they may be developed\ndifferently, so that identical linguistic terms are represented by different fuzzy\nsets. The proposed approach is applicable only to Sugeno type fuzzy inference\nsystem. Using a similar approach, Miyoshi et al [72] adapted fuzzy T-norm and Tconorm operators while Yager et al adapted the defuzzification operator using a\nsupervised learning algorithm [102].\n\n\f7. Integrated Neuro-Fuzzy Systems\nIn an integrated model, neural network learning algorithms are used to determine\nthe parameters of fuzzy inference systems. Integrated neuro-fuzzy systems share\ndata structures and knowledge representations. A fuzzy inference system can\nutilize human expertise by storing its essential components in a rule base and a\ndatabase, and perform fuzzy reasoning to infer the overall output value. The\nderivation of if-then rules and corresponding membership functions depends\nheavily on the a priori knowledge about the system under consideration.\nHowever, there is no systematic way to transform the experiences of knowledge of\nhuman experts in to the knowledge base of a fuzzy inference system. There is also\na need for the adaptability or some learning algorithms to produce outputs within\nthe required error rate. On the other hand, the neural network learning mechanism\ndoes not rely on human expertise. Due to its homogenous structure, it is difficult\nto extract structured knowledge from either the weights or the configuration of the\nnetwork. The weights of the neural network represent the coefficients of the\nhyper-plane that partition the input space into two regions with different output\nvalues. If this hyper-plane structure can be visualized from the training data the\nsubsequent learning procedures in a neural network can be reduced. However, in\nreality, the a priori knowledge is usually obtained from human experts and it is\nmost appropriate to express the knowledge as a set of fuzzy if-then rules and it is\nvery difficult to encode into an neural network. Table 2 summarizes the\ncomparison between neural networks and the fuzzy inference system [4][6]].\nTable 2. Comparison between neural networks and fuzzy inference systems\nArtificial Neural Networks\n\nFuzzy Inference System\n\nPrior rule-based knowledge cannot\nbe used\nLearning from scratch\n\nPrior rule-base can be incorporated\n\nBlack box\nComplicated learning algorithms\n\nCannot learn (use linguistic\nknowledge)\nInterpretable (if-then rules)\nSimple interpretation and\nimplementation\n\nA common way to apply a learning algorithm to a fuzzy system is to represent it in\na special neural network like architecture. Most of the integrated neuro-fuzzy\nmodels use a partitioning method (discussed in Section 4) to set up the initial rule\nbase and then the learning algorithm is used to fine tune the parameters. However\nthe conventional neural network learning algorithms (gradient descent) cannot be\napplied directly to such a system as the functions used in the inference process are\nusually non differentiable. This problem can be tackled by using differentiable\n\n\ffunctions in the inference system or by not using the standard neural learning\nalgorithm. In Sections 7.1 and 7.2, how to model integrated neuro-fuzzy systems\nimplementing Mamdani and Takagi - Sugeno FIS, is discussed.\n\n7.1 Integrated Neuro-Fuzzy System (Mamdani FIS)\nA Mamdani neuro-fuzzy system uses a supervised learning technique\n(backpropagation learning) to learn the parameters of the membership functions.\nThe detailed function of each layer (as depicted in Figure 14) is as follows:\ny\nLayer 5\nrule inference and defuzzification layer\n\nLayer 4\nrule consequent layer\n\nR\n\n1\n\nR\n\n2\n\nLayer 3\nrule antecedent layer\n\nR\n\n3\n\nLayer 2\n(fuzzification layer)\n\nLayer 1\n(input layer)\n\nx1\n\nx2\n\nFigure 14..Mamdani neuro-fuzzy system\n\u2022\n\nLayer -1(input layer): No computation is done in this layer. Each node,\nwhich corresponds to one input variable, only transmits input values to the\nnext layer directly. The link weight in Layer 1 is unity.\n\u2022 Layer-2 (fuzzification layer): Each node corresponds to one linguistic\nlabel (such as excellent, good) to one of the input variables in Layer 1. In\nother words, the output link represent the membership value, which\nspecifies the degree to which an input value belongs to a fuzzy set, is\ncalculated in layer 2. The final shapes of the MFs are fine tuned during\nnetwork learning.\n\u2022\n\nLayer-3 (rule antecedent layer): A node represents the antecedent part of\na rule. Usually a T-norm operator is used. The output of a Layer 3 node\nrepresents the firing strength of the corresponding fuzzy rule.\n\n\f\u2022\n\nLayer-4 (rule consequent layer): This node basically has two tasks: to\ncombine the incoming rule antecedents and determine the degree to which\nthey belong to the output linguistic label (for example, high, medium, low).\nThe number of nodes in this layer are equal to the number of rules.\n\n\u2022\n\nLayer-5 (Combination and defuzzification layer): This node combines all\nthe rules' consequents (normally using a T-conorm operator) and finally\ncomputes the crisp output after defuzzification.\n\n7.2 Integrated Neuro-fuzzy system (Takagi-Sugeno FIS)\nTakagi Sugeno neuro-fuzzy systems make use of a mixture of backpropagation to\nlearn the membership functions and least mean square estimation to determine the\ncoefficients of the linear combinations in the rule consequents. A step in the\nlearning procedure has two parts: in the first, the input patterns are propagated,\nand the optimal conclusion parameters are estimated by an iterative least mean\nsquare procedure, while the antecedent parameters (membership functions) are\nassumed to be fixed for the current cycle through the training set; in the second,\nthe patterns are propagated again, and in this epoch, backpropagation is used to\nmodify the antecedent parameters, while the conclusion parameters remain fixed.\nThis procedure is then iterated. The detailed functioning of each layer (as depicted\nin Figure 15) is as follows:\n\u2022\n\u2022\n\nLayers 1,2 and 3 functions the same way as Mamdani FIS.\nLayer 4 (rule strength normalization): Every node in this layer calculates\nthe ratio of the i-th rule's firing strength to the sum of all rules' firing\nstrength\nwi =\n\n\u2022\n\nwi\n, i = 1,2.... .\nw1 + w 2\n\n(3)\n\nLayer-5 (rule consequent layer): Every node i in this layer has a node\nfunction\nwi f i = wi ( pi x 1 + qi x 2 + ri ) ,\n\n(4)\n\nwhere wi is the output of layer 4, and {pi , qi , ri } is the parameter set. A\nwell-established way is to determine the consequent parameters using the\nleast means squares algorithm.\n\u2022\n\nLayer-6 (rule inference layer) The single node in this layer computes the\noverall output as the summation of all incoming signals:\n\u2211w f\nOverall output = \u2211 wi f i = i i i\n(5)\n\u2211 iwi\ni\n\n\fy\nLayer 6\nrule inference layer\n\nLayer 5\nrule consequent layer\n\nx1\n\nx2\nLayer 4\nrule strength normalization\n\nR\n\n1\n\nR\n\n2\n\nR\n\n3\n\nLayer 3\nrule antecedent layer\n\nLayer 2\n(fuzzification layer)\n\nLayer 1\n(input layer)\n\nx1\n\nx2\n\nFigure 15. Takagi-Sugeno neuro-fuzzy system\nSome of the integrated neuro-fuzzy systems are GARIC [22], FALCON [65],\nANFIS [54], NEFCON, NEFCLASS, NEFPROX [74], FUN [91], SONFIN[55],\nFINEST[77][93], EFuNN [59-60] and EvoNF [1] [12]. A detailed review of the\ndifferent integrated neuro-fuzzy models is presented in [6].\nIn ANFIS the adaptation (learning) process is only concerned with parameter level\nadaptation within fixed structures. For large-scale problems, it will be too\ncomplicated to determine the optimal premise-consequent structures, rule numbers\netc. The structure of ANFIS ensures that each linguistic term is represented by\nonly one fuzzy set. However the learning procedure of ANFIS does not provide\nthe means to apply constraints that restrict the kind of modifications applied to the\nmembership functions. When using Gaussian membership functions, operationally\nANFIS can be compared with a radial basis function network.\nNEFCON make use of a reinforcement type of learning algorithm for learning the\nrule base (structure learning) and a fuzzy backpropagation algorithm for learning\nthe fuzzy sets (parameter learning). NEFCON system is capable of incorporating\nprior knowledge as well as learning from scratch. However the performance of the\nsystem will very much depend on heuristic factors like learning rate, error\nmeasure etc.\n\n\fFINEST provides a mechanism based on the improved generalized modus ponens\nfor fine tuning of fuzzy predicates and combination functions and tuning of an\nimplication function. Parameterization of the inference procedure is very much\nessential for proper application of the tuning algorithm.\nSONFIN is is adaptable to the users specification of required accuracy.\nPrecondition parameters are tuned by backpropagation algorithm and consequent\nparameters by least mean squares or recursive least squares algorithms very\nsimilar to ANFIS.\nEFuNN implements a Mamdani type of fuzzy rule base, based on a dynamic\nstructure (creating and deleting strategy), and single rule inference, established on\nthe winner-takes all rule for the rule node activation, with a one-pass training,\ninstance based learning and reasoning. dmEFuNN is an improved version of the\nEFuNN capable of implementing Takagi-Sugeno fuzzy system, using several (m)\nof the highest activated rule nodes instead of one. The rule node aggregation is\nachieved by a C-means clustering algorithm.\nFUN system is initialized by specifying a fixed number of rules and a fixed\nnumber of initial fuzzy sets for each variable and the network learns through a\nstochastic procedure that randomly changes parameters of membership functions\nand connections within the network structure Since no formal neural network\nlearning technique is used it is questionable to call FUN a neuro-fuzzy system.\nSugeno-type fuzzy systems are high performers (less Root Mean Squared ErrorRMSE) but often requires complicated learning procedures and are\ncomputationally expensive. However, Mamdani-type fuzzy systems can be\nmodeled using faster heuristics but with a compromise on performance (high\nRMSE). There is always a compromise between performance and computational\ntime. The data acquisition and preprocessing training data are also quite important\nfor the success of neuro-fuzzy systems.\nThe success with integrating neural network and fuzzy logic and knowing their\nstrengths and weaknesses, can be used to construct better neuro-fuzzy systems to\nmitigate the limitations and take advantage of the opportunities to produce more\npowerful hybrids than those that could be built with stand alone systems. As a\nguideline, for neuro-fuzzy systems to be at the top of the ladder, some of the major\nrequirements are: fast learning (memory based - efficient storage and retrieval\ncapacities), on-line adaptability (accommodating new features like inputs, outputs,\nnodes, connections), a global error rate and inexpensive computations (fast\nperformance). As the problem become more complicated manual definition of\nneuro-fuzzy architecture/parameters becomes complicated. Especially for tasks\nrequiring an optimal FIS, global optimization approach might be the best solution.\nIn Section 8, EvoNF: a frame work for optimization of FIS using evolutionary\nalgorithms and neural network learning technique is presented. EvoNF approach\ncould be considered as a meta learning approach of evolutionary fuzzy systems.\n\n\f8. Neuro-Fuzzy-Evolutionary (EvoNF) Systems\nIn an integrated neuro-fuzzy model, there is no guarantee that the neural networklearning algorithm will converge and the tuning of fuzzy inference system be\nsuccessful. Optimization of fuzzy inference systems could be further improved\nusing a meta-heuristic approach combining neural network learning algorithm and\nevolutionary algorithms. The proposed technique could be considered as a\nmethodology to integrate neural networks, fuzzy inference systems and\nevolutionary search procedures [1] [3] [12].\nSlow\n\nGlobal search of fuzzy inference system\n(Mamdani FIS, Takagi Sugeno FIS etc)\n\nGlobal search of learning parameters\nGlobal search of inference mechanisms\n(optimal T-norm and T-conorm parameters)\n\nGlobal search of fuzzy rules (architectures)\n(antecedents and consequents)\n\nGlobal search of membership functions\n\nFast\n\n(optimal quantity and shape)\n\nTime scale\n\nFigure 16. General computational framework for EvoNF\nThe EvoNF framework could adapt to Mamdani, Takagi-Sugeno or other fuzzy\ninference systems. The architecture and the evolving mechanism could be\nconsidered as a general framework for adaptive fuzzy systems, that is a fuzzy\nmodel that can change membership functions (quantity and shape), rule base\n(architecture), fuzzy operators and learning parameters according to different\nenvironments without human intervention. Solving multi-objective scientific and\nengineering problems is, generally, a very difficult goal. In these particular\noptimization problems, the objectives often conflict across a high-dimension\nproblem space and may also require extensive computational resources. Proposed\nhere is an evolutionary search procedure wherein the membership functions, rule\nbase (architecture), fuzzy inference mechanism (T-norm and T-conorm operators),\nlearning parameters and finally the type of inference system (Mamdani, TakagiSugeno etc.) are adapted according to the environment. Figure 15 illustrates the\ninteraction of various evolutionary search procedures and shows that for every\nfuzzy inference system, there exists a global search of learning algorithm\nparameters, an inference mechanism, a rule base and membership functions in an\nenvironment decided by the problem. Thus, the evolution of the fuzzy inference\nsystem evolves at the slowest time scale while the evolution of the quantity and\ntype of membership functions evolves at the fastest rate. The function of the other\nlayers could be derived similarly.\n\n\fThe hierarchy of the different adaptation layers (procedures) relies on prior\nknowledge. For example, if there is more prior knowledge about the architecture\nthan the inference mechanism then it is better to implement the architecture at a\nhigher level. If a particular fuzzy inference system best suits the problem, the\ncomputational task could be reduced by minimizing the search space.\nA typical chromosome of EvoNF would be as shown in Figure 17 and the detailed\nmodelling process could be obtained from [1][12]. The chromosome architecture\nis very similar to to the chromosome structure mentioned in Figures 7 and 12.\nFIS1\n\nFIS2\n\nFIS3\n\nFIS4\n\nFIS5\n\nFIS6\n\nFIS7\n\nLR1\n\nLR2\n\nLR3\n\nLR4\n\nLR5\n\nLR6\n\nLR7\n\nFIS8\n\nFuzzy inference system\n\nParameters of learning algorithm\nOP1\n\nOP2\n\nOP3\n\nOP4\n\nOP5\n\nOP6\n\nFuzzy operators\n\nRule1\n\nRule2\n\nRule3\n\nRule4\n\nRule5\n\nFuzzy rules\nMF1\n\nMF2\n\nMF3\n\nMF4\n\nFuzzy membership functions\n\nFigure 17. Chromosome structure of the EvoNF model\nWe have applied the proposed technique to the three well known chaotic time\nseries. Fitness value is calculated based on the RMSE achieved on the test set. We\nhave considered the best-evolved EvoNF model as the best individual of the last\ngeneration. We also explored different learning methods combining evolutionary\nlearning and gradient descent techniques and the importance of tuning of different\nparameters. To reduce the computational complexity of the hierarchical search\nprocedure, we reduced the search space by incorporating some priori knowledge.\nThe genotypes were represented by real coding using floating-point numbers and\nthe initial populations were randomly created. For all the three time series\nconsidered, EvoNF gave the best results on training and test sets [1] when\ncompared to other integrated neuro-fuzzy models. Our experiments using the three\ndifferent learning strategies also reveal the importance of fine-tuning the global\nsearch method using a local search method [3]. Figure 18 illustrates the\ncomparison of EvoNF model with different integrated neuro-fuzzy models for\npredicting the Mackey Glass time series [1]. In Figure 18, test set RMSE values\nare given for each neuro-fuzzy model considered and an artificial neural network\ntrained using BP.\n\n\fFigure 18. Comparison of EvoNF and some popular neuro-fuzzy models\n\n9. Fuzzy Evolutionary Algorithms\nEvolutionary algorithms are relatively easy to implement and, in general, their\nperformance tends to be rather satisfactory in comparison with the small amount\nof knowledge about the problem they need in order to work. However, their\nsuccess relies directly on the carefull selection of algorithm parameters, fitness\nfunction and so on. The use of fuzzy logic to translate and improve heuristic rules\nhas also been applied to manage the resource of evolutionary algorithms such as\npopulation size and selection pressure as the algorithm greedily explores and\nexploits the search space [48]. The technique proposed by Lee [63] to perform a\nrun-time tuning of population size and reproduction operators based on the fitness\nmeasures has shown large improvements in the computational run-time efficiency\nof the evolutionary search process. The fuzzy controller takes the inputs\naverage fitness worst fitness\n,\n, \u2206 best fitness\nbest fitness\naverage fitness\n\nand gives \u2206population size, \u2206crossover rate and \u2206mutation rate to control the\nevolutionary algorithm parameters. The ranges of the parameter changes are also\nlimited to remain within certain bandwidths. This technique could improve not\nonly the search efficiency and convergence but also sometimes could avoid\npremature convergence due to lack of diversity in the population.\nAs mentioned in Section 5, the two ingredients of soft computing, evolutionary\ncomputation and fuzzy inference systems, could be integrated in a way that makes\nthem benefit from one another.\n\n\f10. Soft Computing and Probabilistic Reasoning\nA common feature of soft computing technology and the probabilistic reasoning\nsystem is their depature from classical reasoning and modeling approaches which\nare highly based on analytical models, crisp logic and deterministic search. In the\nprobabilistic modeling process, risk means the uncertainty for which the\nprobability distribution is known. The probabilistic models are used for protection\nagainst adverse uncertainty and exploitation of propitious uncertainty.\nIn a probabilistic neural network (Bayesian learning) probability is used to\nrepresent uncertainty about the relationship being learned. Before any data is seen\nthe prior opinions about what the true relationship might be can be expressed in a\nprobability distribution over the network weights that define this relationship.\nAfter a look at the data, revised opinions are captured by a posterior distribution\nover network weights. Network weights that seemed plausible before, but which\ndo not match the data very well, are now seen as being much less likely, while the\nprobability for values of the weights that do fit the data well have increased.\nTypically, the purpose of training is to make predictions for future cases in which\nonly the inputs to the network are known. The result of conventional network\ntraining is a single set of weights that can be used to make such predictions.\nSeveral research work has exposed the complementary features of probabilistic\nreasoning and fuzzy theory [26]. The development of the theory of belief of a\nfuzzy event\nby Smets [87] helped to establish the orthogonality and\ncomplementarity between probabilistic and possibilistic methods.\n\n11. Conclusions\nIt is predicted that, in the 21st century, the fundamental source of wealth will be\nknowledge and communication rather than natural resources and physical labour.\nWith the exponential growth of information and complexity in this world,\nintelligent systems are needed that could learn from data in a continuous,\nincremental way, and grow as they operate, update their knowledge and refine the\nmodel through interaction with the environment. The intelligence of such systems\ncould be further improved if the adaptation process could learn from successes and\nmistakes and that knowledge be applied to new problems.\nThis chapter has presented some of the architectures and perspectives of hybrid\nintelligent systems involving neural networks, fuzzy inference systems,\nevolutionary computation and probabilistic reasoning. The hybrid soft computing\napproach has many important practical applications in science, technology,\nbusiness and commercial. Compared to the individual constituents (NN, FIS, EC\nans PR) hybrid soft computing frameworks are relatively young. As the strengths\nand weakness of different hybrid architectures are understood, it will be possible\nto use them more efficiently to solve real world problems.\n\n\fThe integration of different intelligent technologies is the most exciting fruit of\nmodern artificial intelligence and is an active area of research. While James\nBezdek [24] defines intelligent systems in a frame called computational\nintelligence, Lotfi Zadeh [108] explains the same by using the soft computing\nframework. Integration issues range from different techniques and theories of\ncomputation to problems of exactly how best to implement hybrid systems. Like\nmost biological systems which can adapt to any environment, adaptable intelligent\nsystems are required to tackle future complex problems involving huge data\nvolume. Most of the existing hybrid soft computing frameworks rely on several\nuser specified network parameters. For the system to be fully adaptable,\nperformance should not be heavily dependant on user-specified parameters.\nFor optimizing neural networks and fuzzy inference systems, there is perhaps no\nbetter algorithm than evolutionary algorithms. However, the real success in\nmodeling such systems will directly depend on the genotype representation of the\ndifferent layers. The population-based collective learning process, self-adaptation,\nand robustness are some of the key features of evolutionary algorithms when\ncompared to other global optimization techniques. Evolutionary algorithms attract\nconsiderable computational effort especially for problems involving complexity\nand huge data volume. Fortunately, evolutionary algorithms work with a\npopulation of independent solutions, which makes it easy to distribute the\ncomputational load among several processors.\n\nAcknowledgements\nAuthor is grateful to Professor Lakhmi Jain (University of South Australia,\nAdelaide) and the three referees for the technical comments, which improved the\nclarity of this chapter.\n\nReferences\n[1]\n\n[2]\n\n[3]\n[4]\n\nAbraham A., EvoNF: A Framework for Optimization of Fuzzy Inference\nSystems Using Neural Network Learning and Evolutionary Computation,\n2002 IEEE International Symposium on Intelligent Control (ISIC'02),\nCanada, IEEE Press, 2002.\nAbraham A., Beyond Neuro-Fuzzy Systems: Reviews, Prospects,\nPerspectives and Directions, Seventh International Mendel Conference on\nSoft Computing, Brno, MENDEL 2001, Matousek Radek et al (Eds.), pp.\n376-372, 2001.\nAbraham A., How Important is Meta-Learning in Evolutionary Fuzzy\nSystems?, In Proceedings of Sixth International Conference on Cognitive\nand Neural Systems, ICCNS 2002, Boston University Press, USA, 2002.\nAbraham A., It is time to Fuzzify Neural Networks, Intelligent Multimedia,\nComputing and Communications: Technologies and Applications of the\n\n\f[5]\n[6]\n\n[7]\n\n[8]\n[9]\n\n[10]\n\n[11]\n\n[12]\n\n[13]\n\n[14]\n\n[15]\n\n[16]\n\nFuture, John Wiley & Sons Inc., Syed M.R. and Baiocchi O.R. (Eds.), pp.\n253-263, 2001.\nAbraham A., Meta-Learning Evolutionary Artificial Neural Networks,\nElsevier Science, Neurocomputing Journal, Netherlands, 2002.\nAbraham A., Neuro-Fuzzy Systems: State-of-the-Art Modeling Techniques,\nConnectionist Models of Neurons, Learning Processes, and Artificial\nIntelligence, LNCS 2084, Mira J. and Prieto A. (Eds.), Springer-Verlag\nGermany, pp. 269-276, 2001.\nAbraham A., Optimization of Evolutionary Neural Networks Using Hybrid\nLearning Algorithms, International Joint Conference on Neural Networks,\n2002 IEEE World Congress on Computational Intelligence, WCCI'02,\nHawaii, IEEE Press, 2002.\nAbraham A. and Nath B., A Neuro-Fuzzy Approach for Forecasting\nElectricity Demand in Victoria, Applied Soft Computing Journal, Elsevier\nScience, Volume 1 (2), pp.127-138, 2001.\nAbraham A. and Nath B., ALEC -An Adaptive Learning Framework for\nOptimizing Artificial Neural Networks, Computational Science, LNCS\n2074, Alexandrov V.N. et al (Eds.), Springer Verlag Germany, pp. 171-180,\n2001.\nAbraham A. and Nath B., Designing Optimal Neuro-Fuzzy Systems for\nIntelligent Control, In proceedings of the Sixth International Conference on\nControl Automation Robotics Computer Vision (ICARCV 2000), CDROM Proceeding, Wang J.L. (Ed.), ISBN 9810434456, Singapore, 2000.\nAbraham A. and Nath B., Evolutionary Design of Fuzzy Control Systems An Hybrid Approach, The Sixth International Conference on Control,\nAutomation, Robotics and Vision, (ICARCV 2000), CD-ROM Proceeding,\nWang J.L. (Ed.), ISBN 9810434456, Singapore, 2000.\nAbraham A. and Nath B., Evolutionary Design of Neuro-Fuzzy Systems - A\nGeneric Framework, In Proceedings of The 4-th Japan-Australia Joint\nWorkshop on Intelligent and Evolutionary Systems, Namatame A. et al\n(Eds.), Japan, pp. 106-113, 2000.\nAbraham A. and Nath B., Failure Prediction of Critical Electronic Systems\nin Power Plants Using Artificial Neural Networks, In Proceedings of First\nInternational Power and Energy Conference, CD-ROM Proceeding, Isreb\nM. (Ed.), ISBN 0732620945, Australia, 1999.\nAbraham A. and Nath B., IT Impact On New Millennium Manufacturing, In\nProceedings of 5th International Conference on Computer Integrated\nManufacturing, Computer Integrated Manufacturing, Singh J. , Lew S. C.\nand Gay R. (Eds.), Singapore, pp. 321-332, 2000.\nAbraham A. and Nath B., Optimal Design of Neural Nets Using Hybrid\nAlgorithms, In proceedings of 6th Pacific Rim International Conference on\nArtificial Intelligence, LNCS 1886, Mizoguchi R. and Slaney J.K. (Eds.),\nSpringer Verlag, Germany, pp. 510-520, 2000.\nAndramonov M., Rubinov A. and Glover B., Cutting Angle Methods in\nGlobal Optimization, Applied Mathematics Letters, 12, pp. 95-100, 1999.\n\n\f[17] Auer P., Herbster M. and Warmuth M., Exponentially Many Local Minima\nfor Single Neurons, Advances in Neural Information Processing Systems,\nTouretzky D. et al (Eds.), MIT Press, USA, Vol 8, pp. 316-322, 1996.\n[18] Baffles P.T. and Zelle J.M., Growing layers of Perceptrons: Introducing the\nExentron Algorithm, Proceedings on the International Joint Conference on\nNeural Networks, Vol 2, pp. 392-397, 1992.\n[19] Baxter J., The Evolution of Learning Algorithms for Artificial Neural\nNetworks, Complex systems, IOS press, Amsterdam, pp. 313-326, 1992.\n[20] Bayes T., An Essay Towards Solving a Problem in the Doctrine of Chances,\nPhilosophical Transactions of the Royal Society of London, 53: pp. 370418, 1763.\n[21] Beliakov G. and Abraham A., Global Optimization of Neural Networks\nUsing a Deterministic Hybrid Approach, Hybrid Information Systems,\nAbraham A. and Koeppen M. (Eds.), Physica-Verlag Germany, pp. 79-92,\n2002.\n[22] Berenji H.R. and Khedkar P., Learning and Tuning Fuzzy Logic Controllers\nthrough Reinforcements, IEEE Transactions on Neural Networks, Vol (3),\npp. 724-740, 1992.\n[23] Bezdek J.C. and Pal S.K., Fuzzy Models for Pattern Recognition, IEEE\nPress, New York, 1992.\n[24] Bezdek J.C., Computational Intelligence Defined\u2013By Everyone!,\nComputational Intelligence: Soft Computing and Fuzzy-Neuro Integration\nwith Applications, Okay Kaynal et al (Eds.), Springer Verlag, Germany,\n1996.\n[25] Bishop C.M., Neural Networks for Pattern Recognition, Oxford Press,\n1995.\n[26] Bonissone P.P., Approximate Reasoning Systems: A Personal Perspective,\nIn Proceedings of the American Association of Artificial Intelligence\n(AAAI'91), California, pp. 923-929, 1991.\n[27] Bonissone P.P., Khedkar P.S., and Chen Y., Genetic Algorithms for\nAutomated Tuning of Fuzzy Controllers: A Train Handling Application. In\nProceedings of the Fifth IEEE International Conference on Fuzzy Systems\n(FUZZ-IEEE'96), Vol. 1, pp. 674-680, 1996.\n[28] Buckley J.J. and Feuring T., Fuzzy and Neural: Interactions and\nApplications, Studies in Fuzziness and Soft Computing, Physica Verlag,\nHeidelberg, Germany, 1999.\n[29] Bunke H. and Kandel A., Neuro-Fuzzy Pattern Recognition, World\nScientific Publishing Company, Singapore, 2000.\n[30] Cherkassky V., Fuzzy Inference Systems: A Critical Review,\nComputational Intelligence: Soft Computing and Fuzzy-Neuro Integration\nwith Applications, Kayak O. et al (Eds.), Springer, pp.177-197, 1998.\n[31] Chong, E.K.P. and Zak S.H., An Introduction to Optimization. John Wiley\nand Sons Inc. New York, 1996.\n[32] Cord\u00f3n O., Herrera F., Hoffmann F., and Magdalena L., Genetic Fuzzy\nSystems: Evolutionary Tuning and Learning of Fuzzy Knowledge Bases,\nWorld Scientific Publishing Company, Singapore, 2001.\n\n\f[33] Czogala E. and Leski J., Fuzzy and Neuro-Fuzzy Intelligent Systems,\nStudies in Fuzziness and Soft Computing, Springer Verlag, Germany, 2000.\n[34] Darwen P.J., Co-evolutionary Learning by Automatic Modularisation with\nSpeciation, PhD Thesis, University of New South Wales, Australia, 1996.\n[35] Dasgupta D. (Ed.), Artificial Immune Systems and Their Applications,\nPublisher: Springer-Verlag, Berlin, January 1999.\n[36] Dempster A.P., Upper and Lower Probabilities induced by a Multivalued\nMapping, Annals of Mathematical Statistics, Vol. 38, pp. 325-339, 1967.\n[37] Fahlman S.E. and Lebiere C., The Cascade \u2013 Correlation Learning\narchitecture, Advances in Neural Information Processing Systems,\nTourretzky D. (Ed.), Morgan Kaufmann, pp. 524-532, 1990.\n[38] Fine T.L., Feedforward Neural Network Methodology, Springer Verlag,\nNew York, 1999.\n[39] Fogel D.B., Blondie24: Playing at the Edge of AI, Morgan Kaufmann\nPublishers, USA, 2001.\n[40] Fogel D.B., Evolutionary Computation: Towards a New Philosophy of\nMachine Intelligence, 2nd Edition, IEEE Press, 2000.\n[41] Frean M., The Upstart Algorithm: A Method for Constructing and Training\nFeed Forward Neural Networks, Neural computations, Vol. 2, pp.198-209,\n1990.\n[42] Fukuda T. and Shibata M., Fuzzy-Neuro-GA Based Intelligent Robotics, In\nZurada J.M. et al (Eds.), Computational Intelligence Imitating Life, IEEE\nPress, pp. 352-362, 1994.\n[43] Fuller R., Introduction to Neuro-Fuzzy Systems, Studies in Fuzziness and\nSoft Computing, Springer Verlag, Germany, 2000.\n[44] Gonzalez A. and Herrera F., Multi-Stage Genetic Fuzzy Systems Based on\nthe Iterative Rule Learning Approach, Mathware and Soft Computing Vol.\n4, pp. 233-249, 1997.\n[45] Gutierrez G., Isasi P., Molina J.M., Sanchis A. and Galvan I.M.,\nEvolutionary Cellular Configurations for Designing Feedforward Neural\nNetwork Architectures, Connectionist Models of Neurons, Learning\nProcesses, and Artificial Intelligence, Mira J. et al (Eds.), Springer Verlag Germany, LNCS 2084, pp. 514-521, 2001.\n[46] Hayashi I., Nomura H., Yamasaki H. and Wakami N., Construction of\nFuzzy Inference Rules by Neural Network Driven Fuzzy Reasoning and\nNeural Network Driven Fuzzy Reasoning With Learning Functions,\nInternational. Journal of Approximate Reasoning, Vol. 6, pp. 241-266,\n1992.\n[47] Hayashi Y. and Buckley J.J., Approximations Between Fuzzy Expert\nSystems and Neural Networks, International Journal of Approximate\nReasoning, Vol. 10, pp.63-73, 1994.\n[48] Herrera F., Lozano M. and Verdegay J.L., Tackling Fuzzy Genetic\nAlgorithms, Genetic Algorithms in Engineering and Computer Science,\nWinter G. Periaux J., Galan M., Ceusta P. (Eds.), John Wiley and Sons, pp.\n167-189, 1995.\n\n\f[49] Hoffmann F., Soft Computing Techniques for the Design of Mobile Robot\nBehaviors, Journal of Information Sciences, 122 (2-4) pp. 241-258, 2000.\n[50] H\u00f6ppner F., Klawonn F., and Kruse R., Fuzzy-Clusteranalyse,\nComputational Intelligence, Vieweg, Braunschweig, 1996.\n[51] Jacobsen H.A., A Generic Architecture for Hybrid Intelligent Systems, In\nProceedings of The IEEE World Congress on Computational Intelligence\n(FUZZ IEEE), USA, Vol. 1, pp. 709 \u2013714, 1998.\n[52] Jain L.C. and Jain R.K. (Eds.), Hybrid Intelligent Engineering Systems,\nWorld Scientific Publishing Company, Singapore, 1997.\n[53] Jain L.C. and Martin N.M. (Eds.), Fusion of Neural Networks, Fuzzy Logic\nand Evolutionary Computing and their Applications, CRC Press USA,\n1999.\n[54] Jang J.S.R., Sun C.T. and Mizutani E., Neuro-Fuzzy and Soft Computing: A\nComputational Approach to Learning and Machine Intelligence, Prentice\nHall Inc, USA, 1997.\n[55] Juang Chia Feng and Lin Chin Teng, An Online Self Constructing Neural\nFuzzy Inference Network and its Applications, IEEE Transactions on Fuzzy\nSystems, Vol. 6, No.1, pp. 12-32, 1998.\n[56] Judd S., Neural Network Design and the Complexity of Learning, MIT\nPress, Cambridge, USA, 1990.\n[57] Judea P., Probabilistic Reasoning in Intelligent Systems: Networks of\nPlausible Inference, Morgan Kaufmann Publishers, USA, 1997.\n[58] Kandel A. and Langholz G. (Eds.), Hybrid Architectures for Intelligent\nSystems, CRC Press, 1992.\n[59] Kasabov N. and Kozma R. (Eds.), Neuro-Fuzzy Techniques for Intelligent\nInformation Systems, Studies in Fuzziness and Soft Computing, Springer\nVerlag, Germany, 1999.\n[60] Kasabov N., Evolving Connectionist and Fuzzy Connectionist Systems \u2013\nTheory and Applications for Adaptive On-line Intelligent Systems, In:\nNeuro-Fuzzy Techniques for Intelligent Information Processing, Kasabov\nN. and Kozma R., (Eds.), Physica Verlag, 1999.\n[61] Kosko B., Fuzzy Engineering, Upper Saddle River, NJ: Prentice Hall, 1997.\n[62] Kosko B., Neural Networks and Fuzzy Systems: A Dynamical Systems\nApproach to Machine Intelligence, Prentice Hall, Englewood Cliffs, New\nJersey, 1992.\n[63] Lee M.A., Automatic Design and Adaptation of Fuzzy Systems and Genetic\nAlgorithms Using Soft Computing Techniques, PhD thesis, University of\nCalifornia, Davis, 1994.\n[64] Li X.H. and Chen C.L.P., The Equivalance Between Fuzzy Logic Systems\nand Feedforward Neural Networks, IEEE Transactions on Neural Networks,\nVol. 11, No. 2, pp. 356-365, 2000.\n[65] Lin C.T. and Lee C.S.G., Neural Network based Fuzzy Logic Control and\nDecision System, IEEE Transactions on Comput. 40 (12): pp. 1320-1336,\n1991.\n[66] Lin C.T. and Lee C.S.G., Neural Fuzzy Systems: A Neuro-Fuzzy Synergism\nto Intelligent Systems, Prentice Hall Inc, USA, 1996.\n\n\f[67] Lotfi A., Learning Fuzzy Inference Systems, PhD Thesis, Department of\nElectrical and Computer Engineering, University of Queensland, Australia,\n1995.\n[68] Mamdani E.H. and Assilian S., An Experiment in Linguistic Synthesis with\na Fuzzy Logic Controller, International Journal of Man-Machine Studies,\nVol. 7, No.1, pp. 1-13, 1975.\n[69] Medsker L.R., Hybrid Intelligent Systems, Kluwer Academic Publishers,\n1995.\n[70] Mezard M. and Nadal J.P., Learning in Feed Forward Layered Networks:\nThe Tiling Algorithm, Journal of Physics A, Vol. 22, pp. 2191-2204, 1989.\n[71] Mitra S. and Hayashi Y., Neuro-Fuzzy Rule Generation: Survey in Soft\nComputing Framework, IEEE Transactions on Neural Networks, Vol. II,\nNo. 3, pp. 748-768, 2000.\n[72] Miyoshi T., Tano S., Kato Y., Arnould T., Operator Tuning in Fuzzy\nProduction Rules Using Neural networks, In Proceedings of the IEEE\nInternational Conference on Fuzzy Systems, San Francisco, pp. 641-646,\n1993.\n[73] Moller A.F., A Scaled Conjugate Gradient Algorithm for Fast Supervised\nLearning, Neural Networks, Vol. 6, pp. 525-533, 1993.\n[74] Nauck D., Klawonn F. and Kruse R., Foundations of Neuro-Fuzzy Systems,\nWiley, 1997.\n[75] Nomura H., Hayashi I. and Wakami N., A Learning Method of Fuzzy\nInference Systems by Descent Method, In Proceedings of the First IEEE\nInternational conference on Fuzzy Systems, San Diego, USA, pp. 203-210,\n1992.\n[76] Cord\u00f3n O., Herrera F., Lozano M., On the Combination of Fuzzy Logic and\nEvolutionary Computation: A Short Review and Bibliography, Fuzzy\nEvolutionary Computation, Pedrycz W. (Ed.), Kluwer Academic, pp. 57-77,\n1997.\n[77] Oyama T., Tano S., Miyoshi T., Kato Y., Arnould T. and Bastian A.,\nFINEST: Fuzzy Inference Environment Software with Tuning, In\nProceedings of IEEE International Conference on Fuzzy Systems, pp 3-4,\n1995.\n[78] Pal S.K. and Mitra S., Neuro-Fuzzy Pattern Recognition: Methods in Soft\nComputing, John Wiley & Sons Inc, USA, 1999.\n[79] Pedrycz W. (Ed.), Fuzzy Evolutionary Computation, Kluwer Academic\nPublishers, USA, 1997.\n[80] Pedrycz W. and Card H.C., Linguistic Interpretation of Self Organizing\nMaps, In Proceedings of the IEEE International Conference on Fuzzy\nSystems, San Diego, pp. 371-378, 1992.\n[81] Pedrycz W., Fuzzy Sets Engineering, CRC Press, 1995.\n[82] Phansalkar V.V. and Thathachar M.A.L., Local and Global Optimization\nAlgorithms for Generalized Learning Automata, Neural Computation, 7, pp.\n950-973, 1995.\n[83] Procyk T.J. and Mamdani E.H., A Linguistic Self Organising Process\nController, Automatica, Vol.15, no.1, pp. 15-30, 1979.\n\n\f[84] Russo M. and Jain L.C. (Eds.), Fuzzy Learning and Applications, CRC\nPress, USA, 2001.\n[85] Sanchez E., Shibata T. and Zadeh L.A. (Eds.), Genetic Algorithms and\nFuzzy Logic Systems: Soft Computing Perspectives, World Scientific\nPublishing Company, Singapore, 1997.\n[86] Shafer G., A Mathematical Theory of Evidence, Princeton University Press,\nPrinceton, NJ, 1976.\n[87] Smets P., The Degree of Belief in a Fuzzy Set, Information Science, 25, pp.\n1-19, 1981\n[88] Stepniewski S.W. and Keane A.J., Pruning Back-propagation Neural\nNetworks Using Modern Stochastic Optimization Techniques, Neural\nComputing & Applications, Vol. 5, pp. 76-98, 1997.\n[89] Sugeno M. and Tanaka K., Successive Identification of a Fuzzy Model and\nits Applications to Prediction of a Complex System, Fuzzy Sets Systems,\nVol.42, no.3, pp. 315-334, 1991.\n[90] Sugeno M., Industrial Applications of Fuzzy Control, Elsevier Science Pub\nCo., 1985.\n[91] Sulzberger S.M., Tschicholg-Gurman N.N. and Vestli S.J., FUN:\nOptimization of Fuzzy Rule Based Systems Using Neural Networks, In\nProceedings of IEEE Conference on Neural Networks, San Francisco, pp .\n312-316, 1993.\n[92] Takagi H., Fusion Technology of Fuzzy Theory and Neural Networks Survey and Future Directions. In Proceedings First International Conference\non Fuzzy Logic & Neural Networks, pp. 13-26, 1990.\n[93] Tano S., Oyama T. and Arnould T., Deep combination of Fuzzy Inference\nand Neural Network in Fuzzy Inference, Fuzzy Sets and Systems, 82 (2) pp.\n151-160, 1996.\n[94] Topchy A.P. and Lebedko O.A., Neural Network Training by Means of\nCooperative Evolutionary Search, Nuclear Instruments & Methods In\nPhysics Research, Section A: accelerators, Spectrometers, Detectors and\nAssociated equipment, Vol. 389, No. 1-2, pp. 240-241, 1997.\n[95] Van Rooij A., Jain L.C., and Johnson R.P., Neural Network Training Using\nGenetic Algorithms, World Scientific Publishing Company, Singapore,\n1996.\n[96] Vapnik V., Golowich S., and Smola A., Support vector method for function\napproximation, regression estimation, and signal processing. In Mozer M.,\nJordan M., and Petsche T. (Eds.), Advances in Neural Information\nProcessing Systems 9, Cambridge, MA, 1997. MIT Press, pp. 281-287,\n1997.\n[97] Vidhyasagar M.M., The Theory of Learning and Generalization, SpringerVerlag, New York, 1997.\n[98] Von Altrock C., Fuzzy Logic and Neuro Fuzzy Applications Explained,\nPrentice Hall Inc, USA, 1995.\n[99] Wang L.X., Adaptive Fuzzy Systems and Control, Prentice Hall Inc, USA,\n1994.\n\n\f[100] Wang L.X. and Mendel J.M., Backpropagation Fuzzy System as Nonlinear\nDynamic System Identifiers, In Proceedings of the First IEEE International\nconference on Fuzzy Systems, San Diego, USA, pp. 1409-1418, 1992.\n[101] Wang L.X. and Mendel J.M., Generating Fuzzy Rules by Learning from\nExamples, IEEE Transactions on Systems, Man and Cybernetics, Vol. 22,\nNo 6, pp. 1414-1427, 1992.\n[102] Yager R.R. and Filev D.P., Adaptive Defuzzification for Fuzzy System\nModeling, In Proceedings of the Workshop of the North American Fuzzy\nInformation Processing Society, pp. 135-142, 1992.\n[103] Yao X. and Liu Y., Making Use of Population Information in Evolutionary\nArtificial Neural Networks, IEEE Transactions on Systems, Man and\nCybernetics, Part B: Cybernetics, 28(3): pp. 417-425, 1998.\n[104] Yao X., Evolving Artificial Neural Networks, Proceedings of the IEEE,\n87(9): pp. 1423-1447, 1999.\n[105] Yoshinari Y., Pedrycz W., Hirota K., Construction of Fuzzy Models\nThrough Clustering Techniques, Fuzzy Sets and Systems, Vol. 54, pp. 157165, 1993.\n[106] Zadeh L.A., Fuzzy Sets, Information and Control, Vol. 8: pp. 338-353,\n1965.\n[107] Zadeh L.A., Outline of a New Approach to the Analysis of Complex\nSystems and Decision Process, IEEE Transactions, System, Man, and\nCybernetics, Vol.3, no.1, pp. 28-44, 1973.\n[108] Zadeh L.A., Roles of Soft Computing and Fuzzy Logic in the Conception,\nDesign and Deployment of Information/Intelligent Systems, Computational\nIntelligence: Soft Computing and Fuzzy-Neuro Integration with\nApplications, Kaynak O. et al (Eds.), pp. 1-9, 1998.\n\n\f"}