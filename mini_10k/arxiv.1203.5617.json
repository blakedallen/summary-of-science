{"id": "http://arxiv.org/abs/1203.5617v1", "guidislink": true, "updated": "2012-03-26T09:58:59Z", "updated_parsed": [2012, 3, 26, 9, 58, 59, 0, 86, 0], "published": "2012-03-26T09:58:59Z", "published_parsed": [2012, 3, 26, 9, 58, 59, 0, 86, 0], "title": "From Minimax Shrinkage Estimation to Minimax Shrinkage Prediction", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.2360%2C1203.2112%2C1203.6394%2C1203.4743%2C1203.1365%2C1203.4980%2C1203.2024%2C1203.2954%2C1203.0315%2C1203.6493%2C1203.2539%2C1203.2180%2C1203.4808%2C1203.4332%2C1203.0337%2C1203.2317%2C1203.3114%2C1203.6383%2C1203.5911%2C1203.1841%2C1203.4325%2C1203.2917%2C1203.6169%2C1203.3083%2C1203.1901%2C1203.5306%2C1203.5963%2C1203.6681%2C1203.5617%2C1203.3066%2C1203.4831%2C1203.3659%2C1203.6316%2C1203.1565%2C1203.0546%2C1203.1796%2C1203.6784%2C1203.2034%2C1203.4166%2C1203.6781%2C1203.2217%2C1203.5693%2C1203.2349%2C1203.5440%2C1203.2615%2C1203.4019%2C1203.6723%2C1203.2305%2C1203.1857%2C1203.6843%2C1203.4402%2C1203.4129%2C1203.2500%2C1203.6033%2C1203.5595%2C1203.2647%2C1203.4244%2C1203.4317%2C1203.4410%2C1203.4122%2C1203.4060%2C1203.0484%2C1203.3140%2C1203.0667%2C1203.1067%2C1203.0368%2C1203.0373%2C1203.4296%2C1203.1824%2C1203.4678%2C1203.2623%2C1203.3727%2C1203.0594%2C1203.3285%2C1203.4340%2C1203.0787%2C1203.0238%2C1203.6730%2C1203.5333%2C1203.4363%2C1203.5257%2C1203.2943%2C1203.1825%2C1203.1616%2C1203.2488%2C1203.0064%2C1203.2576%2C1203.4822%2C1203.5864%2C1203.6235%2C1203.3845%2C1203.0859%2C1203.0236%2C1203.0517%2C1203.3807%2C1203.6549%2C1203.1704%2C1203.3204%2C1203.1619%2C1203.0597%2C1203.4576&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "From Minimax Shrinkage Estimation to Minimax Shrinkage Prediction"}, "summary": "In a remarkable series of papers beginning in 1956, Charles Stein set the\nstage for the future development of minimax shrinkage estimators of a\nmultivariate normal mean under quadratic loss. More recently, parallel\ndevelopments have seen the emergence of minimax shrinkage estimators of\nmultivariate normal predictive densities under Kullback--Leibler risk. We here\ndescribe these parallels emphasizing the focus on Bayes procedures and the\nderivation of the superharmonic conditions for minimaxity as well as further\ndevelopments of new minimax shrinkage predictive density estimators including\nmultiple shrinkage estimators, empirical Bayes estimators, normal linear model\nregression estimators and nonparametric regression estimators.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.2360%2C1203.2112%2C1203.6394%2C1203.4743%2C1203.1365%2C1203.4980%2C1203.2024%2C1203.2954%2C1203.0315%2C1203.6493%2C1203.2539%2C1203.2180%2C1203.4808%2C1203.4332%2C1203.0337%2C1203.2317%2C1203.3114%2C1203.6383%2C1203.5911%2C1203.1841%2C1203.4325%2C1203.2917%2C1203.6169%2C1203.3083%2C1203.1901%2C1203.5306%2C1203.5963%2C1203.6681%2C1203.5617%2C1203.3066%2C1203.4831%2C1203.3659%2C1203.6316%2C1203.1565%2C1203.0546%2C1203.1796%2C1203.6784%2C1203.2034%2C1203.4166%2C1203.6781%2C1203.2217%2C1203.5693%2C1203.2349%2C1203.5440%2C1203.2615%2C1203.4019%2C1203.6723%2C1203.2305%2C1203.1857%2C1203.6843%2C1203.4402%2C1203.4129%2C1203.2500%2C1203.6033%2C1203.5595%2C1203.2647%2C1203.4244%2C1203.4317%2C1203.4410%2C1203.4122%2C1203.4060%2C1203.0484%2C1203.3140%2C1203.0667%2C1203.1067%2C1203.0368%2C1203.0373%2C1203.4296%2C1203.1824%2C1203.4678%2C1203.2623%2C1203.3727%2C1203.0594%2C1203.3285%2C1203.4340%2C1203.0787%2C1203.0238%2C1203.6730%2C1203.5333%2C1203.4363%2C1203.5257%2C1203.2943%2C1203.1825%2C1203.1616%2C1203.2488%2C1203.0064%2C1203.2576%2C1203.4822%2C1203.5864%2C1203.6235%2C1203.3845%2C1203.0859%2C1203.0236%2C1203.0517%2C1203.3807%2C1203.6549%2C1203.1704%2C1203.3204%2C1203.1619%2C1203.0597%2C1203.4576&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In a remarkable series of papers beginning in 1956, Charles Stein set the\nstage for the future development of minimax shrinkage estimators of a\nmultivariate normal mean under quadratic loss. More recently, parallel\ndevelopments have seen the emergence of minimax shrinkage estimators of\nmultivariate normal predictive densities under Kullback--Leibler risk. We here\ndescribe these parallels emphasizing the focus on Bayes procedures and the\nderivation of the superharmonic conditions for minimaxity as well as further\ndevelopments of new minimax shrinkage predictive density estimators including\nmultiple shrinkage estimators, empirical Bayes estimators, normal linear model\nregression estimators and nonparametric regression estimators."}, "authors": ["Edward I. George", "Feng Liang", "Xinyi Xu"], "author_detail": {"name": "Xinyi Xu"}, "author": "Xinyi Xu", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/11-STS383", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1203.5617v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1203.5617v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/11-STS383 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1203.5617v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1203.5617v1", "journal_reference": "Statistical Science 2012, Vol. 27, No. 1, 82-94", "doi": "10.1214/11-STS383", "fulltext": "Statistical Science\n2012, Vol. 27, No. 1, 82\u201394\nDOI: 10.1214/11-STS383\nc Institute of Mathematical Statistics, 2012\n\nFrom Minimax Shrinkage Estimation\nto Minimax Shrinkage Prediction\narXiv:1203.5617v1 [stat.ME] 26 Mar 2012\n\nEdward I. George, Feng Liang and Xinyi Xu\n\nAbstract. In a remarkable series of papers beginning in 1956, Charles\nStein set the stage for the future development of minimax shrinkage\nestimators of a multivariate normal mean under quadratic loss. More\nrecently, parallel developments have seen the emergence of minimax\nshrinkage estimators of multivariate normal predictive densities under\nKullback\u2013Leibler risk. We here describe these parallels emphasizing the\nfocus on Bayes procedures and the derivation of the superharmonic conditions for minimaxity as well as further developments of new minimax\nshrinkage predictive density estimators including multiple shrinkage estimators, empirical Bayes estimators, normal linear model regression\nestimators and nonparametric regression estimators.\nKey words and phrases: Asymptotic minimaxity, Bayesian prediction,\nempirical Bayes, inadmissibility, multiple shrinkage, prior distributions,\nsuperharmonic marginals, unbiased estimates of risk.\ncation invariant estimator for this problem, by showing that, when p \u2265 3, \u03bc\u0302MLE is inadmissible under\nquadratic loss\n\n1. THE BEGINNING OF THE HUNT FOR\nMINIMAX SHRINKAGE ESTIMATORS\n\nPerhaps the most basic estimation problem in Statistics is the canonical problem of estimating a mul- (2)\nRQ (\u03bc, \u03bc\u0302) = E\u03bc k\u03bc\u0302(X) \u2212 \u03bck2 .\ntivariate normal mean. Based on the observation of\na p-dimensional multivariate normal random variable From a decision theory point of view, an important\npart of the appeal of \u03bc\u0302MLE was the protection of(1)\nX|\u03bc \u223c Np (\u03bc, I),\nfered by its minimax property. The worst possible\nthe problem is to find a suitable estimator \u03bc\u0302(x) risk RQ incurred by \u03bc\u0302MLE was no worse than the\nof \u03bc. The celebrated result of Stein (1956) dethroned worst possible risk of any other estimator. Stein's\n\u03bc\u0302MLE (x) = x, the maximum likelihood and best lo- result implied the existence of even better estimators that offered the same minimax protection. He\nhad begun the hunt for these better minimax estiEdward I. George is Professor, Department of\nmators.\nStatistics, The Wharton School, Philadelphia, PA\nIn a remarkable series of follow-up papers Stein\n19104-6340, USA e-mail: edgeorge@wharton.upenn.edu.\nproceeded to set the stage for this hunt. James and\nFeng Liang is Associate Professor, Department of\nStein (1961) proposed a new closed-form minimax\nStatistics, University of Illinois at Urbana-Champaign,\nChampaign, IL 61820, USA e-mail: liangf@illinois.edu.\nshrinkage estimator\n\u0013\n\u0012\nXinyi Xu is Assistant Professor, Department of\np\u22122\nStatistics, The Ohio State University, Columbus, OH\n(3)\nx,\n\u03bc\u0302JS (x) = 1 \u2212\nkxk2\n43210-1247, USA e-mail: xinyi@stat.osu.edu.\nthe now well-known James\u2013Stein estimator, andshowed explicitly that its risk was less than RQ (\u03bc,\n\u03bc\u0302MLE ) \u2261 p for every value of \u03bc when p \u2265 3, that is, it\nuniformly dominated \u03bc\u0302MLE . The appeal of \u03bc\u0302JS under RQ was compelling. It offered the same guaran-\n\nThis is an electronic reprint of the original article\npublished by the Institute of Mathematical Statistics in\nStatistical Science, 2012, Vol. 27, No. 1, 82\u201394. This\nreprint differs from the original in pagination and\ntypographic detail.\n1\n\n\f2\n\nE. I. GEORGE, F. LIANG AND X. XU\n\nteed minimax protection as \u03bc\u0302MLE while also offering\nthe possibility of doing much better.\nStein (1962), though primarily concerned with improved confidence regions, described a parametric\nempirical Bayes motivation for (3), describing\nhow \u03bc\u0302JS (x) could be seen as a data-based approximation to the posterior mean\n\u0012\n\u0013\n1\nE\u03c0 (\u03bc|x) = 1 \u2212\n(4)\nx,\n1+\u03bd\n\nPerhaps even more impressive than the fact\nthat \u03bc\u0302H dominated \u03bc\u0302MLE was the way Stein proved\nit. Making further use of the rich results in Brown\n(1971), the key to his proof was the fact that any\nposterior mean Bayes estimator under a prior \u03c0(\u03bc)\ncan be expressed as\n\nthe Bayes rule which minimizes the average risk\nE\u03c0 RQ (\u03bc, \u03bc\u0302) when \u03bc \u223c Np (0, \u03bdI). He here also proposed the positive-part James\u2013Stein estimator\n\u03bc\u0302JS+ = max{0, \u03bc\u0302JS }, a dominating improvement\nover \u03bc\u0302JS (x), and commented that \"it would be even\nbetter to use the Bayes estimate with respect to\na reasonable prior distribution.\" These observations\nserved as a clear indication that the Bayesian paradigm was to play a major role in the hunt for these\nnew shrinkage estimators, opening up a new direction that was to be ultimately successful for establishing large new classes of shrinkage estimators.\nDominating fully Bayes shrinkage estimators soon\nemerged. Strawderman (1971) proposed \u03bc\u0302a (x) =\nE\u03c0a (\u03bc|x), a class of Bayes shrinkage estimators obtained as posterior means under priors \u03c0a (\u03bc) for which\n\n(8)\n\n(7)\n\n\u03bc\u0302\u03c0 (x) = E\u03c0 (\u03bc|x) = x + \u2207 log m\u03c0 (x),\n\nwhere\nm\u03c0 (x) \u221d\n\nZ\n\n2 /2\n\ne\u2212(x\u2212\u03bc)\n\n\u03c0(\u03bc) d\u03bc\n\nis the marginal distribution of X under \u03c0(\u03bc). [Here\n\u2207 = ( \u2202x\u2202 1 , . . . , \u2202x\u2202 p )\u2032 is the familiar gradient.]\nAt first glance it would appear that (7) has little to do with the risk. However, Stein noted that\ninsertion of (7) into RQ , followed by expansion and\nan integration-by-parts identity, now known as one\nof Stein's Lemmas, yields the following general expression for the difference between the risks of \u03bc\u0302\u03c0\nand \u03bc\u0302MLE :\n\nRQ (\u03bc, \u03bc\u0302MLE ) \u2212 RQ (\u03bc, \u03bc\u0302\u03c0 )\n\u0015\n\u0014\n\u22072 m\u03c0 (X)\n2\n= E\u03bc k\u2207 log m\u03c0 (X)k \u2212 2\nm\u03c0 (X)\np\np\n= E\u03bc [\u22124\u22072 m\u03c0 (X)/ m\u03c0 (X)].\n(10)\n(5)\n\u03bc|s \u223c Np (0, sI), s \u223c (1 + s)a\u22122 .\nP\nStrawderman explicitly showed that \u03bc\u0302a uniformly (Here \u22072 = i \u2202 22 is the familiar Laplacian.)\n\u2202xi\ndominated \u03bc\u0302MLE and was proper Bayes, when p = 5\nBecause the bracketed terms in (9) and (10) do\nand a \u2208 [0.5, 1) or when p \u2265 6 and a \u2208 [0, 1). This was\nnot depend on \u03bc (they are unbiased estimators of\nespecially interesting because any proper Bayes was\nthe risk difference), the domination of \u03bc\u0302MLE by \u03bc\u0302\u03c0\nnecessarily admissible and so could not be improved\nwould follow whenever m\u03c0 was such that these brackupon.\neted terms were nonnegative. As Stein noted, this\nThen, Stein (1974, 1981) showed that \u03bc\u0302H (x), the\nwould be the case in (9) whenever m\u03c0 was superhar\u221a\nBayes estimator under the harmonic prior\nmonic, \u22072 m\u03c0 (x) \u2264 p\n0, and in (10) whenever m\u03c0 was\n(6)\n\u03c0H (\u03bc) = E\u03c0H (\u03bc|x) = k\u03bck\u2212(p\u22122) ,\nsuperharmonic, \u22072 m\u03c0 (x) \u2264 0, a weaker condition.\nThe domination of \u03bc\u0302MLE by \u03bc\u0302H was seen now to\ndominated \u03bc\u0302MLE when p \u2265 3. A special case of \u03bc\u0302a\nbe\nattributable directly to the fact that the margiwhen a = 2, \u03bc\u0302H was only formal Bayes because \u03c0H (\u03bc)\nnal\n(8) under \u03c0H , a mixture of harmonic functions, is\nis improper. Undeterred, Stein pointed out that the\nadmissibility of \u03bc\u0302H followed immediately from the superharmonic when p \u2265 3. However, such an explageneral conditions for the admissibility of general- nation would not work for the domination of \u03bc\u0302MLE\nized Bayes estimators laid out by Brown (1971). by \u03bc\u0302a , because the marginal (8) under \u03c0a in (5) is\nA further key element of the story was Brown's not superharmonic for any a < 1. Indeed, as was\n(1971) powerful result that all such generalized Bayes shown later by Fourdrinier, Strawderman and Wells\nrules (including the proper ones of course) consti- (1998), a superharmonic marginal cannot be obtaituted a complete class for the problem of estimating ned with any proper prior. More importantly, howmultivariate normal mean under quadratic loss. It ever, they were able to establish that the dominawas now clear that the hunt for new minimax shrink- tion by \u03bc\u0302a was attributable to the superharmonicity\n\u221a\nage estimators was to focus on procedures with at of m\u03c0a under \u03c0a when p \u2265 5 (and Strawderman's\nleast some Bayesian motivation.\nconditions on a). In fact, it also followed from their\n(9)\n\n\fMINIMAX SHRINKAGE PREDICTION\n\n\u221a\nresults that m\u03c0a is superharmonic when a \u2208 [1, 2)\nand p \u2265 3, further broadening the class of minimax\nimproper Bayes estimators.\nPrior to the appearance of (9) and (10), minimaxity proofs, though ingenious, had all been tailored\nto suit the specific estimators at hand. The sheer\ngenerality of this new approach was daunting in its\nscope. By restricting attention to priors that gave\nrise to marginal distributions with particular properties, the minimax properties of the implied Bayes\nrules would be guaranteed.\n2. THE PARALLELS IN THE PREDICTIVE\nESTIMATION PROBLEM EMERGE\nThe seminal work of Stein concerned the canonical\nproblem of how to estimate \u03bc based on an observation of X|\u03bc \u223c Np (\u03bc, I). A more ambitious problem is how to use such an X to estimate the entire\nprobability distribution of a future Y from a normal\ndistribution with this same unknown mean \u03bc, the\nso-called predictive density of Y . Such a predictive\ndensity offers a complete description of predictive\nuncertainty.\nTo conveniently treat the possibility of different\nvariances for X and Y , we formulate the predictive\nproblem as follows. Suppose X|\u03bc \u223c Np (\u03bc, vx I) and\nY |\u03bc \u223c Np (\u03bc, vy I) are independent p-dimensional multivariate normal vectors with common unknown\nmean \u03bc but known variances vx and vy . Letting\np(y|\u03bc) denote the density of Y , the problem is to find\nan estimator p\u0302(y|x) of p(y|\u03bc) based on the observation of X = x only. Such a problem arises naturally,\nfor example, for predicting Y |\u03bc \u223c Np (\u03bc, \u03c3 2 I) based\non the observation of X1 , . . . , Xn |\u03bc i.i.d. \u223c Np (\u03bc,\n\u03c3 2 I) which is equivalent to observing X\u0304|\u03bc \u223c Np (\u03bc,\n(\u03c3 2 /n)I). This is exactly our formulation with vx =\n\u03c3 2 /n and vy = \u03c3 2 .\nFor the evaluation of p\u0302(y|x) as an estimator of\np(y|\u03bc), the analogue of quadratic risk RQ for the\nmean estimation problem is the Kullback\u2013Leibler\n(KL) risk\nZ\n(11)\nRKL (\u03bc, p\u0302) = p(x|\u03bc)L(\u03bc, p\u0302(*|x)) dx,\n\n3\n\nimized by the Bayes rule\n(13)\n\np\u0302\u03c0 (y|x) = E\u03c0 [p(y|\u03bc)|x]\nZ\n= p(y|\u03bc)\u03c0(\u03bc|x) d\u03bc,\n\nthe posterior mean of p(y|\u03bc) under \u03c0 (Aitchison,\n1975). It follows from (13) that p\u0302\u03c0 (y|x) is a proper\nprobability distribution over y whenever the marginal\ndensity of x is finite for all z (integrate w.r.t. y and\nswitch the order of integration). Furthermore, the\nmean of p\u0302\u03c0 (y|x) (when it exists) is equal to E\u03c0 (\u03bc|x),\nthe Bayes rule for estimating \u03bc under quadratic loss,\nnamely the posterior mean of \u03bc. Thus, p\u0302\u03c0 also carries the necessary information for that estimation\nproblem. Note also that unless \u03c0 is a trivial point\nprior, such p\u0302\u03c0 (y|x) will not be of the form of p(y|\u03bc)\nfor any \u03bc. The range of the Bayes rules here falls\noutside the target space of the densities which are\nbeing estimated.\nA tempting initial approach to this predictive density estimation problem is to use the simple plug-in\nestimator p\u0302MLE \u2261 p(y|\u03bc = \u03bc\u0302MLE ) to estimate p(y|\u03bc),\nthe so-called estimative approach. This was the conventional wisdom until the appearance of Aitchison\n(1975). He showed that the plug-in estimator p\u0302MLE\nis uniformly dominated under RKL by\np\u0302U (y|x) \u2261 E\u03c0U [p(y|\u03bc)|x]\n(14)\n\u001a\n\u001b\n1\nky \u2212 xk2\n=\nexp \u2212\n,\n2(vx + vy )\n{2\u03c0(vx + vy )}p/2\n\nthe posterior mean of p(y|\u03bc) with respect to the uniform prior \u03c0U (\u03bc) = 1, the so-called predictive approach. In a related vein, Akaike (1978) pointed out\nthat, by Jensen's inequality, the Bayes rule p\u0302\u03c0 (y|x)\nwould dominate the random plug-in estimator\np\u0302(y|\u03bc = \u03bc\u0302) when \u03bc\u0302 is a random draw from \u03c0. Strategies for averaging over \u03bc were looking better than\nplug-in strategies. The hunt for predictive shrinkage\nestimators had turned to Bayes procedures.\nDistinct from p\u0302MLE , p\u0302U was soon shown to be the\nbest location invariant predictive density estimator;\nsee Murray (1977) and Ng (1980). That p\u0302U is best\ninvariant and minimax also follows from the more\nwhere p(x|\u03bc) denotes the density of X, and\nZ\nrecent general results of Liang and Barron (2004),\np(y|\u03bc)\nwho also showed that p\u0302U is admissible when p = 1.\n(12)\ndy\nL(\u03bc, p\u0302(*|x)) = p(y|\u03bc) log\np\u0302(y|x)\nThe minimaxity of p\u0302U was also shown directly by\nGeorge, Liang and Xu (2006). Thus, p\u0302U , rather than\nis the familiar KL loss.\np\u0302\nFor a (possibly improper)\nprior\ndistribution\n\u03c0\non\n\u03bc,\nMLE , here plays the role played by \u03bc\u0302MLE in the\nR\nthe average risk r(\u03c0, p\u0302) = RKL (\u03bc, p\u0302)\u03c0(\u03bc) d\u03bc is min- mean estimation context. Not surprisingly, \u03bc\u0302U = x,\n\n\f4\n\nE. I. GEORGE, F. LIANG AND X. XU\n\nthe posterior mean under the uniform prior \u03c0U is\nidentical to \u03bc\u0302MLE in that context.\nThe parallels between the mean estimation problem and the predictive estimation problem came into\nsharp focus with the stunning breakthrough result of\nKomaki (2001). He proved that when p \u2265 3, p\u0302U (y|x)\nitself is dominated by the Bayes rule\n(15)\n\np\u0302H (y|x) = E\u03c0H [p(y|\u03bc)|x],\n\nunder the harmonic prior \u03c0H (\u03bc) in (6) used by Stein\n(1974). Shortly thereafter Liang (2002) showed\nthat p\u0302U (y|x) is dominated by the proper Bayes rule pa (y|x) under \u03c0a (\u03bc) for which\n(16)\n\n\u03bc|s \u223c Np (0, sv0 I),\n\ns \u223c (1 + s)a\u22122 ,\n\nwhen vx \u2264 v0 , and when p = 5 and a \u2208 [0.5, 1) or\np \u2265 6 and a \u2208 [0, 1), the same conditions that Strawderman had obtained for his estimator. Note\nthat \u03c0a (\u03bc) in (16) is an extension of (5) which depends on the constant v0 . As before, \u03c0H (\u03bc) is the\nspecial case of \u03c0a (\u03bc) when a = 2. Note that p\u0302U is\nnow playing the \"straw-man\" role that was played\nby \u03bc\u0302MLE in the mean estimation problem.\n3. A UNIFIED THEORY FOR MINIMAX\nPREDICTIVE DENSITY ESTIMATION\nThe proofs of the domination of p\u0302U by p\u0302H in Komaki (2001) and by p\u0302a in Liang (2002) were both\ntailored to the specific forms of the dominating estimators. They did not make direct use of the properties of the induced marginal distributions of X\nand Y . From the theory developed by Brown (1971)\nand Stein (1974) for the mean estimation problem, it\nwas natural to ask if there was a theory analogous\nto (7)\u2013(10) which would similarly unify the domination results in the predictive density estimation\nproblem.\nAs it turned out, just such a theory was established in George, Liang and Xu (2006), the main results of which we now proceed to describe. The story\nbegins with a representation, analogous to Brown's\nrepresentation \u03bc\u0302\u03c0 (X) = E\u03c0 (\u03bc|X) = X +\u2207 log m\u03c0 (X)\nin (7), that is available for posterior mean Bayes\nrules in the predictive density estimation problem.\nA key element of the representation is the form of\nthe marginal distributions for our context which we\ndenote by\nZ\n(17)\nm\u03c0 (z; v) = p(z|\u03bc)\u03c0(\u03bc) d\u03bc\nfor Z|\u03bc \u223c Np (\u03bc, vI) and a prior \u03c0(\u03bc). In terms of\nour previous notation (8), m\u03c0 (z) = m\u03c0 (z; 1).\n\nLemma 1. The Bayes rule p\u0302\u03c0 (y|x) in (13) can\nbe expressed as\n(18)\n\np\u0302\u03c0 (y|x) =\n\nm\u03c0 (w; vw )\np\u0302U (y|x),\nm\u03c0 (x; vx )\n\nwhere p\u0302U (y|x) is the Bayes rule under \u03c0U (\u03bc) = 1\ngiven by (14), m\u03c0 (x; vx ) is the marginal distribuv v\ntion of X, and m\u03c0 (w; vw ), where vw = vxx+vyy , is the\nv X+v Y\n\nmarginal distribution of W = yvx +vyx for independent X|\u03bc \u223c Np (\u03bc, vx I) and Y |\u03bc \u223c Np (\u03bc, vy I).\nLemma 1 shows how the form of p\u0302\u03c0 (y|x) is determined entirely by p\u0302U (y|x) and the form of m\u03c0 (x; vx )\nand m\u03c0 (w; vw ). The essential step in its derivation is\nto factor the joint distribution of x and y into terms\nincluding a function of the sufficient statistic w. Inserting the representation (18) into the risk RKL\nleads immediately to the following unbiased estimate for the KL risk difference between p\u0302U (y|x)\nand p\u0302\u03c0 (y|x):\nRKL (\u03bc, p\u0302U ) \u2212 RKL (\u03bc, p\u0302\u03c0 )\nZ Z\np\u0302\u03c0 (y|x)\n(19) =\ndx dy\np(x|\u03bc)p(y|\u03bc) log\np\u0302U (y|x)\n= E\u03bc,vw log m\u03c0 (W ; vw ) \u2212 E\u03bc,vx log m\u03c0 (X; vx ).\nAs one can see from (19) and the fact that vw =\n< vx , p\u0302U (y|x) would be uniformly dominated\nby p\u0302\u03c0 (y|x) whenever E\u03bc,v log m\u03c0 (Z; v) is decreasing\n\u2202\nin v. As if by magic, the sign of \u2202v\nE\u03bc,v log m\u03c0 (Z; v)\nturned out to be directly linked to the same unbiased\nrisk difference estimates (9) and (10) of Stein (1974).\nvx vy\nvx +vy\n\nLemma 2.\n\u2202\nE\u03bc,v log m\u03c0 (Z; v)\n\u2202v\n(20)\n\u0014 2\n\u0015\n\u2207 m\u03c0 (Z; v) 1\n= E\u03bc,v\n\u2212 k\u2207 log m\u03c0 (Z; v)k2\nm\u03c0 (Z; v)\n2\np\np\n(21) = E\u03bc,v [2\u22072 m\u03c0 (Z; v)/ m\u03c0 (Z; v)].\n\nThe proof of Lemma 2 relies on Brown's representation, Stein's Lemma, and the fact that any normal\nmarginal distribution m\u03c0 (z; v) satisfies\n1\n\u2202\nm\u03c0 (z; v) = \u22072 m\u03c0 (z; v),\n\u2202v\n2\nthe well-known heat equation which has a long history in science and engineering; for example, see\nSteele (2001). Combining (19) and Lemma 2 with\nthe fact that p\u0302U (y|x) is minimax yields the following\n(22)\n\n\fMINIMAX SHRINKAGE PREDICTION\n\ngeneral conditions for the minimaxity of a predictive\ndensity estimator, conditions analogous to those obtained by Stein for the minimaxity of a normal mean\nestimator.\nTheorem 1. If m\u03c0 (z; v) is finite for all z, then\np\u0302\u03c0 (y|x) will be minimax if either of the following\nhold for all vw \u2264 v \u2264 vx :\n(i) m\np\u03c0 (z; v) is superharmonic.\nm\u03c0 (z; v) is superharmonic.\n(ii)\n\nAlthough condition (i) implies the weaker condition (ii) above, it is included because of its convenience when it is available. Since a superharmonic\nprior always yields a superharmonic m\u03c0 (z; v) for\nall v, the following corollary is immediate.\n\n5\n\nrules \u03bc\u0302\u03c0 (x) = E\u03c0 (\u03bc|x) shrink x toward the center\nof \u03c0(\u03bc), the mean of \u03c0(\u03bc) when it exists. (Section 6\nwill describe how multimodal priors yield multiple\nshrinkage estimators.) As we saw earlier, x here plays\nthe role both of \u03bc\u0302MLE (x) = x and of the formal\nBayes estimator \u03bc\u0302U (x) = x.\nThe representation (18) reveals how p\u0302\u03c0 (y|x) analogously \"shrinks\" the formal Bayes estimator p\u0302U (y|x),\nbut not p\u0302MLE 6= p\u0302U , by an adaptive multiplicative\nfactor\nm\u03c0 (w; vw )\nb\u03c0 (x, y) =\n(24)\n.\nm\u03c0 (x; vx )\n\nHowever, because p\u0302\u03c0 (y|x) must be a proper probability distribution (whenever m\u03c0 is always finite), it\ncannot be the case that b\u03c0 (x, y) < 1 for all y at any x.\nCorollary 1. If m\u03c0 (z; v) is finite for all z, then Thus, \"shrinkage\" here really refers to a reconcentration of the probability distribution of p\u0302U (y|x).\np\u0302\u03c0 (y|x) will be minimax if \u03c0(\u03bc) is superharmonic.\nFurthermore, since the mean of p\u0302\u03c0 (y|x) is E\u03c0 (\u03bc|x),\nBecause \u03c0H is superharmonic, it is immediate\nfrom this reconcentration, under unimodal priors, is top\nCorollary 1 that p\u0302H is minimax. Because ma (z; v) ward the center of \u03c0(\u03bc), as in the mean estimation\nis superharmonic for all v (under suitable conditions case.\non a), it is immediate from Theorem 1 that p\u0302a is miConsider, for example, what happens under \u03c0H\nnimax. It similarly follows that any of the improper which is symmetric and unimodal about 0. Figure 1\nsuperharmonic t-priors of Faith (1978) or any of the illustrates how this shrinkage occurs for p for varH\nproper generalized t-priors of Fourdrinier, Strawder- ious values of x when p = 5. Figure 1 plots p\u0302 (y|x)\nU\nman and Wells (1998) yield minimax Bayes rules.\nand p\u0302H (y|x) as functions of y = (y1 , y2 , 0, 0, 0)\u2032 when\nThe connections between the unbiased risk differ- v = 1 and v = 0.2. Note first that p\u0302 (y|x) is always\nx\ny\nU\nence estimates for the KL risk and quadratic risk the same symmetric shape centered at x. When x =\nproblems ultimately yields the following identity:\n(2, 0, 0, 0, 0)\u2032 , shrinkage occurs by pushing the concentration of p\u0302H (y|x) = bH (x, y)p\u0302U (y|x) toward 0.\nRKL (\u03bc, p\u0302U ) \u2212 RKL (\u03bc, p\u0302\u03c0 )\n(23)\nAs x moves further from (0, 0, 0, 0, 0)\u2032 to (3, 0, 0, 0, 0)\u2032\nZ vx\n1\n1\nand (4, 0, 0, 0, 0)\u2032 this shrinkage diminishes as p\u0302H (y|x)\n=\n[RQ (\u03bc, \u03bc\u0302U ) \u2212 RQ (\u03bc, \u03bc\u0302\u03c0 )]v dv,\n2\n2 vw v\nbecomes more and more similar to p\u0302U (y|x).\nAs in the problem of mean estimation, the shrinkexplaining the parallel minimax conditions in both\nproblems. Brown, George and Xu (2008) used this age by p\u0302H manifests itself in risk reduction over p\u0302U .\nidentity to further draw out connections to establish To illustrate this, Figure 2 displays the risk dif\u2032\nsufficient conditions for the admissibility of Bayes ference [RKL (\u03bc, p\u0302U ) \u2212 RKL (\u03bc, p\u0302H )] at \u03bc = (c, . . . , c) ,\nrules under KL loss, conditions analogous to those 0 \u2264 c \u2264 4 when vx = 1 and vy = 0.2 for dimensions\nof Brown (1971) and Brown and Hwang (1982), and p = 3, 5, 7, 9. Paralleling the risk reduction offered\nto show that all admissible procedures for the KL by \u03bc\u0302H in the mean estimation problem, the largest\nrisk problems are Bayes rules, a direct parallel of the risk reduction offered by p\u0302H occurs close to \u03bc = 0\ncomplete class theorem of Brown (1971) for quadra- and decreases rapidly to 0 as k\u03bck increases. [RKL (\u03bc,\np\u0302U ) is constant as a function of \u03bc.] At the same time,\ntic risk.\nthe risk reduction by p\u0302H is larger for larger p at each\nfixed k\u03bck.\n4. THE NATURE OF SHRINKAGE IN\nPREDICTIVE DENSITY ESTIMATION\n5. MANY POSSIBLE SHRINKAGE TARGETS\nThe James\u2013Stein estimator \u03bc\u0302JS (x) in (3) provided\nBy a simple shift of coordinates, the modified James\u2013\nan explicit example of how risk improvements for estimating \u03bc are obtained by shrinking X toward 0 by Stein estimator,\n\u0013\n\u0012\np\u22122\nthe adaptive multiplicative factor (1 \u2212 kxk\n2 ). Simip\u22122\nb\n(x \u2212 b),\n\u03bc\u0302JS (x) = b + 1 \u2212\nlarly, under unimodal priors, posterior mean Bayes (25)\nkx \u2212 bk2\n\n\f6\n\nE. I. GEORGE, F. LIANG AND X. XU\n\nFig. 1.\n\nShrinkage of p\u0302U (y|x) to obtain p\u0302H (y|x) when vx = 1, vy = 0.2 and p = 5. Here y = (y1 , y2 , 0, 0, 0)\u2032 .\n\nremains minimax, but now shrinks x toward b \u2208 Rp\nwhere its risk function is smallest. Similarly, minimax Bayes shrinkage estimators of a mean or of\na predictive density can be shifted to shrink toward b, by recentering the prior \u03c0(\u03bc) to \u03c0 b (\u03bc) =\n\u03c0(\u03bc \u2212 b). These shifted estimators are easily obtained by inserting the corresponding translated marginal\n(26)\n\nmb\u03c0 (z; v) = m\u03c0 (z \u2212 b; v)\n\ninto (7) to obtain\n(27)\n\n\u03bc\u0302b\u03c0 (x) = E\u03c0b (\u03bc|x) = x + \u2207 log mb\u03c0 (x; 1),\n\nand into (18) to obtain\n(28)\n\np\u0302b\u03c0 (y|x) =\n\nmb\u03c0 (w; vw )\np\u0302U (y|x).\nmb\u03c0 (x; vx )\n\nb and \u03c0 b yield\nRecentered unimodal priors such as \u03c0H\na\nestimators that now shrink x and p\u0302U (y|x) toward b\nrather than toward 0. Since the superharmonic properties of m\u03c0 are inherited by mb\u03c0 , the minimaxity of\nsuch estimators will be preserved.\nIn his discussion of Stein (1962), Lindley (1962)\nnoted that the James\u2013Stein estimator could be mod-\n\nFig. 2. The risk difference between p\u0302U and p\u0302H when\n\u03bc = (c, . . . , c)\u2032 , vx = 1, vy = 0.2.\n\nified to shrink toward (x\u0304, . . . , x\u0304)\u2032 \u2208 Rp (x\u0304 is the mean\nof the components of x), by replacing b and (p \u2212 2)\nin (25) by (x\u0304, . . . , x\u0304)\u2032 and (p \u2212 3), respectively. The\nresulting estimator remains minimax as long as p \u2265 4\n\n\f7\n\nMINIMAX SHRINKAGE PREDICTION\n\nand offers smallest risk when \u03bc is close to the subspace of \u03bc with identical coordinates, the subspace\nspanned by the vector 1p = (1, . . . , 1)\u2032 . Note that\n(x\u0304, . . . , x\u0304)\u2032 is the projection of x into this subspace.\nMore generally, minimax Bayes shrinkage estimators of a mean or of a predictive density can be similarly modified to obtain shrinkage toward any (possibly affine) subspace B \u2282 Rp , whenever they correspond to spherically symmetric priors. Such priors,\nwhich include \u03c0H and \u03c0a , are functions of \u03bc only\nthrough k\u03bck. Such a modification is obtained by recentering the prior \u03c0(\u03bc) around B via\n(29)\n\n\u03c0 B (\u03bc) = \u03c0(\u03bc \u2212 PB \u03bc),\n\nand predictive density estimators\n(33)\n\np\u0302B\n\u03c0 (y|x) =\n\nmB\n\u03c0 (w; vw )\np\u0302U (y|x),\nmB\n\u03c0 (x; vx )\n\nthat now shrink x and p\u0302U (y|x) toward B rather than\ntoward 0. Shrinkage will be largest when x \u2208 B, and\nwill diminish as x moves away from B. These estimators offer smallest risk when \u03bc \u2208 B, but do not\nimprove in any important way over x and p\u0302U (y|x)\nwhen \u03bc is far from B.\nA superharmonic m\u03c0 will lead to a superharmonic mB\n\u03c0 as long as (p \u2212 dim(B)) is large enough.\nFor example, the recentered marginal mB\nH will be\nsuperharmonic only when (p \u2212 dim(B)) \u2265 3. In such\nB\ncases, the minimaxity of both \u03bc\u0302B\n\u03c0 and p\u0302\u03c0 will be\npreserved.\n\nwhere PB \u03bc = argminb\u2208B k\u03bc \u2212 bk is the projection\nof \u03bc onto B. Effectively, \u03c0 B (\u03bc) puts a uniform prior\non PB \u03bc and applies a suitably modified version of \u03c0\nto (\u03bc \u2212 PB \u03bc). Note that the dimension of (\u03bc \u2212 PB \u03bc),\n6. WHERE TO SHRINK?\nnamely (p \u2212 dim(B)), must be taken into account\nwhen determining the appropriate modification for \u03c0.\nStein's discovery of the existence of minimax shrinFor example, recentering the harmonic prior \u03c0H (\u03bc) = kage estimators such as \u03bc\u0302bJS (x) in (25) demonstrated\nk\u03bck\u2212(p\u22122) around the subspace spanned by 1p yields that costless improvements over the minimax \u03bc\u0302MLE\nwere available near any target preselected by the\nB\n(30)\n\u03c0H\n(\u03bc) = k\u03bc \u2212 \u03bc\u03041p k\u2212(p\u22123) ,\nstatistician. As Stein (1962) put it when referring\nwhere \u03bc\u0304 = \u03bc\u2032 1p /p. Here, the uniform prior is put to the use of such an estimator to center a conon PB \u03bc = \u03bc\u03041p , and the harmonic prior in dimen- fidence region, the target \"should be chosen. . . as\nsion (p \u2212 dim(B)) = (p \u2212 1) (which is different from one's best guess\" of \u03bc. That frequentist considerathe harmonic prior in Rp ) is put on (\u03bc \u2212 \u03bc\u03041p ), the tions had demonstrated the folly of ignoring subjecorthogonal complement of B.\ntive input was quite a shock to the perceived \"obThe marginal mB\njectivity\" of the frequentist perspective.\n\u03c0 corresponding to the recentered \u03c0 B in (29) can be directly obtained by recenAlthough the advent of minimax shrinkage estiB\ntering the spherically symmetric marginal m\u03c0 cor- mators of the form \u03bc\u0302B\n\u03c0 in (32) and p\u0302\u03c0 in (33) opened\nresponding to \u03c0, that is,\nup the possibility of small risk near any preselected\nB\n(affine) subspace B \u2282 Rp (this includes the possi(31)\nm\u03c0 (z; v) = m\u03c0 (z \u2212 PB z; v),\nbility that B is a single point), it also opened up\nwhere PB z is the projection of z onto B. Analo- a challenging new problem, how to best choose such\ngously to \u03c0 B (\u03bc), mB\n\u03c0 (z; v) is uniform on PB z and a B. From the vast number of possible choices, the\napplies a suitably modified version of m\u03c0 to (z \u2212 goal was to choose B close to the unknown \u03bc, othPB z). Here, too, the dimension of (z \u2212 PB z), namely erwise risk reduction would be negligible. To add to\n(p \u2212 dim(B)), must be taken into account when de- the difficulties, low-dimensional B, which offered the\ntermining the appropriate modification for m\u03c0 . For\ngreatest risk reduction, were also the most difficult\nexample, recentering the marginal m\u03c0 around the\nto get close to \u03bc.\nsubspace spanned by 1p would entail replacing kzk\nWhen faced with a number of potentially good\nby kz \u2212 z\u03041p k, where z\u0304 = z \u2032 1p /p, and appropriately\ntarget\nchoices, say B1 , . . . , BN , rather than choose\nmodifying m\u03c0 to apply to Rp\u22121 .\nB\none of them and proceed with \u03bc\u0302B\n\u03c0 or p\u0302\u03c0 , an attracApplying the recentering (29) to priors such as \u03c0H tive alternative is to use a minimax multiple shrinkand \u03c0a , which are unimodal around 0, yields pri- age estimator; see George (1986a, 1986b, 1986c).\nB and \u03c0 B and hence marginals mB and mB ,\nors \u03c0H\na\na\nH\nSuch estimators incorporate all the potential targets\nwhich are unimodal around B. Such recentered marby combining them into an adaptive convex comginals yield mean estimators\nBN for mean estimation, and\n1\nbination of \u03bc\u0302B\n\u03c0 , . . . , \u03bc\u0302\u03c0\nB\nB\nB\nB\nB\n1\nN\n(32) \u03bc\u0302\u03c0 (x) = E\u03c0 (\u03bc|x) = x + \u2207 log m\u03c0 (x; 1),\nof p\u0302\u03c0 , . . . , p\u0302\u03c0 for predictive density estimation. By\n\n\f8\n\nE. I. GEORGE, F. LIANG AND X. XU\n\nadaptively shrinking toward the more promising targets, the region of potential risk reduction is vastly\nenlarged while at the same time retaining the safety\nof minimaxity.\nThe construction of these minimax multiple shrinkage estimators proceeds as follows, again making\nfundamental use of the Bayesian formulation. For\na spherically symmetric prior \u03c0(\u03bc), a set of subspaces B1 , . . . , BN of Rp , and P\na set of nonnegative\nN\nweights w1 , . . . , wN such that\n1 wi = 1, consider\nthe mixture prior\n(34)\n\n\u03c0\u2217 (\u03bc) =\n\nN\nX\n\nwi \u03c0 Bi (\u03bc),\n\nThe adaptive weights p(Bi |x) in (37) and (39) are\nthe posterior probabilities that \u03bc is contained in\neach of the Bi , effectively putting increased weight\non those individual estimators which are shrinking\nmost. Note that the uniform prior estimates \u03bc\u0302U\nand p\u0302U are here doubly shrunk by \u03bc\u0302\u2217 and p\u0302\u2217 (y|x);\nin addition to the individual estimator shrinkage\nthey are further shrunk by the posterior probability p\u0302(Bi |x).\nThe key to obtaining \u03bc\u0302\u2217 and p\u0302\u2217 (y|x) which are\nminimax is simply to use priors which yield superBN\n1\nharmonic mB\n\u03c0 , . . . , m\u03c0 . If such is the case, then\ntrivially from (35)\n\ni=1\n\nwhere each \u03c0 Bi is a recentered prior as in (29). To\nsimplify notation, we consider the case where\neach \u03c0 Bi is a recentering of the same \u03c0, although in\nprinciple such a construction could be applied with\ndifferent priors. The marginal m\u2217 corresponding to\nthe mixture prior \u03c0\u2217 in (34) is then simply\n(35)\n\nm\u2217 (z; v) =\n\nN\nX\n\ni\nwi mB\n\u03c0 (z; v),\n\n1\n\ni\nmB\n\u03c0\n\nwhere\nare the recentered marginals corresponding to the \u03c0 Bi as given by (31).\nApplying Brown's representation \u03bc\u0302\u03c0 = x +\n\u2207 log m\u03c0 (x; 1) from (7) with m\u2217 in (35) immediately\nyields the multiple shrinkage estimator of \u03bc,\n(36)\n\n\u03bc\u0302\u2217 (x) =\n\nN\nX\ni=1\n\n(40)\n\n2\n\n\u2207 m\u2217 =\n\nN\nX\n1\n\ni\nwi \u22072 mB\n\u03c0 \u2264 0,\n\nso that m\u2217 will be superharmonic, and the minimaxity of \u03bc\u0302\u2217 and p\u0302\u2217 (y|x) will follow immediately. Note\nthat marginals whose square root is superharmonic\nwill not be adequate, as this argument will fail.\nThe adaptive shrinkage behavior of \u03bc\u0302\u2217 and p\u0302\u2217 manifests itself as substantial risk reduction whenever \u03bc\nis near any of B1 , . . . , BN . Let us illustrate how that\nhappens for the predictive density estimator p\u0302H \u2217 ,\nthe multiple shrinkage version of p\u0302H . Figure 3 illustrates the risk reduction [RKL (\u03bc, p\u0302U ) \u2212 RKL (\u03bc, p\u0302H \u2217 )]\nat various \u03bc = (c, . . . , c)\u2032 obtained by p\u0302H \u2217 which adaptively shrinks p\u0302U (y|x) toward the closer of the two\npoints b1 = (2, . . . , 2)\u2032 and b2 = (\u22122, . . . , \u22122)\u2032 using\nequal weights w1 = w2 = 0.5. As in Figure 2, we\n\ni\np(Bi |x)\u03bc\u0302B\n\u03c0 (x),\n\nwhere\n(37)\n\nwi mBi (x; 1)\np(Bi |x) = PN \u03c0 B\n.\ni\ni=1 wi m\u03c0 (x; 1)\n\nSimilarly, applying the representation p\u0302\u03c0 (y|x) =\nfrom (18) with m\u2217 immediately\n\nm\u03c0 (w;vw )\nm\u03c0 (x;vx ) p\u0302U (y|x)\n\nyields the multiple shrinkage estimator of p(y|\u03bc),\n(38)\n\np\u0302\u2217 (y|x) =\n\nN\nX\ni=1\n\ni\np(Bi |x)p\u0302B\n\u03c0 (y|x),\n\nwhere\n(39)\n\nwi mBi (x; vx )\np(Bi |x) = PN \u03c0 Bi\n.\ni=1 wi m\u03c0 (x; vx )\n\nThe forms (36) and (38) reveal \u03bc\u0302\u2217 and p\u0302\u2217 to be\nadaptive convex combination of the individual posBi\ni\nterior mean estimators \u03bc\u0302B\n\u03c0 and p\u0302\u03c0 , respectively.\n\nFig. 3. The risk difference between p\u0302U and multiple shrinkage p\u0302H \u2217 when \u03bc = (c, . . . , c)\u2032 , vx = 1, vy = 0.2, b1 = (2, . . . , 2)\u2032 ,\nb2 = (\u22122, . . . , \u22122)\u2032 , and w1 = w2 = 0.5.\n\n\fMINIMAX SHRINKAGE PREDICTION\n\n9\n\np\u0302p\u22122 can be viewed as a shrinkage predictive density\nestimator that \"pulls\" p\u0302U toward 0, its shrinkage\nadaptively determined by the data.\nTo assess the KL risk properties of such empirical\nBayes estimators, Xu and Zhou considered the class\nof estimators p\u0302k of the form (42) with (p \u2212 2) replaced by a constant k, a class of simple normal\nforms centered at shrinkage estimators of \u03bc with\ndata-dependent variances to incorporate estimation\nuncertainty. For this class, they provided general\nsufficient conditions on k and the dimension p for p\u0302k\n7. EMPIRICAL BAYES CONSTRUCTIONS\nto dominate the best invariant predictive density p\u0302U\nand\nthus be minimax. Going further, they also esBeyond their attractive risk properties, the James\u2013\ntablished\nan \"oracle\" inequality which suggests that\nStein estimator \u03bc\u0302JS and its positive-part counterpart \u03bc\u0302JS+ are especially appealing because of their the empirical Bayes predictive density estimator is\nsimple closed forms which are easy to compute. As asymptotically minimax in infinite-dimensional pashown by Xu and Zhou (2011), similarly appealing rameter spaces and can potentially be used to consimple closed-form predictive density shrinkage esti- struct adaptive minimax estimators. It appears that\nmators can be obtained by the same empirical Bayes these minimax empirical Bayes predictive densities\nmay play the same role as the James\u2013Stein estimaconsiderations that motivate \u03bc\u0302JS and \u03bc\u0302JS+ .\nThe empirical Bayes motivation of \u03bc\u0302JS , alluded tor in such problems.\nIt may be of interest to note that a particular\nto in Section 1, simply entails replacing 1/(1 + \u03bd)\n2\npseudo-marginal\nempirical Bayes construction that\nin (4) by (p \u2212 2)/kxk , its unbiased estimate under\nthe marginal distribution of X|\u03bc \u223c Np (\u03bc, I) when works fine for the mean estimation problem appears\n\u03bc \u223c Np (0, \u03bdI). The positive-part \u03bc\u0302JS+ is obtained by not to work for the predictive density estimation\nusing the truncated estimate (p \u2212 2)/ max{1, kxk2 } problem. For instance, the positive-part James\u2013Stein\nwhich avoids an implicitly negative estimate of the estimator \u03bc\u0302JS+ can be expressed as \u03bc\u0302JS+ = x +\n\u2207 log mJS+ (x; 1), where mJS+ (x; v) is the function\nprior variance \u03bd.\nProceeding analogously, Xu and Zhou considered\nmJS+ (x; v)\nthe Bayesian predictive density estimate,\n\uf8f1\n\u0012\u0012\n\u0013\n\u2212(p\u22122) if kxk2 /v \u2265 (p \u2212 2),\n\uf8f4\n\uf8f2 kp kxk\nvx\n1\u2212\np\u0302\u03bd (y|x) \u223c Np\nx,\n= v \u2212(p\u22122)/2 exp{\u2212kxk2 /2v}\nvx + \u03bd\n\uf8f4\n\uf8f3\n(41)\n\u0012\n\u0013\n\u0013\nif kxk2 /v < (p \u2212 2),\nvx\nvx\nvy + 1 \u2212\n(vx + vy ) ,\nwith kp = (e/(p \u2212 2))\u2212(p\u22122)/2 (see Stein, 1974). We\nvx + \u03bd\nvx + \u03bd\nrefer to m(z; v) as a pseudo-marginal because it is\nwhen X|\u03bc \u223c Np (\u03bc, vx I) and Y |\u03bc \u223c Np (\u03bc, vy I) are innot a bona fide marginal obtained by a real prior.\ndependent, and \u03bc \u223c Np (0, \u03bdI). Replacing vx /(vx +\nNonetheless, it plays the formal role of a marginal\n\u03bd) by its truncated unbiased estimate (p \u2212 2)vx /\nin the mean estimation problem, and can be used to\nmax{vx , kxk2 } under the marginal distribution of X,\nthey obtained the empirical Bayes predictive density generate further innovations such as minimax multiple shrinkage James\u2013Stein estimators (see George,\nestimate\n\u0013\n\u0012\u0012\n1986a, 1986b, 1986c).\n(p \u2212 2)vx\nProceeding by analogy, it would seem that m(z; v)\nx;\np\u0302p\u22122 (y|x) \u223c Np\n1\u2212\nkxk2\n+\ncould be inserted into the representation (18) from\n(42)\n\u0012\n\u0013\n\u0013\nLemma 1 to obtain similar results under KL loss.\n(p \u2212 2)vx\nvy + 1 \u2212\nv\nUnfortunately,\nthis does not yield a suitable minix\nkxk2\n+\nmax predictive estimator because p\u0302JS+ (y|x)\nR is not\na\nproper\nprobability\ndistribution.\nIndeed,\np\u0302JS+ (y|\nwhere (*)+ = max{0, *}, an appealing simple closed\nform. Centered at \u03bc\u0302JS+ , p\u0302p\u22122 converges to the best x) dy 6= 1 and varies with x. What has gone wrong?\ninvariant procedure p\u0302U \u223c N (x, vx + vy ) as kxk2 \u2192 Because they do not correspond to real priors, such\n\u221e, and converges to N (0, vy ) as kxk2 \u2192 0. Thus, pseudo-marginals are ultimately at odds with the\nconsidered the case vx = 1, vy = 0.2 for p = 3, 5, 7, 9.\nAs the plot shows, maximum risk reduction occurs\nwhen \u03bc is close to b1 or b2 , and goes to 0 as \u03bc moves\naway from either of these points. At the same time,\nfor each fixed k\u03bck, risk reduction by p\u0302H \u2217 is larger for\nlarger p. It is impressive that the size of the risk reduction offered by p\u0302H \u2217 is nearly the same as each of\nits single target counterparts. The cost of multiple\nshrinkage enhancement seems negligible, especially\ncompared to the benefits.\n\n\f10\n\nE. I. GEORGE, F. LIANG AND X. XU\n\nprobabilistic coherence of a valid Bayesian approach.\nIn contrast to the mean estimation framework, the\npredictive density estimation framework apparently\nrequires stronger fidelity to the Bayesian paradigm.\n8. PREDICTIVE DENSITY ESTIMATION FOR\nCLASSICAL REGRESSION\nMoving into the multiple regression setting, Stein\n(1960) considered the estimation of a p-dimensional\ncoefficient vector under suitably rescaled quadratic\nloss. He there established the minimaxity of the maximum likelihood estimators, and then proved its inadmissibility when p \u2265 3, by demonstrating the existence of a dominating shrinkage estimator.\nIn a similar vein, as one might expect, the theory\nof predictive density estimation presented in Sections 2 and 3 can also be extended to the multiple regression framework. We here describe the main ideas\nof the development of this extension which appeared\nin George and Xu (2008). Similar results, developed\nindependently from a slightly different perspective,\nappeared at the same time in Kobayashi and Komaki (2008).\nConsider the canonical normal linear regression\nsetup:\n(43)\n\nX|\u03b2 \u223c Nm (A\u03b2, \u03c3 2 I),\n\nY |\u03b2 \u223c Nn (B\u03b2, \u03c3 2 I),\n\nwhere A is a full rank, fixed m \u00d7 p, B is a fixed n \u00d7 p\nmatrix, and \u03b2 is a common p \u00d7 1 unknown regression\ncoefficient. The error variance \u03c3 2 is assumed to be\nknown, and set to be 1 without loss of generality.\nThe problem is to find an estimator of p\u0302(y|x) of the\npredictive density p(y|\u03b2), evaluating its performance\nby KL risk\nZ\n(44)\nRKL (\u03b2, p\u0302) = p(x|\u03b2)L(\u03b2, p\u0302(*|x)) dx,\nwhere L(\u03b2, p\u0302(*|x)) is the KL loss between the density\np(y|\u03b2) and its estimator p\u0302(y|x).\nThe story begins with the result, analogous to\nAitchison's (1975) for the normal mean problem,\nthat the plug-in estimator p(y|\u03b2\u0302x ), where \u03b2\u0302x is the\nleast squares estimate of \u03b2 based on x, is dominated\nunder KL risk by the posterior mean of p(y|\u03b2), the\nBayes rule under the uniform prior\n\n(45)\n\n1\n|A\u2032 A + B \u2032 B|\u22121/2\np\u0302U (y|x) =\n(2\u03c0)n/2\n|A\u2032 A|\u22121/2\n\u001a\n\u001b\nRSS x,y \u2212 RSS x\n\u00d7 exp \u2212\n.\n2\n\nHere, too, p\u0302U is minimax (Liang, 2002; Liang and\nBarron, 2004) and plays the straw-man role of the\n\nestimator to beat. The challenge was to determine\nwhich priors \u03c0 would lead to Bayes rules which dominated p\u0302U , and hence would be minimax. Analogously to the representation (18) in Lemma 1 for\nthe normal mean problem, the following representation for a Bayes rule p\u0302\u03c0 (y|x) here, was the key to\nmeeting this challenge.\nR\nLemma 3. The Bayes rule p\u0302\u03c0 (y|x) = p(y|\u03b2) \u00d7\n\u03c0(\u03b2) d\u03b2 can be expressed as\n(46)\n\np\u0302\u03c0 (y|x) =\n\nm\u03c0 (\u03b2\u0302x,y ; \u03a3C )\nm\u03c0 (\u03b2\u0302x ; \u03a3A )\n\np\u0302U (y|x),\n\nwhere \u03a3A = (A\u2032 A)\u22121 , C = A\u2032 A + B \u2032 B, \u03a3C = (C \u2032 C)\u22121 ,\n\u03b2\u0302x is the least squares estimates of \u03b2 based on x,\nand \u03b2\u0302x,y based on x and y, and m\u03c0 (z; \u03a3) is the\nmarginal distribution of Z|\u03b2 \u223c Np (\u03b2, \u03a3) under \u03c0(\u03b2).\nThe representation (46) leads immediately to the\nfollowing analogue of (19) for the KL risk difference\nbetween p\u0302U (y|x) and p\u0302\u03c0 (y|x):\nRKL (\u03b2, p\u0302U ) \u2212 RKL (\u03b2, p\u0302\u03c0 )\n(47)\n\n= E\u03b2,\u03a3C log m\u03c0 (\u03b2\u0302x,y ; \u03a3C )\n\u2212 E\u03b2,\u03a3A log m\u03c0 (\u03b2\u0302x ; \u03a3A ).\n\nThe challenge thus became that of finding conditions\non m\u03c0 to make this difference positive, a challenge\nmade more difficult than the previous one for (19)\nbecause of the complexity of \u03a3A and \u03a3C . Fortunately this could be resolved by rotating the problem as follows to obtain diagonal forms. Since \u03a3A\nand \u03a3C are both symmetric and positive definite,\nthere exists a full rank p \u00d7 p matrix W , such that\n(48)\n\n\u03a3A = W W \u2032 ,\n\n\u03a3C = W DW \u2032 ,\n\nD = diag(d1 , . . . , dp ).\n\n\u2032\n\u22121 where B \u2032 B is nonBecause \u03a3C = (\u03a3\u22121\nA + B B)\nnegative definite, it follows that di \u2208 (0, 1] for all\n1 \u2264 i \u2264 p with at least one di < 1. Thus, the parameters for the rotated problem become\n\n(49)\n\n\u03bc = W \u22121 \u03b2,\n\n\u03bc\u0302x = W \u22121 \u03b2\u0302x \u223c Np (\u03bc, I),\n\n\u03bc\u0302x,y = W \u22121 \u03b2\u0302x,y \u223c Np (\u03bc, D).\n\nLetting Vw = wI + (1 \u2212 w)D for w \u2208 [0, 1], the risk\ndifference (47) could be reexpressed as\nRKL (\u03b2, p\u0302U ) \u2212 RKL (\u03b2, p\u0302\u03c0 )\n(50)\n\n= E\u03bc,D log m\u03c0W (\u03bc\u0302x,y ; D)\n\u2212 E\u03bc,I log m\u03c0W (\u03bc\u0302x ; I)\n\n= h\u03bc (V0 ) \u2212 h\u03bc (V1 ),\n\n\f11\n\nMINIMAX SHRINKAGE PREDICTION\n\nwhere h\u03bc (Vw ) = E\u03bc,Vw log m\u03c0W (Z; Vw ) and \u03c0W (\u03bc) =\n\u03c0(W \u03bc). The minimaxity of p\u0302\u03c0 would now follow\nfrom conditions on m\u03c0 such that (\u2202/\u2202w)h\u03bc (w) < 0\nfor all \u03bc and w \u2208 [0, 1]. The following substantial\ngeneralizations of Theorem 1 and Corollary 1 provide exactly those conditions.\nTheorem 2. Suppose m\u03c0 (z; W W \u2032 ) is finite for\nall z with the invertible matrix W defined as in (48).\nLet H(f (z1 , . . . , zp )) be the Hessian matrix of f .\n(i) If trace{H(m\u03c0 (z; W Vw W \u2032 ))[\u03a3A \u2212 \u03a3C ]} \u2264 0\nfor all w \u2208 [0, 1], then\np p\u0302\u03c0 (y|x) is minimax.\n(ii) If trace{H( m\u03c0 (z; W Vw W \u2032 ))[\u03a3A \u2212 \u03a3C ]} \u2264 0\nfor all w \u2208 [0, 1], then p\u0302\u03c0 (y|x) is minimax.\n\nCorollary 2. Suppose m\u03c0 (z; W W \u2032 ) is finite\nfor all z. Then p\u0302\u03c0 (y|x) is minimax if\ntrace{H(\u03c0(\u03b2))[\u03a3A \u2212 \u03a3C ]} \u2264 0\n\na.e.\n\nAs a consequence of Corollary 2, the scaled harmonic prior \u03c0H (\u03b2|W ) \u221d kW \u22121 \u03b2kp\u22122 can be shown\nto yield minimax predictive density estimators for\nthe regression setting.\nGoing further, George and Xu (2008) went on to\nshow that the minimax Bayes estimators here can\nbe modified to shrink toward different points and\nsubspaces as in Section 5, and that the minimax\nmultiple shrinkage constructions of Section 6 apply\nas well. In particular, they obtained minimax multiple shrinkage estimators that naturally accommodate variable selection uncertainty.\n9. PREDICTIVE DENSITY ESTIMATION FOR\nNONPARAMETRIC REGRESSION\nMoving in another direction, Xu and Liang (2010)\nconsidered predictive density estimation in the context of modern nonparametric regression, a context\nin which the James\u2013Stein estimator has turned out\nto play an important asymptotic minimaxity role;\nsee Wasserman (2006). Their results pertain to the\ncanonical setup for nonparametric regression:\n(51)\n\nY (ti ) = f (ti ) + \u03b5i ,\n\ni = 1, . . . , n,\n\nwhere f is an unknown smooth function in L2 [0, 1],\nti = i/n, and \u03b5i 's are i.i.d. N (0, 1). A central problem here is to estimate f or various functionals of f\nbased on observing Y = (Y (t1 ), . . . , Y (tn )). Transforming the problem with an orthonormal basis, (51)\nis equivalent to estimating the \u03b8i 's in\n\u0012\n\u0013\n1\n(52) yi = \u03b8i + ei , ei \u223c N 0,\n, i = 1, . . . , n,\nn\n\nknown as the Gaussian sequence model. The model\nabove is different from the ordinary multivariate normal model in two aspects: (1) the model dimension n\nis increasing with the sample size, and (2) under\nfunction space assumptions on f , the \u03b8i 's lie\nPin a constrained space, for example, an ellipsoid { i a2i \u03b8i2 \u2264\nC, ai \u2192 \u221e}.\nA large body of literature has been devoted to\nminimax estimation of f under L2 risk over certain\nfunction spaces; see, for example, Johnstone (2003),\nEfromovich (1999), and the references therein. As\nopposed to the ordinary multivariate normal mean\nproblem, exact minimax analysis is difficult for the\nGaussian sequence model (52) when a constraint\non the parameters is considered. This difficulty has\nbeen overcome by first obtaining the minimax risk of\na subclass of estimators of a simple form, and then\nshowing that the overall minimax risk is asymptotically equivalent to the minimax risk of the subclass. For example, an important result from Pinsker\n(1980) is that when the parameter space is constrained to an ellipsoid, the nonlinear minimax risk\nis asymptotically equivalent to the linear minimax\nrisk, namely the minimax risk of the subclass of linear estimators of the form \u03b8\u0302i = ci xi .\nFor nonparametric regression, the following analogue between estimation under L2 risk and predictive density estimation under KL risk was established in Xu and Liang (2010). The prediction problem for nonparametric regression is formulated as\nfollows. Let \u1ef8 = (\u1ef8 (u1 ), . . . , \u1ef8 (um )) be future observations arising at a set of dense (m \u2265 n) and\nequally spaced locations {uj }m\ni=1 . Given f , the predictive density p(\u1ef9|f ) is just a product of Gaussians.\nThe problem is to find an estimator p\u0302(\u1ef9|y) of p(\u1ef9|f ),\nwhere performance is measured by the averaged KL\nrisk\n(53)\n\nR(f, p\u0302) =\n\n1\np(\u1ef8 |f )\nEY,\u1ef8 |f log\n.\nm\np\u0302(\u1ef8 |Y )\n\nIn this formulation, densities are estimated at the m\nlocations simultaneously by p\u0302(\u1ef9|y). As it turned out,\nthe KL risk based on the simultaneous formulation (53) is the analog of the L2 risk for estimation. Indeed, under the KL risk (53), the prediction\nproblem for a nonparametric regression model can\nbe converted to the one for a Gaussian sequence\nmodel.\nBased on this formulation of the problem, minimax analysis proceeds as in the general framework\nfor the minimax study of function estimation used\n\n\f12\n\nE. I. GEORGE, F. LIANG AND X. XU\n\nby, for example, Pinsker (1980) and Belitser and Levit (1995, 1996). The linear estimators there, which\nplay a central role in their minimax analysis, take\nthe same form as posterior means under normal priors. Analogously, predictive density estimates under\nthe same normal priors turned out to play the corresponding role in the minimax analysis for prediction. (The same family of Bayes rules arises from\nthe empirical Bayes approach in Section 7.) Thus,\nXu and Liang (2010) were ultimately able to show\nthat the overall minimax KL risk is asymptotically\nequivalent to the minimax KL risk of this subclass of\nBayes rules, a direct analogue of Pinker's Theorem\nfor predictive density estimation in nonparametric\nregression.\n10. DISCUSSION\nStein's (1956) discovery of the existence of shrinkage estimators that uniformly dominate the minimax maximum likelihood estimator of the mean of\na multivariate normal distribution under quadratic\nrisk when p \u2265 3 was the beginning of a major research effort to develop improved minimax shrinkage\nestimation. In subsequent papers Stein guided this\neffort toward the Bayesian paradigm by providing\nexplicit examples of minimax empirical Bayes and\nfully Bayes rules. Making use of the fundamental\nresults of Brown (1971), he developed a general theory for establishing minimaxity based on the superharmonic properties of the marginal distributions\ninduced by the priors.\nThe problem of predictive density estimation of\na multivariate normal distribution under KL risk has\nmore recently seen a series of remarkably parallel developments. With a focus on Bayes rules catalyzed\nby Aitchison (1975), Komaki (2001) provided a fundamental breakthrough by demonstrating that the\nharmonic prior Bayes rule dominated the best invariant uniform prior Bayes rule. These results suggested the existence of a theory for minimax estimation based on the superharmonic properties of\nmarginals, a theory that was then established in\nGeorge, Liang and Xu (2006). Further developments\nof new minimax shrinkage predictive density estimators now abound, including, as described in this article, multiple shrinkage estimators, empirical Bayes\nestimators, normal linear model regression estimators, and nonparametric regression estimators. Examples of promising further new directions for predictive density estimation can be found in the work\nof Komaki (2004, 2006, 2009) which included results\nfor Poisson distributions, for general location-scale\n\nmodels and for Wishart distributions, in the work\nof Ghosh, Mergel and Datta (2008) which developed\nestimation under alternative divergence losses, and\nin the work of Kato (2009) which established improved minimax predictive domination for the multivariate normal distribution under KL risk when\nboth the mean and the variance are unknown. Minimax predictive density estimation is now beginning\nto flourish.\nACKNOWLEDGMENTS\nThis work was supported by NSF Grants DMS07-32276 and DMS-09-07070. The authors are grateful for the helpful comments and clarifications of an\nanonymous referee.\nREFERENCES\nAitchison, J. (1975). Goodness of prediction fit. Biometrika\n62 547\u2013554. MR0391353\nAkaike, H. (1978). A new look at the Bayes procedure.\nBiometrika 65 53\u201359. MR0501450\nBelitser, E. N. and Levit, B. Y. (1995). On minimax filtering over ellipsoids. Math. Methods Statist. 4 259\u2013273.\nMR1355248\nBrown, L. D. (1971). Admissible estimators, recurrent diffusions, and insoluble boundary value problems. Ann. Math.\nStatist. 42 855\u2013903. MR0286209\nBelitser, E. N. and Levit, B. Y. (1996). Asymptotically\nminimax nonparametric regression in L2 . Statistics 28 105\u2013\n122. MR1405604\nBrown, L. D., George, E. I. and Xu, X. (2008). Admissible\npredictive density estimation. Ann. Statist. 36 1156\u20131170.\nMR2418653\nBrown, L. D. and Hwang, J. T. (1982). A unified admissibility proof. In Statistical Decision Theory and Related Topics, III, Vol. 1 (West Lafayette, Ind., 1981)\n(S. S. Gupta and J. O. Berger, eds.) 205\u2013230. Academic\nPress, New York. MR0705290\nEfromovich, S. (1999). Nonparametric Curve Estimation:\nMethods, Theory, and Applications. Springer, New York.\nMR1705298\nFaith, R. E. (1978). Minimax Bayes estimators of a multivariate normal mean. J. Multivariate Anal. 8 372\u2013379.\nMR0512607\nFourdrinier, D., Strawderman, W. E. and Wells, M. T.\n(1998). On the construction of Bayes minimax estimators.\nAnn. Statist. 26 660\u2013671. MR1626063\nGeorge, E. I. (1986a). Minimax multiple shrinkage estimation. Ann. Statist. 14 188\u2013205. MR0829562\nGeorge, E. I. (1986b). Combining minimax shrinkage estimators. J. Amer. Statist. Assoc. 81 437\u2013445. MR0845881\nGeorge, E. I. (1986c). A formal Bayes multiple shrinkage\nestimator. Comm. Statist. A-Theory Methods 15 2099\u2013\n2114. MR0851859\nGeorge, E. I., Liang, F. and Xu, X. (2006). Improved minimax predictive densities under Kullback\u2013Leibler loss. Ann.\nStatist. 34 78\u201391. MR2275235\n\n\fMINIMAX SHRINKAGE PREDICTION\nGeorge, E. I. and Xu, X. (2008). Predictive density estimation for multiple regression. Econometric Theory 24 528\u2013\n544. MR2391619\nGhosh, M., Mergel, V. and Datta, G. S. (2008). Estimation, prediction and the Stein phenomenon under divergence loss. J. Multivariate Anal. 99 1941\u20131961. MR2466545\nJames, W. and Stein, C. (1961). Estimation with quadratic\nloss. In Proc. 4th Berkeley Sympos. Math. Statist. and\nProb., Vol. I 361\u2013379. Univ. California Press, Berkeley,\nCA. MR0133191\nJohnstone, I. M. (2003). Function estimation and Gaussian\nsequence models. Draft of a Monograph, Dept. Statistics,\nStanford Univ.\nKato, K. (2009). Improved prediction for a multivariate normal distribution with unknown mean and variance. Ann.\nInst. Statist. Math. 61 531\u2013542. MR2529965\nKobayashi, K. and Komaki, F. (2008). Bayesian shrinkage\nprediction for the regression problem. J. Multivariate Anal.\n99 1888\u20131905. MR2466542\nKomaki, F. (2001). A shrinkage predictive distribution for\nmultivariate normal observables. Biometrika 88 859\u2013864.\nMR1859415\nKomaki, F. (2004). Simultaneous prediction of independent Poisson observables. Ann. Statist. 32 1744\u20131769.\nMR2089141\nKomaki, F. (2006). Shrinkage priors for Bayesian prediction.\nAnn. Statist. 34 808\u2013819. MR2283393\nKomaki, F. (2009). Bayesian predictive densities based on\nsuperharmonic priors for the 2-dimensional Wishart model.\nJ. Multivariate Anal. 100 2137\u20132154. MR2560359\nLiang, F. (2002). Exact minimax procedures for predictive\ndensity estimation and data compression. Ph.D. thesis,\nDept. Statistics, Yale Univ. MR2703233\nLiang, F. and Barron, A. (2004). Exact minimax strategies for predictive density estimation, data compression,\nand model selection. IEEE Trans. Inform. Theory 50 2708\u2013\n2726. MR2096988\nLindley, D. V. (1962). Discussion of \"Confidence sets for the\nmean of a multivariate normal distribution\" by C. Stein.\nJ. Roy. Statist. Soc. Ser. B 24 285\u2013287.\n\n13\n\nMurray, G. D. (1977). A note on the estimation of probability density functions. Biometrika 64 150\u2013152. MR0448690\nNg, V. M. (1980). On the estimation of parametric density\nfunctions. Biometrika 67 505\u2013506. MR0581751\nPinsker, M. S. (1980). Optimal filtering of square integrable\nsignals in Gaussian white noise. Problems Inform. Transmission 2 120\u2013133.\nSteele, J. M. (2001). Stochastic Calculus and Financial Applications. Applications of Mathematics (New York) 45.\nSpringer, New York. MR1783083\nStein, C. (1956). Inadmissibility of the usual estimator for\nthe mean of a multivariate normal distribution. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 1954\u20131955, Vol. I 197\u2013206.\nUniv. California Press, Berkeley. MR0084922\nStein, C. (1960). Multiple regression. In Contributions to\nProbability and Statistics (I. Olkin, ed.) 424\u2013443. Stanford Univ. Press, Stanford, Calif. MR0120718\nStein, C. M. (1962). Confidence sets for the mean of a multivariate normal distribution (with discussion). J. Roy.\nStatist. Soc. Ser. B 24 265\u2013296. MR0148184\nStein, C. (1974). Estimation of the mean of a multivariate\nnormal distribution. In Proceedings of the Prague Symposium on Asymptotic Statistics (Charles Univ., Prague,\n1973), Vol. II 345\u2013381. Charles Univ., Prague. MR0381062\nStein, C. M. (1981). Estimation of the mean of a multivariate normal distribution. Ann. Statist. 9 1135\u20131151.\nMR0630098\nStrawderman, W. E. (1971). Proper Bayes minimax estimators of the multivariate normal mean. Ann. Math.\nStatist. 42 385\u2013388. MR0397939\nWasserman, L. (2006). All of Nonparametric Statistics.\nSpringer, New York. MR2172729\nXu, X. and Liang, F. (2010). Asymptotic minimax risk of\npredictive density estimation for non-parametric regression. Bernoulli 16 543\u2013560. MR2668914\nXu, X. and Zhou, D. (2011). Empirical Bayes predictive densities for high-dimensional normal models. J. Multivariate\nAnal. 102 1417\u20131428.\n\n\f"}