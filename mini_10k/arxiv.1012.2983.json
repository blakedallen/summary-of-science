{"id": "http://arxiv.org/abs/1012.2983v2", "guidislink": true, "updated": "2012-06-26T10:55:25Z", "updated_parsed": [2012, 6, 26, 10, 55, 25, 1, 178, 0], "published": "2010-12-14T10:18:27Z", "published_parsed": [2010, 12, 14, 10, 18, 27, 1, 348, 0], "title": "Zero Variance Markov Chain Monte Carlo for Bayesian Estimators", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.2724%2C1012.3335%2C1012.2949%2C1012.1974%2C1012.1760%2C1012.0041%2C1012.1923%2C1012.0925%2C1012.4719%2C1012.5515%2C1012.4908%2C1012.0145%2C1012.4179%2C1012.3074%2C1012.6039%2C1012.2403%2C1012.5607%2C1012.3690%2C1012.2887%2C1012.3397%2C1012.4358%2C1012.0663%2C1012.4337%2C1012.2532%2C1012.1130%2C1012.3170%2C1012.5189%2C1012.3501%2C1012.5109%2C1012.3112%2C1012.2412%2C1012.2832%2C1012.3689%2C1012.3417%2C1012.2969%2C1012.1675%2C1012.5435%2C1012.1520%2C1012.1668%2C1012.5533%2C1012.4697%2C1012.2347%2C1012.5697%2C1012.3948%2C1012.0916%2C1012.6034%2C1012.1562%2C1012.3710%2C1012.0731%2C1012.0205%2C1012.0149%2C1012.5815%2C1012.0005%2C1012.2327%2C1012.2198%2C1012.4606%2C1012.1658%2C1012.1253%2C1012.4598%2C1012.1920%2C1012.1857%2C1012.1345%2C1012.2369%2C1012.2056%2C1012.1741%2C1012.0168%2C1012.4360%2C1012.0474%2C1012.3513%2C1012.2072%2C1012.5713%2C1012.3194%2C1012.5666%2C1012.3475%2C1012.5778%2C1012.5657%2C1012.3224%2C1012.3988%2C1012.4778%2C1012.2195%2C1012.2835%2C1012.1419%2C1012.1754%2C1012.3767%2C1012.1883%2C1012.5411%2C1012.4726%2C1012.1072%2C1012.5277%2C1012.2983%2C1012.1774%2C1012.0021%2C1012.0809%2C1012.5572%2C1012.3453%2C1012.0499%2C1012.0356%2C1012.3171%2C1012.4367%2C1012.3791%2C1012.2837&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Zero Variance Markov Chain Monte Carlo for Bayesian Estimators"}, "summary": "Interest is in evaluating, by Markov chain Monte Carlo (MCMC) simulation, the\nexpected value of a function with respect to a, possibly unnormalized,\nprobability distribution. A general purpose variance reduction technique for\nthe MCMC estimator, based on the zero-variance principle introduced in the\nphysics literature, is proposed. Conditions for asymptotic unbiasedness of the\nzero-variance estimator are derived. A central limit theorem is also proved\nunder regularity conditions. The potential of the idea is illustrated with real\napplications to probit, logit and GARCH Bayesian models. For all these models,\na central limit theorem and unbiasedness for the zero-variance estimator are\nproved (see the supplementary material available on-line).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.2724%2C1012.3335%2C1012.2949%2C1012.1974%2C1012.1760%2C1012.0041%2C1012.1923%2C1012.0925%2C1012.4719%2C1012.5515%2C1012.4908%2C1012.0145%2C1012.4179%2C1012.3074%2C1012.6039%2C1012.2403%2C1012.5607%2C1012.3690%2C1012.2887%2C1012.3397%2C1012.4358%2C1012.0663%2C1012.4337%2C1012.2532%2C1012.1130%2C1012.3170%2C1012.5189%2C1012.3501%2C1012.5109%2C1012.3112%2C1012.2412%2C1012.2832%2C1012.3689%2C1012.3417%2C1012.2969%2C1012.1675%2C1012.5435%2C1012.1520%2C1012.1668%2C1012.5533%2C1012.4697%2C1012.2347%2C1012.5697%2C1012.3948%2C1012.0916%2C1012.6034%2C1012.1562%2C1012.3710%2C1012.0731%2C1012.0205%2C1012.0149%2C1012.5815%2C1012.0005%2C1012.2327%2C1012.2198%2C1012.4606%2C1012.1658%2C1012.1253%2C1012.4598%2C1012.1920%2C1012.1857%2C1012.1345%2C1012.2369%2C1012.2056%2C1012.1741%2C1012.0168%2C1012.4360%2C1012.0474%2C1012.3513%2C1012.2072%2C1012.5713%2C1012.3194%2C1012.5666%2C1012.3475%2C1012.5778%2C1012.5657%2C1012.3224%2C1012.3988%2C1012.4778%2C1012.2195%2C1012.2835%2C1012.1419%2C1012.1754%2C1012.3767%2C1012.1883%2C1012.5411%2C1012.4726%2C1012.1072%2C1012.5277%2C1012.2983%2C1012.1774%2C1012.0021%2C1012.0809%2C1012.5572%2C1012.3453%2C1012.0499%2C1012.0356%2C1012.3171%2C1012.4367%2C1012.3791%2C1012.2837&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Interest is in evaluating, by Markov chain Monte Carlo (MCMC) simulation, the\nexpected value of a function with respect to a, possibly unnormalized,\nprobability distribution. A general purpose variance reduction technique for\nthe MCMC estimator, based on the zero-variance principle introduced in the\nphysics literature, is proposed. Conditions for asymptotic unbiasedness of the\nzero-variance estimator are derived. A central limit theorem is also proved\nunder regularity conditions. The potential of the idea is illustrated with real\napplications to probit, logit and GARCH Bayesian models. For all these models,\na central limit theorem and unbiasedness for the zero-variance estimator are\nproved (see the supplementary material available on-line)."}, "authors": ["Antonietta Mira", "Reza Solgi", "Daniele Imparato"], "author_detail": {"name": "Daniele Imparato"}, "author": "Daniele Imparato", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s11222-012-9344-6", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1012.2983v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1012.2983v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "26 pages, 4 figures. This is an updated version: the results are the\n  same as the previous one, but presentation is more essential", "arxiv_primary_category": {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1012.2983v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1012.2983v2", "journal_reference": "Statistics and Computing, 2012", "doi": "10.1007/s11222-012-9344-6", "fulltext": "Noname manuscript No.\n(will be inserted by the editor)\n\nZero Variance Markov Chain Monte Carlo\nfor Bayesian Estimators\n\narXiv:1012.2983v2 [stat.CO] 26 Jun 2012\n\nAntonietta Mira * Reza Solgi * Daniele\nImparato\n\nAbstract Interest is in evaluating, by Markov chain Monte Carlo (MCMC)\nsimulation, the expected value of a function with respect to a, possibly unnormalized, probability distribution. A general purpose variance reduction technique for the MCMC estimator, based on the zero-variance principle introduced in the physics literature, is proposed. Conditions for asymptotic unbiasedness of the zero-variance estimator are derived. A central limit theorem is\nalso proved under regularity conditions. The potential of the idea is illustrated\nwith real applications to probit, logit and GARCH Bayesian models. For all\nthese models, a central limit theorem and unbiasedness for the zero-variance\nestimator are proved (see the supplementary material available on-line).\nKeywords Control variates * GARCH models * Logistic regression; *\nMetropolis-Hastings algorithm * Variance reduction\n\n1 General idea\nThe expected value of a function f with respect to a, possibly unnormalized,\nprobabilityR distribution \u03c0,R\n\u03bcf = f (x)\u03c0(x)dx/ \u03c0(x)dx is to be evaluated. Markov chain Monte\nCarlo (MCMC) methods estimate integrals using a large but finite set of points,\nxi , i = 1, * * * , N , collected along the sample path of an ergodic Markov chain\nA. Mira\nSwiss Finance Institute, University of Lugano, via Buffi 13, CH-6904 Lugano, Switzerland.\nE-mail: antonietta.mira@usi.ch\nR. Solgi\nSwiss Finance Institute, University of Lugano, via Buffi 13, CH-6904 Lugano, Switzerland.\nE-mail: reza.solgi@usi.ch\nD. Imparato\nDepartment of Economics, University of Insubria, via Monte Generoso 71, 21100 Varese,\nItaly.\nE-mail: daniele.imparato@uninsubria.it\n\n\f2\n\nAntonietta Mira et al.\n\nhaving \u03c0 (normalized) as its unique stationary and limiting distribution \u03bc\u0302f =\nPN\ni\ni=1 f (x )/N .\nIn this paper a general method is suggested to reduce the MCMC error by\nreplacing f with a different function, f \u0303, obtained by properly re-normalizing\nf . The function f \u0303 is constructed so that its expectation, under \u03c0, equals \u03bcf ,\nbut its variance with respect to \u03c0 is much smaller. To this aim, a standard\nvariance reduction technique introduced for Monte Carlo (MC) simulation,\nknown as control variates [39], is exploited.\nIn the rest of this section we briefly explain the zero-variance (ZV) principle\nintroduced in [4, 5]: an almost automatic method to construct control variates\nfor MC simulation, in which an operator, H, acting as a map from functions\nto functions, and a trial function, \u03c8, are introduced.\nIn quantum mechanics, a commonly used operator H is the so-called Hamiltonian, which represents the total energy of the system, that is, the sum of the\nkynetic energy and the potential energy, where the kinetic energy is typically\ndefined as a second-order differential operator. Such operator is Hermitian\n(that is, self-adjoint) if it acts on the restricted class of infinitely differentiable\nfunctions with compact support. If the trial function \u03c8 belongs to this class,\nand if\n\u221a\n(1)\nH \u03c0=0\nthe re-normalized function defined as\nH\u03c8\nf \u0303(x) = f (x) + p\n\u03c0(x)\n\n(2)\n\nsatisfies \u03bcf = \u03bcf \u0303: thus both f and f \u0303 can be used to estimate the desired\nquantity via Monte Carlo or MCMC simulation. However, for general \u03c8 the\ncondition \u03bcf = \u03bcf \u0303 may not hold anymore and ad-hoc assumptions on the\ntarget \u03c0 are necessary: this issue will be further discussed in Section 5.\nInspired by this physical setting, as a general framework H is supposed\nto be a Hermitian operator (self-adjoint and real in all practical applications)\nsatisfying (1), and the re-normalized function is defined as in (2): depending\non the specific choices of H and \u03c8, the condition \u03bcf = \u03bcf \u0303 has to be carefully\nverified.\nOnly a few operators will be considered in the paper, the key one being\nthe Hamiltonian differential operator. An other important\nexample discussed\nR\nbelow is the Markov operator H acting as H\u03c8(x) = K(x, y)\u03c8(y)dy, where\nK(x, y) needs to be symmetric. The re-normalized function, in this case, becomes\nR\nK(x, y)\u03c8(y)dy\n \u0303\np\nf (x) = f (x) +\n.\n(3)\n\u03c0(x)\nand the condition \u03bcf = \u03bcf \u0303 holds as a simple consequence of (1).\nRegardless of the specific choice of the operator and of the trial function,\nthe optimal pair (H, \u03c8), i.e. the one that leads to zero variance, can be obtained by imposing that f \u0303 is constant and equal to its average, f \u0303 = \u03bcf , which\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n3\n\nis equivalent to require that \u03c3 2 (f \u0303) = 0, where \u03c3 2 (*) denotes the variance operator with respect to the target \u03c0. The latter, together with (2), leads to the\nfundamental equation:\np\n(4)\nH\u03c8 = \u2212 \u03c0(x)[f (x) \u2212 \u03bcf ].\nIn most practical applications equation (4) cannot be solved exactly, still, we\npropose to find an approximate solution in the following way. First choose\na Hermitian operator H verifying (1). Second, parametrize \u03c8 and derive the\noptimal parameters by minimizing \u03c3 2 (f \u0303). The optimal parameters are then\nestimated using a first short MCMC simulation. Finally, a much longer MCMC\nsimulation is performed using \u03bc\u0302f \u0303 instead of \u03bc\u0302f as the estimator. This final\nestimator will be called Zero Variance (ZV) estimator through the paper.\nOther research lines aim at reducing the asymptotic variance of MCMC\nestimators by modifying the transition kernel of the Markov chain. These modifications have been achieved in many different ways, for example by trying to\ninduce negative correlation along the chain path ([6, 21, 13, 41, 12]); by trying\nto avoid random walk behavior via successive over-relaxation ([1, 36, 7]); by\nhybrid Monte Carlo ([16, 35, 10, 18, 27]); by exploiting non reversible Markov\nchains ([15, 32]), by delaying rejection in Metropolis-Hastings type algorithms\n([45, 22]), by data augmentation ([46, 22]) and auxiliary variables ([43, 26, 33,\n34]). Up to our knowledge, the only other research line that uses control variates in MCMC estimation follows the PhD thesis by [24] and has its most\nrecent developement in [14]. In [25] it is observed that, for any real-valued\nfunction g defined on the state space of a Markov chain {X n }, the one-step\nconditional expectation U (x) := g(x) \u2212 E[g(X n+1 )|X n = x] has zero mean\nwith respect to the stationary distribution of the chain and can thus be used\nas control variate. The Authors also note that the best choice for the function g\nis the solution of the associated Poisson equation which can rarely be obtained\nanalytically but can be approximated in specific settings. In [14], the use of\nthis type of control variates is further explored in the setting of reversible\nMarkov chains were a closed form expression for U is often available.\nIn [4, 5] unbiasedness and existence of a central limit theorem (CLT) for the\nZV estimator are not discussed, neither in [28], where this estimator is applied\nto a toy example. The main contributions of this paper are, on the one hand, to\nderive the rigorous conditions for unbiasedness and CLT for the ZV estimators\nin MCMC simulation. On the other hand, we apply the ZV principle to some\nwidely used models (probit, logit, and GARCH) and demonstrate that, under\nvery mild restrictions, the necessary conditions for unbiasedness and CLT are\nverified.\n\n2 Choice of H\nIn this section guidelines to choose the operator H, both for discrete and\ncontinuous settings, are given. In a discrete state space, denote with P (x, y) a\ntransition matrix reversible with respect to \u03c0 (a Markov chain will be identified\n\n\f4\n\nAntonietta Mira et al.\n\nwith the corresponding transition matrix or kernel).\nWe restrict our attention\nP\nin this section to operators H acting as Hf := y K(x, y)f (y). The following\nchoice\ns\n\u03c0(x)\nK(x, y) =\n[P (x, y) \u2212 \u03b4(x \u2212 y)]\n(5)\n\u03c0(y)\nsatisfies condition (1), where \u03b4(x \u2212 y) is the Dirac delta function: \u03b4(x \u2212 y) = 1\nif x = y and zero otherwise. It should be noted that the reversibility condition\nimposed on the Markov chain is essential in order to have a symmetric operator\nK(x, y), as required.\n\u221a\nWith this choice of H, letting \u03c8\u0303 = \u03c8/ \u03c0, equation (3) becomes:\nX\nf \u0303(x) = f (x) \u2212\nP (x, y)[\u03c8\u0303(x) \u2212 \u03c8\u0303(y)].\ny\n\nThe same H can also be applied in continuous settings. In this case, P is the\nkernel of the Markov chain and equation (5) can be trivially extended. This\nchoice of H is exploited in [14], where the following fundamental equation is\nfound for the optimal \u03c8\u0303: E[\u03c8\u0303(x1 )|x0 = x] \u2212 \u03c8\u0303(x) = \u03bcf \u2212 f (x). It is easy to\nprove that this equation coincides with our fundamental equation (4), with the\nchoice of H given in (5). The Authors observe that the optimal trial function\nis given by\n\u221e\nX\n[E[f (xn )|x0 = x] \u2212 \u03bcf ],\n(6)\n\u03c8\u0303(x) =\nn=0\n\nthat is, \u03c8\u0303 is the solution to the Poisson equation for f (x). However, an explicit\nsolution cannot be obtained in general.\nAnother operator is proposed in [4]: if x \u2208 Rd consider the Schr\u00f6dingertype Hamiltonian operator:\nd\n\nHf = \u2212\n\n1 X \u22022\nf + V (x)f,\n2 i=1 \u2202x2i\n\n(7)\n\n\u221a\nwhere V (x) is constructed to fulfill equation (1): V = 2\u221a1 \u03c0 \u2206 \u03c0 and \u2206 denotes\nthe Laplacian operator of second order derivatives. In this setting, we obtain\nthe general expression for f \u0303 reported in (2), where now H is the Schr\u00f6dingertype Hamiltonian. These are the operator and the re-normalized function that\nwill be considered throughout this paper. Although it can only be applied to\ncontinuous state spaces, this Schr\u00f6dinger-type operator shows several advantages with respect to the operator (5). First of all, in order to use (5) the\nconditional expectation appearing in (6) has to be available in closed form.\nSecondly, definition (7) does not require reversibility of the Markov chain.\nMoreover, this definition is independent of the kernel P (x, y) and, therefore,\nalso of the type of MCMC algorithm that is used in the simulation. Note that,\nfor calculating f \u0303 both with the operator (7) and (5), the normalizing constant\nof \u03c0 is not needed.\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n5\n\n3 Choice of \u03c8\nThe optimal choice of \u03c8 is the exact solution of the fundamental equation (4).\nIn real applications, typically, only approximate solutions, obtained by minimizing \u03c3 2 (f \u0303), are available. In other words, we select a functional form for\n\u03c8, parameterized by some coefficients of a class of polynomials, and optimize\nthose coefficients by minimizing the fluctuations of the resulting f \u0303. The particular form of \u03c8 is very dependent on the problem\n\u221a at hand, that is on \u03c0, and\non f . In the sequel it will be assumed that \u03c8 = P \u03c0, where P is a polynomial.\nAs one would expect, the higher is the degree of the polynomial, the higher\nis the number of control variates introduced and the higher is the variance\nreduction achieved. It can be easily shown\n\u0001 that in a d dimensional space, using polynomials of order p, provides d+p\n\u2212 1 control variates. However, some\nd\nrestrictions on the coefficients may occur in order to get an unbiased MCMC\nestimator. See Example 1 of Section 5 at this regard.\n\n4 Control Variates and optimal coefficients\nIn this section, general expressions for the control variates in the ZV method\nare derived. Using the Schr\u00f6dinger-type\nHamiltonian H as given in (7) and\np\ntrial function \u03c8(x) = P (x) \u03c0(x), the re-normalized function is:\n1\nf \u0303(x) = f (x) \u2212 \u2206P (x) + \u2207P (x) * z,\n(8)\n2\n\u0010\n\u0011\n\u2202\n\u2202\nwhere z = \u2212 12 \u2207 ln \u03c0(x), \u2207 = \u2202x\n,\n...,\ndenotes the gradient and \u2206 =\n\u2202x\n1\nd\nPd \u2202 2\ni=1 \u2202x2i . Like any other control variate (i.e. zero mean random variables\nunder the distribution of interest), the variable z can be monitored to test\nconvergence along the lines suggested by [11] and [38], where the same control\nvariate z = \u2207 log \u03c0 is used.\nHereafter\nfunction P is assumed to be a polynomial. As a first case,\nPthe\nd\nfor P (x) = j=1 aj xj (1st degree polynomial), one gets:\nH\u03c8(x)\nf \u0303(x) = f (x) + p\n= f (x) + aT z.\n\u03c0(x)\nThe optimal choice of a, that minimizes the variance of f \u0303(x), is:\n\u22121\na = \u2212\u03a3zz\n\u03c3(z, f ),\n\nwhere\n\n\u03a3zz = E(zz T ), \u03c3(z, f ) = E(zf ).\n\nFor a more general approach to the choice of coefficients using control variates,\nreference should be made to [37] and [30]. We anticipate that conditions under\nwhich the ZV-MCMC estimator obeys a CLT (Section 5) guarantee that the\noptimal a is well defined. In ZV-MCMC, the optimal a is estimated in a first\n\n\f6\n\nAntonietta Mira et al.\n\nstage, through a short MCMC simulation1 . When higher-degree polynomials\nare considered, a similar formula for the coefficients associated to the control\nvariates is obtained once an explicit formula for the control variate vector z has\nbeen found. As an example, for quadratic polynomials P (x) = aT x + 12 xT Bx,\nthe re-normalized f \u0303 is :\n1\nf \u0303(x) = f (x) \u2212 tr(B) + (a + Bx)T z.\n2\nUsing second order polynomials yields a vector of control variates of dimension\n1\n2 d(d+3). Therefore, finding the optimal coefficients requires working with \u03a3zz\nwhich is a matrix of dimension of orderd2 . This makes the use of second order\npolynomials computationally expensive when dealing with high-dimensional\nsampling spaces, say of the order of decades.\n5 Unbiasedness and central limit theorem\nAs remarked in Section 1, condition (1) may not be sufficient to ensure unbiasedness of the estimator when the Schr\u00f6dinger operator (7) is used. In this\nsection general conditionys on the target \u03c0 are provided that guarantee that\nthe ZV-MCMC estimator is (asymptotically) unbiased for the class of trial\nfunctions discussed. Details can be found in the on-line supplementary material, Appendix D.\nProposition 1 Let \u03c0 be a d-dimensional density on a bounded open set \u03a9\nwith regular boundary\n\u2202\u03a9, whose first and second derivatives are continuous.\n\u221a\nThen, if \u03c8 = P \u03c0, a sufficient condition for unbiasedness of the ZV-MCMC\n(x)\nestimator is \u03c0(x) \u2202P\n\u2202xj = 0, for all x \u2208 \u2202\u03a9, j = 1, . . . , d.\nThe previous proposition is a consequence of multidimensional integration by\nparts, from which one gets the equality\n\u0015\n\u0014\nZ\n\u221a\n\u221a\nH\u03c8\n1\n[\u03c8\u2207 \u03c0 \u2212 \u03c0\u2207\u03c8] * nd\u03c3,\nE\u03c0 \u221a\n=\n(9)\n2 \u2202\u03a9\n\u03c0\nwhere n denotes the versor orthogonal to \u2202\u03a9.\nWhen \u03c0 has unbounded support, integration by parts cannot be used directly. In this case, we can formulate the following result.\nProposition 2 Let \u03c0 be a d-dimensional density with unbounded support \u03a9,\nwhose first and second derivatives are continuous, and let (Br )r be a sequence\nof bounded subsets, so that Br % \u03a9. Then, a sufficient condition for unbiasedness of the ZV-MCMC estimator is\nZ\nlim\n\u03c0\u2207P * nd\u03c3 = 0.\nr\u2192+\u221e\n\n\u2202Br\n\n1 From a practical point of view there is no need to run two separate chains, one to get the\ncontrol variates and one to get the final ZV estimator: everything can be done on a single\nMarkov chain which is run once to estimate the optimal coefficients of the control variates\nand then post-processed to get the ZV estimator.\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n7\n\nIn the univariate case, if \u03a9 is some interval of the real line, that is, \u03a9 = (l, u),\nwhere u, l \u2208 R \u222a \u00b1\u221e, it is sufficient that\ndP (x)\ndx\n\n\u03c0(l) =\nx=l\n\ndP (x)\ndx\n\n\u03c0(u),\n\n(10)\n\nx=u\n\nwhich is true, for example, if dP\ndx \u03c0 annihilates at the border of the support.\nIn the seminal paper by [4] unbiasedness conditions are not clearly explored\nsince, typically, the target distribution the physicists are interested in, annihilate at the border of the domain with an exponential rate. The following\nexample shows how crucial the choice of trial functions is, in order to have an\nunbiased estimator, even in trivial models.\nExample 1 Let f (x) = x and \u03c0 be exponential: \u03c0(x) = \u03bbe\u2212\u03bbx I{x>0} . If P (x) is\na first order polynomial, (10) does not hold and this choice does not allow for\nd\nln \u03c0(x) is constant\na ZV-MCMC estimator, since the control variate z = \u2212 21 dx\nand \u03c3(x, z) = 0. However, to satisfy equation (10) it is sufficient to consider\nsecond order polynomials. Indeed, if P (x) = a0 + a1 x + a2 x2 equation (10) is\nsatisfied provided that a1 = 0 and the minimization of the variance of f \u0303 can\n1\nbe carried out within this special class. The optimal choice a2 := 2\u03bb\nyields\n2  \u0303\nzero variance: \u03c3 (f ) \u2261 0.\n\n5.1 Central limit theorem\nConditions for existence of a CLT for \u03bc\u0302f are well known in the literature ([44]).\nUsing these classical results, from (8) we have that the ZV-MCMC estimator\nobeys a CLT provided f , \u2206P and \u2207P * z belong to L2+\u03b4 (\u03c0) when the Markov\nchain run for the simulation is geometrically ergodic. In the next corollary, the\ncase of linear and quadratic polynomials P (used in the examples in Section\n6) is considered.\n\u221a\nCorollary 1 Let \u03c8(x) = P (x) \u03c0, where P (x) is a first or second degree\npolynomial. Then, the ZV-MCMC estimator \u03bc\u0302f \u0303 is a consistent estimator of\n\u03bcf which satisfies the CLT, provided one of the following conditions holds:\nC1 : The Markov chain is geometrically ergodic and f , xki zj \u2208 L2+\u03b4 (\u03c0), \u2200i, j,\nfor all k \u2208 {0, deg P \u2212 1} and some \u03b4 > 0.\nC2 : The Markov chain is uniformly ergodic and f , xki zj \u2208 L2 (\u03c0), \u2200i, j and\nfor all k \u2208 {0, deg P \u2212 1}.\nIn the case of linear P , using the definition of control variate, the statement\nof the previous corollary can be reformulated in this simple way: if f \u2208 L2 (\u03c0)\nand the chain is uniformly ergodic, then a sufficient condition to get a CLT is\n\"\u0012\n\u00132 #\n\u2202\nmj = E\u03c0\nln(\u03c0(x))\n< \u221e, \u2200j.\n\u2202xj\n\n\f8\n\nAntonietta Mira et al.\n\nThe quantity mj is known in the literature as Linnik functional (if considered\nas a function of the target distribution, I(\u03c0)) since it was introduced by [29].\nThe quantity mj is also interpretable as the Fisher information of a location\nfamily in a frequentist setting.\n\n5.2 Exponential family\nLet \u03c0 belong to a d-dimensional exponential family: \u03c0(x) \u221d exp(\u03b2 * T(x) \u2212\nKp (\u03b2))p(x), where \u03b2 \u2208 Rd is the vector of natural parameters. The following\ntheorem provides a sufficient condition for a CLT for ZV-MCMC estimators\nwhen the target belongs to the exponential family and a uniformly ergodic\nMarkov Chain is considered. Similar results can be achieved when the Markov\nChain is geometrically ergodic, by considering the 2 + \u03b4 moment. This statement can be easily verified by a direct computation.\n\u2202 log p\n\u2202xj\nk\nif \u2202T\n\u2202xj\n\nTheorem 1 Let \u03c0 belong to an exponential family, with p such that\n\n\u2208\n\nL2 (\u03c0), \u2200i, k. Then, the Linnik functional of \u03c0 is finite if and only\nL2 (\u03c0), \u2200i, k.\n\n\u2208\n\nExample 2 The Gamma density \u0393 (\u03b1, \u03b8) can be written as an exponential family on (0, +\u221e), where p(x) \u2261 1, so that hypotheses of Theorem 1 are satisfied.\nA direct computation shows that the Gamma density \u0393 (\u03b1, \u03b8) has finite Linnik\nfunctional for any \u03b8 and for any \u03b1 \u2208 {1} \u222a (2, +\u221e). Under these conditions, a\nCLT holds for the ZV-MCMC estimator.\n\n6 Examples\nIn the sequel standard statistical models are considered. For these models,\nthe ZV-MCMC estimators are derived in a Bayesian context; from now on,\nthe target \u03c0 = \u03c0(\u03b2|x) is the Bayesian posterior distribution: therefore, the\nargument associated with the state of the Markov chain is denoted by \u03b2 instead\nof x, which represents, now, the vector of data. The operator\u221aH considered is\nthe Schr\u00f6dinger-type Hamiltonian defined in (7), and \u03c8 = P \u03c0, where P is a\npolynomial.\nNumerical simulations are provided, that confirm the effectiveness of variance reduction achieved, by minimizing the variance of f \u0303 within the class\nof trial functions considered. Moreover, conditions for both unbiasedness and\nCLT for f \u0303 are verified for all the examples. For the mathematical derivation\nof the zero-variance estimator and the proofs of unbiasedness and CLT for\nthe models considered, we refer the reader to the appendices of the on-line\nsupplementary material (Appendices A, B and C).\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n9\n\n6.1 Probit Model\n\nTo demonstrate the effectiveness of ZV for probit models, a simple example\nis presented. The bank dataset from [17] contains the measurements of four\nvariables on 200 Swiss banknotes (100 genuine and 100 counterfeit). The four\nmeasured variables xi (i = 1, 2, 3, 4), are the length of the bill, the width of\nthe left and the right edge, and the bottom margin width. These variables\nare used in a probit model as the regressors, and the type of the banknote\nyi , is the response variable (0 for genuine and 1 for counterfeit). Using flat\npriors, the Bayesian estimator of each parameter, \u03b2k , under squared error loss\nfunction, is the expected value of fk (\u03b2) = \u03b2k under \u03c0 (k = 1, 2, * * * , d). The\nBayesian analysis of this problem is discussed in [31]. In order to find the optimal vector of parameters ak of the trial functions, a short Gibbs sampler,\nfollowing ([2]), (of length 2000, after 1000 burn in steps) is run, and the op\u22121\ntimal coefficients are estimated: \u00e2k = \u2212\u03a3\u0302zz\n\u03c3\u0302(z, \u03b2k ). Finally another MCMC\nsimulation of length 2000 is run (and using the estimated optimal values obtained in the previous step), along which fek (\u03b2), for k = 1, . . . , 4 is averaged.\nWe have repeated this experiment 100 times. The MCMC traces of the ordinary MCMC and the ZV-MCMC in one of these MOnte Carlo experiments\nhave been depicted in the left plot of Fig. 1. The blue curves are the traces of\nfk (ordinary MCMC), and the red ones are the traces of fek (ZV-MCMC). It\nis clear from the figure that the variances of the estimator have substantially\ndecreased. Indeed for the linear trial functions, the ratios of the Monte Carlo\nestimates of the asymptotic variances of the two estimators (ordinary MCMC\nand ZV-MCMC) are between 25 and 100. Even better performance can be\nachieved using second degree polynomials to define the trial function. In the\nright column of Fig. 1 the traces of ZV-MCMC with second order P (x) are\nreported along with the traces of the ordinary MCMC. As it can be seen from\nthe figure, the variances of the ZV estimators are negligible: the ratio of the\nMonte Carlo estimates of the asymptotic variances of the two estimators are\nbetween 18, 000 and 90, 000. In this example (with the simulation length and\nburn-in reported above) the CPU time of ZV-MCMC is almost 3 times larger\nthan the one of ordinary MCMC.\n\nIn order to study the unbiasedness of the ZV-estimators empirically, we\nhave run a very long MCMC (of length 108 ) and obtained a very narrow 95%\nconfidence region for each parameter. In Fig. 2 we have depicted the box-plot\nof the ordinary MCMC (first box-plot), and the ZV-estimators (second and\nthird box-plot) along with these 95% confidence regions (the green regions).\nAs it can be seen, the ZV-estimators are concentrated in the 95% confidence\nregions obtained from the very long chain.\n\n\f10\n\nAntonietta Mira et al.\n1st Degree P(x)\n\n2nd Degree P(x)\n\n\u22121\n\n\u22121\n\u03b21\n\n0\n\n\u03b21\n\n0\n\n\u22122\n\u22123\n\n\u22122\n0\n\n500\n\n1000\n\n1500\n\n\u22123\n\n2000\n\n2\n\n2\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n\u03b22\n\n4\n\n\u03b22\n\n4\n\n0\n\n0\n\n\u22122\n\n\u22122\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2\n\n2\n\u03b23\n\n4\n\n\u03b23\n\n4\n\n0\n\n0\n\n\u22122\n\n\u22122\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1.5\n\n1.5\n\u03b24\n\n2\n\n\u03b24\n\n2\n\n1\n\n1\n\n0.5\n\n0.5\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\nFig. 1 Ordinary MCMC (blue) and ZV-MCMC (red) for probit model: rows are parameters,\ncolumns are degree polynomials.\n\u03b21\n\n\u03b22\n\n\u03b23\n\n\u03b24\n\n\u22121.17\n1.08\n1.18\n\n1.02\n\u22121.18\n\n1.06\n1.17\n1\n\n1.04\n\n\u22121.19\n\n1.16\n\n1.02\n0.98\n\n\u22121.2\n1\n\u22121.21\n\n1.15\n0.96\n\n0.98\n\n1.14\n\n0.96\n\n\u22121.22\n\n0.94\n\n1.13\n\n0.94\n\u22121.23\n\n1.12\n\n0.92\n0.92\n\n\u22121.24\n\n1.11\n\n0.9\n\u22121.25\n\n0.9\n1.1\n\n0.88\n0.88\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\nFig. 2 Boxplots of ordinary MCMC estimates (1) and ZV-MCMC estimates (2 and 3) for\nthe probit model, along with the 95% confidence region obtained by an ordinary MCMC of\nlength 108 (green regions).\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\u03b21\n\n\u03b22\n\n11\n\n\u03b23\n\n\u03b24\n2.28\n\n\u22122.45\n2.35\n\n2.26\n2.2\n\n2.3\n\n\u22122.5\n2.24\n2.25\n2.1\n\n2.22\n\n\u22122.55\n\n2.2\n2.2\n2\n\n2.15\n\n\u22122.6\n\n2.18\n2.1\n1.9\n2.16\n2.05\n\n\u22122.65\n\n2.14\n\n1.8\n2\n\n2.12\n\n\u22122.7\n1.95\n\n1.7\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\n2.1\n\n1\n\n2\n\n3\n\nFig. 3 Box-plots of ordinary MCMC estimates (1) and ZV-MCMC estimates (2 and 3) for\nthe logit model, along with the 95% confidence region obtained by an ordinary MCMC of\nlength 108 (green regions).\n\n6.2 Logit Model:\nA logit model is fitted to the same dataset of Swiss banknotes previously introduced. Flat priors are used and, as before, the Bayesian estimator of each\nparameter, \u03b2k , (again under squared error loss functions) is the expected value\nof \u03b2k under \u03c0 (k = 1, 2, * * * , d). Similar to the probit example, in the first stage\na MCMC simulation is run, and the optimal parameters of P (\u03b2) are estimated.\nThen, in the second stage, an independent simulation is performed, and f \u0303k is\naveraged, using the optimal trial function estimated in the first stage (the same\nsimulation length and burn-in, as in the probit example, have been used). For\nlinear polynomial, the ratio of the Monte Carlo estimates of the asymptotic\nvariances of the two estimators (ordinary MCMC and ZV-MCMC) are between\n15 and 50. Using quadratic polynomials, these ratios are between 15, 000 and\n20, 000. In this example the CPU time of the ZV-MCMC is almost 3 times\nhigher than that of ordinary MCMC.\nWe have run a very long MCMC (of length 108 ) and obtained a very\nnarrow 95% confidence region for each parameter. In Fig. 3 we have depicted\nthe box-plot of the ordinary MCMC (first box-plot), and the ZV-estimators\n(second and third box-plot) along with these 95% confidence regions (the green\nregions). Again, as it can be seen, the ZV-estimators are concentrated in the\n95% confidence regions obtained from the very long Markov chain.\n\n\f12\n\nAntonietta Mira et al.\n\nTable 1 GARCH variance reduction: 95% Confidence interval for the ratio of the variances\nof ordinary MCMC estimators and ZV-MCMC estimator.\n\n1st Degree P (x)\n2nd Degree P (x)\n3rd Degree P (x)\n\n\u03c9\u03021\n\n\u03c9\u03022\n\n\u03c9\u03023\n\n8-18\n1200-2700\n21000-47000\n\n13-28\n6100-13500\n48000-107000\n\n12-27\n6200-13800\n26000-58000\n\n6.3 GARCH Model\nGeneralized autoregressive conditional heteroskedasticity (GARCH) models\n([8]) have become one of the most important building blocks of models in financial econometrics, where they are widely used to model returns. Here it\nis shown how the ZV-MCMC principle can be exploited to estimate the parameters of a univariate GARCH model applied to daily returns of exchange\nrates in a Bayesian setting. Let S(t) be the exchange rate at time t. The daily\nreturns are defined as r(t) := [S(t) \u2212 S(t \u2212 1)]/S(t \u2212 1) \u2248 ln (S(t)/S(t \u2212 1)). In\na Normal-GARCH model, we assume the returns are conditionally Normally\n2\ndistributed, r(t)|Ft \u223c N (0, ht ), where ht = \u03c91 + \u03c93 ht\u22121 + \u03c92 rt\u22121\n, and \u03c91 > 0,\n\u03c92 \u2265 0, and \u03c93 \u2265 0 are the parameters of the model. The aim is to estimate\nthe expected value of \u03c9j under the posterior \u03c0, using independent truncated\nnormal priors. As an example, a Normal-GARCH(1, 1) is fitted to the daily\nreturns of the Deutsche Mark vs British Pound exchange rates from January\n1985, to December 1987. In the first stage a short MCMC simulation ([3]) is\nused to estimate the optimal parameters of the trial function (2000 sweeps after 1000 burn-in). In the second stage an independent simulation is run (with\nlength 10000) and f \u0303k (\u03c9) is averaged in order to efficiently estimate the posterior mean of each parameter. We compare this ZV-MCMC with an ordinary\nMCMC of length 10000 (after 1000 burn-in). First, second and third degree\npolynomials in the trial function are used. In order to study the effectiveness of\nZV-MCMC, we have run these simulations (ordinary MCMC and ZV-MCMC)\n100 times. As it can be seen in Table 1, where a 95% confidence interval for\nthe variance reductions are reported, the ZV strategy reduces the variance of\nthe estimators up to ten thousand times. In this example (with the simulation\nand burn-in lengths reported above) the CPU time of the ZV-MCMC is almost\n20% higher than the CPU time of ordinary MCMC.\nIn order to study the unbiasedness of the ZV-estimators empirically, we\nhave run a very long MCMC (of length 107 ) and obtained a narrow 95%\nconfidence region for each parameter. In Fig. 4 we have depicted the boxplot of the ordinary MCMC (first box-plot), and the ZV-estimators (second,\nthird and fourth box-plots) along with these 95% confidence regions (the green\nregions). As it can be seen the ZV-estimators lie in the range obtained by the\nvery long MCMC.\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\u03c9\n\n13\n\n\u03b1\n\n\u03b2\n\n0.252\n\n0.058\n\n0.61\n\n0.25\n0.605\n\n0.057\n0.248\n\n0.6\n0.246\n\n0.056\n\n0.244\n\n0.595\n\n0.055\n0.242\n0.59\n0.054\n\n0.24\n0.585\n0.238\n\n0.053\n0.58\n\n0.236\n1\n\n2\n\n3\n\n4\n\n1\n\n2\n\n3\n\n4\n\n1\n\n2\n\n3\n\n4\n\nFig. 4 Boxplots of ordinary MCMC estimates (1) and ZV-MCMC estimates (2, 3 and 4) for\nthe GARCH model, along with the 95% confidence region obtained by an ordinary MCMC\nof length 107 (green regions).\n\nFinally, note that the ZV strategy can be used in great generality and\ncan be applied also to more complex GARCH models (such as E-GARCH,\nI-GARCH, Q-GARCH, GJR-GARCH, [9]), provided it is possible to analitically compute the necessary derivatives and verify the hypotheses needed for\nunbiasedness and CLT, in a way similar to the proof reported in Appendix C.\n\n7 Discussion\nCross-fertilizations between physics and statistical literature have proved to\nbe quite effective in the past, especially in the MCMC framework. The first\nparadigmatic example is the paper by [23] first and [19] later on.\nBesides translating into statistical terms the paper by [4], the main effort\nof our work has been the discussion of unbiasedness and convergence of the\nZV-MCMC estimator. The study of CLT leads to the condition of finiteness for\nE\u03c0 [( \u2202 log\u2202x\u03c0(x) )2 ]. This quantity has also been used in the recent paper by [20]\nas a metric tensor to improve efficiency in Langevin diffusion and Hamiltonian\nMC methods. Their idea is to choose this metric as an optimal, local tuning of\nthe dynamic, which is able to take into account the intrinsic anisotropy in the\nmodel considered. In our understanding, what makes these methods and our\nextremely efficient, is the common strategy of exploiting information contained\n\n\f14\n\nAntonietta Mira et al.\n\nin the derivatives of the log-target. A combination of the two strategies could\nbe explored: once the derivatives of the log-target are computed, they can\nbe used both to boost the performance of the Markov chain (as suggested\nby [20]) and to achieve variance reduction by using them to design control\nvariates. This is particularly easy since control variates can be constructed by\nsimply post-processing the Markov chain and, thus, there is no need to re-run\nthe simulation.\nThe second main contribution of this paper is the critical discussion of the\nselection of H and \u03c8. A comparison between the variance reduction framework\nexploited in [14] and the choice of different operators H in our context has\nremarked contras and benefits of the two approaches. Different choices of H\nand \u03c8 could provide alternative efficient variance reduction strategies. This\ncan be easily achieved by considering a wider class of trial functions: \u03c8(x) =\nP (x)q(x), where, as before, P (x) denotes a parametric class of polynomials,\nand q(x) is an arbitrary (sufficiently regular) function.\nIn the present research we have explored \u03c8 based on first, second and\nthird degree polynomials. Despite the use of this fairly restrictive class of trial\nfunctions, the degree of variance reduction obtained in the examples in Section\n6 and in other simulation studies (not reported here) is impressive and of the\norder of ten times (for first degree polynomials) and of thousand times (for\nhigher degree polynomials), with practically small extra CPU time needed in\nthe simulation.\nFinally, mention should be made to an alternative, more general renormalized function f \u0303 reported in the paper by [5], defined as:\n\u221a\nH\u03c8 \u03c8(H \u03c0)\n,\n(11)\nf \u0303 = f + \u221a \u2212\n\u03c0\n\u03c0\nwhere, again, H is an Hamiltonian operator and \u03c8 a quite arbitrary trial\nfunction. In this setting, if H = \u2212 21 \u2206 + V , under the same, mild conditions\ndiscussed in Section 5, f \u0303 has the same expectation as f under \u03c0. This is true\nwithout imposing condition (1), so that now V can be also chosen arbitrarily.\nTherefore, the re-normalization (11) allows for a more general class of Hamiltonians.\nSupplementary Materials\nSupplementary materials are available. In Appendices A, B and C the zero\nvariance estimator and the proof of CLT are given for all the examples. In\nAppendix D computations of unbiasedness conditions discussed in Section 5\nare reported and verified for the three examples.\nAcknowledgement\nThanks are due to D. Bressanini, for bringing to our attention the paper\nby Assaraf and Caffarel and helping us translate it into statistical terms; to\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n15\n\nprof. E. Regazzini and F. Nicola, for discussing the CLT conditions for the\nexamples; P. Tenconi, F. Carone and F. Leisen for comments and contributions\nto a preliminary version of this research. And finally, Assaraf and Caffarel\nthemselves have given us interesting and useful comments that have greatly\nimproved the paper.\n\nAppendix A: Probit model\nMathematical formulation\nLet yi be Bernoulli r.v.'s: yi |xi \u223c B(1, pi ), pi = \u03a6(xTi \u03b2), where \u03b2 \u2208 Rd is\nthe vector of parameters of the model and \u03a6 is the c.d.f. of a standard normal\ndistribution. The likelihood function is:\nl(\u03b2|y, x) \u221d\n\nn\nY\n\u0002\n\u0003yi \u0002\n\u00031\u2212yi\n\u03a6(xTi \u03b2)\n1 \u2212 \u03a6(xTi \u03b2)\n.\ni=1\n\nAs it can be seen by inspection, the likelihood function is invariant under the\ntransformation (xi , yi ) \u2192 (\u2212xi , 1 \u2212 yi ). Therefore, for the sake of simplicity,\nin the rest of the example we assume yi = 1 for any i, so that the likelihood\nsimplifies:\nl(\u03b2|y, x) \u221d\n\nn\nY\n\n\u03a6(xTi \u03b2).\n\ni=1\n\nThis formula shows that the contribution of xi = 0 is just a constant \u03a6(xTi \u03b2) =\n\u03a6(0) = 21 , therefore, without loss of generality, we assume for all i, xi 6= 0.\nUsing flat priors, the posterior of the model is proportional to the likelihood, and the Bayesian estimator of each parameter, \u03b2k , is the expected value\nof fk (\u03b2) = \u03b2k under \u03c0 (k = 1, 2, * * * , d).\np\nUsing Schr\u00f6dinger-type Hamiltonians, H and \u03c8k (\u03b2) = Pk (\u03b2) \u03c0(\u03b2), as the\nPd\ntrial functions, where Pk (\u03b2) = j=1 aj,k \u03b2j is a first degree polynomial, one\ngets:\nd\nX\nH\u03c8k (\u03b2)\n= fk (\u03b2) +\naj,k zj ,\nfek (\u03b2) = fk (\u03b2) + p\n\u03c0(\u03b2|y, x)\nj=1\n\nwhere, for j = 1, 2, . . . , d,\nn\n\nzj = \u2212\n\n1 X xij \u03c6(xTi \u03b2)\n,\n2 i=1 \u03a6(xTi \u03b2)\n\nbecause of the assumption yi = 1 for any i.\n\n\f16\n\nAntonietta Mira et al.\n\nCentral limit theorem\nIn the following, it is supposed that P is a linear polynomial. In the Probit\nmodel, the ZV-MCMC estimators obey a CLT if zj have finite 2 + \u03b4 moment\nunder \u03c0, for some \u03b4 > 0:\n\uf8f9\n\uf8ee\n2+\u03b4\nn\nT\nX\n\u0002 2+\u03b4 \u0003\nx\n\u03c6(x\n\u03b2)\nij\ni\n\uf8fb\nE\u03c0 |zj |\n= c1 E\u03c0 \uf8f0\nT \u03b2)\n\u03a6(x\ni\ni=1\nZ\n= c1 c2\nRd\n\nn\nX\nxij \u03c6(xT \u03b2)\ni\n\n2+\u03b4 n\nY\n\n\u03a6(xTi \u03b2)\n\ni=1\n\n\u03a6(xTi \u03b2)d\u03b2 < \u221e.\n\ni=1\n\nwhere c1 = 2\u22122\u2212\u03b4 , and c2 is the normalizing constant of \u03c0 (the target posterior). Define:\n\nK1 (\u03b2) =\nK2 (\u03b2) =\n\nn\nX\nxij \u03c6(xT \u03b2)\ni\n\n\u03a6(xTi \u03b2)\n\ni=1\nn\nY\n\n2+\u03b4\n\n,\n\n\u03a6(xTi \u03b2),\n\ni=1\n\nK(\u03b2) = K1 (\u03b2)K2 (\u03b2)\nand therefore:\n\u0002\n\u0003\nE\u03c0 |zj |2+\u03b4 = c\n\nZ\nK1 (\u03b2)K2 (\u03b2)d\u03b2.\nRd\n\nwhere c = c1 c2 . Before studying the convergence of this integral, the following\nproperty of the likelihood for the probit model is needed.\nProposition 3 Existence and uniqueness of MLE implies that, for any \u03b20 \u2208\nRd \\ {0}, there exists i such that xTi \u03b20 < 0.\nProof (by contradiction). Uniqueness of MLE implies that xT x is full rank,\nthat is, there is no \u03b20 orthogonal to all observations xi . This can be seen by\ncontradiction: singularity of xT x implies existence of a non-zero \u03b20 orthogonal\nto all observations xi . This fact, in turn, implies l(\u03b2|x, y) = l(\u03b2 + c\u03b20 |x, y)\nand, therefore, l(\u2022|x, y) does not have a unique global maximum.\nNext, assume there exists some \u03b20 \u2208 Rd such that, for any i, xTi \u03b20 > 0.\nThen\nPn \u03b20 is a direction of recession for the negative log-likelihood function\n\u2212 i=1 ln \u03a6(xTi \u03b2) (that is a proper closed convex function). This implies that\nthis function does not have non-empty bounded minimum set ([40]), which\nmeans that the MLE does not exist.\n\u0004\nR\nNow, rewriting Rd K(\u03b2)d\u03b2 in hyper-spherical coordinates through the bijective transformation (\u03c1, \u03b81 , . . . , \u03b8d\u22121 ) := F (\u03b2), where F \u22121 is defined as\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n\uf8f1\n\u03b21 = \u03c1 cos(\u03b81 )\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\nQl\u22121\n\u03b2l = \u03c1 cos(\u03b8l ) m=1 sin(\u03b8m ),\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nQd\u22121\n\uf8f3\n\u03b2d = \u03c1 m=1 sin(\u03b8m ),\n\n17\n\nfor l = 2, ..., d \u2212 1\n\n(12)\n\nfor \u03b8 \u2208 \u0398 := {0 \u2264 \u03b8i \u2264 \u03c0, i = 1, . . . , d \u2212 2, 0 \u2264 \u03b8d\u22121 < 2\u03c0} and \u03c1 > 0, one gets\nZ\n\n+\u221e\n\nZ Z\n\nK(F \u22121 (\u03c1, \u03b8))\u03c1d\u22121\n\nK(\u03b2)d\u03b2 =\nRd\n\n0\n\n\u0398\n\nd\u22122\nY\n\nsind\u2212j (\u03b8j\u22121 ) d\u03c1d\u03b8.\n\nj=2\n+\u221e\n\nZ Z\n\nK(F \u22121 (\u03c1, \u03b8))\u03c1d\u22121 d\u03c1d\u03b8\n\n\u2264\n0\n\n\u0398\n\nZ\n:=\n\nA(\u03b8)d\u03b8,\n\u0398\n\nObserve that the integrand is well defined for any (\u03c1, \u03b8) on the domain of\nintegration, so it is enough to study its asymptotic behaviour when \u03c1 goes to\ninfinity, and \u03b8 \u2208 \u0398.\nFirst, analyze\nK1 (F\n\n\u22121\n\n(\u03c1, \u03b8)) =\n\nn\nX\nxij \u03c6(|xi |\u03c1\u03bbi (\u03b8)\ni=1\n\n\u03a6(|xi |\u03c1\u03bbi (\u03b8))\n\n2+\u03b4\n\n,\n\nwhere, for any i, \u03bbi is a suitable function of the angles \u03b8 such that \u03bbi \u2208\n[\u22121, 1], which takes into account the sign of the scalar product in the original\ncoordinates system.\nFor any i, when \u03c1 \u2192 \u221e\n\u2013 if \u03bbi < 0,\n\u2013 if \u03bbi > 0,\n\u2013 if \u03bbi = 0,\n\nxij \u03c6(|xi |\u03c1\u03bbi )\n\u03a6(|xi |\u03c1\u03bbi )\nxij \u03c6(|xi |\u03c1\u03bbi )\n\u03a6(|xi |\u03c1)\u03bbi\nxij \u03c6(|xi |\u03c1\u03bbi )\n\u03a6(|xi |\u03c1\u03bbi )\n\n\u2208 O (\u03c1);\n\u2208 O (\u03c6(\u03bbi \u03c1));\nq\n= xij \u03c02 \u2208 O (1).\n\nTherefore:\nn\nX\nxij \u03c6(|xi |\u03c1\u03bbi )\ni=1\n\n\u03a6(|xi |\u03c1\u03bbi )\n\n\u2208 O (\u03c1)\n\n\u0001\n\u22121\n2+\u03b4\nand,\n. Now, focus on K2 (F \u22121 (\u03c1, \u03b8)) =\nQn for any \u03b8 \u2208 \u0398: K1 (F (\u03c1, \u03b8)) \u2208 O \u03c1\ni=1 \u03a6(|xi |\u03c1\u03bbi (\u03b8)); existence of MLE for the probit model implies that, for any\n\u03b8 \u2208 \u0398, there exists some l (1 \u2264 l \u2264 n), such that \u03bbl (\u03b8) < 0, and therefore:\nK2 (F \u22121 (\u03c1, \u03b8)) < \u03a6(|xl |\u03c1\u03bbl ) \u2208 O (\u03c6(\u03bbl \u03c1)) \u03c1 \u2192 \u221e.\n\n(13)\n\nPutting these results together leads to\nK(F \u22121 (\u03c1, \u03b8)) = K1 (F \u22121 (\u03c1, \u03b8))K2 (F \u22121 (\u03c1, \u03b8)) \u2208 O \u03c12+\u03b4 \u03c6(\u03bbl (\u03b8)\u03c1)\n\n\u0001\n\n\f18\n\nAntonietta Mira et al.\n\nso that, for any \u03b8 \u2208 \u0398,\n\u0001\nK(F \u22121 (\u03c1, \u03b8))\u03c1d\u22121 \u2208 O \u03c11+\u03b4+d \u03c6 (\u03bbl (\u03b8)\u03c1) ,\n\n\u03c1 \u2192 +\u221e.\n\nTherefore, whenever the value \u03b8 \u2208 \u0398, its integrand converges to zero rapidly\nenough when \u03c1 \u2192 +\u221e. This concludes the proof.\nNote 1. In ([42]) it is shown that the existence of the posterior under flat\npriors for probit and logit models is equivalent to the existence and finiteness of MLE. This ensures us that the posterior is well defined in our context. In order to verify the existence of the posterior mean, we can use a\nsimplified\nR\nQn version of the proof given above. In other words we should show\n\u03b2j i=1 \u03a6(xTi \u03b2)d\u03b2 < +\u221e, that is, K1 (\u03b2) = \u03b2j \u2208 O(\u03c1). Therefore, a weaker\nversion of the proof given above can be employed.\nNote 2. In the proof given above we have used flat priors: although this\nassumption simplifies the proof, however a very similar proof can be applied for\nnon-flat priors. Assume the prior is Q\n\u03c00 (\u03b2). Under this assumption the posterior\nn\nis \u03c0(\u03b2) = \u03c00 (\u03b2) l(\u03b2|y, x) \u221d \u03c00 (\u03b2) i=1 \u03a6(xTi \u03b2) and the control variates are:\nPn x \u03c6(xT \u03b2)\n\u03c00 (\u03b2)\n\u2212 21 i=1 ij\u03a6(xT i\u03b2) . Therefore we need to prove the 2 + \u03b4-th\nzj = \u2212 12 d lnd\u03b2\nj\ni\nmoment of zj under \u03c0(\u03b2) is finite. A sufficient condition for this is the finiteness\nPn x \u03c6(xT \u03b2)\n\u03c00 (\u03b2)\nof 2 + \u03b4-th moments of \u2212 21 d lnd\u03b2\nand \u2212 12 i=1 ij\u03a6(xT i\u03b2) under \u03c0(\u03b2). If we\nj\ni\nassume \u03c00 (\u03b2) is bounded above, the latter is a trivial consequence of the proof\ngiven above for the flat priors. Therefore we only need to prove the finiteness\nof the integral\nZ\nn\n2+\u03b4\nY\nd ln \u03c00 (\u03b2)\n\u03a6(xTi \u03b2)d\u03b2.\n\u03c00 (\u03b2)\nd\u03b2j\nRd\ni=1\nAgain if we assume the prior is bounded from above, a sufficient condition for\nthe existence of this integral is the existence of the following integral:\nZ\nn\n2+\u03b4 Y\nd ln \u03c00 (\u03b2)\n\u03a6(xTi \u03b2)d\u03b2\nd\u03b2j\nRd\ni=1\nA proof very similar to the one given above will show that this integral is finite\nfor common choices of priors \u03c00 (\u03b2) (such as Normal, Student's T, etc).\nAppendix B: Logit model\nMathematical formulation\nexp(xT \u03b2)\n\nd\ni\nIn the same setting as the probit model, let pi = 1+exp(x\nis\nT \u03b2) where \u03b2 \u2208 R\ni\nthe vector of parameters of the model. The likelihood function is:\n\u0013yi \u0012\n\u00131\u2212yi\nn \u0012\nY\nexp(xTi \u03b2)\n1\nl(\u03b2|y, x) \u221d\n.\n(14)\n1 + exp(xTi \u03b2)\n1 + exp(xTi \u03b2)\ni=1\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n19\n\nBy inspection, it is easy to verify that the likelihood function is invariant\nunder the transformation:(xi , yi ) \u2192 (\u2212xi , 1 \u2212 yi ). Therefore, for the sake of\nsimplicity, in the sequel we assume yi = 0 for any i, so that the likelihood\nsimplifies as:\nl(\u03b2|y, x) \u221d\n\nn\nY\n\n1\n.\n1 + exp(xTi \u03b2)\ni=1\n\nThe contribution of xi = 0 to the likelihood is just a constant, therefore,\nwithout loss of generality, it is assumed that xi 6= 0 for all i. Using flat priors,\nthe posterior distribution is proportional to (14) and the Bayesian estimator\nof each parameter, \u03b2k , is the expected value of fk (\u03b2) = \u03b2k under \u03c0 (k =\n1, 2, * * * , d). Using the same pair of operator H and test function \u03c8k as in\nAppendinx A, the control variates are:\nn\n\nzj =\n\nexp(xTi \u03b2)\n1X\n,\nxij\n2 i=1\n1 + exp(xTi \u03b2)\n\nfor\n\nj = 1, 2, . . . , d.\n\nCentral limit theorem\nAs for the probit model, the ZV-MCMC estimators obey a CLT if the control\nvariates zj have finite 2 + \u03b4 moment under \u03c0, for some \u03b4 > 0 :\n\uf8ee\n!2+\u03b4 \uf8f9\nn\nT\nX\n\u0002 2+\u03b4 \u0003\nexp(x\n\u03b2)\ni\n\uf8fb\nE\u03c0 zj\n= c1 E\u03c0 \uf8f0\nxij\n1 + exp(xTi \u03b2)\ni=1\nZ\n= c1 c2\nRd\n\nn\nX\n\nexp(xTi \u03b2)\nxij\n1 + exp(xTi \u03b2)\ni=1\n\n!2+\u03b4\n\nn\nY\n\n1\nd\u03b2\nT \u03b2)\n1\n+\nexp(x\ni\ni=1\n\nwhere c1 = 2\u22122\u2212\u03b4 , and c2 is the normalizing constant of \u03c0. The finitiness of the\nintegral is, indeed, trivial. Observe that the function ey /(1 + ey ) is bounded\nby 1; therefore,\n!2+\u03b4\nn\nX\n\u0002 2+\u03b4 \u0003\nE\u03c0 zj\n\u2264\nxij\n< \u221e.\ni=1\n\nAs for probit model, note that it can easily be shown that the posterior\nmeans exist under flat priors. Moreover, a very similar proof (see Note 2 of\nAppendix A) can be used under a normal or Student's T prior distribution.\nAppendix C: GARCH model\nMathematical formulation\nWe assume that the returns are conditionally normal distributed, r(t)|Ft \u223c\nN (0, ht ), where ht is a predictable (Ft\u22121 measurable process): ht = \u03c91 +\n\n\f20\n\nAntonietta Mira et al.\n\n2\n\u03c93 ht\u22121 + \u03c92 rt\u22121\n, where \u03c91 > 0, \u03c92 \u2265 0, and \u03c93 \u2265 0. Let r = (r1 , . . . , rT ) be\nthe observed time series. The likelihood function is equal to:\n\nl (\u03c91 , \u03c92 , \u03c93 |r) \u221d\n\nT\nY\n\n!\u2212 12\nht\n\nt=1\n\nT\n\n1 X rt2\nexp \u2212\n2 t=1 ht\n\n!\n\nand using independent truncated Normal priors for the parameters, the posterior is:\n!\n!\u2212 21\n\u0014\n\u0012\n\u0013\u0015 Y\nT\nT\n1\n\u03c912\n\u03c922\n\u03c932\n1 X rt2\n\u03c0 (\u03c91 , \u03c92 , \u03c93 |r) \u221d exp \u2212\n+\n+\nht\nexp \u2212\n.\n2 \u03c3 2 (\u03c91 ) \u03c3 2 (\u03c92 ) \u03c3 2 (\u03c93 )\n2 t=1 ht\nt=1\nTherefore, the control variates (when the trial function is a first degree polynomial) are:\nT\n\nT\n\n\u2202 ln \u03c0\n\u03c9i\n1 X 1 \u2202ht\n1 X rt2 \u2202ht\n=\u2212 2\n\u2212\n+\n,\n\u2202\u03c9i\n\u03c3 (\u03c9i ) 2 t=1 ht \u2202\u03c9i\n2 t=1 h2t \u2202\u03c9i\n\ni = 1, 2, 3,\n\nwhere:\n1 \u2212 \u03c93t\u22121 \u2202ht\n\u2202ht\n=\n,\n=\n\u2202\u03c91\n1 \u2212 \u03c93\n\u2202\u03c92\n\n\u0012\n\n2\nrt\u22121\n+ \u03c93\n\n\u2202ht\u22121\n\u2202\u03c92\n\n\u0013\nIt>1 ,\n\n\u2202ht\n=\n\u2202\u03c93\n\n\u0012\nht\u22121 + \u03c93\n\n\u2202ht\u22121\n\u2202\u03c93\n\nCentral limit theorem\nIn order to prove the CLT for the ZV-MCMC estimator in the Garch model,\nwe need:\n\u2202 ln \u03c0 \u2202 ln \u03c0 \u2202 ln \u03c0\n,\n,\n\u2208 L2+\u03b4 (\u03c0).\n(15)\n\u2202\u03c92\n\u2202\u03c93\n\u2202\u03c91\nTo this end, ht and its partial derivatives should be expressed as a function of\nh0 and r:\nht = \u03c91 (\n\nt\u22121\nX\n\n1 + \u03c93k ) + \u03c93t h0 + \u03c92 (\n\nk=1\n\n\u2202 ln ht\n=\n\u2202\u03c91\n\nt\u22121\nX\n\n2\n1 + \u03c93k rt\u22121\u2212k\n),\n\nk=1\n1\u2212\u03c93t\u22121\n1\u2212\u03c93 I{t>1} ,\n\n\uf8eb\n\uf8f6\nt\u22122\nX\n\u2202 ln ht\n2\n= \uf8edrt\u22121\n+\n\u03c93t\u22121\u2212j rj2 \uf8f8 I{t>1} ,\n\u2202\u03c92\nj=0\n\uf8eb\n\uf8f6\nt\u22122\nX\n\u2202 ln ht\n\u03c93t\u22121\u2212j hj \uf8f8 I{t>1} .\n= \uf8edht\u22121 +\n\u2202\u03c93\nj=0\n\n\u0013\nIt>1 .\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n21\n\nNext, moving to spherical coordinates, the integral (15) can be written as\nZ\n\n\u221e\n\nZ\n\nZ\nKj (\u03c1; \u03b8, \u03c6)d\u03c1d\u03b8d\u03c6 :=\n\n[0,\u03c0/2]2\n\nAj (\u03b8, \u03c6)d\u03b8d\u03c6,\n[0,\u03c0/2]2\n\n0\n\nwhere, for j = 1, 2, 3, Kj (*; \u03b8, \u03c6) = |Wj |2+\u03b4 \u00d7 W , with\nT\n\n1 \u2212 \u03c1t\u22121 cost\u22121 \u03c6\n,\n1 \u2212 \u03c1 cos \u03c6\n\uf8eb\n\uf8f6\n\u0013\nt\u22122\nT \u0012\n2\nX\nX\n1\nr\n1\n2\n1\n\u03c1 sin \u03b8 sin \u03c6 \u2212\n\u2212 t2 \uf8edrt\u22121\n+\nx3t\u22121\u2212j rj2 \uf8f8 ,\nW2 = \u2212 \u03c32 (\u03c9\n2)\n2 t=2 h\u0303t\nh\u0303t\nj=0\n\uf8eb\n\uf8f6\n\u0013\n\u0012\nT\nt\u22122\n2\nX\nX\n1\nr\n1\n1\n\u03c1 cos \u03c6 \u2212\n\u2212 t2 \uf8edh\u0303t\u22121 +\nx3t\u22121\u2212j h\u0303j \uf8f8 ,\nW3 = \u2212 \u03c32 (\u03c9\n3)\n2 t=2 h\u0303t\nh\u0303t\nj=0\n1\n\u03c1 cos \u03b8 sin \u03c6 \u2212\nW1 = \u2212 \u03c32 (\u03c9\n1)\n\nW = exp\n\n2\n\n1\n\u2212 12 \u03c12 ( \u03c32 (\u03c9\n1)\n\n\u00d7\u03c1 sin \u03b8\n\nT\nY\n\n1X\n2 t=2\n\n\u0012\n\n2\n\n2\n\nr2\n1\n\u2212 t2\nh\u0303t\nh\u0303t\n\ncos \u03b8 sin \u03c6 +\n\n\u0013\n\n1\n\u03c3 2 (\u03c92 )\n\n2\n\n2\n\nsin \u03b8 sin \u03c6 +\n\n1\n\u03c3 2 (\u03c93 )\n\n2\n\ncos \u03c6) \u2212\n\n1\n2\n\nT\nX\nrt2\n2\nt=1 h\u0303t\n\n!\u2212 21\nh\u0303t\n\nt=1\n\n(16)\nand\n\nh\u0303t = \u2212\u03c1 cos \u03b8 sin \u03c6\n\nt\u22121\nX\n\n(1+\u03c1k cosk \u03c6)+h0 \u03c1t cost \u03c6+\u03c1 sin \u03b8 sin \u03c6\n\nk=1\n\nt\u22121\nX\n\n2\n(1+rt\u22121\u2212k\n\u03c1k cosk \u03c6).\n\nk=1\n\nThe aim is to prove that, for any \u03b8, \u03c6 \u2208 [0, \u03c0/2] and for any j, Aj (\u03b8, \u03c6) is finite.\nTo this end, the convergence of Aj for any \u03b8, \u03c6 should be discussed.\nLet us study the proper domain of Kj (*; \u03b8, \u03c6). Observe that Kj (*; \u03b8, \u03c6) is\nnot defined whenever h\u0303t = 0 and, if j = 1 and \u03c6 6= \u03c0/2, also for \u03c1 = 1/ cos \u03c6.\nHowever, the discontinuity of K3 at this point is removable, so that the domain\nof K3 can be extended by continuity also at \u03c1 = 1/ cos \u03c6.\nSince h\u0303t = 0 if and only if \u03c1 = 0, it can be concluded that, for any j and for\nany \u03b8, \u03c6 \u2208 [0, \u03c0/2], the proper domain of Kj (*; \u03b8, \u03c6) is domKj (*; \u03b8, \u03c6) = (0+\u221e).\nBy fixing the value of \u03b8 and \u03c6, let us study the limits of Kj when \u03c1 \u2192 0 and\n\u03c1 \u2192 +\u221e. Observe that, whatever the values of \u03b8 and \u03c6 are, Wj 's are rationale\nfunctions of \u03c1. Therefore, for any j, |Wj |2+\u03b4 cannot grow towards infinity more\nthan polynomially at the boundary of the domain. On the other hand, W goes\nto zero with an exponential rate both when \u03c1 \u2192 0 and \u03c1 \u2192 +\u221e, for any \u03b8 and\n\u03c6. This is sufficient to conclude that, for any \u03b8, \u03c6 \u2208 [0, \u03c0/2], the integral Aj is\nfinite for j = 1, 2, 3 and, therefore, condition (15) holds and the ZV estimators\nfor the GARCH model obeys a CLT.\n\n!\n\u00d7\n\n\f22\n\nAntonietta Mira et al.\n\nAppendix D: unbiasedness\nIn this Appendix, explicit computations are presented, which were omitted in\nSection 5. Moreover, it is proved that all the ZV-MCMC estimators discussed\nin Section 8 are unbiased.\nFollowing the same notations as in Section 5, equation (9) follows because\n\u001c\n\u001d\nZ\n\u221a\nH\u03c8\n\u221a\n:=\nH\u03c8 \u03c0\n\u03c0\n\u03a9\nZ\n\u221a\n\u221a\n1\n=\n(V \u03c8 \u03c0 \u2212 \u2206\u03c8 \u03c0)\n2\n\u03a9\nZ\nZ\nZ\n\u221a\n\u221a\n\u221a\n1\n1\n=\nV \u03c0\u03c8 \u2212\n\u03c0\u2207\u03c8 * nd\u03c3 +\n\u2207 \u03c0 * \u2207\u03c8\n2 \u2202\u03a9\n2 \u03a9\n\u03a9\nZ\nZ\nZ\nZ\n\u221a\n\u221a\n\u221a\n\u221a\n1\n1\n1\n=\nV \u03c0\u03c8 \u2212\n\u03c8\u2207 \u03c0 * nd\u03c3 \u2212\n\u03c8\u2206 \u03c0\n\u03c0\u2207\u03c8 * nd\u03c3 +\n2\n2\n2\n\u03a9\n\u2202\u03a9\n\u2202\u03a9\n\u03a9\nZ\nZ\n\u221a\n\u221a\n\u221a\n1\n=\n[\u03c8\u2207 \u03c0 \u2212 \u03c0\u2207\u03c8] * nd\u03c3\n(H \u03c0)\u03c8 +\n2 \u2202\u03a9\n\u03a9\nZ\n\u221a\n\u221a\n1\n=\n[\u03c8\u2207 \u03c0 \u2212 \u03c0\u2207\u03c8] * nd\u03c3.\n2 \u2202\u03a9\n\u001c\nTherefore,\n\nH\u03c8\n\u221a\n\u03c0\n\n\u001d\n\n\u221a\n\u221a\n\u221a\n= 0 if \u03c8\u2207 \u03c0 = \u03c0\u2207\u03c8 on \u2202\u03a9. Now, let \u03c8 = P \u03c0.\n\nThen,\n\u2207\u03c8 =\n\u001c\nso that\n\nH\u03c8\n\u221a\n\u03c0\n\n\u221a\n\nP\n\u03c0\u2207P + \u221a \u2207\u03c0,\n2 \u03c0\n\n\u001d\n= 0 if\n\u03c0(x)\n\n\u2202P (x)\n= 0, \u2200x \u2208 \u2202\u03a9, j = 1, . . . , d.\n\u2202xj\n\nWhen \u03c0 has unbounded support, following the previous computations integrating over the bounded set Br and taking the limit for r \u2192 \u221e, one gets\n\u001c\n\u001d\nZ\nH\u03c8\n1\n\u221a\nlim\n\u03c0\u2207P * nd\u03c3.\n(17)\n=\n2 r\u2192+\u221e \u2202Br\n\u03c0\nTherefore, unbiasedness in the unbounded case is reached if the limit appearing\nin the right-hand side of (17) is zero.\nNow, the unbiasedness of the ZV-MCMC estimators exploited in Section\n8 is discussed. To this end, condition (17) should be verified. Let B\u03c1 be a\nhyper-sphere of radius \u03c1 and let n := \u03c11 \u03b2 be its normal versor. Then, for linear\nP , (17) equals zero if, for any j = 1, . . . , d,\nZ\n1\nlim\n\u03c0(\u03b2)\u03b2j dS = 0.\n(18)\n\u03c1\u2192+\u221e \u03c1 B\n\u03c1\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n23\n\nThe Probit model is first considered. By using the same notations as in\nAppendix A, the integral in (18) is proportional to\nZ\n1\nlim\nK2 (F \u22121 (\u03c1, \u03b8))\u03c1d d\u03b8.\n(19)\n\u03c1\u2192+\u221e \u03c1 \u0398\nNote that, because of (13), there exist \u03c10 and M such that\nK2 (F \u22121 (\u03c1, \u03b8))\u03c1d \u2264 M \u03c6(\u03bbl(\u03b8) \u03c1)\u03c1d\n\u2264 M \u03c6(\u03bbl(\u03b8) \u03c10 )\u03c1d0 := G(\u03b8) \u2200\u03c1 \u2265 \u03c10 .\nSince G(\u03b8) \u2208 L1 , by the dominated convergence theorem a sufficient condition\nto get unbiasedness is\nlim K2 (F \u22121 (\u03c1, \u03b8))\u03c1d\u22121 = 0,\n\n(20)\n\n\u03c1\u2192+\u221e\n\nwhich is true, because of (13) for the Probit model.\nWe now consider the Logit model for which it is easy to prove that Proposition 3 holds. As done for the Probit model, one can write\nZ\n\u0002 2+\u03b4 \u0003\nE\u03c0 zj\n\u221d\nK1 (\u03b2)K2 (\u03b2)d\u03b2,\nRd\n\nwhere\nK1 (\u03b2) =\nK2 (\u03b2) =\n\nn\nX\n\nexp(xTi \u03b2)\nxij\n1 + exp(xTi \u03b2)\ni=1\n\n!2+\u03b4\n,\n\nn\nY\n\n1\n,\n1 + exp(xTi \u03b2)\ni=1\n\nBy using the hyper-spherical change of variables in (12), we get\nZ Z \u221e\n\u0002 2+\u03b4 \u0003\nE\u03c0 zj\n\u221d\nK1 (F \u22121 (\u03c1, \u03b8))K2 (F \u22121 (\u03c1, \u03b8))\u03c1d\u22121 d\u03c1 :\n\u0398\n\n0\n\nand, as for the Probit model, we must verify Equation (19). Now analyze\nK2 (F \u22121 (\u03c1, \u03b8)); for any \u03b8, existence of MLE implies the existence of some l\n(1 \u2264 l \u2264 n), such that \u03bbl (\u03b8) > 0, and therefore:\nK2 (F \u22121 (\u03c1, \u03b8)) =\n\nn\nY\n\n1\n1\n+\nexp(|x\ni |\u03c1\u03bbi )\ni=1\n\n1\n1 + exp(|xl |\u03c1\u03bbl )\n\u2208 O (exp(\u2212\u03c1\u03bbl )) .\n\n<\n\n(21)\n\n\f24\n\nAntonietta Mira et al.\n\nTherefore, there exist \u03c10 , M such that\nK2 (F \u22121 (\u03c1, \u03b8))\u03c1d \u2264 M exp(\u2212\u03bbl(\u03b8) \u03c1)\u03c1d\n\u2264 M exp(\u2212\u03bbl(\u03b8) \u03c10 )\u03c1d0 := G(\u03b8) \u2200\u03c1 \u2265 \u03c10 ,\nwhere G(\u03b8) \u2208 L1 . These computations allow us to use Equation (20) as a\nsufficient condition to get unbiasedness, and its proof becomes trivial.\nFinally consider the GARCH model. In this case, B\u03c1 is the portion of a\nsphere of radius \u03c1 defined on the positive orthant. Then, the limit\nZ\n1\nW (F \u22121 (\u03c1, \u03b8))\u03c1d d\u03b8,\nlim\n\u03c1\u2192+\u221e \u03c1 [0,\u03c0/2]2\nwhere W was defined in (16), should be discussed. Again, an application of\nthe dominated convergence theorem leads to the simpler condition\nlim W (F \u22121 (\u03c1, \u03b8))\u03c12 = 0,\n\n\u03c1\u2192+\u221e\n\nwhich is true, since W decays with an exponential rate.\n\nReferences\n1. Adler, S.: Over-relaxation method for the Monte Carlo evaluation of the partition function for multiquadratic actions. Phys. Rev. D 23, 2901\u20132904 (1981)\n2. Albert, J., Chib, S.: Bayesian analysis of binary and polychotomous response data.\nJournal of the American Statistical Association 88, 422, 669\u2013679 (1993)\n3. Ardia, D.: Financial risk management with bayesian estimation of GARCH models:\nTheory and applications. In: Lecture Notes in Economics and Mathematical Systems\n612. Springer-Verlag (2008)\n4. Assaraf, R., Caffarel, M.: Zero-Variance principle for Monte Carlo algorithms. Physical\nReview letters 83, 23, 4682\u20134685 (1999)\n5. Assaraf, R., Caffarel, M.: Zero-variance zero-bias principle for observables in quantum\nMonte Carlo: Application to forces. The Journal of Chemical Physics 119, 20, 10,536\u2013\n10,552 (2003)\n6. Barone, P., Frigessi, A.: Improving stochastic relaxation for Gaussian random fields.\nProbability in the Engineering and Informational Sciences 4, 369\u2013389 (1989)\n7. Barone, P., Sebastiani, G., Stander, J.: General over-relaxation Markov chain Monte\nCarlo algorithms for Gaussian densities. Statistics & Probability Letters 52,2, 115\u2013124\n(2001)\n8. Bollerslev, T.: Generalized autoregressive conditional heteroskedasticity. Journal of\nEconometrics 31, 3, 307\u2013327 (1986)\n9. Bollerslev, T.: Glossary to ARCH (GARCH). In: Volatility and Time Series Econometrics, Essays in Honor of Robert Engle, Edited by Tim Bollerslev, Jeffrey Russell and\nMark Watson. Oxford University Press, Oxford, UK (2010)\n10. Brewer, M., Aitken, C., Talbot, M.: A comparison of hybrid strategies for Gibbs sampling in mixed graphical models. Computational Statistics 21, 343\u2013365 (1996)\n11. Brooks, S., Gelman, A.: Some issues in monitoring convergence of iterative simulations.\nComputing Science and Statistics (1998)\n12. Craiu, R., Lemeieux, C.: Acceleration of the multiple-try Metropolis algorithm using\nantithetic and stratified sampling. Journal Statistics and Computing 17, 2, 109\u2013120\n(2007)\n\n\fZero Variance Markov Chain Monte Carlo for Bayesian Estimators\n\n25\n\n13. Craiu, R., Meng, X.: Multiprocess parallel antithetic coupling for backward and forward\nMarkov chain Monte Carlo. The Annals of Statistics 33, 2, 661\u2013697 (2005)\n14. Dellaportas, P., Kontoyiannis, I.: Control variates for estimation based on reversible\nMarkov chain Monte Carlo samplers. Journal of the Royal Statistical Society, Series B.\n74(1), 133\u2013161 (2012)\n15. Diaconis, P., Holmes, S., Neal, R.F.: Analysis of a nonreversible Markov chain sampler.\nAnn. Appl. Probab. 10,3, 726\u2013752 (2000)\n16. Duane, S., Kennedy, A., Pendleton, B., Roweth, D.: Hybrid Monte Carlo. Physics\nLetters B 195, 216\u2013222 (2010)\n17. Flury, B., Riedwyl, H.: Multivariate Statistics. Chapman and Hall (1988)\n18. Fort, G., Moulines, E., Roberts, G., Rosenthal, S.: On the geometric ergodicity of hybrid\nsamplers. Journal of Applied Probability 40, 1, 123\u2013146 (2003)\n19. Gelfand, A., Smith, A.: Sampling-based approaches to calculating marginal densities.\nJ. American Statistical Association 85, 398\u2013409 (1990)\n20. Girolami, M., Calderhead, B.: Riemannian manifold Langevin and Hamiltonian Monte\nCarlo methods. J. R. Statist. Soc. B 73, 2, 1\u201337 (2011)\n21. Green, P., Han, X.: Metropolis methods, Gaussian proposals, and antithetic variables.\nIn: P. Barone, A. Frigessi, M. Piccioni (eds.) Lecture Notes in Statistics, Stochastic\nMethods and Algorithms in Image Analysis, vol. 74, pp. 142\u2013164. Springer Verlag (1992)\n22. Green, P.J., Mira, A.: Delayed rejection in reversible jump Metropolis-Hastings.\nBiometrika 88, 1035\u20131053 (2001)\n23. Hastings, W.K.: Monte Carlo sampling methods using Markov chains and their applications. Biometrika 57, 97\u2013109 (1970)\n24. Henderson, S.: Variance reduction via an approximating Markov process. Ph.D. thesis,\nDepartment of Operations Research, Stanford University, Stanford, CA (1997)\n25. Henderson, S., Glynn, P.: Approximating martingales for variance reduction in Markov\nprocess simulation. Math. Oper. Res. 27, 2, 253\u2013271 (2002)\n26. Higdon, D.: Auxiliary variable methods for Markov chain Monte Carlo with applications.\nJournal of the American Statistical Association 93, 585\u2013595 (1998)\n27. Ishwaran, H.: Applications of hybrid Monte Carlo to Bayesian generalized linear models:\nquasicomplete separation and neural networks. J. Comp. Graph. Statist. 8, 779\u2013799\n(1999)\n28. Leisen, F., Dalla Valle, L.: A new multinomial model and a zero variance estimation.\nCommunications in Statistics - Simulation and Computation 39(4), 846\u2013859 (2010)\n29. Linnik, Y.V.: An information-theoretic proof of the central limit theorem with Lindeberg\nconditions. Theory of Probability and its Applications 4, 288\u2013299 (1959)\n30. Loh, W.: Methods of control variates for discrete event simulation. Ph.D. thesis, Department of Operations Research, Stanford University, Stanford, CA (1994)\n31. Marin, J.M., Robert, C.: Bayesian Core: A Practical Approach to Computational\nBayesian Statistics. Springer (2007)\n32. Mira, A., Geyer, C.J.: On reversible Markov chains. Fields Inst. Communic.: Monte\nCarlo Methods 26, 93\u2013108 (2000)\n33. Mira, A., M\u00f6ller, J., Roberts, G.O.: Perfect slice samplers. Journal of the Royal Statistical Soc. Ser. B 63, 3, 593\u2013606 (2001)\n34. Mira, A., Tierney, L.: Efficiency and convergence properties of slice samplers. Scandinavian Journal of Statistics 29, 1\u201312 (2002)\n35. Neal, R.: An improved acceptance procedure for the hybrid Monte Carlo algorithm.\nJournal of Computational Physics 111, 194\u2013203 (1994)\n36. Neal, R.M.: Suppressing random walks in Markov chain Monte Carlo using ordered\noverrelaxation. Tech. rep., Learning in Graphical Models (1995)\n37. Nelson, B.: Batch size effects on the efficiency of control variates in simulation. European\nJournal of Operational Research 2(27), 184\u2013196 (1989)\n38. Philippe, A., Robert, C.: Riemann sums for MCMC estimation and convergence monitoring. Statistics and Computing 11, 103\u2013105 (2001)\n39. Ripley, B.: Stochastic Simulation. John Wiley & Sons (1987)\n40. Rockafellar, R.: Convex analysis, pp. 264\u2013265. Princeton University Press (1970)\n41. So, M.K.P.: Bayesian analysis of nonlinear and non-Gaussian state space models via\nmultiple-try sampling methods. Statistics and Computing 16, 125\u2013141 (2006)\n\n\f26\n\nAntonietta Mira et al.\n\n42. Speckman P.L. Lee, J., Sun, D.: Existence of the mle and propriety of posteriors for a\ngeneral multinomial choice model. Statistica Sinica 19, 731\u2013748 (2009)\n43. Swendsen, R., Wang, J.: Non universal critical dynamics in Monte Carlo simulations.\nPhys. Rev. Lett. 58, 86\u201388 (1987)\n44. Tierney, L.: Markov chains for exploring posterior distributions. Annals of Statistics\n22, 1701\u20131762 (1994)\n45. Tierney, L., Mira, A.: Some adaptive Monte Carlo methods for Bayesian inference.\nStatistics in Medicine 18, 2507\u20132515 (1999)\n46. Van Dyk, D., Meng, X.: The art of data augmentation. Journal of Computational and\nGraphical Statistics 10, 1\u201350 (2001)\n\n\f"}