{"id": "http://arxiv.org/abs/1104.2407v1", "guidislink": true, "updated": "2011-04-13T08:24:53Z", "updated_parsed": [2011, 4, 13, 8, 24, 53, 2, 103, 0], "published": "2011-04-13T08:24:53Z", "published_parsed": [2011, 4, 13, 8, 24, 53, 2, 103, 0], "title": "Parameter Expansion and Efficient Inference", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.2542%2C1104.4151%2C1104.2061%2C1104.2536%2C1104.0724%2C1104.4626%2C1104.4527%2C1104.1631%2C1104.1341%2C1104.4821%2C1104.1225%2C1104.4664%2C1104.5297%2C1104.1372%2C1104.2562%2C1104.4125%2C1104.3752%2C1104.0407%2C1104.4593%2C1104.3520%2C1104.0718%2C1104.4551%2C1104.1837%2C1104.2767%2C1104.0182%2C1104.1374%2C1104.2753%2C1104.1809%2C1104.4403%2C1104.0139%2C1104.4292%2C1104.5352%2C1104.0154%2C1104.5686%2C1104.1678%2C1104.3974%2C1104.3186%2C1104.3098%2C1104.4541%2C1104.4335%2C1104.2452%2C1104.1400%2C1104.5254%2C1104.1447%2C1104.3110%2C1104.5301%2C1104.4550%2C1104.1536%2C1104.5659%2C1104.0666%2C1104.3706%2C1104.2911%2C1104.4500%2C1104.2780%2C1104.4732%2C1104.1939%2C1104.3839%2C1104.4193%2C1104.0567%2C1104.4462%2C1104.4353%2C1104.3871%2C1104.1912%2C1104.0590%2C1104.0282%2C1104.4766%2C1104.2981%2C1104.1351%2C1104.5632%2C1104.1624%2C1104.1761%2C1104.1629%2C1104.3373%2C1104.2407%2C1104.3038%2C1104.3144%2C1104.1886%2C1104.2430%2C1104.4652%2C1104.0114%2C1104.0036%2C1104.2751%2C1104.3857%2C1104.2666%2C1104.0299%2C1104.5604%2C1104.0658%2C1104.4934%2C1104.4917%2C1104.4010%2C1104.2170%2C1104.1760%2C1104.2711%2C1104.2622%2C1104.3772%2C1104.3021%2C1104.1242%2C1104.3291%2C1104.4872%2C1104.0894%2C1104.5064&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Parameter Expansion and Efficient Inference"}, "summary": "This EM review article focuses on parameter expansion, a simple technique\nintroduced in the PX-EM algorithm to make EM converge faster while maintaining\nits simplicity and stability. The primary objective concerns the connection\nbetween parameter expansion and efficient inference. It reviews the statistical\ninterpretation of the PX-EM algorithm, in terms of efficient inference via bias\nreduction, and further unfolds the PX-EM mystery by looking at PX-EM from\ndifferent perspectives. In addition, it briefly discusses potential\napplications of parameter expansion to statistical inference and the broader\nimpact of statistical thinking on understanding and developing other iterative\noptimization algorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.2542%2C1104.4151%2C1104.2061%2C1104.2536%2C1104.0724%2C1104.4626%2C1104.4527%2C1104.1631%2C1104.1341%2C1104.4821%2C1104.1225%2C1104.4664%2C1104.5297%2C1104.1372%2C1104.2562%2C1104.4125%2C1104.3752%2C1104.0407%2C1104.4593%2C1104.3520%2C1104.0718%2C1104.4551%2C1104.1837%2C1104.2767%2C1104.0182%2C1104.1374%2C1104.2753%2C1104.1809%2C1104.4403%2C1104.0139%2C1104.4292%2C1104.5352%2C1104.0154%2C1104.5686%2C1104.1678%2C1104.3974%2C1104.3186%2C1104.3098%2C1104.4541%2C1104.4335%2C1104.2452%2C1104.1400%2C1104.5254%2C1104.1447%2C1104.3110%2C1104.5301%2C1104.4550%2C1104.1536%2C1104.5659%2C1104.0666%2C1104.3706%2C1104.2911%2C1104.4500%2C1104.2780%2C1104.4732%2C1104.1939%2C1104.3839%2C1104.4193%2C1104.0567%2C1104.4462%2C1104.4353%2C1104.3871%2C1104.1912%2C1104.0590%2C1104.0282%2C1104.4766%2C1104.2981%2C1104.1351%2C1104.5632%2C1104.1624%2C1104.1761%2C1104.1629%2C1104.3373%2C1104.2407%2C1104.3038%2C1104.3144%2C1104.1886%2C1104.2430%2C1104.4652%2C1104.0114%2C1104.0036%2C1104.2751%2C1104.3857%2C1104.2666%2C1104.0299%2C1104.5604%2C1104.0658%2C1104.4934%2C1104.4917%2C1104.4010%2C1104.2170%2C1104.1760%2C1104.2711%2C1104.2622%2C1104.3772%2C1104.3021%2C1104.1242%2C1104.3291%2C1104.4872%2C1104.0894%2C1104.5064&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This EM review article focuses on parameter expansion, a simple technique\nintroduced in the PX-EM algorithm to make EM converge faster while maintaining\nits simplicity and stability. The primary objective concerns the connection\nbetween parameter expansion and efficient inference. It reviews the statistical\ninterpretation of the PX-EM algorithm, in terms of efficient inference via bias\nreduction, and further unfolds the PX-EM mystery by looking at PX-EM from\ndifferent perspectives. In addition, it briefly discusses potential\napplications of parameter expansion to statistical inference and the broader\nimpact of statistical thinking on understanding and developing other iterative\noptimization algorithms."}, "authors": ["Andrew Lewandowski", "Chuanhai Liu", "Scott Vander Wiel"], "author_detail": {"name": "Scott Vander Wiel"}, "author": "Scott Vander Wiel", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/10-STS348", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1104.2407v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1104.2407v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/10-STS348 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1104.2407v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1104.2407v1", "journal_reference": "Statistical Science 2010, Vol. 25, No. 4, 533-544", "doi": "10.1214/10-STS348", "fulltext": "Statistical Science\n2010, Vol. 25, No. 4, 533\u2013544\nDOI: 10.1214/10-STS348\nc Institute of Mathematical Statistics, 2010\n\nParameter Expansion and Efficient\nInference\narXiv:1104.2407v1 [stat.ME] 13 Apr 2011\n\nAndrew Lewandowski, Chuanhai Liu and Scott Vander Wiel\n\nAbstract. This EM review article focuses on parameter expansion, a\nsimple technique introduced in the PX-EM algorithm to make EM converge faster while maintaining its simplicity and stability. The primary\nobjective concerns the connection between parameter expansion and\nefficient inference. It reviews the statistical interpretation of the PXEM algorithm, in terms of efficient inference via bias reduction, and\nfurther unfolds the PX-EM mystery by looking at PX-EM from different perspectives. In addition, it briefly discusses potential applications\nof parameter expansion to statistical inference and the broader impact\nof statistical thinking on understanding and developing other iterative\noptimization algorithms.\nKey words and phrases: EM algorithm, PX-EM algorithm, robit regression, nonidentifiability.\nplementations of the original method. Among these\nEM-type algorithms are the expectation-conditional\nmaximization (ECM) algorithm of Meng and Rubin (1993), the expectation-conditional maximization either (ECME) algorithm of Liu and Rubin\n(1994), the alternating ECM (AECM) algorithm of\nMeng and van Dyk (1997) and, more recently, the\ndynamic ECME (DECME) algorithm of He and Liu\n(2009). This review article focuses on parameter expansion as a way of improving the performance of\nthe EM algorithm through a discussion of the parameter expansion EM (PX-EM) algorithm proposed\nby Liu, Rubin and Wu (1998).\nThe EM algorithm is an iterative algorithm for\nmaximum likelihood (ML) estimation from incomplete data. Let Xobs be the observed data and let\nf (Xobs ; \u03b8) denote the observed-data model with unknown parameter \u03b8, where Xobs \u2208 Xobs and \u03b8 \u2208 \u0398.\nSuppose that the observed-data model can be obtained from a complete-data model, denoted by\ng(Xobs , Xmis ; \u03b8), where Xobs \u2208 Xobs , Xmis \u2208 Xmis , and\n\u03b8 \u2208 \u0398. That is,\nZ\ng(Xobs , Xmis ; \u03b8) dXmis .\nf (Xobs ; \u03b8) =\n\n1. INTRODUCTION\nThe expectation maximization (EM) algorithm of\nDempster, Laird and Rubin (1977) has proven to be\na popular computational method for optimization.\nWhile simple to implement and stable in its convergence, the EM algorithm can converge slowly. Many\nvariants of the original EM algorithm have also been\nproposed in the last 30+ years in order to overcome shortcomings that are sometimes seen in imAndrew Lewandowski is Ph.D. Student, Department of\nStatistics, Purdue University, 150 N. University Street,\nWest Lafayette, Indiana 47907, USA e-mail:\nalewand@purdue.edu. Chuanhai Liu is Professor of\nStatistics, Department of Statistics, Purdue University,\n150 N. University Street, West Lafayette, Indiana\n47907, USA e-mail: chuanhai@purdue.edu; URL:\nwww.stat.purdue.edu. Scott Vander Wiel is Technical\nStaff Member, Statistical Sciences Group, MS F600, Los\nAlamos National Laboratory, Los Alamos, New Mexico\n87545, USA e-mail: scottv@lanl.gov; URL:\nwww.stat.lanl.gov.\nThis is an electronic reprint of the original article\npublished by the Institute of Mathematical Statistics in\nStatistical Science, 2010, Vol. 25, No. 4, 533\u2013544. This\nreprint differs from the original in pagination and\ntypographic detail.\n\nXmis\n\nGiven a starting point \u03b8 (0) \u2208 \u0398, the EM algorithm\niterates for t = 0, 1, . . . between the expectation (E)\nstep and maximization (M) step:\n1\n\n\f2\n\nA. LEWANDOWSKI, C. LIU AND S. VANDER WIEL\n\nE step. Compute the expected complete-data loglikelihood\nQ(\u03b8|\u03b8 (t) )\n(1.1)\n\n= E(ln g(Xobs , Xmis ; \u03b8)|Xobs , \u03b8 = \u03b8 (t) )\n\nas a function of \u03b8 \u2208 \u0398; and\nM step. Maximize Q(\u03b8|\u03b8 (t) ) to obtain\n(1.2)\n\n\u03b8 (t+1) = arg max Q(\u03b8|\u03b8 (t) ).\n\u03b8\u2208\u0398\n\nTwo EM examples are given in Section 2.\nRoughly speaking, the E step can be viewed as creating a complete-data problem by imputing missing\nvalues, and the M step can be understood as conducting a maximum likelihood-based analysis. More\nexactly, for complete-data models belonging to the\nexponential family, the E step imputes the completedata sufficient statistics with their conditional expectations given the observed data and the current\nestimate \u03b8 (t) of the parameter \u03b8. This to some extent explains the simplicity of EM. The particular\nchoice of (1.1) together with Jensen's inequality implies monotone convergence of EM.\nThe PX-EM algorithm is essentially an EM algorithm, but it performs inference on a larger full\nmodel. This model is obtained by introducing extra\nparameters into the complete-data model while preserving the observed-data sampling model. Section\n3.1 presents the structure used in PX-EM. The theoretical results established by Liu, Rubin and Wu\n(1998) show that PX-EM converges no slower than\nits parent EM. This is somewhat surprising, as it\nis commonly believed that optimization algorithms\ngenerally converge slower as the number of dimensions increases. To help understand the behavior of\nPX-EM, Liu, Rubin and Wu (1998) provided a statistical interpretation of the PX-M step in terms of\ncovariance adjustment. This is reviewed in Section\n3.2 in terms of bias reduction using the example of\nbinary regression with a Student-t link (see Mudholkar and George, 1978; Albert and Chib, 1993;\nLiu, 2004), which serves as a simple robust alternative to logistic regression and is called robit regression by Liu (2004).\nTo help further understand why PX-EM can work\nso well, several relevant issues are discussed in Section 4. Section 4.1 provides additional motivation\nbehind why PX-EM can improve upon EM or ECM.\nIn Section 4.2 we argue that parameter expansion\ncan also be used for efficient data augmentation in\n\nthe E step. The resulting EM is effectively the PXEM algorithm.\nIn addition to the models discussed here, parameter expansion has now been shown to have computational advantages in applications such as factor\nanalysis (Liu, Rubin and Wu, 1998) and the analysis of both linear (Gelman et al., 2008) and nonlinear (Lavielle and Meza, 2007) hierarchical models.\nHowever, Gelman (2004) shows that parameter expansion offers more than a computational method\nto accelerate EM. He points out that parameter expansion can be viewed as part of a larger perspective on iterative simulation (see Liu and Wu, 1999;\nMeng and van Dyk, 1999; van Dyk and Meng, 2001;\nLiu, 2003) and that it suggests a new family of prior\ndistributions in a Bayesian framework discussed by\nGelman (2006). One example is the folded noncentral Student-t distribution for between-group variance parameters in hierarchical models. This method\nexploits a parameter expansion technique commonly\nused in hierarchical models, and Gelman (2006) shows\nthat it can be more robust than the more common\ninverse-gamma prior. Inspired by Gelman (2004), we\nbriefly discuss other potential applications of parameter expansion to statistical inference in Section 5.\n2. TWO EM EXAMPLES\n2.1 The Running Example: A Simple\nPoisson\u2013Binomial Mixed-Effects Model\nConsider the complete-data model for the observed\ndata Xobs = X and the missing data Xmis = Z:\nZ|\u03bb \u223c Poisson(\u03bb)\nand\nX|(Z, \u03bb) \u223c Binomial(Z, \u03c0),\nwhere \u03c0 \u2208 (0, 1) is known and \u03bb > 0 is the unknown\nparameter to be estimated.\nThe observed-data model f (X; \u03bb) is obtained from\nthe joint sampling model of (X, Z):\ng(X, Z; \u03bb)\n(2.1)\n=\n\n\u03bbZ \u2212\u03bb\nZ!\ne\n\u03c0 X (1 \u2212 \u03c0)Z\u2212X ,\nZ!\nX!(Z \u2212 X)!\n\nwhere X = 0, 1, . . . , Z, Z = 0, 1, . . . , and \u03bb \u2265 0. That\nis, f (X; \u03bb) is derived from g(X, Z; \u03bb) by integrating\nout the missing data Z as follows:\nf (X; \u03bb)\n\n=\n\n\u221e\nX\nz!\n\u03bbz \u2212\u03bb\ne\n\u03c0 X (1 \u2212 \u03c0)z\u2212X\nz!\nX!(z \u2212 X)!\n\nz=X\n\n\f3\n\nPARAMETER EXPANSION\n\u221e\n\u03bbX \u03c0 X \u2212\u03bb X \u03bbz\u2212X\ne\n(1 \u2212 \u03c0)z\u2212X\nX!\n(z \u2212 X)!\n\n=\n\nz=X\n\nk=z\u2212X\n\n=\n\n\u221e\n\n(\u03bb\u03c0)X \u2212\u03bb X [\u03bb(1 \u2212 \u03c0)]k\ne\nX!\nk!\nk=0\n\n=\n=\n\n(\u03bb\u03c0)X\n\ne\u2212\u03bb e\u03bb(1\u2212\u03c0)\nX!\n(\u03bb\u03c0)X \u2212\u03bb\u03c0\ne\n.\nX!\n\nAlternatively, one can get the result from the wellknown fact related to the infinite divisibility of the\nPoisson distribution; namely, if X1 = X and X2 =\nZ \u2212 X are independent Poisson random variables\nwith rate \u03bb1 = \u03bb\u03c0 and \u03bb2 = \u03bb(1 \u2212 \u03c0), then X1 +\nX2 \u223c Poisson(\u03bb1 + \u03bb2 ) and conditional on X1 + X2 ,\nX1 \u223c Binomial(X1 + X2 , \u03bb1 /(\u03bb1 + \u03bb2 )).\nIt follows that the observed-data model is X|\u03bb \u223c\nPoisson(\u03c0\u03bb). Thus, the ML estimate of \u03bb has a closedform solution, \u03bb\u0302 = X/\u03c0. This artificial example serves\ntwo purposes. First, it is easy to illustrate the general EM derivation. Second, we use this example in\nSection 3.3 to show an extreme case in which PXEM can converge dramatically faster than its parent EM; PX-EM converges in one-step, whereas EM\nconverges painfully slowly.\nThe complete-data likelihood is given by the joint\nsampling model of (X, Z) found in equation (2.1).\nIt follows that the complete-data model belongs to\nthe exponential family with sufficient statistic Z for\n\u03bb. The complete-data ML estimate of \u03bb is given by\n(2.2)\n\n\u03bb\u0302com = Z.\n\nTo derive the E step of EM, the conditional distribution of the missing data Z given both the observed\ndata and the current estimate of the parameter \u03bb is\nused. It is determined as follows:\ng(X, Z; \u03bb)\nh(Z|X, \u03bb) = P\u221e\nz=X g(X, z; \u03bb)\n\n[\u03bb(1 \u2212 \u03c0)]Z\u2212X /(Z \u2212 X)!\n= P\u221e\nz\u2212X /(z \u2212 X)!)\nz=X ([\u03bb(1 \u2212 \u03c0)]\n\n=\n\n[\u03bb(1 \u2212 \u03c0)]Z\u2212X \u03bb(1\u2212\u03c0)\ne\n.\n(Z \u2212 X)!\n\nThus, Z|{X, \u03bb} \u223c X + Poisson(\u03bb(1 \u2212 \u03c0)). This yields\nE(Z|X, \u03bb) = X + \u03bb(1 \u2212 \u03c0).\n\nThus, the EM algorithm follows from the discussion\nof Dempster, Laird and Rubin (1977) on exponential complete-data models. Specifically, given the updated estimate \u03bb(t) at the tth iteration, EM follows\nthese two steps:\nE step. Compute \u1e90 = E(Z|X, \u03bb = \u03bb(t) ) = X + \u03bb(t) \u00d7\n(1 \u2212 \u03c0).\nM step. Replace Z in (2.2) with \u1e90 to obtain \u03bb(t+1) =\n\u1e90.\nIt is clear that the EM sequence {\u03bb(t) : t = 0, 1, . . .}\nis given by\n(2.3)\n\n\u03bb(t+1) = X + \u03bb(t) (1 \u2212 \u03c0) (t = 0, 1, . . .)\n\nconverging to the ML estimate\n\u03bb\u0302 = X/\u03c0.\nRewrite (2.3) as\n\u03bb(t+1) \u2212 \u03bb\u0302 = (1 \u2212 \u03c0)(\u03bb(t) \u2212 \u03bb\u0302)\nto produce a closed-form expression for the convergence rate of EM:\n|\u03bb(t+1) \u2212 \u03bb\u0302|\n|\u03bb(t) \u2212 \u03bb\u0302|\n\n= 1 \u2212 \u03c0.\n\nThis indicates that EM can be very slow when \u03c0 \u2248 0.\n2.2 ML Estimation of Robit Regression via EM\nConsider the observed data consisting of n observations Xobs = {(xi , yi ) : i = 1, . . . , n} with a pdimensional covariate vector xi and binary response\nyi that takes on values of 0 and 1. The binary regression model with Student-t link assumes that, given\nthe covariates, the binary responses yi 's are independent with the marginal probability distributions\nspecified by\n(2.4)\n\nPr(yi = 1|xi , \u03b2) = 1 \u2212 Pr(yi = 0|xi , \u03b2)\n= F\u03bd (x\u2032i \u03b2)\n\n(i = 1, . . . , n),\n\nwhere F\u03bd (*) denotes the c.d.f. of the Student-t distribution with center zero, unit scale and \u03bd degrees\nof freedom. With \u03bd \u2248 7, this model provides a robust approximation to the popular logistic regression model for binary data analysis. Here we consider the case with known \u03bd.\nThe observed-data likelihood\nf (Xobs ; \u03b2)\nn\nY\n= [F\u03bd (x\u2032i \u03b2)]yi [1 \u2212 F\u03bd (x\u2032i \u03b2)]1\u2212yi\ni=1\n\n(\u03b2 \u2208 Rp )\n\n\f4\n\nA. LEWANDOWSKI, C. LIU AND S. VANDER WIEL\n\ninvolves the product of the c.d.f. of the Student-t\ndistribution F\u03bd (*) evaluated at x\u2032i \u03b2 for i = 1, . . . , n.\nThe MLE of \u03b2 does not appear to have a closedform solution. Here we consider the EM algorithm\nfor finding the MLE of \u03b2.\nA complete-data model for implementing EM to\nfind the ML estimate of \u03b2 is specified by introducing\nthe missing data consisting of independent latent\nvariables (\u03c4i , zi ) for each i = 1, . . . , n with\n\nfor i = 1, . . . , n.\nHowever, the EM algorithm can also converge slowly in this example. This is discussed in Section 3.2,\nwhere it is shown that PX-EM can greatly improve\nthe convergence rate.\n\n(2.5)\n\nSuppose that the EM complete-data model can\nbe embedded in a larger model g\u2217 (Xobs , Xmis ; \u03b8\u2217 , \u03b1)\nwith the expanded parameter (\u03b8\u2217 , \u03b1) \u2208 \u0398 \u00d7 A. Assume that the observed-data model is preserved in\nthe sense that, for every (\u03b8\u2217 , \u03b1) \u2208 \u0398 \u00d7 A,\n\n\u03c4i |\u03b2 \u223c Gamma(\u03bd/2, \u03bd/2)\n\nand\nzi |(\u03c4i , \u03b2) \u223c N(x\u2032i \u03b2, 1/\u03c4i ).\n\n(2.6)\nLet\n(2.7)\n\nyi =\n\n\u001a\n\n1, if zi > 0,\n0, if zi \u2264 0\n\n(i = 1, . . . , n).\n\n3. THE PX-EM ALGORITHM\n3.1 The Algorithm\n\n(3.1)\n\nf (Xobs ; \u03b8) = f\u2217 (Xobs ; \u03b8\u2217 , \u03b1)\n\nRholds for some \u03b8 \u2208 \u0398, where f\u2217 (Xobs ; \u03b8\u2217 , \u03b1) =\nXmis g\u2217 (Xobs , Xmis ; \u03b8\u2217 , \u03b1) dXmis . The condition (3.1)\ndefines a mapping \u03b8 = R(\u03b8\u2217 , \u03b1), called the reduction function, from the expanded parameter space\n\u0398 \u00d7 A to the original parameter space \u0398. For conn\nn\nX\nX\nvenience, assume that the expanded parameters are\n\u03c4i xi zi\u2032 .\n(2.8) S\u03c4 xx\u2032 =\n\u03c4i xi x\u2032i and S\u03c4 xz =\nrepresented in such a way that the original completei=1\ni=1\ndata and observed-data models are recovered by fixThe complete-data ML estimate of \u03b2 is given by\ning \u03b1 at \u03b10 . Formally, assume that there exists a null\nvalue of \u03b1, denoted by \u03b10 , such that \u03b8 = R(\u03b8, \u03b10 )\n(2.9)\n\u03b2\u0302com = S\u03c4\u22121\nS\n,\n\u2032\n\u03c4\nxz\nxx\nfor every \u03b8 \u2208 \u0398. When applied to the parameterleading to the following EM algorithm.\nexpanded\ncomplete-data model g\u2217 (Xobs , Xmis ; \u03b8\u2217 , \u03b1),\nStarting with \u03b2 (0) , say, \u03b2 (0) = (0, . . . , 0), EM itercalled the PX-EM algorithm, creates for t = 0, 1, . . . with iteration t + 1 consisting of the EM algorithm, (t)\n(t)\nates\na\nsequence\n{(\u03b8\n\u2217 , \u03b1 )} in \u0398 \u00d7 A. In the original\nthe following E and M steps:\nparameter space \u0398, PX-EM generates a sequence\nE step. Compute \u015c\u03c4 xx\u2032 = E(S\u03c4 xx\u2032 |\u03b2 = \u03b2 (t) , Xobs ) and\n(t)\n{\u03b8 (t) = R(\u03b8\u2217 , \u03b1(t) )} and converges no slower than\n\u015c\u03c4 xz = E(S\u03c4 xz |\u03b2 = \u03b2 (t) , Xobs ).\nthe corresponding EM based on g(Xobs , Xmis ; \u03b8); see\nM step. Update the estimate of \u03b2 to obtain \u03b2 (t+1) =\nLiu,\nRubin and Wu (1998).\n\u015c\u03c4\u22121\nxx\u2032 \u015c\u03c4 xz .\nFor simplicity and stability, Liu, Rubin and Wu\nLet f\u03bd (*) denote the p.d.f. of F\u03bd (*). The E step can (1998) use (\u03b8 (t) , \u03b10 ) instead of (\u03b8\u2217(t) , \u03b1(t) ) for the E step.\nbe coded by using the following results derived in As a result, PX-EM shares with EM its E step and\n(t+1)\nLiu (2004):\n, \u03b1(t+1) ) to the\nmodifies its M step by mapping (\u03b8\u2217\n(t+1)\n, \u03b1(t+1) ). More preoriginal space \u03b8 (t+1) = R(\u03b8\u2217\n\u03c4\u0302i = E(\u03c4i |\u03b2 = \u03b2 (t) , Xobs )\n(2.10)\ncisely, the PX-EM algorithm is defined by replacing\nyi \u2212 (2yi \u2212 1)F\u03bd+2 (\u2212(1 + 2/\u03bd)1/2 x\u2032i \u03b2 (t) )\nthe E and M steps of EM with the following:\n,\n=\n\u2032\n(t)\nyi \u2212 (2yi \u2212 1)F\u03bd (\u2212xi \u03b2 )\nPX-E step. Compute\n(2.11) \u03c4i\u02c6zi = E(\u03c4i zi |\u03b2 = \u03b2 (t) , Xobs ) = \u03c4\u0302i z\u02c6i ,\nQ(\u03b8\u2217 , \u03b1|\u03b8 (t) , \u03b10 )\nwhere\n= E(ln g\u2217 (Xobs , Xmis ; \u03b8\u2217 , \u03b1)|Xobs , \u03b8\u2217 = \u03b8 (t) ,\n\u1e91i \u2261 x\u2032i \u03b2 (t)\n\u03b1 = \u03b10 )\n(2yi \u2212 1)f\u03bd (x\u2032i \u03b2 (t) )\n+\nas a function of (\u03b8\u2217 , \u03b1) \u2208 \u0398 \u00d7 A.\nyi \u2212 (2yi \u2212 1)F\u03bd+2 (\u2212(1 + 2/\u03bd)1/2 x\u2032i \u03b2 (t) )\n\nThen the marginal distribution of yi is preserved and\nis given by (2.4). The complete-data model belongs\nto the exponential family and has the following sufficient statistics for \u03b2:\n\n\f5\n\nPARAMETER EXPANSION\n\nPX-M step. Find\n(t+1)\n\n(\u03b8\u2217\n\n, \u03b1(t+1) ) = arg max Q(\u03b8\u2217 , \u03b1|\u03b8 (t) , \u03b10 )\n\u03b8\u2217 ,\u03b1\n\nand update\n(t+1)\n\n\u03b8 (t+1) = R(\u03b8\u2217\n\n, \u03b1(t+1) ).\n\nSince it is the ordinary EM applied to the parameter expanded complete-data model, PX-EM shares\nwith EM its simplicity and stability. Liu, Rubin and\nWu (1998) established theoretical results to show\nthat PX-EM can converge no slower than EM. Section 3.2 uses the robit regression example to give\nthe statistical interpretation of Liu, Rubin and Wu\n(1998) in terms of covariance adjustment. With the\ntoy example, Section 3.3 demonstrates that PX-EM\ncan be dramatically faster than its parent EM. A\ndiscussion of why PX-EM can perform better than\nEM is given in Section 4.\n3.2 Efficient Analysis of Imputed Missing Data:\nRobit Regression\nThe E step of EM imputes the sufficient statistics\nS\u03c4 xx\u2032 and S\u03c4 xz with their expectations based on the\npredictive distribution of the missing (\u03c4i , zi ) data\nconditioned on the observed data Xobs and \u03b2 (t) , the\ncurrent estimate of \u03b2 at the tth iteration. Had the\nML estimate of \u03b2, \u03b2\u0302, been used to specify the predictive distribution, EM would have converged on\nthe following M step, which in this case performs\ncorrect ML inference. We call the predictive distribution using \u03b2\u0302 the correct imputation model. Before\nconvergence, that is, \u03b2 (t) 6= \u03b2\u0302, the E step imputes the\nsufficient statistics S\u03c4 xx\u2032 and S\u03c4 xz using an incorrect\nimputation model. The M step also uses a wrong\nmodel since it does not take into account that the\ndata were incorrectly imputed based on an assumed\nparameter value \u03b2 (t) 6= \u03b2\u0302. The M step moves the estimate \u03b2 (t+1) toward \u03b2\u0302, but the difference between\n\u03b2 (t+1) and \u03b2\u0302 can be regarded as bias due to the use\nof the \u03b2 (t) .\nThe bias induced by the E step can be reduced\nby making use of recognizable discrepancies between\nimputed statistics and their values under the correct\nimputation model. To capture such discrepancies,\nLiu, Rubin and Wu (1998) considered parameters\nthat are statistically identified in the complete-data\nmodel but not in the observed-data model. These\nparameters are fixed at their default values to render\nthe observed-data model identifiable. In the context\nof EM for robit regression, these parameters are the\n\nscale parameters of \u03c4i and zi , denoted by \u03b1 and \u03c3.\nIn the observed-data model, they take the default\nvalues \u03b10 = 1 and \u03c30 = 1.\nWhen activated, the extra parameters are estimated by the M step and these estimates converge\nto the default values to produce ML parameter estimates for the observed data model. Thus, in the\nrobit regression model, we identify the default values of the extra parameters as MLEs: \u03b10 = \u03b1\u0302 = 1\nand \u03c30 = \u03c3\u0302 = 1. Denote the corresponding EM estimates by \u03b1(t+1) and \u03c3 (t+1) . The discrepancies between (\u03b1(t+1) , \u03c3 (t+1) ) and (\u03b1\u0302, \u03c3\u0302) reveal the existence\nof bias induced by the wrong imputation model.\nThese discrepancies can be used to adjust the estimate of the parameter of interest, \u03b2, at each iteration. This is exactly what PX-EM is formulated\nto do, and the resulting algorithm converges faster\nthan the original EM.\nFormally, the extra parameter (\u03b1, \u03c3) introduced\nto capture the bias in the imputed values of \u03c4i and\nzi is called the expansion parameter. The completedata model is thus both data-augmented as well as\nparameter-augmented. For correct inference at convergence, data augmentation is required to preserve\nthe observed-data model after integrating out missing data. Likewise, parameter expansion needs to\nsatisfy the observed-data model preservation condition (3.1). In the robit regression model, let (\u03b2\u2217 , \u03b1, \u03c3)\nbe the expanded parameter with \u03b2\u2217 playing the same\nrole as \u03b2 in the original model. The preservation\ncondition states that for every expanded parameter\nvalue (\u03b2\u2217 , \u03b1, \u03c3), there exists a value of \u03b2 such that\nthe sampling model of the yi 's obtained from the\nparameter expanded model is the same as the original sampling model given \u03b2. This condition defines\na mapping \u03b2 = R(\u03b2\u2217 , \u03b1, \u03c3), the reduction function.\nThis reduction function is used in PX-EM to adjust\nthe value of \u03b2 (t+1) produced by the M step.\nThe detailed implementation of PX-EM for robit regression is as follows. The parameter-expanded\ncomplete-data model is obtained by replacing (2.5)\nand (2.6) with\n(3.2)\n\n(\u03c4i /\u03b1)|(\u03b2\u2217 , \u03b1, \u03c3) \u223c Gamma(\u03bd/2, \u03bd/2)\n\nand\n(3.3)\n\nzi |(\u03c4i , \u03b2\u2217 , \u03b1, \u03c3) \u223c N(x\u2032i \u03b2\u2217 , \u03c3 2 /\u03c4i )\n\nfor i = 1, . . . , n. Routine algebraic operation yields\nthe reduction function\n(3.4)\n\n\u03b2 = R(\u03b2\u2217 , \u03b1, \u03c3)\n= (\u03b11/2 /\u03c3)\u03b2\u2217\n\n(\u03b2\u2217 \u2208 Rp ; \u03b1 > 0; \u03c3 > 0).\n\n\f6\n\nA. LEWANDOWSKI, C. LIU AND S. VANDER WIEL\n\nThe expanded parameterization in (3.2) and (3.3)\nis a natural choice if the missing data are viewed as\nreal and a parameterization is sought that provides a\nmodel that is flexible while preserving the observed\ndata model and allowing the original parameterization to be recovered through the reduction function.\nFor example, if \u03c4i is treated as fixed, the model for\nzi is a regression model with fixed variance. Adding\n\u03c3 2 and \u03b1 allows the variance of zi and the scale of\n\u03c4i to be estimated freely in the expanded model.\nThe sufficient statistics for the expanded parameter (\u03b2\u2217 , \u03b1, \u03c3) now become\nS\u03c4 =\n\nn\nX\n\n\u03c4i ,\n\nn\nX\n\n\u03c4i zi2 ,\n\nS\u03c4 xx\u2032 =\n\nS\u03c4 z 2 =\n\n\u03c4i xi x\u2032i ,\n\ni=1\n\ni=1\n\n(3.5)\n\nn\nX\n\nS\u03c4 xz =\n\nn\nX\n\n\u03c4i xi zi\u2032 .\n\ni=1\n\ni=1\n\nThe complete-data ML estimate of \u03b2\u2217 is the same as\nthat of \u03b2 in the original complete-data model. The\ncomplete-data ML estimates of \u03b1 and \u03c3 are given\nby\n1\n\u03b1\u0302com = S\u03c4 and\nn\n(3.6)\n1\n2\n\u03c3\u0302com\n= (S\u03c4 z 2 \u2212 S\u03c4 xz S\u03c4\u22121\nxx\u2032 S\u03c4 xz ).\nn\nThe PX-EM algorithm is simply an EM applied to\nthe parameter expanded complete-data model with\nan M step followed by (or modified to contain) a reduction step. The reduction step uses the reduction\nfunction to map the estimate in the expanded parameter space to the original parameter space. For\nthe robit example, PX-EM is obtained by modifying\nthe E and M steps as follows.\nPX-E step. This is the same as the E step of EM except for the evaluation of two additional expected\nsufficient statistics:\nn\nX\n\u03c4\u0302i\n\u015c\u03c4 = E(S\u03c4 |\u03b2 = \u03b2 (t) , Xobs ) =\ni=1\n\nand\n\n2\n\u22121 \u00d7\nPX-M step. Compute \u03b2\u0302\u2217 = \u015c\u03c4\u22121\nxx\u2032 \u015c\u03c4 xz , \u03c3\u0302\u2217 = n\n\u22121\n(\u015c\u03c4 z 2 \u2212 \u015c\u03c4 xz \u015c\u03c4 xx\u2032 \u015c\u03c4 xz ), and \u03b1\u0302\u2217 = n\u22121 \u015c\u03c4 and then\n1/2\nuse the reduction to obtain \u03b2\u0302 (t+1) = (\u03b1\u0302\u2217 /\u03c3\u0302\u2217 )\u03b2\u0302\u2217 .\n\nFor a numerical example, consider the data of\nFinney (1947), which consist of 39 binary responses\ndenoting the presence (y = 1) or absence (y = 0) of\nvaso-constriction of the skin of the subjects after inspiration of a volume V of air at inspiration rate\nR. The data were obtained from repeated measurements on three individual subjects, the numbers of\nobservations per subject being 9, 8 and 22. Finney\n(1947) found no evidence of inter-subject variability, treated the data as 39 independent observations,\nand analyzed the data using the probit regression\nmodel with V and R in the logarithm scale as covariates. This data set was also analyzed by Liu (2004)\nto illustrate robit regression. Due to three outlying\nobservations, the MLE of the degrees of freedom \u03bd\nis very small, \u03bd\u0302 = 0.11.\nHere we use this data set with ln(V ) and ln(R) as\nthe covariates and take the fixed \u03bd = 2 as a numerical example to compare EM and PX-EM. Numerical results comparing the rates of convergence of\nEM and PX-EM are displayed in Figure 1. PX-EM\nshows a clear and dramatic convergence gain over\nEM. For convenience we choose to report the detailed results over iterations. The algorithms were\ncoded in R, which makes CPU comparison unreliable. Since extra computation for the PX-EM implementation is minor, we believe the same conclusion\nholds in terms of CPU times.\n3.3 PX-EM with Fast Convergence:\nThe Toy Example\nThe model X|(Z, \u03bb) \u223c Binomial(Z, \u03c0) may not fit\nthe imputed value of missing data Z very well in the\nsense that X/\u1e90 is quite different from \u03c0. This mismatch can be used to adjust \u03bb(t+1) . To adjust \u03bb(t+1) ,\nwe activate \u03c0 and let \u03b1 denote the activated parameter with \u03b10 = \u03c0. Now the parameter-expanded\ncomplete-data model becomes\n\n\u015c\u03c4 z 2 = E(S\u03c4 z 2 |\u03b2 = \u03b2 (t) , Xobs )\n\nZ|(\u03bb\u2217 , \u03b1) \u223c Poisson(\u03bb\u2217 )\n\n= n(\u03bd + 1)\n\u2212\u03bd\n\nn\nX\ni=1\n\n\u03c4\u0302i +\n\nn\nX\n\nand\n\u03c4\u0302i x\u2032i \u03b2 (t) (2\u1e91i\n\n\u2212 x\u2032i \u03b2 (t) ),\n\ni=1\n\nwhere \u03c4\u0302i 's and \u1e91i 's are available from the E step\nof EM.\n\nX|(Z, \u03bb\u2217 , \u03b1) \u223c Binomial(Z, \u03b1),\nwhere \u03bb\u2217 > 0 and \u03b1 \u2208 (0, 1). If the missing data\nwere treated as being observed, this model allows\n\n\f7\n\nPARAMETER EXPANSION\n\nFig. 1. EM (solid) and PX-EM (dashed) sequences of the regression coefficients \u03b20 (a), \u03b21 (b), \u03b22 (c), and log-likelihood in the robit regression with x = (1, ln(V ), ln(R)). The rates of convergence of EM (solid) and PX-EM (dashed) are\nshown in (e) by |l(t+1) \u2212 l(\u221e) |/|l(t) \u2212 l(\u221e) |, where l(t) denotes the log-likelihood value at the tth iteration, and in (f) by\n(t+1)\n(\u221e)\n(t)\n(\u221e)\n|\u03b2j\n\u2212 \u03b2j |/|\u03b2j \u2212 \u03b2j | for j = 0, 1 and 2.\n\nthe mean parameters for both X and Z to be estimated. The two corresponding observed-data models are Poisson(\u03bb\u03c0) and Poisson(\u03bb\u2217 \u03b1), giving the reduction function\n\u03b1\n(3.7)\n\u03bb = R(\u03bb\u2217 , \u03b1) = \u03bb\u2217 .\n\u03c0\nThe complete-data sufficient statistics are Z and\nX. The complete-data ML estimates of \u03bb\u2217 and \u03b1 are\n\ngiven by\n(3.8)\n\n\u03bb\u0302\u2217,com = Z\n\nand \u03b1\u0302com =\n\nX\n.\nZ\n\nThe resulting PX-EM has the following E and M\nsteps:\nPX-E step. This is the same as the E step of EM.\n\n\f8\n\nA. LEWANDOWSKI, C. LIU AND S. VANDER WIEL\n\nPX-M step. Replace Z in (3.8) with \u1e90 to obtain\n(t+1)\n= \u1e90 and \u03b1(t+1) = X/\u1e90. Update \u03bb using the\n\u03bb\u2217\nreduction function and obtain\nX\nX\n\u1e90 = .\n\u03bb(t+1) =\n\u03c0\n\u03c0 \u1e90\nThe PX-EM algorithm in this case converges in one\nstep. Although artificial, this toy example shows again\nthat PX-EM can converge dramatically faster than\nits parent EM.\n4. UNFOLDING THE MYSTERY OF PX-EM\nThe statistical interpretation in terms of covariance adjustment, explained by the robit example\nabove and the Student-t example in Liu, Rubin and\nWu (1998), and the theoretical results of Liu, Rubin and Wu (1998) help reveal the PX-EM magic.\nTo further unfold the mystery of PX-EM, we discuss\nthe nonidentifiability of expanded parameters in the\nobserved-data model in Section 4.1 and take a look\nat PX-EM from the point of view of efficient data\naugmentation in Section 4.2.\n4.1 Nonidentifiability of Expanded Parameters\nand Applicability of PX-EM\nIt is often the case in PX-EM that, even though\nthe expanded parameter (\u03b8\u2217 , \u03b1) is identifiable from\nQ(\u03b8\u2217 , \u03b1|\u03b8 (t) , \u03b10 ) (the expected parameter-expanded\ncomplete-data log-likelihood), it is not identifiable\nfrom the corresponding observed-data loglikelihood\nL\u2217 (\u03b8\u2217 , \u03b1) = ln f\u2217 (Xobs ; \u03b8\u2217 , \u03b1).\nIt is helpful to consider L\u2217 (\u03b8\u2217 , \u03b1) for understanding PX-EM, as the PX-M step directly increases\nL\u2217 (\u03b8\u2217 , \u03b1) through maximizing Q(\u03b8\u2217 , \u03b1|\u03b8 (t) , \u03b10 ). Naturally, from a mathematical point of view, unfolding\nthe actual likelihood in the larger or expanded parameter space \u0398 \u00d7 A shows how PX-EM steps can\nlead to increases in the likelihood function faster\nthan can moves in the original space \u0398.\n4.1.1 The observed-data log-likelihood surface over\n\u0398 \u00d7 A The observed-data log-likelihood, as a function of (\u03b8\u2217 , \u03b1), is determined by the actual log-likelihood L(\u03b8) = ln f (Xobs ; \u03b8) with \u03b8 replaced by \u03b8 =\nR(\u03b8\u2217 , \u03b1) so that\n(4.1) L\u2217 (\u03b8\u2217 , \u03b1) = L(R(\u03b8\u2217 , \u03b1))\n\n((\u03b8\u2217 , \u03b1) \u2208 \u0398 \u00d7 A).\n\nThus, each point \u03b8 \u2208 \u0398 corresponds to a subspace\n{(\u03b8\u2217 , \u03b1) \u2208 \u0398 \u00d7 A, R(\u03b8\u2217 , \u03b1) = \u03b8}, over which L\u2217 (\u03b8\u2217 , \u03b1)\nis constant.\n\nFig. 2. Perspective plots of the parameter-expanded observed-data log-likelihood L(\u03bb\u2217 , \u03b1) (top) and the parameter\u2013\nexpanded complete-data log-likelihood Q(\u03bb\u2217 , \u03b1|\u03bb(t) ) (bottom)\nin the toy example with X = 8, \u03c0 = 0.25, and \u03bb(t) = 8.\n\nFor example, when \u03b8 and \u03b1 are one-dimensional\nparameters, L(\u03b8) can be represented by a curve in\nthe two-dimensional space \u0398 \u00d7 L(\u0398), whereas L\u2217 (\u03b8\u2217 ,\n\u03b1) is a family of curves indexed by \u03b1. The family\nof curves L\u2217 (\u03b8\u2217 , \u03b1) form a surface in the style of a\nmountain range in the three-dimensional space \u0398 \u00d7\nA \u00d7 L(\u0398). For the toy example, this is depicted in\nFigure 2 by the top panel 3-D perspective plot and\nin Figure 3 by the image with dashed contours or\n\"elevation\" lines. The mode of L(\u03b8) now becomes\na set of modes of the same \"altitude,\" one for each\nfixed \u03b1. That is, the mode of L(\u03b8) is expanded into\nthe \"ridge\" shown, for example, by the thick line in\nFigure 3.\n4.1.2 Likelihood maximization in PX-EM The E\nstep in PX-EM implicitly computes a family of expected complete-data log-likelihood functions, which\nare the Q-functions used in (1.1), over the original parameter space indexed by the expansion parameter \u03b1. This is because PX-EM introduces no\nadditional or different missing data in the larger\n\n\f9\n\nPARAMETER EXPANSION\n\nand (ii) an M step with any fixed \u03b1, which finds\n(t+1)\n\n\u03b8\u2217\n\n= arg max Q(\u03b8\u2217 , \u03b1|\u03b8 (t) , \u03b10 ),\n\u03b8\u2217\n\n(t+1)\n\n, \u03b1) is simfollowed by the reduction \u03b8 (t+1) = R(\u03b8\u2217\nply a conditional maximization step. Additionally,\nin the context of the ECM algorithm of Meng and\nRubin (1993), the parent EM is an incomplete ECM\nwith only one single CM step over \u0398 \u00d7 A. This relationship is explored in greater detail in the next\nsection.\n4.1.3 PX-EM vs. (efficient) ECM over \u0398 \u00d7 A In\ntheory, PX-EM has a single M step over the entire\nspace \u0398 \u00d7 A. Note that\nmax Q(\u03b8\u2217 , \u03b1|\u03b8 (t) , \u03b10 ) = max max Q(\u03b8\u2217 , \u03b1|\u03b8 (t) , \u03b10 ).\n\u03b1\n\n(\u03b8\u2217 ,\u03b1)\n\n\u03b8\u2217\n\nWhen\nFig. 3. PX-EM for the toy example with X = 8, \u03c0 = 0.25,\nand \u03bb(t) = 8. The parameter-expanded observed-data log-likelihood function L(\u03bb\u2217 , \u03b1) is shown by shading and dashed contours with a maximum along the ridge indicated by a solid\nthick line. The expected parameter-expanded complete-data\nlog-likelihood Q(\u03bb\u2217 , \u03b1|\u03bb(t) ) is shown by the ellipse-like solid\ncontours. In this example, maximization of Q(\u03bb\u2217 , \u03b1|\u03bb(t) ) over\n(\u03bb\u2217 , \u03b1) can be obtained in two conditional maximization steps,\nlabeled as the two ECM updates. The PX-M step moves to\na point on the ridge of L(\u03bb\u2217 , \u03b1), and the subsequent reduction-step moves this point along the the ridge of L(\u03bb\u2217 , \u03b1) to\nthe point with \u03b1 = \u03c0.\n\ncomplete-data model. In other words, the parent E\nstep effectively computes a surface over \u0398 \u00d7 A that\ncan be used as a Q-function to approximate the expanded loglikelihood L\u2217 (\u03b8\u2217 , \u03b1) defined in (4.1). This\nQ-function for the toy example is shown in Figure\n2 by the bottom panel 3-D perspective plot and in\nFigure 3 by the nearly-elliptical contours. For this\none-step convergence PX-EM example, the mode of\nthis Q-function is on the ridge of the expanded loglikelihood L\u2217 (\u03b8\u2217 , \u03b1). We note that this is typically\nnot the case in more realistic examples. In the general case, the mode of the Q-function would typically\nbe located on one elevation line that is higher than\nthe elevation line where the update (\u03b8 (t) , \u03b10 ) found\nby EM is located.\nSomewhat surprisingly, any such Q-function for\neach fixed \u03b1 is EM-valid. By EM-valid, we mean\nthat increasing the Q-function results in an increase\nof the actual likelihood in the expanded space and\nthereby in the original space after the reduction step.\nThis is due to two facts: (i) the joint Q-function is\nEM-valid for L\u2217 (\u03b8\u2217 , \u03b1) and, thus, for L(\u03b8) as well,\n\n(t+1)\n\n\u03b8\u0302\u2217\n\n= arg max Q(\u03b8\u2217 , \u03b1|\u03b8 (t) , \u03b10 )\n\u03b8\u2217\n\ndoes not depends on \u03b1, as is often the case in many\nPX-EM examples, the PX-M step is equivalent to\na cycle of two CM steps: one is the M step of EM,\n(t+1)\n. This\nand the other updates \u03b1 with \u03b8\u2217 fixed at \u03b8\u2217\nversion of ECM for the toy example is illustrated in\nFigure 3. In this case, ECM is efficient for it generates the PX-EM update.\nTo summarize, denote by ECM{\u03b1,\u03b8\u2217 } the above version of ECM over \u0398 \u00d7 A. Typically, the algorithms\ncan then be ordered in terms of performance as\n(4.2)\n\nEM \u0016 ECM{\u03b1,\u03b8\u2217 } \u0016 PX \u2212 EM\n\nover \u0398 \u00d7 A. It should be noted that by typically, we\nmean the conclusion is reached in an analogy with\ncomparing the EM algorithm and the Generalized\nEM algorithm (GEM) (Dempster, Laird and Rubin,\n1977), that is, EM typically converges faster than\nGEM, but counter examples exist; see, for example, Section 5.4 of van Dyk and Meng (2010) and\nthe alternative explanation from an ECME point\nof view in Section 4.3 of Liu and Rubin (1998) on\nwhy ECM can be faster than EM. To elaborate it\nfurther with our robit example, it may be also interesting to note that when the reduction function\n(3.4) is modified by replacing the adjustment factor (\u03b11/2 /\u03c3) with (\u03b1/\u03c3), a typo made in the earlier versions of the PX-EM for the robit regression\nmodel, the resulting (wrong) PX-EM converges actually faster than the (correct) PX-EM for the numerical example in Section 3.2. In general, more efficiency can be gained by replacing the CM step of\n\n\f10\n\nA. LEWANDOWSKI, C. LIU AND S. VANDER WIEL\n\nECM over \u03b1 with a CM step maximizing the corresponding actual constrained likelihood in the parameter expanded space. This is effectively a parameterexpanded ECME algorithm; see such an example\nfor the Student-t distribution given in Liu (1997).\nMore discussion on ECME and other state-of-theart methods for accelerating the EM algorithm can\nbe found in He and Liu (2009). Their discussion\non the method termed SOR provides a relevant explanation why the above wrong PX-EM and other\nwrong PX-EM versions, such as the one using the\nwrong reduction function \u03b2 = (\u03b1/\u03c3 2 )\u03b2\u2217 in the numerical robit example, can converge faster than the\ncorrect PX-EM.\nPerhaps most importantly, the above discussion\nfurther explains why PX-EM can perform better\nthan EM can, and unfolds the mystery of PX-EM,\nin addition to the covariance adjustment interpretation.\n4.2 Efficient Data Augmentation via Parameter\nExpansion\nMeng and van Dyk (1997) consider efficient data\naugmentation for creating fast converging algorithms.\nThey search for efficient augmenting schemes by working with the fraction of missing-data information.\nHere we show that PX-EM can also be viewed as\nan alternative way of doing efficient data augmentation. Unlike Meng and van Dyk (1997), who find a\nfixed augmenting scheme that works for all EM iterations, the following procedure is a way to choose an\nadaptive augmenting scheme for each EM iteration.\nRather than control the fraction of missing-data information, this procedure reduces bias through the\nexpansion parameter. For the sake of clarity, we use\nthe artificial example of Section 2.1 to make our argument.\nConsider the parameter-expanded complete-data\nlikelihood obtained from (2.1) by activating \u03b10 = \u03c0,\nthat is,\nZ!\n\u03bbZ\n\u2217 \u2212\u03bb\u2217\ne\n\u03b1X (1 \u2212 \u03b1)Z\u2212X\nZ!\nX!(Z \u2212 X)!\n(\u03bb\u2217 > 0; 0 < \u03b1 < 1),\nwhich has the canonical representation\nh(X, Z)c(\u03bb\u2217 , \u03b1)eZ ln[\u03bb\u2217 (1\u2212\u03b1)]+X ln \u03b1/(1\u2212\u03b1)\n(\u03bb\u2217 > 0; 0 < \u03b1 < 1).\nThus, when fixed at the given value, \u03c0, for identifiability, the complete-data ML estimate \u03b1\u0302 = X/Z\n\nplays the role of an ancillary statistic; see Ghosh,\nReid and Fraser (2010) for an introduction to ancillary statistics. With the correct imputation model,\nor at convergence, the imputed value \u1e90 satisfies\n(4.3)\n\n\u03c0=\n\nX\n\nor\n\n\u1e90 =\n\nX\n.\n\u03c0\n\n\u1e90\nThus, we can consider modifying the E step of EM to\nproduce an imputed statistic \u1e90 that satisfies (4.3).\nIn the context of PX-EM, the current estimate \u03bb(t)\ncorresponds to the following subset of the expanded\nparameter space:\n(t)\n\n(4.4)\n\n\u03a9\u2217 \u2261 {(\u03bb\u2217 , \u03b1) : R(\u03bb\u2217 , \u03b1) = R(\u03bb(t) , \u03b10 )}\n= {(\u03bb\u2217 , \u03b1) : \u03bb(t) \u03c0 = \u03b1\u03bb\u2217 }.\n\nThus, we can use the imputation model defined by\nthe parameter-expanded complete-data model con(t)\nditioned on an arbitrary point (\u03bb\u0303\u2217 , \u03b1\u0303) \u2208 \u03a9\u2217 . For efficient data augmentation, we choose a particular\n(t)\npoint (\u03bb\u0303\u2217 , \u03b1\u0303) \u2208 \u03a9\u2217 , if it exists, so that (4.3) holds.\nSince\n\u1e90 = E(Z|X, \u03bb\u2217 , \u03b1) = X + \u03bb\u2217 (1 \u2212 \u03b1),\nto obtain the desired imputation model, we solve\nX + \u03bb\u0303\u2217 (1 \u2212 \u03b1\u0303) =\n\nX\n,\n\u03c0\n\n\u03bb(t) \u03c0 = \u03b1\u0303\u03bb\u0303\u2217\nfor (\u03bb\u0303\u2217 , \u03b1\u0303). This system of equations has the solution\n\u03bb\u0303\u2217 = X\n\n1\u2212\u03c0\n+ \u03bb(t) \u03c0\n\u03c0\n\nand\n\u03b1\u0303 =\n\n\u03bb(t) \u03c0\n.\nX((1 \u2212 \u03c0)/\u03c0) + \u03bb(t) \u03c0\n\nThe E step of the EM algorithm based on the corresponding imputation model produces \u1e90 = X/\u03c0. The\nfollowing M step of EM gives \u03bb(t+1) = \u1e90 = X/\u03c0.\nThe resulting EM algorithm is effectively the PXEM algorithm. This implies that PX-EM can be\nunderstood from the perspective of efficient data\naugmentation via parameter expansion. Similar arguments can be made for other PX-EM examples\nhaving imputed ancillary statistics. In the general\ncase, such an efficient data augmentation amounts to\nmodifying imputed complete-data sufficient statistics and can be viewed as re-imputation of missing\nsufficient statistics.\n\n\f11\n\nPARAMETER EXPANSION\n\n5. DISCUSSION\nGelman (2004) notes that \"progress in statistical computation often leads to advances in statistical modeling,\" which opens our eyes to the broader\npicture. Statistical interpretations of EM and PXEM reveal that statistical thinking can aid in understanding and developing iterative algorithms. It\nseems natural to apply fundamental concepts from\nstatistical inference to address statistical problems\nsuch as ML estimation and Bayesian estimation (see,\ne.g., Liu and Wu, 1999; van Dyk and Meng, 2001; Qi\nand Jaakkola, 2007; Hobert and Marchev, 2008). A\nrecent example is the work of Yu and Meng (2008,\n2010), which uses relationships motivated by the\nconcepts of ancillarity and sufficiency in order to\nfind optimal parameterizations for data augmentation algorithms used in Bayesian inference. However,\nstatistical thinking can also be helpful for generalpurpose optimization algorithms such as in the improvements to the quasi-Newton algorithm developed by Liu and Vander Wiel (2007).\nThinking outside the box, here we briefly discuss\nother potential applications of parameter expansion\nto statistical inference. The fundamental idea of PXEM-the use of expanded parameters to capture information in data-leads immediately to a possible\napplication of parameter expansion for \"dimensionmatching\" in Fisher's conditional inference and fiducial inference (see, e.g., Fisher, 1973), where difficulties arise when the dimensionality of the minimal\nsufficient statistics is larger than the number of free\nparameters to be inferred. It is well known that,\nwhile attempting to build a solid foundation for statistical inference, the ideas behind Fisher's fiducial\ninference have not been well developed. Nevertheless, it is expected that parameter expansion can\nbe useful in developing new ideas for statistical inference. For example, a Dempster\u2013Shafer or fiduciallike method, called the inferential model (IM) framework, has been proposed by Zhang and Liu (2011)\nand Martin, Zhang and Liu (2010). Of particular\ninterest is the parameter expansion technique proposed by Martin, Hwang and Liu (2010) for what\nthey call weak marginal inference using IMs. Using this parameter expansion technique, they provide satisfactory resolutions to the famous Stein's\nparadox and the Behrens\u2013Fisher problem.\nAlthough brief, the above discussion shows that\nparameter expansion has the potential to contribute\nto a variety of applications in computation and statistical inference. To conclude this review article, we\n\nspeculate on one possible application of parameter\nexpansion to the method of maximum likelihood for\nwhich the EM algorithm has proven to be a useful computational tool. The prospect of applying\ngeneral statistical ideas to computational problems\nhas also led us to thinking about model checking\nor goodness of fit to solve the unbounded likelihood\nproblem in fitting Student-t and mixture models,\nfor which EM is often the first choice. In the case\nwith unbounded likelihood functions, for example, a\nhigh-likelihood model may not fit the observed data\nwell and then inferential results can be nonsensical.\nIt would be interesting to see if the general idea of\nparameter expansion for efficient inference can be\nextended for \"valid inference\" as well. However, it\nis not our intention here to discuss these open problems in depth. Based on the past success in this area,\nit can be expected that parameter expansion methods will continue to aid computation and inference.\nACKNOWLEDGMENTS\nThe authors thank the editors and their reviewers\nfor their helpful comments and suggestions on earlier\nversions of this article. Chuanhai Liu was partially\nsupported by NSF Grant DMS-10-07678.\nREFERENCES\nAlbert, J. H. and Chib. S. (1993). Bayesian analysis of binary and polychotomous response data. J. Amer. Statist.\nAssoc. 88 669\u2013679. MR1224394\nDempster, A. P., Laird, N. M. and Rubin, D. B. (1977).\nMaximum likelihood estimation from incomplete data via\nthe EM algorithm (with discussion). J. Roy. Statist. Soc.\nSer. B 39 1\u201338. MR0501537\nFisher, R. A. (1973). Statistical Methods for Scientific Induction, 3rd ed. Hafner, New York. MR0346954\nFinney, D. J. (1947). The estimation from individual records\nof the relationship between dose and quantal response.\nBiometrika 34 320\u2013334.\nGelman, A. (2004). Parametrization and Bayesian modeling.\nJ. Amer. Statist. Assoc. 99 537\u2013545. MR2109315\nGelman, A. (2006). Prior distributions for variance parameters in hierarchical models. Bayesian Anal. 1 515\u2013533.\nMR2221284\nGelman, A., van Dyk, A. A., Huang, Z. and Boscardin,\nJ. W. (2008). Using redundant parameters to fit hierarchical models. J. Comput. Graph. Statist. 17 95\u2013122.\nMR2424797\nGhosh, M., Reid, N. and Fraser, D. A. S. (2010). Ancillary\nstatistics: A review. Statist. Sinica 20 1309\u20131332.\nHe, Y. and Liu, C. (2009). The dynamic ECME algorithm.\nTechnical report, Dept. Statistics, Purdue Univ.\n\n\f12\n\nA. LEWANDOWSKI, C. LIU AND S. VANDER WIEL\n\nHobert, J. P. and Marchev, D. (2008). A theoretical\ncomparison of the data augmentation, marginal augmentation and PX-DA algorithms. Ann. Statist. 36 532\u2013554.\nMR2396806\nLavielle, M. and Meza, C. (2007). A parameter expansion\nversion of the SAEM algorithm. Statist. Comput. 17 121\u2013\n130. MR2380641\nLiu, C. (1997). ML estimation of the multivariate t distribution and the EM algorithm. J. Multivariate Anal. 63\n296\u2013312. MR1484317\nLiu, C. (2003). Alternating subspace-spanning resampling to\naccelerate Markov chain Monte Carlo simulation. J. Amer.\nStatist. Assoc. 98 110\u2013117. MR1965678\nLiu, C. (2004). Robit regression: A simple robust alternative to logistic and probit. In Applied Bayesian Modeling\nand Causal Inference from Incomplete-Data Perspectives\n(A. Gelman and X. L. Meng, eds.) 227\u2013238. Wiley, London. MR2138259\nLiu, C. and Rubin, D. B. (1994). The ECME algorithm: An\nsimple extension of EM and ECM with faster monotone\nconvergence. Biometrika 81 633\u2013648. MR1326414\nLiu, C. and Rubin, D. B. (1998). Ellipsoidally symmetric extensions of the general location model for mixed categorical\nand continuous data. Biometrika 85 673\u2013688. MR1665830\nLiu, C., Rubin, D. B. and Wu, Y. N. (1998). Parameter expansion to accelerate EM: The PX-EM algorithm.\nBiometrika 85 755\u2013770. MR1666758\nLiu, C. and Vander Wiel, S. A. (2007). Statistical quasiNewton: A new look at least change. SIAM J. Optim. 18\n1266\u20131285. MR2373301\nLiu, J. S. and Wu, Y. N. (1999). Parameter expansion for\ndata augmentation. J. Amer. Statist. Assoc. 94 1264\u20131274.\nMR1731488\nMartin, R., Zhang, J. and Liu, C. (2010). Dempster\u2013Shafer\ntheory and statistical inference with weak beliefs. Statist.\nSci. 25 72\u201387.\n\nMartin, R., Hwang, J.-S. and Liu, C. (2010). General theory of inferential models II. Marginal inference. Technical\nreport, Dept. Statistics, Purdue Univ.\nMeng, X. L. and Rubin, D. B. (1993). Maximum likelihood\nestimation via the ECM algorithm: A general framework.\nBiometrika 80 267\u2013278. MR1243503\nMeng, X. L. and van Dyk, D. (1997). The EM algorithm-\nan old folk-song sung to a fast new tune (with discussion).\nJ. Roy. Statist. Soc. Ser. B 59 511\u2013567. MR1452025\nMeng, X. L. and van Dyk, D. (1999). Seeking efficient data\naugmentation schemes via conditional and marginal augmentation. Biometrica 86 301\u2013320.\nMudholkar, G. S. and George E. O. (1978). A remark on\nthe shape of the logistic distribution. Biometrika 65 667\u2013\n668.\nQi, A. and Jaakkola, T. S. (2007). Parameter expanded\nvariational Bayesian methods. In Adv. Neural Info. Proc.\nSyst. 19. MIT Press, Cambridge.\nvan Dyk, D. A. and Meng, X. L. (2001). The art of data\naugmentation (with discussion). J. Comput. Graph. Statist.\n10 1\u2013111. MR1936358\nvan Dyk, D. A. and Meng, X. L. (2010). Cross-fertilizing\nstrategies for better EM mountain climbing and DA field\nexploration: A graphical guide book. Statist. Sci. To appear.\nYu, Y. and Meng, X. L. (2008). Espousing classical statistics\nwith modern computation: Sufficiency, ancillarity and and\ninterweacing generation of MCMC. Technical report, Dept.\nStatistics, Univ. California, Irvine.\nYu, Y. and Meng, X. L. (2010). To center or not to center,\nthat is not the question: An ancillarity-sufficiency interweaving strategy (ASIS) for boosting MCMC efficiency. J.\nComput. Graph. Statist. To appear.\nZhang, J. and Liu, C. (2011). Dempster\u2013Shafer inference\nwith weak beliefs. Statist. Sinica. To appear.\n\n\f"}