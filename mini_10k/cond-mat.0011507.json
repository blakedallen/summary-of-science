{"id": "http://arxiv.org/abs/cond-mat/0011507v1", "guidislink": true, "updated": "2000-11-29T18:58:07Z", "updated_parsed": [2000, 11, 29, 18, 58, 7, 2, 334, 0], "published": "2000-11-29T18:58:07Z", "published_parsed": [2000, 11, 29, 18, 58, 7, 2, 334, 0], "title": "Causality is an effect", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0011234%2Ccond-mat%2F0011137%2Ccond-mat%2F0011095%2Ccond-mat%2F0011233%2Ccond-mat%2F0011048%2Ccond-mat%2F0011479%2Ccond-mat%2F0011023%2Ccond-mat%2F0011410%2Ccond-mat%2F0011175%2Ccond-mat%2F0011373%2Ccond-mat%2F0011440%2Ccond-mat%2F0011172%2Ccond-mat%2F0011177%2Ccond-mat%2F0011256%2Ccond-mat%2F0011059%2Ccond-mat%2F0011171%2Ccond-mat%2F0011073%2Ccond-mat%2F0011384%2Ccond-mat%2F0011502%2Ccond-mat%2F0011068%2Ccond-mat%2F0011227%2Ccond-mat%2F0011334%2Ccond-mat%2F0011028%2Ccond-mat%2F0011187%2Ccond-mat%2F0011020%2Ccond-mat%2F0011408%2Ccond-mat%2F0011132%2Ccond-mat%2F0011395%2Ccond-mat%2F0011465%2Ccond-mat%2F0011094%2Ccond-mat%2F0011379%2Ccond-mat%2F0011146%2Ccond-mat%2F0011400%2Ccond-mat%2F0011255%2Ccond-mat%2F0011329%2Ccond-mat%2F0011209%2Ccond-mat%2F0011284%2Ccond-mat%2F0011345%2Ccond-mat%2F0011022%2Ccond-mat%2F0011336%2Ccond-mat%2F0011105%2Ccond-mat%2F0011222%2Ccond-mat%2F0011291%2Ccond-mat%2F0011030%2Ccond-mat%2F0011491%2Ccond-mat%2F0011503%2Ccond-mat%2F0011315%2Ccond-mat%2F0011401%2Ccond-mat%2F0011179%2Ccond-mat%2F0011251%2Ccond-mat%2F0011121%2Ccond-mat%2F0011245%2Ccond-mat%2F0011458%2Ccond-mat%2F0011370%2Ccond-mat%2F0011340%2Ccond-mat%2F0011411%2Ccond-mat%2F0011481%2Ccond-mat%2F0011044%2Ccond-mat%2F0011224%2Ccond-mat%2F0011302%2Ccond-mat%2F0011064%2Ccond-mat%2F0011399%2Ccond-mat%2F0011176%2Ccond-mat%2F0011221%2Ccond-mat%2F0011258%2Ccond-mat%2F0011128%2Ccond-mat%2F0011309%2Ccond-mat%2F0011314%2Ccond-mat%2F0011521%2Ccond-mat%2F0011157%2Ccond-mat%2F0011347%2Ccond-mat%2F0011382%2Ccond-mat%2F0011144%2Ccond-mat%2F0011518%2Ccond-mat%2F0011070%2Ccond-mat%2F0011082%2Ccond-mat%2F0011322%2Ccond-mat%2F0011198%2Ccond-mat%2F0011024%2Ccond-mat%2F0011438%2Ccond-mat%2F0011332%2Ccond-mat%2F0011444%2Ccond-mat%2F0011447%2Ccond-mat%2F0011219%2Ccond-mat%2F0011426%2Ccond-mat%2F0011295%2Ccond-mat%2F0011349%2Ccond-mat%2F0011530%2Ccond-mat%2F0011149%2Ccond-mat%2F0011018%2Ccond-mat%2F0011310%2Ccond-mat%2F0011371%2Ccond-mat%2F0011211%2Ccond-mat%2F0011174%2Ccond-mat%2F0011337%2Ccond-mat%2F0011317%2Ccond-mat%2F0011145%2Ccond-mat%2F0011507%2Ccond-mat%2F0011240%2Ccond-mat%2F0011283%2Ccond-mat%2F0011372&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Causality is an effect"}, "summary": "Using symmetric boundary conditions at separated times, I show analytically\nthat both the time ordering of (macroscopic) causality and the direction of\nentropy increase follow from these boundary conditions. In particular, when the\nendpoints have low entropy, these arrows of time point away from the ends and\ntoward the middle. Causality in this context means that when perturbations are\napplied, the effect of the perturbation---the macroscopic change in the\nsystem's behavior---is confined to one temporal side of the perturbations.\nThese results hold for both mixing and integrable systems, although relaxation\nfor integrable systems is incomplete. Simulations are presented for purposes of\nillustration.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0011234%2Ccond-mat%2F0011137%2Ccond-mat%2F0011095%2Ccond-mat%2F0011233%2Ccond-mat%2F0011048%2Ccond-mat%2F0011479%2Ccond-mat%2F0011023%2Ccond-mat%2F0011410%2Ccond-mat%2F0011175%2Ccond-mat%2F0011373%2Ccond-mat%2F0011440%2Ccond-mat%2F0011172%2Ccond-mat%2F0011177%2Ccond-mat%2F0011256%2Ccond-mat%2F0011059%2Ccond-mat%2F0011171%2Ccond-mat%2F0011073%2Ccond-mat%2F0011384%2Ccond-mat%2F0011502%2Ccond-mat%2F0011068%2Ccond-mat%2F0011227%2Ccond-mat%2F0011334%2Ccond-mat%2F0011028%2Ccond-mat%2F0011187%2Ccond-mat%2F0011020%2Ccond-mat%2F0011408%2Ccond-mat%2F0011132%2Ccond-mat%2F0011395%2Ccond-mat%2F0011465%2Ccond-mat%2F0011094%2Ccond-mat%2F0011379%2Ccond-mat%2F0011146%2Ccond-mat%2F0011400%2Ccond-mat%2F0011255%2Ccond-mat%2F0011329%2Ccond-mat%2F0011209%2Ccond-mat%2F0011284%2Ccond-mat%2F0011345%2Ccond-mat%2F0011022%2Ccond-mat%2F0011336%2Ccond-mat%2F0011105%2Ccond-mat%2F0011222%2Ccond-mat%2F0011291%2Ccond-mat%2F0011030%2Ccond-mat%2F0011491%2Ccond-mat%2F0011503%2Ccond-mat%2F0011315%2Ccond-mat%2F0011401%2Ccond-mat%2F0011179%2Ccond-mat%2F0011251%2Ccond-mat%2F0011121%2Ccond-mat%2F0011245%2Ccond-mat%2F0011458%2Ccond-mat%2F0011370%2Ccond-mat%2F0011340%2Ccond-mat%2F0011411%2Ccond-mat%2F0011481%2Ccond-mat%2F0011044%2Ccond-mat%2F0011224%2Ccond-mat%2F0011302%2Ccond-mat%2F0011064%2Ccond-mat%2F0011399%2Ccond-mat%2F0011176%2Ccond-mat%2F0011221%2Ccond-mat%2F0011258%2Ccond-mat%2F0011128%2Ccond-mat%2F0011309%2Ccond-mat%2F0011314%2Ccond-mat%2F0011521%2Ccond-mat%2F0011157%2Ccond-mat%2F0011347%2Ccond-mat%2F0011382%2Ccond-mat%2F0011144%2Ccond-mat%2F0011518%2Ccond-mat%2F0011070%2Ccond-mat%2F0011082%2Ccond-mat%2F0011322%2Ccond-mat%2F0011198%2Ccond-mat%2F0011024%2Ccond-mat%2F0011438%2Ccond-mat%2F0011332%2Ccond-mat%2F0011444%2Ccond-mat%2F0011447%2Ccond-mat%2F0011219%2Ccond-mat%2F0011426%2Ccond-mat%2F0011295%2Ccond-mat%2F0011349%2Ccond-mat%2F0011530%2Ccond-mat%2F0011149%2Ccond-mat%2F0011018%2Ccond-mat%2F0011310%2Ccond-mat%2F0011371%2Ccond-mat%2F0011211%2Ccond-mat%2F0011174%2Ccond-mat%2F0011337%2Ccond-mat%2F0011317%2Ccond-mat%2F0011145%2Ccond-mat%2F0011507%2Ccond-mat%2F0011240%2Ccond-mat%2F0011283%2Ccond-mat%2F0011372&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Using symmetric boundary conditions at separated times, I show analytically\nthat both the time ordering of (macroscopic) causality and the direction of\nentropy increase follow from these boundary conditions. In particular, when the\nendpoints have low entropy, these arrows of time point away from the ends and\ntoward the middle. Causality in this context means that when perturbations are\napplied, the effect of the perturbation---the macroscopic change in the\nsystem's behavior---is confined to one temporal side of the perturbations.\nThese results hold for both mixing and integrable systems, although relaxation\nfor integrable systems is incomplete. Simulations are presented for purposes of\nillustration."}, "authors": ["L. S. Schulman"], "author_detail": {"name": "L. S. Schulman"}, "author": "L. S. Schulman", "arxiv_comment": "For the proceedings of the conference, \"Time's Arrows, Quantum\n  Measurements and Superluminal Behavior,\" Naples, Italy, October 2000. To be\n  published by the Italian CNR. Contains two three-part figures", "links": [{"href": "http://arxiv.org/abs/cond-mat/0011507v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cond-mat/0011507v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cond-mat/0011507v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cond-mat/0011507v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:cond-mat/0011507v1 [cond-mat.stat-mech] 29 Nov 2000\n\nFor the proceedings of the conference, \"Time's\nArrows, Quantum Measurements and Superluminal Behavior,\" Naples, Italy, October 2000. To be\npublished by the Italian CNR.\n\nCausality is an effect\nL. S. Schulman\nPhysics Department, Clarkson University\nPotsdam, NY 13699-5820 USA\nemail: schulman@clarkson.edu\nABSTRACT\nUsing symmetric boundary conditions at separated times, I show analytically that both the time ordering of (macroscopic) causality and the direction of\nentropy increase follow from these boundary conditions. In particular, when the\nendpoints have low entropy, these arrows of time point away from the ends and\ntoward the middle. Causality in this context means that when perturbations are\napplied, the effect of the perturbation-the macroscopic change in the system's\nbehavior-is confined to one temporal side of the perturbations. These results\nhold for both mixing and integrable systems, although relaxation for integrable\nsystems is incomplete. Simulations are presented for purposes of illustration.\n\n1. Introduction\nBy \"causality\" I mean that if a system is perturbed the macroscopic effect\noccurs subsequent to the perturbation. There is a lot of baggage in this definition.\nFirst, I am not talking about the microscopic causality of relativistic quantum field\ntheory, which is a statement about the vanishing of commutators (or anticommutators) at spacelike separations. Second, I am trying to avoid the many and subtle\ndefinitions that have appeared in the philosophical literature, some of which are\nclose to mine, some of which are not. Then there is the word \"perturbation,\" which\nsuggests a kind of control or free will. Finally, there is the term \"macroscopic,\"\nequivalent to a notion of coarse grains, yet another nontrivial concept.\nDefining causality in terms of sequential order emphasizes its relation to the\nthermodynamic arrow of time. Indeed, some consider causality (with similar meaning and baggage) to be the primary concept [1, pp. 163\u2013164], with other kinds of\nordering (in particular, the second law of thermodynamics) consequences of it.\nI will take neither of these concepts to be primary, and will instead derive both\nfrom a model, or caricature, of the expansion of the universe. This follows the\nideas of Gold [2] and my own elaboration of them [3,4], in particular emphasizing\nthe notion of two-time boundary conditions. It is clear that it would be pointless\n\n\fto study causality as defined above using initial values for macroscopic problems,\nsince such a formulation forces the effect of a perturbation to be subsequent. So\nin studying causality, as in studying the arrow of time, one should formulate the\nproblem time-symmetrically if one's conclusions are to be noncircular.\nI will find that both macroscopic causality and the second law, meaning entropy\nincrease, can be derived in the appropriate two-time boundary condition context.\nFor sufficiently chaotic dynamical systems both features flow naturally from the\nformalism. For integrable systems, relaxation can be imposed by averaging over\nfrequencies. But with future conditioning additional time scales enter, and while\none can still get relaxation and causality, there is not the same simplicity as for\nchaotic systems.\nIn Sec. 2 I introduce the general context for this discussion as well as notation.\nIn the following section there is an analytic derivation of symmetric entropy increase\nfor systems having appropriate two-time boundary conditions. Causality, treated in\nSec. 4, is established using the same methods. In the last section numerical work is\nshown to illustrate the results of the previous sections. There are two appendices.\nIn the first I give a general derivation of entropy increase when coarse graining is\nimplemented at each time step. This is a master equation approach and is mainly\npedagogical. In the second appendix I indicate how the notion of \"perturbation\"\nneed not depend on philosophical questions concerning free will.\n\n2. Framework and notation\nAs in previous publications [3\u20136], my context is a two time boundary value\nproblem in which macroscopic data are specified at an early time, \"0,\" and a late\ntime, \"T .\" At both boundary times the system is in a restricted state (i.e., low\nentropy). For the systems previously studied the entropy increases, with varying\ndegrees of monotonicity, away from both boundary times. Moreover, for chaotic\nsystems, if the interval between the boundary times exceeds twice the system's\nrelaxation time, the initial relaxation is macroscopically indistinguishable from\nunconditioned time evolution. Furthermore, the evolution away from the final point\n(i.e., from T to smaller values of the parameter t) is the symmetric image of the\ninitial relaxation-this assertion is true even with conditions on the time evolution\nthat are weaker than time reversal invariance. All these features have been used to\nargue for Gold's thesis. In some of the references above I have elaborated on my\nrationale for taking this approach, and will not repeat the argument here. Most of\nmy previous demonstrations have used the cat map [7] as the dynamical system,\nand computer simulations to provide the evidence.\nIn this article I will argue more generally, extending both the systems studied\nand the method of justification. In effect this explains why the simulations work,\nalthough a discussion without explicit equations occurs in [4] and embodies the\nessential ideas to be presented below.\n2\n\n\fConsider classical mechanics on a (phase) space \u03a9. Let \u03bc be the measure on\n\u03a9 and \u03bc(\u03a9) = 1. Let the dynamics be given by a measure-preserving map \u03c6(t) on\n\u03a9, with \u03c6(t) (\u03c9) the time-t image of an initial point \u03c9 \u2208 \u03a9. The time parameter, t,\nmay be either continuous or discrete.\nThe notion of \"macroscopic\" is provided by a coarse graining on \u03a9. This is a\nfinite set of sets of strictly positive measure that cover \u03a9: {\u2206\u03b1 }, \u03b1 = 1, . . . , G, with\n\u222a\u03b1 \u2206\u03b1 = \u03a9, \u2206\u03b1 \u2229 \u2206\u03b2 = \u2205 for \u03b1 6= \u03b2. Let \u03c7\u03b1 be the characteristic function of \u2206\u03b1\nand let v\u03b1 = \u03bc(\u2206\u03b1 ). If f is a function on \u03a9, its coarse graining is defined to be\nZ\nX \u03c7\u03b1 (\u03c9)\nX\nf\u03b1\nfb(\u03c9) \u2261\nf\u03b1 =\n\u03c7\u03b1 hf i\u03b1 , with f\u03b1 = d\u03bc \u03c7\u03b1 (\u03c9)f (\u03c9) , hf i\u03b1 \u2261\n.\nv\nv\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n(1)\nR\nR\nb\nThus hf i\u03b1 is the average of f on \u2206\u03b1 and \u03a9 f = \u03a9 f .\nLet the system's distribution in \u03a9 be described by a density function \u03c1(\u03c9). One\ncan think of this distribution in more than one way. In terms of the ideal gas of\ncat map atoms that I have used before, \u03c1 can be thought of as the density of atoms\non the phase space, I 2 , on which a single cat map lives. (In this case \u03c1 is a sum\nof \u03b4-functions.) More generally, one can think of \u03a9 as the 6N -dimensional phase\nspace of N particles in three space dimensions. In this way there is no restriction\nin allowing interactions among the particles.\nIf one takes a primitive notion of entropy as\nZ\nSprim = \u2212\n\u03c1(\u03c9) log(\u03c1(\u03c9)) d\u03bc ,\n\u03a9\n\nthen Sprim is constant in time, trivially by virtue of the measure preserving property\nof \u03c6(t) (and its invertibility). The entropy that I will use for studying irreversibility\ninvolves coarse graining and is defined as\nZ\nS(\u03c1) = Sprim (b\n\u03c1) = \u2212\n\u03c1b log \u03c1b d\u03bc .\n(2)\n\u03a9\n\nIt is easy to show that\n\nwhere, as in Eq. (1), \u03c1\u03b1 =\ndefined by\n\nR\n\nS(\u03c1) = S(\u03c1\u03b1 |v\u03b1 ) ,\n\u2206\u03b1\n\n\u03c1 d\u03bc, and the function S(p|q) is the relative entropy\n\nS(p|q) \u2261 \u2212\n\nX\n\np(x) log\n\nx\n\n\u0012\n\np(x)\nq(x)\n\n\u0013\n\n,\n\nwith p andPq probability\ndistributions such that p(x) vanishes only if q(x) does.\nR\nNote that\n\u03c1\u03b1 = \u03c1 = 1, and that all v\u03b1 are nonzero [8].\n3\n\n\f3. Time-dependence of the entropy, with and without future conditioning\nThe system is required to start (t = 0) in a subset \u01eb0 \u2282 \u03a9 and end (t = T ) in\na subset \u01ebT \u2282 \u03a9. The points of \u03a9 satisfying this two-time boundary condition are\n\u01eb = \u01eb0 \u2229 \u03c6(\u2212T ) (\u01ebT ) .\n\n(3)\n\nThe set \u01eb can be empty. I have argued though [9,4] that for chaotic dynamics and\nfor sufficiently long times T there exist solutions, i.e., \u01eb 6= \u2205. Moreover, for such\ntimes\n\u03bc(\u01eb) \u223c \u03bc(\u01eb0 )\u03bc(\u01ebT ) .\n(4)\nTo see how this comes about, consider mixing dynamics. The map \u03c6(t) is mixing if\n\u0010\n\u0011\n(t)\nlim \u03bc A \u2229 \u03c6 (B) = \u03bc(A)\u03bc(B)\n(5)\nt\u2192\u221e\n\nfor measurable subsets A and B of \u03a9. For such systems Eq. (4) will be satisfied\nin the t \u2192 \u221e limit. This limit says nothing about rates of convergence, but I will\nassume that there is some time \u03c4 such that the decorrelation condition (Eq. (5))\nholds to good accuracy for t \u2265 \u03c4 [10]. The set \u01eb will therefore be nonempty for\nt \u2265 \u03c4 . Under \u03c6(t) , \u01eb becomes\n\u01eb(t) = \u03c6(t) (\u01eb0 ) \u2229 \u03c6(t\u2212T ) (\u01ebT ) .\nTo calculate the entropy, the density, which was \u03c1(0) = \u03c7\u01eb /\u03bc(\u01eb) at time-0, must be\ncoarse grained. The important quantity for the entropy calculation is\n\u0001\n\u03bc \u2206\u03b1 \u2229 \u03c6(t) (\u01eb0 ) \u2229 \u03c6(t\u2212T ) (\u01ebT )\n\u03bc (\u2206\u03b1 \u2229 \u01eb(t))\n\u03c1\u03b1 (t) =\n=\n.\n\u03bc(\u01eb)\n\u03bc(\u01eb)\nIf T \u2212 t > \u03c4 then the following will hold\n\u0010\n\u0011\n\u0010\n\u0011 \u0010\n\u0011\n\u03bc \u2206\u03b1 \u2229 \u03c6(t) (\u01eb0 ) \u2229 \u03c6(t\u2212T ) (\u01ebT ) =\u03bc \u2206\u03b1 \u2229 \u03c6(t) (\u01eb0 ) \u03bc \u03c6(t\u2212T ) (\u01ebT ) ,\n\u0010\n\u0011\n\u03bc(\u01eb) =\u03bc(\u01eb0 )\u03bc \u03c6(\u2212T ) (\u01ebT ) .\nUsing the measure-preserving property of \u03c6(t) , the factors \u03bc(\u01ebT ) in both numerator\nand denominator of \u03c1\u03b1 cancel, leading to\n\u0010\n\u0011.\n\u03c1\u03b1 = \u03bc \u2206\u03b1 \u2229 \u03c6(t) (\u01eb0 )\n\u03bc(\u01eb0 ) .\nThis is precisely what one gets without future conditioning, so that all macroscopic\nquantities, and in particular the entropy, are indistinguishable from their unconditioned values.\n4\n\n\fWorking backward from time-T one obtains an analogous result. Define a\nvariable s \u2261 T \u2212 t and set \u01eb\u0303(s) \u2261 \u01eb(T \u2212 s). Then\n\u01eb\u0303(s) = \u03c6(T \u2212s) (\u01eb0 ) \u2229 \u03c6(\u2212s) (\u01ebT ) .\nIf s satisfies T \u2212 s > \u03c4 , then when the density associated with \u01eb\u0303(s) is calculated, its\ndependence on \u01eb0 will drop out. It follows that\n\u0010\n\u0011.\n(\u2212s)\n\u03c1\u03b1 (s) = \u03bc \u03c6\n(\u01ebT )\n\u03bc(\u01ebT ) .\nFor a time-reversal invariant dynamics this will give the entropy the same time dependence coming back from T as going forward from 0. It is interesting that the\ncat map is not strictly time-reversal invariant (by definitions of the form given in\n[11]) but, as I have shown repeatedly, its entropy as a function of time is symmetric.\nThe reason is that the Lyapunov exponent is the same for the map and its inverse.\nFor the cat map, there isn't much choice: the 2 \u00d7 2 matrix has only two eigenvalues\nand their product is unity. But I expect the similarity of macroscopic dynamics\nin both directions to obtain even for richer systems. Thus, comparing true physical dynamics with its time-reversed counterpart, ordinary macroscopic relaxation\nshould be the same, yielding symmetric entropy dependence. I justify this expectation by the absence (so far) of any time-reversal or CP violating observations at the\natomic level, as well as the assumption that ordinary physical relaxation processes,\naccounting for the thermodynamic arrow of our experience, occur at grosser levels\nthan those at which CP violation has been detected.\nIt is worth putting into words the essence of the mathematical argument just\ngiven. The set \u01eb is a subset of \u01eb0 ; which points of \u01eb0 are also in \u01eb is determined by the\nchoppy characteristic function of the set \u03c6(\u2212T ) (\u01ebT ). For long enough times, T , the\ngood points of \u01eb are Poisson distributed within \u01eb0 [4]. Thus following \u01eb forward in\ntime (with \u03c6) is like following a random subset of \u01eb0 . But such time evolution is one\nway of studying \u01eb0 itself. If you wanted to do a Monte Carlo study for the evolution\nof \u01eb0 , your technique would be to follow the time dependence of a random subset.\nThe pseudo-randomness imposed by the characteristic function of \u03c6(\u2212T ) (\u01ebT ) is not\nworse than other kinds of pseudo-randomness.\nThe same pseudo-randomness holds for \u01eb(t) (t > 0), provided the time to the\nfinal point, T \u2212 t, is greater than \u03c4 . I have used the mixing property to argue\nfor randomness, but I expect weaker conditions of ergodicity to be sufficient in\nphysically relevant situations.\nIntegrable systems (relaxation)\nWithout mixing or some kind of ergodicity the foregoing arguments fail. However, harmonic oscillators can be quite useful in studies of relaxation [12], although\nin previous two-time boundary value studies [3] deficiencies were noted. The general idea is that although an individual oscillator does not spread in phase space, if\nenough different frequencies are taken there is relaxation.\n5\n\n\fRather than work with sets, as above, I consider an \"ideal gas\" of N oscillators,\nwith oscillator #k having position x(k) and frequency \u03bd (k) . (For convenience I take\nthe period of these oscillators to be 1 (rather than 2\u03c0) and use frequency rather\n(k)\nthan angular frequency.) Time evolution is given by x(k) (t) = x0 + \u03bd (k) t (mod 1).\nThe boundary conditions are\n(k)\n\n(k)\n\n0 \u2264 x0 \u2264 \u03b4x & 0 \u2264 x(k) (T ) \u2264 \u03b4x , with x(k) (T ) = x0 + \u03bd (k) T (mod 1) ,\n\u03bd0 \u2264 \u03bd (k) \u2264 \u03bd0 + \u03b4\u03bd\n\n(\u03bd does not change in time) .\n\n(6)\n\nBoth x(k) and \u03bd (k) can be randomly selected consistent with these conditions.\nFrom the final-time condition on x it follows that for sufficiently large T there\nis a nonempty finite set of integers {nl } so that\n(k)\n\n(k)\n\n\u2212x0\nT\n\n\u2264\u03bd\n\n(k)\n\nnl\n\u03b4x \u2212 x0\n\u2212\n\u2264\nT\nT\n\n,\n\n(7)\n\nwith \u03bd (k) within the permitted range. One can plot the set of allowed initial points\nin the x-\u03bd plane. Within the rectangle [0, \u03b4x] \u00d7[\u03bd0 , \u03bd0 +\u03b4\u03bd] the solution points fall in\na sequence of parallel parallelograms. Each is bounded by vertical lines at x = 0, \u03b4x\nand by lines of slope \u22121/T defining the upper and lower values for each nl (except\nthat parallelograms going outside [\u03bd0 , \u03bd0 + \u03b4\u03bd] are cut off). For large T there will\nbe many such parallelograms; for small T , few or none.\nA natural coarse graining is to divide the x-range, [0, 1], into G intervals and\nlook only at values of x to compute entropy, since \u03bd does not change. For present\npurposes the boundary value quantity, \u03b4x, will be taken smaller than 1/G.\nI first examine the equilibration of this system without future conditions. Initially all points are in [0, \u03b4x]; they separate from one another only by virtue of\npossessing different frequencies. Equilibration is marked by \u03b4\u03bd t \u2248 1, leading to the\ndefinition of a relaxation time \u03c4osc = 1/\u03b4\u03bd. It follows that on a time scale of \u03c4osc the\nentropy will rise from \u2212 log G to 0 (with the stated condition, \u03b4x < 1/G).\nNow consider the situation with the future conditioning of Eq. (6). For each nl\nof Eq. (7), the points of its parallelogram either remain in a single grain (\u2200t \u2264 T )\nor at worst overlap two. This does not contribute significantly to entropy increase.\nRather, the separation of individual parallelograms is required for equilibration. The\nnumber of such parallelograms is estimated by replacing \u03bd by n/T in the second part\nof Eq. (6). It follows that the number of n values is T \u03b4\u03bd. Neglecting grain overlap,\nthis implies that the maximum entropy for the two-time boundary value problem\nis log(T \u03b4\u03bd) \u2212 log G. Therefore the system cannot fully relax unless T > G/\u03b4\u03bd. This\nis much longer than the unconditioned relaxation time, \u03c4osc = 1/\u03b4\u03bd, and is in sharp\ncontrast to the behavior of mixing systems-for the cat map the \"T \" necessary for\nnormal initial relaxation is only twice the usual relaxation time.\nThere are further defects in the relaxation. Consider what happens when t =\nT /2. Write \u03bd = (n + \u03b2)/T , with \u03b2 a number on the order of \u03b4x (cf. Eq. (7)). Then\n6\n\n\f(k)\n\nx(k) (T /2) = x0 + n/2 + \u03b2/2. For even n this means that many of the points\nare back in the original interval, or close to it. Thus the entropy will drop. The\nsame happens, but less dramatically, for other divisors. Finally, there is an inherent\nweakness in any oscillator equilibrium, in that only half the dynamical variables\nrelax at all-the frequencies (\u03bd) do not change.\nIn terms of the big bang-big crunch cosmological model considered in [4\u20136]\nthese defects are probably irrelevant. It appears that the \"oscillators\" in our cosmos are mostly the degrees of freedom of the electromagnetic field. These reach\nequilibrium through being coupled to massive matter, which presumably does relax appropriately. If they do satisfy a two-time boundary condition with, say, the\nboundary times at the decoupling epoch and its pre-big crunch partner, then I would\nnot expect the timing to be so precise and coincident that one would get photon\nentropy-lowering at the cosmological midpoint (as in our \"even n\" condition above).\nMoreover, photons do not equilibrate very well: witness the preservation of indications of spatial structure as deduced from cosmic background radiation. On the\nother hand, the spectrum of this radiation corresponds very well to equilibrium, the\nreason being interaction with matter prior to decoupling.\n\n4. Causality and peturbations\nThe notion of macroscopic causality used here involves a perturbation. One\nimposes two-time boundary conditions and considers dynamical evolution with both\nunperturbed and perturbed dynamics. When solving the same boundary value\nproblem, these rules will select different microscopic solutions. Although I will\nconsider perturbations occurring only at a single moment in time, the microscopic\nsolutions will (in general) differ everywhere. But it is the macroscopic solutions\nthat allow a notion of causality. In principle macroscopic behavior could also differ\nat all times (except for the boundaries), but in a system with causality they will\ndiffer on only one side of the perturbation. For the usual causality, they will differ\nonly after the perturbation. But we will also find that they can differ only before,\nwhere in this sentence and the last the words \"before\" and \"after\" are defined with\nrespect to a microscopic time parameter, that, as will be seen, may differ from the\nnatural thermodynamic time.\nThere is a delicate point here that is discussed in Appendix B. The term\n\"perturbation\" suggests free will, while two-time boundary conditions sound like\nthe opposite. Resolving issues of free will is not my objective, and the appendix is\ndevoted to formulating the concept of perturbation in a purely physical context.\nAlthough I will later give examples (figures) in terms of discrete time, for\nformal purposes it is easiest to work in continuous time and to imagine that the\nperturbation is instantaneous. The time interval for the boundary value problem\nis [0, T ]. Call the unperturbed system A; its history, time evolution, dynamics and\nboundary conditions are exactly as described in the previous section. That is, it\n7\n\n\fevolves under \u03c6(t) , its boundary conditions are \u01eb0 and \u01ebT , and its microstates are\nin the set\n\u01ebA = \u01eb0 \u2229 \u03c6(\u2212T ) (\u01ebT )\n(8)\n(formerly called \u01eb). System B, the perturbed case, has an additional transformation\nact on it at time-t0 . Call this transformation \u03c8. It should not be dissipative-I do\nnot want the arrow to arise from such an asymmetry alone [13]. \u03c8 is thus invertible\nand measure preserving. Successful solutions must go from \u01eb0 to \u01ebT under the\ntransformation \u03c6(T \u2212t0 ) \u03c8\u03c6(t0 ) . The microstates for system B are therefore in\n\u01ebB = \u01eb0 \u2229 \u03c6(\u2212t0 ) \u03c8 \u22121 \u03c6(\u2212T +t0 ) (\u01ebT )\n\n(9)\n\nClearly, \u01ebA and \u01ebB are different. But as I shall now show, for mixing dynamics\nand for sufficiently large T , the following hold: 1) for t0 close to 0, the only differences in macroscopic behavior between A and B are for t > t0 ; 2) for t0 close to\nT , the only differences in macroscopic behavior between A and B are for t < t0 .\nThis means (recalling Sec. 3) that the direction of causality follows the direction of\nentropy increase.\nThe proof is nearly the same as that of the previous section. Again we use the\ntime \u03c4 such that the mixing decorrelation holds for time intervals longer than \u03c4 .\nFirst consider t0 close to 0. The observable macroscopic quantities are the densities\nin grain-\u2206\u03b1 , which are, for t < t0 ,\n\u0010\n\u0011.\nA\n(t)\n(t\u2212T )\n\u03c1\u03b1 (t) = \u03bc \u2206\u03b1 \u2229 \u03c6 (\u01eb0 ) \u2229 \u03c6\n(\u01ebT )\n\u03bc(\u01ebA ) ,\ni\n\u0011.\n\u0010\nh\n(t)\n(t\u2212t0 ) \u22121 (t0 \u2212T )\n(\u01eb\n)\n\u03bc(\u01ebB ) .\n\u03c1B\n(t)\n=\n\u03bc\n\u2206\n\u2229\n\u03c6\n(\u01eb\n)\n\u2229\n\u03c6\n\u03c8\n\u03c6\nT\n\u03b1\n0\n\u03b1\n\n\u0001\n(t)\nAs before, the mixing property, for T \u2212t > \u03c4 , yields \u03c1A\n\u03b1 (t) = \u03bc \u2206\u03b1 \u2229 \u03c6 (\u01eb0 ) /\u03bc(\u01eb0 ),\nwhich is the initial-value-only macroscopic time evolution. For \u03c1B\n\u03b1 , the only differ\u22121\n\u22121\nence is to add a step, \u03c8 . Unless \u03c8 is diabolically contrived to undo \u03c6(\u2212u) for\nlarge u, this will not affect the argument that showed that the dependence on \u01ebT\ndisappears. Thus A and B have the same macrostates before t0 .\nB\nFor t > t0 , \u03c1A\n\u03b1 (t) continues its behavior as before. For \u03c1\u03b1 (t) things are different:\n\u0010\nh\ni\n\u0011.\n(t\u2212t0 )\n(t0 )\n(t\u2212T )\n\u03c1B\n(t)\n=\n\u03bc\n\u2206\n\u2229\n\u03c6\n\u03c8\u03c6\n(\u01eb\n)\n\u2229\n\u03c6\n(\u01eb\n)\n\u03bc(\u01ebB ) (t > t0 ).\n\u03b1\n0\nT\n\u03b1\n\nNow I require T \u2212 t > \u03c4 . If this is satisfied the \u01ebT dependence drops out and\ni\n\u0011.\n\u0010\nh\n(t\u2212t0 )\n(t0 )\n(\u01eb\n)\n\u03bc(\u01eb0 ) .\n\u03c1B\n(t)\n=\n\u03bc\n\u2206\n\u2229\n\u03c6\n\u03c8\u03c6\n0\n\u03b1\n\u03b1\nThe shows that the effect of \u03c8 is the usual initial-conditions-only phenomenon.\nIf we repeat these arguments for t such that T \u2212 t is small, then just as we\nshowed in Sec. 3, the effect of \u03c8 will only be at times t less than t0 .\n8\n\n\fThis manifestation of causality has a clear intuitive origin. As the perturbation\ntime, t0 , choose a value small enough that the system has not equilibrated [14]. All\npoints, a \u2208 \u01ebA and b \u2208 \u01ebB , start in \u01eb0 and end in \u01ebT . Working backward from t0 ,\nwhat can b do? It must get to \u01eb0 . Since t0 is less than the relaxation time, the\nplaces it can be are essentially the same places that a can be. However, after the\nperturbation the need to arrive in \u01ebT places no macroscopic restriction on b, because\nfrom any coarse grain in \u03a9 you can find your way into \u01ebT . This is precisely because\nworking backward from T , the set \u01ebT spreads throughout \u03a9 in time T \u2212 t0 (and\nin particular \u03c6(t0 \u2212T ) \u01ebT enters the coarse grain into which \u03c8 would send b if there\nwere no future conditioning). Thus, satisfying the changed boundary conditions is\naccomplished by keeping a and b close to one another before t0 , and allowing the\nperturbation a free hand in moving b away from a, after t0 .\nIntegrable systems (causality)\nAs before, without mixing or some kind of ergodicity our arguments fail. Nevertheless, just as frequency smearing gave relaxation, however imperfect, it can give\ncausality. Again an extended time scale is needed, but the intuitive reasoning just\ngiven continues to hold in the integrable case as well.\nConsider a particular example, an oscillator of the sort discussed in Sec. 3. Take\n\u03b4x so small that the condition in Eq. (6) forces all the points to have essentially\nthe time dependence, x = \u03bdt, with \u03bdT = n. The angular frequency \u03bd therefore\nsatisfies \u03bd = n/T , with n selected so that \u03bd0 \u2264 \u03bd \u2264 \u03bd0 + \u03b4\u03bd; for large T , this allows\nan extensive range of n values. Now consider the following perturbation: at the\nmoment t0 , x is displaced by a macroscopic angle \u03b3, i.e., \u03b3 > 1/G. Solving the\nsame boundary value problem gives x = \u03bdt before t0 , and x = \u03bdt + \u03b3 after t0 . With\nthe perturbation, \u03bd must satisfy \u03bd = n/T \u2212 \u03b3/T , again yielding an extensive range\nof n values for large T . The difference between the two ranges of n values is \u03b3/T ,\nwhich for large enough T will be a small fraction of all n values that are common\nto the perturbed and unperturbed motion. Such n are henceforth dropped from\nconsideration.\nFor n values that are common to the two solution sets, the difference between\nsolutions with the same n arises from the \u03b3-dependent difference in \u03bd:\n\u001a\n\u03b3t/T\nt < t0\nEffectn = xunperturbed \u2212 xperturbed =\n,\n(10)\n\u2212 (1 \u2212 t/T ) \u03b3\nt > t0\nwhich is independent of n. It follows that there is a difference between perturbed\nand unperturbed motion that is of order \u03b3 through most of the time period [0, T ].\nThe effect of the perturbation is felt both before and after. It thus appears that\nthere is no causality, but closer consideration shows this conclusion to be wrong.\nRecall that it is only meaningful to consider perturbations that take place\nbefore the system has relaxed (or close enough to T that the reverse process has\ncommenced). Thus the perturbation should occur for t0 < \u03c4osc = 1/\u03b4\u03bd. On the other\n9\n\n\fEntropy, S\n\n0\n\n0\n\n0\n\n\u22120.5\n\n\u22120.5\n\n\u22120.5\n\n\u22121\n\n\u22121\n\n\u22121\n\n\u22121.5\n\n\u22121.5\n\n\u22121.5\n\n\u22122\n\n\u22122\n\n\u22122\n\n\u22122.5\n\n\u22122.5\n\n\u22122.5\n\n\u22123\n\n\u22123\n\n\u22123\n\n\u22123.5\n\n\u22123.5\n\n\u22123.5\n\n\u22124\n\n\u22124\n\n\u22124\n\n\u22124.5\n\n\u22124.5\n0\n\n2\n\n4\n\n6\n\n8\n\nt\u2192\n\n10\n\n12\n\n14\n\n16\n\n\u22124.5\n0\n\n2\n\n4\n\n6\n\n8\n\nt\u2192\n\n10\n\n12\n\n14\n\n16\n\n0\n\n2\n\n4\n\n6\n\n8\n\nt\u2192\n\n10\n\n12\n\n14\n\nFigure 1. Entropy, S, as a function of time for a mixing system, with two-time\nconditioning. For the left figure (2a) there is no perturbation. In the middle (2b)\nthere is a perturbation at time 3. On the right (2c) the perturbation is nominally\nat time 14, although because of the way entropy is calculated (after a time step, in\nterms of the nonthermodynamic parameter t) it is effectively at time 13 21 .\nhand, for full relaxation the value of T should be greater than G/\u03b4\u03bd, as discussed in\nSec. 3. From Eq. (10) the maximum value of the precursor-the noncausal term-is\nt0 /T , just before the perturbation. These considerations are combined to yield\nt0\n\u03c4osc\n1\n1\n* Maximum noncausal precursor =\n<\n= .\n\u03b3\nT\nG/\u03b4\u03bd\nG\nBut the size of a coarse grain is 1/G, so that this precursor is in fact microscopic.\nDouble arrow systems\nIn [5] I showed that causality obtains in opposite directions in systems containing opposite arrows. The general principle is the same as that presented here\nalthough a detailed presentation would be more complicated by virtue of the simultaneous presence of two directions for causality. I will not provide an analytic\ndemonstration and only mention this matter here for completeness.\n5. Numerical illustrations\nAlthough the purpose of the present article is to go beyond the numerical\nsimulations of previous publications, I will illustrate the phenomena studied here.\nIn Fig. 1 I show the effect of using two-time boundary values on the dynamics\nof the cat map, \u03c6C . (This is a map of the unit square into itself with the rule:\nx\u2032 \u2261 x + y, y \u2032 \u2261 x + 2y, mod 1. It is a mixing transformation, intensively exploited\nin ergodic theory [7] and I have used it as an example for two-time boundary value\nproblems in many places, [4], etc.)\nOn the left (1a) there is no perturbation. The boundary conditions are that\nthe system must be in a particular coarse grain (of size 0.1\u00d70.1) at times 0 and 16.\n10\n\n16\n\n\fEntropy, S\n\n0\n\n0\n\n0\n\n\u22120.5\n\n\u22120.5\n\n\u22120.5\n\n\u22121\n\n\u22121\n\n\u22121\n\n\u22121.5\n\n\u22121.5\n\n\u22121.5\n\n\u22122\n\n\u22122\n\n\u22122\n\n\u22122.5\n\n\u22122.5\n\n\u22122.5\n\n\u22123\n\n\u22123\n\n\u22123\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nt\u2192\n\n120\n\n140\n\n160\n\n180\n\n200\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\nt\u2192\n\n12\n\n14\n\n16\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\nt\u2192\n\nFigure 2. Entropy, S, as a function of time, for oscillators with two-time conditioning. The left figure (2a) shows an entire run of 200 time steps. Both perturbed and\nunperturbed motion appear. They differ in many places. Note also the half-time\ndepression in entropy, as well as other, smaller reductions. In the middle (2b) is\nshown only the first few time steps. The perturbation is at time-4. Causality is\nevident. On the right (2c) only 50 time steps are used. Although from the previous\nfigures it is clear that the entropy can reach its maximum values within about 10\ntime steps, when the conditioning time is 50 (as in 2c) the system cannot get near\nthe equilibrium value. (All figures show the same numerical range of entropy.)\n\nEvidently the entropy is a more or less symmetric function of time. (The statistical\nerror comes from using a sample of 500 points, rather than the set \u01eb.)\nIn terms of the direction of entropy increase it is that clear this arrow is a\nconsequence of the boundary values given.\nIn the middle figure (1b), a perturbation is applied at time-3. At that time,\ninstead of \u03c6C , a different map is applied (x\u2032 \u2261 2x + 3y, y \u2032 \u2261 x + 2y, mod 1; this\nis more chaotic, and equilibrates faster than \u03c6C ). The entropy is calculated (in the\ncomputer simulation) after time 3, \"after\" in the sense of the nonthermodynamic\nparameter t, but reports this as the time-3 entropy. Because of this convention one\ncan think of the perturbation as taking place at time-2 21 . The deviation between\nthe perturbed and unperturbed entropy is for times 3 and 4 (by time-5 both systems\nare in equilibrium). Because the perturbed system received a bigger kick at time-3\nits entropy increases more rapidly.\nThe point of this figure is that the difference between the curves is confined to\ntimes later than the perturbation. The system shows causality.\nThe right hand figure (1c) shows a system that is perturbed at time-14. As\nbefore, the entropy-calculating convention makes this effectively a perturbation at\ntime 13 21 . In this case, the difference between perturbed and unperturbed systems is\nbefore the perturbation, \"before\" in the sense of the nonthermodynamic parameter\nt. However, in terms of the direction of entropy increase, the entropy increase arrow\nand the causality arrow agree. This could be called reverse causality, but it is just\nnormal causality with a bad choice of nonthermodynamic time parameter.\n11\n\n50\n\n\fFinally I show what happens for harmonic oscillators. The perturbation is\nslightly different from that studied analytically above. Rather than a displacement,\nthe system advances by 3\u03bd instead of \u03bd. The results are essentially the same and\nby this small change one also can see a level of robustness of the phenomenon.\nFor Fig. 2 there are 25 coarse grains along the \"x\" direction and the frequency\ninterval is of width 1/10. Thus unconditioned relaxation should take place in about\n10 time steps, but full equilibration should take about 250. For Fig. 2a and 2b\n(which are from the same run) both aspects are evident. With 200 time steps the\nsystem does approach S = 0 and the relaxation time is about 10 (as is seen more\neasily in Fig. 2b). On the other hand, in Fig. 2c, with conditioning for 50 time steps,\nS is far from 0, so that the potential for 10-time-step relaxation is thwarted by the\nfuture condition. In all cases there is a perturbation at time-4. Fig. 2b clearly\nshows that there is causality in this case. On the other hand, for the third figure,\nthe system's relaxation is so compromised that the action of the perturbation takes\nplace when the system has reached its maximum, although reduced, entropy level.\n\nAcknowledgements\nI am grateful to the Istituto Italiano per gli Studi Filosofici, Naples, for hosting\nthe conference where this material was presented. My research is supported in part\nby the United States National Science Foundation grant PHY 97 21459.\n\nAppendix A. Entropy increase, stochastic dynamics and coarse graining\nThe formalism developed above is useful for a general derivation of entropy\nnondecrease. The derivation also holds for quantum mechanics.\nProposition: Coarse graining a distribution function, evolving it forward, and then\nagain coarse graining, either increases the entropy or leaves it unchanged.\nLet the distribution function for a classical system at time-0 be \u03c1. It is coarse\ngrained to yield \u03c1b, which is taken as \u03c1(0). Thus\n\u03c1(0) =\n\nX \u03c7\u03b1\nv\u03b1\n\n\u03c1\u03b1\n\nwith \u03c1\u03b1 =\n\nZ\n\n\u03c7\u03b1 \u03c1 .\n\nP\nFrom Sec. 2, the entropy of this distribution is S(\u03c1\u03b1 |v\u03b1 ) = \u2212 \u03c1\u03b1 log(\u03c1\u03b1 /v\u03b1 ).\n(Recall that v\u03b1 = \u03bc(\u2206\u03b1 ), the volume of coarse grain \u03b1.) At time-t, \u03c1b becomes\n\u03c1b(t) =\n\nX \u03c7\u03b1\u0303\nv\u03b1\n\n\u03c1\u03b1\n\nwith \u03c7\u03b1\u0303 the characteristic function of \u03b1\u0303 \u2261 \u03c6(t) (\u2206\u03b1 ). Now coarse grain again. This is\nthe step where entropy nondecrease is forced, and I discuss its physical significance\n12\n\n\fTable 1. Classical-quantum correspondence for the entropy increase proposition.\n\n\u03a9\n\u2206\u03b1\n\u03c7\u03b1\n\u03c6(t)\n\nH (Hilbert space)\nH\u03b1 (subspace)\nP\u03b1 (projector)\nUt\n\n\u03c1\n\u03bc\nv\u03b1\n\n\u03c1 (density matrix)\nTrace\ndimension of H\u03b1\n\nbelow. Coarse graining the function \u03c7\u03b1\u0303 , the distribution function becomes\nd\n\u03c1(t) = \u03c1b\n(t) =\nwith\n\u03c1\u2032\u03b2 =\n\nX\n\nX\n\n\u03c7\u03b2 \u03c1\u03b1\n\n\u03b1,\u03b2\n\nR(\u03b2, \u03b1)\u03c1\u03b1\n\n\u03b1\n\n\u0010\n\u0011 X\u03c7\n1\n\u03b2 \u2032\n\u03bc \u2206\u03b2 \u2229 \u03c6(t) (\u2206\u03b1 ) =\n\u03c1 ,\nv\u03b1 v\u03b2\nv\u03b2 \u03b2\n\nand\n\n\u0010\n\u0011.\nR(\u03b2, \u03b1) \u2261 \u03bc \u2206\u03b2 \u2229 \u03c6(t) (\u2206\u03b1 )\nv\u03b1 .\n\nIt follows that the entropy of the distribution \u03c1\u2032 is S ((R\u03c1)\u03b2 |v\u03b2 ). Thus to establish\nthe proposition above I must show that S(R\u03c1|v) \u2265 S(\u03c1|v).\nFirst I show that the matrix R is stochastic,\ni.e., its \u0002elements are nonnegative\n\u0003\nP\nand each column sums to one. The sum is \u03b2 R(\u03b2, \u03b1) = \u03bc (\u222a\u03b2 \u2206\u03b2 ) \u222a \u03c6(t) (\u2206\u03b1 ) /v\u03b1 .\nSince \u03c6(t) is measure preserving this sum gives unity. Furthermore, Rv = v, with v\nthe vector of grain volumes.\nIt is a theorem [15,16] that for any pair of distributions, p and q, for which\nS(p|q) is defined, and for any stochastic matrix M , 0 \u2265 S(M p|M q) \u2265 S(p|q).\nApply this to \u03c1 and \u03c1\u2032 . Making use of Rv = v, the proposition stated above on\nentropy nondecrease is established for classical dynamics.\nThe physical content of this derivation was incorporated in the replacement\nof \u03c1b(t) by its coarse grained smearing. The assumption is that within each grain\nthe phase space points have spread uniformly. Thus for physical application of\nthis proposition t cannot be arbitrarily small. It must exceed a microscopic relaxation time associated with the coarse grains. Moreover, in coarse graining there is a\ndestruction of information-monotonic entropy behavior contradicts the entropy reversal one gets using low-entropy two-time boundary conditions, as well as the more\ntraditional counterexamples arising from Poincar\u00e9 recurrence and time reversal.\nThe quantum version of this proposition involves no new mathematics, only a\ncorrespondence between the classical and quantum quantities. See Table 1. I find\n\u03c1b =\n\nX P\u03b1\nv\u03b1\n\n\u03c1\u03b1 , with \u03c1\u03b1 = Tr P\u03b1 \u03c1 and v\u03b1 = Tr P\u03b1 .\n\nEntropy is again S(\u03c1\u03b1 |v\u03b1 ). (The vs no longer sum to unity, but this makes no\nessential difference.) Time evolution is given by a unitary operator, Ut , acting\nin the usual way: \u03c1(t) = Ut \u03c1(0)Ut\u2020 . Carrying through the same steps as for the\n13\n\n\fclassical case, coarse graining, evolving in time and coarse graining again, leads to\nthe same equations, but with the matrix \"R\" now given by\ni.\nh\nv\u03b1 .\nR(\u03b2, \u03b1) = Tr P\u03b2 Ut P\u03b1 Ut\u2020\nStochasiticity of R is readily established and entropy nondecrease follows as above.\n\nAppendix B. Perturbation in a deterministic system\nA perturbation is often thought of as an act of control. In contrast, it would\nseem that imposing future conditions denies the possibility of modified evolution.\nPut differently, perturbing is an act of free will; future conditions-along with the\ndeterministic context for their imposition-fly in the face of that concept.\nThis is not the place for a discussion of free will, except to mention that contrary\nto the impression of many physicists, some philosophers find justification for free\nwill, not from the supposed indeterminism of quantum mechanics, but from chaos\nin deterministic dynamical systems [17, p. 152].\nBut one need not imagine an independent actor to obtain the \"perturbation\"\nof Sec. 4. Consider the following situation, within the context of the cosmological\nscenario described in [4] or [6]. Two systems, A and B, are small parts of a big\nuniverse, but they are isolated, or nearly so, between the times to be used for the\nboundary value problem. The actual macroscopic boundary values for the two of\nthem are the same. Now imagine that one of them, say B, is not perfectly isolated,\nbut at some intermediate time, t0 , in its history, is struck by something coming\nin from the outside. This \"outside\" is simply another part of the universe, not A\nand not B. Its main properties are its lack of correlation with what is otherwise\nhappening to A and B, and its ability to pack a macroscopic wallop in B. Despite\nthe outside force, I still require the same boundary values for A and B.\nNow compare the macroscopic motions of A and of B. Were it not for the\noutside force, they should be the same. With the force, having changes occur only\non one (temporal) side of the perturbation is what I call macroscopic causality.\n\nReferences\n[1] R. G. Newton, Thinking about Physics (Princeton University Press, Princeton,\n2000).\n[2] T. Gold, The Arrow of Time, Am. J. Phys. 30, 403 (1962).\n[3] L. S. Schulman, Correlating Arrows of Time, Phys. Rev. D 7, 2868 (1973).\n[4] L. S. Schulman, Time's Arrows and Quantum Measurement (Cambridge University Press, Cambridge, 1997).\n14\n\n\f[5] L. S. Schulman, Opposite Thermodynamic Arrows of Time, Phys. Rev. Lett.\n83, 5419 (1999) (cond-mat/9911101).\n[6] L. S. Schulman, A compromised arrow of time, to appear in Proc. of \u00c9quations\naux D\u00e9riv\u00e9es Partielles et Physique Math\u00e9matique, Paris, June 2000, ed. B.\nGaveau et al. (cond-mat/0009139).\n[7] V. I. Arnold and A. Avez, Ergodic Problems of Classical Mechanics (Benjamin,\nNew York, 1968).\n[8] This\nP definition differs slightly from that used in [4], etc. The difference is\n\u2212 v\u03b1 log v\u03b1 . Thus the maximum (old definition) entropy for G equal-volume\ncoarse grains is log G. With the definition here the maximum is zero.\n[9] L. S. Schulman, Accuracy of the semiclassical approximation for the time dependent propagator, J. Phys. A 27, 1703 (1994).\n[10] I will not try to look for minimal values of \u03c4 , since keeping track of this would\nnecessarily include dependence on which sets (\u01ebs) are considered, as well as on\nthe coarse grains. For any given collection of grains and boundary conditions,\nthe mixing property guarantees that a \u03c4 can be found. In simple systems and\nwith the \u2206s and \u01ebs all about the same size, \u03c4 \u223c \u2212 log \u03bc(\u2206).\n[11] L. S. Schulman, Time Reversal for Unstable Particles, Ann. Phys. 72, 489\n(1972).\n[12] H. S. Robertson, Statistical Thermophysics (Prentice-Hall, Englewood Cliffs,\nNew Jersey 1993); M. A. Huerta and H. S. Robertson, Entropy, Information\nTheory, and the Approach to Equilibrium of Coupled Harmonic Oscillator Systems, J. Stat. Phys. 1, 393 (1969); M. A. Huerta and H. S. Robertson, Approach\nto Equilibrium of Coupled Harmonic Oscillator Systems. II, J. Stat. Phys. 3,\n171 (1971).\n[13] In earlier work an arrow was derived from an asymmetric, dissipative perturbation, rather than from proximity to one or another boundary-value-induced\nlow entropy state. See L. S. Schulman and R. Shtokhamer, Thermodynamic\nArrow for a Mixing System, Int. J. Theor. Phys. 16, 287 (1977).\n[14] There is no point in taking t0 in the time interval in which the system has\nequilibrated. Since the perturbation is nondissipative, it will have no impact\nin either direction of time (i.e., the system stays in equilibrium).\n[15] T. M. Cover and J. A. Thomas, Elements of Information Theory (Wiley, New\nYork, 1991).\n[16] B. Gaveau and L. S. Schulman, Master equation based formulation of nonequilibrium statistical mechanics, J. Math. Phys. 37, 3897 (1996).\n[17] D. C. Dennett, Elbow Room: The Varieties of Free Will Worth Wanting (MIT\nPress, Cambridge, Mass., 1984).\n\n15\n\n\f"}