{"id": "http://arxiv.org/abs/1002.1480v1", "guidislink": true, "updated": "2010-02-07T19:58:46Z", "updated_parsed": [2010, 2, 7, 19, 58, 46, 6, 38, 0], "published": "2010-02-07T19:58:46Z", "published_parsed": [2010, 2, 7, 19, 58, 46, 6, 38, 0], "title": "A Minimum Relative Entropy Controller for Undiscounted Markov Decision\n  Processes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.3523%2C1002.2125%2C1002.4493%2C1002.4340%2C1002.0600%2C1002.2252%2C1002.0961%2C1002.1212%2C1002.2486%2C1002.1109%2C1002.3006%2C1002.1033%2C1002.0234%2C1002.0734%2C1002.0830%2C1002.2014%2C1002.2480%2C1002.2577%2C1002.4325%2C1002.1164%2C1002.3302%2C1002.4430%2C1002.0567%2C1002.4392%2C1002.0674%2C1002.3383%2C1002.2588%2C1002.2325%2C1002.4973%2C1002.3376%2C1002.1832%2C1002.2871%2C1002.2876%2C1002.2847%2C1002.4844%2C1002.2042%2C1002.2692%2C1002.3968%2C1002.2705%2C1002.1626%2C1002.1529%2C1002.1106%2C1002.3526%2C1002.0982%2C1002.2053%2C1002.3663%2C1002.0578%2C1002.0144%2C1002.2958%2C1002.1138%2C1002.4758%2C1002.4209%2C1002.1295%2C1002.4917%2C1002.4823%2C1002.1527%2C1002.1480%2C1002.1346%2C1002.4106%2C1002.0994%2C1002.0612%2C1002.0248%2C1002.2354%2C1002.4647%2C1002.3604%2C1002.2733%2C1002.3427%2C1002.2471%2C1002.0868%2C1002.1700%2C1002.1043%2C1002.1021%2C1002.2927%2C1002.4378%2C1002.1254%2C1002.0437%2C1002.1818%2C1002.0902%2C1002.0793%2C1002.3956%2C1002.1300%2C1002.3940%2C1002.0297%2C1002.2579%2C1002.2774%2C1002.1489%2C1002.2788%2C1002.4026%2C1002.4784%2C1002.2512%2C1002.2387%2C1002.3907%2C1002.1967%2C1002.2713%2C1002.0211%2C1002.1914%2C1002.0198%2C1002.4722%2C1002.2022%2C1002.0615%2C1002.3867&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Minimum Relative Entropy Controller for Undiscounted Markov Decision\n  Processes"}, "summary": "Adaptive control problems are notoriously difficult to solve even in the\npresence of plant-specific controllers. One way to by-pass the intractable\ncomputation of the optimal policy is to restate the adaptive control as the\nminimization of the relative entropy of a controller that ignores the true\nplant dynamics from an informed controller. The solution is given by the\nBayesian control rule-a set of equations characterizing a stochastic adaptive\ncontroller for the class of possible plant dynamics. Here, the Bayesian control\nrule is applied to derive BCR-MDP, a controller to solve undiscounted Markov\ndecision processes with finite state and action spaces and unknown dynamics. In\nparticular, we derive a non-parametric conjugate prior distribution over the\npolicy space that encapsulates the agent's whole relevant history and we\npresent a Gibbs sampler to draw random policies from this distribution.\nPreliminary results show that BCR-MDP successfully avoids sub-optimal limit\ncycles due to its built-in mechanism to balance exploration versus\nexploitation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.3523%2C1002.2125%2C1002.4493%2C1002.4340%2C1002.0600%2C1002.2252%2C1002.0961%2C1002.1212%2C1002.2486%2C1002.1109%2C1002.3006%2C1002.1033%2C1002.0234%2C1002.0734%2C1002.0830%2C1002.2014%2C1002.2480%2C1002.2577%2C1002.4325%2C1002.1164%2C1002.3302%2C1002.4430%2C1002.0567%2C1002.4392%2C1002.0674%2C1002.3383%2C1002.2588%2C1002.2325%2C1002.4973%2C1002.3376%2C1002.1832%2C1002.2871%2C1002.2876%2C1002.2847%2C1002.4844%2C1002.2042%2C1002.2692%2C1002.3968%2C1002.2705%2C1002.1626%2C1002.1529%2C1002.1106%2C1002.3526%2C1002.0982%2C1002.2053%2C1002.3663%2C1002.0578%2C1002.0144%2C1002.2958%2C1002.1138%2C1002.4758%2C1002.4209%2C1002.1295%2C1002.4917%2C1002.4823%2C1002.1527%2C1002.1480%2C1002.1346%2C1002.4106%2C1002.0994%2C1002.0612%2C1002.0248%2C1002.2354%2C1002.4647%2C1002.3604%2C1002.2733%2C1002.3427%2C1002.2471%2C1002.0868%2C1002.1700%2C1002.1043%2C1002.1021%2C1002.2927%2C1002.4378%2C1002.1254%2C1002.0437%2C1002.1818%2C1002.0902%2C1002.0793%2C1002.3956%2C1002.1300%2C1002.3940%2C1002.0297%2C1002.2579%2C1002.2774%2C1002.1489%2C1002.2788%2C1002.4026%2C1002.4784%2C1002.2512%2C1002.2387%2C1002.3907%2C1002.1967%2C1002.2713%2C1002.0211%2C1002.1914%2C1002.0198%2C1002.4722%2C1002.2022%2C1002.0615%2C1002.3867&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adaptive control problems are notoriously difficult to solve even in the\npresence of plant-specific controllers. One way to by-pass the intractable\ncomputation of the optimal policy is to restate the adaptive control as the\nminimization of the relative entropy of a controller that ignores the true\nplant dynamics from an informed controller. The solution is given by the\nBayesian control rule-a set of equations characterizing a stochastic adaptive\ncontroller for the class of possible plant dynamics. Here, the Bayesian control\nrule is applied to derive BCR-MDP, a controller to solve undiscounted Markov\ndecision processes with finite state and action spaces and unknown dynamics. In\nparticular, we derive a non-parametric conjugate prior distribution over the\npolicy space that encapsulates the agent's whole relevant history and we\npresent a Gibbs sampler to draw random policies from this distribution.\nPreliminary results show that BCR-MDP successfully avoids sub-optimal limit\ncycles due to its built-in mechanism to balance exploration versus\nexploitation."}, "authors": ["Pedro A. Ortega", "Daniel A. Braun"], "author_detail": {"name": "Daniel A. Braun"}, "author": "Daniel A. Braun", "arxiv_comment": "8 pages, 3 figures, 3 tables", "links": [{"href": "http://arxiv.org/abs/1002.1480v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1002.1480v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.RO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1002.1480v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1002.1480v1", "journal_reference": null, "doi": null, "fulltext": "A Minimum Relative Entropy Controller for Undiscounted Markov\nDecision Processes\n\narXiv:1002.1480v1 [cs.AI] 7 Feb 2010\n\nPedro A. Ortega\nDaniel A. Braun\nDept. of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK\n\nAbstract\nAdaptive control problems are notoriously\ndifficult to solve even in the presence of plantspecific controllers. One way to by-pass the\nintractable computation of the optimal policy is to restate the adaptive control as the\nminimization of the relative entropy of a controller that ignores the true plant dynamics from an informed controller. The solution is given by the Bayesian control rule-\na set of equations characterizing a stochastic adaptive controller for the class of possible plant dynamics. Here, the Bayesian control rule is applied to derive BCR-MDP, a\ncontroller to solve undiscounted Markov decision processes with finite state and action\nspaces and unknown dynamics. In particular, we derive a non-parametric conjugate\nprior distribution over the policy space that\nencapsulates the agent's whole relevant history and we present a Gibbs sampler to draw\nrandom policies from this distribution. Preliminary results show that BCR-MDP successfully avoids sub-optimal limit cycles due\nto its built-in mechanism to balance exploration versus exploitation.\n\n1. Introduction\nAdaptive control problems, i.e. the design of controllers for plants with unknown dynamics, are notoriously difficult. Even when the plant dynamics is\nknown to belong to a particular class for which optimal\ncontrollers are available, constructing the corresponding optimal adaptive controller is in general intractable\n(Duff, 2002). Thus, virtually all of the effort of the re-\n\nCopyright 2010 by the authors.\n\npeortega@dcc.uchile.cl\ndab54@cam.ac.uk\n\nsearch community is centered around the development\nof tractable approximations.\nRecently, new formulations of the adaptive control\nproblem that are based on the minimization of a relative entropy criterion have attracted the interest of\nthe reinforcement learning (RL) community. For example, it has been shown that a large class of optimal control problems can be solved very efficiently if\nthe problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov,\n2006; 2009; Kappen et al., 2009). A similar approach\nminimizes the deviation of the causal input/outputrelationship of a Bayesian mixture of controllers from\nthe true controller, obtaining an explicit solution called\nthe Bayesian control rule (Ortega & Braun, 2010).\nThis control rule is particularly interesting because it\nleads to stochastic controllers that infer the optimal\ncontroller on-line by combining the plant-specific controllers, implicitly using the uncertainty of the dynamics to trade-off exploration versus exploitation.\nMarkov decision processes (MDPs) with undiscounted/averaged rewards constitute an important\nproblem class in RL that has been far less studied\nthan their discounted counterpart. While discounted\nrewards are suitable in many applications, a wide variety of tasks-such as those found in control tasks\nwhere the optimal trajectory is a limit cycle, e.g. network load balancing, automatic assembly, queue management and control of embedded systems-are more\nnaturally stated in terms of optimizing the average\nreward. However, finding an optimal policy for the\naverage reward function is significantly more difficult\nthan the discounted reward. Unlike the discounted\ncase, in undiscounted MDPs the Bellman optimality\nequations are strongly coupled and the effective horizon is unbounded. A systematic study in Mahadevan\n(1996) has shown that exploration plays a crucial role\nin undiscounted MDP algorithms, as insufficient exploration may lead to the convergence to a sub-optimal\n\n\fA Minimum Relative Entropy Controller for Undiscounted Markov Decision Processes\n\nlimit cycle. Several algorithms have been proposed for\nundiscounted MDPs, most notably R-learning and its\nvariants (Schwartz, 1993; Singh, 1994), which are inspired by Watkins' Q-learning (Watkins, 1989) and are\nsimple to implement; and E 3 (Kearns & Singh, 1998)\nand R-max (Brafman & Tennenholtz, 2001), which are\nadvanced algorithms that attain near-optimal average\nreward in polynomial time.\nThe aim of this paper is to demonstrate how the\nBayesian control rule can be used to solve adaptive\ncontrol problems, illustrating its generality and conceptual simplicity. In particular, undiscounted MDPs\nwith finite state and action space and unknown dynamics. We derive an adaptive controller, which we\ncall BCR-MDP, that employs a conjugate prior distribution over the policy space to concisely encapsulate the agent's history and to infer the optimal policy. Furthermore, we introduce a Gibbs sampler implementing the controller.\n\n2. Background\n2.1. Bayesian control rule\nLet O and A be two finite sets of symbols, where\nthe former is the set of inputs (observations) and\nthe second the set of outputs (actions). Actions and\nobservations at time t are denoted as at \u2208 A and\not \u2208 O respectively, and we use the shorthand a\u2264t :=\na1 , a2 , . . . , at and the like to simplify the notation of\nstrings. We assume that the interaction between the\ncontroller and the plant proceeds in cycles t = 1, 2, . . .\nwhere in cycle t the controller issues action at and the\nplant responds with an observation ot . A controller\nis defined as a probability distribution P over the input/output (I/O) stream, and it is fully characterized\nby the conditional probabilities\n\nthe resulting distribution P maximizes a desired utility criterion. We say that P is tailored to Q. In\nmany cases the conditional probabilities P (at |a<t , o<t )\nwill be deterministic, but there are situations (e.g.\nin repeated games) where the designer might prefer\nstochastic policies instead.\nIf the plant is unknown then one faces an adaptive control problem. Assume we know that the plant Q\u03b8 is\ngoing to be drawn randomly from a set Q := {Q\u03b8 }\u03b8\u2208\u0398\nof possible plants indexed by \u0398. Assume further we\nhave available a set of controllers P := {P\u03b8 }\u03b8\u2208\u0398 , where\neach P\u03b8 is tailored to Q\u03b8 . How can we now construct a\ncontroller P such that its behavior is as close as possible to the tailored controller P\u03b8 under any realization\nof Q\u03b8 \u2208 Q?\nA na\u0131\u0308ve approach would be to minimize the relative\nentropy of the controller P with respect to the true\ncontroller P\u03b8 , averaged over all possible values of \u03b8.\nHowever, this is syntactically incorrect. The important observation made in Ortega & Braun (2010) is\nthat we do not want to minimize the deviation of P\nfrom P\u03b8 , but the deviation of the causal I/O dependencies in P from the causal I/O dependencies in P\u03b8 .\nIntuitively speaking, we do not want to predict actions\nand observations, but to predict the observations (effect) given actions (causes). More specifically, they\npropose to minimize a set of (causal) divergences C\ndefined by\n\nC := lim sup\nt\u2192\u221e\n\nC\u03c4 :=\n\nX\n\nrepresenting the probabilities of emitting action at and\ncollecting observation ot given the respective I/O history. Similarly, a plant is defined as a probability distribution Q characterized by the conditional probabilities\nQ(ot |a\u2264t , o<t )\nrepresenting the probabilities of emitting observation\not given the I/O history.\nIf the plant is known, i.e. if the conditional probabilities Q(ot |a\u2264t , o<t ) are known, then the designer can\nbuild a suitable controller by equating the observation streams as P (ot |a\u2264t , o<t ) = Q(ot |a\u2264t , o<t ) and by\ndefining action probabilities P (at |a<t , o<t ) such that\n\n\u03b8\n\nP (\u03b8)\n\nt\nX\n\nC\u03c4\n\n\u03c4 =1\n\nP\u03b8 (\u00e2<\u03c4 , o<\u03c4 )C\u03c4 (\u00e2<\u03c4 , o<\u03c4 )\n\n(1)\n\no<\u03c4\n\nC\u03c4 (h) :=\n\nXX\na\u03c4\n\nP (at |a<t , o<t ) and P (ot |a\u2264t , o<t )\n\nX\n\nP\u03b8 (a\u03c4 , o\u03c4 |h) log\n\no\u03c4\n\nP\u03b8 (a\u03c4 , o\u03c4 |h)\n,\nP (a\u03c4 , o\u03c4 |h)\n\nwhere P (\u03b8) is the prior probability of \u03b8 \u2208 \u0398, \u00e2\u03c4 denotes an intervened (not observed) action at time \u03c4 ,\nand \u00e21 , \u00e22 , \u00e23 , . . . is an arbitrary sequence of intervened\nactions.\nIn Ortega & Braun (2010), it is shown that the controller P that minimizes C in Equation (1) for any\nsequence of intervened actions is given by the conditional probabilities\nP (at |\u00e2<t , o<t ) :=\n\nX\n\nP\u03b8 (at |a<t , o<t )P (\u03b8|\u00e2<t , o<t )\n\n\u03b8\n\nP (ot |\u00e2\u2264t , o<t ) :=\n\nX\n\nP\u03b8 (ot |a\u2264t , o<t )P (\u03b8|\u00e2<t , o<t )\n\n\u03b8\n\n(2)\n\n\fA Minimum Relative Entropy Controller for Undiscounted Markov Decision Processes\n\nwhere\nP\u03b8 (ot |a\u2264t , o<t )P (\u03b8|\u00e2<t , o<t )\n.\n\u2032\n\u03b8 \u2032 P\u03b8 \u2032 (ot |a\u2264t , o<t )P (\u03b8 |\u00e2<t , o<t )\n(3)\nEquations (2) and (3) constitute the Bayesian control\nrule. This result is obtained by using properties of\ninterventions using causal calculus (Pearl, 2000). It is\nworth to point out that the resulting controller is fully\ndefined in terms of its constituent controllers in P. It\nis customary to use the notation\nP (\u03b8|\u00e2\u2264t , o\u2264t ) := P\n\nP (at |\u03b8, a<t , o<t ) := P\u03b8 (at |a<t , o<t )\nP (ot |\u03b8, a\u2264t , o<t ) := P\u03b8 (ot |a\u2264t , o<t ),\nthat is, treating the different controllers as hypotheses of a Bayesian model. The resulting control law\nis in general stochastic. Also, note that by construction, an adaptive code for the I/O stream based on the\nBayesian control rule is optimal for the class of plants\nconsidered (MacKay, 2003).\n2.2. MDPs\nDefinitions. An MDP is defined as a tuple\n(X , A, T, r): X is the state space; A is the action\nspace; Ta (x; x\u2032 ) = Pr(x\u2032 |a, x) is the probability that\nan action a \u2208 A taken in state x \u2208 X will lead to state\nx\u2032 \u2208 X ; and r(x, a) \u2208 R := R is the immediate reward obtained in state x \u2208 X and action a \u2208 A. The\ninteraction proceeds in time steps t = 1, 2, . . . where\nat time t, action at \u2208 A is issued in state xt\u22121 \u2208 X ,\nleading to a reward rt = r(xt\u22121 , at ) and a new state\nxt that starts the next time step t + 1. Hence, starting\nfrom an initial state x0 \u2208 X , an I/O sequence has the\nform\nx0 \u2192 a1 \u2192 (r1 , x1 ) \u2192 a2 \u2192 (r2 , x2 ) \u2192 * * *\n* * * \u2192 at\u22121 \u2192 (rt\u22121 , xt\u22121 ) \u2192 at \u2192 (rt , xt ) \u2192 * * *\nA stationary closed-loop control policy \u03c0 : X \u2192 A\nassigns an action to each state. For MDPs there always exists an optimal stationary deterministic policy and thus one only needs to consider such policies.\nFor undiscounted MDPs, the goal is to find P\na policy\nt\nthat maximizes the time-averaged reward 1t \u03c4 =1 r\u03c4\nas t \u2192 \u221e.\nBellman optimality equations. In undiscounted\nMDPs the average reward per time step for a fixed\npolicy \u03c0 with initial state\nPt x is defined as follows:\n\u03c1\u03c0 (x) = limt\u2192\u221e E\u03c0 [ 1t \u03c4 =0 r\u03c4 ]. It can be shown\n(Bertsekas, 1987) that \u03c1\u03c0 (x) = \u03c1\u03c0 (x\u2032 ) for all x, x\u2032 \u2208 X\nunder the assumption that the Markov chain for policy \u03c0 is ergodic. Here, we assume that the MDPs are\n\nergodic for all stationary policies. Following the Qnotation of Watkins (1989), the optimal policy \u03c0 \u2217 can\nbe characterized in terms of the optimal average reward \u03c1 and the optimal relative Q-values Q(x, a) for\neach state-action pair (x, a) that are solutions to the\nfollowing system of non-linear equations (Singh, 1994):\nfor any state x \u2208 X and action a \u2208 A,\nh\ni\nX\nQ(x, a) + \u03c1 = r(x, a) +\nPr(x\u2032 |x, a) max\nQ(x\u2032 , a\u2032 )\n\u2032\na\n\ny\u2208X\n\nh\ni\n\u2032 \u2032\nQ(x\n,\na\n)\nx,\na\n.\n= r(x, a) + Ex\u2032 max\n\u2032\na\n\n(4)\nFor this setup, the optimal policy is defined as \u03c0 \u2217 (x) :=\narg maxa Q(x, a) for any state x \u2208 X .\n\n3. Derivation of the Controller\nOne can exploit the Bellman optimality equations\nin (4) to define a space of optimal controllers. In\nparticular, any \u03c1 \u2208 R and collection of Q-values\nQ(x, a) \u2208 R where x \u2208 N and a \u2208 A characterize\nan optimal controller. Hence, one can parameterize\nthe space of controllers with a vector \u03b8 \u2208 \u0398 := R\u221e\ncontaining the average reward and all the Q-values.\nTo apply the Bayesian control rule, we need to derive\nprobabilistic models for actions and observations.\nNoting that in cycle t the controller issues an action\nat \u2208 A and receives a reward rt \u2208 R and a state xt \u2208\nX , one can define the space of actions and observations\nfor the Bayesian control rule as A and O := R \u00d7 X\nrespectively.\nLet x = xt\u22121 , a = at , r = rt and x\u2032 = xt . Given the\ncontroller's parameter vector \u03b8, the only additional information needed to apply the optimal policy is given\nby the last state x. Hence, we impose the independence property\nP (at , ot |\u03b8, a<t , o<t ) = P (a, r, x\u2032 |\u03b8, x).\nFurthermore, this can be decomposed as a product of\nthree conditional probabilities:\nP (a, r, x\u2032 |\u03b8, x) = P (a|\u03b8, x)P (x\u2032 |\u03b8, x, a)P (r|\u03b8, x, a, x\u2032 ).\n(5)\nThe first term, i.e. the probability of action a given\n\u03b8, x and a, is given by:\nP (a|\u03b8,x) = P (a|{Q(x, a\u2032 )}a\u2032 \u2208A )\n(\n1 if a = arg maxa\u2032 Q(x, a\u2032 )\n=\n0 else,\n\n(6)\n\nwhich is just the action taken by the optimal policy \u03c0 \u2217\nin state x.\n\n\fA Minimum Relative Entropy Controller for Undiscounted Markov Decision Processes\n\nFor the second term in (5), i.e. the state transition\nprobabilities given the past interactions, we observe\nthat the average reward \u03c1 and the Q-values Q(x, a)\nencoded in \u03b8 do not provide enough information to\nencode the transition probabilities. Thus we conclude\nthat they are independent of the parameter, that is:\nfor any \u03b8, \u03b8\u2032 \u2208 \u0398,\n\nP (x\u2032 |\u03b8, x, a) = P (x\u2032 |\u03b8\u2032 , x, a). (7)\n\nFinally we derive P (r|\u03b8, x, a, x\u2032 ), i.e. the probabilities\nof rewards given the past interactions and the next\nstate x. Note that the reward function r(x, a) is not\nparameterized by \u03b8, thus we cannot know the exact\nvalue of r from (\u03b8, x, a, x\u2032 ).\nLet \u03be(x, a, x\u2032 ) be the mean instantaneous reward defined by\n\u03be(x, a, x\u2032 ) := Q(x, a) + \u03c1 \u2212 max\nQ(x\u2032 , a\u2032 ).\n\u2032\na\n\n(8)\n\nThis quantity represents the mean of the instantaneous reward r(x, a) as estimated indirectly using the\npre- and post-action Q-values. Indeed, it is seen from\nEquation (4) that\nr(x, a) = \u03be(x, a, x\u2032 ) + \u03bd,\n\n(9)\n\nwhere we have replaced the sum by an integration\nover \u0398\u0303, the finite-dimensional real space containing\nonly the average reward and the Q-values of the observed states, and where we have simplified the term\nP (x\u2032 |\u03b8, x, a) because it is constant for all \u03b8\u2032 \u2208 \u0398\u0303.\nBy inspection of Equation (11), one sees that \u03b8 encodes a set of independent normal distributions over\nthe immediate reward having means \u03be(x, a, x\u2032 ) indexed\nby triples (x, a, x\u2032 ) \u2208 X \u00d7 A \u00d7 X . In other words,\ngiven (x, a, x\u2032 ), the rewards are drawn from a normal distribution with unknown mean \u03be(x, a, x\u2032 ) and\nknown variance \u03c3 2 . The sufficient statistics are given\nby n(x, a, x\u2032 ), the number of times that the transition\nx \u2192 x\u2032 under action a, and r\u0304(x, a, x\u2032 ), the mean of the\nrewards obtained in the same transition. The conjugate prior distribution is well known and given by a\nnormal distribution with hyperparameters \u03bc0 and \u03bb0 :\nP (\u03be(x, a, x\u2032 )) = N (\u03bc0 , 1/\u03bb0 )\nr\nn\n\u00012 o\n\u03bb0\nexp \u2212 \u03bb20 \u03be(x, a, x\u2032 ) \u2212 \u03bc0\n=\n. (12)\n2\u03c0\n\nThe posterior distribution is given by\n\nP (\u03be(x, a, x\u2032 )|\u00e2\u2264t , o\u2264t ) = N (\u03bc(x, a, x\u2032 ), 1/\u03bb(x, a, x\u2032 ))\n\nwhere\n\nwhere the posterior hyperparameters are computed as\n\n\u03bd := max\nQ(x\u2032 , a\u2032 ) \u2212 E[max\nQ(x\u2032 , a\u2032 )|x, a].\n\u2032\n\u2032\na\n\na\n\nHere, the term \u03bd is a deviation from r(x, a) that can\nbe interpreted as random observation noise. Assuming\nthat \u03bd can be reasonably approximated by a normal\ndistribution N (0, 1/p) with precision p, then we can\nwrite down a likelihood model for the immediate reward r using the Q-values and the average reward, i.e.\nr\no\nn p\np\n\u2032\nexp \u2212 (r \u2212 \u03be(x, a, x\u2032 ))2 .\nP (r|\u03b8, x, a, x ) =\n2\u03c0\n2\n(10)\nThis completes our model of the controller with parameter vector \u03b8.\nTo apply the Bayesian control rule over the controllers in \u0398, the intervened posterior distribution\nP (\u03b8|\u00e2\u2264t , o\u2264t ) defined in Equation (3) needs to be computed. Fortunately, due to the simplicity of the likelihood model, one can easily devise a conjugate prior\ndistribution.\nInserting the likelihood into Equation (3), one obtains\nP (\u03b8|\u00e2\u2264t , o\u2264t )\nP (x\u2032 |\u03b8, x, a)P (r|\u03b8, x, a, x\u2032 )P (\u03b8|\u00e2<t , o<t )\nP (x\u2032 |\u03b8\u2032 , x, a)P (r|\u03b8\u2032 , x, a, x\u2032 )P (\u03b8\u2032 |\u00e2<t , o<t ) d\u03b8\u2032\n\u0398\u0303\n\n=R\n\nP (r|\u03b8, x, a, x\u2032 )P (\u03b8|\u00e2<t , o<t )\n,\nP (r|\u03b8\u2032 , x, a, x\u2032 )P (\u03b8\u2032 |\u00e2<t , o<t ) d\u03b8\u2032\n\u0398\u0303\n\n=R\n\n(11)\n\n\u03bb0 \u03bc0 + p n(x, a, x\u2032 ) r\u0304(x, a, x\u2032 )\n\u03bb0 + p n(x, a, x\u2032 )\n\u03bb(x, a, x\u2032 ) = \u03bb0 + p n(x, a, x\u2032 ).\n\n\u03bc(x, a, x\u2032 ) =\n\n(13)\n\nFinally, the conjugate distribution of the parameter\nvector \u03b8 is simply the product\nP (\u03b8|\u00e2\u2264t , o\u2264t ) =\n\nY\n\nP (\u03be(x, a, x\u2032 )|\u00e2\u2264t , o\u2264t )\n\nx,a,x\u2032\n\nn 1 X\n\u00012 o\n\u221d exp \u2212\n\u03bb(x, a, x\u2032 ) \u03be(x, a, x\u2032 )\u2212\u03bc(x, a, x\u2032 )\n2\n\u2032\nx,a,x\n\n(14)\n\nbecause the \u03be(x, a, x\u2032 ) are independent but at the same\ntime functions of \u03b8 (Equation 8). Thus, the BCR-MDP\ncontroller is fully specified by the actions probabilities\nin Equation (6), the likelihood models in Equations (7)\nand (10), and the prior distribution (12).\n\n4. Inference and Acting\nInference can be carried out by sampling \u03b8 from the\nposterior distribution in Equation (14). The actions\nissued by BCR-MDP are by-products of the inference\nprocess. Here we derive an approximate Gibbs sampler\nfor \u03b8. We introduce the following symbols: \u03b8\u2212\u03c1 and\n\n\fA Minimum Relative Entropy Controller for Undiscounted Markov Decision Processes\n\n\u03b8\u2212Q(x,a) stand for the parameter set removing \u03c1 and\nQ(x, a) respectively; \u03bc and \u03bb are matrices collecting\nthe values of the posterior hyperparameters \u03bc(x, a, x\u2032 )\nand \u03bb(x, a, x\u2032 ) respectively; and M (x) := maxa Q(x, a)\nis a shorthand.\nSubstituting \u03be(x, a, x\u2032 ) in Equation (14) by its definition (Equation 8) and conditioning on the Q-values,\nwe obtain the conditional distribution of \u03c1:\nP (\u03c1|\u03b8\n\n\u2212\u03c1\n\n, \u03bc, \u03bb) = N (\u03c1\u0304, 1/S)\n\n(15)\n\nAlgorithm 1 BCR-MDP Gibbs sampler.\nInitialize entries of \u03b8, \u03bb and \u03bc to zero.\nSet initial state to x \u2190 x0 .\nfor t = 1, 2, 3, . . . do\n{ Interaction }\nSet a \u2190 arg maxa\u2032 Q(x, a\u2032 ) and issue a.\nObtain o = (r, x\u2032 ) from plant.\n{Update hyperparameters}\n\u2032\n)\u03bc(x,a,x\u2032 )+p r\n\u03bc(x, a, x\u2032 ) \u2190 \u03bb(x,a,x\n\u03bb(x,a,x\u2032 )+p\n\u03bb(x, a, x\u2032 ) \u2190 \u03bb(x, a, x\u2032 ) + p\n\nwhere\n\u03c1\u0304 =\n\n1 X\n\u03bb(x, a, x\u2032 )(\u03bc(x, a, x\u2032 ) \u2212 Q(x, a) + M (x\u2032 )),\nS\nx,a,x\u2032\nX\nS=\n\u03bb(x, a, x\u2032 ).\nx,a,x\u2032\n\nThe conditional distribution over the Q-values is more\ndifficult to obtain, because each Q(x, a) enters the\nposterior distribution both linearly and non-linearly\nthrough \u03bc. However, if we fix Q(x, a) within the max\noperations, which amounts to treating each M (x) as\na constant within a single Gibbs step, then the conditional distribution can be approximated by\n\u0010\n\u0011\nP (Q(x, a)|\u03b8\u2212Q(x,a) , \u03bb, \u03bc) \u2248 N Q\u0304(x, a), 1/S(x, a)\n\n{Gibbs sweep}\nSample \u03c1 using (15).\nfor all Q(y, b) of visited states do\nSample Q(y, b) using (16).\nend for\nSet x \u2192 x\u2032 .\nend for\n\n5. Preliminary Empirical Results\n\nWe have tested BCR-MDP in two toy examples: a\ngrid-world domain, and on a suite of randomly generated MDPs. To give an intuition of the achieved\nperformance, the results are contrasted with those\nachieved by R-learning. We have used the R-learning\nvariant presented in Singh (1994, Algorithm 3) to(16)\ngether with the uncertainty exploration strategy (Mawhere\nhadevan, 1996). The corresponding update equations\nare\nX\n1\n\u03bb(x, a, x\u2032 )(\u03bc(x, a, x\u2032 ) \u2212 \u03c1 + M (x\u2032 )),\nQ\u0304(x, a) =\n\u0001\nS(x, a) \u2032\nQ(x, a) \u2190 (1 \u2212 \u03b1)Q(x, a) + \u03b1 r \u2212 \u03c1 + max\nQ(x\u2032 , a\u2032 )\nx\n\u2032\na\nX\n\u0001\n\u2032 \u2032\nS(x, a) =\n\u03bb(x, a, x\u2032 ).\n\u03c1 \u2190 (1 \u2212 \u03b2)\u03c1 + \u03b2 r + max\nQ(x\n,\na\n) \u2212 Q(x, a) ,\n\u2032\nx\u2032\n\nWe expect this approximation to hold because the resulting update rule constitutes a contraction operation\nthat forms the basis of most stochastic approximation\nalgorithms (Mahadevan, 1996). As a result, the Gibbs\nsampler draws all the values from normal distributions.\nIn each cycle of the adaptive controller, one can carry\nout several Gibbs sweeps to obtain a sample of \u03b8 to improve the mixing of the Markov chain. However, our\nexperimental results have shown that a single Gibbs\nsweep per state transition performs reasonably well.\nOnce a new parameter vector \u03b8 is drawn, BCR-MDP\nproceeds by taking the optimal action given by Equation (6). The resulting algorithm is listed in Algorithm 4. Note that only the \u03bc and \u03bb entries of the\ntransitions that have occurred need to be represented\nexplicitly; similarly, only the Q-values of visited states\nneed to be represented explicitly.\n\na\n\n(17)\nwhere \u03b1, \u03b2 > 0 are learning rates. The exploration\nstrategy chooses with fixed probability pexp > 0 the\nC\n, where C is a\naction a that maximizes Q(x, a) + F (x,a)\nconstant, and F (x, a) represents the number of times\nthat action a has been tried in state x. Thus, higher\nvalues of C enforce increased exploration.\nGrid-world domain. In Mahadevan (1996), a gridworld is described that is especially useful as a test bed\nfor the analysis of RL algorithms. For our purposes,\nit is of particular interest because it is easy to design\nexperiments containing suboptimal limit-cycles.\nFigure 1, panel (a), illustrates the 7 \u00d7 7 grid-world.\nA controller has to learn a policy that leads it from\nany initial location to the goal state. At each step,\nthe agent can move to any adjacent space (up, down,\nleft or right). If the agent reaches the goal state then\n\n\fA Minimum Relative Entropy Controller for Undiscounted Markov Decision Processes\n\ngoal\n\nmembranes\n\nb) BCR-MDP\n\nc) R-learning, C=5\n\ninitial 5,000 steps\n\na) 7x7 Maze\n\nd) R-learning, C=30\n\ne) Average Reward\n0.4\n0.3\nR-learning, C=30\n0.2\n\nPSfrag replacements\na)\n\nlow\nprobability\n\nR-learning, C=5\nlast 5,000 steps\n\nhigh\nprobability\n\n0.1\nBCR-MDP\n0.0\n0\n\n125\n\n250\n\n375\n\n500\n\nx1000 time steps\n\nFigure 1. Results for the 7\u00d77 grid-world domain. Panel (a) illustrates the setup. Columns (b)-(d) illustrate the behavioral\nstatistics of the algorithms. The upper and lower row have been calculated over the first and last 5,000 time steps of\nrandomly chosen runs. The probability of being in a state is color-encoded, and the arrows represent the most frequent\nactions taken by the agents. Panel (e) presents the curves obtained by averaging ten runs.\n\nits next position is randomly set to any square of the\ngrid (with uniform probability) to start another trial.\nThere are also \"one-way membranes\" that allow the\nagent to move into one direction but not into the other.\nIn these experiments, these membranes form \"inverted\ncups\" that the agent can enter from any side but can\nonly leave through the bottom, playing the role of a\nlocal maximum. Transitions are stochastic: the agent\n9\nmoves to the correct square with probability p = 10\nand to any of the free adjacent spaces (uniform dis1\n. Rewards are\ntribution) with probability 1 \u2212 p = 10\nassigned as follows. The default reward is r = 0. If\nthe agent traverses a membrane it obtains a reward of\nr = 1. Reaching the goal state assigns r = 2.5.\nThe parameters chosen for this simulation were the\nfollowing. For BCR-MDP, we have chosen hyperparameters \u03bc0 = 1 and \u03bb0 = 1 and precision p = 1. For\nR-learning, we have chosen learning rates \u03b1 = 0.5 and\n\u03b2 = 0.001, and the exploration constant has been set\nto C = 5 and to C = 30.\nA total of 10 runs were carried out for each algorithm.\nThe results are presented in Figure 1 and Table 1. Rlearning only learns the optimal policy given sufficient\nexploration (panels c & d, bottom row), whereas BCRMDP learns the policy successfully. In Figure 1e, the\nlearning curve of R-learning is initially steeper than\nthe Bayesian controller. However, the latter attains a\nhigher average reward around time step 125,000 onwards. We attribute this shallow initial transient to\nthe phase where the distribution over the operation\nmodes is flat, which is also reflected by the initially\nrandom exploratory behavior.\nTo test wether the performance of BCR-MDP scales\nup with a larger problem, we have conducted a sec-\n\nTable 1. Average reward attained by the different algorithms at the end of the run. The mean and the standard\ndeviation has been calculated based on 10 runs.\nAverage Reward\nBCR-MDP\nR-learning, C = 30\nR-learning, C = 5\n\n0.3582 \u00b1 0.0038\n0.3056 \u00b1 0.0063\n0.2049 \u00b1 0.0012\n\nond grid-world experiment with where the number of\nstates has roughly been doubled. The results for this\n10\u00d710 maze are illustrated in Figure 2. The reward for\nreaching the goal state has been set to r = 10 in this\ncase. The precision for this experiment has been set to\np = 1/3 to reflect higher uncertainty. This is still very\nlow given that the range of possible rewards is [0; 10].\nWe have simulated one run of one million time steps.\nAgain, one can see that the algorithm moves from a\nhighly exploratory phase to an exploitative phase (Figure 2, left panels), eventually converging towards the\noptimal policy. The learning curve shows a steady\nincrease in performance (Figure 2, right panel). Interestingly, around time step 300,000 the curve shows an\nabrupt change in slope. Presumably this is due to a\nchange of the belief state: the algorithm was exploiting\none of the two suboptimal limit cycles when it discovered the optimal limit cycle. This confirms our intuition, because the inference process cannot converge as\nlong as there is still uncertainty over the policy space.\nRandomly generated MDPs. The purpose of this\nexperiment is to test the robustness of the algorithm\nunder different environments. In this second test bed,\nrandom ergodic MDPs have been generated: a) with\n\n\fA Minimum Relative Entropy Controller for Undiscounted Markov Decision Processes\ninitial 1000 time steps\n\nlast 1000 time steps\n\nAverage Reward\n0.7\n\nhigh\nprobability\n\n0.6\n0.4\n0.2\n\nlow\nprobability\n0.0\n\nPSfrag replacements\na)\n\n0\n\n250\n\n500\n\n750\n\n1000\n\nx1000 time steps\n\nFigure 2. 10x10 maze task.\nAverage Reward\n(10-state 5-action MDPs)\n\n0.75\n0.70\n\n0.70\n\n0.65\n\n0.65\n\nBCR-MDP\n\nPolicy iteration\n\n0.60\n\nPSfrag replacements\na)\n\n0.55\n\nR-learning, C=20\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\nBCR-MDP\n\nPolicy iteration\n\n0.60\n\n0.55\n0.50\n\nAverage Reward\n(20-state 5-action MDPs)\n\n0.75\n\n60\n\nx1000 time steps\n\n0.50\n\nR-learning, C=20\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\nx1000 time steps\n\nFigure 3. Comparison of the average reward for BCR-MDP, policy iteration and R-learning. A total of 60 different MDPs\nwhere randomly generated, one half having 10 states and 5 actions (left panel), and the other half having 20 states and\n5 actions (right panel). The three algorithms where tested on these MDPs and their learning curves averaged.\n\nTable 2. Average reward attained by the different algorithms at the end of the run. The mean and the standard\ndeviation has been calculated based on 30 runs.\nAverage Reward :\n\n10-state, 5-actions\n\n20-state, 5-actions\n\nPolicy iteration\nBCR-MDP\nR-learning, C = 20\n\n0.7114 \u00b1 0.0207\n0.6998 \u00b1 0.0211\n0.6061 \u00b1 0.0216\n\n0.6906 \u00b1 0.0101\n0.6743 \u00b1 0.0102\n0.5677 \u00b1 0.0104\n\n10 states and 5 actions and b) with 20 states and 5 actions. The transition and payoff matrices have been\nconstructed randomly: all transitions had non-zero\nprobabilities and all rewards took on values in [0; 1]. In\neach run, a new MDP is generated and the three agents\nare tested on it: BCR-MDP with precision p = 1; Rlearning with \u03b1 = 0.5, \u03b2 = 0.001 and C = 20; and\npolicy iteration. The latter has been used to estimate\nthe maximum performance, i.e. the performance of an\ninformed agent. We have simulated a total of 30 runs\nwith 60,000 time steps for both cases and averaged the\ncurves. The results, presented in Figure 3 and Table 2,\nshow that BCR-MDP quickly approximates the optimal average reward, in both cases significantly faster\nthan R-learning.\n\n6. Summary and Conclusion\nThe reformulation of the adaptive control problem\nas the minimization of the relative entropy over the\ncausal dependencies stated in Equation (1) leads to\nan explicit solution given by the Bayesian control rule.\nThis rule constitutes a general method to construct\nadaptive controllers from plant-specific controllers. Its\nmain advantage is that it allows replacing the intractable calculation of the optimal policy for the class\nof plants by an on-line inference procedure, where actions are simply by-products of the inference process.\nConceptually, the Bayesian control rule instantiates\nseveral well-known ideas: the action selection strategy is a probability matching method (Wyatt, 1997);\nmixing task-optimal controllers is a mixture of experts\ntechnique (Jacobs et al., 1991); and minimizing the\nrelative entropy to design a controller is equivalent\nto maximizing the compression of the controller's I/O\nstream (MacKay, 2003).\nTo illustrate the potential of the Bayesian control rule,\nwe have derived BCR-MDP, an adaptive controller to\nsolve undiscounted MDPs with finite state and action spaces and unknown dynamics. BCR-MDP is\nvery simple to understand and to implement using the\nGibbs sampler proposed in Section 4. Empirical results show that the built-in exploration-exploitation\nstrategy avoids getting trapped in local minima.\n\n\fA Minimum Relative Entropy Controller for Undiscounted Markov Decision Processes\n\nUsing Bayesian techniques in RL has a long history\nand has shown to be useful because they provide a systematic way of incorporating prior knowledge and domain assumptions into the problem and updating them\nas more data are observed. This allows quantifying the\nuncertainty of the quantity of interest, e.g. the value\nfunction, action-value function, etc. The idea of restating RL as an inference problem has also been proposed in Toussaint et al. (2006). This approach uses\nthe expectation-maximization (EM) algorithm to infer\nthe optimal policy, and special pruning techniques to\nreduce the computational complexity. It is interesting\nto point out that in the case of undiscounted MDPs,\nBayesian Q-learning (Dearden et al., 1998) resembles\nclosely BCR-MDP. Our contribution is to show that\nsuch an algorithm can be derived from a more general relative entropy minimization principle, including\nsome features like the implicit exploration-exploitation\ntrade-off.\nWe expect similar simplifications to hold for the design of adaptive controllers for other classes of plant\ndynamics. In particular, potential applications of the\nBayesian control rule include extensions to continuous state and action spaces and to partially observable\nMarkov processes.\n\nMacKay, D.J.C. Information Theory, Inference, and\nLearning Algorithms. Cambridge University Press, 2003.\nMahadevan, S. Average reward reinforcement learning:\nFoundations, algorithms, and empirical results. Machine\nLearning, 22(1-3):159\u2013195, 1996.\nOrtega, P.A. and Braun, D.A. A bayesian rule for adaptive\ncontrol based on causal interventions. In Proceedings\nof the third conference on general artificial intelligence,\n2010.\nPearl, J. Causality: Models, Reasoning, and Inference.\nCambridge University Press, Cambridge, UK, 2000.\nSchwartz, A. A reinforcement learning method for maximizing undiscounted rewards. In Proceedings of the\nTenth International Conference on Machine Learning,\npp. 298\u2013305. Morgan Kaufmann, 1993.\nSingh, S. P. Reinforcement learning algorithms for averagepayoff markovian decision processes. In National Conference on Artificial Intelligence, pp. 700\u2013705, 1994.\nTodorov, E. Linearly solvable markov decision problems.\nIn Advances in Neural Information Processing Systems,\nvolume 19, pp. 1369\u20131376, 2006.\nTodorov, E. Efficient computation of optimal actions. Proceedings of the National Academy of Sciences U.S.A.,\n106:11478\u201311483, 2009.\nToussaint, M., Harmeling, S., and Storkey, A. Probabilistic\ninference for solving (po)mdps, 2006.\n\nReferences\nBertsekas, D. Dynamic Programming: Deterministic and\nStochastic Models. Prentice-Hall, Upper Saddle River,\nNJ, 1987.\nBrafman, R.I. and Tennenholtz, M. R-MAX - a general\npolynomial time algorithm for near-optimal reinforcement learning. In IJCAI, pp. 953\u2013958, 2001.\nDearden, R., Friedman, N., and Russell, S. Bayesian qlearning. In AAAI '98/IAAI '98: Proceedings of the\nfifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence,\npp. 761\u2013768, Menlo Park, CA, US, 1998. American Association for Artificial Intelligence. ISBN 0-262-51098-7.\nDuff, M.O. Optimal learning: computational procedures for\nbayes-adaptive markov decision processes. PhD thesis,\n2002. Director-Andrew Barto.\nJacobs, R.A., Jordan, M.I., Nowlan, S.J., and Hinton, G.E.\nAdaptive mixtures of local experts. Neural Computation,\n3:79\u201387, 1991.\nKappen, B., Gomez, V., and Opper, M. Optimal control as\na graphical model inference problem. arXiv:0901.0633,\n2009.\nKearns, M. and Singh, S. Near-optimal reinforcement\nlearning in polynomial time. In Proc. 15th International\nConf. on Machine Learning, pp. 260\u2013268. Morgan Kaufmann, San Francisco, CA, 1998.\n\nWatkins, C. Learning from Delayed Rewards. PhD thesis,\nUniversity of Cambridge, Cambridge, England, 1989.\nWyatt, J. Exploration and Inference in Learning from Reinforcement. PhD thesis, Department of Artificial Intelligence, University of Edinburgh, 1997.\n\n\f"}