{"id": "http://arxiv.org/abs/0801.2600v1", "guidislink": true, "updated": "2008-01-17T02:12:17Z", "updated_parsed": [2008, 1, 17, 2, 12, 17, 3, 17, 0], "published": "2008-01-17T02:12:17Z", "published_parsed": [2008, 1, 17, 2, 12, 17, 3, 17, 0], "title": "Some thoughts on the asymptotics of the deconvolution kernel density\n  estimator", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0801.0351%2C0801.3111%2C0801.0614%2C0801.2451%2C0801.1794%2C0801.4627%2C0801.1605%2C0801.0182%2C0801.1744%2C0801.3312%2C0801.2676%2C0801.2435%2C0801.3671%2C0801.2498%2C0801.4939%2C0801.1788%2C0801.0286%2C0801.4155%2C0801.3077%2C0801.3517%2C0801.0674%2C0801.4353%2C0801.1137%2C0801.3055%2C0801.2136%2C0801.1963%2C0801.4232%2C0801.3956%2C0801.2894%2C0801.4369%2C0801.4125%2C0801.2071%2C0801.2522%2C0801.1505%2C0801.0735%2C0801.2661%2C0801.1368%2C0801.3074%2C0801.4084%2C0801.2701%2C0801.3536%2C0801.3653%2C0801.4661%2C0801.1793%2C0801.0190%2C0801.1475%2C0801.3533%2C0801.0380%2C0801.1396%2C0801.2600%2C0801.1287%2C0801.2733%2C0801.0244%2C0801.0342%2C0801.1162%2C0801.0340%2C0801.1947%2C0801.4277%2C0801.2164%2C0801.3518%2C0801.0277%2C0801.1204%2C0801.2820%2C0801.2144%2C0801.4442%2C0801.3615%2C0801.3351%2C0801.4168%2C0801.2239%2C0801.3104%2C0801.0645%2C0801.2161%2C0801.2776%2C0801.4867%2C0801.2246%2C0801.3932%2C0801.1256%2C0801.1147%2C0801.2364%2C0801.2632%2C0801.0754%2C0801.3744%2C0801.1849%2C0801.3059%2C0801.1164%2C0801.3086%2C0801.2519%2C0801.2628%2C0801.2317%2C0801.0915%2C0801.3381%2C0801.1696%2C0801.4887%2C0801.1796%2C0801.1470%2C0801.4549%2C0801.2971%2C0801.2497%2C0801.4223%2C0801.3866%2C0801.2790&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Some thoughts on the asymptotics of the deconvolution kernel density\n  estimator"}, "summary": "Via a simulation study we compare the finite sample performance of the\ndeconvolution kernel density estimator in the supersmooth deconvolution problem\nto its asymptotic behaviour predicted by two asymptotic normality theorems. Our\nresults indicate that for lower noise levels and moderate sample sizes the\nmatch between the asymptotic theory and the finite sample performance of the\nestimator is not satisfactory. On the other hand we show that the two\napproaches produce reasonably close results for higher noise levels. These\nobservations in turn provide additional motivation for the study of\ndeconvolution problems under the assumption that the error term variance\n$\\sigma^2\\to 0$ as the sample size $n\\to\\infty.$", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0801.0351%2C0801.3111%2C0801.0614%2C0801.2451%2C0801.1794%2C0801.4627%2C0801.1605%2C0801.0182%2C0801.1744%2C0801.3312%2C0801.2676%2C0801.2435%2C0801.3671%2C0801.2498%2C0801.4939%2C0801.1788%2C0801.0286%2C0801.4155%2C0801.3077%2C0801.3517%2C0801.0674%2C0801.4353%2C0801.1137%2C0801.3055%2C0801.2136%2C0801.1963%2C0801.4232%2C0801.3956%2C0801.2894%2C0801.4369%2C0801.4125%2C0801.2071%2C0801.2522%2C0801.1505%2C0801.0735%2C0801.2661%2C0801.1368%2C0801.3074%2C0801.4084%2C0801.2701%2C0801.3536%2C0801.3653%2C0801.4661%2C0801.1793%2C0801.0190%2C0801.1475%2C0801.3533%2C0801.0380%2C0801.1396%2C0801.2600%2C0801.1287%2C0801.2733%2C0801.0244%2C0801.0342%2C0801.1162%2C0801.0340%2C0801.1947%2C0801.4277%2C0801.2164%2C0801.3518%2C0801.0277%2C0801.1204%2C0801.2820%2C0801.2144%2C0801.4442%2C0801.3615%2C0801.3351%2C0801.4168%2C0801.2239%2C0801.3104%2C0801.0645%2C0801.2161%2C0801.2776%2C0801.4867%2C0801.2246%2C0801.3932%2C0801.1256%2C0801.1147%2C0801.2364%2C0801.2632%2C0801.0754%2C0801.3744%2C0801.1849%2C0801.3059%2C0801.1164%2C0801.3086%2C0801.2519%2C0801.2628%2C0801.2317%2C0801.0915%2C0801.3381%2C0801.1696%2C0801.4887%2C0801.1796%2C0801.1470%2C0801.4549%2C0801.2971%2C0801.2497%2C0801.4223%2C0801.3866%2C0801.2790&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Via a simulation study we compare the finite sample performance of the\ndeconvolution kernel density estimator in the supersmooth deconvolution problem\nto its asymptotic behaviour predicted by two asymptotic normality theorems. Our\nresults indicate that for lower noise levels and moderate sample sizes the\nmatch between the asymptotic theory and the finite sample performance of the\nestimator is not satisfactory. On the other hand we show that the two\napproaches produce reasonably close results for higher noise levels. These\nobservations in turn provide additional motivation for the study of\ndeconvolution problems under the assumption that the error term variance\n$\\sigma^2\\to 0$ as the sample size $n\\to\\infty.$"}, "authors": ["Bert van Es", "Shota Gugushvili"], "author_detail": {"name": "Shota Gugushvili"}, "author": "Shota Gugushvili", "arxiv_comment": "18 pages, 8 figures, 6 tables", "links": [{"href": "http://arxiv.org/abs/0801.2600v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0801.2600v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62G07", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0801.2600v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0801.2600v1", "journal_reference": null, "doi": null, "fulltext": "Some thoughts on the asymptotics of the\ndeconvolution kernel density estimator\n\narXiv:0801.2600v1 [stat.ME] 17 Jan 2008\n\nBert van Es\nKorteweg-de Vries Instituut voor Wiskunde\nUniversiteit van Amsterdam\nPlantage Muidergracht 24\n1018 TV Amsterdam\nThe Netherlands\nvanes@science.uva.nl\nShota Gugushvili\u2217\nEurandom\nTechnische Universiteit Eindhoven\nP.O. Box 513\n5600 MB Eindhoven\nThe Netherlands\ngugushvili@eurandom.tue.nl\n\nNovember 12, 2018\n\nAbstract\nVia a simulation study we compare the finite sample performance of\nthe deconvolution kernel density estimator in the supersmooth deconvolution problem to its asymptotic behaviour predicted by two asymptotic normality theorems. Our results indicate that for lower noise\nlevels and moderate sample sizes the match between the asymptotic\ntheory and the finite sample performance of the estimator is not satisfactory. On the other hand we show that the two approaches produce\nreasonably close results for higher noise levels. These observations in\nturn provide additional motivation for the study of deconvolution problems under the assumption that the error term variance \u03c3 2 \u2192 0 as the\nsample size n \u2192 \u221e.\nKeywords: finite sample behavior, asymptotic normality, deconvolution kernel density estimator, Fast Fourier Transform.\n\u2217\nThe research of this author was financially supported by the Nederlandse Organisatie\nvoor Wetenschappelijk Onderzoek (NWO). Part of the work was done while this author\nwas at the Korteweg-de Vries Instituut voor Wiskunde in Amsterdam.\n\n1\n\n\fAMS subject classification: 62G07\n\n2\n\n\f1\n\nIntroduction\n\nLet X1 , . . . , Xn be i.i.d. observations, where Xi = Yi + Zi and the Y 's and\nZ's are independent. Assume that the Y 's are unobservable and that they\nhave the density f and also that the Z's have a known density k. The\ndeconvolution problem consists in estimation of the density f based on the\nsample X1 , . . . , Xn .\nA popular estimator of f is the deconvolution kernel density estimator,\nwhich is constructed via Fourier inversion and kernel smoothing. Let w be\na kernel function and h > 0 a bandwidth. The kernel deconvolution density\nestimator fnh is defined as\n\u0012\n\u0013\nZ \u221e\nn\nx \u2212 Xj\n\u03c6w (ht)\u03c6emp (t)\n1 X\n1\ndt =\nwh\ne\u2212itx\n,\n(1)\nfnh (x) =\n2\u03c0 \u2212\u221e\n\u03c6k (t)\nnh\nh\nj=1\n\nwhere \u03c6emp denotes the empirical characteristic function of the sample, i.e.\nn\n\n\u03c6emp (t) =\n\n1 X itXj\ne\n,\nn\nj=1\n\n\u03c6w and \u03c6k are Fourier transforms of the functions w and k, respectively, and\nZ \u221e\n\u03c6w (t)\n1\ndt.\ne\u2212itx\nwh (x) =\n2\u03c0 \u2212\u221e\n\u03c6k (t/h)\nThe estimator (1) was proposed in Carroll and Hall (1988) and Stefanski and Carroll\n(1990) and there is a vast amount of literature dedicated to it (for additional\nbibliographic information see e.g. van Es and Uh (2004) and van Es and Uh\n(2005)).\nDepending on the rate of decay of the characteristic function \u03c6k at plus\nand minus infinity, deconvolution problems are usually divided into two\ngroups, ordinary smooth deconvolution problems and supersmooth deconvolution problems. In the first case it is assumed that \u03c6k decays algebraically\nand in the second case the decay is essentially exponential. This rate of\ndecay, and consequently the smoothness of the density k, has a decisive influence on the performance of (1). The general picture that one sees is that\nsmoother k is, the harder the estimation of f becomes, see e.g. Fan (1991a).\nAsymptotic normality of (1) in the ordinary smooth case was established\nin Fan (1991b), see also Fan and Liu (1997). The limit behaviour in this\ncase is essentially the same as that of a kernel estimator of a higher order\nderivative of a density. This is obvious in certain relatively simple cases\nwhere the estimator is actually equal to the sum of derivatives of a kernel\ndensity estimator, cf. van Es and Kok (1998).\nOur main interest, however, lies in asymptotic normality of (1) in the\nsupersmooth case. In this case under certain conditions on the kernel w and\nthe unknown density f, the following theorem was proved in Fan (1992).\n3\n\n\fTheorem 1.1. Let fnh be defined by (1). Then\n\u221a\nn\nD\n(fnh (x) \u2212 E [fnh (x)]) \u2192 N (0, 1)\n(2)\nsn\nP\n2 , or s2 is the sample variance\nas n \u2192 \u221e. Here either s2n = (1/n) nj=1 Znj\nn\nof Zn1 , . . . , Znn with Znj = (1/h)wh ((x \u2212 Xj )/h).\nThe asymptotic variance of fnh itself does not follow from this result. On\nthe other hand van Es and Uh (2004), see also van Es and Uh (2005), derived a central limit theorem for (1) where the normalisation is deterministic\nand the asymptotic variance is given.\nFor the purposes of the present work it is sufficient to use the result of\nvan Es and Uh (2005). However, before recalling the corresponding theorem, we first formulate conditions on the kernel w and the density k.\nCondition 1.1. Let \u03c6w be real-valued, symmetric and have support [\u22121, 1].\nLet \u03c6w (0) = 1, and assume \u03c6w (1 \u2212 t) = At\u03b1 + o(t\u03b1 ) as t \u2193 0 for some\nconstants A and \u03b1 \u2265 0.\nThe simplest example of such a kernel is the sinc kernel\nw(x) =\n\nsin x\n.\n\u03c0x\n\n(3)\n\nIts characteristic function equals \u03c6w (t) = 1[\u22121,1] (t). In this case A = 1 and\n\u03b1 = 0.\nAnother kernel satisfying Condition 1.1 is\n\u0012\n\u0013\n\u0012\n\u0013\n15\n144 sin x\n5\n48 cos x\n1\u2212 2 \u2212\n2\u2212 2 .\n(4)\nw(x) =\n\u03c0x4\nx\n\u03c0x5\nx\nIts corresponding Fourier transform is given by \u03c6w (t) = (1 \u2212 t2 )3 1[\u22121,1] (t).\nHere A = 8 and \u03b1 = 3. The kernel (4) was used for simulations in Fan\n(1992) and its good performance in deconvolution context was established\nin Delaigle and Hall (2006).\nYet another example is\n\u0012\n\u0013\n3 sin(x/4) 4\n.\n(5)\nw(x) =\n8\u03c0\nx/4\nThe corresponding Fourier transform equals\n\u03c6w (t) = 2(1 \u2212 |t|)3 1[1/2,1] (|t|) + (6|t|3 \u2212 6t2 + 1)1[\u22121/2,1/2] (t).\nHere A = 2 and \u03b1 = 3. This kernel was considered in Wand (1998) and\nDelaigle and Hall (2006).\nNow we formulate the condition on the density k.\n4\n\n\f\u0002\n\u0003\nCondition 1.2. Assume that \u03c6k (t) \u223c C|t|\u03bb0 exp \u2212|t|\u03bb /\u03bc as |t| \u2192 \u221e, for\nsome \u03bb > 1, \u03bc > 0, \u03bb0 and some constant C. Furthermore, let \u03c6k (t) 6= 0 for\nall t \u2208 R.\nThe following theorem holds true, see van Es and Uh (2005).\nTheorem 1.2. Assume Conditions 1.1 and 1.2 and let E [X 2 ] < \u221e. Then,\nas n \u2192 \u221e and h \u2192 0,\n\u0013\n\u0012\n\u221a\nn\nA2 \u0010 \u03bc \u00112+2\u03b1\nD\n2\n(\u0393(\u03b1\n+\n1))\n.\n(f\n(x)\u2212E\n[f\n(x)])\n\u2192\nN\n0,\nnh\nnh\n2\u03c0 2 \u03bb\nh\u03bb(1+\u03b1)+\u03bb0 \u22121 e1/(\u03bch\u03bb )\nHere \u0393 denotes the gamma function.\nThe goal of the present note is to compare the theoretical behaviour of\nthe estimator (1) predicted by Theorem 1.2 to its behaviour in practice,\nwhich will be done via a limited simulation study. The obtained results\ncan be used to compare Theorem 1.1 to Theorem 1.2, e.g. whether it is\npreferable to use the sample standard deviation sn in the construction of\npointwise confidence intervals (computation of sn is more involved) or to\nuse the normalisation of Theorem 1.2 (this involves evaluation of a simpler\nexpression). The rest of the paper is organised as follows: in Section 2 we\npresent some simulation results, while in Section 3 we discuss the obtained\nresults and draw conclusions.\n\n2\n\nSimulation results\n\nAll the simulations in this section were done in Mathematica. We considered\nthree target densities. These densities are:\n1. density # 1: Y \u223c N (0, 1);\n2. density # 2: Y \u223c \u03c72 (3);\n3. density # 3: Y \u223c 0.6N (\u22122, 1) + 0.4N (2, 0.82 ).\nThe density # 2 was chosen because it is skewed, while the density # 3\nwas selected because it has two unequal modes. We also assumed that the\nnoise term Z was N (0, 0.42 ) distributed. Notice that the noise-to-signal\nratio NSR = Var[Z]/ Var[Y ]100% for the density # 1 equals 16%, for the\ndensity # 2 it is equal to 2.66%, and for the density # 3 it is given by 3%.\nWe have chosen the sample size n = 50 and generated 500 samples from\nthe density g = f \u2217 k. Notice that such n was also used in simulations in\ne.g. Delaigle and Gijbels (2004). Even though at the first sight n = 50\nmight look too small for normal deconvolution, for the low noise level that\nwe have the deconvolution kernel density estimator will still perform well,\ncf. Wand (1998). As a kernel we took the kernel (4). For each model\n5\n\n\fthat we considered, the theoretically optimal bandwidth, i.e. the bandwidth\nminimising\n\u0015\n\u0014Z\n\u221e\n\nMISE[fnh ] = E\n\n\u2212\u221e\n\n(fnh (x) \u2212 f (x))2 dx ,\n\n(6)\n\nthe mean-squared error of the estimator fnh , was selected by evaluating (6)\nfor a grid of values of hk = 0.01k, k = 1, . . . , 100, and selecting the h that\nminimised MISE[fnh ] on that grid. Notice that it is easier to evaluate (6) by\nrewriting it in terms of the characteristic functions, which can be done via\nParseval's identity, cf. Stefanski and Carroll (1990). For real data of course\nthe above method does not work, because (6) depends on the unknown\nf. We refer to Delaigle and Gijbels (2004) for data-dependent bandwidth\nselection methods in kernel deconvolution.\nFollowing the recommendation of Delaigle and Gijbels (2007), in order\nto avoid possible numerical issues, the Fast Fourier Transform was used to\nevaluate the estimate (1). Several outcomes for two sample sizes, n = 50\nand n = 100, are given in Figure 1. We see that the fit in general is quite\nreasonable. This is in line with results in Wand (1998), where it was shown\nby finite sample calculations that the deconvolution kernel density estimator\nperforms well even in the supersmooth noise distribution case, if the noise\nlevel is not too high.\nIn Figure 2 we provide histograms of estimates fnh (x) that we obtained\nfrom our simulations for x = 0 and x = 0.92 (the densities # 1 and # 2)\nand for x = 0 and x = 2.04 (the density # 3). For the density # 1 points\nx = 0 and x = 0.92 were selected because the first corresponds to its mode,\nwhile the second comes from the region where the value of the density is\nmoderately high. Notice that x = 0 is a boundary point for the support of\ndensity # 2 and that the derivative of density # 2 is infinite there. For the\ndensity # 3 the point x = 0 corresponds to the region between its two modes,\nwhile x = 2.04 is close to where it has one of its modes. The histograms\nlook satisfactory and indicate that the asymptotic normality is not an issue.\nOur main interest, however, is in comparison of the sample standard\ndeviation of (1) at a fixed point x to the theoretical standard deviation\ncomputed using Theorem 1.2. This is of practical importance e.g. for construction of confidence intervals. The theoretical standard deviation can be\nevaluated as\nTSD =\n\nA\u0393(\u03b1 + 1)h\u03bb+\u03b1+\u03bb0 \u22121 e1/(\u03bch\n\u221a\n2n\u03c0 2\n\n\u03bb)\n\n\u0010 \u03bc \u00111+\u03b1\n\u03bb\n\n,\n\nupon noticing that in our case, i.e. when using kernel (4) and the error\ndistribution N (0, 0.42 ), we have A = 8, \u03b1 = 3, \u03bb0 = 0, \u03bb = 2, \u03bc = 2/0.42 .\nAfter comparing this theoretical value to the sample standard deviation of\nthe estimator fnh at points x = 0 and x = 0.92 (the densities # 1 and # 2)\n6\n\n\f-3\n\n-2\n\n0.4\n\n0.4\n\n0.3\n\n0.3\n\n0.2\n\n0.2\n\n0.1\n\n0.1\n\n1\n\n-1\n\n3\n\n-4\n\n0.5\n\n0.5\n\n0.4\n\n0.4\n\n0.3\n\n0.3\n\n0.2\n\n0.2\n\n0.1\n\n0.1\n\n1\n\n-4\n\n-2\n\n2\n\n3\n\n4\n\n2\n\n-2\n\n0.6\n\n-1\n\n-6\n\n2\n\n0.6\n\n1\n\n-1\n\n2\n\n0.20\n\n0.20\n\n0.15\n\n0.15\n\n0.10\n\n0.10\n\n0.05\n\n0.05\n\n2\n\n4\n\n6\n\n4\n\n3\n\n4\n\n5\n\n-5\n\nFigure 1: The estimate fnh (dotted line) and the true density f (thin line)\nfor the densities # 1, # 2 and # 3. The left column gives results for n = 50,\nwhile the right column provides results for n = 100.\nand at points x = 0 and x = 2.04 (the density # 3), see Table 1, we notice\na considerable discrepancy (by a factor 10 for the density # 1 and even\nlarger discrepancy for densities # 2 and # 3). At the same time the sample\nmeans evaluated at these two points are close to the true values of the target\ndensity and broadly correspond to the expected theoretical value f \u2217 wh (x).\nNote here that the bias of fnh (x) is equal to the bias of an ordinary kernel\ndensity estimator based on a sample from f, see e.g. Fan (1991a).\nTo gain insight into this striking discrepancy, recall how the asymptotic\nnormality of fnh (x) was derived in van Es and Uh (2005). Adapting the\nproof from the latter paper to our example, the first step is to rewrite fnh (x)\nas\n\u0012\n\u0013\nZ 1\nn\nn\nx \u2212 Xj\n1X\n1X\n1\n\u03bb\n\u03bb\ncos\n\u03c6w (s) exp[s /(\u03bch )]ds\n+\nR\u0303n,j , (7)\n\u03c0h 0\nn\nh\nn\nj=1\n\n7\n\nj\n\n\f50\n\n50\n\n40\n\n40\n\n30\n\n30\n\n20\n\n20\n\n10\n\n10\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.15\n\n60\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n60\n\n50\n\n50\n\n40\n\n40\n\n30\n\n30\n\n20\n\n20\n\n10\n\n10\n\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n50\n\n40\n\n40\n30\n30\n20\n20\n10\n10\n\n0.025\n\n0.05\n\n0.075\n\n0.1\n\n0.125\n\n0.15\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\nFigure 2: The histograms of estimates fnh (x) for x = 0 and x = 0.92 for\nthe density # 1 (top two graphs), for x = 0 and x = 0.92 for the density\n# 2 (middle two graphs), and for x = 0 and x = 2.04 for the density # 3\n(bottom two graphs).\nwhere the remainder terms R\u0303n,j are defined in van Es and Uh (2005). Then\nby estimating the variance of the second summand in (7), one can show that\nit can be neglected when considering the asymptotic normality of (7) as\nn \u2192 \u221e and h \u2192 0. Turning to the first term in (7), one uses the asymptotic\nequivalence, cf. Lemma 5 in van Es and Uh (2005),\nZ\n\n0\n\n1\n\n\u03c6w (s) exp[s\u03bb /(\u03bch\u03bb )]ds \u223c A\u0393(\u03b1 + 1)\n\n\u0010\u03bc\n\u03bb\n\nh\u03bb\n\n\u00111+\u03b1\n\n\u03bb\n\ne1/(\u03bch ) ,\n\n(8)\n\nwhich explains the shape of the normalising constant in Theorem 1.2. However, this is precisely the point which causes a large discrepancy between\nthe theoretical standard deviation and the sample standard deviation. The\napproximation is good asymptotically as h \u2192 0, but it is quite inaccurate\n8\n\n\ff\n#1\n#2\n#3\n\nh\n0.24\n0.18\n0.25\n\n\u03bc\u03021\n0.343\n0.066\n0.074\n\n\u03bc\u03022\n0.252\n0.389\n0.159\n\n\u03c3\u03021\n0.0423\n0.035\n0.025\n\n\u03c3\u03022\n0.039\n0.067\n0.037\n\n\u03c3\n0.429\n0.169\n0.512\n\n\u03c3\u0303\n0.072\n0.114\n0.068\n\nTable 1: Sample means \u03bc\u03021 and \u03bc\u03022 and sample standard deviations \u03c3\u03021 and\n\u03c3\u03022 evaluated at x = 0 and x = 0.92 (densities # 1 and # 2) and x = 0 and\nx = 2.04 (the density # 3) together with the theoretical standard deviation\n\u03c3 and the corrected theoretical standard deviation \u03c3\u0303. The bandwidth is\ngiven by h.\n\nfor larger values of h. Indeed, consider the ratio of the left-hand side of (8)\nwith the right-hand side. We have plotted this ratio as a function of h for\nh ranging between 0 and 1, see Figure 3. One sees that the ratio is close to\n1 for extremely small values of h and is quite far from 1 for larger values of\nh. It is equally easy to see that the poor approximation in (8) holds true for\nkernels (3) and (5) as well, see e.g. Figure 3, which plots the ratio of both\nsides of (8) for the kernel (3). This poor approximation, of course, is not\n1.5\n\n1.2\n\n1.0\n1.0\n\n0.8\n\n0.6\n0.5\n\n0.4\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0.2\n\n1.0\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nFigure 3: Accuracy of (8) as a function of h for the kernels (4) (left figure)\nand (3) (right figure).\ncharacteristic of only the particular \u03bc and \u03bb that we used in our simulations,\nbut also holds true for other values of \u03bc and \u03bb.\nObviously, one can correct for the poor approximation of the sample\nstandard deviation by the theoretical standard deviation by using the lefthand side of (8) instead of its approximation. The theoretical standard\ndeviation corrected in such a way is given in the last column of Table 1. As\nit can be seen from the table, this procedure led to an improvement of the\nagreement between the theoretical standard deviation and its sample counterpart for all three target densities. Nevertheless, the match is not entirely\nsatisfactory, since the corrected theoretical standard deviation and the sample standard deviation differ by factor 2 or even more. A perfect match is\nimpossible to obtain, because we neglect the remainder term in (7) and h is\n9\n\n\fstill fairly large. We further notice that the concurrence between the results\nis better for x = 0 than for x = 0.92 for densities # 1 and # 2, and for\nx = 2.04 than for x = 0 for the density # 3. We also performed simulations\nfor the sample sizes n = 100 and n = 200 to check the effect of having larger\nsamples. For brevity we will report only the results for density # 2, see\nFigure 4 and Table 2, since this density is nontrivial to deconvolve, though\nnot as difficult as the density # 3. Notice that the results did not improve\ngreatly for n = 100, while for the case n = 200 the corrected theoretical\nstandard deviation became a worse estimate of the sample standard deviation than the theoretical standard deviation. Explanation of this curious\nphenomenon is given in Section 3.\n40\n\n40\n\n30\n\n30\n\n20\n\n20\n\n10\n\n10\n\n0\n\n0.025\n\n0.05\n\n0.075\n\n0.1\n\n0.125\n\n0.15\n\n0.25\n\n40\n\n40\n\n30\n\n30\n\n20\n\n20\n\n10\n\n10\n\n0\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n0.1\n\n0.12\n\n0.3\n\n0.3\n\n0.35\n\n0.35\n\n0.4\n\n0.4\n\n0.45\n\n0.45\n\n0.5\n\n0.5\n\n0.55\n\n0.55\n\nFigure 4: The histograms of estimates fnh (x) for x = 0 and x = 0.92 for\nthe density # 2 for n = 100 (top two graphs) and for n = 200 (bottom two\ngraphs).\nn\n# 100\n# 200\n\nh\n0.17\n0.15\n\n\u03bc\u03021\n0.063\n0.052\n\n\u03bc\u03022\n0.393\n0.402\n\n\u03c3\u03021\n0.025\n0.023\n\n\u03c3\u03022\n0.051\n0.049\n\n\u03c3\n0.108\n0.070\n\n\u03c3\u0303\n0.090\n0.084\n\nTable 2: Sample means \u03bc\u03021 and \u03bc\u03022 and sample standard deviations \u03c3\u03021 and\n\u03c3\u03022 evaluated at x = 0 and x = 0.92 for the density # 2, together with\nthe theoretical standard deviation \u03c3 and the corrected theoretical standard\ndeviation \u03c3\u0303.\n\n10\n\n\fFurthermore, note that\n\uf8ee\n\n\uf8f9\n\u0012\n\u0013\nn\nX\nx \u2212 Xj \uf8fb\n1\n1\nVar \uf8f0 \u221a\ncos\n\u2192\nn\nh\n2\nj=1\n\nas n \u2192 \u221e and h \u2192 0, see van Es and Uh (2005). This explains the appearance of the factor 1/2 in the asymptotic variance in Theorem 1.2. One\nmight also question the goodness of this approximation and propose to use\ninstead some estimator of Var[cos((x \u2212 X)h\u22121 )], e.g. its empirical counterpart based on the sample X1 , . . . , Xn . However, in the simulations that we\nperformed for all three target densities (with n and h as above), the resulting estimates took values close to the true value 1/2. E.g. for the density #\n3 the sample mean turned out to be 0.502298, while the sample standard\ndeviation was equal to 0.0535049, thus showing that there was insignificant\nvariability around 1/2 in this particular example. On the other hand, for\nother distributions and for different sample sizes, it could be the case that\nthe direct use of 1/2 will lead to inaccurate results.\nNext we report some simulation results relevant to Theorem 1.1. This\ntheorem tells us that for a fixed n we have that\n\u221a\nn\n(fnh (x) \u2212 E [fnh (x)])\n(9)\nsn\nis approximately normally distributed with zero mean and variance equal to\none. Upon using the fact that E [fnh (x)] = f \u2217 wh (x), we used the data that\nwe obtained from our previous simulation examples to plot the histograms of\n(9) and to evaluate the sample means and standard deviations, see Figure 5\nand Table 3. One notices that the concurrence of the theoretical and sample\nvalues is quite good for the density # 1. For the density # 2 it is rather\nunsatisfactory for x = 0, which is explainable by the fact that in general\nthere are very few observations originating from the neighbourhood of this\npoint. Finally, we notice that the match is reasonably good for the density\n# 3, given the fact that it is difficult to estimate, at the point x = 2.04, but\nis still unsatisfactory at the point x = 0. The latter is explainable by the\nfact that there are less observations originating from the neighbourhood of\nthis point. An increase in the sample size (n = 100 and n = 200) leads to\nan improvement of the match between the theoretical and the sample mean\nand standard deviation at the point x = 0 for the density # 2, see Figure\n6 and Table 4, however the results are still largely inaccurate for this point.\nIn essence similar conclusions were obtained for the density # 3. These are\nnot reported here.\nNote that in all three models that we studied the noise level is not\nhigh. We also studied the case when the noise level is very high. For\nbrevity we present the results only for the density # 1 and for sample size\nn = 50. We considered three cases of the error distribution: in the first\n11\n\n\f50\n50\n40\n40\n\n30\n\n30\n\n20\n\n20\n\n10\n\n10\n\n-2\n\n0\n\n-1\n\n1\n\n2\n\n-4\n\n250\n\n100\n\n200\n\n80\n\n150\n\n60\n\n100\n\n40\n\n50\n\n20\n\n-200\n\n-150\n\n-100\n\n0\n\n-50\n\n0\n\n-2\n\n-6\n\n-4\n\n2\n\n0\n\n-2\n\n2\n\n80\n\n200\n\n60\n\n150\n\n100\n\n40\n\n50\n\n20\n\n-40\n\n-30\n\n-20\n\n-10\n\n0\n\n-10\n\n-7.5\n\n-5\n\n-2.5\n\n0\n\n2.5\n\nFigure 5: The histograms of (9) for x = 0 and x = 0.92 for the density #\n1 (top two graphs), for x = 0 and x = 0.92 for the density # 2 (middle\ntwo graphs), and for x = 0 and x = 2.04 for the density # 3 (bottom two\ngraphs).\ncase Z \u223c N (0, 1), in the second case Z \u223c N (0, 22 ) and in the third case\nZ \u223c N (0, 42 ). Notice that the NSR is equal to 100%, 400% and 1600%,\nrespectively. The simulation results are summarised in Figures 7 and 8\nand Tables 5 and 6. We see that the sample standard deviation and the\ncorrected theoretical standard deviation are in better agreement among each\nother compared to the low noise level case. Also the histograms of the\nvalues of (9) look better. On the other hand the resulting curves fnh were\nnot too satisfactory when compared to the true density f in the two cases\nZ \u223c N (0, 1), and Z \u223c N (0, 22 ) (especially in the second case) and were\ntotally unacceptable in the case Z \u223c N (0, 42 ). This of course does not\nimply that the estimator (1) is bad, rather the deconvolution problem is\nvery difficult in these cases.\n\n12\n\n\ff\n#1\n#2\n#3\n\nh\n0.24\n0.18\n0.25\n\n\u03bc\u03021\n-0.046\n-3.984\n-0.768\n\n\u03bc\u03022\n-0.093\n-0.084\n-0.141\n\n\u03c3\u03021\n0.953\n17.2\n4.03\n\n\u03c3\u03022\n1.127\n1.28\n1.63\n\nTable 3: Sample means \u03bc\u03021 and \u03bc\u03022 and sample standard deviations \u03c3\u03021 and\n\u03c3\u03022 evaluated at x = 0 and x = 0.92 (densities # 1 and # 2) and x = 0 and\nx = 2.04 (the density # 3).\n\n250\n80\n\n200\n60\n\n150\n40\n\n100\n\n20\n\n50\n\n-150\n\n-100\n\n0\n\n-50\n\n-6\n\n-4\n\n0\n\n-2\n\n2\n\n70\n\n150\n60\n50\n\n100\n40\n30\n\n50\n\n20\n10\n\n-50\n\n-40\n\n-30\n\n-20\n\n-10\n\n0\n\n-6\n\n-4\n\n-2\n\n0\n\n2\n\n4\n\nFigure 6: The histograms of (9) for x = 0 and x = 0.92 for the density # 2\nfor n = 100 (top two graphs)and n = 200 (bottom two graphs).\nFinally, we mention that results qualitatively similar to the ones presented in this section were obtained for the kernel (3) as well. These are not\nreported here because of space restrictions.\n\n3\n\nDiscussion\n\nIn the simulation examples considered in Section 2 for Theorem 1.2, we\nnotice that the corrected theoretical asymptotic standard deviation is always\nconsiderably larger than the sample standard deviation given the fact that\nthe noise level is not high. We conjecture, that this might be true for the\ndensities other than # 1, # 2 and # 3 as well in case when the noise level is\nlow. This possibly is one more explanation of the fact of a reasonably good\nperformance of deconvolution kernel density estimators in the supersmooth\n13\n\n\fn\n100\n200\n\nh\n0.17\n0.15\n\n\u03bc\u03021\n-1.33\n-1.02\n\n\u03bc\u03022\n-0.015\n-0.015\n\n\u03c3\u03021\n9.89\n6.36\n\n\u03c3\u03022\n1.31\n1.58\n\nTable 4: Sample means \u03bc\u03021 and \u03bc\u03022 and sample standard deviations \u03c3\u03021 and\n\u03c3\u03022 evaluated at x = 0 and x = 0.92 for the density # 2 for two sample sizes:\nn = 100 and n = 200.\nNSR\n100%\n400%\n1600%\n\nh\n0.36\n0.59\n0.89\n\n\u03bc\u03021\n0.294\n0.214\n0.150\n\n\u03bc\u03022\n0.236\n0.189\n0.156\n\n\u03c3\u03021\n0.046\n0.053\n0.279\n\n\u03c3\u03022\n0.045\n0.053\n0.289\n\n\u03c3\n0.057\n0.046\n0.251\n\n\u03c3\u0303\n0.075\n0.076\n0.342\n\nTable 5: Sample means \u03bc\u03021 and \u03bc\u03022 and sample standard deviations \u03c3\u03021 and\n\u03c3\u03022 together with theoretical standard deviation \u03c3 and corrected theoretical\nstandard deviation \u03c3\u0303 evaluated at x = 0 and x = 0.92 for the density # 1\nfor three noise levels: NSR = 100%, NSR = 400% and NSR = 1600%.\n\nerror case for relatively small sample sizes which was noted in Wand (1998).\nOn the other hand the match between the sample standard deviation and\nthe corrected theoretical standard deviation is much better for higher levels\nof noise. These observations suggest studying the asymptotic distribution\nof the deconvolution kernel density estimator under the assumption \u03c3 \u2192 0\nas n \u2192 \u221e, cf. Delaigle (2007), where \u03c3 denotes the standard deviation of\nthe noise term.\nOur simulation examples suggest that the asymptotic standard deviation\nevaluated via Theorem 1.2 in general will not lead to an accurate approximation of the sample standard deviation, unless the bandwidth is small\nenough, which implies that the corresponding sample size must be rather\nlarge. The latter is hardly ever the case in practice. On the other hand,\nwe have seen that in certain cases this poor approximation can be improved\nby using the left-hand side of (8) instead of the right-hand side. A perfect\nNSR\n100%\n400%\n1600%\n\nh\n0.36\n0.59\n0.89\n\n\u03bc\u03021\n-0.038\n-0.079\n-0.015\n\n\u03bc\u03022\n-0.098\n-0.134\n0.035\n\n\u03c3\u03021\n1.091\n1.155\n1.027\n\n\u03c3\u03022\n1.228\n1.193\n1.086\n\nTable 6: Sample means \u03bc\u03021 and \u03bc\u03022 and sample standard deviations \u03c3\u03021 and\n\u03c3\u03022 of (9) evaluated at x = 0 and x = 0.92 for the density # 1 for two noise\nlevels: NSR = 400% and NSR = 1600%.\n\n14\n\n\f50\n40\n40\n30\n30\n20\n20\n\n10\n\n10\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.1\n\n0.4\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n40\n40\n\n30\n30\n\n20\n\n20\n\n10\n\n10\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.1\n\n0.35\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n40\n30\n30\n\n20\n20\n\n10\n\n10\n\n-0.5\n\n-0.25\n\n0\n\n0.25\n\n0.5\n\n0.75\n\n1\n\n-0.5\n\n-0.25\n\n0\n\n0.25\n\n0.5\n\n0.75\n\n1\n\nFigure 7: The histograms of fnh (x) for x = 0 and x = 0.92 for the density\n# 1 for n = 50 and three noise levels: NSR = 100% (top two graphs),\nNSR = 400% (middle two graphs) and NSR = 1600% (bottom two graphs).\nmatch is impossible to obtain given that we still neglect the remainder term\nin (7). However, even after the correction step, the corrected theoretical\nstandard deviation still differs from the sample standard deviation considerably for small sample sizes and lower levels of noise. Moreover, in some\ncases the corrected theoretical standard deviation is even farther from the\nsample standard deviation than the original uncorrected version. The latter\nfact can be explained as follows:\n1. It seems that both the theoretical and corrected theoretical standard\ndeviation overestimate the sample standard deviation.\n2. The value of the bandwidth h, for which the match between the corrected theoretical standard deviation and the sample standard deviation become worse, belongs to the range where the corrected theo15\n\n\f50\n80\n40\n60\n30\n40\n20\n\n20\n\n10\n\n-4\n\n-3\n\n-2\n\n0\n\n-1\n\n1\n\n2\n\n-6\n\n50\n\n50\n\n40\n\n40\n\n30\n\n30\n\n20\n\n20\n\n10\n\n10\n\n-3\n\n-2\n\n0\n\n-1\n\n1\n\n2\n\n3\n\n-3\n\n-4\n\n-2\n\n0\n\n-2\n\n0\n\n-1\n\n2\n\n1\n\n2\n\n3\n\n60\n50\n50\n40\n40\n30\n30\n20\n20\n10\n\n10\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\nFigure 8: The histograms of (9) for x = 0 and x = 0.92 for the density\n# 1 for n = 50 and three noise levels: NSR = 400% (top two graphs),\nNSR = 400% (middle two graphs) and NSR = 1600% (bottom two graphs).\nretical standard deviation is larger than the theoretical standard deviation. In view of item 1 above, it is not surprising that in this case\nthe theoretical value turns out to be closer to the sample standard\ndeviation than the corrected theoretical value.\nThe consequence of the above observations is that a naive attempt to\ndirectly use Theorem 1.2, e.g. in the construction of pointwise confidence\nintervals, will lead to largely inaccurate results. An indication of how large\nthe contribution of the remainder term in (7) can be can be obtained only\nafter a thorough simulation study for various distributions and sample sizes,\na goal which is not pursued in the present note. From the three simulation examples that we considered, it appears that the contribution of the\nremainder term in (7) is quite noticeable for small sample sizes. For now\nwe would advise to use Theorem 1.2 for small sample sizes and lower noise\n16\n\n\flevels with caution. It seems that the similar cautious approach is needed\nin case of Theorem 1.1 as well, at least for some values of x.\nUnlike for the ordinary smooth case, see Bissantz et al. (2007), there\nis no study dealing with the construction of uniform confidence intervals\nin the supersmooth case. In the latter paper a better performance of the\nbootstrap confidence intervals was demonstrated in the ordinary smooth case\ncompared to the asymptotic confidence bands obtained from the expression\nfor the asymptotic variance in the central limit theorem. The main difficulty\nin the supersmooth case is that the asymptotic distribution of the supremum\ndistance between the estimator fnh and the true density f is unknown. Our\nsimulation results seem to indicate that the bootstrap approach is more\npromising for the construction of pointwise confidence intervals than e.g.\nthe direct use of Theorems 1.1 or 1.2. Moreover, the simulations suggest\nthat at least Theorem 1.2 is not appropriate when the noise level is low.\n\nReferences\nN. Bissantz, L. D\u00fcmbgen, H. Holzmann and A. Munk, Non-parametric confidence bands in deconvolution density estimation, 2007, J. Roy. Statist.\nSoc. Ser. B 69, 483\u2013506.\nR. J. Carroll and P. Hall, Optimal rates of convergence for deconvolving a\ndensity, 1988, J. Amer. Stat. Assoc. 83, 1184\u20131186.\nA. Delaigle, An alternative view of the deconvolution problem, 2007, to\nappear in Statist. Sinica.\nA. Delaigle and P. Hall, On optimal kernel choice for deconvolution, 2006,\nStatist. Probab. Lett. 76, 1594\u20131602.\nA. Delaigle and I. Gijbels, Practical bandwidth selection in deconvolution\nkernel density estimation, 2004, Comput. Statist. Data Anal. 45, 249\u2013267.\nA. Delaigle and I. Gijbels, Frequent problems in calculating integrals and\noptimizing objective functions: a case study in density deconvolution,\n2007, Stat. Comput. 17, 349\u2013355.\nA. J. van Es and A. R. Kok, Simple kernel estimators for certain nonparametric deconvolution problems, 1998, Statist. Probab. Lett. 39, 151\u2013160.\nA. J. van Es and H.-W. Uh, Asymptotic normality of nonparametric kerneltype deconvolution density estimators: crossing the Cauchy boundary,\n2004, J. Nonparametr. Stat. 16, 261\u2013277.\nA. J. van Es and H.-W. Uh, Asymptotic normality of kernel type deconvolution estimators, 2005, Scand. J. Statist. 32, 467\u2013483.\n17\n\n\fJ. Fan, On the optimal rates of convergence for nonparametric deconvolution\nproblems, 1991a, Ann. Statist. 19, 1257\u20131272.\nJ. Fan, Asymptotic normality for deconvolution kernel density estimators,\n1991b, Sankhy\u0101 Ser. A 53, 97\u2013110.\nJ. Fan, Deconvolution for supersmooth distributions, 1992, Canad. J.\nStatist. 20, 155\u2013169.\nJ. Fan and Y. Liu, A note on asymptotic normality for deconvolution kernel\ndensity estimators, 1997, Sankhy\u0101 Ser. A 59, 138\u2013141.\nL. Stefanski and R. J. Carroll, Deconvoluting kernel density estimators,\n1990, Statistics 2, 169\u2013184.\nM. P. Wand, Finite sample performance of deconvolving density estimators,\n1998, Statist. Probab. Lett. 37, 131\u2013139.\n\n18\n\n\f"}