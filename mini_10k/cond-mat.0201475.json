{"id": "http://arxiv.org/abs/cond-mat/0201475v1", "guidislink": true, "updated": "2002-01-25T16:21:40Z", "updated_parsed": [2002, 1, 25, 16, 21, 40, 4, 25, 0], "published": "2002-01-25T16:21:40Z", "published_parsed": [2002, 1, 25, 16, 21, 40, 4, 25, 0], "title": "Optimization and Parallelization of a force field for silicon using\n  OpenMP", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0201237%2Ccond-mat%2F0201028%2Ccond-mat%2F0201003%2Ccond-mat%2F0201358%2Ccond-mat%2F0201567%2Ccond-mat%2F0201280%2Ccond-mat%2F0201491%2Ccond-mat%2F0201268%2Ccond-mat%2F0201582%2Ccond-mat%2F0201079%2Ccond-mat%2F0201315%2Ccond-mat%2F0201319%2Ccond-mat%2F0201230%2Ccond-mat%2F0201262%2Ccond-mat%2F0201057%2Ccond-mat%2F0201475%2Ccond-mat%2F0201222%2Ccond-mat%2F0201305%2Ccond-mat%2F0201157%2Ccond-mat%2F0201471%2Ccond-mat%2F0201011%2Ccond-mat%2F0201408%2Ccond-mat%2F0201479%2Ccond-mat%2F0201518%2Ccond-mat%2F0201066%2Ccond-mat%2F0201407%2Ccond-mat%2F0201189%2Ccond-mat%2F0201267%2Ccond-mat%2F0201200%2Ccond-mat%2F0201530%2Ccond-mat%2F0201094%2Ccond-mat%2F0201257%2Ccond-mat%2F0201438%2Ccond-mat%2F0201421%2Ccond-mat%2F0201499%2Ccond-mat%2F0201372%2Ccond-mat%2F0201446%2Ccond-mat%2F0201183%2Ccond-mat%2F0201483%2Ccond-mat%2F0201162%2Ccond-mat%2F0201528%2Ccond-mat%2F0201332%2Ccond-mat%2F0201501%2Ccond-mat%2F0201363%2Ccond-mat%2F0201106%2Ccond-mat%2F0201550%2Ccond-mat%2F0201089%2Ccond-mat%2F0201077%2Ccond-mat%2F0201151%2Ccond-mat%2F0201176%2Ccond-mat%2F0201524%2Ccond-mat%2F0201148%2Ccond-mat%2F0201575%2Ccond-mat%2F0201413%2Ccond-mat%2F0201352%2Ccond-mat%2F0201284%2Ccond-mat%2F0201448%2Ccond-mat%2F0201404%2Ccond-mat%2F0201213%2Ccond-mat%2F0201329%2Ccond-mat%2F0201195%2Ccond-mat%2F0201206%2Ccond-mat%2F0201357%2Ccond-mat%2F0201169%2Ccond-mat%2F0201225%2Ccond-mat%2F0201412%2Ccond-mat%2F0201004%2Ccond-mat%2F0201347%2Ccond-mat%2F0201069%2Ccond-mat%2F0201175%2Ccond-mat%2F0201493%2Ccond-mat%2F0201416%2Ccond-mat%2F0201462%2Ccond-mat%2F0201046%2Ccond-mat%2F0201245%2Ccond-mat%2F0201300%2Ccond-mat%2F0201051%2Ccond-mat%2F0201422%2Ccond-mat%2F0201279%2Ccond-mat%2F0201067%2Ccond-mat%2F0201377%2Ccond-mat%2F0201187%2Ccond-mat%2F0201134%2Ccond-mat%2F0201111%2Ccond-mat%2F0201231%2Ccond-mat%2F0201509%2Ccond-mat%2F0201293%2Ccond-mat%2F0201061%2Ccond-mat%2F0201058%2Ccond-mat%2F0201587%2Ccond-mat%2F0201533%2Ccond-mat%2F0201234%2Ccond-mat%2F0201020%2Ccond-mat%2F0201426%2Ccond-mat%2F0201348%2Ccond-mat%2F0201375%2Ccond-mat%2F0201009%2Ccond-mat%2F0201073%2Ccond-mat%2F0201191%2Ccond-mat%2F0201139%2Ccond-mat%2F0201591&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Optimization and Parallelization of a force field for silicon using\n  OpenMP"}, "summary": "The force field by Lenosky and coworkers is the latest force field for\nsilicon which is one of the most studied materials. It has turned out to be\nhighly accurate in a large range of test cases. The optimization and\nparallelization of this force field using OpenMp and Fortan90 is described\nhere. The optimized program allows us to handle a very large number of silicon\natoms in large scale simulations. Since all the parallelization is hidden in a\nsingle subroutine that returns the total energies and forces, this subroutine\ncan be called from within a serial program in an user friendly way.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0201237%2Ccond-mat%2F0201028%2Ccond-mat%2F0201003%2Ccond-mat%2F0201358%2Ccond-mat%2F0201567%2Ccond-mat%2F0201280%2Ccond-mat%2F0201491%2Ccond-mat%2F0201268%2Ccond-mat%2F0201582%2Ccond-mat%2F0201079%2Ccond-mat%2F0201315%2Ccond-mat%2F0201319%2Ccond-mat%2F0201230%2Ccond-mat%2F0201262%2Ccond-mat%2F0201057%2Ccond-mat%2F0201475%2Ccond-mat%2F0201222%2Ccond-mat%2F0201305%2Ccond-mat%2F0201157%2Ccond-mat%2F0201471%2Ccond-mat%2F0201011%2Ccond-mat%2F0201408%2Ccond-mat%2F0201479%2Ccond-mat%2F0201518%2Ccond-mat%2F0201066%2Ccond-mat%2F0201407%2Ccond-mat%2F0201189%2Ccond-mat%2F0201267%2Ccond-mat%2F0201200%2Ccond-mat%2F0201530%2Ccond-mat%2F0201094%2Ccond-mat%2F0201257%2Ccond-mat%2F0201438%2Ccond-mat%2F0201421%2Ccond-mat%2F0201499%2Ccond-mat%2F0201372%2Ccond-mat%2F0201446%2Ccond-mat%2F0201183%2Ccond-mat%2F0201483%2Ccond-mat%2F0201162%2Ccond-mat%2F0201528%2Ccond-mat%2F0201332%2Ccond-mat%2F0201501%2Ccond-mat%2F0201363%2Ccond-mat%2F0201106%2Ccond-mat%2F0201550%2Ccond-mat%2F0201089%2Ccond-mat%2F0201077%2Ccond-mat%2F0201151%2Ccond-mat%2F0201176%2Ccond-mat%2F0201524%2Ccond-mat%2F0201148%2Ccond-mat%2F0201575%2Ccond-mat%2F0201413%2Ccond-mat%2F0201352%2Ccond-mat%2F0201284%2Ccond-mat%2F0201448%2Ccond-mat%2F0201404%2Ccond-mat%2F0201213%2Ccond-mat%2F0201329%2Ccond-mat%2F0201195%2Ccond-mat%2F0201206%2Ccond-mat%2F0201357%2Ccond-mat%2F0201169%2Ccond-mat%2F0201225%2Ccond-mat%2F0201412%2Ccond-mat%2F0201004%2Ccond-mat%2F0201347%2Ccond-mat%2F0201069%2Ccond-mat%2F0201175%2Ccond-mat%2F0201493%2Ccond-mat%2F0201416%2Ccond-mat%2F0201462%2Ccond-mat%2F0201046%2Ccond-mat%2F0201245%2Ccond-mat%2F0201300%2Ccond-mat%2F0201051%2Ccond-mat%2F0201422%2Ccond-mat%2F0201279%2Ccond-mat%2F0201067%2Ccond-mat%2F0201377%2Ccond-mat%2F0201187%2Ccond-mat%2F0201134%2Ccond-mat%2F0201111%2Ccond-mat%2F0201231%2Ccond-mat%2F0201509%2Ccond-mat%2F0201293%2Ccond-mat%2F0201061%2Ccond-mat%2F0201058%2Ccond-mat%2F0201587%2Ccond-mat%2F0201533%2Ccond-mat%2F0201234%2Ccond-mat%2F0201020%2Ccond-mat%2F0201426%2Ccond-mat%2F0201348%2Ccond-mat%2F0201375%2Ccond-mat%2F0201009%2Ccond-mat%2F0201073%2Ccond-mat%2F0201191%2Ccond-mat%2F0201139%2Ccond-mat%2F0201591&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The force field by Lenosky and coworkers is the latest force field for\nsilicon which is one of the most studied materials. It has turned out to be\nhighly accurate in a large range of test cases. The optimization and\nparallelization of this force field using OpenMp and Fortan90 is described\nhere. The optimized program allows us to handle a very large number of silicon\natoms in large scale simulations. Since all the parallelization is hidden in a\nsingle subroutine that returns the total energies and forces, this subroutine\ncan be called from within a serial program in an user friendly way."}, "authors": ["Stefan Goedecker"], "author_detail": {"name": "Stefan Goedecker"}, "author": "Stefan Goedecker", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/S0010-4655(02)00466-6", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cond-mat/0201475v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cond-mat/0201475v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "The program can be obtained upon request from the author\n  (SGoedecker@cea.fr)", "arxiv_primary_category": {"term": "cond-mat", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cond-mat/0201475v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cond-mat/0201475v1", "journal_reference": null, "doi": "10.1016/S0010-4655(02)00466-6", "fulltext": "Optimization and Parallelization of a force field for silicon using OpenMP\nStefan Goedecker\n\narXiv:cond-mat/0201475v1 25 Jan 2002\n\nD\u00e9partement de recherche fondamentale sur la mati\u00e8re condens\u00e9e,\nSP2M/NM, CEA-Grenoble, 38054 Grenoble cedex 9, France\n(October 29, 2018)\nThe force field by Lenosky and coworkers is the latest force field for silicon which is one of\nthe most studied materials. It has turned out to be highly accurate in a large range of test cases.\nThe optimization and parallelization of this force field using OpenMp and Fortan90 is described\nhere. The optimized program allows us to handle a very large number of silicon atoms in large\nscale simulations. Since all the parallelization is hidden in a single subroutine that returns the total\nenergies and forces, this subroutine can be called from within a serial program in an user friendly\nway.\n\nI. PROGRAM SURVEY\n\nTitle of program: siliconiap\nComputer hardware and operating system: Any shared memory computer running under Unix or Linux\nProgramming Language: Fortran90 with OpenMP compiler directives\nMemory requirements: roughly 150 words per atom\nNo. of bits in a word: 64\nNo. of processors used: tested on up to 4 processors\nHas the code been vectorized or parallelized: Parallelized withe OpenMP\nNo. of bytes in distributed program, including test data, etc: 50 000\nDistribution format: Compressed tar file\nKeywords: Silicon, Interatomic potential, Force field, Molecular dynamics\nNature of physical problem: condensed matter physics\nMethod of solution: Interatomic potential\nRestrictions on the complexity of the problem: None\nTypical running time: 30 \u03bcsec per step and per atom on a Compaq DEC Alpha\nUnusual features of the program: None\nII. INTRODUCTION\n\nDue to its technological importance, silicon is one of the most studied materials. For small system sizes abinitio density functional calculations [1] are the preferred approach. Unfortunately this kind of calculation becomes\nunfeasible for larger systems required to study problems such as interfaces or extended defects. For this type of\ncalculations one resorts to force fields which are several orders of magnitude faster. Recent progress in the development\nof force fields has demonstrated that they can be a reliable tool for such studies. A highly accurate silicon force field has\nbeen developed by Lenosky and coworkers [6]. Its transferability has been demonstrated by extensive tests containing\nboth bulk and cluster systems [6]. Its accuracy is in part due to the fact that second nearest neighbor interactions are\nincluded. This makes it unfortunately somewhat slower than force fields containing only nearest neighbor interactions.\nIn the following a highly optimized parallel implementation of this force field will be presented that allows large scale\ncalculations with this force field. The parallelization is achieved by using OpenMP, an emerging industry standard\nfor medium size shared memory parallel computers.\nMolecular dynamics calculations [2] have also been parallelized on distributed memory supercomputers [3]. This\napproach is considerably more complex than the one presented here. Since few researches have access to massively\nparallel supercomputers and are willing to overcome the complexities of doing molecular dynamics on such machines,\nmedium scale parallelization [4] of molecular dynamics has an important place in practice.\n\n1\n\n\fIII. CALLING THE SUBROUTINE\n\nUser friendliness was one of the major design goals in the development of this routine. Using Fortran90 made it\npossible to hide all the complexities in an object oriented fashion from the user. The calling sequence is just\ncall lenosky(nat,alat,rxyz,fxyz,ener,coord,ener_var,coord_var,count)\nOn input the user has to specify the number of atoms, nat, the vector alat containing the 3 lattice constant of the\northorhombic periodic volume and the atomic positions rxyz. The program then returns the total energy, ener, the\nforces, f xyz, the average coordination number, the variation of the energy per atom and of the coordination number\nas well as an counter that is increased in each call. In particular the user has not to supply any Verlet list.\nSince the calculation of the forces is typically much more expensive than the update of the atomic positions in\nmolecular dynamics or geometry optimizations, we expect that the subroutine will be called in most cases from within\na serial program. In case the user is on a shared memory machine the subroutine will then nevertheless be executed\nin parallel if the program is compiled with the appropriate OpenMP options.\nIn addition the subroutine can of course also be used on a serial machine. In this case all the parallelization\ndirectives are considered by the compiler to be comments.\nIV. CALCULATION OF THE VERLET LIST\n\nThe Verlet list gives all the atoms that are contained within the potential cutoff distance cut of any given atom.\nTypically the Verlet list consists of two integer arrays. The first array, called lsta in this work, points to the\nfirst/last neighbor position in the second array lstb that contains the numbering of the atoms that are neighbors.\nA straightforward implementation for a non-periodic system containing nat atoms is shown below. In this simple\ncase the search through all atoms is sequential with respect to their numbering and it is redundant to give both the\nstarting positions lsta(1, iat) and the ending position lsta(2, iat), since lsta(1, iat) = lsta(2, iat \u2212 1) + 1. But in the\nmore complicated linear scaling algorithm to be presented below, both will be needed.\n\nc\n\nc\n\n20\nc\n10\n\nindc=0\ndo 10 iat=1,nat\nstarting position\nlsta(1,iat)=indc+1\ndo 20 jat=1,nat\nif (jat.ne.iat) then\nxrel1= rxyz(1,jat)-rxyz(1,iat)\nxrel2= rxyz(2,jat)-rxyz(2,iat)\nxrel3= rxyz(3,jat)-rxyz(3,iat)\nrr2=xrel1**2 + xrel2**2 + xrel3**2\nif ( rr2 .le. cut**2 ) then\nindc=indc+1\nnearest neighbor numbers\nlstb(indc)=jat\nendif\nendif\ncontinue\nending position\nlsta(2,iat)=indc\ncontinue\n\nThis straightforward implementations has a quadratic scaling with respect to the numbers of atoms. Due to this\nscaling the calculation of the Verlet list starts to dominate the linear scaling calculation of the energies and forces\nfor system sizes of more than 10 000 atoms. It is therefore good practice to calculate the Verlet list with a modified\nalgorithm that has linear scaling [2,5] as well.\nTo do this one first subdivides the system into boxes that have a side length that is equal to or larger than cut\nand then finds all the atoms that are contained in each box. The CPU time for this first step is less than 1 percent\n2\n\n\fof the entire Verlet list calculation. Hence this part was not parallelized. It could significantly affect the parallel\nperformance according to Amdahls law [7] only if more than 50 processors are used. The largest SMP machines at\nour disposal had however only 4 processors.\nTo implement periodic boundary conditions all atoms within a distance cut of the boundary of the periodic volume\nare replicated on the opposite part as shown in Figure 1. This part is equally well less than 1 percent of the CPU\ntime for the Verlet list for a 8000 atom system. Being a surface term it becomes even smaller for larger systems.\nConsequently it wasn't parallelized either.\n\n21\n\n22\n\n13\n\n14\n\n17\n\n18\n\n21\n\n22\n\n13\n\n14\n\n23\n\n24\n\n15\n\n16\n\n19\n\n20\n\n23\n\n24\n\n15\n\n16\n\n9\n\n10\n\n1\n\n2\n\n5\n\n6\n\n9\n\n10\n\n1\n\n2\n\n11\n\n12\n\n3\n\n4\n\n7\n\n8\n\n11\n\n12\n\n3\n\n4\n\n21\n\n22\n\n13\n\n14\n\n17\n\n18\n\n21\n\n22\n\n13\n\n14\n\n23\n\n24\n\n15\n\n16\n\n19\n\n20\n\n23\n\n24\n\n15\n\n16\n\n9\n\n10\n\n1\n\n2\n\n5\n\n6\n\n9\n\n10\n\n1\n\n2\n\n11\n\n12\n\n3\n\n4\n\n7\n\n8\n\n11\n\n12\n\n3\n\n4\n\nFIG. 1. Illustration of the construction of the cell structure necessary for a linear scaling calculation of the nearest neighbor\nlist for a 2-dimensional case. The periodic volume is indicated by the dark background. The bright cells are replicated dark\ncells.\n\nAfter these two preparing steps one has to search only among all the atoms in the reference cell containing the\natom for which one wants to find its neighbors as well as all the atoms in the cells neighboring this reference cell (26\ncells in 3 dimensions). This implies that starting and ending positions lsta for the atoms 1 to nat are not calculated\nin a sequential way, necessitating, as mentioned before, separate starting and ending positions in the array lsta.\nThe corresponding parallel code is shown below. The indices l1, l2, l3 refer to the cells, icell(l1, l2, l3, 0) contains the\nnumber of atoms in cell l1, l2, l3 and icell(l1, l2, l3, \u2217) their numbering. The array rel saves the relative positions and\ndistances that will again be needed in the loop calculating the forces and energies. Each thread has its own starting\nposition iam \u2217 myspace + 1 in the shared memory space lstb and these starting positions are uniformly distributed.\nThis approach allows the different threads to work independently. The resulting speedup is much higher than the one\nthat one would obtain by calculating in the parallel version an array lstb that is identical to the one from the serial\nversion. If there are on the average more neighbors than expected (24 by default) the allocated space becomes too\nsmall. In this case the array lstb is deallocated and a new larger version is allocated. This check for sufficient memory\nrequires some minimal amount of coordination among the processors and is implemented by a critical section. If a\nreallocation is necessary, a message is written into an file to alert the user of the inefficiency due to the need of a\nsecond calculation of the Verlet list.\n\n2345\n\nallocate(lsta(2,nat))\nnnbrx=24\nnnbrx=3*nnbrx/2\nallocate(lstb(nnbrx*nat),rel(5,nnbrx*nat))\nindlstx=0\n\n!$omp\n!$omp\n!$omp\n!$omp\n\nparallel &\nprivate(iat,cut2,iam,ii,indlst,l1,l2,l3,myspace,npr) &\nshared (indlstx,nat,nn,nnbrx,ncx,ll1,ll2,ll3,icell,lsta,lstb,lay, &\nrel,rxyz,cut,myspaceout)\n3\n\n\f!$\n!$\n\nnpr=1\nnpr=omp_get_num_threads()\niam=0\niam=omp_get_thread_num()\n\ncut2=cut**2\nmyspace=(nat*nnbrx)/npr\nif (iam.eq.0) myspaceout=myspace\n! Verlet list, relative positions\nindlst=0\ndo 6000,l3=0,ll3-1\ndo 6000,l2=0,ll2-1\ndo 6000,l1=0,ll1-1\ndo 6600,ii=1,icell(0,l1,l2,l3)\niat=icell(ii,l1,l2,l3)\nif ( ((iat-1)*npr)/nat .eq. iam) then\nlsta(1,iat)=iam*myspace+indlst+1\ncall sublstiat(iat,nn,ncx,ll1,ll2,ll3,l1,l2,l3,myspace, &\nrxyz,icell,lstb(iam*myspace+1),lay,rel(1,iam*myspace+1),cut2,indlst)\nlsta(2,iat)=iam*myspace+indlst\nendif\n6600\ncontinue\n6000\ncontinue\n!$omp critical\nindlstx=max(indlstx,indlst)\n!$omp end critical\n!$omp end parallel\nif (indlstx.gt.myspaceout) then\nwrite(10,*) count,'NNBRX too\ndeallocate(lstb,rel)\ngoto 2345\nendif\n\nsmall', nnbrx\n\nsubroutine sublstiat(iat,nn,ncx,ll1,ll2,ll3,l1,l2,l3,myspace, &\nrxyz,icell,lstb,lay,rel,cut2,indlst)\nimplicit real*8 (a-h,o-z)\ndimension rxyz(3,nn),lay(nn),icell(0:ncx,-1:ll1,-1:ll2,-1:ll3), &\nlstb(0:myspace-1),rel(5,0:myspace-1)\ndo 6363,k3=l3-1,l3+1\ndo 6363,k2=l2-1,l2+1\ndo 6363,k1=l1-1,l1+1\ndo 6363,jj=1,icell(0,k1,k2,k3)\njat=icell(jj,k1,k2,k3)\nif (jat.eq.iat) goto 6363\nxrel= rxyz(1,iat)-rxyz(1,jat)\nyrel= rxyz(2,iat)-rxyz(2,jat)\nzrel= rxyz(3,iat)-rxyz(3,jat)\nrr2=xrel**2 + yrel**2 + zrel**2\nif ( rr2 .le. cut2 ) then\n4\n\n\f6363\n\nindlst=min(indlst,myspace-1)\nlstb(indlst)=lay(jat)\ntt=sqrt(rr2)\ntti=1.d0/tt\nrel(1,indlst)=xrel*tti\nrel(2,indlst)=yrel*tti\nrel(3,indlst)=zrel*tti\nrel(4,indlst)=tt\nrel(5,indlst)=tti\nindlst= indlst+1\nendif\ncontinue\nreturn\nend\nV. CALCULATION OF THE ENERGIES AND FORCES\n\nThe computationally most important part taking some 80 percent of the CPU time is the calculation of the energies\nand forces. The energy expression for the Lenosky force field is given by\n\uf8eb\n\uf8f6\nX\nX\nX\nX\n\u03c1(rij ) +\nU\uf8ed\nf (rij )f (rik )g(cos(\u03b8jik ))\uf8f8\n(1)\n\u03c6(rij ) +\nE=\ni,j\n\ni\n\nj\n\nj,k\n\nAll the functions (\u03c6, U , \u03c1, f , g) in this energy expression are given by cubic splines. The subroutine for evaluating the\ncubic spline is listed below. The case that the argument is outside the cubic spline interval [tmin, tmax] is rare and\nunimportant for performance considerations. The important cubic spline case is characterized by many dependencies.\nIn the case of such dependencies the latency of the functional unit pipeline comes into play and reduces the attainable\nspeed [7]. A latency of some 20 cycles comes from the first two statements ( tt=(x-tmin)*hi ; klo=tt) alone, requiring\narithmetic operations and a floating point to integer conversion. For this reason the calculation of tt was taken out\nof the (most likely occurring) else block to overlap its evaluation with the evaluation of the if clauses. To further\nspeed up the evaluation of the splines the structure of the energy expression 1 was exploited. In the computationally\nmost important loop over j and k two splines (f (rik ) and g(cos(\u03b8jik)) ) have to be evaluated. Inlining by hand the\nsubroutine splint for both evaluations and calculating alternatingly one step of the first spline evaluation and one\nstep of the second spline evaluation introduces two independent streams. This reduces the effect of latencies and\nboosts speed. Compilers are not able to do these complex type of optimizations. The best performance after these\noptimizations was obtained with low level compiler optimization flags (-O3 -qarch=pwr3 -qtune-pwr3 on IBM Power3,\n-O2 on the Compaq DEC Alpha, -O2 -xW on Intel Pentium4 )\nsubroutine splint(ya,y2a,tmin,tmax,hsixth,h2sixth,hi,n,x,y,yp)\nimplicit real*8 (a-h,o-z)\ndimension y2a(0:n-1),ya(0:n-1)\n! interpolate if the argument is outside the cubic spline interval [tmin,tmax]\ntt=(x-tmin)*hi\nif (x.lt.tmin) then\nyp=hi*(ya(1)-ya(0)) - &\n( y2a(1)+2.d0*y2a(0) )*hsixth\ny=ya(0) + (x-tmin)*yp\nelse if (x.gt.tmax) then\nyp=hi*(ya(n-1)-ya(n-2)) + &\n( 2.d0*y2a(n-1)+y2a(n-2) )*hsixth\ny=ya(n-1) + (x-tmax)*yp\n! otherwise evaluate cubic spline\nelse\n5\n\n\fklo=tt\nkhi=klo+1\nya_klo=ya(klo)\ny2a_klo=y2a(klo)\nb=tt-klo\na=1.d0-b\nya_khi=ya(khi)\ny2a_khi=y2a(khi)\nb2=b*b\ny=a*ya_klo\nyp=ya_khi-ya_klo\na2=a*a\ncof1=a2-1.d0\ncof2=b2-1.d0\ny=y+b*ya_khi\nyp=hi*yp\ncof3=3.d0*b2\ncof4=3.d0*a2\ncof1=a*cof1\ncof2=b*cof2\ncof3=cof3-1.d0\ncof4=cof4-1.d0\nyt1=cof1*y2a_klo\nyt2=cof2*y2a_khi\nypt1=cof3*y2a_khi\nypt2=cof4*y2a_klo\ny=y + (yt1+yt2)*h2sixth\nyp=yp + ( ypt1 - ypt2 )*hsixth\nendif\nreturn\nend\nThe final single processor performance for the entire subroutine is 460 Mflops on a Compaq DEC Alpha at 833\nMHz, 300 Mflops on a IBM Power3 at 350 MHz and 550 Mflops on a Pentium 4. In order to obtain a high parallel\nspeedup in this central part of the subroutine the threads are completely decoupled. This was done by introducing\nprivate copies for each thread to accumulate the energies tener and forces txyz. The global energy and force are\nsummed up in an additional loop at the end of the parallel region in a critical section.\n!$omp\n!$omp\n!$omp\n!$omp\n\n!$\n!$\n\nparallel &\nprivate(iam,npr,iat,iat1,iat2,lot,istop,tcoord,tcoord2, &\ntener,tener2,txyz,f2ij,f3ij,f3ik,npjx,npjkx) &\nshared (nat,nnbrx,lsta,lstb,rel,ener,ener2,fxyz,coord,coord2,istopg)\nnpr=1\nnpr=omp_get_num_threads()\niam=0\niam=omp_get_thread_num()\nnpjx=300 ; npjkx=3000\nistopg=0\n\nif (npr.ne.1) then\n! PARALLEL CASE\n! create temporary private scalars for reduction sum on energies and\n!\ntemporary private array for reduction sum on forces\n6\n\n\f!$omp critical\nallocate(txyz(3,nat),f2ij(3,npjx),f3ij(3,npjkx),f3ik(3,npjkx))\n!$omp end critical\nif (iam.eq.0) then\nener=0.d0\nener2=0.d0\ncoord=0.d0\ncoord2=0.d0\ndo 121,iat=1,nat\nfxyz(1,iat)=0.d0\nfxyz(2,iat)=0.d0\n121\nfxyz(3,iat)=0.d0\nendif\nlot=nat/npr+.999999999999d0\niat1=iam*lot+1\niat2=min((iam+1)*lot,nat)\ncall subfeniat(iat1,iat2,nat,lsta,lstb,rel,tener,tener2, &\ntcoord,tcoord2,nnbrx,txyz,f2ij,npjx,f3ij,npjkx,f3ik,istop)\n!$omp critical\nener=ener+tener\nener2=ener2+tener2\ncoord=coord+tcoord\ncoord2=coord2+tcoord2\nistopg=istopg+istop\ndo 8093,iat=1,nat\nfxyz(1,iat)=fxyz(1,iat)+txyz(1,iat)\nfxyz(2,iat)=fxyz(2,iat)+txyz(2,iat)\nfxyz(3,iat)=fxyz(3,iat)+txyz(3,iat)\n8093\ncontinue\n!$omp end critical\ndeallocate(txyz,f2ij,f3ij,f3ik)\nelse\n! SERIAL CASE\niat1=1\niat2=nat\nallocate(f2ij(3,npjx),f3ij(3,npjkx),f3ik(3,npjkx))\ncall subfeniat(iat1,iat2,nat,lsta,lstb,rel,ener,ener2, &\ncoord,coord2,nnbrx,fxyz,f2ij,npjx,f3ij,npjkx,f3ik,istop)\ndeallocate(f2ij,f3ij,f3ik)\nendif\n!$omp end parallel\nif (istopg.gt.0) stop 'DIMENSION ERROR (see WARNING above)'\nener_var=ener2/nat-(ener/nat)**2\ncoord=coord/nat\ncoord_var=coord2/nat-coord**2\ndeallocate(rxyz,icell,lay,lsta,lstb,rel)\nend\n\nsubroutine subfeniat(iat1,iat2,nat,lsta,lstb,rel,tener,tener2, &\n7\n\n\ftcoord,tcoord2,nnbrx,txyz,f2ij,npjx,f3ij,npjkx,f3ik,istop)\nimplicit real*8 (a-h,o-z)\ndimension lsta(2,nat),lstb(nnbrx*nat),rel(5,nnbrx*nat),txyz(3,nat)\ndimension f2ij(3,npjx),f3ij(3,npjkx),f3ik(3,npjkx)\ninitialize data ........\n! create temporary private scalars for reduction sum on energies and\ntener=0.d0\ntener2=0.d0\ntcoord=0.d0\ntcoord2=0.d0\nistop=0\ndo 121,iat=1,nat\ntxyz(1,iat)=0.d0\ntxyz(2,iat)=0.d0\n121\ntxyz(3,iat)=0.d0\n! calculation of forces, energy\ndo 1000,iat=iat1,iat2\ndens2=0.d0\ndens3=0.d0\njcnt=0\njkcnt=0\ncoord_iat=0.d0\nener_iat=0.d0\ndo 2000,jbr=lsta(1,iat),lsta(2,iat)\njat=lstb(jbr)\njcnt=jcnt+1\nif (jcnt.gt.npjx) then\nwrite(6,*) 'WARNING: enlarge npjx'\nistop=1\nendif\nfxij=rel(1,jbr)\nfyij=rel(2,jbr)\nfzij=rel(3,jbr)\nrij=rel(4,jbr)\nsij=rel(5,jbr)\n! coordination number calculated with soft cutoff between first and\n! second nearest neighbor\nif (rij.le.2.36d0) then\ncoord_iat=coord_iat+1.d0\nelse if (rij.ge.3.83d0) then\nelse\nx=(rij-2.36d0)*(1.d0/(3.83d0-2.36d0))\ncoord_iat=coord_iat+(2*x+1.d0)*(x-1.d0)**2\nendif\n! pairpotential term\ncall splint(cof_phi,dof_phi,tmin_phi,tmax_phi, &\nhsixth_phi,h2sixth_phi,hi_phi,10,rij,e_phi,ep_phi)\nener_iat=ener_iat+(e_phi*.5d0)\n8\n\n\ftxyz(1,iat)=txyz(1,iat)-fxij*(ep_phi*.5d0)\ntxyz(2,iat)=txyz(2,iat)-fyij*(ep_phi*.5d0)\ntxyz(3,iat)=txyz(3,iat)-fzij*(ep_phi*.5d0)\ntxyz(1,jat)=txyz(1,jat)+fxij*(ep_phi*.5d0)\ntxyz(2,jat)=txyz(2,jat)+fyij*(ep_phi*.5d0)\ntxyz(3,jat)=txyz(3,jat)+fzij*(ep_phi*.5d0)\n! 2 body embedding term\ncall splint(cof_rho,dof_rho,tmin_rho,tmax_rho, &\nhsixth_rho,h2sixth_rho,hi_rho,11,rij,rho,rhop)\ndens2=dens2+rho\nf2ij(1,jcnt)=fxij*rhop\nf2ij(2,jcnt)=fyij*rhop\nf2ij(3,jcnt)=fzij*rhop\n! 3 body embedding term\ncall splint(cof_fff,dof_fff,tmin_fff,tmax_fff, &\nhsixth_fff,h2sixth_fff,hi_fff,10,rij,fij,fijp)\ndo 3000,kbr=lsta(1,iat),lsta(2,iat)\nkat=lstb(kbr)\nif (kat.lt.jat) then\njkcnt=jkcnt+1\nif (jkcnt.gt.npjkx) then\nwrite(6,*) 'WARNING: enlarge npjkx'\nistop=1\nendif\n! begin optimized version\nrik=rel(4,kbr)\nif (rik.gt.tmax_fff) then\nfikp=0.d0 ; fik=0.d0\ngjik=0.d0 ; gjikp=0.d0 ; sik=0.d0\ncostheta=0.d0 ; fxik=0.d0 ; fyik=0.d0 ; fzik=0.d0\nelse if (rik.lt.tmin_fff) then\nfxik=rel(1,kbr)\nfyik=rel(2,kbr)\nfzik=rel(3,kbr)\ncostheta=fxij*fxik+fyij*fyik+fzij*fzik\nsik=rel(5,kbr)\nfikp=hi_fff*(cof_fff(1)-cof_fff(0)) - &\n( dof_fff(1)+2.d0*dof_fff(0) )*hsixth_fff\nfik=cof_fff(0) + (rik-tmin_fff)*fikp\ntt_ggg=(costheta-tmin_ggg)*hi_ggg\nif (costheta.gt.tmax_ggg) then\ngjikp=hi_ggg*(cof_ggg(8-1)-cof_ggg(8-2)) + &\n( 2.d0*dof_ggg(8-1)+dof_ggg(8-2) )*hsixth_ggg\ngjik=cof_ggg(8-1) + (costheta-tmax_ggg)*gjikp\nelse\nklo_ggg=tt_ggg\nkhi_ggg=klo_ggg+1\ncof_ggg_klo=cof_ggg(klo_ggg)\ndof_ggg_klo=dof_ggg(klo_ggg)\nb_ggg=tt_ggg-klo_ggg\na_ggg=1.d0-b_ggg\ncof_ggg_khi=cof_ggg(khi_ggg)\n9\n\n\fdof_ggg_khi=dof_ggg(khi_ggg)\nb2_ggg=b_ggg*b_ggg\ngjik=a_ggg*cof_ggg_klo\ngjikp=cof_ggg_khi-cof_ggg_klo\na2_ggg=a_ggg*a_ggg\ncof1_ggg=a2_ggg-1.d0\ncof2_ggg=b2_ggg-1.d0\ngjik=gjik+b_ggg*cof_ggg_khi\ngjikp=hi_ggg*gjikp\ncof3_ggg=3.d0*b2_ggg\ncof4_ggg=3.d0*a2_ggg\ncof1_ggg=a_ggg*cof1_ggg\ncof2_ggg=b_ggg*cof2_ggg\ncof3_ggg=cof3_ggg-1.d0\ncof4_ggg=cof4_ggg-1.d0\nyt1_ggg=cof1_ggg*dof_ggg_klo\nyt2_ggg=cof2_ggg*dof_ggg_khi\nypt1_ggg=cof3_ggg*dof_ggg_khi\nypt2_ggg=cof4_ggg*dof_ggg_klo\ngjik=gjik + (yt1_ggg+yt2_ggg)*h2sixth_ggg\ngjikp=gjikp + ( ypt1_ggg - ypt2_ggg )*hsixth_ggg\nendif\nelse\nfxik=rel(1,kbr)\ntt_fff=rik-tmin_fff\ncostheta=fxij*fxik\nfyik=rel(2,kbr)\ntt_fff=tt_fff*hi_fff\ncostheta=costheta+fyij*fyik\nfzik=rel(3,kbr)\nklo_fff=tt_fff\ncostheta=costheta+fzij*fzik\nsik=rel(5,kbr)\ntt_ggg=(costheta-tmin_ggg)*hi_ggg\nif (costheta.gt.tmax_ggg) then\ngjikp=hi_ggg*(cof_ggg(8-1)-cof_ggg(8-2)) + &\n( 2.d0*dof_ggg(8-1)+dof_ggg(8-2) )*hsixth_ggg\ngjik=cof_ggg(8-1) + (costheta-tmax_ggg)*gjikp\nkhi_fff=klo_fff+1\ncof_fff_klo=cof_fff(klo_fff)\ndof_fff_klo=dof_fff(klo_fff)\nb_fff=tt_fff-klo_fff\na_fff=1.d0-b_fff\ncof_fff_khi=cof_fff(khi_fff)\ndof_fff_khi=dof_fff(khi_fff)\nb2_fff=b_fff*b_fff\nfik=a_fff*cof_fff_klo\nfikp=cof_fff_khi-cof_fff_klo\na2_fff=a_fff*a_fff\ncof1_fff=a2_fff-1.d0\ncof2_fff=b2_fff-1.d0\nfik=fik+b_fff*cof_fff_khi\nfikp=hi_fff*fikp\ncof3_fff=3.d0*b2_fff\ncof4_fff=3.d0*a2_fff\ncof1_fff=a_fff*cof1_fff\n10\n\n\fcof2_fff=b_fff*cof2_fff\ncof3_fff=cof3_fff-1.d0\ncof4_fff=cof4_fff-1.d0\nyt1_fff=cof1_fff*dof_fff_klo\nyt2_fff=cof2_fff*dof_fff_khi\nypt1_fff=cof3_fff*dof_fff_khi\nypt2_fff=cof4_fff*dof_fff_klo\nfik=fik + (yt1_fff+yt2_fff)*h2sixth_fff\nfikp=fikp + ( ypt1_fff - ypt2_fff )*hsixth_fff\nelse\nklo_ggg=tt_ggg\nkhi_ggg=klo_ggg+1\nkhi_fff=klo_fff+1\ncof_ggg_klo=cof_ggg(klo_ggg)\ncof_fff_klo=cof_fff(klo_fff)\ndof_ggg_klo=dof_ggg(klo_ggg)\ndof_fff_klo=dof_fff(klo_fff)\nb_ggg=tt_ggg-klo_ggg\nb_fff=tt_fff-klo_fff\na_ggg=1.d0-b_ggg\na_fff=1.d0-b_fff\ncof_ggg_khi=cof_ggg(khi_ggg)\ncof_fff_khi=cof_fff(khi_fff)\ndof_ggg_khi=dof_ggg(khi_ggg)\ndof_fff_khi=dof_fff(khi_fff)\nb2_ggg=b_ggg*b_ggg\nb2_fff=b_fff*b_fff\ngjik=a_ggg*cof_ggg_klo\nfik=a_fff*cof_fff_klo\ngjikp=cof_ggg_khi-cof_ggg_klo\nfikp=cof_fff_khi-cof_fff_klo\na2_ggg=a_ggg*a_ggg\na2_fff=a_fff*a_fff\ncof1_ggg=a2_ggg-1.d0\ncof1_fff=a2_fff-1.d0\ncof2_ggg=b2_ggg-1.d0\ncof2_fff=b2_fff-1.d0\ngjik=gjik+b_ggg*cof_ggg_khi\nfik=fik+b_fff*cof_fff_khi\ngjikp=hi_ggg*gjikp\nfikp=hi_fff*fikp\ncof3_ggg=3.d0*b2_ggg\ncof3_fff=3.d0*b2_fff\ncof4_ggg=3.d0*a2_ggg\ncof4_fff=3.d0*a2_fff\ncof1_ggg=a_ggg*cof1_ggg\ncof1_fff=a_fff*cof1_fff\ncof2_ggg=b_ggg*cof2_ggg\ncof2_fff=b_fff*cof2_fff\ncof3_ggg=cof3_ggg-1.d0\ncof3_fff=cof3_fff-1.d0\ncof4_ggg=cof4_ggg-1.d0\ncof4_fff=cof4_fff-1.d0\nyt1_ggg=cof1_ggg*dof_ggg_klo\nyt1_fff=cof1_fff*dof_fff_klo\nyt2_ggg=cof2_ggg*dof_ggg_khi\n11\n\n\fyt2_fff=cof2_fff*dof_fff_khi\nypt1_ggg=cof3_ggg*dof_ggg_khi\nypt1_fff=cof3_fff*dof_fff_khi\nypt2_ggg=cof4_ggg*dof_ggg_klo\nypt2_fff=cof4_fff*dof_fff_klo\ngjik=gjik + (yt1_ggg+yt2_ggg)*h2sixth_ggg\nfik=fik + (yt1_fff+yt2_fff)*h2sixth_fff\ngjikp=gjikp + ( ypt1_ggg - ypt2_ggg )*hsixth_ggg\nfikp=fikp + ( ypt1_fff - ypt2_fff )*hsixth_fff\nendif\nendif\n! end optimized version\ntt=fij*fik\ndens3=dens3+tt*gjik\nt1=fijp*fik*gjik\nt2=sij*(tt*gjikp)\nf3ij(1,jkcnt)=fxij*t1 + (fxik-fxij*costheta)*t2\nf3ij(2,jkcnt)=fyij*t1 + (fyik-fyij*costheta)*t2\nf3ij(3,jkcnt)=fzij*t1 + (fzik-fzij*costheta)*t2\n\n3000\n2000\n\nt3=fikp*fij*gjik\nt4=sik*(tt*gjikp)\nf3ik(1,jkcnt)=fxik*t3 + (fxij-fxik*costheta)*t4\nf3ik(2,jkcnt)=fyik*t3 + (fyij-fyik*costheta)*t4\nf3ik(3,jkcnt)=fzik*t3 + (fzij-fzik*costheta)*t4\nendif\ncontinue\ncontinue\ndens=dens2+dens3\ncall splint(cof_uuu,dof_uuu,tmin_uuu,tmax_uuu, &\nhsixth_uuu,h2sixth_uuu,hi_uuu,8,dens,e_uuu,ep_uuu)\nener_iat=ener_iat+e_uuu\n\n! Only now ep_uu is known and the forces can be calculated, lets loop again\njcnt=0\njkcnt=0\ndo 2200,jbr=lsta(1,iat),lsta(2,iat)\njat=lstb(jbr)\njcnt=jcnt+1\ntxyz(1,iat)=txyz(1,iat)-ep_uuu*f2ij(1,jcnt)\ntxyz(2,iat)=txyz(2,iat)-ep_uuu*f2ij(2,jcnt)\ntxyz(3,iat)=txyz(3,iat)-ep_uuu*f2ij(3,jcnt)\ntxyz(1,jat)=txyz(1,jat)+ep_uuu*f2ij(1,jcnt)\ntxyz(2,jat)=txyz(2,jat)+ep_uuu*f2ij(2,jcnt)\ntxyz(3,jat)=txyz(3,jat)+ep_uuu*f2ij(3,jcnt)\n! 3 body embedding term\ndo 3300,kbr=lsta(1,iat),lsta(2,iat)\nkat=lstb(kbr)\nif (kat.lt.jat) then\njkcnt=jkcnt+1\ntxyz(1,iat)=txyz(1,iat)-ep_uuu*(f3ij(1,jkcnt)+f3ik(1,jkcnt))\ntxyz(2,iat)=txyz(2,iat)-ep_uuu*(f3ij(2,jkcnt)+f3ik(2,jkcnt))\n12\n\n\ftxyz(3,iat)=txyz(3,iat)-ep_uuu*(f3ij(3,jkcnt)+f3ik(3,jkcnt))\ntxyz(1,jat)=txyz(1,jat)+ep_uuu*f3ij(1,jkcnt)\ntxyz(2,jat)=txyz(2,jat)+ep_uuu*f3ij(2,jkcnt)\ntxyz(3,jat)=txyz(3,jat)+ep_uuu*f3ij(3,jkcnt)\ntxyz(1,kat)=txyz(1,kat)+ep_uuu*f3ik(1,jkcnt)\ntxyz(2,kat)=txyz(2,kat)+ep_uuu*f3ik(2,jkcnt)\ntxyz(3,kat)=txyz(3,kat)+ep_uuu*f3ik(3,jkcnt)\n\n3300\n2200\n\nendif\ncontinue\ncontinue\ntener=tener+ener_iat\ntener2=tener2+ener_iat**2\ntcoord=tcoord+coord_iat\ntcoord2=tcoord2+coord_iat**2\n\n1000\n\ncontinue\nreturn\nend\n\nIn addition to the energy and the forces the program still returns the coordination number as well as the variance of\nthe energy per atom and the coordination number. The coordination number is calculated using a soft cutoff between\nthe first and second nearest neighbor distance. These extra calculations are very cheap and not visible as an increase\nin the CPU time\nVI. PARALLEL PERFORMANCE RESULTS\n\nTable I shows the final overall speedups obtained by the program. The results were obtained for an 8000 atom\nsystem, but the CPU time per call and atom is nearly independent of system size.\nTABLE I. Timings in \u03bcsec for a combined evaluation of the forces and the energy per particle as well as the corresponding\nspeedups (in parentheses) on an IBM SP3 based on a 375 MHz Power3 processor, on a Compaq SC 232 based on a 833Mhz\nEV67 processor and on an Intel Pentium4 biprocessor at 2 GHz\nnumber of processors\n1\n2\n4\n8\n\nIBM Power3\n46\n24 (1.9)\n13 (3.5)\n7.7 (6.0)\n\nCompaq EV67\n30\n16 (1.9)\n8.6 (3.5)\n\n13\n\nIntel P4\n25\n13 (1.9)\n\n\fObtaining such high speedups was not straightforward. Only the Compaq Fortran90 compiler was able to use in the\noriginal version of the program the OpenMP 'parallel do' directive to obtain a good speedup. Both the IBM compiler\nand the Intel compiler failed. In order to get the performances of Table I, it was necessary to encapsulate the workload\nof the different threads into the subroutines 'sublstias' and 'subfen', which amounts to doing the parallelization quasi\nby hand. Using allocatable arrays in connection with OpenMP turned also out to be tricky. Because of these problems,\nthe parallelization was much more painful that one might expect for a shared memory model.\nVII. CONCLUSIONS\n\nThe results show that simulations for very large silicon systems are feasible on relatively cheap serial or parallel\ncomputers accessible to a large number of researches.\nI thank Tom Lenosky for his help in the implementation of the interatomic potential and Cyrus Umrigar for\nproviding me with the timings on the IBM machine.\n\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n\nW. Kohn, Rev. Mod. Phys., 718, 1253 (1999)\nM. Allen and D. Tildesley, Computer Simulations of Liquids, Claredon Press, Oxford, 1987\nD. Beazley and P. Lomdahl, Parallel Computing 20, 173 (1994);\nR. Couturier and C. Chipot, Comp. Phys. Commun. 14, 49 (2000);\nR. Hockney and J. Eastwood, Computer Simulations using Particles, McGraw Hill, New York, 1981\nT. J. Lenosky, B. Sadigh, E. Alonso, V. Bulatov, T. Diaz de la Rubia, J. Kim, A.F. Voter adn J. D. Kress, Modelling Simul.\nMater. Sci. Eng. 8, 825 (2000);\n[7] S. Goedecker, A. Hoisie, Performance Optimization of numerically intensive codes, SIAM publishing company, Philadelphia,\nUSA 2001 (ISBN 0-89871-484-2)\n\n14\n\n\f"}