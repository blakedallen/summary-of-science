{"id": "http://arxiv.org/abs/1108.5037v2", "guidislink": true, "updated": "2011-09-09T04:31:58Z", "updated_parsed": [2011, 9, 9, 4, 31, 58, 4, 252, 0], "published": "2011-08-25T08:35:39Z", "published_parsed": [2011, 8, 25, 8, 35, 39, 3, 237, 0], "title": "Orthonormal Expansion l1-Minimization Algorithms for Compressed Sensing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.5706%2C1108.4916%2C1108.4763%2C1108.3730%2C1108.0116%2C1108.5625%2C1108.2325%2C1108.2752%2C1108.1895%2C1108.0830%2C1108.5422%2C1108.5912%2C1108.3731%2C1108.2854%2C1108.2495%2C1108.2118%2C1108.2133%2C1108.1804%2C1108.1502%2C1108.0876%2C1108.2630%2C1108.1245%2C1108.0721%2C1108.1020%2C1108.5071%2C1108.3676%2C1108.3507%2C1108.5894%2C1108.5547%2C1108.4296%2C1108.0913%2C1108.5037%2C1108.4967%2C1108.3252%2C1108.2743%2C1108.1255%2C1108.5757%2C1108.2635%2C1108.5298%2C1108.5968%2C1108.2248%2C1108.4679%2C1108.2740%2C1108.5620%2C1108.4820%2C1108.1712%2C1108.4588%2C1108.3005%2C1108.5667%2C1108.0979%2C1108.0616%2C1108.4034%2C1108.5150%2C1108.0546%2C1108.3221%2C1108.0728%2C1108.4320%2C1108.4093%2C1108.3554%2C1108.5198%2C1108.6306%2C1108.4081%2C1108.5027%2C1108.4082%2C1108.1397%2C1108.6039%2C1108.3964%2C1108.5244%2C1108.2681%2C1108.5585%2C1108.2015%2C1108.2050%2C1108.5320%2C1108.1018%2C1108.1104%2C1108.3264%2C1108.4443%2C1108.0086%2C1108.4304%2C1108.3572%2C1108.4016%2C1108.0262%2C1108.1024%2C1108.4477%2C1108.5432%2C1108.0576%2C1108.0457%2C1108.3294%2C1108.0935%2C1108.3631%2C1108.4322%2C1108.4746%2C1108.3769%2C1108.5822%2C1108.5844%2C1108.4070%2C1108.2270%2C1108.1154%2C1108.0349%2C1108.6162%2C1108.0188&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Orthonormal Expansion l1-Minimization Algorithms for Compressed Sensing"}, "summary": "Compressed sensing aims at reconstructing sparse signals from significantly\nreduced number of samples, and a popular reconstruction approach is\n$\\ell_1$-norm minimization. In this correspondence, a method called orthonormal\nexpansion is presented to reformulate the basis pursuit problem for noiseless\ncompressed sensing. Two algorithms are proposed based on convex optimization:\none exactly solves the problem and the other is a relaxed version of the first\none. The latter can be considered as a modified iterative soft thresholding\nalgorithm and is easy to implement. Numerical simulation shows that, in dealing\nwith noise-free measurements of sparse signals, the relaxed version is\naccurate, fast and competitive to the recent state-of-the-art algorithms. Its\npractical application is demonstrated in a more general case where signals of\ninterest are approximately sparse and measurements are contaminated with noise.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.5706%2C1108.4916%2C1108.4763%2C1108.3730%2C1108.0116%2C1108.5625%2C1108.2325%2C1108.2752%2C1108.1895%2C1108.0830%2C1108.5422%2C1108.5912%2C1108.3731%2C1108.2854%2C1108.2495%2C1108.2118%2C1108.2133%2C1108.1804%2C1108.1502%2C1108.0876%2C1108.2630%2C1108.1245%2C1108.0721%2C1108.1020%2C1108.5071%2C1108.3676%2C1108.3507%2C1108.5894%2C1108.5547%2C1108.4296%2C1108.0913%2C1108.5037%2C1108.4967%2C1108.3252%2C1108.2743%2C1108.1255%2C1108.5757%2C1108.2635%2C1108.5298%2C1108.5968%2C1108.2248%2C1108.4679%2C1108.2740%2C1108.5620%2C1108.4820%2C1108.1712%2C1108.4588%2C1108.3005%2C1108.5667%2C1108.0979%2C1108.0616%2C1108.4034%2C1108.5150%2C1108.0546%2C1108.3221%2C1108.0728%2C1108.4320%2C1108.4093%2C1108.3554%2C1108.5198%2C1108.6306%2C1108.4081%2C1108.5027%2C1108.4082%2C1108.1397%2C1108.6039%2C1108.3964%2C1108.5244%2C1108.2681%2C1108.5585%2C1108.2015%2C1108.2050%2C1108.5320%2C1108.1018%2C1108.1104%2C1108.3264%2C1108.4443%2C1108.0086%2C1108.4304%2C1108.3572%2C1108.4016%2C1108.0262%2C1108.1024%2C1108.4477%2C1108.5432%2C1108.0576%2C1108.0457%2C1108.3294%2C1108.0935%2C1108.3631%2C1108.4322%2C1108.4746%2C1108.3769%2C1108.5822%2C1108.5844%2C1108.4070%2C1108.2270%2C1108.1154%2C1108.0349%2C1108.6162%2C1108.0188&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Compressed sensing aims at reconstructing sparse signals from significantly\nreduced number of samples, and a popular reconstruction approach is\n$\\ell_1$-norm minimization. In this correspondence, a method called orthonormal\nexpansion is presented to reformulate the basis pursuit problem for noiseless\ncompressed sensing. Two algorithms are proposed based on convex optimization:\none exactly solves the problem and the other is a relaxed version of the first\none. The latter can be considered as a modified iterative soft thresholding\nalgorithm and is easy to implement. Numerical simulation shows that, in dealing\nwith noise-free measurements of sparse signals, the relaxed version is\naccurate, fast and competitive to the recent state-of-the-art algorithms. Its\npractical application is demonstrated in a more general case where signals of\ninterest are approximately sparse and measurements are contaminated with noise."}, "authors": ["Zai Yang", "Cishen Zhang", "Jun Deng", "Wenmiao Lu"], "author_detail": {"name": "Wenmiao Lu"}, "author": "Wenmiao Lu", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TSP.2011.2168216", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1108.5037v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1108.5037v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "7 pages, 2 figures, 1 table", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1108.5037v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1108.5037v2", "journal_reference": "IEEE Transactions on Signal Processing, vol. 59, no. 12, pp.\n  6285-6290, 2011", "doi": "10.1109/TSP.2011.2168216", "fulltext": "1\n\nOrthonormal Expansion `1-Minimization Algorithms\nfor Compressed Sensing\n\narXiv:1108.5037v2 [cs.IT] 9 Sep 2011\n\nZai Yang, Cishen Zhang, Jun Deng, and Wenmiao Lu\n\nAbstract-Compressed sensing aims at reconstructing sparse\nsignals from significantly reduced number of samples, and a\npopular reconstruction approach is `1 -norm minimization. In\nthis correspondence, a method called orthonormal expansion is\npresented to reformulate the basis pursuit problem for noiseless\ncompressed sensing. Two algorithms are proposed based on\nconvex optimization: one exactly solves the problem and the other\nis a relaxed version of the first one. The latter can be considered\nas a modified iterative soft thresholding algorithm and is easy\nto implement. Numerical simulation shows that, in dealing with\nnoise-free measurements of sparse signals, the relaxed version\nis accurate, fast and competitive to the recent state-of-the-art\nalgorithms. Its practical application is demonstrated in a more\ngeneral case where signals of interest are approximately sparse\nand measurements are contaminated with noise.\nIndex Terms-Augmented Lagrange multiplier, Compressed\nsensing, `1 minimization, Orthonormal expansion, Phase transition, Sparsity-undersampling tradeoff.\n\nI. I NTRODUCTION\nOMPRESSED sensing (CS) aims at reconstructing a\nsignal from its significantly reduced measurements with\na priori knowledge that the signal is (approximately) sparse\n[1]\u2013[3]. In CS, the signal xo \u2208 RN of interest is acquired by\ntaking measurements of the form\n\nC\n\nb = Axo + e,\nwhere A \u2208 Rn\u00d7N is the sampling matrix, b \u2208 Rn is the vector\nof our measurements or samples, e \u2208 Rn is the measurement\nnoise, with N and n being sizes of the signal and acquired\nsamples, respectively. A standard approach to reconstructing\nthe original signal xo is to solve\n(BP\u000f )\n\nmin kxk1 , subject to kb \u2212 Axk2 \u2264 \u000f,\nx\n\nwhich is known as the basis pursuit denoising (BPDN) problem, with kek2 \u2264 \u000f. Another frequently discussed approach is\nto solve the problem in its Lagrangian form\n\u001a\n\u001b\n1\n2\n(QP\u03bb ) min \u03bb kxk1 + kb \u2212 Axk2 .\nx\n2\nTo appear in IEEE Transactions on Signal Processing. Copyright (c) 2011\nIEEE. Personal use of this material is permitted. However, permission to use\nthis material for any other purposes must be obtained from the IEEE by\nsending a request to pubs-permissions@ieee.org.\nZ. Yang, J. Deng and W. Lu are with the School of Electrical and Electronic\nEngineering, Nanyang Technological University, 639798, Singapore (e-mail:\nyang0248@e.ntu.edu.sg; de0001un@e.ntu.edu.sg; wenmiao@ntu.edu.sg).\nC. Zhang is with the Faculty of Engineering and Industrial Sciences,\nSwinburne University of Technology, Hawthorn VIC 3122, Australia (e-mail:\ncishenzhang@swin.edu.au).\n\nIt follows from the knowledge of convex optimization that\n(BP\u000f ) and (QP\u03bb ) are equivalent with appropriate choices of \u000f\nand \u03bb. In general, \u03bb decreases as \u000f decreases. In the limiting\ncase of \u03bb, \u000f \u2192 0, both (BP\u000f ) and (QP\u03bb ) converges to the\nfollowing basis pursuit (BP) problem in noiseless CS:\n(BP)\n\nmin kxk1 , subject to Ax = b.\nx\n\nIt is important to develop accurate and computationally\nefficient algorithms to deal with the `1 minimization problems\nof high dimensional signals in CS, such as an image of\n512 \u00d7 512 pixels. One popular approach for solving (QP\u03bb )\nis the iterative soft thresholding (IST) algorithm of the form\n(stating from x0 = 0) [4], [5]\n\u0001\nxt+1 = S\u03bb xt + A0 zt , zt = b \u2212 Axt ,\n(1)\nwhere 0 denotes the transpose operator and S\u03bb (*) is the soft\nthresholding operator with a threshold \u03bb, which will be defined\nmore precisely in Section II-A. IST has a concise form and\nis easy to implement, but its convergence can be very slow\nin some cases [5], especially for small \u03bb. Other algorithms\nproposed to solve (QP\u03bb ) include interior-point method [6],\nconjugate gradient method [7] and fixed-point continuation\n[8]. Few algorithms can accurately solve large-scale BPDN\nproblem (BP\u000f ) with a low computational complexity. The `1 magic package [9] includes a primal log barrier code solving\n(BP\u000f ), in which the conjugate gradient method may not find\na precise Newton step in the large-scale mode. NESTA [10]\napproximately solves (BP\u000f ) based on Nesterov's smoothing\ntechnique [11], with continuation.\nIn the case of strictly sparse signals and noise-free measurements, many fast algorithms have been proposed to exactly\nreconstruct xo . One class of algorithms uses the greedy pursuit\nmethod, which iteratively refines the support and entries of\na sparse solution to yield a better approximation of xo ,\nsuch as OMP [12], StOMP [13] and CoSaMP [14]. These\nalgorithms, however, may not produce satisfactory sparsityundersampling tradeoff compared with (BP) because of their\ngreedy operations. As mentioned before, (QP\u03bb ) is equivalent\nto (BP) as \u03bb \u2192 0. Hence, (BP) can be solved with high\naccuracy using algorithms for (QP\u03bb ) by setting \u03bb to a small\nvalue, e.g. 1 \u00d7 10\u22126 . IST has attracted much attention because\nof its simple form. In the case where \u03bb is small, however,\nthe standard IST in (1) can be very slow. To improve its\nspeed, a fixed-point continuation (FPC) strategy is exploited\n[8], in which \u03bb is decreased in a continuation scheme and a\nq-linear convergence rate is achieved. Further, FPC-AS [15] is\ndeveloped to improve the performance of FPC by introducing\n\n\f2\n\nan active set, inspired by greedy pursuit algorithms. An\nalternative approach to improving the speed of IST is to use\nan aggressive continuation, which takes the form\n0\n\n\u0001\n\nxt+1 = S\u03bbt xt + A zt ,\n\nzt = b \u2212 Axt ,\n\n(2)\n\nwhere \u03bbt may decrease in each iteration. The algorithm of this\nform typically has a worse sparsity-undersampling tradeoff\nthan (BP) [16]. Such a disadvantage is partially overcome\nby approximately message passing (AMP) algorithm [17], in\nwhich a modification is introduced in the current residual zt :\n\u0001\nxt+1 = S\u03bbt xt + A0 zt ,\n\nzt = b \u2212 Axt +\n\nN kxt k0\nzt\u22121 , (3)\nn\n\nwhere kxk0 counts the number of nonzero entries of x. It\nis noted that AMP having the same spasity-undersampling\ntradeoff as (BP) is only established based on heuristic arguments and numerical simulations. Moreover, it cannot be easily\nextended to deal with more general complex-valued sparse\nsignals, though real-valued signals are only considered in this\ncorrespondence.\nThis correspondence focuses on solving the basis pursuit\nproblem (BP) in noiseless CS. We assume that AA0 is an\nidentity matrix, i.e., the rows of A are orthonormal. This is\nreasonable since most fast transforms in CS are of this form,\nsuch as discrete cosine transform (DCT), discrete Fourier\ntransform (DFT) and some wavelet transforms, e.g. Haar\nwavelet transform. Such an assumption has also been used\nin other algorithms, e.g. NESTA. A novel method called orthonormal expansion is introduced to reformulate (BP) based\non this assumption. The exact OrthoNormal Expansion `1\nminimization (eONE-L1) algorithm is then proposed to exactly solve (BP) based on the augmented Lagrange multiplier\n(ALM) method, which is a convex optimization method.\nThe relaxed ONE-L1 (rONE-L1) algorithm is further developed to simplify eONE-L1. It is shown that rONE-L1\nconverges at least exponentially and is in the form of modified\nIST in (2). In the case of strictly sparse signals and noisefree measurements, numerical simulations show that rONE-L1\nhas the same sparsity-undersampling tradeoff as (BP) does.\nUnder the same conditions, rONE-L1 is compared with stateof-the-art algorithms, including FPC-AS, AMP and NESTA.\nIt is shown that rONE-L1 is faster than AMP and NESTA\nwhen the number of measurements is just enough to exactly\nreconstruct the original sparse signal using `1 minimization.\nIn a general case of approximately sparse signals and noisecontaminated measurements, where AMP is omitted for its\npoor performance, an example of 2D image reconstruction\nfrom its partial-DCT measurements demonstrates that rONEL1 outperforms NESTA and FPC-AS in terms of computational efficiency and reconstruction quality, respectively.\nThe rest of the correspondence is organized as follows.\nSection II introduces the proposed exact and relaxed ONE-L1\nalgorithms followed by their implementation details. Section\nIII reports the efficiency of our algorithm through numerical\nsimulations in both noise-free and noise-contaminated cases.\nConclusions are drawn in Section IV.\n\nII. ONE-L1 A LGORITHMS\nA. Preliminary: Soft Thresholding Operator\nFor w \u2208 R, the soft thresholding of w with a threshold\n\u03bb \u2208 R+ is defined as:\nS\u03bb (w) = sgn (w) * (|w| \u2212 \u03bb)+ ,\nwhere (*)+ = max(*, 0) and\n\u001a\nw/ |w| ,\nsgn (w) =\n0,\n\nw=\n6 0;\nw = 0.\n\nThe operator S\u03bb (*) can be extended to vector variables by its\nelement-wise operation.\nThe soft thresholding operator can be applied to solve the\nfollowing `1 -norm regularized least square problem [4], i.e.,\n\u001b\n\u001a\n1\n2\n(4)\nS\u03bb (w) = arg min \u03bb kvk1 + kw \u2212 vk2 .\nv\n2\nwhere v, w \u2208 Rn , and S\u03bb (w) is the unique minimizer.\nB. Problem Description\nConsider the `1 -minimization problem (BP) with the sampling matrix A satisfying that AA0 = I, where I is an\nidentity matrix. We call that A is a partially orthonormal\nmatrix hereafter as its rows are usually randomly selected\nfrom an orthonormal matrix in practice, e.g. partial-DCT matrix. Hence, there exists another partially orthonormal matrix\nB \u2208 R(N \u2212n)\u00d7N\n\u0014 , \u0015whose rows are orthogonal to those of A,\nA\nsuch that \u03a6 =\nis orthonormal. Let p = \u03a6x. The problem\nB\n(BP) is then equivalent to\n(BPo )\n\nmin\nx,p,\u0393(p)=b\n\nkxk1 , subject to \u03a6x = p,\n\nwhere \u0393(p) is an operator projecting the vector p onto its first\nn entries.\nIn (BPo ), the sampling matrix A is expanded into an\northonormal matrix \u03a6. It corresponds to the scenario where\nthe full sampling is carried out with the sampling matrix \u03a6\nand p is the vector containing all measurements. Note that only\nb, as a part of p, is actually observed. To expand the sampling\nmatrix A into an orthonormal matrix \u03a6 is a key step to show\nthat the ALM method exactly solves (BPo ) and, hence, (BP).\nThe next subsection describes the proposed algorithm, referred\nto as orthonormal expansion `1 -minimization.\nC. ALM Based ONE-L1 Algorithms\nIn this subsection we solve the `1 -minimization problem\n(BPo ) using the ALM method [18], [19]. The ALM method\nis similar to the quadratic penalty method except an additional Lagrange multiplier term. Compared with the quadratic\npenalty method, ALM method has some salient properties, e.g.\nthe ease of parameter tuning and the convergence speed. The\naugmented Lagrangian function is\n\u03bc\n2\n(5)\nL(x, p, y, \u03bc) = kxk1 + hp \u2212 \u03a6x, yi + kp \u2212 \u03a6xk2 ,\n2\n\n\f3\n\nwhere Lagrange multiplier y \u2208 RN , \u03bc \u2208 R+ and hu, vi =\nu0 v \u2208 R is the inner product of u, v \u2208 RN . Eq. (5) can be\nexpressed as follows:\nL(x, p, y, \u03bc) = kxk1 +\n\n\u03bc\np \u2212 \u03a6x + \u03bc\u22121 y\n2\n\n2\n2\n\n\u2212\n\n1\n2\nkyk2 . (6)\n2\u03bc\n\nSubsequently, we have the following optimization problem\n(SP ):\n(SP )\nmin L(x, p, y, \u03bc).\nx,p,\u0393(p)=b\n\nInstead of solving (SP ), let us consider the two related\nproblems\n(SP1 ) min L(x, p, y, \u03bc),\nx\n\nand\n(SP2 )\n\nmin L(x, p, y, \u03bc).\np,\u0393(p)=b\n\nNote that problem (SP1 ) is similar to the `1 -regularized\nproblem in (4). In general, (SP1 ) cannot be directly solved\nusing the soft thresholding operator as that in (4) since there\nis a matrix product of \u03a6 and x in the term of `2 -norm.\nHowever, the soft thresholding operator does apply to the\nspecial case where \u03a6 is orthonormal. Given k\u03a6uk2 = kuk2\nfor any u \u2208 RN , we can apply the soft thresholding to obtain\n\u0001\u0001\nS\u03bc\u22121 \u03a60 p + \u03bc\u22121 y = arg min L(x, p, y, \u03bc).\n(7)\nx\n\nTo solve (SP2 ), we let \u2202\u0393(p) L(x, p, y, \u03bc) = 0 to obtain \u0393(p) =\n\u0001\n\u0393 \u03a6x \u2212 \u03bc\u22121 y , i.e.,\n\u0015\n\u0014\nb\n\u0001\n= arg min L(x, p, y, \u03bc),\n(8)\n\u0393 \u03a6x \u2212 \u03bc\u22121 y\np,\u0393(p)=b\nwhere \u0393 (*) is the operator projecting the variable to its last\nN \u2212 n entries. As a result, an iterative solution of (SP ) is\nstated in the following Lemma 1.\nLemma 1: For fixed y and \u03bc, the iterative algorithm given\nby\n\u0001\u0001\nxj+1 = S\u03bc\u22121 \u03a60 pj + \u03bc\u22121 y ,\n(9)\n\u0014\n\u0015\nb\n\u0001\npj+1 =\n(10)\n\u0393 \u03a6xj+1 \u2212 \u03bc\u22121 y\nconverges to an optimal solution of (SP ).\nProof: Denote L (x, p, y, \u03bc) as L (x, p), for simplicity.\nj+1\nBy the optimality \u0001and uniqueness\nand pj+1 , we\n\u0001 of x\nj+1 j+1\nj j\nhave L x , p\n\u2264 \u0001L x , p and\n\u0001 the equality holds if\nj+1 j+1\nj j\nand\nonly\nif\nx\n,\np\n=\nx\n,\np\n. Hence, the sequence\n\b\n\u0001\n\u2217\nL xj , p\u0001j is bounded and converges to a constant L\n\b j, i.e.,\nj j\n\u2217\nL x , p \u2192 L as j \u2192 +\u221e. Since\nx is\n\u0001 the1 sequence\n2\nalso bounded by xj 1 \u2264 L xj , pj + 2\u03bc\nkyk2 , there exists\n\b\n+\u221e\nji\na sub-sequence xji i=1 such that x\b\n\u2192 x\u2217s as i \u2192 +\u221e,\n\u2217\nj\nwhere xs is an accumulation point of x . Correspondingly,\npji \u2192 p\u2217s , and moreover, L (x\u2217s , p\u2217s ) = L\u2217 .\nWe now use contradiction to show that (x\u2217s , p\u2217s ) is a\nfixed point of the algorithm. Suppose that there exist\n(xs , ps ) 6= (x\u2217s , p\u2217s ) such that xs = arg minx L (x, p\u2217s ) and\nps = arg minp,\u0393(p)=b L (x\u2217s , p), and L (xs , ps ) < L\u2217 . By\n(9), (10) and [4, Lemma 2.2], we have xji +1 \u2212 xs 2 \u2264\nxji \u2212 x\u2217s 2 \u2192 0, i.e., xji +1 \u2192 xs , as i \u2192 +\u221e. Meanwhile,\n\n\u0001\npji +1 \u2192 ps . Hence, L xj\u0001i +1 , pji +1 \u2192 L (xs , ps ) < L\u2217 ,\nwhich contradicts L xj , pj \u2192 L\u2217 , resulting in that (x\u2217s , p\u2217s )\nis a fixed point. Moreover, it follows from xji +q \u2212 x\u2217s 2 \u2264\nxji \u2212 x\u2217s 2 \u2192 0 for any positive integer q, that xj \u2192 x\u2217s , as\nj \u2192 +\u221e.\n\u0014 \u0015\nA\nNote that orthonormal matrix \u03a6 =\nand \u03a60 \u03a6 = A0 A +\nB\nB0 B = I. We can obtain\n\u0001\u0001\nx\u2217s = S\u03bc\u22121 x\u2217s + A0 b + \u03bc\u22121 \u0393(y) \u2212 Ax\u2217s .\n(11)\nMeanwhile, (SP ) is equivalent to\n\u0015\u0013\n\u0012 \u0014\nb\n\u0001\nminL x,\nx\n\u0393 \u03a6x \u2212 \u03bc\u22121 y\n\u03bc\n= kxk1 +\nAx \u2212 b \u2212 \u03bc\u22121 \u0393(y)\n2\n\n2\n2\n\n\u2212\n\n1\n2\nkyk2 .\n2\u03bc\n\n(12)\n\nBy [4, Proposition 3.10], x\u2217s is an optimal solution of the\nproblem in (12) and equivalently, (x\u2217s , p\u2217s ) is an optimal\nsolution of (SP ).\nRemark 1:\n(1) Lemma 1 shows that to solve problem (SP ) is equivalent\nto solve, iteratively, problems (SP1 ) and (SP2 ).\n(2) Reference [4, Proposition 3.10] only deals with the special case kAk2 < 1 and it is, in fact, straightforward to\nextend the result to arbitrary A.\nFollowing from the framework of the ALM method [19]\nand Lemma 1, the ALM based ONE-L1 algorithm is outlined\nin Algorithm 1, where (x\u2217t , p\u2217t ) is the optimal solution to\n(SP ) in the tth iteration and y\u2217t is the corresponding Lagrange\nmultiplier.\nAlgorithm 1: Exact ONE-L1 Algorithm via ALM Method\nInput: Expanded \u0014orthonormal\nmatrix \u03a6 and observed sample data b.\n\u0015\nb\n\u2217\n\u2217\n\u2217\n1. x0 = 0; p0 =\n; y0 = 0; \u03bc0 > 0; t = 0.\n0\n2. while not converged do\n\u0001\n3. Lines 4-9 solve x\u2217t+1 , p\u2217t+1 = arg min(x,p,\u0393(p)=b) L (x, p, y\u2217t , \u03bct );\n0\n\u2217\n0\n\u2217\n4. xt+1 = xt , pt+1 = pt , j = 0;\n5. while not converged\n\u0010 do\n\u0010\n\u0011\u0011\n\u22121 \u2217\n0 pj\n6.\nxj+1\n;\nt+1 = S\u03bc\u22121 \u03a6\nt+1 + \u03bct yt\n#\n\" t\nb\n\u0010\n\u0011\nj+1\n7.\npt+1 =\n;\n\u22121 \u2217\n\u0393 \u03a6xj+1\nt+1 \u2212 \u03bct yt\n8.\nset j = j + 1;\n9. end while\n\u0001\n\u2217\n10. yt+1 = y\u2217t + \u03bct p\u2217t+1 \u2212 \u03a6x\u2217t+1 ;\n11. choose \u03bct+1 > \u03bct ;\n12. set t = t + 1;\n13. end while\nOutput: (x\u2217t , p\u2217t ).\n\nThe convergence of Algorithm 1 is stated in the following\ntheorem.\nTheorem 1: Any accumulation point (x\u2217 , p\u2217 ) of sequence\n+\u221e\n{(x\u2217t , p\u2217t )}t=1 of Algorithm 1 is an optimal solution of (BPo )\nand the convergence rate with \u0001respect to the outer iteration\nloop index t is at least O \u03bc\u22121\nt\u22121 in the sense that\n\u0001\nkx\u2217t k1 \u2212 x\u2020 = O \u03bc\u22121\nt\u22121 ,\nwhere x\u2020 = kx\u2217 k1 .\n\n\f4\n\n\u2217\nProof: We first show that the\n\u0001 sequence {yt } is bounded.\n\u2217\n\u2217\nBy the optimality of xt+1 , pt+1 we have\n\u0001\n0 \u2208 \u2202x L x\u2217t+1 , p\u2217t+1 , y\u2217t , \u03bct = \u2202 x\u2217t+1 1 \u2212 \u03a60 y\u2217t+1 ,\n\u0001\n\u0001\n0 = \u2202\u0393(p) L x\u2217t+1 , p\u2217t+1 , y\u2217t , \u03bct = \u0393 y\u2217t+1 ,\n\nwhere \u2202x denotes the partial differential operator with respect\nto x resulting in a set of subgradients. Hence, \u03a60 y\u2217t+1 \u2208\n\u2217\n\u2202 x\u2217t+1 1 . It follows that \u03a60 y\u2217t+1 \u221e\n\u0001 \u2264 1 and {yt } is\n\u2020\n\u2217\n\u2217\n\u2217\nbounded. By x \u2265 L xt+1 , pt+1 , yt , \u03bct ,\n\u0011\n\u0001\n1 \u0010 \u2217\n2\n2\nx\u2217t+1 1 = L x\u2217t+1 , p\u2217t+1 , y\u2217t , \u03bct \u2212\nyt+1 2 \u2212 ky\u2217t k2\n2\u03bct\n\u0011\n1 \u0010 \u2217\n2\n2\n\u2020\n\u2264x \u2212\nyt+1 2 \u2212 ky\u2217t k2 .\n2\u03bct\nBy {y\u2217t } is bounded,\nx\u2217t+1\n\n1\n\n\u0001\n\u2264 x\u2020 + O \u03bc\u22121\n.\nt\n\u2217\n\n(13)\n\nx\u2217t ,\n\nFor any accumulation point x of\nwithout loss of generality,\n\u2217\nx\u2020 . In the\nwe have x\u2217t \u2192 x\u2217 as t \u2192 +\u221e. Hence, kx\n\u0001 k1 \u2264\n\u22121\n\u2217\n\u2217\n\u2217\n\u2217\n\u2217\nmean time, pt+1 = \u03a6xt+1 +\u03bct yt+1 \u2212 yt \u2192 p and \u03a6x\u2217 =\no\np\u2217 result in that (x\u2217 , p\u2217 ) is \u0002an optimal solution to (BP\n\u0001\u0003 ).\n\u22121\n\u2217\n\u2217\n\u2217\n0\n\u2217\nMoreover, by xt+1 = \u03a6 pt+1 \u2212 \u03bct yt+1 \u2212 yt and\nx\u2020 =\n\nmin\n\u03a6x=p,\u0393(p)=b\n\nx\u2217t+1 1\n\nkxk1 = min k\u03a60 pk1 \u2264 \u03a60 p\u2217t+1\np,\u0393(p)=b\n\n\u2020\n\n\u03bc\u22121\nt\n\n1\n\n,\n\n\u0001\n\nwe have\n\u2265 x \u2212O\n, which establishes the\ntheorem with (13).\nAlgorithm 1 contains, respectively, an inner and an outer\niteration loops. Theorem 1 presents only the convergence rate\nof the outer loop. A natural way to speed up Algorithm 1 is\nto terminate the inner loop without convergence and use the\nobtained inner-loop solution as the initialization for the next\niteration. This is similar to a continuation strategy and can be\nrealized with reasonably set precision and step size \u03bct [19].\nWhen the continuation parameter \u03bct increases very slowly,\nin a few iterations, the inner loop can produce a solution\nwith high accuracy. In particular, for the purpose of fast and\nsimple computing, we may update the variables in the inner\nloop only once before stepping into the outer loop operation.\nThis results in a relaxed version of exact ONE-L1 algorithm\n(eONE-L1), namely relaxed ONE-L1 algorithm (rONE-L1)\noutlined in Algorithm 2.\nAlgorithm 2: Relaxed ONE-L1 Algorithm\nInput: Expanded\u0014 orthonormal\nmatrix \u03a6 and observed sample data b.\n\u0015\nb\n1. x0 = 0; p0 =\n; y0 = 0; \u03bc0 > 0; t = 0.\n0\n2. while not converged\n\u0010 do\n\u0010\n\u0011\u0011\n3. xt+1 = S\u03bc\u22121 \u03a60 pt + \u03bc\u22121\n;\nt yt\n\" t\n#\nb\n\u0010\n\u0011\n4. pt+1 =\n;\n\u0393 \u03a6xt+1 \u2212 \u03bc\u22121\nt yt\n\u0001\n5. yt+1 = yt + \u03bct pt+1 \u2212 \u03a6xt+1 ;\n6. choose \u03bct+1 > \u03bct ;\n7. set t = t + 1;\n8. end while\nOutput: (xt , pt ).\n\nTheorem 2: The iterative solution (xt , pt ) of Algorithm\n2\nconverges to a feasible solution (xf , pf ) of (BPo ) if\nP+\u221e\n\u22121\n< +\u221e. It converges at least exponentially to\nt=1 \u03bct\nf\nf\n(x , p ) if {\u03bct } is an exponentially increasing sequence.\n\nProof: We show first that sequences {\u0177t } and\u0001 {yt } are\nbounded, where \u0177t = yt\u22121 + \u03bct\u22121 pt\u22121 \u2212 \u03a6xt . By the\noptimality of xt+1 and pt+1 we have\n0 \u2208 \u2202x L (xt+1 , pt , yt , \u03bct ) = \u2202 kxt+1 k1 \u2212 \u03a60 \u0177t+1 ,\n\u0001\n\u0001\n0 = \u2202\u0393(p) L xt+1 , pt+1 , yt , \u03bct = \u0393 yt+1 .\nHence, \u03a60 \u0177t+1 \u221e \u2264 1 and it follows\n\u0001\n\u0001 that {\u0177t } is bounded.\nSince y\u0001t+1 = \u0177t+1 + \u03bct pt+1 \u2212 pt , we\u0001 obtain \u0393 yt+1 =\n\u0393 \u0177t+1 . This together with \u0393 yt+1\n= 0 results in\nyt+1 2 \u2264 \u0177t+1 2 and \u0001the boundedness of {yt }. By pt+1 \u2212\npt = \u03bc\u22121\nyt+1 \u2212 \u0177t+1 , we have pt+1 \u2212 pt 2 \u2264 C\u03bc\u22121\nt\nt\nwith\nC\nbeing\na\nconstant.\nThen\n{p\n}\nis\na\nCauchy\nsequence\nt\nP+\u221e\nf\nif t=1 \u03bc\u22121\nt < +\u221e, resulting in pt \u2192 p as t \u2192\n\u0001 +\u221e. In the\nf\nf\nf\nmean time, xt \u2192 x , \u03a6x = p . Hence, xf , pf is a feasible\nsolution of (BPo ). Suppose that {\u03bct } is an exponentially\nincreasing sequence, i.e., \u03bct+1 = r\u03bct with r > 1. By the\nboundedness of {yt } and {\u0177t } we have\npt \u2212 pf\n\n2\n\n=\n\n+\u221e\nX\n\npi \u2212 pi+1\n\ni=t\n\n\u2264 C\u03bc\u22121\nt\n\n\u0001\n\n\u2264\n2\n\n+\u221e\nX\n\n+\u221e\nX\n\npi \u2212 pi+1\n\n2\n\ni=t\n\n\u0001\nr\u2212i = O \u03bc\u22121\n.\nt\n\ni=0\nf\nHence,\n\b \u22121 {pt } converges at least exponentially to p since\n\u03bct\nexponentially converges to 0, and the same result holds\nfor {xt }.\nRemark 2: It is shown in Theorem 2 that faster growth\nof {\u03bct } can result in faster convergence of {xt }. Intuitively,\nthe reduced number of iterations for the inner loop problem\n(SP ) may result in some error from the optimal solution x\u2217t\nof the inner loop. This will likely affect the accuracy of the\nfinal solution xf for (BP). Therefore, the growth speed of\n{\u03bct } provides a tradeoff between the convergence speed of the\nalgorithm and the precision of the final solution, which will\nbe illustrated in Section III through numerical simulations.\n\nD. Relationship Between rONE-L1 and IST\nThe studies and applications of IST type algorithms have\nbeen very active in recent years because of their concise presentations. This subsection considers the relationship between\nrONE-L1 and IST. Note that \u0393 (yt ) = 0 in Algorithm 2 and\n\u03a60 \u03a6 = A0 A+B0 B = I. After some derivations, it can be shown\nthat the rONE-L1 algorithm is equivalent to the following\niteration (starting from xt = 0, as t \u2264 0, and zt = 0, as\nt < 0):\n\u0001\nxt+1 = S\u03bbt xt + A0 zt ,\n(14)\nzt = b \u2212 A [(1 + \u03bat ) xt \u2212 \u03bat xt\u22121 ] + \u03bat zt\u22121 ,\nwhere \u03bbt = \u03bc\u22121\nand \u03bat = \u03bc\u03bct\u22121\n. Compared with the general\nt\nt\nform of IST in (2), one more term \u03bat zt\u22121 is added when\ncomputing the current residual zt in rONE-L1. Moreover, a\nweighted sum (1+\u03bat )xt \u2212\u03bat xt\u22121 is used instead of the current\nsolution xt . It will be shown later that these two changes\nessentially improve the sparsity-undersampling tradeoff.\nRemark 3: Equations in (14) show that the expansion from\nthe partially orthonormal matrix A to orthonormal \u03a6 is not at\n\n\f5\n\nall involved in the actual implementation and computation of\nrONE-L1. The same claim also holds for eONE-L1 algorithm.\nNevertheless, the orthonormal expansion is a key instrumentation in the derivation and analysis of Algorithms 1 and 2.\n\n0.9\nL1, theoretical\nrONE\u2212L1, Gaussian, recommended r\nrONE\u2212L1, DCT, recommended r\nrONE\u2212L1, DCT, r = 1+0.2\u03b4\nIST, DCT, recommended r\nIST, DCT, r = 1+0.2\u03b4\n\n0.8\n0.7\n0.6\n\nE. Implementation Details\n\nIII. N UMERICAL S IMULATIONS\nA. Sparsity-Undersampling Tradeoff\nThis subsection considers the sparsity-undersampling tradeoff of rONE-L1 in the case of strictly sparse signals and\nnoise-free measurements. Phase transition is a measure of the\nsparsity-undersampling tradeoff in this case. Let the sampling\nratio be \u03b4 = n/N and the sparsity ratio be \u03c1 = k/n, where k\nis a measure of sparsity of x, and we call that x is k-sparse\nif at most k entries of x are nonzero. As k, n, N \u2192 \u221e with\nfixed \u03b4 and \u03c1, the behavior of the phase transition of (BP)\nis controlled by (\u03b4, \u03c1) [20], [21]. We denote this theoretical\ncurve by \u03c1 = \u03c1T (\u03b4), which is plotted in Fig 1.\nWe estimate the phase transition of rONE-L1 using a Monte\nCarlo method as in [17], [22]. Two matrix ensembles are considered, including Gaussian with N = 1000 and partial-DCT\nwith N = 1024. Here the finite-N phase transition is defined\nas the value of \u03c1 at which the success probability to recover the\noriginal signal is 50%. We consider 33 equispaced values of\n\u03b4 in {0.02, 0.05, * * * , 0.98}. For each \u03b4, 21 equispaced values\nof \u03c1 are generated in the interval [\u03c1T (\u03b4) \u2212 0.1, \u03c1T (\u03b4) + 0.1].\nThen M = 20 random problem instances are generated and\nsolved with respect to each combination of (\u03b4, \u03c1), where\nn = d\u03b4N e, k = d\u03c1ne, and nonzero entries of sparse signals\nare generated from the standard Gaussian distribution. Success\nis declared if the relative root mean squared error (relative\n\n0.5\n\n\u03c1\n\nAs noted in Remark 3, the expansion from A to \u03a6 is\nnot involved in the computing of ONE-L1 algorithms. In our\nimplementations, we consider using exponentially increasing\n\u03bct , i.e., fixing r > 1 and \u03bct = rt \u03bc0 . Let\n\u0001 Q\u03b1 (*) be an \u03b1quantile operator and \u03bc0 = 1/Q\u03b1 A0 b , with |*| applying\nto the vector variable elementwise, \u03bc\u22121\nbeing the threshold\n0\nin the first iteration and \u03b1 = 0.99. In eONE-L1, a large\nr can speed up the convergence of the outer loop iteration\naccording to Theorem 1. However, simulations show that a\nlarger r can result in more iterations in the inner loop. We use\nr = 1 + n/N as default. In rONE-L1, the value of r provides\na tradeoff between the convergence speed of the algorithm\nand the precision of the final solution. Our recommendation\nof r to achieve the optimal sparsity-undersampling tradeoff\nis r = min (1 + 0.04n/N, 1.02), which will be illustrated in\nSection III-A.\nAn iterative algorithm needs a termination criterion.\nThe\nkAx\u2217\nt \u2212bk2\n< \u03c41\neONE-L1 algorithm is considered converged if kbk\n2\nwith \u03c41 being a user-defined tolerance. The inner iteration is\nj+1\nj\nkx \u2212x k\nconsidered converged if t xj t 2 < \u03c42 . In our implemenk t k2\n\u0001\ntation, the default values are (\u03c41 , \u03c42 ) = 10\u22125 , 10\u22126 . The\nkAxt \u2212bk2\nrONE-L1 algorithm is considered converged if kbk\n< \u03c4,\n2\n\u22125\nwith \u03c4 = 10 as default.\n\n0.4\n0.3\n0.2\n0.1\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n\u03b4\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFig. 1. Observed phase transitions of rONE-L1, and comparison with those\nof IST. Note that, 1) the observed phase transitions of rONE-L1 with the\nrecommended r strongly agree with the theoretical calculation based on (BP);\n2) rONE-L1 has significantly enlarged success phases compared with IST.\n\nkx\u0302\u2212xo k\n\nRMSE) kxo k 2 < 10\u22124 , where x\u0302 is the recovered signal.\n2\nThe number of success among M experiments is recorded.\nFinally, a generalized linear model is used to estimate the\nphase transition as in [22].\nThe experiment result is presented in Fig. 1. The observed\nphase transitions using the recommended value of r strongly\nagree with the theoretical result of (BP). It shows that the\nrONE-L1 algorithm has the optimal sparsity-undersampling\ntradeoff in the sense of `1 minimization.\nB. Comparison with IST\nThe rONE-L1 algorithm can be considered as a modified version of IST in (2). In this subsection we compare\nthe sparsity-undersampling tradeoff and speed of these two\nalgorithms. A similar method is adopted to estimate the\nphase transition of IST, which is implemented using the\nsame parameter values as rONE-L1. Only nine values of \u03b4\nin {0.1, 0.2, * * * , 0.9} are considered with the partial-DCT\nmatrix ensemble for time consideration. Another choice of\nr = 1 + 0.2\u03b4 is considered besides the recommended one.\nCorrespondingly, the phase transition of rONE-L1 with r =\n1 + 0.2\u03b4 is also estimated.\nThe observed phase transitions are shown in Fig. 1. As a\nmodified version of IST, obviously, rONE-L1 makes a great\nimprovement over IST in the sparsity-undersampling tradeoff.\nMeanwhile, comparison of the averaged number of iterations\nof the two algorithms shows that rONE-L1 is also faster than\nIST. For example, as \u03b4 = 0.2 and the recommended r is used,\nrONE-L1 is about 6 times faster than IST.\nC. Comparison with AMP, FPC-AS and NESTA in Noise-free\nCase\nIn this subsection, we report numerical simulation results\ncomparing rONE-L1 with state-of-the-art algorithms, including AMP, FPC-AS and NESTA, in the case of sparse signals\n\n\f6\n\nTABLE I\nC OMPARISON R ESULTS OF ONE-L1 A LGORITHMS W ITH\nS TATE - OF - THE - ART M ETHODS\n\u03c1\n\nMethod\neONE-L1\nrONE-L1\n0.1\nAMP\nFPC-AS\nNESTA\neONE-L1\nrONE-L1\n0.22 AMP\nFPC-AS\nNESTA\n\u2217 Three numbers are\n\nA0\n\nD. A Practical Example\n(10\u22125 )\n\n# calls A &\nCPU time (s)\nError\n1819 (1522,2054)\u2217\n5.62 (4.67,6.52)\n0.42 (0.11,0.94)\n515.4 (286,954)\n2.14 (1.19,3.92)\n1.08 (0.53,1.30)\n222.7 (216,234)\n0.80 (0.76,0.86)\n1.02 (0.85,1.15)\n150.2 (135,170)\n0.50 (0.44,0.56)\n1.13 (1.07,1.23)\n468.9 (458,484)\n2.70 (2.55,2.98)\n1.05 (0.99,1.13)\n9038 (7270,11194)\n28.5 (22.0,35.8)\n1.87 (0.46,2.66)\n722.3 (440,972)\n2.61 (1.63,3.93)\n1.80 (1.37,3.05)\n1708 (1150,2252)\n6.21 (4.19,9.11)\n10.5 (6.96,15.8)\n589.4 (476,803)\n2.10 (1.65,2.80)\n1.96 (1.46,3.60)\n1084 (890,1244)\n6.47 (5.22,7.50)\n2.90 (1.62,3.98)\naveraged, minimum and maximum values, respectively.\n\nand noise-free measurements. Our experiments used FPC-AS\nv.1.21, NESTA v.1.1, and AMP codes provided by the author.\nWe choose parameter values for FPC-AS and NESTA such that\neach method produces a solution with approximately the same\nprecision as that produced by rONE-L1. In our experiments we\nconsider the recovery of exactly sparse signals from partialDCT measurements. We set N = 214 and \u03b4 = 0.2, and an\n'easy' case where \u03c1 = 0.1 and a 'hard' case where \u03c1 = 0.22\nare considered, respectively. 1 Twenty random problems are\ncreated and solved for each algorithm with each combination\nof (\u03b4, \u03c1), and the minimum, maximum and averaged relative\nRMSE, number of calls of A and A0 , and CPU time usages\nare recorded. All experiments are carried on Matlab v.7.7.0 on\na PC with a Windows XP system and a 3GHz CPU. Default\nparameter values are used in eONE-L1 and rONE-L1.\nAMP: terminating if\n\nkAxt \u2212bk2\nkbk2\n\u22126\n\n< 10\u22125 .\n\nFPC-AS: \u03bb = 2 \u00d7 10 and gtol = 1 \u00d7 10\u22126 , where gtol\nis the termination criterion on the maximum norm of subgradient. FPC-AS solves the problem (QP\u03bb ).\nNESTA: \u03bb = 2 \u00d7 10\u22126 , \u000f = 0 and the termination criterion\ntolvar = 1 \u00d7 10\u22128 . NESTA solves (BP\u000f ) using the Nesterov\nalgorithm [11], with continuation.\nOur experiment results are presented in Table I. In both\n'easy' and 'hard' cases, rONE-L1 is much faster than eONEL1. In the 'easy' case, the proposed rONE-L1 algorithm takes\nthe most number of calls of A and A0 , except that of eONEL1, due to a conservative setting of r. But this number of\ncalls (515.4) is very close to that of NESTA (468.9), and\nfurthermore, the CPU time usage of rONE-L1 (2.14 s) is\nless than that of NESTA (2.70 s) because of its concise\nimplementation. In the 'hard' case, rONE-L1 has the second\nbest performance with significantly less CPU time than that\nof AMP and NESTA. AMP has the second worst CPU time\nand the worst accuracy as the dynamic threshold in each\niteration depends on the mean squared error of the current\niterative solution, which cannot be calculated exactly in the\nimplementation.\n\n1 Here 'easy' and 'hard' refer to the difficulty degree in recovering a\nsparse signal from a specific number of measurements. The setting (\u03b4, \u03c1) =\n(0.2, 0.22) is close to the phase transition of (BP).\n\nThis subsection demonstrates the efficiency of rONE-L1 in\nthe general CS where the signal of interest is approximately\nsparse and measurements are contaminated with noise. We\nseek to reconstruct the Mondrian image of size 256 \u00d7 256,\nshown in Fig. 2, from its noise-contaminated partial-DCT\ncoefficients. This image presents a challenge as its wavelet\nexpansion is compressible but not exactly sparse. The sampling pattern, which is inspired by magnetic resonance imaging\n(MRI) and is shown in Fig. 2, is adopted as most energy\nof the image concentrates at low-frequency components after the DCT transform. The measurement vector b contains\nn = 7419 DCT measurements (\u03b4 = 0.113). White Gaussian\nnoisepwith standard\ndeviation \u03c3 = 1 is then added. We set\n\u221a\n\u000f = n + 2 2n\u03c3. Haar wavelet with a decomposition level 4\nis chosen as the sparsifying transform W. Hence, the problem\nto be solved is (BP\u000f ) with A = Fp W 0 , where Fp is the partialDCT transform. The reconstructed image is \u0124 = W 0 x\u0302 with x\u0302\nbeing the reconstructed wavelet coefficients and reconstruction\nk\u0124\u2212Ho k\nerror is calculated as kHo k F , where Ho is the original\nF\nimage and k*kF denotes the Frobenius norm. We compare the\nperformance of rONE-L1 with NESTA and FPC-AS.\nRemark 4: AMP is omitted for its poor performance in this\napproximately-sparse-signal case. For AMP, the value of the\ndynamic threshold \u03bbt and the term kxt k0 in (3) depend on the\ncondition that the signal to reconstruct is strictly sparse.\nIn such a noisy measurement case, an exact solution for\n(BP\u000f ) is not sought after in the rONE-L1 simulation. The\ncomputation of the rONE-L1 algorithm is set to terminate if\nkAxt \u2212bk2\n\u000f\n\u2264 \u03c4 = kbk\n, i.e., rONE-L1 outputs the first xt when\nkbk2\n2\nit becomes a feasible solution of (BP\u000f ).\nFPC-AS: \u03bb = 1 \u00d7 10\u22123 , gtol = 1 \u00d7 10\u22123 , gtol scale x =\n1 \u00d7 10\u22126 and the maximum number of iterations for subspace\noptimization sub mxitr = 10. The parameters are set according to [15, Section 4.4].\nNESTA: \u03bb = 1 \u00d7 10\u22124 , and tolvar = 1 \u00d7 10\u22126 . The\nparameters are tuned to achieve the minimum reconstruction\nerror.\nFig. 2 shows the experiment results where rONE-L1, FPCAS and NESTA produce faithful reconstructions of the original\nimage. The rONE-L1 algorithm produces a reconstruction\nerror (0.0741) lower than that of FPC-AS (0.0809) with comparable computation times (11.1 s and 11.4 s, respectively).\nWhile NESTA results in a slightly lower reconstruction error\n(0.0649), it incurs about twice more computation time (29.4\ns).\nIV. C ONCLUSION\nIn this work, we have presented novel algorithms to solve\nthe basis pursuit problem for noiseless CS. The proposed\nrONE-L1 algorithm, based on the augmented Lagrange multiplier method and heuristic simplification, can be considered\nas a modified IST with an aggressive continuation strategy.\nThe following two cases of CS have been studied: 1) exact\nreconstruction of sparse signals from noise-free measurements,\nand 2) reconstruction of approximately sparse signals from\nnoise-contaminated measurements. The proposed rONE-L1\n\n\f7\n\nFig. 2. An example of 2D image reconstruction from noise-contaminated\npartial-DCT measurements. Upper left: original Mondrian image; upper right:\nsampling pattern. The lower three are reconstructed images respectively by\nrONE-L1 (lower left, error: 0.0741, time: 11.1 s), FPC-AS (lower middle,\nerror: 0.0809, time: 11.4 s) and NESTA (lower right, error: 0.0649, time:\n29.4 s).\n\noutperforms AMP, which is a well-known IST type algorithm,\nin Case 2 and also in Case 1 when the setting of (\u03b4, \u03c1) is\nclose to the phase transition of basis pursuit. It is faster than\nNESTA in both Case 1 and Case 2. The numerical experiments\nfurther show that rONE-L1 outperforms FPC-AS in Case 2.\nApart from the high computational efficiency and accurate\nreconstruction result, another useful property of rONE-L1 is\nits ease of parameter tuning. It only needs to set a termination\ncriterion \u03c4 if the recommended r is used and the value of \u03c4\nis explicit in Case 2. While this correspondence focuses on\nreconstruction of real-valued signals, it is straightforward to\napply ONE-L1 algorithms to the reconstruction of complexvalued signals [23]. More rigorous analysis of rONE-L1 is\ncurrently under investigation.\nACKNOWLEDGMENT\nThe authors are grateful to the anonymous reviewers\nfor helpful comments. Z. Yang wishes to thank\nArian Maleki for providing the AMP codes. The\nMatlab codes of ONE-L1 algorithms are available at\nhttp://sites.google.com/site/zaiyang0248.\nR EFERENCES\n[1] E. Cand\u00e8s, J. Romberg, and T. Tao, \"Robust uncertainty principles: Exact\nsignal reconstruction from highly incomplete frequency information,\"\nIEEE Trans. Info. Theo., vol. 52, no. 2, pp. 489\u2013509, 2006.\n[2] E. Cand\u00e8s and J. Romberg, \"Sparsity and incoherence in compressive\nsampling,\" Inverse Problems, vol. 23, pp. 969\u2013985, 2007.\n[3] D. Donoho, \"Compressed sensing,\" IEEE Transactions on Information\nTheory, vol. 52, no. 4, pp. 1289\u20131306, 2006.\n[4] I. Daubechies, M. Defrise, and C. De Mol, \"An iterative thresholding\nalgorithm for linear inverse problems with a sparsity constraint,\" Communications on Pure and Applied Mathematics, vol. 57, no. 11, pp.\n1413\u20131457, 2004.\n[5] K. Bredies and D. Lorenz, \"Linear convergence of iterative softthresholding,\" Journal of Fourier Analysis and Applications, vol. 14,\nno. 5, pp. 813\u2013837, 2008.\n[6] S. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky, \"An InteriorPoint Method for Large-Scale `1 -Regularized Least Squares,\" Selected\nTopics in Signal Processing, IEEE Journal of, vol. 1, no. 4, pp. 606\u2013617,\n2008.\n\n[7] M. Lustig, D. Donoho, and J. Pauly, \"Sparse MRI: The application of\ncompressed sensing for rapid MR imaging,\" Magnetic Resonance in\nMedicine, vol. 58, no. 6, pp. 1182\u20131195, 2007.\n[8] E. Hale, W. Yin, and Y. Zhang, \"A fixed-point continuation method for\nl1-regularized minimization with applications to compressed sensing,\"\nCAAM TR07-07, Rice University, 2007.\n[9] E.\nCand\u00e8s\nand\nJ.\nRomberg,\n\"`1-magic:\nRecovery\nof\nsparse\nsignals\nvia\nconvex\nprogramming,\"\nURL:\nwww.acm.caltech.edu/l1magic/downloads/l1magic.pdf.\n[10] S. Becker, J. Bobin, and E. Candes, \"NESTA: A fast and accurate firstorder method for sparse recovery,\" SIAM J. Imaging Sciences, vol. 4,\nno. 1, pp. 1\u201339, 2011.\n[11] Y. Nesterov, \"Smooth minimization of non-smooth functions,\" Mathematical Programming, vol. 103, no. 1, pp. 127\u2013152, 2005.\n[12] J. Tropp and A. Gilbert, \"Signal recovery from random measurements\nvia orthogonal matching pursuit,\" IEEE Transactions on Information\nTheory, vol. 53, no. 12, pp. 4655\u20134666, 2007.\n[13] D. Donoho, Y. Tsaig, I. Drori, and J. Starck, \"Sparse solution of\nunderdetermined linear equations by stagewise orthogonal matching\npursuit,\" submitted to IEEE Trans. on Signal Processing. Available at:\nwww.cs.tau.ac.il/\u223cidrori/StOMP.pdf, 2006.\n[14] D. Needell and J. Tropp, \"CoSaMP: Iterative signal recovery from\nincomplete and inaccurate samples,\" Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301\u2013321, 2009.\n[15] Z. Wen, W. Yin, D. Goldfarb, and Y. Zhang, \"A fast algorithm for\nsparse reconstruction based on shrinkage, subspace optimization and\ncontinuation,\" SIAM Journal on Scientific Computing, vol. 32, no. 4,\npp. 1832\u20131857, 2010.\n[16] A. Maleki and D. Donoho, \"Optimally tuned iterative thresholding algorithms for compressed sensing,\" IEEE J. Select. Areas Signal Processing,\nvol. 4, pp. 330\u2013341, 2010.\n[17] D. Donoho, A. Maleki, and A. Montanari, \"Message-passing algorithms\nfor compressed sensing,\" Proceedings of the National Academy of\nSciences, vol. 106, no. 45, pp. 18 914\u201318 919, 2009.\n[18] J. Nocedal and S. Wright, Numerical Optimization. New York: Springer\nverlag, 2006.\n[19] D. Bertsekas, Constrained optimization and Lagrange multiplier methods. Boston: Academic Press, 1982.\n[20] D. Donoho and J. Tanner, \"Counting the faces of randomly-projected\nhypercubes and orthants, with applications,\" Discrete and Computational\nGeometry, vol. 43, no. 3, pp. 522\u2013541, 2010.\n[21] M. Stojnic, \"Various thresholds for l1-optimization in compressed sensing,\" arXiv:0907.3666, 2009.\n[22] D. Donoho and J. Tanner, \"Observed universality of phase transitions in\nhigh-dimensional geometry, with implications for modern data analysis\nand signal processing,\" Philosophical Transactions of the Royal Society\nA, vol. 367, no. 1906, pp. 4273\u20134293, 2009.\n[23] Z. Yang and C. Zhang, \"Sparsity-undersampling tradeoff of compressed\nsensing in the complex domain,\" in Proceedings of ICASSP 2011, 2011.\n\n\f"}