{"id": "http://arxiv.org/abs/1110.0378v1", "guidislink": true, "updated": "2011-10-03T15:17:46Z", "updated_parsed": [2011, 10, 3, 15, 17, 46, 0, 276, 0], "published": "2011-10-03T15:17:46Z", "published_parsed": [2011, 10, 3, 15, 17, 46, 0, 276, 0], "title": "Exact Dynamic Support Tracking with Multiple Measurement Vectors using\n  Compressive MUSIC", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.1687%2C1110.2632%2C1110.1915%2C1110.4564%2C1110.1193%2C1110.6017%2C1110.4857%2C1110.2716%2C1110.0874%2C1110.0267%2C1110.1026%2C1110.5983%2C1110.1374%2C1110.1076%2C1110.3377%2C1110.0432%2C1110.4700%2C1110.5713%2C1110.2681%2C1110.5153%2C1110.2731%2C1110.0951%2C1110.0456%2C1110.5853%2C1110.6890%2C1110.5887%2C1110.1712%2C1110.2765%2C1110.3648%2C1110.5949%2C1110.5511%2C1110.3395%2C1110.6414%2C1110.4706%2C1110.2763%2C1110.4760%2C1110.2273%2C1110.5660%2C1110.5060%2C1110.2065%2C1110.4612%2C1110.3883%2C1110.2745%2C1110.3542%2C1110.4541%2C1110.6911%2C1110.3728%2C1110.1309%2C1110.1566%2C1110.2699%2C1110.0009%2C1110.3693%2C1110.2562%2C1110.2017%2C1110.0839%2C1110.5816%2C1110.4461%2C1110.4482%2C1110.1931%2C1110.3604%2C1110.1693%2C1110.6785%2C1110.5968%2C1110.4478%2C1110.6758%2C1110.3726%2C1110.0083%2C1110.2039%2C1110.1627%2C1110.2955%2C1110.1639%2C1110.4454%2C1110.2517%2C1110.6613%2C1110.3593%2C1110.2258%2C1110.0464%2C1110.0661%2C1110.4183%2C1110.3526%2C1110.0880%2C1110.2736%2C1110.1091%2C1110.6194%2C1110.2372%2C1110.5532%2C1110.1375%2C1110.0378%2C1110.2655%2C1110.0520%2C1110.1359%2C1110.1777%2C1110.6731%2C1110.2589%2C1110.3342%2C1110.5086%2C1110.5297%2C1110.1063%2C1110.0453%2C1110.6273%2C1110.5682&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Exact Dynamic Support Tracking with Multiple Measurement Vectors using\n  Compressive MUSIC"}, "summary": "Dynamic tracking of sparse targets has been one of the important topics in\narray signal processing. Recently, compressed sensing (CS) approaches have been\nextensively investigated as a new tool for this problem using partial support\ninformation obtained by exploiting temporal redundancy. However, most of these\napproaches are formulated under single measurement vector compressed sensing\n(SMV-CS) framework, where the performance guarantees are only in a\nprobabilistic manner. The main contribution of this paper is to allow\n\\textit{deterministic} tracking of time varying supports with multiple\nmeasurement vectors (MMV) by exploiting multi-sensor diversity. In particular,\nwe show that a novel compressive MUSIC (CS-MUSIC) algorithm with optimized\npartial support selection not only allows removal of inaccurate portion of\nprevious support estimation but also enables addition of newly emerged part of\nunknown support. Numerical results confirm the theory.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.1687%2C1110.2632%2C1110.1915%2C1110.4564%2C1110.1193%2C1110.6017%2C1110.4857%2C1110.2716%2C1110.0874%2C1110.0267%2C1110.1026%2C1110.5983%2C1110.1374%2C1110.1076%2C1110.3377%2C1110.0432%2C1110.4700%2C1110.5713%2C1110.2681%2C1110.5153%2C1110.2731%2C1110.0951%2C1110.0456%2C1110.5853%2C1110.6890%2C1110.5887%2C1110.1712%2C1110.2765%2C1110.3648%2C1110.5949%2C1110.5511%2C1110.3395%2C1110.6414%2C1110.4706%2C1110.2763%2C1110.4760%2C1110.2273%2C1110.5660%2C1110.5060%2C1110.2065%2C1110.4612%2C1110.3883%2C1110.2745%2C1110.3542%2C1110.4541%2C1110.6911%2C1110.3728%2C1110.1309%2C1110.1566%2C1110.2699%2C1110.0009%2C1110.3693%2C1110.2562%2C1110.2017%2C1110.0839%2C1110.5816%2C1110.4461%2C1110.4482%2C1110.1931%2C1110.3604%2C1110.1693%2C1110.6785%2C1110.5968%2C1110.4478%2C1110.6758%2C1110.3726%2C1110.0083%2C1110.2039%2C1110.1627%2C1110.2955%2C1110.1639%2C1110.4454%2C1110.2517%2C1110.6613%2C1110.3593%2C1110.2258%2C1110.0464%2C1110.0661%2C1110.4183%2C1110.3526%2C1110.0880%2C1110.2736%2C1110.1091%2C1110.6194%2C1110.2372%2C1110.5532%2C1110.1375%2C1110.0378%2C1110.2655%2C1110.0520%2C1110.1359%2C1110.1777%2C1110.6731%2C1110.2589%2C1110.3342%2C1110.5086%2C1110.5297%2C1110.1063%2C1110.0453%2C1110.6273%2C1110.5682&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Dynamic tracking of sparse targets has been one of the important topics in\narray signal processing. Recently, compressed sensing (CS) approaches have been\nextensively investigated as a new tool for this problem using partial support\ninformation obtained by exploiting temporal redundancy. However, most of these\napproaches are formulated under single measurement vector compressed sensing\n(SMV-CS) framework, where the performance guarantees are only in a\nprobabilistic manner. The main contribution of this paper is to allow\n\\textit{deterministic} tracking of time varying supports with multiple\nmeasurement vectors (MMV) by exploiting multi-sensor diversity. In particular,\nwe show that a novel compressive MUSIC (CS-MUSIC) algorithm with optimized\npartial support selection not only allows removal of inaccurate portion of\nprevious support estimation but also enables addition of newly emerged part of\nunknown support. Numerical results confirm the theory."}, "authors": ["Jong Min Kim", "Ok Kyun Lee", "Jong Chul Ye"], "author_detail": {"name": "Jong Chul Ye"}, "author": "Jong Chul Ye", "links": [{"href": "http://arxiv.org/abs/1110.0378v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1110.0378v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "94A13, 94A20", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1110.0378v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1110.0378v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "1\n\nExact Dynamic Support Tracking with\nMultiple Measurement Vectors using\nCompressive MUSIC\narXiv:1110.0378v1 [cs.IT] 3 Oct 2011\n\nJong Min Kim, Ok Kyun Lee and Jong Chul Ye\n\nAbstract\nDynamic tracking of sparse targets has been one of the important topics in array signal\nprocessing. Recently, compressed sensing (CS) approaches have been extensively investigated as\na new tool for this problem using partial support information obtained by exploiting temporal\nredundancy. However, most of these approaches are formulated under single measurement vector\ncompressed sensing (SMV-CS) framework, where the performance guarantees are only in a\nprobabilistic manner. The main contribution of this paper is to allow deterministic tracking of\ntime varying supports with multiple measurement vectors (MMV) by exploiting multi-sensor\ndiversity. In particular, we show that a novel compressive MUSIC (CS-MUSIC) algorithm with\noptimized partial support selection not only allows removal of inaccurate portion of previous\nsupport estimation but also enables addition of newly emerged part of unknown support. Numerical\nresults confirm the theory.\n\nIndex Terms\nCompressed sensing, joint sparsity, time varying signal, compressive MUSIC, optimized partial\nsupport selection\n\nCorrespondence to:\nJong Chul Ye, Ph.D. Associate Professor\nDept. of Bio and Brain Engineering, KAIST\n373-1 Guseong-dong Yuseong-gu, Daejon 305-701, Korea\nEmail: jong.ye@kaist.ac.kr\nTel: 82-42-350-4320\nFax: 82-42-350-4310\n\n\f2\n\nI. I NTRODUCTION\nDynamic target tracking problem that addresses the estimation of time varying support of moving\ntarget has been one of the important classical topics in array signal processing including radar,\ncommunication, and medical imaging applications. For example, in electroencephalography (EEG)\nor magnetoencephalography (MEG) source localization problems, it has been shown that the\nposition of the dipole moments during epileptic activities varies according to time and we are\ninterested in their spatio-temporal dynamics [1]. Dynamic MRI problem that tracks the motion of\nhearts also belongs to this class of problem.\nRecently, there have been renewed interests for this problem with the help of a modern mathematical tool called compressed sensing [2], [3]. These approaches try to exploit knowledges of\npartial support information obtained at the previous time point. More specifically, consider the\nfollowing time varying support estimation problem:\nmin kx(t)k0 , subject to b(t) = Ax(t), t = 0, 1, * * * ,\n\n(1)\n\nx(t)\n\nwhere b(t) \u2208 Rm , and x(t) \u2208 Rn are noiseless measurement vector, and sparse signal at time t.\nAssuming that the support is assumed to change slowly, theoretical results [4] have demonstrated\nthat we can reduce the required sampling in compressed sensing reconstruction if we have partially\nknown support from the prior estimation results. For example, Vaswani and Lu proposed modifiedCS algorithm [4] which addresses the exact reconstruction of noiseless case with partially known\nsupport:\nmin k(x(t))I(t\u22121)c k0 , subject to b(t) = Ax(t), t = 1, 2, * * * ,\n\n(2)\n\nx(t)\n\nwhere I(t \u2212 1) is the previously estimated support, and (x(t))I(t\u22121)c denotes a subvector after\nremoving the elements that correspond to the index set I(t \u2212 1). Suppose, furthermore, k =\n|suppx(t)|0 , u = |I(t) \\ I(t \u2212 1)|, and e = |I(t \u2212 1) \\ I(t)|. Then, if the restricted isometry\n\nconstant (RIP) for the sensing matrix A satisfies\n\u03b4k+e+u < 1,\n\n(3)\n\nthen the solution x(t) of Eq. (2) is the unique solution [4]. This is much weaker than 0 \u2264 \u03b42k < 1\nfor the original SMV-CS problem [5], in case of slowly time varying support with u \u226a k and\n\n\f3\n\ne \u226a k. They further showed an l1 convex relaxation of Eq. (2) can provide the same l0 solution\n\nof Eq. (2), if the following RIP condition is satisfied:\n2\n2\n< 1,\n+ 2\u03b4k+e+u\n2\u03b42u + \u03b43u + \u03b4k+e\u2212u + \u03b4k+e\n\nwhich is again relaxed sampling requirement than that of original CS problem \u03b42k <\n\n(4)\n\u221a\n\n2\u22121\n\n[5]. Therefore, exploiting the temporal redundancy has significant impact for reducing sampling\nrequirement for dynamic support tracking.\nRather than solving the tracking problem Eq. (1) at each time, batch type approaches such as\nT-SBL (temporal sparse Bayesian learning) [1] collect the multiple snapshot data (for example,\n{b(t)}N\nt=1 ) and process them together to estimate the dynamic varying support. Note that if the\n\nsupport changes slowly over time, then the resulting collection of problem becomes an multiple\nmeasurement vector problem. Accordingly, T-SBL converts the resulting MMV problem into a\nblock-sparse SMV problem, after which each block statistics are modeled using a specific Gaussian\nform temporal correlation structure. The update rule using the expectation-maximization (EM)\nmethod and its accelerated version can be then used to solve the resulting Bayesian problem [1].\nHowever, these approaches for dynamic support tracking is with SMV-CS framework and their\nperformance guarantees is in a probabilistic sense. In practice, there are many situations where\nwe can obtain multiple measurement vector information for time varying objects. For example, in\nsingle-input multiple-output (SIMO) multiple access channel (MAC), multiple antenna can observe\nlinear combination of individual codewords multiplied by the unknown channel gain from the\nindividual user [6]. In parallel MR cardiac imaging, multiple coils simultaneous obtain k-space\nmeasurements of temporally varying hearts with distinct coil sensitivities. In EEG/MEG source\nlocalization problem, the dipole moments can be assumed relatively stationary during a short time\nwindow from which multiple snapshot of the sensor measurement can be obtained. All these\nexamples acquire multiple measurement of the unknown signal vectors that share the same support\nwith different weighting through identical sensing matrices.\nA fundamental question under this setup is what kind of diversity gain we can obtain over\nSMV-CS support tracking. To our knowledge, we are not aware of any prior investigation in this\nregard. One of the main contributions of this paper is to show that a multiple measurement vector\n(MMV) framework not only extend the SMV counterpart, but also provides a unique advantage\nof \"deterministic\" support tracking for slow varying support estimation. Recall that MMV can\n\n\f4\n\nmeasure multiple information of a set of vector that share the same sparsity pattern through the\nidentical sensing matrix. This paper shows that this joint sparsity pays off significantly in dynamic\nsupport tracking by relaxing probabilistic guarantee to a deterministic guarantee. The feasibility of\nthe exact support tracking has significant impacts in practice.\nThe breakthrough is based on our novel compressive multiple signal classification (CS-MUSIC)\nalgorithm in MMV compressed sensing problem [7], in which a part of supports are found\nprobabilistically using the conventional CS, after which the remaining supports are determined\ndeterministically using the generalized MUSIC criterion. In addition, CS-MUSIC allows us to find\nall k support as long as at least k \u2212 r + 1 support out of any k-support estimate are correct [8],\nwhere r denote the rank of the measurement matrix. This result provides an important clue for\ndeterministic and exact dynamic support tracking under MMV setup, in which the probabilistic\ncompressed sensing support estimation step is replaced by the support estimate from the previous\nsnapshots, after which the CS-MUSIC algorithm eliminates the incorrect portion of previous time\npoint support estimation and then add newly updated support deterministically. This update scheme\nguarantees the exact support tracking in noiseless case under an appropriate sampling condition.\nOther contributions of our method include that the support error does not propagate along time\ndue to the self-correction step. Furthermore, using large system model, we can derive conditions\nwith which the proposed algorithm correct track the time varying support even in noisy cases.\nWe believe that with these noticeable advantages of our algorithm we may find many important\napplications in radar, communication as well as biomedical application.\nThis paper consist of following. Section II reviews the compressive MUSIC and support correction criterion for MMV setup. In Section III, we derive our main theoretical results on sampling\ncondition for deterministic support tracking. Numerical results are given in Section IV, which is\nfollowed by conclusion in Section V.\n\nA. Notations and Mathematical Preliminaries\nThroughout the paper, xi and xj correspond to the i-th row and the j -th column of matrix X ,\nrespectively. When S is an index set, X S , AS corresponds to a submatrix collecting corresponding\nrows of X and columns of A, respectively. The following definitions are also used throughout the\npaper.\n\n\f5\n\nDefinition 1: [9] The rows (or columns) in Rn are in general position if any n collection of\nrows (or columns) are linearly independent.\nDefinition 2: [10] Spark(A) denotes the smallest number of linearly dependent columns of a\nmatrix A.\nDefinition 3 (Restricted Isometry Property (RIP)): A sensing matrix A \u2208 Rm\u00d7n is said to have\na k-restricted isometry property (RIP) if there exist left and right RIP constants 0 < \u03b4kL , \u03b4kR < 1\nsuch that\n(1 \u2212 \u03b4kL )kxk2 \u2264 kAxk2 \u2264 (1 + \u03b4kR )kxk2\n\nfor all x \u2208 Rn such that kxk0 \u2264 k. A single RIP constant \u03b4k = max{\u03b4kL , \u03b4kR } is often referred to\nas the RIP constant.\nII. MMV C OMPRESSIVE S ENSING\n\nUSING\n\nC OMPRESSIVE MUSIC: A R EVIEW\n\nLet m, n and r be a positive integers (m < n) that represents the number of sensor elements,\nthe ambient space dimension, and the number of snapshots, respectively. Suppose that we are\ngiven a multiple-measurement vector B \u2208 Rm\u00d7r , X = [x1 , * * * , xr ] \u2208 Rn\u00d7r , and a sensing matrix\nA \u2208 Rm\u00d7n . A canonical form MMV problem [7] is given by the following optimization problem:\nminimize\nsubject to\n\nkXk0\n\n(5)\n\nB = AX,\n\nwhere kXk0 = |suppX| = k, suppX = {1 \u2264 i \u2264 n : xi 6= 0}, and the measurement matrix B is\nfull rank, i.e. rank(B) = r \u2264 kXk0 .\nRecall that every MMV problem can be converted to a canonical form MMV using a singular\nvalue decomposition and dimension reduction as described in [7]. Now, We can easily expect that\nthe diversity due to the joint sparsity can improve the recovery performance over SMV compressed\nsensing. Indeed, Chen and Huo [11], Feng and Bresler [12] and recently Davies and Elder [13]\nshowed that X \u2208 Rn\u00d7r is the unique solution of AX = B if and only if\nkXk0 <\n\nspark(A) + rank(B) \u2212 1\n\u2264 spark(A) \u2212 1 .\n2\n\n(6)\n\nNote that we can expect rank(B)/2 gains over SMV thanks to the MMV diversity. Furthermore,\nFeng and Bresler [12] showed that the noiseless l0 bound in Eq. (6) is achievable using MUSIC\nalgorithm as long as r = rank(B) = k. More specifically, suppose that the columns of a sensing\n\n\f6\n\nmatrix A \u2208 Rm\u00d7n are in general position. Then, according to [12], [14], for any j \u2208 {1, * * * , n},\nj \u2208 suppX if and only if\n\nQ\u2217 aj = 0,\n\n(7)\n\nwhere Q \u2208 Rm\u00d7(m\u2212r) consists of orthonormal columns such that Q\u2217 B = 0 so that R(Q)\u22a5 =\nR(B), which is often called \"noise subspace\". Using the compressive sensing terminology, Eq. (7)\n\nimplies that the recoverable sparsity level by MUSIC (with a probability 1 for the noiseless\nmeasurement case) is given by\nkXk0 < m = spark(A) \u2212 1,\n\n(8)\n\nwhere the last equality comes from the definition of the spark. Therefore, the l0 bound (6) can be\nachieved by MUSIC bound in (8) when r = k [12].\nHowever, for any r < k, the MUSIC condition (7) does not hold. This is a major drawback\nof MUSIC compared to CS algorithms that allow perfect reconstruction with a extremely large\nprobability by increasing the sensor elements m. One the other hand, even thought the conventional\nCS algorithms for MMV such as simultaneous OMP (S-OMP), p-thresholding [15], [16] have good\nrecovery performance when r \u226a k, but they exhibit performance saturation as r increases and\nnever achieve the l0 bound with finite snapshot even in noiseless case. Recently, we showed that\nthis drawback of the existing approaches can be overcome by the following generalized MUSIC\ncriterion [7].\nTheorem 1: [7] Assume that A \u2208 Rm\u00d7n , X \u2208 Rn\u00d7r , and B \u2208 Rm\u00d7r satisfy AX = B .\nFurthermore, we assume that kXk0 = k and A satisfies the RIP condition with the left RIP constant\nL\n< 1. If we are given Ik\u2212r \u2282 suppX with |Ik\u2212r | = k \u2212 r and AIk\u2212r \u2208 Rm\u00d7(k\u2212r) ,\n0 < \u03b42k\u2212r+1\n\nwhich consists of columns whose indices are in Ik\u2212r , then for any j \u2208 {1, * * * , n} \\ Ik\u2212r ,\ni\nh\na\u2217j PR(Q) \u2212 PR(PR(Q) AIk\u2212r ) aj = 0\n\n(9)\n\nif and only if j \u2208 suppX .\nL\n< 1 for generalized MUSIC is equivalent to\nIn [7], we demonstrate that the condition 0 < \u03b42k\u2212r+1\n\nl0 bound (6), which implies that a computational expensive combinatorial optimization problem is\n\nnow reduced to |Ik\u2212r | support estimation from the original |Ik | support estimation1 . Furthermore,\n1\n\nWhen r = k, the condition (9) is the same as the MUSIC criterion (7) and no combinatorial algorithm is necessary.\n\n\f7\n\nby Theorem 1, we can develop a computationally tractable relaxation algorithm called Compressive\nMUSIC (CS-MUSIC) that relaxes the combinatorial optimization step of finding Ik\u2212r support using\nthe conventional MMV-CS algorithms [7]. The algorithm can be stated as following:\n\u2022\n\n(Step 1: compressed sensing step) Find k \u2212 r indices of suppX by any MMV compressive\nsensing algorithms such as 2-thresholding or SOMP. Let Ik\u2212r be set of selected indices and\nS = Ik\u2212r .\n\n\u2022\n\n(Step 2: generalized MUSIC step) For j \u2208 {1, * * * , n} \\ Ik\u2212r , calculate the quantities \u03b7(j) =\n/ Ik\u2212r . Make an ascending ordering of \u03b7(j), j \u2208\n/ Ik\u2212r\na\u2217j [PR(Q) \u2212 PR(PR(Q) AIk\u2212r ) ]aj for all j \u2208\n\nand choose indices that correspond to the first r elements and put these indices into S .\nIn compressive MUSIC, we determine k \u2212 r indices of suppX with CS-based algorithms such\nas 2-thresholding or S-OMP rather than l0 optimization, where the exact identification of k \u2212 r\nindices is a probabilistic matter. After that process, we recover remaining r indices of suppX with\na generalized MUSIC criterion, which is given in Theorem 1, and this reconstruction process is\ndeterministic. This hybridization makes the compressive MUSIC applicable for all ranges of r ,\noutperforming all the existing methods. Similar observation have been made independently by Lee\nand Bresler [17] in their subspace augmented MUSIC (SA-MUSIC) algorithm.\nTo analyze the performance of the compressive MUSIC, we should find the number of measurements with which we can identify the support of X . Due to the reduction of uncertainty from\n|Ik | to |Ik\u2212r |, we can expect more relaxed sampling condition. In [7], we derived the sampling\n\nrequirements when subspace S-OMP or 2-thresholding is used as a compressed sensing step for\ncompressive MUSIC. The results can be summarized as following. The number of measurements\nfor subspace S-OMP for partial support recovery exhibits two distinct characteristics depending\non the number of the measurement vectors. First, if the number of multiple measurement vectors\nr is sufficiently small, then the number of samples for S-OMP is reciprocally proportional to the\n\nnumber of multiple measurement vectors. On the other hand, we have sufficiently large number of\nsnapshots such that limn\u2192\u221e (log n)/r is close to 0, then the number of measurements for S-OMP\nvaries from 4k to k according to the ratio of r and k so that the log n is not necessary. In particular,\nif the number of snapshots approaches the sparsity k, then we can identify the indices of suppX\nwith only k measurements, which is equivalent to the required number of multiple measurement\nvectors for the success of conventional MUSIC. Furthermore, we demonstrated that the required\n\n\f8\n\nSNR for the success of support recovery can be reduced and when the asymptotic ratio of the\nnumber of snapshots and the sparsity level (that is, limn\u2192\u221e r/k) is nonzero in the large system\nlimit, only finite SNR is required, which is significant improvement over SMV-CS.\nIn the original form of CS-MUSIC, the performance is, however, very dependent on the selection\nof k \u2212 r correct indices of the support of X . In practice, even though the consecutive k \u2212 r steps\nof S-OMP may not be correct, there are chances that among the estimates of k-sparse solution,\npart of the supports could be correct. Hence, if we have a mean to identify k \u2212 r correct support\nin any order out of any k-sparse, then we can expect that the performance of the compressive\n\u0001\nk\nMUSIC will be improved. Of course, when k\u2212r\nis small, we may apply the exhaustive search,\nbut if both k \u2212 r and r are not small, then the exhaustive search is hard to apply so that we have to\nfind some alternative method to identify the correct indices from the estimate of suppX . Indeed,\nthe following support selection criterion can address the problem [8].\nTheorem 2: [8] Assume that we have a canonical MMV model AX = B where A \u2208 Rm\u00d7n ,\nX \u2208 Rn\u00d7r , kXk0 = k and r < k < m < n. If there is an index set Ik \u2282 {1, * * * , n} such that\n|Ik | = min{k, spark(A) \u2212 r} and |Ik \u2229 suppX| \u2265 k \u2212 r + 1, then for any j \u2208 Ik , j \u2208 suppX if\n\nand only if\nPQk,j aj = 0,\n\n(10)\n\nwhere Qk,j is the orthogonal complement for R([B AIk \\{j} ]), AIk \\{j} consists of columns of A\n\u22a5\nwhose index belongs to Ik \\{j} and PR([B\n\nAIk \\{j} ])\n\nis the orthogonal projection on R([B AIk \\{j} ])\u22a5 .\n\nIn particular, if the columns of A are in general position, then we can take index set Ik with\n|Ik | = min{k, m \u2212 r + 1}. Also, if A has an RIP condition with 0 < \u03b42k < 1, then we can take\n|Ik | = k since r \u2264 k.\n\nTheorem 2 informs us that we only require the success of partial support recover out of k-sparse\nestimate, rather than k \u2212 r consecutive correct CS step [7]. Accordingly, the compressive MUSIC\nwith optimized partial support is then performed by following procedure.\n\u2022\n\n[Step 1: compressed sensing] Estimate k indices of suppX by any MMV compressive\nsensing algorithm. Let Ik be the set of indices which are taken in step 1.\n\n\u2022\n\n[Step 2: support deletion] For j \u2208 Ik , calculate the quantities \u03b6(j) = kPQk,j aj k2 . Make\nan ascending ordering of \u03b6(j), j \u2208 Ik and choose indices that corresponds the first k \u2212 r\nelements and put these indices into S and remove the remaining ones.\n\n\f9\n\n\u2022\n\n[Step 3: support addition] For j \u2208 {1, * * * , n} \\ S , calculate the quantities\n\u03b7(j) = a\u2217j [PR(Q) \u2212 PR(PR(Q) AIk\u2212r ) ]aj .\n\nMake an asending ordering of \u03b7(j), j \u2208\n/ S and choose indices that correspond to the first r\nelements and put these indices into S .\nThe step 1 in the above algorithm need not to be greedy so that we can also apply the convex\noptimization algorithm such as l2,1 minimization [18] or belief propagation [19].\nIII. D ETERMINISTIC S UPPORT T RACKING\n\nUSING\n\nC OMPRESSIVE MUSIC\n\nA. Noiseless Cases\nIn this section, we will show how the compressive MUSIC with optimized partial support can\nbe used for dynamic support tracking, whose joint support suppX(t) changes slowly along time\nas illustrated in Fig.1. First, we define a canonical form of dynamic MMV problem.\nDefinition 4: A canonical form of noiseless dynamic MMV problem is given by set of MMV\nproblem with time varying k-sparse vectors X(t) \u2208 Rn\u00d7r that satisfies Y (t) = AX(t) as described\nin following formulation:\nmin kX(t)k0 , subject to B(t) = AX(t), t = 0, 1, * * * ,\nX(t)\n\n(11)\n\nwhere suppX(t) = {1 \u2264 i \u2264 n : x(t)i 6= 0} and |suppX(t)| = k(t), the measurement matrix\nB(t) is full rank, i.e. rank(B(t)) \u2264 k(t). Here we assume that rank(B(t)) is constant so that we\n\nlet r := rank(B(t)).\nNote that the canonical form MMV has the additional constraints that the measurement matrix is\nfull rank and rank(B(t)) = r \u2264 k(t). This is not problematic since every dynamic MMV problem\ncan be converted into a canonical form using the following dimension reduction similar to [7] .\n\u2022\n\nSuppose we are given the following linear sensor observations: B(t) = AX(t) where A \u2208\nRm\u00d7n and X \u2208 Rn\u00d7l satisfies kX(t)k0 = k(t).\n\n\u2022\n\nCompute the SVD as B(t) = U Dr V \u2217 , where Dr is an r \u00d7 r diagonal matrix, V \u2208 Cl\u00d7r\nconsists of right singular vectors, and r = rank(B), respectively.\n\n\u2022\n\nReduce the dimension as BSV (t) = B(t)V and XSV (t) = X(t)V .\n\n\u2022\n\nThe resulting canonical form MMV becomes BSV (t) = AXSV (t).\n\n\f10\n\nFig. 1.\n\nMMV problem for slowly time varying sparsity pattern.\n\nWe can easily show that rank(BSV ) = r \u2264 k(t) and full rank and the sparsity k(t) := kX(t)k0 =\nkXSV (t)k0 with probability 1. Therefore, without loss of generality, the canonical form of dynamic\n\nMMV in Definition 4 is assumed throughout the paper.\nFor such dynamic support tracking, we can apply our CS-MUSIC algorithm. However, if the\nnumber of snapshots is not sufficient, the amount of support estimation that need to be done by\nCS step is significantly larger than those recovered by the deterministic generalized MUSIC step.\nSince CS step allows the support recovery in a probabilistic sense, it is more prone to error; so we\nare interested in finding a deterministic algorithm that significantly outperform the existing one.\nThe following Theorem 3 shows that if we have a correct estimation for the initial support I(0)\nof X(0) and the support changes are sufficiently small and the sparsity k(t) is fixed for all time\npoint, then we can recursively identify the support of time-varying input signals in a deterministic\nmanner.\nTheorem 3: Suppose a noiseless canonical form of dynamic MMV problem satisfies\n|suppX(t) \\ suppX(t \u2212 1)| \u2264 r \u2212 1,\n\n(12)\n\nfor all t = 1, 2, * * * . Furthermore we assume that r \u2264 k(t) \u2264 kmax for a positive integer kmax and\n0 \u2264 \u03b42kmax (A) < 1. Then, if we have a correct initial support estimation for X(0), then we can\n\nidentify the correct support for all t > 0 by applying the following procedure recursively:\n\u2022\n\n[Initial support estimation] Let I(t \u2212 1) be the support estimation of X(t \u2212 1);\n\n\u2022\n\n[Support deletion] Find an index set I(t)a \u2282 I(t \u2212 1) such that I(t)a := {j \u2208 I(t \u2212 1) :\na\u2217j PQ(t)k,j aj = 0}, where Q(t)k.j is the orthogonal complement for\nR[B(t), AI(t\u22121)\\{j} ];\n\n\f11\n\n\u2022\n\n[Support addition] Find an index set I(t) such that\nI(t) = {j : a\u2217j [PR(Q(t)) \u2212 PR(PR(Q(t))AI(t)a ) ]aj = 0}, where Q(t) \u2208 Rm\u00d7(m\u2212r) consists of\n\northonormal columns such that Q(t)\u2217 B(t) = 0;\n\u2022\n\nSet k\u0302(t) := |I(t)| be the sparsity estimate for X(t) and I(t) be the support estimate for X(t).\nProof: See Appendix A.\n\nL\nL\n(A) < 1.\n(A) < 1, instead of 0 \u2264 \u03b42k\nIn Theorem 3, we assume the RIP condition 0 \u2264 \u03b42k\nmax \u2212r+1\nmax\n\nL\n(A) < 1, when r > 1 + kmax \u2212 k(t)/2, we may\nIf we assuming the RIP condition 0 \u2264 \u03b42k\nmax \u2212r+1\n\nhave |Ik | < k(t). However, we can modify the support deletion procedure in Theorem 3 as the\nfollowing, under the condition |suppX(t) \\ suppX(t \u2212 1)| \u2264 kmax /2.\n\u2022\n\n[Support deletion] Find an index set I(t)a \u2282 I(t \u2212 1) such that I(t)a := {j \u2208 I(t \u2212 1) :\na\u2217j PQ(t)k,j aj = 0}, where Q(t)k.j is the orthogonal complement for R[B\u0303(t), AI(t\u22121)\\{j} ] and\nB\u0303(t) consists of 1 + [ kmax\n2 ] columns of B(t).\n\nB. Noisy Cases\nIn practice, the measurements are noisy, so the theory we derived for noiseless measurement\nshould be modified. In the noisy case, when the sparsity are known a priori and does not change\nalong time, we can apply the following procedure.\n\u2022\n\nLet t = 0 and let I(0) be the support estimation of X(0).\n\n\u2022\n\nFor all t = 1, 2, * * * , do\n\u2013 Let I(t) = \u2205.\n\u2013 For all j \u2208 I(t \u2212 1), calculate the quantities \u03b6(j) = kPQ(t)j,k aj k2 .\n\u2013 Make an ascending ordering of \u03b6(j) and choose indices that correspond to the first k \u2212 r\nelements and put these indices into I(t).\ni\nh\n\u2013 For j \u2208 {1, * * * , n}\\I(t), calculate the quantities \u03b7(j) = a\u2217j PR(Q(t)) \u2212 PR(PR(Q(t))AI(t) ) aj .\n\u2013 Make an ascending ordering of \u03b7(j), j \u2208\n/ I(t) and choose indices that correspond to the\nfirst r indices and add these indices to I(t).\n\u2013 I(t) is the estimation of suppX(t) and let t = t + 1.\n\nHowever, if the sparsity changes along time, in the noisy cases, some of the steps in Theorem\n3 should be modified as follows:\n\n\f12\n\n\u2022\n\n[Support deletion] Set \u01eb1 > 0 and find an index set I(t)a such that I(t)a = {j \u2208 I(t \u2212 1) :\na\u2217j PQ(t)k,j aj < \u01eb1 } where Q(t)k,j is the orthogonal complement for R[Y (t) AI1 (t)\\{j} ], where\nI1 (t) \u2282 I(t \u2212 1) such that\nnrank[Y (t) AI1 (t) ] = nrank[Y (t) AI(t\u22121) ] = r + |I1 (t)|,\n\nwhere nrank(A) denotes the numerical rank of A.\n\u2022\n\n[Support addition] Set \u01eb2 > 0 and find an index set I(t)b such that\nI(t)b = {j \u2208\n/ I(t)a : a\u2217j PR([Y (t)\n\nAI2 (t) ])\u22a5 aj\n\n< \u01eb2 },\n\nwhere an index set I2 (t) \u2282 I(t)a such that\nnrank[Y (t) AI2 (t) ] = nrank[Y (t) AI(t)a ] = r + |I2 (t)|.\n\nIn this section, we derive sufficient conditions for the threshold values and signal to noise ratio\nthat guarantee the correct identification of time varying support. For CS-MUSIC [7], we derived an\nexpression of SNR and the minimum number of sensor elements. Even though these derivation is\nbased on a large system model with a Gaussian sensing matrix, it has provided very useful insight.\nTherefore, we employed a large system model to derive a sufficient condition for the success of\nproposed algorithm.\nDefinition 5: A large system noisy canonical form of dynamic MMV is defined as an estimation\nproblem of k(t)-sparse vectors X(t) \u2208 Rn\u00d7r that shares a common sparsity pattern through multiple\nnoisy snapshots Y (t) = AX(t) + N (t) using the following formulation:\nminimize\nsubject to\n\nkX(t)k0\n\n(13)\n\nY (t) = AX(t) + N (t),\n\nwhere A \u2208 Rm\u00d7n is a random matrix with i.i.d. N (0, 1/m) entries, N = [n1 , * * * , nr ] \u2208 Rm\u00d7r\nis an additive noise matrix, m \u2192 \u221e, k \u2192 \u221e as n \u2192 \u221e and rank(AX(t)) = r(t) \u2264 k(t) =\nkX(t)k0 . Here, we assume that \u03c1 := limn\u2192\u221e m/n > 0 and \u03b3 = limn\u2192\u221e kmax /m > 0, \u03b1 :=\nlimn\u2192\u221e r/kmax \u2265 0 exist and \u03b1 \u2264 1 \u2212 \u01eb for some 0 < \u01eb < 1.\n\nUnder the large system model, we have the following theorem.\nTheorem 4: Consider the large system model dynamic MMV in Definition 5. Suppose a minimum SNR satisfies\nSNRmin (Y (t)) :=\n\n4(\u03ba(B(t)) + 1)\n\u03c3min (B(t))\n>1+\n,\nkN k\n1 \u2212 \u03b3(1 + \u03b1)\n\n(14)\n\n\f13\n\nwhere \u03c3min (B(t)) is the minimum singular value for B(t), kN k is the spectral norm of N \u2208 Rm\u00d7r\nand B(t) is the noiseless measurements, and \u03b1 = limn\u2192\u221e r/kmax , \u03b3 = limn\u2192\u221e kmax /m. Then,\nfor the noisy canonical form dynamic MMV problem for slowly time varying pattern that satisfies\nEq. (12), the threshold values for support deletion and addition criterion to the correct partial\nsupport for X(t) are given by\n\u01eb1 := (1 \u2212 \u03b3(1 + \u03b1))/2,\n\n\u01eb2 := (1 \u2212 \u03b3)/2.\n\n(15)\n\nProof: See Appendix B.\nIV. NUMERICAL RESULTS\nThe first simulation is to demonstrate the performance of the proposed method to solve the\ntime varying MMV problem in Eq. (11) for different number of changes in supports at each\ntime. We declared the algorithm as a success if the estimated support is the same as the true\nsuppX , and the success rates were averaged for 5000 experiments. The simulation parameters\n\nwere as follows: m = 40, n = 100, r = 9, and k \u2208 {1, 2, * * * , 30}, respectively. Elements of\nsensing matrix A were generated by i.i.d. Gaussian random variable\n\n\u221a1 N (0, 1),\nm\n\nand Gaussian\n\nnoise of SNR = 40dB was added to each measurement vectors. At each time point, X(t)suppX(t) is\ngenerated by N (0, 1). Fig.2 shows the recovery rates of time varying MMV problem using support\ntracking method for t = 1, 2, * * * , 5 when the number of changed supports are 4, 6, 7, and 8 at\neach time point for Fig.2(a)\u223c(d), respectively. We used CS-MUSIC algorithm with S-OMP and\nthen applied optimized partial support selection at t = 1, and time varying supports are estimated\nby support tracking method recursively from t = 2 to t = 5. In Fig.2, we can observe that the\nperformance gracefully decreases as the number of changes in supports increases. An interesting\nobservation is that the performance of the proposed method rather improves over time in Fig.2(a)\nand (b). However, the recovery ratio is getting lower but converges over time when the number of\nchanges in supports is close to the upper bound r \u2212 1 for perfect recovery in noiseless case.\nNext, we applied the proposed algorithm to target tracking problem in 2D image and compared\nit to MUSIC algorithm. The first row of Fig.3 indicates the original targets moving toward the\ndirection of red arrows over time. Each column (from left to right) indicates the sampled image\nat t = 1, 13, 27, and t = 41, respectively. The simulation setting is the same with the previous\none except m = 50, n = 900, t = 1, 2, * * * , 45, and each target have a chance to move with\n\n\f14\n\nFig. 2.\n\nRecovery rates of time varying MMV problem using support tracking method when m = 40, n = 100, r = 9,\n\nSNR= 40dB, and t = 1, 2, * * * , 5. The number of changes in supports at each time point is (a) 4, (b) 6, (c) 7, and (d)\n8.\n\nprobability of\n\n1 r\u22121\n2 k\n\nat each time point. The number of target k is 24. Here, we considered the\n\nnumber of measurement vectors is 50 in the resting state, and used MUSIC algorithm to find\nsupports at t = 0. The second and third row of Fig.3 indicate the results of support tracking\nmethod and MUSIC algorithm, respectively. Note that the proposed method successfully follows\nthe movement of original targets.\nV. CONCLUSION\nThis paper expanded the sparse recovery with partially known supports in single measurement\nvector problem to multiple measurement vector problem with joint sparsity and proposed the\nsupport tracking algorithm to recover the slowly time varying supports. It is based on the recently\ndeveloped compressive MUSIC algorithm with optimized partial support selection. The estimated\nsupports at previous time can be used in optimized partial support selection to recover partial\nsupports at current time and it can be used in generalized MUSIC criterion to find remaining\nsupports. We also provided the maximum allowable number of changes in supports with support\n\n\f15\n\nFig. 3.\n\nThe results of the target tracking problem in 2D image when m = 50, n = 900, k = 24, and SNR= 40dB. We\n\nset r = 50 when t = 0, and r = 9 for t > 0. The first row indicates the original targets moving toward the direction of\nred arrows over time. The second and third row indicate the results of support tracking method and MUSIC algorithm,\nrespectively. Each column (from left to right) indicates the sampled image at t = 1, 13, 27, and t = 41, respectively.\n\ntracking algorithm for exact reconstruction in noiseless case. Numerical results demonstrated that\nthe proposed algorithm reliably reconstructs the time varying supports for various level of changes\nand successfully solves the target tracking problem in 2D image.\nA PPENDIX A\nProof: We only need to show that if we have a correct support for X(t \u2212 1), then we can also\nobtain a correct support estimation for X(t) by the support selection criterion and the generalized\nMUSIC criterion. By the assumption, we have m \u2265 2kmax \u2265 k(t \u2212 1) + r so that if we have\n|suppX(t) \u2229 I(t \u2212 1)| \u2265 k(t) \u2212 r + 1, then by Theorem 2 we have for any j \u2208 I(t \u2212 1),\nj \u2208 suppX(t) if and only if a\u2217j PQ(t)k,j aj = 0\n\nwhere Q(t)k,j is the orthogonal complement of R([Y (t) AI(t\u22121)\\{j} ). Since we have a noiseless\nMMV problem with slowly time varying pattern, we have |suppX(t) \\ suppX(t \u2212 1)| \u2264 r \u2212 1 so\n\n\f16\n\nthat we have |suppX(t) \u2229 I(t \u2212 1)| \u2265 k(t) \u2212 r + 1 and we can identify the correct partial support\nof X(t) which has at least k(t) \u2212 r + 1 elements. Then, if we let I(t)a be the set of indices such\nthat\nI(t)a = {j \u2208 I(t \u2212 1) : a\u2217j PQ(t)k,j aj = 0},\n\nwe have I(t)a \u2282 suppX and R([Y (t) AI(t)a ]) \u2282 R(AsuppX(t) ). On the other hand, if we take a\nset I(t, r) \u2282 I(t)a \u2282 suppX such that |I(t, r)| = k(t) \u2212 r , we have\nR([Y (t) AI(t)a ]) \u2283 R([Y (t) AI(t,r) ]) = R(AsuppX(t) )\n\nwhich implies R([Y (t) AI(t)a ) = R(Y (t) AI(t,r) ). Since 0 \u2264 \u03b42k(t)\u2212r+1 (A) \u2264 \u03b42kmax \u2212r+1 (A) < 1,\nwe can apply the generalized MUSIC criterion with I(t, r) \u2282 suppX where |I(t, r)| = k(t) \u2212 r .\nFor j \u2208 I(t, r), we can easily see that\ni\nh\na\u2217j PR(Q(t)) \u2212 PR(PR(Q(t))AI(t)a ) aj = a\u2217j PR([Y\n\nAI(t)a ])\u22a5 aj\n\n= a\u2217j PR([Y\n\nAI(t,r) ])\u22a5 aj\n\n= 0.\n\nOn the other hand, for j \u2208\n/ I(t, r), by the generalized MUSIC criterion, we have j \u2208 suppX(t) if\nand only if\ni\nh\ni\nh\na\u2217j PR(Q(t)) \u2212 PR(PR(Q(t))AI(t)a ) aj = a\u2217j PR(Q(t)) \u2212 PR(PR(Q(t))AI(t,r) ) aj = 0.\nSince I(t, r) \u2282 suppX , we have j \u2208 suppX if and only if\ni\nh\na\u2217j PR(Q(t)) \u2212 PR(PR(Q(t))AI(t)a ) aj = 0.\nHence, |I(t)| = k(t) and I(t) = suppX(t).\nA PPENDIX B\nProof: Here, we let B(t) = AX(t), \u03c3min (B(t))(or \u03c3min (B(t))) be the minimum (or the\nmaximum) nonzero singular value of B(t). Then Y (t) = B(t) + N (t) is also of full column rank\nif kN (t)k < \u03c3min (B(t)). By [7], for such an N (t), we have\nkPR(Y (t)) \u2212 PR(B(t)) k \u2264\n\n2[\u03c3max (B(t)) + \u03c3min (B(t))]kN (t)k\n.\n\u03c3min (B(t))(\u03c3min (B(t)) \u2212 kN (t)k)\n\n(16)\n\nBy the projection update rule, we have\nPR([B(t)\n\nAI1 (t)\\{j} ])\n\n\u22a5\n= PR(AI1 (t)\\{j} ) + PR(PR(A\n\nI1 (t)\\{j} )\n\nB(t))\n\n(17)\n\n\f17\n\nand\nPR([Y (t)\n\nAI1 (t)\\{j} ])\n\n\u22a5\n= PR(AI1 (t)\\{j} ) + PR(PR(A\n\nI1 (t)\\{j} )\n\nY (t)) .\n\n(18)\n\nSince [B(t) AI1 (t)\\{j} ] and [Y (t) AI1 (t)\\{j} ] are of full column rank, by applying (17) and (18) as\ndone in [17], we have\n\u22a5\nkPR([B(t)\n\nAI1 (t)\\{j} ])\n\n\u22a5\n\u2212 PR([Y\n(t)\n\nAI1 (t)\\{j} ]) k\n\n\u22a5\n= kPR(PR(A\n\nI1 (t)\\{j} )\n\nB(t))\n\n\u22a5\n\u2212 PR(PR(A\n\nI1 (t)\\{j} )\n\nY (t)) k\n\n\u2264 kPR(B(t)) \u2212 PR(Y (t)) k.\n\n(19)\n\nThen for any j \u2208 I1 (t) \\ suppX , we have\n\u22a5\na\u2217j PR([Y\n(t)\n\nAI1 (t)\\{j} ]) aj\n\n\u22a5\n= a\u2217j PR([B(t)\nAI1 (t)\\{j} ]) aj\nh\n\u22a5\n\u22a5\n+ a\u2217j PR([Y\n(t) AI (t)\\{j} ]) \u2212 PR([B(t)\n1\n\n\u2265\n\n\u22a5\nmin a\u2217j PR([B(t)\n\nj \u2208suppX\n/\n\nAI1 (t)\\{j} ]) aj\n\nAI1 (t)\\{j} ])\n\ni\n\naj\n\n(20)\n\n\u2212 max kaj k2 kPR(Y (t)) \u2212 PR(B(t)) k.\n1\u2264j\u2264n\n\nHere, for each 1 \u2264 j \u2264 n, mkaj k2 is a chi-square random variable with degree of freedom m so\nthat we have by Lemma 3 in [20], limn\u2192\u221e max1\u2264j\u2264n kaj k2 = 1 since limn (log n)/m = 0. Further\u22a5\nmore, for any j \u2208\n/ suppX , aj is independent of PR([Y\n(t)\n\nAI(t\u22121)\\{j} ]) ,\n\n\u22a5\nso that ma\u2217j PR([Y\n(t)\n\nAI1 (t)\\{j} ]) aj\n\nis a chi-squared random variable whose degree of freedom is at least m \u2212 k(t) \u2212 r + 1 since\n\u22a5\nPR([Y\n(t)\n\nAI1 (t)\\{j} ])\n\nis a projection operator onto the orthogonal couplement of R([Y (t) AI1 (t)\\{j} ]).\n\nSince limn\u2192\u221e (log (n \u2212 k(t)))/(m \u2212 k(t) \u2212 r + 1) = 0, again by Lemma 3 in [20], we have\n\u22a5\nmin ma\u2217j PR([Y\n(t)\n\nlim\n\nn\u2192\u221e\n\nj \u2208suppX\n/\n\nAI1 (t)\\{j} ]) aj\n\nm \u2212 k(t) \u2212 r + 1\n\n\u22651\n\nso that\nlim\n\n\u22a5\nmin a\u2217j PR([Y\n(t)\n\nn\u2192\u221e j \u2208suppX\n/\n\nAI1 (t)\\{j} ]) aj\n\n\u2265 1 \u2212 \u03b3(1 + \u03b1)\n\n(21)\n\nsince k(t) \u2264 kmax for all t = 0, 1, * * * . On the other hand, if we use the definition of SNRmin (Y (t))\nand the definition of the condition number of B(t) on (16), i.e. \u03ba(B(t)) = (\u03c3max (B(t)))/(\u03c3min (B(t))),\nwe have\nkPR(Y (t)) \u2212 PR(B(t)) k \u2264\n\n1 \u2212 \u03b3(1 + \u03b1)\n2(\u03ba(B(t)) + 1)\n<\n,\nSNRmin (Y (t)) \u2212 1\n2\n\n(22)\n\nby the condition (14). Combining (20), (21) and (22), we have for any j \u2208 I(t \u2212 1) \\ suppX , we\nhave\n\u22a5\na\u2217j PR([Y\n(t)\n\nAI1 (t)\\{j} ]) aj\n\n>\n\n1 \u2212 \u03b3(1 + \u03b1)\n.\n2\n\n\f18\n\n\u22a5\nOn the other hand, for j \u2208 I(t\u22121)\u2229suppX(t), we have a\u2217j PR([B(t)\n\nAI1 (t)\\{j} ]) aj\n\n= 0 by the support\n\nselection criterion. Then, by the similar reasoning as above, we have for any j \u2208 I1 (t) \u2229 suppX(t),\nwe have\n\u22a5\na\u2217j PR([Y\n(t)\n\nAI1 (t)\\{j} ]) aj\n\n<\n\n1 \u2212 \u03b3(1 + \u03b1)\n.\n2\n\nThis completes the proof for the threshold values for support selection criterion. The proof for\n\u22a5\nthe threshold values for generalized MUSIC are the same except that ma\u2217j PR([Y\n(t)\n\nAI2 (t) ]) aj\n\nis a\n\nchi-squared random variable whose degree of freedom is m \u2212 k(t) for j \u2208\n/ suppX(t).\nACKNOWLEDGMENT\nThis work was supported by the Korea Science and Engineering Foundation (KOSEF) grant\nfunded by the Korea government (MEST) (No.2010-0000855).\nR EFERENCES\n[1] Z. Zhang and B.D. Rao, \"Sparse signal recovery with temporally correlated source vectors using joint sparse\nbayesian learning,\" IEEE J. of Selected Topics in Signal Processing, vol. 5, no. 5, pp. 912\u2013926, 2011.\n[2] D. L. Donoho, \"Compressed sensing,\" IEEE Trans. on Information Theory, vol. 52, no. 4, pp. 1289\u20131306, April\n2006.\n[3] E. Candes, J. Romberg, and T. Tao, \"Robust uncertainty principles: Exact signal reconstruction from highly\nincomplete frequency information,\" IEEE Trans. on Information Theory, vol. 52, no. 2, pp. 489\u2013509, Feb. 2006.\n[4] N. Vaswani and W. Lu, \"Modified-CS: modifying compressive sensing for problems with partially known support,\"\nIEEE Trans. Signal Process., vol. 58, no. 9, pp. 4595\u20134607, 2010.\n[5] E. Candes and T. Tao, \"Decoding by linear programming,\" IEEE Trans. on Information Theory, vol. 51, no. 12,\npp. 4203\u20134215, Dec. 2005.\n[6] Y. Jin and B.D. Rao, \"Support recovery of sparse signals in the presence of multiple measurement vectors,\" arXiv\npreprint, 2011, http://arxiv.org/pdf/1109.1895.\n[7] J. M. Kim, O. K. Lee, and J. C. Ye, \"Compressive MUSIC: a missing link between compressive sensing and array\nsignal processing,\" to appear in IEEE Trans. Inf. Theory, 2011.\n[8] J. M. Kim, O. K. Lee, and J. C. Ye, \"Compressive MUSIC with optimized partial support for joint sparse recovery,\"\nin Proc. IEEE Int. Symp. Inf. Theory (ISIT), 2011.\n[9] D. L. Donoho, \"Neighborly polytopes and sparse solution of underdetermined linear equations,\" Tech. report,\nDepartment of Statistics, Stanford University, 2005.\n[10] D. L. Donoho and M. Elad, \"Optimally sparse representation in general (non-orthogonal) dictionaries via l1\nminimization,\" Proceedings of the National Academy of Sciences of the United States of America, vol. 100, no. 5,\npp. 2197\u20132202, 2003.\n[11] J. Chen and X. Huo, \"Theoretical results on sparse representations of multiple measurement vectors,\" IEEE Trans.\non Signal Processing, vol. 54, no. 12, pp. 4634\u20134643, 2006.\n\n\f19\n\n[12] P. Feng, Universal minimum-rate sampling and spectrum-blind reconstruction for multiband signals, Ph.D.\ndissertation, University of Illinois, Urbana-Champaign, 1997.\n[13] M. E. Davies and Y. C. Eldar,\n\n\"Rank awareness for joint sparse recovery,\"\n\npreprint, 2010,\n\nhttp://arxiv.org/PS cache/arxiv/pdf/1004/1004.4529v1.pdf.\n[14] R. Schmidt, \"Multiple emitter location and signal parameter estimation,\" IEEE Trans. on Antennas and Propagation,\nvol. 34, no. 3, pp. 276\u2013280, 1986.\n[15] R. Gribonval, H. Rauhut, K. Schnass, and P. Vandergheynst, \"Atoms of all channels, unite! Average case analysis\nof multi-channel sparse recovery using greedy algorithms,\" Journal of Fourier Analysis and Applications, vol. 14,\nno. 5, pp. 655\u2013687, 2008.\n[16] Y.C. Eldar and H. Rauhut, \"Average case anlysis of multichannel sparse recovery using convex relaxation,\" IEEE\nTrans. on Information Theory, vol. 56, pp. 505\u2013519, 2010.\n[17] K. Lee and Y. Bresler,\n\n\"Subspace-augmented music for joint sparse recovery,\"\n\npreprint, 2010,\n\nhttp://arxiv.org/PS cache/arxiv/pdf/1004/1004.3071.pdf.\n[18] D. Malioutov, M. Cetin, and AS Willsky, \"A sparse signal reconstruction perspective for source localization with\nsensor arrays,\" IEEE Trans. on Signal Processing, vol. 53, no. 8, pp. 3010\u20133022, 2005.\n[19] J.M. Kim, W.H. Chang, B.C. Jung, D. Baron, and J.C. Ye, \"Belief propagation for joint sparse recovery,\" arXiv\npreprint, 2011, http://arxiv.org/pdf/1102.3289.\n[20] S. Rangan A.K. Fletcher and V.K. Goyal, \"Necessary and sufficient conditions for sparsity pattern recovery,\" IEEE\nTrans. on Inform. Theory, vol. 55, no. 12, pp. 5758\u20135772, December 2009.\n\n\f"}