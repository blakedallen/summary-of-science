{"id": "http://arxiv.org/abs/0707.3715v2", "guidislink": true, "updated": "2008-11-14T11:26:18Z", "updated_parsed": [2008, 11, 14, 11, 26, 18, 4, 319, 0], "published": "2007-07-25T11:44:04Z", "published_parsed": [2007, 7, 25, 11, 44, 4, 2, 206, 0], "title": "Exponential inequalities for self-normalized martingales with\n  applications", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0707.2594%2C0707.4398%2C0707.3683%2C0707.4185%2C0707.0318%2C0707.4250%2C0707.3131%2C0707.0621%2C0707.4489%2C0707.3589%2C0707.1444%2C0707.2362%2C0707.3273%2C0707.3044%2C0707.2024%2C0707.1590%2C0707.1897%2C0707.4242%2C0707.0686%2C0707.1526%2C0707.2153%2C0707.2824%2C0707.3913%2C0707.0176%2C0707.4219%2C0707.3971%2C0707.3986%2C0707.2093%2C0707.4214%2C0707.4089%2C0707.1273%2C0707.4178%2C0707.0744%2C0707.1054%2C0707.3189%2C0707.1267%2C0707.2461%2C0707.1205%2C0707.1196%2C0707.0843%2C0707.1717%2C0707.3715%2C0707.0273%2C0707.2535%2C0707.1475%2C0707.4048%2C0707.0734%2C0707.1021%2C0707.3646%2C0707.3718%2C0707.4342%2C0707.3576%2C0707.0025%2C0707.2566%2C0707.4189%2C0707.0069%2C0707.0223%2C0707.0760%2C0707.3764%2C0707.0103%2C0707.2692%2C0707.1117%2C0707.3466%2C0707.2772%2C0707.2246%2C0707.0830%2C0707.2545%2C0707.1282%2C0707.0385%2C0707.4526%2C0707.4598%2C0707.1248%2C0707.0697%2C0707.3058%2C0707.0028%2C0707.1888%2C0707.3923%2C0707.2144%2C0707.2340%2C0707.2456%2C0707.3173%2C0707.1708%2C0707.2544%2C0707.0350%2C0707.3688%2C0707.3508%2C0707.2000%2C0707.2623%2C0707.1994%2C0707.0882%2C0707.3910%2C0707.4246%2C0707.2076%2C0707.0193%2C0707.2174%2C0707.2748%2C0707.3679%2C0707.0994%2C0707.2522%2C0707.1302%2C0707.3481&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Exponential inequalities for self-normalized martingales with\n  applications"}, "summary": "We propose several exponential inequalities for self-normalized martingales\nsimilar to those established by De la Pe\\~{n}a. The keystone is the\nintroduction of a new notion of random variable heavy on left or right.\nApplications associated with linear regressions, autoregressive and branching\nprocesses are also provided.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0707.2594%2C0707.4398%2C0707.3683%2C0707.4185%2C0707.0318%2C0707.4250%2C0707.3131%2C0707.0621%2C0707.4489%2C0707.3589%2C0707.1444%2C0707.2362%2C0707.3273%2C0707.3044%2C0707.2024%2C0707.1590%2C0707.1897%2C0707.4242%2C0707.0686%2C0707.1526%2C0707.2153%2C0707.2824%2C0707.3913%2C0707.0176%2C0707.4219%2C0707.3971%2C0707.3986%2C0707.2093%2C0707.4214%2C0707.4089%2C0707.1273%2C0707.4178%2C0707.0744%2C0707.1054%2C0707.3189%2C0707.1267%2C0707.2461%2C0707.1205%2C0707.1196%2C0707.0843%2C0707.1717%2C0707.3715%2C0707.0273%2C0707.2535%2C0707.1475%2C0707.4048%2C0707.0734%2C0707.1021%2C0707.3646%2C0707.3718%2C0707.4342%2C0707.3576%2C0707.0025%2C0707.2566%2C0707.4189%2C0707.0069%2C0707.0223%2C0707.0760%2C0707.3764%2C0707.0103%2C0707.2692%2C0707.1117%2C0707.3466%2C0707.2772%2C0707.2246%2C0707.0830%2C0707.2545%2C0707.1282%2C0707.0385%2C0707.4526%2C0707.4598%2C0707.1248%2C0707.0697%2C0707.3058%2C0707.0028%2C0707.1888%2C0707.3923%2C0707.2144%2C0707.2340%2C0707.2456%2C0707.3173%2C0707.1708%2C0707.2544%2C0707.0350%2C0707.3688%2C0707.3508%2C0707.2000%2C0707.2623%2C0707.1994%2C0707.0882%2C0707.3910%2C0707.4246%2C0707.2076%2C0707.0193%2C0707.2174%2C0707.2748%2C0707.3679%2C0707.0994%2C0707.2522%2C0707.1302%2C0707.3481&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose several exponential inequalities for self-normalized martingales\nsimilar to those established by De la Pe\\~{n}a. The keystone is the\nintroduction of a new notion of random variable heavy on left or right.\nApplications associated with linear regressions, autoregressive and branching\nprocesses are also provided."}, "authors": ["Bernard Bercu", "Abderrahmen Touati"], "author_detail": {"name": "Abderrahmen Touati"}, "author": "Abderrahmen Touati", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/07-AAP506", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0707.3715v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0707.3715v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/07-AAP506 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "60E15, 60G42 (Primary) 60G15, 60J80 (Secondary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0707.3715v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0707.3715v2", "journal_reference": "Annals of Applied Probability 2008, Vol. 18, No. 5, 1848-1869", "doi": "10.1214/07-AAP506", "fulltext": "arXiv:0707.3715v2 [math.ST] 14 Nov 2008\n\nThe Annals of Applied Probability\n2008, Vol. 18, No. 5, 1848\u20131869\nDOI: 10.1214/07-AAP506\nc Institute of Mathematical Statistics, 2008\n\nEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED\nMARTINGALES WITH APPLICATIONS\nBy Bernard Bercu and Abderrahmen Touati\nUniversit\u00e9 Bordeaux 1 and Facult\u00e9 des Sciences de Bizerte\nWe propose several exponential inequalities for self-normalized\nmartingales similar to those established by De la Pe\u00f1a. The keystone\nis the introduction of a new notion of random variable heavy on left or\nright. Applications associated with linear regressions, autoregressive\nand branching processes are also provided.\n\n1. Introduction. Let (Mn ) be a locally square integrable real martingale\nadapted to a filtration F = (Fn ) with M0 = 0. The predictable quadratic\nvariation and the total quadratic variation of (Mn ) are respectively given by\nhM in =\n\nn\nX\n\nk=1\n\nE[\u2206Mk2 |Fk\u22121 ] and [M ]n =\n\nn\nX\n\n\u2206Mk2\n\nk=1\n\nwhere \u2206Mn = Mn \u2212 Mn\u22121 . The celebrated Azuma\u2013Hoeffding inequality\n[4, 16, 18] is as follows.\nTheorem 1.1 (Azuma\u2013Hoeffding's inequality). Let (Mn ) be a locally\nsquare integrable real martingale such that, for each 1 \u2264 k \u2264 n, ak \u2264 \u2206Mk \u2264\nbk a.s. for some constants ak < bk . Then, for all x \u2265 0,\n(1.1)\n\n\u0012\n\n\u0013\n\n2x2\n.\nP(|Mn | \u2265 x) \u2264 2 exp \u2212 Pn\n2\nk=1 (bk \u2212 ak )\n\nAnother result which involves the predictable quadratic variation (hM in )\nis the so-called Freedman inequality [13].\nTheorem 1.2 (Freedman's inequality). Let (Mn ) be a locally square\nintegrable real martingale such that, for each 1 \u2264 k \u2264 n, |\u2206Mk | \u2264 c a.s. for\nReceived July 2007; revised December 2007.\nAMS 2000 subject classifications. Primary 60E15, 60G42; secondary 60G15, 60J80.\nKey words and phrases. Exponential inequalities, martingales, autoregressive processes, branching processes.\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Applied Probability,\n2008, Vol. 18, No. 5, 1848\u20131869. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nB. BERCU AND A. TOUATI\n\nsome constant c > 0. Then, for all x, y > 0,\n(1.2)\n\n\u0012\n\n\u0013\n\nx2\nP(Mn \u2265 x, hM in \u2264 y) \u2264 exp \u2212\n.\n2(y + cx)\n\nOver the last decade, extensive study has been made to establish exponential inequalities for (Mn ) relaxing the boundedness assumption on its\nincrements. On the one hand, under the standard Bernstein condition that\nfor n \u2265 1, p \u2265 2 and for some constant c > 0,\nn\nX\n\nk=1\n\nE[|\u2206Mk |p |Fk\u22121 ] \u2264\n\ncp\u22122 p!\nhM in ,\n2\n\nPinelis [21] and De la Pe\u00f1a [8] recover (1.2). Van de Geer [12] also proves\n(1.2) replacing hM in by a suitable increasing process. On the other hand, if\n(Mn ) is conditionally symmetric which means that for n \u2265 1, the conditional\ndistribution of \u2206Mn given Fn\u22121 is symmetric, then De la Pe\u00f1a [8] establishes\nthe nice following result.\nTheorem 1.3 (De la Pe\u00f1a's inequality). Let (Mn ) be a locally square\nintegrable and conditionally symmetric real martingale. Then, for all x, y >\n0,\n(1.3)\n\n\u0012\n\nP(Mn \u2265 x, [M ]n \u2264 y) \u2264 exp \u2212\n\n\u0013\n\nx2\n.\n2y\n\nSome extensions of the above inequalities in a more general framework\nincluding discrete-time martingales can also be found in [9, 11] where the\nconditionally symmetric assumption is still required for (1.3). We also refer\nthe reader to the recent survey of De la Pe\u00f1a, Klass and Lai [10].\nBy a careful reading of [8], one can see that (1.3) is a two-sided exponential\ninequality. More precisely, if (Mn ) is conditionally symmetric, then, for all\nx, y > 0,\n(1.4)\n\n\u0012\n\nP(|Mn | \u2265 x, [M ]n \u2264 y) \u2264 2 exp \u2212\n\n\u0013\n\nx2\n.\n2y\n\nBy comparing (1.4) and (1.1), we are only halfway to Azuma\u2013Hoeffding's\ninequality which holds without the total quadratic variation [M ]n .\nThe purpose of this paper is to establish several exponential inequalities\nin the spirit of the original work of De la Pe\u00f1a [8]. In Section 2, we shall\npropose two-sided exponential inequalities involving hM in as well as [M ]n\nwithout any assumption on the martingale (Mn ). Section 3 is devoted to the\nintroduction of a new concept of random variables heavy on left or right.\nThis notion is really useful if one is only interested in obtaining a one-sided\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES\n\n3\n\nexponential inequality for (Mn ). It also provides a clearer understanding of\nDe la Pe\u00f1a's conditional symmetric assumption. We shall show in Section\n4 that this new concept allows us to prove (1.3). As in [8], we shall also\npropose exponential inequalities for (Mn ) self-normalized by [M ]n or hM in .\nSection 5 is devoted to applications on linear regressions, autoregressive and\nbranching processes. All technical proofs are postponed to the Appendix.\n2. Two-sided exponential inequalities. This section is devoted to twosided exponential inequalities involving hM in and [M ]n . We start with the\nfollowing basic lemma.\nLemma 2.1. Let X be a square integrable random variable with mean\nzero and variance \u03c3 2 > 0. For all t \u2208 R, denote\n\u0014\n\n\u0012\n\nt2\nL(t) = E exp tX \u2212 X 2\n2\n\n(2.1)\n\n\u0013\u0015\n\n.\n\nThen, we have for all t \u2208 R,\nL(t) \u2264 1 +\n\n(2.2)\n\nt2 2\n\u03c3 .\n2\n\nProof. The proof is given in Appendix A. \u0003\nOur first result, without any assumption on (Mn ), is as follows.\nTheorem 2.1.\nfor all x, y > 0,\n\nLet (Mn ) be a locally square integrable martingale. Then,\n\u0012\n\n\u0013\n\nx2\n.\nP(|Mn | \u2265 x, [M ]n + hM in \u2264 y) \u2264 2 exp \u2212\n2y\n\n(2.3)\n\nRemark 2.1. A similar result for continuous-time locally square integrable martingale may be found in the first part of Proposition 4.2.3 of\nBarlow, Jacka and Yor [5].\nFor self-normalized martingales, we obtain the following result.\nTheorem 2.2. Let (Mn ) be a locally square integrable martingale. Then,\nfor all x, y > 0, a \u2265 0 and b > 0,\n(2.4)\n\n\u0012\n\n\u0013\n\n\u0012\n\n\u0012\n\n|Mn |\nb2 y\nP\n\u2265 x, hM in \u2265 [M ]n + y \u2264 2 exp \u2212x2 ab +\na + bhM in\n2\n\n\u0013\u0013\n\n.\n\n\f4\n\nB. BERCU AND A. TOUATI\n\nMoreover, we also have\n\n(2.5)\n\n\u0012\n\n|Mn |\n\u2265 x, [M ]n \u2264 yhM in\nP\na + bhM in\n\u0012 \u0014\n\n\u0012\n\n\u2264 2 inf E exp \u2212(p \u2212 1)\np>1\n\n\u0013\n\n\u0012\n\nx2\nb2\nab + hM in\n(1 + y)\n2\n\n\u0013\u0013\u0015\u00131/p\n\n.\n\nProof. The proof is given in Appendix B. \u0003\nRemark 2.2. It is not hard to see that (2.4) and (2.5) also hold exchanging the roles of hM in and [M ]n .\n3. Random variables heavy on left or right. This section deals with our\nnew notion of random variables heavy on left or right. It allows us to improve\nLemma 2.1.\nDefinition 3.1. We shall say that an integrable random variable X is\nheavy on left if E[X] = 0 and, for all a > 0, E[Ta (X)] \u2264 0 where\nTa (X) = min(|X|, a) sign(X)\n\nis the truncated version of X. Moreover, X is heavy on right if \u2212X is heavy\non left.\nRemark 3.1. Let F be the cumulative distribution function associated\nwith X. Standard calculation leads to E[Ta (X)] = \u2212H(a) where H is the\nfunction defined, for all a > 0, by\nH(a) =\n\nZ\n\na\n\n0\n\nF (\u2212x) \u2212 (1 \u2212 F (x\u2212)) dx\n\nwhere F (x\u2212) stands for the left limit of F at point x. Consequently, X is\nheavy on left if E[X] = 0 and, for all a > 0, H(a) \u2265 0. Moreover, H is equal\nto zero at infinity as\nlim H(a) = \u2212E[X] = 0.\n\na\u2192\u221e\n\nFurthermore, one can observe that a random variable X is symmetric if and\nonly if X is heavy on left and on right.\nThe following lemma is the keystone of our one-sided exponential inequalities.\nLemma 3.1.\n\nFor a random variable X and for all t \u2208 R, let\n\u0014\n\n\u0012\n\nL(t) = E exp tX \u2212\n\nt2 2\nX\n2\n\n\u0013\u0015\n\n.\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES\n\n5\n\n(1) If X is heavy on left, then for all t \u2265 0, L(t) \u2264 1.\n(2) If X is heavy on right, then for all t \u2264 0, L(t) \u2264 1.\n(3) If X is symmetric, then for all t \u2208 R, L(t) \u2264 1.\nProof. The proof is given in Appendix A. \u0003\nWe shall now provide several examples of random variables heavy on\nleft. More details concerning these examples may be found in Appendix E.\nWe wish to point out that most of all positive random variables centered\naround their mean are heavy on left. As a matter of fact, let Y be a positive\nintegrable random variable with mean m and denote\nX = Y \u2212 m.\nDiscrete random variables.\n(1) If Y has a Bernoulli distribution B(p) with parameter 0 < p < 1, then\nX is heavy on left, heavy on right, or symmetric if p < 1/2, p > 1/2, or\np = 1/2, respectively.\n(2) If Y has a Geometric distribution G(p) with parameter 0 < p < 1, then\nX is always heavy on left.\n(3) If Y has a Poisson distribution P(\u03bb) with parameter \u03bb > 0, then X is\nheavy on left as soon as\n2 exp(\u2212\u03bb)\n\n[\u03bb] k\nX\n\u03bb\n\nk=0\n\nk!\n\n\u2265 1.\n\nOne can observe that this condition is always fulfilled if \u03bb is a positive\ninteger; see Lemma 1 of [1].\nContinuous random variables.\n(1) If Y has an exponential distribution E(\u03bb) with parameter \u03bb > 0, then\nX is always heavy on left.\n(2) If Y has a Gamma distribution G(a, \u03bb) with parameters a, \u03bb > 0, then\nX is always heavy on left.\n(3) If Y has a Pareto distribution with parameters a, \u03bb > 0, that is, Y =\na exp(Z) where Z has an exponential distribution E(\u03bb), then X is always\nheavy on left.\n(4) If Y has a log-normal distribution with parameters m \u2208 R and \u03c3 2 > 0,\nthat is, Y = exp(Z) where Z has a Normal distribution N (m, \u03c3 2 ), then\nX is always heavy on left.\n\n\f6\n\nB. BERCU AND A. TOUATI\n\n4. One-sided exponential inequalities. Our next results are related to\nmartingales heavy on left in the sense of the following definition.\nDefinition 4.1. Let (Mn ) be a locally square integrable martingale\nadapted to a filtration F = (Fn ). We shall say that (Mn ) is heavy on left\nif all its increments are conditionally heavy on left. In other words, for all\nn \u2265 1 and for any a > 0, E[Ta (\u2206Mn )|Fn\u22121 ] \u2264 0. Moreover, (Mn ) is heavy on\nright if (\u2212Mn ) is heavy on left.\nWe shall recover Theorem 1.3 under the assumption that (Mn ) is heavy\non left.\nTheorem 4.1. Let (Mn ) be a locally square integrable martingale heavy\non left. Then, for all x, y > 0,\n\u0012\n\n\u0013\n\nx2\nP(Mn \u2265 x, [M ]n \u2264 y) \u2264 exp \u2212\n.\n2y\n\n(4.1)\n\nFor self-normalized martingales, our results are as follows.\nTheorem 4.2. Let (Mn ) be a locally square integrable martingale heavy\non left. Then, for all x > 0, a \u2265 0 and b > 0,\nP\n\n\u0012\n\n\u0013\n\n(4.2)\nand, for all y > 0,\n(4.3)\n\n\u0012 \u0014\n\n\u0012\n\n\u0012\n\nb2\nMn\n\u2265 x \u2264 inf E exp \u2212(p \u2212 1)x2 ab + [M ]n\np>1\na + b[M ]n\n2\n\u0012\n\n\u0013\n\n\u0012\n\n\u0012\n\nMn\nb2 y\nP\n\u2265 x, [M ]n \u2265 y \u2264 exp \u2212x2 ab +\na + b[M ]n\n2\n\n\u0013\u0013\u0015\u00131/p\n\n,\n\n\u0013\u0013\n\n.\n\nMoreover, we also have\n\n(4.4)\n\n\u0012\n\nMn\n\u2265 x, [M ]n \u2264 yhM in\nP\na + bhM in\n\u0012 \u0014\n\n\u0012\n\n\u0012\n\n\u0013\n\nx2\nb2\n\u2264 inf E exp \u2212(p \u2212 1)\nab + hM in\np>1\ny\n2\n\n\u0013\u0013\u0015\u00131/p\n\n.\n\nProof. The proof is given in Appendix C. \u0003\nRemark 4.1. In the particular case p = 2, Theorem 4.2 is due to De\nla Pe\u00f1a [8] under the conditional symmetric assumption on (Mn ). The only\ndifference between (2.5) and (4.4) is that (1 + y) is replaced by y in the\nupper-bound of (4.4).\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES\n\n7\n\nRemark 4.2. A locally square integrable martingale (Mn ) is Gaussian if, for all n \u2265 1, the distribution of its increments \u2206Mn given Fn\u22121\nis N (0, \u2206hM in ). Moreover, (Mn ) is called sub-Gaussian if there exists some\nconstant \u03b1 > 0 such that, for all n \u2265 1 and t \u2208 R,\nE[exp(t\u2206Mn )|Fn\u22121 ] \u2264 exp\n\n(4.5)\n\n\u0012\n\n\u0013\n\n\u03b12 t2\n\u2206hM in .\n2\n\nIt is well known that if the increments of (Mn ) are bounded or if (Mn ) is\nGaussian, then (Mn ) is sub-Gaussian. In addition, if (Mn ) satisfies (4.5),\nthen inequalities (4.1), (4.2) and (4.3) hold with appropriate upper-bounds,\nreplacing [M ]n by hM in everywhere. For example, (4.2) can be rewritten as\nP\n(4.6)\n\n\u0012\n\nMn\n\u2265x\na + bhM in\n\u0012 \u0014\n\n\u0013\n\n\u0012\n\n\u2264 inf E exp \u2212(p \u2212 1)\np>1\n\n\u0012\n\nx2\nb2\nhM in\nab\n+\n\u03b12\n2\n\n\u0013\u0013\u0015\u00131/p\n\n.\n\n5. Applications.\n5.1. Linear regressions. Consider the stochastic linear regression given,\nfor all n \u2265 0, by\n(5.1)\n\nXn+1 = \u03b8\u03c6n + \u03b5n+1\n\nwhere Xn , \u03c6n and \u03b5n are the observation, the regression variable and the\ndriven noise, respectively. We assume that (\u03c6n ) is a sequence of independent and identically distributed random variables. We also assume that (\u03b5n )\nis a sequence of identically distributed random variables, with mean zero\nand variance \u03c3 2 > 0. Moreover, we suppose that, for all n \u2265 0, the random\nvariable \u03b5n+1 is independent of Fn where Fn = \u03c3(\u03c60 , \u03b51 , . . . , \u03c6n\u22121 , \u03b5n ). In order to estimate the unknown parameter \u03b8, we make use of the least-squares\nestimator \u03b8bn given, for all n \u2265 1, by\n\u03b8bn =\n\n(5.2)\n\nPn\n\n\u03c6k\u22121 Xk\n.\n2\nk=1 \u03c6k\u22121\n\nk=1\nP\nn\n\nIt immediately follows from (5.1) and (5.2) that\n\u03b8bn \u2212 \u03b8 = \u03c3 2\n\n(5.3)\nwhere\nMn =\n\nn\nX\n\nk=1\n\n\u03c6k\u22121 \u03b5k\n\nand\n\nMn\nhM in\nhM in = \u03c3 2\n\nn\nX\n\nk=1\n\n\u03c62k\u22121 .\n\n\f8\n\nB. BERCU AND A. TOUATI\n\nLet H and L be the cumulant generating functions of the sequences (\u03c62n )\nand (\u03b52n ), respectively given, for all t \u2208 R, by\nH(t) = log E[exp(t\u03c62n )] and\n\nL(t) = log E[exp(t\u03b52n )].\n\nCorollary 5.1. Assume that L is finite on some interval [0, c] with\nc > 0 and denote by I its Fenchel\u2013Legendre transform on [0, c],\nI(x) = sup {xt \u2212 L(t)}.\n0\u2264t\u2264c\n\nThen, for all n \u2265 1, x > 0 and y > 0, we have\n(5.4)\n\nP(|\u03b8bn \u2212 \u03b8| \u2265 x)\n\n\u0012\n\n\u0012\n\n(p \u2212 1)x2\nn\nH \u2212 2\n\u2264 2 inf exp\np>1\np\n2\u03c3 (1 + y)\n\n\u0013\u0013\n\n\u0012\n\n\u0012\n\n\u03c32 y\n+ exp \u2212nI\nn\n\n\u0013\u0013\n\n.\n\nRemark 5.1. Corollary 5.1 is also true if (\u03c6n , \u03b5n ) is a sequence of independent and identically distributed random vectors of R2 such that the\nmarginal distribution of \u03b5n is symmetric. By use of (4.4), inequality (5.4)\nholds replacing (1 + y) by y in the argument of H.\nRemark 5.2. As soon as the sequence (\u03b5n ) is bounded, the right-hand\nside of (5.4) vanishes since we may directly compare [Mn ] with hM in . For\nexample, assume that (\u03b5n ) is distributed as a centered Bernoulli B(p) distribution with parameter 0 < p < 1. If r = max(p, q), we clearly have for all\nn \u2265 0,\n[M ]n \u2264\n\nr2\nhM in .\npq\n\nConsequently, we immediately infer from (2.5) that for all n \u2265 1 and x > 0,\n\u0012\n\nP(|\u03b8bn \u2212 \u03b8| \u2265 x) \u2264 2 exp\n\n\u0012\n\nx2\nn\nH \u2212 2\n2\n4r\n\n\u0013\u0013\n\n.\n\nFurthermore, assume that (\u03c6n ) is distributed as a normal N (0, \u03c4 2 ) distribution with variance \u03c4 2 > 0. Then, we deduce that for all n \u2265 1 and x > 0,\n\u0012\n\u0012\n\u0013\u0013\n\u03c4 2 x2\nn\nb\nP(|\u03b8n \u2212 \u03b8| \u2265 x) \u2264 2 exp \u2212 log 1 +\n.\n2\n\n4\n\nProof of Corollary 5.1.\nx > 0 and y > 0,\n\u0012\n\n2r\n\nIt follows from (2.5) that, for all n \u2265 1,\n\nP(|\u03b8bn \u2212 \u03b8| \u2265 x) = P |Mn | \u2265\n\n\u0013\n\nx\nhM in \u2264 Pn (x, y) + Qn (y)\n\u03c32\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES\n\n9\n\nwhere Qn (y) = P([M ]n > yhM in ) and\n\u0012 \u0014\n\n\u0012\n\nx2\nPn (x, y) = 2 inf E exp \u2212(p \u2212 1) 4\nhM in\np>1\n2\u03c3 (1 + y)\n\u0012\n\n\u0012\n\nn\n(p \u2212 1)x2\n= 2 inf exp\nH \u2212 2\np>1\np\n2\u03c3 (1 + y)\nIn addition, for all y > 0 and 0 \u2264 t \u2264 c,\nQn (y) \u2264 P\n\nn\nX\n\nk=1\n\n\u03b52k\n\n2\n\n!\n\n\u0013\u0013\n\n.\n\n2\n\n\"\n\n\u0012\n\n\u0012\n\n> \u03c3 y \u2264 exp(\u2212\u03c3 ty)E exp t\n\n\u2264 exp(\u2212\u03c3 2 ty + nL(t)) \u2264 exp \u2212nI\nwhich achieves the proof of Corollary 5.1. \u0003\n\n\u0013\u0015\u00131/p\n\nn\nX\n\nk=1\n\n\u03b52k\n\n!#\n\n\u0013\u0013\n\u03c32 y\n\nn\n\n,\n\n5.2. Autoregressive processes. Consider the autoregressive process given,\nfor all n \u2265 0, by\n\n(5.5)\n\nXn+1 = \u03b8Xn + \u03b5n+1\n\nwhere Xn and \u03b5n are the observation and the driven noise, respectively. We\nassume that (\u03b5n ) is a sequence of independent and identically distributed\nrandom variables with standard N (0, \u03c3 2 ) distribution where \u03c3 2 > 0. The\nprocess is said to be stable if |\u03b8| < 1, unstable if |\u03b8| = 1 and explosive if\n|\u03b8| > 1. We can estimate the unknown parameter \u03b8 by the least-squares or\nthe Yule\u2013Walker estimators given, for all n \u2265 1, by\nPn\nPn\nXk\u22121 Xk\nk=1 Xk\u22121 Xk\ne\nb\nPn\n(5.6)\nand \u03b8n = k=1\n.\n\u03b8n = Pn\n2\n2\nk=1 Xk\u22121\nk=0 Xk\nIt is well known that \u03b8bn and \u03b8en both converge almost surely to \u03b8 and their\nfluctuations can be found in [22]. In the stable case |\u03b8| < 1, the large deviation principles were established in [6]. More precisely, set\n\u221a\n\u221a\n\u03b8 + \u03b82 + 8\n\u03b8 \u2212 \u03b82 + 8\nand b =\n.\na=\n4\n4\nAssume that X0 is independent of (\u03b5n ) with N (0, \u03c3 2 /(1 \u2212 \u03b8 2 )) distribution.\nThen, (\u03b8bn ) and (\u03b8en ) satisfy large deviation principles with good rate functions respectively given by\n\uf8f1\n\u0012\n\u0013\n1 + \u03b8 2 \u2212 2\u03b8x\n\uf8f21\nlog\n,\nif x \u2208 [a, b],\nI(x) = 2\n1 \u2212 x2\n\uf8f3\nlog |\u03b8 \u2212 2x|,\notherwise,\n\uf8f1\n\uf8f21\n\n\u0012\n\n\u0013\n\n1 + \u03b8 2 \u2212 2\u03b8x\nlog\n,\nJ(x) = 2\n1 \u2212 x2\n\uf8f3\n+\u221e,\n\nif x \u2208 ]\u22121, 1[,\notherwise.\n\n\f10\n\nB. BERCU AND A. TOUATI\n\nIt is only recently that sharp large deviation principles were established for\nthe Yule\u2013Walker estimator \u03b8en in the stable, unstable and explosive cases\n[7]. Much work remains to be done for the least-squares estimator \u03b8bn . Our\ngoal is to propose, whatever the value of \u03b8 is, a very simple exponential\ninequality for both \u03b8bn and \u03b8en . For the sake of simplicity, we assume that X0\nis independent of (\u03b5n ) with N (0, \u03c4 2 ) distribution where \u03c4 2 \u2265 \u03c3 2 .\nCorollary 5.2.\n(5.7)\n\nFor all n \u2265 1 and x > 0, we have\n\u0012\n\nP(|\u03b8bn \u2212 \u03b8| \u2265 x) \u2264 2 exp \u2212\n\n\u0013\n\nnx2\n2(1 + yx )\n\nwhere yx is the unique positive solution of the equation h(yx ) = x2 and h is\nthe function h(x) = (1 + x) log(1 + x) \u2212 x. Moreover, for all n \u2265 1 and x > 0,\nwe also have\n\u0013\n\u0012\nnx2\ne\n(5.8)\n.\nP(|\u03b8n \u2212 \u03b8| \u2265 x + |\u03b8|) \u2264 2 exp \u2212\n2(1 + yx )\nProof. The proof is given in Appendix D. \u0003\n\nRemark 5.3. Inequality (5.7) can be very simple if x is small enough.\nAs a matter of fact, one can easily see that for all 0 < x < 1, h(x) < x2 /4.\nConsequently, it immediately follows from (5.7) that, for all 0 < x < 1/2,\n\u0012\n\nP(|\u03b8bn \u2212 \u03b8| \u2265 x) \u2264 2 exp \u2212\n\n\u0013\n\nnx2\n.\n2(1 + 2x)\n\nMoreover, if \u03b8 > 0, we can deduce from (5.6) that, for all x > 0,\n\u0012\ne\nP(\u03b8n \u2212 \u03b8 \u2265 x) \u2264 exp \u2212\n\n\u0013\n\nnx2\n.\n2(1 + yx )\n\n5.3. Branching processes. Consider the Galton\u2013Watson process starting\nfrom X0 = 1 and given, for all n \u2265 1, by\n(5.9)\n\nXn =\n\nXX\nn\u22121\n\nYn,k\n\nk=1\n\nwhere (Yn,k ) is a sequence of independent and identically distributed, nonnegative integer-valued random variables. The distribution of (Yn,k ), with\nfinite mean m and variance \u03c3 2 , is commonly called the offspring or reproduction distribution. Hereafter, we shall assume that m > 1. In order to\nestimate the offspring mean m, we can make use of the Lotka\u2013Nagaev or\nthe Harris estimators given, for all n \u2265 1, by\n(5.10)\n\nXn\nen =\nm\nXn\u22121\n\nand\n\nPn\nXk\nb n = Pnk=1\n.\nm\nk=1 Xk\u22121\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES 11\n\nWithout loss of generality, we can suppose that the set of extinction of the\nen\nprocess (Xn ) is negligible. Consequently, the Lotka\u2013Nagaev estimator m\ne n and m\nb n both converge\nis always well defined. It is well known that m\nalmost surely to m and their fluctuations are given in [3, 14, 15]. Moreover,\ne n ) may be found in [2, 19,\nthe large deviation properties associated with (m\n20]. Our goal is now to establish, as in the previous sections, exponential\ne n and m\nb n . Denote by L the cumulant generating\ninequalities for both m\nfunction associated with the centered offspring distribution given, for all\nt \u2208 R, by L(t) = log E[exp(t(Yn,k \u2212 m))].\nCorollary 5.3. Assume that L is finite on some interval [\u2212c, c] with\nc > 0 and let I be its Fenchel\u2013Legendre transform,\nI(x) = sup {xt \u2212 L(t)}.\n\u2212c\u2264t\u2264c\n\nThen, for all n \u2265 1 and x > 0,\n(5.11)\n\ne n \u2212 m| \u2265 x) \u2264 2E[exp(\u2212J(x)Xn\u22121 )]\nP(|m\n\nwhere J(x) = min(I(x), I(\u2212x)). Moreover, we also have\n(5.12)\n\ne n \u2212 m| \u2265 x) \u2264 2 inf (E[exp(\u2212(p \u2212 1)J(x)Xn\u22121 )])1/p .\nP(|m\np>1\n\nIn addition, if Sn =\n(5.13)\n\nPn\n\nk=0 Xk ,\n\nwe have for all n \u2265 1 and x > 0,\n\nb n \u2212 m| \u2265 x) \u2264 2 inf (E[exp(\u2212(p \u2212 1)J(x)Sn\u22121 )])1/p .\nP(|m\np>1\n\nProof. The proof is given in Appendix E. \u0003\n\nRemark 5.4. On the one hand, inequality (5.12) obviously holds for the\nb n since we always have Sn \u2265 Xn . On the other hand, in\nHarris estimator m\norder to specify the right-hand side of (5.11), (5.12) or (5.13), it is necessary\nto find an upper-bound or to provide an explicit expression of the moment\ngenerating function of Xn . One can easily carry out this calculation when\nthe offspring distribution is the geometric G(p) distribution with parameter\n0 < p < 1. As a matter of fact, in that particular case, the offspring mean\nm = 1/p and it follows from formula (7.3) of [15] that for all 0 < s < 1,\nE[sXn ] \u2264\n\npn s\n.\n1\u2212s\n\nConsequently, for all n \u2265 1 and x > 0, we obtain the simple inequality\ne n \u2212 m| \u2265 x) \u2264\nP(|m\n\n2pn exp(\u2212J(x))\n.\np(1 \u2212 exp(\u2212J(x)))\n\n\f12\n\nB. BERCU AND A. TOUATI\n\nIf the offspring distribution is not geometric, one can precisely estimate the\nmoment generating function of Xn using Theorem 1, page 80 of [3] which\ngives a good approximation of the distribution of Xn based on the limiting\ndistribution\nXn\nn\u2192\u221e mn\n\nW = lim\n\na.s.\n\nAPPENDIX A\nThis appendix is devoted to the proofs of Lemma 2.1 and Lemma 3.1.\nLemma 2.1 immediately follows from Jensen's inequality. As a matter of\nfact, (2.1) implies that for all t \u2208 R,\n\u0012 \u0014\n\nL(t) \u2265 exp E tX \u2212\n\nt2 2\nX\n2\n\n\u0015\u0013\n\n\u0012\n\n\u2265 exp \u2212\n\n\u0013\n\nt2 2\n\u03c3 .\n2\n\nConsequently, we obtain that for all t \u2208 R,\nL(t) \u2265 1 \u2212\n\n(A.1)\nFurthermore, for all t \u2208 R,\n(A.2)\n\n\u0014\n\nt2 2\n\u03c3 .\n2\n\n\u0012\n\n\u0013\n\n\u0015\n\nt2\nL(t) + L(\u2212t) = 2E exp \u2212 X 2 cosh(tX) \u2264 2\n2\n\nby the well-known inequality cosh(x) \u2264 exp(x2 /2). Hence, we obtain from\n(A.1) together with (A.2) that for all t \u2208 R,\nL(t) \u2264 2 \u2212 L(\u2212t) \u2264 1 +\n\nt2 2\n\u03c3 .\n2\n\nLemma 3.1 is much more difficult to prove. Let f be the function defined,\nfor all x \u2208 R, by\n\u0012\n\n\u0013\n\nx2\nf (x) = exp x \u2212\n.\n2\nWe clearly have f \u2032 (x) = (1 \u2212 x)f (x) and f \u2032 (\u2212x) = (1 + x)f (\u2212x). We shall\nalso make use of the functions a and b defined, for all x \u2208 R, by a(x) = f \u2032 (\u2212x)\nand b(x) = f \u2032 (\u2212x) \u2212 f \u2032 (x). One can realize that, for all x > 0, 0 < a(x) < 1,\n0 < b(x) < 2 and a\u2032 (x) < 0 as a\u2032 (x) = \u2212(2x + x2 )f (\u2212x). After those simple\npreliminaries, we are in position to prove Lemma 3.1. For all t \u2208 R,\n\u0014\n\n\u0012\n\nL(t) = E exp tX \u2212\n\nt2 X 2\n2\n\n\u0013\u0015\n\n=\n\nZ\n\nR\n\nf (tx) dF (x)\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES 13\n\nwhere F is the distribution function associated with X. Integrating by parts,\nwe have for all t \u2208 R,\nL(t) = \u2212t\n(A.3)\n\n= \u2212t\n= \u2212t\n\nZ\n\nf \u2032 (tx)F (x) dx\n\nR\n\nZ\n\n0\n\n\u2032\n\n\u2212\u221e\n\nZ\n\nf (tx)F (x) dx \u2212 t\n\n+\u221e\n\nZ\n\n+\u221e\n\n0\n\n\u2032\n\nf (\u2212tx)F (\u2212x) dx \u2212 t\n\n0\n\nf \u2032 (tx)F (x) dx\n\nZ\n\n+\u221e\n\nf \u2032 (tx)F (x) dx.\n\n0\n\nConsequently, as\n\u2212t\n\nZ\n\n\u221e\n\n0\n\n\u0014\n\n\u0012\n\nt 2 x2\nf (tx) dx = \u2212 exp tx \u2212\n2\n\u2032\n\n\u0013\u0015+\u221e\n\n= 1,\n\n0\n\nwe obtain from (A.3) that, for all t \u2208 R, L(t) = 1 \u2212 tI(t) where\nI(t) =\n(A.4)\n\nZ\n\n+\u221e\n\n0\n\nf \u2032 (\u2212tx)F (\u2212x) dx \u2212\n\nZ\n\n0\n\n+\u221e\n\nf \u2032 (tx)(1 \u2212 F (x)) dx\n\n= A(t) + B(t)\nwith\nA(t) =\n\nZ\n\n+\u221e\n\na(tx)(F (\u2212x) \u2212 (1 \u2212 F (x))) dx,\n\n0\n\nB(t) =\n\nZ\n\n+\u221e\n\nb(tx)(1 \u2212 F (x)) dx.\n\n0\n\nFirst of all, assume that X is heavy on left. Our goal is to show that, for\nall t > 0, the integral I(t) is nonnegative. We obviously have, for all t > 0,\nB(t) \u2265 0 as b(tx) > 0. In addition, for any a > 0, let\nH(a) =\n\nZ\n\n0\n\na\n\nF (\u2212x) \u2212 (1 \u2212 F (x\u2212)) dx.\n\nSince H \u2032 (a) = F (\u2212a) \u2212 (1 \u2212 F (a)) almost everywhere, integrating once again\nby parts, we find that\nA(t) =\n(A.5)\n\nZ\n\n+\u221e\n\n0\n\n= \u2212t\n\nZ\n\na(tx)H \u2032 (x) dx = [a(tx)H(x)]+\u221e\n\u2212\n0\n\n+\u221e\n\nZ\n\n+\u221e\n\nta\u2032 (tx)H(x) dx\n\n0\n\na\u2032 (tx)H(x) dx\n\n0\n\nas H(0) = 0 and H vanishes at infinity. Hereafter, as X is heavy on left,\nH(a) \u2265 0 for all a \u2265 0. Moreover, we recall that, for all x > 0, a\u2032 (x) < 0.\nHence, we immediately deduce from (A.5) that, for all t > 0, A(t) \u2265 0. Consequently, relation (A.4) leads to I(t) \u2265 0 and L(t) \u2264 1 for all t > 0, which\n\n\f14\n\nB. BERCU AND A. TOUATI\n\ncompletes the proof of part (1) of Lemma 3.1. Next, if X is heavy on right,\n\u2212X is heavy on left. Hence, we immediately infer from (2.1) and part (1) of\nLemma 3.1 that L(t) \u2264 1 for all t < 0. Finally, part (3) of Lemma 3.1 follows\nfrom the conjunction of parts (1) and (2). Another straightforward way to\nprove part (3) is as follows. If X is symmetric, we have for all t \u2208 R,\nL(t) =\n\nZ\n\nf (tx) dF (x) =\n\nR\n\n=2\n\nZ\n\n0\n\n+\u221e\n\nZ\n\n+\u221e\n\n(f (tx) + f (\u2212tx)) dF (x)\n0\n\nexp(\u2212t2 x2 /2) cosh(tx) dF (x) \u2264 1\n\nby the well-known inequality cosh(x) \u2264 exp(x2 /2).\nAPPENDIX B\nIn order to prove Theorems 2.1 and 2.2, we shall often make use of the\nfollowing lemma.\nLemma B.1. Let (Mn ) be a locally square integrable martingale. For all\nt \u2208 R and n \u2265 0, denote\n\u0012\n\nVn (t) = exp tMn \u2212\n\n\u0013\n\nt2\n([M ]n + hM in ) .\n2\n\nThen, for all t \u2208 R, (Vn (t)) is a positive supermartingale with E[Vn (t)] \u2264 1.\nProof. For all t \u2208 R and n \u2265 1, we have\n\u0012\n\n\u0013\n\nt2\nVn (t) = Vn\u22121 (t) exp t\u2206Mn \u2212 (\u2206[M ]n + \u2206hM in )\n2\n\nwhere \u2206Mn = Mn \u2212 Mn\u22121 , \u2206[M ]n = \u2206Mn2 and \u2206hM in = E[\u2206Mn2 |Fn\u22121 ].\nHence, we deduce from Lemma 2.1 that for all t \u2208 R,\n\u0012\n\nt2\nE[Vn (t)|Fn\u22121 ] \u2264 Vn\u22121 (t) exp \u2212 \u2206hM in\n2\n\n\u0013\u0012\n\nt2\n1 + \u2206hM in\n2\n\n\u0013\n\n\u2264 Vn\u22121 (t).\nConsequently, for all t \u2208 R, (Vn (t)) is a positive supermartingale such that,\nfor all n \u2265 1, E[Vn (t)] \u2264 E[Vn\u22121 (t)] which implies that E[Vn (t)] \u2264 E[V0 (t)] =\n1. \u0003\nWe are now in position to prove Theorems 2.1 and 2.2 inspired by the\noriginal work of De la Pe\u00f1a [8]. First of all, denote\nZn = [M ]n + hM in .\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES 15\n\nFor all x, y > 0, let\nAn = {|Mn | \u2265 x, Zn \u2264 y}.\n\n\u2212\n+\nWe have the decomposition An = A+\nn \u222a An where An = {Mn \u2265 x, Zn \u2264 y}\n\u2212\nand An = {Mn \u2264 \u2212x, Zn \u2264 y}. By Markov's inequality, we have for all t > 0,\n\n\u0014\n\n\u0012\n\nP(A+\nn ) \u2264 E exp\n\u0014\n\n\u0013\n\ntx\nt\nMn \u2212\n1A+n\n2\n2\n\n\u0012\n\n\u0013\n\n\u0015\n\n\u0012\n\n\u0013\n\nt\nt2\nt2\ntx\n\u2264 E exp Mn \u2212 Zn exp\n1A+n\nZn \u2212\n2\n4\n4\n2\n\u0012\n\n\u0013\n\nt2 y tx q\n\u2212\n\u2264 exp\nE[Vn (t)]P(A+\nn ).\n4\n2\nHence, we deduce from Lemma B.1 that for all t > 0,\nP(A+\nn ) \u2264 exp\n\n(B.1)\n\nDividing both sides of (B.1) by\nfind that\n\n\u0012\n\nq\n\nt2 y tx\n\u2212\n4\n2\n\n\u0015\n\n\u0013q\n\nP(A+\nn ).\n\nP(A+\nn ) and choosing the value t = x/y, we\n\u0012\n\nP(A+\nn ) \u2264 exp\n\n\u0013\n\nx2\n\u2212\n.\n2y\n\nWe also find the same upper-bound for P(A\u2212\nn ) which immediately leads to\n(2.3).\nWe next proceed to the proof of Theorem 2.2 in the special case a = 0 and\nb = 1 inasmuch as the proof for the general case follows exactly the same\nlines. For all x, y > 0, let\nBn = {|Mn | \u2265 xhM in , hM in \u2212 [M ]n \u2265 y} = Bn+ \u222a Bn\u2212\nwhere\nBn+ = {Mn \u2265 xhM in , hM in \u2212 [M ]n \u2265 y},\n\nBn\u2212 = {Mn \u2264 \u2212xhM in , hM in \u2212 [M ]n \u2265 y}.\n\nBy Cauchy\u2013Schwarz's inequality, we have for all t > 0,\n\n(B.2)\n\nP(Bn+ ) \u2264 E\n\n\u0014\n\n\u0014\n\n\u0012\n\n\u0013\n\nt\ntx\nexp Mn \u2212 hM in 1Bn+\n2\n2\n\u0012\n\n\u0013\n\n\u0012\n\n\u0015\n\n\u0013\n\nConsequently, we obtain from (B.2) with the particular choice t = x that\n(B.3)\n\n\u0015\n\nt\nt\nt2\nt2\n\u2264 E exp Mn \u2212 Zn exp (t \u2212 2x)hM in + [M ]n 1Bn+ .\n2\n4\n4\n4\n\u0012\n\nP(Bn+ ) \u2264 exp \u2212\n\nx2 y\n4\n\n\u0013q\n\nP(Bn+ ).\n\n\f16\n\nB. BERCU AND A. TOUATI\n\nTherefore, if we divide both sides of (B.3) by\nP(Bn+ ) \u2264 exp\n\n\u0012\n\nq\n\nP(Bn+ ), we find that\n\n\u0013\n\nx2 y\n\u2212\n.\n2\n\nThe same upper-bound holds for P(Bn\u2212 ) which clearly implies (2.4). Furthermore, for all x, y > 0, let\nCn = {|Mn | \u2265 xhM in , [M ]n \u2264 yhM in } = Cn+ \u222a Cn\u2212\nwhere\nCn+ = {Mn \u2265 xhM in , [M ]n \u2264 yhM in },\n\nCn\u2212 = {Mn \u2264 \u2212xhM in , [M ]n \u2264 yhM in }.\n\nBy H\u00f6lder's inequality, we have for all t > 0 and q > 1,\n\u0014\n\n\u0012\n\nP(Cn+ ) \u2264 E exp\n(B.4)\n\n\u0014\n\n\u0012\n\n\u2264 E exp\n\u0012 \u0014\n\n\u0013\n\nt\ntx\nMn \u2212 hM in 1Cn+\nq\nq\n\u0013\n\n\u0012\n\n\u0015\n\n\u0013\n\nt\nt2\nt\nMn \u2212 Zn exp\n(t \u2212 2x + ty)hM in 1Cn+\nq\n2q\n2q\n\u0012\n\ntp\n\u2264 E exp\n(t \u2212 2x + ty)hM in\n2q\n\n\u0013\u0015\u00131/p\n\n\u0015\n\n.\n\nConsequently, as p/q = p \u2212 1, we can deduce from (B.4) and the particular\nchoice t = x/(1 + y) that\n\u0012 \u0014\n\n\u0012\n\nP(Cn+ ) \u2264 inf E exp \u2212(p \u2212 1)\np>1\n\nx2\nhM in\n2(1 + y)\n\n\u0013\u0015\u00131/p\n\n.\n\nWe also find the same upper-bound for P(Cn\u2212 ) which completes the proof of\nTheorem 2.2.\nAPPENDIX C\nThe proofs of Theorems 4.1 and 4.2 are based on the following lemma.\nLemma C.1. Let (Mn ) be a locally square integrable martingale. For all\nt \u2208 R and n \u2265 0, denote\n\u0012\n\n\u0013\n\nt2\nWn (t) = exp tMn \u2212 [M ]n .\n2\n\n(1) If (Mn ) is heavy on left, then for all t \u2265 0, (Wn (t)) is a supermartingale\nwith E[Wn (t)] \u2264 1.\n(2) If (Mn ) is heavy on right, then for all t \u2264 0, (Wn (t)) is a supermartingale\nwith E[Wn (t)] \u2264 1.\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES 17\n\n(3) If (Mn ) is conditionally symmetric, then for all t \u2208 R, (Wn (t)) is a\nsupermartingale with E[Wn (t)] \u2264 1.\nProof. Lemma C.1 part (3) is due to De la Pe\u00f1a [8], Lemma 6.1. Our\napproach is totally different as it mainly relies on Lemma 3.1. Assume that\n(Mn ) is heavy on left. For all t \u2208 R and n \u2265 1, we have\n\u0012\n\nWn (t) = Wn\u22121 (t) exp t\u2206Mn \u2212\n\nt2\n\u2206[M ]n\n2\n\n\u0013\n\nwhere \u2206[M ]n = \u2206Mn2 . We infer from Lemma 3.1 part (1) that for all n \u2265 1\nand for all t \u2265 0,\n\u0014\n\n\u0012\n\nE exp t\u2206Mn \u2212\n\nt2\n\u2206Mn2\n2\n\n\u0013\n\n\u0015\n\nFn\u22121 \u2264 1.\n\nConsequently, for all t \u2265 0, (Wn (t)) is a positive supermartingale such that,\nfor all n \u2265 1, E[Wn (t)] \u2264 E[Wn\u22121 (t)] which leads to E[Wn (t)] \u2264 E[W0 (t)] = 1.\nThe rest of the proof is also a straightforward application of Lemma 3.1. \u0003\nBy use of Lemma C.1, the proof of Theorem 4.1 is quite analogous to that\nof Theorem 2.1 and therefore is left to the reader. We shall proceed to the\nproof of Theorem 4.2 in the special case a = 0 and b = 1. For all x > 0, let\nAn = {Mn \u2265 x[M ]n }. By H\u00f6lder's inequality, we have for all t > 0 and q > 1,\n\u0014\n\n\u0012\n\n\u0013\n\nt\ntx\nP(An ) \u2264 E exp Mn \u2212 [M ]n 1An\nq\nq\n\n(C.1)\n\n\u0014\n\n\u2264 E exp\n\u0012 \u0014\n\n\u0012\n\n\u0013\n\n\u0015\n\n\u0012\n\n\u0013\n\nt\nt2\nt\nMn \u2212 [M ]n exp\n(t \u2212 2x)[M ]n 1An\nq\n2q\n2q\n\n\u0012\n\ntp\n(t \u2212 2x)[M ]n\n\u2264 E exp\n2q\n\n\u0013\u0015\u00131/p\n\n\u0015\n\n(E[Wn (t)])1/q .\n\nSince (Mn ) is heavy on left, it follows from Lemma C.1 that for all t \u2265 0,\nE[Wn (t)] \u2264 1. Consequently, as p/q = p \u2212 1, we can deduce from (C.1) and\nthe particular choice t = x that\n\u0012 \u0014\n\n\u0012\n\nP(An ) \u2264 inf E exp \u2212(p \u2212 1)\np>1\n\nx2\n[M ]n\n2\n\n\u0013\u0015\u00131/p\n\n.\n\nFurthermore, for all x, y > 0, let Bn = {Mn \u2265 x[M ]n , [M ]n \u2265 y}. As before,\nwe find that for all 0 < t < 2x,\n\u0014\n\n\u0012\n\nP(Bn ) \u2264 E exp\n\u2264 exp\n\n\u0012\n\n\u0013\n\n\u0012\n\n\u0013\n\nt\nt\nt2\nMn \u2212 [M ]n exp (t \u2212 2x)[M ]n 1Bn\n2\n4\n4\n\u0013 \u0014\n\n\u0012\n\n\u0013\n\nt\nt2\nty\n(t \u2212 2x) E exp Mn \u2212 [M ]n 1Bn\n4\n2\n4\n\n\u0015\n\n\u0015\n\n\f18\n\nB. BERCU AND A. TOUATI\n\n\u0013q\nty\n(t \u2212 2x)\nP(Bn )\n\u2264 exp\n4\n\u0012\n\n\u0012\n\n\u0013\n\nx2 y\n,\n2\nchoosing the value t = x. Finally, the last inequality of Theorem 4.2 is left\nto the reader as its proof follows exactly the same arguments as (4.3).\n\u2264 exp \u2212\n\nAPPENDIX D\nWe shall now focus our attention on the proof of Corollary 5.2. It immediately follows from (5.5) together with (5.6) that for all n \u2265 1,\n\u03b8bn \u2212 \u03b8 = \u03c3 2\n\n(D.1)\nwhere\nMn =\n\nn\nX\n\nXk\u22121 \u03b5k\n\nand\n\nk=1\n\nMn\nhM in\n\nhM in = \u03c3 2\n\nn\nX\n\n2\nXk\u22121\n.\n\nk=1\n\nThe driven noise (\u03b5n ) is a sequence of independent and identically distributed\nrandom variables with N (0, \u03c3 2 ) distribution. Consequently, for all n \u2265 1, the\n2 )\ndistribution of the increments \u2206Mn = Xn\u22121 \u03b5n given Fn\u22121 is N (0, \u03c3 2 Xn\u22121\nwhich implies that (Mn ) is a Gaussian martingale. Therefore, we infer from\ninequality (4.6) that for all n \u2265 1 and x > 0,\n(D.2)\n\n\u0012\n\nP(|\u03b8bn \u2212 \u03b8| \u2265 x) = P |Mn | \u2265\n\u0012 \u0014\n\n\u0013\n\n\u0012\n\nx\nx\nhM in = 2P Mn \u2265 2 hM in\n2\n\u03c3\n\u03c3\n\u0012\n\n\u2264 2 inf E exp \u2212(p \u2212 1)\np>1\n\nx2\nhM in\n2\u03c3 4\n\n\u0013\u0015\u00131/p\n\n\u0013\n\n.\n\nSimilar result may be found in [17, 23, 24]. We are now halfway to our goal\nand it remains to find a suitable upper-bound for the\n\u221a right-hand side of\n2\n(D.2). For all t \u2208 R such that 1 \u2212 2\u03c3 t > 0, if \u03b1 = 1/ 1 \u2212 2\u03c3 2 t, we deduce\nfrom (5.5) that, for all n \u2265 1,\n2\nE[exp(tXn2 )|Fn\u22121 ] = exp(t\u03b8 2 Xn\u22121\n)E[exp(2\u03b8tXn\u22121 \u03b5n + t\u03b52n )|Fn\u22121 ]\n\nZ\n\n\u0012\n\n\u0013\n\n2 )\nexp(t\u03b8 2 Xn\u22121\nx2\n\u221a\nexp \u2212 2 2 exp(2\u03b8tXn\u22121 x) dx.\n2\u03b1 \u03c3\n\u03c3 2\u03c0\nR\nHence, if \u03b2 = 2t\u03b1\u03c3\u03b8Xn\u22121 , we find via the change of variables y = x/\u03b1\u03c3 that\n\n=\n\nE[exp(tXn2 )|Fn\u22121 ] =\n\n2 )\n\u03b1 exp(t\u03b8 2 Xn\u22121\n\u221a\n2\u03c0\n\n\u0012\n\nZ\n\n2\n= \u03b1 exp t\u03b8 2 Xn\u22121\n+\n\n\u0012\n\n\u0013\n\ny2\nexp \u2212 + \u03b2y dy\n2\nR\n\u03b22\n2\n\n\u0013\n\n2\n= \u03b1 exp(t\u03b12 \u03b8 2 Xn\u22121\n),\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES 19\n\nwhich implies that, for all t < 0 and n \u2265 1,\n(D.3)\n\nE[exp(tXn2 )|Fn\u22121 ] \u2264 \u03b1.\n\nFurthermore, as X0 is N (0, \u03c4 2 ) distributed with \u03c4 2 \u2265 \u03c3 2 , E[exp(tX02 )] \u2264 \u03b1.\nIt immediately follows from (D.3) together with the tower property of the\nconditional expectation that for all t < 0 and n \u2265 0,\n(D.4)\n\nE[exp(thM in )] \u2264 (1 \u2212 2\u03c3 4 t)\u2212n/2 .\n\nConsequently, we deduce from the conjunction of (D.2) and (D.4) with the\nvalue t = \u2212(p \u2212 1)x2 /2\u03c3 4 and the change of variables y = (p \u2212 1)x2 that for\nall x > 0 and n \u2265 1,\n\u0013\n\u0012\nnx2\nb\nl(y)\nP(|\u03b8n \u2212 \u03b8| \u2265 x) \u2264 2 inf exp \u2212\ny>0\n\n2\n\nwhere the function l is given by\n\nl(y) =\n\nlog(1 + y)\n.\nx2 + y\n\nWe clearly have\nl\u2032 (y) =\n\nx2 \u2212 h(y)\n(1 + y)(x2 + y)2\n\nwhere h(y) = (1 + y) log(1 + y) \u2212 y. One can observe that the function h is\nthe Cramer transform of the centered Poisson distribution with parameter\n1. Let yx be the unique positive solution of the equation h(yx ) = x2 . The\nvalue yx maximizes the function l and this natural choice clearly leads to\n(5.7). Finally, it follows from (5.6) and (5.7) that for all x > 0 and n \u2265 1,\n(D.5)\n\n\u0012\ne\nP(|\u03b8n \u2212 \u03b8 + \u03b8fn | \u2265 x) \u2264 2 exp \u2212\n\nnx2\n2(1 + yx )\n\n\u0013\n\nwhere the random variable 0 \u2264 fn \u2264 1. Hence, (D.5) implies (5.8) which\ncompletes the proof of Corollary 5.2.\nAPPENDIX E\nWe shall now proceed to the proof of Corollary 5.3. We only focus our\nattention on the Harris estimator inasmuch as the proof for the Lotka\u2013\nNagaev estimator follows essentially the same lines. First of all, relation\n(5.9) can be rewritten as\n(E.1)\n\nXn = mXn\u22121 + \u03ben\n\n\f20\n\nB. BERCU AND A. TOUATI\n\nwhere \u03ben = Xn \u2212 E[Xn |Fn\u22121 ]. Consequently, we obtain from (5.10) together\nwith (E.1) that for all n \u2265 1,\nMn\nSn\u22121\n\nbn\u2212m=\nm\n\nwhere Mn =\n\nn\nX\n\n\u03bek .\n\nk=1\n\nMoreover, for all n \u2265 1 and 0 \u2264 t \u2264 c, E[exp(t\u03ben )|Fn\u22121 ] = exp(Xn\u22121 L(t))\nwhich implies that\nE[exp(tMn \u2212 L(t)Sn\u22121 )] = 1.\n\n(E.2)\n\nb n \u2212 m| \u2265 x}.\nWe are in position to prove (5.13). For all x > 0, let Dn = {|m\n+\n\u2212\n+\nb n \u2212 m \u2265 x} and\nWe have the decomposition Dn = Dn \u222a Dn where Dn = {m\nb n \u2212 m \u2264 \u2212x}. By H\u00f6lder's inequality together with (E.2), we have\nDn\u2212 = {m\nfor all 0 \u2264 t \u2264 c and q > 1,\n\u0014\n\nP(Dn+ ) \u2264 E exp\n\n(E.3)\n\n\u0014\n\n\u2264 E exp\n\u0012 \u0014\n\n\u0012\n\n\u0012\n\n\u0013\n\ntx\nt\nMn \u2212 Sn\u22121 1Dn+\nq\nq\n\u0013\n\n\u0015\n\n\u0012\n\n\u0013\n\nt\n1\nL(t)\nMn \u2212\nSn\u22121 exp (L(t) \u2212 tx)Sn\u22121 1Dn+\nq\nq\nq\n\n\u0012\n\np\n\u2264 E exp (L(t) \u2212 tx)Sn\u22121\nq\n\n\u0013\u0015\u00131/p\n\n\u0015\n\n.\n\nTaking the infimum over the interval [0, c], we infer from (E.3) that\n(E.4)\n\nP(Dn+ ) \u2264 (E[exp(\u2212(p \u2212 1)I(x)Sn\u22121 )])1/p .\n\nAlong the same lines, we also find that\n(E.5)\n\nP(Dn\u2212 ) \u2264 (E[exp(\u2212(p \u2212 1)I(\u2212x)Sn\u22121 )])1/p .\n\nFinally, (5.13) immediately follows from (E.4) and (E.5).\nAPPENDIX F\nThis appendix is devoted to some justifications about the examples of\nrandom variables heavy on left or right. Consider an integrable random\nvariable X with zero mean and denote by F its cumulative distribution\nfunction. Let H be the function defined, for all a > 0, by\nH(a) =\n\nZ\n\n0\n\na\n\nF (\u2212x) \u2212 (1 \u2212 F (x\u2212)) dx.\n\nWe already saw that X is heavy on left if, for all a > 0, H(a) \u2265 0 while X\nis heavy on right if, for all a > 0, H(a) \u2264 0. Let Y be a positive integrable\nrandom variable with mean m and denote\nX = Y \u2212 m.\n\n\fEXPONENTIAL INEQUALITIES FOR SELF-NORMALIZED MARTINGALES 21\n\nDiscrete random variables. Assume that Y is a discrete random variable\ntaking its values in N. For all n \u2265 0, let\nsn =\n\nn\nX\n\nP(Y = k).\n\nk=0\n\nAfter some straightforward calculations, we obtain that, for all a > 0,\n[m+a]\n\nH(a) = \u2212a +\n\nX\n\nk=[m\u2212a]\n\nsk \u2212 s[m+a] + {m + a}s[m+a] \u2212 {m \u2212 a}s[m\u2212a]\n\nwhere, for all x \u2208 R, [x] stands for the integer part of x and its fractional\npart {x} is given by {x} = x \u2212 [x] and, of course, sn = 0 for all n < 0.\nContinuous random variables. Assume that Y is a real random variable\nabsolutely continuous with respect to the Lebesgue measure. Denote by g\nits probability density function. It is not hard to see that, for all a > 0,\nH(a) = \u2212a + 2a\n\nZ\n\n0\n\nam\n\ng(x) dx +\n\nZ\n\nm+a\n\nam\n\n(m + a \u2212 x)g(x) dx\n\nwhere am = inf{m \u2212 a, 0}. Consequently, in order to check that X is heavy\non left, it is only necessary to show that, for all a > 0, H(a) \u2265 0.\nAcknowledgments. The authors would like to thank V. De la Pe\u00f1a, T.\nL. Lai and M. Ledoux for fruitful discussions.\nREFERENCES\n[1] Adell, J. A. and Jodr\u00e1, P. (2005). The median of the Poisson distribution. Metrika\n61 337\u2013346. MR2230380\n[2] Athreya, K. B. (1994). Large deviation rates for branching processes I. Single type\ncase. Ann. Appl. Probab. 4 779\u2013790. MR1284985\n[3] Athreya, K. B. and Ney, P. E. (1972). Branching Processes. Springer, Berlin.\nMR0373040\n[4] Azuma, K. (1967). Weighted sums of certain dependent random variables. T\u00f4kuku\nMath. J. 19 357\u2013367. MR0221571\n[5] Barlow, M. T., Jacka, S. D. and Yor, M. (1986). Inequalities for a pair of\nprocesses stopped at a random time. Proc. London Math. Soc. 52 142\u2013172.\nMR0812449\n[6] Bercu, B., Gamboa, F. and Rouault, A. (1997). Large deviations for quadratic\nforms of stationary Gaussian processes. Stochastic Process. Appl. 71 75\u201390.\nMR1480640\n[7] Bercu, B. (2001). On large deviations in the Gaussian autoregressive process: Stable,\nunstable and explosive cases. Bernoulli 7 299\u2013316. MR1828507\n[8] De la Pe\u00f1a, V. H. (1999). A general class of exponential inequalities for martingales\nand ratios. Ann. Probab. 27 537\u2013564. MR1681153\n[9] De la Pe\u00f1a, V. H., Klass, M. J. and Lai, T. L. (2004). Self-normalized processes:\nExponential inequalities, moments bounds and iterated logarithm law. Ann.\nProbab. 32 1902\u20131933. MR2073181\n\n\f22\n\nB. BERCU AND A. TOUATI\n\n[10] De la Pe\u00f1a, V. H., Klass, M. J. and Lai, T. L. (2007). Pseudo-maximization and\nself-normalized processes. Probab. Surveys 4 172\u2013192. MR2368950\n[11] Dzhaparidze, K. and Van Zanten, J. H. (2001). On Bernstein-type inequalities\nfor martingales. Stochastic Process. Appl. 93 109\u2013117. MR1819486\n[12] Van de Geer, S. (1995). Exponential inequalities for martingales, with application\nto maximum likelihood estimation for counting processes. Ann. Statist. 23 1779\u2013\n1801. MR1370307\n[13] Freedman, D. A. (1975). On tail probabilities for martingales. Ann. Probab. 3 100\u2013\n118. MR0380971\n[14] Guttorp, P. (1991). Statistical Inference for Branching Processes. Wiley, New York.\nMR1254434\n[15] Harris, T. E. (1963). The Theory of Branching Processes. Springer, Berlin.\nMR0163361\n[16] Hoeffding, W. J. (1963). Probability inequalities sums of bounded random variables. J. Amer. Statist. Assoc. 58 713\u2013721. MR0144363\n[17] Liptser, R. and Spokoiny, V. (2000). Deviation probability bound for martingales\nwith applications to statistical estimation. Statist. Probab. Lett. 46 347\u2013357.\nMR1743992\n[18] McDiarmid, C. (1998). Concentration. In Probabilistic Methods for Algorithmic Discrete Mathematics (M. Habib, C. McDiarmid, T. Ramirez-Alfonsin and B. Reed,\neds.) 195\u2013248. Springer, Berlin. MR1678578\n[19] Ney, P. E. and Vidyashankar, A. N. (2003). Harmonic moments and large deviation rates for supercritical branching processes. Ann. Appl. Probab. 13 475\u2013489.\nMR1970272\n[20] Ney, P. E. and Vidyashankar, A. N. (2004). Local limit theory and large deviations for supercritical branching processes. Ann. Appl. Probab. 14 1135\u20131166.\nMR2071418\n[21] Pinelis, I. (1994). Optimum bounds for the distributions of martingales in Banach\nspaces. Ann. Probab. 22 1679\u20131706. MR1331198\n[22] White, J. S. (1958). The limit distribution of the serial correlation in the explosive\ncase. Ann. Math. Statist. 29 1188\u20131197. MR0100952\n[23] Worms, J. (1999). Moderate deviations for stable Markov chains and regression\nmodels. Electron. J. Probab. 4 1\u201328. MR1684149\n[24] Worms, J. (2001). Large and moderate deviations upper bounds for the Gaussian\nautoregressive process. Statist. Probab. Lett. 51 235\u2013243. MR1822730\nInstitut de Math\u00e9matiques de Bordeaux\nUniversit\u00e9 Bordeaux 1\nUMR 5251\n351 cours de la lib\u00e9ration\n33405 Talence cedex\nFrance\nE-mail: Bernard.Bercu@math.u-bordeaux1.fr\n\nD\u00e9partement de Math\u00e9matiques\nFacult\u00e9 des Sciences de Bizerte\n7021 Zarzouna\nTunisie\nE-mail: Abder.Touati@fsb.rnu.tn\n\n\f"}