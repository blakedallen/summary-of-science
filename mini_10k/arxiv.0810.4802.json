{"id": "http://arxiv.org/abs/0810.4802v1", "guidislink": true, "updated": "2008-10-27T12:30:43Z", "updated_parsed": [2008, 10, 27, 12, 30, 43, 0, 301, 0], "published": "2008-10-27T12:30:43Z", "published_parsed": [2008, 10, 27, 12, 30, 43, 0, 301, 0], "title": "Robust nonparametric estimation via wavelet median regression", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0810.2048%2C0810.0807%2C0810.1284%2C0810.2782%2C0810.1478%2C0810.1250%2C0810.4624%2C0810.0658%2C0810.3128%2C0810.3450%2C0810.2026%2C0810.1693%2C0810.4158%2C0810.0533%2C0810.2758%2C0810.4853%2C0810.4483%2C0810.3471%2C0810.1222%2C0810.5765%2C0810.1425%2C0810.0163%2C0810.5128%2C0810.4462%2C0810.0463%2C0810.5187%2C0810.2600%2C0810.1035%2C0810.4550%2C0810.1540%2C0810.2225%2C0810.5421%2C0810.4704%2C0810.5026%2C0810.1432%2C0810.5656%2C0810.5705%2C0810.1356%2C0810.2474%2C0810.4533%2C0810.1411%2C0810.4425%2C0810.5395%2C0810.3089%2C0810.0289%2C0810.0674%2C0810.2667%2C0810.3148%2C0810.0002%2C0810.4011%2C0810.1646%2C0810.4901%2C0810.0393%2C0810.5740%2C0810.4245%2C0810.1308%2C0810.4811%2C0810.4257%2C0810.5185%2C0810.5141%2C0810.1059%2C0810.2465%2C0810.2505%2C0810.5563%2C0810.0499%2C0810.0806%2C0810.5039%2C0810.2109%2C0810.3888%2C0810.4718%2C0810.3020%2C0810.5384%2C0810.4975%2C0810.2220%2C0810.4673%2C0810.2592%2C0810.4507%2C0810.0613%2C0810.1360%2C0810.4171%2C0810.1586%2C0810.4418%2C0810.5190%2C0810.0039%2C0810.0316%2C0810.3274%2C0810.1484%2C0810.3336%2C0810.0399%2C0810.3455%2C0810.4802%2C0810.2177%2C0810.2892%2C0810.4286%2C0810.1269%2C0810.3747%2C0810.4847%2C0810.3666%2C0810.1861%2C0810.4723%2C0810.0976&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Robust nonparametric estimation via wavelet median regression"}, "summary": "In this paper we develop a nonparametric regression method that is\nsimultaneously adaptive over a wide range of function classes for the\nregression function and robust over a large collection of error distributions,\nincluding those that are heavy-tailed, and may not even possess variances or\nmeans. Our approach is to first use local medians to turn the problem of\nnonparametric regression with unknown noise distribution into a standard\nGaussian regression problem and then apply a wavelet block thresholding\nprocedure to construct an estimator of the regression function. It is shown\nthat the estimator simultaneously attains the optimal rate of convergence over\na wide range of the Besov classes, without prior knowledge of the smoothness of\nthe underlying functions or prior knowledge of the error distribution. The\nestimator also automatically adapts to the local smoothness of the underlying\nfunction, and attains the local adaptive minimax rate for estimating functions\nat a point. A key technical result in our development is a quantile coupling\ntheorem which gives a tight bound for the quantile coupling between the sample\nmedians and a normal variable. This median coupling inequality may be of\nindependent interest.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0810.2048%2C0810.0807%2C0810.1284%2C0810.2782%2C0810.1478%2C0810.1250%2C0810.4624%2C0810.0658%2C0810.3128%2C0810.3450%2C0810.2026%2C0810.1693%2C0810.4158%2C0810.0533%2C0810.2758%2C0810.4853%2C0810.4483%2C0810.3471%2C0810.1222%2C0810.5765%2C0810.1425%2C0810.0163%2C0810.5128%2C0810.4462%2C0810.0463%2C0810.5187%2C0810.2600%2C0810.1035%2C0810.4550%2C0810.1540%2C0810.2225%2C0810.5421%2C0810.4704%2C0810.5026%2C0810.1432%2C0810.5656%2C0810.5705%2C0810.1356%2C0810.2474%2C0810.4533%2C0810.1411%2C0810.4425%2C0810.5395%2C0810.3089%2C0810.0289%2C0810.0674%2C0810.2667%2C0810.3148%2C0810.0002%2C0810.4011%2C0810.1646%2C0810.4901%2C0810.0393%2C0810.5740%2C0810.4245%2C0810.1308%2C0810.4811%2C0810.4257%2C0810.5185%2C0810.5141%2C0810.1059%2C0810.2465%2C0810.2505%2C0810.5563%2C0810.0499%2C0810.0806%2C0810.5039%2C0810.2109%2C0810.3888%2C0810.4718%2C0810.3020%2C0810.5384%2C0810.4975%2C0810.2220%2C0810.4673%2C0810.2592%2C0810.4507%2C0810.0613%2C0810.1360%2C0810.4171%2C0810.1586%2C0810.4418%2C0810.5190%2C0810.0039%2C0810.0316%2C0810.3274%2C0810.1484%2C0810.3336%2C0810.0399%2C0810.3455%2C0810.4802%2C0810.2177%2C0810.2892%2C0810.4286%2C0810.1269%2C0810.3747%2C0810.4847%2C0810.3666%2C0810.1861%2C0810.4723%2C0810.0976&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper we develop a nonparametric regression method that is\nsimultaneously adaptive over a wide range of function classes for the\nregression function and robust over a large collection of error distributions,\nincluding those that are heavy-tailed, and may not even possess variances or\nmeans. Our approach is to first use local medians to turn the problem of\nnonparametric regression with unknown noise distribution into a standard\nGaussian regression problem and then apply a wavelet block thresholding\nprocedure to construct an estimator of the regression function. It is shown\nthat the estimator simultaneously attains the optimal rate of convergence over\na wide range of the Besov classes, without prior knowledge of the smoothness of\nthe underlying functions or prior knowledge of the error distribution. The\nestimator also automatically adapts to the local smoothness of the underlying\nfunction, and attains the local adaptive minimax rate for estimating functions\nat a point. A key technical result in our development is a quantile coupling\ntheorem which gives a tight bound for the quantile coupling between the sample\nmedians and a normal variable. This median coupling inequality may be of\nindependent interest."}, "authors": ["Lawrence D. Brown", "T. Tony Cai", "Harrison H. Zhou"], "author_detail": {"name": "Harrison H. Zhou"}, "author": "Harrison H. Zhou", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/07-AOS513", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0810.4802v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0810.4802v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/07-AOS513 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62G08 (Primary) 62G20 (Secondary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0810.4802v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0810.4802v1", "journal_reference": "Annals of Statistics 2008, Vol. 36, No. 5, 2055-2084", "doi": "10.1214/07-AOS513", "fulltext": "arXiv:0810.4802v1 [math.ST] 27 Oct 2008\n\nThe Annals of Statistics\n2008, Vol. 36, No. 5, 2055\u20132084\nDOI: 10.1214/07-AOS513\nc Institute of Mathematical Statistics, 2008\n\nROBUST NONPARAMETRIC ESTIMATION VIA WAVELET\nMEDIAN REGRESSION\nBy Lawrence D. Brown, T. Tony Cai1 and Harrison H. Zhou2\nUniversity of Pennsylvania, University of Pennsylvania and Yale\nUniversity\nIn this paper we develop a nonparametric regression method that\nis simultaneously adaptive over a wide range of function classes for\nthe regression function and robust over a large collection of error distributions, including those that are heavy-tailed, and may not even\npossess variances or means. Our approach is to first use local medians to turn the problem of nonparametric regression with unknown\nnoise distribution into a standard Gaussian regression problem and\nthen apply a wavelet block thresholding procedure to construct an\nestimator of the regression function. It is shown that the estimator\nsimultaneously attains the optimal rate of convergence over a wide\nrange of the Besov classes, without prior knowledge of the smoothness\nof the underlying functions or prior knowledge of the error distribution. The estimator also automatically adapts to the local smoothness\nof the underlying function, and attains the local adaptive minimax\nrate for estimating functions at a point.\nA key technical result in our development is a quantile coupling\ntheorem which gives a tight bound for the quantile coupling between\nthe sample medians and a normal variable. This median coupling\ninequality may be of independent interest.\n\n1. Introduction. A standard nonparametric regression model involves\nobservation of {xi , Yi } where\n\n(1)\n\nYi = f (xi ) + \u03bei ,\n\ni = 1, . . . , n.\n\nMost of the theory that has so far been developed for such a model involves\nan assumption that the errors \u03bei are independent and identically-distributed\nReceived June 2007; revised June 2007.\nSupported in part by NSF Grants DMS-00-72578 and DMS-03-06576.\n2\nSupported in part by NSF Career Award DMS-06-45676.\nAMS 2000 subject classifications. Primary 62G08; secondary 62G20.\nKey words and phrases. Adaptivity, asymptotic equivalence, James\u2013Stein estimator,\nmoderate large deviation, nonparametric regression, quantile coupling, robust estimation,\nwavelets.\n1\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2008, Vol. 36, No. 5, 2055\u20132084. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\n(i.i.d.) normal variables. These assumptions are suitable for a wide range of\napplications of the model. In the Gaussian noise setting many smoothing\ntechniques including wavelet thresholding methods have been developed and\nshown to be highly adaptive. However, when the noise \u03bei has a heavy-tailed\ndistribution, these techniques are not readily applicable. For example, in\nCauchy regression where \u03bei has a Cauchy distribution, typical realizations\nof \u03bei contain a few extremely large observations of order n since\n\u0012\n\n1\n1\narctan(n) +\nP (max{\u03bei } \u2265 n) =\n\u03c0\n2\n\n\u0013n\n\n\u0012\n\n\u0013\n\n1\n\u2192 exp \u2212\n.\n\u03c0\n\nIn contrast,\n\u221a the largest observation of the noise \u03bei in Gaussian regression is\nof order log n. It is thus clear that the classical denoising methods designed\nfor Gaussian noise would fail if they are applied directly to the sample\n{Yi } when the noise in fact has a Cauchy distribution. Standard wavelet\nthresholding procedures would also fail in such a heavy-tailed noise setting.\nSee Section 3.2 for further discussions.\nIn the usual nonparametric regression case the regression function f is\noften alternatively described as the conditional expectation f (xi ) = E(Yi |xi ).\nHowever, if the error distributions fail to have a mean, then this conditional\nexpectation will not exist. Even when the conditional expectation exists,\nestimating the conditional expectation may be a very non-robust goal, and\nnot suitable for particular applications. For error distributions that may be\nheavy tailed it seems more suitable to estimate the conditional median of\nYi . Hence, in the sequel we assume (1) holds with\n(2)\n\n\u03bei i.i.d.\n\nand median(\u03bei ) = 0.\n\nThere are practical situations for which the normality assumption is not\nsatisfactory. See, for example, Stuck and Kleiner (1974), Stuck (2000) and\nreferences therein. It is necessary to develop methods to be used in such\ncases, and to establish the theoretical properties of these methods. In this\npaper we develop an estimation method that is simultaneously adaptive over\na wide range of function classes for f and robust over a large collection of\nerror distributions for \u03bei , including those that are heavy-tailed, and may not\neven possess variances or means. In brief, our method may be summarized\nas a blockwise wavelet thresholding implementation built from the medians\nof suitably binned data. We first divide the interval [0, 1] into a number of\nequal-length subintervals, then take the median of the observations in each\nsubinterval, and finally apply the BlockJS wavelet thresholding procedure\ndeveloped in Cai (1999) to the local medians together with a bias correction\nto obtain an estimator of the regression function f .\nUnlike most wavelet methods, the performance of the algorithm here is\nnot sensitive to the tail behavior of the distribution of \u03bei , and hence can be\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\n3\n\nshown to have the necessary robustness property. We show that the estimator enjoys a high degree of adaptivity and robustness. It is shown that the\nestimator simultaneously attains the exact optimal rate of convergence over\na wide range of the Besov classes, without prior knowledge of the smoothness\nof the underlying function or prior knowledge of the error distribution. The\nestimator also automatically adapts to the local smoothness of the underlying function, and attains the local adaptive minimax rate for estimating\nfunctions at a point.\nDonoho and Yu (2000) considered this model for \u03b1-stable noise, but the\nrisk properties of their proposal are unclear. In the wavelet regression setting,\nHall and Patil (1996) studied nonparametric location models and achieved\nthe optimal minimax rate up to a logarithmic term, but under an assumption that \u03bei has a finite fourth moment. As we noted, our results do not need\nthe existence of the mean for the noise or prior knowledge of the error distribution. Most closely related to our work is Averkamp and Houdr\u00e9 (2003,\n2005) where the optimal minimax rate of global risk is studied. But their\nnoise is assumed to be known, and their results are not adaptive.\nThe key technical result in our development is a quantile coupling theorem\nthat is used to connect our problem with a more familiar Gaussian setting.\nThe theorem gives a tight bound for the quantile coupling between the\nmedians of i.i.d. random variables and a normal variable. The result enables\nus to treat the medians of the observations in the subintervals as if they\nwere normal random variables. The coupling theorem may be of independent\ninterest, since analogous coupling theorems for means have proved to be an\nimportant general tool in many contexts. See Section 2 for this result and\nfor further discussion and citations to the literature on quantile coupling.\nThe paper is organized as follows. In Section 2 we derive a quantile coupling inequality for medians and obtain a moderate large deviation result.\nThis coupling inequality is needed for the proof of the asymptotic properties of our estimation procedure, and may be of independent interest for\nother statistical applications. Our procedure is defined in Section 3.2 and its\nasymptotic properties are described in Section 4. Section 5 contains further\ndiscussion of our results, and formal proofs are contained in Section 6. The\nreader interested only in the definition of our wavelet regression procedure\nand a description of its properties can skip Section 2 and proceed directly\nto Section 3.\n2. Quantile coupling for median. We begin with a brief introduction to\nquantile coupling. Let X be a random variable with distribution G and Y\nwith a continuous distribution F . Define\n(3)\n\ne = G\u22121 (F (Y )),\nX\n\n\f4\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\ne = L(X) [cf. Pollard (2001),\nwhere G\u22121 (x) = inf{u : G(u) \u2265 x}, then L(X)\ne and Y are now defined on the same probability\npage 41]. Note that X\ne and Y .\nspace. This makes it possible to give a pointwise bound between X\nThe first tight bound of quantile coupling between the sum of i.i.d. random\nvariables with a normal random variable was given in Koml\u00f3s, Major and\nTusn\u00e1dy (1975). A bound for the coupling of a Binomial random variable\nwith a normal random variable is given as follows. For X \u223c Binomial(n, 1/2)\ne ) be defined as in equation (3). Then for some\nand Y \u223c N (n/2, n/4), let X(Y\ne \u2264 \u03b5n,\nconstant C > 0 and \u03b5 > 0, when |X|\n\ne 2\n|X|\n.\nn\nThis result plays a key role in the KMT/Hungarian construction to couple\nthe empirical distribution with a Brownian bridge. A detailed proof of the\nresult can be found in Mason (2001) and Bretagnolle and Massart (1989).\nA general theory for improving the classical quantile coupling bound was\ngiven in Zhou (2005).\nStandard coupling inequalities are mostly focused on the coupling of the\nmean of i.i.d. random variables with a normal variable. In this section we\nstudy the coupling of a median statistic with a normal variable. We derive\na moderate deviation result for the median statistic and obtain a quantile\ncoupling inequality similar to the classical KMT bound for the mean. This\ncoupling result plays a crucial role in this paper. It is the main tool for\nreducing the problem of robust estimation with unknown noise to a well\nstudied problem of Gaussian regression with unknown variance. The result\nhere may be of independent interest because of the fundamental role played\nby the median in statistics.\nLet X1 , . . . , Xn be i.i.d. random variables with density function h. Denote\ne med\nthe sample median by Xmed . We will construct a new random variable X\ne\nby using quantile coupling in (3) such that L(Xmed ) = L(Xmed ) and show\ne\nthat X\nmed can be well approximated by a normal random variable as equation (4). We need the following assumptions on the density function h(x) to\nderive the quantile coupling inequality.\n\n(4)\n\nAssumption (A1).\nx = 0.\n\ne \u2212Y|\u2264C +C\n|X\n\nR0\n\n\u2212\u221e h(x)\n\n= 21 , h(0) > 0, and h(x) is Lipschitz at\n\nHere the Lipschitz condition at 0 means that there is a constant C > 0\nsuch that |h(x) \u2212 h(0)| \u2264 C|x| in an open neighborhood of 0. This condition\nimplies that h is continuous at 0. We assume h(0) > 0 so that the median\nof the distribution is unique and the distribution of the sample median\nis asymptotically normal [cf. Casella and Berger (2002), page 483]. The\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\n5\n\nLipschitz condition is assumed so that a moderate large deviation result\nfor the distribution of sample median can be obtained to derive a quantile\ncoupling inequality as in equation (4).\nTheorem 1. Let Z be a standard normal random variable and let X1 , . . . ,\nXn be i.i.d. with density function h where n = 2k + 1 for some integer\nk \u2265 1. Let Assumption (A1) hold. Then for every n there is a mapping\ne med (Z) : R 7\u2192 R such that L(X\ne med (Z)) = L(Xmed ) and\nX\n\u221a\nC \u221a\nC\n2\ne\ne\ne\n(5) | 4nh(0)X\nwhen |X\nmed \u2212 Z| \u2264 \u221a + \u221a | 4nh(0)Xmed |\nmed | \u2264 \u03b5\nn\nn\nwhere C, \u03b5 > 0 depend on h but not on n.\nThe quantile coupling bound here is similar to the classical KMT bound\n(4) for the sample mean. This result has close connection to strong approximation of quantile process in Cs\u00f6rg\u0151 and R\u00e9v\u00e9sz (1978). The condition of\nTheorem 1 here is weaker. Only a Lipschitz condition at x = 0 is assumed\nhere to establish the non-uniform bound given in (5). As shown in Zhou\n(2005), the classical quantile coupling bound for the mean can be improved\nwhen the distribution of Xi is symmetric. Similarly, if we assume\nh\u2032 (0) = 0,\n\u221a\nthe bound in Theorem 1 can be improved from the rate 1/ n to the rate\n1/n. See section 4 for more details. The bound in Theorem 1 can also be\nexpressed in terms of Z, as follows.\ne\nCorollary 1. Under the assumption of Theorem 1, the mapping X\nmed (Z)\nin Theorem 1 satisfies\n\u221a\n\u221a\nC\nemed \u2212 Z| \u2264 \u221a\n| 4nh(0)X\n(1 + |Z|2 )\nwhen |Z| \u2264 \u03b5 n\n(6)\nn\nwhere C, \u03b5 > 0 do not depend on n.\n\nRemark 1. When n = 2k is even, the sample median Xmed is usually\ntaken to be (X(k) + X(k+1) )/2. Similar quantile coupling inequalities as (5)\nbe the median of the original\nand (6) can be obtained. For each i, let X\u2212i,med\nP\nsample with Xi removed. Then Xmed = n1 ni=1 X\u2212i,med . Let Gn\u22121 be the distribution of the median of n \u2212 1 i.i.d. observations with density h and define\n\u22121\ne\n(Zi )1\u2264i\u2264n \u223c L(\u03a6\u22121 \u25e6 Gn\u22121 (X\u2212i,med ), 1 \u2264 i \u2264 n). Let X\n\u2212i,med = Gn\u22121 \u03a6(Zi ).\ne \u2212i,med , 1 \u2264 i \u2264 n) = L(X\u2212i,med , 1 \u2264 i \u2264 n). Now a direct applicaThen L(X\ntion of Theorem 1 gives\n\u221a\nC\n2\ne | + |X\ne\ne med \u2212 Z| \u2264 \u221a\n(1 + | 4nh(0)(|X\n|X\n(k)\n(k+1) |)| )\nn\nP\n\nn\n1\ne | + |X\ne\nwhen |X\n(k)\n(k+1) | \u2264 \u03b5, and Z = n\ni=1 Zi . So in Sections 3 and 5 we\nassume the number of observations in each bin is odd without loss of generality.\n\n\f6\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nThe coupling result given in Theorem 1 in fact holds uniformly over a rich\ncollection of distributions. For 0 < \u01eb1 < 1 and \u01eb2 > 0 define\n\n(7)\n\n\u001a\n\nH\u01eb1 ,\u01eb2 = h :\n\nZ\n\n0\n\n1\n1\nh(x) = , \u01eb1 \u2264 h(0) \u2264 ,\n2\n\u01eb1\n\u2212\u221e\n\n|h(x) \u2212 h(0)| \u2264\n\n\u001b\n\n|x|\nfor all |x| < \u01eb2 .\n\u01eb1\n\nIt can be shown that Theorem 1 holds uniformly for the whole family of\nh \u2208 H\u01eb1 ,\u01eb2 .\nTheorem 2. Let X1 , . . . , Xn be i.i.d. with density h \u2208 H\u01eb1 ,\u01eb2 . For every\ne med (Z) : R 7\u2192 R such that\nn = 2k + 1 with integer k \u2265 1, there is a mapping X\ne\nL(Xmed (Z)) = L(Xmed ) and for two constants C\u01eb1 ,\u01eb2 , \u03b5\u01eb1 ,\u01eb2 > 0 depending\nonly on \u01eb1 and \u01eb2\n\u221a\nC\u01eb ,\u01eb \u221a\n\u01eb1 ,\u01eb2\nemed \u2212 Z| \u2264 C\u221a\nemed |2\n| 4nh(0)X\n+ \u221a1 2 | 4nh(0)X\nn\nn\n\nuniformly over all h \u2208 H\u01eb1 ,\u01eb2 .\n\nRemark 2. The quantile coupling inequalities in Corollary 1 and Remark 1 also hold uniformly over H\u01eb1 ,\u01eb2 by replacing C and \u03b5 there with two\nconstants depending \u01eb1 and \u01eb2 .\n3. Methodology for robust wavelet regression. We now define our robust\nnonparametric regression estimator. Then we apply the median quantile\ncoupling results developed in the previous section to establish its asymptotic\nproperties.\nAs we have mentioned, the first key step in our approach is to bin the data\naccording to the values of the independent variable. The sample median is\nthen computed within each bin. This leads to a new data situation in which\nthe bin centers are treated as the independent variables in a nonparametric\nregression, with the bin medians being the dependent variables. This new\nsituation can then be satisfactorily viewed as if it were a Gaussian regression\nproblem. It is important that the number of bins be chosen in a suitable\nrange. For the applications in our paper it turns out to be appropriate to\nchoose the number of bins to be T \u224d n3/4 , where n is the original sample\nsize. It appears that such a choice of T would also be suitable for use with\nmany other Gaussian nonparametric regression methods.\nProceeding in this way one should expect as a heuristic principle that\nthe resulting nonparametric procedure will inherit the asymptotic optimality properties of the Gaussian nonparametric regression technique that is\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\n7\n\nemployed. Of course, this heuristic principle needs to be established in particular cases. The difficulty of doing so will depend on the nature of the\nGaussian technique and the generality of the asymptotic assumptions.\nIn the present treatment we choose to employ a Gaussian wavelet method\ninvolving a block James\u2013Stein wavelet estimator. Implementation of the procedure is straightforward since the number of bins can be chosen as a power\nof 2, as is especially convenient for wavelet implementation. This estimator\nenjoys excellent asymptotic adaptivity properties in the Gaussian setting.\nWe show that the current binned-median version has analogous properties\nover nearly the same range of Besov balls as does the original Gaussian procedure. The precise statement of asymptotic properties is contained in Theorems 3 and 4. The full strength of the asymptotic properties of our wavelet\nprocedure in a Gaussian setting depends on detailed moderate-deviation\nproperties of the Gaussian distribution. For this reason our proof of asymptotic properties of the binned median version requires careful treatment of\nmoderate-deviation properties of the binned medians, as in the coupling\nresults established in Section 2.\nWe shall focus on the case where the design points {xi }, are equally\nspaced on the interval [0, 1]. The more general case will be discussed at the\nend of Section 4. The procedure, which will be described in detail in the\nnext section, can be briefly summarized as follows. Let the sample {Yi , i =\n1, . . . , n} be given as in (1) where xi = ni and the noise variables \u03bei are\ni.i.d. with an unknown density h. Let J = \u230alog2 n3/4 \u230b. Set T = 2J and m =\nn/T . We divide the interval [0, 1] into T equal-length subintervals. Note that\nj\nT \u224d n3/4 . For 1 \u2264 j \u2264 T , let Ij = {Yi : xi \u2208 ( j\u22121\nT , T ]} be the jth bin and let\nXj be the median of the observations in Ij . We treat Xj as if it were a\nnormal random variable with mean f ( Tj ) + bm and variance 1/(4mh2 (0))\n(see Theorem 1), where\n(9)\n\nbm = E{median(\u03be1 , . . . , \u03bem )}.\n\nThen apply a nonparametric Gaussian regression procedure. In this paper, we apply the BlockJS wavelet thresholding procedure developed in Cai\n(1999) to construct an estimator of f . The final estimator f\u02c6 is given in\nequations (16) and (18).\nWe begin in Section 3.1 with a brief introduction to wavelet block thresholding in the Gaussian regression setting and then give a detailed description\nof our wavelet procedure for robust estimation in Section 3.2.\n3.1. Wavelet block thresholding for Gaussian regression. Let {\u03c6, \u03c8} be a\npair of father and mother wavelets.\nThe functions \u03c6 and \u03c8 are assumed to\nR\nbe compactly supported and \u03c6 = 1. Dilation and translation of \u03c6 and \u03c8\n\n\f8\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\ngenerates an orthonormal wavelet basis. For simplicity in exposition, in the\npresent paper we work with periodized wavelet bases on [0, 1]. Let\n\u03c6pj,k (t) =\n\n\u221e\nX\n\nl=\u2212\u221e\n\np\n\u03c8j,k\n(t) =\n\n\u03c6j,k (t \u2212 l),\n\n\u221e\nX\n\nl=\u2212\u221e\n\n\u03c8j,k (t \u2212 l)\n\nfor t \u2208 [0, 1]\n\nwhere \u03c6j,k (t) = 2j/2 \u03c6(2j t \u2212 k) and \u03c8j,k (t) = 2j/2 \u03c8(2j t \u2212 k). The collection\np\n, j \u2265 j0 \u2265 0, k = 1, . . . , 2j } is then an orthonormal\n{\u03c6pj0 ,k , k = 1, . . . , 2j0 ; \u03c8j,k\n2\nbasis of L [0, 1], provided the primary resolution level j0 is large enough\nto ensure that the support of the scaling functions and wavelets at level j0\nis not the whole of [0, 1]. The superscript \"p\" will be suppressed from the\nnotation for convenience. An orthonormal wavelet basis has an associated\northogonal Discrete Wavelet Transform (DWT) which transforms sampled\ndata into the wavelet coefficients. See Daubechies (1992) and Strang (1992)\nfor further details about the wavelets and discrete wavelet transform. A\nsquare-integrable function f on [0, 1] can be expanded into a wavelet series:\nj\n\n(10)\n\nf (t) =\n\n20\nX\n\nj\n\n\u03b8\u0303j0 ,k \u03c6j0 ,k (t) +\n\n2\n\u221e X\nX\n\n\u03b8j,k \u03c8j,k (t)\n\nj=j0 k=1\n\nk=1\n\nwhere \u03b8\u0303j,k = hf, \u03c6j,k i, \u03b8j,k = hf, \u03c8j,k i are the wavelet coefficients of f .\nThe BlockJS procedure was proposed in Cai (1999) for Gaussian nonparametric regression and was shown to achieve simultaneously three objectives:\nadaptivity, spatial adaptivity, and computational efficiency. The procedure\ncan be most easily explained in the sequence space setting. Suppose we\nobserve the wavelet sequence data:\n(11)\n\nyj,k = \u03b8j,k + \u03c3zj,k ,\n\nj \u2265 j0 , k = 1, 2, . . . , 2j\n\nwhere zj,k are i.i.d. N (0, 1) and the noise level \u03c3 is known. The mean vector \u03b8\nis the object of interest. The BlockJS procedure is as follows. Let J = [log2 n].\nDivide each resolution level j0 \u2264 j < J into nonoverlapping blocks of length\nL = [log n] (or L = 2\u230alog2 (log n)\u230b \u2248 log n). Let Bji denote the set of indices of\nthe coefficients in the i-th block at level j, that is, Bji = {(j, k) : (i \u2212 1)L + 1 \u2264\n2\n2 \u2261P\nk \u2264 iL}. Let Sj,i\n(j,k)\u2208B i yj,k denote the sum of squared empirical wavelet\nj\n\ncoefficients in block Bji . A James\u2013Stein type shrinkage rule is then applied\nto each block Bji . For (j, k) \u2208 Bji ,\n\uf8f1\u0012\n\uf8f2\n\n\u0013\n\n\u03bb\u2217 L\u03c3 2\n1\u2212\nyj,k ,\nfor (j, k) \u2208 Bji , j0 \u2264 j < J,\n2\n\u03b8\u0302j,k =\n(12)\nS\n+\nj,i\n\uf8f3\n0,\nfor j \u2265 J,\nwhere \u03bb\u2217 = 4.50524 is a constant satisfying \u03bb\u2217 \u2212 log \u03bb\u2217 = 3. The threshold\n\u03bb\u2217 = 4.50524 is selected according to a block thresholding oracle inequality\nand a minimax criterion. See Cai (1999) for further details.\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\n9\n\n3.2. Wavelet procedure for robust regression. Now we are ready to give\na detailed description of our procedure for robust estimation. Hereafter we\nshall set g(t) = f (t) + bm where bm is given as in (9).\nApply the discrete wavelet transform to the binned medians X = (X1 , . . . , XT ),\nand let U = T \u22121/2 W X be the empirical wavelet coefficients, where W is the\ndiscrete wavelet transformation matrix. Write\n(13) U = (yej0 ,1 , . . . , yej0 ,2j0 , yj0 ,1 , . . . , yj0 ,2j0 , . . . , yJ\u22121,1 , . . . , yJ\u22121,2J \u22121 )\u2032 .\n\nHere yej0 ,k are the gross structure terms at the lowest resolution level,\nand yj,k (j = j0 , . . . , J \u2212 1, k = 1, . . . , 2j ) are empirical wavelet coefficients\nat level j which represent fine structure at scale 2j . The empirical wavelet\ncoefficients can be written as\n1\n\u221a zj,k + \u03bej,k ,\n(14)\nyj,k = \u03b8j,k + \u01ebj,k +\n2h(0) n\nwhere \u03b8j,k are the true wavelet coefficients of g = f + bm , \u01ebj,k are \"small\"\ndeterministic approximation errors, zj,k are i.i.d. N (0, 1), and \u03bej,k are some\n\"small\" stochastic errors. The theoretical calculations given in Section 6 will\nshow that both the approximation errors \u01ebj,k and the stochastic errors \u03bej,k\nare negligible in certain sense. If these negligible errors are ignored then we\nhave\n1\n\u221a zj,k ,\n(15)\nyj,k \u2248 \u03b8j,k +\n2h(0) n\nwhich is the\u221asame as the idealized sequence model (11) with noise level\n\u03c3 = 1/(2h(0) n).\nThe BlockJS procedure is then applied to the empirical coefficients yj,k as\nif they are distributed as in (15). More specifically, at each resolution level j,\nthe empirical wavelet coefficients yj,k are grouped into nonoverlapping blocks\nof length L. As in the sequence estimation setting let Bji = {(j, k) : (i \u2212 1)L +\n2 \u2261P\n2\n2\nb2\n1 \u2264 k \u2264 iL} and let Sj,i\n(j,k)\u2208Bji yj,k . Let h (0) be an estimator of h (0)\n[see equation (38) for an estimator]. A modified James\u2013Stein shrinkage rule\nis then applied to each block Bji , that is,\n(16)\n\n\u0012\n\n\u03b8\u0302j,k = 1 \u2212\n\n\u03bb\u2217 L\n\nb 2 (0)nS 2\n4h\nj,i\n\n\u0013\n\n+\n\nyj,k\n\nfor (j, k) \u2208 Bji ,\n\nwhere \u03bb\u2217 = 4.50524 is the solution to the equation \u03bb\u2217 \u2212 log \u03bb\u2217 = 3 and\nb 2 (0)n in the shrinkage factor of (16) is due to the fact that the noise\n4h\n\u221a\nlevel in (15) is \u03c3 = 1/(2h(0) n). For the gross structure terms at the low\u02c6\nest resolution level j0 , we set \u03b8\u0303j0 ,k = \u1ef9j0 ,k . The estimate of g at the equally\ni\nspaced sample points { T : i = 1, . . . , T } is then obtained by applying the\n\n\f10\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\ninverse discrete wavelet transform (IDWT) to the denoised wavelet coeffi[\ni\n) : i = 1, . . . , T }\ncients. That is, {g( i ) : i = 1, . . . , T } is estimated by gb = {g(\nT\n\nT\n\nwith gb = T 1/2 W \u22121 * \u03b8\u0302. The estimate of the whole function g = f + bm is given\nby\nj\n\ngb(t) =\n\n20\nX\n\u02c6\n\n\u03b8\u0303j0 ,k \u03c6j0 ,k (t) +\n\nJ\u22121\n2j\nXX\n\n\u03b8\u0302j,k \u03c8j,k (t).\n\nj=j0 k=1\n\nk=1\n\nTo get an estimator of f we need to also estimate bm . This is done as follows.\nDivide each bin Ij into two sub-bins with the first bin of the size \u230a m\n2 \u230b. Let\nXj\u2217 be the median of observations in the first sub-bin. We set\n(17)\n\nb\u0302m =\n\n1X \u2217\n(Xj \u2212 Xj )\nT j\n\nand define\nj\n\n(18)\n\nfbn (t) = gbn (t) \u2212 b\u0302m =\n\n20\nX\n\u02c6\n\n\u03b8\u0303 j0 ,k \u03c6j0 ,k (t) +\n\n2j\nJ\u22121\nXX\n\nj=j0 k=1\n\nk=1\n\n\u03b8\u0302j,k \u03c8j,k (t) \u2212 b\u0302m .\n\nRemark 3. The quantity bm is the systematic bias due to the expectation of the median of the noise \u03bei in each bin. Lemma 5 in Section 6 shows\nh\u2032 (0)\n\u22121 + O(m\u22122 ). Hence this systematic bias can possibly\nthat bm = \u2212 8h\n3 (0) m\nbe dominant if it is ignored. The estimate b\u0302m serves as \"bias correction.\"\nLemma 5 shows that the estimation error of b\u0302m is negligible relative to the\nminimax risk of f\u02c6n when m = O(n1/4 ).\n\n4. Adaptivity and robustness of the procedure. We study the theoretical properties of our procedure over the Besov spaces that are by now standard for the analysis of wavelet regression methods. Besov spaces are a very\nrich class of function spaces and contain as special cases many traditional\nsmoothness spaces such as H\u00f6lder and Sobolev spaces. Roughly speaking,\n\u03b1 contains functions having \u03b1 bounded derivatives in\nthe Besov space Bp,q\nLp norm, the third parameter q gives a finer gradation of smoothness. Full\ndetails of Besov spaces are given, for example, in Triebel (1992) and DeVore\nand Popov (1988). For a given r-regular mother wavelet \u03c8 with r > \u03b1 and\na fixed primary resolution level j0 , the Besov sequence norm k * kb\u03b1p,q of the\nwavelet coefficients of a function f is then defined by\n(19)\n\nkf kb\u03b1p,q = k\u03be j kp +\n0\n\n\u221e\nX\n\nj=j0\n\njs\n\nq\n\n(2 k\u03b8 j kp )\n\n!1/q\n\nwhere \u03be j is the vector of the father wavelet coefficients at the primary\n0\nresolution level j0 , \u03b8j is the vector of the wavelet coefficients at level j, and\n\n\f11\n\nROBUST NONPARAMETRIC ESTIMATION\n\ns = \u03b1 + 21 \u2212 p1 > 0. Note that the Besov function norm of index (\u03b1, p, q) of a\nfunction f is equivalent to the sequence norm (19) of the wavelet coefficients\nof the function. See Meyer (1992). We define\n\u03b1\nBp,q\n(M ) = {f ; kf kb\u03b1p,q \u2264 M }.\n\n(20)\n\nIn the case of Gaussian noise Donoho and Johnstone (1998) show that the\n\u03b1 (M ),\nminimax risk of estimating f over the Besov body Bp,q\n\u03b1\nR\u2217 (Bp,q\n(M )) = inf\n\n(21)\n\nfb\n\nsup\n\u03b1 (M )\nf \u2208Bp,q\n\nEkf\u02c6 \u2212 f k22 ,\n\nconverges to 0 at the rate of n\u22122\u03b1/(1+2\u03b1) as n \u2192 \u221e.\nIn addition to Assumption (A1) in Section 2, we need the following weak\ncondition on the density h of \u03bei .\nAssumption (A2).\n\nR\n\n|x|\u01eb3 h(x) dx < \u221e for some \u01eb3 > 0.\n\nThis assumption guarantees that the moments of the median of the binned\ndata are well approximated by those of the normal random variable. Note\nthat Assumption (A2) is satisfied by Cauchy distribution for any 0 < \u01eb3 < 1.\nFor 0 < \u01eb1 < 1, \u01ebi > 0, i = 2, 3, 4, define H = H(\u01eb1 , \u01eb2 , \u01eb3 , \u01eb4 ) by\n\u001a\n\n(3)\n\n(22) H = h : h \u2208 H\u01eb1 ,\u01eb2 , |h\n\n(x)| \u2264 \u01eb4 for |x| \u2264 \u01eb3 and\n\nZ\n\n\u01eb3\n\n\u001b\n\n|x| h(x) dx < \u01eb4 .\n\nThe following theorem shows that our estimator achieves optimal global\n\u03b1 (M ) defined in (20) and\nadaptation for a wide range of Besov balls Bp,q\nuniformly over the family of noise distributions given in (22).\nTheorem 3. Suppose the wavelet \u03c8 is r-regular. Then the estimator f\u02c6n\n2 \u2212\u03b1/3\n> p1 ,\ndefined in (18) satisfies, for p \u2265 2, \u03b1 \u2264 r and 2\u03b11+2\u03b1\nsup\n\nsup\n\n\u03b1 (M )\nh\u2208H f \u2208Bp,q\n\nand for 1 \u2264 p < 2, \u03b1 \u2264 r and\nsup\n\nsup\n\n\u03b1 (M )\nh\u2208H f \u2208Bp,q\n\nEkfbn \u2212 f k22 \u2264 Cn\u22122\u03b1/(1+2\u03b1) ,\n\n2\u03b12 \u2212\u03b1/3\n1+2\u03b1\n\n> p1 ,\n\nEkfbn \u2212 f k22 \u2264 Cn\u22122\u03b1/(1+2\u03b1) (log n)(2\u2212p)/(p(1+2\u03b1)) .\n\nTheorem 3 shows that the estimator simultaneously attains the optimal\nrate of convergence over a wide range of the Besov classes for f and a\nlarge collection of the unknown error distributions for \u03bei . In this sense, the\nestimator enjoys a high degree of adaptivity and robustness.\n\n\f12\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nFor functions of spatial inhomogeneity, the local smoothness of the functions varies significantly from point to point and global risk given in Theorem\n3 cannot wholly reflect the performance of estimators at a point. The local\nrisk measure\nR(fb(t0 ), f (t0 )) = E(fb(t0 ) \u2212 f (t0 ))2\n\n(23)\n\nis used for spatial adaptivity.\nThe local smoothness of a function can be measured by its local H\u00f6lder\nsmoothness index. For a fixed point t0 \u2208 (0, 1) and 0 < \u03b1 \u2264 1, define the local\nH\u00f6lder class \u039b\u03b1 (M, t0 , \u03b4) as follows:\n\u039b\u03b1 (M, t0 , \u03b4) = {f : |f (t) \u2212 f (t0 )| \u2264 M |t \u2212 t0 |\u03b1 , for t \u2208 (t0 \u2212 \u03b4, t0 + \u03b4)}.\n\nIf \u03b1 > 1, then\n\n\u2032\n\n\u039b\u03b1 (M, t0 , \u03b4) = {f : |f (\u230a\u03b1\u230b) (t) \u2212 f (\u230a\u03b1\u230b) (t0 )| \u2264 M |t \u2212 t0 |\u03b1 for t \u2208 (t0 \u2212 \u03b4, t0 + \u03b4)}\n\nwhere \u230a\u03b1\u230b is the largest integer less than \u03b1 and \u03b1\u2032 = \u03b1 \u2212 \u230a\u03b1\u230b.\nIn Gaussian nonparametric regression setting, it is a well-known fact that\nfor estimation at a point, one must pay a price for adaptation. The optimal rate of convergence for estimating f (t0 ) over function class \u039b\u03b1 (M, t0 , \u03b4)\nwith \u03b1 completely known is n\u22122\u03b1/(1+2\u03b1) . Lepski (1990) and Brown and Low\n(1996a, 1996b) showed that one has to pay a price for adaptation of at least\na logarithmic factor. It is shown that the local adaptive minimax rate over\nthe H\u00f6lder class \u039b\u03b1 (M, t0 , \u03b4) is (log n/n)2\u03b1/(1+2\u03b1) .\nThe following theorem shows that our estimator achieves optimal local\nadaptation with the minimal cost uniformly over the family of noise distributions defined in (22).\nTheorem 4. Suppose the wavelet \u03c8 is r-regular with r \u2265 \u03b1 > 1/6. Let\nt0 \u2208 (0, 1) be fixed. Then the estimator f\u02c6n defined in (18) satisfies\n(24)\n\nsup\n\nsup\n\nh\u2208H f \u2208\u039b\u03b1 (M,t0 ,\u03b4)\n\nE(fbn (t0 ) \u2212 f (t0 ))2 \u2264 C *\n\n\u0012\n\nlog n\nn\n\n\u00132\u03b1/(1+2\u03b1)\n\n.\n\nTheorem 4 shows that the estimator automatically attains the local adaptive minimax rate for estimating functions at a point, without prior knowledge of the smoothness of the underlying functions or prior knowledge of\nthe error distribution.\nRemark 4. After binning and taking the medians, in principle any\nstandard wavelet thresholding estimators could then be used. For example,\nthe VisuShrink\nprocedure of Donoho and Johnstone (1994) with threshold\n\u221a\n\u03bb = \u03c3 2 log n can be applied. In this case the resulting estimator satisfies\nsup\n\nsup\n\n\u03b1 (M )\nh\u2208H f \u2208Bp,q\n\nEkfbn \u2212 f k22 \u2264 C\n\n\u0012\n\nlog n\nn\n\n\u00132\u03b1/(1+2\u03b1)\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\nfor 1 \u2264 p \u2264 \u221e, \u03b1 \u2264 r and\n(25)\n\nsup\n\nsup\n\nh\u2208H f \u2208\u039b\u03b1 (M,t0 ,\u03b4)\n\nfor r \u2265 \u03b1 > 1/6.\n\n2\u03b12 \u2212\u03b1/3\n1+2\u03b1\n\n>\n\n1\np\n\n13\n\nand\n\nE(fbn (t0 ) \u2212 f (t0 ))2 \u2264 C *\n\n\u0012\n\nlog n\nn\n\n\u00132\u03b1/(1+2\u03b1)\n\nWe have so far focused on the equally spaced design case. When the design\nis not equally spaced, one can either group the sample using equal-length\nsubintervals as in Section 3.2 or bin the sample so that each bin contains\nthe same number of observations, and then take the median of each bin.\nThe first method produces equally spaced medians that are heteroskedastic\nwith the variances depending on the number of observations in the bins. In\nthis case a wavelet procedure for heteroskedastic Gaussian noise can then\nbe applied to the medians to obtain an estimator of f . The second method\nproduces unequally spaced medians that are homoskedastic since the number\nof observations in the bins are the same. A wavelet procedure for unequally\nspaced observations with homoskedastic Gaussian noise can then be used to\nget an estimator of f . For wavelet procedures for heteroskedastic Gaussian\nnoise or unequally spaced samples, see, for example, Cai and Brown (1998),\nKovac and Silverman (2000) and Antoniadis and Fan (2001).\n5. Further discussion. Theorem 1 gives a general quantile coupling inequality between the median of i.i.d. random variables X1 , . . . , Xn and a\nnormal random variable. The collection of the distributions of the i.i.d. random variables includes the Cauchy and Gaussian distributions as special\ncases. Note that for both Cauchy and Gaussian distributions, h\u2032 (0) = 0,\nwhich suggests we may have a tighter quantile coupling bound as in Zhou\n(2005). Let us further assume that h\u2032 (0) = 0, and h\u2032\u2032 (0) exists. We can derive\na sharper moderate large deviation result for the median and then obtain\na tighter quantile coupling inequality\nwhich improves the classical quantile\n\u221a\ncoupling bounds with a rate 1/ n under certain smoothness conditions for\nthe distribution function. For every n, we can show that there is a mape\ne\nping X\nmed (Z) : R 7\u2192 R such that the random variable Xmed (Z) has the same\ndistribution as the median Xmed of X1 , . . . , Xn and\n\u221a\n\u221a\nemed \u2212 Z| \u2264 C 1 (1 + |Z|3 )\n| 4nh(0)X\nwhen |Z| \u2264 \u03b5 n\nn\nwhere C, \u03b5 > 0 do not depend on n. We can even establish an asymptotic\nequivalence result in Le Cam's sense. Assume that\nf \u2208 F = {f : |f (y) \u2212 f (x)| \u2264 M |x \u2212 y|d }\nwith d > 3/4. In the current setting, we modify the procedure with T =\nn2/3 / log n. Then m = n/T = n1/3 log n. Recall that Xj is the median of\n\n\f14\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nthe observations on each bin Ij with 1 \u2264 j \u2264 T . Let \u03b7j be the median of\ncorresponding noise, then\n\u0012 \u0013\n\n\u0012 \u0013\n\ni\ni\n\u2264 Xj \u2212 \u03b7j \u2264\nmax\nf\n.\nmin\nf\nn\nn\n(j\u22121)m+1\u2264i\u2264jm\n(j\u22121)m+1\u2264i\u2264jm\nWe need to give an asymptotic justification that it is fine treating Xj as if\nit were a normal random variable with mean f (j/T ) and variance 4mh12 (0) .\nWe can show that observing {Xj } is asymptotically equivalent to observing\nXj\u2020 = f\n\n\u0012\n\nj\nT\n\n\u0013\n\n+ Zj ,\n\ni.i.d.\n\n\u0012\n\nZj \u223c N 0,\n\n\u0013\n\n1\n,\n4mh2 (0)\n\n1\u2264j \u2264T\n\nin Le Cam's sense by showing that the total variation distance between the\ndistributions of Xj 's and Xj\u2020 's tends to 0, that is,\n|L({Xj }) \u2212 L({Xj\u2020 })|TV \u2192 0.\nThe result shows that asymptotically there is no difference between observing Xj 's and observing Xj\u2020 's. That means all optimal statistical procedures\nfor the Gaussian model can be carried over to nonparametric robust estimation for bounded losses. For instance, the asymptotic equivalence here\nimplies that adaptive procedures including SureShrink of Donoho and Johnstone (1995), the empirical Bayes estimation of Zhang (2005) and SureBlock\nof Cai and Zhou (2006) can be carried over from the Gaussian regression\nto the Cauchy regression or more general regression. The details of our results will be reported elsewhere. Readers may find recent developments in\nthe asymptotic equivalence theory in Brown and Low (1996a, 1996b), Nussbaum (1996), Grama and Nussbaum (1998) and Golubev, Nussbaum and\nZhou (2005).\n6. Proofs. We shall prove the main results in the order of Theorem 3,\nTheorem 4 and then Theorems 1 and 2. In this section C denotes a positive\nconstant not depending on n that may vary from place to place and we set\nd \u2261 min(\u03b1 \u2212 1p , 1). For simplicity we shall assume that n is divisible by T in\nthe proof. We first collect necessary tools that are needed for the proofs of\nTheorems 3 and 4.\n6.1. Preparatory results. In our procedure, there are two steps: (1) binning the data and taking the median in each bin; (2) applying wavelet transform to the medians and using BlockJS to construct an estimator of f . In\nthis section, we give two results associated with these two steps. Recall that\nwe denote by Xj the median of each bin Ij in step 1 and treat Xj as if it were\na normal random variable with mean f (j/T )\u2212 bm and variance 1/(4mh2 (0)).\n\n\f15\n\nROBUST NONPARAMETRIC ESTIMATION\n\n\u03b1 (M ) can be emThe coupling inequality and the fact that a Besov ball Bp,q\n1\nbedded into a H\u00f6lder ball with smoothness d = min(\u03b1 \u2212 p , 1) > 0 [cf. Meyer\n(1992)] enable us to precisely control the difference between Xj and that\nnormal variable. Proposition 1 gives the bounds for both the deterministic\nand stochastic errors. In Proposition 2 we obtain two risk bounds for the\nBlockJS procedure used in step 2. These two bounds are used to study global\nand local adaptation in the following sections.\n\u03b1 (M ).\nProposition 1. Let Xj be given as in our procedure and let f \u2208 Bp,q\nThen Xj can be written as\n\n\u221a\n\n(26)\n\nmXj =\n\n\u221a\n\nmf\n\n\u0012\n\nj\nT\n\n\u0013\n\n+\n\n\u221a\n\n1\nmbm + Zj + \u01ebj + \u03b6j\n2\n\nwhere:\ni.i.d.\n\n(i) Zj \u223c N (0, h21(0) );\n\n\u221a\nP\n(ii) \u01ebj are constants satisfying |\u01ebj | \u2264 C mT \u2212d and so n1 Ti=1 \u01eb2j \u2264 CT \u22122d ;\n(iii) \u03b6j are independent and \"stochastically small\" random variables satisfying with E\u03b6j = 0, for any l > 0\nE|\u03b6j |l \u2264 Cl m\u2212l/2 + Cl ml/2 T \u2212dl\n\n(27)\nand for any a > 0\n(28)\n\nP (|\u03b6j | > a) \u2264 Cl (a2 m)\u2212l/2 + Cl (a2 T 2d /m)\u2212l/2\n\nwhere Cl > 0 is a constant depending on l only.\nProof. Let \u03b7j = median({\u03bei : (j \u2212 1)m + 1 \u2264 i \u2264 jm}). We define Zj =\nwhere G is the distribution of \u03b7j . It follows from Theorem\n\n1\n\u22121 (G(\u03b7 ))\nj\nh(0) \u03a6 \u221a\n\n1 that\nSet\n\n4m\u03b7j is well approximated by Zj whose distribution is N (0, h21(0) ).\n\u01ebj =\n\n\u221a\n\n=E\n\nmEXj \u2212\n\n\u001a\n\n\u221a\n\n\u221a\n\nmXj \u2212\n\nmf\n\n\u221a\n\n\u0012\n\nj\nT\n\n\u0012\n\n\u0013\n\nj\nmf\nT\n\n\u2212\n\n\u0013\n\n\u221a\n\n\u2212\n\nmbm\n\n\u221a\n\n\u001b\n\nm\u03b7j .\n\nThis is the deterministic component of the approximation error due to binning. It is easy to see that\nmin\n\n(j\u22121)m+1\u2264i\u2264jm\n\n(29)\n\n\u0014 \u0012 \u0013\n\nf\n\n\u0012\n\ni\nj\n\u2212f\nn\nT\n\u0012\n\nj\n\u2264 Xj \u2212 \u03b7j \u2212 f\nT\n\n\u0013\n\n\u2264\n\n\u0013\u0015\n\nmax\n\n(j\u22121)m+1\u2264i\u2264jm\n\n\u0014 \u0012 \u0013\n\n\u0012\n\ni\nj\nf\n\u2212f\nn\nT\n\n\u0013\u0015\n\n.\n\n\f16\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nSince f is in a H\u00f6lder ball with smoothness d = min(\u03b1 \u2212 p1 , 1), then equation\n(29) implies\n\n(30)\n\n|\u01ebj | \u2264\n\u2264\n\n\u0012\n\n\u221a\n\nj\nmE Xj \u2212 f\nT\n\n\u221a\n\nm\n\n\u0013\n\nmax\n\n(j\u22121)m+1\u2264i\u2264jm\n\nSet\n\u221a\n\u221a\n\u03b6j = mXj \u2212 mf\n\n\u2212 \u03b7j\nf\n\n\u0012\n\n\u0012 \u0013\n\nj\nT\n\n\u0012\n\nj\ni\n\u2212f\nn\nT\n\n\u0013\n\n\u2212\n\n\u221a\n\n\u0013\n\n\u221a\n\u2264 C mT \u2212d .\n\n1\nmbm \u2212 \u01ebj \u2212 Zj .\n2\n\n\u221a\n\u221a\n1\nThen E\u03b6j = 0 and mXj = mf (j/T\n\u221a ) + \u01ebj +\u221a2 Zj + \u03b6j . The\n\u221a random error\n\u03b6j is \u221a\nthe sum of two terms, \u03b61j = mXj \u2212 mf (j/T ) \u2212 m\u03b7j \u2212 \u01ebj and\n\u03b62j = m\u03b7j \u2212 21 Zj , where \u03b61j is the random component of the approximation\nerror due to binning, and \u03b62j is the error of approximating\u221athe median by\nthe Gaussian variable. From equation (29) we have|\u03b61j | \u2264 C mT \u2212d and so\nE|\u03b61j |l \u2264 Cl ml/2 T \u2212dl .\n\n(31)\n\nA bound for the approximation error \u03b62j is given in Corollary 1,\n(32)\n\n|\u03b62j | \u2264\n\n\u221a\nwhen |Zj | \u2264 \u03b5 m\n\nC\n(1 + |Zj |2 )\nm1/2\n\n\u221a\nfor some \u03b5 > 0, and the probability of |Zj | > \u03b5 m is exponentially small.\nHence for any finite integer l \u2265 1 (here l is fixed and m = n\u03b3 \u2192 \u221e),\n\u221a\n\u221a\nE|\u03b62j |l = E|\u03b62j |l {|Zj | \u2264 \u03b5 m} + E|\u03b62j |l {|Zj | > \u03b5 m}\n\u221a\n\u2264 Cl m\u2212l/2 + (E|\u03b62j |2l )1/2 [P {|Zj | > \u03b5 m}]1/2\nfor some constant Cl > 0, where\n\u0012\n\n\u221a\n1\n\u03b52\nP {|Z| > \u03b5 m} \u2264 exp \u2212 m\n2\n2\n\n\u0013\n\nby Mill's ratio inequality\n(33)\n\n\u001a\n\n\u03c6(x)\n2\n> max x, \u221a\n1 \u2212 \u03a6(x)\n2\u03c0\n\n\u001b\n\n\u2265\n\n\u0012\n\n1\n2\nx+ \u221a\n2\n2\u03c0\n\n\u0013\n\nfor x > 0\n\nand\n(34)\n\n\u221a\nE| m\u03b7j |2l \u2264 ml E|\u03b7j |2l \u2264 Dl ml\n\nfor some constant Dl > 0 because of Assumption (A2), so we have\n(35)\n\nE|\u03b62j |l \u2264 Cl m\u2212l/2 .\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\n17\n\nDetails for equation (34) are as follows. Assumption (A2) implies\nP (|\u03bei | \u2265 |x|) \u2264\n\nC\n.\n|x|\u01eb3\n\nFor m = 2v + 1 i.i.d. \u03bei , from equation (65) the density of the sample median\nis\n\u221a\n\u0012 \u0012 \u0013\u0013\n8v\n1\nv\ng(x) = \u221a [4H(x)(1 \u2212 H(x))] h(x) exp O\nv\n2\u03c0\n\u221a \u0014\n\u0012 \u0012 \u0013\u0013\n\u0015v\n8v 4C\n1\nh(x) exp O\n\u2264\u221a\n\u01eb3\n|x|\nv\n2\u03c0\n\u221a \u0014\n\u0015\n\u0012 \u0012 \u0013\u0013\n8v 4C v 1\n1\n.\nh(x) exp O\n=\u221a\n\u01eb\n/2\nv\u01eb\n/2\nv\n|x| 3\n2\u03c0 |x| 3\nWhen |x|\u01eb3 /2 \u2265 8C, we have\n\u221a\n\u221a \u0014\n\u0015\n8v 4C v\n8v\n\u221a\n\u2264\u221a\n\u01eb\n/2\n3\n|x|\n2\u03c0\n2\u03c02v\n\nwhich is bounded for all v. This implies as v \u2192 \u221e (m \u223c n\u03b3 in our procedure)\nthe median has any finite moments.\nThus we have\nE|\u03b6j |l \u2264 2l\u22121 (E|\u03b61j |l + E|\u03b62j |l ) \u2264 Cl m\u2212l/2 + Cl ml/2 T \u2212dl\nfrom equations (31) and (35). Equation (28) then follows from Chebyshev's\ninequality. \u0003\nRemark 5. In the proof of Proposition 2, we will see that the noise\n\u03b6j has negligible contribution to the risk of our procedure comparing with\nthe Gaussian noise 12 Zj , when the tail bound P (|\u03b6j | > a) decays faster than\nany polynomial of n. For m = n\u03b3 we have T 2d /m = n2d\u2212\u03b3(2d+1) . Then from\n2d\nequation (28) it is enough to require 0 < \u03b3 < 2d+1\n, that is,\n(36)\n\n\u0012\n\n\u0013\n\n\u03b3\n1\nd = min \u03b1 \u2212 , 1 >\np\n2(1 \u2212 \u03b3)\n\nwhich is satisfied under our assumption (see also Remark 7).\nRemark 6. In the proofs of Theorems 3 and 4, we shall assume without\nloss of generality that h(0) is known and equal to 1 since it can be estimated\nb\naccurately in the sense that there is an estimator h(0)\nsuch that\n(37)\n\nb \u22122 (0) \u2212 h\u22122 (0)| > n\u2212\u03b4 } \u2264 c n\u2212l\nP {|h\nl\n\n\f18\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nfor some \u03b4 > 0 and all l \u2265 1. For instance, we may estimate h\u22122 (0) by\nX\nb \u22122 (0) = 8m\n(38)\n(X2k\u22121 \u2212 X2k )2 .\nh\nT\n\u221a\nP\nNote that E Tm/2 (X2k\u22121 \u2212 X2k )2 = 41 h\u22122 (0) + O( mT \u2212d ), and it is easy to\nshow\nl\n\u221a\n8m X\n(X2k\u22121 \u2212 X2k )2 \u2212 h\u22122 (0) \u2264 Cl ( mT \u2212d )l\nE\nT\n\u221a\n\u2212d\nwhere mT = n\u2212\u03b4 with \u03b4 > 0 in our assumption. Then equation (37) holds\nby Chebyshev inequality. It is very important to see that the asymptotic\nrisk properties of our estimator (16) does not change when replacing \u03bb\u2217 by\n\u03bb\u2217 (1 + O(n\u2212\u03b4 )), thus in the rest of our analysis we may just assume that\nh(0) = 1 without loss of generality.\nWe now consider the wavelet transform of the medians of the binned data.\nFrom Proposition 1 we may write\nZi\n\u03b6i\ng(i/T )\n\u01ebi\n1\n\u221a Xi = \u221a\n+\u221a + \u221a +\u221a .\nn 2 n\nn\nT\nT\nLet (yj,k ) = T \u22121/2 W * X be the discrete wavelet transform of the binned\ndata. Then one may write\n1\n\u2032\n(39)\nyj,k = \u03b8j,k\n+ \u01ebj,k + \u221a zj,k + \u03bej,k\n2 n\n\u2032\nwhere \u03b8j,k\nare the discrete wavelet transform of (g( Ti ))1\u2264i\u2264T which are approximately equal to the true wavelet coefficients of g, zj,k are the transform of the Zi 's and so are i.i.d. N (0, 1) and \u01ebj,k and \u03bej,k are respectively\nthe transforms of ( \u221a\u01ebin ) and ( \u221a\u03b6in ). The following proposition studies the risk\nof BlockJS procedure in Step 2. For each single block the risk bounds here\nfor BlockJS are similar to results in Cai (1999) where Gaussian noise was\nconsidered. But in the current setting the error terms \u01ebj,k and \u03bej,k make the\nproblem more complicated.\n\u2032 +\u01eb\nProposition 2. Let the empirical wavelet coefficients yj,k = \u03b8j,k\nj,k +\n1\n\u221a\nz + \u03bej,k be given as in (39) and let the block thresholding estimator \u03b8\u0302j,k\n2 n j,k\nbe defined as in (16). Then:\n\n(i) for some constant C > 0\nE\n(40)\n\nX\n\n(j,k)\u2208Bji\n\n\u2032\n(\u03b8\u0302j,k \u2212 \u03b8j,k\n)2\n\n(\n\n\u2264 min 4\n+6\n\nX\n\n(j,k)\u2208Bji\n\nX\n\n(j,k)\u2208Bji\n\n\u2032\n(\u03b8j,k\n)2 , 8\u03bb\u2217 Ln\u22121\n\n\u01eb2j,k + CLn\u22122 ;\n\n)\n\n\f19\n\nROBUST NONPARAMETRIC ESTIMATION\n\n(ii) for any 0 < \u03c4 < 1, there exists a constant C\u03c4 > 0 depending on \u03c4 only\nsuch that for all (j, k) \u2208 Bji\n\u2032\n(41) E(\u03b8\u0302j,k \u2212 \u03b8j,k\n)2 \u2264 C\u03c4 * min\n\n\u001a\n\n\u001b\n\n\u2032\n+ \u01ebj,k )2 }, Ln\u22121 + n\u22122+\u03c4 .\nmax {(\u03b8j,k\n\n(j,k)\u2208Bji\n\nWe need the following lemmas to prove Proposition 2. These three lemmas\nare from Brown et al. (2006). See also Cai (1999).\nLemma 1. Let X1 , . . . , Xn be independent random variables with E(Xi ) =\n0 for i = 1, . . . , n. Suppose that E|Xi |k < Mk for all i and all k > 0 with\nMk > 0 some constant not depending on n. Let Y = W X be an orthogonal\ntransform of X = (X1 , . . . , Xn )\u2032 . Then there exist constants Mk\u2032 not depending on n such that E|Yi |k < Mk\u2032 for all i = 1, . . . , n and all k > 0.\nLemma 2. Suppose yi = \u03b8i + zi , i = 1, . . . , L, where \u03b8i are constants and\nP\n\u03bbL\n2\nzi are random variables. Let S 2 = L\ni=1 yi and let \u03b8\u0302i = (1 \u2212 S 2 )+ yi . Then\nEk\u03b8\u0302 \u2212 \u03b8k22 \u2264 k\u03b8k22 \u2227 4\u03bbL + 4E[kzk22 I(kzk22 > \u03bbL)].\n\n(42)\n\nLemma 3.\n\nLet X \u223c \u03c72L and \u03bb > 1. Then\nP (X \u2265 \u03bbL) \u2264 e\u2212(L/2)(\u03bb\u2212log \u03bb\u22121)\n\n(43)\n\nand\n\nEXI(X \u2265 \u03bbL) \u2264 \u03bbLe\u2212(L/2)(\u03bb\u2212log \u03bb\u22121) .\n\nProof of Proposition\nfor (i). From PropoR\n\u221a 2. We only givePthe proof\nsition 1, we have |\u01ebj | \u2264 C mT \u2212d and \u01ebj,k = i \u221a\u01ebin \u03c6J,i \u03c8j,k . Hence\n(44)\n\n|\u01ebj,k | \u2264 sup\nx\n\nX\ni\n\n\u01eb\n\u221ai \u03c6J,i (x) *\nn\n\nZ\n\n|\u03c8j,k (x)| dx \u2264 CT \u2212d 2\u2212j/2 .\n\nThis, as well as Proposition 1, yields that\nXX\n\n(45)\n\nj\n\nk\n\n\u01eb2j,k =\n\n1X 2\n\u01eb \u2264 CT \u22122d .\nn i i\n\nIt is easy to see from Lemma 1 and Proposition 1 that\nE|\u03bej,k |l \u2264 Cl\u2032 (mn)\u2212l/2 + Cl\u2032 (T 2d n/m)\u2212l/2\n\n(46)\nand for any a > 0\n(47)\n\nP (|\u03bej,k | > a) \u2264 Cl\u2032 (a2 mn)\u2212l/2 + Cl\u2032 (a2 T 2d n/m)\u2212l/2 .\n\n\f20\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nIt follows from Lemma 2 that\nX\n\nE\n\n(j,k)\u2208Bji\n\n\u2032\n(\u03b8\u0302j,k \u2212 \u03b8j,k\n)2\n\n\u2264 2E\n\nX\n\n(j,k)\u2208Bji\n\n(\n\n\u2264 2 min\n\n\u2032\n[\u03b8\u0302j,k \u2212 (\u03b8j,k\n+ \u01ebj,k )]2 + 2\n\nX\n\n\u2032\n(\u03b8j,k\n\n(j,k)\u2208Bji\n\n\u221a zj,k + \u03bej,k\n2 n\n\n(j,k)\u2208Bji\n\n(\n\n\u2264 min 4\n+ 2n\n\n+ \u01ebj,k ) , 4\u03bb\u2217 Ln\n\nX \u0012 1\n\n+ 8E\n\nX\n\n(j,k)\u2208Bji\n\n\u22121\n\nE\n\n2\n\n\u00132\n\n\u2032\n(\u03b8j,k\n)2 , 8\u03bb\u2217 Ln\u22121\n\nX\n\nX\n\n(j,k)\u2208Bji\n\n\u22121\n\n)\n\nX\n\n+2\n\n\u01eb2j,k\n\n(j,k)\u2208Bji\n\nX \u0012 1\n\nI\n\n)\n\n\u01eb2j,k\n\n\u221a zj,k + \u03bej,k\n2 n\n\n(j,k)\u2208Bji\n\n+6\n\n\u00132\n\n\u03bb\u2217 L\n>\n4n\n\nX\n\n\u01eb2j,k\n\nX\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 > \u03bb\u2217 L .\n\n(j,k)\u2208Bji\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 I\n\n(j,k)\u2208Bji\n\n(j,k)\u2208Bji\n\n!\n\nDenote by A the event that all |\u03bej,k | are bounded by 2\u221a1nL , that is\n\u221a\nA = {|2 n\u03bej,k | \u2264 L\u22121 for all (j, k) \u2208 Bji }.\nThen it follows from (47) that for any l \u2265 1\nX\n\u221a\nP (Ac ) \u2264\nP (|2 n\u03bej,k | > L\u22121 )\n(j,k)\u2208Bji\n\n(48)\n\n\u2264 Cl\u2032 (L\u22122 m)\u2212l/2 + Cl\u2032 (L\u22122 T d /m)\u2212l/2 .\n\nHence\nD=E\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 I\n\nX\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 I A \u2229\n\n(j,k)\u2208Bji\n\n(j,k)\u2208Bji\n\n=E\n\nX\n\nX\n\n(j,k)\u2208Bji\n\n+E\n\nX\n\n(j,k)\u2208Bji\n\n= D1 + D2 .\n\n!\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 > \u03bb\u2217 L\n\nX\n\n(j,k)\u2208Bji\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 I Ac \u2229\n\n!\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 > \u03bb\u2217 L\n\nX\n\n(j,k)\u2208Bji\n\n!\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 > \u03bb\u2217 L\n\n!\n\n\f21\n\nROBUST NONPARAMETRIC ESTIMATION\n\nL\nx2 + Ly 2 for all x and y. It then\nNote that for any L > 1, (x + y)2 \u2264 L\u22121\nfollows from Lemma 3 and H\u00f6lder's inequality that\n\nX\n\nD1 = E\n\n(j,k)\u2208Bji\n\n\u2264 2E\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 I A \u2229\n\nX\n\n2\nzj,k\nI\n\n(j,k)\u2208Bji\n\n2\nzj,k\n\n2\n\u03bej,k\nI\n\n(j,k)\u2208Bji\n\nX\n\n2\nzj,k\n\n(j,k)\u2208Bji\n\n+ 8n\n\n2p 1/p\n)\n(E\u03bej,k\n\n(j,k)\u2208Bji\n\n!\n\n!\n!\n\n> \u03bb\u2217 L \u2212 \u03bb\u2217 \u2212 1\n\n\u2264 2(\u03bb\u2217 L \u2212 \u03bb\u2217 \u2212 1)e\u2212L/2(\u03bb\u2217 \u2212(\u03bb\u2217 +1)L\nX\n\n(j,k)\u2208Bji\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 > \u03bb\u2217 L\n\n> \u03bb\u2217 L \u2212 \u03bb\u2217 \u2212 1\n\n(j,k)\u2208Bji\n\nX\n\n+ 8nE\n\nX\n\nX\n\n\u22121 \u2212log(\u03bb \u2212(\u03bb +1)L\u22121 )\u22121)\n\u2217\n\u2217\n\nX\n\nP\n\n2\nzj,k\n\n(j,k)\u2208Bji\n\n!!1/q\n\n> \u03bb\u2217 L \u2212 \u03bb\u2217 \u2212 1\n\nwhere p, q > 1 and 1p + 1q = 1. For m = n\u01eb we take\nfrom Lemma 3 and (46) that\n\n1\nq\n\n,\n\n= 1 \u2212 \u01eb. Then it follows\n\nD1 \u2264 \u03bb\u2217 e(\u03bb\u2217 +1)/2 Ln\u22121 + CLm\u22121 n\u22121\u2212\u01eb = CLn\u22121 .\nOn the other hand, it follows from (46) and (48) (by taking l sufficiently\nlarge) that\nX\n\nD2 = E\n\n(j,k)\u2208Bji\n\n\u2264E\n\u2264\n\nX\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 I Ac \u2229\n\nX\n\n(j,k)\u2208Bji\n\n\u221a\n(zj,k + 2 n\u03bej,k )2 > \u03bb\u2217 L\n\n2\n2\n(2zj,k\n+ 8n\u03bej,k\n)I(Ac )\n\n(j,k)\u2208Bji\n\nX\n\n(j,k)\u2208Bji\n\n4 1/2\n4 1/2\n[2(Ezj,k\n) + 8n(E\u03bej,k\n) ] * (P (Ac ))1/2\n\n\u2264 n\u22121 .\nHence, D = D1 + D2 \u2264 CLn\u22121 and consequently\nE\n\nX\n\n(j,k)\u2208Bji\n\n\u2032\n(\u03b8\u0302j,k \u2212 \u03b8j,k\n)2\n\n(\n\n\u2264 min 4\n+6\n\nX\n\n(j,k)\u2208Bji\n\nX\n\n(j,k)\u2208Bji\n\n\u2032\n(\u03b8j,k\n)2 , 8\u03bb\u2217 Ln\u22121\n\n\u01eb2j,k + CLn\u22122\n\n)\n\n!\n\n\f22\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nfor some constant C > 0. \u0003\n\u2032 's are the discrete wavelet transform of (f ( i ))\nRecall that \u03b8j,k\nT 1\u2264i\u2264T and\n\u03b8j,k 's are true wavelet coefficients of f . The following lemma will be used to\n\u2032 's and \u03b8 's. The proof is straightforward and is\nbound the difference of \u03b8j,k\nj,k\nthus omitted.\n\nLet T = 2J and let fJ (x) =\n\nLemma 4.\nsup\n\n\u03b1 (M )\nf \u2208Bp,q\n\nkfJ \u2212 f k22 \u2264 CT \u22122d\n\nPT\n\nk=1\n\n\u221a1\nT\n\nwhere d = min(\u03b1 \u2212 1/p, 1).\n\n\u2032 \u2212 \u03b8 | \u2264 CT \u2212d 2\u2212j/2 and consequently\nAlso, |\u03b8j,k\nj,k\n\u22122d\nCT\n.\n\nPJ\u22121 P\nj=j0\n\n\u2032\nk (\u03b8j,k\n\n\u2212 \u03b8j,k )2 \u2264\n\nLet bm and b\u0302m be defined as in (9) and (17), respectively.\n\nLemma 5.\nThen\n(49)\n\nsup bm +\n\nh\u2208H\n\n(50)\n\nf ( Tk )\u03c6J,k (x). Then\n\nsup\n\nsup\n\n\u03b1 (M )\nh\u2208H f \u2208Bp,q\n\nh\u2032 (0)\n\u2264 Cm\u22122 ,\n8h3 (0)m\n\nE(b\u0302m \u2212 bm )2 \u2264 C * max{T \u22122d , m\u22124 }.\n\nProof. It suffices to consider the case that m = 2v + 1 with v \u2208 N (cf.\nRemark 1), then\nE\u03bemed =\n\nZ\n\nx\n\n(2v + 1)! v\nH (x)(1 \u2212 H(x))v dH(x),\n2\n(v!)\n\nwhere H is the distribution function of \u03be1 . For any \u03b4 > 0, set A\u03b4 = {x : |H(x)\u2212\n1\n2 | \u2264 \u03b4}. It follows from the definition of H that there exists a constant \u03b4 > 0\nsuch that for some \u01eb > 0 we have\n|h(3) (x)| \u2264 1/\u01eb\n\n(51)\n\nand\n\n\u01eb \u2264 h(x) \u2264 1/\u01eb\n\nuniformly over all h \u2208 H for all x \u2208 A\u03b4 . This property implies H \u22121 (x) is well\ndefined and differentiable up to the fourth order for x \u2208 A\u03b4 . Decompose the\nexpectation of the median into two parts:\nE\u03bemed =\n\n\u0012Z\n\nA\u03b4\n\n+\n\nZ\n\nAc\u03b4\n\n\u0013\n\nx\n\n(2v + 1)! v\nH (x)(1 \u2212 H(x))v dH(x) \u2261 Q1 + Q2 .\n2\n(v!)\n\nSince the median has finite moments from equation (34), it is easy to see Q2\ndecays to 0 exponentially fast as v = O(n1/4 ) \u2192 \u221e by the Cauchy\u2013Schwarz\n\n\f23\n\nROBUST NONPARAMETRIC ESTIMATION\n\ninequality and tail probability equations (63) and (64). We now turn to Q1 .\nNote that\nQ1 =\n=\n\nZ\n\n1/2+\u03b4 \u0012\n\n1/2\u2212\u03b4\n\nZ\n\nH \u22121 (x) \u2212 H \u22121\n\n1/2+\u03b4 \u0014 1\n\n1/2\u2212\u03b4\n\n2\n\n\u00d7\n\n(H \u22121 )\u2032\u2032\n\n\u0012 \u0013\u0013\n\n\u0012 \u0013\u0012\n\n1\n2\n\n1\n2\n\nx\u2212\n\n1\n2\n\n(2v + 1)! v\nx (1 \u2212 x)v dx\n(v!)2\n\n\u00132\n\n+\n\n\u0012\n\n(H \u22121 )(4) (\u03c2)\n1\nx\u2212\n24\n2\n\n\u00134 \u0015\n\n(2v + 1)! v\nx (1 \u2212 x)v dx\n(v!)2\n\nv\nv\nsince xv (1 \u2212 x)v is symmetric around x = 21 . Note that (2v+1)!\n(v!)2 x (1 \u2212 x) is\nthe density function of Beta(v + 1, v + 1), and equation (51) implies that\n(H \u22121 )(4) (\u03c2) is uniformly bounded over all h \u2208 H, then\n\n\u0012\n\n\u0012 \u0013\n\n1\n1\n1\n(v + 1)2\nQ1 = (H \u22121 )\u2032\u2032\n+O\n2\n2\n2 (2v + 2) (2v + 3)\nm2\n\n\u0013\n\n\u0012\n\n1\nh\u2032 (0)\n+O\n=\u2212 3\n8h (0)m\nm2\n\n\u0013\n\nand (49) is established.\nNote that for m = 2v + 1, \u230a m\n2 \u230b = v. From Proposition 1 we have\n\u0012\n\nj\nXj = f\nT\n\n\u0013\n\n1\n1\n1\n+ bm + \u221a Zj + \u221a \u01ebj + \u221a \u03b6j .\n2 m\nm\nm\n\nSimilarly we may write\nXj\u2217\n\n\u0012\n\n\u0013\n\nj \u2212 1/2\n1\n1\n1\n=f\n+ bv + \u221a Zj\u2217 + \u221a \u01eb\u2217j + \u221a \u03b6j\u2217\nT\n2 v\nv\nv\n\nwith Zj\u2217 , \u01eb\u2217j and \u03b6j\u2217 satisfying properties (i), (ii), (iii) of Proposition 1, reP\nspectively. Then b\u0302m \u2212 bm = T1 j (Xj\u2217 \u2212 Xj ) \u2212 bm can be written as a sum of\nfive terms as follows:\n\u0012 \u0012\n\u0013\n\u0012 \u0013\u0013\nj \u2212 1/2\nj\n1X\nf\n\u2212f\n+ (bv \u2212 2bm )\nb\u0302m \u2212 bm =\nT j\nT\nT\n\"\n\n1 1X \u2217\n1 1X\n+ \u221a\n\u01ebj \u2212 \u221a\n\u01ebj\nvT j\nmT j\n\n#\n\n\"\n\n1 1X\n1 1X \u2217\nZj \u2212 \u221a\nZj\n+ \u221a\n2 vT j\n2 mT j\n\"\n\n1 1X\n1 1X \u2217\n\u03b6j \u2212 \u221a\n\u03b6j\n+ \u221a\nvT j\nmT j\n\u2261 R1 + R2 + R3 + R4 + R5 .\n\n#\n\n#\n\n\f24\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\n\u22124\n\u22122d and sup\n2\n2\nIt is easy to see that supf \u2208Bp,q\n\u03b1 (M ) R1 \u2264 CT\nh\u2208H R2 \u2264 Cm .\n\u22122d . Note that Z \u2217 \u2212 Z are\n2\nProposition 1 yields suph\u2208H,f \u2208Bp,q\n\u03b1 (M ) R3 \u2264 CT\nj\nj\n1 1\n1\n1\n\u22121\n2\nindependent for j = 1, . . . , T . So ER4 \u2264 h2 (0) ( v + m ) T \u2264 Cn . Similarly,\n\u03b6j\u2217 \u2212 \u03b6j are independent and it then follows from Proposition 1 that ER52 =\no(n\u22121 ). Hence,\n\nsup\n\u03b1 (M )\nh\u2208H,f \u2208Bp,q\n\nE(b\u0302m \u2212 bm )2 \u2264 5R12 + 5R22 + 5R32 + 5ER42 + 5ER52\n\u2264 C max{T \u22122d , m\u22124 }.\n\n\u0003\n\n6.2. Global adaptation: Proof of Theorem 3. Let f\u02c6n be given as in (18).\nNote that\nEkf\u02c6n \u2212 f k2 \u2264 2Ek\u011dn \u2212 gk2 + 2E(b\u0302m \u2212 bm )2 .\n2\n\n2\n\n)2\n\n= o(n\u22122\u03b1/(2\u03b1+1) )\n\nLemma 5 yields that E(b\u0302m \u2212 bm\nand so we need only to\nfocus on bounding Ek\u011dn \u2212 gk22 . Note that the functions f and g differ only\nbm and so the wavelet coefficients coincide, that is, \u03b8j,k =\nRby a constant\nR\nf \u03c8j,k = g\u03c8j,k . Decompose Ek\u011dn \u2212 gk22 into three terms as follows:\nEk\u011dn \u2212 gk22 =\n\n(52)\n\nX\nk\n\n\u221e X\nJ\u22121\nX\nXX\n\u02c6\n2\nE(\u03b8\u0302j,k \u2212 \u03b8j,k )2 +\n\u03b8j,k\nE(\u03b8\u0303 j0 ,k \u2212 \u03b8\u0303j,k )2 +\nj=j0 k\n\nj=J k\n\n\u2261 S1 + S2 + S3 .\n\nIt is easy to see that the first term S1 and the third term S3 are small:\nS1 = 2j0 n\u22121 \u01eb2 = o(n\u22122\u03b1/(1+2\u03b1) ).\n\n(53)\n\nNote that for x \u2208 Rm and 0 < p1 \u2264 p2 \u2264 \u221e\n\nkxkp2 \u2264 kxkp1 \u2264 m1/p1 \u22121/p2 kxkp2 .\n\n(54)\n\nP2j\n\n\u03b1 (M ), so 2js (\nSince f \u2208 Bp,q\n\n(55)\n\nS3 =\n\np 1/p\nk=1 |\u03b8j,k | )\n\n\u221e X\nX\n\nj=J k\n\n\u2264 M . Now (54) yields that\n\n2\n\u03b8j,k\n\u2264 C2\u22122J(\u03b1\u2227(\u03b1+1/2\u22121/p)) .\n\nProposition 2, Lemma 4 and equation (45) yield that\nS2 \u2264 2\n(56)\n\n\u2264\n\nJ\u22121\nXX\n\nj=j0 k\n\nj /L\nJ\u22121\nX 2X\n\nj=j0 i=1\n\n\u2032\nE(\u03b8\u0302j,k \u2212 \u03b8j,k\n)2 + 2\n\n(\n\nmin 8\n\nX\n\n(j,k)\u2208Bji\n\nJ\u22121\nXX\n\nj=j0 k\n\n\u2032\n(\u03b8j,k\n\u2212 \u03b8j,k )2\n\n2\n\u03b8j,k\n, 8\u03bb\u2217 Ln\u22121\n\n)\n\n+6\n\nJ\u22121\nXX\n\nj=j0 k\n\n\u01eb2j,k\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\n+ Cn\u22121 + 10\n\nJ\u22121\nXX\n\nj=j0 k\n\n\u2264\n\nj /L\nJ\u22121\nX 2X\n\n(\n\nj=j0 i=1\n\n\u2032\n(\u03b8j,k\n\u2212 \u03b8j,k )2\n\nX\n\nmin 8\n\n25\n\n(j,k)\u2208Bji\n\n2\n\u03b8j,k\n, 8\u03bb\u2217 Ln\u22121\n\n)\n\n+ Cn\u22121 + CT \u22122d .\n\n1\n\u00d7\nWe now divide into two cases. First consider the case p \u2265 2. Let J1 = [ 1+2\u03b1\nJ\n1/(1+2\u03b1)\n1\nlog2 n]. So, 2 \u2248 n\n. Then (56) and (54) yield\n\nS2 \u2264 8\u03bb\u2217\n\nj /L\nJX\n1 \u22121 2X\n\nLn\u22121 + 8\n\nj=j0 i=1\n\nJ\u22121\nX\n\nX\n\nj=J1 k\n\n2\n\u03b8j,k\n+ Cn\u22121 + CT \u22122d \u2264 Cn\u22122\u03b1/(1+2\u03b1) .\n\nBy combining this with (53) and (55), we have Ekf\u02c6n \u2212 f k22 \u2264 Cn\u22122\u03b1/(1+2\u03b1)\nfor p \u2265 2.\nNow let us consider the case p < 2. First we state the following lemma\nwithout proof.\nP\n\nLemma 6. Let 0 < p < 1 and S = {x \u2208 I Rk : ki=1 xpi \u2264 B, xi \u2265 0, i =\nP\n1, . . . , k}. Then for A > 0, supx\u2208S ki=1 (xi \u2227 A) \u2264 B * A1\u2212p .\n\nLet J2 be an integer satisfying 2J2 \u224d n1/(1+2\u03b1) (log n)(2\u2212p)/p(1+2\u03b1) . Note\nthat\n2j /L\n\nX\ni=1\n\nX\n\n2\n\u03b8j,k\n\n(j,k)\u2208Bji\n\n!p/2\n\nj\n\n\u2264\n\n2\nX\n\n2 p/2\n(\u03b8j,k\n) \u2264 M 2\u2212jsp .\n\nk=1\n\nIt then follows from Lemma 6 that\nj /L\nJ\u22121\nX 2X\n\nmin 8\n\nj=J2 i=1\n\n(57)\n\n(\n\nX\n\n(j,k)\u2208Bji\n\n2\n\u03b8j,k\n, 8\u03bb\u2217 Ln\u22121\n\n)\n\n\u2264 Cn\u22122\u03b1/(1+2\u03b1) (log n)(2\u2212p)/(p(1+2\u03b1)) .\n\nOn the other hand,\nj /L\nJX\n2 \u22121 2X\n\nmin 8\n\nj=j0 i=1\n\n(58)\n\n\u2264\n\n(\n\nJX\n2 \u22121 X\nj=j0\n\nb\n\nX\n\n(j,k)\u2208Bji\n\n2\n\u03b8j,k\n, 8\u03bb\u2217 Ln\u22121\n\n)\n\n8\u03bb\u2217 Ln\u22121 \u2264 Cn\u22122\u03b1/(1+2\u03b1) (log n)(2\u2212p)/(p(1+2\u03b1)) .\n\n\f26\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nWe finish the proof for the case p < 2 by putting (53), (55), (57) and (58)\ntogether:\nEkf\u02c6n \u2212 f k2 \u2264 Cn\u22122\u03b1(1+2\u03b1) (log n)(2\u2212p)/(p(1+2\u03b1)) .\n2\n\nRemark 7. To make the risk of b\u0302m negligible we need to have m\u22124 =\no(n\u22122\u03b1/(1+2\u03b1) ) (see Lemma 5), and to make the approximation error kfJ \u2212\nf k22 negligible, we need to have T \u22122((\u03b1\u22121/p)\u22271) = O(n\u22122\u03b1/(1+2\u03b1) ) (see Lemma\n4). These constraints lead to our choice of m = n1/4 and T = n3/4 . Then we\n2 \u2212\u03b1/3\n2\u03b1\nneed 23 (\u03b1 \u2212 p1 ) > 1+2\u03b1\nor equivalently 2\u03b11+2\u03b1\n> p1 . This last condition is\npurely due to approximation error over Besov spaces.\n6.3. Local adaptation: Proof of Theorem 4. For simplicity, we give the\nproof for H\u00f6lder classes \u039b\u03b1 (M ) instead of local H\u00f6lder classes \u039b\u03b1 (M, t0 , \u03b4).\nNote that for all f \u2208 \u039b\u03b1 (M ), |\u03b8j,k | = |hf, \u03c8j,k i| \u2264 C2\u2212j(1/2+\u03b1) for some constant C > 0 not depending\non f P\n. Note also that for any random variables\nP\nXi , i = 1, . . . , n, E( ni=1 Xi )2 \u2264 ( ni=1 (EXi2 )1/2 )2 . It then follows that\nE(f\u02c6n (t0 ) \u2212 f (t0 ))2\n=E\n\n\" 2j0\nX \u02c6\n\nj\n\n(\u03b8\u0303 j0 ,k \u2212 \u03b8\u0303j0 ,k )\u03c6j0 ,k (t0 ) +\n\nk=1\n\n\u221e X\n2\nX\n\n(\u03b8\u0302j,k \u2212 \u03b8j,k )\u03c8j,k (t0 )\n\nj=j0 k=1\n\n#2\n\n\u2212 (b\u0302m \u2212 bm )\n\"\n\nj\n\n2 1/2\n\n\u2264 (E(b\u0302m \u2212 bm ) )\n+\n\nJ\u22121\nX\n\n+\n\n20\nX\n\n(E(\u02c6\u03b8\u0303 j0 ,k \u2212 \u03b8\u0303j0 ,k )2 \u03c62j0 ,k (t0 ))1/2\n\nk=1\n\n2j\n\nX\n\nj\n\n2\n\n(E(\u03b8\u0302j,k \u2212 \u03b8j,k )\n\nj=j0 k=1\n\n2\n\u03c8j,k\n(t0 ))1/2\n\n+\n\n\u221e X\n2\nX\n\nj=J k=1\n\n#2\n\n|\u03b8j,k \u03c8j,k (t0 )|\n\n\u2261 (Q1 + Q2 + Q3 + Q4 )2 .\nLemma 5 yields that\n(59)\n\nQ1 = (E(b\u0302m \u2212 bm )2 )1/2 = o(n\u2212\u03b1/(2\u03b1+1) ).\n\nSince the wavelet \u03c8 is compactly supported, so there are at most N basis\nfunctions \u03c8j,k at each resolution level j that are nonvanishing at t0 , where N\nis the length of the support of \u03c8. Denote K(t0 , j) = {k : \u03c8j,k (t0 ) 6= 0}. Then\n|K(t0 , j)| \u2264 N. It is easy to see that both Q2 and Q4 are small:\nj\n\n(60)\n\nQ2 =\n\n20\nX\n\n(E(\u02c6\u03b8\u0303 j0 ,k \u2212 \u03b8\u0303j0,k )2 )1/2 |\u03c6j0 ,k (t0 )| = O(n\u22121 )\n\nk=1\n\n\f27\n\nROBUST NONPARAMETRIC ESTIMATION\n\nand\nj\n\n(61)\n\nQ4 =\n\n\u221e X\n2\nX\n\nj=J k=1\n\n|\u03b8j,k ||\u03c8j,k (t0 )| \u2264\n\n\u221e\nX\n\nj=J\n\nN k\u03c8k\u221e 2j/2 C2\u2212j(1/2+\u03b1) \u2264 CT \u2212\u03b1 .\n\nWe now consider the third term Q3 . Applying the bound (41) in Proposition\n2 with \u03c4 < 1/(1 + 2\u03b1) together with Lemma 4 and the bound for \u01ebj,k given\nin (44), we have\nQ3 \u2264\n(62)\n\nJ\u22121\nX\n\nX\n\nj=j0 k\u2208K(t0 ,j)\n\n\u2264C\n\nJ\u22121\nX\n\n2j/2 k\u03c8k\u221e (E(\u03b8\u0302j,k \u2212 \u03b8j,k )2 )1/2\n1\n\n2j/2 [min(2\u2212j(1+2\u03b1) + T \u22122(\u03b1\u22271) 2\u2212j , Ln\u22121 ) + n\u22122+\u03c4 ] 2\n\nj=j0\n\n\u0012\n\nlog n\n\u2264C\nn\n\n\u0013\u03b1/(1+2\u03b1)\n\n.\n\nCombining equations (59)\u2013(63) we have\nE(f\u02c6n (t0 ) \u2212 f (t0 ))2 \u2264 C(log n/n)2\u03b1/(1+2\u03b1) .\n6.4. Proofs of Theorems 1 and 2. Let G(x) be the cumulative distribution function of Xmed and let \u03c6(z) and \u03a6(z) denote respectively the density\nand cumulative distribution function of a standard normal random variable.\nUsing similar arguments in the proof of Theorem 3 in Zhou (2005) or a\nsketch in Section 6 of Koml\u00f3s, Major and Tusn\u00e1dy (1975), we need only to\nshow\n\u221a\n(63) G(x) = \u03a6( 8kx) exp(O(k|x|3 + |x| + k\u22121/2 ))\nfor \u2212 \u03b5 \u2264 x \u2264 0\nand\n(64)\n\n\u221a\n1 \u2212 G(x) = (1 \u2212 \u03a6( 8kx)) exp(O(k|x|3 + |x| + k\u22121/2 ))\nfor 0 \u2264 x \u2264 \u03b5,\n\nwhere O(x) means a value between \u2212Cx and Cx uniformly for some constant\nC > 0. Related asymptotic expansions for the distribution of median can be\nfound in current literature, for instance, Burnashev (1996), but the major\ntheorems there are not sufficient to establish the median coupling inequality.\nLet H(x) be distribution function of X1 . The density of the median X(k+1)\nis\ng(x) =\n\n(2k + 1)! k\nH (x)(1 \u2212 H(x))k h(x).\n(k!)2\n\n\f28\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nStirling's formula, j! =\n\n\u221a\n\n2\u03c0j j+1/2 exp(\u2212j + \u01ebj ) with \u01ebj = O(1/j), gives\n\n(2k + 1)!\n[4H(x)(1 \u2212 H(x))]k h(x)\n4k (k!)2\n\u221a\n\u0012\n\u0012 \u0012 \u0013\u0013\n\u0013\n1\n2 2k + 1 2k + 1 2k+1\nk\n\u221a\n=\n[4H(x)(1 \u2212 H(x))] h(x) exp O\n.\n2k\nk\ne 2\u03c0\n\u221a\n\u221a\nIt is easy to see | 2k + 1/ 2k \u2212 1| \u2264 k\u22121 , and\ng(x) =\n\n\u0012\n\n2k + 1\n2k\n\n\u00132k+1\n\n\u0012\n\n\u0012\n\n1\n= exp \u2212(2k + 1) log 1 \u2212\n2k + 1\n\n\u0013\u0013\n\n\u0012\n\n= exp 1 + O\n\n\u0012 \u0013\u0013\n\n1\nk\n\n.\n\nThen we have, when 0 < H(x) < 1,\n\u221a\n\u0012 \u0012 \u0013\u0013\n1\n8k\nk\n(65)\n.\ng(x) = \u221a [4H(x)(1 \u2212 H(x))] h(x) exp O\nk\n2\u03c0\nFrom the Lipschitz assumption in the theorem, Taylor's expansion gives\n4H(x)(1 \u2212 H(x)) = 1 \u2212 4(H(x) \u2212 H(0))2\n\u0014Z\n\n=1\u22124\n\nx\n\n0\n\n(h(t) \u2212 h(0)) dt + h(0)x\n\n\u00152\n\n= 1 \u2212 4(h(0)x + O(x2 ))2\n\nfor 0 \u2264 |x| \u2264 \u03b5, that is, log(4H(x)(1 \u2212 H(x))) = \u22124h2 (0)x2 + O(|x|3 ) when\n|x| \u2264 2\u03b5 for some \u03b5 > 0. Here \u03b5 is chosen small enough such that h(x) > 0\nfor |x| \u2264 2\u03b5. The Lipschitz assumption in the theorem also implies h(x)\nh(0) =\n1 + O(|x|) = exp(O(|x|)) for |x| \u2264 2\u03b5. Thus\n\u221a\n8kh(0)\ng(x) = \u221a\nexp(\u22128kh2 (0)x2 /2 + O(k|x|3 + |x| + k\u22121 ))\nfor |x| \u2264 2\u03b5.\n2\u03c0\nNow we approximate the distribution function of Xmed by the distribution\nfunction of normal random variable. Without loss of generality we assume\nh(0) = 1. We write\n\u221a\n8k\nfor |x| \u2264 2\u03b5.\ng(x) = \u221a exp(\u22128kx2 /2 + O(k|x|3 + |x| + k\u22121 ))\n2\u03c0\nNow we use this approximation of density functions to give the desired\napproximation of distribution functions. Specifically we shall show\nZ x\n\u221a\ng(t) dt \u2264 \u03a6( 8kx) exp(Ck|x|3 + C|x| + Ck\u22121 )\nG(x) =\n(66)\n\u2212\u221e\n\nand\n(67)\n\n\u221a\nG(x) \u2265 \u03a6( 8kx) exp(\u2212Ck|x|3 \u2212 C|x| \u2212 Ck \u22121 )\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\n29\n\nfor all \u2212\u03b5 \u2264 x \u2264 0 and some C > 0. The proof for 0 \u2264 x \u2264 \u03b5 is similar. Now\nwe give the proof for inequality (66). Note that\n\u221a\n(\u03a6( 8kx) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 ))\u2032\n\u221a\n\u221a\n(68)\n= 8k\u03c6( 8kx) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 )\n\u221a\n\u2212 \u03a6( 8kx)(3Ckx2 \u2212 C) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 ).\n\u221a\n\u221a\n\u221a\nFrom Mill's ratio, inequality (33), we have \u03a6( 8kx)(\u2212 8kx) < \u03c6( 8kx)\nand hence\n\u221a\n\u2212\u03a6( 8kx)(3Ckx2 ) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 )\n\u0013\n\u0012\n\u221a\n\u221a\n3C\nx exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 ).\n\u2265 8k\u03c6( 8kx)\n8\nThis and (68) yield\n\u221a\n(\u03a6( 8kx) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 ))\u2032\n\u0013\n\u0012\n\u221a\n\u221a\n3C\nx exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 )\n\u2265 8k\u03c6( 8kx) 1 +\n8\n\u221a\n\u221a\n\u2265 8k\u03c6( 8kx) exp(Cx/2) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 )\n\u0013\n\u0012\n\u221a\n\u221a\nC 3 C\n\u22121\n.\n\u2265 8k\u03c6( 8kx) exp \u2212 kx \u2212 x + Ck\n2\n2\nHere in the second inequality we apply (1 + C3x/8) \u2265 exp(Cx/2) when\n|Cx| \u2264 C(2\u03b5) < 1/2. Thus we have\n\u221a\n(\u03a6( 8kx) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 ))\u2032\n\u221a\n\u221a\n\u2265 8k\u03c6( 8kx) exp(O(k|x|3 + |x| + k\u22121 ))\nfor C sufficiently large and for \u22122\u03b5 \u2264 x \u2264 0, then\nZ x\nZ x\n\u221a\n(\u03a6( 8kt) exp(\u2212Ckt3 \u2212 Ct + Ck \u22121 ))\u2032\ng(t) dt \u2264\n\u22122\u03b5\n\u22122\u03b5\n\u221a\n\u0014\n\u0015\n\u03a6( \u221a\n8kx) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 )\n=\n\u2212\u03a6( 8k * (2\u03b5)) exp(C(k(2\u03b5)3 + k\u22121 ))\n\u221a\n\u2264 \u03a6( 8kx) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 ).\nIn (65) we see\nZ\n\n\u22122\u03b5\n\ng(t) dt =\n\nZ\n\n\u22122\u03b5\n\n\u2212\u221e\n\n\u2212\u221e\n\n=\n\nZ\n\n0\n\n(2k + 1)! k\nH (t)(1 \u2212 H(t))k h(t) dt\n(k!)2\n\nH(\u22122\u03b5)\n\n(2k + 1)! k\nu (1 \u2212 u)k du\n(k!)2\n\n\f30\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\n= o(k\n\n\u22121\n\n)\n\n\u2264 o(k\u22121 )\n= o(k\u22121 )\n\nZ\n\nH(\u2212\u03b5)\n\nH(\u22123\u03b5/2)\n\nZ\n\nH(x)\n\nH(\u22122\u03b5)\n\nZ\n\n(2k + 1)! k\nu (1 \u2212 u)k du\n(k!)2\n\n(2k + 1)! k\nu (1 \u2212 u)k du\n(k!)2\n\nx\n\ng(t) dt,\n\n\u22122\u03b5\n\nwhere the third equality is from the fact that uk1 (1\u2212 u1 )k = o(k\u22121 )uk2 (1\u2212 u2 )k\nuniformly for u1 \u2208 [0, H(\u22122\u03b5)] and u2 \u2208 [H(\u22123\u03b5/2), H(\u2212\u03b5)]. Thus we have\n\u221a\nG(x) \u2264 \u03a6( 8kx) exp(\u2212Ckx3 \u2212 Cx + Ck\u22121 ),\nwhich is equation (66). Equation (67) can be established in a similar way.\nRemark. Note that in the proof of Theorem 1 it can be seen easily\nthat constants C and \u01eb in equation (5) depend only on the ranges of h(0)\nand the bound of Lipschitz constants of h at a fixed open neighborhood of\n0. Theorem 2 then follows from the proof of Theorem 1 together with this\nobservation.\nREFERENCES\nAntoniadis, A. and Fan, J. (2001). Regularization of wavelets approximations (with\ndiscussion). J. Amer. Statist. Assoc. 96 939\u2013967. MR1946364\nAverkamp, R. and Houdr\u00e9, C. (2003). Wavelet thresholding for non-necessarily Gaussian noise: Idealism. Ann. Statist. 31 110\u2013151. MR1962501\nAverkamp, R. and Houdr\u00e9, C. (2005). Wavelet thresholding for non-necessarily Gaussian noise: Functionality. Ann. Statist. 33 2164\u20132193. MR2211083\nBretagnolle, J. and Massart, P. (1989). Hungarian constructions from the nonasymptotic view point. Ann. Probab. 17 239\u2013256. MR0972783\nBrown, L. D., Cai, T. T., Zhang, R., Zhao, L. H. and Zhou, H. H. (2006). The Rootunroot algorithm for density estimation as implemented via wavelet block thresholding.\nUnpublished manuscript. Available at http://www-stat.wharton.upen.edu/ \u0303lbrown/.\nBrown, L. D. and Low, M. G. (1996a). A constrained risk inequality with applications\nto nonparametric functional estimation. Ann. Statist. 24 2524\u20132535. MR1425965\nBrown, L. D. and Low, M. G. (1996b). Asymptotic equivalence of nonparametric regression and white noise. Ann. Statist. 24 2384\u20132398. MR1425958\nBurnashev, M. V. (1996). Asymptotic expansions for median estimate of a parameter.\nTheory Probab. Appl. 41 632\u2013645. MR1687125\nCai, T. (1999). Adaptive wavelet estimation: A block thresholding and oracle inequality\napproach. Ann. Statist. 27 898\u2013924. MR1724035\nCai, T. and Brown, L. D. (1998). Wavelet shrinkage for nonequispaced samples. Ann.\nStatist. 26 1783\u20131799. MR1673278\nCai, T. T. and Zhou, H. H. (2006). A data-driven block thresholding approach to wavelet\nestimation. Available at http://www.stat.yale.edu/ \u0303hz68.\nCasella, G. and Berger R. L. (2002). Statistical Inference. Duxbury Press, Pacific\nGrove, CA. MR2059318\n\n\fROBUST NONPARAMETRIC ESTIMATION\n\n31\n\nCs\u00f6rg\u0151, M. and R\u00e9v\u00e9sz, P. (1978). Strong approximation of the quantile process. Ann.\nStatist. 6 882\u2013894. MR0501290\nDaubechies, I. (1992). Ten Lectures on Wavelets. SIAM, Philadelphia. MR1162107\nDeVore and Popov (1988). Interpolation of Besov spaces. Trans. Amer. Math. Soc. 305\n397\u2013414.\nDonoho, D. L. and Johnstone, I. (1994). Ideal spatial adaptation via wavelet shrinkage.\nBiometrika 81 425\u2013455. MR1311089\nDonoho, D. L. and Johnstone, I. M. (1995). Adapting to unknown smoothness via\nwavelet shrinkage. J. Amer. Statist. Assoc. 90 1200\u20131224. MR1379464\nDonoho, D. L. and Johnstone, I. M. (1998). Minimax estimation via wavelet shrinkage.\nAnn. Statist. 26 879\u2013921. MR1635414\nDonoho, D. L. and Yu, T. P.-Y. (2000). Nonlinear pyramid transforms based on medianinterpolation. SIAM J. Math. Anal. 31 1030\u20131061. MR1759198\nGrama, I. and Nussbaum, M. (1998). Asymptotic equivalence for nonparametric generalized linear models. Probab. Theory Related Fields 111 167\u2013214. MR1633574\nHall, P. and Patil, P. (1996). On the choice of smoothing parameter, threshold and\ntruncation in nonparametric regression by wavelet methods. J. Roy. Statist. Soc. Ser.\nB 58 361\u2013377. MR1377838\nKoml\u00f3s, J., Major, P. and Tusn\u00e1dy, G. (1975). An approximation of partial sums of independent rv's and the sample df. I Z. Wahrsch. Verw. Gebiete 32 111\u2013131. MR0375412\nKovac, A. and Silverman, B. W. (2000). Extending the scope of wavelet regression\nmethods by coefficient-dependent thresholding. J. Amer. Statist. Assoc. 95 172\u2013183.\nLepski, O. V. (1990). On a problem of adaptive estimation in white Gaussian noise.\nTheor. Probab. Appl. 35 454\u2013466. MR1091202\nMason, D. M. (2001). Notes on the KMT Brownian bridge approximation to the uniform\nempirical process. In Asymptotic Methods in Probability and Statistics with Applications\n(N. Balakrishnan, I. A. Ibragimov and V. B. Nevzorov, eds.) 351\u2013369. Birkh\u00e4user,\nBoston. MR1890338\nMeyer, Y. (1992). Wavelets and Operators. Cambridge Univ. Press. MR1228209\nNussbaum, M. (1996). Asymptotic equivalence of density estimation and Gaussian white\nnoise. Ann. Statist. 24 2399\u20132430. MR1425959\nPollard, D. P. (2001). A User's Guide to Measure Theoretic Probability. Cambridge\nUniv. Press.\nStuck, B. W. and Kleiner, B. (1974). A statistical analysis of telephone noise. Bell\nSystem Technical J. 53 1263\u20131320.\nStuck, B. W. (2000). A historical overview of stable probability distributions in signal\nprocessing. IEEE International Conference on Acoustics, Speech, and Signal Processing\n6 3795\u20133797.\nStrang, G. (1992). Wavelet and dilation equations: A brief introduction. SIAM Rev. 31\n614\u2013627. MR1025484\nTriebel, H. (1992). Theory of Function Spaces. II. Birkh\u00e4user, Basel. MR1163193\nZhang, C.-H. (2005). General empirical Bayes wavelet methods and exactly adaptive\nminimax estimation. Ann. Statist. 33 54\u2013100. MR2157796\nZhou, H. H. (2005). A note on quantile coupling inequalities and their applications.\nAvailable at http://www.stat.yale.edu/ \u0303hz68.\n\n\f32\n\nL. D. BROWN, T. T. CAI AND H. H. ZHOU\n\nL. D. Brown\nT. T. Cai\nThe Wharton School\nUniversity of Pennsylvania\nPhiladelphia, Pennsylvania 19104\nUSA\nE-mail: lbrown@wharton.upenn.edu\ntcai@wharton.upenn.edu\n\nH. H. Zhou\nDepartment of Statistics\nYale University\nNew Haven, Connecticut 06511\nUSA\nE-mail: huibin.zhou@yale.edu\n\n\f"}