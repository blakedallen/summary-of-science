{"id": "http://arxiv.org/abs/1104.3966v1", "guidislink": true, "updated": "2011-04-20T08:35:16Z", "updated_parsed": [2011, 4, 20, 8, 35, 16, 2, 110, 0], "published": "2011-04-20T08:35:16Z", "published_parsed": [2011, 4, 20, 8, 35, 16, 2, 110, 0], "title": "On inference for fractional differential equations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.4122%2C1104.3875%2C1104.1934%2C1104.1597%2C1104.0896%2C1104.3321%2C1104.5212%2C1104.2373%2C1104.0163%2C1104.2075%2C1104.0758%2C1104.4687%2C1104.3812%2C1104.4481%2C1104.4099%2C1104.3241%2C1104.3278%2C1104.4805%2C1104.5581%2C1104.0228%2C1104.0611%2C1104.3682%2C1104.1412%2C1104.3702%2C1104.0880%2C1104.2610%2C1104.2243%2C1104.4060%2C1104.5183%2C1104.4092%2C1104.1031%2C1104.3027%2C1104.3703%2C1104.0892%2C1104.1484%2C1104.2115%2C1104.4922%2C1104.3284%2C1104.1456%2C1104.5086%2C1104.5246%2C1104.4585%2C1104.4343%2C1104.4435%2C1104.0211%2C1104.0626%2C1104.2655%2C1104.5295%2C1104.5339%2C1104.3966%2C1104.1967%2C1104.1390%2C1104.3546%2C1104.3344%2C1104.0119%2C1104.3338%2C1104.0229%2C1104.4460%2C1104.2736%2C1104.5034%2C1104.0597%2C1104.4401%2C1104.0810%2C1104.1232%2C1104.1314%2C1104.2456%2C1104.0035%2C1104.2166%2C1104.2885%2C1104.1188%2C1104.2880%2C1104.4892%2C1104.0585%2C1104.3873%2C1104.1989%2C1104.1927%2C1104.1638%2C1104.4306%2C1104.4372%2C1104.3799%2C1104.5322%2C1104.2626%2C1104.2169%2C1104.4950%2C1104.2316%2C1104.2470%2C1104.4322%2C1104.2221%2C1104.2208%2C1104.4155%2C1104.5329%2C1104.2260%2C1104.3236%2C1104.4224%2C1104.0507%2C1104.3880%2C1104.5173%2C1104.1581%2C1104.4275%2C1104.2466%2C1104.4938&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On inference for fractional differential equations"}, "summary": "Based on Malliavin calculus tools and approximation results, we show how to\ncompute a maximum likelihood type estimator for a rather general differential\nequation driven by a fractional Brownian motion with Hurst parameter H>1/2.\nRates of convergence for the approximation task are provided, and numerical\nexperiments show that our procedure leads to good results in terms of\nestimation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.4122%2C1104.3875%2C1104.1934%2C1104.1597%2C1104.0896%2C1104.3321%2C1104.5212%2C1104.2373%2C1104.0163%2C1104.2075%2C1104.0758%2C1104.4687%2C1104.3812%2C1104.4481%2C1104.4099%2C1104.3241%2C1104.3278%2C1104.4805%2C1104.5581%2C1104.0228%2C1104.0611%2C1104.3682%2C1104.1412%2C1104.3702%2C1104.0880%2C1104.2610%2C1104.2243%2C1104.4060%2C1104.5183%2C1104.4092%2C1104.1031%2C1104.3027%2C1104.3703%2C1104.0892%2C1104.1484%2C1104.2115%2C1104.4922%2C1104.3284%2C1104.1456%2C1104.5086%2C1104.5246%2C1104.4585%2C1104.4343%2C1104.4435%2C1104.0211%2C1104.0626%2C1104.2655%2C1104.5295%2C1104.5339%2C1104.3966%2C1104.1967%2C1104.1390%2C1104.3546%2C1104.3344%2C1104.0119%2C1104.3338%2C1104.0229%2C1104.4460%2C1104.2736%2C1104.5034%2C1104.0597%2C1104.4401%2C1104.0810%2C1104.1232%2C1104.1314%2C1104.2456%2C1104.0035%2C1104.2166%2C1104.2885%2C1104.1188%2C1104.2880%2C1104.4892%2C1104.0585%2C1104.3873%2C1104.1989%2C1104.1927%2C1104.1638%2C1104.4306%2C1104.4372%2C1104.3799%2C1104.5322%2C1104.2626%2C1104.2169%2C1104.4950%2C1104.2316%2C1104.2470%2C1104.4322%2C1104.2221%2C1104.2208%2C1104.4155%2C1104.5329%2C1104.2260%2C1104.3236%2C1104.4224%2C1104.0507%2C1104.3880%2C1104.5173%2C1104.1581%2C1104.4275%2C1104.2466%2C1104.4938&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Based on Malliavin calculus tools and approximation results, we show how to\ncompute a maximum likelihood type estimator for a rather general differential\nequation driven by a fractional Brownian motion with Hurst parameter H>1/2.\nRates of convergence for the approximation task are provided, and numerical\nexperiments show that our procedure leads to good results in terms of\nestimation."}, "authors": ["Alexandra Chronopoulou", "Samy Tindel"], "author_detail": {"name": "Samy Tindel"}, "author": "Samy Tindel", "arxiv_comment": "33 pages, 2 figures", "links": [{"href": "http://arxiv.org/abs/1104.3966v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1104.3966v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1104.3966v1", "affiliation": "IECN", "arxiv_url": "http://arxiv.org/abs/1104.3966v1", "journal_reference": "Stat. Inference Stoch. Process. 16, 1 (2013) 29-61", "doi": null, "fulltext": "ON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\narXiv:1104.3966v1 [math.PR] 20 Apr 2011\n\nALEXANDRA CHRONOPOULOU AND SAMY TINDEL\nAbstract. Based on Malliavin calculus tools and approximation results, we show how\nto compute a maximum likelihood type estimator for a rather general differential equation driven by a fractional Brownian motion with Hurst parameter H > 1/2. Rates of\nconvergence for the approximation task are provided, and numerical experiments show\nthat our procedure leads to good results in terms of estimation.\n\n1. Introduction\nIn this introduction, we first try to motivate our problem and outline our results. We\nalso argue that only a part of the question can be dealt with in a single paper. We briefly\nsketch a possible program for the remaining tasks in a second part of the introduction.\n1.1. Motivations and outline of the results. The inference problem for diffusion processes is now a fairly well understood problem. In particular, during the last two decades,\nseveral advances have allowed to tackle the problem of inference based on discretely observed diffusions [10, 36, 40], which is of special practical interest.\nMore specifically, consider a family of stochastic differential equations of the form\nYt = a +\n\nZ\n\nt\n\n\u03bc(Ys ; \u03b8) ds +\n0\n\nd Z\nX\n\n0\n\nl=1\n\nt\n\n\u03c3 l (Ys ; \u03b8) dBsl ,\n\nt \u2208 [0, T ],\n\n(1)\n\nwhere a \u2208 Rm , \u03bc(*; \u03b8) : Rm \u2192 Rm and \u03c3(*; \u03b8) : Rm \u2192 Rm,d are smooth enough functions,\nB is a d-dimensional Brownian motion and \u03b8 is a parameter varying in a subset \u0398 \u2282 Rq .\nIf one wishes to identify \u03b8 from a set of discrete observations of Y , most of the methods\nwhich can be found in the literature are based on (or are closely linked to) the maximum\nlikelihood principle. Indeed, if B is a Brownian motion and Y is observed at some equally\ndistant instants ti = i\u03c4 for i = 0, . . . , n, then the log-likelihood of a sample (Yt1 , . . . , Ytn )\ncan be expressed as\nn\nX\n\u0001\u0001\nln (\u03b8) =\nln p \u03c4, Yti\u22121 , Yti ; \u03b8 ,\n(2)\ni=1\n\nDate: June 12, 2018.\n2010 Mathematics Subject Classification. Primary 60H35; Secondary 60H07, 60H10, 65C30, 62M09.\nKey words and phrases. Fractional Brownian motion, Stochastic differential equations, Malliavin calculus, Inference for stochastic processes.\nS. Tindel is partially supported by the ANR grant ECRU. Both authors are part of the BIGS (Biology,\nGenetics and Statistics) team at INRIA Nancy.\n1\n\n\f2\n\nA. CHRONOPOULOU AND S. TINDEL\n\nwhere p stands for the transition semi-group of the diffusion Y . If Y enjoys some ergodic\nproperties, with invariant measure \u03bd\u03b80 under P\u03b80 , then we get\n1\n(3)\na.s.\u2212 lim ln (\u03b8) = E\u03b80 [p (\u03c4, Z1 , Z2 ; \u03b8)] , J\u03b80 (\u03b8),\nn\u2192\u221e n\nwhere Z1 \u223c \u03bd\u03b80 and L(Z2 | Z1) = p(\u03c4, Z1 , * ; \u03b8). Furthermore, it can be shown in a general\ncontext that \u03b8 7\u2192 J\u03b80 (\u03b8) admits a maximum at \u03b8 = \u03b80 . This opens the way to a MLE\nanalysis which is similar to the one performed in the case of i.i.d observations, at least\ntheoretically.\nHowever, in many interesting cases, the transition semi-group p is not amenable to\nexplicit computations, and thus expression (2) has to be approximated in some sense.\nThe most common approach, advocated for instance in [36], is based on a linearization of\neach p(\u03c4, Yti\u22121 , Yti ; \u03b8), which transforms it into a Gaussian density\n\u0001\nN Yti\u22121 + \u03bc(Yti\u22121 ; \u03b8) \u03c4, \u03c3\u03c3 \u2217 (Yti\u22121 ; \u03b8) \u03c4 .\n\nThis linearization procedure is equivalent to the approximation of equation (1) by an\nEuler (first order) numerical scheme. Refinements of this procedure, based on Milstein\ntype discretizations, are proposed in [10].\nSome special situations can be treated differently (and often more efficiently): for instance, in case of a constant diffusion coefficient, the continuous time likelihood can be\ncomputed explicitly by means of Girsanov's theorem. When the dimension of the driving\nBrownian motion B is d = 1, one can also apply It\u00f4's formula in order to be back to\nan equation with constant diffusion coefficient, or use Doss-Sousman representation of\nsolutions to (1). Let us also mention that statistical inference for SDEs driven by L\u00e9vy\nprocesses is currently intensively investigated, with financial motivations in mind.\nThe current article is concerned with the estimation problem for equations of the\nform (1), when the driving process B is a fractional Brownian motion. Let us recall\nthat a fractional Brownian motion B with Hurst parameter H \u2208 (0, 1), defined on a complete probability space (\u03a9, F , P), is a d-dimensional centered Gaussian process. Its law\nis thus characterized by its covariance function, which is given by\n\u0002\n\u0003 1 2H\n\u0001\nE Bti Bsj =\nt + s2H \u2212 |t \u2212 s|2H 1(i=j) ,\ns, t \u2208 R+ .\n2\nThe variance of the increments of B is then given by\nh\n\u00012 i\nE Bti \u2212 Bsi\n= |t \u2212 s|2H ,\ns, t \u2208 R+ , i = 1, . . . , d,\n\nand this implies that almost surely the fBm paths are \u03b3-H\u00f6lder continuous for any \u03b3 < H.\nFurthermore, for H = 1/2, fBm coincides with the usual Brownian motion, converting\nthe family {B H ; H \u2208 (0, 1)} into the most natural generalization of this classical process.\nIn the last decade, some important advances have allowed to solve [33, 43] and understand [19, 34] differential systems driven by fBm for H \u2208 (1/2, 1). The rough paths\nmachinery also allows to handle fBm with H \u2208 (1/4, 1/2), as nicely explained in [11, 14,\n27, 29]. However, the irregular situation H \u2208 (1/4, 1/2) is not amenable to useful moments\nestimates for the solution Y to (1) together with its Jacobian (that is the derivative with\nrespect to the initial condition). This is why we concentrate, in the sequel, on the simpler\ncase H > 1/2 for our estimation problem. In any case, many real world noisy systems are\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n3\n\ncurrently modeled by equations like (1) driven by fBm, and this is particularly present in\nthe Biophysics literature, as assessed by [25, 35], or for Finance oriented applications as\nin [5, 13, 20, 21, 39, 42]. This leads to a demand for rigorous estimation procedures for\nSDEs driven by fractional Brownian motion, which is the object of our paper.\nConcerns about the inference problem for fractional diffusion processes started a decade\nago with the analysis of fractional Ornstein-Uhlenbeck processes [23]. Then a more recent\nrepresentative set of references on the topic includes [37, 41]. More specifically, [41] handles\nthe case of a one-dimensional equation of the form\nZ t\nYt = a + \u03b8\n\u03bc(Ys ) ds + Bt ,\nt \u2208 [0, T ],\n(4)\n0\n\nwhere \u03bc is regular enough, and where B is a fBm with H \u2208 (0, 1). The simple dependence\non the parameter \u03b8 and the fact that an additive noise is considered enables the use of\nGirsanov's transform in order to get an exact expression for the MLE. Convergence of the\nestimator is then obtained through an extensive use of Malliavin calculus.\nAs far as [37] is concerned, it is focused on the case of a polynomial equation, for\nwhich the exact moments of the solution can be computed. The estimator relies then\non a generalization of the moment method, which tries to fit empirical moments of the\nsolution with their theoretical value. The range of application of this method is however\nconfined to specific situations, for the following reasons:\n\u2022 It assumes that N independent runs of equation (1) can be obtained, which is\nusually not the case.\n\u2022 It hinges on multiple integrals computations, which are time consuming and are\navoided in most numerical schemes.\nAs can be seen from this brief review, parameter estimation for rough equations is still in\nits infancy. We shall also argue that it is a hard problem.\nIndeed, if one wishes to transpose the MLE methods used for diffusion processes to the\nfBm context, an equivalent of the log-likelihood functions (2) should first be produced.\nBut the covariance structure of B is quite complex and the attempts to put the law of Y\ndefined by (1) into a semigroup setting are cumbersome, as illustrated by [1, 17, 31]. We\nhave thus decided to consider a highly simplified version of the log-likelihood. Namely,\nstill assuming that Y is observed at a discrete set of instants 0 < t1 < * * * < tn < \u221e, set\nln (\u03b8) =\n\nn\nX\n\nln (f (ti , Yti ; \u03b8)) ,\n\n(5)\n\ni=1\n\nwhere we suppose that under P\u03b8 the random variable Yti admits a density z 7\u2192 f (ti , z; \u03b8).\nNotice that in case of an elliptic diffusion coefficient \u03c3 the density f (ti , *; \u03b8) is strictly\npositive, and thus expression (5) makes sense by a straightforward application of [11,\nProposition 19.6]. However, the successful replication of the strategy implemented for\nBrownian diffusions (that we have tried to summarize above) relies on some highly non\ntrivial questions: existence of an invariant measure for equation (1), rate of convergence\nto this invariant measure, convergence of expressions like (5), characterization of the limit\nin terms of \u03b8 as in (3), to mention just a few. We shall come back to these considerations\n\n\f4\n\nA. CHRONOPOULOU AND S. TINDEL\n\nin the next section, but let us insist at this point on the fact that all those questions would\nfit into a research program over several years.\nOur aim in this paper is in a sense simpler: we assume that quantities like (5) are\nmeaningful for estimation purposes. Then we shall implement a method which enables to\ncompute ln (\u03b8) and optimize it in \u03b8, producing thus a pseudo MLE estimator. We focused\nfirst on this specific aspect of the problem for the following reasons:\n(1) From a statistical point of view, it is obviously essential to obtain a computationally efficient estimation procedure. This will allow us for instance to evaluate\nnumerically the accuracy of our method.\n(2) The procedure itself is nontrivial, and requires the use of advanced stochastic\nanalysis tools: probabilistic representation of the density, Malliavin type integration by parts, Stratonovich-Skorohod correction terms, discretization of systems\nof pathwise stochastic differential equations for instance.\nWe have thus decided to tackle the implementation issues first. If it turns out to be\nsatisfying, we shall then try to proceed to a full justification of our method.\nLet us also mention that it might not be clear to the reader that ln (\u03b8) can be meaningful\nin terms of statistical estimation, since it only involves evaluations at single points Yti .\nHowever our numerical experiments indicate that this quantity behaves nicely for our\npurposes. Moreover, it will become clear from the forthcoming computations that our\nmethodology can be extended to handle quantities like\nn\nX\n\u0001\nl\u0303n (\u03b8) :=\nln f (ti , ti+1 , Yti , Yti+1 ; \u03b8) ,\ni=1\n\nwhere f (s, t, x, z; \u03b8) stands for the density of the couple (Ys , Yt ). This kind of pseudo\nlog-likelihood is obviously closer in spirit to the diffusion case. Densities of tuples could\nalso be considered at the price of technical complications.\nLet us now try to give a flavor of the kind of result we shall obtain in this article, in a\nvery loose form:\n\nTheorem 1.1. Consider Equation (1) driven by a d-dimensional fractional Brownian\nmotion B with Hurst parameter H > 1/2. Assume \u03bc and \u03c3 are smooth enough coefficients,\nand that \u03c3\u03c3 \u2217 is strictly elliptic. For a sequence of times t0 < * * * < tn < \u221e, let yti ,\ni = 1, . . . , n be the corresponding observations. Then:\n(i) The gradient of the log-likelihood function admits the following probabilistic represenP\nVi (\u03b8)\ntation: \u2207l ln (\u03b8) = ni=1 W\n, with\ni (\u03b8)\n\u0014\n\u0010\n\u0011\u0015\n(6)\nWi (\u03b8) = E 1(Yti (\u03b8)>yti ) H(1,...,m) Yti (\u03b8)\nwhere H(j1 ,...,jn ) (Yti (\u03b8)) is an expression involving Malliavin derivatives an Skorohod integrals of Y (\u03b8). A similar expression is also available for Vi (\u03b8).\n(ii) A computational procedure is constructed in order to obtain H(1,...,m) (Yti (\u03b8)) in a suitable way.\n(iii) When Yt is replaced by its Euler scheme approximation with step T /M and expected\nvalues in (6) are approximated thanks to N Monte Carlo steps, we show that\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n5\n\n\u2022 N can be chosen in function of M in an optimal way (see Proposition 4.7).\n\u2022 The corresponding approximation of \u2207l ln (\u03b8) converges to the real one with rate\nn\u2212(2\u03b3\u22121) for any 1/2 < \u03b3 < H.\nAll those results are stated in a more rigorous way in the remainder of the article.\nHere is how our article is structured: we give some preliminaries and notations on Young\nand Malliavin calculus for fractional Brownian motion at Section 2. The probabilistic\nrepresentation for the log-likelihood is given at Section 3. Discretization procedures are\ndesigned at Section 4, and finally numerical examples are given at Section 5.\n1.2. Remaining open problems. We emphasized above the fact that only a part of\nthe problem at stake was going to be solved in the current article. We now briefly sketch\nthe remaining tasks to be treated.\nThe most important obstacle in order to fully justify our methodology is to get a\nsuitable convergence theorem for ln (\u03b8)/n, where ln (\u03b8) is defined by (5). In a natural way,\nthis should be based on some strong ergodicity properties for Yt . After a glance at the\nliterature on ergodicity for fractional systems, one can distinguish two cases:\n(i) When \u03c3(*; \u03b8) is constant, the convergence of L(Yt ) as t \u2192 \u221e is established in [15],\nwith a (presumably non optimal) rate of convergence t\u22121/8 .\n(ii) For a general smooth and elliptic coefficient \u03c3, only the uniqueness of the invariant\nmeasure is shown in [17], with an interesting extension to the hypoelliptic case in [18].\nNothing is known about the convergence of L(Yt), not to mention rates.\nThis brief review already indicates that the convergence to invariant measures is still quite\nmysterious for fractional differential equations, at least for a non constant coefficient\n\u03c3. Moreover, recall that if \u03bd(\u03b8) stands for the invariant measure corresponding to the\nsystem with coefficients \u03bc(*; \u03b8), \u03c3(*; \u03b8), we also wish to retrieve some information on the\ndependence \u03b8 7\u2192 \u03bd(\u03b8) (See [16] for some partial results in this direction).\nLet us mention another concrete problem: even in the case of a constant \u03c3, the convergence of L(Yt ) to an invariant measure \u03bd(\u03b8) is proven in [15] in the total variation sense.\nIn terms of the density p(t, x; \u03b8) of Yt , it means that p(t, *; \u03b8) converges to the density of\n\u03bd in L1 topology. However, in order to get a limit for ln (\u03b8)/n, one expects to use at least\na convergence in some Sobolev space W \u03b1,p for \u03b1, p large enough.\nOne possibility in order to get this sharper convergence is to bound first the density\n\u2032 \u2032\np(t, *; \u03b8) in another Sobolev space W \u03b1 ,p and then to use interpolation theory. It seems\nthus sufficient to obtain Gaussian bounds on p(t, *; \u03b8), uniformly in t. In case of Brownian\ndiffusions, these Gaussian bounds are obtained by analytic tools, thanks to the Markov\nproperty. This method being obviously not available for systems driven by fBm, a possible\ninspiration is contained in the upper Gaussian bounds for the stochastic wave equation\nwhich can be found in [6]. The latter technical results stem from an intensive use of Malliavin calculus, which should also be invoked in our case, and notice the recent efforts [2, 3]\nin this direction.\nFinally, let us mention that it seems possible to produce some reasonable convergent\nparametric estimators for equations driven by fBm in a rather general context. Among\nthe methods which can be adapted from the diffusion case with the current stochastic\nanalysis techniques, let us mention the least square estimator of [22], as well as the local\n\n\f6\n\nA. CHRONOPOULOU AND S. TINDEL\n\nasymptotic normality property shown in [12]. However, it seems obvious that the road to\na complete picture of parameter estimation for stochastic equations driven by fBm is still\nhard and long. We hope to complete it in some subsequent communications.\n2. Preliminaries and notations\nAs mentioned in the introduction, we are concerned with equations driven by a ddimensional fractional Brownian motion B. We recall here some basic facts about the\nway to solve those equations, and some Malliavin calculus tools which will be needed later\non. Let us introduce first some general notation for H\u00f6lder type spaces:\nNotation 2.1. We will denote by C \u03b1 (V ) the set of V -valued \u03b1-H\u00f6lder functions for any\n\u03b1 \u2208 (0, 1), and by Cbn (U; V ) the set of n times differentiable functions, bounded together\nwith all their derivatives, from U to V . In the previous notation, U and V stand for two\nfinite dimensional vector spaces. The state space V can be omitted for notational sake\nwhen its value is non ambiguous. When we want to stress the fact that we are working on\na finite interval [0, T ], we write CT\u03b1 (V ) for the space of \u03b1-H\u00f6lder functions f from [0, T ]\nto V . The corresponding H\u00f6lder norms shall be denoted by kf k\u03b1,T .\n2.1. Differential equations driven by fBm. Recall that the equation we are interested\nin is of the form (1). Before stating the assumptions on our coefficients we need an\nadditional notation:\nNotation 2.2. For n, p \u2265 1, a function f \u2208 C p (Rn ; R) and any tuple (i1 , . . . ip ) \u2208\npf\n{1, . . . , d}p , we set \u2202i1 ...ip f for \u2202xi \u2202...\u2202x\n. Similarly, consider a function g\u03b8 \u2208 C p (\u0398; R), for\nip\n1\nn, p \u2265 1 and a vector of parameters \u03b8 \u2208 \u0398 \u2282 Rq . For any tuple (i1 , . . . ip ) \u2208 {1, . . . , q}p ,\n\u2202 p g\u03b8i\nwe set \u2207i1 ...ip g\u03b8i for \u2202\u03b8i ...\u2202\u03b8\n, where i = 1, . . . , n.\ni\n1\n\np\n\nUsing this notation, we work under the following set of assumptions:\nHypothesis 2.3. For any \u03b8 \u2208 \u0398, we assume that \u03bc(*; \u03b8) : Rm \u2192 Rm and \u03c3(*; \u03b8) : Rm \u2192\nRm,d are Cb2 coefficients. Furthermore, we have\nsup\n\u03b8\u2208\u0398\n\n2\nX\n\nX\n\nk\u2207li1 ***il \u03bc(*; \u03b8)k\u221e + k\u2207li1 ***il \u03c3(*; \u03b8)k\u221e < \u221e.\n\nl=0 1\u2264i1 ,...,il \u2264q\n\nWhen equation (1) is driven by a fBm with Hurst parameter H > 1/2 it can be\nsolved, thanks to a fixed point argument, with the stochastic integral interpreted in the\n(pathwise) Young sense (see e.g. [14]). Let us recall that Young's integral can be defined\nin the following way:\nPropositionR 2.4. Let f \u2208 C \u03b3 , g \u2208 C \u03ba with \u03b3 + \u03ba > 1, and 0 \u2264 s \u2264 t \u2264 1. Then\nt\nthe integral s g\u03be df\u03be is well-defined as limit of Riemann sums along partitions of [s, t].\nMoreover, the following estimation is fulfilled:\nZ t\n(7)\ng\u03be df\u03be \u2264 Ckf k\u03b3 kgk\u03ba|t \u2212 s|\u03b3 ,\ns\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n7\n\nwhere the constant C only depends on \u03b3 and \u03ba. A sharper estimate is also available:\nZ t\ng\u03be df\u03be \u2264 |gs | kf k\u03b3 |t \u2212 s|\u03b3 + c\u03b3,\u03ba kf k\u03b3 kgk\u03ba |t \u2212 s|\u03b3+\u03ba .\n(8)\ns\n\nWith this definition in mind and under assumptions 2.3, we can solve our differential\nsystem of interest, and the following moments bounds are proven in [11, 19]:\nProposition 2.5. Consider a fBm B with Hurst parameter H > 1/2. Then:\n(1) Under Hypothesis 2.3, equation (1) driven by B admits a unique \u03b2-H\u00f6lder continuous\nsolution Y , for any \u03b2 < H.\n(2) Furthermore,\n1/\u03b2\n\nkY kT,\u03b2 \u2264 |a| + cf,T kBk\u03b2,T .\n(3) If we denote by Y a the solution to (1) with initial condition a, then\n\u0011\n\u0010\n1/\u03b2\nkY b \u2212 Y a kT,\u03b2 \u2264 |b \u2212 a| exp cf,T kBk\u03b2,T .\n(4) If we only assume that f has linear growth, with \u2207f, \u22072 f bounded, the following\nestimate holds true:\n\u0011\n\u0010\n1/\u03b2\nsupt\u2208[0,T ] |Yt | \u2264 (1 + |a|) exp cf,T kBk\u03b2,T .\nRemark 2.6. The framework of fractional integrals is used in [19] in order to define integrals\nwith respect to B. It is however easily seen to be equivalent to the Young setting we have\nchosen to work with.\n\nSome differential calculus rules for processes controlled by fBm will also be useful in\nthe sequel:\nProposition 2.7. Let B be a d-dimensional fBm with Hurst parameter H > 1/2. Consider a, \u00e2 \u2208 R, b, b\u0302 \u2208 CT\u03b1 (Rd ) with \u03b1 + H > 1, and c, \u0109 \u2208 CT (R) (all these assumptions are\nunderstood in the almost sure sense). Define two processes z, \u1e91 on [0, T ] by\nzt = a +\n\nd Z\nX\nj=1\n\nt\n0\n\nbju\n\ndBuj\n\n+\n\nZ\n\nt\n\ncu du,\n\nand \u1e91t = \u00e2 +\n\n0\n\nd Z\nX\nj=1\n\n0\n\nt\n\nb\u0302ju\n\ndBuj\n\n+\n\nZ\n\nt\n\n\u0109u du.\n\n0\n\nThen for t \u2208 [0, T ], one can decompose the product zt \u1e91t into\nZ t\nn Z th\ni\nX\nj\nj\nj\nzt \u1e91t = a \u00e2 +\n\u1e91u bu + zu b\u0302u dBu +\n[zu \u0109u + \u1e91u cu ] du,\nj=1\n\n0\n\n0\n\nwhere all the integrals with respect to B are understood in the Young sense.\nThe proof of this elementary and classical result is omitted here. See [28, Proposition 2.8] for the proof of a similar rule.\n\n\f8\n\nA. CHRONOPOULOU AND S. TINDEL\n\n2.2. Malliavin calculus techniques. Our representation of the density for the solution\nto (1) obviously relies on Malliavin calculus tools that we proceed now to recall. As already\nmentioned in the introduction, on a finite interval [0, T ] and for some fixed H \u2208 (1/2, 1), we\nconsider (\u03a9, F , P ) the canonical probability space associated with a fractional Brownian\nmotion with Hurst parameter H. That is, \u03a9 = C0 ([0, T ]; Rd) is the Banach space of\ncontinuous functions vanishing at 0 equipped with the supremum norm, F is the Borel\nsigma-algebra and P is the unique probability measure on \u03a9 such that the canonical\nprocess B = {Bt , t \u2208 [0, T ]} is a d-dimensional fractional Brownian motion with Hurst\nparameter H. Remind that this means that B has d independent coordinates, each one\nbeing a centered Gaussian process with covariance RH (t, s) = 12 (s2H + t2H \u2212 |t \u2212 s|2H ).\n2.2.1. Functional spaces. Let E be the space of d-dimensional elementary functions on\n[0, T ]:\nnj \u22121\nn\nX j\nE = f = (f1 , . . . , fd ); fj =\nai 1[tj ,tj\ni\n\n0 = t0 < tj1 < * * * < tjnj \u22121 < tjnj = T,\n\n,\n\ni+1 )\n\ni=0\n\no\n\nfor j = 1, . . . , d . (9)\nWe call H the completion of E with respect to the semi-inner product\nhf, giH =\n\nd\nX\n\nhfi , gi iH0 ,\n\nwhere h1[0,t] , 1[0,s]iH0 := R(s, t),\n\ns, t \u2208 [0, T ].\n\ni=1\n\n\u2217\nThen, one constructs an isometry KH\n: H \u2192 L2 ([0, 1]; Rd ) such that\n\u0001\n\u0001\n\u2217\nKH\n1[0,t1 ] , . . . , 1[0,td ] = 1[0,t1 ] KH (t1 , *), . . . , 1[0,td ] KH (td , *) ,\n\nwhere the kernel KH is given by\n\nKH (t, s) = cH s\nR s\u2227t\n\n1\n\u2212H\n2\n\nZ\n\nt\n\n3\n\n1\n\n(u \u2212 s)H\u2212 2 uH\u2212 2 du\ns\n\nand verifies that RH (t, s) = 0 KH (t, r)KH (s, r) dr, for some constant cH . Moreover, let\n\u2217\nus observe that KH\ncan be represented in the following form: for \u03c6 = (\u03c61 , . . . , \u03c6d ) \u2208 H,\n\u2217\nwe have KH\n\u03c6\nZ 1\n\u0001\n\u2217\n\u2217 1\n\u2217 d\n\u2217 i\nKH \u03c6 = KH \u03c6 , . . . , KH \u03c6 , where [KH \u03c6 ]t =\n\u03c6ir \u2202r KH (r, t) dr.\nt\n\n2.2.2. Malliavin derivatives. Let us start by defining the Wiener integral with respect to\nB: for any element f in E whose expression is given as in (9), we define the Wiener\nintegral of f with respect to B as\nB(f ) :=\n\nj \u22121\nd nX\nX\n\nj=1 i=0\n\nWe also denote this integral as\nrespect to B.\n\nRT\n0\n\naji (Btjj\n\ni+1\n\n\u2212 Btjj ) .\ni\n\nft dBt , since it coincides with a pathwise integral with\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n9\n\nFor \u03b8 : R \u2192 R, and j \u2208 {1, . . . , d}, denote by \u03b8[j] the function with values in Rd having\nall the coordinates equal to zero except the j-th coordinate that equals to \u03b8. It is readily\nseen that\nh \u0010\n\u0011 \u0010\n\u0011i\n[j]\n[k]\nE B 1[0,s) B 1[0,t) = \u03b4j,k Rs,t .\n\nThis definition can be extended by linearity and closure to elements of H, and we obtain\nthe relation\nE [B(f ) B(g)] = hf, giH,\nvalid for any couple of elements f, g \u2208 H. In particular, B(*) defines an isometric map\nfrom H into a subspace of L2 (\u03a9).\nWe can now proceed to the definition of Malliavin derivatives. With this notation 2.2\nin hand, let us consider S be the family of smooth functionals F of the form\n(10)\n\nF = f (B(h1 ), . . . , B(hn )),\n\nwhere h1 , . . . , hn \u2208 H, n \u2265 1, and f is a smooth function with polynomial growth,\ntogether with all its derivatives. Then, the Malliavin derivative of such a functional F is\nthe H-valued random variable defined by\nn\nX\nDF =\n\u2202i f (B(h1 ), . . . , B(hn )) hi .\ni=1\n\nFor all p > 1, it is known that the operator D is closable from Lp (\u03a9) into Lp (\u03a9; H) (see\ne.g. [32, Section 1]). We will still denote by D the closure of this operator, whose domain\nis usually denoted by D1,p and is defined as the completion of S with respect to the norm\n1\n\nkF k1,p := (E(|F |p) + E(kDF kpH )) p .\nIt should also be noticed that partial Malliavin derivatives with respect to each component\nB j of B will be invoked: they are defined, for a functional F of the form (10) and\nj = 1, . . . , d, as\nn\nX\n[j]\nj\n\u2202i f (B(h1 ), . . . , B(hn ))hi ,\nD F =\ni=1\n\nand then extended by closure arguments again. We refer to [32, Section 1] for the definition\nof higher derivatives and Sobolev spaces Dk,p for k > 1. Another essential object related\nto those derivatives is the so-called Malliavin matrix of a Rm -valued random variable\nF \u2208 D1,2 , defined by\n\u0012D\nE\u0013\ni\nj\n\u03b3F =\nDF , DF\n.\n(11)\n1\u2264i,j\u2264m\n\n2.2.3. Skorohod integrals. We will denote by \u03b4 the adjoint of the operator D (also referred\nto as the divergence operator). This operator is closed and its domain, denoted by Dom(\u03b4),\nis the set of H-valued square integrable random variables u \u2208 L2 (\u03a9; H) such that\n|E [hDF, uiH] | \u2264 C kF k2 ,\n1,2\n\nfor all F \u2208 D , where C is some constant depending on u. Moreover, for u \u2208 Dom(\u03b4),\n\u03b4(u) is the element of L2 (\u03a9) characterized by the duality relationship:\nE [F \u03b4(u)] = E [hDF, uiH] ,\n\nfor any F \u2208 D1,2 .\n\n(12)\n\n\f10\n\nA. CHRONOPOULOU AND S. TINDEL\n\nThe quantity \u03b4(u) is usually called Skorohod integral of the process u.\nSkorohod integrals are obviously analytic objects, not suitable for easy numerical implementations. However, they can be related to the Young type integrals introduced at\nProposition 2.4. For this, we need to define another functional space as follows:\nNotation 2.8. We call |H| the space of measurable functions \u03c6 : [0, T ] \u2192 Rd such that\nZ 1Z 1\n2\nk\u03c6k|H| := cH\n|\u03c6r ||\u03c6u ||r \u2212 u|2H\u22122 drdu < +\u221e,\n0\n\n0\n\nwhere cH = H(2H \u2212 1), and we denote by h*, *i|H| the associated inner product. We also\nwrite Dk,p(|H|) for the space of Dk,p functionals with values in |H|.\nThe following proposition is then a slight extension of [32, Proposition 5.2.3]:\nProposition 2.9. Let {uij\nt , t \u2208 [0, 1]}, for i = 1, . . . , m and j = 1, . . . , d, be a stochastic\nprocess in D1,2 (|H|) such that\nd Z 1Z 1\nX\n2H\u22122\n|Dsj uij\ndsdt < +\u221e a.s.\n(13)\nt | |t \u2212 s|\nj=1\n\n0\n\n0\n\nWe also assume\nthat almost surely, u has \u03b2-H\u00f6lder paths with \u03b2 + H > 1. Then the Young\nP RT\nj\nintegral dj=1 0 uij\nt dBt exists and for all i = 1, . . . , m can be written as\nd Z T\nd Z T Z T\nX\nX\nij\nj\ni\n2H\u22122\nut dBt = \u03b4(u ) +\nDsj uij\ndsdt,\nt |t \u2212 s|\nj=1\n\n0\n\nj=1\n\n0\n\n0\n\nwhere \u03b4(u) stands for the Skorohod integral of u.\n\n3. Probabilistic expression for the log-likelihood\nRecall that we are focusing on equation (1) driven by a d-dimensional fBm B, and that\nwe have chosen to use expression (5) as a substitute to the log-likelihood function. We\nhave thus reduced the initial maximization problem to the solution of \u2207l ln (\u03b8) = 0. This\nwill be performed numerically by means of a root approximation algorithm.\nObserve first that in order to define (5), the density of Yt (\u03b8) must exist for any t > 0.\nLet us thus recall the classical setting (given in [19]) under which Yt admits a smooth\ndensity:\nHypothesis 3.1. Let \u03bc and \u03c3 be coefficients satisfying Hypothesis 2.3. For \u03be \u2208 Rm and\n\u03b8 \u2208 \u0398, set \u03b1(\u03be) = \u03c3(\u03be, \u03b8)\u03c3 \u2217(\u03be, \u03b8). Then we assume that\n(i) For any k \u2265 0 and j1 , . . . , jk \u2208 {1, . . . , m} we have\nsup\n\u03b8\u2208\u0398\n\n2\nX\n\nX\n\nk\u2207lp1 ***pl \u2202jk1 ,...,jk \u03bc(*; \u03b8)k\u221e + k\u2207lp1 ***pl \u2202jk1 ,...,jk \u03c3(*; \u03b8)k\u221e \u2264 ck ,\n\nl=0 1\u2264p1 ,...,pl \u2264q\n\nfor a strictly positive constant ck .\n(ii) There exists a strictly positive constant \u03b5 such that h\u03b1(\u03be; \u03b8)\u03b7, \u03b7iRm \u2265 \u03b5|\u03b7|2Rm for any\ncouple of vectors \u03b7, \u03be \u2208 Rm , uniformly in \u03b8 \u2208 \u0398.\nThen the density result for Yt can be read as follows:\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n11\n\nTheorem 3.2. Consider the stochastic differential equation (1) with initial condition\na \u2208 Rm . Assume Hypothesis 3.1 is satisfied. Then, for any t > 0 and \u03b8 \u2208 \u0398, the law of\nYt (\u03b8) admits a C \u221e density, denoted by f (t, *; \u03b8), with respect to Lebesgue's measure.\nIn the sequel, we shall suppose that the density f (t, *; \u03b8) exists without further mention,\nthe aim of this section being to produce a probabilistic representation of f (t, *; \u03b8) for\ncomputational purposes. To this aim, we shall first give the equations governing the\nMalliavin derivatives of the processes Y (\u03b8) and \u2207Y (\u03b8), and then use a stochastic analysis\nformula in order to represent our log-likelihood. We separate these tasks in two different\nsubsections.\n3.1. Some Malliavin derivatives. This section is devoted to a series of preliminary\nlemmas which will enable to formulate our probabilistic representation of f (t, *; \u03b8). Let\nus first introduce a notation which will prevail until the end of the paper:\nNotation 3.3. For a set of indices or coordinates (k1 , . . . , kr ) of length r \u2265 1 and 1 \u2264\nj \u2264 r, we denote by (k1 , . . . , \u01e9j , . . . , kr ) the set of indices or coordinates of length r \u2212 1\nwhere kj has been omitted.\nWe now give a general expression for the higher order derivatives of Yt , borrowed\nfrom [34].\nLemma 3.4. Assume Hypothesis 2.3 and 3.1 hold true. For n \u2265 1 and (i1 , . . . , in ) \u2208\n{1, . . . , d}n , denote by D i1 ,...,in Yti (\u03b8) the nth Malliavin derivative of Yti (\u03b8) with respect to\nthe coordinates B i1 , . . . , B in of B. Then D i1 ,...,in Yti (\u03b8), considered as an element of H\u2297n ,\nsatisfies the following linear equation: for t \u2265 r1 \u2228 * * * \u2228 rn ,\ni\nn\nDri11,...,i\n,...,rn Yt (\u03b8)\n\n=\n\nn\nX\n\n\u03b1iip ,i1 ...,\u0131\u030cp ,...,in (rp ; r1 , . . . , \u0159p , . . . , rn ; \u03b8)\n\np=1\n\n+\n\nZ\n\nt\n\nr1 \u2228***\u2228rn\n\n\u03b2ii1 ,...,in (s; r1 , . . . , rn ; \u03b8)\n\nds +\n\nd Z\nX\nl=1\n\nt\nr1 \u2228***\u2228rn\n\ni\n\u03b1l,i\n(s; r1 , . . . , rn ; \u03b8) dBsl , (14)\n1 ,...,in\n\nwhere\ni\n\u03b1j,i\n(s; r1 , . . . , rn ; \u03b8)\n1 ,...,in\n\n=\n\n\u03b2ii1 ,...,in (s; r1 , . . . , rn ; \u03b8) =\n\nX\n\nm\nX\n\nk1 ,...,k\u03bd =1\nm\nX X\n\ni(I )\n\ni(I )\n\n\u2202k\u03bd1 ...k\u03bd \u03c3 ij (Ys (\u03b8); \u03b8) Dr(I11 ) Ysk1 (\u03b8) . . . Dr(I\u03bd\u03bd ) Ysk\u03bd (\u03b8)\ni(I )\n\ni(I )\n\n\u2202k\u03bd1 ...k\u03bd \u03bci (Ys (\u03b8); \u03b8) Dr(I11 ) Ysk1 (\u03b8) . . . Dr(I\u03bd\u03bd ) Ysk\u03bd (\u03b8).\n\nk1 ,...,k\u03bd =1\n\nIn the expressions above, the first sums are extended to the set of all partitions I1 , . . . , I\u03bd of\ni(K)\n{1, . . . , n} and for any subset K = {i1 , . . . , i\u03b7 } of {1, . . . , n} we set Dr(K) for the derivative\ni ,...,i\n\ni\nn\noperator Dr11 ,...,r\u03b7\u03b7 . Notice that Dri11,...,i\n,...,rn Yt (\u03b8) = 0 whenever t < r1 \u2228 * * * \u2228 rn .\n\nThe formulas above might seem intricate. The following example illustrate their use in\na simple enough situation:\n\n\f12\n\nA. CHRONOPOULOU AND S. TINDEL\n\nExample 3.5. The second order derivative Dr1,3\nY 2 (\u03b8) can be computed as:\n1 ,r2 t\n2\n2\nDr1,3\nY 2 (\u03b8) = \u03b11,3\n(r1 , r2 ; \u03b8) + \u03b13,1\n(r2 , r1 ; \u03b8)\n1 ,r2 t\nZ t\nd Z\nX\n2\n+\n\u03b21,3 (s, r1 , r2 ; \u03b8) ds +\nr1 \u2228r2\n\nl=1\n\nt\n\nr1 \u2228r2\n\n2\n\u03b1l,1,3\n(s, r1 , r2 ; \u03b8)dBsl ,\n\nwhere\n2\n\u03b11,3\n(r1 , r2 ; \u03b8)\n\n=\n\n2\n\u03b13,1\n(r2 , r1 ; \u03b8) =\n\nm\nX\n\nk=1\nm\nX\n\n\u2202k \u03c3 21 (Yr2 (\u03b8); \u03b8) Dr32 Yrk1 (\u03b8),\n\u2202k \u03c3 23 (Yr1 (\u03b8); \u03b8) Dr11 Yrk2 (\u03b8)\n\nk=1\n\nand\n2\n2\n\u03b21,3\n(s, r1 , r2 ; \u03b8) = \u2202kk\n\u03bc2 (Ys (\u03b8); \u03b8)Dr1,3\nY k (\u03b8) + \u2202k21 k2 \u03bc2 (Ys (\u03b8); \u03b8)Dr11 Ysk1 (\u03b8)Dr32 Ysk2 (\u03b8),\n1 ,r2 s\n2\n2\n\u03b1l,1,3\n(s, r1 , r2 ; \u03b8) = \u2202kk\n\u03c3 2l (Ys (\u03b8); \u03b8)Dr1,3\nY k (\u03b8) + \u2202k21 k2 \u03c3 2l (Ys (\u03b8); \u03b8)Dr11 Ysk1 (\u03b8)Dr32 Ysk2 (\u03b8),\n1 ,r2 s\n\nwhere we have used the convention of summation over repeated indices.\nOur formula for the log-likelihood will also involve some derivatives of the process Y (\u03b8)\nwith respect to the parameter \u03b8. The existence of this derivative is assessed below:\nProposition 3.6. Under the same hypothesis as for Lemma 3.4, the random variable\nYti (\u03b8) is a smooth function of \u03b8 for any t \u2265 0. We denote by \u2207l Yti (\u03b8) the derivative of\nYti (\u03b8) with respect to the lth element of the vector of parameters \u03b8. This process satisfies\nthe following SDE:\n\u2207l Yti (\u03b8)\n\n=\n\nZ\n\nt\n0\n\n+\n\n[\u2202i \u03bci (Yu (\u03b8); \u03b8)\u2207l Yui (\u03b8) + \u2207l \u03bci (Yu (\u03b8); \u03b8)]du\nd Z\nX\nj=1\n\nt\n0\n\n[\u2202\u03c3 ij (Yu (\u03b8); \u03b8)\u2207l Yui (\u03b8) + \u2207l \u03c3 ij (Yu (\u03b8); \u03b8)]dBuj .\n\nProof. The proof goes exactly along the same lines as for [34, Proposition 4], and the\ndetails are left to the reader.\n\u0003\nWe shall also need some equations governing the Malliavin derivatives of \u2207l Y (\u03b8). This\nis the aim of the following lemma:\nLemma 3.7. For any l \u2208 {1, . . . , q} and n \u2265 1, the process \u2207l D i1 ,...,in Y (\u03b8) is n-times\ndifferentiable in the Malliavin calculus sense. Moreover, taking up the notations of Lemma 3.4, the process \u2207l D i1 ,...,in Yti (\u03b8) satisfies the following linear equation: for t \u2265 r1 \u2228\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n13\n\n* * * \u2228 rn ,\ni\nn\n\u2207l Dri11,...,i\n,...,rn Yt (\u03b8)\n\n=\n\nn\nX\n\n\u03b1\u0302ii,lp ,i1 ...,\u0131\u030cp ,...,n (rip , r1 , . . . , \u0159p , . . . , rn ; \u03b8)\n\np=1\n\n+\n\nZ\n\nt\nr1 \u2228***\u2228rn\n\n\u03b2\u0302ii,l1 ,...,in (s; r1 , . . . , rn ; \u03b8)\n\nds +\n\nd Z\nX\nl=1\n\nt\n\nr1 \u2228***\u2228rn\n\ni,l\n\u03b1\u0302l,i\n(s; r1 , . . . , rn ; \u03b8) dBsl ,\n1 ,...,in\n\ni,l\ni,l\ni,p\ni\nwhere \u03b1\u0302j,i\nand \u03b2\u0302j,i\n= \u2207l \u03b1j,i\n= \u2207l \u03b2ii1 ,...,in . More specifically, \u03b2\u0302j,i\nis\n1 ,...,in\n1 ,...,in\n1 ,...,in\n1 ,...,in\ndefined recursively by\n\n\u03b2\u0302ii,p\n(s; r1 , . . . , rn ; \u03b8)\n1 ,...,in\nm\nn\nX\nX\ni(I )\ni(I )\n\u2207p [\u2202k\u03bd1 ...k\u03bd \u03bci (Ys (\u03b8); \u03b8)] Dr(I11 ) Ysk1 (\u03b8) * * * Dr(I\u03bd\u03bd ) Ysk\u03bd (\u03b8)\n=\nI1 \u222a...\u222aI\u03bd k1 ,...,k\u03bd =1\n\n+\n\n\u03bd\nX\n\n\u2202k\u03bd1 ...k\u03bd \u03bci (Ys (\u03b8); \u03b8)\n\np=1\n\nwhere we have set\n\no\ni(I )\n\u0131\u030c(I )\ni(I )\ni(I )\n\u2207p Dr(Ipp ) Yskp (\u03b8) Dr(I11 ) Ysk1 (\u03b8) * * * D\u0159(Ipp ) Ys\u01e9p (\u03b8) * * * Dr(I\u03bd\u03bd ) Ysk\u03bd (\u03b8) ,\n\n\u2207p [\u2202k\u03bd1 ...k\u03bd \u03bci (Ys (\u03b8); \u03b8)] = \u2207p \u2202k\u03bd1 ...k\u03bd \u03bci (Ys (\u03b8); \u03b8) + \u2202\u2202k\u03bd1 ...k\u03bd \u03bci (Ys (\u03b8); \u03b8)\u2207p Ys (\u03b8).\nNotice that the same kind of equation (skipped here for sake of conciseness) holds true for\ni,l\nthe coefficients \u03b1\u0302j,i\n.\n1 ,...,in\nThe next object we need for our calculations is the inverse of the Malliavin matrix \u03b3Yt (\u03b8)\nof Yt (\u03b8). Recall that according to (11), the Malliavin matrix of Yt (\u03b8) is defined by\n\u0001\n(15)\n\u03b3t (\u03b8) := \u03b3Yt (\u03b8) = D* Yti (\u03b8), D* Ytj (\u03b8) 1\u2264i,j\u2264m ,\nwhere we have set \u03b3t (\u03b8) := \u03b3Yt (\u03b8) for notational sake in the computations below. We shall\nnow compute \u03b3t\u22121 (\u03b8) as the solution to a SDE:\n\nProposition 3.8. The matrix valued process \u03b3t\u22121 (\u03b8) is the unique solution to the following\nlinear equation in \u03b7:\nd Z t\nX\n\u22121\n[\u03b7u (\u03b8)\u03b1\u0303l (Yu (\u03b8); \u03b8) + \u03b1\u0303lT (Yu (\u03b8); \u03b8)\u03b7u ]dBul\n\u03b7t (\u03b8) = \u03b1\u03030 (Yt (\u03b8); \u03b8) \u2212\nl=1\n\n\u2212\nwith\n\u03b1\u03030 (Yt (\u03b8); \u03b8) =\n\nZ\n\nt\n\n[\u03b7u (\u03b8)\u03b2\u0303(Yu (\u03b8); \u03b8) + \u03b2\u0303 T (Yu (\u03b8); \u03b8)\u03b7u (\u03b8)]du,\n\n(16)\n\n0\n\nm Z tZ\nX\nj=1\n\n0\n\n0\n\nt\n\u2032\n\n\u03c3 ij (Yr (\u03b8); \u03b8)\u03c3 i j (Yr\u2032 (\u03b8); \u03b8) |r \u2212 r \u2032 |2H\u22122 dr dr \u2032 , i, i\u2032 = 1, . . . , m\n\n0\n\nand where the other coefficients \u03b1\u0303 and \u03b2\u0303 are defined by\n\u0010\n\u0011\n\u0011\n\u0010\n\u2032\n\u2032\n\u03b1\u0303l (Yu (\u03b8); \u03b8) = \u2202k \u03c3 i l (Yu (\u03b8); \u03b8)\nand \u03b2\u0303(Yu (\u03b8); \u03b8) = \u2202k \u03bci (Yu (\u03b8); \u03b8)\n1\u2264i\u2032 ,k\u2264m\n\n1\u2264i\u2032 ,k\u2264m\n\n.\n\n(17)\n\n\f14\n\nA. CHRONOPOULOU AND S. TINDEL\n\nProof. The proof of this fact is an adaptation of [19, Theorem 7] to the case of a SDE\nwith drift. We include it here for sake of completeness, and we drop the dependence of Y\non \u03b8 for notational sake in the computations below.\nLet us start by invoking Proposition 2.7 and equation (14) in order to compute the\nproduct of two first-order Malliavin derivatives:\nDrj Yti Drj\u2032 Yti = \u03c3 ij (Yr )\u03c3 i j (Yr\u2032 ) +\n(Z d \u0014\n\u0015\nm\ntX\nX\nj\nj\nil\ni\u2032\nj k\ni\u2032 l\nj i\nk\n\u2202k \u03c3 (Yu ) Dr\u2032 Yu Dr Yu + \u2202k \u03c3 (Yu ) Dr Yu Dr\u2032 Yu dBul\n+\n\u2032\n\n0\n\nk=1\n\n+\n\nZ t\u0014\n0\n\n\u2032\n\n(18)\n\nl=1\n\n\u2202k \u03bci (Yu ) Drj\u2032 Yui Drj Yuk + \u2202k \u03bci (Yu ) Drj Yui Drj\u2032 Yuk du\n\u2032\n\n\u2032\n\n\u0015)\n\n.\n\nMoreover, recall that \u03b3t is defined by (15). Thus, the covariance matrix becomes\n\u2032\n\u03b3tii\n\n=\n\nd D\nX\n\nD\n\nj\n\nYti ,\n\nD\n\nj\n\nj=1\n\n\u2032\nYti\n\nE\n\nH\n\n= cH\n\nd Z tZ\nX\nj=1\n\n0\n\nt\n\nDrj Yti (\u03b8) Drj\u2032 Yti (\u03b8) |r \u2212 r \u2032 |2H\u22122 dr dr \u2032.\n\u2032\n\n0\n\n\u2032\n\nPlugging (18) into this relation, we end up with the following equation for \u03b3 ii :\n\u2032\n\u03b3tii\n\n=\n\n\u2032\n\u03b1\u03030ii\n\n\u0015\nm \u0014\nd Z tX\nX\nil\ni\u2032 k\ni\u2032 l\nik\n\u2202k \u03c3 (Yu ) \u03b3u + \u2202k \u03c3 (Yu ) \u03b3u dBul\n+\nl=1\n\n0 k=1\n\n+\n\nZ tX\nm \u0014\n0 k=1\n\ni\n\n\u2202k \u03bc (Yu )\n\n\u2032\n\u03b3ui k\n\ni\u2032\n\n+ \u2202k \u03bc (Yu )\n\n\u03b3ui k\n\n\u0015\n\ndu.\n\nUsing our notation (17) and matrix product rules, we obtain that \u03b3t is solution to:\n\u03b3t =\n\nd Z\nX\nl=1\n\n0\n\nt\n\n(\u03b1\u0303l (Yu )\u03b3u +\n\n\u03b3u \u03b1\u0303lT (Yu ))dBul\n\n+\n\nZ\n\nt\n\n(\u03b2\u0303(Yu )\u03b3u + \u03b3u \u03b2\u0303 T (Yu ))du.\n\n0\n\nConsider now \u03b7 solution to (16). Applying again Proposition 2.7, it is readily checked\nthat \u03b3t \u03b7t = Id for any t \u2208 [0, T ], which ends the proof.\n\u0003\nRemark 3.9. Gathering equation (16) and Proposition 2.5, it is easily seen that for any\nt > 0 and \u03b8 \u2208 \u0398, Yt (\u03b8) is a non degenerate random variable in the sense given at [32,\nDefinition 2.1.2]: we have det(\u03b3t\u22121 ) \u2208 Lp (\u03a9) for any p > 1.\nNow that we have derived an equation for \u03b7 = \u03b3 \u22121 , an equation for the Malliavin\nderivative of \u03b7 is also available:\nProposition 3.10. For any l \u2208 {1, . . . , q} and n \u2265 1, the process \u03b7t = \u03b3t\u22121 is n-time\ndifferentiable in the Malliavin calculus sense. Moreover, the process D i1 ,...,in \u03b7t satisfies the\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n15\n\nfollowing equation: for t \u2265 r1 \u2228 * * * \u2228 rn ,\nn ij\nDri11,...,i\n,...,rn \u03b7t (\u03b8) = \u2212\n\nn X\nk1\nX\n\ni1 ,...,i\n\ni1 ,i2 ,...,i\n\ni1 ,...,i\n\nn\u2212k1\n\u22121 ij\n2\n(Dr1 ,...,rkk22 \u03b1\u03030\u22121 Dr1 ,r2 ,...,rkk11\u2212k\n\u2212k2 \u03b1\u03030 Dr1 ,...,rn\u2212k1 \u03b1\u03030 )\n\nk1 =1 k2 =1\n\n\u2212\n\nd Z t\nX\nl=1\n\nwhere\n\nr1 \u2228...\u2228rn\n\nij\n(s; r1 , . . . , rn ; \u03b8)dBsl\nCl,i\n1 ,...,in\n\nAij\ni1 ,...,in (s; r1 , . . . , rn ; \u03b8)\n\n=\n\n\u2212\n\nZ\n\nt\n\nr1 \u2228...\u2228rn\n\nAij\ni1 ,...,in (s; r1 , . . . , rn ; \u03b8)ds,\n\nm\nm\nX\nX X\nk1 ,...,k\u03bd k=1\n\nn\ni(I )\ni(I )\ni(I )\ni(I )\n[\u2202k\u03bd1 ,...,k\u03bd (\u03b2\u0303(Yu (\u03b8); \u03b8))kj Dr(I11 ) \u03b7sik (\u03b8) . . . Dr(I\u03bd\u03bd ) \u03b7sik (\u03b8) Dr(I11 ) Ysi (\u03b8) . . . Dr(I\u03bd\u03bd ) Ysi (\u03b8)]\n\no\ni(I )\ni(I )\ni(I )\ni(I )\n+[\u2202k\u03bd1 ,...,k\u03bd (\u03b2\u0303(Yu (\u03b8); \u03b8))ik Dr(I11 ) \u03b7skj (\u03b8) . . . Dr(I\u03bd\u03bd ) \u03b7skj (\u03b8) Dr(I11 ) Ysj (\u03b8) . . . Dr(I\u03bd\u03bd ) Ysj (\u03b8)]\n\nij\n(s; r1 , . . . , rn ; \u03b8), with the coefficients \u03b2\nand the same kind of equation holds for Cl,i\n1 ,...,in\nreplaced by \u03b1l .\n\nProof. The proof of this proposition is based on Lemma 3.4 and the fact that\ndA\u03bb \u22121\n\u2212A\u22121\n\u03bb d\u03bb A\u03bb .\n\ndA\u22121\n\u03bb\nd\u03bb\n\n=\n\u0003\n\nFinally, one can also differentiate \u03b7 with respect to our standing parameter \u03b8, which\nyields:\nLemma 3.11. The derivative of the inverse of the Malliavin matrix \u03b7t with respect to \u03b8\nsatisfies the following SDE\nd Z t\nX\n\u22121\n{\u2207l \u03b7u (\u03b8)\u03b1\u0303l (Yu (\u03b8); \u03b8) + \u03b7u (\u03b8)\u2207l [\u03b1\u0303l (Yu (\u03b8); \u03b8)]\n\u2207l \u03b7t (\u03b8) = \u2207l \u03b1\u03030 \u2212\nl=1\n\n+\u2207l [\u03b1\u0303lT (Yu (\u03b8); \u03b8)]\u03b7u (\u03b8)\n\n0\n\n+\n\n\u03b1\u0303lT (Yu (\u03b8); \u03b8)\u2207l \u03b7u (\u03b8)}dBul\n\n\u2212\n\nZ\n\nt\n\n{\u2207l \u03b7u (\u03b8)\u03b2\u0303(Yu (\u03b8); \u03b8)\n\n0\n\n+\u03b7u (\u03b8)\u2207l [\u03b2\u0303(Yu (\u03b8); \u03b8)] + \u2207l [\u03b2\u0303 T (Yu (\u03b8); \u03b8)]\u03b7u (\u03b8) + \u03b2\u0303 T (Yu (\u03b8); \u03b8)\u2207l \u03b7u (\u03b8)}du,\nwhere \u2207l [\u03b2\u0303l (Yu )] = \u2202 \u03b2\u0303l (Yu )\u2207l Yu + \u2207l \u03b2\u0303l (Yu ) and \u2207l [\u03b1\u0303l (Yu )] = \u2202 \u03b1\u0303l (Yu )\u2207l Yu + \u2207l \u03b1\u0303l (Yu ).\n3.2. Probabilistic representation of the likelihood. We have chosen to represent the\nlog-likelihood of our sample thanks to the following formula borrowed from the stochastic\nanalysis literature:\nProposition 3.12. Let F be a Rm -valued non degenerate random variable (see Remark 3.9 for references on this concept), and let f be the density of F . For n \u2265 1\nand (j1 , . . . , jn ) \u2208 {1, . . . , m}n , let H(j1 ,...,jn ) (F ) be defined recursively by H(j1 ) (F ) =\nPm\n\u22121 j1 j\nj\nj=1 \u03b4((\u03b3F ) DF ) and\nm\n\u0010\n\u0011\nX\n\u0001\n\u22121 jn j\nj\nH(j1 ,...,jn) (F ) =\n\u03b4 \u03b3F\nDF H(j1 ,...,jn\u22121 ) (F ) ,\n(19)\nj=1\n\n\f16\n\nA. CHRONOPOULOU AND S. TINDEL\n\nwhere the Skorohod operator \u03b4 is defined at Section 2.2.3. Then one can write\n\u0002\n\u0003\n\u0002\n\u0003\nf (x) = E 1(F >x) H(1,...,m) (F ) = E (F \u2212 x)+ H(1,...,m,1,...,m) (F ) ,\nQ\nQm\ni\nwhere 1(F >x) := m\ni=1 1(F i >xi ) and (F \u2212 x)+ :=\ni=1 (F \u2212 xi )+ .\n\n(20)\n\nProof. The first formula is a direct application of [32, Proposition 2.1.5]. The second one\nis obtained along the same lines, integrating by parts m additional times with respect to\nthe first one.\n\u0003\nThe formula above can obviously be applied to Yt (\u03b8) for any strictly positive t, since\nwe have noticed at Remark 3.9 that Yt (\u03b8) is a non-degenerate random variable. However,\nthe expression of H(j1 ,...,jn ) (Yt (\u03b8)) given by (19) is written in terms of Skorohod integrals,\nwhich are not amenable to numerical computations. We will thus recast this expression\nin terms of Young integrals plus some correction terms:\n\u22121 pj i j\nProposition 3.13. Under Hypothesis 2.3 and 3.1, let us define Qpji\nst := (\u03b3s ) Ds Yt (\u03b8)\nfor 0 \u2264 s < t \u2264 T , p, j \u2208 {1, . . . , m} and i \u2208 {1, . . . , d}. Consider p \u2208 {1, . . . , m} and a\nreal valued random variable G which is smooth in the Malliavin calculus sense. Set\nZ t\nm X\nd\nm X\nd Z tZ t\nX\nX\n\u0003\n\u0002\npji\n2H\u22122\ni\nUp (G) =\nG\ndrds, (21)\nQst dBs \u2212 cH\nDsi GQpji\nrt |r \u2212 s|\ni=1 j=1\n\n0\n\ni=1 j=1\n\n0\n\n0\n\nwhere the integral with respect to B is understood in the Young sense. Then the quantities\nH(j1 ,...,jn) (Yt (\u03b8)) defined at Proposition 3.12 can be expressed as\nH(j1 ,...,jn ) (Yt (\u03b8)) =\n\nm\nX\nj=1\n\n\u0001\nUjn \u25e6 * * * \u25e6 Uj1 Ytj (\u03b8) .\n\n(22)\n\nProof. It is an immediate consequence of Proposition 2.9, since we have noticed in our\nRemark 3.9 that Yt (\u03b8) is a non-degenerate random variable.\n\u0003\nThe previous proposition is still not sufficient to warranty an effective computation\nof the log-likelihood. Indeed, the right hand side of (21) contains terms of the form\nDs [GQpji\nrt ], which should be given in a more explicit form. This is the content of our next\nproposition.\nProposition 3.14. Set H(j1 ,...,jn ) (Yt (\u03b8)) := Kj1 ...jn . Then the term Ds [Kj1 ...jn Qpji\nrt ] in (21)\ncan be computed inductively as follows:\npji\npji\npji\n(i) We have Ds [Kj1 ...jn Qpji\nrt ] = Ds Kj1 ...jn Qrt + Kj1 ...jn Ds Qrt , and Ds Qrt is computed by\ninvoking Proposition 3.10 for the derivative of \u03b3t\u22121 and Lemma 3.4 for the derivative of\nYt (\u03b8). We are thus left with the computation of Ds Kj1 ...jn .\n\n(ii) Assume now that we can compute n \u2212 r Malliavin derivatives of Kj1 ...jr . Notice that\nthis condition is met for r = 0, since Yt (\u03b8) itself can be differentiated n times in an\nexplicit way according to Lemma 3.4 again. Then for any j1 , . . . , jr+1 and k \u2264 n \u2212 r \u2212 1,\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n17\n\nthe quantity Kj1 ...jr+1 can be differentiated k times, with a Malliavin derivative given by\nk\nD\u03c1i11,...,i\n...\u03c1k Kj1 ...jr+1\n\nk\nX\n\n=\n\n\u01d0l ...,ik\nD\u03c1i11,...,\n...\u03c1\u030cl ...\u03c1k (Kj1 ...jr\n\nQpji\n\u03c1l t )\n\n+\n\nj=1\n\nl=1\n\n\u2212cH\n\nd Z\nX\n\nZ tZ\n0\n\nt\n0\n\n0\n\nt\n\npji\nj\nk\nD\u03c1i11,...,i\n...\u03c1k (Kj1 ...jr Qst )dBs\n\n2H\u22122\ndr1 dr2 .\nDrk+1\n(Kj1 ...jr Qpji\nr2 t )|r1 \u2212 r2 |\n1 \u03c11 ...\u03c1k\n\n(23)\n\nProof. We focus on the induction step (ii), the other one being straightforward: for a\nsmooth random variable W , one easily gets by induction that\np\nDri11,...,i\n...rp \u03b4(W )\n\n=\n\np\nX\n\ni ,...,\u01d0 ...,i\n\np\nDr11 ...\u0159ll...rp p Wrl + \u03b4(Dri11,...,i\n...rp W ).\n\n(24)\n\nl=1\n\nSuppose we know the n \u2212 r Malliavin derivatives for Ujr \u25e6 * * * \u25e6 Uj1 (F ) := Kj1 ...jr . Recall\nmoreover that\nKj1 ...jr+1 = Ujr+1 (Kjr ...j1 ) = \u03b4(Kjr ...j1 Q*t )\nApplying directly relation (24) we thus get, for k \u2264 m \u2212 1:\nk\nD\u03c1i11,...,i\n...\u03c1k \u03b4(Kjr ...j1\n\nQ*t ) =\n\nk\nX\n\ni ,...,\u01d0 ,...,i\n\nk\nD\u03c111 ...\u03c1\u030cll...\u03c1k k\u22121 (Kjr ...j1 Q\u03c1l t ) + \u03b4(D\u03c1i11,...,i\n...\u03c1k (Kjr ...j1 Q*t )).\n\nl=1\n\nOur formula (23) is now obtained by applying Proposition 2.9 to the Skorohod integral\n\u03b4(D\u03c1k1 ...\u03c1k (Kjr ...j1 Q*t )) above.\n\u0003\nExample 3.15. As an illustration of the proposition above, we compute U2 \u25e6 U1 (F ) for\nF = Yti , i \u2208 {1, . . . , m} and our d-dimensional fBm B.\nWrite first U1 (Yti ) = \u03b4(Yti (\u03b3 \u22121 )1j1 D j1 Yti ), and since this quantity has to be expressed\nin a suitable way for numerical approximations, we have\nU1 (Yti )\n\n=\n\nd\nX\n\nj1 =1\n\nYti\n\nZ\n\nt\n0\n\n1ij1\nQut\ndBuj1\n\n\u2212 cH\n\nd Z tZ\nX\n\nj1 =1\n\n0\n\nt\n\n0\n\nDuj11 [Yti Qu1ij2 t1 ]|u1 \u2212 u2 |2H\u22122 du1 du2 ,\n\nwhere Q is defined at Proposition 3.13 and where the first integral in the right hand side is\nunderstood in the Young sense. In order to compute the second one, we have to compute\nMalliavin derivatives. This is done through Lemma 3.4 for Y and Proposition 3.10 for Q.\nWe now have to differentiate U1 (Yti ): the derivation rules for Skorohod integrals immediately yield\nDuj22 [U1 (Yti )]\n\n=\n\nd\nX\n\nj2 =1\n\nYti Qu2ij2 t2\n\n+\n\nd\nX\n\nj2 =1\n\n\u03b4(Duj22 Yti Q*t2ij2 ).\n\n\f18\n\nA. CHRONOPOULOU AND S. TINDEL\n\nOnce again, the Skorohod integral above is not suitable for numerical approximations.\nWrite thus\nDuj22 [U1 (Yti )]\n\n=\n\nd\nX\n\nYti Qu2ij2 t2\n\n+\n\nj2 =1\n\nd Z\nX\n\nj2 =1\n\nt\n0\n\n2ij2\nDuj22 [Yti Qrt\n]dBrj2\nd Z tZ\nX\n\n\u2212 cH\n\n0\n\nj2 =1\n\nt\n0\n\nDuj22 Duj11 [Yti Qu2ij2 t2 ]|u2 \u2212 u1 |2H\u22122 du1du2 ,\n\nand compute the Malliavin derivatives of the products Y Q thanks to Lemma 3.4 for Y\nand Proposition 3.10 for Q. Once this is done, just write\nU2 (U1 (Yti )) = \u03b4(U1 (Yti )Qi*t )\nZ t\nd\nd Z tZ t\nX\nX\n2ij2\ni\nj2\n=\nU1 (Yt )\nQut dBu \u2212 cH\nDuj22 [U1 (Yti )Qu2ij1 t2 ]|u2 \u2212 u1 |2H\u22122 du1 du2 .\n0\n\nj2 =1\n\nj2 =1\n\n0\n\n0\n\nIn order to give our formula for the derivative of the log-likelihood, we still need to\ncompute the derivative with respect to \u03b8 of H(j1 ,...,jn) (Yt (\u03b8)). For this we state the following\nlemma\nLemma 3.16. The derivative with respect to \u03b8 of Up (Yt (\u03b8)) can be written as\n\u2207l Up (Yti (\u03b8))\n\n=\n\nd\nX\n\nZ\n\nt\n\nd Z tZ\nX\n\nt\n\n[\u2207l Yti (\u03b8)\n\nj=1\n\n\u2212cH\n\nj=1\n\n0\n\n0\n\n0\n\nj\nQpij\nst (\u03b8) dBs\n\n+\n\nYti (\u03b8)\n\nZ\n\nt\n\n0\n\nj\n\u2207l [Qpij\nst (\u03b8)] dBs ]\n\n2H\u22122\n\u2207l [Dsj Yti (\u03b8)Qpij\ndrds,\nrt (\u03b8)]|r \u2212 s|\n\nwhere \u2207l Yti (\u03b8) is computed according to Proposition 3.6 and \u2207l [Dsj Yti ] is given by Lemma 3.7. As far as \u2207l [Qpj\nst (\u03b8)] is concerned, it is obtained through the following equation:\nj\nj\npj\npj\n\u2207l [Qpj\nst (\u03b8)] = \u2207l \u03b7s (\u03b8) Ds Yt (\u03b8) + \u03b7s (\u03b8)\u2207l [Ds Yt (\u03b8)],\n\nwhere the expression for \u2207l \u03b7spj (\u03b8) is a consequence of Lemma 3.11.\nWe are now ready to state our probabilistic expression for the log-likelihood function (5).\nTheorem 3.17. Assume Hypothesis 2.3 and 3.1 hold true. Let yti , i = 1, . . . , n be the\nobservation arriving at time ti . Let also Yti be the solution to the SDE (1) at time ti . Then,\nthe gradient of the log-likelihood function admits the following probabilistic representation:\nP\nVi (\u03b8)\n, with\n\u2207l ln (\u03b8) = ni=1 W\ni (\u03b8)\n\u0014\n\u0010\n\u0011\u0015\nWi (\u03b8) = E 1(Yti (\u03b8)>yti ) H(1,...,m) Yti (\u03b8)\n\n(25)\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n19\n\nand\n\u0014\n\u0010\n\u0011\nVi (\u03b8) = E \u2207l Yti (\u03b8) 1(Yti (\u03b8)>yti ) H(1,...,m,1,...,m) Yti (\u03b8)\n\u0010\n\n+ Yti (\u03b8) \u2212 yti\n\n\u0011\n\n+\n\n\u0010\n\n\u2207l H(1,...,m,1,...,m) Yti (\u03b8)\n\n\u0011\u0015\n\n, (26)\n\nwhere (i) H(j1 ,...,jn ) (Yti (\u03b8)) is given recursively by (22) and computed at Proposition 3.14\n(ii) \u2207l Yti (\u03b8) is given by Proposition 3.6 (iii) \u2207l H(1,...,m,1,...,m) is obtained by applying Lemma 3.16.\nProof. Recall that under Hypothesis 2.3 and 3.1, Yt (\u03b8) admits a C \u221eP\ndensity f (t, *; \u03b8) for\nany t > 0 and \u03b8 \u2208 \u0398. Moreover, we have defined ln (\u03b8) as ln (\u03b8) = ni=1 ln(f (ti , yti ; \u03b8)).\nThus\nn\nn\nX\nX\n\u2207l f (ti , yti ; \u03b8)\nVi (\u03b8)\n\u2207l ln (\u03b8) =\n:=\n.\nf (ti , yti ; \u03b8)\nWi (\u03b8)\ni=1\ni=1\n\nNow Wi (\u03b8) can be expressed like (25) by a direct application of (20), first relation. As far\nas Vi (\u03b8) is concerned, write\n\u0002\n\u0003\nf (ti , yti ; \u03b8) = E (Yti (\u03b8) \u2212 yti )+ H(1,...,m,1,...,m) (Yti (\u03b8)) ,\naccording to the second relation in (20). By using standard arguments, one is allowed to\ndifferentiate this expression within the expectation, which directly yields (26).\n\u0003\n4. Discretization of the log-likelihood\nThe expression of the log-likelihood that we derived in Proposition 3.17 is a fraction of\ntwo expectations that do not have explicit formulas even in the one-dimensional case. In\naddition, our goal is to find the root of this non-explicit expression, the ML estimator,\nwhich is an even harder task. To solve this problem in practice we first use a stochastic\napproximation algorithm in order to find the root of \u2207l ln (\u03b8). In each iteration of the\nalgorithm we compute the value of the expression using Monte-Carlo (MC) simulations.\nFor each Monte-Carlo simulation, since we do not have available an exact way of simulating\nthe kernels of the expectation, we use an Euler approximation scheme. More specifically,\nwe simulate using Euler approximation terms such as Yt , DYt, which are solutions to\nfractional stochastic differential equations.\nTherefore, in our approach we have three types of error in the computation of the\nMLE: the error of the stochastic approximation algorithm, the Monte-Carlo error and the\ndiscretization bias introduced by the Euler approximation for the stochastic differential\nequations. Our aim here is to combine the Monte Carlo and Euler approximations in an\noptimal way in order to get a global error bound for the computation of \u2207l ln (\u03b8).\n4.1. Pathwise convergence of the Euler scheme. The Euler scheme is the main\nsource of error in our computations. There is always a trade-off between the number of\nEuler steps and the number of simulations, but what is usually computationally costly is\nthe number of Euler steps. This is even worse when we deal with fractional SDEs, since\nthe rate of convergence depends on H and the closer the value of H to 1/2, the more\nsteps are required for the simulation.\n\n\f20\n\nA. CHRONOPOULOU AND S. TINDEL\n\nIn this section, we compute the magnitude of the discretization error we introduce. We\nmeasure the bias of the Euler scheme via the root mean square error. That is, we want to\nestimate the quantity sup\u03c4 \u2208[0,T ] (E|Y\u03c4 (\u03b8) \u2212 \u0232\u03c4M (\u03b8)|2 )1/2 , where Yt (\u03b8) is the solution to the\nSDE (1) and \u0232\u03c4M (\u03b8) is the Euler approximation of Y\u03c4 (\u03b8) given on the grid {\u03c4k ; k \u2264 M}\nby\n\u0232\u03c4M\n(\u03b8)\nk+1\n\n=\n\n\u0232\u03c4M\n(\u03b8)\nk\n\n+\n\n\u03bc(\u0232\u03c4M\n(\u03b8); \u03b8)(\u03c4k+1\nk\n\n\u2212 \u03c4k ) +\n\nd\nX\n\n\u03c3 j (\u0232\u03c4M\n(\u03b8); \u03b8)\u03b4B\u03c4M,j\n,\nk\nk \u03c4k+1\n\n(27)\n\nj=1\n\nin which we denote \u03b4B\u03c4M,j\n= B\u03c4M,j\n\u2212 B\u03c4M,j\nand \u03c4k = kT\nfor k = 0, . . . , M \u2212 1. Notice\nM\nk \u03c4k+1\nk+1\nk\nthat those estimates can be found in [8, 11, 30]. We include their proof here because it\nis simple enough, and also because they can be easily generalized to the case of a linear\nequation. This latter case is of special interest for us, since it corresponds to Malliavin\nderivatives, and is not included in the aforementioned references.\nNotation 4.1. For simplicity, in this section we write Y := Y (\u03b8).\nProposition 4.2. Let T > 0 and recall that Y \u0304M is defined by equation (27). Then, there\nexists a random variable C with finite Lp moments such that for all \u03b3 < H and H > 1/2\nwe have\nkYt \u2212 \u0232 k\u03b3,T \u2264 CT M 1\u22122\u03b3\n\n(28)\n\nConsequently, we obtain that the MSE is of order O(M 1\u22122\u03b3 ).\nProof. In order to prove (28) we apply techniques of the classical numerical analysis for\nthe flow of an ordinary differential equation driven by a smooth path. Namely, the exact\nflow of (1) is given by \u03a6(y; s, t) := Yt , where Yt is the unique solution of (1) when t \u2208 [s, T ]\nand the initial condition is Ys = y. Introduce also the numerical flow\n\u03a8(y; \u03c4k , \u03c4k+1 ) := y + \u03bc(y)(\u03c4k+1 \u2212 \u03c4k ) +\n\nd\nX\n\n\u03c3 j (y)\u03b4B\u03c4M,j\n,\nk \u03c4k+1\n\n(29)\n\nj=1\n\nwhere \u03c4k =\n\nkT\n,\nM\n\nk = 0, . . . , M \u2212 1. Thus, we can write that\n\u0010\n\u0011\nM\nM\n\u0232\u03c4k+1 = \u03a8 \u0232\u03c4k ; \u03c4k , \u03c4k+1 , k = 0, . . . , M \u2212 1\nY0M = \u03b1.\n\nFor q > k we also have that\n\u03a8(y; \u03c4k , \u03c4q ) := \u03a8(*; \u03c4q\u22121 , \u03c4q ) \u25e6 \u03a8(*; \u03c4q\u22122 , \u03c4q\u22121 ) \u25e6 . . . \u25e6 \u03a8(y; \u03c4k , \u03c4k+1 ).\nThe one-step error computes as\nrk = \u03a6(y; \u03c4k , \u03c4k+1 ) \u2212 \u03a8(y; \u03c4k , \u03c4k+1)\nZ \u03c4k+1 h\nZ \u03c4k+1 h\ni\ni\n\u03c3(Ys ) \u2212 \u03c3(y) dBs\n\u03bc(Ys ) \u2212 \u03bc(y) ds +\n=\n\u03c4k\n\n\u03c4k\n\n(30)\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\nFurthermore, since Y \u2208 C \u03b3 and B \u2208 C \u03b3 for \u03b3 > 1/2, using (7) we have\nZ \u03c4k+1 h\n2\u03b3\ni\nT\n\u03c3(Ys ) \u2212 \u03c3(y) dBs \u2264 c\u03b3 k\u2202\u03c3k\u221e kY k\u03b3 kBk\u03b3\nM\n\u03c4k\n\n2\u03b3\n\nT\nM\n\n\u2264 c\u03b3,\u03c3 k\u2202\u03c3k\u221e kBk1/\u03b3\nkBk\u03b3\n\u03b3\n\n21\n\n,\n\n1/\u03b3\n\nwhere we used the fact that kY k\u03b3 \u2264 c\u03c3 kBk\u03b3 (see Proposition 2.5). Similarly, for the\ndrift part we have\nZ \u03c4k+1 h\n\u03b3+1\ni\nT\n\u03bc(Ys ) \u2212 \u03bc(y) ds \u2264 c\u03b3 k\u2202\u03bck\u221e kY k\u03b3\nM\n\u03c4k\n\u2264\n\nT\nM\n\nc\u03b3,\u03bc k\u2202\u03bck\u221e kBk1/\u03b3\n\u03b3\n\n\u03b3+1\n\n.\n\nTherefore, the one-step error (30) satisfies\n|rk | \u2264 c\u03bc,\u03c3 kBk1+1/\u03b3\n\u03b3\n\nT\nM\n\n2\u03b3\n\n(31)\n\n.\n\nNow, we can write the classical decomposition of the error in terms of the exact and\nnumerical flow. Since \u0232\u03c4M\n= \u03a6(\u0232\u03c4M\n; \u03c4k , \u03c4k ) and Y\u03c4k = \u03a6(\u0232\u03c4M\n; \u03c40 , \u03c4k ) we have\n0\nk\nk\n\u0232\u03c4M\nq\n\nq\u22121 \u0010\n\u0011\nX\nM\n\u03a6(\u0232\u03c4k ; \u03c4k , \u03c4q ) \u2212 \u03a6(\u0232\u03c4k+1 ; \u03c4k+1 , \u03c4q ) . (32)\n\u2212 Y\u03c4q = \u03a6(\u0232\u03c40 ; \u03c40 , \u03c4k ) \u2212 \u03a6(\u0232\u03c4q ; \u03c4q , \u03c4q ) =\nk=0\n\n\u0010\n\u0011\n\u0010\n\u0011\nM\nSince \u03a6 \u0232\u03c4M\n;\n\u03c4\n,\n\u03c4\n=\n\u03a6\n\u03a6(\n\u0232\n;\n\u03c4\n,\n\u03c4\n);\n\u03c4\n,\n\u03c4\nwe obtain\nk\nq\nk\nk+1\nk+1\nq\n\u03c4k\nk\n\u03a6(\u0232\u03c4M\n; \u03c4k , \u03c4q ) \u2212 \u03a6(\u0232\u03c4M\n; \u03c4k+1 , \u03c4q )\nk\nk+1\n\n=\n\n\u0010\n\u0011\nM\n\u03a6 \u03a6(\u0232\u03c4M\n;\n\u03c4\n,\n\u03c4\n);\n\u03c4\n,\n\u03c4\nk q\nk+1 q \u2212 \u03a6(\u0232\u03c4k+1 ; \u03c4k+1 , \u03c4q )\nk\n\n\u2264 CT (kBk\u03b3 ) |\u03a6(\u0232\u03c4M\n; \u03c4k , \u03c4k+1 ) \u2212 \u0232\u03c4M\n|,\nk\nk+1\nwhere we have used the fact that\n|\u03a6(\u03b1; t, s) \u2212 \u03a6(\u03b2; t, s) \u2264 CT (kBk\u03b3 )|\u03b1 \u2212 \u03b2|,\n\nwhere CT is a subexponential function (see Proposition 2.5 again). Moreover, owing to\nrelation (31),\n|\u03a6(\u0232\u03c4M\n; \u03c4k , \u03c4q )\nk\n\n\u2212\n\n\u0232\u03c4M\n|\nk+1\n\n= |rk | \u2264\n\nc\u03bc,\u03c3 kBk1+1/\u03b3\n\u03b3\n\nT\nM\n\nTherefore, replacing (33) in (32) for any q \u2264 n we obtain\n|\u0232\u03c4M\nq\n\n\u2212 Y\u03c4q | \u2264\n\nc\u03bc,\u03c3 kBk1+1/\u03b3\n\u03b3\n\nq\u22121\nX\nT\nM\nk=0\n\n2\u03b3\n\n2\u03b3\n\n.\n\n(33)\n\n\f22\n\nA. CHRONOPOULOU AND S. TINDEL\n\nLet us push forward this analysis to H\u00f6lder type norms on the grid 0 \u2264 \u03c41 < . . . < \u03c4n = T .\nWe have for q \u2265 p\n\u0011\n\u0010\n\u03b4 Y \u2212 \u0232 M\n\u03c4p \u03c4q\n\u0011\n\u0011 \u0010\n\u0010\nn\n;\n\u03c4\n,\n\u03c4\n)\n\u2212\n\u0232\n= \u03a6(Y\u03c4p ; \u03c4p , \u03c4q ) \u2212 Y\u03c4p \u2212 \u03a8(\u0232\u03c4M\np q\n\u03c4p\np\n\u0011\n\u0011 \u0010\n\u0011 \u0010\n\u0010\nM\nM\nM\n;\n\u03c4\n,\n\u03c4\n)\n;\n\u03c4\n,\n\u03c4\n)\n\u2212\n\u03a6(\n\u0232\n\u2212\n\u03a8(\n\u0232\n;\n\u03c4\n,\n\u03c4\n)\n\u2212\n\u0232\n= \u03a6(Y\u03c4p ; \u03c4p , \u03c4q ) \u2212 Y\u03c4p \u2212 \u03a6(\u0232\u03c4M\np\nq\np\nq\np\nq\n\u03c4p\n\u03c4p\n\u03c4p\np\n\u0012\u0010\n\u0011\u0013 \u0010\n\u0011\n\u0011 \u0010\nM\nM\nM\n\u2212\n\u03a8(\n\u0232\n;\n\u03c4\n,\n\u03c4\n)\n\u2212\n\u03a6(\n\u0232\n;\n\u03c4\n,\n\u03c4\n)\n.\n\u2212\n\u0232\n=\n\u03a6(Y\u03c4p ; \u03c4p , \u03c4q ) \u2212 \u03a6(\u0232\u03c4M\n;\n\u03c4\n,\n\u03c4\n)\n\u2212\nY\np q\np q\np q\n\u03c4p\n\u03c4p\n\u03c4p\n\u03c4p\np\nSimilar to the calculations leading to (33) we obtain\n; \u03c4p , \u03c4q )\n\u03a8(\u0232\u03c4M\np\n\n\u2212\n\n; \u03c4p , \u03c4q )\n\u03a6(\u0232\u03c4M\np\n\n\u2264\n\nc\u03bc,\u03c3 kBk1+1/\u03b3\n\u03b3\n\nq\u22121\nX\nT\nM\nk=p\n\n2\u03b3\n\n.\n\nMoreover, owing to Proposition 2.5 part (2), observe that\n\u0011\n\u0011 \u0010\n\u0010\nM\n\u2212\n\u0232\n;\n\u03c4\n,\n\u03c4\n)\n\u2212\nY\n\u03a6(Y\u03c4p ; \u03c4p , \u03c4q ) \u2212 \u03a6(\u0232\u03c4M\np\nq\n\u03c4\np\n\u03c4p\np\n\u2264 c(kBk\u03b3 ) |Y\u03c4p \u2212 \u0232\u03c4M\n|.\np\n\u03b3\n|\u03c4q \u2212 \u03c4p |\nConsequently, we have that for 0 \u2264 p < q \u2264 M\n\u0010\n\u0011\n\u03b4 Y \u2212 \u0232 M\n\n\u03c4p \u03c4q\n\n\u2264c\n\n\u2032\n\n(kBk1+1/\u03b3\n)\n\u03b3\n\nq\u22121\nnX\nT\nM\nk=p\n\n2\u03b3\n\nq\nX\nT\n+ |\u03c4q \u2212 \u03c4p |\nM\nk=0\n\u03b3\n\n2\u03b3 o\n\nwhich easily yields that\n\u0010\n\u0011\n\u03b4 Y \u2212 \u0232 M\n\n\u03c4p \u03c4q\n\nsup\np,q=0,1,...,M \u22121,p6=q\n\n|\u03c4p \u2212 \u03c4q\n\n\u2264 c(kBk\u03b3 ) M 1\u22122 \u03b3 .\n\n|\u03b3\n\nBy \"lifting\" this error estimate to [0, T ] and since |t \u2212 s| \u2264 T /M,\nkYt \u2212 \u0232 k\u03b3,\u221e,T \u2264 C M 1\u22122 \u03b3 ,\n\n(34)\n\nwhich concludes the first part of the proof.\nRegarding the order of the Mean Square Error, it suffices to note that the constant C\nhas finite Lp moments.\n\u0003\nAs mentioned before, an elaboration of Proposition 4.2 is needed in the sequel. Indeed,\nin the expression of the log-likelihood in Proposition 3.17 we need to discretize more\ncomplicated quantities of the underlying process, such as (14) or (16). To this aim, let us\nnotice first that all those equations can be written under the following generic form:\nZ t\nZ t\n2\nZt = \u03b1 +\n\u03beu Zu du +\n\u03beu1,j Zt dBuj ,\n(35)\n0\n\n0\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n23\n\nwhere \u03be 1 , \u03be 2 are stochastic processes with bounded moments of any order. The corresponding Euler discretization is\nZ\u0304\u03c4Mk = Z\u0304\u03c4Mk + \u03be\u03c42k Z\u0304\u03c4Mk (\u03c4k+1 \u2212 \u03c4k ) +\n\nd\nX\n\n\u03be\u03c41,j\nZ\u0304\u03c4k \u03b4B\u03c4j,M\n,\nk\nk \u03c4k+1\n\n(36)\n\nj=1\n\nand we give first an approximation result in this general context:\nProposition 4.3. Let T > 0, and consider the Rq -valued solution Z to equation (35),\nwhere \u03b1 \u2208 Rq , \u03be 2 , \u03be 1,j \u2208 Rq,q and we suppose that k\u03be 2 k\u03b3 and k\u03be 1,j k\u03b3 belong to Lp (\u03a9) for\nany value of p \u2265 1. Let Z\u0304 M be defined by equation (36). Then, there exists a random\n\u2032\nvariable C with Lp finite moments, such that for all \u03b3 < H and H > 1/2 we have\n\u2032\n\nkZ \u2212 Z\u0304k\u03b3,T \u2264 CT M 1\u22122\u03b3\n\n(37)\n\nConsequently, we obtain that the Mean Square Error is of order O(M 1\u22122\u03b3 ).\nProof. We follow a similar approach as in the previous proposition. Thus, the exact flow\nis equal to \u03a6(\u03b6; s, t) := Zt , where Zt is the unique solution of equation (35) when t \u2208 [s, T ]\nand the initial condition is Zs = \u03b6. Consider also the numerical flow\n\u03a8(\u03b6; \u03c4k , \u03c4k+1 ) := \u03b6 +\n\n\u03beu2 \u03b6(\u03c4k+1\n\n\u2212 \u03c4k ) +\n\nd\nX\n\n\u03beu1,j \u03b6\u03b4B\u03c4j,M\n,\nk \u03c4k+1\n\nj=1\n\nwhere \u03c4k = kT /M, n = 0, . . . , M \u2212 1. Thus, we have\nZ\u0304\u03c4Mk+1 = \u03a8(Z\u0304\u03c4Mk ; \u03c4k+1, \u03c4k ), k = 0, . . . , M \u2212 1\nZ\u03040M = \u03b1.\nIn this case, the one-step error can be written as\nrk = \u03a6(\u03b6; \u03c4k , \u03c4k+1 ) \u2212 \u03a8(\u03b6; \u03c4k , \u03c4k+1 )\nZ \u03c4k+1\nZ \u03c4k+1\n2\n=\n\u03beu (Zs \u2212 \u03b6)du +\n\u03beu1 (Zs \u2212 \u03b6)dBu\n\u03c4k\n\n\u03c4k\n\n1/\u03b3\n\nWe now treat each term separately. Therefore, using the fact that kZk\u03b3 \u2264 exp(ckBk\u03b3 ),\nwhich is recalled at Proposition 2.5 point (4) in a slightly different context, we have that\nZ \u03c4k+1\n2\u03b3\nT\n1\n1\n\u03bes (Zs \u2212 \u03b6)dBs \u2264 c\u03b3 kZ\u03be k\u03b3 kBk\u03b3\nM\n\u03c4k\n\u2264\nSimilarly, we also have\nZ \u03c4k+1\n\u03c4k\n\n\u03bes2 (Zs \u2212 \u03b6)ds\n\nc\u03b3 k exp(kBk1/\u03b3\n\u03b3 )\n\n\u2264 c\u03b3 kZ\u03be 2 k\u03b3 kBk\u03b3\n\nkBk\u03b3\n\nT\nM\n\nT\nM\n\n2\u03b3\n\n.\n\n2\u03b3\n\n\u2264 c\u03b3 k exp(kBk1/\u03b3\n\u03b3 ) kBk\u03b3\n\nT\nM\n\n2\u03b3\n\n.\n\n\f24\n\nA. CHRONOPOULOU AND S. TINDEL\n\nTherefore, the one-step error satisfies the following inequality\n|rk | \u2264\n\nc\u03b3 exp(kBk1/\u03b3\n\u03b3 )\n\nkBk\u03b3\n\nT\nM\n\n2\u03b3\n\n.\n\nAlong the same lines as for Proposition 4.2, the decomposition of the error in terms of\nthe exact and numerical flow becomes\nq\u22121 \u0010\n\u0011\nX\nM\nM\nM\nM\nM\n\u03a6(Z\u0304\u03c4k+1 ; \u03c4k+1 , \u03c4q ) \u2212 \u03a6(Z\u0304\u03c4k ; \u03c4k , \u03c4q ) ,\nZ\u0304\u03c4q \u2212 Z\u03c4q = \u03a6(Z\u0304\u03c4q ; \u03c4q , \u03c4q ) \u2212 \u03a6(Z\u0304\u03c40 ; \u03c40 , \u03c4k ) =\nk=0\n\nand the same inequalities allowing to go from (32) to (33) yield\n|Z\u0304\u03c4q \u2212 Z\u03c4q | \u2264 c\u03b3\n\nT\nM\n\n2\u03b3\n\n.\n\nThe claim of the proposition follows now as in Proposition 4.2.\n\u0003\nWe now use the previous proposition in order to approximate the kernels of the expectations in \u2207l ln (\u03b8). Let us first introduce the following notation:\nNotation 4.4. Let Wi (\u03b8), Vi (\u03b8) as in (25) and (26) respectively and define wi (\u03b8) and\nvi (\u03b8) as\n\u0010\n\u0011\nwi (\u03b8) = 1(Yti (\u03b8)>yti ) H(1,...,m) Yti (\u03b8)\n(38)\n\u0010\n\u0011\nvi (\u03b8) = \u2207l Yti (\u03b8) 1(Yti (\u03b8)>yti ) H(1,.,m,1,.,m) + Yti (\u03b8) \u2212 yti \u2207l H(1,.,m,1,.,m) . (39)\n+\n\nLet also w\u0304iM and v\u0304iM to be the Euler discretized versions of (38) and (39) using 4.3, and\nset W\u0304iM (\u03b8) = E[w\u0304iM ] and V\u0304iM (\u03b8) = E[v\u0304iM ].\nOur convergence result for \u2207l ln (\u03b8) can be read as follows:\nTheorem 4.5. Recall from Theorem 3.17 that \u2207l ln (\u03b8) can be decomposed as \u2207l ln (\u03b8) =\nPn Vi (\u03b8)\ni=1 Wi (\u03b8) . Then the following approximation result holds true:\nVi (\u03b8) \u2212 V\u0304iM (\u03b8) + Wi (\u03b8) \u2212 W\u0304iM (\u03b8) \u2264\n\nc\n\nM 2\u03b3\u22121\n\n,\n\nfor a strictly positive constant c.\nProof. We focus on the bound for |Vi (\u03b8) \u2212 V\u0304iM (\u03b8)|, the other one being very similar.\nNow, applying Proposition 4.3 to the particular case of the equations governing Malliavin\nderivatives, we easily get\nkvt \u2212 v\u0304k\u03b3,T \u2264 C2 M 1\u22122\u03b3 ,\nfor an integrable random variable C2 . The proof is now easily finished by invoking the\ninequality\nVi (\u03b8) \u2212 V\u0304iM (\u03b8) \u2264 E [kvt \u2212 v\u0304k\u03b3,T ] .\n\u0003\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n25\n\nRemark 4.6. We have given two separate approximations for Vi (\u03b8) and Wi (\u03b8). In order\nto fully estimate (Vi (\u03b8)/Wi (\u03b8)) \u2212 (V\u0304iM (\u03b8)/W\u0304iM (\u03b8)), one should also prove that Wi (\u03b8) is\nbounded away from 0. This requires a lower bound for densities of differential equations\ndriven by fractional Broawnian motion, which are out of the scope of the current article.\n4.2. Efficiency of the Monte Carlo simulation. In this section we aim to study the\ncomputational tradeoff between the length of a time period in the Euler discretization\n(i.e. 1/M) and the number of Monte Carlo simulations of the sample path (i.e. N). In\norder to do so we consider w\u0304iM and v\u0304iM as above.\nRecall that, given t units of computer time, the Monte-Carlo estimators for Wi (\u03b8) and\nVi (\u03b8) can be written as\n1\n\n1\n\nc1 (t, M )\nc2 (t, M )\nX\nX\n1\n1\nM\nM\nwi,k ,\nvi,k\n1\n1\nc1 (t, M ) k=1\nc2 (t, M ) k=1\n\nM\nM\nwhere {wi,l\n; l \u2265 1} (resp. {vi,l\n; l \u2265 1}) is a sequence of i.i.d. copies of wiM (resp. of viM ),\nand c1 (t, M1 ), c2 (t, M1 ) are the maximal number of runs one is allowed to consider with t\nunits of computer time. Using the result by [10] we can state the following proposition:\n\nProposition 4.7. Let N be the number of Monte Carlo simulations and M the number\nof steps of the Euler scheme, then the tradeoff between N and M for computing Wi (\u03b8)\n(and similarly Vi (\u03b8)) is\n\u03b3\u0303\nN \u224d M 2\u03b3\u22121 \u22123 ,\nfor all 1/2 < \u03b3 < H and \u03b3\u0303 = T m(d + 1), where T is the time horizon, m the dimension\nof the observed process and d the dimension of the noise process.\nProof. We discuss the proof only for Wi , by following exactly the same steps we can obtain\nthe same result for Vi .\nWe only need to check that our process w satisfies the conditions of Theorem 1 in [10].\n(i) We can easily see that the discretized w\u0304tMi converges uniformly to wti .\n(ii) In addition, we have bounded moments of wti , thus E[W\u0304t2i ] \u2192 E[wt2i ].\n(iii) From Proposition 4.5 we have that the rate of convergence of the Euler scheme of\nw\u0304tMi is M 1\u22122\u03b3 , for 1/2 < \u03b3 < H.\n(iv) The computer time required to generate w\u0304tMi is given by \u03c4 (1/M), which satisfies:\n\u03c4 (1/M) = T m(d + 1)M = \u03b3\u0303M\nwhere T is the length of the time period, m is the dimension of the SDE, d is the\ndimension of the fBm and M is the number of Euler steps.\nBy applying Theorem 1 (by [10]) the optimal rule for choosing the number of MonteCarlo simulations and the number of Euler steps is chosen such that the asymptotic error\nis minimized. Therefore, for t the total budget of computer time, as t increases, then the\n1\u22122\u03b3\nor equivalently:\nEuler step should converge to zero with order \u03b3\u0303+2\u22124\u03b3\n1\u22122\u03b3\n\u03b3\u0303+2\u22124\u03b3\n1\n\u224d t \u03b3\u0303+2\u22124\u03b3 thus t \u224d M \u2212 1\u22122\u03b3 .\nM\nBut the number of operations needed for an arbitrary Monte Carlo simulation t0 is equal\n\u03b3\u0303+2\u22124\u03b3\n\u0003\nto \u03b3\u0303MN. Thus, we finally obtain that N \u224d M \u2212 1\u22122\u03b3 \u22121 .\n\n\f26\n\nA. CHRONOPOULOU AND S. TINDEL\n\n4.3. Discretization of the score function. Consider the following discretized version\nof the score function, i.e. \u2207l ln (\u03b8):\nPN M\n1\nv\u0304i,k\nV\u0302\ni\nN\n\u02c6 l ln (\u03b8) =\n(40)\n:= 1 PNk=1 M ,\n\u2207\n\u0174i\nk=1 w\u0304i,k\nN\n\nM\nM\nM\nM\nwhere w\u03041,k\n, w\u03042,k\n, . . . and v\u03041,k\n, v\u03042,k\n, . . . are iid copies of w\u0304iM and v\u0304iM respectively. Our aim in\nthis section is to give a global bound for the mean square error obtained by approximating\n\u02c6 l ln (\u03b8).\n\u2207l ln (\u03b8) by \u2207\n\n\u02c6 l ln (\u03b8) converges to the continuous\nProposition 4.8. The discretized score function \u2207\nscore function \u2207l ln (\u03b8) with rate of convergence of order M \u2212(2\u03b3\u22121) , where 1/2 < \u03b3 < H\nand M is the number of Euler steps used in the discretization.\nProof. We discuss the idea of the proof for the Wi term first:\n!2\nN\n\u0010\n\u00112\n1 X M\nE \u0174i \u2212 Wi\n= E\nw\u0304 \u2212 E[wi (\u03b8)]\nN k=1 i,k\n\u00132\n\u0012 X\nN\nN\nN\n1 X\n1 X\n1\nM\nw\u0304 \u2212\nwi,k +\nwi,k \u2212 E[wi (\u03b8)] .\n= E\nN k=1 i,k N k=1\nN k=1\nThanks now to the independence property between Monte Carlo runs, we get\n\u0012 X\n\u00132\nN\nN\n\u0010\n\u00112\n1\n2 X\nM\n2\nE(w\u0304i,k \u2212 wi,k ) + 2 E\nwi,k \u2212 E[wi (\u03b8)]\nE \u0174i \u2212 Wi \u2264\nN k=1\nN k=1\nN\n1\n1 X\n(Euler MSE)2 + (Monte Carlo MSE)2 \u224d (M 1\u22122\u03b3 )2 + ,\n=\nN k=1\nN\n\nand thus\n\u0010\n\u0011 r\n1\nMSE \u0174i \u2212 Wi \u224d (M 1\u22122\u03b3 )2 + .\nN\n\u03b3\u0303+2\u22124\u03b3\n\nNow, if we use Proposition 4.7, i.e. N \u224d M \u2212 1\u22122\u03b3 \u22121 , for all 1/2 < \u03b3 < H, and\n\u03b3\u0303 = T m(d + 1), where T is the time horizon, m the dimension of the observed process\nand d the dimension of the noise process, we have\n\u0010\n\u0011 q\n\u03b3\u0303\nMSE \u0174i \u2212 Wi \u224d M 2\u22124\u03b3 + M 1\u22122\u03b3 +3 \u224d M 1\u22122\u03b3 ,\n\nsince the first is the dominant term above.\nFollowing the same procedure, we can show that MSE(V\u0302i \u2212 Vi ) \u224d M 1\u22122\u03b3 and thus the\nclaim of the proposition follows easily.\n\u0003\nRemark 4.9. In Proposition 4.8 the rate of convergence is independent of the dimension\nof the problem, i.e. it is independent of the parameter \u03b3\u0303 = T m(d + 1).\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n27\n\n5. Numerical Examples\nIn this section our aim is to investigate the performance of the suggested maximum likelihood method in practice. We study the one-dimensional fractional Ornstein-Uhlenbeck\nprocess, a linear two-dimensional system of fractional SDEs and then some real data given\nby a financial time series. Before presenting our results, we first discuss some technical\nissues raised by the algorithmic implementation of our method.\nThe goal is to find the root of the quantity \u2207l ln (\u03b8) with respect to \u03b8. We can divide this\nprocedure in two parts. The first part consists in computing the root of the log-likelihood\nusing a stochastic approximation algorithm. This is a stochastic optimization technique\nfirstly introduced by Robbins and Monro (1951) that is used when only noisy observations\nof the function are available. In our case it is appropriate, since we want to solve\n\u2207l ln (\u03b8) = 0,\n\u02c6 l ln (\u03b8). Thus,\nwhere \u2207l ln (\u03b8) is given by Theorem 3.17 and has to be approximated by \u2207\nthe recursive procedure is of the following form\n\u02c6 l ln (\u03b8\u0302k ).\n\u03b8\u0302k+1 = \u03b8\u0302k \u2212 ak \u2207\n\n(41)\n\n\u02c6 l ln is the estimate of \u2207l ln at the k-th iteration based on the observations and\nwhere \u2207\nP\u221e 2\nP\nak is a sequence of real numbers such that \u221e\nk=1 ak < \u221e. Under\nk=1 ak = \u221e and\nappropriate conditions (see for example [4]), the iteration in (41) converges to \u03b8 almost\nsurely. The step sizes satisfy ak > 0 and the way that we choose them can be found\nin [26].\n\u02c6 l ln (\u03b8\u0302k ) at each step of the stochasThe second part consists of the computation of \u2207\ntic approximation algorithm. Thus, for a given value of \u03b8k (the one computed at the\n\u02c6 l ln (\u03b8k ) when we are given n discrete observations\nk-th iteration) we want to compute \u2207\nof the process: yti , i = 1, . . . , n. Here, we describe the main idea of the algorithm we\nuse for only one step. Thus, assume that we are at [ti\u22121 , ti ], and at time ti we obtain\nthe i-th observation. We want to compute Wi (\u03b8) and Vi (\u03b8) according to expressions (25)\nand (26) respectively. To compute the expectations we use simple Monte-Carlo simulations.Therefore, we discretize the time interval into N steps\nti\u22121 = s0 < s1 < * * * < sN = ti .\nFrom each simulated path (apart from that of fBm) we only need to keep the terminal\nvalue which is the value of the process at time ti . The algorithm is the following\n(1) Simulate N values of fBm in the interval [ti\u22121 , ti ] using for example the circulant\nmatrix method (any exact -preferably- simulation technique can be used).\n(2) Using the simulated values from step 1 and an Euler scheme for the SDE (1),\nsimulate the value of the process at time ti . For example, for k = 0, . . . , N\n\u0232sM\nk\n\n=\n\n\u0232sM\nk\u22121\n\n+\n\n\u03bc(\u0232sM\n)(sk\nk\n\n\u2212 sk\u22121 ) +\n\nd\nX\n\n\u03c3 (j) (\u0232sM\n)(Bs(j)\n\u2212 Bs(j)\n).\nk\u22121\nk\nk\u22121\n\nj=1\n\n(3) Using step 2 and the observation at time ti , compute the indicator function\n1(Yti (\u03b8)>yti ) .\n\n\f28\n\nA. CHRONOPOULOU AND S. TINDEL\n\n(4) Using step 1 and an Euler scheme simulate Dti Y\u03c4i , as given in Lemma 3.3 for n = 1\n-first Malliavin derivative-.\n(5) Using step 1 and an Euler scheme simulate \u03b7tkji , k, j = 1, . . . , m, as given in Proposition 3.7.\n(6) Steps 4 and 5 are used to compute Qpj\nsti , p \u2208 {1, . . . , m}, j \u2208 {1, . . . , d} as defined\nin Propositions 3.12 and 3.14.\n(7) Simulate the Malliavin derivative of the product Ds [Yt Qpj\nrt ].\n(8) Using the previous steps, numerical integration for the double integral and numerical integration for the stochastic integral we compute Up (Yti (\u03b8)) as defined in\nProposition 3.12.\n(9) Recursively compute H(1,...,m) (Yti (\u03b8)) as given in (19).\n(10) Combine steps 3 and 9 to obtain the kernel Wi (\u03b8).\n(11) We repeat steps 1 through 10 N times and we average these values to obtain an\nestimate for the expectation Wi (\u03b8).\nUsing a similar procedure we can obtain an estimate for the expectation Vi (\u03b8). Finally,\nfor each i = 1, . . . , n we compute Vi (\u03b8)/Wi (\u03b8) and sum over i to obtain the desired value\nof the log-likelihood at \u03b8k .\nWe have completed the study of our numerical approximation of the log-likelihood, and\nare now ready for the analysis of some numerical examples.\n5.1. Fractional Ornstein-Uhlenbeck process. Though our method can be applied to\nhighly nonlinear contexts, we focus here on some linear situations, which allow easier\ncomparisons with existing methods or exact computations. Let us first study the onedimensional fractional Ornstein-Uhlenbeck process, i.e.\ndYt = \u2212\u03bbYt dt + dBt ,\n(42)\nR t \u2212\u03bb(t\u2212s)\nwhere the solution is given Yt (\u03bb) = 0 e\ndBs (notice the existence of an explicit\nsolution here). In this case our methodology is quite simplified. The log-likelihood can\nbe written as follows:\n\u0014\n\u0015\n\u0010\n\u0011\n\u2202\u03bb H(1,1) (\u03bb)\nn E \u2202\u03bb Yt (\u03bb) 1(Yt (\u03bb)>y) H(1,1) (\u03bb) + Yt (\u03bb) \u2212 y\nX\n+\n\u0014\n\u2202\u03bb l(\u03bb; y) =\n.\n\u0010\n\u0011\u0015\ni=1\nE 1(Yt (\u03bb)>y) H(1) Yt (\u03bb), 1\n\nThe Malliavin derivative of Yt (\u03bb) satisfies the following ODE\nZ t\nDs Yt (\u03bb) = 1 \u2212 \u03bb\nDs Yu (\u03bb)du,\ns\n\n\u2212\u03bb t\n\nwith solution Ds Yt (\u03bb) = e\n\n1{s\u2264t} . The corresponding norm is\nZ tZ t\n2\nkD* Yt (\u03bb)k = cH\ne\u2212\u03bb(u+v) |u \u2212 v|2H\u22122 dudv.\ns\n\ns\n\nThe higher order derivatives of Yt (\u03bb) are equal to zero. Therefore,\nZ t\n\u0010\n\u0011\n1\ne\u2212\u03bbu dBu\nH(1) Yt (\u03bb) =\n2\nkD* Yt (\u03bb)k s\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n29\n\nand thus\n1\nH(1,1) (\u03bb) =\nkD* Yt (\u03bb)k4\n\nZ tZ\ns\n\nt\n\ne\u2212\u03bb(u+v) dBu dBv \u2212 cH kD* Yt (\u03bb)k\u22122 .\n\ns\n\nThe derivative with respect to the unknown parameter \u03bb satisfies\nZ t\n\u2202\u03bb Yt (\u03bb) = \u2212\nYs (\u03bb) \u2212 \u03bb\u2202\u03bb Ys (\u03bb)ds\n0\n\nRt\n\n(t \u2212 s)e\u2212\u03bb(t\u2212s) dBs . The last term we need to compute is:\n\u0014\nZ tZ t\n1\n4\n\u2202\u03bb H(1,1) (\u03bb) =\n\u2212(u + v)e\u2212\u03bb(u+v) dBu dBv\nkD* Yt (\u03bb)k\n8\nkD* Yt (\u03bb)k\ns\nr\n\u0015\nZ tZ t\n2\n\u2212\u03bb(u+v)\n2H\u22122\n\u22122cH kD* Yt (\u03bb)k\n\u2212(u + v)e\n|u \u2212 v|\ndudv\ns\nr\nRtRt\nc2H s r \u2212(u + v)e\u2212\u03bb(u+v) |u \u2212 v|2H\u22122 dudv\n.\n\u2212\nkD* Yt (\u03bb)k4\n\nwith solution \u2202\u03bb Yt (\u03bb) =\n\n0\n\nNow, we compute the MLE following the algorithm we described above. The results\nwe obtained are summarized in the following table:\nTrue \u03bb MLE \u03bb\u0302 Standard Error\n0.5\n0.497\n0.00369\n4\n3.861\n0.00127\nRemark 5.1. The value of H used for the simulation of the process is 0.6. The number of\nobservations is n = 50, the number of Euler steps is M = 500, the number of stochastic\napproximation steps is K = 50 and the number of MC simulations N = 500.\n5.2. Two-dimensional fractional SDE. In this section we study the following system\nof fractional OU processes:\n(1)\n\ndYt\n\n(2)\n\ndYt\n\n(2)\n\n(1)\n\n= \u2212\u03b1Yt dt + \u03b2dBt\n(1)\n\n(2)\n\n= \u2212\u03b2Yt dt + \u03b2dBt .\n\n(43)\n\nIn this case, the computations are more involved even though the SDEs are linear functions\nof Y . Furthermore, the parameter we want to estimate is two-dimensional as well (\u03b8 =\n(\u03b1, \u03b2)T ), which complicated the optimization procedure. Therefore, instead of computing\nonly one derivative, we need to compute both derivatives with respect to \u03b1 and \u03b2 and\nthen compute the solution of the system of two equations\n\u2207\u03b1 l(\u03b1, \u03b2; y) = 0,\n\n\u2207\u03b2 l(\u03b1, \u03b2; y) = 0,\n\nwhere\n\u2207l l(\u03b1, \u03b2; y) =\n\nn\nX\n\n[E[1(Yt (\u03b1,\u03b2)>y) H(1,2) (Yt (\u03b1, \u03b2))]\u22121\n\ni=1\n\n\u00d7 {E[\u2207l Yt (\u03b1, \u03b2) 1(Yt (\u03b1,\u03b2)>y) H(1,2,1,2) (\u03b1, \u03b2) + (Yt (\u03b1, \u03b2) \u2212 y)+ \u2207l H(1,2,1,2) (\u03b1, \u03b2)]}\n\n\f30\n\nA. CHRONOPOULOU AND S. TINDEL\n\nand l = \u03b1 or \u03b2. The Malliavin derivative of Yt computes as follows:\nZ t\nZ t\n(1)\n(2)\n(2)\nDs Y t = \u03b2 \u2212 \u03b1\nDs Yu du\nDs Y t = \u03b2 \u2212 \u03b2\nDs Yu(1) du.\ns\n\ns\n\nThe covariance matrix \u03b3t is given by\nmatrix satisfies the following SDE\n\u03b3t\u22121\n\n=\u2212\n\n(hD* Yti , D* Ytj i)1\u2264i,j\u22642.\n\nZ\n\n0\n\nwhere\n\nThe inverse of the covariance\n\nt\n\n[\u03b3u\u22121 M + M T \u03b3u\u22121 ]du,\n\u0014\n\n\u0015\n0 \u03b1\nM=\n\u03b2 0\nNow, it remains to compute the quantities H(1,2) and H(1,2,1,2) . This can be done using\nthe recursive formulas in Proposition 3.12, but we need to keep in mind that higher order\nderivatives of Y are equal to zero, thus they will be simplified. Indeed,\nZ t\nZ tZ t\n2\nX\nj\n\u22121 1j\nj\nH(1) (Yt ) =\nYt\n(\u03b3s ) Ds Yt dBs \u2212 cH\nDs Ytj Qrt |r \u2212 s|2H\u22122 drds.\n0\n\nj=1\n\n0\n\n0\n\nMoreover, we can easily see that\nZ t\nZ tZ t\nH(1,2) (Yt ) = H(1) (Yt )\nQst dBs \u2212 cH\nDs H(1) (Yt )Qrt |r \u2212 s|2H\u22122 drds\n0\n\nH(1,2,1,2) (Yt ) = H(1,2,1) (Yt )\nQpj\nst\n\nZ\n\n0\n\nt\n\nQst dBs \u2212 cH\n\n0\n\u22121 pj\n(\u03b3s ) Ds Ytj .\n\n0\n\nZ tZ\n0\n\nt\n\nDs H(1,2,1) (Yt )Qrt |r \u2212 s|2H\u22122 drds\n\n0\n\nOf course, recall that\n=\nIn practice, these quantities are computed recursively. The last step is to compute the derivative of H(1,2,1,2) (Yt ) with respect to \u03b1 and\n\u03b2, which in this case is not as complicated and compute the MLEs using the algorithm\ndiscussed in the previous section. The table below summarizes our results, and we have\nplotted the corresponding histograms in Figure 1.\nParameter True Value\n\u03b1\n2\n\u03b2\n4\n\nMLE Standard Error\n2.003\n0.0518\n3.987\n0.0157\n\nRemark 5.2. The value of H used for the simulation of the process is 0.6. The number of\nobservations is n = 50, the number of Euler steps is N = 500, the number of stochastic\napproximation steps is K = 50 and the number of MC simulations M = 500.\n5.3. Application to financial data. One of the most popular applications of fractional\nSDEs is in finance. Hu and Oksendal, [20], introduced the fractional Black-Scholes model\nin order to account for inconsistencies of the existing models in practice. More specifically,\nthe stock price is described therein by a fractional geometric Brownian motion with Hurst\nparameter 1/2 < H < 1. The choice of this model is based on empirical studies that\ndisplayed the presence of long-range dependence on stock prices, for example in Willinger,\nTaqqu and Teverovsky, [42].\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\nHistogram of beta\n\n4\n\nDensity\n\n0\n\n0\n\n2\n\n5\n\nDensity\n\n10\n\n6\n\n8\n\n15\n\nHistogram of alpha\n\n31\n\n1.90\n\n1.95\n\n2.00\n\n2.05\n\n2.10\n\n3.80\n\nalpha\n\n3.85\n\n3.90\n\n3.95\n\n4.00\n\n4.05\n\n4.10\n\n4.15\n\nbeta\n\nDrift Parameter.\n\nDiffusion Parameter.\n\nFigure 1. Empirical Distribution of the estimators for \u03b1 and \u03b2.\nHowever, the presence of fractional Brownian motion in the model allows for arbitrage\nin the general setting. It has been shown that arbitrage opportunities can be avoided in a\nnumber of ways, for example the reader can refer to Rogers [39], Dasgupta and Kallianpur\n[7] and Cheridito [5]. We choose to model the stock price as as follows:\ndSt = \u03bcSt dt + \u03c3dBt ,\n\n(44)\n\nwhere B is a fractional Brownian motion with Hurst index 1/2 < H < 1. For this SDE\n(as well as for a more general class of fractional SDEs) Guasoni, [13], proved that there\nis no arbitrage when transaction costs are present.\nOur goal is to estimate the unknown parameters \u03bc and \u03c3 based on daily observations of\nthe S&P 500 index (data from June 2010 until December 2010). Since the Hurst parameter\nis piece-wise constant, we devide the data in three groups (of 50 daily observations each)\nand we compute for each one the Hurst index using the Rescaled-Range (R/S) statistic.\nWe obtain that for the first group of data \u01241 = 0.59, for the second \u01242 = 0.63 and for\nthe third one \u01243 = 0.61. For the different groups, we apply our maximum likelihood\napproach in order to estimate \u03bc and \u03c3. The estimates are summarized in the following\ntable:\nEstim. Parameters Group 1: \u01241 = 0.59 Group 2: \u01242 = 0.63 Group 3: \u01243 = 0.61\n\u03bc\u0302\n0.015 (0.0123)\n0.019 (0.0144)\n0.011 (0.0214)\n\u03c3\u0302\n0.352 (0.058)\n0.339 (0.046)\n0.341 (0.024)\nRemark 5.3. The volatility is computed in years. In addition, during this period of time\nthe historical volatility is around 0.38, which is coherent with our own estimation.\n\n\f32\n\nA. CHRONOPOULOU AND S. TINDEL\n\nReferences\n[1] F. Baudoin, L. Coutin: Operators associated with a stochastic differential equation driven by fractional Brownian motions. Stoch. Proc. Appl. 117 (2007), no. 5, 550\u2013574.\n[2] F. Baudoin, C. Ouyang: Gradient Bounds for Solutions of Stochastic Differential Equations Driven\nby Fractional Brownian Motions. Arxiv Preprint (2011).\n[3] F. Baudoin, C. Ouyang, S. Tindel: Gaussian bounds for the density of solutions of stochastic differential equations driven by fractional Brownian motions. In preparation.\n[4] J. R. Blum: Multidimensional stochastic approximation methods. The Annals of Mathematical Statistics. 25(1954), no. 4, 737\u2013744.\n[5] P. Cheridito: Arbitrage in fractional Brownian motion models. Finance Stoch., 7 (2003), no. 4,\n533\u2013553.\n[6] R. Dalang, E. Nualart: Potential theory for hyperbolic SPDEs. Ann. Probab. 32 (2004), no. 3A,\n2099\u20132148.\n[7] A. Dasgupta and G. Kallianpur: Arbitrage opportunities for a class of Gladyshev processes, Appl.\nMath. Optim., 41 (2000), no. 3, 377-\u00e2\u0102\u015e385.\n[8] A. Deya, A. Neuenkirch, S. Tindel: A Milstein-type scheme without L\u00e9vy area terms for SDEs\ndriven by fractional Brownian motion. Arxiv preprint (2010). To appear in Ann. Inst. Henri Poincar\nProbab. Stat..\n[9] D. Duffie, P. Glynn: Efficient Monte Carlo simulation of security prices. Ann. Appl. Probab. 5 (4)\n(1995), 897\u2013905.\n[10] G. Durham, A. Gallant: Numerical techniques for maximum likelihood estimation of continuous-time\ndiffusion processes. J. Bus. Econom. Statist. 20 (2002), no. 3, 297\u2013338.\n[11] P. Friz and N. Victoir: Multidimensional stochastic processes as rough paths. Theory and applications.\nCambridge University Press, 2010.\n[12] E. Gobet: Local asymptotic mixed normality property for elliptic diffusion: a Malliavin calculus\napproach. Bernoulli 7 (2001), no. 6, 899\u2013912.\n[13] P. Guasoni: No arbitrage under transaction costs, with fractional Brownian motion and beyond.\nMathematical Finance, 16 (2006), no. 3, 569\u2013582.\n[14] M. Gubinelli: Controlling rough paths. J. Funct. Anal. 216, 86-140 (2004).\n[15] M. Hairer: Ergodicity of stochastic differential equations driven by fractional Brownian motion. Ann.\nProbab. 33 (2005), no. 2, 703\u2013758.\n[16] M. Hairer, A. Majda: A simple framework to justify linear response theory. Nonlinearity 23 (2010),\nno. 4, 909\u2013922.\n[17] M. Hairer, A. Ohashi: Ergodicity theory of SDEs with extrinsic memory. Ann. Probab. 35 (2007),\nno. 5, 1950\u20131977.\n[18] M. Hairer, S. Pilai: Ergodicity of hypoelliptic SDEs driven by fractional Brownian motion. Arxiv\nPreprint (2009).\n[19] Y. Hu, D. Nualart: Differential equations driven by H\u00f6lder continuous functions of order greater\nthan 1/2. Abel Symp. 2 (2007), 349-413.\n[20] Y. Hu and B. Oksendal: Fractional white noise calculus and applications to finance, Infnite dimentional analysis, quantum probbility and related topics, 6 (2003), no. 1, 1\u201332.\n[21] Y. Hu, B. Oksendal and A. Sulem: Optimal sonsumption and portfolio in a Black-Scholes market\ndriven by fractional Brownian motion, Infnite dimentional analysis, quantum probbility and related\ntopics, 6 (2003), no. 4, 519\u2013536.\n[22] R. Kasonga: Maximum likelihood theory for large interacting systems. SIAM J. Appl. Math. 50\n(1990), no. 3, 865\u2013875.\n[23] M. Kleptsyna, A. Le Breton: Statistical analysis of the fractional Ornstein-Uhlenbeck type process.\nStat. Inference Stoch. Process. 5 (2002), no. 3, 229\u2013248.\n[24] A. Kohatsu-Higa: Lower bounds for densities of uniformly elliptic random variables on Wiener space.\nProbab. Theory Related Fields 126 (2003), no. 3, 421\u2013457.\n[25] S. Kou, X. Sunney-Xie: Generalized Langevin equation with fractional Gaussian noise: subdiffusion\nwithin a single protein molecule. Phys. Rev. Lett. 93, no. 18 (2004).\n\n\fON INFERENCE FOR FRACTIONAL DIFFERENTIAL EQUATIONS\n\n33\n\n[26] H. J. Kushner, G.G. Yin: Stochastic approximation algorithms and applications. Springer-Verlag,\n1997.\n[27] A. Lejay (2003): An Introduction to Rough Paths. S\u00e9minaire de probabilit\u00e9s 37, vol. 1832 of Lecture\nNotes in Mathematics, 1-59.\n[28] J. Le\u00f3n, S. Tindel: Malliavin calculus for fractional delay equations. Arxiv Preprint (2009).\n[29] T. Lyons, Z. Qian (2002): System control and rough paths. Oxford University Press.\n[30] Y. Mishura, G. Shevchenko: The rate of convergence for Euler approximations of solutions of stochastic differential equations driven by fractional Brownian motion. Stochastics 80(5) (2008), 489\u2013511.\n[31] A. Neuenkirch, I. Nourdin, A. R\u00f6ssler, S. Tindel : Trees and asymptotic developments for fractional\ndiffusion processes. Ann. Inst. Henri Poincar Probab. Stat. 45 (2009), no. 1, 157\u2013174.\n[32] D. Nualart: Malliavin Calculus and Related Topics. Springer-Verlag, 2006.\n[33] D. Nualart, A. R\u01ce\u015fcanu: Differential equations driven by fractional Brownian motion. Collect. Math.\n53 no. 1 (2002), 55-81.\n[34] D. Nualart, B. Saussereau: Malliavin calculus for stochastic differential equations driven by a fractional Brownian motion. Stochastic Process. Appl. 119 (2009), no. 2, 391\u2013409.\n[35] D. Odde, E. Tanaka, S. Hawkins, H. Buettner: Stochastic dynamics of the nerve growth cone and its\nmicrotubules during neurite outgrowth. Biotechnology and Bioengineering 50, no. 4, 452-461 (1996).\n[36] A. Pedersen: Consistency and asymptotic normality of an approximate maximum likelihood estimator for discretely observed diffusion processes. Bernoulli 1 (1995), no. 3, 257\u2013279.\n[37] A. Papavasiliou, C. Ladroue: Parameter estimation for rough differential equations. Arxiv preprint\n(2009).\n[38] Ll. Quer, D. Nualart: Gaussian density estimates for solutions to quasi-linear stochastic partial\ndifferential equations. Stochastic Process. Appl. 119 (2009), no. 11, 3914\u20133938.\n[39] L. C. G. Rogers: Arbitrage with fractional Brownian motion, Math. Finance, 7 (1997), no. 1, 95\u2013105.\n[40] M. Sorensen: Parametric inference for discretely sampled stochastic differential equations. In Andersen, T.G. Davis, R.A., Kreiss, J.-P. and Mikosch, T. (eds.): Handbook of Financial Time Series,\nSpringer (2009), 531 - 553.\n[41] C. Tudor, F. Viens: Statistical aspects of the fractional stochastic calculus. Ann. Statist. 35 (2007),\nno. 3, 1183\u20131212.\n[42] W. Willinger, M. S. Taqqu and V. Teverovsky: Stock market prices and long-range dependence,\nFinance Stoch., 3 (1999), no. 1, 1\u201313.\n[43] M. Z\u00e4hle: Integration with respect to fractal functions and stochastic calculus I. Probab. Theory\nRelat. Fields 111 (1998), 333-374.\nAlexandra Chronopoulou, Samy Tindel: Institut \u00c9lie Cartan Nancy, B.P. 239, 54506 Vandoeuvre-l\u00e8sNancy Cedex, France. Email: Alexandra.Chronopoulou@iecn.u-nancy.fr, tindel@iecn.u-nancy.fr\n\n\f"}