{"id": "http://arxiv.org/abs/0906.1310v2", "guidislink": true, "updated": "2011-02-03T07:53:30Z", "updated_parsed": [2011, 2, 3, 7, 53, 30, 3, 34, 0], "published": "2009-06-06T21:23:08Z", "published_parsed": [2009, 6, 6, 21, 23, 8, 5, 157, 0], "title": "Bootstrap consistency for general semiparametric $M$-estimation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.5359%2C0906.1569%2C0906.2717%2C0906.3526%2C0906.2289%2C0906.2068%2C0906.1699%2C0906.0890%2C0906.2560%2C0906.4473%2C0906.3899%2C0906.2078%2C0906.0778%2C0906.1517%2C0906.0738%2C0906.1786%2C0906.3644%2C0906.3242%2C0906.5227%2C0906.1310%2C0906.1490%2C0906.2029%2C0906.5421%2C0906.2062%2C0906.2030%2C0906.5311%2C0906.3801%2C0906.2463%2C0906.4399%2C0906.0505%2C0906.0004%2C0906.2290%2C0906.1243%2C0906.3433%2C0906.0784%2C0906.4485%2C0906.3769%2C0906.4810%2C0906.4728%2C0906.5404%2C0906.2910%2C0906.3069%2C0906.1706%2C0906.2998%2C0906.4960%2C0906.2374%2C0906.0421%2C0906.1407%2C0906.1497%2C0906.5376%2C0906.5402%2C0906.5481%2C0906.3586%2C0906.1468%2C0906.4643%2C0906.2774%2C0906.2444%2C0906.3781%2C0906.3641%2C0906.5513%2C0906.1109%2C0906.5450%2C0906.0426%2C0906.5571%2C0906.5446%2C0906.4887%2C0906.3779%2C0906.1987%2C0906.3491%2C0906.3431%2C0906.3816%2C0906.4971%2C0906.4021%2C0906.1138%2C0906.4710%2C0906.4588%2C0906.1422%2C0906.3676%2C0906.0329%2C0906.4803%2C0906.3130%2C0906.4007%2C0906.1353%2C0906.0471%2C0906.0211%2C0906.0753%2C0906.4462%2C0906.0600%2C0906.4024%2C0906.0293%2C0906.2087%2C0906.3521%2C0906.1359%2C0906.1210%2C0906.5443%2C0906.2743%2C0906.2578%2C0906.1583%2C0906.4144%2C0906.4116%2C0906.4965&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Bootstrap consistency for general semiparametric $M$-estimation"}, "summary": "Consider $M$-estimation in a semiparametric model that is characterized by a\nEuclidean parameter of interest and an infinite-dimensional nuisance parameter.\nAs a general purpose approach to statistical inferences, the bootstrap has\nfound wide applications in semiparametric $M$-estimation and, because of its\nsimplicity, provides an attractive alternative to the inference approach based\non the asymptotic distribution theory. The purpose of this paper is to provide\ntheoretical justifications for the use of bootstrap as a semiparametric\ninferential tool. We show that, under general conditions, the bootstrap is\nasymptotically consistent in estimating the distribution of the $M$-estimate of\nEuclidean parameter; that is, the bootstrap distribution asymptotically\nimitates the distribution of the $M$-estimate. We also show that the bootstrap\nconfidence set has the asymptotically correct coverage probability. These\ngeneral conclusions hold, in particular, when the nuisance parameter is not\nestimable at root-$n$ rate, and apply to a broad class of bootstrap methods\nwith exchangeable bootstrap weights. This paper provides a first general\ntheoretical study of the bootstrap in semiparametric models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.5359%2C0906.1569%2C0906.2717%2C0906.3526%2C0906.2289%2C0906.2068%2C0906.1699%2C0906.0890%2C0906.2560%2C0906.4473%2C0906.3899%2C0906.2078%2C0906.0778%2C0906.1517%2C0906.0738%2C0906.1786%2C0906.3644%2C0906.3242%2C0906.5227%2C0906.1310%2C0906.1490%2C0906.2029%2C0906.5421%2C0906.2062%2C0906.2030%2C0906.5311%2C0906.3801%2C0906.2463%2C0906.4399%2C0906.0505%2C0906.0004%2C0906.2290%2C0906.1243%2C0906.3433%2C0906.0784%2C0906.4485%2C0906.3769%2C0906.4810%2C0906.4728%2C0906.5404%2C0906.2910%2C0906.3069%2C0906.1706%2C0906.2998%2C0906.4960%2C0906.2374%2C0906.0421%2C0906.1407%2C0906.1497%2C0906.5376%2C0906.5402%2C0906.5481%2C0906.3586%2C0906.1468%2C0906.4643%2C0906.2774%2C0906.2444%2C0906.3781%2C0906.3641%2C0906.5513%2C0906.1109%2C0906.5450%2C0906.0426%2C0906.5571%2C0906.5446%2C0906.4887%2C0906.3779%2C0906.1987%2C0906.3491%2C0906.3431%2C0906.3816%2C0906.4971%2C0906.4021%2C0906.1138%2C0906.4710%2C0906.4588%2C0906.1422%2C0906.3676%2C0906.0329%2C0906.4803%2C0906.3130%2C0906.4007%2C0906.1353%2C0906.0471%2C0906.0211%2C0906.0753%2C0906.4462%2C0906.0600%2C0906.4024%2C0906.0293%2C0906.2087%2C0906.3521%2C0906.1359%2C0906.1210%2C0906.5443%2C0906.2743%2C0906.2578%2C0906.1583%2C0906.4144%2C0906.4116%2C0906.4965&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Consider $M$-estimation in a semiparametric model that is characterized by a\nEuclidean parameter of interest and an infinite-dimensional nuisance parameter.\nAs a general purpose approach to statistical inferences, the bootstrap has\nfound wide applications in semiparametric $M$-estimation and, because of its\nsimplicity, provides an attractive alternative to the inference approach based\non the asymptotic distribution theory. The purpose of this paper is to provide\ntheoretical justifications for the use of bootstrap as a semiparametric\ninferential tool. We show that, under general conditions, the bootstrap is\nasymptotically consistent in estimating the distribution of the $M$-estimate of\nEuclidean parameter; that is, the bootstrap distribution asymptotically\nimitates the distribution of the $M$-estimate. We also show that the bootstrap\nconfidence set has the asymptotically correct coverage probability. These\ngeneral conclusions hold, in particular, when the nuisance parameter is not\nestimable at root-$n$ rate, and apply to a broad class of bootstrap methods\nwith exchangeable bootstrap weights. This paper provides a first general\ntheoretical study of the bootstrap in semiparametric models."}, "authors": ["Guang Cheng", "Jianhua Z. Huang"], "author_detail": {"name": "Jianhua Z. Huang"}, "author": "Jianhua Z. Huang", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/10-AOS809", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0906.1310v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0906.1310v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/10-AOS809 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0906.1310v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0906.1310v2", "journal_reference": "Annals of Statistics 2010, Vol. 38, No. 5, 2884-2915", "doi": "10.1214/10-AOS809", "fulltext": "arXiv:0906.1310v2 [math.ST] 3 Feb 2011\n\nThe Annals of Statistics\n2010, Vol. 38, No. 5, 2884\u20132915\nDOI: 10.1214/10-AOS809\nc Institute of Mathematical Statistics, 2010\n\nBOOTSTRAP CONSISTENCY FOR GENERAL SEMIPARAMETRIC\nM -ESTIMATION\nBy Guang Cheng1 and Jianhua Z. Huang2\nPurdue University and Texas A&M University\nConsider M -estimation in a semiparametric model that is\ncharacterized by a Euclidean parameter of interest and an infinitedimensional nuisance parameter. As a general purpose approach to\nstatistical inferences, the bootstrap has found wide applications in\nsemiparametric M -estimation and, because of its simplicity, provides\nan attractive alternative to the inference approach based on the\nasymptotic distribution theory. The purpose of this paper is to provide theoretical justifications for the use of bootstrap as a semiparametric inferential tool. We show that, under general conditions, the\nbootstrap is asymptotically consistent in estimating the distribution\nof the M -estimate of Euclidean parameter; that is, the bootstrap distribution asymptotically imitates the distribution of the M -estimate.\nWe also show that the bootstrap confidence set has the asymptotically correct coverage probability. These general conclusions hold, in\nparticular, when the nuisance parameter is not estimable at root-n\nrate, and apply to a broad class of bootstrap methods with exchangeable bootstrap weights. This paper provides a first general theoretical\nstudy of the bootstrap in semiparametric models.\n\n1. Introduction. Due to its flexibility, semiparametric modeling has provided a powerful statistical modeling framework for complex data, and\nproven to be useful in a variety of contexts, see [2, 7, 20, 44, 45]. Semiparametric models are indexed by a Euclidean parameter of interest \u03b8 \u2208 \u0398 \u2282 Rd\nand an infinite-dimensional nuisance function \u03b7 belonging to a Banach space\nH with a norm k * k. M -estimation, including the maximum likelihood estimation as a special case, refers to a general method of estimation, where the\nReceived October 2009.\nSupported by NSF Grant DMS-09-06497.\n2\nSupported in part by NSF Grants DMS-06-06580, DMS-09-07170, NCI Grant\nCA57030 and Award Number KUS-CI-016-04, made by King Abdullah University of Science and Technology (KAUST).\nAMS 2000 subject classifications. Primary 62F40; secondary 62G20.\nKey words and phrases. Bootstrap consistency, bootstrap confidence set, semiparametric model, M -estimation.\n1\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2010, Vol. 38, No. 5, 2884\u20132915. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nG. CHENG AND J. Z. HUANG\n\nestimates are obtained by optimizing some objective functions [10, 28, 42].\nThe asymptotic theories and inference procedures for semiparametric maximum likelihood estimation, or more generally M -estimation, have been extensively studied in [4, 11, 22, 24, 28, 32].\nIt is well known that the asymptotic inferences of semiparametric models\noften face practical challenges. In particular, the confidence set construction and the asymptotic variance estimation of the estimator for the Euclidean parameter both involve estimating and inverting a hard-to-estimate\ninfinite-dimensional operator. The difficulty in dealing with such an infinitedimensional operator motivated the development of the profile sampler [8,\n9, 24], where the inference of the Euclidean parameter is based on sampling from the posterior of the profile likelihood [24]. However, because of\nthe way it is designed, the profile sampler method has the typical caveats\nof the Bayesian methods. First, one needs to specify a prior distribution.\nSecond, since the Markov chain Monte Carlo (McMC) is used for sampling\nfrom the posterior distribution, there are a number of controversial issues\nin generating the stationary Markov chain. For example, it is considerably\ndifficult to determine the burn-in period and stopping time of the chain [16].\nIn particular, it may take a long time to run the Markov chain in order to\ngive accurate inferences for \u03b8 when \u03b7 is estimable at a slow convergence rate\n[8, 9]. Moreover, when the sample size is small, the profile likelihood may\nbecome nonsmooth or may not approximate well the desired parabolic form,\nviolating the main theoretical basis of the profile sampler.\nOn the other hand, as a general data-resampling based statistical inference tool, the bootstrap method does not have the drawbacks of the profile\nsampler; see [6, 19, 21, 28, 37, 43] for its application in semiparametric models. In fact, the bootstrap method has several methodological advantages\nover the profile sampler: it is straightforward to implement; there is no need\nto specify a prior distribution and to check Markov chain convergence. In\naddition, the finite sample performance of the bootstrap can be controlled\nby choosing from a rich pool of resampling techniques; see Section 3 of [33].\nMoreover, unlike the profile sampler which focuses on \u03b8, one can make bootstrap inferences for both \u03b8 and \u03b7.\nUnfortunately, a systematic theoretical study on the bootstrap inference\nin semiparametric models is almost\n\u221a nonexistent, especially when the nuisance function parameter \u03b7 is not n estimable, despite the rich literature\non the bootstrap theory for parametric models [3, 18, 30, 36]. The current\nliterature only considered the bootstrap consistency for the joint estima\u221a\ntor of (\u03b8, \u03b7) in some special case of semiparametric models where \u03b7 is nestimable, that is, [21]. In a recent monograph, Kosorok pointed out that\n\"convergence rate and asymptotic normality results are quite difficult to\nestablish for the nonparametric bootstrap (based on multinomial\nweights),\n\u221a\nespecially for models with parameters not estimable at the n rate\" [22].\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n3\n\nIn fact, the lack of theoretical justifications of the bootstrap in the semiparametric context is one of the main motivations for developing the profile\nsampler. The purpose of this paper is to develop a general theory on bootstrap consistency in semiparametric models, for a broad class of bootstrap\nmethods including Efron's (nonparametric) bootstrap as a special case. We\nfocus on the inference of the Euclidean parameter and leave study of the\nbootstrap inference of the nuisance parameter for future research, although\nwe give some useful convergence rate results (see Section 5).\nOur main results are summarized as follows. The semiparametric M b \u03b7b) and the bootstrap M -estimator (\u03b8b\u2217 , \u03b7b\u2217 ) are obtained by\nestimator (\u03b8,\noptimizing the objective function m(\u03b8, \u03b7) based on the i.i.d. observations\n(X1 , . . . , Xn ) and the bootstrap sample (X1\u2217 , . . . , Xn\u2217 ), respectively:\n(1)\n\nb \u03b7b) = arg sup\n(\u03b8,\n\nn\nX\n\n\u03b8\u2208\u0398,\u03b7\u2208H i=1\n\n(2)\n\n(\u03b8b\u2217 , \u03b7b\u2217 ) = arg sup\n\nn\nX\n\n\u03b8\u2208\u0398,\u03b7\u2208H i=1\n\nm(\u03b8, \u03b7)(Xi ),\nm(\u03b8, \u03b7)(Xi\u2217 ),\n\nwhere (X1\u2217 , . . . , Xn\u2217 ) are independent draws with replacement from the original sample. Note that we can express\n(3)\n\n(\u03b8b\u2217 , \u03b7b\u2217 ) = arg sup\n\nn\nX\n\n\u03b8\u2208\u0398,\u03b7\u2208H i=1\n\nWni m(\u03b8, \u03b7)(Xi ),\n\nand the bootstrap weights (Wn1 , . . . , Wnn ) \u223c Multinomial(n, (n\u22121 , . . . , n\u22121 )).\nIn this paper, we consider the more general exchangeable bootstrap weighting scheme that includes Efron's bootstrap and its smooth alternative [27],\nfor example, Bayesian bootstrap, as special cases. The general resampling\nscheme was first proposed in [34], and extensively studied by [1], who suggested the name \"weighted bootstrap,\" and in [30, 33]. Note that other variations of Efron's bootstrap are also studied in [5] using the term \"generalized bootstrap.\" The practical usefulness of the more general scheme is\nwell-documented in the literature. For example, in semiparametric survival\nmodels, for example, Cox regression model, the nonparametric bootstrap\noften gives many ties when it is applied to censored survival data due to its\n\"discreteness\" and the general weighting scheme comes to the rescue. As one\nmain contribution\n\u221a of theb paper, we show that the nonparametric bootstrap\nconditional on the observed data, asymptotically\ndistribution of n(\u03b8b\u2217 \u2212 \u03b8),\n\u221a\nimitates the distribution of n(\u03b8b \u2212 \u03b80 ), where \u03b80 is the true value of \u03b8. As\na consequence, we also establish the consistency of the bootstrap confidence\nset of \u03b8, which means that the coverage probability converges to the nominal level. Our results hold when the estimate of the nuisance function has\n\n\f4\n\nG. CHENG AND J. Z. HUANG\n\neither root-n or slower than root-n convergence rate. This paper can also\nbe viewed as a nontrivial extension of [5] to account for the presence of an\ninfinite-dimensional nuisance parameter.\nIn a related paper, Ma and Kosorok [28] obtained some theoretical results when the bootstrap weights are assumed to be i.i.d. There is a crucial\ndifference between their work and ours: They treated the bootstrap estimator as the regular weighted estimator and used the unconditional arguments\nrather than the usual conditional arguments as we employ in this paper.\nNote that the i.i.d. assumption rules out all interesting bootstrap schemes\nconsidered in this paper, and their theoretical approach cannot be extended\nto obtain our results. Indeed, they stated in the paper that the independence\nassumption makes their proofs easier and the relaxation to the dependent\nweights appears to be quite difficult. Another related work is the piggyback\nbootstrap [11], which is invented\nsolely to draw inferences for the functional\n\u221a\nparameter \u03b7 when it is n-estimable. The piggyback bootstrap is not the\nstandard bootstrap and relies on a valid random draw from the asymptotic\ndistribution of the estimate of \u03b8, which is hard to estimate in general. Other\nrelated work includes interesting results on bootstrap (in)-consistency in\nnonparametric estimation; see [23, 35, 41]. An m out of n bootstrap was\ndeveloped for nonstandard M -estimation with nuisance parameters in parametric models [25].\nSection 2 provides the necessary background of M -estimation in semiparametric models. Our main results, including the bootstrap consistency\ntheorem, are presented in Section 3. Sections 4 and 5 discuss how to verify\nvarious technical conditions needed for the main results. Section 6 illustrates\nthe applications of our main results in three examples. Section 7 contains the\nproof of the main results in Section 3. Some useful lemmas and additional\nproofs are postponed to Appendix.\n2. Background. We first introduce a paradigm for the semiparametric M -estimation [28, 42], which parallels the efficient influence function\nparadigm used for the MLEs [where m(\u03b8, \u03b7) is the log likelihood]. Next,\nwe present the model assumptions needed for the remainder of the paper,\nand, finally, we review some known results on the asymptotic distribution of\nsemiparametric M -estimators, which are needed in studying the asymptotic\nproperties of the bootstrap.\nLet\n\u2202\n\u2202\n,\nm(\u03b8, \u03b7) and m2 (\u03b8, \u03b7)[h] = m(\u03b8, \u03b7(t))\nm1 (\u03b8, \u03b7) =\n\u2202\u03b8\n\u2202t\nt=0\nwhere h is a \"direction\" along which \u03b7(t) \u2208 H approaches \u03b7 as t \u2192 0, running\nthrough some index set H \u2286 L02 (P\u03b8,\u03b7 ). Similarly, we also define\nm11 (\u03b8, \u03b7) =\n\n\u2202\n\u2202\nm1 (\u03b8, \u03b7) and m12 (\u03b8, \u03b7)[h] = m1 (\u03b8, \u03b7(t))\n\u2202\u03b8\n\u2202t\n\nt=0\n\n,\n\n\f5\n\nSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n\u2202\n\u2202\nm2 (\u03b8, \u03b7)[h] and m22 (\u03b8, \u03b7)[h, g] = m2 (\u03b8, \u03b72 (t))[h]\n\u2202\u03b8\n\u2202t\nwhere h, g \u2208 H and (\u2202/\u2202t)\u03b72 (t)|t=0 = g. Define\nm21 (\u03b8, \u03b7)[h] =\n\nt=0\n\n,\n\nm2 (\u03b8, \u03b7)[H] = (m2 (\u03b8, \u03b7)[h1 ], . . . , m2 (\u03b8, \u03b7)[hd ])\u2032 ,\n\nm22 [H, h] = (m22 (\u03b8, \u03b7)[h1 , h], . . . , m22 (\u03b8, \u03b7)[hd , h])\u2032 ,\nwhere H = (h1 , . . . , hd ) and hj \u2208 H for j = 1, . . . , d. Assume there exists an\nH \u2020 (\u03b8, \u03b7) = (h\u20201 (\u03b8, \u03b7), . . . , h\u2020d (\u03b8, \u03b7))\u2032 ,\n\nwhere each h\u2020j (\u03b8, \u03b7) \u2208 H, such that for any h \u2208 H\n(4)\n\nE\u03b8,\u03b7 {m12 (\u03b8, \u03b7)[h] \u2212 m22 (\u03b8, \u03b7)[H \u2020 , h]} = 0.\n\nFollowing the idea of the efficient score function, we define the function\nm(\u03b8,\ne \u03b7) = m1 (\u03b8, \u03b7) \u2212 m2 (\u03b8, \u03b7)[H \u2020 (\u03b8, \u03b7)].\n\nWe assume that the observed data are from the probability space (X , A, PX ),\nand that\n(5)\n\nPX m(\u03b8\ne 0 , \u03b70 ) = 0,\n\nR\nwhere PX f is the customary operator notation defined as f dPX . The\nassumption (5) is common in semiparametric M -estimation [28, 42] and\nusually holds by the model specifications, for example, the semiparametric\nregression models with \"panel count data\" [42]. In particular, when m(\u03b8, \u03b7) =\nlog lik(\u03b8, \u03b7), (5) trivially holds and m(\u03b8,\ne \u03b7) becomes the well studied efficient\nb \u03b7b) is assumed\nscore function for \u03b8 in semiparametric models, see [4]. Since (\u03b8,\nPn\nb \u03b7b) satisfies\nto be the maximizer of i=1 m(\u03b8, \u03b7)(Xi ), (\u03b8,\n(6)\n\nPn\n\nb \u03b7b) = 0,\nPn m(\ne \u03b8,\n\nwhere Pn f denotes\ni=1 f (Xi )/n. The theory developed in this paper is\nb \u03b7b) is not the exact maximizer.\ngeneral enough to deal with the case that (\u03b8,\nInstead of (6), we only assume the following \"nearly-maximizing\" condition\n(7)\n\nb \u03b7b) = oo (n\u22121/2 ),\nPn m(\ne \u03b8,\nPX\n\nwhere the superscript \"o\" denotes the outer probability.\nThroughout the rest of the paper, we use the shortened notation H0\u2020 =\nb \u03b7b). For a probability space (\u03a9, A, P )\nH \u2020 (\u03b80 , \u03b70 ), m\ne 0 = m(\u03b8\ne 0 , \u03b70 ) and m\nb = m(\ne \u03b8,\nand a map T : \u03a9 7\u2192 R\u0304 that need not be measurable, the notation E o T , OPo (1),\nand ooP (1) represent the outer expectation of T w.r.t. P , bounded and converging to zero in outer probability, respectively. More precise definitions\ncan be found on page 6 of [38]. Let V \u22972 represent V V \u2032 for any vector V .\nDefine x \u2228 y (x \u2227 y) to be the maximum (minimum) value of x and y.\n\n\f6\n\nG. CHENG AND J. Z. HUANG\n\nWe now state some general conditions that will be used throughout the\nwhole paper. We assume that the true value \u03b80 of the Euclidean parameter\nis an interior point of the compact set \u0398. Define\n(8) A = PX {(\u2202/\u2202\u03b8)|\u03b8=\u03b80 m(\u03b8,\ne \u03b70 )} = PX {m11 (\u03b80 , \u03b70 ) \u2212 m21 (\u03b80 , \u03b70 )[H0\u2020 ]},\n\n(9) B = Var{m\ne 0 (X)} = PX [{m1 (\u03b80 , \u03b70 ) \u2212 m2 (\u03b80 , \u03b70 )[H0\u2020 ]}\u22972 ].\n\nI. Positive information condition: the matrices A and B are both nonsingular.\nCondition I above is used to ensure the nonsingularity of the asymptotic\nb which will be shown to be A\u22121 B(A\u22121 )\u2032 ; see Proposition 1.\nvariance of \u03b8,\n\u221a\nFor the empirical process Gn = n(Pn \u2212PX ), denote its norm with respect\nto a function class Fn as kGn kFn = supf \u2208Fn |Gn f |. For any fixed \u03b4n > 0,\ndefine a class of functions Sn as\n\u001b\n\u001a\nm(\u03b8\ne 0 , \u03b7) \u2212 m(\u03b8\ne 0 , \u03b70 )\n: k\u03b7 \u2212 \u03b70 k \u2264 \u03b4n\n(10)\nSn \u2261 Sn (\u03b4n ) =\nk\u03b7 \u2212 \u03b70 k\nand a shrinking neighborhood of (\u03b80 , \u03b70 ) as\n(11)\n\nCn \u2261 Cn (\u03b4n ) = {(\u03b8, \u03b7) : k\u03b8 \u2212 \u03b80 k \u2264 \u03b4n , k\u03b7 \u2212 \u03b70 k \u2264 \u03b4n }.\n\nThe next two conditions S1 and S2 imply that the empirical processes indexed by m(\u03b8,\ne \u03b7) are well behaved and m(\u03b8,\ne \u03b7) is smooth enough around\n(\u03b80 , \u03b70 ).\nS1. Stochastic equicontinuity condition: for any \u03b4n \u2192 0,\nkGn kSn = OPo X (1)\n\n(12)\nand\n(13)\n\nGn (m(\u03b8,\ne \u03b7) \u2212 m(\u03b8\ne 0 , \u03b7)) = OPo X (k\u03b8 \u2212 \u03b80 k)\n\nfor (\u03b8, \u03b7) \u2208 Cn .\n\nS2. Smoothness condition:\n(14)\n\nPX (m(\u03b8,\ne \u03b7) \u2212 m\ne 0 ) = A(\u03b8 \u2212 \u03b80 ) + O(k\u03b8 \u2212 \u03b80 k2 \u2228 k\u03b7 \u2212 \u03b70 k2 )\n\nfor (\u03b8, \u03b7) in some neighborhood of (\u03b80 , \u03b70 ).\nFor any fixed \u03b8, define\n\n\u03b7b\u03b8 = arg sup Pn m(\u03b8, \u03b7).\n\u03b7\u2208H\n\nThe next condition says that \u03b7b\u03b8 should be close to \u03b70 if \u03b8 is close to \u03b80 .\n\nS3. Convergence rate condition: there exists a \u03b3 \u2208 (1/4, 1/2] such that\n(15)\n\nkb\n\u03b7\u03b8e \u2212 \u03b70 k = OPo X (k\u03b8e \u2212 \u03b80 k \u2228 n\u2212\u03b3 )\n\ne\nfor any consistent \u03b8.\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n7\n\nThe above range requirement of \u03b3 is always satisfied for regular semiparametric models; see Section 3.4 of [38]. Verifications of conditions S1\u2013S3 will\nbe discussed in Sections 4 and 5, and illustrated with examples in Section 6.\nThe following proposition summarizes a known result on the the asymptotic normality of the semiparametric M -estimator \u03b8b [22, 28, 42], which\nplays an important role in proving bootstrap consistency in next section.\n\nb \u03b7b)\nProposition 1. Suppose that conditions I, S1\u2013S3 hold and that (\u03b8,\nsatisfies (7). If \u03b8b is consistent, then\n\u221a\n\u221a\n(16)\nn(\u03b8b \u2212 \u03b80 ) = \u2212 nA\u22121 Pn m\ne 0 + ooPX (1).\nConsequently,\n(17)\n\n\u221a\n\nd\n\nn(\u03b8b \u2212 \u03b80 ) \u2212\u2192 N (0, \u03a3),\n\nwhere \u03a3 \u2261 A\u22121 B(A\u22121 )\u2032 , A and B are given in (8) and (9), respectively.\nWe assume consistency of \u03b8b in Proposition 1. The consistency can usually\nbe guaranteed under the following \"well-separated\" condition\n(18)\n\nPX m(\u03b80 , \u03b70 ) > sup PX m(\u03b8, \u03b7)\n(\u03b8,\u03b7)\u2208G\n/\n\nfor any open set G \u2282 \u0398 \u00d7 H containing (\u03b80 , \u03b70 ), see Theorem 5.7 in [39]. For\nmaximum likelihood estimation, that is, m(\u03b8, \u03b7) = log lik(\u03b8, \u03b7), it is easy to\nsee that A = \u2212B and \u03a3 = B \u22121 , and thus \u03a3\u22121 becomes the efficient information matrix.\nRemark 1.\n(19)\n\nb of \u03a3, we have\nGiven any consistent estimator \u03a3\n\u221a \u22121/2\nd\nb\nn\u03a3\n(\u03b8b \u2212 \u03b80 ) \u2212\u2192 N (0, I)\n\nb can be\nby Proposition 1 and Slutsky's theorem. In practice, a consistent \u03a3\nobtained via either the observed profile information approach [31] or the\nprofile sampler approach [24].\n3. Main results: Bootstrap consistency. In this section, we establish the\nconsistency of bootstrapping \u03b8 under general conditions in the framework\nof semiparametric M -estimation. Define\nn\nX\n\u2217\nWni f (Xi ),\nPn f = (1/n)\ni=1\n\nwhere Wni 's are the bootstrap weights defined on the probability space\n(W, \u03a9, PW ). In view of (3), the bootstrap estimator can be rewritten as\n(20)\n\n(\u03b8b\u2217 , \u03b7b\u2217 ) = arg sup P\u2217n m(\u03b8, \u03b7).\n\u03b8\u2208\u0398,\u03b7\u2208H\n\n\f8\n\nG. CHENG AND J. Z. HUANG\n\nThe definition of (\u03b8b\u2217 , \u03b7b\u2217 ), that is, (20), implies that\nP\u2217n m(\ne \u03b8b\u2217 , \u03b7b\u2217 ) = 0.\n\n(21)\n\nSimilar to (7), we weaken (21) to the following \"nearly-maximizing\" condition\n(22)\nP\u2217 m(\ne \u03b8b\u2217 , \u03b7b\u2217 ) = oo (n\u22121/2 ),\nn\n\nPXW\n\nwhere PXW is a probability measure on a product space that we will formally\ndefine later.\nThe bootstrap weights Wni 's are assumed to belong to the class of exchangeable bootstrap weights introduced in [33]. Specifically, they satisfy:\n\nW1. The vector Wn = (Wn1 , . . . , Wnn )\u2032 is exchangeable for all n = 1, 2, . . . ,\nthat is, for any permutation \u03c0 = (\u03c01 , . . . , \u03c0n ) of (1, 2, . . . , n), the joint\ndistribution of \u03c0(Wn ) = P\n(Wn\u03c01 , . . . , Wn\u03c0n )\u2032 is the same as that of Wn .\nW2. Wni \u2265 0 for all n, i and ni=1 Wni = n for all n.\nW3. For some positive\nconstant C < \u221e, lim supn\u2192\u221e kWn1 k2,1 \u2264 C, where\nR\u221ep\nkWn1 k2,1 = 0\nPW (Wn1 \u2265 u) du.\nW4. lim\u03bb\u2192\u221e lim supn\u2192\u221e supt\u2265\u03bb t2 PW (Wn1 > t) = 0.\nP\nPW 2\nW5. (1/n) ni=1 (Wni \u2212 1)2 \u2212\u2192\nc > 0.\nThe bootstrap weights corresponding to Efron's nonparametric bootstrap\nsatisfy W1\u2013W5. Another important class of bootstrap whose weights satisfy\nW1\u2013W5 is the multiplier bootstrap in which Wni = \u03c9i /\u03c9\u0304n and (\u03c91 , . . . , \u03c9n )\ni.i.d.\nare i.i.d. positive r.v.s with k\u03c91 k2,1 < \u221e. By taking \u03c9i \u223c Exp(1), we obtain\nthe Bayesian bootstrap of [34]. The multiplier bootstrap is often thought to\nbe a smooth alternative to the nonparametric bootstrap [27]. In general,\nconditions W3\u2013W5 are easily satisfied under some moment conditions on\nWni ; see Lemma 3.1 of [33]. The sampling schemes that satisfy conditions\nW1\u2013W5 include the double bootstrap, the urn bootstrap and the grouped or\ndelete-h Jackknife [13]; see [33]. The value of c in W5 is independent of n and\ndepends on the resampling method, for example,\n\u221a c = 1 for the nonparametric\nbootstrap and Bayesian bootstrap, and c = 2 for the double bootstrap.\nThere exist two sources of randomness for the bootstrapped quantity, for\nexample, \u03b8b\u2217 and \u03b7b\u2217 : one comes from the observed data; another comes from\nthe resampling done by the bootstrap, that is, randomness in Wni 's. Therefore, in order to rigorously state our theoretical results for the bootstrap, we\nneed to specify relevant probability spaces and define the related stochastic\norders.\nWe view Xi as the ith coordinate projection from the canonical probability space (X \u221e , A\u221e , PX\u221e ) onto the ith copy of X . For the joint randomness\ninvolved, the product probability space is defined as\n(X \u221e , A\u221e , PX\u221e ) \u00d7 (W, \u03a9, PW ) = (X \u221e \u00d7 W, A\u221e \u00d7 \u03a9, PXW ).\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n9\n\nIn this paper, we assume that the bootstrap weights Wni 's are independent\nof the data Xi 's, thus PXW = PX\u221e \u00d7 PW . We write PX\u221e as PX for simplicity\no\nthereafter. Define EXW\nas the outer expectation w.r.t. PXW . The notation\no\no\nEW |X , EX and EW are defined similarly.\nGiven a real-valued function \u2206n defined on the above product probability space, for example, \u03b8b\u2217 , we say that \u2206n is of an order ooPW (1) in PXo probability if for any \u03b5, \u03b4 > 0,\n\n(23)\n\no\nPXo {PW\n|X (|\u2206n | > \u03b5) > \u03b4} \u2212\u2192 0\n\nas n \u2192 \u221e,\n\nand that \u2206n is of an order OPo W (1) in PXo -probability if for any \u03b4 > 0, there\nexists a 0 < M < \u221e such that\n(24)\n\no\nPXo {PW\n|X (|\u2206n | \u2265 M ) > \u03b4} \u2212\u2192 0\n\nas n \u2192 \u221e.\n\nGiven a function \u0393n defined only on (X \u221e , A\u221e , PX\u221e ), if it is of an order\nooPX (1) [OPo X (1)], then it is also of an order ooPXW (1) [OPo XW (1)] based on the\nfollowing argument:\no\no\nPXW\n(|\u0393n | > \u03b5) = EXW\n1{|\u0393n | > \u03b5} = EX EW |X 1{|\u0393n | > \u03b5}o\n\n= EX 1{|\u0393n | > \u03b5}o = PXo {|\u0393n | > \u03b5},\n\nwhere the third equation holds since \u0393n does not depend on the bootstrap\nweight. More results on transition of various stochastic orders are given in\nLemma 3 of the Appendix. Such results are used repeatedly in proving our\nbootstrap consistency theorem.\nTo establish the bootstrap consistency, we need some additional conditions. The first condition is the measurability condition, denoted as M (PX ).\nWe say a class of functions F \u2208 M (PX ) if F possesses enough measurability so that Pn can be randomized, that is, we can replace (\u03b4Xi \u2212 PX ) by\n(Wni \u2212 1)\u03b4Xi , and Fubini's theorem can be used freely. The detailed description for M (PX ) is spelled out in [17] and also given in the Appendix of this\npaper. Define T = {m(\u03b8,\ne \u03b7) : k\u03b8 \u2212 \u03b80 k + k\u03b7 \u2212 \u03b70 k \u2264 R} for some R > 0. For\nthe rest of the paper, we assume T \u2208 M (PX ).\nThe second class of conditions parallels conditions S1\u2013S3 used for obtaining asymptotic normality of \u03b8b and is only slightly stronger. Thus, the\nbootstrap consistency for \u03b8 is almost automatically guaranteed once \u03b8b is\nshown to be asymptotically normal. Let Sn (x) be the envelop function of\nthe class Sn = Sn (\u03b4n ) defined in (10), that is,\nSn (x) =\n\nsup\nk\u03b7\u2212\u03b70 k\u2264\u03b4n\n\nm(\u03b8\ne 0 , \u03b7) \u2212 m\ne0\n.\nk\u03b7 \u2212 \u03b70 k\n\nThe next condition controls the tail of this envelop function.\n\n\f10\n\nG. CHENG AND J. Z. HUANG\n\nSB1. Tail probability condition:\nlim lim sup sup t2 PXo (Sn (X1 ) > t) = 0\n\n(25)\n\n\u03bb\u2192\u221e\n\nn\u2192\u221e t\u2265\u03bb\n\nfor any sequence \u03b4n \u2192 0.\nLet \u1e6a = {\u2202 m(\u03b8,\ne \u03b7)/\u2202\u03b8 : (\u03b8, \u03b7) \u2208 Cn }, where Cn = Cn (\u03b4n ) is defined in (11).\n\nSB2. We assume that \u1e6a \u2208 M (PX ) \u2229 L2 (PX ) and that \u1e6a is P -Donsker.\n\nCondition SB2 ensures that the size of the function\n\u1e6a is reasonable\n\u221a class\n\u2217\n\u2217\nso that the bootstrapped empirical processes Gn \u2261 n(Pn \u2212 Pn ) indexed by\nT \u0307 has a limiting process conditional on the observations; see Theorem 2.2\nin [33].\nFor any fixed \u03b8, define\n\u03b7b\u03b8\u2217 = arg max P\u2217n m(\u03b8, \u03b7).\n\u03b7\u2208H\n\nThe next condition says that\n\n\u03b7b\u03b8\u2217\n\nshould be close to \u03b70 if \u03b8 is close to \u03b80 .\n\nSB3. Bootstrap convergence rate condition: there exists a \u03b3 \u2208 (1/4, 1/2]\nsuch that\n(26)\n\nkb\n\u03b7\u03b8\u2217e \u2212 \u03b70 k = OPo W (k\u03b8e \u2212 \u03b80 k \u2228 n\u2212\u03b3 )\n\nin PXo -probability\n\no\n\nPXW\nfor any \u03b8e \u2212\u2192\n\u03b80 .\n\nVerifications of conditions SB1\u2013SB2 will be discussed in Section 4. Two\ngeneral theorems are given in Section 5 to aid verification of condition SB3.\nNow we are ready to present\nresults. Theorem 1 below says that\n\u221aour main\n\u2217\nb\nb conditional on the observations,\nthe bootstrap distribution of ( n/c)(\u03b8 \u2212 \u03b8),\n\u221a\nasymptotically imitates the unconditional distribution of n(\u03b8b \u2212 \u03b80 ). Let\nPW |Xn denote the conditional distribution given the observed data Xn .\nTheorem 1. Suppose that \u03b8b and \u03b8b\u2217 satisfy (7) and (22), respectively.\no\nPW\nPX\n\u03b80 in PXo -probability. In addition, assume\nAssume that \u03b8b \u2212\u2192\n\u03b80 and \u03b8b\u2217 \u2212\u2192\nthat conditions I, S1\u2013S3, SB1\u2013SB3 and W1\u2013W5 hold. We have that\nk\u03b8b\u2217 \u2212 \u03b80 k = OPo W (n\u22121/2 )\n\n(27)\n\nin PXo -probability. Furthermore,\n\u221a b\u2217 b\n(28)\nn(\u03b8 \u2212 \u03b8) = \u2212A\u22121 G\u2217n m\ne 0 + ooPW (1)\n\nin PXo -probability. Consequently,\n\u221a\nb \u2264 x) \u2212 P (N (0, \u03a3) \u2264 x)| = oo (1),\nsup |PW |Xn (( n/c)(\u03b8b\u2217 \u2212 \u03b8)\n(29)\nPX\nx\u2208Rd\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n11\n\nwhere \"\u2264\" is taken componentwise, c is given in W5 and \u03a3 \u2261 A\u22121 B(A\u22121 )\u2032\nwith A and B given in (8) and (9), respectively. Thus, we have\n(30)\n\no\n\u221a\n\u221a\nPX\nb \u2264 x) \u2212 PX ( n(\u03b8b \u2212 \u03b80 ) \u2264 x)| \u2212\u2192\n0.\nsup |PW |Xn (( n/c)(\u03b8b\u2217 \u2212 \u03b8)\n\nx\u2208Rd\n\nThe consistency assumption for \u03b8b\u2217 can be established by adapting the\nArgmax theorem, that is, Corollary 3.2.3 in [38]. Briefly, we need two conditions for accomplishing this. The first one is the \"well-separated\" condition\n(18). The second one is\n(31)\n\nsup\n(\u03b8,\u03b7)\u2208\u0398\u00d7H\n\nPo\n\nXW\n|P\u2217n m(\u03b8, \u03b7) \u2212 PX m(\u03b8, \u03b7)| \u2212\u2192\n0.\n\nBy the multiplier Glivenko\u2013Cantelli theorem, that is, Lemma 3.6.16 in [38],\nand (69) in the Appendix, we know that (31) holds if {m(\u03b8, \u03b7) : \u03b8 \u2208 \u0398, \u03b7 \u2208 H}\nis shown to be P -Glivenko\u2013Cantelli.\no\n\nRemark 2.\n\n(32)\n\nPX\nXW\nb \u2212\u2192\nb \u2217 P\u2212\u2192\n\u03a3 and \u03a3\n\u03a3, we have\nFor any consistent \u03a3\n\u221a\n\u2217\n\u22121/2\n\u2217\nb \u2264 x)\nb )\n(\u03b8b \u2212 \u03b8)\nsup |PW |Xn (( n/c)(\u03a3\n\nx\u2208Rd\n\no\n\u221a \u22121/2\nPX\nb\n\u2212 PX ( n\u03a3\n(\u03b8b \u2212 \u03b80 ) \u2264 x)| \u2212\u2192\n0\n\nby the arguments in proving Theorem 1, Slutsky's theorem and Lemma 3.\nb \u2217 is the block jackknife proposed\nA possible candidate for the consistent \u03a3\nin [29].\nRemark 3. Our arguments in proving Theorem 1 can also be used to\nimprove the remainder term in (28) from \"ooPW (1) in PXo -probability\" to\n\"OPo W (n\u22122\u03b3+1/2 ) in PXo -probability\" if we strengthen the \"nearly maximizing\" condition (22) to the exactly maximizing condition (21). A similar result\nholds in Proposition 1 where the remainder term ooPX (1) in (16) can be improved to OPX (n\u22122\u03b3+1/2 ) if (7) is strengthened to (6). It is interesting to\nnote that the rate of convergence of the remainder term depends on how accurately\n\u221a the nuisance function parameter \u03b7 can be estimated. In particular,\nif \u03b7 is n-estimable, then the remainder is of the order of O(n\u22121/2 ).\nThe distribution consistency result of the bootstrap estimator \u03b8b\u2217 proven in\n(30) can be used to prove the consistency of a variety of bootstrap confidence\nsets, that is, percentile, hybrid and t types.\n\u2217 \u2208 Rd\nA lower \u03b1th quantile of bootstrap distribution is any quantity \u03c4n\u03b1\n\u2217 = inf{\u03b5 : P\nb\u2217\nsatisfying \u03c4n\u03b1\nW |Xn (\u03b8 \u2264 \u03b5) \u2265 \u03b1}, where \u03b5 is an infimum over the\n\n\f12\n\nG. CHENG AND J. Z. HUANG\n\ngiven set only if there does not exist a \u03b51 < \u03b5 in Rd such that PW |Xn (\u03b8b\u2217 \u2264\n\u03b51 ) \u2265 \u03b1. Because of the assumed smoothness of the criterion function m(\u03b8, \u03b7)\n\u2217 )=\nin our setting, we can, without loss of generality, assume PW |Xn (\u03b8b\u2217 \u2264 \u03c4n\u03b1\n\u03b1. Due to the distribution consistency result proven in (30), we can approx\u2217 \u2212 \u03b8)/c.\nb\nThus,\nimate the \u03b1th quantile of the distribution of (\u03b8b \u2212 \u03b80 ) by (\u03c4n\u03b1\nwe define the percentile-type bootstrap confidence set as\n\u0014\n\u2217\n\u2217\n\u03c4n(1\u2212\u03b1/2)\n\u2212 \u03b8b\u0015\n\u03c4n(\u03b1/2)\n\u2212 \u03b8b\n, \u03b8b +\n.\nBCp (\u03b1) = \u03b8b +\nc\nc\n\u221a\nSimilarly, we can approximate the \u03b1th quantile of n(\u03b8b \u2212 \u03b80 ) by \u03ba\u2217n\u03b1 , where\n\u221a\nb that is,\n\u03ba\u2217n\u03b1 is the \u03b1th quantile of the hybrid quantity ( n/c)(\u03b8b\u2217 \u2212 \u03b8),\n\u221a\n\u2217\n\u2217\nb\nb\nPW |Xn (( n/c) \u00d7 (\u03b8 \u2212 \u03b8) \u2264 \u03ban\u03b1 ) = \u03b1. Thus, we define the hybrid-type bootstrap confidence set as\n\u0014\n\u03ba\u2217n(\u03b1/2) \u0015\n\u03ba\u2217n(1\u2212\u03b1/2)\n\u221a\n, \u03b8b \u2212 \u221a\nBCh (\u03b1) = \u03b8b \u2212\n.\nn\nn\n\n\u2217 and \u03ba\u2217 are not unique since \u03b8 is assumed to be a vector.\nNote that \u03c4n\u03b1\nn\u03b1\nWe now prove the consistency of the above bootstrap confidence sets by\nusing the arguments in Lemma 23.3 of [39]. First, it follows from (17) and\n(29) that, for any x \u2208 Rd ,\n\u221a\nPX ( n(\u03b8b \u2212 \u03b80 ) \u2264 x) \u2212\u2192 \u03a8(x),\n(33)\no\n\u221a\nPX\nb \u2264 x) \u2212\u2192\n(34)\n\u03a8(x),\nPW |Xn (( n/c)(\u03b8b\u2217 \u2212 \u03b8)\n\nwhere \u03a8(x) = P (N (0, \u03a3) \u2264 x). The quantile convergence theorem, that is,\nLemma 21.1 in [39], applied to (34) implies that \u03ba\u2217n\u03b1 \u2192 \u03a8\u22121 (\u03b1) almost\nsurely. When applying quantile convergence theorem, we use the almost sure\nrepresentation Theorem 2.19 in [39] and argue along subsequences. Then\n\u221a\nthe Slutsky's lemma implies that n(\u03b8b \u2212 \u03b80 ) \u2212 \u03ba\u2217n(\u03b1/2) weakly converges to\nN (0, \u03a3) \u2212 \u03a8\u22121 (\u03b1/2). Thus,\n\u0012\n\u03ba\u2217n(\u03b1/2) \u0013\n\u221a\nb\n= PXW ( n(\u03b8b \u2212 \u03b80 ) \u2265 \u03ba\u2217n(\u03b1/2) )\nPXW \u03b80 \u2264 \u03b8 \u2212 \u221a\nn\n\u2192 PXW (N (0, \u03a3) \u2265 \u03a8\u22121 (\u03b1/2))\n= 1 \u2212 \u03b1/2.\n\nThis argument yields the consistency of the hybrid-type bootstrap confidence\nset, that is, (36) below, and can also be applied to justify the percentiletype bootstrap confidence set, that is, (35) below. The following Corollary\n1 summarizes the above discussion.\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\nCorollary 1.\n\n13\n\nUnder the conditions in Theorem 1, we have\n\n(35)\n\nPXW (\u03b80 \u2208 BCp (\u03b1)) \u2212\u2192 1 \u2212 \u03b1,\n\n(36)\n\nPXW (\u03b80 \u2208 BCh (\u03b1)) \u2212\u2192 1 \u2212 \u03b1\n\nas n \u2192 \u221e.\nIt is well known that the above bootstrap confidence sets can be computed\neasily through routine bootstrap sampling.\nInvestigating the consistency of the bootstrap variance estimator is also\nof great interest. However, the usual sufficient condition for moment consistency, that is, uniform integrability condition, becomes very hard to verify\ndue to the existence of an infinite-dimensional parameter \u03b7. An alternative resampling method to obtain the variance estimator in semiparametric\nmodels is the block jackknife approach, which was proposed and theoretically justified in [29]. We do not pursue this topic further in this paper.\nb \u2217 and \u03a3\nb are availRemark 4. Provided consistent variance estimators \u03a3\nable, we can define the t-type bootstrap confidence set as\n\u0014\n\n\u0015\nb 1/2 \u03c9 \u2217\nb 1/2 \u03c9 \u2217\n\u03a3\n\u03a3\nn(1\u2212\u03b1/2) b\nn(\u03b1/2)\n\u221a\n\u221a\n,\u03b8 \u2212\n,\nn\nn\n\u221a\n\u2217 satisfies P\nb \u2264 \u03c9 \u2217 ) = \u03b1. By applying\nb \u2217 \u22121/2 (\u03b8b\u2217 \u2212 \u03b8)\nwhere \u03c9n\u03b1\nW |Xn (( n/c)(\u03a3 )\nn\u03b1\nagain the arguments in Lemma 23.3 of [39] to (19) and (32), we can prove\nthat\nBCt (\u03b1) = \u03b8b \u2212\n\nPXW (\u03b80 \u2208 BCt (\u03b1)) \u2212\u2192 1 \u2212 \u03b1\nas n \u2192 \u221e.\n4. Verifications of conditions S1, S2 and SB1, SB2.\n4.1. Verifications of conditions S1 and S2. The continuity modulus condition (12) in S1 can be checked via one of the following two approaches. The\no kG k\nfirst approach is to show the boundedness of EX\nn Sn by using Lemma\n3.4.2 in [38]. The second approach is to calculate the bracketing entropy\nnumber of Sn and apply Lemma 5.13 in [40] if L2 -norm is used on the nuisance parameter. As for (13), we can verify it easily if we can show that the\nclass of functions {(\u2202/\u2202\u03b8)m(\u03b8,\ne \u03b7) : (\u03b8, \u03b7) \u2208 Cn } is P -Donsker.\nNext, we discuss how to verify the smoothness condition S2. We first write\nPX (m(\u03b8,\ne \u03b7) \u2212 m\ne 0 ) as the sum of PX (m(\u03b8,\ne \u03b7) \u2212 m(\u03b8\ne 0 , \u03b7)) and PX (m(\u03b8\ne 0 , \u03b7) \u2212\n\n\f14\n\nG. CHENG AND J. Z. HUANG\n\nm\ne 0 ). We apply the Taylor expansion to obtain\nPX (m(\u03b8,\ne \u03b7) \u2212 m(\u03b8\ne 0 , \u03b7))\n\n= PX {m11 (\u03b80 , \u03b7) \u2212 m21 (\u03b80 , \u03b7)[H \u2020 (\u03b80 , \u03b7)]}(\u03b8 \u2212 \u03b80 ) + O(k\u03b8 \u2212 \u03b80 k2 )\n= A(\u03b8 \u2212 \u03b80 ) + (\u03b8 \u2212 \u03b80 )O(k\u03b7 \u2212 \u03b70 k) + O(k\u03b8 \u2212 \u03b80 k2 ),\n\nwhere A is defined in (8), the first and second equality follows from the\nTaylor expansion of \u03b8 7\u2192 PX m(\u03b8,\ne \u03b7) around \u03b80 and\n\u03b7 7\u2192 PX {m11 (\u03b80 , \u03b7) \u2212 m21 (\u03b80 , \u03b7)[H \u2020 (\u03b80 , \u03b7)]}\n\naround \u03b70 , respectively. By applying the second-order Taylor expansion to\n\u03b7 7\u2192 PX m(\u03b8\ne 0 , \u03b7) around \u03b70 and considering (4), we can show that P (m(\u03b8\ne 0 , \u03b7)\u2212\nm\ne 0 ) = O(k\u03b7 \u2212 \u03b70 k2 ). In summary, condition S2 usually holds in models where\nthe map \u03b7 7\u2192 m(\u03b8\ne 0 , \u03b7) is smooth in the sense that the Fr\u00e9chet derivative of\n\u03b7 7\u2192 PX ((\u2202/\u2202\u03b8)m(\u03b8\ne 0 , \u03b7)) around \u03b70 and the second order Fr\u00e9chet derivative\nof \u03b7 7\u2192 PX m(\u03b8\ne 0 , \u03b7) around \u03b70 are bounded as discussed above.\n\n4.2. Verifications of conditions SB1 and SB2. We can verify\ncondition SB1 by showing either Sn (x) is uniformly bounded, that is,\nlim supn\u2192\u221e Sn (x) \u2264 M < \u221e for every x \u2208 X , or more generally,\nlim supn\u2192\u221e E[{Sn (X1 )}2+\u03b4 ] < \u221e for some \u03b4 > 0. That the moment condition implies condition SB1 follows from the Chebyshev's inequality. In our\nexamples in Section 6, the uniformly boundedness condition is usually satisfied. Hence, we focus on how to show Sn (x) is uniformly bounded here.\nBy the Taylor expansion in a Banach space, we can write m(\u03b8\ne 0 , \u03b7) \u2212 m\ne0 =\nD\u03b7e[\u03b7 \u2212 \u03b70 ], where \u03b7e lies on the line segment between \u03b7 and \u03b70 , and D\u03be [h] is\nthe Fr\u00e9chet derivative of \u03b7 7\u2192 m(\u03b8\ne 0 , \u03b7) at \u03be along the direction h. Since we\nrequire k\u03b7 \u2212 \u03b70 k \u2264 \u03b4n \u2192 0, the bounded Fr\u00e9chet derivative at \u03b70 will imply\nthat Sn (x) is uniformly bounded. The method in verifying (13) of condition\nS1 can be applied to check condition SB2; see the discussion in the previous\nsubsection.\n5. Convergence rates of bootstrap estimate of functional parameter. In\nthis section, we present two general theorems for calculating the convergence\nrate of the bootstrap estimate of the functional parameter. These results can\nbe applied to verify condition SB3. Condition S3 can also be verified based on\nthese theorems by assuming the weights Wni = 1. Note that both theorems\nextend general results on M -estimators [31, 38] to bootstrap M -estimators\nand are also of independent interest.\nSeparate treatments are given to the\n\u221a\ncases that the estimate\n\u03b7\nhas\nn\nconvergence\nrate, that is, Section 5.1, and\n\u221a\nhas slower than n rate, that is, Section 5.2.\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n15\n\n5.1. Root-n rate. We consider a collection of measurable objective functions x 7\u2192 k(\u03b8, \u03b7)[g](x) indexed by the parameter (\u03b8, \u03b7) \u2208 \u0398 \u00d7 H and an arbitrary index set g \u2208 G. For example, k(\u03b8, \u03b7)[g] can be the score function for\n\u03b7 given any fixed \u03b8 indexed by g \u2208 G. Define\nUn\u2217 (\u03b8, \u03b7)[g] = P\u2217n k(\u03b8, \u03b7)[g],\nUn (\u03b8, \u03b7)[g] = Pn k(\u03b8, \u03b7)[g],\nU (\u03b8, \u03b7)[g] = PX k(\u03b8, \u03b7)[g].\nWe assume that the maps g 7\u2192 Un\u2217 (\u03b8, \u03b7)[g], g 7\u2192 Un (\u03b8, \u03b7)[g] and g 7\u2192 U (\u03b8, \u03b7)[g]\nare uniformly bounded, so that Un\u2217 , Un and U are viewed as maps from the\nparameter set \u0398 \u00d7 H into l\u221e (G). The following conditions are assumed in\nTheorem 2 below:\n(37)\n\n{k(\u03b8, \u03b7)[g] : k\u03b8 \u2212 \u03b80 k + k\u03b7 \u2212 \u03b70 k \u2264 \u03b4, g \u2208 G} \u2208 M (PX ) \u2229 L2 (PX )\n\nand is P -Donsker for some \u03b4 > 0,\n(38)\n\nsup PX {k(\u03b8, \u03b7)[g] \u2212 k(\u03b80 , \u03b70 )[g]}2 \u2192 0\n\nas k\u03b8 \u2212 \u03b80 k + k\u03b7 \u2212 \u03b70 k \u2192 0.\n\ng\u2208G\n\nLet\nDn =\n\n\u001a\n\nk(\u03b8, \u03b7)[g] \u2212 k(\u03b80 , \u03b70 )[g]\n\u221a\n\u221a\n: g \u2208 G, k\u03b8 \u2212 \u03b80 k + k\u03b7 \u2212 \u03b70 k \u2264 \u03b4n\n1 + nk\u03b8 \u2212 \u03b80 k + nk\u03b7 \u2212 \u03b70 k\n\n\u001b\n\nand Dn (X) be the envelop function of the class of functions Dn . For any\nsequence \u03b4n \u2192 0, we assume that Dn (X) satisfies\nlim lim sup sup t2 PXo (Dn (X1 ) > t) = 0.\n\n(39)\n\n\u03bb\u2192\u221e\n\nn\u2192\u221e t\u2265\u03bb\n\nNow we consider the convergence rate of \u03b7b\u2217e satisfying:\n\u03b8\n\n(40)\n\no\n\ne \u03b7b\u2217 )[g] = O o (n\u22121/2 )\nUn\u2217 (\u03b8,\nPXW\n\u03b8e\n\nPXW\nfor any \u03b8e \u2212\u2192\n\u03b80 and g ranging over G. In Theorem 2 below, we will show\nthat \u03b7b\u2217e has the root-n convergence rate under conditions (37)\u2013(39).\n\u03b8\n\nTheorem 2. Suppose that U : \u0398 \u00d7 H 7\u2192 l\u221e (G) is Fr\u00e9chet differentiable\nat (\u03b80 , \u03b70 ) with bounded derivative U\u0307 : Rd \u00d7 lin H 7\u2192 l\u221e (G) such that the map\nU\u0307 (0, *) : lin H 7\u2192 l\u221e (G) is invertible with an inverse that is continuous on its\nrange. Furthermore, assume that (37)\u2013(39) hold, and that U (\u03b80 , \u03b70 ) = 0, then\n(41)\n\nkb\n\u03b7\u03b8\u2217e \u2212 \u03b70 k = OPo W (k\u03b8e \u2212 \u03b80 k \u2228 n\u22121/2 )\no\n\no\n\nPXW\nPXW\n\u03b70 .\nin PXo -probability, given that \u03b8e \u2212\u2192\n\u03b80 and \u03b7b\u2217e \u2212\u2192\n\u03b8\n\nThe proof of Theorem 2 is given in Appendix A.4.\n\n\f16\n\nG. CHENG AND J. Z. HUANG\n\n5.2. Slower\n\u221a than root-n rate. We next present a result that deals with\nslower than n convergence rate for the bootstrap M -estimate of the functional parameter. This result is so general that it can be applied to the sieve\nestimate of nuisance parameter [15]. The essence of the sieve method is that\na sequence of increasing spaces (sieves), that is, Hn , is employed to approximate the large parameter space, for example, H. In other words, for any\n\u03b7 \u2208 H, there exists a \u03c0n \u03b7 \u2208 Hn such that k\u03b7 \u2212 \u03c0n \u03b7k \u2192 0 as n \u2192 \u221e.\nNow, we consider the M -estimate \u03b7b\u03b8\u2217 \u2208 Hn satisfying\n\n(42)\n\nP\u2217n v(\u03b8, \u03b7b\u03b8\u2217 ) \u2265 P\u2217n v(\u03b8, \u03b7n )\n\nfor any \u03b8 \u2208 \u0398 and some \u03b7n \u2208 Hn ,\n\nwhere x 7\u2192 v(\u03b8, \u03b7)(x) is a measurable objective function. Let \".\" and \"&\"\ndenote greater than or smaller than, up to an universal constant. We assume\nthe following conditions hold for every \u03b4 > 0:\n(43)\n(44)\n(45)\n\no\nEX\n\nEX (v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n )) . \u2212d2 (\u03b7, \u03b7n ) + k\u03b8 \u2212 \u03b80 k2 ,\nsup\n\u03b8\u2208\u0398,\u03b7\u2208Hn ,k\u03b8\u2212\u03b80 k\u2264\u03b4,d(\u03b7,\u03b7n )\u2264\u03b4\n\no\nEXW\n\n|Gn (v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n ))| . \u03c8n (\u03b4),\n\nsup\n\u03b8\u2208\u0398,\u03b7\u2208Hn ,k\u03b8\u2212\u03b80 k\u2264\u03b4,d(\u03b7,\u03b7n )\u2264\u03b4\n\n|G\u2217n (v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n ))| . \u03c8n\u2217 (\u03b4).\n\nHere d2 (\u03b7, \u03b7n ) may be thought of as the square of a distance, for example, k\u03b7 \u2212 \u03b7n k2 , but our theorem is also true for any arbitrary function\n\u03b7 7\u2192 d2 (\u03b7, \u03b7n ).\nTheorem 3. Suppose that conditions (43)\u2013(45) hold. We assume (44)\n[and (45)] is valid for functions \u03c8n (\u03c8n\u2217 ) such that \u03b4 7\u2192 \u03c8n (\u03b4)/\u03b4\u03b1 [\u03b4 7\u2192\ne \u03b7b\u2217 ) satis\u03c8n\u2217 (\u03b4)/\u03b4\u03b1 ] is decreasing for some 0 < \u03b1 < 2. Then for every (\u03b8,\n\u03b8e\n\u2217\ne\nfying P (\u03b8 \u2208 \u0398, \u03b7b \u2208 Hn ) \u2192 1, we have\n\u03b8e\n\nd(b\n\u03b7\u03b8\u2217e, \u03b7n ) \u2264 OPo W (\u03b4n \u2228 k\u03b8e \u2212 \u03b80 k)\n\nin PXo -probability,\nfor any sequence\n\u221a 2 of positive numbers \u03b4n satisfying both\n\u221a 2\n\u2217\n\u03c8n (\u03b4n ) \u2264 n\u03b4n and \u03c8n (\u03b4n ) \u2264 n\u03b4n for large n.\nThe proof of Theorem 3 is given in Appendix A.5.\nIn application of Theorem 3, the parameter \u03b7n is taken to be some element\nin Hn that is very close to \u03b70 . When Hn = H, a natural choice for \u03b7n is \u03b70\nand we can directly use Theorem 3 to derive the convergence rate d(b\n\u03b7 \u2217e, \u03b70 )\n\u03b8\nas shown in the examples of Section 6. In general, \u03b7n may be taken as\nthe maximizer of the mapping \u03b7 7\u2192 PX v(\u03b80 , \u03b7) over Hn , the projection of\n\u03b70 onto Hn . Then we need to consider the approximation rate of the sieve\nspace Hn to H, that is, d(\u03b7n , \u03b70 ), since d(b\n\u03b7 \u2217e, \u03b70 ) \u2264 d(b\n\u03b7 \u2217e, \u03b7n ) + d(\u03b7n , \u03b70 ). The\n\u03b8\n\n\u03b8\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n17\n\napproximation rate d(\u03b7n , \u03b70 ) depends on the choices of sieves and is usually\nderived in the mathematical literature.\nNow we discuss verification of the nontrivial conditions (43)\u2013(45). The\nsmoothness condition for v(\u03b8, \u03b7), that is, (43), is implied by\n(46)\n(47)\n\nEX (v(\u03b8, \u03b7) \u2212 v(\u03b80 , \u03b7n )) . \u2212d2 (\u03b7, \u03b7n ) \u2212 k\u03b8 \u2212 \u03b80 k2 ,\n\nEX (v(\u03b8, \u03b7n ) \u2212 v(\u03b80 , \u03b7n )) & \u2212k\u03b8 \u2212 \u03b80 k2 .\n\nThe two conditions depict the quadratic behaviors of the criterion functions\n(\u03b8, \u03b7) 7\u2192 EX v(\u03b8, \u03b7) and \u03b8 7\u2192 EX v(\u03b8, \u03b7n ) around the maximum point (\u03b80 , \u03b7n )\nand \u03b80 , respectively. We next present one useful lemma for verifying the\ncontinuity modulus of (bootstrapped) empirical processes, that is, (44) and\n(45). Denote\n(48)\n\nV\u03b4 = {x 7\u2192 [v(\u03b8, \u03b7)(x) \u2212 v(\u03b8, \u03b7n )(x)] : d(\u03b7, \u03b7n ) \u2264 \u03b4, k\u03b8 \u2212 \u03b80 k \u2264 \u03b4}\n\nand define the bracketing entropy integral of V\u03b4 as\nZ \u03b4q\n(49)\nK(\u03b4, V\u03b4 , L2 (PX )) =\n1 + log N[*] (\u03b5, V\u03b4 , L2 (PX )) d\u03b5,\n0\n\nwhere log N[*] (\u03b4, A, d) is the \u03b4-bracketing entropy number for the class A\nunder the distance measure d.\nLemma 1. Suppose that the functions (x, \u03b8, \u03b7) 7\u2192 v\u03b8,\u03b7 (x) are uniformly\nbounded for (\u03b8, \u03b7) ranging over some neighborhood of (\u03b80 , \u03b7n ) and that\n(50)\n\nEX (v\u03b8,\u03b7 \u2212 v\u03b8,\u03b7n )2 . d2 (\u03b7, \u03b7n ) + k\u03b8 \u2212 \u03b80 k2 .\n\nThen condition (44) is satisfied for any functions \u03c8n such that\n\u0012\n\u0013\nK(\u03b4, V\u03b4 , L2 (PX ))\n\u221a\n\u03c8n (\u03b4) \u2265 K(\u03b4, V\u03b4 , L2 (PX )) 1 +\n(51)\n.\n\u03b42 n\nLet Vn (X) be the envelop function of the class V\u03b4n . If we further assume\nthat, for each sequence \u03b4n \u2192 0, the envelop functions Vn satisfies\n(52)\n\nlim lim sup sup t2 PXo (Vn (X1 ) > t) = 0,\n\n\u03bb\u2192\u221e\n\nn\u2192\u221e t\u2265\u03bb\n\nthen condition (45) is satisfied for any functions \u03c8n\u2217 such that\n\u0012\n\u0013\nK(\u03b4, V\u03b4 , L2 (PX ))\n\u221a\n\u03c8n\u2217 (\u03b4) \u2265 K(\u03b4, V\u03b4 , L2 (PX )) 1 +\n(53)\n.\n\u03b42 n\n\n\u221a\n\u221a 2\n\u2217\n. n\u03b42\nRemark 5. Note that the inequalities\n\u221a 2 \u03c8n (\u03b4) . n\u03b4 and \u03c8n (\u03b4)\nare equivalent to K(\u03b4, V\u03b4 , L2 (PX )) . n\u03b4 when we let \u03c8n and \u03c8n\u2217 be equal\nto the right-hand side of (51) and (53), respectively. Consequently, the convergence rate of \u03b7b\u2217e calculated in Theorem 3, that is, \u03b4n , is determined by\n\u03b8\nthe bracketing entropy integral of V\u03b4n .\n\n\f18\n\nG. CHENG AND J. Z. HUANG\n\nRemark 6. The assumptions of Lemma 1 are relaxable to great extent.\nFor example, we can drop the uniform bounded condition on the class of\nfunctions v(\u03b8, \u03b7) by using the \"Bernstein norm,\" that is, kf kP,B = (2P (e|f | \u2212\n1 \u2212 |f |))1/2 , instead of the L2 -norm. In some cases, the bracketing entropy\nintegral diverges at zero. Then we can change the limit of the integration\nin (49) from [0, \u03b4] to [a\u03b42 \u2227 \u03b4/3, \u03b4] for some small positive constant a, see\nLemma 3.4.3 and page 326 in [38].\n6. Examples. In this section, we apply the main results in Section 3\nto justify the bootstrap validity of drawing semiparametric inferences in\nthree examples of semiparametric models. In the Cox regression models with\ncensored data, we use the log-likelihood as the criterion function, while in the\npartially linear model, the least squares criterion is used. The M -estimate of\nthe nuisance functional parameters have different convergence rates in these\nexamples. Indeed, the advantages of using bootstrap approach in all of the\nthree examples were considered in the literature, for example, [14, 26]. This\nsection also serves the purpose of illustration on verification of the technical\nconditions used in the general results.\n6.1. Cox regression model with right censored data. In the Cox regression\nmodel, the hazard function of the survival time T of a subject with covariate\nZ is modeled as\n1\n(54) \u03bb(t|z) \u2261 lim P (t \u2264 T < t + \u2206|T \u2265 t, Z = z) = \u03bb(t) exp(\u03b8 \u2032 z),\n\u2206\u21920 \u2206\nwhere \u03bb is an unspecified baseline hazard function and \u03b8 is a regression\nvector. In this model, we are usually\nR y interested in \u03b8 while treating the cumulative hazard function \u03b7(y) = 0 \u03bb(t) dt as the nuisance parameter. The\nMLE for \u03b8 is proven to be semiparametric efficient and widely used in apb which corresponds to treating\nplications. Here we consider bootstrapping \u03b8,\nlog-likelihood as the criterion function m(\u03b8, \u03b7) in our general formulation.\nWith right censoring of survival time, the data observed is X = (Y, \u03b4, Z),\nwhere Y = T \u2227 C, C is a censoring time, \u03b4 = I{T \u2264 C}, and Z is a regression covariate belonging to a compact set Z \u2282 Rd . We assume that C is\nindependent of T given Z. The log-likelihood is obtained as\n(55)\n\nm(\u03b8, \u03b7) = \u03b4\u03b8 \u2032 z \u2212 exp(\u03b8 \u2032 z)\u03b7(y) + \u03b4 log \u03b7{y},\n\nwhere \u03b7{y} = \u03b7(y) \u2212 \u03b7(y\u2212) is a point mass that denotes the jump of \u03b7 at\npoint y. The parameter space H is restricted to a set of nondecreasing cadlag functions on the interval [0, \u03c4 ] with \u03b7(\u03c4 ) \u2264 M for some constant M . By\nsome algebra, we have\nm(\u03b8,\ne \u03b7)(x) = m1 (\u03b8, \u03b7) \u2212 m2 (\u03b8, \u03b7)[H \u2020 (\u03b8, \u03b7)]\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n= [\u03b4z \u2212 z exp(\u03b8 \u2032 z)\u03b7(y)]\n\u0014\nZ\n\u2212 \u03b4H \u2020 (\u03b8, \u03b7)(y) \u2212 exp(\u03b8 \u2032 z)\n\ny\n0\n\n19\n\n\u0015\nH \u2020 (\u03b8, \u03b7)(u) d\u03b7(u) ,\n\nwhere\nH \u2020 (\u03b8, \u03b7)(y) =\n\nE\u03b8,\u03b7 Z exp(\u03b8 \u2032 Z)1{Y \u2265 y}\n.\nE\u03b8,\u03b7 exp(\u03b8 \u2032 Z)1{Y \u2265 y}\n\nConditions I, S1\u2013S3 in guaranteeing the asymptotic normality of \u03b8b have\nbeen verified in [8]. In particular, the convergence rate of the estimated\nnuisance parameter is established in Theorem 3.1 of [31], that is,\n(56)\n\nkb\n\u03b7\u03b8en \u2212 \u03b70 k\u221e = OPX (n\u22121/2 + k\u03b8en \u2212 \u03b80 k),\n\nwhere k * k\u221e denotes the supreme norm. We next verify the bootstrap consistency conditions, that is, SB1\u2013SB3. Condition SB1 trivially holds since\nit is easy to show that \u03b7 7\u2192 m(\u03b8\ne 0 , \u03b7) has bounded Fr\u00e9chet derivative around\n\u03b70 . The P -Donsker condition SB2 has been verified when verifying (13)\nin condition S1. In the end, we will verify the bootstrap convergence rate\ncondition kb\n\u03b7 \u2217e \u2212 \u03b70 k\u221e = OPo XW (k\u03b8e \u2212 \u03b80 k \u2228 n\u22121/2 ) via Theorem 2. Since \u03b7b\u03b8\u2217\n\u03b8\nmaximizes P\u2217n m(\u03b8, \u03b7) for fixed \u03b8, we set k(\u03b8, \u03b7)[g] = m2 (\u03b8, \u03b7)[g] and have\nUn\u2217 (\u03b8, \u03b7b\u03b8\u2217 )[g] = P\u2217n m2 (\u03b8, \u03b7b\u03b8\u2217 )[g] = 0. The invertibility of \u1e86 (0, *), conditions (37)\nand (38) have been verified in [31] when they showed (56). Now we only need\nto consider condition (39): for n so large that \u03b4n \u2264 R\n\u001a\n|(m2 (\u03b8, \u03b7)[g]) \u2212 m2 (\u03b80 , \u03b70 )[g]|\n\u221a\nDn (x) \u2261 sup\n, g \u2208 G,\n1 + n(k\u03b8 \u2212 \u03b80 k + k\u03b7 \u2212 \u03b70 k\u221e )\n\u001b\nk\u03b8 \u2212 \u03b80 k + k\u03b7 \u2212 \u03b70 k\u221e \u2264 \u03b4n\n\u2264 2 sup{|m2 (\u03b8, \u03b7)[g]|, g \u2208 G, k\u03b8 \u2212 \u03b80 k + k\u03b7 \u2212 \u03b70 k\u221e \u2264 R}\n\u2264 some constant.\nThe last inequality follows from the assumption that G is Ra class of funcy\ntions of bounded total variation and the inequality that 0 g(u) d\u03b7(u) \u2264\n\u03b7(\u03c4 )kgkBV , where kgkBV is the total variation of the function g. Thus, condition (39) holds trivially.\n6.2. Cox regression model with current status data. We next consider the\ncurrent status data when each subject is observed at a single examination\ntime C to determine if an event has occurred. The event time T cannot\nbe known exactly. Then the observed data are n i.i.d. realizations of X =\n\n\f20\n\nG. CHENG AND J. Z. HUANG\n\n(C, \u03b4, Z) \u2208 R+ \u00d7 {0, 1} \u00d7 Z, where \u03b4 = I{T \u2264 C}. The corresponding criterion\nfunction, that is, the log-likelihood, is derived as\n(57)\n\nm(\u03b8, \u03b7) = \u03b4 log[1 \u2212 exp(\u2212\u03b7(c) exp(\u03b8 \u2032 z))] \u2212 (1 \u2212 \u03b4) exp(\u03b8 \u2032 z)\u03b7(c).\n\nWe make the following assumptions throughout the rest of this subsection:\n(i) T and C are independent given Z; (ii) the covariance of Z \u2212 E(Z|C) is\npositive definite, which guarantees the efficient information to be positive\ndefinite; (iii) C possesses a Lebesgue density which is continuous and positive on its support [\u03c3, \u03c4 ], for which the true nuisance parameter \u03b70 satisfies\n\u03b70 (\u03c3\u2212) > 0 and \u03b70 (\u03c4 ) < M < \u221e, and this density is continuously differentiable on [\u03c3, \u03c4 ] with derivative bounded above and bounded below by zero.\nThe form of m(\u03b8,\ne \u03b7) can be found in [9] as follows\nm(\u03b8,\ne \u03b7) = m1 (\u03b8, \u03b7) \u2212 m2 (\u03b8, \u03b7)[H \u2020 (\u03b8, \u03b7)]\n\n= (z\u03b7(c) \u2212 H \u2020 (\u03b8, \u03b7)(c))Q(x; \u03b8, \u03b7),\n\nwhere\n\n\u03b8\u2032 z\n\nQ(x; \u03b8, \u03b7) = e\n\n\u0014\n\n\u0015\n\u03b4\n\u2212 (1 \u2212 \u03b4)\nexp(e\u03b8\u2032 z \u03b7(c)) \u2212 1\n\nand the form of H \u2020 (\u03b8, \u03b7)(c) is given in (4) of [9].\nConditions I and S1\u2013S3 are verified in [9]. Conditions SB1 and SB2 can\nbe checked similarly as in the previous example. Note that the convergence\nrate for the nuisance parameter becomes slower, that is,\n(58)\n\nkb\n\u03b7\u03b8en \u2212 \u03b70 k2 = OPX (k\u03b8en \u2212 \u03b80 k + n\u22121/3 ),\n\nwhere k * k2 denotes the regular L2 -norm, as shown in [31]. By Theorem 3,\nwe can show that the same convergence rate, that is, n\u22121/3 , also holds for\n\u03b7b\u03b8\u2217 . The assumptions (43) and (44) in Theorem 3 are verified in [31] when\nshowing (58). We apply Lemma 1 to verify assumption (45). We show that\ncondition (52) on the envelop function Vn (x) holds: for n so large that \u03b4n \u2264 R\nVn (x) \u2261 sup{|m(\u03b8, \u03b7) \u2212 m(\u03b8, \u03b70 )| : k\u03b7 \u2212 \u03b70 k2 \u2264 \u03b4n , k\u03b8 \u2212 \u03b80 k \u2264 \u03b4n }\n\u2264 2 sup{|m(\u03b8, \u03b7)| : k\u03b7 \u2212 \u03b70 k2 \u2264 R, k\u03b8 \u2212 \u03b80 k \u2264 R}\n\u2264 some constant.\n6.3. Partially linear models. In this example, a continuous outcome variable Y , depending on the covariates (W, Z) \u2208 [0, 1]2 , is modeled as\nY = \u03b8W + f (Z) + \u03be,\nwhere \u03be is independent of (W, Z) and f is an unknown smooth function\nR1\nbelonging to H \u2261 {f : [0, 1] 7\u2192 [0, 1], 0 (f (k) (u))2 du \u2264 M } for a fixed 0 < M <\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n21\n\n\u221e. In addition, we assume E(Var(W |Z)) is positive definite and E{f (Z)} =\n0. We want to estimate (\u03b8, f ) using the least square criterion:\nm(\u03b8, f ) = \u2212(y \u2212 \u03b8w \u2212 f (z))2 .\n\n(59)\n\nNote that the above model would be more flexible if we did not require\nknowledge of M . A sieve estimator could be obtained if we replaced M with\na sequence Mn \u2192 \u221e. The theory we develop in this paper will be applicable\nin this setting, but, in order to maintain clarity of exposition, we have elected\nnot to pursue this more complicated situation here. Another approach is to\nuse penalization, the study of which is beyond the scope of this paper.\nSimple calculations give\nm(\u03b8,\ne \u03b7)(x) = m1 (\u03b8, \u03b7) \u2212 m2 (\u03b8, \u03b7)[H \u2020 (\u03b8, \u03b7)]\n\n= 2(y \u2212 \u03b8w \u2212 f (z))(w \u2212 H \u2020 (\u03b8, \u03b7)(z)),\n\nwhere\nH \u2020 (\u03b8, \u03b7)(z) =\n\nE\u03b8,\u03b7 (W (Y \u2212 \u03b8W \u2212 f (Z))2 |Z = z)\n.\nE\u03b8,\u03b7 ((Y \u2212 \u03b8W \u2212 f (Z))2 |Z = z)\n\nThe finite variance condition I follows from E[W {W \u2212 H \u2020 (\u03b80 , \u03b70 )(Z)}] > 0.\nThe distribution of \u03be is assumed to have finite second moment and satisfy\n(5), for example, \u03be \u223c N (0, 1). Conditions S1\u2013S3 and SB2 can be verified using\nsimilar arguments in Example 3 of [9], in particular, kfb\u03b8e \u2212 f0 k2 = OPX (k\u03b8e \u2212\n\u03b80 k \u2228 n\u2212k/(2k+1) ) in (15). It is easy to show that the Fr\u00e9chet derivative of\n\u03b7 7\u2192 m(\u03b8\ne 0 , \u03b7) is bounded around \u03b70 , and thus the tail condition SB1 holds. To\nprove kfbe\u2217 \u2212f0 k2 = OPo XW (k\u03b8e\u2212\u03b80 k\u2228n\u2212k/(2k+1) ) via Theorem 3, we proceed as\n\u03b8\nin the previous example, checking assumption (52) using similar arguments,\nthat is, Vn (x) is uniformly bounded.\n7. Proof of Theorem 1 (bootstrap consistency theorem). To prove Theorem 1, we need the following lemma whose proof is given in Appendix A.3.\n\nLemma 2.\n(60)\n\nUnder the assumptions of Theorem 1, we have\n\nG\u2217n (m(\u03b8,\ne \u03b7) \u2212 m(\u03b8\ne 0 , \u03b70 )) = OPo W (k\u03b8 \u2212 \u03b80 k \u2228 k\u03b7 \u2212 \u03b70 k)\n\nin PXo -probability for (\u03b8, \u03b7) \u2208 Cn .\n\nWe shall use repeatedly Lemma 3 in the Appendix, which concerns about\nthe transition of stochastic orders among different probability spaces.\n\n\f22\n\nG. CHENG AND J. Z. HUANG\n\n\u221a\n\u221a\nWe first prove (27). Recall that Gn = n(Pn \u2212 PX ) and G\u2217n = n(P\u2217n \u2212\nPn ). Define m\nb \u2217 as m(\ne \u03b8b\u2217 , \u03b7b\u2217 ). By some algebra, we have\n\u221a\nb\u2217 \u2212m\ne 0)\nG\u2217n m\ne 0 + Gn m\ne 0 + nPX (m\n\u221a\n\u2217\n\u2217\nb \u2217,\n= Gn (m\ne0 \u2212m\nb ) + Gn (m\ne0 \u2212m\nb \u2217 ) + nP\u2217n m\nsince PX m\ne 0 = 0. Thus, we have the following inequality:\n\u221a\nk nPX (m\nb\u2217 \u2212m\ne 0 )k \u2264 kG\u2217n m\ne 0 k + kGn m\ne 0 k + kG\u2217n (m\ne0 \u2212m\nb \u2217 )k\n\u221a\n(61)\nb \u2217k\n+ kGn (m\ne0 \u2212m\nb \u2217 )k + k nP\u2217n m\n\u2261 L1 + L2 + L3 + L4 + L5 .\n\nBased on Theorem 2.2 in [33], we have L1 = OPo W (1) in PXo -probability.\nThe CLT implies L2 = OPo X (1). We next consider L3 and L4 . By condition\nSB3, we can show that kb\n\u03b7 \u2217 \u2212 \u03b70 k = ooPW (1) in PXo -probability since \u03b8b\u2217 is\nassumed to be consistent, that is, k\u03b8b\u2217 \u2212 \u03b80 k = ooPW (1) in PXo -probability, and\nby (69) and (73) in Lemma 3. Then, we have L3 = ooPW (1) in PXo -probability\nbased on Lemma 2 and (73) in Lemma 3. Next, we obtain that L4 = ooPW (1)\nin PXo -probability based on condition S1 and (71) in Lemma 3. Finally,\nL5 = ooPXW (1) based on (22). In summary, (61) can be rewritten as:\n\u221a\nk nPX (m\n(62)\nb\u2217 \u2212m\ne 0 )k \u2264 OPo W (1) + OPo X (1)\nin PXo -probability.\nLet \u03b1n = k\u03b8b\u2217 \u2212 \u03b80 k. Combining (14) with (62) and noticing (26), we have\n\u221a\n\u221a\n(63)\nnkA\u03b1n k \u2264 OPo W (1) + OPo X (1) + OPo W ( n\u03b12n \u2228 n\u22122\u03b3+1/2 )\n\nin PXo -probability. By considering the consistency of \u03b8b\u2217 and condition I, we\ncomplete the proof of (27) based on (63).\nWe next prove (28). Write\n\u221a\ne0 \u2212m\nb \u2217 ),\nI1 = \u2212G\u2217n (m\nb\u2217 \u2212m\ne 0 ) = n(P\u2217n \u2212 Pn )(m\n\u221a\nI2 = Gn (m\nb \u2212m\ne 0 ) = n(Pn \u2212 PX )(m\nb \u2212m\ne 0 ),\n\u221a\ne0 \u2212m\nb \u2217 ),\nI3 = \u2212Gn (m\nb\u2217 \u2212m\ne 0 ) = n(Pn \u2212 PX )(m\n\u221a \u2217 \u2217 \u221a\nI4 = nPn m\nb \u2212 nPn m.\nb\nP\n\u221a\nb \u2217 \u2212 m)\nb + G\u2217n m\ne 0 = 4j=1 Ij .\nBy some algebra, we obtain that nPX (m\nBy the definition (24), we can show that An \u00d7 Bn = OPo W (1) in PXo probability if An and Bn are both of the order OPo W (1) in PXo -probability.\nThen the root-n consistency of \u03b8b\u2217 proven in (27) together with SB3 implies\n(64)\n\nkb\n\u03b7 \u2217 \u2212 \u03b70 k \u2228 k\u03b8b\u2217 \u2212 \u03b80 k = OP\u2217 W (n\u2212\u03b3 )\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n23\n\nin PXo -probability. Thus, by Lemma 2, we know I1 = OPo W (n\u2212\u03b3 ) in PXo probability. Note that (12) and (13) of condition S1 imply\n(65)\n\nGn (m(\u03b8,\ne \u03b7) \u2212 m\ne 0 ) = OPo X (k\u03b8 \u2212 \u03b80 k \u2228 k\u03b7 \u2212 \u03b70 k)\n\nfor (\u03b8, \u03b7) in the shrinking neighborhood Cn of (\u03b80 , \u03b70 ). Considering (65),\nS3 and Proposition 1, we have I2 = OPo X (n\u2212\u03b3 ). By (64), (65) and (72), we\nknow the order of I3 is OPo W (n\u2212\u03b3 ) in PXo -probability. We also obtain I4 =\nooPX (1) + ooPXW (1) by using (7) and (22).\nTherefore, we have established\n\u221a\n(66)\nnPX (m\nb \u2217 \u2212 m)\nb = \u2212G\u2217n m\ne 0 + ooPX (1) + ooPW (1)\n\nin PXo -probability. To\n\u221a analyze the left-hand side of (66), we rewrite it as\n\u221a\nnPX (m\nb\u2217 \u2212m\ne 0 ) \u2212 nPX (m\nb \u2212m\ne 0 ). Applying condition S2, we obtain\n\u221a\nb\nnPX (m11 (\u03b80 , \u03b70 ) \u2212 m21 (\u03b80 , \u03b70 )[H0\u2020 ])(\u03b8b\u2217 \u2212 \u03b8)\n(67)\n\n= \u2212G\u2217n m\ne 0 + ooPX (1) + ooPW (1) + OPo X (n1/2\u22122\u03b3 ) + OPo W (n1/2\u22122\u03b3 )\n\n= \u2212G\u2217n m\ne 0 + ooPX (1) + ooPW (1)\n\nin PXo -probability, by considering condition S3, SB3 and the range of \u03b3. Note\nthat ooPX (1) in (67) is also of the order ooPXW (1), and thus is of the order\nooPW (1) in PXo -probability by (69). Moreover, according to condition I we\nhave that A = PX (m11 (\u03b80 , \u03b70 ) \u2212 m21 (\u03b80 , \u03b70 )[H0\u2020 ]) is nonsingular. We obtain\n(28) by multiplying A\u22121 on both sides of (67).\nBy applying Lemma 4.6 in [33] under the bootstrap weight conditions, we\nobtain (29). Proposition 1 together with Lemma 2.11 in [39] implies that\n\u221a\nsup |PX ( n(\u03b8b \u2212 \u03b80 ) \u2264 x) \u2212 P (N (0, \u03a3) \u2264 x)| = o(1).\n(68)\nx\u2208Rd\n\nCombining (29) and (68), we obtain (30).\n\nAPPENDIX\nA.1. Measurability and stochastic orders. Measurability condition M (P ):\nwe say that a class of random functions F \u2208 M (P ) if F is nearly linearly\ndeviation measurable for P and that both F 2 and F \u20322 are nearly linearly\nsupremum measurable for P . Here F 2 and F \u20322 denote the classes of squared\nfunctions and squared differences of functions from F , respectively. It is\nknown that if F is countable, or if {Pn }\u221e\nn=1 are stochastically separable in\nF , or if F is image admissible Suslin [12], then F \u2208 M (P ). More precise\ndescriptions can be found in pages 853 and 854 of [17].\nThe following lemma is very important since it accurately describes the\ntransition of stochastic orders among different probability spaces. We implicitly assume the random quantities in Lemma 3 posses enough measurability\nso that the usual Fubini theorem can be used freely.\n\n\f24\n\nG. CHENG AND J. Z. HUANG\n\nLemma 3.\n\nSuppose that\nQn = ooPW (1)\n\nin PXo -probability,\n\nRn = OPo W (1)\n\nin PXo -probability.\n\nWe have\n(69)\n\nAn = ooPXW (1)\n\nAn = ooPW (1)\n\nin PXo -probability,\n\n(70)\n\nBn = OPo XW (1)\n\nBn = OPo W (1)\n\nin PXo -probability,\n\n=\u21d2\n\nCn = ooPW (1)\n\nin PXo -probability,\n\n(72) Dn = Rn \u00d7 OPo X (1)\n\n=\u21d2\n\nDn = OPo W (1)\n\nin PXo -probability,\n\n(73)\n\n=\u21d2\n\nEn = ooPW (1)\n\nin PXo -probability.\n\n(71) Cn = Qn \u00d7 OPo X (1)\nEn = Qn \u00d7 Rn\n\n\u21d0\u21d2\n\u21d0\u21d2\n\nProof. To verify (69), we have for every \u03b5, \u03bd > 0,\n\n(74)\n\n1 o o\no\nPXo {PW\n|X (|An | \u2265 \u03b5) \u2265 \u03bd} \u2264 EX PW |X (|An | \u2265 \u03b5)\n\u03bd\n1 o o\nEW |X 1{|An | \u2265 \u03b5}\n\u2264 EX\n\u03bd\n\nby Markov's inequality. According to Lemmas 6.5 and 6.14 in [22], we have\no Eo\no\no\nEX\nW |X 1{|An | \u2265 \u03b5} \u2264 EXW 1{|An | \u2265 \u03b5} = PXW (|An | \u2265 \u03b5), and thus\n(75)\n\n1 o\no\nPXo {PW\n|X (|An | \u2265 \u03b5) \u2265 \u03bd} \u2264 PXW (|An | \u2265 \u03b5).\n\u03bd\n\nFrom (75), we can conclude that if An = ooPXW (1), then An = ooPW (1) in PXo probability. Another direction of (69) follows from the following inequalities:\nfor any \u03b5, \u03b7 > 0,\no\no\no\nPXW\n(|An | \u2265 \u03b5) = EX\n{PW\n|X (|An | \u2265 \u03b5)}\n\no\no\no\n= EX\n{PW\n|X (|An | \u2265 \u03b5)1{PW |X (|An | \u2265 \u03b5) \u2265 \u03b7}}\n\n(76)\n\no\no\no\n+ EX\n{PW\n|X (|An | \u2265 \u03b5)1{PW |X (|An | \u2265 \u03b5) < \u03b7}}\n\no\no\n\u2264 EX\n{1{PW\n|X (|An | \u2265 \u03b5) \u2265 \u03b7}} + \u03b7\no\n\u2264 PXo {PW\n(|An | \u2265 \u03b5) \u2265 \u03b7} + \u03b7.\n\nNote that the first term in (76) can be made arbitrarily small by the assumption that An = ooPW (1) in PXo -probability. Since \u03b7 can be chosen arbitrarily\no (|A | \u2265 \u03b5) = 0 for any \u03b5 > 0. This completes\nsmall, we can show limn\u2192\u221e PXW\nn\nthe proof of (69). (70) can be shown similarly by using the inequalities (74)\nand (76).\n\n\f25\n\nSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\nAs for (71), we establish the following inequalities:\no\no\nPXo {PW\n|X (|Qn \u00d7 OPX (1)| \u2265 \u03b5) \u2265 \u03bd}\n\no\no\n\u2264 PXo {PW\n|X (|Qn | \u2265 \u03b5/|OPX (1)|) \u2265 \u03bd}\n\no\no\no\n\u2264 PXo {PW\n|X (|Qn | \u2265 \u03b5/M ) + PW |X (|OPX (1)| \u2265 M ) \u2265 \u03bd}\n\no\no\no\no\n\u2264 PXo {PW\n|X (|Qn | \u2265 \u03b5/M ) \u2265 \u03bd/2} + PX {PW |X (|OPX (1)| \u2265 M ) \u2265 \u03bd/2}\n\n2 o\no\no\n\u2264 PXo {PW\n|X (|Qn | \u2265 \u03b5/M ) \u2265 \u03bd/2} + PX (|OPX (1)| \u2265 M )\n\u03bd\nfor any \u03b5, \u03bd, M > 0. Since M can be chosen arbitrarily large, we can show\n(71) by considering the definition of OPo X (1). The proof of (72) is similar by\nusing the above set of inequalities. The proof of (71) can be carried over to\nprove (73). Similarly, we establish the following inequalities:\no\nPXo {PW\n|X (|Qn \u00d7 Rn | \u2265 \u03b5) \u2265 \u03b7}\n\no\no\no\n\u2264 PXo {PW\n|X (|Qn | \u2265 \u03b5/M ) \u2265 \u03b7/2} + PX {PW |X (|Rn | \u2265 M ) \u2265 \u03b7/2}\n\nfor any \u03b5, \u03b7, M > 0. Then by selecting sufficiently large M , we can show that\no\nPXo {PW\n|X (|Qn \u00d7 Rn | \u2265 \u03b5) \u2265 \u03b7} \u2192 0\n\nas n \u2192 \u221e for any \u03b5, \u03b7 > 0. \u0003\nA.2. Two useful inequalities. Here we give two key inequalities used in\nproving Lemmas 1 and 2.\nMultiplier inequality (Lemma 4.1 of [41]).\nLet Wn = (Wn1 , . . . , Wnn )\u2032 be nonnegativeR exchangeable\nrandom variables\n\u221ep\non (W, \u03a9, PW ) such that, for every n, Rn = 0\nPW (Wn1 \u2265 u) du < \u221e. Let\nZni , i = 1, 2, . . . , n, be i.i.d. random elements in (X \u221e , A\u221e , PX\u221e ) with values\nin l\u221e (Fn ), and write k * kn = supf \u2208Fn |Zni (f )|. It is assumed that Zni 's are\nindependent of Wn . Then for any n0 such that 1 \u2264 n0 < \u221e and any n > n0 ,\nthe following inequality holds:\nn\n\no\nEXW\n\n(77)\n\n1 X\n\u221a\nWni Zni\nn\ni=1\n\nn\n\no\n\u2264 n0 EX\nkZn1 kn *\n\n+ Rn * max\n\nEW (max1\u2264i\u2264n Wni )\n\u221a\nn\n\n(\n\nn0 <i\u2264n\n\no 1\n\u221a\nEX\ni\n\ni\nX\n\nj=n0 +1\n\n)\n\n.\n\nZnj\nn\n\n\f26\n\nG. CHENG AND J. Z. HUANG\n\nHoffmann\u2013Jorgensen inequality for moments (Proposition A.1.5 in [38]).\nLet 1 \u2264 p < \u221e and suppose that V1 , . . . , Vn are independent stochastic\nprocesses with mean zero indexed by an arbitrary index set T . Then there\nexist constants Kp and 0 < vp < 1 such that\nE\n\no\n\nn\nX\n\np\n\nVi\n\no\nn\n\u2264 Kp E o max kVk kp + [G\u22121 (vp )]p ,\n1\u2264k\u2264n\n\ni=1\n\nwhere kY k = supt |Yt | denotes P\nthe supremum of a stochastic process {Yt , t \u2208\nT }, and G\u22121 (v) = inf{u : P o (k ni=1 Vi k \u2264 v) \u2265 u}.\n\ne \u03b7) \u2212 m\ne 0 ) as the sum of\nA.3. Proof of Lemma 2. We first write G\u2217n (m(\u03b8,\n\u2217\n\u2217\nGn (m(\u03b8,\ne \u03b7) \u2212 m(\u03b8\ne 0 , \u03b7)) and Gn (m(\u03b8\ne 0 , \u03b7) \u2212 m\ne 0 ). By the Taylor expansion, the\nfirst term becomes (\u03b8 \u2212 \u03b80 )\u2032 G\u2217n (\u2202/\u2202\u03b8)m(\ne \u03b8\u0304, \u03b7), where \u03b8\u0304 is between \u03b8 and \u03b80 .\nBy SB2 and Theorem 2.2 in [33], we know that the first term is of the order\nOPo W (k\u03b8 \u2212 \u03b80 k) in PXo -probability. We next consider the second term. Let\n\u001b\n\u001a \u2217\ne 0 , \u03b7) \u2212 m\ne 0 )k\nkGn (m(\u03b8\n(78)\n,\n\u2206n = sup\nk\u03b7 \u2212 \u03b70 k\n\u03b7\u2208Un\n\nwhere Un = {\u03b7 : k\u03b7 \u2212 \u03b70 k \u2264 \u03b4n } for any \u03b4n \u2192 0. Note that we can write \u2206n =\nkG\u2217n kSn , where kG\u2217n kSn = supf \u2208Sn |G\u2217n f |. By (70), to verify the bootstrap\ne 0 , \u03b7) \u2212 m\ne 0 ) = OPo W (k\u03b7 \u2212 \u03b70 k) in PXo equicontinuity condition that G\u2217n (m(\u03b8\nprobability, it suffices to show\no\nlim sup EXW\n\u2206n < \u221e.\n\n(79)\n\nn\u2192\u221e\n\nNote that\nn\n\nn\n\ni=1\n\ni=1\n\n1 X\n1 X\nG\u2217n = \u221a\n(Wni \u2212 1)\u03b4Xi = \u221a\n(Wni \u2212 1)(\u03b4Xi \u2212 PX )\nn\nn\n\n\u2032 , . . . , W \u2032 ) be exchangeable bootstrap\nby condition W2. Let Wn\u2032 = (Wn1\nnn\nweights generated from PW \u2032 , an independent copy of PW . The bootstrap\n\u2032 = 1 for i = 1, . . . , n. Let\nweight conditions W1 and W2 imply that EW \u2032 Wni\n\nmn (\u03b7, \u03b70 ) =\nThen we have\n\nm(\u03b8\ne 0 , \u03b7) \u2212 m\ne0\n.\nk\u03b7 \u2212 \u03b70 k\n\no\no\nEXW\n\u2206n = EXW\nsup kG\u2217n mn (\u03b7, \u03b70 )k\n\u03b7\u2208Un\n\nn\n\n1 X\no\n(Wni \u2212 1)(\u03b4Xi \u2212 PX )mn (\u03b7, \u03b70 )\n= EXW\nsup \u221a\nn\n\u03b7\u2208Un\ni=1\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n27\n\nn\n\n1 X\n\u2032\no\n)(\u03b4Xi \u2212 PX )mn (\u03b7, \u03b70 )\n(Wni \u2212 EW \u2032 Wni\n= EXW\nsup \u221a\nn\n\u03b7\u2208Un\ni=1\n\nn\n\n1 X\no\no\n\u2032\n\u221a\n\u2264 EXW\nEW\n(Wni \u2212 Wni\n)(\u03b4Xi \u2212 PX )mn (\u03b7, \u03b70 ) .\n\u2032 sup\nn\n\u03b7\u2208Un\ni=1\n\no\nEXW\n\u2206n ,\n\nTo further bound\nwe employ the symmetrization argument familiar\nin the empirical process literature to obtain\nn\n\no\nEXW\n\u2206n\n\no\n\u2264 EXW\n\n1 X\nsup \u221a\nWni (\u03b4Xi \u2212 PX )mn (\u03b7, \u03b70 )\nn\n\u03b7\u2208Un\ni=1\n\nn\n\n(80)\n\n1 X \u2032\nsup \u221a\nWni (\u03b4Xi \u2212 PX )mn (\u03b7, \u03b70 )\nn\n\u03b7\u2208Un\n\no\no\n+ EXW\nEW\n\u2032\n\ni=1\n\n1\no\n= 2EXW\nsup \u221a\nn\n\u03b7\u2208Un\n\nn\nX\ni=1\n\nWni (\u03b4Xi \u2212 PX )mn (\u03b7, \u03b70 ) .\n\nWe next apply the multiplier inequality (77) to (80) with Zni = {(\u03b4Xi \u2212\nPX )mn (\u03b7, \u03b70 ) : \u03b7 \u2208 Un }. Define\nkZni kn = sup k(\u03b4Xi \u2212 PX )mn (\u03b7, \u03b70 )k.\n\u03b7\u2208Un\n\nTo show (79), we need only to show\n\u0011 \u221a\n\u0010\n(81)\nEW max Wni / n \u2192 0,\n1\u2264i\u2264n\n\no kZ k\nlim supn EX\nn1 n\n\n(82)\n\n< \u221e, and\n\nlim sup max\n\nn n0 <i\u2264n\n\no\nEX\n\ni\n1 X\nsup \u221a\nZni < \u221e\ni j=n0 +1\n\u03b7\u2208Un\n\nfor some n0 < \u221e. The bootstrap weight conditions W3 and W4 together\nwith Lemma 4.7 in [33] imply (81). Note that\no\no\nEX\nkZn1 kn = EX\nsup k(\u03b4X1 \u2212 PX )mn (\u03b7, \u03b70 )k\n\u03b7\u2208Un\n\no\no\n\u2264 EX\nsup kmn (\u03b7, \u03b70 )(X1 )k + EX\nsup kEX mn (\u03b7, \u03b70 )k\n\u03b7\u2208Un\n\no\n\u2264 2EX\nSn (X1 ),\n\n\u03b7\u2208Un\n\nwhere Sn is the envelop of the class Sn defined in (10), and the first inequality\nfollows from the Fatou's lemma. Condition SB1 implies\n1 o\n\u221a EX\n(83)\nmax Sn (Xk ) \u2212\u2192 0,\n1\u2264k\u2264n\nn\n\n\f28\n\nG. CHENG AND J. Z. HUANG\no\nlim sup EX\nSn (X1 ) < \u221e;\n\n(84)\n\nn\u2192\u221e\n\no kZ k < \u221e.\nsee page 120 of [38]. The result (84) implies lim supn EX\nn1 n\nIt remains to show (82). We apply the Hoffmann\u2013Jorgensen inequality\nwith p = 1 in Appendix A.2. First, we establish\n\u001a\n\u001b\nn\n1 o\n1 X\n\u22121\no\nZni \u2264 K1 \u221a EX max kZnk kn + Gn (v1 )\nEX sup \u221a\n1\u2264k\u2264n\nn\nn\n\u03b7\u2208Un\ni=1\n(85)\n\u2264 I1 + I2 ,\n\nwhere K1 and 0 < v1 < 1 are constants and\nGn (t) = PXo\n\nn\n\n\u22121/2\n\nn\nX\ni=1\n\nZni\nn\n\n!\n\n\u2264t .\n\nObviously, (83) implies that I1 \u2192 P\n0. We next consider I2 . Note that assumption S1 implies kGn kSn = kn\u22121/2 ni=1 Zni kn = OPo X (1). Hence, there exists\na finite constant Mt such that lim inf n Gn (Mt ) \u2265 t for every 1 > t > 0. It follows that lim supn Gn\u22121 (v1 ) \u2264 Mv1 < \u221e since 0 < v1 < 1. Thus, the left-hand\nside of (85) is bounded away from infinity, and therefore (82) holds in light\nof the following result from the triangular inequality\nmax\n\nn0 <i\u2264n\n\no\nEX\n\ni\ni\n1 X\n1 X\no\nsup \u221a\nZnj \u2264 max EX sup \u221a\nZnj\nn0 <i\u2264n\ni j=n0+1\ni j=1\n\u03b7\u2208Un\n\u03b7\u2208Un\nn\n\n0\n1 X\no\nZnj .\n+ EX\nsup \u221a\nn0\n\u03b7\u2208Un\n\nj=1\n\nThe proof of Lemma 2 is complete.\nA.4. Proof of Theorem 2. Using (40) and the fact that U (\u03b80 , \u03b70 ) = 0, we\nhave\n\n(86)\n\ne \u03b7b\u2217 ) \u2212 U (\u03b80 , \u03b70 )\nU (\u03b8,\n\u03b8e\n\ne \u03b7b\u2217 ) \u2212 U \u2217 (\u03b8,\ne \u03b7b\u2217 ) + Oo (n\u22121/2 )\n= U (\u03b8,\nn\nPXW\n\u03b8e\n\u03b8e\n\ne \u03b7b\u2217 ) \u2212 (Un \u2212 U )(\u03b8,\ne \u03b7b\u2217 ) + Oo (n\u22121/2 )\n= \u2212(Un\u2217 \u2212 Un )(\u03b8,\nPXW\n\u03b8e\n\u03b8e\n= L1 + L2 + OPo XW (n\u22121/2 ).\n\nFurther, based on conditions (37) and (39), we apply Lemma 4.2 in [41] to\n\u03b7 \u2217e \u2212 \u03b70 k). By\nobtain that L1 = \u2212(Un\u2217 \u2212 Un )(\u03b80 , \u03b70 ) + ooPXW (n\u22121/2 \u2228 k\u03b8e\u2212 \u03b80 k \u2228 kb\n\u03b8\nLemma 3.3.5 in [38] given (37) and (38), we have L2 = \u2212(Un \u2212 U )(\u03b80 , \u03b70 ) +\n\n\f29\n\nSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n\u03b7 \u2217e \u2212 \u03b70 k). By applying CLT and Theorem 2.2 in\nooPXW (n\u22121/2 \u2228 k\u03b8e \u2212 \u03b80 k \u2228 kb\n\u03b8\n[33] under condition (37) to L1 and L2 , we have\ne \u03b7b\u2217 ) \u2212 U (\u03b80 , \u03b70 ) = Oo (n\u22121/2 ) + oo (k\u03b8e \u2212 \u03b80 k \u2228 kb\n(87) U (\u03b8,\n\u03b7\u03b8\u2217e \u2212 \u03b70 k).\nPXW\nPXW\n\u03b8e\nWe next apply the Taylor expansion to get\ne \u03b7b\u2217 ) \u2212 U (\u03b80 , \u03b70 )\nU (\u03b8,\n\u03b8e\n\n= U\u0307 (\u03b8e \u2212 \u03b80 , \u03b7b\u03b8\u2217e \u2212 \u03b70 ) + o(k\u03b8e \u2212 \u03b80 k \u2228 kb\n\u03b7\u03b8\u2217e \u2212 \u03b70 k)\n\n= U\u0307 (\u03b8e \u2212 \u03b80 , 0) + U\u0307 (0, \u03b7b\u03b8\u2217e \u2212 \u03b70 ) + o(k\u03b8e \u2212 \u03b80 k \u2228 kb\n\u03b7\u03b8\u2217e \u2212 \u03b70 k)\n\nby the assumed Fr\u00e9chet differentiability of U and linearity of U\u0307 . Note that U\nhas bounded Fr\u00e9chet derivative and U\u0307 (0, *) is continuously invertible. Thus,\nwe can conclude that\ne \u03b7b\u2217 ) \u2212 U (\u03b80 , \u03b70 ) \u2265 ckb\nU (\u03b8,\n\u03b7 \u2217 \u2212 \u03b70 k + O(k\u03b8e \u2212 \u03b80 k) + o(k\u03b8e \u2212 \u03b80 k \u2228 kb\n\u03b7 \u2217 \u2212 \u03b70 k)\n\u03b8e\n\n\u03b8e\n\n\u03b8e\n\nfor some c > 0. Combining the above inequality with (87), we can establish\nthe following inequality:\nkb\n\u03b7\u03b8\u2217e \u2212 \u03b70 k . OPo XW (k\u03b8e \u2212 \u03b80 k \u2228 n\u22121/2 ) + ooPXW (kb\n\u03b7\u03b8\u2217e \u2212 \u03b70 k),\n\nwhich implies (41).\n\nA.5. Proof of Theorem 3. According to (70), we need only to show that\no\nPXW\n(d(b\n\u03b7\u03b8\u2217e, \u03b7n ) \u2265 2Mn (\u03b4n \u2228 k\u03b8e \u2212 \u03b80 k), \u03b8e \u2208 \u0398, \u03b7b\u03b8\u2217e \u2208 Hn ) \u2212\u2192 0\n\n(88)\n\nas n \u2192 \u221e and Mn \u2192 \u221e. The basic idea in proving (88) is first to partition\nthe whole parameter space into \"shells,\" and then bound the probability of\neach shell under conditions (43)\u2013(45).\nFor now we fix M = Mn and then allow it to increase to infinity. We first\ndefine the shell Sn,j,M as\nSn,j,M = {(\u03b8, \u03b7) \u2208 \u0398 \u00d7 Hn : 2j\u22121 \u03b4n < d(\u03b7, \u03b7n ) \u2264 2j \u03b4n , d(\u03b7, \u03b7n ) \u2265 2M k\u03b8 \u2212 \u03b80 k}\n\nwith j ranging over the integers and M > 0. Obviously, the event {\u03b8e \u2208 \u0398, \u03b7b\u2217e \u2208\n\u03b8\nHn : d(b\n\u03b7 \u2217 , \u03b7n ) \u2265 2M (\u03b4n \u2228 k\u03b8e \u2212 \u03b80 k)} is contained in the union of the events\n\u03b8e\n\ne \u03b7b\u2217 ) \u2208 Sn,j,M } for j \u2265 M . Thus, we have\n{(\u03b8,\ne\n\u03b8\n\no\nPXW\n(d(b\n\u03b7\u03b8\u2217e, \u03b7n ) \u2265 2M (\u03b4n \u2228 k\u03b8e \u2212 \u03b80 k), \u03b8e \u2208 \u0398, \u03b7b\u03b8\u2217e \u2208 Hn )\nX\no\ne \u03b7b\u2217 ) \u2208 Sn,j,M )\n\u2264\nPXW\n((\u03b8,\n\u03b8e\nj\u2265M\n\n\u2264\n\nX\n\nj\u2265M\n\no\nPXW\n\n\u0010\n\nsup\n\n(\u03b8,\u03b7)\u2208Sn,j,M\n\n\u0011\nP\u2217n (v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n )) \u2265 0 .\n\n\f30\n\nG. CHENG AND J. Z. HUANG\n\nThe second inequality follows from the definition of \u03b7b\u2217e. By the smoothness\n\u03b8\ncondition on v(\u03b8, \u03b7), that is, (43), we have the following inequality when\n(\u03b8, \u03b7) \u2208 Sj,n,M for j \u2265 M :\nPX (v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n )) . \u2212d(\u03b7, \u03b7n )2 + k\u03b8 \u2212 \u03b80 k2 . \u221222j\u22122 \u03b4n2\n\n(89)\n\nfor sufficiently large M .\nConsidering (89), we have\n\no\nPXW\n(d(b\n\u03b7\u03b8\u2217e, \u03b7n ) \u2265 2M (\u03b4n \u2228 k\u03b8e \u2212 \u03b80 k), \u03b8e \u2208 \u0398, \u03b7b\u03b8\u2217e \u2208 Hn )\n\u0010\n\u0011\nX\n\u221a\n\u221a\no\nsup\n\u2264\nPXW\nn(P\u2217n \u2212 PX )(v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n )) & n22j\u22122 \u03b4n2\n(\u03b8,\u03b7)\u2208Sn,j,M\n\nj\u2265M\n\n\u2264\n\nX\n\no\nPXW\n\nsup\n\n(\u03b8,\u03b7)\u2208Sn,j,M\n\nj\u2265M\n\n+ PXo\n.\n\n\u0010\n\n\u0010\n\nsup\n\n(\u03b8,\u03b7)\u2208Sn,j,M\n\n|G\u2217n (v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n ))| &\n\n|Gn (v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n ))| &\n\n\u221a\n\nX \u03c8 \u2217 (2j \u03b4n ) \u03c8n (2j \u03b4n )\n\u221an 2 2j + \u221a 2 2j\nn\u03b4n 2\nn\u03b4n 2\n\n\u221a 2j\u22123 2 \u0011\nn2\n\u03b4n\n\nn22j\u22123 \u03b4n2\n\n\u0011\n\nj\u2265M\n\n.\n\nX\n\n2j(\u03b1\u22122) ,\n\nj\u2265M\n\nwhere the third inequality follows from the Markov inequality and (44) and\n(45). Note that the assumption that \u03b4 7\u2192 \u03c8n (\u03b4)/\u03b4\u03b1 [\u03b4 7\u2192 \u03c8n\u2217 (\u03b4)/\u03b4\u03b1 ] is decreasing for some 0 < \u03b1 < 2 implies that \u03c8n (c\u03b4) \u2264 c\u03b1 \u03c8\u221a\nComn (\u03b4) for every c > 1. \u221a\n\u2217\n2\nbining these with the assumption that \u03c8n (\u03b4n ) \u2264 n\u03b4n and \u03c8n (\u03b4n ) \u2264 n\u03b4n2 ,\nwe obtain the last inequality in the above display. By letting M = Mn \u2192 \u221e,\nwe complete the proof of (88), and thus Theorem 3.\nA.6. Proof of Lemma 1. The result (51) is an immediate consequence\nof Lemma 3.4.2 in [38]. To show (53), we first apply the symmetrization\narguments used in the proof of Lemma 2. For sufficiently small \u03b4, the lefthand side of (45) is bounded by\nn\n\n(90)\n\n1 X\no\n\u221a\nWni Yni\n2EXW\nn\ni=1\n\n,\nV\u03b4\n\nwhere Wni 's are the assumed bootstrap weights and\nYni = {(\u03b4Xi \u2212 PX )(v(\u03b8, \u03b7) \u2212 v(\u03b8, \u03b7n )) : d(\u03b7, \u03b7n ) \u2264 \u03b4, k\u03b8 \u2212 \u03b80 k \u2264 \u03b4}.\n\nNext, the multiplier inequality (77) is employed to further bound (90). In\nview of (77), we need only to figure out the upper bound for\n(91)\n\no\nEX\nkYn1 kV\u03b4\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n31\n\nand\n(92)\n\ni\n1 X\no\n\u221a\nmax EX\nYnj\nn0 \u2264i\u2264n\ni j0 +1\n\nV\u03b4\n\nfor some n0 \u2265 1 given assumptions W3 and W4 on the bootstrap weights.\nBy a similar argument as in the proof of Lemma 2, we know\no\no\nEX\nkYn1 kV\u03b4 \u2264 2EX\nVn (X1 ),\n\nwhere Vn is the envelop function of the class V\u03b4 defined in (48). The assumption (52), together with the analysis of assumption SB1, implies that\no kY k\nlim supn EX\nn1 V\u03b4 < \u221e. Next, Lemma 3.4.2 in [38] implies that\n\u0012\n\u0013\nK(\u03b4, V\u03b4 , L2 (P ))\no\n\u221a\n.\nEX kGn kV\u03b4 \u2264 K(\u03b4, V\u03b4 , L2 (P )) 1 +\n\u03b42 n\nBy the triangular inequality, we know that (92) has the same upper bound\no kG k . This concludes the proof of (53).\nas EX\nn V\u03b4\nAcknowledgments. The authors thank Professor Anirban DasGupta for\ncontinuous encouragement and Professors Michael Kosorok and Jon Wellner\nfor many helpful discussions. The authors also thank the Co-editor Susan\nMurphy and two referees for insightful comments which led to important\nimprovements over an earlier draft.\nREFERENCES\n[1] Barbe, P. and Bertail, P. (1995). The Weighted Bootstrap. Lecture Notes in Statistics 98. Springer, New York. MR2195545\n[2] Banerjee, M., Mukherjee, D. and Mishra, S. (2009). Semiparametric binary regression models under shape constraints with an application to Indian schooling\ndata. J. Econometrics 149 101\u2013117. MR2518501\n[3] Bickel, P. J. and Freedman, D. A. (1981). Some asymptotic theory for the bootstrap. Ann. Statist. 9 1196\u20131217. MR0630103\n[4] Bickel, P. J., Klaassen, C. A. J., Ritov, Y. and Wellner, J. A. (1998). Efficient and Adaptive Estimation for Semiparametric Models. Springer, New York.\nMR1623559\n[5] Chatterjee, S. and Bose, A. (2005). Generalized bootstrap for estimating equations. Ann. Statist. 33 414\u2013436. MR2157808\n[6] Chen, X. and Pouzo, D. (2009). Efficient estimation of semiparametric conditional\nmoment models with possibly nonsmooth residuals. J. Econometrics 152 46\u201360.\nMR2562763\n[7] Cheng, G. (2008). Semiparametric additive isotonic regression. J. Statist. Plann.\nInference 100 345\u2013362. MR2497554\n[8] Cheng, G. and Kosorok, M. (2008). Higher order semiparametric frequentist inference with the profile sampler. Ann. Statist. 36 1786\u20131818. MR2435456\n[9] Cheng, G. and Kosorok, M. (2008). General frequentist properties of the posterior\nprofile distribution. Ann. Statist. 36 1819\u20131853. MR2435457\n\n\f32\n\nG. CHENG AND J. Z. HUANG\n\n[10] Delecroix, M., Hristache, M. and Patilea, V. (2006). On semiparametric M estimation in single-index regression. J. Statist. Plann. Inference 136 730\u2013769.\nMR2181975\n[11] Dixon, J., Kosorok, M. and Lee, B. L. (2005). Functional inference in semiparametric models using the piggyback bootstrap. Ann. Inst. Statist. Math. 57\n255\u2013277. MR2160650\n[12] Dudley, R. M. (1984). A Course on Empirical Processes. Lecture Notes in Math.\n1097 2\u2013142. Springer, Berlin. MR0876079\n[13] Efron, B. (1982). The Jackknife, the Bootstrap and Other Resampling Plans. SIAM,\nPhiladelphia. MR0659849\n[14] Efron, B. and Tibshirani, R. (1986). Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statist. Sci. 1 54\u201375.\nMR0833275\n[15] Grenander, U. (1981). Abstract Inference. Wiley, New York. MR0599175\n[16] Gelman, A., Carlin, J., Stern, H. and Rubin, D. (2003). Bayesian Data Analysis,\n2nd ed. Chapman and Hall, London. MR1385925\n[17] Gine, E. and Zinn, J. (1990). Bootstrapping general empirical functions. Ann.\nProbab. 18 851\u2013869. MR1055437\n[18] Hall, P. (1992). The Bootstrap and Edgeworth Expansion. Springer, New York.\nMR1145237\n[19] Hardle, W., Huet, S., Mammen, E. and Sperlich, S. (2004). Bootstrap inference\nin semiparametric generalized additive models. Econometric Theory 20 265\u2013300.\nMR2044272\n[20] Huang, J. (1999). Efficient estimation of the partly linear Cox model. Ann. Statist.\n27 1536\u20131563. MR1742499\n[21] Kosorok, M., Lee, B. L. and Fine, J. P. (2004). Robust inference for univariate proportional hazards frailty regression models. Ann. Statist. 32 1448\u20131491.\nMR2089130\n[22] Kosorok, M. (2008). Introduction to Empirical Processes and Semiparametric Inference. Springer, New York.\n[23] Kosorok, M. (2008). Boostrapping the Grenander estimator. In Beyond Parametrics\nin Interdisciplinary Research: Festschrift in Honor of Professor Pranab K. Sen.\nIMS Collections 1 282\u2013292. IMS, Beachwood, OH. MR2462212\n[24] Lee, B. L., Kosorok, M. R. and Fine, J. P. (2005). The profile sampler. J. Amer.\nStatist. Assoc. 100 960\u2013969. MR2201022\n[25] Lee, S. M. S. and Pun, M. C. (2006). On m out of n bootstrapping for nonstandard\nM -estimation with nuisance parameters. J. Amer. Statist. Assoc. 101 1185\u20131197.\nMR2328306\n[26] Liang, H., H\u00e4rdle, W. and Sommerfeld, V. (2000). Bootstrap approximations\nin a partially linear regression model. J. Statist. Plann. Inference 91 413\u2013426.\nMR1814793\n[27] Lo, A. Y. (1993). A Bayesian bootstrap for censored data. Ann. Statist. 21 100\u2013123.\nMR1212168\n[28] Ma, S. and Kosorok, M. (2005). Robust semiparametric M -estimation and the\nweighted bootstrap. J. Multivariate Anal. 96 190\u2013217. MR2202406\n[29] Ma, S. and Kosorok, M. (2005). Penalized log-likelihood estimation for partly linear\ntransformation models with current status data. Ann. Statist. 33 2256\u20132290.\nMR2211086\n[30] Mason, D. and Newton, M. (1992). A rank statistic approach to the consistency\nof a general bootstrap. Ann. Statist. 20 1611\u20131624. MR1186268\n\n\fSEMIPARAMETRIC BOOTSTRAP CONSISTENCY\n\n33\n\n[31] Murphy, S. A. and van der Vaart, A. W. (1999). Observed information in semiparametric models. Bernoulli 5 381\u2013412. MR1693616\n[32] Murphy, S. A. and van der Vaart, A. W. (2000). On profile likelihood. J. Amer.\nStatist. Assoc. 95 1461\u20131474. MR1803168\n[33] Praestgaard, J. and Wellner, J. (1993). Exchangeably weighted bootstraps of\nthe general empirical process. Ann. Probab. 21 2053\u20132086. MR1245301\n[34] Rubin, D. (1981). The Bayesian bootstrap. Ann. Statist. 9 130\u2013134. MR0600538\n[35] Sen, B., Banerjee, M. and Woodroofe, M. B. (2010). Inconsistency of bootstrap:\nThe Grenander estimator. Ann. Statist. 38 1953\u20131977.\n[36] Singh, K. (1981). On the asymptotic accuracy of Efron's bootstrap. Ann. Statist. 9\n1187\u20131195. MR0630102\n[37] Strawderman, R. (2006). A regression model for dependent gap times. Int. J. Biostat. 2 Article 1, 34 pp. (electronic). MR2275896\n[38] van der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical Processes: With Applications to Statistics. Springer, New York. MR1385671\n[39] van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge Univ. Press, Cambridge. MR1652247\n[40] van de Geer, S. (2000). Empirical Processes in M -Estimation. Cambridge Univ.\nPress, Cambridge.\n[41] Wellner, J. A. and Zhan, Y. (1996). Bootstrapping Z-estimators. Technical Report\n308, Univ. Washington.\n[42] Wellner, J. A. and Zhang, Y. (2007). Two likelihood-based semiparametric estimation methods for panel count data with covariates. Ann. Statist. 35 2106\u2013\n2142. MR2363965\n[43] Young, J. G., Jewell, N. P. and Samuels, S. J. (2008). Regression analysis of a\ndisease onset distribution using diagnosis data. Biometrics 64 20\u201328. MR2422815\n[44] Zeng, D. L. and Lin, D. Y. (2007). Maximum likelihood estimation in semiparametric models with censored data (with discussion). J. R. Stat. Soc. Ser. B Stat.\nMethodol. 69 507\u2013564. MR2370068\n[45] Zhang, C. M. and Yu, T. (2008). Semiparametric detection of significant activation\nfor brain FMRI. Ann. Statist. 36 1693\u20131725. MR2435453\nDepartment of Statistics\nPurdue University\nWest Lafayette, Indiana 47907-2066\nUSA\nE-mail: chengg@purdue.edu\n\nDepartment of Statistics\nTexas A&M University\nCollege Station, Texas 77843-3143\nUSA\nE-mail: jianhua@stat.tamu.edu\n\n\f"}