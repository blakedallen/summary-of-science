{"id": "http://arxiv.org/abs/1205.3054v2", "guidislink": true, "updated": "2012-05-18T06:56:47Z", "updated_parsed": [2012, 5, 18, 6, 56, 47, 4, 139, 0], "published": "2012-05-14T15:01:31Z", "published_parsed": [2012, 5, 14, 15, 1, 31, 0, 135, 0], "title": "Approximate Modified Policy Iteration", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1205.3450%2C1205.3272%2C1205.1254%2C1205.0559%2C1205.1788%2C1205.4069%2C1205.3103%2C1205.0237%2C1205.3260%2C1205.0055%2C1205.2103%2C1205.1069%2C1205.2751%2C1205.4280%2C1205.4323%2C1205.0930%2C1205.0682%2C1205.1319%2C1205.2718%2C1205.3988%2C1205.4394%2C1205.3803%2C1205.0740%2C1205.0461%2C1205.0450%2C1205.1559%2C1205.0060%2C1205.4013%2C1205.2369%2C1205.3728%2C1205.2214%2C1205.0578%2C1205.3453%2C1205.1926%2C1205.3630%2C1205.2154%2C1205.3539%2C1205.2498%2C1205.2296%2C1205.2729%2C1205.4397%2C1205.2983%2C1205.0804%2C1205.3611%2C1205.3625%2C1205.0496%2C1205.4363%2C1205.3002%2C1205.3814%2C1205.2122%2C1205.1482%2C1205.1169%2C1205.3861%2C1205.0847%2C1205.3054%2C1205.1372%2C1205.0161%2C1205.1566%2C1205.2335%2C1205.2436%2C1205.3914%2C1205.0094%2C1205.3383%2C1205.0150%2C1205.2385%2C1205.0465%2C1205.0796%2C1205.2924%2C1205.4243%2C1205.2502%2C1205.0263%2C1205.2056%2C1205.0661%2C1205.1542%2C1205.2545%2C1205.0069%2C1205.0851%2C1205.2011%2C1205.3014%2C1205.2759%2C1205.3481%2C1205.0484%2C1205.3779%2C1205.3828%2C1205.3954%2C1205.1751%2C1205.4325%2C1205.3388%2C1205.3009%2C1205.1851%2C1205.0673%2C1205.1338%2C1205.2318%2C1205.0378%2C1205.4114%2C1205.1905%2C1205.2455%2C1205.3323%2C1205.2402%2C1205.2648%2C1205.3721&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Approximate Modified Policy Iteration"}, "summary": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that\ncontains the two celebrated policy and value iteration methods. Despite its\ngenerality, MPI has not been thoroughly studied, especially its approximation\nform which is used when the state and/or action spaces are large or infinite.\nIn this paper, we propose three implementations of approximate MPI (AMPI) that\nare extensions of well-known approximate DP algorithms: fitted-value iteration,\nfitted-Q iteration, and classification-based policy iteration. We provide error\npropagation analyses that unify those for approximate policy and value\niteration. On the last classification-based implementation, we develop a\nfinite-sample analysis that shows that MPI's main parameter allows to control\nthe balance between the estimation error of the classifier and the overall\nvalue function approximation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1205.3450%2C1205.3272%2C1205.1254%2C1205.0559%2C1205.1788%2C1205.4069%2C1205.3103%2C1205.0237%2C1205.3260%2C1205.0055%2C1205.2103%2C1205.1069%2C1205.2751%2C1205.4280%2C1205.4323%2C1205.0930%2C1205.0682%2C1205.1319%2C1205.2718%2C1205.3988%2C1205.4394%2C1205.3803%2C1205.0740%2C1205.0461%2C1205.0450%2C1205.1559%2C1205.0060%2C1205.4013%2C1205.2369%2C1205.3728%2C1205.2214%2C1205.0578%2C1205.3453%2C1205.1926%2C1205.3630%2C1205.2154%2C1205.3539%2C1205.2498%2C1205.2296%2C1205.2729%2C1205.4397%2C1205.2983%2C1205.0804%2C1205.3611%2C1205.3625%2C1205.0496%2C1205.4363%2C1205.3002%2C1205.3814%2C1205.2122%2C1205.1482%2C1205.1169%2C1205.3861%2C1205.0847%2C1205.3054%2C1205.1372%2C1205.0161%2C1205.1566%2C1205.2335%2C1205.2436%2C1205.3914%2C1205.0094%2C1205.3383%2C1205.0150%2C1205.2385%2C1205.0465%2C1205.0796%2C1205.2924%2C1205.4243%2C1205.2502%2C1205.0263%2C1205.2056%2C1205.0661%2C1205.1542%2C1205.2545%2C1205.0069%2C1205.0851%2C1205.2011%2C1205.3014%2C1205.2759%2C1205.3481%2C1205.0484%2C1205.3779%2C1205.3828%2C1205.3954%2C1205.1751%2C1205.4325%2C1205.3388%2C1205.3009%2C1205.1851%2C1205.0673%2C1205.1338%2C1205.2318%2C1205.0378%2C1205.4114%2C1205.1905%2C1205.2455%2C1205.3323%2C1205.2402%2C1205.2648%2C1205.3721&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that\ncontains the two celebrated policy and value iteration methods. Despite its\ngenerality, MPI has not been thoroughly studied, especially its approximation\nform which is used when the state and/or action spaces are large or infinite.\nIn this paper, we propose three implementations of approximate MPI (AMPI) that\nare extensions of well-known approximate DP algorithms: fitted-value iteration,\nfitted-Q iteration, and classification-based policy iteration. We provide error\npropagation analyses that unify those for approximate policy and value\niteration. On the last classification-based implementation, we develop a\nfinite-sample analysis that shows that MPI's main parameter allows to control\nthe balance between the estimation error of the classifier and the overall\nvalue function approximation."}, "authors": ["Bruno Scherrer", "Victor Gabillon", "Mohammad Ghavamzadeh", "Matthieu Geist"], "author_detail": {"name": "Matthieu Geist"}, "author": "Matthieu Geist", "links": [{"href": "http://arxiv.org/abs/1205.3054v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1205.3054v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1205.3054v2", "affiliation": "UMI2958", "arxiv_url": "http://arxiv.org/abs/1205.3054v2", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Approximate Modified Policy Iteration\n\nBruno Scherrer\nINRIA Nancy - Grand Est, Maia Team, FRANCE\n\nBruno.Scherrer@inria.fr\n\narXiv:1205.3054v2 [cs.AI] 18 May 2012\n\nVictor Gabillon\nMohammad Ghavamzadeh\nINRIA Lille - Nord Europe, Sequel Team, FRANCE\nMatthieu Geist\nSuplec, IMS Research Group, Metz, FRANCE\n\nMatthieu.Geist@supelec.fr\n\nAbstract\n\nwhere G vk is a greedy policy w.r.t. vk , T\u03c0k is the Bellman operator associated to the policy \u03c0k , and m \u2265 1 is\na parameter. MPI generalizes the well-known dynamic\nprogramming algorithms Value Iteration (VI) and Policy Iteration (PI) for values m = 1 and m = \u221e, respectively. MPI has less computation per iteration than PI\n(in a way similar to VI), while enjoys the faster convergence of the PI algorithm (Puterman & Shin, 1978).\nIn problems with large state and/or action spaces, approximate versions of VI (AVI) and PI (API) have\nbeen the focus of a rich literature (see e.g. Bertsekas\n& Tsitsiklis 1996; Szepesv\u00e1ri 2010). The aim of this\npaper is to show that, similarly to its exact form, approximate MPI (AMPI) may represent an interesting\nalternative to AVI and API algorithms.\n\nModified policy iteration (MPI) is a dynamic\nprogramming (DP) algorithm that contains\nthe two celebrated policy and value iteration methods. Despite its generality, MPI\nhas not been thoroughly studied, especially\nits approximation form which is used when\nthe state and/or action spaces are large or\ninfinite. In this paper, we propose three implementations of approximate MPI (AMPI)\nthat are extensions of well-known approximate DP algorithms: fitted-value iteration,\nfitted-Q iteration, and classification-based\npolicy iteration. We provide error propagation analyses that unify those for approximate policy and value iteration. On the last\nclassification-based implementation, we develop a finite-sample analysis that shows that\nMPI's main parameter allows to control the\nbalance between the estimation error of the\nclassifier and the overall value function approximation.\n\n1. Introduction\nModified Policy Iteration (MPI) (Puterman & Shin,\n1978) is an iterative algorithm to compute the optimal\npolicy and value function of a Markov Decision Process\n(MDP). Starting from an arbitrary value function v0 ,\nit generates a sequence of value-policy pairs\n\u03c0k+1 = G vk\nm\n\nvk+1 = (T\u03c0k+1 ) vk\n\nVictor.Gabillon@inria.fr\nMohammad.Ghavamzadeh@inria.fr\n\n(greedy step)\n\n(1)\n\n(evaluation step)\n\n(2)\n\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)/owner(s).\n\nIn this paper, we propose three implementations of\nAMPI (Sec. 3) that generalize the AVI implementations of Ernst et al. (2005); Antos et al. (2007); Munos\n& Szepesv\u00e1ri (2008) and the classification-based API\nalgorithm of Lagoudakis & Parr (2003); Fern et al.\n(2006); Lazaric et al. (2010); Gabillon et al. (2011). We\nthen provide an error propagation analysis of AMPI\n(Sec. 4), which shows how the Lp -norm of its performance loss can be controlled by the error at each iteration of the algorithm. We show that the error propagation analysis of AMPI is more involved than that\nof AVI and API. This is due to the fact that neither\nthe contraction nor monotonicity arguments, that the\nerror propagation analysis of these two algorithms rely\non, hold for AMPI. The analysis of this section unifies\nthose for AVI and API and is applied to the AMPI implementations presented in Sec. 3. We detail the analysis of the classification-based implementation of MPI\n(CBMPI) of Sec. 3 by providing its finite sample analysis in Sec. 5. Our analysis indicates that the parameter\nm allows us to balance the estimation error of the classifier with the overall quality of the value approxima-\n\n\fApproximate Modified Policy Iteration\n\ntion. We report some preliminary results of applying\nCBMPI to standard benchmark problems and comparing it with some existing algorithms in (Scherrer et al.,\n2012, Appendix G).\n\nestimated as follows:\n\n2. Background\n\nwhere \u2200a \u2208 A and 1 \u2264 j \u2264 M , ra and sa are\nsamples of rewards and next states when action a\nis taken in state s. Thus, approximating the greedy\naction in a state s requires M |A| samples. The algorithm works as follows. It first samples N states\nFrom\nfrom a distribution \u03bc, i.e., {s(i) }N\ni=1 \u223c \u03bc.\neach sampled state s(i) , it generates a rollout of size\n(i) (i) (i)\n(i)\n(i)\n(i) \u0001\nm, i.e., s(i) , a0 , r0 , s1 , . . . , am\u22121 , rm\u22121 , sm , where\n(i)\n(i)\nat is the action suggested by \u03c0k+1 in state st ,\n(i)\n(i)\ncomputed using Eq. 3, and rt and st+1 are the reward and next state induced by this choice of action. For each s(i) , we then compute a rollout estimate\nPm\u22121\n(i)\n(i)\nvbk+1 (s(i) ) = t=0 \u0002\u03b3 t rt +\u0001\u03b3 m vk\u0003(sm ), which is an unm\nbiased estimate of T\u03c0k+1 vk (s(i) ). Finally, vk+1\nis computed as the best fit in F to these estimates,\ni.e.,\n\u0012n\n\u0013\n\u0001oN\nvk+1 = FitF\ns(i) , vbk+1 (s(i) )\n.\n\nWe consider a discounted MDP hS, A, P, r, \u03b3i, where S\nis a state space, A is a finite action space, P (ds0 |s, a),\nfor all (s, a), is a probability kernel on S, the reward function r : S \u00d7 A \u2192 R is bounded by Rmax ,\nand \u03b3 \u2208 (0, 1) is a discount factor. A deterministic policy is defined as a mapping \u03c0 : S \u2192 A.\u0001 For\na policy \u03c0, we may write\n\u0001 r\u03c0 (s) = r s, \u03c0(s) and\nP\u03c0 (ds0 |s) = P ds0 |s, \u03c0(s) . The value of policy \u03c0 in\na state s is defined as the expected discounted sum\nof rewards received starting from\nstate s and follow\u0002P\n\u221e\nt\ning the policy \u03c0,\u0003 i.e.,v\u03c0 (s) = E\nt=0 \u03b3 r\u03c0 (st )| s0 =\ns, st+1 \u223c P\u03c0 (*|st ) . Similarly, the action-value function\nof a policy \u03c0 at a state-action pair (s, a), Q\u03c0 (s, a), is\nthe expected discounted sum of rewards received starting from state s, taking action a, and then following\nthe policy. Since the rewards are bounded by Rmax ,\nthe values and action-values should be bounded by\nVmax = Qmax = Rmax /(1 \u2212 \u03b3). The Bellman operator T\u03c0 of policy \u03c0 takes a function f on S as input\nand\nas \u2200s, [T\u03c0 f ](s) =\n\u0002 returns the function T\u03c0 f defined\n\u0003\nE r\u03c0 (s) + \u03b3f (s0 ) | s0 \u223c P\u03c0 (.|s) , or in compact form,\nT\u03c0 f = r\u03c0 + \u03b3P\u03c0 f . It is known that v\u03c0 is the unique\nfixed-point of T\u03c0 . Given a function f on S, we say\nthat a policy \u03c0 is greedy w.r.t. f , and write it as\n\u03c0 = G f , if \u2200s, (T\u03c0 f )(s) = maxa (Ta f )(s), or equivalently T\u03c0 f = max\u03c00 (T\u03c00 f ). We denote by v\u2217 the optimal value function. It is also known that v\u2217 is the\nunique fixed-point of the Bellman optimality operator\nT : v \u2192 max\u03c0 T\u03c0 v = TG(v) v, and that a policy \u03c0\u2217 that\nis greedy w.r.t. v\u2217 is optimal and its value satisfies\nv\u03c0 \u2217 = v\u2217 .\n\n3. Approximate MPI Algorithms\nIn this section, we describe three approximate MPI\n(AMPI) algorithms. These algorithms rely on a function space F to approximate value functions, and in\nthe third algorithm, also on a policy space \u03a0 to represent greedy policies. In what follows, we describe the\niteration k of these iterative algorithms.\n3.1. AMPI-V\nFor the first and simplest AMPI algorithm presented\nin the paper, we assume that the values vk are represented in a function space F \u2286 R|S| . In any state\ns, the action \u03c0k+1 (s) that is greedy w.r.t. vk can be\n\n\u03c0k+1 (s) = arg max\na\u2208A\n\nM\n\u0011\n1 \u0010 X (j)\nra + \u03b3vk (s(j)\na ) ,\nM j=1\n(j)\n\n(3)\n\n(j)\n\ni=1\n\nEach iteration of AMPI-V requires N rollouts of size\nm, and in each rollout any of the |A| actions needs\nM samples to compute Eq. 3. This gives a total of\nN m(M |A|+1) transition samples. Note that the fitted\nvalue iteration algorithm (Munos & Szepesv\u00e1ri, 2008)\nis a special case of AMPI-V when m = 1.\n3.2. AMPI-Q\nIn AMPI-Q, we replace the value function v : S \u2192 R\nwith an action-value function Q : S \u00d7 A \u2192 R. The\nBellman operator for a policy \u03c0 at a state-action pair\n(s, a) can then be written as\n\u0002\n\u0003\n[T\u03c0 Q](s, a) = E r\u03c0 (s, a) + \u03b3Q(s0 , \u03c0(s0 ))|s0 \u223c P (*|s, a) ,\nand the greedy operator is defined as\n\u03c0 = G Q \u21d4 \u2200s\n\n\u03c0(s) = arg max Q(s, a).\na\u2208A\n\nIn AMPI-Q, action-value functions Qk are represented\nin a function space F \u2286 R|S\u00d7A| , and the greedy action\nw.r.t. Qk at a state s, i.e., \u03c0k+1 (s), is computed as\n\u03c0k+1 (s) \u2208 arg max Qk (s, a).\na\u2208A\n\n(4)\n\nThe evaluation step is similar to that of AMPI-V,\nwith the difference that now we work with stateaction pairs. We sample N state-action pairs from\na distribution \u03bc on S \u00d7 A and build a rollout set\n\n\fApproximate Modified Policy Iteration\nInput: Value function space F, policy space \u03a0, state\ndistribution \u03bc\nInitialize: Let \u03c01 \u2208 \u03a0 be an arbitrary policy and\nv0 \u2208 F an arbitrary value function\nfor k = 1, 2, . . . do\n\u2022 Perform rollouts:\n(i) iid\nConstruct the rollout set Dk = {s(i) }n\n\u223c\u03bc\ni=1 , s\n(i)\nfor all states s \u2208 Dk do\nPerform a rollout and return vbk (s(i) )\nend for\n(i) iid\nConstruct the rollout set Dk0 = {s(i) }N\n\u223c\u03bc\ni=1 , s\n(i)\n0\nfor all states s \u2208 Dk and actions a \u2208 A do\nfor j = 1 to M do\nPerform a rollout and return Rkj (s(i) , a)\nend for\nb k (s(i) , a) = 1 PM Rj (s(i) , a)\nQ\nk\nj=1\nM\nend for\n\u2022 Approximate value function:\nvk \u2208 argmin LbF\n\u03bc; v)\n(regression)\nk (b\nv\u2208F\n\n\u2022 Approximate greedy policy:\n\u03bc; \u03c0)\n(classification)\n\u03c0k+1 \u2208 argmin Lbk\u03a0 (b\n\u03c0\u2208\u03a0\n\nend for\nFigure 1. The pseudo-code of the CBMPI algorithm.\n(i) (i)\nDk = {(s(i) , a(i) )}N\nFor each\ni=1 , (s , a ) \u223c \u03bc.\n(i) (i)\n(s , a ) \u2208 Dk , we generate a rollout of size m,\n(i) (i) (i)\n(i) (i) \u0001\ni.e., s(i) , a(i) , r0 , s1 , a1 , * * * , sm , am , where the\n(i)\nfirst action is a(i) , at for t \u2265 1 is the action sug(i)\ngested by \u03c0k+1 in state st computed using Eq. 4, and\n(i)\n(i)\nrt and st+1 are the reward and next state induced\nby this choice of action. For each (s(i) , a(i) ) \u2208 Dk , we\nthen compute the rollout estimate\n\nb k+1 (s(i) , a(i) ) =\nQ\n\nm\u22121\nX\n\nplicit representation for the policies \u03c0k , in addition to\nthe one used for value functions vk . The idea is similar\nto the classification-based PI algorithms (Lagoudakis\n& Parr, 2003; Fern et al., 2006; Lazaric et al., 2010;\nGabillon et al., 2011) in which we search for the greedy\npolicy in a policy space \u03a0 (defined by a classifier)\ninstead of computing it from the estimated value or\naction-value function (like in AMPI-V and AMPI-Q).\nIn order to describe CBMPI, we first rewrite the MPI\nformulation (Eqs. 1 and 2) as\nvk = (T\u03c0k )m vk\u22121\n\u0002\n\u0003\n\u03c0k+1 = G (T\u03c0k )m vk\u22121\n\n(evaluation step)\n\n(5)\n\n(greedy step)\n\n(6)\n\nNote that in the new formulation both vk and \u03c0k+1\nare functions of (T\u03c0k )m vk\u22121 . CBMPI is an approximate version of this new formulation. As described\nin Fig. 1, CBMPI begins with arbitrary initial policy\n\u03c01 \u2208 \u03a0 and value function v0 \u2208 F.1 At each iteration\nk, a new value function vk is built as the best approximation of the m-step Bellman operator (T\u03c0k )m vk\u22121\nin F (evaluation step). This is done by solving a regression problem whose target function is (T\u03c0k )m vk\u22121 .\nTo set up the regression problem, we build a rollout\nset Dk by sampling n states i.i.d. from a distribution\n\u03bc.2 For each state s(i) \u2208 Dk , we generate a roll(i) (i) (i)\n(i)\n(i)\n(i) \u0001\nout s(i) , a0 , r0 , s1 , . . . , am\u22121 , rm\u22121 , sm of size m,\n(i)\n(i)\n(i)\n(i)\nwhere at = \u03c0k (st ), and rt and st+1 are the reward\nand next state induced by this choice of action. From\nthis\u0002 rollout, we \u0003compute an unbiased estimate vbk (s(i) )\nof (T\u03c0k )m vk\u22121 (s(i) ) as\nvbk (s(i) ) =\n\nm\u22121\nX\n\n(i)\n\n\u03b3 t rt + \u03b3 m vk\u22121 (s(i)\nm ),\n\n(7)\n\nt=0\n\n(i)\n\n(i)\n\u03b3 t rt + \u03b3 m Qk (s(i)\nm , am ),\n\nt=0\n\nwhich\nis \u0003\nan\nunbiased\nestimate\nof\n\u0002\n(T\u03c0k+1 )m Qk (s(i) , a(i) ). Finally, Qk+1 is the best fit\nto these estimates in F, i.e.,\n\u0012n\n\u0013\n\u0001oN\nb k+1 (s(i) , a(i) )\nQk+1 = FitF\n(s(i) , a(i) ), Q\n.\ni=1\n\nEach iteration of AMPI-Q requires N m samples,\nwhich is less than that for AMPI-V. However, it\nuses a hypothesis space on state-action pairs instead\nof states. Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special\ncase of AMPI-Q when m = 1.\n3.3. Classification-Based MPI\nThe third AMPI algorithm presented in this paper,\ncalled classification-based MPI (CBMPI), uses an ex-\n\n\b\n\u0001 n\nand use it to build a training set s(i) , vbk (s(i) ) i=1 .\nThis training set is then used by the regressor to compute vk as an estimate of (T\u03c0k )m vk\u22121 .\nThe greedy step at iteration k computes\nthe policy\n\u03c0k+1\n\u0002\n\u0003\nas the best approximation of G (T\u03c0k )m vk\u22121 by solving a cost-sensitive classification problem.\nFrom the\n\u0002\n\u0003\ndefinition of a greedy policy, if \u03c0 = G (T\u03c0k )m vk\u22121 ,\nfor each s \u2208 S, we have\n\u0002\n\u0003\n\u0002\n\u0003\nT\u03c0 (T\u03c0k )m vk\u22121 (s) = max Ta (T\u03c0k )m vk\u22121 (s). (8)\na\u2208A\n\n\u0002\n\u0003\nBy defining Qk (s, a) = Ta (T\u03c0k )m vk\u22121 (s), we may\n1\nNote that the function space F and policy space \u03a0 are\nautomatically defined by the choice of the regressor and\nclassifier, respectively.\n2\nHere we used the same sampling distribution \u03bc for both\nregressor and classifier, but in general different distributions may be used for these two components.\n\n\fApproximate Modified Policy Iteration\n\nrewrite Eq. 8 as\n\u0001\nQk s, \u03c0(s) = max Qk (s, a).\n\n(9)\n\na\u2208A\n\nThe cost-sensitive error function used by CBMPI is of\nthe form\nL\u03a0\n\u03c0k ,vk\u22121 (\u03bc; \u03c0)\n\n=\n\nZ h\n\n\u0001i\n\nmax Qk (s, a) \u2212 Qk s, \u03c0(s) \u03bc(ds).\na\u2208A\n\nS\n\nTo simplify the notation we use L\u03a0\nk instead of\nL\u03a0\n\u03c0k ,vk\u22121 . To set up this cost-sensitive classification\nproblem, we build a rollout set Dk0 by sampling N\nstates i.i.d. from a distribution \u03bc. For each state\ns(i) \u2208 Dk0 and each action a \u2208 A, we build M independent rollouts of size m + 1, i.e.,3\n\u0001\n(i,j) (i,j) (i,j)\n(i,j) (i,j) M\ns(i) , a, r0 , s1 , a1 , . . . , a(i,j)\nm , rm , sm+1 j=1 ,\n(i,j)\n\n(i,j)\n\n(i,j)\n\nwhere for t \u2265 1, at\n= \u03c0k (st ), and rt\nand\n(i,j)\nst+1 are the reward and next state induced by this\nchoice of action. From these rollouts, we compute\nb k (s(i) , a) =\nan unbiased estimate of Qk (s(i) , a) as Q\nP\nM\nj\n1\n(i)\nj=1 Rk (s , a) where\nM\nRkj (s(i) , a) =\n\nm\nX\n\n(i,j)\n\n\u03b3 t rt\n\n(i,j)\n\n+ \u03b3 m+1 vk\u22121 (sm+1 ).\n\nt=0\n\nGiven the outcome of the rollouts, CBMPI uses a costsensitive classifier to return a policy \u03c0k+1 that minimizes the following empirical error\nN\n\u0001i\n1 Xh\nb k (s(i) , a) \u2212 Q\nb k s(i) , \u03c0(s(i) ) ,\nLbk\u03a0 (b\n\u03bc; \u03c0) =\nmax Q\nN i=1 a\u2208A\n\nwith the goal of minimizing the true error L\u03a0\nk (\u03bc; \u03c0).\nEach iteration of CBMPI requires nm+M |A|N (m+1)\n(or M |A|N (m + 1) in case we reuse the rollouts, see\nFootnote 3) transition samples. Note that when m\ntends to \u221e, we recover the DPI algorithm proposed\nand analyzed by Lazaric et al. (2010).\n\n4. Error propagation\nIn this section, we derive a general formulation for\npropagation of error through the iterations of an AMPI\nalgorithm. The line of analysis for error propagation\nis different in VI and PI algorithms. VI analysis is\nbased on the fact that this algorithm computes the\nfixed point of the Bellman optimality operator, and\nthis operator is a \u03b3-contraction in max-norm (Bertsekas & Tsitsiklis, 1996; Munos, 2007). On the other\n\nhand, it can be shown that the operator by which PI\nupdates the value from one iteration to the next is not\na contraction in max-norm in general. Unfortunately,\nwe can show that the same property holds for MPI\nwhen it does not reduce to VI (i.e., m > 1).\nProposition 1. If m > 1, there exists no norm for\nwhich the operator that MPI uses to update the values\nfrom one iteration to the next is a contraction.\nProof. Consider a deterministic MDP with two states\n{s1 , s2 }, two actions {change, stay}, rewards r(s1 ) =\n0, r(s2 ) = 1, and transitions Pch (s2 |s1 ) = Pch (s1 |s2 ) =\nPst (s1 |s1 ) = Pst (s2 |s2 ) = 1. Consider the following\ntwo value functions v = (\u000f, 0) and v 0 = (0, \u000f) with \u000f >\n0. Their corresponding greedy policies are \u03c0 = (st, ch)\n0\nand \u03c0 0 = (ch, st), and the next\n\u0012 iterates\n\u0013 of v and v can\nm\n\u03b3 \u000f\nbe computed as (T\u03c0 )m v =\nand (T\u03c00 )m v 0 =\n1 + \u03b3m\u000f\n!\n!\n\u03b3\u2212\u03b3 m\n\u03b3\u2212\u03b3 m\nm\n1\u2212\u03b3 + \u03b3 \u000f\n1\u2212\u03b3\nm\n0\nm\n. Thus, (T\u03c00 ) v \u2212(T\u03c0 ) v = \u03b3\u2212\u03b3 m\n1\u2212\u03b3 m\nm\n1\u2212\u03b3 + \u03b3 \u000f \u0012\n1\u2212\u03b3\n\u0013\n\u2212\u000f\n0\nwhile v \u2212 v =\n. Since \u000f can be arbitrarily small,\n\u000f\nthe norm of (T\u03c00 )m v 0 \u2212(T\u03c0 )m v can be arbitrarily larger\nthan the norm of v \u2212 v 0 as long as m > 1.\nWe also know that the analysis of PI usually relies on\nthe fact that the sequence of the generated values is\nnon-decreasing (Bertsekas & Tsitsiklis, 1996; Munos,\n2003). Unfortunately, it can easily be shown that for\nm finite, the value functions generated by MPI may\ndecrease (it suffices to take a very high initial value).\nIt can be seen from what we just described and Proposition 1 that for m 6= 1 and \u221e, MPI is neither contracting nor non-decreasing, and thus, a new line of proof is\nneeded for the propagation of error in this algorithm.\nTo study error propagation in AMPI, we introduce an\nabstract algorithmic model that accounts for potential\nerrors. AMPI starts with an arbitrary value v0 and\nat each iteration k \u2265 1 computes the greedy policy\nw.r.t. vk\u22121 with some error \u000f0k , called the greedy step\nerror. Thus, we write the new policy \u03c0k as\nb \u000f0 vk\u22121 .\n\u03c0k = G\nk\n\n(10)\n\nEq. 10 means that for any policy \u03c0 0 ,\nT\u03c00 vk\u22121 \u2264 T\u03c0k vk\u22121 + \u000f0k .\nAMPI then generates the new value function vk with\nsome error \u000fk , called the evaluation step error\nvk = (T\u03c0k )m vk\u22121 + \u000fk .\n\n(11)\n\n3\n\nWe may implement CBMPI more sample efficient by\nreusing the rollouts generated for the greedy step in the\nevaluation step.\n\nBefore showing how these two errors are propagated\nthrough the iterations of AMPI, let us first define them\n\n\fApproximate Modified Policy Iteration\n\nin the context of each of the algorithms presented in\nSection 3 separately.\n\nLemma 1 (Proof in (Scherrer et al., 2012, Ap\u2206\npendix A)). Let k \u2265 1, xk = (I \u2212 \u03b3P\u03c0k )\u000fk + \u000f0k+1\n\nAMPI-V: \u000fk is the error in fitting the value function\nvk . This error can be further decomposed into two\nparts: the one related to the approximation power\nof F and the one due to the finite number of samples/rollouts. \u000f0k is the error due to using a finite number of samples M for estimating the greedy actions.\n\nand yk = \u2212\u03b3P\u03c0\u2217 \u000fk + \u000f0k+1 . We have:\n\n\u2206\n\nbk \u2264 (\u03b3P\u03c0k )m bk\u22121 + xk ,\ndk+1 \u2264 \u03b3P\u03c0\u2217 dk + yk +\n\nvk = (T\u03c0k )m vk\u22121 + \u000fk\nb \u000f0 [(T\u03c0 )m vk\u22121 ]\n\u03c0k+1 = G\nk+1\n\nk\n\nUnfortunately, this does not exactly match with the\nmodel described in Eqs. 10 and 11. By introducing\n\u2206\nthe auxiliary variable wk = (T\u03c0k )m vk\u22121 , we have vk =\nwk + \u000fk , and thus, we may write\nb \u000f0 [wk ] .\n\u03c0k+1 = G\nk+1\n\n(12)\n\nUsing vk\u22121 = wk\u22121 + \u000fk\u22121 , we have\nwk = (T\u03c0k )m vk\u22121 = (T\u03c0k )m (wk\u22121 + \u000fk\u22121 )\n= (T\u03c0k )m wk\u22121 + (\u03b3P\u03c0k )m \u000fk\u22121 .\n\n(13)\n\nNow, Eqs. 12 and 13 exactly match Eqs. 10 and 11 by\nreplacing vk with wk and \u000fk with (\u03b3P\u03c0k )m \u000fk\u22121 .\nThe rest of this section is devoted to show how the\nerrors \u000fk and \u000f0k propagate through the iterations of an\nAMPI algorithm. We only outline the main arguments\nthat will lead to the performance bound of Thm. 1 and\nreport most proofs in (Scherrer et al., 2012). We follow\nthe line of analysis developped by Thiery & Scherrer\n(2010). The results are obtained using the following\nthree quantities:\n1) The distance between the optimal value function\nand the value before approximation at the k th itera\u2206\ntion: dk = v\u2217 \u2212 (T\u03c0k )m vk\u22121 = v\u2217 \u2212 (vk \u2212 \u000fk ).\n2) The shift between the value before approximation\n\u2206\nand the value of the policy at the k th iteration: sk =\nm\n(T\u03c0k ) vk\u22121 \u2212 v\u03c0k = (vk \u2212 \u000fk ) \u2212 v\u03c0k .\n3) The Bellman residual at the k\nvk \u2212 T\u03c0k+1 vk .\n\nth\n\n\u2206\n\niteration: bk =\n\nWe are interested in finding an upper bound on the\n\u2206\nloss lk = v\u2217 \u2212 v\u03c0k = dk + sk . To do so, we will upper bound dk and sk , which requires a bound on the\nBellman residual bk . More precisely, the core of our\nanalysis is to prove the following point-wise inequalities for our three quantities of interest.\n\n(\u03b3P\u03c0k+1 )j bk ,\n\nj=1\n\nsk = (\u03b3P\u03c0k ) (I \u2212 \u03b3P\u03c0k )\u22121 bk\u22121 .\nm\n\nAMPI-Q: \u000f0k = 0 and \u000fk is the error in fitting the\nstate-action value function Qk .\nCBMPI: This algorithm iterates as follows:\n\nm\u22121\nX\n\nSince the stochastic kernels are non-negative, the\nbounds in Lemma 1 indicate that the loss lk will be\nbounded if the errors \u000fk and \u000f0k are controlled. In fact,\nif we define \u000f as a uniform upper-bound on the errors\n|\u000fk | and |\u000f0k |, the first inequality in Lemma 1 implies\nthat bk \u2264 O(\u000f), and as a result, the second and third\ninequalities gives us dk \u2264 O(\u000f) and sk \u2264 O(\u000f). This\nmeans that the loss will also satisfy lk \u2264 O(\u000f).\nOur bound for the loss lk is the result of careful expansion and combination of the three inequalities in\nLemma 1. Before we state this result, we introduce\nsome notations that will ease our formulation.\nDefinition 1. For a positive integer n, we define Pn as\nthe set of transition kernels that are defined as follows:\n1) for any set of n policies\n(\u03b3P\u03c01 )(\u03b3P\u03c02 ) . . . (\u03b3P\u03c0n ) \u2208 Pn ,\n\n{\u03c01 , . . . , \u03c0n },\n\n2) for any \u03b1 \u2208 (0, 1) and (P1 , P2 ) \u2208 Pn \u00d7 Pn , \u03b1P1 +\n(1 \u2212 \u03b1)P2 \u2208 Pn .\nFurthermore, we use the somewhat abusive notation\n\u0393n for denoting any element of Pn . For example, if we\nwrite a transition kernel P as P = \u03b11 \u0393i + \u03b12 \u0393j \u0393k =\n\u03b11 \u0393i +\u03b12 \u0393j+k , it should be read as there exist P1 \u2208 Pi ,\nP2 \u2208 Pj , P3 \u2208 Pk , and P4 \u2208 Pk+j such that P =\n\u03b11 P1 + \u03b12 P2 P3 = \u03b11 P1 + \u03b12 P4 .\nUsing the notation introduced in Definition 1, we now\nderive a point-wise bound on the loss.\nLemma 2 (Proof in (Scherrer et al., 2012, Appendix B)). After k iterations, the losses of AMPI-V\nand AMPI-Q satisfy\nlk \u2264 2\n\nk\u22121\n\u221e\nXX\n\n\u0393j |\u000fk\u2212i | +\n\ni=1 j=i\n\nk\u22121\n\u221e\nXX\n\n\u0393j |\u000f0k\u2212i | + h(k),\n\ni=0 j=i\n\nwhile the loss of CBMPI satisfies\nlk \u2264 2\n\nk\u22122\nX\n\n\u221e\nX\n\n\u0393j |\u000fk\u2212i\u22121 | +\n\ni=1 j=i+m\n\u2206\n\nwhere h(k) = 2\n\nk\u22121\n\u221e\nXX\n\n\u0393j |\u000f0k\u2212i | + h(k),\n\ni=0 j=i\n\nP\u221e\n\nj=k\n\n\u2206\n\n\u0393j |d0 | or h(k) = 2\n\nP\u221e\n\nj=k\n\n\u0393j |b0 |.\n\n\fApproximate Modified Policy Iteration\n\nRemark 1. A close look at the existing point-wise\nerror bounds for AVI (Munos, 2007, Lemma 4.1) and\nAPI (Munos, 2003, Corollary 10) shows that they do\nnot consider error in the greedy step (i.e., \u000f0k = 0) and\nthat they have the following form:\nlim supk\u2192\u221e lk \u2264 2 lim supk\u2192\u221e\n\nk\u22121\n\u221e\nXX\n\nwith the following concentrability coefficients\n\n|f | \u2264\n\nXX\ni\u2208I j\u2208Ji\n\n\u0393j |gi | =\n\nn X X\nX\n\n\u0393j |gi |.\n\nl=1 i\u2208Il j\u2208Ji\n\nThen for all p, q and q 0 such that 1q + q10 = 1, and for\nall distributions \u03c1 and \u03bc, we have\nn\nX\nXX\n\u00011/p\nkf kp,\u03c1 \u2264\nCq (l)\nsup kgi kpq0 ,\u03bc\n\u03b3j ,\nl=1\n\ni\u2208Il\n\ni\u2208Il j\u2208Ji\n\n4\nNote however that the dependence on m will reappear\nif we make explicit what is hidden in the terms \u0393j .\n\nP\n\nj\u2208Ji\n\nP\n\n\u03b3 j cq (j)\n\nP\n\nj\u2208Ji\n\n\u03b3j\n\n,\n\nwith the Radon-Nikodym derivative based quantity\nd(\u03c1P\u03c01 P\u03c02 * * * P\u03c0j )\nd\u03bc\n\n\u2206\n\ncq (j) = max\n\n\u03c01 ,*** ,\u03c0j\n\ni=1 j=i\n\nThe next step is to show how the point-wise bound of\nLemma 2 can turn to a bound in weighted Lp -norm,\nwhich for any function f : S \u2192 R and any distribu\u00011/p\n\u2206 R\ntion \u03bc on S is defined as kf kp,\u03bc =\n|f (x)|p \u03bc(dx)\n.\nMunos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and\nthe recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show\nhow to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics\nof the MDP. We now state a lemma that generalizes\nthe analysis of Farahmand et al. (2010) to a larger class\nof concentrability coefficients. We will discuss the potential advantage of this new class in Remark 4. We\nwill also show through the proofs of Thms. 1 and 3,\nhow the result of Lemma 3 provides us with a flexible tool for turning point-wise bounds into Lp -norm\nbounds. Thm. 3 in (Scherrer et al., 2012, Appendix D)\nprovides an alternative bound for the loss of AMPI,\nwhich in analogy with the results of Farahmand et al.\n(2010) shows that the last iterations have the highest impact on the loss (the influence exponentially decreases towards the initial iterations).\nLemma 3 (Proof in (Scherrer et al., 2012, Appendix C)). Let I and (Ji )i\u2208I be sets of positive integers, {I1 , . . . , In } be a partition of I, and f and\n(gi )i\u2208I be functions satisfying\n\ni\u2208Il\n\ni\u2208Il\n\n\u0393j |\u000fk\u2212i |.\n\nThis indicates that the bound in Lemma 2 not only\nunifies the analysis of AVI and API, but it generalizes\nthem to the case of error in the greedy step and to a\nfinite horizon k. Moreover, our bound suggests that\nthe way the errors are propagated in the whole family\nof algorithms VI/PI/MPI does not depend on m at\nthe level of the abstraction suggested by Definition 1.4\n\nP\n\n\u2206\n\nCq (l) =\n\n(14)\nq,\u03bc\n\nWe now derive a Lp -norm bound for the loss of the\nAMPI algorithm by applying Lemma 3 to the pointwise bound of Lemma 2.\nTheorem 1 (Proof in (Scherrer et al., 2012, Appendix D)). Let \u03c1 and \u03bc be distributions over states.\nLet p, q, and q 0 be such that 1q + q10 = 1. After k\niterations, the loss of AMPI satisfies\n\u00011\n2(\u03b3 \u2212 \u03b3 k ) Cq1,k,0 p\nsup k\u000fj kpq0 ,\u03bc\n(15)\n(1 \u2212 \u03b3)2\n1\u2264j\u2264k\u22121\n\u00011\n(1 \u2212 \u03b3 k ) Cq0,k,0 p\n+\nsup k\u000f0j kpq0 ,\u03bc + g(k),\n(1 \u2212 \u03b3)2\n1\u2264j\u2264k\n\nklk kp,\u03c1 \u2264\n\nwhile the loss of CBMPI satisfies\nklk kp,\u03c1 \u2264\n\n2\u03b3 m (\u03b3 \u2212 \u03b3 k\u22121 ) Cq2,k,m\n(1 \u2212 \u03b3)2\n\n\u0001 p1\nsup\n\nk\u000fj kpq0 ,\u03bc\n\n1\u2264j\u2264k\u22122\n\n(16)\nk\n\n+\n\n(1 \u2212 \u03b3 )\n(1 \u2212\n\n\u00011\n\nCq1,k,0 p\n\u03b3)2\n\nsup k\u000f0j kpq0 ,\u03bc + g(k),\n1\u2264j\u2264k\n\nwhere for all q, l, k and d, the concentrability coefficients Cql,k,d are defined as\n\u2206\n\nCql,k,d =\n\nk\u22121 \u221e\n(1 \u2212 \u03b3)2 X X j\n\u03b3 cq (j + d),\n\u03b3l \u2212 \u03b3k\nj=i\ni=l\n\nwith cq (j) given by Eq. 14, and g(k) is defined as\n\u00011\n\u0001\n\u2206 2\u03b3 k\ng(k) = 1\u2212\u03b3\nCqk,k+1 p min kd0 kpq0 ,\u03bc , kb0 kpq0 ,\u03bc .\nRemark 2. When p tends to infinity, the first bound\nof Thm. 1 reduces to\nklk k\u221e \u2264\n\n2(\u03b3 \u2212 \u03b3 k )\n(1 \u2212 \u03b3)2\n\nsup\n\nk\u000fj k\u221e +\n\n1\u2264j\u2264k\u22121\n\n1 \u2212 \u03b3k\nsup k\u000f0j k\u221e\n(1 \u2212 \u03b3)2 1\u2264j\u2264k\n\n2\u03b3 k\n+\nmin(kd0 k\u221e , kb0 k\u221e ).\n1\u2212\u03b3\n\n(17)\n\nWhen k goes to infinity, Eq. 17 gives us a generalization of the API (m = \u221e) bound of Bertsekas &\nTsitsiklis (1996, Prop. 6.2), i.e.,\nlim sup klk k\u221e \u2264\nk\u2192\u221e\n\n2\u03b3 supj k\u000fj k\u221e + supj k\u000f0j k\u221e\n.\n(1 \u2212 \u03b3)2\n\nMoreover, since our point-wise analysis generalizes\nthose of API and AVI (as noted in Remark 1), the\nLp -bound of Eq. 15 unifies and generalizes those for\nAPI (Munos, 2003) and AVI (Munos, 2007).\n\n\fApproximate Modified Policy Iteration\n\nRemark 3. Canbolat & Rothblum (2012) recently\n(and independently) developped an analysis of an approximate form of MPI. Also, as mentionned, the proof\ntechnique that we used is based on that of Thiery &\nScherrer (2010). While Canbolat & Rothblum (2012)\nonly consider the error in the greedy step and Thiery\n& Scherrer (2010) that in the value update, our work is\nmore general in that we consider both sources of error\n\u2013 this is required for the analysis of CBMPI. Thiery\n& Scherrer (2010) and Canbolat & Rothblum (2012)\nprovide bounds when the errors are controlled in maxnorm, while we consider the more general Lp -norm.\nAt a more technical level, Th. 2 in (Canbolat & Rothblum, 2012) bounds the norm of the distance v\u2217 \u2212 vk\nwhile we bound the loss v\u2217 \u2212 v\u03c0k . If we derive a bound\non the loss (using e.g., Th. 1 in (Canbolat & Rothblum, 2012)), this leads to a bound on the loss that\nis looser than ours. In particular, this does not allow\nto recover the standard bounds for AVI/API, as we\nmanaged to (c.f. Remark 2).\nRemark 4. We can balance the influence of the concentrability coefficients (the bigger the q, the higher\nthe influence) and the difficulty of controlling the errors (the bigger the q 0 , the greater the difficulty in\ncontrolling the Lpq0 -norms) by tuning the parameters\nq and q 0 , given the condition that 1q + q10 = 1. This\npotential leverage is an improvement over the existing\nbounds and concentrability results that only consider\nspecific values of these two parameters: q = \u221e and\nq 0 = 1 in Munos (2007); Munos & Szepesv\u00e1ri (2008),\nand q = q 0 = 2 in Farahmand et al. (2010).\nRemark 5. For CBMPI, the parameter m controls\nthe influence of the value function approximator, cancelling it out in the limit when m tends to infinity\n(see Eq. 16). Assuming a fixed budget of sample transitions, increasing m reduces the number of rollouts\nused by the classifier and thus worsens its quality; in\nsuch a situation, m allows to make a trade-off between\nthe estimation error of the classifier and the overall\nvalue function approximation.\n\nLemma 4 (Proof in (Scherrer et al., 2012, Appendix E)). Let \u03a0 be a policy space with finite VCdimension h = V C(\u03a0) and \u03bc be a distribution over the\n0\nstate space S. Let N be the number of states in Dk\u22121\ndrawn i.i.d. from \u03bc, M be the number of rollouts per\nb k\u22121 , and\nstate-action pair used in the estimation of Q\n\u03a0\nb\n\u03c0k = argmin\u03c0\u2208\u03a0 Lk\u22121 (b\n\u03bc, \u03c0) be the policy computed at\niteration k \u2212 1 of CBMPI. Then, for any \u03b4 > 0, we\nhave\n\u03a0\n0\n0\nk\u000f0k k1,\u03bc = L\u03a0\nk\u22121 (\u03bc; \u03c0k ) \u2264 inf Lk\u22121 (\u03bc; \u03c0) + 2(\u000f1 + \u000f2 ),\n\u03c0\u2208\u03a0\n\nwith probability at least 1 \u2212 \u03b4, where\nr\n\n2\n32 \u0001\neN\n+ log\n= 16Qmax\nh log\n,\nN\nh\n\u03b4\nr\n2\n32 \u0001\neM N\n+ log\n\u000f02 (N, M, \u03b4) = 8Qmax\nh log\n.\nMN\nh\n\u03b4\n\u000f01 (N, \u03b4)\n\nWe now consider the evaluation step error. The evaluation step at iteration k of CBMPI is a regression\nproblem\n(T\u03c0k )m vk\u22121 and a training\n\b (i)with the\n\u0001target\nn\n(i)\nset\ns , vbk (s ) i=1 in which the states s(i) are\ni.i.d. samples from \u03bc and vbk (s(i) ) are unbiased estimates of the target computed according to Eq. 7. Different function spaces F (linear or non-linear) may\nbe used to approximate (T\u03c0k )m vk\u22121 . Here we consider a linear architecture with parameters \u03b1 \u2208 Rd and\nbounded (by L) basis functions {\u03c6j }dj=1 , k\u03c6j k\u221e \u2264 L.\n\u0001>\nWe denote by \u03c6 : X \u2192 Rd , \u03c6(*) = \u03c61 (*), . . . , \u03c6d (*)\nthe feature vector, and by F the linear function space\nspanned by the features \u03c6j , i.e., F = {f\u03b1 (*) = \u03c6(*)> \u03b1 :\n\u03b1 \u2208 Rd }. Now if we define vk as the truncation (by\nVmax ) of the solution of the above linear regression\nproblem, we may bound the evaluation step error using the following lemma.\nLemma 5 (Proof in (Scherrer et al., 2012, Appendix F)). Consider the linear regression setting described above, then we have\nk\u000fk k2,\u03bc \u2264 4 inf k(T\u03c0k )m vk\u22121 \u2212 f k2,\u03bc + \u000f1 + \u000f2 ,\n\n5. Finite-Sample Analysis of CBMPI\nIn this section, we focus on CBMPI and detail the possible form of the error terms that appear in the bound\nof Thm. 1. We select CBMPI among the proposed algorithms because its analysis is more general than the\nothers as we need to bound both greedy and evaluation\nstep errors (in some norm), and also because it displays\nan interesting influence of the parameter m (see Remark 5). We first provide a bound on the greedy step\nerror. From the definition of \u000f0k for CBMPI (Eq. 12)\nand the description of the greedy step in CBMPI, we\ncan easily observe that k\u000f0k k1,\u03bc = L\u03a0\nk\u22121 (\u03bc; \u03c0k ).\n\nf \u2208F\n\nwith probability at least 1 \u2212 \u03b4, where\nr\n\n\u0010 27(12e2 n)2(d+1) \u0011\n2\n,\nlog\nn\n\u03b4\nr\n\u0010\n\u0011 2\n9\n\u000f2 (n, \u03b4) = 24 Vmax + k\u03b1\u2217 k2 * sup k\u03c6(x)k2\nlog ,\nn\n\u03b4\nx\n\u000f1 (n, \u03b4) = 32Vmax\n\nand \u03b1\u2217 is such that f\u03b1\u2217 is the best approximation\n(w.r.t. \u03bc) of the target function (T\u03c0k )m vk\u22121 in F.\nFrom Lemmas 4 and 5, we have bounds on k\u000f0k k1,\u03bc\nand k\u000fk k1,\u03bc \u2264 k\u000fk k2,\u03bc . By a union bound argument,\n\n\fApproximate Modified Policy Iteration\n\nwe thus control the r.h.s of Eq. 16 in L1 norm. In the\ncontext of Th. 1, this means p = 1, q 0 = 1 and q = \u221e,\nand we have the following bound for CBMPI:\nTheorem 2. Let d0 = supg\u2208F ,\u03c00 inf \u03c0\u2208\u03a0 L\u03a0\n\u03c0 0 ,g (\u03bc; \u03c0)\nand dm = supg\u2208F ,\u03c0 inf f \u2208F k(T\u03c0 )m g \u2212 f k2,\u03bc . With\nthe notations of Th. 1 and Lemmas 4-5, after k iterations, and with probability 1 \u2212 \u03b4, the expected loss\nE\u03bc [lk ] = klk k1,\u03bc of CBMPI is bounded by\n\u0013\n\u0012\n2,k,m\n2\u03b3 m (\u03b3 \u2212 \u03b3 k\u22121 )C\u221e\n\u03b4\n\u03b4\n) + \u000f2 (n,\n)\ndm + \u000f1 (n,\n(1 \u2212 \u03b3)2\n2k\n2k\n\u0013\n\u0012\n1,k,0\n(1 \u2212 \u03b3 k )C\u221e\n\u03b4\n\u03b4\n+\n) + \u000f02 (N, M,\n) + g(k).\nd0 + \u000f01 (N,\n(1 \u2212 \u03b3)2\n2k\n2k\n\nRemark 6. This result leads to a quantitative version of Remark 5. Assume that we have a fixed\nbudget for the actor and the critic B = nm =\nN M |A|m.\nThen, up to constants and logarithmic\nfactors,\nthe bound has the \u0013form klk k1,\u03bc \u2264\n\u0012\nq\npm\u0001\nm\nO \u03b3 dm + B + d0 + M |A|m\n. It shows the\nB\ntrade-off in the tuning of m: a big m can make the influence of the overall (approximation and estimation)\nvalue error small, but that of the estimation error of\nthe classifier bigger.\n\n6. Summary and Extensions\nIn this paper, we studied a DP algorithm, called modified policy iteration (MPI), that despite its generality\nthat contains the celebrated policy and value iteration methods, has not been thoroughly investigated in\nthe literature. We proposed three approximate MPI\n(AMPI) algorithms that are extensions of the wellknown ADP algorithms: fitted-value iteration, fittedQ iteration, and classification-based policy iteration.\nWe reported an error propagation analysis for AMPI\nthat unifies those for approximate policy and value\niteration. We also provided a finite-sample analysis\nfor the classification-based implementation of AMPI\n(CBMPI), whose analysis is more general than the\nother presented AMPI methods. Our results indicate that the parameter of MPI allows us to control\nthe balance of errors (in value function approximation\nand estimation of the greedy policy) in the final performance of CBMPI. Although AMPI generalizes the\nexisting AVI and classification-based API algorithms,\nadditional experimental work and careful theoretical\nanalysis are required to obtain a better understanding\nof the behaviour of its different implementations and\ntheir relation to the competitive methods. Extension\nof CBMPI to problems with continuous action space\nis another interesting direction to pursue.\n\nReferences\nAntos, A., Munos, R., and Szepesv\u00e1ri, Cs. Fitted Qiteration in continuous action-space MDPs. In Proceedings of NIPS, pp. 9\u201316, 2007.\nBertsekas, D. and Tsitsiklis, J. Neuro-Dynamic Programming. Athena Scientific, 1996.\nCanbolat, Pelin and Rothblum, Uriel. (approximate) iterated successive approximations algorithm for sequential\ndecision processes. Annals of Operations Research, pp.\n1\u201312, 2012. ISSN 0254-5330.\nErnst, D., Geurts, P., and Wehenkel, L. Tree-based batch\nmode reinforcement learning. Journal of Machine Learning Research, 6:503\u2013556, 2005.\nFarahmand, A., Munos, R., and Szepesv\u00e1ri, Cs. Error\npropagation for approximate policy and value iteration.\nIn Proceedings of NIPS, pp. 568\u2013576, 2010.\nFern, A., Yoon, S., and Givan, R. Approximate Policy Iteration with a Policy Language Bias: Solving Relational\nMarkov Decision Processes. Journal of Artificial Intelligence Research, 25:75\u2013118, 2006.\nGabillon, V., Lazaric, A., Ghavamzadeh, M., and Scherrer,\nB. Classification-based policy iteration with a critic. In\nProceedings of ICML, pp. 1049\u20131056, 2011.\nLagoudakis, M. and Parr, R. Reinforcement Learning as\nClassification: Leveraging Modern Classifiers. In Proceedings of ICML, pp. 424\u2013431, 2003.\nLazaric, A., Ghavamzadeh, M., and Munos, R. Analysis\nof a Classification-based Policy Iteration Algorithm. In\nProceedings of ICML, pp. 607\u2013614, 2010.\nMunos, R. Error Bounds for Approximate Policy Iteration.\nIn Proceedings of ICML, pp. 560\u2013567, 2003.\nMunos, R. Performance Bounds in Lp -norm for Approximate Value Iteration. SIAM J. Control and Optimization, 46(2):541\u2013561, 2007.\nMunos, R. and Szepesv\u00e1ri, Cs. Finite-Time Bounds for\nFitted Value Iteration. Journal of Machine Learning\nResearch, 9:815\u2013857, 2008.\nPuterman, M. and Shin, M. Modified policy iteration algorithms for discounted Markov decision problems. Management Science, 24(11), 1978.\nScherrer, Bruno, Gabillon, Victor, Ghavamzadeh, Mohammad, and Geist, Matthieu. Approximate Modified Policy\nIteration. Technical report, INRIA, May 2012.\nSzepesv\u00e1ri, Cs. Reinforcement Learning Algorithms for\nMDPs. In Wiley Encyclopedia of Operations Research.\nWiley, 2010.\nThiery, Christophe and Scherrer, Bruno. Performance\nbound for Approximate Optimistic Policy Iteration.\nTechnical report, INRIA, 2010.\n\n\fApproximate Modified Policy Iteration\n\nSupplementary Material for\nApproximate Modified Policy Iteration\nA. Proof of Lemma 1\nBefore we start, we recall the following definitions:\nbk = vk \u2212 T\u03c0k+1 vk ,\n\ndk = v\u2217 \u2212 (T\u03c0k )m vk\u22121 = v\u2217 \u2212 (vk \u2212 \u000fk ),\n\nsk = (T\u03c0k )m vk\u22121 \u2212 v\u03c0k = (vk \u2212 \u000fk ) \u2212 v\u03c0k .\n\nBounding bk\n(a)\n\nbk = vk \u2212 T\u03c0k+1 vk = vk \u2212 T\u03c0k vk + T\u03c0k vk \u2212 T\u03c0k+1 vk \u2264 vk \u2212 T\u03c0k vk + \u000f0k+1\n(b)\n\n= vk \u2212 \u000fk \u2212 T\u03c0k vk + \u03b3P\u03c0k \u000fk + \u000fk \u2212 \u03b3P\u03c0k \u000fk + \u000f0k+1 = vk \u2212 \u000fk \u2212 T\u03c0k (vk \u2212 \u000fk ) + (I \u2212 \u03b3P\u03c0k )\u000fk + \u000f0k+1 .\n\n(18)\n\nUsing the definition of xk , i.e.,\n\u2206\n\nxk = (I \u2212 \u03b3P\u03c0k )\u000fk + \u000f0k+1 ,\n\n(19)\n\nwe may write Eq. (18) as\n(c)\n\nbk \u2264 vk \u2212 \u000fk \u2212 T\u03c0k (vk \u2212 \u000fk ) + xk = (T\u03c0k )m vk\u22121 \u2212 T\u03c0k (T\u03c0k )m vk\u22121 + xk = (T\u03c0k )m vk\u22121 \u2212 (T\u03c0k )m (T\u03c0k vk\u22121 ) + xk\n= (\u03b3P\u03c0k )m (vk\u22121 \u2212 T\u03c0k vk\u22121 ) + xk = (\u03b3P\u03c0k )m bk\u22121 + xk .\n\n(20)\n\n(a) From the definition of \u000f0k+1 , we have \u2200\u03c0 0 T\u03c00 vk \u2264 T\u03c0k+1 vk + \u000f0k+1 , thus this inequality holds also for \u03c0 0 = \u03c0k .\n(b) This step is due to the fact that for every v and v 0 , we have T\u03c0k (v + v 0 ) = T\u03c0k v + \u03b3P\u03c0k v 0 .\n(c) This is from the definition of \u000fk , i.e., vk = (T\u03c0k )m vk\u22121 + \u000fk .\nBounding dk\ndk+1 = v\u2217 \u2212 (T\u03c0k+1 )m vk = T\u03c0\u2217 v\u2217 \u2212 T\u03c0\u2217 vk + T\u03c0\u2217 vk \u2212 T\u03c0k+1 vk + T\u03c0k+1 vk \u2212 (T\u03c0k+1 )m vk\n(a)\n\n\u2264 \u03b3P\u03c0\u2217 (v\u2217 \u2212 vk ) + \u000f0k+1 + gk+1 = \u03b3P\u03c0\u2217 (v\u2217 \u2212 vk ) + \u03b3P\u03c0\u2217 \u000fk \u2212 \u03b3P\u03c0\u2217 \u000fk + \u000f0k+1 + gk+1\n\nm\u22121\nX\n\u0001\n(c)\n= \u03b3P\u03c0\u2217 v\u2217 \u2212 (vk \u2212 \u000fk ) + yk + gk+1 = \u03b3P\u03c0\u2217 dk + yk + gk+1 = \u03b3P\u03c0\u2217 dk + yk +\n(\u03b3P\u03c0k+1 )j bk .\n\n(b)\n\n(21)\n\nj=1\n\n(a) This step is from the definition of \u000f0k+1 (see step (a) in bounding bk ) and by defining gk+1 as follows:\n\u2206\n\ngk+1 = T\u03c0k+1 vk \u2212 (T\u03c0k+1 )m vk .\n\n(22)\n\n(b) This is from the definition of yk , i.e.,\n\u2206\n\nyk = \u2212\u03b3P\u03c0\u2217 \u000fk + \u000f0k+1 .\n\n(23)\n\n(c) This step comes from rewriting gk+1 as\ngk+1 = T\u03c0k+1 vk \u2212 (T\u03c0k+1 )m vk =\n\nm\u22121\nX\n\nX\u0002\n\u0002\n\u0003 m\u22121\n\u0003\n(T\u03c0k+1 )j vk \u2212 (T\u03c0k+1 )j+1 vk =\n(T\u03c0k+1 )j vk \u2212 (T\u03c0k+1 )j (T\u03c0k+1 vk )\n\nj=1\n\n=\n\nj=1\n\nm\u22121\nX\n\nm\u22121\nX\n\nj=1\n\nj=1\n\n(\u03b3P\u03c0k+1 )j (vk \u2212 T\u03c0k+1 vk ) =\n\n(\u03b3P\u03c0k+1 )j bk .\n\n(24)\n\n\fApproximate Modified Policy Iteration\n\nBounding sk\n\nWith some slight abuse of notation, we have\nv\u03c0k = (T\u03c0k )\u221e vk\n\nand thus:\n\n(a)\n\nsk = (T\u03c0k )m vk\u22121 \u2212 v\u03c0k = (T\u03c0k )m vk\u22121 \u2212 (T\u03c0k )\u221e vk\u22121 = (T\u03c0k )m vk\u22121 \u2212 (T\u03c0k )m (T\u03c0k )\u221e vk\u22121\n\u221e\nX\n\u0001\n\u0002\n\u0003\n= (\u03b3P\u03c0k )m vk\u22121 \u2212 (T\u03c0k )\u221e vk\u22121 = (\u03b3P\u03c0k )m\n(T\u03c0k )j vk\u22121 \u2212 (T\u03c0k )j+1 vk\u22121\nj=0\n\n= (\u03b3P\u03c0k )m\n\n\u221e\n\u221e\n\u0010X\n\u0011\nX\n\u0002\n\u0003\n(T\u03c0k )j vk\u22121 \u2212 (T\u03c0k )j T\u03c0k vk\u22121 = (\u03b3P\u03c0k )m\n(\u03b3P\u03c0k )j (vk\u22121 \u2212 T\u03c0k vk\u22121 )\nj=0\n\nj=0\n\n= (\u03b3P\u03c0k )m (I \u2212 \u03b3P\u03c0k )\u22121 (vk\u22121 \u2212 T\u03c0k vk\u22121 ) = (\u03b3P\u03c0k )m (I \u2212 \u03b3P\u03c0k )\u22121 bk .\n(a) For any v, we have v\u03c0k = (T\u03c0k )\u221e v. This step follows by setting v = vk\u22121 , i.e., v\u03c0k = (T\u03c0k )\u221e vk\u22121 .\n\n(25)\n\n\fApproximate Modified Policy Iteration\n\nB. Proof of Lemma 2\nWe begin by focusing our analysis on AMPI. Here we are interested in bounding the loss lk = v\u2217 \u2212 v\u03c0k = dk + sk .\nBy induction, from Eqs. (20) and (21), we obtain\nk\nX\n\n\u0393m(k\u2212i) xi + \u0393mk b0 ,\n\n(26)\n\nk\u22121\nX\n\nm\u22121\n\u0010\n\u0011\nX\n\u0393k\u22121\u2212j yj +\n\u0393l bj + \u0393k d0 .\n\n(27)\n\nj=0\n\nl=1\n\nbk \u2264\n\ni=1\n\ndk \u2264\n\nin which we have used the notation introduced in Definition 1. In Eq. (27), we also used the fact that from\nPm\u22121\nEq. (24), we may write gk+1 = j=1 \u0393j bk . Moreover, we may rewrite Eq. (25) as\nsk = \u0393m\n\n\u221e\nX\n\n\u0393j bk\u22121 =\n\nj=0\n\nBounding lk\n\n\u221e\nX\n\n\u0393m+j bk\u22121 .\n\n(28)\n\nj=0\n\nFrom Eqs. (26) and (27), we may write\ndk \u2264\n\nk\u22121\nX\n\n\u0393\n\nk\u22121\u2212j\n\nyj +\n\nm\u22121\nX\n\nj=0\n\n=\n\nk\nX\n\n\u0393\n\nl\n\nj\n\u0010X\n\nxi + \u0393\n\nmj\n\nb0\n\n\u0011\n\n!\n+ \u0393k d0\n\ni=1\n\nl=1\n\n\u0393i\u22121 yk\u2212i +\n\n\u0393\n\nm(j\u2212i)\n\nj\nk\u22121\nX m\u22121\nXX\n\n\u0393k\u22121\u2212j+l+m(j\u2212i) xi + zk ,\n\n(29)\n\nj=0 l=1 i=1\n\ni=1\n\nwhere we used the following definition\n\u2206\n\nzk =\n\nk\u22121\nX m\u22121\nX\n\n\u0393k\u22121+l+j(m\u22121) b0 + \u0393k d0 =\n\nmk\u22121\nX\n\nj=0 l=1\n\n\u0393i b0 + \u0393k d0 .\n\ni=k\n\nThe triple sum involved in Eq. (29) may be written as\nj\nk\u22121\nX m\u22121\nXX\n\n\u0393k\u22121\u2212j+l+m(j\u2212i) xi =\n\nk\u22121\nX m\u22121\nX\nX k\u22121\n\n\u0393k\u22121+l+j(m\u22121)\u2212mi xi =\n\n=\n\nk\u22121\nX m(k\u2212i)\u22121\nX\ni=1\n\nmk\u22121\nX\n\n\u0393j\u2212mi xi\n\ni=1 j=mi+k\u2212i\n\ni=1 j=i l=1\n\nj=0 l=1 i=1\n\nk\u22121\nX\n\n\u0393j xi =\n\nk\u22121\nX mi\u22121\nX\n\n\u0393j xk\u2212i .\n\n(30)\n\ni=1 j=i\n\nj=k\u2212i\n\nUsing Eq. (30), we may write Eq. (29) as\ndk \u2264\n\nk\nX\n\n\u0393i\u22121 yk\u2212i +\n\ni=1\n\nk\u22121\nX mi\u22121\nX\n\n\u0393j xk\u2212i + zk .\n\n(31)\n\ni=1 j=i\n\nSimilarly, from Eqs. (28) and (26), we have\nsk \u2264\n\n\u221e\nX\n\n\u0393\n\nm+j\n\nj=0\n\n=\n\nk\u22121\n\u221e\nXX\ni=1 j=0\n\n\u0010 k\u22121\nX\n\nm(k\u22121\u2212i)\n\n\u0393\n\nxi + \u0393\n\nm(k\u22121)\n\ni=1\n\n\u0393j+m(k\u2212i) xi +\n\n\u0011\n\nb0 =\n\n\u221e \u0010 k\u22121\nX\nX\nj=0\n\n\u221e\nX\nj=0\n\n\u0393j+mk b0 =\n\nk\u22121\n\u221e\nXX\ni=1 j=0\n\n\u0393m+j+m(k\u22121\u2212i) xi + \u0393m+j+m(k\u22121) b0\n\n\u0011\n\ni=1\n\n\u0393j+mi xk\u2212i +\n\n\u221e\nX\nj=mk\n\n\u0393j b0 =\n\nk\u22121\nX\n\n\u221e\nX\n\ni=1 j=mi\n\n\u0393j xk\u2212i + zk0 ,\n\n(32)\n\n\fApproximate Modified Policy Iteration\n\nwhere we used the following definition\n\u221e\nX\n\n\u2206\n\nzk0 =\n\n\u0393j b0 .\n\nj=mk\n\nFinally, using the bounds in Eqs. (31) and (32), we obtain the following bound on the loss\nlk \u2264 dk + sk \u2264\n\nk\nX\n\n\u0393i\u22121 yk\u2212i +\n\ni=1\nk\nX\n\n=\n\nk\u22121\nX \u0010 mi\u22121\nX\ni=1\n\n\u0393i\u22121 yk\u2212i +\n\ni=1\n\nk\u22121\n\u221e\nXX\n\n\u0393j +\n\nj=i\n\n\u221e\nX\n\n\u0011\n\u0393j xk\u2212i + zk + zk0\n\nj=mi\n\n\u0393j xk\u2212i + \u03b7k ,\n\n(33)\n\ni=1 j=i\n\nwhere we used the following definition\n\u2206\n\n\u03b7k = zk + zk0 =\n\n\u221e\nX\n\n\u0393 j b0 + \u0393 k d 0 .\n\n(34)\n\nj=k\n\nNote that we have the following relation between b0 and d0\nb0 = v0 \u2212 T\u03c01 v0 = v0 \u2212 v\u2217 + T\u03c0\u2217 v\u2217 \u2212 T\u03c0\u2217 v0 + T\u03c0\u2217 v0 \u2212 T\u03c01 v0 \u2264 (I \u2212 \u03b3P\u03c0\u2217 )(\u2212d0 ) + \u000f01 ,\n\n(35)\n\nIn Eq. (35), we used the fact that v\u2217 = T\u03c0\u2217 v\u2217 , \u000f0 = 0, and T\u03c0\u2217 v0 \u2212 T\u03c01 v0 \u2264 \u000f01 (this is because the policy \u03c01 is\n\u000f01 -greedy w.r.t. v0 ). As a result, we may write |\u03b7k | either as\n|\u03b7k | \u2264\n\n\u221e\nX\n\n\u221e\n\u221e\n\u221e\nX\nX\nX\n\u0002\n\u0003\n\u0002\n\u0003\n\u0393j (I \u2212 \u03b3P\u03c0\u2217 )|d0 | + |\u000f01 | + \u0393k |d0 | \u2264\n\u0393j (I + \u03931 )|d0 | + |\u000f01 | + \u0393k |d0 | = 2\n\u0393j |d0 | +\n\u0393j |\u000f01 |, (36)\n\nj=k\n\nj=k\n\nj=k\n\nj=k\n\nor using the fact that from Eq. (35), we have d0 \u2264 (I \u2212 \u03b3P\u03c0\u2217 )\u22121 (\u2212b0 + \u000f01 ), as\n|\u03b7k | \u2264\n\n\u221e\nX\n\n\u0393j |b0 | + \u0393k\n\n\u221e\n\u221e\n\u221e\n\u221e\n\u221e\nX\nX\nX\nX\n\u0001\n\u0001 X\n\u0393j |\u000f01 |. (37)\n\u0393j |b0 | +\n\u0393j |b0 | + \u0393k\n\u0393j |b0 | + |\u000f01 | = 2\n(\u03b3P\u03c0\u2217 )j |b0 | + |\u000f01 | =\nj=0\n\nj=k\n\nj=0\n\nj=k\n\nj=k\n\nj=k\n\nNow, using the definitions of xk and yk in Eqs. (19) and (23), the bound on |\u03b7k | in Eq. (36) or (37), and the fact\nthat \u000f0 = 0, we obtain\n|lk | \u2264\n\nk\nX\n\n\u0393\n\ni\u22121\n\n\u221e\nXX\n\u0002\n\u0003\n\u0002 1\n\u0003 k\u22121\n0\n\u0393j (I + \u03931 )|\u000fk\u2212i | + |\u000f0k\u2212i+1 | + |\u03b7k |\n\u0393 |\u000fk\u2212i | + |\u000fk\u2212i+1 | +\ni=1 j=i\n\ni=1\n\n=\n\nk\u22121\nX\u0010\n\n\u0393i +\n\ni=1\n\n=2\n\nk\u22121\n\u221e\nXX\ni=1 j=i\n\n\u221e\nX\n\nk\u22121\n\u221e\n\u221e\n\u0011\n\u0011\nX\u0010\nX\nX\n(\u0393j + \u0393j+1 ) |\u000fk\u2212i | + \u0393k |\u000f0 | +\n\u0393i\u22121 +\n\u0393j |\u000f0k\u2212i+1 | + \u0393k\u22121 |\u000f01 | +\n\u0393j |\u000f01 | + h(k)\n\nj=i\n\n\u0393j |\u000fk\u2212i | +\n\ni=1\nk\u22121\nX\n\n\u221e\nX\n\n\u0393j |\u000f0k\u2212i+1 | +\n\ni=1 j=i\u22121\n\n\u221e\nX\n\nj=i\n\nj=k\n\n\u0393j |\u000f01 | + h(k) = 2\n\nk\u22121\n\u221e\nXX\n\n\u0393j |\u000fk\u2212i | +\n\ni=1 j=i\n\nj=k\u22121\n\nk\u22121\n\u221e\nXX\n\n\u0393j |\u000f0k\u2212i | + h(k),\n\ni=0 j=i\n\n(38)\nwhere we used the following definition\n\u2206\n\nh(k) = 2\n\n\u221e\nX\nj=k\n\n\u0393j |d0 |,\n\nor\n\n\u2206\n\nh(k) = 2\n\n\u221e\nX\n\n\u0393j |b0 |.\n\nj=k\n\nWe end this proof by adapting the error propagation to CBMPI. As expressed by Eqs. 12 and 13 in Sec. 4,\nan analysis of CBMPI can be deduced from that we have just done by replacing vk with the auxiliary variable\n\n\fApproximate Modified Policy Iteration\n\nwk = (T\u03c0k )m vk\u22121 and \u000fk with (\u03b3P\u03c0k )m \u000fk\u22121 = \u0393m \u000fk\u22121 . Therefore, using the fact that \u000f0 = 0, we can rewrite the\nbound of Eq. 38 for CBMPI as follows:\nlk \u2264 2\n\nk\u22121\n\u221e\nXX\n\n\u0393\n\nj+m\n\n|\u000fk\u2212i\u22121 | +\n\ni=1 j=i\n\n=2\n\nk\u22122\nX\n\n\u221e\nX\n\ni=1 j=m+i\n\nk\u22121\n\u221e\nXX\n\n\u0393j |\u000f0k\u2212i | + h(k)\n\ni=0 j=i\n\n\u0393j |\u000fk\u2212i\u22121 | +\n\nk\u22121\n\u221e\nXX\ni=0 j=i\n\n\u0393j |\u000f0k\u2212i | + h(k).\n\n(39)\n\n\fApproximate Modified Policy Iteration\n\nC. Proof of Lemma 3\nFor any integer t and vector z, the definition of \u0393t and the H\u00f6lder's inequality imply that\n\u03c1\u0393t |z| = \u0393t |z|\n\n1,\u03c1\n\n\u0010\n\u0011 10\n0\nq\n.\n\u2264 \u03b3 t cq (t)kzkq0 ,\u03bc = \u03b3 t cq (t) \u03bc|z|q\n\nWe define\n\u2206\n\nK=\n\nn\nX\n\n\uf8f6\n\n\uf8eb\n\u03bel \uf8ed\n\n(40)\n\nXX\n\n\u03b3j \uf8f8 ,\n\ni\u2208Il j\u2208Ji\n\nl=1\n\nwhere {\u03bel }nl=1 is a set of non-negative numbers that we will specify later. We now have\nkf kpp,\u03c1 = \u03c1|f |p\nPn P\nl=1\n\n\u2264 K p\u03c1\n\nl=1 \u03bel\n\n\u2264 K p\u03c1\n\n(b)\n\n\u2264K\n\nP\n\ni\u2208Il\n\nP\n\nj\u2208Ji\n\nPn\n\nP\n\nK\nP\n\nPn\n\nP\n\nP\n\nPn\n\n\u0010P\n\nP\n\nPn\n\n\u0010P\n\nP\n\nl=1 \u03bel\n\ni\u2208Il\n\np\n\n\u0393j\n\n\u0010\n\n\uf8eb Pn\n\n!p\n\nl=1 \u03bel\n\n= K p\u03c1 \uf8ed\n\n|gi |\n\u03bel\n\n\u0011p\n\ni\u2208Il\n\nj\nj\u2208Ji \u0393\n\nP\n\n\u0010\n\n|gi |\n\u03bel\n\nl=1 \u03bel\n\nP\n\ni\u2208Il\n\nP\n\nj\u2208Ji\n\n\u0011 \uf8f6p\n\uf8f8\n\nK\n\nPn\n= Kp\n\nP\n\n\u03c1\u0393j\n\n\u0010\n\n|gi |\n\u03bel\n\n\u0011p\n\nK\n\n\u0012 \u0010 \u0011 0 \u0013 q10\npq\n|gi |\nj\nj\u2208Ji \u03b3 cq (j) \u03bc\n\u03bel\nK\n\nl=1 \u03bel\n\n= Kp\n\ni\u2208Il\n\nj\nj\u2208Ji \u03b3 cq (j)\n\n\u0010\n\nkgi kpq0 ,\u03bc\n\u03bel\n\n\u0011p\n\nK\n\n\u2264 Kp\n(c)\n\nj\nj\u2208Ji \u0393 |gi |\n\nK\nPn\n\n(a)\n\ni\u2208Il\n\nP\n\n= Kp\n\nl=1 \u03bel\n\nl=1 \u03bel\n\ni\u2208Il\n\ni\u2208Il\n\nj\u2208Ji\n\n\u0011p\n\u0011 \u0010 sup\ni\u2208Il kgi kpq 0 ,\u03bc\n\u03b3 j cq (j)\n\u03bel\n\nK\n\u0011p\n\u0011\n\u0010 sup\ni\u2208Il kgi kpq 0 ,\u03bc\nj\n\u03b3\nC\n(l)\nq\nj\u2208Ji\n\u03bel\nK\n\n,\n\nwhere (a) results from Jensen's inequality, (b) from Eq. 40, and (c) from the definition of Cq (l). Now, by setting\n\u00011/p\n\u03bel = Cq (l)\nsupi\u2208Il kgi kpq0 ,\u03bc , we obtain\n\u0010P\n\u0011\nPn\nP\nj\n\u03be\n\u03b3\nl=1 l\ni\u2208Il\nj\u2208Ji\nkf kpp,\u03c1 \u2264 K p\n= K p,\nK\nwhere the last step follows from the definition of K.\n\n\fApproximate Modified Policy Iteration\n\nD. Proof of Theorem 1 & other Bounds on the Loss\nProof. We only detail the proof for AMPI (the proof being similar for CBMPI). We define I = {1, 2, * * * , 2k},\nthe partition I = {I1 , I2 , I3 } as I1 = {1, . . . , k \u2212 1}, I2 = {k, . . . , 2k \u2212 1}, and I3 = {2k}, and for each i \u2208 I\n\uf8f1\n\uf8f1\nif 1 \u2264 i \u2264 k \u2212 1,\nif 1 \u2264 i \u2264 k \u2212 1,\n\uf8f2 2\u000fk\u2212i\n\uf8f2 {i, i + 1, * * * }\n\u000f0k\u2212(i\u2212k)\nif k \u2264 i \u2264 2k \u2212 1,\n{i \u2212 k, i \u2212 k + 1, * * * } if k \u2264 i \u2264 2k \u2212 1,\ngi =\nand\nJi =\n\uf8f3\n\uf8f3\n{k, k + 1, * * * }\nif i = 2k.\n2d0 (or 2b0 ) if i = 2k,\nNote that here we have divided the terms in the point-wise bound of Lemma 2 into three groups: the evaluation\n0 k\nerror terms {\u000fj }k\u22121\nj=1 , the greedy step error terms {\u000fj }j=1 , and finally the residual term h(k). With the above\ndefinitions and the fact that the loss lk is non-negative, Lemma 2 may be rewritten as\n|lk | \u2264\n\n3 X X\nX\n\n\u0393j |gi |.\n\nl=1 i\u2208Il j\u2208Ji\n\nThe result follows by applying Lemma 3 and noticing that\n\nPk\u22121 P\u221e\ni=i0\n\nj=i\n\n\u03b3j =\n\n\u03b3 i0 \u2212\u03b3 k\n(1\u2212\u03b3)2 .\n\nHere in oder to show the flexibility of Lemma 3, we group the terms differently and derive an alternative Lp bound for the loss of AMPI and CBMPI. In analogy with the results of Farahmand et al. (2010), this new bound\nshows that the last iterations have the highest influence on the loss (the influence exponentially decreases towards\nthe initial iterations).\nTheorem 3. With the notations of Theorem 1, after k iterations, the loss of AMPI satisfies\nklk kp,\u03c1 \u2264 2\n\nk\u22121\nX\ni=1\n\nk\u22121\n\nX \u03b3i\n\u00011\n\u00011\n\u03b3i\nCqi,i+1 p k\u000fk\u2212i kpq0 ,\u03bc +\nCqi,i+1 p k\u000f0k\u2212i kpq0 ,\u03bc + g(k).\n1\u2212\u03b3\n1\u2212\u03b3\ni=0\n\nwhile the loss of CBMPI satisfies\nklk kp,\u03c1 \u2264 2\u03b3 m\n\nk\u22122\nX\ni=1\n\nk\u22121\n\nX \u03b3i\n\u00011\n\u00011\n\u03b3i\nCqi,i+1 p k\u000fk\u2212i\u22121 kpq0 ,\u03bc +\nCqi,i+1 p k\u000f0k\u2212i kpq0 ,\u03bc + g(k).\n1\u2212\u03b3\n1\u2212\u03b3\ni=0\n\nProof. Again, we only detail the proof for AMPI (the proof being similar for CBMPI). We define I, (gi ) and\n(Ji ) as in the proof of Theorem 1. We then make as many groups as terms, i.e., for each n \u2208 {1, 2, . . . , 2k \u2212 1},\nwe define In = {n}. The result follows by application of Lemma 3.\n\n\fApproximate Modified Policy Iteration\n\nE. Proof of Lemma 4\nThe proof of this lemma is similar to the proof of Theorem 1 in Lazaric et al. (2010). Before stating the proof,\nwe report the following two lemmas that are used in the proof.\nLemma 6. Let \u03a0 be a policy space with finite VC-dimension h = V C(\u03a0) < \u221e and N be the number of states\nin the rollout set Dk\u22121 drawn i.i.d. from the state distribution \u03bc. Then we have\n\u0015\n\u0014\n\u03a0\n>\n\u000f\n\u2264\u03b4,\n(b\n\u03bc\n;\n\u03c0)\n\u2212\nL\n(\u03bc;\n\u03c0)\nPDk\u22121 sup L\u03a0\nk\u22121\nk\u22121\n\u03c0\u2208\u03a0\n\nwith \u000f = 16Qmax\n\nq\n\n2\nN\n\nh log\n\neN\nh\n\n+ log\n\n8\n\u03b4\n\n\u0001\n\n.\n\nProof. This is a restatement of Lemma 1 in Lazaric et al. (2010).\nLemma 7. Let \u03a0 be a policy space with finite VC-dimension h = V C(\u03a0) < \u221e and s(1) , . . . , s(N ) be an arbitrary\nsequence of states. At each state we simulate M independent rollouts of the form , then we have\n\uf8ee\n\uf8f9\nN\nM\nN\nX\nX\nX\n\u0001\n\u0001\n1\n1\n1\nj\nRk\u22121\ns(i,j) , \u03c0(s(i,j) ) \u2212\nQk\u22121 s(i,j) , \u03c0(s(i,j) ) > \u000f\uf8fb \u2264 \u03b4 ,\nP \uf8f0 sup\nN i=1\n\u03c0\u2208\u03a0 N i=1 M j=1\nwith \u000f = 8Qmax\n\nq\n\n2\nMN\n\neM N\nh\n\nh log\n\n+ log\n\n8\n\u03b4\n\n\u0001\n\n.\n\nProof. The proof is similar to the one for Lemma 6.\nProof. (Lemma 4) Let a\u2217 (*) = argmaxa\u2208A Qk\u22121 (*, a) be the greedy action. To simplify the notation, we remove\nthe dependency of a\u2217 on states and use a\u2217 instead of a\u2217 (xi ) in the following. We prove the following series of\ninequalities:\n(a)\n\n\u03a0\nL\u03a0\n\u03bc; \u03c0k ) + \u000f01\nw.p. 1 \u2212 \u03b4 0\nk\u22121 (\u03bc; \u03c0k ) \u2264 Lk\u22121 (b\nN\n\u0001i\n1 Xh\n=\nQk\u22121 (xi , a\u2217 ) \u2212 Qk\u22121 xi , \u03c0k (xi ) + \u000f01\nN i=1\n(b)\n\n\u2264\n\nN\n\u0001i\n1 Xh\nb k\u22121 xi , \u03c0k (xi ) + \u000f01 + \u000f02\nQk\u22121 (xi , a\u2217 ) \u2212 Q\nN i=1\n\nw.p. 1 \u2212 2\u03b4 0\n\nN\n\u0001i\n1 Xh\nb k\u22121 xi , \u03c0 \u2217 (xi ) + \u000f0 + \u000f0\nQk\u22121 (xi , a\u2217 ) \u2212 Q\n\u2264\n1\n2\nN i=1\n\n(c)\n\n\u2264\n\nN\n\u0001i\n1 Xh\nQk\u22121 (xi , a\u2217 ) \u2212 Qk\u22121 xi , \u03c0 \u2217 (xi ) + \u000f01 + 2\u000f02\nN i=1\n\n\u2217\n0\n0\n= L\u03a0\n\u03bc; \u03c0 \u2217 ) + \u000f01 + 2\u000f02 \u2264 L\u03a0\nk\u22121 (b\nk\u22121 (\u03bc; \u03c0 ) + 2(\u000f1 + \u000f2 )\n\nw.p. 1 \u2212 3\u03b4 0\nw.p. 1 \u2212 4\u03b4 0\n\n0\n0\n= inf L\u03a0\nk\u22121 (\u03bc; \u03c0) + 2(\u000f1 + \u000f2 ).\n\u03c0\u2208\u03a0\n\nThe statement of the theorem is obtained by \u03b4 0 = \u03b4/4.\n(a) This follows from Lemma 6.\nb k\u22121 by bounding\n(b) Here we introduce the estimated action-value function Q\n\u0014\nsup\n\u03c0\u2208\u03a0\n\n\u0015\nN\nN\n\u0001\n\u0001\n1 Xb\n1 X\nQk\u22121 s(i) , \u03c0(s(i) ) \u2212\nQk\u22121 s(i) , \u03c0(s(i) )\nN i=1\nN i=1\n\n\fApproximate Modified Policy Iteration\n\nusing Lemma 7.\n(c) From the definition of \u03c0k in CBMPI, we have\nN\n\u0001\n1 Xb\n\u03c0k = argmin Lb\u03a0\nQk\u22121 s(i) , \u03c0(s(i) ) ,\n\u03bc; \u03c0) = argmax\nk\u22121 (b\nN\n\u03c0\u2208\u03a0\n\u03c0\u2208\u03a0\ni=1\n\nthus, \u22121/N\n\nPN\n\ni=1\n\n\u0001\nb k\u22121 s(i) , \u03c0k (s(i) ) can be maximized by replacing \u03c0k with any other policy, particularly with\nQ\n\u0013\nZ \u0012\n\u0001\n\u03c0 \u2217 = argmin\nmax Qk\u22121 (s, a) \u2212 Qk\u22121 s, \u03c0(s) \u03bc(ds).\n\u03c0\u2208\u03a0\n\nS\n\na\u2208A\n\n\fApproximate Modified Policy Iteration\n\nFigure 2. The vectors used in the proof.\n\nF. Proof of Lemma 5\n\u0010\u0002\n\u0011>\n\u0003\n\u0002\n\u0003\nLet us define two n-dimensional vectors z =\n(T\u03c0k )m vk\u22121 (s(1) ), . . . , (T\u03c0k )m vk\u22121 (s(n) )\nand y =\n\u0001\n>\nb and yb =\nand their orthogonal projections onto the vector space Fn as zb = \u03a0z\nvbk (s(1) ), . . . , vbk (s(n) )\n\u0001>\n(1)\n(n)\nb = vek (s ), . . . , vek (s ) , where vek is the result of linear regression and its truncation (by Vmax ) is vk ,\n\u03a0y\ni.e., vk = T(e\nvk ) (see Figure 2). What we are interested is to find a bound on the regression error kz \u2212 ybk (the\ndifference between the target function z and the result of the regression yb). We may decompose this error as\nb n + kz \u2212 zbkn ,\nkz \u2212 ybkn \u2264 kb\nz \u2212 ybkn + kz \u2212 zbkn = k\u03bek\n\n(41)\n\nb with the noise vector \u03be = z \u2212 y defined as\nwhere\u0002 \u03beb = zb \u2212 yb is\nprojected noise (estimation error) \u03beb = \u03a0\u03be,\n\u0003 the\nm\n(i)\n\u03bei = (T\u03c0k ) vk\u22121 (s ) \u2212 vbk (s(i) ). It is easy to see that noise is zero mean, i.e., E[\u03bei ] = 0 and is bounded by\n2Vmax , i.e., |\u03bei | \u2264 2Vmax . We may write the estimation error as\nb 2 = h\u03be,\nb \u03bei\nb = h\u03be, \u03bei,\nb\nkb\nz \u2212 ybk2n = k\u03bek\nn\nwhere the last equality follows from the fact that \u03beb is the orthogonal projection of \u03be. Since \u03beb \u2208 Fn , let f\u03b1 \u2208 F\nbe any function whose values at {s(i) }ni=1 equals to {\u03bei }ni=1 . By application of a variation of Pollard's inequality (Gy\u00f6rfi et al., 2002), we obtain\ns\n\u0012\n\u0013\nn\n2 d+1\nX\n1\n(i)\nb n 2 log 3(9e n)\nb =\n\u03bei f\u03b1 (s ) \u2264 4Vmax k\u03bek\nh\u03be, \u03bei\n,\nn i=1\nn\n\u03b40\nwith probability at least 1 \u2212 \u03b4 0 . Thus, we have\ns\nb n \u2264 4Vmax\nkb\nz \u2212 ybkn = k\u03bek\n\n2\nlog\nn\n\n\u0012\n\n\u0013\n3(9e2 n)d+1\n.\n\u03b40\n\n(42)\n\nFrom Eqs. 41 and 42, we have\ns\nm\n\nm\n\nb \u03c0 )m vk\u22121 k\u03bcb + 4Vmax\nk(T\u03c0k ) vk\u22121 \u2212 vek k\u03bcb \u2264 k(T\u03c0k ) vk\u22121 \u2212 \u03a0(T\nk\n\n2\nlog\nn\n\n\u0012\n\n\u0013\n3(9e2 n)d+1\n,\n\u03b40\n\n(43)\n\nwhere \u03bc\nb is the empirical norm induced from the n i.i.d. samples from \u03bc.\n\u0002\n\u0003\nb \u03c0 )m vk\u22121 (s(i) ), and\nNow in order to obtain a random design bound, we first define f\u03b1b\u2217 \u2208 F as f\u03b1b\u2217 (s(i) ) = \u03a0(T\nk\nthen define f\u03b1\u2217 = \u03a0(T\u03c0k )m vk\u22121 that is the best approximation (w.r.t. \u03bc) of the target function (T\u03c0k )m vk\u22121 in\nF. Since f\u03b1b\u2217 is the minimizer of the empirical loss, any function in F different than f\u03b1b\u2217 has a bigger empirical\nloss, thus we have\nkf\u03b1b\u2217 \u2212 (T\u03c0k )m vk\u22121 k\u03bcb \u2264 kf\u03b1\u2217 \u2212 (T\u03c0k )m vk\u22121 k\u03bcb \u2264 2kf\u03b1\u2217 \u2212 (T\u03c0k )m vk\u22121 k\u03bc\n\n\fApproximate Modified Policy Iteration\n\n\u0010\n\n+ 12 Vmax + k\u03b1\u2217 k2 sup k\u03c6(x)k2\nx\n\n\u0011r 2\nn\n\nlog\n\n3\n,\n\u03b40\n\n(44)\n\nwith probability at least 1 \u2212 \u03b4 0 , where the second inequality is the application of a variation of Theorem 11.2 in\nthe book by Gy\u00f6rfi et al., (2002) with kf\u03b1\u2217 \u2212 (T\u03c0k )m vk\u22121 k\u221e \u2264 Vmax + k\u03b1\u2217 k2 supx k\u03c6(x)k2 . Similarly, we can\nwrite the left-hand-side of Equation 43 as\nr\n2\nm\nm\nm\n2k(T\u03c0k ) vk\u22121 \u2212 vek k\u03bcb \u2265 2k(T\u03c0k ) vk\u22121 \u2212 T(e\nvk )k\u03bcb \u2265 k(T\u03c0k ) vk\u22121 \u2212 T(e\nvk )k\u03bc \u2212 24Vmax\n\u039b(n, d, \u03b4 0 ),\n(45)\nn\n\u0001\nwith probability at least 1 \u2212 \u03b4 0 , where \u039b(n, d, \u03b4 0 ) = 2(d + 1) log n + log \u03b4e0 + log 9(12e)2(d+1) . Putting together\nEquations 43, 44, and 45 and using the fact that T(e\nvk ) = vk , we obtain\nm\n\n\u0010\n\nm\n\nk\u03b7k k2,\u03bc = k(T\u03c0k ) vk\u22121 \u2212 vk k\u03bc \u2264 2 2k(T\u03c0k ) vk\u22121 \u2212 f\u03b1\u2217 k\u03bc + 12 Vmax + k\u03b1\u2217 k2 sup k\u03c6(x)k2\nx\n\ns\n+ 4Vmax\n\n2\nlog\nn\n\n\u0012\n\nr\n\u0013!\n2\n3(9e2 n)d+1\n+ 24Vmax\n\u039b(n, d, \u03b4 0 ).\n0\n\u03b4\nn\n\nThe result follows by setting \u03b4 = 3\u03b4 0 and some simplification.\n\n\u0011r 2\nn\n\nlog\n\n3\n\u03b40\n\n\fApproximate Modified Policy Iteration\nValues of m in CBMPI\n\n350\n\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\n4\n10\n\n20\n\n300\n\n1\n2\n\n250\n200\n\n250\n200\n150\n\nDPI\n\n150\n\nAveraged steps to the goal\n\n20\n\nDPI\n\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\nLSPI\n\n100\n\n4\n10\n\n300\n\n1\n2\n\n100\n\nAveraged steps to the goal\n\n350\n\nValues of m in CBMPI\n\n50\n\n50\n\nLSPI\n0.0\n\n0.2\n\n0.4\n\n0.6\n\nCritic ratio (p)\n\n0.8\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nCritic ratio (p)\n\nFigure 3. Performance of the learned policies in mountain car with two different 2 \u00d7 2 RBF grids, the one with good\napproximation of the value function is on the left and the one with poor performance in approximating the value function\nis on the right. The total budget B is set to 200. The objective is to minimize the number of steps to the goal.\n\nG. Experimental Results\nIn this section, we report the empirical evaluation of CBMPI and compare it to DPI and LSPI. In the experiments,\nwe show that CBMPI, by combining policy and value function approximation, can improve over DPI and LSPI.\nIn these experiments, we are using the same setting as in Gabillon et al. (2011) to facilitate the comparison.\nG.1. Setting\nWe consider the mountain car (MC) problem with its standard formulation in which the action noise is bounded\nin [\u22121, 1] and \u03b3 = 0.99. The value function is approximated using a linear space spanned by a set of radial basis\nfunctions (RBFs) evenly distributed over the state space.\nEach CBMPI-based algorithm is run with the same fixed budget B per iteration. CBMPI splits the budget into\na rollout budget BR = B(1 \u2212 p) used to build the training set of the greedy step and a critic budget BC = Bp\nused to build the training set of the evaluation step , where p \u2208 (0, 1) is the critic ratio. The rollout budget is\ndivided into M rollouts of length m for each action in A and each state in the rollout set D0 , i.e., BR = mM N |A|.\nThe critic budget is divided into one rollout of length m for each action in A and each state in the rollout set\nD, i.e., BC = mn|A|.\nIn Fig. 3, we report the performance of DPI, CBMPI, and LSPI. In MC, the performance is evaluated as\nthe number of steps-to-go with a maximum of 300. The results are averaged over 1000 runs. We report the\nperformance of DPI and LSPI at p = 0 and p = 1, respectively. DPI can be seen as a special case of CBMPI\nwhere p = 0. We tested the performance of DPI and CBMPI on a wide range of parameters (m, M, N, n) but\nwe only report their performance for the best choice of M (M = 1 was the best choice in all the experiments)\nand different values of m.\nG.2. Experiments\nAs discussed in Remark 5, the parameter m balances between the error in evaluating the value function and\nthe error in evaluating the policy. The value function approximation error tends to zero for large values of\nm. Although this would suggest to have large values for m, the size of the rollout sets would correspondingly\ndecrease as N = O(B/m) and n = O(B/m), thus decreasing the accuracy of both the regression and classification\nproblems. This leads to a trade-off between long rollouts and the number of states in the rollout sets. The solution\nto this trade-off strictly depends on the capacity of the value function space F. A rich value function space would\nlead to solve the trade-off for small values of m. On the other hand, when the value function space is poor, or\nas in the DPI case, m should be selected in a way to guarantee a sufficient number of informative rollouts, and\n\n\fApproximate Modified Policy Iteration\n\nat the same time, a large enough rollout sets.\nFigure 3 shows the learning results in MC with budget B = 200. On the left panel, the function space is rich\nenough to approximate v \u2217 . Therefore LSPI has almost optimal results (about 80 steps to reach the goal). On\nthe other hand, DPI achieves a poor performance of about 150 steps, which is obtained by setting m = 12 and\nN = 5. We also report the performance of CBMPI for different values of m and p. When p is large enough,\nthe value function approximation becomes accurate enough so that the best solution is to have m = 1. This\nboth corresponds to rollouts built almost entirely on the basis of the approximated value function and to a large\nnumber of states in the training set N . For m = 1 and p \u2248 0.8, CBMPI achieves a slightly better performance\nthan LSPI.\nIn the next experiment, we show that CBMPI is able to outperform both DPI and LSPI when F has a lower\naccuracy. The results are reported on the right panel of Figure 3. The performance of LSPI now worsens to\n190 steps. Simultaneously one can notice m = 1 is no longer the best choice for CBMPI. Indeed in the case\nwhere m = 1, CBMPI becomes an approximated version of the value iteration algorithm relying on a function\nspace not rich enough to approximate v\u2217. Notice that relying on this space is still better than setting the value\nfunction to zero which is the case in DPI. Therefore, we notice an improvement of CBMPI over DPI for m = 4\nwhich trade-off between the estimates of the value function and the rewards collected by the rollouts. Combining\nthose two, CBMPI also improves upon LSPI.\n\n\f"}