{"id": "http://arxiv.org/abs/1011.2538v1", "guidislink": true, "updated": "2010-11-11T00:28:34Z", "updated_parsed": [2010, 11, 11, 0, 28, 34, 3, 315, 0], "published": "2010-11-11T00:28:34Z", "published_parsed": [2010, 11, 11, 0, 28, 34, 3, 315, 0], "title": "mVideoCast: Mobile, real time ROI detection and streaming", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.3898%2C1011.5626%2C1011.4853%2C1011.4978%2C1011.4919%2C1011.4201%2C1011.6280%2C1011.5201%2C1011.0266%2C1011.3757%2C1011.6216%2C1011.4526%2C1011.4472%2C1011.4022%2C1011.5608%2C1011.2564%2C1011.2770%2C1011.1067%2C1011.2000%2C1011.1282%2C1011.5290%2C1011.1086%2C1011.1779%2C1011.4099%2C1011.2070%2C1011.4529%2C1011.0749%2C1011.2928%2C1011.2727%2C1011.4821%2C1011.4597%2C1011.0376%2C1011.1486%2C1011.4464%2C1011.6485%2C1011.0226%2C1011.5781%2C1011.4592%2C1011.3096%2C1011.2781%2C1011.5337%2C1011.3768%2C1011.0185%2C1011.2500%2C1011.1316%2C1011.4981%2C1011.1237%2C1011.4862%2C1011.0400%2C1011.1441%2C1011.6488%2C1011.3452%2C1011.6587%2C1011.4551%2C1011.1821%2C1011.4931%2C1011.2261%2C1011.6384%2C1011.0908%2C1011.0360%2C1011.3386%2C1011.1858%2C1011.5602%2C1011.0316%2C1011.2538%2C1011.1613%2C1011.2541%2C1011.3550%2C1011.0696%2C1011.5434%2C1011.4561%2C1011.3961%2C1011.5611%2C1011.2479%2C1011.5227%2C1011.5318%2C1011.0812%2C1011.4974%2C1011.4382%2C1011.2865%2C1011.0990%2C1011.1669%2C1011.4540%2C1011.0653%2C1011.3278%2C1011.4602%2C1011.4613%2C1011.4848%2C1011.1303%2C1011.0074%2C1011.1321%2C1011.2614%2C1011.6436%2C1011.1141%2C1011.0431%2C1011.1267%2C1011.0349%2C1011.2521%2C1011.5340%2C1011.5036%2C1011.2249&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "mVideoCast: Mobile, real time ROI detection and streaming"}, "summary": "A variety of applications are emerging to support streaming video from mobile\ndevices. However, many tasks can benefit from streaming specific content rather\nthan the full video feed which may include irrelevant, private, or distracting\ncontent. We describe a system that allows users to capture and stream targeted\nvideo content captured with a mobile device. The application incorporates a\nvariety of automatic and interactive techniques to identify and segment desired\ncontent in the camera view, allowing the user to publish a more focused video.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.3898%2C1011.5626%2C1011.4853%2C1011.4978%2C1011.4919%2C1011.4201%2C1011.6280%2C1011.5201%2C1011.0266%2C1011.3757%2C1011.6216%2C1011.4526%2C1011.4472%2C1011.4022%2C1011.5608%2C1011.2564%2C1011.2770%2C1011.1067%2C1011.2000%2C1011.1282%2C1011.5290%2C1011.1086%2C1011.1779%2C1011.4099%2C1011.2070%2C1011.4529%2C1011.0749%2C1011.2928%2C1011.2727%2C1011.4821%2C1011.4597%2C1011.0376%2C1011.1486%2C1011.4464%2C1011.6485%2C1011.0226%2C1011.5781%2C1011.4592%2C1011.3096%2C1011.2781%2C1011.5337%2C1011.3768%2C1011.0185%2C1011.2500%2C1011.1316%2C1011.4981%2C1011.1237%2C1011.4862%2C1011.0400%2C1011.1441%2C1011.6488%2C1011.3452%2C1011.6587%2C1011.4551%2C1011.1821%2C1011.4931%2C1011.2261%2C1011.6384%2C1011.0908%2C1011.0360%2C1011.3386%2C1011.1858%2C1011.5602%2C1011.0316%2C1011.2538%2C1011.1613%2C1011.2541%2C1011.3550%2C1011.0696%2C1011.5434%2C1011.4561%2C1011.3961%2C1011.5611%2C1011.2479%2C1011.5227%2C1011.5318%2C1011.0812%2C1011.4974%2C1011.4382%2C1011.2865%2C1011.0990%2C1011.1669%2C1011.4540%2C1011.0653%2C1011.3278%2C1011.4602%2C1011.4613%2C1011.4848%2C1011.1303%2C1011.0074%2C1011.1321%2C1011.2614%2C1011.6436%2C1011.1141%2C1011.0431%2C1011.1267%2C1011.0349%2C1011.2521%2C1011.5340%2C1011.5036%2C1011.2249&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A variety of applications are emerging to support streaming video from mobile\ndevices. However, many tasks can benefit from streaming specific content rather\nthan the full video feed which may include irrelevant, private, or distracting\ncontent. We describe a system that allows users to capture and stream targeted\nvideo content captured with a mobile device. The application incorporates a\nvariety of automatic and interactive techniques to identify and segment desired\ncontent in the camera view, allowing the user to publish a more focused video."}, "authors": ["Scott Carter", "Laurent Denoue", "John Adcock"], "author_detail": {"name": "John Adcock"}, "author": "John Adcock", "links": [{"href": "http://arxiv.org/abs/1011.2538v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.2538v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "H.5.2", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.2538v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.2538v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "mVideoCast: Mobile, real time ROI detection and\nstreaming\n\narXiv:1011.2538v1 [cs.HC] 11 Nov 2010\n\nScott Carter, Laurent Denoue, John Adcock\nFX Palo Alto Laboratory\n3400 Hillview Ave.\nPalo Alto, CA 94304 USA\n(carter,denoue,adcock)@fxpal.com\nABSTRACT\n\nA variety of applications are emerging to support streaming\nvideo from mobile devices. However, many tasks can benefit from streaming specific content rather than the full video\nfeed which may include irrelevant, private, or distracting\ncontent. We describe a system that allows users to capture\nand stream targeted video content captured with a mobile\ndevice. The application incorporates a variety of automatic\nand interactive techniques to identify and segment desired\ncontent in the camera view, allowing the user to publish a\nmore focused video.\nAuthor Keywords\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nMobile, multimedia, capture\nACM Classification Keywords\n\nH.5.2 Information Interfaces and Presentation: User Interfaces\nGeneral Terms\n\nDesign, Human Factors\n\n(e)\nFigure 1. Screen detection. The application's first estimate shown\nin yellow (a) is inaccurate, but the second (b) correctly identifies the\nscreen. When the user locks the ROI it changes color to red and the application displays a transparent image of the warped and cropped ROI\nin the lower-right, (c). As the user repositions the device, the application detects the motion and begins finding a new ROI, (d). The user can\npress a button to lock the ROI to the new guess, (e).\n\nINTRODUCTION\n\nAs the processing power of mobile devices improves, they\nare being used for more computationally intensive tasks. Services have begun to offer live video streaming from mobile devices (e.g., Qik [10]), potentially allowing anyone to\nstream any event anywhere at anytime. However, as we have\nseen in the desktop world, unfiltered streaming, while useful,\nis not appropriate for every task. By filtering streamed content, presenters can better focus the audience's attention, improve bandwidth efficiency, and mitigate privacy concerns.\nFurthermore, mobile content capture faces challenges not\npresent on desktops \u2013 for example the recording device may\nbe off-axis from the desired content and may be hand-held\nand therefore unstable.\n\nThe system we present, mVideoCast, helps filter and correct\nvideo captured and streamed from a mobile device (see Figure 1). The application can detect, segment, and stream content shown on screens or boards, faces, or arbitrary regions.\nThis can allow anyone to stream task-specific content without needing to develop hooks into external software (e.g.,\nscreen recorder software). While past work has supported\nstreaming ROIs from video, mVideoCast is the first tool to\ndo so live from a mobile device. In this paper we describe\nthe system, algorithms we use to detect regions-of-interest\n(ROIs) in video frames, and our experiences with early prototypes of the application.\nMVIDEOCAST\n\nmVideoCast is architected as a client-server system in which\nmobile clients publish content to a remote machine. The\nmobile application, implemented on the Android platform,\ncan both record captured media on the device locally as well\nas stream content to a remote server in real time, allowing\nremote users to view live content. Users can control when\nFXPAL-TR-10-003. Copyright (2010) held by the authors.\n\n\f(a)\n\n(b)\n\nFigure 3. Viewing a stream. The server publishes a web page for each\nclient that shows the cropped and warped ROI from the latest frame as\nwell as the client's audio stream (disabled here).\n\nFigure 2. Data flow. Audio (a) can be saved locally and streamed to a\nremote server. Video frames (b) can be saved, streamed, and used to\ndetect ROIs.\n\nSELECTING A REGION\n\nthe application is recording content locally and when content\nis streaming live to a remote server.\n\nManual selection\n\nCapturing and correcting the image frame\n\nThe mobile application extracts a quadrilateral ROI from\neach video frame, warping and cropping the frame to match\na pre-defined output resolution and aspect ratio (see Figure\n2). The application runs three separate threads in order to\nsave content locally, stream content to a remote server, and\ndetect ROIs. The application first opens the camera and requests to receive preview frames. It stores received frames\nin a queue, and if it is in record mode it saves queued images\nlocally. It then pushes frame events to the other two threads.\nThe stream thread checks to see that it is in stream mode\nand that it has processed the last event. It then generates a\ncompressed color image of the frame and posts it to a remote\nserver (full images are sent, rather than only the ROI, to support potential post hoc edits to the stream). The detection\nthread similarly checks that detection is enabled and that it\nhas processed the last frame. It then generates a grayscale\nimage and runs the screen detection algorithm (methods for\ndefining the ROI are described in the next section). Once a\ncandidate quadrilateral is found, it is set in the main thread\nand shown on the display as an overlay drawn over the video\npreview (see Figure 1).\nAudio\n\nBecause video frames undergo several processing steps and\nmay not maintain a constant frame rate, audio is captured,\nsaved, and streamed separately. While recording, captured\naudio is saved to a local file. While streaming, audio is\nSpeex [12] encoded and forwarded to an Icecast [4] server\nrunning remotely. A web page associated with each user\nmerges the audio with processed frames (see Figure 3).\n\nUsers can define a ROI using a variety of methods ranging\nfrom fully manual to fully automatic.\nUsers can set the ROI by tapping on the screen to set the\nfour corners of the quadrilateral. The corner nearest the\ntapped point is set to the tapped location. There are also\nshortcuts to set certain standard sizes more quickly; clicking\non the upper-left and lower-right corners of the quadrilateral\nin rapid succession define a rectangle, and double-tapping\nanywhere on the screen sets the entire preview frame as the\ncaptured region.\nLight tags\n\nIf the user has control of the display he wants to capture, he\ncan switch the mobile application to a mode that detects light\ntags (e.g., LEDs) attached to the display. In this mode the\nmobile application detects the bright points corresponding\nto the light tags to determine the corners of the ROI (see\nFigure 4).\nScreen detection\n\nUsers can enable a screen detection mode that will automatically attempt to determine screen regions within frames. We\nmake use of the JJIL toolkit [5] in order to detect screens\nin a frame as follows: 1) Run a Canny edge detector over\na grayscale version of the frame; 2) Remove all but the top\n5% most significant edges from the result; 3) Divide the remaining points into four regions representing the top-half,\nbottom-half, left-half, and right-half of the image; 4) Send\nthe subregions to a Hough line fit algorithm to find the dominant line in each subregion; 5) Construct a quadrilateral from\nthe resulting lines.\nThis approach is typically not immediately accurate, so while\nin screen detection mode the application presents the current\nROI estimate to the user every few seconds (see Figure 1).\nWhen the user is satisfied with a result, he can use a hard-\n\n\fFigure 4. Light tag detection. The mobile application can search for\nlight tags (e.g., LEDs) attached to displays.\n\nware button to \"lock\" the current ROI coordinates in place.\nWhen a user locks an ROI, the application displays a thumbnailed view of the warped and cropped region in the lowerright of the screen.\nWe anticipate that the user will move around while recording, and after moving the ROI will no longer be accurate. To\naddress this issue, the mobile application continuously monitors the mobile device's onboard accelerometer and compass to detect a change in position. When it determines\nthat the device has moved, it begins generating new potential ROIs. While it generates guesses it also maintains the\npast ROI. When the user decides that a new estimate is a\nbetter match, he can click the hardware button to set it as the\ncurrent ROI.\nThe user can also adjust the corners of the quadrilateral at\nany time using the manual controls described above.\nFace detection\n\nIn another mode the application uses Android's face detector libraries to set the ROI. In this mode, the application automatically updates the ROI with the location of the most\nsalient face detected in each frame (see Figure 5). In this\nmode the application does not adjust the output aspect ratio\nand only crops the frame.\n\nFigure 5. Face tracking. The mobile application can set the ROI to be\nthe most salient face in a frame.\n\nSharing other mobile screens: A designer wants to document how a web page renders on his iPhone. He uses\na second mobile device with mVideoCast to record his\niPhone screen. The application picks up the iPhone screen\nboundaries in the video stream, unwarps it and streams it\nto remote viewers.\nTroubleshooting: Alice is trying to send a FAX using the\nher office's multifunction printer, but the machine stops\nunexpectedly while processing her paper. She decides to\ncall support and puts her phone on loud-speaker, launches\nmVideoCast, and points her device at the printer's LCD\nscreen. The software automatically detects the boundaries\nof the LCD screen, un-warps its image and sends it to a\nmember of the support staff who can more easily view it\nand guide her to a solution.\nEXPERIENCE\n\nWe asked four subjects to provide initial feedback on a prototype of the system running on an Android Nexus One device. We asked them to imagine a scenario in which they\nwere sending live video of their desktop monitor to a remote\nparty. We explained the goal of the system was to help them\nautomatically crop around the ROI, and that manual adjustments were possible by dragging the automatically selected\nedges to other locations or tapping the screen.\n\nSCENARIOS\n\nmVideoCast can be used for a variety of tasks.\nReporting: A technology reporter uses his mobile phone's\ncamera to record the screen of a demonstration of new\nsoftware at a local startup. mVideoCast lets him generate\na clean, bandwidth efficient upstream for his followers to\nsee live or asynchronously.\nA news reporter is interviewing a subject on the street. By\nusing the face detection mode, mVideoCast allows the reporter to easily stream a focused view of the interviewee,\nreducing distractors in the video and possibly preserving\nthe privacy of bystanders.\nRemote demonstrations: A business user wants to present\na demonstration of a software application while sitting at\na cafe. He starts mVideoCast on his phone and can stream\nonly his screen to the remote participants; the system will\ncrop regions out of the screen such as the cafe surroundings and his croissant.\n\nUser feedback provided several potential areas of improvement. Because many edges are present in the image of a\ntypical computer desktop, the subjects often had to manually readjust the area automatically selected by the system.\nThis proved problematic, as dragging or touching corners\nwith fingers would sometimes hide most of the screen and\nmade people move the device's screen, causing irregularities with the selected ROI. In general, holding the device\nsteady was a challenge, and user's hands would naturally\nmove when they had to look at the object they wanted to capture \u2013 looking at the device's display was not enough. One\nuser would always let the system guess the good tracking\nrectangles, never locking a guess by pressing the designated\nbutton. Another noticed that guesses where sometimes correct, but he did not have time to press the button before the\nnext guess happened. He suggested a \"back\" button to validate previous system guesses. Also, pressing the hardware\nbutton resulted in significant device motion, occasionally interfering with the ROI selection. A user suggested replacing\nthis action by a touch in the middle of the detected area.\n\n\fFigure 6. Using image stabilization to improve screen detection. We\nare experimenting with optical flow techniques that can make minor\nadjustments to the original frames (top) in order to keep the ROI registered without user intervention (bottom).\n\nUser feedback also revealed new use cases for the tool. Three\nsubjects had to step far back in order to capture their 30\"\nmonitor, which prompted them to try capturing individual\nwindows within their desktop area. They actually expressed\ninterest in this use case, capturing for example a login window or a web browser window. One user said he would actually like this system to capture an unwarped picture of a\nwindow, not necessarily a video recording. Another participant spontaneously and successfully used the application to\ncapture parts of his whiteboard.\nRELATED WORK\n\nWhile other research projects have explored video retargeting, or automatically selecting salient subregions of a video\nfor redisplay on smaller screens such as mobile devices [7],\nmVideoCast uniquely allows users to stream specific ROIs\nfrom a mobile device.\nThere have been a variety of applications that have used\nROIs in video in non-mobile contexts. Researchers have investigated user- and group-defined ROIs to control cameras\nfor remote collaboration tasks [8, 11]. Similarly, the Diver\nsystem allows users to create videos from cropped clips of a\nprerecorded, panoramic video [9]. Other tools have explored\nautomated solutions. El-Alfy et al. investigated automatically cropping surveillance videos to salient events [2]. Another focus of past work is the removal of individuals from\nvideo recordings or video conference streams (such as [1]).\nThere is existing work concerning the detection of documents in still images, including business card detection [3].\nIt is becoming more common for compact digital still cameras to include a document mode which will attempt to automatically identify the outline of a document and allow the\nuser to save a perspective-corrected version of the image.\nThe iPhone application, JotNot [6], provides a similar quadrilateral region crop-and-correct capability for still images, but\nrequires manual selection of the target corners.\nCONCLUSION AND FUTURE WORK\n\nIn the past remote communication suffered primarily from\na lack of bandwidth. Today networked, mobile multimedia\ndevices are ubiquitous, and the core challenge is not how to\ntransmit more information but rather how to communicate\nthe right information. mVideoCast is a small but important\nstep toward this goal.\nWe are working to improve mVideoCast in a number of ways.\nFirst, we are investigating new single-handed controls for\nsetting the ROI manually. Furthermore, we are exploring the\n\nuse of optical motion compensation to maintain the alignment of the ROI between explicit detections. The step of\nlocking coordinates provides an anchor to which subsequent\nframes can be registered. That is, if the ROI is locked at\nframe 0, subsequent frames 1 ... N can be aligned to this\nreference frame 0 with well known automatic image registration techniques. For instance, we can compute a transformation given a set of corresponding image coordinates, determined by matching image features (see Figure 6). In this\nway, the initial lock can be used without losing the position\ndue to camera motion.\nREFERENCES\n\n1. D. Chen, Y. Chang, R. Yan, and J. Yang. Tools for\nprotecting the privacy of specific individuals in video.\nEURASIP J. Appl. Signal Process., 2007(1):107\u2013107,\n2007.\n2. H. El-Alfy, D. Jacobs, and L. Davis. Multi-scale video\ncropping. In ACM MM '07, pages 97\u2013106, New York,\nNY, USA, 2007. ACM.\n3. G. Hua, Z. Liu, Z. Zhang, and Y. Wu. Automatic\nbusiness card scanning with a camera. In International\nConference on Image Processing (ICIP), Los Alamitos,\nCA, USA, 2006. IEEE.\n4. Icecast.org. http://www.icecast.org.\n5. Jon's Java Imaging Library, for mobile image\nprocessing.\nhttp://code.google.com/p/jjil/, 2010.\n6. JotNot. http://www.jotnot.com.\n7. F. Liu and M. Gleicher. Video retargeting: automating\npan and scan. In ACM MM '06, pages 241\u2013250, New\nYork, NY, USA, 2006. ACM.\n8. Q. Liu, D. Kimber, J. Foote, L. Wilcox, and\nJ. Boreczky. Flyspec: a multi-user video camera system\nwith hybrid human and automatic control. In ACM MM\n'02, pages 484\u2013492, New York, NY, USA, 2002. ACM.\n9. R. Pea, M. Mills, J. Rosen, K. Dauber, W. Effelsberg,\nand E. Hoffert. The diver project: Interactive digital\nvideo repurposing. IEEE MultiMedia, 11(1):54\u201361,\n2004.\n10. Qik. http://qik.com/, 2010.\n11. D. Song, A. F. van der Stappen, and K. Goldberg. Exact\nand distributed algorithms for collaborative camera\ncontrol. In In The Workshop on Algorithmic\nFoundations of Robotics, pages 167\u2013183, Berlin,\nGermany, 2002. Springer-Verlag.\n12. Speex: A free codec for free speech.\nhttp://www.speex.org.\n\n\f"}