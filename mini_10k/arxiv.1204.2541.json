{"id": "http://arxiv.org/abs/1204.2541v1", "guidislink": true, "updated": "2012-04-11T07:01:31Z", "updated_parsed": [2012, 4, 11, 7, 1, 31, 2, 102, 0], "published": "2012-04-11T07:01:31Z", "published_parsed": [2012, 4, 11, 7, 1, 31, 2, 102, 0], "title": "Employing Subsequence Matching in Audio Data Processing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.1179%2C1204.4210%2C1204.5180%2C1204.3249%2C1204.3007%2C1204.5081%2C1204.2657%2C1204.2004%2C1204.3194%2C1204.2539%2C1204.6301%2C1204.4007%2C1204.5933%2C1204.1341%2C1204.1473%2C1204.4212%2C1204.1910%2C1204.3876%2C1204.6430%2C1204.5140%2C1204.2620%2C1204.3589%2C1204.3486%2C1204.5041%2C1204.6184%2C1204.1333%2C1204.3807%2C1204.1667%2C1204.5591%2C1204.2660%2C1204.4138%2C1204.2546%2C1204.0485%2C1204.4159%2C1204.5803%2C1204.0160%2C1204.0568%2C1204.3552%2C1204.3989%2C1204.0477%2C1204.0950%2C1204.0624%2C1204.3169%2C1204.0322%2C1204.4015%2C1204.2339%2C1204.0852%2C1204.5989%2C1204.2915%2C1204.5604%2C1204.6086%2C1204.3596%2C1204.5259%2C1204.5723%2C1204.1689%2C1204.2870%2C1204.0701%2C1204.2390%2C1204.0778%2C1204.2425%2C1204.6413%2C1204.0373%2C1204.0881%2C1204.3657%2C1204.6384%2C1204.2518%2C1204.0508%2C1204.3734%2C1204.1413%2C1204.3904%2C1204.5991%2C1204.5787%2C1204.6289%2C1204.5025%2C1204.3572%2C1204.2886%2C1204.2477%2C1204.6570%2C1204.5083%2C1204.3554%2C1204.4263%2C1204.3010%2C1204.3826%2C1204.2070%2C1204.3316%2C1204.4488%2C1204.3386%2C1204.4063%2C1204.3200%2C1204.1857%2C1204.4103%2C1204.6406%2C1204.5271%2C1204.3852%2C1204.2541%2C1204.3355%2C1204.0552%2C1204.5725%2C1204.1059%2C1204.4723%2C1204.3415&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Employing Subsequence Matching in Audio Data Processing"}, "summary": "We overview current problems of audio retrieval and time-series subsequence\nmatching. We discuss the usage of subsequence matching approaches in audio data\nprocessing, especially in automatic speech recognition (ASR) area and we aim at\nimproving performance of the retrieval process. To overcome the problems known\nfrom the time-series area like the occurrence of implementation bias and data\nbias we present a Subsequence Matching Framework as a tool for fast\nprototyping, building, and testing similarity search subsequence matching\napplications. The framework is build on top of MESSIF (Metric Similarity Search\nImplementation Framework) and thus the subsequence matching algorithms can\nexploit advanced similarity indexes in order to significantly increase their\nquery processing performance. To prove our concept we provide a design of\nquery-by-example spoken term detection type of application with the usage of\nphonetic posteriograms and subsequence matching approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.1179%2C1204.4210%2C1204.5180%2C1204.3249%2C1204.3007%2C1204.5081%2C1204.2657%2C1204.2004%2C1204.3194%2C1204.2539%2C1204.6301%2C1204.4007%2C1204.5933%2C1204.1341%2C1204.1473%2C1204.4212%2C1204.1910%2C1204.3876%2C1204.6430%2C1204.5140%2C1204.2620%2C1204.3589%2C1204.3486%2C1204.5041%2C1204.6184%2C1204.1333%2C1204.3807%2C1204.1667%2C1204.5591%2C1204.2660%2C1204.4138%2C1204.2546%2C1204.0485%2C1204.4159%2C1204.5803%2C1204.0160%2C1204.0568%2C1204.3552%2C1204.3989%2C1204.0477%2C1204.0950%2C1204.0624%2C1204.3169%2C1204.0322%2C1204.4015%2C1204.2339%2C1204.0852%2C1204.5989%2C1204.2915%2C1204.5604%2C1204.6086%2C1204.3596%2C1204.5259%2C1204.5723%2C1204.1689%2C1204.2870%2C1204.0701%2C1204.2390%2C1204.0778%2C1204.2425%2C1204.6413%2C1204.0373%2C1204.0881%2C1204.3657%2C1204.6384%2C1204.2518%2C1204.0508%2C1204.3734%2C1204.1413%2C1204.3904%2C1204.5991%2C1204.5787%2C1204.6289%2C1204.5025%2C1204.3572%2C1204.2886%2C1204.2477%2C1204.6570%2C1204.5083%2C1204.3554%2C1204.4263%2C1204.3010%2C1204.3826%2C1204.2070%2C1204.3316%2C1204.4488%2C1204.3386%2C1204.4063%2C1204.3200%2C1204.1857%2C1204.4103%2C1204.6406%2C1204.5271%2C1204.3852%2C1204.2541%2C1204.3355%2C1204.0552%2C1204.5725%2C1204.1059%2C1204.4723%2C1204.3415&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We overview current problems of audio retrieval and time-series subsequence\nmatching. We discuss the usage of subsequence matching approaches in audio data\nprocessing, especially in automatic speech recognition (ASR) area and we aim at\nimproving performance of the retrieval process. To overcome the problems known\nfrom the time-series area like the occurrence of implementation bias and data\nbias we present a Subsequence Matching Framework as a tool for fast\nprototyping, building, and testing similarity search subsequence matching\napplications. The framework is build on top of MESSIF (Metric Similarity Search\nImplementation Framework) and thus the subsequence matching algorithms can\nexploit advanced similarity indexes in order to significantly increase their\nquery processing performance. To prove our concept we provide a design of\nquery-by-example spoken term detection type of application with the usage of\nphonetic posteriograms and subsequence matching approach."}, "authors": ["Petr Volny", "David Novak", "Pavel Zezula"], "author_detail": {"name": "Pavel Zezula"}, "author": "Pavel Zezula", "links": [{"href": "http://arxiv.org/abs/1204.2541v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.2541v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.2541v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.2541v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "}w\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\u000e\u0010\u0011\u0012\u0013\u0014\u0015\u0016\u0017\u0018\u0019\u001a\u001f!\"#$%&'()+,-./012345<yA| FI MU\nFaculty of Informatics\n\narXiv:1204.2541v1 [cs.SD] 11 Apr 2012\n\nMasaryk University Brno\n\nEmploying Subsequence Matching\nin Audio Data Processing\nby\n\nPetr Voln\u00fd\nDavid Nov\u00e1k\nPavel Zezula\n\nFI MU Report Series\nCopyright c 2011, FI MU\n\nFIMU-RS-2011-04\nAugust 2011\n\n\fCopyright c 2011, Faculty of Informatics, Masaryk University.\nAll rights reserved.\nReproduction of all or part of this work\nis permitted for educational or research use\non condition that this copyright notice is\nincluded in any copy.\n\nPublications in the FI MU Report Series are in general accessible\nvia WWW:\nhttp://www.fi.muni.cz/reports/\n\nFurther information can be obtained by contacting:\nFaculty of Informatics\nMasaryk University\nBotanick 68a\n602 00 Brno\nCzech Republic\n\n\fEmploying Subsequence Matching\nin Audio Data Processing\nPetr Voln\u00fd\n\nDavid Nov\u00e1k\n\nxvolny1@fi.muni.cz\n\nxnovak8@fi.muni.cz\n\nPavel Zezula\nMasaryk University, Brno, Czech Republic\nzezula@fi.muni.cz\nNovember 9, 2018\n\n\fAbstract\nWe overview current problems of audio retrieval and time-series subsequence matching. We\ndiscuss the usage of subsequence matching approaches in audio data processing, especially in\nautomatic speech recognition (ASR) area and we aim at improving performance of the retrieval\nprocess. To overcome the problems known from the time-series area like the occurrence of implementation bias and data bias we present a Subsequence Matching Framework as a tool for\nfast prototyping, building, and testing similarity search subsequence matching applications. The\nframework is build on top of MESSIF (Metric Similarity Search Implementation Framework) and\nthus the subsequence matching algorithms can exploit advanced similarity indexes in order to\nsignificantly increase their query processing performance. To prove our concept we provide a design of query-by-example spoken term detection type of application with the usage of phonetic\nposteriograms and subsequence matching approach.\n\n\f1\n\nIntroduction\n\nIn a past couple of decades we have witnessed an enormous rise of the amount of digital data.\nFirst, it is caused by the digitization of the data from many branches of human's activity and the\nsharing of possibly any knowledge today is realized through a digital channels. Furthermore,\nand especially in recent years in the age of blogs, social networks and services like YouTube,\npeople produce vast amount of digital content. Machines themselves are big content creators\ntoo. Let's just mention modern CT scanners or mesh nets of small devices producing constant\nflow of data about a measured phenomenon.\nOne of the data domains that people have accepted to handle in the digital form is audio.\nEvery moment, we can track a vast amount of new audio data being created. Radio stations,\ntelevision broadcasting, podcasts, lectures, voice chats \u2013 these all are the instruments of the creation of digitalized audio data and, more specifically, digitalized spoken utterances. The problem\nof an Automatic Speech Recognition (ASR) and analysis is more than three decades old. Many\napproaches were developed to cope with the speech analysis sub-problems like speaker verification, speech transcription, spoken term detection and many others. Solutions suitable for\nthese problems were often related to signal/time-series processing. Modern techniques, like the\nlarge vocabulary continuous speech recognition (LVCSR) use large orthographically transcribed\nspeech data to train their sophisticated acoustic and language statistical models and recognizers\nto achieve good results in automatic speech recognition. The problem can be seen as a classification of a speech where parts of the speech are segmented and classified into known classes, i.e.\nwords from vocabulary. It is expected that subsequent data-mining tasks will be performed on\nthe result of LVCSR.\nIt is not always feasible to construct such data sets due to the time and expense associated with\nthe annotation of large quantities of audio. This is typically the case for low resource languages\nfor which performing LVCSR is economically less profitable. For example, due to the lack of automatic transcription tools, only a few percent of the recordings of 4,000+ endangered languages,\ncurrently being made by linguists, can be analyzed [5]. It also fails when some out-of-vocabulary\nwords are present in an utterance.\nPossible drawbacks of LVCSR can be overcome by other approaches like unsupervised methods. Such approaches expects no (or only a minimum) knowledge about the examined utterance\nand its language specifics and they can be used for pattern discovery in speech, word discovery,\nor the whole key-phrase detection. It was argued in the literature [4, 35] that the unsupervised\nway of analyzing speech, compared with the LVCSR acoustic/language models approach, is\nmuch closer to the way of human speech perception and elaboration. The problem is that without a large preprocessing that is present in the LVCSR techniques, the unsupervised methods\nare very computational expensive because often the traditional similarity distance functions like\nDTW are extensively employed in the process. This is the problem that we would like to address in our work. We aim mainly on the performance improvements that could be achieved by\njoining our previous achievements in indexing, similarity searching and subsequence matching\nareas and one of the state of the art ASR methods. Especially when talking about unsupervised\nmethods, which are much more similar to the classical time series data-mining tasks, we can find\nmany places where our knowledge could enhance the performance of the possible applications.\nTo demonstrate this, we have decided to implement demo application that would employ our\n1\n\n\fgeneral Subsequence Matching Framework, which we are developing, together with one of the\nknown ASR methods and show that many sub-problems of a general ASR can be solved with the\nsubsequence matching approaches.\nWhile time series and subsequence matching are natural part in music or general audio retrieval process, we believe that we can employ our framework there also.\nThe rest of this report is organized as follows. In Section 2 we overview an audio retrieval\nproblems form the broader perspective, including music and general audio retrieval. The Section\n3 describes the state of the art time series and signal similarity search approaches. In Section 4\nwe discuss the subsequence matching problem and its applicability for enhancing time series\nretrieval. In Section 5, our Subsequence Matching Framework and its benefits is introduced and\nfinally the description of our demo query-by-example spoken term detection application based\non the framework is presented. The whole report is finished with conclusion and future work\ndirections in Section 6.\n\n2\n\n\f2\n\nAudio Retrieval Overview\n\nAudio retrieval is a very broad discipline ranging from general sound matching over music retrieval to sophisticated speech related methods like speaker identification and automatic speech\nrecognition. In this chapter we make a brief overview of the approaches and techniques for audio\nrelated problems that are recognized by the majority of the research community and where, as we\nbelieve, it could be helpful to employ advanced techniques for indexing, time series similarity,\nand subsequence matching.\n\n2.1\n\nGeneral Sound Similarity\n\nIn the case of general sound similarity, we have no prior knowledge about the sound like what is\nthe origin of the sound. It must be presumed that it could be emitted by anything from singing\nbirds or see waves to digital sound processor, so one cannot even tell whether the sound is of\nartificial or natural origin. Because of that, we can use only very low-level features for the sound\ndescription. MPEG-7 multimedia description framework [41] is one that is considered as a standard for describing multimedia content and it also includes the basic set of seventeen low level\naudio descriptors for audio features which can be divided into these six groups:\n\u2022 basic descriptors \u2013 instantaneous waveform and power values,\n\u2022 basic spectral descriptors \u2013 log-frequency power spectrum and spectral features (for e.g.\nspectral centroid, spectral spread, spectral flatness),\n\u2022 basic signal parameters \u2013 fundamental frequency and harmonicity of signals,\n\u2022 spectral timbral descriptors \u2013 log attack time and temporal centroid,\n\u2022 temporal timbral descriptors \u2013 log attack time and temporal centroid,\n\u2022 spectral basis representations \u2013 a number of features used in conjunction for sound recognition for projections into a low-dimensional space.\nDepending on specific application objectives, these descriptors can be combined in various ways.\n\n2.2\n\nMusic Information Retrieval\n\nMusic information retrieval, herein referred to as MIR, covers a broad range of topics. Although,\nthe idea of using computers for the purpose of MIR is about four decades old, the true explosion\nof interest in this topic took place in 1990's. One of the factors that contributed to it was an\nestablishment of MIDI format as basis for music information sharing. MIDI, as clearly-defined\nmathematical-based musical format, simplified music analysis and processing and brought new\npossibilities for music indexing. The next milestone was wide utilization of compression formats\nsuch as MP3 and Ogg Vorbis together with the increasing amount of freely available musical\ndatabases available for searching. Particular approaches for MIR are partly derived from the\nrepresentation of the music that one works with; therefore we make a short overview of these.\n\n3\n\n\fFigure 1: Hierarchy of MPEG-7 audio descrirptors. The structure depicts inner classes of an\nMPEG-7 framework.\n\n4\n\n\fSampled Audio\n\nVery accurate when it comes to interpreting the music but very expensive from\n\nthe MIR point of view. The reason for this is that it consists of thousands of samples of sound\ntaken every second resulting in potentially very large files. Furthermore, sampled audio representation is not robust against noise or small variations of music so it can eventually decrease the\nperformance of the query. Therefore, sampled audio representation is rarely used as an audio\ndescription format.\nMusical Instrument Digital Interface\n\nMIDI is a comprehensive type of a representation. It\n\nmeans that it is suitable for quite accurate recreating of an original music data. In contrast to\nsampled audio, it is much less verbose format. Instead of producing thousands samples per second depicting frequency and amplitude of the sound, it works with discrete alphabet of tones\nand instruments and it tries to capture when the particular tone starts and where it ends. Although, the re-created sound cannot reach the quality of sampled audio, it is accurate enough for\nthe listener to recognize the original tune. Moreover it is much more robust against background\nnoise and it is much cheaper to process.\nReduced Representations\n\nThe goal of members of this class of representations is not to tran-\n\nscribe complete musical information but only some of its features. It is not usually possible\nto reconstruct the original music but it gives us good amount of information about features on\nwhich basis we can quite easily compare two pieces of an audio data. Here, again, we would\nlike to mention MPEG-7 standards and its audio descriptors. These can be expressible by sole\nnumber - scalar descriptors, or holding more complex information in a form of multidimensional\nvectors. MPEG-7 descriptors and theirs membership to one of the mentioned groups can be seen\non Fig. 1.\nMIR Applications\nThere are several types of applications that employ MIR and they differ in a purpose of usage.\nOne big class is a query by humming applications where an application expects human voice\nas an input which can be either a bunch of tones that should form a melody or even a part of\nthe song hummed with lyrics. Other type of such an application could be genre detection or\ninstrument detection; there are also applications that try to guess the mood of the tune and so on.\nEach needs a bit different set of features to be extracted or comparing them in a bit different way.\nQuery by Humming\n\nQuery by humming usually referrers to a collection of input methods\n\nwhere the human voice is required to produce the query. The method was popularized by Ghias\net. al in 1995 [17] and many works tried to enhance the query by humming approach [48] and\nsometimes it was also with the usage of MPEG-7 descriptors [1]. Nowadays, the technology\nin indexing and querying vast musical databases allows commercial projects to offer query by\nhumming type of application to the widest audience. As an example lets mention SoundHound;\nit offers mobile application where user hums a small part of tune (maybe he wants to know the\nname of the song or want to buy it) and gets full name of the song and other metadata with the\nimmediate possibility to buy it.\n\n5\n\n\fFigure 2: Overview of the speech processing areas.\n\n2.3\n\nSpeech Related Areas\n\nFor humans, speech is the most natural form of communication, which leads to the fact that\nspeech processing is one of the most exciting areas of the signal processing. In spite of the great\neffort of the research community, there is still significant communication gap between humans\nand machines. The spoken word would be the perfect interface for many real-life applications.\nAlthough we can observe rising number of applications that uses human voice as an input, solving speech processing problems is still very challenging.\nIn this section we make a brief overview of the most significant areas related to speech, which\nare depicted on Fig. 2. We name three basic application classes: speaker recognition, speech\nrecognition, language identification. Speaker recognition can be further divided into speaker\nverification, identification and diarisation. Speaker verification, text dependent or text independent, is the case where we want to verify the identity of the person through specifics of his voice.\nThat means that the system knows which person should be verified and the output is either yes\nor no or maybe some probability.\nOn the other hand, in the case of speaker identification, the system tries to guess who is\nspeaking. In other words, there is a database of speakers and an application matches an input\nvoice with indexed entries. Speaker diarisation is a method for distinguishing between speakers\nin the multi-person conversation and it tries to divide the conversation into utterances that belong\nto particular speaker.\nThere are also very specific areas like gender recognition for recognizing a gender of a speaker\nor language identification. The recognition method is usually based on statistical models of\nmale/female speakers and models for specific languages.\nFinally, there is a wide area of speech recognition applications, where one can find many\nvariables for the particular application like whether it is speaker dependent or speaker independent, whether it needs to perform the recognition for continuous speech or preprocessed short\nutterances, or whether the speaker is aware that his words are to be recognized and she sort of\ndictates the text or if the system should recognize any spontaneous speaking style. All these\nvariables influence the choice of the proper ASR approach for the specific application.\n\n6\n\n\fFigure 3: Taxonomy of techniques used in ASR.\nAutomatic Speech Recognition\nOne of the most popular approaches for the ASR is the large vocabulary continuous speech recognition (LVSCR). This technique usually uses specific models for each language and a decoder that\nextracts posterior probabilities of sub-word phonetic units. With the aid of the trained models\n(usually Hidden Markov Models) the input in the form of sub-word phonemes is classified as\none of the words in vocabulary. These sub-words phonetic units are recognized by so-called phonetic recognizers. In this phase the acoustic model of the language is used to produce posterior\nprobabilities of the sub-word units for the raw audio input. The output can be represented as a\nconfusion network. In the case that we also have the knowledge about the speaker of the analyzed utterance, we can also employ a speaker specific acoustic model that was trained on the\nsample data produced by that speaker.\nBasic components of a speech recognition process are front end and decoder. Decoder usually\ncan not work with raw audio data. It works only with features extracted by front end. A good\nexample of a wide spread representation is Mel-Frequncey Cepstral Coefficients (MFCC) [44].\nMFCC is the most widely used among acoustic signal representations [34, 36] and not only for\nthe sake of speech recognition (it is widely used for a speaker recognition tasks too).\nThe taxonomy of speech recognition related techniques can be seen on Fig. 3.\nUnsupervised Methods\nUnlike the model based approaches, the pattern recognition based on unsupervised methods\nhave no, or only small, prior knowledge about the language being used or the speaker. In other\nwords, it is possible to use this approach even for languages or dialects where no linguistic corpora are available. The unsupervised methods [21, 4, 35, 43] represent quite recent stream of\nresearch and it can be seen as a 'back to the roots' approach because it has much more common\nwith the early ASR methods than with the current state of the art model based ways of ASR.\nUnsupervised methods represent departure from traditional models of speech recognition,\nwhere the goal is to classify speech into categories defined by a given inventory of lexical units \u2013\nphonemes or words. Instead, such an inventory is discovered in an unsupervised manner, where\nwords, sub-words, word-like units or generally patterns in speech are extracted by exploiting the\nstructure of repeating patterns within untranscribed audio stream.\n\n7\n\n\fAs we observed, methods used for unsupervised extraction of such a patterns are often modifications of linear programming algorithms based on DTW approach. This might be one of the\ncauses for performance issues of the unsupervised approaches and the reason for the fact that\ncurrent state of unsupervised techniques do not allow to achieve the same performance as a\nmodel based approaches.\n\n8\n\n\f3\n\nTime Series Processing: State of the Art\n\nTime series data mining tasks got a lot of attention in last decade. It was caused by the data explosion in many branches and the need to handle vast amounts of data effectively and efficiently.\nIn this section, we provide the reader with an introduction to the mentioned topics. We overview\nthe most important similarity functions and data representations that are meaningful from our\npoint of view. Furthermore, we discuss recent problems related to the outlined areas. Generally,\nthere are four different tasks in time series data mining research [24]:\n\u2022 Indexing (Query by content): Given a query Q, similarity function D(Q, S) and indexed\ndata in DB, find the nearest match in DB.\n\u2022 Clustering: Find natural groupings of the time series in database DB under some similarity/dissimilarity measure D(Q, S).\n\u2022 Classification: Given an unlabeled time series Q, assign it to one of two or more predefined\nclasses.\n\u2022 Segmentation: Given a time series Q containing n data points, construct a model Q , from\nK piecewise segments (K << n) such that Q closely approximates Q.\nWe aim mostly at the first case, but you will see that some approaches can be used to solve more\nthan one of the mentioned tasks. First, we explain the lower bounding lemma and why it is so\nimportant.\n\n3.1\n\nLower Bounding\n\nThe lower bounding lemma was introduced in [15] and it is very desirable property of representations and similarity models. It enables to prune big parts of the space being searched in a\ncomputationally cheap way and it guarantees no false dismissals in an answer (but it can contain\nfalse alarms). The key advantage of this approach is that we use the expensive true distance function only on the small subset of the indexed data which were selected as the answer candidates\nby the cheaper lower bounding mechanism.\nLemma 1. To guarantee no false dismissals for range queries, the extraction function F and distance\nfunctions Dist and LB_Dist should satisfy the following formula:\n\nLB_Dist(F(S), F(Q)) \u2264 Dist(S, Q)\nFor instance, it was proven [15] that DFT is lower bounding transformation for Euclidena\nDistance.\nTo compare representations according to their lower bounding properties the tightness of\nlowerbound measure [14, 23] is used. Its vale is between 0 and 1 inclusive and is computed as:\nTLB = LowerBoundDist(S, Q)/TrueEuclideanDist(S, Q)\nThe higher the TLB value, the better the lower bound. TLB seems to be very meaningful measure\nand it seems to be the current consensus in the literature [6, 9, 10, 22]. The advantage of the\nmeasure is that it is implementation-free and independent on external variables, e.g. software or\nhardware testing infrastructure. With this measure one can easily predict indexing performance.\n9\n\n\fFigure 4: Basic hierarchy of representations. Those with asterisk after the name allows lower\nbounding.\n\n3.2\n\nRepresentations\n\nDimensionality reduction is one of the most important tasks in time series processing. The desirable features of the transformed representation are a good ratio between the level of reduction\nand the accuracy of an approximation of the original time series data and the ability to provide\nlower bounding options for a distance function. In this section we provide a brief overview of\nmeaningful representations that were widely accepted by the research community (See Fig. 4).\nGenerally, we can look at representations from several points of view:\n\u2022 data adaptive/non-data adaptive \u2013 the difference is whether the method reflects the features of the time series being transformed,\n\u2022 continues/discrete domain representations \u2013 coefficients are from some continuous domain, e.g. real numbers, or from finite set of values (string representations) symbols; The\nlatter is often called string representation,\n\u2022 allows/disallows lower bounding \u2013 is it proven, that we can lower bound true distance on\nthe original time series with the reduced representation?\nThe basic division of representations that we are going to overview can be seen in Fig. 4.\nNon-data Adaptive Representations\nDiscrete Fourier Transform\n\nDiscrete Fourier Transform was the first proposed method for time-\n\nseries dimensionality reduction [15]. It transforms a time-series from the time/space domain\ninto the frequency domain. A fast algorithm exists for such a transformation in \u0398(n log n) time\n[12]. The DFT represents the time-series data by using Fourier coefficients (a combination of sine\nand/or cosine waves, each represented as a complex number). Only the first few coefficients\nare required to approximate the original time-series since the slow changing/low frequency part\nof the sequence is the underlying shape of the sequence. The coefficients beyond the first few\nhave such low amplitude that they only provide a very small gain in representative accuracy\nbut produce a greater decrease in the compression. An advantage of DFT is that it maintains\n\n10\n\n\fthe property of allowing Euclidean distance calculations. Due to the exclusion of some coefficients, and therefore some positive values, the Euclidean distance calculation of the two DFT's\nguarantees the lower bound of the Euclidean Distance [15].\nDiscrete Cosine Transform\n\nA discrete cosine transform (DCT) [28] expresses a sequence of\n\nfinitely many data points in terms of a sum of cosine functions oscillating at different frequencies. In particular, a DCT is similar to the discrete Fourier transform (DFT), but using only real\nnumbers. DCT is operating on real data with even symmetry (since the Fourier transform of a\nreal and even function is real and even), where in some variants the input and/or output data\nare shifted by half a sample.\nDCT also takes place in a process of extracting more sophisticated audio descriptors like Mel\nFrequency Cepstrum Coefficients (MFCC).\nDiscrete Wavelet Transform Discrete Wavelet Transform (DWT) [8] is similar to DFT in many\naspects, except that rather than using sine and cosine functions, it uses wavelets as base function.\nWavelets are basis functions used in representing data or other functions. Wavelet algorithms\nprocess data at different scales or resolutions in contrast with DFT where only frequency components are considered.\nAnother difference is the result of the transform.\n\nWhile the DFT transform the time-\n\nseries into the frequency domain, DWT transforms the time-series into the time/frequency or\nspace/frequency domain. Haar wavelets [8] are often used as s good example of DWT approach\nbecause of its good pruning power (i.e it is a good lower bound of Euclidean distance).\nPiecewise Aggregate Approximation According to Piecewise Aggregate Approximation\n(PAA), to reduce the data from n dimensions to N dimensions, the data is divided into N equisized \"frames\". The mean value of the data falling within a frame is calculated and a vector of\nthese values becomes the data reduced representation [23].\nData Adaptive Representations\nSingle Value Decomposition Singular Value Decomposition (SVD) [28, 39] is an optimal dimensionality reduction technique on matrices under the Euclidean metric. The SVD reduction is\nsuitable for use on any matrix, including applications to indexing images and other multimedia\nobjects. Keogh et al. [23] first used the SVD on time-series data, noting its advantages and disadvantages. The difference with the other proposed techniques as opposed to this one is that the\nothers are local transformations, but SVD is a global transformation.\nAdaptive Piecewise Aggregate Approximation APCA is a modification of PAA that allows arbitrary length frames in contrast with PAA that divides a time-series into N frame of equal length.\nThis allows indexing using standard multi-dimensional index structures by mapping each mean\nvalue to each dimension. APCA differs from PAA in that by allowing arbitrary length frames,\nan extra value must be stored to identify the length of each window. This led researchers to believe APCA did not allow indexing [45]. Chakrabarti et al. [7] showed that APCA could indeed\nbe indexed using multi-dimensional indexing structures. An APCA representation requires two\nnumbers per window.\n11\n\n\f3.3\n\nDistance Measures\n\nFigure 5: Basic hierarchy of distance functions.\nMeasuring the distance between objects and thus the similarity is essential for any data mining task. In a perfect case, the distance measure would be cheap to compute, invariant against all\npossible transformations and with the possibility to parametrize it to give results that are close to\nhuman perception of similarity on the given objects domain. Futhermore, it is desireable for the\nfunction to obey axioms of a metric space, because when those axioms are satisfied, we can use\nadvanced indexing techniques and perform queries on the indexed data very effectively [47].\nMetric space M = (D, d) is defined as a set of objects D and the distances between them. The\ndistance is computed by the function d : D \u00d7 D \u2192 R that must satisfy the following porperties.\n\u2200x, y, z \u2208 D hold:\n\u2022 d(x, y) = 0 \u21d4 x = y (identity)\n\u2022 d(x, y) = d(y, x) (symetry)\n\u2022 d(x, z) \u2264 d(x, y) + d(y, z) (triangle inequality)\nThe key axiom is triangle inequality which can be exploited to prune big parts of the indexed\nspace and return the best matching objects very quickly. Common types of queries is a range\nquery and kNN query. First one returns a set of objects that are within the given diameter r from\nthe query object q. The latter one returns k nearest objects to the query object q.\nLock-step Measures\nThis class of distance functions is suitable only for time-series with the same length. It measures\nthe distance between every particular part of the sequence and expects that these parts were\ntaken in the same time interval.\nLp Norms Lp norms is a set of functions that assign strictly positive value to a vector in a Lp\nspace. We will consider only two special cases of Lp norms. The ubiquitous Euclidean norm and\nManhattan norm.\nAlthough very simple and straightforward, Euclidean distance is one of the most used similarity measures in time series data mining. It is a classical lock-step similarity method so it can\nbe used only for comparing segments of the same length. It is not robust against time shifting,\n\n12\n\n\fscaling and to any other transformations. As we said the Euclidean norm is a special case of Lp\nnorm where p = 2, so the length of vector x = (x1 , ..., xn ) is defined as:\nv\nu n\nuX 2\n||x|| = t\n|xi |\ni=1\n\nIt has been shown that even with the outlined disadvantages of Euclidean distance function it is\nvery reasonable to it use in many fields of time series data mining. Good feature of this measure\nis that it satisfies properties of a metric function.\nThe second Lp norm we are about to mention is a Taxicab norm or Manhattan norm. In this\ncase the p = 1.\n\nFigure 6: Euclidean distance vs. DTW.\n\nElastic Measures\nThese measures allows to compare sequence even if they have different lengths. It is allowed by\nthe ability of time warping (See Fig. 6).\nDynamic Time Warping Dynamic Time Warping (DTW) ads sort of elasticity to the process\nof the time series comparison. It allows warping of sequences in a time to eliminate scaling or\ngaps on the time axis. Semantically, it means that for example in an audio retrieval process, we\ncan identify a spoken word even if it was spoken in a different tempo than is the tempo of the\nindexed reference word. On Fig. 6 you can see the difference in a way of pairing values of the\nseries being compared by fixed step Euclidean Distance and elastic DTW. Generally, DTW tries\nto find an optimal match between two sequences. More formally, Dynamic Time Warping is a\nlinear programming method for finding a minimum cost path in an accumulation matrix.\nThe problem of DTW is a performance and sometimes inappropriate behavior, when it applies\na lot of warping. The performance problem of DTW is also given by the fact, that it is not a metric\nfunction and thus cannot be indexed as a metric space.\nThe problem of performance and misbehavior was addressed by the introduction of SakoeChiba band [40] and Itakura Parallelogram [34] which ideas were to constraint the search space\nonly to a part of the matrix along the diagonal path (See Fig. 7).\nTo enhance the performance of the data retrieval applications that were using the DTW\nmethod, the lower bounding functions for the DTW that would be indexable were proposed.\nThe lower bounding function introduced by Kim et al. [26] (known as LB_Kim) extracts 4 numbers as a feature vector for each sequence. The features are the first and last elements and the\nminimum and maximum values.\n\n13\n\n\fFigure 7: Global constraints limit the scope of the warping path, restricting them to the gray\nareas. The two most common constraints in the literature are the Sakoe-Chiba Band and the\nItakura Parallelogram [25].\nAnother lower bounding function LB_Yi was introduced by Yi et al. [46] and it exploits the\nfact that all points in one sequence that are larger (smaller) than the maximum (minimum) must\ncontribute at least squared difference of their value and the maximum (minimum) value of the\nother sequence in the final DTW distance.\nAlthough, they both added the desirable property of lower bounding, the lower bound was\nstill too loose. This issue was addressed by Keogh et al. in the work [25] introducing LB_Keogh,\nwhich idea is to construct envelopes (based on the idea of global constraint of Itakura parallelogram) around the original signal and use PAA representation for both Upper and Lower envelope borders. On these reduced envelope representations the special lower bounding function\nLB_PAA (computes the ED between the envelopes) is computed, and Minimum Bounding Rectangles (MBR) covering the original sequence are created. Then with the aid of MINDIST (Q, R)\nfunction, where Q is the query sequence and R is the MBR, the lower bounding distance is computed.\nTo our best knowledge, LB_Keogh is the best lower bounding technique for DTW approaches.\nEdit Distance Based Functions To overcome the fact that DTW, in its pure form, is not eligible\nfor exact indexing, i.e. indexing in metric spaces, the inspiration from edit distance based approaches gave a birth to the Edit Distance on Real Sequence (EDR) [11] and Edit Distance with\nReal Penalty (ERP) [9]. They both exploit the observation that the warping in one sequence can\nbe seen as a gap addition in the other sequence, which takes the problem of warping to the level\nof edit distance like problem. The main difference between the DTW approach and edit distance\nbased approaches is the penalization for warping which the DTW approach lacks.\n\n14\n\n\f4\n\nSubsequence Matching strategies\n\nSo far have we discussed representations and similarity models that are used for general time series data mining tasks but we have not mentioned anything about subsequence matching yet. In\ngeneral, subsequence matching is a specific problem in the area of sequence matching. Therefore,\nall the outlined methods are substantial to both sequence matching and subsequence matching.\nThe latter differs in indexing and retrieval strategies but somewhere in the process it goes down\nto one of the mentioned similarity measuring methods on the given representations.\nA specific application is heavily determined by its data meaning and interpretation. So prior\nto the inventing subsequence matching strategies one must ask few substantial questions. What\nis the meaning of the data? Do we seek for trends or patterns? Is it possible to compare data\nwith different lengths? Is time warping a desirable feature during search? What will be the size\nof the window? What will be the volume of the indexed data? etc. As you can see the process\nhas many variables that we must take into account before we start tailoring the subsequence\nmatching approach for the specific problem.\nThis section covers basic problems and techniques used in subsequence matching applications. We will discuss usage of sliding and disjoint windows during indexing and querying, the\neffect of the chosen windows size on the performance of the application, discussion on where to\nemploy dynamic time warping and where to use Euclidean distance, and finally specific problem\nof finding motifs in the time series.\nIn the following text we will use this notation:\n\n4.1\n\nSymbols\n\nDefinitions\n\nLen[S]\n\nThe length of sequence S\n\nS[k]\n\nThe k-th value of the sequence S\n\nS[i : j]\n\nSubsequence of S including values between S[i] to S[j] inclusive.\n\nD(Q, S)\n\nDistance between two sequences Q and S\n\nsi\n\nThe i-th disjoint window of sequence S\n\n\u03c9\n\nLength of the (sliding/disjoint) window\n\n\u000f\n\nUser defined tolerance\n\nSliding And Disjoint Windows\n\nFor the sake of subsequence matching, we need to extract subsequences both when indexing and\nquerying data. There are two related major techniques for creating subsequences:\n\u2022 disjoint window \u2013 it divides the given sequence into disjoint windows. where one starts\nwhere the other ends,\n\u2022 sliding window \u2013 it creates all possible windows of given length \u03c9 that can be extracted\nfrom the given series. In other words, if one starts at position i within the sequence, the\nother start at position i + 1.\nBoth, sliding and disjoint windows are used in subsequence matching techniques to achieve the\nstate, where each window created from the query can be compared to each window in an in15\n\n\fdex. First time, this concept was used in field opening paper [15] by Faloutsos et al. (FRM in\nshort). There, they use sliding windows for indexed data and disjoint windows for queries. To\nreduce the amount of subsequence to subsequence distance computations, not all subsequences\nare indexed in the database, but minimum bounding rectangles representing a bunch of windows\n(MBR) are used instead. So in the case the range query is initiated, query disjoint windows are\nfirst compared to the indexed MBRs and only those MBRs, where at least part of MBR is in the\nrange, are selected as a candidates for the answer. Then all disjoint windows from the candidate\nMBRs are taken and all false alarms (windows not in range) are found and dismissed from the\nprecise answer. Although, this lower bounding technique ensures no false dismissals, it is not\nvery efficient and produces a lot of false alarms.\nThe opposite way of using sliding and disjoint windows was introduced by Moon et al. [32].\nThey divide data sequences into disjoint windows and the query sequences into sliding windows.\nHence, this approached exploits duality in constructing windows, it was called DualMatch. It has\nbeen shown that the duality based approach is correct (i.e. it incurs no false dismissals) and that it\noutperforms FRM. DualMatch reduced drastically the number of points that need to be stored to\n1/\u03c9 of that of FRM. The disadvantage of DualMatch is the upper bound for window size which\nin return caused additional false alarms due to the window size effect (see section 5.2)\nAnother method, GeneralMatch, was brought by Moon et al. in 2002 [31]. It is based on a\ngeneralization of constructing windows. The authors introduced the concept of J-sliding and\nJ-disjoint windows with the specified J sliding factor and they defined them as follows:\nDefinition 1. A J-sliding window (1 < J < \u03c9) sJi of size \u03c9 of the sequence S is defined as the subsequence\nof length \u03c9 starting from S[(i \u2212 1) \u2217 J + 1](1 < i <\n\nLen(S)\u2212\u03c9\nj\n\n+ 1).\n\nIntuitively speaking, if we have \u03c9 = 16 and J = 4, we construct windows by shifting subsequence of length 16 and by 4 entries, and thus, the starting points of the 4-sliding windows are\nS[1], S[5], S[9], ..., respectively.\nDefinition 2. A J-disjoint window (1 < J < \u03c9) qJ(i,j) of size \u03c9 of the sequence Q is defined as the\nsubsequence of length \u03c9 starting from Q[i + (j \u2212 1) \u2217 \u03c9](1 \u2264 i \u2264 J, 1 \u2264 j \u2264\n\nLen(S)\u2212i+1\n))\n\u03c9\n\nin Q\n\nAgain, if we have an example where \u03c9 = 16 and J = 4, we construct windows Q[i : i +\n\u03c9 \u2212 1], Q[i + \u03c9 : i + 2\u03c9 \u2212 1],... by dividing Q[i:Len(Q)] into disjoint windows for every i where\n(1 < i < 4).\nI/O Issues Logically, when we divide the sequences into many subsequences, the amount of\ncomparisons during the query processing rises. This also implies that subsequence matching\nalgorithms do lots and lots of I/O operations needed for fetching particular subsequences one\nby one again and again. One of the advanced subsequence matching methods addressed this\nproblem by introducing deferred group subsequence retrieval [19]. This technique tries to cope with\nthe fact that, because of excessive disk I/O operations and bad buffer utilization, we might have\nto read same data pages repeatedly from the disk. The basic idea of the proposed solution is to\ndelay a fixed size set of subsequence retrieval and enable batch retrieval.\n\n16\n\n\f4.2\n\nWindow Size Effect\n\nThe effect of a window size on a performance was first discussed in previously mentioned DualMatch approach [32]. The windows size effect is caused by the application of two lemmas that\nwere introduced in FRM:\nLemma 2. When two sequences S and Q of the same length are divided into p windows si and qi (1 \u2264\n\u221a\ni \u2264 p) respectively, if S and Q are in \u000f-match, then at least one of the pairs (si , qi ) are in \u000f/ p-match.\nThat is, the following equation holds:\nD(S, Q) \u2264 \u000f \u21d2\n\np\n_\n\n\u221a\nD(si , qi ) \u2264 \u000f/ p\n\ni=1\n\nLemma 3. If two sequences S and Q of the same length are in \u000f-match, then any pair of subsequences\n(S[i : j], Q[i : j]) are also in \u000f-match. That is, the following equation holds:\nD(S, Q) \u2264 \u000f \u21d2 D(S[i : j], Q[i : j]) \u2264 \u000f\nApplication of Lemmas 2 and 3 for long query sequences causes false alarms. That is, when\ntwo sequences S and Q are divided into p windows si and qi (1 \u2264 i \u2264 p) respectively, although\n\u221a\na pair (si , qi ) is in \u000f/ p-match, the distance between S and Q may be greater than \u000f. To reduce\nthis kind of false alarms, it is reasonable to use as large windows as possible. For example, let the\nwindow size of the method A be twice as large as that of the method B. Then, by Lemma 2 or 3,\na candidate subsequence of the method A must also be a candidate of the method B. However,\nthe inverse does not hold. This effect was defined as the window size effect [32]. The size of the\nwindow, however, must be less than or equal to the length of the query sequence; thus, the maximum window size depends on the length of the query sequence. Typically, the algorithm have\nthe problem that the performance decreases as the difference between the query sequence length\nand the window size increases. This problem was addressed in [27, 29]. The proposed techniques\nemploy creating multiple indexes and their usage based on the query parameters. The dependency of the performance degradation on the number of queries and query vs. window size was\nrevealed and it was claimed that the need for multiple indexes tailored for the variable query\nlengths is crucial. Both mentioned articles introduced heuristic methods for determination of the\nwindow sizes for particular indexes based on the distribution of the query sequence lengths.\n\n4.3\n\nWhen We Need To Warp?\n\nEuclidean Distance and Dynamic Time Warping based techniques have both many advocates.\nEuclidean distance is mostly favored for its simplicity, lower bounding options and the fact that\nit could be easily computed. On the other side stands much more sophisticated linear programming approach of dynamic time warping. During the past few decades the latter pushed the\nformer out of many applications and it was claimed that the dominance of DTW over ED is\ninevitable. There are areas where this has become true like in speech recognition and query-byhumming where the warping ability is crucial to match the spoken terms with words from the\ndatabase or where one can expect that the humming will not have the same tempo as the indexed\ndata that we want to match with the query.\nDespite the fact that DTW outperforms ED in these areas, arguments in the favor of ED were\nraised in the literature [38, 37]. It was argued that the bigger the data collection, the lesser the\n17\n\n\fneed for DTW because it often degrades to the same result as the ED measure. This is caused by\nthe fact that the probability, that there are some data in the collection that are very similar to the\npossible query, grows with a size of the indexed data collection. So the usage of DTW and ED in\nthis case would bring very similar results but ED is cheaper to compute.\nOn the other hand, this reasoning cannot be applied generally. We believe that for some domains the warping feature of the DTW is crucial even if the collection is large enough. Again, on\nthe example of the spoken term detection, we do not want to find only the one best match, but to\nfind possibly all variations of the spoken term and this is the case where DTW, or other functions\nwith the warping ability, are irreplaceable. In the case that the warping would be needed only\nbecause the different offset of the data, than, when considering subsequence matching, it is better\nto use ED. We do not need DTW in this case because the offset difference of the sequence can be\novercome by the sliding/disjoint windows.\n\n4.4\n\nFinding Motifs In Time Series\n\nWe can say that finding motifs [30] is specific sub-problem of time series data mining. It aims on\nfinding regular patterns in the time series, usually omitting the exact values and taking only the\ndevelopment or shape of the time series signal. Sometimes this problem is also referred as a rule\ndiscovery.\nOne of the common approaches is to have a predefined set of primitive shapes and than\nto classify the windows of the original sequence into these shapes. The set of primitives can\nbe found by clustering of windows of predefined length. Each sequence is sliced into disjoint\nwindows and than all the windows are clustered into the set of shape primitives. Each primitive\nis represented by a symbol and than each sequence can be represented by the string of these\nsymbols [13].\n\n18\n\n\f5\n\nSubsequence Matching Framework\n\nAs we have outlined in the previous sections, the ubiquity of the time series and its usability\nin a wide range of applications led the research community to the invention of many various\napproaches. It was shown that the time series whole matching approach is not sufficient in many\nareas. Therefore the subsequence matching problem had risen and has been addressed by many\nresearches since the first paper on this topic was published [15]. Faloutsos et al. have introduced\nGEMINI framework, an application model for dealing with subsequence matching which in short\nwe can explain in four steps that the application has to perform:\n\u2022 slicing the time series sequences into smaller subsequences (originally using sliding window for the indexed data and disjoint window for the query)\n\u2022 maping each time series subsequence in a lower dimension. We can call this step segmentation (originally using Fast Fourier Transform)\n\u2022 indexing them in multi-dimensional indexing structure (originally R-tree)\n\u2022 performing search with a distance function that obeys the rule of lower bounding lemma\n(originally Euclidean Distance).\nDespite the fact that the application model is almost two decades old, it is still vital and suits\nmany cases where the subsequence matching procedure has to be employed. Works that followed that Faloutsos's paper usually tried to enhance only the part of the problem mainly by\nintroducing new methods for dimensionality reduction, new distance functions or a bit of a variance in an indexing and slicing strategy for the subsequence bits. We have mentioned the most\nsignificant ones in the previous sections. Those new approaches enhanced the performance of\nthe GEMINI framework but a lot of them were tested only on a constrained, and often artificially\ncreated, datasets. It has been shown in [14] that the comparison of those methods is not as clear\nas some of the authors were trying to declare and so that the results can not be taken as a ground\ntruth. It was argued [24] that the experimental results were computed in various environments\nwith various data sets and with various implementations. This problem was previously called\nimplementation and data bias.\nThe need for the unified testing dataset was settled and fulfilled by the authors of [14] and a\ncollection of testing datasets has been maintained since then, and the authors of new papers were\nasked to perform the tests on this collection to bypass at least the data bias mentioned above. On\nthe other hand, the implementation bias often remains.\nWe would like to address this problem and to present a versatile subsequence matching\nframework that would be usable for rapid prototyping and testing of the current, but mainly\nnew approaches and would serve as trusted platform for performing these tests. By framework,\nwe mean a complex implementation framework for quick but efficient realization of a wide range\nof subsequence matching related applications and approaches. It will contain a library of subcomponents, a way to combine them, and an option to create new ones.\nWe are aware that this goal is not easy one to achieve, because the balance of the boundaries of\nthe framework and the real usability and extensibility must be found so that the developers and\nscientists would adopt it. Nevertheless, we believe that we can deliver such a framework that\ncan helpful to the community. We can offer the experience with building the MESSIF framework\n19\n\n\f[3], part of the MUFIN [2] project that allows fast prototyping and testing of general similarity\nsearch applications literally on any data domain. Until now, from this paper's point of view, these\ntechnologies allowed only dealing with the vectors whole matching problem. What we did was\nthat we used these technologies that were proven by the time and many projects and enriched\nthem with the ability of the subsequence matching.\nTo prove that our solution is really usable for many applications areas, we have decided to\ndemonstrate it on the application related in speech recognition and analysis that covers many\nproblems and many approaches to solve them and where it may not be clear on the first look that\nthe related applications can be solved or enhanced by the state of the art subsequence matching\nsolutions. Later in the text, we will discuss the demo application that demonstrates the combination of state of the art technologies both from the world of speech analysis and subsequence\nmatching.\n\n5.1\n\nIdea\n\nOur Subsequence Matching Framework relies on the MESSIF library that is designed to help to\nimplement similarity search applications in general. We provide several basic classes and an easy\nunderstandable architecture for rapid development and testing of subsequence similarity search\napplications. We have tried to identify the common sub-problems of the subsequence matching\nprocess. We have mentioned some in the rough description of the GEMINI framework but we\nadded some new and still let the architecture opened for implementing brand new approaches.\nTo achieve this we build the framework in a modular way. You can imagine modules dealing with:\n\u2022 normalization of the time-series/sequence data,\n\u2022 transformation for data sequences,\n\u2022 storage index for subsequences,\n\u2022 slicing of the sequences into the windows,\n\u2022 distance function implementations,\n\u2022 lower bounding techniques.\nThe concept of modules (Fig. 8) should allow good re-usability of the code and will help to overcome the implementation bias because once the module responsible for solving some part of the\nproblem is implemented, it should be used by others. The architecture allows to implement subroutines with functionality independent of the specific data type. As you can see on the Fig 8,\nanother substantial part of the framework is the definition of algorithms. The algorithms are responsible for the logic of the subsequence matching, indexing and the execution of the queries.\nEach algorithm declares its parameters. You can imagine that as slots in which the modules are\nplugged in. So in the example, the Simple algorithm is working with two slicing modules, one\ntransformation module, one distance function that obeys the lower bounding lemma and two\nindexes \u2013 one for the original sequences and one for the subsequences. Under the definition of\nthe algorithm we can see two instances altering in the usage of modules. This is possible because\nthe implementation of the modules is independent of the data types (to some level of abstraction) and the algorithm itself only uses the interface common to the modules from the same class.\n20\n\n\fThese features allow to quickly prototype and alter the algorithm by changing the modules that\nare used. So when we have the mechanism of the algorithm implemented using such an interface\nto the the modules declared as a parameters of this algorithm and have implemented a bunch of\nmodules for these classes, we can easily perform lots of combinations and gather the results.\n\nFigure 8: SMF principles.\nAnother great feature of the framework is that altering these combinations does not require to\nre-compile the code. With the usage of the MESSIF batch files one can instantiate new algorithm\nwith new parameters/modules (also instantiated through the batch file) and script the whole series of tests in a very comfortable and fast way.\nThe binding with the MESSIF framework also brings support for performing various similarity queries including classical range queries, KNN-queries etc.\nWe can also leverage many of state of the art metric indexes previously developed for MESSIF\napplications like M-index [33].\n\n5.2\n\nImplementation\n\nAs mentioned above, the subsequence matching framework extends the MESSIF platform and is\na part of the MUFIN project. The whole MUFIN ecosystem and the SMF are developed using Java\nand it works with related technologies like Remote Method Invocation (RMI), Java Server Pages\n(JSP) and use the latest Java features like reflection. We can say that the Subsequence Matching\nFramework layer extends the MESSIF core layer and that the applications written with aid of the\nframework can use the MESSIF-UI module for creating the web application demos. The MESSIF offers lots of functionality when developing any similarity search application like indexing\nand querying in a distributed environment or constructing multi-layered indexing network or\ncomposing combined queries.\nSo far we have developed basic skeleton of the subsequence matching framework and now\nwe are trying to proof the concept by implementing various subsequence matching applications\nwith different data domains and different demands. This should help us to find cons of the\nframework and make the architecture better and more usable for different purposes. The last\napplication domain that has drawn our attention is the speech analysis and generally the audio\nretrieval.\n\n21\n\n\f6\n\nUse Case: Spoken Term Detection\n\nTo prove the applicability of the SMF, we are in a process of implementing an ASR application\nthat would be dealing with the problem of query-by-example spoken term detection. Besides the\nframework itself, the application will employ one of the state of the art metric indexes. Similarly\nas in [20], the application will use the indexing based on posteriogram templates that will be extracted with the aid of BUT phonetic recognizer [42]. In contrast with [20] we will be using other\ndistance function than DTW, which can be seen as an origin for the performance issues related to\nthe computational demands of DTW and the fact that the DTW itself cannot be indexed as a metric space. We believe that we can achieve very similar result in the terms of quality with the usage\nof some edit distance based metric function like ERD or ERP which allow better indexing in metric structures like M-index [33] and that we can achieve better performance results. We would\nalso like to investigate the possibilities for approximate search in the spoken term detection area.\nThe concept of phonetic posteriogram is based on the knowledge of posterior probabilities of\nevery phoneme in the phonetic vocabulary for every time frame. It can be represented as graph\nof phoneme posterior probability in a time (see Fig. 9). For the evaluation of our approaches we\nhave obtained TIMIT [16] and NIST Spoken Term Detection Development Set [18] corpora.\n\nFigure 9: An example posteriorgram representation for the spoken phrase \"basketball and baseball\" [20].\n\n7\n\nConclusion And Future Work\n\nWe have overviewed basic problems and approaches in audio retrieval area and time-series, or\ngenerally sequence, processing. We have outlined that we can apply solutions from subsequence\nmatching area suitable for problems from the audio retrieval area. Our framework should contribute to the solutions in an audio retrieval process by employing subsequence matching techniques. In the further research we would like to search for more possibilities for enhancing the\n22\n\n\fperformance aspects of the audio retrieval by using metric space approach and advanced subsequence matching techniques. Things like tuning index parameters or deciding on the best\nsubsequence matching strategy for the particular application are yet to be properly investigated.\n\nAcknowledgments\nThis work was supported by national research projects VF20102014004, MSMT 1M0545, GACR\n103/10/0886, and GACR P202/10/P220.\n\n23\n\n\fBibliography\n[1] J Batke, G Eisenberg, P Weishaupt, and T Sikora. A Query by Humming system using\nMPEG-7 Descriptors. In 116th AES Convention, pages 588\u2013601. AES, 2004.\n[2] Michal Batko, Vlastislav Dohnal, David Novak, and Jan Sedmidubsky. MUFIN: A Multifeature Indexing Network. 2009 Second International Workshop on Similarity Search and Applications, pages 158\u2013159, 2009.\n[3] Michal Batko, D Novak, and P Zezula. MESSIF : Metric Similarity Search Implementation\nFramework. Lecture Notes in Computer Science, 4877(102):1, 2007.\n[4] Louis Bosch and Bert Cranen. A computational model for unsupervised word discovery.\nOrder A Journal On The Theory Of Ordered Sets And Its Applications, pages 1\u20134, 2007.\n[5] Lou Boves, Rolf Carlson, Erhard Hinrichs, David House, Steven Krauwer, Lothar Lemnitzer,\nMartti Vainio, and Peter Wittenburg. Resources for Speech Research: Present and Future\nInfrastructure Needs, 2009.\n[6] Yuhan Cai and Raymond Ng. Indexing spatio-temporal trajectories with Chebyshev polynomials.\nSIGMOD '04. ACM Press, New York, New York, USA, 2004.\n[7] Kaushik Chakrabarti, Eamonn Keogh, Sharad Mehrotra, and Michael Pazzani. Locally\nadaptive dimensionality reduction for indexing large time series databases. ACM Transactions on Database Systems (TODS), 27(2):188, 2002.\n[8] Kin-Pong Chan and Ada Wai-chee Fu. Efficient Time Series Matching by Wavelets, 1999.\n[9] Lei Chen and Raymond Ng. On the Marriage of L_p-norms and Edit Distance, 2004.\n[10] Lei Chen and M. Tamer \u00d6zsu. Using multi-scale histograms to answer pattern existence and\nshape match queries, 2005.\n[11] Lei Chen, M. Tamer \u00d6zsu, and Vincent Oria. Robust and fast similarity search for moving object\ntrajectories. SIGMOD '05. ACM Press, New York, New York, USA, 2005.\n[12] James W. Cooley and John W. Tukey. An Algorithm for the Machine Calculation of Complex\nFourier Series. Mathematics of Computation, 19(90):297, April 1965.\n[13] Gautam Das, King-ip Lin, Heikki Mannila, Gopal Renganathan, and Padhraic Smyth. Rule\nDiscovery From Time Series, 1998.\n\n24\n\n\f[14] Hui Ding, Goce Trajcevski, Xiaoyue Wang, and Eamonn Keogh. Querying and Mining of\nTime Series Data: Experimental Comparison of Representations and Distance Measures,\n2008.\n[15] Christos Faloutsos, M Ranganathan, and Yannis Manolopoulos. Fast Subsequence Matching\nin Time-Series Databases, 1994.\n[16] John S Garofolo. TIMIT Acoustic-Phonetic Continuous Speech Corpus. Linguistic Data Consortium, Philadelphia, 1993.\n[17] Asif Ghias, Jonathan Logan, David Chamberlin, and Brian C Smith. Query by Humming:\nMusical Information Retrieval in an Audio Database. In ACM Multimedia, pages 231\u2013236.\nACM Press, 1995.\n[18] NIST Multimodal Information Group. 2006 NIST Spoken Term Detection Development Set.\nLinguistic Data Consortium, Philadelphia, 2011.\n[19] Wook-Shin Han, Jinsoo Lee, Yang-Sae Moon, and Haifeng Jiang. Ranked subsequence\nmatching in time-series databases. Very Large Data Bases, pages 423\u2013434, 2007.\n[20] Timothy J. Hazen, Wade Shen, and Christopher White. Query-by-example spoken term detection\nusing phonetic posteriorgram templates. IEEE, December 2009.\n[21] Marijn Huijbregts, Mitchell McLaren, and David van Leeuwen. UNSUPERVISED ACOUSTIC SUB-WORD UNIT DETECTION FOR QUERY-BY-EXAMPLE SPOKEN TERM DETECTION. Language and Speech, pages 4436\u20134439, 2011.\n[22] Eamonn Keogh. A Decade of Progress in Indexing and Mining Large Time Series Databases.\nManagement, (1994):1994\u20131994, 2006.\n[23] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra. Dimensionality Reduction for Fast Similarity Search in Large Time Series Databases, 2000.\n[24] Eamonn Keogh and Shruti Kasetty. On the Need for Time Series Data Mining Benchmarks:\nA Survey and Empirical Demonstration, 2002.\n[25] Eamonn Keogh and Chotirat Ann Ratanamahatana. Exact indexing of dynamic time warping. Knowledge and Information Systems, 7(3):358\u2013386, May 2004.\n[26] S Kim, S Park, and W Chu. An index-based approach for similarity search supporting time\nwarping in large sequence databases. Proceedings of International Conference on Data Engineering, pages 607\u2013614, 2001.\n[27] Hyun-Gil Koh, Woong-Kee Loh, and Sang-Wook Kim. Innovations in Applied Artificial Intelligence, volume 3533 of Lecture Notes in Computer Science. Springer Berlin Heidelberg, Berlin,\nHeidelberg, June 2005.\n[28] Flip Korn, H. V. Jagadish, and Christos Faloutsos. Efficiently supporting ad hoc queries in\nlarge datasets of time sequences. ACM SIGMOD Record, 26(2):289\u2013300, June 1997.\n\n25\n\n\f[29] Seung-Hwan Lim, Heejin Park, and Sang-Wook Kim. Using multiple indexes for efficient\nsubsequence matching in time-series databases. Information Sciences, 177(24):5691\u20135706, December 2007.\n[30] Jessica Lin, Eamonn Keogh, Stefano Lonardi, and Pranav Patel. Finding motifs in time series.\nTime, pages 53\u201368, 2002.\n[31] Yang-Sae Moon, Kyu-Young Whang, and Wook-Shin Han. General match: a subsequence\nmatching method in time-series databases based on generalized windows. International Conference on Management of Data, page 382, 2002.\n[32] Yang-Sae Moon, Kyu-Young Whang, and Woong-Kee Loh. Duality-Based Subsequence\nMatching in Time-Series Databases. Proceedings of the 17th International Conference on Data\nEngineering, page 263, 2001.\n[33] David Novak and Michal Batko. Metric Index: An Efficient and Scalable Solution for Similarity Search. In Second International Workshop on Similarity Search and Applications (SISAP\n2009), volume 9, pages 65\u201373. IEEE, 2009.\n[34] Lawrence O'Rabiner and Biing-Hwang Juang. Fundamentals of Speech Recognition, volume\n103 of Prentice Hall signal processing series. Prentice Hall, 1993.\n[35] Alex S Park and James R Glass. Unsupervised Pattern Discovery in Speech. IEEE Transactions\nOn Audio Speech And Language Processing, 16(1):186\u2013197, 2008.\n[36] J W Picone. Signal modeling techniques in speech recognition. Proceedings of the IEEE,\n81(9):1215\u20131247, 1993.\n[37] Chotirat Ann Ratanamahatana and Eamonn Keogh. Everything you know about Dynamic\nTime Warping is Wrong, 2004.\n[38] Chotirat Ann Ratanamahatana and Eamonn Keogh. Three myths about dynamic time warping data mining. In Proceedings of SIAM International Conference on Data Mining SDM'05,\nvolume 21, pages 506\u2013510. Citeseer, 2005.\n[39] K. V. Ravi Kanth, Divyakant Agrawal, and Ambuj Singh. Dimensionality reduction for\nsimilarity searching in dynamic databases. ACM SIGMOD Record, 27(2):166\u2013176, June 1998.\n[40] H Sakoe and S Chiba. Dynamic programming algorithm optimization for spoken word\nrecognition. IEEE Transactions on Acoustics Speech and Signal Processing, 26(1):43\u201349, 1978.\n[41] Phillipe Salembier and Thomas/Manjunath Sikora. Introduction to MPEG-7: Multimedia\nContent Description Interface. June 2002.\n[42] P Schwarz, P Matejka, and J Cernocky. Towards Lower Error Rates in Phoneme Recognition.\n2004.\n[43] Dirk Von Zeddelmann, Frank Kurth, and Meinard M\u00fcller. Perceptual Audio Features for\nUnsupervised Key-Phrase Detection. In Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing ICASSP, Lecture Notes in Computer Science, 2010.\n\n26\n\n\f[44] Han Wei, Chan Cheong-Fat, Choy Chiu-Sing, and Pun Kong-Pang. An efficient MFCC extraction method in speech recognition. In 2006 IEEE International Symposium on Circuits and\nSystems, page 4. IEEE, 2006.\n[45] Byoung-Kee Yi and Christos Faloutsos. Fast Time Sequence Indexing for Arbitrary Lp\nNorms. pages 385\u2013394, September 2000.\n[46] Byoung-Kee Yi, H.V. Jagadish, and Christos Faloutsos. Efficient retrieval of similar time\nsequences under time warping. Proceedings 14th International Conference on Data Engineering,\npages 201\u2013208, 1998.\n[47] Pavel Zezula, Giuseppe Amato, Michal Batko, and Vlastislav Dohnal. Similarity Search: The\nMetric Space Approach. Springer US, 2005.\n[48] Yunyue Zhu and Dennis Shasha. Warping indexes with envelope transforms for query by\nhumming. In Proceedings of the 2003 ACM SIGMOD international conference on on Management\nof data - SIGMOD '03, page 181, New York, New York, USA, June 2003. ACM Press.\n\n27\n\n\f"}