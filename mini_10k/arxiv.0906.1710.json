{"id": "http://arxiv.org/abs/0906.1710v1", "guidislink": true, "updated": "2009-06-09T12:29:13Z", "updated_parsed": [2009, 6, 9, 12, 29, 13, 1, 160, 0], "published": "2009-06-09T12:29:13Z", "published_parsed": [2009, 6, 9, 12, 29, 13, 1, 160, 0], "title": "The S-Estimator in Change-Point Random Model with Long Memory", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.4555%2C0906.1742%2C0906.2554%2C0906.5338%2C0906.4497%2C0906.2943%2C0906.2512%2C0906.3250%2C0906.3087%2C0906.1567%2C0906.4788%2C0906.4435%2C0906.4802%2C0906.2424%2C0906.4362%2C0906.3413%2C0906.4165%2C0906.1563%2C0906.4200%2C0906.1168%2C0906.0279%2C0906.4510%2C0906.1710%2C0906.5365%2C0906.1529%2C0906.2355%2C0906.0627%2C0906.4970%2C0906.0773%2C0906.3220%2C0906.1554%2C0906.1067%2C0906.5585%2C0906.1461%2C0906.1123%2C0906.3917%2C0906.2000%2C0906.2149%2C0906.1391%2C0906.0713%2C0906.2239%2C0906.5308%2C0906.4645%2C0906.1174%2C0906.1142%2C0906.4689%2C0906.2815%2C0906.0403%2C0906.0073%2C0906.3851%2C0906.0770%2C0906.4648%2C0906.5049%2C0906.4876%2C0906.2115%2C0906.1704%2C0906.3233%2C0906.3857%2C0906.3139%2C0906.3313%2C0906.1507%2C0906.3880%2C0906.4549%2C0906.1764%2C0906.4768%2C0906.2407%2C0906.5032%2C0906.4265%2C0906.2261%2C0906.0177%2C0906.0795%2C0906.4455%2C0906.1887%2C0906.1318%2C0906.0837%2C0906.1175%2C0906.2476%2C0906.4483%2C0906.2851%2C0906.1766%2C0906.1557%2C0906.2859%2C0906.3247%2C0906.4795%2C0906.0009%2C0906.3637%2C0906.2127%2C0906.5611%2C0906.0045%2C0906.0499%2C0906.4246%2C0906.5029%2C0906.1097%2C0906.0915%2C0906.1319%2C0906.4891%2C0906.4339%2C0906.0589%2C0906.4514%2C0906.2359%2C0906.5463&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The S-Estimator in Change-Point Random Model with Long Memory"}, "summary": "The paper considers two-phase random design linear regression models. The\nerrors and the regressors are stationary long-range dependent Gaussian. The\nregression parameters, the scale parameters and the change-point are estimated\nusing a method introduced by Rousseeuw and Yohai(1984). This is called\nS-estimator and it has the property that is more robust than the classical\nestimators; the outliers don't spoil the estimation results. Some asymptotic\nresults, including the strong consistency and the convergence rate of the\nS-estimators, are proved.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.4555%2C0906.1742%2C0906.2554%2C0906.5338%2C0906.4497%2C0906.2943%2C0906.2512%2C0906.3250%2C0906.3087%2C0906.1567%2C0906.4788%2C0906.4435%2C0906.4802%2C0906.2424%2C0906.4362%2C0906.3413%2C0906.4165%2C0906.1563%2C0906.4200%2C0906.1168%2C0906.0279%2C0906.4510%2C0906.1710%2C0906.5365%2C0906.1529%2C0906.2355%2C0906.0627%2C0906.4970%2C0906.0773%2C0906.3220%2C0906.1554%2C0906.1067%2C0906.5585%2C0906.1461%2C0906.1123%2C0906.3917%2C0906.2000%2C0906.2149%2C0906.1391%2C0906.0713%2C0906.2239%2C0906.5308%2C0906.4645%2C0906.1174%2C0906.1142%2C0906.4689%2C0906.2815%2C0906.0403%2C0906.0073%2C0906.3851%2C0906.0770%2C0906.4648%2C0906.5049%2C0906.4876%2C0906.2115%2C0906.1704%2C0906.3233%2C0906.3857%2C0906.3139%2C0906.3313%2C0906.1507%2C0906.3880%2C0906.4549%2C0906.1764%2C0906.4768%2C0906.2407%2C0906.5032%2C0906.4265%2C0906.2261%2C0906.0177%2C0906.0795%2C0906.4455%2C0906.1887%2C0906.1318%2C0906.0837%2C0906.1175%2C0906.2476%2C0906.4483%2C0906.2851%2C0906.1766%2C0906.1557%2C0906.2859%2C0906.3247%2C0906.4795%2C0906.0009%2C0906.3637%2C0906.2127%2C0906.5611%2C0906.0045%2C0906.0499%2C0906.4246%2C0906.5029%2C0906.1097%2C0906.0915%2C0906.1319%2C0906.4891%2C0906.4339%2C0906.0589%2C0906.4514%2C0906.2359%2C0906.5463&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The paper considers two-phase random design linear regression models. The\nerrors and the regressors are stationary long-range dependent Gaussian. The\nregression parameters, the scale parameters and the change-point are estimated\nusing a method introduced by Rousseeuw and Yohai(1984). This is called\nS-estimator and it has the property that is more robust than the classical\nestimators; the outliers don't spoil the estimation results. Some asymptotic\nresults, including the strong consistency and the convergence rate of the\nS-estimators, are proved."}, "authors": ["Gabriela Ciuperca"], "author_detail": {"name": "Gabriela Ciuperca"}, "author": "Gabriela Ciuperca", "links": [{"href": "http://arxiv.org/abs/0906.1710v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0906.1710v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0906.1710v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0906.1710v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "THE S-ESTIMATOR IN CHANGE-POINT\nRANDOM MODEL WITH LONG MEMORY\n\narXiv:0906.1710v1 [math.ST] 9 Jun 2009\n\nGABRIELA CIUPERCA\nUniversit\u00e9 de Lyon, Universit\u00e9 Lyon 1,\nCNRS, UMR 5208, Institut Camille Jordan,\nBat. Braconnier, 43, blvd du 11 novembre 1918,\nF - 69622 Villeurbanne Cedex, France,\nemail: Gabriela.Ciuperca@univ-lyon1.fr\ntel: 33(0)4.72.43.16.90, fax: 33(0)4.72.43.16.87\n\nAbstract\nThe paper considers two-phase random design linear regression models. The errors\nand the regressors are stationary long-range dependent Gaussian. The regression\nparameters, the scale parameters and the change-point are estimated using a method\nintroduced by Rousseeuw and Yohai [33]. This is called S-estimator and it has the\nproperty that is more robust than the classical estimators; the outliers don't spoil\nthe estimation results. Some asymptotic results, including the strong consistency\nand the convergence rate of the S-estimators, are proved.\nKey words: Change-points, S-estimator, Long-memory, Asymptotic properties\nAMS 2000 subject classifications: primary 62F12; secondary 62H12, 60G15.\n\n1\n\nIntroduction\n\nConsider the two-phase linear regression model:\nYt = Xt \u03b21 111\u2264t\u2264[n\u03c0] + Xt \u03b22 11[n\u03c0]+1\u2264t\u2264n + \u03b5t ,\n\nt = 1, ..., n\n\n(1)\n\nwhere 11(.) is the indicator function and \u03c0 \u2208 (0, 1), \u03be = (\u03b21 , \u03b22 , \u03c0), \u03b21 , \u03b22 \u2208 \u03a5. The\nset \u03a5 is a compact of Rd , d \u2265 1. For this model, Yt denotes the response variable,\nXt is a p-vector of regressors and \u03b5t is the error.\nPreprint submitted to Elsevier\n\n13 November 2018\n\n\fThe model parameters are: regression parameters \u03b21 and \u03b22 , change-point \u03c0 and\nerror variance \u03c3 2 , with \u03c3 2 \u2208 (0, \u221e). Let us denote \u03be 0 = (\u03b210 , \u03b220 , \u03c0 0 ) and \u03c302 the true\nvalues of these parameters. In this paper we consider the problem of estimating of\n\u03be and \u03c3 2 , based on the observation of (Yt , Xt )1\u2264t\u2264n .\nClassical estimation methods studied in the statistic literature are the least squares\n(LS), maximum likelihood (ML) or a wider class M-estimation methods. For each\nof these methods one has to distinguish the cases when the errors are independent\nor not, and in the dependent case it is necessary to take into account the covariance\nstructure. The same conditions can be considered for regressors Xt . In traditional\nmethodology, these variables are usually assumed to be independent or with shortmemory. So, if the errors are i.i.d. or with short-memory, the statistic literature\nrelated to the parametric change-point estimation is very vast. Recent developments for the LS estimation include Feder ([13], [14]), Bai and Perron [3], Kim\nand Kim [22]. Bai [1] considers also the least squares estimation of a shift in linP\near process. The process \u03b5t is given by: \u03b5t = \u221e\nis white noise\nj=0 cj uk\u2212j , where u\nPj\nwith mean zero and variance \u03c3 2 and the coefficients cj satisfy \u221e\nj=0 j|cj | < \u221e.\nThis condition excludes long-memory. For the ML estimation we refer to Bhattacharya [7], Koul and Qian [23], Ciuperca and Dapzol [9]. In the general case of\nthe M-estimator, we can cite the papers of Rukhin and Vajda [34], Koul et al. [25].\nObviously, the list is not exhaustive, the subject is so large and productive that\nwe cannot give all the papers. The convergence rate and limiting distributions of\nthe change-point and of the regression parameters M-estimators are derived for\nthe model (1) by Fiteni [15], under restrictive and numerous assumptions. Among\nthese conditions she considers that (Yt , Xt ) is a random vector, L0 -NED, on a\nstrong mixing base {wt ; t = ..., 0, 1, ...}, \u03c1\u2032 (\u03b5t + \u03b8Xt )Xt is a random sequence of\nmean zero, L2 -NED of size 1/2 on a strong mixing base {wt ; t = ..., 0, 1, ...} and\nsupt\u2264n IE[k\u03c1\u2032 (\u03b5t + \u03b8Xt )Xt kr for some r > 2. Under the same dependence assumptions, Fiteni [16] considers the \u03c4 -estimators.\nOn the contrary, in the case of long-memory errors or regressors, the statistical\nliterature related to the parametric change-point estimation is less vast. For the\nsimpler model:\nYt = \u03bc111\u2264t\u2264k\u2217 + (\u03bc + \u03b4)11k\u2217 <t\u2264n + \u03b5t\n\n(2)\n\nwhen the errors \u03b5t are long-memory Gaussian, Horvath and Kokoszka [20] consid\u2217\nered the estimator\nby k\u0302 = min {k; |Uk | = max1\u2264i<n |Ui |}, where, for\n\u0010 of k\u0011\u03b3 defined\nPi\nn\n\u03b3 \u2208 [0, 1), Ui = i(n\u2212i)\nj=1 (Xj \u2212 X\u0304n ). The estimator converges to functionals\nof fractional Brownian motion. For the same model, Hidalgo and Robinson [19],\nSibbertsen [35] consider the LS estimator of k, \u03bc and \u03b4. A more complex model:\nYt = \u03bc + (\u03b2 + \u03b411t<[\u03c4 n] )Xt + \u03b5t is considered in the paper of Lazarov\u00e1 [27], but with\nthe supposition that \u03c4 fixed. The limiting distribution of the LS estimator of the\nparameters \u03b2 and \u03b4 is given.\n2\n\n\fConcerning now the estimation method, it is well known that one outlier may\ncause a large error in a LS-estimator, ML-estimator or more generally in classical\nM-estimator. Nunes et al. [29], Kuan and Hsu [26] observed that, for the data that\nhave long-memory, the LS-estimator may suggest a spurious change-point when\nthere is none. In that case, the parameters of the model can be estimated by using\nleast absolute deviations (LAD) method. If the errors are independent, Bai [2]\nstudies the LAD estimator for a multiple regime linear regression and Ciuperca\n[10] for a nonlinear change-point model. A more robust estimator was introduced\nby Rousseeuw and Yohai [33], by defining the S-estimators as the minimizers of a\nM-estimator of the residual scale. The interest of the S-estimator in respect to the\nLAD-estimators, is the breakdown point introduced by Hampel [18]. The breakdown point amounts to determining the smallest contaminating mass that can\ncause the estimator to take on value arbitrary for from the true value. Instead,\nconcerning this method, to the author's knowledge, the past papers treat only regression models without change-point. For a linear regression model, Davies [11]\nproves the consistency and weak convergence of S-estimator under the assumption that the errors are i.i.d. random variables. The asymptotic behaviour of the\nS-estimator in a linear regression, without change-point can be also found in the\npapers Zhengyan et al. [36], Roeland et al. [32].\nIn the present paper, we consider a linear regression model with a change-point\nin an unknown point. The regressors and the errors are assumed to be Gaussian\nvectors, and respectively variables, with long-memory. The regression parameters,\nthe scale parameter and the change-point location are estimated by the S-method.\nThe difficulty of study of the asymptotic properties of these S-estimators comes\nespecially from the dependence on change-point in the expression of the scale parameter estimators. We first prove that the estimators are strongly convergent and\nafterwards their convergence rates are obtained. These rates depend of covariance\nstructure of Xt and \u03b5t and of Hermite rank of \u03c1(\u03b5t /\u03c30 ) \u2212 IE[\u03c1(\u03b5t /\u03c30 )], where \u03c1 is\nthe function used to construct the S-estimator. For the regression parameters and\nthe scale parameter, we obtain the same convergence rate as in a model without\nchange-point, let us denote it vn . The S-estimator of the change-point has a faster\nconvergence rate, more precisely n\u22121 vn . This result is totally different from those\nobtained in the other papers where the dependence between observations is considered. Especially, let us notice that our change-point S-estimator converges more\nquickly towards true value than in the independence or the short-memory case or\na classic estimation method.\nThe plan of this paper is as follows. In Section 2 we make some notations and\nassumptions afterwards we define the S-estimator for a model with change-point.\nIn Section 3, the asymptotic behaviour of these estimator is studied. The proofs\nof theorems are given in Section 4. Finally, Section 5 contains some lemmas which\nare useful to prove the main results.\n3\n\n\f2\n\nNotation and assumptions\n\nLong-memory (long-range dependent) processes arise in numerous physical and\nsocial sciences. For several examples, see e.g. Baillie [4], Cheung [8], Lo [28] among\nothers. We also mention Guo and Kuol [17], where some currency exchange data\nsets with long-memory are considered. Another long-memory example in economy\nwe find also in Ding et al. [12] on S&P daily 500 stock market returns. In that\npaper, they found that although the returns themselves contain little serial correlation, the absolute value of returns has significantly positive serial correlation up\nto 2700 lags.\nFor the construction of the S-estimators, a function \u03c1 : R \u2212\u2192 [0, 1] is needed.\nThroughout our article, we assume that the following classic conditions are satisfied\nby \u03c1:\n\u2022 \u03c1 is symmetric, continuously differentiable on R and \u03c1(0) = 0.\n\u2022 \u03c1 is increasing in [0, c), for some c > 0, and constant in [c, \u221e).\nLet us denote: \u03c8(z) = \u03c1\u2032 (z).\nAn example of \u03c1 satisfying these conditions was proposed by Beaton and Tukey\n[5], for some c > 0:\n\u03c1(x) =\n\n\uf8f1\n\uf8f4\n\uf8f2 3(x/c)2\n\uf8f4\n\uf8f3\n\n\u2212 3(x/c)4 + (x/c)6 , if |x| \u2264 c\n1,\n\nif |x| > c\n\n(3)\n\nFor model (1), the following assumptions are considered:\n(A1) Xt is a sequence of d-dimensional stationary long-range dependent Gaussian\nvectors, with IE[Xt ] = 0, covariance matrix \u0393(t) = IE[X1 Xt+1 ] = L(t)T N(t)L(t),\nwhere N(t) = diag(t\u2212\u03b81 , ..., t\u2212\u03b8d ), \u03b81 , ..., \u03b8d \u2208 (0, 1) for t \u2265 1 and \u0393(0) = V ar(X1 ).\nL(x) a d \u00d7 d orthogonal matrix of slowly varying functions;\n(A2) \u03b5t a sequence of stationary long-range dependent Gaussian variables, with\nIE[\u03b5t ] = 0, \u03b3(0) = V ar[\u03b5t ] = \u03c302 and the covariance \u03b3(t) = IE[\u03b51 \u03b5t+1 ] = t\u2212\u03b1 L(t),\n\u03b1 \u2208 (0, 1) for t \u2265 1. L(x) a positive slowly varying function;\n(A3) the errors \u03b5t are independent of Xt .\nThe values of \u03b81 , ..., \u03b8d , \u03b1 and the functions expressions of L(x), L(x) are known.\nRecall that a positive measurable function h is slowly varying in Karamata's sense\nif and only if, for any \u03bb > 0, h(\u03bbx)/h(x) converges to 1 as x tends to infinity.\nExamples of slowly varying functions: log x, log log x, log log log x.....\n4\n\n\fInterested readers are referred to Beran [6] or Robinson [31] for a complete reference\non long-memory processes.\nAn example of process Xt = (Xt1 , ..., Xtd ) is obtained when, for some 0 < d1 < 1/2:\nXtj =\n\nd X\nX\n\nBjl (t \u2212 v)\u03c2v,j ,\n\nBjl (v) = v \u2212(1\u2212d1 ) Ljl (v),\n\nv \u2265 1,\n\nj, l = 1, ..., d\n\nl=1 v\u2208Z\n\nwhere Ljl are slowly varying functions and where \u03c2v = (\u03c2v,1 , ..., \u03c2v,d )T , v \u2208 Z are\ni.i.d. with \u03c2v,j , j = 1, ..., d standard Gaussian variables (see Koul and Baillie [24]).\nFor the residual function, let us consider classical notation rt (\u03b2) = Yt \u2212 Xt \u03b2 and\nlet K the constant given by K = IE\u03a6 [\u03c1(\u03b51 /\u03c30 )], where \u03a6 is standard Gaussian\ndistribution.\nIn order to construct the S-estimator in a change-point model (1), we proceed as\nfollows:\n- first, for (\u03b21 , \u03b22 , \u03c0) \u2208 \u03a5 \u00d7 \u03a5 \u00d7 (0, 1) fixed, scale parameter \u03c3 is estimated by the\npositive solution sn (\u03be) = sn (\u03b21 , \u03b22 , \u03c0) of the equation:\n[n\u03c0]\n\u22121\n\nn\n\nn\nX\nrt (\u03b21 )\nrt (\u03b22 )\n\u03c1\n+ n\u22121\n\u03c1\nsn (\u03be)\nsn (\u03be)\nt=1\nt=[n\u03c0]+1\n\nX\n\n!\n\n!\n\n=K\n\n(4)\n\n- at the second stage, the regression parameters are estimated by the argument of\nthe minimum of solution sn (\u03be) obtained of the previous phase:\n\u0010\n\n\u0011\n\n\u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0) = arg\n\nmin\n\n(\u03b21 ,\u03b22 )\u2208\u03a5\u00d7\u03a5\n\nsn (\u03b21 , \u03b22 , \u03c0)\n\n(5)\n\n\u0011\n\n(6)\n\n- in the end, the change-point is estimated by:\n\u0010\n\n\u03c0\u0302n = arg min sn \u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0\n\u03c0\u2208[0,1]\n\nWe shall make the usual identifiability assumption that the two segments are\ndifferent:\n\u03b21 6= \u03b22 ,\n\u2200\u03be \u2208 \u03a5 \u00d7 \u03a5 \u00d7 (0, 1)\n(7)\ni.e. at least one of the coefficients of Xt has a shift. Thus the jump at \u03c0 is non-zero.\nThis condition implies that the solution of (6) is unique and it will be essential in\nthe proof of the strong consistency.\nIf solution sn (\u03be) to (4) exists then it is well-defined, bounded, strictly positive, with\na probability arbitrarily large (see Lemma 5.1). These results are valid regardless\nof the covariance structure of Xt , of \u03b5t and their distribution. What matters is\ntheir average is worth 0 and their variance is bounded.\nIf (4) has more than one solution, sn (\u03be) is defined as the supremum of all solutions.\n5\n\n\fObviously, if function \u03c1 is given by (3), thus equation (4) has at least a solution.\n\u0010\n\n\u0011\n\nIn this context, we define \u03c3\u0302n = sn \u03b2\u03031n (\u03c0\u0302n ), \u03b2\u03032n (\u03c0\u0302n ), \u03c0\u0302n as the S-estimator of \u03c3\nand (\u03b2\u03021n , \u03b2\u03022n ) = (\u03b2\u03031n (\u03c0\u0302n ), \u03b2\u03032n (\u03c0\u0302n )) that of (\u03b21 , \u03b22 ). We shall study the asymptotic\nbehaviour of \u03c3\u0302n , (\u03b2\u03021n , \u03b2\u03022n ) and of \u03c0\u0302n , in the case that equation (4) has at least a\nsolution.\nFor any \u03c6 twice differentiable function, for x, h \u2208 R, throughout this paper we are\ngoing to use the mean value theorem under the form:\n\u0014\n\n\u2032\n\n\u03c6(x + h) = \u03c6(x) + h \u03c6 (x) + h\n\nZ\n\n0\n\n1\n\n\u2032\u2032\n\n(1 \u2212 s)\u03c6 (x + sh)ds\n\n\u0015\n\n(8)\n\nFor a vector V = (v1 , * * * , vm ), let us denote by kV k its Euclidean norm and we\nmake the convention that |V | = (|v1 |, * * * , |vm |).\nIn the following, we denote by C a generic positive finite constant that may be\ndifferent in different context, but will never depend on n.\n\n3\n\nAsymptotic behaviour\n\nThis section establishes asymptotic properties of the S-estimator in model (1). For\nthis purpose, first let us calculate, for solution sn (\u03be) of equation (4), the partial\nderivatives with respect to \u03b21 and \u03b22 . Differentiating (4) with respect to \u03b21 , we\nobtain:\n[n\u03c0]\n\nX\nt=1\n\n[n\u03c0]\nn\nX\nX Xt\nrt (\u03b21 ) \u2202sn (\u03be)\nrt (\u03b22 ) \u2202sn (\u03be)\nrt (\u03b21 )\nrt (\u03b22 )\nrt (\u03b21 )\n+\n+\n\u03c8\n\u03c8\n\u03c8\nsn (\u03be) \u2202\u03b21\nsn (\u03be)\ns (\u03be) \u2202\u03b21\nsn (\u03be)\nsn (\u03be)\nt=1 sn (\u03be)\nt=[n\u03c0]+1 n\n\n!\n\n!\n\n!\n\nConsidering the following notation:\n[n\u03c0]\n\u22121\n\nDn (\u03be) = n\n\nX\nt=1\n\nn\nX\nrt (\u03b21 )\nrt (\u03b22 )\nrt (\u03b21 )\nrt (\u03b22 )\n+ n\u22121\n\u03c8\n\u03c8\nsn (\u03be)\nsn (\u03be)\ns (\u03be)\nsn (\u03be)\nt=[n\u03c0]+1 n\n\n!\n\n!\n\n(9)\n\nand by making similar calculation for \u2202sn (\u03be)/\u2202\u03b22 , we obtain:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\n\u2202sn (\u03be)\n\u2202\u03b21\n\u2202sn (\u03be)\n\u2202\u03b22\n\n=\n\nP[n\u03c0]\n\n\u0010\n\n\u0011\n\nrt (\u03b21 )\nsn (\u03be)\n\u0011\n\u0010\nrt (\u03b22 )\nXt\n\u22121\n\u22121 Pn\n\u2212n Dn (\u03be)\nt=[n\u03c0]+1 sn (\u03be) \u03c8 sn (\u03be)\n\n= \u2212n\u22121 Dn (\u03be)\u22121\n\n6\n\nXt\nt=1 sn (\u03be) \u03c8\n\n(10)\n\n=0\n\n\fSince \u03c1 is symmetric and increasing in [0, c)( and choosing suitably c) we have:\nx\u03c8(x)\n\n\uf8f1\n\uf8f4\n\uf8f2>\n\n\uf8f4\n\uf8f3=\n\n0, if x \u2208 (\u2212c, c) \\ {0}\n\n(11)\n\n0, if x = 0 or |x| \u2265 c\n\nBy means of Lemma 5.2, we prove that the random process Dn (\u03be)\u22121 is bounded\nwith a probability close to 1. In fact, the covariance structure of Xt and of \u03b5t ,\nrespectively, plays no role in this result. Moreover, if both random variables are\nno more Gaussian, Lemma 5.2 holds if Xt and \u03b5t are bounded with a probability\nclose to 1.\nIn order to prove the consistency we require that function \u03c8 also is differentiable\nand strictly increasing on (0, c). This condition will be used for the Taylor's expansion of \u03c1, around (\u03b210 , \u03b220 ), up to second order.\n(H1) \u03c8(.) is differentiable and \u03c8 \u2032 (u) > 0, \u2200u \u2208 (0, c).\nTheorem 3.1 Under assumptions (A1)-(A3), (H1), (7), we have that estimator\na.s.\n\u03be\u02c6n = (\u03b2\u03021n , \u03b2\u03022n , \u03c0\u0302n ) is strongly consistent: \u03be\u02c6n \u2212\u2192 \u03be0 .\nn\u2192\u221e\n\nRemark 3.1 Statement of Theorem 3.1 remains valid, if Xt is not Gaussian, but\nit is i.i.d. and IE[Xt XtT ] < \u221e. If \u03b5t is not Gaussian, it has to be bounded with a\nprobability close to 1.\nAs a consequence of relation (10), the first two stages (4) and (5) in the construction\nof the parameters estimators, are the solutions to the equations system:\n(a) n\u22121\n\nt=1 \u03c1\n\n\u22121 P[n\u03c0]\n\n\u0010\n\nrt (\u03b21 )\n\u03c3\n\n\u0010\n\n\u0011\n\n+ n\u22121\n\n\u0011\n\nPn\n\nt=[n\u03c0]+1 \u03c1\n\nrt (\u03b21 )\nXt = 0\n\u03c3\n\u0010\n\u0011\nP\n2)\nn\u22121 nt=[n\u03c0]+1 \u03c8 rt (\u03b2\nXt =\n\u03c3\n\n(b) n\n(c)\n\nP[n\u03c0]\nt=1\n\n\u03c8\n\n\u0010\n\nrt (\u03b22 )\n\u03c3\n\n\u0011\n\n\u2212K =0\n(12)\n\n0\n\nSince the change-point intervention is essential, the convergence study of the scale\nparameter estimator is realized separately. According to Theorem 3.1, we fix \u03c0\nin a neighbourhood V(\u03c0 0 ) of \u03c0 0 . In order to show the convergence of the scale\nparameter estimator, supplementary assumptions are needed.\n(H2) \u03c8 is twice differentiable with bounded second derivative.\n(H3) \u03c8(x)/x is nonincreasing for x > 0.\n7\n\n\fObviously, function (3) satisfies assumptions (H1)-(H3). As will be seen below,\nassumption (H2) is needed to control the rest in the Taylor's expansion of sn (\u03be),\nwhile (H3) is used in order to apply results of Zhengyan et al. [36] on the consistency of the scale S-estimator in a model without change-point. Moreover, in the\npaper of Zhengyan et al. [36], the assumption (H3) is needed to show the convergence of the regression parameter estimator, which is not the case here.\nTheorem 3.2 Under (A1)-(A3), (H1)-(H3), (7), for all \u03c0 in a neighbourhood\na.s.\nV(\u03c0 0 ) of \u03c0 0 , the estimator of \u03c30 is strongly consistent: sn (\u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0) \u2212\u2192\nn\u2192\u221e\n\u03c30 .\nCorollary 3.1 Under (A1)-(A3), (H1)-(H3), (7), scale parameter S-estimator\n\u03c3\u0302n = sn (\u03b2\u03021n , \u03b2\u03022n , \u03c0\u0302n ) is strongly consistent for \u03c30 .\nRemark 3.2 In a model without change-point, the assumption (H2) is needed for\nfound the convergence rate and the asymptotic distribution of the estimators but\nnot in the convergence proof.\nRemark 3.3 The convergence result of Theorem 3.2 holds if random vector Xt is\nnot more Gaussian but i.i.d. with IE[Xt ] = 0 and IE[Xt XtT ] < \u221e.\nIn order to find the convergence rate, we will use the Hermite expansion for a\nfunction of standard Gaussian variable (for details about the Hermite expansion\nsee for example Palma [30]). Let us consider function\u0010 \u03c7(.)\n\u0011 := \u03c1(.) \u2212 K, where\n\u03b51\nK = IE\u03a6 [\u03c1(\u03b51 /\u03c30 )]. Suppose that the Hermite rank of \u03c7 \u03c30 is q1 . Because function\n\u03c1 is symmetric and \u03c1(0) = 0, we have q1 \u2265 2. If we denote \u03bdt = \u03b5t /\u03c30 , then:\n\u03c7(\u03bdt ) =\n\nJq (\u03c7)\nHq (\u03bdt )\nq!\nq\u2265q1\nX\n\nwith Hq the Hermite polynomial, Jq (\u03c7) = IE[\u03c7(\u03bd1 )Hq (\u03bd1 )] and for all t, t\u2032 = 1, * * * n:\nIE[Hp (\u03bdt )Hq (\u03bdt\u2032 )] = q!\u03b3 q (t \u2212 t\u2032 )11p=q\n\n(13)\n\nLet also k = min{(\u03b1q1 )/2, (\u03b8i + \u03b1)/2, 1 \u2264 i \u2264 d}.\nIn order to have the rate of convergence of the estimators in a model without\nchange-point, following assumptions are imposed by Zhengyan et al [36]: \u03b1q1 < 1\nand max{\u03b1 + \u03b8j ; 1 \u2264 j \u2264 d}.\nThe following theorem gives the convergence rate of the regression parameters and\nof the scale parameter estimators. These rates are the same that in a model with8\n\n\fout change-point.\nTheorem 3.3 For all \u03c0 \u2208 (0, 1), if (A1)-(A3), (H1)-(H3), (7) hold, we have\n\u0010\n\n\u0011\n\n\u0010\n\nk\u03b2\u03031n (\u03c0) \u2212 \u03b210 k = OIP (n\u03c0)\u2212k L1 (n\u03c0) = OIP n\u2212k L\u03031 (n)\n\u0010\n\n\u0011\n\n\u0011\n\n\u0010\n\nk\u03b2\u03032n (\u03c0) \u2212 \u03b220 k = OIP (n(1 \u2212 \u03c0))\u2212k L1 (n(1 \u2212 \u03c0)) = OIP n\u2212k L\u03031 (n)\n\n\u0011\n\nwhere L1 and L\u03031 are slowly varying functions. For the scale\nparameter,\nputting\n\u0010\n\u0011\n0\n\u2212k\ns\u0303n (\u03c0) := sn (\u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0), we have |s\u0303n (\u03c0) \u2212 \u03c3 | = OIP n L\u03031 (n) .\nNow let us study the convergence rate of the change-point estimator:\n\u0010\n\n\u0011\n\nh\n\n\u0010\n\n\u0011\n\n\u03c0\u0302n = arg min sn \u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0 = arg min sn \u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0 \u2212 sn (\u03b210 , \u03b220 , \u03c0)\n\u03c0\n\n\u03c0\n\nFor that we consider one of the last two equations of (12), for instance (c):\nn\u22121\n\nn\nX\n\nt=[n\u03c0]+1\n\n\uf8eb\n\n\u03c8\uf8ed\n\nrt (\u03b2\u03032n (\u03c0))\n\u0010\n\nsn \u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0\n\n\uf8f6\n\n\u0011 \uf8f8 Xt\n\n=0\n\nTheorem 3.4 Under assumptions (A1)-(A3), (H1)-(H3), (7), we have \u03c0\u0302n \u2212 \u03c0 0 =\nOIP (n\u22121\u2212k L\u03031 (n)) with L\u03031 (n) a slowly varying function.\nExample. If \u03b1 \u2265 maxi=1,...,d \u03b8i , then k = (\u03b1 + mini=1,...,d \u03b8i )/2 \u2264 \u03b1.\nWhat is remarkable comparatively to the independence or the short-memory case\nis that \u03c0\u0302n converges faster towards \u03c00 when Xt or \u03b5t are long-range dependent.\nConsider the particular case \u03b1 = \u03b81 = ... = \u03b8d , then k = \u03b1. Further if \u03b1 \u2208 (1/2, 1),\nthen, for the estimators of \u03b21 and \u03b22 , we have a faster convergence rate than in the\nindependence or short-memory case. Finally, the long-memory brings about that\nthe true values of the parameters are faster approached.\nRemark also that the obtained convergence rate completely differs from that of\nchange-point \u03c4 -estimators when Xt are NED-dependent (Fiteni [15], [16]). If Xt are\nindependent, the convergence rate is n\u22121 for the change-point estimator and n\u22121/2\nfor the parameters regression estimator, indifferently of used method: M-method\n(Koul et al. [25]), ML-method (Ciuperca and Dapzol [9]), LS-estimation (Bai and\nPerron [3]). Same convergence rate, n\u22121 , is obtained for change-point LS-estimator\nin a model with correlated errors, but not with long-memory (Bai [1]).\n9\n\ni\n\n\fIt is interesting to note that the rate convergence of the change-point estimator\nin the mean of Gaussian variable (2), having long-range dependence, considered\nby Horvath and Kokoszka [20], is n\u22121 g \u22121 (1/\u03b4) with g a regular varying function.\nThus, the estimator of Horvath and Kokoszka [20] is slower than our estimator.\nOn the other hand, let us remark that convergence rate of the S-estimators depends\nof the Hermite rank of \u03c1(\u03b51 /\u03c30 ) \u2212 K and of the covariance structure of Xt and \u03b5t .\n\n4\n\nProofs of Theorems\n\nProof of Theorem 3.1. Let us consider the function e(\u03be) = IE[sn (\u03b7, \u03c0)\u2212sn (\u03b7 0 , \u03c0 0 )],\nwith supposition, without loss the generality, that \u03c0 \u2264 \u03c0 0 . Using the same arguments as for (37), we obtain that: IE[|sn (\u03b7, \u03c0) \u2212sn (\u03b7, \u03c0 0 )|] \u2264 Ck\u03b21 \u2212\u03b22 k * |\u03c0 \u2212\u03c0 0| <\n\u221e and similarly to (34): IE[|sn (\u03b7, \u03c0 0) \u2212 sn (\u03b7 0 , \u03c0 0 )|] \u2264 Ck\u03b7 \u2212 \u03b7 0 k. Thus, function\ne(\u03be) is well-defined. By Lemma 5.3, function e(\u03be) is continuous and furthermore\ne(\u03be 0 ) = 0. For using an argument like the one in Huber [21], we will to prove that:\nIE[sn (\u03b7, \u03c0) \u2212 sn (\u03b7 0 , \u03c0 0 )] > 0, for every \u03be 6= \u03be 0 . Since sn (\u03be) and sn (\u03be 0) are both\n(0)\n(1)\n(0)\n(1)\n(0)\n(1)\nsolutions of equation (4), we have 0 = (S1,n + S1,n ) + (S2,n + S2,n ) + (S3,n + S3,n ),\nwith:\n(0)\nS1,n\n\n\u22121\n\n\u2261n\n\n[n\u03c0] \"\n\nX\nt=1\n\n(0)\nS2,n\n\nrt (\u03b210 )\nrt (\u03b210 )\n\u03c1\n\u2212\u03c1\nsn (\u03be)\nsn (\u03be 0 )\n\n[n\u03c0 0 ]\n\u22121\n\n\u2261n\n\nX\n\nt=[n\u03c0]+1\n(0)\nS3,n\n\n\u22121\n\n\u2261n\n\nn\nX\n\n!\n\n!\n\n(1)\nS1,n\n\n,\n\nrt (\u03b210 )\nrt (\u03b210 )\n\u2212\u03c1\n\u03c1\nsn (\u03be)\nsn (\u03be 0)\n\n\"\n\nt=[n\u03c0 0 ]+1\n\n!#\n\n!\n\n\u2261n\n\n[n\u03c0] \"\n\nX\nt=1\n\n!#\n\nrt (\u03b220 )\nrt (\u03b220 )\n\u03c1\n\u2212\u03c1\nsn (\u03be)\nsn (\u03be 0 )\n\n\"\n\n\u22121\n\n,\n\nrt (\u03b21 )\nrt (\u03b210 )\n\u03c1\n\u2212\u03c1\nsn (\u03be)\nsn (\u03be)\n!\n\n[n\u03c0 0 ]\n\n(1)\nS2,n\n\nX\n\n\u22121\n\n\u2261n\n\nt=[n\u03c0]+1\n\n!#\n(0)\n\n,\n\n(1)\nS3,n\n\n\u22121\n\n\u2261n\n\nn\nX\n\n\"\n\nrt (\u03b210 )\nrt (\u03b22 )\n\u2212\u03c1\n\u03c1\nsn (\u03be)\nsn (\u03be)\n\nt=[n\u03c0 0 ]+1\n(0)\n\n!#\n\n!\n\n\"\n\nrt (\u03b22 )\nrt (\u03b220 )\n\u03c1\n\u2212\u03c1\nsn (\u03be)\nsn (\u03be)\n!\n\n(0)\n\nThen, by the mean value theorem (TVM), S1,n + S2,n + S3,n can be written as:\n\u22121\n\nn\n\n! \uf8ee[n\u03c0]\n\n[n\u03c0 0 ]\n\nX\nX\nrt (\u03b210 )\n1\nrt (\u03b210 )\n1\n0\n0\n\uf8f0\nr\n(\u03b2\n)\u03c8\n+\n\u2212\nr\n(\u03b2\n)\u03c8\nt 1\nt 1\n(1)\n(2)\nsn (\u03be) sn (\u03be 0 )\nun (\u03b7 0 , \u03c0, \u03c0 0 )\nun (\u03b7 0 , \u03c0, \u03c0 0)\nt=1\nt=[n\u03c0]+1\n\n+\n\n!\n\nn\nX\n\nrt (\u03b220 )\u03c8\n\nt=[n\u03c0 0 ]+1\n\nrt (\u03b220 )\n(3)\n\nun (\u03b7 0 , \u03c0, \u03c0 0 )\n\n!\uf8f9\n\uf8fb\n\n(2)\n(3)\nwith u(1)\nn , un , un defined in the same way as in the proof of the Lemma 5.3.\n(0)\n(0)\n(0)\nMoreover, using property (11), we have the following: S1,n + S2,n + S3,n = [sn (\u03be 0) \u2212\nsn (\u03be)]Vn , where Vn is a positive random variable with probability close to 1.\n\n10\n\n!#\n\n!\n\n!#\n\n\f(1)\n\n(1)\n\n(1)\n\nMoreover, using Taylor's expansion, the expressions of S1,n , S2,n and S3,n can be\nwritten as:\n(1)\nS1,n\n\n[n\u03c0]\n\u22121\n\n=n\n\nX\n\nXt (\u03b210 \u2212\u03b21 )\n\nt=1\n\n(1)\nS2,n\n\nrt (\u03b210 )\n1\n\u03b5t + \u03b41 Xt (\u03b210 \u2212 \u03b21 )\n\u03c8\n+ \u03c8\u2032\n(\u03b210 \u2212 \u03b21 )T XtT\nsn (\u03be)\n2\nsn (\u03be)\n\n\"\n\n!\n\n[n\u03c0 0 ]\n\nX\n\n\u22121\n\n=n\n\nXt (\u03b210 \u2212\u03b22 )\n\nt=[n\u03c0]+1\n(1)\nS3,n\n\nn\nX\n\n\u22121\n\n=n\n\n#\n\nrt (\u03b210 )\n1\n\u03b5t + \u03b42 Xt (\u03b210 \u2212 \u03b22 )\n\u03c8\n+ \u03c8\u2032\n(\u03b210 \u2212 \u03b22 )T XtT\nsn (\u03be)\n2\nsn (\u03be)\n\n\"\n\nXt (\u03b220 \u2212\u03b22 )\n\nt=[n\u03c0 0 ]+1\n\n!\n\n!\n\n!\n\n1\n\u03b5t + \u03b43 Xt (\u03b220 \u2212 \u03b22 )\nrt (\u03b220 )\n+ \u03c8\u2032\n(\u03b220 \u2212 \u03b22 )T XtT\n\u03c8\nsn (\u03be)\n2\nsn (\u03be)\n!\n\n\"\n\n!\n\nwith \u03b41 , \u03b42 , \u03b43 \u2208 (0, 1). By the ergodic theorem, we obtain:\nn\u22121\n\nP[n\u03c0]\nt=1\n\nXt (\u03b210 \u2212 \u03b21 )\u03c8\n\n\u22121 Pn\n\nn\n\n0\nt=[n\u03c0 0 ]+1 Xt (\u03b22\n\n\u0010\n\nrt (\u03b210 )\nsn (\u03be)\n\n\u2212 \u03b22 )\u03c8\n\n\u0010\n\n\u0011\n\n= oIP (1), n\u22121\n\nrt (\u03b220 )\nsn (\u03be)\n\n\u0011\n\n= oIP (1)\n\nP[n\u03c00 ]\n\n0\nt=[n\u03c0]+1 Xt (\u03b21 \u2212 \u03b22 )\u03c8\n\n\u0010\n\nrt (\u03b210 )\nsn (\u03be)\n\n\u0011\n\n= oIP (1),\n\n(14)\nRelation (14) and assumption (H1) imply: for any \u03be 6= \u03be 0 , for all \u01eb > 0, there exits\na > 0, such that\n(1)\n(1)\n(1)\nIP [S1,n + S2,n + S3,n > a] > 1 \u2212 \u01eb\n(15)\n(1)\n\n(1)\n\n(1)\n\n(0)\n\n(0)\n\n(0)\n\nAssumption (7), the above relation and S1,n + S2,n + S3,n = \u2212(S1,n + S2,n + S3,n ) =\n[sn (\u03be) \u2212 sn (\u03be 0 )]Vn , with Vn > 0, imply the conclusion IE[sn (\u03b7, \u03c0) \u2212 sn (\u03b7 0 , \u03c0 0 )] >\n0, for all \u03be 6= \u03be 0. Using this, the compactness of the parameter space, \u03be\u02c6n =\narg min\u03be\u2208\u03a5\u00d7\u03a5\u00d7[0,1] sn (\u03be) and an argument like one in Huber [21], the strongly convergence of \u03be\u02c6n results.\n\u0004\nProof of Theorem 3.2. We first prove that, if we consider in (1) the true value\nfor \u03b7 and \u03c0, then the scale parameter estimator is strongly consistent:\na.s.\n\nsn (\u03b7 0 , \u03c0 0 ) \u2212\u2192 \u03c30\n\n(16)\n\nn\u2192\u221e\n\nLet us observe that in fact sn (\u03b7 0 , \u03c0 0 ) is the solution of a problem without breaking:\n[n\u03c0 0 ]\n\u22121\n\nK=n\n\nX\nt=1\n\n#\n\nn\nX\n\u03b5t\n\u03b5t\n\u22121\n\u03c1\n+n\n\u03c1\n0\nsn (\u03be )\nsn (\u03be 0 )\nt=[n\u03c0 0 ]+1\n\n!\n\n!\n\n\u22121\n\n=n\n\nn\nX\n\n\u03b5t\n\u03c1\nsn (\u03be 0)\nt=1\n\n!\n\nand then, relation (16) is obtained by Theorem 3.1 of Zhengyan et al. [36]. Now,\nas a consequence of Theorem 3.1, we may consider only the case (\u03b7, \u03c0) in a neigh11\n\n#\n\n\fbourhood V(\u03b7 0 , \u03c0 0 ) of (\u03b7 0 , \u03c0 0 ). Consider the decomposition:\nsn (\u03b7, \u03c0)\u2212sn (\u03b7 0 , \u03c0 0 ) = [sn (\u03b7, \u03c0)\u2212sn (\u03b7 0 , \u03c0)]+[sn (\u03b7 0 , \u03c0)\u2212sn (\u03b7 0 , \u03c0 0 )] :\u2261 S1 (n)+S2(n)\n(17)\nSince S1 (n), depends only on the regression parameters, by Theorem 3.1, taking\ninto account relations (10) and (32), we readily obtain:\na.s.\n\nsup |sn (\u03b7, \u03c0) \u2212 sn (\u03b7 0 , \u03c0)| \u2212\u2192 0\n\n(18)\n\nn\u2192\u221e\n\n\u03b7\u2208V(\u03b70 )\n\nFor S2 (n), an argument like the one used for (35) yield that sn (\u03b7 0 , \u03c0) \u2212 sn (\u03b7 0 , \u03c0 0 )\nbehaves as:\n!\n[n\u03c0 0 ]\nX\nr\u0303t (\u03b210 , \u03b220 )\n\u22121\nn\nXt \u03c8\nsn (\u03b7 0 , \u03c0)\nt=[n\u03c0]+1\n\nwhere r\u0303t (\u03b210 , \u03b220 ) = rt (\u03b210 )+mt [rt (\u03b220 )\u2212rt (\u03b210 )], with 0 < mt < 1. Let us remark that\nr\u0303t (\u03b210 , \u03b220) = \u03b5t . We write Taylor's expansion of \u03c8 (\u03b5t /sn (\u03b7 0 , \u03c0)) around \u03c8 (\u03b5t /\u03c30 )\nup to second order:\n[n\u03c0 0 ]\n\u22121\n\nn\n\nX\nt=1\n\n\u03b5t\nXt \u03c8\nsn (\u03b7 0 , \u03c0)\n\n[n\u03c0 0 ]\n\n!\n\n\u22121\n\n=n\n\nX\nt=1\n\n0\n\n[n\u03c0 ]\n, \u03c0) \u2212 \u03c30 )2 X \u2032\u2032\n\u03c8 (\u03c2t )\u03b52t Xt\n0\n2\u03c30 sn (\u03b7 , \u03c0) t=1\n\n\u22121 (sn (\u03b7\n\n+n\n\n0\n\n\u0012 \u0013\n\u0013\n[n\u03c0 ]\n0\nX\n\u03b5t\n\u03b5t\n\u2032\n\u22121 sn (\u03b7 , \u03c0) \u2212 \u03c30\nXt \u03c8\nXt \u03b5 t \u03c8\n\u2212n\n0\n\u03c30\n\u03c30 sn (\u03b7 , \u03c0) t=1\n\u03c30\n\u0012\n\n0\n\nwith \u03c2t = \u03b5t [sn (\u03b7 0 , \u03c0) + \u03c5t (\u03c30 \u2212 sn (\u03b7 0 , \u03c0))]/(\u03c30 sn (\u03b7 0 , \u03c0)), \u03c5t \u2208 (0, 1). Since \u03c8 \u2032\u2032 is\nP[n\u03c00 ]\nbounded, we have n\u22121 t=1 \u03c8 \u2032\u2032 (\u03c2t )\u03b52t Xt < \u221e with probability 1. Moreover:\n\u03c0\n\n0\n\n\uf8ee\n\uf8f0\n\n1\n[n\u03c0 0 ]\n\nP[n\u03c00 ]\n\n[n\u03c0 0 ]\n\nX\nt=1\n\nXt \u03b5 t \u03c8\n\n\u2032\n\n\u0012\n\n\uf8f9\n\u0013\n\u03b5t \uf8fb\n\n\u03c30\n\na.s.\n\n0\n\n\u0014\n\n\u2212\u2192 \u03c0 IE Xt \u03b5t \u03c8\n\nn\u2192\u221e\n\n\u2032\n\n\u0012\n\n\u03b5t\n\u03c30\n\n\u0013\u0015\n\n=0\n\nHence: n\u22121 t=1 Xt [\u03c8 (\u03b5t /sn (\u03b7 0 , \u03c0)) \u2212 \u03c8 (\u03b5t /\u03c30 )] = oIP (sn (\u03b7 0 , \u03c0) \u2212 \u03c30 ). This relaP[n\u03c00 ]\na.s.\ntion and n\u22121 t=1 Xt \u03c8 (\u03b5t /\u03c30 ) \u2212\u2192 0 yield that S2 (n) = sn (\u03b7 0 , \u03c0) \u2212 sn (\u03b7 0 , \u03c0 0 ) =\nn\u2192\u221e\noIP (1) + oIP (sn (\u03b7 0 , \u03c0) \u2212 \u03c30 ) = oIP (1) + oIP (S2 (n)), for the last relation we have used\na.s.\n(16). Then sup\u03c0\u2208V(\u03c00 ) |S2 (n)| \u2212\u2192 0. This fact, with relation (18), together with\nn\u2192\u221e\ndecomposition (17) and relation (16), yield the Theorem.\n\u0004\nProof of Theorem 3.3. For \u03c0 \u2208 (0, 1) fixed, the convergence rate of the regression\nparameters estimator \u03b2\u03031n (\u03c0) and \u03b2\u03032n (\u03c0) is obtained by the application of Zhengyan\net al. [36] results on every segment. On the other hand, the study of the convergence\nrate of \u015dn is more difficult because it interferes in both segments. For notational\n12\n\n\fsimplicity, in the rest of this proof, we denote \u03b2\u03031n = \u03b2\u03031n (\u03c0), \u03b2\u03032n = \u03b2\u03032n (\u03c0) and\ns\u0303n = s\u0303n (\u03c0). The study will be made in two stages. First, we are going to write\nequation (12)(a) in another form, putting in evidence \u03c30 by a limited development.\nAfterwards, in the second stage, the obtained form is studied by taking into account\nthe convergence rate of the regression parameters estimators and what Xt , \u03b5t are\nlong-memory Gaussian.\nStage 1. Equation (12)(a) can be expressed as:\n[n\u03c0]\n\u22121\n\nn\n\nn\nX\nrt (\u03b2\u03031n )\nrt (\u03b2\u03032n )\n\u03c7\n+ n\u22121\n\u03c7\ns\u0303n\ns\u0303n\nt=1\nt=[n\u03c0]+1\n\n!\n\nX\n\n!\n\n=0\n\n(19)\n\nWe apply (8) to function \u03c7 and for: \u0010\n\u0011\nt = 1, * * * , [n\u03c0], xt = rt (\u03b2\u03031n )/s\u0303n , ht = \u03c30\u22121 \u2212 s\u0303\u22121\nrt (\u03b2\u03031n )\nn\n\u0010\n\n\u0011\n\nt = [n\u03c0] + 1, * * * , n, xt = rt (\u03b2\u03032n )/s\u0303n , ht = \u03c30\u22121 \u2212 s\u0303\u22121\nrt (\u03b2\u03032n )\nn\nHence, for the part t = 1, * * * , [n\u03c0], we have:\n\u22121\n\nn\n\n[n\u03c0]\nP\nt=1\n\n\u22121\n\n\u03c7(xt + ht ) = n\n\n+\n\n[n\u03c0]\nP\nt=1\n\ns\u0303n \u2212\u03c30\n\u03c30 s\u0303n\n\n\u03c7(xt ) +\n\n0\nn\u22121 s\u0303\u03c3n0\u2212\u03c3\ns\u0303n\n\n\"\n\n[n\u03c0]\nP\nt=1\n\nn\u22121\n\n[n\u03c0]\nP\nt=1\n\nR\nrt2 (\u03b2\u03031n ) 01 (1\n\nrt (\u03b2\u03031n )\u03c8(xt )\n\n\u2212 s)\u03c8\n\n\u2032\n\n\u0012\n\nrt (\u03b2\u03031n )\n\u03c30\n\n+ s * rt (\u03b2\u03031n )\n\n\u0010\n\n1\n\u03c30\n\n\u2212\n\n1\ns\u0303n\n\n(20)\nThus, in order to study the first sum of (19), we shall analyse the terms of the\nright-hand side of (20).\nWe first consider the last term of the right-hand side of (20). Elementary algebra\nyields that:\n\uf8ee\n\n[n\u03c0]\n[n\u03c0]\n[n\u03c0]\nX\ns\u0303n \u2212 \u03c30 X 2\ns\u0303n \u2212 \u03c30 \uf8f0 \u22121 X 2\nn\u22121\nrt (\u03b2\u03031n ) =\n\u03b5t + n\u22121 (\u03b2\u03031n \u2212 \u03b210 )T X T Xt (\u03b2\u03031n \u2212 \u03b210 )\nn\n\u03c30 s\u0303n t=1\n\u03c30 s\u0303n\nt=1\nt=1\n\n\uf8f9\n\n[n\u03c0]\n\n+ 2n\u22121\n\nX\nt=1\n\n\u22121 P[n\u03c0]\n\n\u03b5t Xt (\u03b2\u03031n \u2212 \u03b210 )\uf8fb\n\nP[n\u03c0]\n\n2\n\u22121\nT\nBy the ergodic theorem n\nt=1 \u03b5t = OIP (1), n\nt=1 X Xt = OIP (1) and since\nP\n[n\u03c0]\n\u03b5t and Xt are independent, we have n\u22121 t=1 \u03b5t Xt = oIP (1). Thus, since \u03c8 \u2032 is\nbounded, s\u0303n \u2212 \u03c30 = oIP (1), s\u0303n > 0 with probability 1, the last term of the righthand side of (20) is oIP (1).\nWe now consider the second term of the right-hand side of (20). For the sum, we\nhave:\n! [n\u03c0]\n!\n[n\u03c0]\nX\nX\nrt (\u03b210 )\nrt (\u03b2\u03031n )\n0\n\u22121\nrt (\u03b21 )\u03c8\n\u2212\nrt (\u03b2\u03031n )\u03c8\nn\ns\u0303n\n\u03c30\nt=1\nt=1\n\n13\n\n\u0011\u0013\n\nds\n\n#\n\n\f[n\u03c0] h\n\nX\n\n\u22121\n\n\u2264n\n\nrt (\u03b2\u03031n ) \u2212\n\nrt (\u03b210 )\n\nt=1\n\ni\n\nrt (\u03b2\u03031n )\n\u03c8\ns\u0303n\n\n[n\u03c0]\n\n!\n\nX\n\n\u22121\n\n+n\n\nrt (\u03b210 )\n\nt=1\n\nand since \u03c8 is bounded:\nX\n\n!\n\n!#\n\n[n\u03c0]\n\n[n\u03c0]\n\n\u2264 Ck\u03b2\u03031n \u2212 \u03b210 kn\u22121\n\nrt (\u03b2\u03031n )\nrt (\u03b210 )\n\u03c8\n\u2212\u03c8\ns\u0303n\n\u03c30\n\n\"\n\nkXt k + Cn\u22121\n\nX\n\nk\u03b5t k\n\nt=1\n\nt=1\n\nThe above inequality, with the ergodic theorem, IE[kXt k] < \u221e, IE[k\u03b5t k] < \u221e and\n\u03b2\u03031n \u2212 \u03b210 = oIP (1), imply that\n[n\u03c0]\n\nn\n\n[n\u03c0]\n\nX\nrt (\u03b2\u03031n )\nrt (\u03b210 )\nrt (\u03b2\u03031n )\u03c8\nrt (\u03b210 )\u03c8\n\u2212 n\u22121\ns\u0303n\n\u03c30\nt=1\nt=1\n!\n\nX\n\u22121\n\n!\n\n= oIP (1)\n\nThus, the second term of the right-hand side of (20) can be expressed:\n\uf8eb\n\n[n\u03c0]\n\n\uf8f6\n\nX\ns\u0303n \u2212 \u03c30 \uf8ed \u22121 [n\u03c0]\n\u03b5t\n\u22121\nht \u03c8(xt ) =\n\u03b5t \u03c8\nn\n+ oIP (1)\uf8f8\nn\n\u03c3\ns\u0303\n\u03c3\n0\nn\n0\nt=1\nt=1\nX\n\n\u0012\n\n\u0013\n\nThen, relation (20) becomes:\n[n\u03c0]\n\n[n\u03c0]\n\nX\n\nX\n\n\uf8eb\n\n[n\u03c0]\n\n\uf8f6\n\ns\u0303n \u2212 \u03c30 \uf8ed \u22121 X\n\u03b5t\n\u03b5t \u03c8\n\u03c7(xt ) +\n+ oIP (1)\uf8f8\nn\n\u03c7(xt + ht ) = n\u22121\nn\u22121\n\u03c30 s\u0303n\n\u03c30\nt=1\nt=1\nt=1\n\u0012\n\n\u0013\n\n(21)\n\nA similar relation holds for the part t = [n\u03c0] + 1, * * * , n:\n\uf8eb\n\n\uf8f6\n\nn\n\u03b5t\ns\u0303n \u2212 \u03c30 \uf8ed \u22121 X\n+ oIP (1)\uf8f8\nn\nn\u22121\n\u03b5t \u03c8\n\u03c7(xt +ht ) = n\u22121\n\u03c7(xt )+\n\u03c3\ns\u0303\n\u03c3\n0\nn\n0\nt=[n\u03c0]+1\nt=[n\u03c0]+1\nt=[n\u03c0]+1\n(22)\nAdding (21) and (22), taking into account the relation (19), we obtain:\nn\nX\n\nn\nX\n\n[n\u03c0]\n\u22121\n\n0=n\n\n\u0012\n\n\u0013\n\nn\nn\nX\nX\nrt (\u03b2\u03031n )\n\u03b5t\ns\u0303n \u2212 \u03c30\nrt (\u03b2\u03032n )\n\u03c7\n\u03b5t \u03c8\n+n\u22121\n+\nn\u22121\n+ oIP (1)\n\u03c7\n\u03c30\n\u03c30\n\u03c30 s\u0303n\n\u03c30\nt=1\nt=1\nt=[n\u03c0]+1\n\n!\n\nX\n\n!\n\n\u0012\n\n\u0013\n\nIP\n\nBy ergodic theorem: n\u22121 nt=1 \u03b5t \u03c8 (\u03b5t /\u03c30 ) \u2212\u2192 IE [\u03b51 \u03c8 (\u03b51 /\u03c30 )].\nn\u2192\u221e\nStage 2. Then, the convergence rate of s\u0303n will be obtained by studying:\nP\n\n[n\u03c0]\n\u22121\n\nn\n\nn\nX\nrt (\u03b2\u03031n )\nrt (\u03b2\u03032n )\n\u22121\n\u03c7\n+n\n\u03c7\n\u03c30\n\u03c30\nt=1\nt=[n\u03c0]+1\n\nX\n\n!\n\n14\n\n!\n\n\u03c30 \u2212 s\u0303n\n\u03b51\n=\nIE \u03b51 \u03c8\n\u03c30 s\u0303n\n\u03c30\n\u0014\n\n\u0014\n\n\u0012\n\n\u0013\u0015\n\n+ oIP (1)\n\n\u0015\n\n(23)\n\n!\n\n\fFor t = 1, * * * , [n\u03c0], making\nthe Taylor's\nexpansion of \u03c7 up to second order, we\n\u0010\n\u0011\n\u22121\n\u22121 P[n\u03c0]\nobtain that n\nt=1 \u03c7 \u03c30 rt (\u03b2\u03031n ) can be written as:\n\uf8f1\n\uf8f2[n\u03c0]\nX\n\n\uf8fc\n\n\u0012 \u0013\n\u0013\n[n\u03c0]\n[n\u03c0]\n\uf8fd\n1 X \u2032 \u03b5t\n1 X \u2032\u2032 \u03b5t \u2212 \u03b4t Xt (\u03b2\u03031n \u2212 \u03b210 )\n\u03b5t\n0\n\u22121\n\u03c7\n\u2212\n\u03c7\nXt (\u03b2\u03031n \u2212 \u03b21 ) \u2212 2\n[Xt (\u03b2\u03031n \u2212 \u03b210 )]2 \uf8fe\nn \uf8f3 \u03c7\n\u03c30\n\u03c30 t=1\n\u03c30\n2\u03c30 t=1\n\u03c30\nt=1\n(24)\n!\n\n\u0012\n\nLet us analyse the three terms of the previous equation separately.\n\u2022 For the first term, let us \u03bdt = \u03b5t /\u03c30 \u223c N (0, 1) denote. We use the Hermite\nP[n\u03c0]\nexpansion for t=1 \u03c7(\u03bdt ). Because the Hermite rank of \u03c7(\u03bdt ) is q1 , q1 \u2265 2, by (13)\nbelow:\n[n\u03c0]\n\nX\n\n\u03c7(\u03bdt ) =\n\nt=1\n\n[n\u03c0]\nX X Jq (\u03c7)\nX\nJq1 (\u03c7) [n\u03c0]\nHq1 (\u03bdt ) +\nHq (\u03bdt ) :\u2261 T1,n + T2,n\nq1 ! t=1\nq!\nt=1 q\u2265q1 +1\n\n(25)\n\nFor T1,n we have:\n\uf8ee\n\n\uf8f9\n\n[n\u03c0]\u22121\nX\nX [n\u03c0]\nX\nJ 2 (\u03c7) [n\u03c0]\nJ 2 (\u03c1)\n2\n([n\u03c0] \u2212 t)\u03b3 q1 (t)\uf8fb\nIE[T1,n\n] = q1 2\n(q1 !)\u03b3 q1 (|t\u2212j|) = (q1 !) 1 2 \uf8f0[n\u03c0]\u03b3 q1 (0) + 2\n(q1 !) t=1 j=1\n(q1 !)\nt=1\n\n\uf8ee\n\n\uf8f9\n\n[n\u03c0]\u22121\ni\nX\nJ 2 (\u03c1) h\nJ12 (\u03c1) \uf8f0\n2\u2212\u03b1q1\nq1\n\u2212\u03b1q1 q1\n\uf8fb= 1\nO(n)\n+\nO(n\n)L\n([n\u03c0])\n([n\u03c0]\n\u2212\nt)t\nL\n(t)\nO(n)\n+\n2\n= (q1 !)\n(q1 !)2\nq1 !\nt=1\n\n= O(n2\u2212\u03b1q1 )Lq1 ([n\u03c0])\n\nFor T2,n we have:\n\uf8f9\n\n\uf8ee\n\nX Jq2 (\u03c1) [n\u03c0]\nX\nX\nX [n\u03c0]\nX [n\u03c0]\nJq2 (\u03c1) [n\u03c0]\n2\nq\n\uf8fb\n\uf8f0\nIE[T2,n ] =\n\u03b3 (|t \u2212 j|) \u2264\n\u03b3 q1 +1 (|t \u2212 j|)\nq!\nq!\nt=1 j=1\nt=1 j=1\nq\u2265q1 +1\nq\u2265q1 +1\nX\n\nX Jq2 (\u03c1) [n\u03c0]\u22121\nX\nX\nJq2 (\u03c1) [n\u03c0]\u22121\nq1 +1\n([n\u03c0]\u2212t)\u03b3\n(t) \u2264 O(n)+2\n([n\u03c0]\u2212t)t\u2212(q1 +1)\u03b1 Lq1 +1 (t)\n= O(n)+2\nq!\nq!\nt=1\nt=1\nq\u2265q1 +1\nq\u2265q1 +1\nX\n\n= O(n2\u2212(q1 +1)\u03b1 Lq1 +1 ([n\u03c0]))\n2\n2\nHence IE[T2,n\n] = o(IE[T1,n\n]). Then, for equation (25), we straightforwardly have:\n[n\u03c0]\n\nX\nt=1\n\n\u0010\n\n\u00111/2\n\n2\n\u03c7(\u03bdt ) = OIP IE[T1,n\n]\n\n= OIP (n1\u2212\u03b1q1 /2 )Lq1 /2 ([n\u03c0])\n\n(26)\n\n\u2022 For the second term of (24), since \u03bdt and Xt are independent, by ergodic theorem,\n15\n\n\fwe have:\n\n[n\u03c0]\n\u22121\n\nn\n\nX\n\n\u03c7\u2032 (\u03bdt ) Xt (\u03b2\u03031n \u2212 \u03b210 ) = oIP (k\u03b2\u03031n \u2212 \u03b210 k)\n\n(27)\n\nt=1\n\n\u2022 For the third term of (24), since \u03c8 \u2032 is bounded and n\u22121\nhave:\n\nP[n\u03c0]\nt=1\n\nXt XtT = OIP (1), we\n\n[n\u03c0]\n\nn\u22121\n\nX\n\n\u03c7\u2032\u2032 (\u03bdt ) [Xt (\u03b2\u03031n \u2212 \u03b210 )]2 = OIP (k\u03b2\u03031n \u2212 \u03b210 k2 ) = oIP (k\u03b2\u03031n \u2212 \u03b210 k)\n\n(28)\n\nt=1\n\nThen, by taking (26), (27), (28) into account, the behaviour of (24) is given by (26)\nand it is OIP (n\u2212\u03b1q1 /2 )Lq1 /2 ([n\u03c0]) + oIP (k\u03b2\u03031n \u2212 \u03b210 k). Similar one reasoning\nis made\n\u0010\n\u0011\n\u22121\n\u22121 Pn\nfor the part t = [n\u03c0] + 1, * * * , n and we obtain that: n\n\u03c7\n\u03c3\nr\n(\n\u03b2\u0303\n)\n=\nt 2n\n0\nt=[n\u03c0]+1\nOIP (n\u2212\u03b1q1 /2 )Lq1 /2 (n(1 \u2212 [\u03c0])) + oIP (k\u03b2\u03032n \u2212 \u03b220 k). Then, for relation (23), we have:\n\u03b51\n\u03c30 \u2212 s\u0303n\nIE \u03b51 \u03c8\n\u03c30 s\u0303n\n\u03c30\n\u0014\n\n\u0014\n\n\u0012\n\n\u0013\u0015\n\n\u0015\n\n+ oIP (1) = OIP (n\u2212\u03b1q1 /2 )Lq1 /2 (n)+oIP (k\u03b2\u03031n \u2212\u03b210 k+k\u03b2\u03032n \u2212\u03b220 k)\n\nand the convergence rate of s\u0303n follows.\n\n\u0004\n\nProof of Theorem 3.4. As a consequence of Theorem 3.1, we consider \u03c0 in a\nneighbourhood of \u03c0 0 . We suppose, without loss of generality, that \u03c0 < \u03c0 0 . Considering relation (12)(c), we have:\n[n\u03c0 0 ]\n\n\uf8eb\n\nX\n\n\u03c8\uf8ed\n\n[n\u03c0 0 ]\n\n\uf8eb\n\nn\u22121\n\nrt (\u03b2\u03032n (\u03c0))\n\u0010\n\n\u0011 \uf8f8 Xt\n\n= \u2212n\u22121\n\nn\nX\n\n\uf8eb\n\n\u03c8\uf8ed\n\nrt (\u03b2\u03032n (\u03c0))\n\u0010\n\nsn \u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0\n(29)\nSince k\u03b2\u03032n (\u03c0) \u2212 \u03b220 k = OIP (n\u2212k L\u03031 (n)), an argument like the one used for relation\n(27) yield that the right-hand side of (29) is OIP (n\u2212k L\u03031 (n)).\nX (\u03b2\u03032n (\u03c0)\u2212\u03b210 )\n. For\nWe apply (8) to function \u03c8, for: xt = s \u03b2\u0303 (\u03c0),\u03b5t\u03b2\u0303 (\u03c0),\u03c0 , ht = \u2212 s \u03b2\u0303t (\u03c0),\n\u03b2\u03032n (\u03c0),\u03c0 )\n)\u0011\nn ( 1n\n2n\n\u0010 n ( 1n\nthe left-hand side of (29), since sn \u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0 \u2192 \u03c30 a.s. for n \u2192 \u221e, and\n\u03b210 6= \u03b220 , we obtain :\nt=[n\u03c0]+1\n\n\u22121\n\nn\n\nX\n\nt=[n\u03c0]+1\n\n\u03c8\uf8ed\n\nsn \u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0\n\n\uf8f6\n\nrt (\u03b2\u03032n (\u03c0))\n\u0010\n\nsn \u03b2\u03031n (\u03c0), \u03b2\u03032n (\u03c0), \u03c0\n\nt=[n\u03c0 0 ]+1\n\n[n\u03c0 0 ]\n\n\uf8f6\n\n\u0011 \uf8f8 Xt\n\n\u22121\n\n=n\n\n\u03b5t\nXt +OIP (n(\u03c0 0 \u2212\u03c0))\n\u03c8\n\u03c3\n0\nt=[n\u03c0]+1\nX\n\n\u0012\n\n\u0013\n\n(30)\n\nBut, making Hermite expansion of \u03c8(\u03bdt ), we get:\n[n\u03c0 0 ]\n\n[n\u03c0 0 ]\n\n[n\u03c0 0 ]\n\nX X Jq (\u03c8)\nJ1 (\u03c8) X\n\u03b5t\nXt =\nHq (\u03bdt )Xt :\u2261 I1,n + I2,n\n\u03b5 t Xt +\n\u03c8\n\u03c30\n\u03c30 t=[n\u03c0]+1\nq!\nt=[n\u03c0]+1 q>1\nt=[n\u03c0]+1\nX\n\n\u0012\n\n\u0013\n\n16\n\n\uf8f6\n\n\u0011 \uf8f8 Xt\n\n\fwhere: Jq (\u03c8) = IE[\u03c8(\u03bd1 )Hq (\u03bd1 )]. On the other hand, as in the proof of Theorem\n3.3, we have I2,n = oIP (I1,n ). The variance of I1,n is:\nT\nIE[I1,n I1,n\n]\n\nJ 2 (\u03c8)\n= 1 2\n\u03c30\n\n[n(\u03c0 0 \u2212\u03c0)] [n(\u03c0 0 \u2212\u03c0)]\n\nX\n\nX\n\n\u03b3(|i \u2212 j|)\u0393(|i \u2212 j|)\n\nj=1\n\ni=1\n\n[n(\u03c0 0 \u2212\u03c0)]\n0\n\n= [n(\u03c0 \u2212 \u03c0)]\u03b3(0)\u0393(0) + 2\n\nX\n\n[n(\u03c0 0 \u2212 \u03c0) \u2212 i]\u03b3(i)\u0393(i)\n\ni=1\n\n\u0010\n\n= O L(n(\u03c0 0 \u2212 \u03c0))LT (n(\u03c0 0 \u2212 \u03c0))M(n(\u03c0 0 \u2212 \u03c0))L(n(\u03c0 0 \u2212 \u03c0))\nWhat implies:\n[n\u03c0 0 ]\n\n\u22121\n\nn\n\n\u0011\n\n\u0010\n\u0011\n\u03b5t\n\u03c8\nXt = OIP (n(\u03c0 0 \u2212 \u03c0))\u2212 min(\u03b8i +\u03b1)/2 L1/2 (n(\u03c0 0 \u2212 \u03c0))LT (n(\u03c0 0 \u2212 \u03c0))L(n(\u03c0 0 \u2212 \u03c0))\n\u03c30\nt=[n\u03c0]+1\nX\n\n\u0012\n\n\u0013\n\nThis last relation together with (29), (30) and since the right-hand side of (29) is\nOIP (n\u2212k L\u03031 (n)) imply: OIP (n\u2212k L\u03031 (n))\n\u0010\n\n= OIP (n(\u03c0 0 \u2212\u03c0))+OIP (n(\u03c0 0 \u2212 \u03c0))\u2212 min(\u03b8i +\u03b1)/2 L1/2 (n(\u03c0 0 \u2212 \u03c0))LT (n(\u03c0 0 \u2212 \u03c0))L(n(\u03c0 0 \u2212 \u03c0))\nWe obtain that: \u03c0\u0302n \u2212 \u03c0 0 = OIP (n\u22121\u2212k L\u03031 (n)).\n\n5\n\n\u0004\n\nLemmas\n\nLemma 5.1 If solution sn (\u03be) of equation (4) exists, then it is well-defined, bounded,\nstrictly positive, with a probability arbitrarily large.\nProof of Lemma 5.1. Since IE[rt (\u03b2) = 0] and V ar[rt (\u03b2)] = V ar[\u03b5t ]+\u03b2V ar[X]\u03b2 t <\n\u221e, by Bienaym\u00e9-Tchebichev inequality, we obtain that rt (\u03b2) is bounded with a\nprobability arbitrarily large.\nWe prove that sn (\u03be) is bounded by reduction to absurdity. If sn (\u03be) is not bounded\nthen: there exists \u03be \u2208 \u03a5 \u00d7 \u03a5 \u00d7 (0, 1) and n\u03be \u2208 N such that for all n > n\u03be , M > 0,\nexists \u01eb > 0 such that: IP [sn (\u03be) > M] \u2265 1 \u2212 \u01eb. Since \u03c1 is continuous and \u03c1(0) = 0,\nthen:\n!\nrt (\u03b2)\nIP\n\u03c1\n\u2212\u2192 0,\nt = 1, ..., n\n(31)\nsn (\u03be) n\u2192\u221e\n17\n\n\u0011\n\n\fand\n\n[n\u03c0]\n\nn\nrt (\u03b21 )\n1 X\nrt (\u03b22 )\n1X\n\u03c1\n+\n\u03c1\nn t=1\nsn (\u03be)\nn t=[n\u03c0]+1\nsn (\u03be)\n\n!\n\n!\n\n!\n\n!\n\n[n\u03c0]\nn \u2212 [n\u03c0]\nrt (\u03b21 )\nrt (\u03b22 )\n\u2264\n+\nmax \u03c1\nmax \u03c1\n[n\u03c0]+1\u2264t\u2264n\nn 1\u2264t\u2264[n\u03c0]\nsn (\u03be)\nn\nsn (\u03be)\nwhich, by (31), converges to 0 in probability, for n \u2192 \u221e. What is contradictory\nwith (4). To prove that sn (\u03be) > 0, let us consider function g(\u03b2, s) = (\u03b5 \u2212 X\u03b2)/s,\nwith \u03b2 in a compact of Rd containing 0 and s \u2208 (0, \u221e). Since \u03b5 \u2212 X\u03b2 is bounded\nwith a probability close to 1, if sn (\u03be) = 0, thus lims\u21920 |g(\u03b2, s)| = \u221e, what\nis contradictory with (4). Hence, for all \u01eb > 0, there exists \u03b4 > 0 such that\nIP [inf \u03be\u2208\u03a5\u00d7\u03a5\u00d7[0,1] sn (\u03be) > \u03b4] > 1 \u2212 \u01eb.\n\u0004\nLemma 5.2 Under assumptions (A1)-(A3), for any \u01eb \u2208 (0, 1), \u03be \u2208 \u03a5 \u00d7 \u03a5 \u00d7 [0, 1],\nthere exists a positive constant \u03b4 such that: IP [\ninf\nDn (\u03be) > \u03b4] > 1 \u2212 \u01eb.\n\u03be\u2208\u03a5\u00d7\u03a5\u00d7[0,1]\n\nProof of Lemma 5.2. Because \u03be belongs to a compact and taking into account\nrelation (11), we have to prove that for all \u01eb > 0, \u03be \u2208 \u03a5 \u00d7 \u03a5 \u00d7 [0, 1], there exists a\n\u03b4 > 0 such that:\n\uf8ee\n\n\uf8ee\n\n[n\u03c0]\n\n!\uf8f9\n\n\uf8f9\n\nn\nX\nrt (\u03b21 )\nrt (\u03b22 ) \uf8fb\nIP \uf8f0n\u22121 \uf8f0 rt (\u03b21 )\u03c8\n+\nrt (\u03b22 )\u03c8\n> \u03b4\uf8fb > 1 \u2212 \u01eb\ns\n(\u03be)\ns\n(\u03be)\nn\nn\nt=1\nt=[n\u03c0]+1\n\n!\n\nX\n\u0010\n\n\u0011\n\nSince rt (\u03b2), \u03c8 srnt (\u03b2)\nhave the same sign and since \u03c8 is continuous, we are going\n(\u03be)\nto show only that, for all \u01eb > 0, for all \u03b2 in compact set \u03a5, there exists a \u03b41 > 0\nsuch that: IP [|\u03b5 \u2212 X\u03b2| > \u03b41 ] > 1 \u2212 \u01eb.\nRandom variables \u03b5 and X are Gaussian\nand independent.\nThen: IP [|\u03b5 \u2212 X\u03b2| >\n\u0010\n\u0011\n\u03b41\n\u03b41 ] = 2IP [\u03b5 \u2212 X\u03b2 < \u2212\u03b41 ] = 2\u03a6 \u2212 [\u03b3(0)+\u03b2\u0393(0)\u03b2 T ]1/2 . We recall that \u03a6 denotes\nthe standard Gaussian distribution.\n\u0010\n\u0011 Then, the Lemma results by setting: \u03b41 =\nT 1/2\n\u22121 1\u2212\u01eb\ninf \u03b2\u2208\u03a5 [\u03b3(0) + \u03b2\u0393(0)\u03b2 ]\n\u03a6\n.\n\u0004\n2\nThe key for strong convergence proof is the following uniform convergence result.\nLemma 5.3 For all \u033a > 0, under assumptions (A1)-(A3), for\n\u03a9\u033a (\u03be) = {\u03be \u2217 \u2208 \u03a5 \u00d7 \u03a5 \u00d7 [0, 1]; k\u03b7 \u2212 \u03b7 \u2217 k < \u033a, |\u03c0 \u2212 \u03c0 \u2217 | < \u033a}, we have:\nIE\n\n\"\n\n#\n\nsup |sn (\u03b7, \u03c0) \u2212 sn (\u03b7 \u2217 , \u03c0 \u2217 )| \u2212\u21920\n\n\u03be \u2217 \u2208\u03a9\u033a (\u03be)\n\n18\n\n\u033a\u21920\n\n\fProof of Lemma 5.3. We have the triangular inequality:\n|sn (\u03b7, \u03c0) \u2212 sn (\u03b7 \u2217 , \u03c0 \u2217 )| \u2264 |sn (\u03b7, \u03c0) \u2212 sn (\u03b7, \u03c0 \u2217 )| + |sn (\u03b7, \u03c0 \u2217 ) \u2212 sn (\u03b7 \u2217 , \u03c0 \u2217 )|. First, we\nwill study sn (\u03b7, \u03c0 \u2217 ) \u2212 sn (\u03b7 \u2217 , \u03c0 \u2217 ). By the mean value theorem (TVM), we have:\nsn (\u03b7, \u03c0 \u2217 ) \u2212 sn (\u03b7 \u2217 , \u03c0 \u2217 ) = (\u03b21 \u2212 \u03b21\u2217 )\n\n\u2202sn \u2217\n\u2202sn\n(\u03b2\u03031 , \u03b22\u2217, \u03c0 \u2217 ) + (\u03b22 \u2212 \u03b22\u2217 )\n(\u03b2 , \u03b2\u03032 , \u03c0 \u2217 ) (32)\n\u2202\u03b21\n\u2202\u03b22 1\n\nwhere \u03b2\u03031 = \u03b21 + \u03c51 (\u03b21 \u2212 \u03b21\u2217 ), \u03b2\u03032 = \u03b22 + \u03c52 (\u03b22 \u2212 \u03b22\u2217 ), \u03c51 , \u03c52 \u2208 (0, 1). By Lemma\n5.2, applying Cauchy-Schwarz inequality in (10) and taking into account that \u03c8 is\nbounded, we obtain:\nIE\n\n\"\n\n\u2202sn (\u03b2\u03031 , \u03b22\u2217, \u03c0 \u2217 )\n\u2202\u03b21\n\n[n\u03c0]\n\n#\n\n\u22121\n\n\u2264 Cn\n\nX\n\nIE[Xt2 ]IE\n\nt=1\n\n\"\n\n\u03c8\n\n2\n\nrt (\u03b2\u03031 )\nsn (\u03b2\u03031 , \u03b22\u2217, \u03c0 \u2217 )\n\n!#!1/2\n\n< C (33)\n\nThen, writing a similar relation for (\u2202sn /\u2202\u03b22 )(\u03b21\u2217 , \u03b2\u03032 , \u03c0 \u2217 ), we have for (32):\nIE [|sn (\u03b7, \u03c0 \u2217 ) \u2212 sn (\u03b7 \u2217 , \u03c0 \u2217 )|] \u2212\u2192 0,\n\nfor \u033a \u2192 0\n\n(34)\n\nLet us remark that if \u03c0 \u2217 = 0 or \u03c0 \u2217 = 1, then in relation (32), the term in \u03b21 ,\nrespectively \u03b22 , does not appear.\nNow, we study |sn (\u03b7, \u03c0) \u2212 sn (\u03b7, \u03c0 \u2217 )|, supposing that \u03c0 < \u03c0 \u2217 . Since sn (\u03b7, \u03c0) and\nsn (\u03b7, \u03c0 \u2217 ) are both solutions of (4), we have:\n\u22121\n\nn\n\n[n\u03c0 \u2217 ] \"\n\n!\n\nrt (\u03b21 )\nrt (\u03b21 )\n\u03c1\n\u2212\u03c1\nsn (\u03b7, \u03c0)\nsn (\u03b7, \u03c0 \u2217 )\n\nX\nt=1\n\n[n\u03c0 \u2217 ]\n\u22121\n\n=n\n\nX\n\n[n\u03c0]+1\n\n!#\n\n\u22121\n\n+n\n\nn\nX\n\nt=[n\u03c0 \u2217 ]+1\n\n\"\n\n!\n\n\"\n\nrt (\u03b22 )\nrt (\u03b22 )\n\u2212\u03c1\n\u03c1\nsn (\u03b7, \u03c0)\nsn (\u03b7, \u03c0 \u2217 )\n\n!\n\nrt (\u03b21 )\nrt (\u03b22 )\n\u03c1\n\u2212\u03c1\nsn (\u03b7, \u03c0)\nsn (\u03b7, \u03c0)\n\n!#\n\n!#\n\nThus, applying the MVT:\nn\u22121 [sn (\u03b7, \u03c0 \u2217 ) \u2212 sn (\u03b7, \u03c0)]\n\u2217\n\u22121 P[n\u03c0 ]\n\n=n\n\n\u0014\n\nP[n\u03c0\u2217 ]\nt=1\n\n[n\u03c0]+1 [Xt (\u03b21 \u2212 \u03b22 )]\u03c8\n\n\u0010\n\nrt (\u03b21 )\u03c8\n\nr\u0303t (\u03b21 ,\u03b22 )\nsn (\u03b7,\u03c0)\n\n\u0011\n\n\u0012\n\nrt (\u03b21 )\n(1)\nun (\u03b7,\u03c0,\u03c0 \u2217 )\n\n\u0013\n\n+\n\nPn\n\nt=[n\u03c0 \u2217 ]+1\n\nrt (\u03b22 )\u03c8\n\n\u0012\n\nrt (\u03b22 )\n(2)\nun (\u03b7,\u03c0,\u03c0 \u2217 )\n\n(35)\nwhere u , u are two positive bounded functions, not necessarily solutions of (4)\nand r\u0303t (\u03b21 , \u03b22 ) = rt (\u03b21 ) + mt [rt (\u03b22 ) \u2212 rt (\u03b21 )], with 0 < mt < 1. By relation (11):\n(1)\n\n(2)\n\n[n\u03c0 \u2217 ]\n\nX\nt=1\n\nrt (\u03b21 )\u03c8\n\nrt (\u03b21 )\n(1)\n\nun (\u03b7, \u03c0, \u03c0 \u2217)\n\n!\n\n+\n\nn\nX\n\nt=[n\u03c0 \u2217 ]+1\n\n19\n\nrt (\u03b22 )\u03c8\n\nrt (\u03b22 )\n(2)\n\nun (\u03b7, \u03c0, \u03c0 \u2217)\n\n!\n\n>0\n\n(36)\n\n\u0013\u0015\n\n\fwith a probability close to 1. On the other hand: r\u0303t (\u03b21 , \u03b22 ) = Yt \u2212 Xt [\u03b21 + mt (\u03b22 \u2212\n\u03b21 )] = rt (\u03b21 + mt (\u03b22 \u2212 \u03b21 )). Using the same arguments as for (33), we obtain that:\n[n\u03c0 \u2217 ]\n\u22121\n\nn\n\nX\n\nt=[n\u03c0]+1\n\nIE[Xt2 ]IE\n\n\"\n\n\u03c8\n\n2\n\nr\u0303t (\u03b21 , \u03b22 )\nsn (\u03b7, \u03c0)\n\n!#!1/2\n\n\u2264 C1 (\u03c0 \u2217 \u2212 \u03c0)\n\nwhere C1 is a vector with all bounded components. Taking into account also (36),\nwe obtain for (35):\nIE[|sn (\u03b7, \u03c0) \u2212 sn (\u03b7, \u03c0 \u2217 )|] \u2264 Ck\u03b21 \u2212 \u03b22 k * |\u03c0 \u2212 \u03c0 \u2217 | < C\u033a\u2212\u21920\n\u033a\u21920\n\n(37)\n\nRelations (34) and (37) imply the Lemma.\n\n\u0004\n\nReferences\n[1] J. Bai, Least squares estimation of a shift in linear processes,\nSeries Analysis 15 (1994) 453-472.\n\nJournal of Time\n\n[2] J. Bai, Estimation of multiple-regime regressions with least absolute deviation,\nJournal of Statistical Planning Inference, 74 (1998) 103-134.\n[3] J. Bai, P. Perron, Estimating and testing linear models with multiple structural\nchanges, Econometrica 66 (1998) 47-78.\n[4] R. Baillie, Long memory processes and fractional integration in econometrics,\nJournal of Econometrics, 73 (1996) 5-59.\n[5] A.E. Beaton, J.W. Tukey, The fitting of power series, meaning polynomials,\nillustrated on band-spectroscopic data, Technometrics 16 (1974) 147-185.\n[6] J. Beran, Statistics for long-memory process, Chapman & Hall, New York, 1994.\n[7] P.K.Bhattacharya, Some aspects of change-point analysis. IMS Lecture NotesMonograph Series, Vol. 23, Hayward, CA 1994, pp.28-56.\n[8] Y.X. Cheung, Long memory in foreign exchange rates, Journal of Bussiness and\nEconomic Statistics 11(1993) 93-101.\n[9] G. Ciuperca, N. Dapzol, Maximum likelihood estimator in a multi-phase random\nregression model, Statistics 42 (2008) 363-381.\n[10] G. Ciuperca Estimating nonlinear regression with and without change-points by the\nLAD-method, Annals of the Institute of Statistical Mathematics in revision.\n\n20\n\n\f[11] L. Davies, The Asymptotics of S-Estimators in the Linear Regression Model, Annals\nof Statistics 18 (1990) 1651-1675.\n[12] Z. Ding, C.W.J. Granger, R.F. Engle, A long memory property of stock market\nreturns and a new model, Journal of Empirical Finance 1 (1993) 83-106.\n[13] P.I. Feder, On asymptotic distribution theory in segmented regression problemsidentified case, Ann. Statist. 3 (1975) 49-83.\n[14] P.I. Feder, The log likelihood ratio in segmented regression, Ann. Statist. 3 (1975)\n84-97.\n[15] I. Fiteni, Robust estimation of structural break points,\n(2002) 349-386.\n\nEconometric Theory 18\n\n[16] I. Fiteni, \u03c4 -estimators of regression models with structural change of unknown\nlocation, Journal of Econometrics 119 (2004) 19-44.\n[17] H. Guo, H.L. Koul, Nonparametric regression with heteroscedastic long memory\nerrors, Journal of Statistical Planning Inference 137 (2007) 379-404.\n[18] F.R.. Hampel, A general quantitative definition of robustness,\nMathematical Statistics 42 (1971) 1887-1896.\n\nAnnals of\n\n[19] J. Hidalgo, P.M. Robinson, Testing for structural change in a long-memory\nenvironment, Journal of Econometrics 1 (1996) 159-174.\n[20] L. Horvath, P. Kokoszka, The effect of long-range dependence on change-point\nestimators, Journal of Statistical Planning Inference 64 (1997) 57-81.\n[21] P.J. Huber, The behaviour of maximum likelihood estimates under nonstandard\nconditions. Proceedings of the Fifth Berkeley Symposium on Mathematics Statistic\nand Probability, Vol 1, University California Press, Berkeley, 1967, pp 221-234.\n[22] J. Kim , H.J Kim, Asymptotic results in segmented multiple regression, Journal of\nMultivariate Analysis 99 (2008) 2016-2038.\n[23] H.L. Koul, L. Qian, Asymptotics of maximum likelihood estimator in a two-phase\nlinear regression model, Journal of Statistical Planning and Inference 108 (2002)\n99-119.\n[24] H.L. Koul, R.T. Baillie, Asymptotics of M-estimators in non-linear regression with\nlong memory design, Statistics and Probability Letters 61 (2003) 237-252.\n[25] H.L. Koul, L. Qian, D. Surgailis, Asymptotics of M-estimators in two-phase linear\nregression models, Stochastic Processes and their Applications 103 (2003) 123-154.\n[26] C.M. Kuan, C.C. Hsu, Change-point estimation of fractionally integrated processes,\nJournal of Time Series Analysis 19 (1998) 693-708.\n\n21\n\n\f[27] S. Lazarov\u00e1, Testing for structural change in regression with long memory processes,\nJournal of Econometrics 129 (2005) 329-372.\n[28] A.W. Lo, Long term memory in stock market prices,\n1279-1313.\n\nEconometrica 59 (1991)\n\n[29] L.C. Nunes, C.M. Kuan, P. Newbold, Spurious break Econometric Theory 11(1995)\n736-749.\n[30] W. Palma, Long-Memory Time Series, Theory and Methods, Wiley, New Jersey,\n2007.\n[31] P.M. Robinson, Time series with strong dependence, In: C.A. Sims, Advances in\nEconometrics, Sixth World Congress, Cambridge Univ. Press, 1994.\n[32] E. Roelant, S. Van Aelst, C. Croux, Multivariate generalized S-estimators Journal\nof Multivariate Analysis (2008) in press.\n[33] P.J. Rousseeuw, V.J. Yohai, Robust regression by means of S-estimators, In: Robust\nand Nonlinear Time Series Analysis, Franke, J., Hrdle, W., Martin, R.D. (Eds.),\nLecture Notes in Statistics, Vol. 26, Springer, New York, 1984, pp. 256-272.\n[34] A.L. Rukhin, I. Vajda, Change-point estimation as a nonlinear regression problem,\nStatistics 30 (1997) 181-200.\n[35] Ph. Sibbertsen, Long memory versus structural breaks: an overview,\nPaper 4 (2004) 465-515.\n\nStatistical\n\n[36] L. Zhengyan,L. Degui, C. Jia, Asymptotic behavior for S-estimators in random\ndesign linear model with long-range-dependent errors, Metrika 66 (2007) 289-303.\n\n22\n\n\f"}