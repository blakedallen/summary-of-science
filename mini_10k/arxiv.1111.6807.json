{"id": "http://arxiv.org/abs/1111.6807v1", "guidislink": true, "updated": "2011-11-29T13:27:01Z", "updated_parsed": [2011, 11, 29, 13, 27, 1, 1, 333, 0], "published": "2011-11-29T13:27:01Z", "published_parsed": [2011, 11, 29, 13, 27, 1, 1, 333, 0], "title": "On the problem of reversibility of the entropy power inequality", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1111.2717%2C1111.2185%2C1111.0666%2C1111.5156%2C1111.5927%2C1111.1767%2C1111.1879%2C1111.1773%2C1111.6388%2C1111.4397%2C1111.3549%2C1111.6029%2C1111.4077%2C1111.4708%2C1111.4770%2C1111.4275%2C1111.4182%2C1111.4825%2C1111.0849%2C1111.1305%2C1111.0707%2C1111.3523%2C1111.2529%2C1111.0624%2C1111.0347%2C1111.7044%2C1111.1916%2C1111.1050%2C1111.5385%2C1111.5667%2C1111.0508%2C1111.1614%2C1111.2840%2C1111.5313%2C1111.0918%2C1111.1373%2C1111.2849%2C1111.3879%2C1111.3895%2C1111.4804%2C1111.2604%2C1111.2404%2C1111.3955%2C1111.4029%2C1111.3133%2C1111.7069%2C1111.6153%2C1111.1684%2C1111.6451%2C1111.1810%2C1111.6929%2C1111.3099%2C1111.4314%2C1111.5440%2C1111.0606%2C1111.6210%2C1111.3902%2C1111.2813%2C1111.3006%2C1111.6140%2C1111.1736%2C1111.5033%2C1111.5144%2C1111.6057%2C1111.4882%2C1111.1646%2C1111.1465%2C1111.2121%2C1111.3025%2C1111.5061%2C1111.4891%2C1111.1531%2C1111.6360%2C1111.0761%2C1111.3971%2C1111.5674%2C1111.7224%2C1111.2541%2C1111.4296%2C1111.5721%2C1111.4007%2C1111.6763%2C1111.3051%2C1111.0273%2C1111.4306%2C1111.6035%2C1111.3948%2C1111.5625%2C1111.3597%2C1111.1536%2C1111.5500%2C1111.6807%2C1111.4812%2C1111.6002%2C1111.5465%2C1111.4578%2C1111.6088%2C1111.6400%2C1111.7288%2C1111.6740%2C1111.1688&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On the problem of reversibility of the entropy power inequality"}, "summary": "As was shown recently by the authors, the entropy power inequality can be\nreversed for independent summands with sufficiently concave densities, when the\ndistributions of the summands are put in a special position. In this note it is\nproved that reversibility is impossible over the whole class of convex\nprobability distributions. Related phenomena for identically distributed\nsummands are also discussed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1111.2717%2C1111.2185%2C1111.0666%2C1111.5156%2C1111.5927%2C1111.1767%2C1111.1879%2C1111.1773%2C1111.6388%2C1111.4397%2C1111.3549%2C1111.6029%2C1111.4077%2C1111.4708%2C1111.4770%2C1111.4275%2C1111.4182%2C1111.4825%2C1111.0849%2C1111.1305%2C1111.0707%2C1111.3523%2C1111.2529%2C1111.0624%2C1111.0347%2C1111.7044%2C1111.1916%2C1111.1050%2C1111.5385%2C1111.5667%2C1111.0508%2C1111.1614%2C1111.2840%2C1111.5313%2C1111.0918%2C1111.1373%2C1111.2849%2C1111.3879%2C1111.3895%2C1111.4804%2C1111.2604%2C1111.2404%2C1111.3955%2C1111.4029%2C1111.3133%2C1111.7069%2C1111.6153%2C1111.1684%2C1111.6451%2C1111.1810%2C1111.6929%2C1111.3099%2C1111.4314%2C1111.5440%2C1111.0606%2C1111.6210%2C1111.3902%2C1111.2813%2C1111.3006%2C1111.6140%2C1111.1736%2C1111.5033%2C1111.5144%2C1111.6057%2C1111.4882%2C1111.1646%2C1111.1465%2C1111.2121%2C1111.3025%2C1111.5061%2C1111.4891%2C1111.1531%2C1111.6360%2C1111.0761%2C1111.3971%2C1111.5674%2C1111.7224%2C1111.2541%2C1111.4296%2C1111.5721%2C1111.4007%2C1111.6763%2C1111.3051%2C1111.0273%2C1111.4306%2C1111.6035%2C1111.3948%2C1111.5625%2C1111.3597%2C1111.1536%2C1111.5500%2C1111.6807%2C1111.4812%2C1111.6002%2C1111.5465%2C1111.4578%2C1111.6088%2C1111.6400%2C1111.7288%2C1111.6740%2C1111.1688&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "As was shown recently by the authors, the entropy power inequality can be\nreversed for independent summands with sufficiently concave densities, when the\ndistributions of the summands are put in a special position. In this note it is\nproved that reversibility is impossible over the whole class of convex\nprobability distributions. Related phenomena for identically distributed\nsummands are also discussed."}, "authors": ["Sergey G. Bobkov", "Mokshay M. Madiman"], "author_detail": {"name": "Mokshay M. Madiman"}, "author": "Mokshay M. Madiman", "arxiv_comment": "13 pages", "links": [{"href": "http://arxiv.org/abs/1111.6807v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1111.6807v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.FA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.FA", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1111.6807v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1111.6807v1", "journal_reference": "\"Limit Theorems in Probability, Statistics and Number Theory (in\n  honor of Friedrich G\\\"otze)\", P. Eichelsbacher et al. (ed.), Springer\n  Proceedings in Mathematics and Statistics 42, pp. 61-74, Springer-Verlag,\n  2013", "doi": null, "fulltext": "arXiv:1111.6807v1 [math.FA] 29 Nov 2011\n\nON THE PROBLEM OF REVERSIBILITY OF THE\nENTROPY POWER INEQUALITY\nSERGEY G. BOBKOV AND MOKSHAY M. MADIMAN\nDedicated to Friedrich G\u00f6tze on the occasion of his sixtieth birthday\nAbstract. As was shown recently by the authors, the entropy power\ninequality can be reversed for independent summands with sufficiently\nconcave densities, when the distributions of the summands are put in\na special position. In this note it is proved that reversibility is impossible over the whole class of convex probability distributions. Related\nphenomena for identically distributed summands are also discussed.\n\n1. The reversibility problem for the entropy power inequality\nGiven a random vector X in Rn with density f , introduce the entropy\nfunctional (or Shannon's entropy)\nZ\nf (x) log f (x) dx,\nh(X) = \u2212\nRn\n\nand the entropy power\nH(X) = e2h(X)/n ,\nprovided that the integral exists in the Lebesgue sense. For example, if X\nis uniformly distributed in a convex body A \u2282 Rn , we have\nH(X) = |A|2/n ,\n\nh(X) = log |A|,\n\nwhere |A| stands for the n-dimensional volume of A.\nThe entropy power inequality due to Shannon and Stam indicates that\nH(X + Y ) \u2265 H(X) + H(Y ),\n\n(1.1)\n\nfor any two independent random vectors X and Y in Rn , for which the\nentropy is defined ([Sha, Sta], cf. also [CC, DCT, SV]). This is one of the\nfundamental results in Information Theory, and it is of large interest to see\nhow sharp (1.1) is.\n2010 Mathematics Subject Classification. 60F05.\nKey words and phrases. Entropy, Brunn-Minkowski inequality, convex measures, entropy power inequality.\nSergey G. Bobkov was supported in part by the NSF grant DMS-1106530.\nMokshay M. Madiman was supported in part by the NSF CAREER grant DMS1056996.\n1\n\n\f2\n\nSERGEY G. BOBKOV AND MOKSHAY M. MADIMAN\n\nThe equality here is only achieved, when X and Y have normal distributions with proportional covariance matrices. Note that the right-hand\nside is unchanged when X and Y are replaced with affine volume-preserving\ntransformation, that is, with random vectors\ne = T1 (X),\nX\n\nYe = T2 (Y )\n\n(|detT1 | = |detT2 | = 1).\n\n(1.2)\n\ne + Ye ) essentially depends on the\nOn the other hand, the entropy power H(X\nchoice of T1 and T2 . Hence, it is reasonable to consider a formally improved\nvariant of (1.1),\ne + Ye ) \u2265 H(X) + H(Y ),\ninf H(X\n(1.3)\nT1 ,T2\n\nwhere the infimum is running over all affine maps T1 , T2 : Rn \u2192 Rn subject\nto (1.2). (Note that one of these maps may be taken to be the identity\noperator.) Now, equality in (1.3) is achieved, whenever X and Y have\nnormal distributions with arbitrary positive definite covariance matrices.\nA natural question arises: When are both the sides of (1.3) of a similar\norder? For example, within a given class of probability distributions (of X\nand Y ), one wonders whether or not it is possible to reverse (1.3) to get\ne + Ye ) \u2264 C(H(X) + H(Y ))\ninf H(X\n\nT1 ,T2\n\n(1.4)\n\nwith some constant C.\nThe question is highly non-trivial already for the class of uniform distributions on convex bodies, when it becomes to be equivalent (with a different\nconstant) to the inverse Brunn-Minkowski inequality\n\u0011\n\u0010\ne+B\ne 1/n \u2264 C |A|1/n + |B|1/n .\ninf A\n(1.5)\nT1 ,T2\n\ne+B\ne = {x + y : x \u2208 A,\ne y \u2208 B}\ne stands for the Minkowski sum of the\nHere A\ne = T1 (A), B\ne = T2 (B) of arbitrary convex bodies A and B in Rn .\nimages A\nTo recover such an equivalence, one takes for X and Y independent random\nvectors uniformly distributed in A and B. Although the distribution of\nX + Y is not uniform in A + B, there is a general entropy-volume relation\n1\n|A + B|2/n \u2264 H(X + Y ) \u2264 |A + B|2/n ,\n4\neB\ne and X,\ne Ye (cf. [BM3]).\nwhich may also be applied to the images A,\nThe inverse Brunn-Minkowski inequality (1.5) is indeed true and represents a deep result in Convex Geometry discovered by V. D. Milman in the\nmid 1980s (cf. [M1, M2, M3, Pis]). It has connections with high dimensional\nphenomena, and we refer an interested reader to [BKM, KT, KM, AMO].\nThe questions concerning possible description of the maps T1 and T2 and related isotropic properties of the normalized Gaussian measures are discussed\nin [Bob2].\n\n\fON THE PROBLEM OF REVERSIBILITY OF THE ENTROPY POWER INEQUALITY 3\n\nBased on (1.5), and involving Berwald's inequality in the form of C. Borell\n[Bor1], the inverse entropy power inequality (1.4) has been established recently [BM1, BM3] for the class of all probability distributions having logconcave densities. Involving additionally a general submodularity property\nof entropy [Mad], it turned out also possible to consider more general densities of the form\nf (x) = V (x)\u2212\u03b2 ,\nx \u2208 Rn ,\n(1.6)\nwhere V are positive convex functions on Rn and \u03b2 \u2265 n is a given parameter.\nMore precisely, the following statement can be found in [BM3].\nTheorem 1.1. Let X and Y be independent random vectors in Rn with\ndensities of the form (1.6) with \u03b2 \u2265 2n + 1, \u03b2 \u2265 \u03b20 n (\u03b20 > 2). There exist\nlinear volume preserving maps Ti : Rn \u2192 Rn such that\n\u0001\ne + Ye \u2264 C\u03b2 (H(X) + H(Y )),\nH X\n(1.7)\n0\ne = T1 (X), Ye = T2 (Y ), and where C\u03b2 is a constant, depending on\nwhere X\n0\n\u03b20 , only.\n\nThe question of what maps T1 and T2 can be used in Theorem 1.1 is\nrather interesting, but certainly the maps that put the distributions of X\nand Y in M -position suffice (see [BM3] for terminology and discussion). In\na more relaxed form, one needs to have in some sense \"similar\" positions for\nboth distributions. For example, when considering identically distributed\nrandom vectors, there is no need to appeal in Theorem 1.1 to some (not\nvery well understood) affine volume-preserving transformations, since the\ndistributions of X and Y have the same M -ellipsoid. In other words, we\nhave for X and Y drawn independently from the same distribution (under\nthe same assumption on form of density as Theorem 1.1) that\nH(X + Y ) \u2264 C\u03b20 (H(X) + H(Y )) = 2C\u03b20 H(X).\n\n(1.8)\n\nSince the distributions of X and \u2212Y also have the same M -ellipsoid, it is\nalso true that\nH(X \u2212 Y ) \u2264 C\u03b20 (H(X) + H(Y )) = 2C\u03b20 H(X).\n\n(1.9)\n\nWe strengthen this observation by providing a quantitative version with\nexplicit constants below (under, however, a convexity condition on the convolved measure). Moreover, one can give a short and relatively elementary\nproof of it without appealing to Theorem 1.1.\nTheorem 1.2. Let X and Y be independent identically distributed random\nvectors in Rn with finite entropy. Suppose that X \u2212 Y has a probability\ndensity function of the form (1.6) with \u03b2 \u2265 max{n + 1, \u03b20 n} for some fixed\n\u03b20 > 1. Then\nH(X \u2212 Y ) \u2264 D\u03b20 H(X)\n\n\f4\n\nSERGEY G. BOBKOV AND MOKSHAY M. MADIMAN\n\nand\nH(X + Y ) \u2264 D\u03b220 H(X),\n0\n).\nwhere D\u03b20 = exp( \u03b22\u03b2\n0 \u22121\n\nLet us return to Theorem 1.1 and the class of distributions involved there.\nFor growing \u03b2, the families (1.6) shrink and converge in the limit as \u03b2 \u2192\n+\u221e to the family of log-concave densities which correspond to the class\nof log-concave probability measures. Through inequalities of the BrunnMinkowski-type, the latter class was introduced by A. Pr\u00e9kopa, while the\ngeneral case \u03b2 \u2265 n was studied by C. Borell [Bor2, Bor3], cf. also [BL,\nBob1]. In [Bor2, Bor3] it was shown that probability measures \u03bc on Rn\nwith densities (1.6) (and only they, once \u03bc is absolutely continuous) satisfy\nthe geometric inequality\n\u0001 \u0002\n\u00031/\u03ba\n\u03bc tA + (1 \u2212 t)B \u2265 t\u03bc(A)\u03ba + (1 \u2212 t)\u03bc(B)\u03ba\n(1.10)\n\nfor all t \u2208 (0, 1) and for all Borel measurable sets A, B \u2282 Rn , with negative\npower\n1\n.\n\u03ba=\u2212\n\u03b2\u2212n\nSuch \u03bc's form the class of so-called \u03ba-concave measures. In this hierarchy\nthe limit case \u03b2 = n corresponds to \u03ba = \u2212\u221e and describes the largest class\nof measures on Rn , called convex, in which case (1.10) turns into\n\u03bc(tA + (1 \u2212 t)B) \u2265 min{\u03bc(A), \u03bc(B)}.\n\nThis inequality is often viewed as the weakest convexity hypothesis about a\ngiven measure \u03bc.\nOne may naturally wonder whether or not it is possible to relax the assumption on the range of \u03b2 in (1.7)-(1.9), or even to remove any convexity\nhypotheses. In this note we show that this is impossible already for the class\nof all one-dimensional convex probability distributions. Note that in dimene = X and\nsion one there are only two admissible linear transformations, X\ne = \u2212X, so that one just wants to estimate H(X + Y ) or H(X \u2212 Y ) from\nX\nabove in terms of H(X). As a result, the following statement demonstrates\nthat Theorem 1.1 and its particular cases (1.8)-(1.9) are false over the full\nclass of convex measures.\nTheorem 1.3. For any constant C, there is a convex probability distribution\n\u03bc on the real line with a finite entropy, such that\nmin{H(X + Y ), H(X \u2212 Y )} \u2265 C H(X),\nwhere X and Y are independent random variables, distributed according to\n\u03bc.\n\n\fON THE PROBLEM OF REVERSIBILITY OF THE ENTROPY POWER INEQUALITY 5\n\nA main reason for H(X +Y ) and H(X \u2212Y ) to be much larger than H(X)\nis that the distributions of the sum X + Y and the difference X \u2212 Y may\nlose convexity properties, when the distribution \u03bc of X is not \"sufficiently\nconvex\". For example, in terms of the convexity parameter \u03ba (instead of \u03b2),\nthe hypothesis of Theorem 1.1 is equivalent to\n\u03ba\u2265\u2212\n\n1\n(\u03b20 > 2),\n(\u03b20 \u2212 1)n\n\n\u03ba\u2265\u2212\n\n1\n.\nn+1\n\nThat is, for growing dimension n we require that \u03ba be sufficiently close\nto zero (or the distributions of X and Y should be close to the class of\nlog-concave measures). These conditions ensure that the convolution of \u03bc\nwith the uniform distribution on a proper (specific) ellipsoid remains to be\nconvex, and its convexity parameter can be controled in terms of \u03b20 (a fact\nused in the proof of Theorem 1.1). However, even if \u03ba is close to zero, one\ncannot guarantee that X + Y or X \u2212 Y would have convex distributions.\nWe prove Theorem 1.2 in Section 2 and Theorem 1.3 in Section 3, and then\nconclude in Section 4 with remarks on the relationship between Theorem 1.3\nand recent results about Cramer's characterization of the normal law.\n2. A \"difference measure\" inequality for convex measures\nGiven two convex bodies A and B in Rn , introduce A \u2212 B = {x \u2212 y : x \u2208\nA, y \u2208 B}. In particular, A \u2212 A is called the \"difference body\" of A. Note\nit is always symmetric about the origin.\nThe Rogers-Shephard inequality [RS] states that, for any convex body\nA \u2282 Rn ,\nn\n|A \u2212 A| \u2264 C2n\n|A|,\n(2.1)\nn!\nwhere Cnk = k!(n\u2212k)!\ndenote usual combinatorial coefficients. Observe that\nputting the Brunn-Minkowski inequality and (2.1) together immediately\nyields that\n1\n\n2\u2264\n\n|A \u2212 A| n\n|A|\n\n1\nn\n\n1\n\nn n\n\u2264 [C2n\n] < 4,\n\nwhich constrains severely the volume radius of the difference body of A\nrelative to that of A itself. In analogy to the Rogers-Shephard inequality,\nwe ask the following question for entropy of convex measures.\nQuestion. Let X and Y be independent random vectors in Rn , which are\nidentically distributed with density V \u2212\u03b2 , with V positive convex, and \u03b2 \u2265\nn + \u03b3. For what range of \u03b3 > 0 is it true that H(X \u2212 Y ) \u2264 C\u03b3 H(X), for\nsome constant C\u03b3 depending only on \u03b3?\nTheorems 1.2 and 1.3 partially answer this question. To prove the former,\nwe need the following lemma about convex measures, proved in [BM2].\n\n\f6\n\nSERGEY G. BOBKOV AND MOKSHAY M. MADIMAN\n\nLemma 2.1. Fix \u03b20 > 1. Assume a random vector X in Rn has a density\nf = V \u2212\u03b2 , where V is a positive convex function on the supporting set. If\n\u03b2 \u2265 n + 1 and \u03b2 \u2265 \u03b20 n, then\n\u22121\nlog kf k\u22121\n\u221e \u2264 h(X) \u2264 c\u03b20 n + log kf k\u221e ,\n\nwhere one can take for the constant c\u03b20 =\n\n(2.2)\n\n\u03b20\n\u03b20 \u22121 .\n\nIn other words, for sufficiently convex probability measures, the entropy\nmay be related to the L\u221e -norm kf k\u221e = supx f (x) of the density f (which\nis necessarily finite). Observe that the left inequality in (2.2) is general:\nIt trivially holds without any convexity assumption. On the other hand,\nthe right inequality is an asymptotic version of a result from [BM2] about\nextremal role of the multidimensional Pareto distributions.\nNow, let f denote the density of the random variable W = X \u2212 Y in\nTheorem 1.2. It is symmetric (even) and thus maximized at zero, by the\nconvexity hypothesis. Hence, by Lemma 2.1,\n\u22121\nh(W ) \u2264 log kf k\u22121\n+ c\u03b20 n.\n\u221e + c\u03b20 n = log f (0)\nR\n2\nBut, if p is the density of X, then f (0) = Rn p(x) dx, and hence\nZ\nZ\n\u22121\np(x)[\u2212 log p(x)] dx\np(x) * p(x) dx \u2264\nlog f (0) = \u2212 log\nRn\n\nRn\n\nby using Jensen's inequality. Combining the above two displays immediately\nyields the first part of Theorem 1.2.\nTo obtain the second part, we need an observation from [MK] that follows\nfrom the following lemma on the submodularity of the entropy of sums\nproved in [Mad].\nLemma 2.2. Given independent random vectors X, Y, Z in Rn with absolutely continuous distributions, we have\nh(X + Y + Z) + h(Z) \u2264 h(X + Z) + h(Y + Z),\nprovided that all entropies are well-defined and finite.\nTaking X, Y and \u2212Z to be identically distributed, and using the monotonicity of entropy (after adding an independent summand), we obtain\nh(X + Y ) + h(Z) \u2264 h(X + Y + Z) + h(Z) \u2264 h(X + Z) + h(Y + Z)\nand hence\nh(X + Y ) + h(X) \u2264 2h(X \u2212 Y ).\n(This is the relevant observation from [MK].) Combining this bound with\nthe first part of Theorem 1.2 immediately gives the second part.\nIt would be more natural to state Theorem 1.2 under a shape condition\non the distribution of X rather than on that of X \u2212 Y , but for this we need\n\n\fON THE PROBLEM OF REVERSIBILITY OF THE ENTROPY POWER INEQUALITY 7\n\nto have better understanding of the convexity parameter of the convolution\nof two \u03ba-concave measures when \u03ba < 0.\nObserve that in the log-concave case of Theorem 1.2 (which is the case of\n\u03b2 \u2192 \u221e, but can easily be directly derived in the same way without taking\na limit), one can impose only a condition on the distribution of X (rather\nthan that of X \u2212 Y ) since closedness under convolution is guaranteed by the\nPr\u00e9kopa-Leindler inequality.\nCorollary 2.3. Let X and Y be independent random vectors in Rn with\nlog-concave densities. Then\nh(X \u2212 Y ) \u2264 h(X) + n,\nh(X + Y ) \u2264 h(X) + 2n.\nIn particular, observe that putting the entropy power inequality (1.1) and\nCorollary 2.3 together immediately yields that\n2\u2264\n\nH(X \u2212 Y )\n\u2264 e2 ,\nH(X)\n\nwhich constrains severely the entropy power of the \"difference measure\" of\n\u03bc relative to that of \u03bc itself.\n3. Proof of Theorem 1.3\nGiven a (large) parameter b > 1, let a random variable Xb have a truncated Pareto distribution \u03bc, namely, with the density\n1\nf (x) =\n1\n(x).\nx log b {1<x<b}\nBy the construction, \u03bc is supported on a bounded interval (1, b) and is\nconvex.\nFirst we are going to test the inequality\nH(Xb + Yb ) \u2264 CH(Xb )\n\n(3.1)\n\nfor growing b, where Yb is an independent copy of Xb . Note that\nZ b\nf (x) log(x log b) dx\nh(Xb ) =\n1\n\n1\n= log log b +\nlog b\n2\n\nZ\n\n1\n\nb\n\nlog x\n1\ndx = log log b + log b,\nx\n2\n\nso H(Xb ) = b log b.\nNow, let us compute the convolution of f with itself. The sum Xb + Yb\ntakes values in the interval (2, 2b). Given 2 < x < 2b, we have\nZ \u03b2\nZ +\u221e\ndy\n1\n,\nf (x \u2212 y)f (y) dy =\ng(x) = (f \u2217 f )(x) =\n2\nlog b \u03b1 (x \u2212 y)y\n\u2212\u221e\n\n\f8\n\nSERGEY G. BOBKOV AND MOKSHAY M. MADIMAN\n\nwhere the limits of integration are determined to satisfy the constraints\n1 < y < b, 1 < x \u2212 y < b. So,\n\u03b1 = max(1, x \u2212 b),\nand using\n\n1\n(x\u2212y)y\n\n=\n\n( y1 +\n\n1\nx\n\n1\nx\u2212y ),\n\n\u03b2 = min(b, x \u2212 1),\n\nwe find that\n\n1\n1\ny\n\u03b2\nlog(y) \u2212 log(x \u2212 y) x=\u03b1 =\nlog\n2\n2\nx\u2212y\nx log b\nx log b\n\u0013\n\u0012\n\u03b1\n1\n\u03b2\n\u2212 log\n.\nlog\n2\nx\u2212\u03b2\nx\u2212\u03b1\nx log b\n\ng(x) =\n=\n\n\u03b2\nx=\u03b1\n\nNote that x \u2212 \u03b1 = x \u2212 max(1, x \u2212 b) = min(b, x \u2212 1) = \u03b2. Hence,\ng(x) =\n\n2\n2\n\u03b2\nmin(b, x \u2212 1)\n.\nlog =\nlog\n2\n2\n\u03b1\nmax(1, x \u2212 b)\nx log b\nx log b\n\nEquivalently,\n2\nlog(x \u2212 1), for 2 < x < b + 1,\nx log2 b\nb\n2\nlog\ng(x) =\n, for b + 1 < x < 2b.\n2\nx\u2212b\nx log b\n\ng(x) =\n\nNow, on the second interval b + 1 < x < 2b, we have\n2\n2\n2\n2 log b = x log b < (b + 1) log b < 1,\nx log b\n\ng(x) \u2264\n\nwhere the last bound holds for b \u2265 e, for example. Similarly, on the first\ninterval 2 < x < b + 1, using log(x \u2212 1) < log b, we get\ng(x) \u2264\n\n2\n1\n<\n\u2264 1.\nx log b\nlog b\n\nThus, as soon as b \u2265 e, we have g \u2264 1 on the support interval. From this,\nh(Xb + Yb ) =\n\nZ\n\n2b\n\ng(x) log(1/g(x)) dx \u2265\n\nZ\n\nb\n\ng(x) log(1/g(x)) dx.\n\n2\n\n2\n\nNext, using on the first interval the bound g(x) \u2264\nb \u2265 e2 , we get for such values of b that\nh(Xb + Yb ) \u2265\n\nZ\n\n2\n\nb\n\ng(x) log x dx =\n\n2\nlog2 b\n\nZ\n\n2\n\nb\n\n2\nx log b\n\n\u2264\n\n1\nx,\n\nvalid for\n\nlog(x \u2212 1) log x\ndx.\nx\n\n\fON THE PROBLEM OF REVERSIBILITY OF THE ENTROPY POWER INEQUALITY 9\n\nTo further simplify, we may write x \u2212 1 \u2265 x2 , which gives\nZ\n\nb\n2\n\nlog(x \u2212 1) log x\ndx \u2265\nx\n=\n>\n\nHence, h(Xb + Yb ) >\n\n2\n3\n\nZ\n\nZ b\nlog2 x\nlog x\ndx \u2212 log 2\ndx\nx\nx\n2\n2\n\u0001 log 2\n\u0001\n1\nlog3 b \u2212 log3 2 \u2212\nlog2 b \u2212 log2 2\n3\n2\n1\nlog\n2\nlog3 b \u2212\nlog2 b.\n3\n2\nb\n\nlog b \u2212 log 2, and so\n\nH(Xb + Yb ) >\n\n1 4/3\nb\n2\n\n(b \u2265 e2 ).\n\nIn particular,\nH(Xb + Yb )\nb1/3\n>\n\u2192 +\u221e,\nH(Xb )\n2 log b\n\nas b \u2192 +\u221e.\n\nHence, the inequality (3.1) may not hold for large b with any prescribed\nvalue of C.\nTo test the second bound\nH(Xb \u2212 Yb ) \u2264 CH(Xb ),\n\n(3.2)\n\none may use the previous construction. The random variable Xb \u2212 Yb can\ntake any value in the interval |x| < b\u2212 1, where it is described by the density\nZ \u03b2\nZ +\u221e\ndy\n1\n.\nf (x + y)f (y) dy =\nh(x) =\n2\n(x\n+\ny)y\nlog b \u03b1\n\u2212\u221e\nHere the limits of integration are determined to satisfy 1 < y < b and\n1 < x + y < b. So, assuming for simplicity that 0 < x < b \u2212 1, the limits are\n\u03b1 = 1,\nWriting\n\n1\n(x+y)y\n\nh(x) =\n\n=\n\n1\nx log2 b\n\n1\nx\n\n( y1 \u2212\n\n1\nx+y ),\n\n\u03b2 = b \u2212 x.\n\nwe find that\n\nlog(y) \u2212 log(x + y)\n\n\u03b2\nx=\u03b1\n\n=\n\n1\n(b \u2212 x)(x + 1)\n.\nlog\n2\nb\nx log b\n\nIt should also be clear that\n1\nh(0) =\nlog2 b\n\nZ\n\nb\n1\n\n1 \u2212 1b\ndy\n=\n.\ny2\nlog2 b\n\n< log(x + 1) < x, we obtain that h(x) <\nUsing log (b\u2212x)(x+1)\nb\nfor b \u2265\n\ne2 .\n\n1\nlog2 b\n\n\u2264 1,\n\n\f10\n\nSERGEY G. BOBKOV AND MOKSHAY M. MADIMAN\n\n< b, we also have that h(x) \u2264\nIn this range, since (b\u2212x)(x+1)\nb\nHence, in view of the symmetry of the distribution of Xb \u2212 Yb ,\nZ b\u22121\nh(x) log(1/h(x)) dx\nh(Xb \u2212 Yb ) = 2\n\u2265 2\n=\n\nZ\n\n1\nx log b\n\n\u2264\n\n1\nx.\n\n0\n\nb/2\n\nh(x) log x dx\n0\n\n2\nlog2 b\n\nZ\n\n2\n\nb/2\n\n(b \u2212 x)(x + 1)\nlog x\nlog\ndx.\nx\nb\n\nBut for 0 < x < b/2,\nlog\n\n(b \u2212 x)(x + 1)\nx+1\n> log\n> log x \u2212 log 2,\nb\n2\n\nso\n\nZ b/2\nlog2 x \u2212 log 2 log x\n2\ndx\nh(Xb \u2212 Yb ) >\nx\nlog2 b 2\n\u0013\n\u0012\n2\nlog 2\n1\n3\n3\n2\n2\n=\n(log\n(b/2)\n\u2212\nlog\n2)\n\u2212\n(log\n(b/2)\n\u2212\nlog\n2)\n2\nlog2 b 3\n\u0012\n\u0013\n1\n2\n1\n>\nlog3 (b/2) \u2212 log2 (b/2)\n2\n2\nlog b 3\n2\n\u223c\nlog b.\n3\nTherefore, like on the previous step, H(Xb \u2212 Yb ) is bounded from below by\na function, which is equivalent to b4/3 . Thus, for large b, the inequality (3.2)\nmay not hold either.\nTheorem 1.3 is proved.\n4. Remarks\n\nFor a random variable X having a density, consider the entropic distance\nfrom the distribution of X to normality\nD(X) = h(Z) \u2212 h(X),\nwhere Z is a normal random variable with parameters EZ = EX, Var(Z) =\nVar(X). This functional is well-defined for the class of all probability distributions on the line with finite second moment, and in general 0 \u2264 D(X) \u2264\n+\u221e.\nThe entropy power inequality implies that\n\u03c322\n\u03c312\nD(X)\n+\nD(X)\n\u03c312 + \u03c322\n\u03c312 + \u03c322\n\u2264 max(D(X), D(Y )),\n\nD(X + Y ) \u2264\n\nwhere \u03c312 = Var(X), \u03c322 = Var(Y ).\n\n(4.1)\n\n\fON THE PROBLEM OF REVERSIBILITY OF THE ENTROPY POWER INEQUALITY11\n\nIn turn, if X and Y are identically distributed, then Theorem 1.3 reads\nas follows: For any positive constant c, there exists a convex probability\nmeasure \u03bc on R with X, Y independently distributed according to \u03bc, with\nD(X \u00b1 Y ) \u2264 D(X) \u2212 c.\nThis may be viewed as a strengthened variant of (4.1). That is, in Theorem 1.3 we needed to show that both D(X + Y ) and D(X \u2212 Y ) may be\nmuch smaller than D(X) in the additive sense. In particular, D(X) has\nto be very large when c is large. For example, in our construction of the\nprevious section\nb2 \u2212 1\nb\u22121\n, EXb2 =\n,\nEXb =\nlog b\n2 log b\nwhich yields\nD(Xb ) \u223c\n\n3\nlog b,\n2\n\nD(Xb + Yb ) \u223c\n\n4\nlog b,\n3\n\nas b \u2192 +\u221e.\nIn [BCG1, BCG2] a slightly different question, raised by M. Kac and\nH. P. McKean [McK] (with the desire to quantify in terms of entropy the\nCramer characterization of the normal law), has been answered. Namely,\nit was shown that D(X + Y ) may be as small as we wish, while D(X) is\nseparated from zero. In the examples of [BCG2], D(X) is of order 1, while\nfor Theorem 1.3 it was necessary to use large values for D(X), arbitrarily\nclose to infinity. In addition, the distributions in [BCG1, BCG2] are not\nconvex.\nReferences\n[AMO] S. Artstein-Avidan, V. Milman, and Y. Ostrover (2008): The M -ellipsoid, symplectic capacities and volume. Comment. Math. Helv., 83(2), 359\u2013369.\n[Bob1] S. G. Bobkov (2007): Large deviations and isoperimetry over convex probability\nmeasures. Electron. J. Probab., 12, 1072\u20131100.\n[Bob2] S. G. Bobkov (2011): On Milman's ellipsoids and M -position of convex bodies. In:\nConcentration, Functional Inequalities and Isoperimetry, Contemporary Mathematics, vol. 545, American Mathematical Society, 23\u201334.\n[BCG1] S. G. Bobkov, G. P. Chistyakov, and F. G\u00f6tze (2009): Entropic instability of\nCramer's characterization of the normal law. Preprint, to appear in: The Selected\nWorks of Willem van Zwet, a separate volume by the IMS, Springer.\n[BCG2] S. G. Bobkov, G. P. Chistyakov, and F. G\u00f6tze (2011): Stability and instability of\nCramer's characterization in case of identically distributed summands. Preprint,\nsubmitted to: Theory Probab. Appl.\n[BM1] S. Bobkov and M. Madiman (2011): Dimensional behaviour of entropy and information. C. R. Acad. Sci. Paris S\u00e9r. I Math., 349, 201\u2013204.\n[BM2] S. Bobkov and M. Madiman (2011): The entropy per coordinate of a random\nvector is highly constrained under convexity conditions. IEEE Transactions on\nInformation Theory, 57(8), 4940\u20134954.\n\n\f12\n\nSERGEY G. BOBKOV AND MOKSHAY M. MADIMAN\n\n[BM3] S. Bobkov and M. Madiman (2011): Reverse Brunn-Minkowski and reverse entropy power inequalities for convex measures. Preprint, tentatively accepted by:\nJ. Funct. Anal.\n[Bor1] C. Borell (1973): Complements of Lyapunov's inequality. Math. Ann., 205, 323\u2013\n331.\n[Bor2] C. Borell (1974): Convex measures on locally convex spaces. Ark. Math., 12,\n239\u2013252.\n[Bor3] C. Borell (1975): Convex set functions in d-space. Period. Math. Hungar., 6(2),\n111\u2013136.\n[BKM] J. Bourgain, B. Klartag, and V. D. Milman (2004): Symmetrization and isotropic\nconstants of convex bodies. In Geometric aspects of functional analysis (1986/87),\nLecture Notes in Math., vol. 1850, 101\u2013115, Springer, Berlin.\n[BL]\nH. J. Brascamp, and E. H. Lieb (1976): On extensions of the Brunn-Minkowski\nand Pr\u00e9kopa-Leindler theorems, including inequalities for log concave functions,\nand with an application to the diffusion equation. J. Funct. Anal., 22(4), 366\u2013389.\n[CC]\nM. Costa, and T. M. Cover (1984): On the similarity of the entropy power inequality and the Brunn-Minkowski inequality. IEEE Trans. Inform. Theory, IT-30,\n837\u2013839.\n[DCT] A. Dembo, T. Cover, and J. Thomas (1991): Information-theoretic inequalities.\nIEEE Trans. Inform. Theory, 37(6), 1501\u20131518.\n[KM] B. Klartag, and V. D. Milman (2005): Geometry of log-concave functions and\nmeasures. Geom. Dedicata, 112, 169\u2013182.\n[KT]\nH. Koenig and N. Tomczak-Jaegermann (2005): Geometric inequalities for a class\nof exponential measures. Proc. Amer. Math. Soc., 133(4), 1213\u20131221.\n[McK] H. P. McKean, Jr. (1966): Speed of approach to equilibrium for Kac's caricature\nof a Maxwellian gas. Arch. Rational Mech. Anal. 21, 343\u2013367.\n[Mad] M. Madiman (2008): On the entropy of sums. In Proceedings of the IEEE Information Theory Workshop, Porto, Portugal, IEEE.\n[MK] M. Madiman and I. Kontoyiannis (2010): The entropies of the sum and the difference of two IID random variables are not too different. In Proceedings of the\nIEEE International Symposium on Information Theory, Austin, Texas, IEEE.\n[M1]\nV. D. Milman (1986): An inverse form of the Brunn-Minkowski inequality, with\napplications to the local theory of normed spaces. C. R. Acad. Sci. Paris S\u00e9r. I\nMath., 302(1), 25\u201328.\n[M2]\nV. D. Milman (1988): Isomorphic symmetrizations and geometric inequalities. In\nGeometric aspects of functional analysis (1986/87), Lecture Notes in Math., vol.\n1317, 107\u2013131, Springer, Berlin.\n[M3]\nV. D. Milman (1988): Entropy point of view on some geometric inequalities. C.\nR. Acad. Sci. Paris S\u00e9r. I Math., 306(14), 611\u2013615.\n[Pis]\nG. Pisier (1989): The volume of convex bodies and Banach space geometry. Cambridge Tracts in Mathematics, vol. 94, Cambridge University Press, Cambridge.\n[Pre]\nA. Pr\u00e9kopa (1971): Logarithmic concave measures with applications to stochastic\nprogramming. Acta Sci. Math. Szeged, 32, 301\u2013316.\n[RS]\nC. A. Rogers and G. C. Shephard (1957): The difference body of a convex body.\nArch. Math. (Basel), 8, 220\u2013233.\n[Sha] C. E. Shannon (1948): A mathematical theory of communication. Bell System\nTech. J., 27, 379\u2013423, 623\u2013656.\n[Sta]\nA. J. Stam (1959): Some inequalities satisfied by the quantities of information of\nFisher and Shannon. Information and Control, 2, 101\u2013112.\n\n\fON THE PROBLEM OF REVERSIBILITY OF THE ENTROPY POWER INEQUALITY13\n\n[SV]\n\nS. Szarek and D. Voiculescu (2000): Shannon's entropy power inequality via restricted Minkowski sums. In Geometric aspects of functional analysis, Lecture\nNotes in Math., vol. 1745, 257-262, Springer, Berlin.\n\nSergey G. Bobkov, School of Mathematics, University of Minnesota, Vincent Hall 228, 206 Church St SE, Minneapolis MN 55455, USA\nE-mail address: bobkov@math.umn.edu\nMokshay M. Madiman, Department of Statistics, Yale University, 24 Hillhouse Avenue, New Haven CT 06511, USA\nE-mail address: mokshay.madiman@yale.edu\n\n\f"}