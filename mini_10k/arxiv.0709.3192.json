{"id": "http://arxiv.org/abs/0709.3192v3", "guidislink": true, "updated": "2008-06-12T11:44:16Z", "updated_parsed": [2008, 6, 12, 11, 44, 16, 3, 164, 0], "published": "2007-09-20T11:17:23Z", "published_parsed": [2007, 9, 20, 11, 17, 23, 3, 263, 0], "title": "A quantile-copula approach to conditional density estimation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0709.1234%2C0709.2360%2C0709.0333%2C0709.0501%2C0709.0791%2C0709.2740%2C0709.1282%2C0709.3613%2C0709.3248%2C0709.2744%2C0709.1109%2C0709.2093%2C0709.1170%2C0709.2475%2C0709.3783%2C0709.4646%2C0709.3988%2C0709.3152%2C0709.0009%2C0709.4308%2C0709.3787%2C0709.4041%2C0709.3919%2C0709.4018%2C0709.0767%2C0709.0422%2C0709.2242%2C0709.0811%2C0709.2410%2C0709.1789%2C0709.4019%2C0709.1944%2C0709.3696%2C0709.4127%2C0709.1750%2C0709.3345%2C0709.2669%2C0709.4375%2C0709.1004%2C0709.1256%2C0709.4660%2C0709.2050%2C0709.0861%2C0709.3192%2C0709.3144%2C0709.2805%2C0709.4572%2C0709.4672%2C0709.2646%2C0709.2500%2C0709.0083%2C0709.0349%2C0709.4023%2C0709.3141%2C0709.1915%2C0709.1613%2C0709.0258%2C0709.1295%2C0709.2876%2C0709.4135%2C0709.0145%2C0709.3683%2C0709.0363%2C0709.4643%2C0709.2286%2C0709.2234%2C0709.0139%2C0709.0609%2C0709.3899%2C0709.1897%2C0709.1123%2C0709.4176%2C0709.4086%2C0709.1776%2C0709.3875%2C0709.3870%2C0709.0054%2C0709.4343%2C0709.0541%2C0709.4653%2C0709.2543%2C0709.1527%2C0709.1847%2C0709.2569%2C0709.0764%2C0709.0854%2C0709.4210%2C0709.2136%2C0709.0626%2C0709.4279%2C0709.3559%2C0709.2957%2C0709.3136%2C0709.0296%2C0709.0656%2C0709.0364%2C0709.3751%2C0709.3122%2C0709.2812%2C0709.1252%2C0709.1093&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A quantile-copula approach to conditional density estimation"}, "summary": "We present a new non-parametric estimator of the conditional density of the\nkernel type. It is based on an efficient transformation of the data by quantile\ntransform. By use of the copula representation, it turns out to have a\nremarkable product form. We study its asymptotic properties and compare its\nbias and variance to competitors based on nonparametric regression.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0709.1234%2C0709.2360%2C0709.0333%2C0709.0501%2C0709.0791%2C0709.2740%2C0709.1282%2C0709.3613%2C0709.3248%2C0709.2744%2C0709.1109%2C0709.2093%2C0709.1170%2C0709.2475%2C0709.3783%2C0709.4646%2C0709.3988%2C0709.3152%2C0709.0009%2C0709.4308%2C0709.3787%2C0709.4041%2C0709.3919%2C0709.4018%2C0709.0767%2C0709.0422%2C0709.2242%2C0709.0811%2C0709.2410%2C0709.1789%2C0709.4019%2C0709.1944%2C0709.3696%2C0709.4127%2C0709.1750%2C0709.3345%2C0709.2669%2C0709.4375%2C0709.1004%2C0709.1256%2C0709.4660%2C0709.2050%2C0709.0861%2C0709.3192%2C0709.3144%2C0709.2805%2C0709.4572%2C0709.4672%2C0709.2646%2C0709.2500%2C0709.0083%2C0709.0349%2C0709.4023%2C0709.3141%2C0709.1915%2C0709.1613%2C0709.0258%2C0709.1295%2C0709.2876%2C0709.4135%2C0709.0145%2C0709.3683%2C0709.0363%2C0709.4643%2C0709.2286%2C0709.2234%2C0709.0139%2C0709.0609%2C0709.3899%2C0709.1897%2C0709.1123%2C0709.4176%2C0709.4086%2C0709.1776%2C0709.3875%2C0709.3870%2C0709.0054%2C0709.4343%2C0709.0541%2C0709.4653%2C0709.2543%2C0709.1527%2C0709.1847%2C0709.2569%2C0709.0764%2C0709.0854%2C0709.4210%2C0709.2136%2C0709.0626%2C0709.4279%2C0709.3559%2C0709.2957%2C0709.3136%2C0709.0296%2C0709.0656%2C0709.0364%2C0709.3751%2C0709.3122%2C0709.2812%2C0709.1252%2C0709.1093&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present a new non-parametric estimator of the conditional density of the\nkernel type. It is based on an efficient transformation of the data by quantile\ntransform. By use of the copula representation, it turns out to have a\nremarkable product form. We study its asymptotic properties and compare its\nbias and variance to competitors based on nonparametric regression."}, "authors": ["Olivier P. Faugeras"], "author_detail": {"name": "Olivier P. Faugeras"}, "author": "Olivier P. Faugeras", "arxiv_comment": "with short simulations", "links": [{"href": "http://arxiv.org/abs/0709.3192v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0709.3192v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62G007, 62M20, 62M10", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0709.3192v3", "affiliation": "LSTA", "arxiv_url": "http://arxiv.org/abs/0709.3192v3", "journal_reference": null, "doi": null, "fulltext": "A quantile- opula approa h to\n\nonditional\n\ndensity estimation.\n\narXiv:0709.3192v3 [stat.ME] 12 Jun 2008\n\nOlivier P. Faugeras\n\nL.S.T.A, Universit\u00e9 Paris 6\n175, rue du Chevaleret, 75013 Paris, Fran e\nTel:+(33) 1 44 27 85 62\nFax:+(33) 1 44 27 33 42\n\nAbstra t\nWe present a new non-parametri estimator of the onditional density of the kernel\ntype. It is based on an e\u001e ient transformation of the data by quantile transform. By\nuse of the opula representation, it turns out to have a remarkable produ t form. We\nstudy its asymptoti properties and ompare its bias and varian e to ompetitors\nbased on nonparametri regression. A omparative numeri al simulation is provided.\nonditional density, kernel estimation, opula, quantile transform,\nnonparametri regression,\n1991 MSC: 62G007, 62M20, 62M10\n\nKey words:\n\n1 Introdu tion\n1.1 Motivation\nLet\n\n((Xi , Yi ); i = 1, . . . , n)\n\nbe an independent identi ally distributed sample\n\n(X, Y ) sitting on a given probability spa e.\nY of the input variable X at a given lo ation x, it\n\nfrom real-valued random variables\nFor predi ting the response\n\nis of great interest of estimating not only the\n\nonditional mean or\n\nregression\n\nfun tion E(Y |X = x), but the full onditional density f (y|x). Indeed, estimating the\n\nonditional density is mu h more informative, sin e it allows not only\n\nto re al ulate the\n\nEmail address:\n\nonditional expe ted value\n\nE(Y |X) and\n\nolivier.faugeras\bgmail. om\n\nPreprint submitted to Elsevier\n\nonditional varian e\n\n(Olivier P. Faugeras ).\nNovember 1, 2018\n\n\ffrom the density, but also to provide the general shape of the\n\nonditional den-\n\nsity. This is espe ially important for multi-modal or skewed densities, whi h\noften arise from nonlinear or non-Gaussian phenomenas, where the expe ted\nvalue might be nowhere near a mode, i.e. the most likely value to appear.\nMoreover, for situations in whi h\nestimates, the estimated\n\non\u001cden e intervals are preferred to point\n\nonditional density is an obje t of obvious interest.\n\n1.2 Estimation by kernel smoothing\nA natural approa h to estimate the\n\nX =x\n\nonditional density\n\nof\n\nY\n\ngiven\n\nwould be to exploit the identity\n\nf (y|x) =\nwhere\n\nf (y|x)\n\nfXY\n\nand\n\nfX\n\nfXY (x, y)\nfX (x)\n\ndenote the joint density of\n\n(1)\n\n(X, Y )\n\nand\n\nX,\n\nrespe tively. By\n\nintrodu ing Parzen-Rosenblatt kernel estimators of these densities, namely\n\nn\n1X\nK \u2032 \u2032 (Xi \u2212 x)Kh (Yi \u2212 y)\nf\u02c6n,XY (x, y) : =\nn i=1 h\nn\n1X\n\u02c6\nKh\u2032 \u2032 (Xi \u2212 x)\nfn,X (x) : =\nn i=1\nwhere\n\nKh (.) = 1/hK(./h)\n\nwith their asso iated sequen e\nas\n\nn \u2192 \u221e,\n\none\n\nan\n\nKh\u2032 \u2032 (.) = 1/h\u2032 K \u2032 (./h\u2032 ) are (res aled) kernels\n\u2032\n\u2032\nof bandwidth h = hn and h = hn going to zero\n\nand\n\nonstru t the quotient\n\nf\u02c6n,XY (x, y)\nf\u02c6nR (y|x) :=\nf\u02c6n,X (x)\nand obtain an estimator of the\n\nonditional density. Su h an estimator was \u001crst\n\nstudied by Rosenblatt [26\u2104, and more re ently by Hyndman et al. [17\u2104, who\nslightly improved on Rosenblatt's kernel based estimator.\n\n1.3 Estimation by regression te hniques\nAs pointed out by numerous authors, see e.g. Fan and Yao [7\u2104\napproa h is equivalent to the one arising from\n\nhapter 6, this\n\nonsidering this\n\nonditional\n\nF (y|x) be\nX = x. It stems\n\ndensity estimation problem in a regression framework. Indeed, let\nthe\n\numulative\n\nonditional distribution fun tion of\n\nY\n\ngiven\n\nfrom the fa t that\n\n\u0010\n\n\u0011\n\nE 1|Y \u2212y|\u2264h |X = x = F (y + h|x) \u2212 F (y \u2212 h|x) \u2248 2h.f (y|x)\n2\n\n\fas\n\nh \u2192 0,\n\nempiri al\n\nthat, if one repla e the expe tation in the above expression by its\nounterpart, one\n\nan apply the usual lo al averaging methods and\n\nperform a regression estimation on the syntheti\n\n1, . . . , n).\n\nBy a Bo hner type theorem, one\n\ndata\n\n((1/2h)1|Yi \u2212y|\u2264h ; i =\n\nan even repla e the transformed\n\ndata by its smoothed version\n\nYi\u2032\n\n1\nYi \u2212 y\n:= Kh (Yi \u2212 y) := K\n.\nh\nh\n\u0012\n\n\u0013\n\nIn parti ular, the popular Nadaraya-Watson regression estimator\n\nPn\n\n\u2032\n\n\u2032\n\ni=1 Yi Kh\u2032 (Xi \u2212 x)\nf\u02c6nN W (y|x) := P\nn\n\u2032\ni=1 Kh\u2032 (Xi \u2212 x)\n\nredu es itself to the same estimator of the\n\nonditional density of the double\n\nkernel type as before\n\nf\u02c6nN W (y|x) :=\n\nPn\n\ni=1\n\nKh (Yi \u2212 y)Kh\u2032 \u2032 (Xi \u2212 x)\n= f\u02c6nR (y|x).\nPn\n\u2032\ni=1 Kh\u2032 (Xi \u2212 x)\n\nTaking advantage of this regression formulation, Fan, Yao and Tong [8\u2104 proposed a\n\nonditional density estimator whi h generalizes the kernel one by use\n\nof the lo al polynomial te hniques. In parti ular, it allows to ta kle with the\nbias issues of the kernel smoothing. However, and unlike the former, it is no\nlonger guaranteed to have positive value nor to integrate to 1 with respe t\nto\n\ny.\n\nWith these issues in mind, Hyndman and Yao [18\u2104 built on lo al poly-\n\nnomial te hniques and suggested two improved methods, the \u001crst one based\non lo ally \u001ctting a log-linear model and the se ond one on\npolynomial modeling. An overview\n\nonstrained lo al\n\nan be found in Fan and Yao [7\u2104 ( hapter\n\n6 and 10). Very re ently, Gy\u00f6r\u001c and Kohler [15\u2104 studied a partitioning type\nestimate and studied its properties in total variation norm and La our [20\u2104 a\nproje tion-type estimate for Markov\n\nhains.\n\n1.4 A produ t shaped estimator\nHowever, these two equivalent approa hes su\u001ber from several drawba ks: \u001crst,\nby its form as a quotient of two estimators, the probabilisti\nNadaraya-Watson estimator (or its lo al polynomial\nstudy. It is usually dealt with by a\n\nbehavior of the\n\nounterpart) is tri ky to\n\nentering at expe tation for both numerator\n\nand denominator and a linearizing of the inverse, see e.g. [7\u2104, or [1\u2104 for details.\nSe ond, at a\n\non eptual level, one\n\nould argue that implementing regression\n\nestimation te hniques in this setting is, in a sense, unnatural: estimating a\ndensity, even if it is a\n\nonditional one, should resort to density estimation\n\nte hniques only. Finally, pra ti al implementations of these estimators\nlead to numeri al instability when the denominator is\n\n3\n\nlose to zero.\n\nan\n\n\fTo remedy these problems, we propose an estimator whi h builds on the idea\nof using syntheti\n\ndata, i.e. a representation of the data more adapted to the\n\nproblem than the original one. By transforming the data by quantile transforms and making use of the\na remarkable\n\nprodu t\n\nopula fun tion, the estimator turns out to have\n\nform\n\nf\u02c6n (y|x) = f\u02c6Y (y)\u0109n (Fn (x), Gn (y))\nf\u02c6Y , \u0109n , Fn (x), Gn (y) are estimators of the density fY of Y ,\ndensity c, the .d.f. F of X and G of Y respe tively (see next se\n\nwhere\n\nthe\n\nopula\n\ntion below\n\nfor de\u001cnitions). Its study then reveals to be parti ularly simple: it redu es to\nthe ones already done on nonparametri\n\ndensity estimation.\n\nThe rest of the paper is organized as follows: in se tion 2, we introdu e the\nquantile transform and the\n\nopula representation whi h leads to the de\u001cnition\n\nof our estimator. In se tion 3, the main asymptoti\nompared in se tion 4 to those of other\n\nresults are established and\n\nompetitors. Proofs are mainly based\n\non a series of auxiliary lemmas whi h are given in se tion 5.\n\n2 Presentation of the estimator\nFor sake of simpli ity and\n\nlarity of exposition, we limit ourselves to unidi-\n\nmensional real valued input variables\n\nX . However, all the results of this arti\n\nan be easily extended to the multivariate\n\nle\n\nase.\n\n2.1 The quantile transform\nThe idea of transforming the data is not new. It has been used to improve\nthe range of appli ability and performan e of\n\nlassi al estimation te hniques,\n\ne.g. to deal with skewed data, heavy tails, or restri tions on the support (see\ne.g. Devroye and Lugosi [6\u2104\nVan der Vaart [35\u2104\n\nhapter 14 and the referen es therein, and also\n\nhapter 3.2 for the related topi\n\ntransformations in a parametri\n\nX,\n\nof varian e stabilizing\n\nontext). In order to make inferen e on\n\nY\n\nfrom\n\na natural question whi h then arises is, what is the \u0010best\u0011 transformation,\n\nif this question has a sense. As one\n\nan note from the above referen es, the\n\n\u0010best\u0011 transformation is very linked to the distribution of the underlying data.\nWe will see below that, for our problem, the natural\n\nandidate is the quantile\n\ntransform.\nThe quantile transform is a well-known probabilisti\n\ntri k whi h is used to\n\nredu e proofs, e.g. in empiri al pro ess theory, for arbitrary real valued random variables\n\nX\n\nto ones for random variables\n\n4\n\nU\n\nuniformly distributed on the\n\n\finterval\n\n[0, 1].\n\nontinuous, the random variable\nand that\n\nF is\n(0, 1)\n\nIt is based on the following well-known fa t that whenever\n\nonversely, when\n\ndom variable on\n\n(0, 1), X\n\nF\n\nU = F (X)\n\nis uniformly distributed on\n\nU is a uniformly distributed ran\u22121\nto F\n(U), where F \u22121 = Q is the\nX . (See e.g. [28\u2104, hapter 1).\n\nis arbitrary, if\n\nis equal in law\n\ngeneralized inverse or quantile fun tion of\n\n(X1 , . . . , Xn ) of random variables with ommon ontinuous .d.f. F sitting on a probability spa e (\u03a9, A, P), one an always enlarge this probability spa e to arry a sequen e (U1 , . . . , Un ) of uniform\n(0, 1) random variables su h that Ui = F (Xi ), that is to say to onstru t a\nAs a\n\nonsequen e, given a sample\n\npseudo-sample with a\n\npres ribed uniform\n\nmarginal distribution.\n\n2.2 The opula representation\nFormally, a\n\nopula is a bi-(or multi)variate distribution fun tion whose margi-\n\nnal distribution fun tions are uniform on the interval\n\n[0, 1].\n\nIndeed, Sklar [29\u2104\n\nproved the following fundamental result:\n\nTheorem 2.1 For any bivariate umulative distribution fun tion FX,Y on R2 ,\nwith marginal umulative distribution fun tions F of X and G of Y , there exists some fun tion C : [0, 1]2 \u2192 [0, 1], alled the dependen e or opula fun tion,\nsu h as\nFX,Y (x, y) = C(F (x), G(y))\n\n, \u2212 \u221e \u2264 x, y \u2264 +\u221e.\n\n(2)\n\nIf F and G are ontinuous, this representation is unique with respe t to (F, G).\nThe opula fun tion C is itself a umulative distribution fun tion on [0, 1]2 with\nuniform marginals.\nThis theorem gives a representation of the bivariate\nunivariate\n\n.d.f. In other words, the\n\nstru ture among the\n\nomponents\n\nof the marginal distribution\n\nF\n\nX\n\nand\n\nopula fun tion\nand\n\nG.\n\nY\n\n.d.f. as a fun tion of ea h\naptures the dependen e\n\nof the ve tor\n\n(X, Y ), irrespe\n\ntively\n\nSimply put, it allows to deal with the\n\nrandomness of the dependen e stru ture and the randomness of the marginals\n\nseparately.\n\nCopulas appears to be naturally linked with the quantile transform as formula\nC(u, v) = FX,Y (F \u22121 (u), G\u22121(v)). For more details regarding op-\n\n2 entails that\n\nulas and their properties, one\n\nan\n\nonsult for example the book of Joe [19\u2104.\n\nCopulas have witnessed a renewed interest in statisti s, espe ially in \u001cnan e,\nsin e the pioneering work of Deheuvels [4\u2104, who introdu ed the empiri al\n\nop-\n\nula pro ess. Weak onvergen e of the empiri al opula pro ess was investigated\nby Deheuvels [5\u2104, Van der Vaart and Wellner [36\u2104, Fermanian, Radulovi\nWegkamp [11\u2104. For the estimation of the\n\n5\n\nand\n\nopula density, refer to Gijbels and\n\n\fMielni zuk [13\u2104, Fermanian [9\u2104 and Fermanian and S aillet [10\u2104.\nFrom now on, we assume that the\n\nopula fun tion C(u, v) has a density c(u, v)\n[0, 1]2 and that F and G are stri tly\n\nwith respe t to the Lebesgue measure on\n\nin reasing and di\u001berentiable with densities\nthen the\n\nf\n\nand\n\ng . C(u, v)\n\nand\n\nc(u, v)\n\nare\n\numulative distribution fun tion ( .d.f.) and density respe tively of\n\nthe transformed variables\n\n(U, V ) = (F (X), G(Y )).\n\nBy di\u001berentiating formula\n\n(2), we get for the joint density,\n\nfXY (x, y) =\n\n\u2202 2 C(u,v)\nis the above mentioned opula density. Eventually,\n\u2202u\u2202v\nan obtain the following expli it formula of the onditional density\n\nwhere\nwe\n\n\u2202 2 FXY (x, y)\n= f (x)g(y)c(F (x), G(y))\n\u2202x\u2202y\n\nc(u, v) :=\n\nfY |X (x, y) =\n\nfXY (x, y)\n= g(y)c(F (x), G(y)).\nf (x)\n\n(3)\n\n2.3 Constru tion of the estimator\nStarting from the previously stated produ t type formula (3), a natural plug-in\napproa h to build an estimator of the\n\n\u2022\n\n\u2022\n\nonditional density is to use\n\na Parzen-Rosenblatt kernel type non parametri\ndensity\n\ng\n\nof\n\nY,\n\nestimator of the marginal\n\n\u0012\n\u0013\nn\ny \u2212 Yi\n1 X\nK0\n\u011dn (y) :=\nnhn i=1\nhn\n\nthe empiri al distribution fun tions\n\nFn (x)\n\nand\n\nGn (y)\n\nfor\n\nF (x)\n\nand\n\nG(y)\n\nrespe tively,\n\nFn (x) =\nCon erning the\n\nn\n1X\n1X \u2264x\nn j=1 j\n\nand\n\nGn (y) :=\n\nn\n1X\n1Y \u2264y .\nn j=1 j\n\nc(u, v), we noted that c(u, v) is the joint density\n(U, V ) = (F (X), G(Y )). Therefore, c(u, v) an\n\nopula density\n\nof the transformed variables\n\nbe estimated by the bivariate Parzen-Rosenblatt kernel type non parametri\ndensity (pseudo) estimator,\n\n\u0013\n\u0012\nn\n1 X\nu \u2212 Ui v \u2212 Vi\ncn (u, v) :=\n,\nK\nnan bn i=1\nan\nbn\nwhere\n\nK\n\nis a bivariate kernel and\n\nan , bn\n\nits asso iated bandwidth. For simpli -\n\nity, we restri t ourselves to produ t kernels, i.e.\nthe same bandwidths\n\nan = bn .\n6\n\n(4)\n\nK(u, v) = K1 (u)K2 (v)\n\nwith\n\n\fNonetheless, sin e\n\nF\n\nand\n\nG are unknown, the random variables (Ui , Vi ) are not\n\nobservable, i.e. cn is not a true statisti . Therefore, we approximate the pseudo-\n\n(Ui , Vi ), i = 1, . . . , n by its empiri al ounterpart (Fn (Xi ), Gn (Yi )), i =\n1, . . . , n. We therefore obtain a genuine estimator of c(u, v)\nsample\n\nn\n1 X\nu \u2212 Fn (Xi )\nv \u2212 Gn (Yi )\n\u0109n (u, v) := 2\nK1\nK2\n.\nnan i=1\nan\nan\n\n!\n\nEventually, the\n\nf\u02c6n (y|x) :=\n\n!\n\n(5)\n\nonditional density estimator is written as\n\n\u0013\n\u0012\nn\nn\n1 X\n1 X\nFn (x) \u2212 Fn (Xi )\ny \u2212 Yi\n.\nK1\nK0\n2\nnhn i=1\nhn\nnan i=1\nan\n!#\nFn (y) \u2212 Gn (Yi )\nK2\nan\n# \"\n\n\"\n\nor, under a more\n\n!\n\nompa t form,\n\nf\u02c6n (y|x) := \u011dn (y)\u0109n (Fn (x), Gn (y)).\n\n(6)\n\nRemark 1 To our knowledge, the estimator studied in this paper has never\nbeen proposed in the literature. However, some onne tions an be made with\nthe nearest neighbor one proposed by Stute [32\u2104, [33\u2104 and [34\u2104 for onditional\numulative distribution fun tion and the Gasser and M\u00fcller [12\u2104 and Priestley\nand Chao [24\u2104 one in the ontext of regression estimation. Indeed, these estimators ta kle the issue of having a random denominator by \u001crst transforming\nthe design X1 , . . . , Xn to a uniform (random) one. This result in assigning\nthe surfa es under the kernel fun tion instead of its heights as weights. Contrary to our estimator, they do not make transformations of the data in both\ndire tions X and Y .\n3 Asymptoti results\n3.1 Notations and assumptions\nWe note the ith moment of a generi kernel (possibly multivariate) K as\nR\nR\nmi (K) := ui K(u)du, and the Lp norm of a fun tion h by ||h||p := hp . We\n\n\u2243 to denote the order of the bandwidths, i.e. hn \u2243 un means that\nhn = cn un with cn \u2192 c > 0. The support of the densities fun tion f and c are\n2\nnoted as supp(f ) = {x \u2208 R; f (x) > 0} and supp(c) = {(u, v) \u2208 R ; c(u, v) >\n0}, respe tively.\nuse the sign\n\nFor stating our results, we will have to make some regularity assumptions\n\n7\n\n\fon the kernels and the densities whi h, although far from being minimal, are\nsomehow\n\nustomary in kernel density estimation (see subse tion 5.2 for dis us-\n\nsions and details). Set\n\nx\n\nand\n\ny\n\ntwo \u001cxed points in the interior of supp(f ) and\n\nsupp(g) respe tively. In the remainder of this paper, we will always suppose\nthat\ni) the\n\n.d.f\n\nF\n\nof\n\nii) the densities\n\nX and G of Y are stri tly in reasing and di\u001berentiable;\ng and c are twi e di\u001berentiable with ontinuous bounded\n\nse ond derivatives on their support.\nMoreover, we assume that the kernels\n(i)\n(ii)\n(iii)\n\nK0\n\nand\n\nK\n\nsatisfy the following:\n\nK and K0 are of bounded support and of bounded variation;\n0 \u2264 K \u2264 C and 0 \u2264 K0 \u2264 C for some onstant C ;\nK and K0 are \u001crst order kernels: m0 (K) = 1, m1 (K) = 0 and m2 (K) <\n+\u221e, and the same for K0 .\n\nIn addition, in order to approximate\n\n\u0109n\n\nstringent assumption on the bivariate\n\ncn , we will impose the slightly more\nkernel K , that it is twi e di\u001berentiable\nby\n\nwith bounded se ond partial derivatives.\n\n3.2 Weak and strong onsisten y of the estimator\nWe have the following pointwise weak\n\nonsisten y theorem:\n\nTheorem 3.1 Let the regularity onditions on the densities and kernels be\nsatis\u001ced, if hn and an tends to zero as n \u2192 \u221e in su h a way that nhn \u2192 \u221e,\nna2n \u2192 \u221e, then\n\uf8f6\n\n\uf8eb\n\n1\n1\n+ a2n \uf8f8 .\nf\u02c6n (y|x) = f (y|x) + OP \uf8ed \u221a\n+ h2n + q\n2\nnhn\nnan\n\nProof. Re\nc\n\nall from 4 and 5 that\n\ncn\n\nand\n\n\u0109n\n\nare estimators of the\n\nbased respe tively on unobservable pseudo-data\n\napproximations\n\n(Fn (Xi ), Gn (Yi )).\n\nopula density\n\n(F (Xi ), G(Yi ),\n\nand their\n\nThe main ingredient of the proof follows\n\nfrom the de omposition:\n\nf\u02c6n (y|x) \u2212 f (y|x) = \u011dn (y)\u0109n (Fn (x), Gn (y)) \u2212 g(y)c(F (x), G(y))\n= [\u011dn (y) \u2212 g(y)] \u0109n (Fn (x), Gn (y))\n+ g(y) [\u0109n (Fn (x), Gn (y)) \u2212 c(F (x), G(y))]\n: = D1 + D2\n\n8\n\n\fWe pro eed one step further in the de omposition of ea h terms, by \u001crst\nentering at \u001cxed lo ations,\n\nD1 = [\u011dn (y) \u2212 g(y)] [\u0109n (Fn (x), Gn (y)) \u2212 \u0109n (F (x), G(y))]\n+ [\u011dn (y) \u2212 g(y)] [\u0109n (F (x), G(y)) \u2212 cn (F (x), G(y))]\n+ [\u011dn (y) \u2212 g(y)] [cn (F (x), G(y)) \u2212 c(F (x), G(y))]\n+ [\u011dn (y) \u2212 g(y)] [c(F (x), G(y))]\n\n(7)\n\nD2 = g(y) [\u0109n (Fn (x), Gn (y)) \u2212 \u0109n (F (x), G(y))]\n+ g(y) [\u0109n (F (x), G(y)) \u2212 cn (F (x), G(y))]\n+ g(y) [cn (F (x), G(y)) \u2212 c(F (x), G(y))]\n\n(8)\n\nConvergen e results for the kernel density estimators of se tion 5.2 entail that\n\nq\n\n\u011dn (y) \u2212 g(y) = Op (h2n + 1/ nhn )\nq\n\ncn (F (x), G(y)) \u2212 c(F (x), G(y)) = Op (a2n + 1/ na2n )\n\nby lemma 5.2 and 5.3 respe tively. Approximation lemmas 5.4 and 5.5 of\nse tions 5.4 and 5.5 entail that\n\nq\n\n\u0109n (F (x), G(y)) \u2212 cn (F (x), G(y)) = oP (a2n + 1/ na2n )\nq\n\n\u0109n (Fn (x), Gn (y)) \u2212 \u0109n (F (x), G(y)) = oP (a2n + 1/ na2n ).\nWe therefore obtain that\n\nD1 = O P\n\n\u0012\n\nh2n\n\nq\n\n\u0013\n\n+ 1/ nhn OP\n\n\u0012\n\nq\n\n\u0013\n\n\u0012\n\na2n\n\n+ 1/\n\n\u0012\n\nq\n\nna2n\n\nq\n\n\u0013\n\nD2 = oP a2n + 1/ na2n + OP a2n + 1/ na2n\nand the\n\nondition\n\nonvergen e of the\n\n+ OP\n\u0013\n\n\u0012\n\nh2n\n\nq\n\n+ 1/ nhn\n\nan \u2192 0, hn \u2192 0, na2n \u2192 +\u221e, nhn \u2192 +\u221e\nestimator. \u2737\n\n\u0013\n\nentails the\n\nRemark 2 As a orollary, we get the rate of onvergen e, by hoosing the\nbandwidths whi h balan e the bias and varian e trade-o\u001b: for an optimal hoi e\nof hn \u2243 n\u22121/5 and an \u2243 n\u22121/6 , we get\nf\u02c6(y|x) = f (y|x) + OP (n\u22121/3 ).\n\nTherefore, our estimator is rate optimal in the sense that it rea hes the minimax rate n\u22121/3 of onvergen e, a ording to Stone [30\u2104.\nAlmost sure results\nstrong\n\nan be proved in the same way: we have the following\n\nonsisten y result\n\n9\n\n\fTheorem 3.2 Let the regularity onditions on the densities and kernels be\nsatis\u001ced. If in addition nhn /(ln ln n) \u2192 \u221e and na2n /(ln ln n) \u2192 \u221e , then\n\uf8eb\n\nf\u02c6n (y|x) = f (y|x) + Oa.s. \uf8eda2n +\n\nProof. It\n\ns\n\nln ln n\n+ h2n +\nna2n\n\ns\n\n\uf8f6\n\nln ln n \uf8f8\n.\nnhn\n\nfollows the same lines as the pre eding theorem, but uses the a.s.\n\nresults of the\n\nonsisten y of the kernel density estimators of lemmas 5.2 and\n\n5.3 and of the approximation lemmas 5.4 and 5.5. It is therefore omitted.\n\n\u2737\n\nRemark 3 For hn \u2243 (ln ln n/n)1/5 and an \u2243 (ln ln n/n)1/6 whi h is the optimal trade-o\u001b between the bias and the sto hasti term, one gets the optimal\nrate (ln ln n/n)1/3 .\n3.3 Convergen e in distribution\n\nTheorem 3.3 Let the regularity onditions on the densities and kernels be\nsatis\u001ced. hn \u2192 0, an \u2192 0, nhn \u2192 \u221e and na2n \u2192 \u221e entail\nq\n\nd\nna2n f\u02c6n (y|x) \u2212 f (y|x) \u2740 N 0, g(y)f (y|x)||K||22 .\n\n\u0010\n\n\u0011\n\n\u0010\n\nFor hn \u2243 n\u22121/5 , an \u2243 n\u22121/6 one gets the usual rate n\u22121/3 .\n\nProof.\n\n\u0011\n\nWith the\n\nonditions on the bandwidths, all the terms in the pre2 \u22121/2\nvious de omposition 7 and 8, are negligible ompared to (nan )\nex ept\ncn (F (x), G(y)) \u2212 c(F (x), G(y)), whi h is asymptoti ally normal by the result\n\nof se tion 5, lemma 5.3\n\nq\n\nd\n\n\u0010\n\n\u0011\n\nna2n g(y) [cn (F (x), G(y)) \u2212 c(F (x), G(y))] \u2740 N 0, g 2(y)c(F (x), G(y)) kKk22 .\n\nAn appli ation of Slutsky's lemma yields the desired result.\nFor a ve tor\n\n(y1 , . . . , yd ),\n\none\n\nvergen e in distribution (\u001cdi\n\n\u2737\n\nan get a multidimensional version of the\n\non-\n\nonvergen e):\n\nCorollary 3.4 With the same assumptions, for (y1 , . . . , yd ) in the interior of\nsupp(g) su h that g(yi)f (yi|x) 6= 0,\n\uf8f6\n\n\uf8eb\uf8eb\n\n\uf8f6\n\nf\u02c6n (yi |x) \u2212 f (yi |x) \uf8f8\nd\n, i = 1, ..., m\uf8f8 \u2740 N (m)\nna2n \uf8ed\uf8ed q\ng(yi )f (yi |x) kKk2\n\nq\n\nwhere N (m) is the standard m-variate entered normal distribution with identity varian e matrix.\n10\n\n\fProof. It simply follows from the use of the Cram\u00e9r-Wold devi\nfore omitted. For details, see e.g. [1\u2104, theorem 2.3.\n\ne and is there-\n\n\u2737\n\n3.4 Asymptoti Bias, Varian e and Mean square error\nThe asymptoti\n\nbias is\n\nal ulated in the following proposition.\n\nProposition 3.5 With the assumptions of Theorem 3.1, we have\n2\n\na\nB0 := E(f\u02c6n (y|x)) \u2212 f (y|x) = g(y)BK (c, x, y) n + o(a2n )\n2\n\nwith BK (c, x, y) := m2 (K1 ) \u2202\n\nProof. (Sket\n\n2 c(F (x),G(y))\n\n\u2202u2\n\n+ m2 (K2 ) \u2202\n\n2 c(F (x),G(y))\n\n\u2202v2\n\n.\n\nh). By taking expe tation in the de omposition 7 and 8,\n\nED1 = c(F (x), G(y))E[\u011dn(y) \u2212 g(y)] + R1\nED2 = g(y)E ([cn (F (x), G(y)) \u2212 c(F (x), G(y))]) + R2\nwhere we made appear the bias of\n\n\u011dn\n\nand\n\ncn\n\nand where\n\nR1\n\nand\n\nR2\n\nstand for\n\nthe remaining terms. With the assumptions on the bandwidths and derivations\nmade tedious by the transformation of the data by the empiri al margins, (see\nFermanian [9\u2104 theorem 1 for su h a\nompared to the bias of\n\ncn .\n\nal ulation), the terms in\n\nThe bias of\n\ncn ,\n\nbivariate kernel density estimator, is of order\nprodu t terms in\n\nD1 by Cau\n\nR2\n\nare negligible\n\nwhi h is simply the bias of a\n\na2n .\n\nSimilarly, by bounding the\n\nhy-S hwarz inequality, routine analysis show that\n\nthe terms in R1 are negligible ompared to the bias of \u011dn , whi h is of order\nh2n . Sin e h2n is itself negligible to a2n , the main term in the de omposition is\ng(y)E(cn(F (x), G(y)) \u2212 C(F (x), G(y))). Plugging the expression of the bias\ngiven in lemma 5.3, yields the desired result. \u2737\nThe asymptoti\n\nvarian e has already been derived in theorem 3.3,\n\nV0 := V ar(f\u02c6(y|x)) = 1/(na2n )g(y)f (y|x)||K||22 + o(1/(na2n )).\nTogether with the\n\nomputation of the asymptoti\n\nmean squared error as a\n\nbias, we get the asymptoti\n\norollary:\n\nCorollary 3.6 With the previous assumptions, the Asymptoti Mean Squared\nError (AMSE) E0 at (x, y) is\nE0 := B02 + V0\n1\na4n g 2(y) (Bk (c, x, y))2 g(y)f (y|x)||K||22\n+ o a4n + 2\n+\n=\n2\n4\nnan\nnan\n\n11\n\n!\n\n\fwhi h gives, for the hoi e of the usual bandwidths mentioned above,\n2\nBK\n(c, x, y)\n+ c(F (x), G(y))||K||22 + o(n\u22122/3 ).\ng (y)\n4\n\n!\n\n\u22122/3 2\n\nE0 = n\n\n4 Comparison with other estimators\n4.1 Presentation of alternative estimators\nFor\n\nonvenien e, we re all below the de\u001cnition of other estimators of the\n\non-\n\nditional density en ountered in the literature and summarize their bias and\nf\u02c6ni (y|x) by Ei\nand its varian e by Vi .\nvarian e properties. We will note the bias of the ith estimator\n\n(1)\n\nDouble kernel estimator: as de\u001cned in the introdu\n\ntion se tion of our\n\npaper by the following ratio,\n\nf\u02c6n(1) (y|x) :=\n\nwhere\n\n\u2022\n\nh1\n\nand\n\nh2\n\ni=1\n\nKh\u2032 1 (Xi \u2212 x)Kh2 (Yi \u2212 y)\n1\nn\n\nn\nP\n\ni=1\n\nKh\u2032 1 (Xi \u2212 x)\n\n.\n\nare the bandwidths. One then have, see e.g. [17\u2104,\n\nh21 m2 (K)\n2\n\u0010\n\n+ o h21 + h22\nVarian e:\n\nV1 =\n(2)\n\nn\nP\n\nBias:\n\nB1 =\n\n\u2022\n\n1\nn\n\n\uf8eb\n\n\uf8ed2\n\n\u2032\n\n2\n\nf (x) \u2202f (y|x) \u2202 f (y|x)\nh2\n+\n+\n2\nf (x) \u2202x\n\u2202x\nh1\n\n\u0011\n\n!2\n\n\uf8f6\n\n2\n\n\u2202 f (y|x) \uf8f8\n\u2202y 2\n\n\u0011\n1\nkKk22 f (y|x) \u0010\nkKk22 \u2212 h2 f (y|x) + o\nnh1 h2 f (x)\nnh1 h2\n\u0012\n\n\u0013\n\n\u00132\n\nKh\u2032 1 (Xi \u2212 x),\n\nLo al polynomial estimator: Set\nR(\u03b8, x, y) :=\n\nn \u0012\nX\ni=1\n\nKh2 (Yi \u2212 y) \u2212\n\nXr\n\n\u03b8 (Xi \u2212 x)j\nj=0 j\n\nthen the lo al polynomial estimator is de\u001cned as\n\nf\u02c6n(2) (y|x) := \u03b8\u03020 ,\nwhere\n\n\u03b8\u0302xy := (\u03b8\u03020 , \u03b8\u03021 , . . . , \u03b8\u0302r )\n\nis the value of\n\n\u03b8\n\nwhi h minimizes\n\nR(\u03b8, x, y).\n\nThis lo al polynomial estimator, although it has a superior bias than\n\n12\n\n\fthe kernel one, is no longer restri ted to be non-negative and does not\nintegrate to 1, ex ept in the spe ial\n\nase\n\nr = 0.\n\nFrom results of [8\u2104, we\n\nget for the lo al linear estimator (see also [7\u2104 p. 256),\n\n\u2022\n\nBias:\n\nB2 =\n\u2022\n(3)\n\nh21 m2 (K \u2032 ) \u2202 2 f (y|x) h22 m2 (K) \u2202 2 f (y|x)\n+\n+ o(h21 + h22 )\n2\n\u2202x2\n2\n\u2202y 2\n\nVarian e:\n\n1\n||K||22||K \u2032 ||22 f (y|x)\n+o\nV2 =\nnh1 h2 f (x)\nnh1 h2\n\u0012\n\nLo al parametri estimator: As in [18\u2104 and [7\u2104, set\nR1 (\u03b8, x, y) :=\n\nn\nX\ni=1\n\nA(x, \u03b8) = l\n+\nmapping R 7\u2192 R ,\n\nwhere\n\n\u0013\n\n(Kh2 (Yi \u2212 y) \u2212 A(Xi \u2212 x, \u03b8))2 Kh\u2032 1 (Xi \u2212 x)\n\n\u0010P\nr\n\n\u0011\n\nj\nj=0 \u03b8j (Xi \u2212 x)\ne.g. l(u) = exp(u).\n\nand\n\nl(.)\n\nis a monotoni\n\nfun tion\n\nThen,\n\nf\u02c6n(3) (y|x) := A(0, \u03b8\u0302) = l(\u03b8\u03020 ).\n\u2022\n\nBias:\n\nB3 =\n\nh22 m2 (K) \u2202 2 f (y|x)\n\u2202 2 f (y|x) \u2202 2 A(0, \u03b8xy )\n+\n\u2212\n\u2202x2\n\u2202x2\n2\n\u2202y 2\n!\n\nh21 \u03b7(K \u2032 )\n\n+ o(h21 + h22 )\n\u2022\n\nVarian e:\n\n1\n\u03c4 (K, K \u2032 )2 f (y|x)\n+o\nV3 =\nnh1 h2 f (x)\nnh1 h2\n\u0012\n\nwhere\n(4)\n\n\u03b7\n\nand\n\n\u03c4\n\nare kernel dependent\n\n\u0013\n\nonstants.\n\nConstrained lo al polynomial estimator:\n\nA simple devi e to for e\n\nthe lo al polynomial estimator to be positive is to set\n\n\u03b80 = exp(\u03b1)\n\nin\n\nthe de\u001cnition of R0 to be minimized. The onstrained lo al polynomial\nf\u02c6n4 (y|x) is then de\u001cned analogously as the lo al polynomial\n2\nestimator f\u02c6n (y|x). We have, as in [18\u2104 and [7\u2104:\nestimator\n\n\u2022\n\nBias:\n\nB4 := h21\n\u2022\n\n2\nm2 (K \u2032 ) \u2202 2 f (y|x)\n2 m2 (K) \u2202 f (y|x)\n+\nh\n+ o(h21 + h22 )\n2\n2\n\u2202x2\n2\n\u2202y 2\n\nVarian e:\n\nkKk22 f (y|x)\n1\nV4 =\n+o\nnh1 h2 f (x)\nnh1 h2\n\u0012\n\n13\n\n\u0013\n\n\f4.2 Asymptoti Bias and Varian e omparison\nAll estimators have (hopefully) the same order\ntoti\n\nn\u22121/3 and n\u22122/3 in their asymp-\n\nbias and varian e terms, for the usual bandwidths\n\ndi\u001beren e lies in the\n\nhoi e. The main\n\nonstant terms whi h depend on unknown densities.\n\nBias: Contrary to all the alternative estimators whose bias involves derivatives\nof the full\n\nonditional density, one\n\ninvolves the density of\n\nY\n\nan note that our estimator's bias only\n\nand the derivatives of the\n\nopula density. To make\n\nthings more expli it, the terms involved, e.g. in the lo al polynomial estimator,\nwrite themselves as the sum of the derivatives of the\n\nh\u22122\nn B2 \u2248\n\nonditional density,\n\n\u2202 2 f (y|x) \u2202 2 f (y|x)\n+\n\u2202x2\n\u2202y 2\n\nthat is to say,\n\nh\u22122\nn B2\n\nwhereas our\n\n\u2202c(F (x), G(y))\n\u2202 2 c(F (x), G(y))\n2\n\u2248 f (x)g(y)\n+ f (x)g(y)\n\u2202u\n\u2202u2\n2\n\u2202c(F (x), G(y))\n\u2202 c(F (x), G(y))\n+ 2g \u2032(y)g(y)\n+ g 3 (y)\n\u2202v\n\u2202v 2\n\u2032\n\n(g(y)/2)BK (c, x, y)\n\nterm, modulo the\n\nonstants involved by the\n\nkernel, is written as\n\na\u22122\nn B0\nIt then be omes\n\n\u2202 2 c(F (x), G(y)) \u2202 2 c(F (x), G(y))\n\u2248 g(y)\n.\n+\n\u2202u2\n\u2202v 2\n!\n\nlear that we have a simpler expression, with less unknown\n\nterms, as is the ase for ompetitors whi h do involve the density\n\u2032\n\u2032\nderivative f of X and the derivative g of the Y density.\nIn a \u001cxed bandwidth and asymptoti\n\nontext, it seems di\u001e ult to\n\nf\n\nand its\n\nompare\n\nfurther. Nonetheless, we believe this feature of our estimator would be pra tially relevant when it\n\nomes to\n\nhoosing the bandwidths. Indeed, bandwidth\n\nsele tion is usually performed by minimizing lo al or global asymptoti\nriteria su h as Asymptoti\n\nMean Square Error (AMSE) or Asymptoti\n\nerror\nMean\n\nIntegrated Square Error (AMISE), in whi h unknown terms have to be estimated. Sin e in our approa h, the asymptoti\nunknown terms, we expe t that a higher a\n\nbias and varian e involve less\n\nura y\n\nould be obtained in this\n\npre-estimation stage. Moreover, by having managed to separate the estimation\nproblem of the marginal from the\n\nopula density, we\n\nould use known optimal\n\ndata-dependent bandwidths sele tion pro edures for density estimation su h\nas\n\nross validation, separately for the density of\n\nY\n\nand for the\n\nopula density.\n\nRemark 4 Sin e the opula density c has a ompa t support [0, 1]2 , our estimator may su\u001ber from bias issues on the boundaries, i.e. in the tails of X and\n14\n\n\fY . To\n\norre t these issues, one ould apply one of the several known te hniques\nto redu e the bias of the kernel estimator on the edges (see e.g [7\u2104 hapter 5.5,\nboundary kernels, re\u001de tion, transformation and lo al polynomial \u001ctting). In\nthe tail of the distribution of X , this bias issue in the opula density estimator\nis balan ed by the improved varian e, as shown below.\n\nVarian e:\ng(y)\n\nof\n\nY\n\nThe varian e of our estimator involves a produ t of the density\n\nby the\n\nonditional density\n\nf (y|x),\n\nna2n V0 \u2248 g(y)f (y|x) = g 2 (y)c(F (x), G(y)\nwhereas\n\nompetitors involve the ratio of\n\nf (y|x)\n\nby the density\n\nf (x)\n\nof\n\nX\n\ng(y)\nf (y|x)\n=\nc(F (x), G(y)).\nf (x)\nf (x)\nIt is a remarkable feature of the estimator we propose, that its varian e does\nnot involve dire tly\nbution to\n\nf (x), as is the\n\nY , through the\n\nthe introdu tion of the\n\nase for the\n\nompetitors, but only its\n\nontri-\n\nopula density. This re\u001de ts the ability announ ed in\n\nopula representation to have e\u001be tively separated the\n\nrandomness pertaining to\n\nY\n\nalone, from the dependen e stru ture of\n\n(X, Y ).\n\nMoreover, our estimator also does not su\u001ber from the unstable nature of\npetitors who, due to their intrinsi\nfor small value of the density\n\nf (x),\n\ne.g. in the tail of the distribution of\n\nom-\n\nratio stru ture, get an explosive varian e\nmaking\n\nonditional estimation di\u001e ult,\n\nX.\n\nRemark 5 To make estimators omparable, we have restri ted ourselves to\nso- alled \u001cxed bandwidths estimators, i.e. nonparametri estimators where the\nbandwidths are of the generi form hn = bn\u03b1 or hn = b(ln n/n)\u03b1 with \u03b1\nand b real numbers. Improved behavior for all the pre eding estimators an be\nobtained with data-dependent bandwidths where hn = Hn (X1 , . . . , Xn , x) an\nbe fun tions of the lo ation and of the data.\n4.3 Finite sample numeri al simulation\n4.3.1 Pra ti al implementation of the estimator\nAlthough the proposed estimator seems to\nsome pitfalls linked to the\n\nompare favorably asymptoti ally,\n\nopula density estimation may show up in the\n\npra ti al implementation:\n\nIn\u001cnities at the orners:\ntheir\n\nmany\n\nopula densities exhibit in\u001cnite values at\n\norners. Therefore, to avoid that\n\n(Fn (Xi ), Gn (Yi )) be equal to (1, 1), we\nFn and Gn to n/(n + 1)Fn and\n\nhange the empiri al distribution fun tions\n\nn/(n + 1)Gn\n\nrespe tively.\n\n15\n\n\fBoundary bias:\n\nsin e the\n\nopula density is of\n\nompa t support\n\n[0, 1]2 ,\n\nthe\n\nkernel method of estimation may su\u001ber from boundary bias. To alleviate this\nissue, we suggest to use boundary- orre ted kernels su h as the beta kernels\n\nKx,b (t) = \u03b2x/b+1,(1\u2212x)/b+1 (t),\n\nwhere\n\n\u03b2a,b (t)\n\ndenotes the pdf of a Beta(a,b) dis-\n\ntribution, advo ated by Chen [2\u2104, and used e.g. by [14\u2104 for estimating loss\ndistributions. The modi\u001ced opula density pseudo estimator is thus de\u001cned as\nP\ncn (u, v) = n\u22121 ni=1 Ku,an (Ui )Kv,an (Vi ).\n\nBandwidth sele tion:\n\nperforman e of nonparametri\n\nru ially on the bandwidths. For\n\nestimators depends\n\nonditional density, bandwidth sele tion is a\n\nmore deli ate matter than for density estimation due to the multidimensional\nnature of the problem. Moreover, for ratio-type estimators, the di\u001e ulty is\nin reased by the lo al dependen e of the bandwidths\nditioning near\n\nx.\n\nFor the\n\non\n\nhx\n\nimplied by\n\nopula estimator, a supplemental issue\n\nthe fa t that the pseudo-data\ntion of the AMISE of the\n\nhy\n\nF (Xi ), G(Yi )\n\non-\n\nomes from\n\nis not dire tly a\n\nessible. Inspe -\n\nopula-based estimator suggest we\n\nan separate the\n\nh for \u011d(y) from the bandwidth hoi e of an the opula\n\u0109n . A rationale for a data-dependent method is to separately\nsele t h on the Yi data alone (e.g. by ross-validation or plug-in), from the an\nof the opula density c based on the approximate data Fn (Xi ), Gn (Yi ). How-\n\nbandwidth\n\nhoi e of\n\ndensity estimator\n\never, su h a bandwidth sele tion would require deeper analysis and we leave a\ndetailed study of a pra ti al data-dependent method for bandwidth sele tion\nof the\n\nopula-quantile estimator, together with a global and lo al\n\nomparison\n\nof the estimators at their respe tive optimal bandwidths for further resear h.\n\n4.3.2 Model and omparison results\nWe simulated a sample of\n\nX, Y\n\nn = 100 variables (Xi , Yi ), from the following model:\nN (0, 1) and linked via Frank Copula .\n\nis marginally distributed as\n\nln[(\u03b8 + \u03b8u+v \u2212 \u03b8u \u2212 \u03b8v )/(\u03b8 \u2212 1)]\nC(u, v, \u03b8) =\nln \u03b8\nwith parameter\n\n\u03b8 = 100.\n\nWe restri ted ourselves to simple, \u001cxed for all\n\nx, y ,\n\nrule-of-thumb methods\n\nan\nFn (Xi ).\n\nbased on Normal referen e rule to get a \u001crst pi ture. For the sele tion of\nof the\n\nopula density estimator, we applied S ott's Rule on the data\n\nWe used Epane hnikov kernels for\nthe\nand\n\n\u011d(y)\n\nand the other estimators. We plotted\n\nonditional density along with its estimations on the domain\n\ny \u2208 [\u22123, 3]\n\non \u001cgure 1. A\n\nomparison plot at\n\n16\n\nx=2\n\nx \u2208 [\u22125, 5]\n\nis shown on \u001cgure 2.\n\n\fFigure 1. 3D Plots. From left to right, top to bottom: true density, quantile- opula\nestimator, double kernel, lo al polynomial ( lipped).\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\ny\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\nFigure 2. Comparison at x=2: onditional density=thi k urve, quantile- opula= ontinuous line, double kernel=dotted urve, lo al polynomial=dashed urve.\n17\n\n\f4.3.3 Clipping and Estimation in the tails\nAs mentioned earlier, as the performan e of the estimators depends on the\nperforman e of the bandwidths sele tion method, it is deli ate to give a\nlusive answer. However, we would like to illustrate at least one\nthe proposed estimator\n\nlearly outperforms its\n\non-\n\nase where\n\nompetitors. Indeed, one major\n\nissue of alternative estimators already mentioned is their numeri al explosion\nwhen the estimated density\nof\n\nf\u02c6(x)\n\nis\n\nlose to zero. In parti ular, if the kernel is\n\nompa t support, the denominator is zero for the\n\nlosest\n\nXi\n\nx whose distan\n\ne from the\n\nex eeds half the bandwidth times the length of the support, thereby\n\nallowing estimation only on a\n\nlosed subset of\n\nX\n\nin luded in\n\n[min Xi , max Xi ].\n\nThis is one of the reason why simulation studies are often performed either\nwith a marginal\n\nX\n\ndensity of bounded support and/or with a Gaussian ker-\n\nnel. Note that the problem remains with a Gaussian kernel sin e the estimated\ndensity\n\nan be ome qui kly lower than the ma hine pre ision. To prevent from\n\nthis numeri al explosion, the de\u001cnition of the\n\nonditional density estimators\n\nhave to be modi\u001ced either by\n\nf\u02c6(y|x) =\n\nwhere\n\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\nf\u02c6XY (x,y)\nif\nf\u02c6X (x)\n\n\u00e2(y)\n\nif\n\nf\u02c6X (x) > c\nf\u02c6X (x) = 0\n\nc > 0 is an arbitrary amount of\n\nestimator (usually\n\nf\u02c6(y|x) =\n\nlipping, and\n\nhosen to be zero or\n\nAn illustration of these issues\n\nor by,\n\nf\u02c6XY (x, y)\nmax{f\u02c6(x), c}\n\n\u00e2(.) is an arbitrary density\n\n\u011d(y)).\n\nlearly appears in \u001cgure 1. The un lipped version\n\nof the double kernel estimator is unable to estimate the\n\nonditional density for\n\n|x| roughly > 3, and the lipped version of the lo al polynomial estimator with\nc = 0.00001 and \u00e2(y) = \u011d(y) gives a wrong estimation in the tails, re\u001de ting\nthe arbitrary\n\nhoi es in the\n\nlipping de ision. To the\n\nopula estimator is surprisingly able to estimate the\nat lo ations\n\nX.\n\nx\n\nontrary, the quantile-\n\nonditional density\n\nAn explanation of this apparently paradoxal phenomenon\n\nfa t that the estimator is partially based on the ranks of\nit\n\nan re over \u0010hidden\u0011 information on the density of\n\nthe pairs\n\nf (y|x)\n\nwhere there is \u0010no data\u0011, i.e. in the tails of the distribution of\n\n(Xi , Yi ).\n\nX\n\nXi\n\nomes from the\n\nand\n\nYi . Therefore,\n\nfrom the ordering of\n\nSee Ho\u001b [16\u2104 for a detailed explanation. We believe that\n\nthis feature might be of potential interest for appli ations, e.g. in statisti al\ninferen e of extreme values and rare events.\n\nDis ussion\nThe quantile transform and use of the\n\nopula formula has thus turned the\n\non-\n\nditional density formula (1) of the ratio type into the produ t one (3). This\n\n18\n\n\fformula was the ba kbone of our arti le where this produ t form appeared\nto be espe ially appealing for statisti al estimation:\nsults where obtained by simple\n\nonsisten y and limit re-\n\nombination of the previous known ones on\n\n(un onditional) density estimation. The estimator obtained shows interesting\nasymptoti\n\nbias and varian es properties\n\nompared to\n\nits \u001cnite sample implementation does not give yet a\n\nompetitors. Although\n\nlear and\n\non lusive pi -\n\nture, it already yields some promising results, e.g. for estimation in the tails\nof\n\nX,\n\nwhere the proposed estimator does not su\u001ber from\n\nlipping issues.\n\n5 Appendix : auxiliary results\nIn this se tion, we gather some preliminary results whi h we will need as basi\ntools for the demonstrations of se tion 3. In subse tion 5.1, we re all\nresults about the\n\nlassi al\n\nonvergen e of the Kolmogorov-Sminorv statisti . Next, we\n\nmake a brief overview of kernel density estimation and apply these results to\nthe estimators\n\n\u011dn\n\n(se tion 5.2) and\n\napproximation lemmas of\n\n\u0109n\n\nby\n\ncn\n\ncn\n\n(se tion 5.3). Eventually, we need two\n\nin se tions 5.4 and 5.5.\n\n5.1 Approximation of the pseudo-variables F (Xi ) by their estimates Fn (Xi )\n(Xi , i = 1, . . . , n) an i.i.d. sample of a real random variable X with ommon\n.d.f. F , the Kolmogorov-Smirnov statisti is de\u001cned as Dn := kFn \u2212 F k\u221e .\n\nFor\n\nGlivenko-Cantelli, Kolmogorov and Smirnov, Chung, Donsker among others\nhave studied its\n\nonvergen e properties in in reasing generality (See [28\u2104 and\n\n[36\u2104 for re ent a\n\nounts). For our purpose, we only need to formulate these\n\nresults in the following rough form:\n\nLemma 5.1 For an i.i.d. sample from a ontinuous .d.f. F ,\nkFn \u2212 F k\u221e\n\nln ln n \uf8f8\n= Oa.s. \uf8ed\nn\n\nkFn \u2212 F k\u221e = OP\nSin e\n\nF\n\nis unknown, the random variables\n\nFn (Xi ).\n\n(9)\n\n!\n\n1\n\u221a\n.\nn\n\nUi = F (Xi )\n\nonsequen e of the pre eding lemma 5.1, one\nvariables by the statisti s\n\n\uf8f6\n\n\uf8ebs\n\n(10)\n\nare not observed. As a\n\nan naturally approximate these\n\nIndeed,\n\n|F (Xi ) \u2212 Fn (Xi )| \u2264 sup |F (x) \u2212 Fn (x)| = kFn \u2212 F k\u221e\nx\u2208R\n\n19\n\na.s.\n\n\fThus,\n\n|F (Xi )\u2212Fn (Xi )| is no more than an OP ((ln ln n/n)1/2 ) or an Oa.s. (n\u22121/2 ).\n\nThese rates of approximation appears to be faster than those of statisti al\nestimation of densities, as is shown in the next subse tion.\n\n5.2 Convergen e of the kernel density estimator \u011dn\nWe re all below some\n\nlassi al results about the\nestimator f\u02c6n\n\nRosenblatt kernel non-parametri\n\nonvergen e of the Parzen-\n\nof a d-variate density. Sin e its\n\nin eption by Rosenblatt [25\u2104 and Parzen [22\u2104, it has been studied by a great\ndeal of authors. See e.g. S ott [27\u2104, Prakasa Rao [23\u2104, Nadaraya [21\u2104 for details.\nSee also Bosq [1\u2104\n\nhapter 2.\n\nIt is well known that the bias of the kernel density estimator depends on the\ndegree of smoothness of the underlying density, measured by its number of\nderivatives or its Lips hitz order. In order to get the\n\nontinuous (See [22\u2104). To get\n\nto zero, it su\u001e es to assume that the density is\nfurther information on the rate of\n\nonvergen e of the bias\n\nonvergen e of the estimator, it is ne essary\n\nto make further assumptions. Moreover, for kernel fun tions with unbounded\nsupport, the rate of\n\nonvergen e also depends on the tail behavior of the\n\nkernel (See Stute [31\u2104). Therefore, for\nnotations, we will make the\n\nlarity of exposition and simpli ity of\n\nustomary assumptions that the density is twi e\n\ndi\u001berentiable and that the kernel is of bounded support. We then have the\nfollowing results:\n\n\u2022\n\nBias: With the previous assumptions, for a\nhn \u2192 0 and nhdn \u2192 \u221e entail that\n\nh2n Z\n\u02c6\nE fn (x) = f (x) +\n2 d\nWith the multivariate kernel\n\nx\n\nin the interior of\n\nR\n\n\u2202 2 f (x)\nzi zj K(z)dz + o(h2n ).\n\u2202x\n\u2202x\ni\nj\n1\u2264i,j\u2264d\n\nK\n\nas a produ t of\n\nX\n\nd\n\norder one kernels\n\nabove sum redu es to the diagonal terms.\n\nE f\u02c6n (x) = f (x) +\n\u2022\n\nh2n X\n\u2202 2 f (x)\n+ o(h2n ).\nm2 (Ki )\n2 1\u2264i\u2264d\n\u2202x2i\n\nVarian e: with the same assumptions,\n\n!\n\n1\nf (x)\nkKk22 + o\n.\nV ar f\u02c6n (x) =\nd\nnhn\nnhdn\nh\n\n\u2022\n\nPointwise asymptoti\n\nq\n\nsupp(f ),\n\ni\n\nnormality: under the previous\n\nonditions,\n\nd\nnhdn f\u02c6n (x) \u2212 E f\u02c6n (x) \u2740 N (0, f (x) kKk22 ).\n\n\u0010\n\n\u0011\n\n20\n\nKi ,\n\nthe\n\n\fFor a hoi e of the bandwidth as\n\nhn \u2243 n\u22121/(d+4) , whi\n\nh realizes the optimal\n\u22122/(d+4)\ntrade-o\u001b between the bias and varian e, one gets the rate n\n, whi h\nis the optimal speed of\n\n\u2022\n\nonvergen e in the minimax sense in the\n\nlass of\n\ndensity fun tions with bounded se ond derivatives, a ording to [30\u2104.\nd\nPointwise almost sure onvergen e: if moreover nhn /(ln ln n) \u2192 \u221e (see [3\u2104),\nwe have that\ns\n\n!\n\nln ln n\n.\nnhdn\n\nf\u02c6n (x) \u2212 E f\u02c6n (x) = Oa.s.\n\nhoi e of the bandwidth as hn\n2/(d+4)\nonvergen e ((ln ln n)/n)\n:\n\nFor a\nof\n\n\u2243 ((ln ln n)/n)1/(d+4) ,\n\uf8eb\n\nln ln n\nf\u02c6n (x) \u2212 f (x) = Oa.s. \uf8ed\nn\nApplied to our\n\nase (d\n\n= 1),\n\nwe\n\nwe get the rate\n\n!2/(d+4) \uf8f6\n\n\uf8f8.\n\nan summarize these results for further ref-\n\neren e in the following lemma for the estimator\n\n\u011dn\n\nof the density\n\ng\n\nof\n\nY:\n\nLemma 5.2 With the previous assumptions, for a point y in the interior of\nthe support of g , and a bandwidth hosen su h as hn \u2243 n\u22121/5 , we have\n|\u011dn (y) \u2212 g(y)| = Op (n\u22122/5 )\nd\n\n\u0011\n\n\u0010\n\nn2/5 [\u011dn (y) \u2212 g(y)] \u2740 N 0, g(y) kK0 k22 .\n\nWith the same assumptions, but for a bandwidth hoi e of hn \u2243 (ln ln n/n)1/5 ,\n\u011dn (y) \u2212 g(y) =\n\n\uf8eb\n\n!2/5 \uf8f6\nln ln n\n\uf8f8.\nOa.s. \uf8ed\n\n(11)\n\nn\n\n5.3 Convergen e of cn (u, v)\nAs mentioned before, the assumptions that\nstri tly in reasing entail that\n\n(U, V ) := (F (X), G(Y )).\n\nc\n\nF\n\nand\n\nG\n\nbe di\u001berentiable and\n\nis the density of the transformed variables\n\nTherefore, on e one\n\nis simply the kernel density estimator of the bivariate density\npseudo-variables\n\n(U, V ),\n\none dire tly draws its\n\ncn (u, v)\nc(u, v) of the\n\nonvin es oneself that\n\nonvergen e properties by ap-\n\nplying the results of the pre eding subse tion with\n\nd = 2:\n\nLemma 5.3 For a hoi e of an \u2243 n\u22121/6 , for every (u, v) \u2208 (0, 1)2 , similar\nresults of those of lemma 5.2 hold for \u0109n with a rate of onvergen e of n\u22121/3\nand (ln ln n/n)1/3 respe tively.\n21\n\n\f5.4 An approximation lemma of \u0109n (u, v) by cn (u, v)\nThe lemma of this se tion gives the rate of approximation of the kernel\ndensity estimator\nanalogue\n\ncn (u, v)\n\n\u0109n (u, v)\n\nopula\n\n(Fn (Xi ), Gn (Yi )) by its\n(Ui , Vi ) := (F (Xi ), G(Yi )). A\n\nomputed on the real data\n\nomputed on the pseudo-data\n\nsimilar result, but with a di\u001berent proof, has been obtained in Fermanian [9\u2104\ntheorem 1.\n\nLemma 5.4 Let (u, v) \u2208 (0, 1)2. If the kernel K(u, v) = K1 (u)K2 (v) is twi e\ndi\u001berentiable with bounded se ond derivatives, then\n|\u0109n (u, v) \u2212 cn (u, v)| =\n\noP (a2n\n\n|\u0109n (u, v) \u2212 cn (u, v)| = oa.s.\n\nProof.\n1\nna2n\n\nn\nP\n\ni=1\n\nWe note\n\n\u2206i,n (u, v)\n\n||.||\n\n+ 1/ na2n )\ns\n\na norm for ve tors. Set\n\nwith\n\nq\n\nln ln n\nna2n\n\n!\n\n\u2206 := \u0109n (u, v) \u2212 cn (u, v) =\n\n!\n\nu \u2212 Fn (Xi ) v \u2212 Gn (Yi )\nu \u2212 F (Xi ) v \u2212 G(Yi )\n\u2206i,n (u, v) := K\n\u2212K\n,\n,\nan\nan\nan\nan\n\n!\n\nand de\u001cne\n\n\uf8eb\n\n\uf8f6\n\n\uf8ec F (Xi ) \u2212 Fn (Xi ) \uf8f7\n\nZi,n := \uf8ed\n\nG(Yi ) \u2212 Gn (Yi )\n\n\uf8f8.\n\n|Fn (Xi ) \u2212 F (Xi )| \u2264 ||Fn \u2212 F ||\u221e and |Gn (Yi ) \u2212\nG(Yi )| \u2264 ||Gn \u2212 G||\u221e a.s. for every i = 1, . . . , n. Lemma 5.1 thus entails that\nthe norm of Zi,n is independent of i and su h that\nAs mentioned in se tion 5.1,\n\n\u221a\n||Zi,n || = OP (1/ n) , i = 1, . . . , n\n\n(12)\n\nq\n\n||Zi,n || = Oa.s. ( ln ln n/n) , i = 1, . . . , n\nNow, for every \u001cxed\n\n(u, v) \u2208 [0, 1]2 ,\n\nsin e the kernel\n\nK is twi e di\u001berentiable,\n\u0168i,n and \u1e7ci,n su h that,\n\nthere exists, by Taylor expansion, random variables\nalmost surely,\n\nn\nu \u2212 F (Xi ) v \u2212 G(Yi )\n1 X\nT\nZi,n\n\u2207K\n,\n\u2206= 3\nnan i=1\nan\nan\n\n!\n\nn\n1 X\nu \u2212 \u0168i,n v \u2212 \u1e7ci,n\nT\n+\n,\nZi,n := \u22061 + \u22062\nZi,n\n\u22072 K\n4\n2nan i=1\nan\nan\n\n!\n\n22\n\n(13)\n\n\fwhere\n\nT\nZi,n\n\nZi,n\n\ndenotes the transpose of the ve tor\n\nand\n\n\u2207K\n\nand\n\n\u22072 K\n\ngradient and the Hessian respe tively of the multivariate kernel fun tion\n\n\u2207K =\n\n\uf8eb\n\uf8ec\n\uf8ed\n\n\u2202K\n\u2202u\n\u2202K\n\u2202v\n\n\uf8f6\n\n,\n\n\uf8f7\n\uf8f8\n\n2\n\n\u2207K=\n\n\uf8eb\n\n\u22022K\n\u2202u\u2202v\n\n\u22022K\n\u2202u2\n\n\uf8ec\n\uf8ed\n\n\u22022K \u22022K\n\u2202u\u2202v \u2202v2\n\nthe\n\nK\n\n\uf8f6\n\uf8f7\n\uf8f8\n\nNegligibility of \u22062 : By the boundedness assumption on the se\n\nond-order deriva-\n\ntives of the kernel, and equations 12 and 13,\n\n\u22062 = OP\n\nNegligibility of \u22061 : By\n\n1\nna4n\n\n!\n\nand\n\n!\n\nln ln n\n.\nna4n\n\n\u22062 = Oa.s.\n\nentering at expe tations,\n\nn\nu \u2212 F (Xi )\nu \u2212 F (Xi )\n1 X\nT\nZi,n\n\u2207K\n, . . . \u2212 E\u2207K\n,...\n\u22061 = 3\nnan i=1\nan\nan\n!\nn\n1 X\nu \u2212 F (Xi ) v \u2212 G(Yi )\nT\n+ 3\nZ E\u2207K\n:= \u220611 + \u220612\n,\nnan i=1 i,n\nan\nan\n\n!\n\n!!\n\nNegligibility of \u220612 : Bias results on the bivariate gradient kernel estimator (See\nS ott [27\u2104\n\nhapter 6) entail that\n\nu \u2212 F (Xi ) v \u2212 G(Yi )\nE\u2207K\n,\nan\nan\n\n!\n\n= a3n \u2207c (u, v) + O(a5n )\n\nCau hy-S hwarz inequality yields that\n\nu \u2212 F (Xi ) v \u2212 G(Yi )\n,\nan\nan\n\nn||Zi,n ||\n|\u220612 | \u2264\nE\u2207K\nna3n\n\n!\n\nIn turn, with equations 12 and 13,\n\n\u221a\n\u220612 = OP (1/ n)\n\nNegligibility of \u220611 : Set Ai = \u2207K\n\nand\n\n\u0010\n\nq\n\n\u220612 = Oa.s ( ln ln n/n).\n\u0011\n\nu\u2212F (Xi )\n,...\nan\n\n\u2212 E\u2207K\n\nn\n||Zn || X\n||Ai ||\n|\u220611 | \u2264\nna3n i=1\n\n\u0010\n\n\u0011\n\nu\u2212F (Xi )\n, . . . . Then,\nan\n\n||Ai || \u2264 2C\nbounded by M ,\n\nBoundedness assumption on the derivative of the kernel imply that\na.s. We apply Hoe\u001bding inequality for independent,\nbut non identi ally distributed random variables\n\nn\nX\n\nentered,\n\n(\u03b7j )\n\n(e.g. see [1\u2104),\n\nt2\nP ( \u03b7j > t) \u2264 exp \u2212\n.\n2nM 2\nj=1\n!\n\n23\n\n(14)\n\n\fHere, for every\n\n\u01eb > 0,\n\nwith\n\nwe get that\n\nP\n\nM = 2C , \u03b7i = ||Ai || \u2212 E||Ai||, t = \u01ebn1/2 (ln ln n)1/2 ,\n\n\u0011\n\u221a\n\u01eb2 ln ln n\n(||Ai || \u2212 E||Ai ||) > \u01eb n ln ln n 6 exp \u2212\ni=1\n4M 2\n\n\u0010 Xn\n\nwith a \u03b4 > 0 and where the r.h.s. goes to zero as\n\u221a\nPn\ni=1 (||Ai || \u2212 E||Ai ||) = OP ( n ln ln n).\n\n!\n\n=\n\nn \u2192 \u221e.\n\n1\n(ln n)\u03b4\nTherefore,\n\nFor the almost sure negligibility, we get similarly by inequality 14 that, for\n\u01eb > 0, with t = \u01ebn(1+\u03b4)/2 and \u03b4 > 0,\n\nevery\n\nP\n\n\u0010 Xn\n\ni=1\n\n(1+\u03b4)/2\n\n(||Ai || \u2212 E||Ai ||) > \u01ebn\n\n\u0011\n\n\u2212\u01eb2 n\u03b4\n6 exp\n4M 2\n\n!\n\nand the series on the r.h.s is onvergent. In turn, the Borell-Cantelli lemma\nPn\n(1+\u03b4)/2\n).\nimply that\ni=1 (||Ai || \u2212 E||Ai ||) = Oa.s. (n\nIt remains to evaluate\n\nE||Ai ||.\n\nE||Ai || \u2264 2E||\u2207K((u \u2212\ne K is di\u001berentiable and of produ t form K(u, v)\nFirst, we have that\n\nF (Xi ))/an , . . .)||. Se ond, sin\n= K1 (u)K2 (v), ea h sub-kernel\n\nis of bounded variations and\n\nan be written\n\nas a di\u001beren e of two monotone in reasing fun tions. For example, set\nK1a \u2212 K1b and de\u001cne K \u2217 := (K1a + K1b )K2 . We have,\n\nK1 =\n\n\u0010\n\u0011\n\u2202K\n\u2202K \u2217\n6 |(K1a )\u2032 | + |(K1b )\u2032 | K2 = ((K1a )\u2032 + (K1b )\u2032 )K2 :=\n\u2202u\n\u2202u\nwhere the equality pro eeds from the positivity of the derivatives. As a\n\non-\n\nsequen e,\n\nE\n\n\u2202K\n\u2202K \u2217\n((u \u2212 F (Xi ))/an , . . .) \u2264 E\n((u \u2212 F (Xi ))/an , . . .)\n\u2202u\n\u2202u\n\nand similarly for the other partial derivative. The r.h.s. of the previous inequal3\nity is, after an integration by parts, of order an by the results on the kernel\nestimator of the gradient of the density (See S ott [27\u2104 hapter 6). Therefore,\nPn\n3\ni=1 E||Ai || = O(nan ).\nRe olle ting all elements, we eventually obtain that\n\n\u220611 = OP\n\u220611\n\n\u221a\n\nn ln ln n + na3n\n\u221a\nnna3n\n\n\uf8eb\n\nn(1+\u03b4)/2 + na3n\n= Oa.s. \uf8ed\nna3n\n\uf8ebs\n\n!\n\ns\n\n\u221a\n\n= OP\n\uf8f6\n\nln ln n \uf8f8\nn\n\n1\nln ln n\n+\n= Oa.s. \uf8ed\n2\n(1\u2212\u03b4)/2\nnan n\na2n\n\nln ln n\n1\n+\u221a\n3\nnan\nn\n\ns\n\n24\n\n\uf8f6\n\nln ln n \uf8f8\n= oa.s.\nn\n\n!\n\n\uf8eb\n\n1\n\n\uf8f6\n\n\uf8f8.\n= oP \uf8ed q\nna2n\n\ns\n\nln ln n\nna2n\n\n!\n\n\ffor\n\n\u03b4\n\nsmall enough (<\n\n1/3\n\nfor\n\nan \u2243 n\u22121/6 ). \u2737\n\n5.5 An approximation lemma for \u0109n (Fn (x), Gn (y)) by \u0109n (F (x), G(y))\nThe lemma of this subse tion gives the rate of deviation of the kernel\ndensity estimator\n\n\u0109n\n\nfrom a varying lo ation\n\n(Fn (x), Gn (y)) to a \u001cxed lo\n\nopula\nation\n\n(F (x), G(y)).\n\nLemma 5.5 With the same assumptions as in the pre eding lemma, we have\n\u0109n (Fn (x), Gn (y)) \u2212 \u0109n (F (x), G(y)) =\n\n\uf8eb\n\noP \uf8eda2n\n\n1\n\n\uf8f6\n\n\uf8f8\n+q\nna2n\n\uf8f6\n\n\uf8ebs\n\nln ln n \uf8f8\n\u0109n (Fn (x), Gn (y)) \u2212 \u0109n (F (x), G(y)) = Oa.s. \uf8ed\nn\n\nProof. We pro\n\need similarly as in the pre eding lemma. Set\n\n\u2206n (x, y) := \u0109n (Fn (x), Gn (y)) \u2212 \u0109n (F (x), G(y)) =\n\nn\n1 X\n\u2206\u2032 (x, y)\nna2n i=1 i,n\n\n(15)\n\nwith\n\n\u2206\u2032i,n (x, y)\n\nFn (x) \u2212 Fn (Xi ) Gn (y) \u2212 Gn (Yi )\n,\n:= K\nan\nan\n!\nF (x) \u2212 Fn (Xi ) G(y) \u2212 Gn (Yi )\n\u2212K\n,\nan\nan\n\n!\n\nand de\u001cne\n\n\uf8eb\n\n\uf8f6\n\n\uf8ec Fn (x) \u2212 F (x) \uf8f7\n\uf8f8\n\nZn (x, y) := \uf8ed\nWe \u001crst express\n\n\u2206\u2032i,n (x, y) at\n\nGn (y) \u2212 G(y)\n\na \u001cxed lo ation\n\n(F (x), G(y)) by\n\na Taylor expan-\n\nsion and by bounding uniformly the se ond order terms,\n\n\u2206\u2032i,n (x, y)\n\n=\n\n\u2207K\nZnT (x, y)\nan\n\n||Zn ||2\u221e\nF (x) \u2212 Fn (Xi ) G(y) \u2212 Gn (Yi )\nR1\n,\n+\nan\nan\na2n\n!\n\n(16)\n\nR1 is uniformly bounded almost surely: R1 = Oa.s. (1). We then go from\nthe data (Fn (Xi ), Gn (Yi )) to the pseudo but \u001cxed w.r.t. n data (F (Xi ), G(Yi )).\nwhere\n\n25\n\n\fBy a se ond Taylor expansion,\n\n!\n\n\u2207K F (x) \u2212 Fn (Xi ) G(y) \u2212 Gn (Yi )\n,\nan\nan\nan\n!\n\u2207K F (x) \u2212 F (Xi ) G(y) \u2212 G(Yi )\n=\n,\nan\nan\nan\n!\n2\n||Zn ||\u221e\nF (x) \u2212 F (Xi ) G(y) \u2212 G(Yi )\nT \u2207 K\n+\nR2 .\n,\n+ Zi,n 2\n2an\nan\nan\na2n\nwhere\n\nR2 = oa.s. (1)\n\nuniformly in i,\n\nx\n\nand\n\ny.\n\n(17)\n\nTherefore, plugging 16 and 17 in\n\n15, we get\n\nn\n\u2207K F (x) \u2212 F (Xi ) G(y) \u2212 G(Yi )\nZnT (x, y) X\n,\n\u2206n (x, y) =\n2\nnan i=1 an\nan\nan\n!\nn\nT\n2\nZn (x, y) X t \u2207 K F (x) \u2212 F (Xi ) G(y) \u2212 G(Yi )\n+\nZ\n,\nna2n i=1 i,n 2a2n\nan\nan\n2\n||Zn ||\u221e\n.\n+ R3\na4n\n\n!\n\nwith the remainder term\n\nR3 = Oa.s. (1)\n\nuniformly. As before, the properties of\n\nthe kernel (derivate) density estimator (See S ott [27\u2104\n\nn\n1 X\n\u2207K\nna3n i=1\n\nF (x) \u2212 F (Xi ) G(y) \u2212 G(Yi )\n,\nan\nan\n\n!\n\n=\n\nhapter 6) entails that\n\nOP (a2n\n\nq\n\n+ 1/ na4n ).\n\nTherefore, using 12 and bounding uniformly the Hessian, 15 be omes\n\n\uf8eb\n\n\u2206n (x, y) = OP \uf8eda2n ||Zn ||\u221e + q\nna4n\n\uf8eb\n\n\uf8f6\n\n1\n\n\uf8f6\n\n||Zn ||\u221e \uf8f8\n\n+ OP\n\n||Zn ||2\u221e\na4n\n\n!\n\n\uf8f8.\n= oP \uf8eda2n + q\n2\nnan\nSimilarly, one gets with 13 and the strong\ngradient of the density that\n\n\u2206n (x, y) =\n\nonsisten y of the estimator of the\n\u0011\n\u0010q\nln ln n\n. \u2737\nOa.s.\nn\n\nReferen es\nNonparametri statisti s for sto hasti pro esses, volume 110 of\nLe ture Notes in Statisti s. Springer-Verlag, New York, se ond edition,\n\n[1\u2104\n\nD. Bosq.\n\n[2\u2104\n\nS. X. Chen. Beta kernel estimators for density fun tions.\n\n1998. Estimation and predi tion.\n\nData Anal., 31(2):131\u0015145, 1999.\n\n26\n\nComput. Statist.\n\n\f[3\u2104\n\nP. Deheuvels.\n\nConditions n\u00e9 essaires et su\u001esantes de\n\nonvergen e\n\npon tuelle presque s\u00fbre et uniforme presque s\u00fbre des estimateurs de la\ndensit\u00e9.\n[4\u2104\n\nC. R. A ad. S i. Paris S\u00e9r. A, 278:1217\u00151220, 1974.\n\nP. Deheuvels. La fon tion de d\u00e9pendan e empirique et ses propri\u00e9t\u00e9s. Un\ntest non param\u00e9trique d'ind\u00e9pendan e.\n\n(5), 65(6):274\u0015292, 1979.\n[5\u2104\n\nP. Deheuvels.\n\nA Kolmogorov-Smirnov type test for independen e and\n\nmultivariate samples.\n\nRev. Roumaine Math. Pures Appl., 26(2):213\u0015226,\n\n1981.\n[6\u2104\n\nA ad. Roy. Belg. Bull. Cl. S i.\n\nL. Devroye and G. Lugosi.\n\nCombinatorial methods in density estimation.\n\nSpringer Series in Statisti s. Springer-Verlag, New York, 2001.\n[7\u2104\n\nJ. Fan and Q. Yao.\n\nNonlinear time series.\n\nSpringer Series in Statisti s.\n\nSpringer-Verlag, New York, se ond edition, 2005.\nparametri\n[8\u2104\n\nand\n\nmethods.\n\nJ. Fan, Q. Yao, and H. Tong. Estimation of\n\nonditional densities and sen-\n\nsitivity measures in nonlinear dynami al systems.\n\nBiometrika, 83(1):189\u0015\n\n206, 1996.\n[9\u2104\n\nNonparametri\n\nJ.-D. Fermanian. Goodness-of-\u001ct tests for\n\nopulas.\n\nJ. Multivariate Anal.,\n\n95(1):119\u0015152, 2005.\n[10\u2104 J.-D. Fermanian and S aillet O. Nonparametri\n\nJournal of Risk, 5(4):25\u001554, 2003.\n\ntime series.\n\nestimation of\n\n[11\u2104 J.-D. Fermanian, D. Radulovi\u00a2, and M. Wegkamp. Weak\nempiri al\n\nopula pro esses.\n\nopulas for\n\nonvergen e of\n\nBernoulli, 10(5):847\u0015860, 2004.\n\n[12\u2104 T. Gasser and H.-G. M\u00fcller. Kernel estimation of regression fun tions.\nIn Smoothing te hniques for urve estimation (Pro . Workshop, Heidelberg, 1979), volume 757 of Le ture Notes in Math., pages 23\u001568. Springer,\nBerlin, 1979.\n[13\u2104 I. Gijbels and J. Mielni zuk. Estimating the density of a\n\nComm. Statist. Theory Methods, 19(2):445\u0015464, 1990.\n\nopula fun tion.\n\n[14\u2104 J. Gustafsonn, M. Hagmann, J.P. Nielsen, and O. S aillet. Lo al transformation kernel density estimation of loss distributions.\n\nJournal of Business and E onomi Statisti s, 2007.\n\n[15\u2104 L. Gy\u00f6r\u001c and M. Kohler. Nonparametri\ntributions.\n\nonditional dis-\n\nIEEE Trans. Inform. Theory, 53(5):1872\u00151879, 2007.\n\n[16\u2104 P. D. Ho\u001b.\nestimation.\n\nestimation of\n\nForth oming in\n\nExtending the rank likelihood for semiparametri\n\nAnnals Appl. Stats., 1(1):265\u0015283, 2007.\n\nopula\n\n[17\u2104 R. J. Hyndman, D. M. Bashtannyk, and G. K. Grunwald. Estimating and\nvisualizing\n\nonditional densities.\n\nJ. Comput. Graph. Statist.,\n\n5(4):315\u0015\n\n336, 1996.\n[18\u2104 R. J. Hyndman and Q. Yao. Nonparametri\ntests for\n\nonditional density fun tions.\n\n278, 2002.\n\nestimation and symmetry\n\nJ. Nonparametr. Stat., 14(3):259\u0015\n\nMultivariate models and dependen e on epts, volume 73 of Monographs on Statisti s and Applied Probability. Chapman & Hall, London,\n\n[19\u2104 H. Joe.\n1997.\n\n27\n\n\f[20\u2104 C. La our.\n\nAdaptive estimation of the transition density of a markov\n\nAnn. Inst. H. Poin ar\u00e9 Probab. Statist., 43(5):571\u0015597, 2007.\nNonparametri estimation of probability densities and\nregression urves, volume 20 of Mathemati s and its Appli ations (Soviet\nSeries). Kluwer A ademi Publishers Group, Dordre ht, 1989. Translated\nhain.\n\n[21\u2104 \u00c8. A. Nadaraya.\n\nfrom the Russian by Samuel Kotz.\n[22\u2104 E. Parzen.\n[23\u2104\n\nOn estimation of a probability density fun tion and mode.\n\nAnn. Math. Statist., 33:1065\u00151076, 1962.\nB. L. S. Prakasa Rao. Nonparametri fun tional estimation.\nity and Mathemati al Statisti s. A ademi\n\nProbabil-\n\nPress In . [Har ourt Bra e\n\nJovanovi h Publishers\u2104, New York, 1983.\n[24\u2104 M. B. Priestley and M. T. Chao. Non-parametri\n\nStatist. So . Ser. B, 34:385\u0015392, 1972.\n\n[25\u2104 M. Rosenblatt. Remarks on some nonparametri\nfun tion.\n\nAnn. Math. Statist., 27:832\u0015837, 1956.\n\nfun tion \u001ctting.\n\nJ. Roy.\n\nestimates of a density\n\n[26\u2104 M. Rosenblatt. Conditional probability density and regression estimators.\n\nMultivariate Analysis, II (Pro . Se ond Internat. Sympos., Dayton,\nOhio, 1968), pages 25\u001531. A ademi Press, New York, 1969.\nD. W. S ott. Multivariate density estimation. Wiley Series in Probability\nIn\n\n[27\u2104\n\nand Mathemati al Statisti s: Applied Probability and Statisti s. John\nWiley & Sons In ., New York, 1992. Theory, pra ti e, and visualization,\nA Wiley-Inters ien e Publi ation.\n[28\u2104 G. R. Shora k and J. A. Wellner.\n\nto statisti s.\n\nEmpiri al pro esses with appli ations\n\nWiley Series in Probability and Mathemati al Statisti s:\n\nProbability and Mathemati al Statisti s. John Wiley & Sons In ., New\nYork, 1986.\n[29\u2104 M. Sklar. Fon tions de r\u00e9partition \u00e0\n\nn\n\ndimensions et leurs marges.\n\nInst. Statist. Univ. Paris, 8:229\u0015231, 1959.\n\n[30\u2104 C. J. Stone. Optimal rates of\n\nonvergen e for nonparametri\n\nAnn. Statist., 8(6):1348\u00151360, 1980.\n\n[31\u2104 W. Stute.\n\n[32\u2104 W. Stute. Asymptoti\nestimates.\n[33\u2104 W. Stute.\n\nestimators.\n\nA law of the logarithm for kernel density estimators.\n\nProbab., 10(2):414\u0015422, 1982.\n\nPubl.\n\nAnn.\n\nnormality of nearest neighbor regression fun tion\n\nAnn. Statist., 12(3):917\u0015926, 1984.\nConditional empiri al pro esses. Ann. Statist., 14(2):638\u0015647,\n\n1986.\n[34\u2104 W. Stute. On almost sure\n\nonvergen e of\n\nonditional empiri al distribu-\n\nAnn. Probab., 14(3):891\u0015901, 1986.\nAsymptoti statisti s, volume 3 of Cambridge Series\nin Statisti al and Probabilisti Mathemati s. Cambridge University Press,\ntion fun tions.\n\n[35\u2104 A. W. van der Vaart.\nCambridge, 1998.\n\n[36\u2104 A. W. van der Vaart and J. A. Wellner.\n\npro esses.\n\nWeak onvergen e and empiri al\n\nSpringer Series in Statisti s. Springer-Verlag, New York, 1996.\n\nWith appli ations to statisti s.\n\n28\n\n\f"}