{"id": "http://arxiv.org/abs/1101.2301v1", "guidislink": true, "updated": "2011-01-12T09:28:22Z", "updated_parsed": [2011, 1, 12, 9, 28, 22, 2, 12, 0], "published": "2011-01-12T09:28:22Z", "published_parsed": [2011, 1, 12, 9, 28, 22, 2, 12, 0], "title": "A Factorial Experiment on Scalability of Search Based Software Testing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1101.3774%2C1101.4907%2C1101.2977%2C1101.5286%2C1101.4391%2C1101.1566%2C1101.2421%2C1101.5795%2C1101.3679%2C1101.0242%2C1101.2326%2C1101.4299%2C1101.3040%2C1101.1229%2C1101.2701%2C1101.0266%2C1101.5375%2C1101.1840%2C1101.0891%2C1101.3458%2C1101.3010%2C1101.3299%2C1101.2042%2C1101.0279%2C1101.3021%2C1101.1153%2C1101.0643%2C1101.2425%2C1101.1770%2C1101.1806%2C1101.1166%2C1101.1279%2C1101.5055%2C1101.3648%2C1101.1060%2C1101.3068%2C1101.3637%2C1101.3152%2C1101.2712%2C1101.3885%2C1101.3276%2C1101.4285%2C1101.2979%2C1101.1945%2C1101.3727%2C1101.4405%2C1101.2020%2C1101.1904%2C1101.4220%2C1101.4327%2C1101.1074%2C1101.3594%2C1101.1678%2C1101.0428%2C1101.5730%2C1101.2751%2C1101.0359%2C1101.1842%2C1101.2320%2C1101.3175%2C1101.4736%2C1101.2324%2C1101.3559%2C1101.3264%2C1101.5826%2C1101.3992%2C1101.5070%2C1101.1070%2C1101.0304%2C1101.0081%2C1101.6073%2C1101.1536%2C1101.5613%2C1101.3450%2C1101.2723%2C1101.3733%2C1101.1574%2C1101.4659%2C1101.4541%2C1101.1731%2C1101.1691%2C1101.3474%2C1101.3025%2C1101.5695%2C1101.5425%2C1101.3362%2C1101.2525%2C1101.0518%2C1101.2164%2C1101.4910%2C1101.4185%2C1101.0955%2C1101.2301%2C1101.0999%2C1101.0716%2C1101.1877%2C1101.2790%2C1101.0365%2C1101.4438%2C1101.2206%2C1101.5767&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Factorial Experiment on Scalability of Search Based Software Testing"}, "summary": "Software testing is an expensive process, which is vital in the industry.\nConstruction of the test-data in software testing requires the major cost and\nto decide which method to use in order to generate the test data is important.\nThis paper discusses the efficiency of search-based algorithms (preferably\ngenetic algorithm) versus random testing, in soft- ware test-data generation.\nThis study differs from all previous studies due to sample programs (SUTs)\nwhich are used. Since we want to in- crease the complexity of SUTs gradually,\nand the program generation is automatic as well, Grammatical Evolution is used\nto guide the program generation. SUTs are generated according to the grammar we\nprovide, with different levels of complexity. SUTs will first undergo genetic\nal- gorithm and then random testing. Based on the test results, this paper\nrecommends one method to use for automation of software testing.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1101.3774%2C1101.4907%2C1101.2977%2C1101.5286%2C1101.4391%2C1101.1566%2C1101.2421%2C1101.5795%2C1101.3679%2C1101.0242%2C1101.2326%2C1101.4299%2C1101.3040%2C1101.1229%2C1101.2701%2C1101.0266%2C1101.5375%2C1101.1840%2C1101.0891%2C1101.3458%2C1101.3010%2C1101.3299%2C1101.2042%2C1101.0279%2C1101.3021%2C1101.1153%2C1101.0643%2C1101.2425%2C1101.1770%2C1101.1806%2C1101.1166%2C1101.1279%2C1101.5055%2C1101.3648%2C1101.1060%2C1101.3068%2C1101.3637%2C1101.3152%2C1101.2712%2C1101.3885%2C1101.3276%2C1101.4285%2C1101.2979%2C1101.1945%2C1101.3727%2C1101.4405%2C1101.2020%2C1101.1904%2C1101.4220%2C1101.4327%2C1101.1074%2C1101.3594%2C1101.1678%2C1101.0428%2C1101.5730%2C1101.2751%2C1101.0359%2C1101.1842%2C1101.2320%2C1101.3175%2C1101.4736%2C1101.2324%2C1101.3559%2C1101.3264%2C1101.5826%2C1101.3992%2C1101.5070%2C1101.1070%2C1101.0304%2C1101.0081%2C1101.6073%2C1101.1536%2C1101.5613%2C1101.3450%2C1101.2723%2C1101.3733%2C1101.1574%2C1101.4659%2C1101.4541%2C1101.1731%2C1101.1691%2C1101.3474%2C1101.3025%2C1101.5695%2C1101.5425%2C1101.3362%2C1101.2525%2C1101.0518%2C1101.2164%2C1101.4910%2C1101.4185%2C1101.0955%2C1101.2301%2C1101.0999%2C1101.0716%2C1101.1877%2C1101.2790%2C1101.0365%2C1101.4438%2C1101.2206%2C1101.5767&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Software testing is an expensive process, which is vital in the industry.\nConstruction of the test-data in software testing requires the major cost and\nto decide which method to use in order to generate the test data is important.\nThis paper discusses the efficiency of search-based algorithms (preferably\ngenetic algorithm) versus random testing, in soft- ware test-data generation.\nThis study differs from all previous studies due to sample programs (SUTs)\nwhich are used. Since we want to in- crease the complexity of SUTs gradually,\nand the program generation is automatic as well, Grammatical Evolution is used\nto guide the program generation. SUTs are generated according to the grammar we\nprovide, with different levels of complexity. SUTs will first undergo genetic\nal- gorithm and then random testing. Based on the test results, this paper\nrecommends one method to use for automation of software testing."}, "authors": ["Arash Mehrmand", "Robert Feldt"], "author_detail": {"name": "Robert Feldt"}, "author": "Robert Feldt", "arxiv_comment": "3d Artificial Intelligence Techniques in Software Engineering\n  Workshop, 7 October, 2010, Larnaca, Cyprus", "links": [{"href": "http://arxiv.org/abs/1101.2301v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1101.2301v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1101.2301v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1101.2301v1", "journal_reference": null, "doi": null, "fulltext": "A Factorial Experiment on Scalability of Search\nBased Software Testing\nArash Mehrmand , Robert Feldt\n\narXiv:1101.2301v1 [cs.SE] 12 Jan 2011\n\nBlekinge Institute of Technology, SE-371 79 Karlskrona, Sweden\n\nAbstract. Software testing is an expensive process, which is vital in\nthe industry. Construction of the test-data in software testing requires\nthe major cost and to decide which method to use in order to generate\nthe test data is important. This paper discusses the efficiency of searchbased algorithms (preferably genetic algorithm) versus random testing,\nin software test-data generation. This study differs from all previous\nstudies due to sample programs ot software under test (SUT) which are\nused. Since we want to increase the complexity of SUTs gradually, and\nthe program generation is automatic as well, Grammatical Evolution is\nused to guide the program generation. SUTs are generated according to\nthe grammar we provide, with different levels of complexity. SUTs will\nfirst undergo genetic algorithm and then random testing. Based on the\ntest results, this paper recommends one method to use for automation\nof software testing.\nKeywords: Automated Software Testing, Search-based Software Testing, Genetic Algorithms, Random Testing, Grammatical Evolution\n\n1\n\nIntroduction\n\nSoftware testing is one of the most critical and at the same time most time and\ncost consuming activity in software development process [1]. In order to cut this\ncosts as much as possible we need to employ effective techniques to automate\nthis process. What actually needs to be automated is generating the test data\nwhich is a demanding process [2]. Automation of generating test data helps to\nhave fast, cheap and error free process [2].\nIf we can optimize the test cases generation process, then we save a lot of time\nand money. Among optimization techniques, search based optimization is the one\nwhich has been applied to different areas of Software Engineering applications\n[2].\nSoftware testing was not an exception in Software Engineering area and the\nterm \"Search Based Software Testing (SBST)\" proves it. The recent years show\na dramatic rise of interest in Search Based Testing (SBT) [2]. SBT has been\napplied to different testing areas such as structural, functional, non-functional,\nmutation, regression and so on [2].\nThe term \"Search Based Testing\" in general means using some sort of search\nalogorithms to generate test cases for testing a software.\n\n\f2\n\nThere are many Search Based techniques and each of them uses a different\nalgorithm to optimize the search and the results.\nThe three popular ones which are in use for testing purpose are Neighborhood\nSearch, Simulated annealing and Genetic algorithms (GA). And of course there\nare simple techniques as well such as random testing which does not use any\noptimization and search technique. [3]. According to [3] flexibility and strength of\nthese heuristic optimization techniques help a lot in finding the optimal solutions\nin large and complex search spaces.\nThis study focuses on using the SBT techniques or as we said heuristic optimization techniques for software testing, when we have different programs\nregarding to complexity. We will apply the technique which is called Genetic\nalgorithm to wide variety of programs to see the effectiveness of Genetic algorithm compared to random testing. The final results of this paper could help\nSoftware engineers and especially Software testers to compare the efficiency of\nsearch based techniques versus random testing when it comes to get the maximum percentage of coverage on each code, as they deal with programs with\ndifferent levels of complexity.\nThe structure of this paper is as follows: Next section, presents the existing\nresearch in this area. Section 3 explains the experimental approach of this study\nand study design. Section 4 talks about the design of experiment in this study.\nSections 5 and 6 include the results and discussion about them. And finally\nconclusion and further work will come under section 7.\n\n2\n\nPrevious Research and Related Work\n\nThere exist many studies in software testing automation area and SBST. Some\nof them deal with software complexity as well but we could have not found any\nof them similar to this study as we deal with increasing of software complexity\nalong our study. For example, authors in [4] discuss the experiments with test\ncase generation with large and complex programs and they conclude that, there\nis a wide gap between the techniques based on GA and those based on random\ntesting. They have chosen some programs which are written in C language and\nused them as SUT.\nCompared to what the authors tried to explain in [4], our study has a major\ndifference and that is the use of many Java programs that vary in size and\ncomplexity.\nMany other studies tried to experiment software testing using genetic algorithms and compare the results with other techniques. Authors in [5] performed\nexperiments on some small and a few complex test objects and they came up\nwith this results: Particle Swarm Optimization overcomes GA in coverage of\nmany code elements so not always GA is the best solution. Another paper also\ntalks about GA based software testing and the main idea is to find problematic\nsituations while testing programs with GA [6].\nAlba and Chicano in their paper [7] describe how canonical genetic algorithms\nand evolutionary strategies can help in software testing. They used a benchmark\n\n\f3\n\nof twelve test programs and applied both Evolutionary Strategies and GA and\ncompared the results at the end.\nHowever, many existing research and studies (e.g. [8],[9],[10]) have used SBT\nspecially GA in order to test the software and compare the results with other\ntechniques.\n\n3\n\nExperiment Approach\n\nThe chosen approach for this study is factorial experiment which compares GA\nand random testing in efficiency. Factorial experiment is an experiment whose\ndesign consists of two or more factors and each factor has discrete possible values which we call them levels [1]. Factors, mentioned in table 1, are the test\nsearch techniques which has GA and random as values, complexity which has\nlow, medium and high as levels , statement coverage and branch coverage with\ntheir defined levels. Since we have to increase the complexity of our generated\nprograms gradually, there are some factors which we have to increase or decrease,\nin order to change the complexity the programs. Code complexity is actually an\nindependent variable in our experiment. Statement and branch are independent\nof each other but dependent to the complexity level; the more complexity level\nwe have, the more branch or statement we deal with. Below is a table which\nshows the factorial design of this experiment.\nTable 1. Factorial Design of The Experiment\nFactor's name TST\nComplexity Statement\nGA\nLow\n75\nLevel\nRandom Medium\n150\nHigh\n300\n\nBranch\n25\n50\n100\n\nFor both statement and branch coverage, we have source codes with low,\nmedium and high complexity. For each coverage method, we test our codes with\nboth GA and random testing and record the coverage percentages as you can\nsee in plots under section 5.\nThese factors (variables) are explained in details in section 4 which is Experiment Design. After making all the experiments, we will have two tables and\nsix graphs which show the efficiency of GA compared to random testing; these\nresults will come in section 6 which is Experiment Results.\n\n4\n\nExperiment Design\n\nIn order to design a fully automated process for our experiment, we need to\nfollow a step by step structure. These steps are the following subsections:\n\n\f4\n\n4.1\n\nGenerate Software Under Test (SUT) Using Grammatical\nEvolution (GE)\n\nGrammatical Evolution (GE) is a kind of Genetic Programming based on grammar. GE combines basis and rules of molecular biology with representational\npower of formal grammars. GE has a unique flexibility due to its rich modularity,\nand makes it possible to use alternative search strategies, whether evolutionary,\nor other heuristic, be it stochastic or deterministic, and to radically change its\nbehavior by only slight changes to the grammar supplied. One of the main advantages of GE, is the easiness of modifying the output structures by simply\nediting the plain text grammar. This advantage actually originates from the use\nof grammar to describe the structures that are generated by GE [12].\nTo use GE, for our SUTs generation, we used an open source software implementation of grammatical evolution in Java. Grammatical Evolution in Java\n(GEVA) is the name of the tool which provides the possibility to use GE with\nour supplied grammar, in order to generate evolved programs. However, for us,\nto use GEVA, there was a need to make some modifications and supply our\nown material to reach our goal which is generating Java programs with different\nlevels of complexity.\nFirst of all we need to define our problem, which is generating different Java\nprograms. For this purpose, we wrote a Java code which automatically generates\nthe header and footer of the programs and makes the contents of the body of the\nprogram, according to the BNF which we will provide. So next step is to write a\nBNF. Using Backus-Naur form (BNF), it is possible to provide the specification\nof a programming language structure, which is Java in this case. Since we have to\nhave a subset of grammars, we should define our BNF according to what subsets\nof Java programming language we need to generate. The genetic configuration of\nGEVA is also changed according to our need. Following is a part of our properties,\nused in GEVA:\n\u2013\n\u2013\n\u2013\n\u2013\n\nPopulation size: 200\nGeneration: 10000\nInitial chromosome-size: 200\nSelection type: Roulette wheel [17].\n\nProviding GEVA, with the problem definition and BNF file, it can generate\nour desired programs. Because we want to increase the complexity of our programs, we need to run the generation, three different times. When the fitness\nfunction is set to the number of statements, we run the generation, once with 75\nstatements, once with 150 and finally with 300 statements. And when the fitness\nfunction is set to the number of branches in the code, we run the generation once\nwith 25 branches, once with 50 branches and finally with 100 branches. However,\nthe complexity levels could be extended to more than three levels, e.g. very low,\nlow, medium, high, very high or we can have the same three levels with different\nnumbers; the results and the effectiveness of GA over random search will not\nchange though. So three levels which are used in this experiment are sufficient\nenough for us to conclude which method is more efficient.\n\n\f5\n\nIt is worth mentioning that our sample programs are just regular codes written in Java and they involve different kinds of conditions and statements. By\nthis we mean we did not follow any specific problem to generate the sample\nprograms.\n4.2\n\nGA\n\nAfter generating the SUTs, we need to apply GA to our code so it will act\ngenetically during the testing process. The library which is used for this purpose,\nin this study, is called JGAP. JGAP is actually a Genetic Algorithm and Genetic\nProgramming component which is provided as Java framework. Using JGAP it\nis possible to apply evolutionary principles to problem solutions since it provides\nus, the basic genetic mechanisms [13].\nUsing JGAP, first step is to plan the chromosomes, so we have to decide on\ntype and quantity of genes that we are going to use. Each chromosome is made\nof genes, and in this study each gene is actually a test data. And depending on\nthe size and number of input variables, we decide on number of genes. Second\nstep is to implement fitness function. Although JGAP is designed to do almost\nall the evolutionary steps in different problems but it does not know, by itself,\nwhether or not a potential solution is better than the other ones. Our fitness\nfunction, depending on which part of experiment we are, is the coverage percentage of the generated test data. For example if our complexity metric is the\nnumber of statements, then the best chromosomes are those which provide better\nstatement coverage on the code. Third step is to setup the configuration object\nof JGAP which means deciding on how big our population is, and all other GA\nconfigurations. In this experiment the population size is set as 200 and number\nof generations is set to 10000. Fourth and fifth steps are creating and evolving\nthe population. And obviously after each generation, the best test data of each\npopulation are taken. However, the followings are the steps, GA basically, uses\nto test a software where P (t) is the population and t is the generation [17]:\nInitialize P(t)\nEvaluate P(t)\nWhile the termination condition\nis not reached, do\nselect P(t+1) from P(t)\nrecombine P(t+1)\nevaluate P(t+1)\nt=t+1\nEvaluating the fitness function depends on which experiment we are running;\nso for statement coverage and branch coverage it differs. Please refer to 3.1.3,\nwhere defining the fitness of the experiment is explained in more details. This\nwas the whole strategy of GA application to our SUTs.\nDefining The Fitness Function What genetic algorithm does is actually\nsearching for the suitable test case, to kill a given mutant; for this purpose\n\n\f6\n\nfitness function is used, which assigns a non-negative cost to each candidate\ninput value. The more cost each input value has, the more appropriate it is, so\nwe aim to maximize this value and therefore maximize the fitness function [21].\nWe used the term which is called function minimization [17]. Function minimization helps to find the desired input for each condition to be executed. Using\nthis way, each condition in the code has its own function. Genetic search, then,\ntries to find the input which minimizes the value of that specific function. For\nexample, imagine we have this condition on line 300 of the program:\nif (x >= z+10)\nand we aim to execute the true condition of this branch. Then the, the minimization function is defined as below:\n\u001a\n(z300 + 10) \u2212 x300 if Z300 + 10 \u2264 x300\nf (n) =\n0\notherwise\nThe above function says, if z300 + 10 is less than or equal to x300 then try\nto set the value of z300 and x300 so that the value of (z300 + 10) \u2212 x300 is as\nminimum as possible. Please notice that 300 here is the line number. We have\nto define the line number since the value of input parameters (x and z), change\nover time, as the programs run.\nWhen the fitness function is statement coverage, we need to count the number\nof statements which are blocked between braces of correspondent branch. We use\nit as the weight and divide the value of [the correspondent] function with this\nweight. For branch coverage, we do not need this weight since we are looking for\nthe covered branches not the statements.\nBecause our sample programs execute repeatedly over time, the input parameters change every time the program executes. The minimization function\ndecides, how closed the input parameters are to the desired values which satisfy the conditions and we use GA in order to minimize the function; the fittest\nchromosomes mean the closer results to minimize the function.\n4.3\n\nRandom Testing\n\nRandom testing is the chosen approach in this study, to be compared with SBST.\nAccording to [16] random testing is actually the use of randomly generated of\ntest data which has some advantages such as easy and simple implementation\nand speed of execution, which means less run time. When running the test using\nrandom testing, we have to define a parameter; this parameter defines how many\ntimes we should generate test data using random testing. For example if we set\nthe parameter to 100, then it generates 100 set of test cases. Out of these [e.g.]\n100, we take the best and compare it with the fittest result of GA. However, in\nthis experiment, this number is set to 100000.\nUsing random testing we need to define our input domain, for test data, so\nthe numbers will be randomly generated form this domain. In this study this set\nstarts from \u22121000000 to +1000000.\n\n\f7\n\n5\n\nExperiment Results\n\nAfter testing the generated SUTs, we came up with the results. As you can see\nin the following plots, GA outperforms the random testing in almost, all the\nexperiments which we have done. As mentioned before, the whole experiment\nincludes testing 60 programs; 30 (10 with low, 10 with medium, and 10 with\nhigh complexity) with statement coverage purpose and 30 (10 with low, 10 with\nmedium, and 10 with high complexity) with branch coverage purpose. So the\nfitness function was different for each purpose. The generated programs are also\ndifferent since they are generated with two different fitness function. The sample\nprograms for statement coverage purpose, are generated with number of statements purpose and the ones which are used for branch coverage purpose, are\ngenerated with number of branches purpose. Please not that the following plots\ninclude 10 programs each, which are randomly chosen from multiple runs that\nwe had.\n\nFig. 1. GA vs Random-low complexity\n\nFig. 2. GA vs Random-medium complexity\n\nFig. 3. GA vs Random-high complexity\n\nFig. 4. GA vs Random-low complexity\n\n\f8\n\nFig. 5. GA vs Random-medium complexity\n\nFig. 6. GA vs Random-high complexity\n\nAs mentioned above, these plots represent the result of 10 chosen programs,\nbut to see how efficient GA is, please refer to the tables II and III, where they\nshow the average percentage of coverage with both GA and random search.\nTable 2. Factors and their levels\nTest Search Technique\nGA\nRandom\nCmplx1 Stmt2 Mean Stdev Mean Stdev\nLow\n75 95.87 12.13 93.87 13.95\nMedium 150 96.12 8.74 94.73 8.98\nHigh\n300 95.39 5.63 94.13 6.16\na\nb\nc\n\nActual CL3\n44.42\n45.41\n58.84\n\ncomplexity\nstatements\nConfidence Level\n\nIn table II, for statement coverage, from low to high, t-test shows that means\nare not significantly different from the Selected Confidence Level. Although we\nmight not be able to prove GA outperforms random search statistically because\nof the actual confidence level here, but still the overall mean of coverage in GA\nis more than random search in all cases.\nNote: desired confidence level is set to 90\nAnd in Table III, for branch coverage, from low to high, t-test shows that\nmeans are significantly different from the Selected Confidence Level.\nNote: desired confidence level is set to 90\n\n6\n\nDiscussion\n\nthe results from the plots in section 6, illustrate GA has outperformed random\ntesting in most cases and in very few cases they have same coverage percentage.\n\n\f9\nTable 3. Factors and their levels\nTest Search Technique\nGA\nRandom\nCmplx4 Branch Mean Stdev Mean Stdev\nLow\n25 87.27 6.98 82.4 7.62\nMedium 50 79.33 6.99 75 5.82\nHigh\n100 76.57 8.25 73 5.76\nd\ne\n\nActual CL5\n98.76\n98.84\n94.32\n\ncomplexity\nConfidence Level\n\nThe design of this experiment is factorial so that we can assure in different\nsituations, with different complexity levels and different source codes, random\ntesting can never outperform GA from the coverage and efficiency point of view.\nHowever, there exist some issues which need to be discussed here, some of\nthem could be serious problems though, if they are not handled properly.\nOur SUTs here are generated using GE. As mentioned in previous sections,\nfor GE to generate programs, we have to provide it with a BNF. Unfortunately it\nis impossible to define semantics (logic) in BNF file which means there is a very\nlow chance of controlling what you are generating. During these experiments,\nwe had many programs which had same statements, or same loops or even same\nblocks due to GA behavior of generating fittest chromosome.\nBig numbers, which are made throughout the process of multiplication, addition and subtraction, are out of range of integer and double variable. These\nout of range variables can cause the code not to be tested completely because\nwhen the test reaches these numbers, stops running. We did not define divisions\nin our BNF since it can cause many problems e.g. division by zero.\nAnother important issue is the infinite loops in the code. And , we controlled\nthem using exit-loop which means having maximum iteration and break it after\nthe value of maximum iterations have been reached. However, having breaks,\neven if we assign maximum iteration to a large number, causes the code not to\nget tested completely [14,15].\nConditions in the loops are very important during testing, because if we can\nsatisfy the condition and go through the block of that condition, we can execute\nmore statements. There are two kind of conditions which can cause problems.\nFirst one is the condition which is not reachable at all and second kind is a very\neasy condition which both GA and random testing can cover it easily.\nThis problem is solved in our BNF which means we have the minimum number of easy and unreachable conditions so we do not face serious problems during\nour experiment.\nUsing GE and BNF to generate programs has some difficulties and limitations\nwhich make it very hard to generate a piece of code without infinite loops, hard\nbut possible to cover conditions. And testing the programs which are generated\nusing GE has its own limitations since we have limited the number of iterations\nof existing loops, and eliminate the nested loops because of exit-loop problem.\n\n\f10\n\nThe authors in [18] do not recommend using nested loops; instead they suggest\nanother way which is called LoopN. In LoopN the use of nested loops is forbidden.\nThey think loop does not do much harm if it is not nested. However limiting\nthe code, and not having the nested loops can decrease the overall complexity,\nwhich was not our aim but there was no other way than eliminating them.\nRather than having the problem with BNF, instrumentation itself, is a big\nissue. This is mainly because, the more details we need to instrument, the less\nspeed we have. For example in case of nested loops; instrumenting nested loops\nneeds us to keep track of each loop (inner and outer), each branch, and each\ncondition more precisely.\n\n7\n\nConclusion\n\nIn this paper, we have reported on results from a factorial experiment, where\nthe effectiveness of GA is compared to random testing in automation of software\ntesting. In this experiment, the process of generating SUTs were also automated,\nwhich means we used GE to generate our sample Java programs. To our knowledge, the presented results we had are not yet reported in the software test data\ngeneration area, because of having automatically generated programs, different\nlevels of complexity and different kinds of coverage at the same time. However,\nit is worth mentioning, this study focuses on the efficiency of the techniques and\ndoes not consider the other attributes such as time, cost etc. The followings are\nour observations from the experiments and the taken results.\n\u2013 GA can outperform random testing and it did actually in this study with our\nautomatically generated SUTs. We proved it using a factorial experiment.\nThis factorial experiment with different levels of code complexity and two\nmethods of code coverage, shows that in most of the situations GA has better\nefficiency than random testing.\n\u2013 Random testing could be, however, a faster solution but it is not recommended since in real life we look for a software with less bugs, not a software\nwhich is tested quickly. This means more bugs that we find in our programs,\nmore cost we save; this is what happens in real world.\n\u2013 Number of statements, is not a good metric for software complexity because\nadding extra statement hardly makes the code more complex. What makes\nthe code really complex, is the conditions.\nThis study can be extended by applying well known software complexities\ni.e. cyclomatic complexity.\n\nReferences\n1. Catelani, M.,, Ciani, L.,, Scarano, V. ,, Bacioccola, A.: A Novel Approach To Automated Testing To Increase Software Reliability. Instrumentation and Measurement\nTechnology Conference Proceedings 1499\u20131502 (2008)\n\n\f11\n2. Harman, Mark: Automated Test Data Generation using Search Based Software Engineering. AST '07: Proceedings of the Second International Workshop on Automation of Software Test, 2 (2007)\n3. Nigel James Tracy: A Search-Based Automated Test-Data Generation Framework\nfor Safety-Critical Software. University of York (2000)\n4. Christoph C. Michael and Gary E. Mcgraw and Michael A and Curtis C. Walton:\nGenetic algorithms for dynamic test data generation. In Proc. ASE97, 307\u2013308\n(1997)\n5. Windisch,, Andreas and Wappler,, Stefan and Wegener,, Joachim: Applying particle\nswarm soptimization to software testing. GECCO '07: Proceedings of the 9th annual\nconference on Genetic and evolutionary computation, 1121\u20131128 (2007)\n6. Alander, Jarmo T. and Mantere, Timo and Turunen, Pekka. : Genetic algorithm\nbased software testing. Turku Centre for Computer Science, Springer-Verlag (1998)\n7. Alba,, Enrique and Chicano,, Francisco. : Observations in using parallel and sequential evolutionary algorithms for automatic software testing. Comput. Oper. Res.,\n3161\u20133183 (2008)\n8. Ribeiro,, Jos\u00e9 Carlos Bregieiro : Search-based test case generation for object-oriented\njava software using strongly-typed genetic programming. GECCO '08: Proceedings\nof the 2008 GECCO conference companion on Genetic and evolutionary computation, 1819\u20131822 (2008)\n9. Harmen-Hinrich Sthamer : The Automatic Generation of Software Test Data Using\nGenetic Algorithms. University of Glamorgan (1995)\n10. Mark Last,, Shay Eyal,, Abraham Kandel : Effective Black-Box Testing with Genetic Algorithms. Hardware and Software, Verification and Testing, 134\u2013148 (2006)\n11. Bechhofer,, Robert E.: Selection in factorial experiments. WSC '77: Proceedings\nof the 9th conference on Winter simulation 65\u201370 (1977)\n12. O'Neill,, Michael and Hemberg,, Erik and Gilligan,, Conor and Bartley,, Eliott\nand McDermott,, James and Brabazon,, Anthony: GEVA: grammatical evolution\nin Java. SIGEVOlution,ACM, 17\u201322 (2008)\n13. Klaus Meffert , Neil Rotstan: JGAP: Java Genetic Algorithms Package.\n14. Wu,, Lieh-Ming and Wang,, Kuochen and Chiu,, Chuang-Yi: A BNF-based automatic test program generator for compatible microprocessor verification ACM\nTrans. Des. Autom. Electron. Syst., 105\u2013132 (2004)\n15. BMiyake, J. and Brown, G.: Automatic test generation for functional verification\nof microprocessors. In Proceedings of the Third Asian Test Symposium , 292\u2013297\n(1994)\n16. Ciupa,, Ilinca and Leitner,, Andreas and Oriol,, Manuel and Meyer,, Bertrand.:\nExperimental assessment of random testing for object-oriented software. ISSTA '07:\nProceedings of the 2007 international symposium on Software testing and analysis,\n84\u201394 (2007)\n17. Gary Mcgraw and Christoph Michael and Michael Schatz: Generating Software\nTest Data by Evolution. IEEE Transactions on Software Engineering, 1085\u20131110\n(1997)\n18. Yuesheng Qi,, Baozhong Wang,, Lishan Kang. : Genetic Programming with simple\nloops. Journal of Computer Science and Technology,429\u2013434 (1999)\n19. Wolfgang Stcher: Designing and Prototyping a Functor Language Using Denotational Semantics. RISC Report Series, University of Linz, Austria, (1997)\n20. Derk, M. D.: Towards a simpler method of operational semantics for language\ndefinition. SIGPLAN Not., ACM, 39\u201344 (2005)\n21. Bottaci, Leonardo.: Genetic Algorithm Fitness Function for Mutation Testing. Department of Computer Science, The University of Hull, Hull HU6 7RX, U.K. (2001)\n\n\f"}