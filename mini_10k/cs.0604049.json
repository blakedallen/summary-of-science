{"id": "http://arxiv.org/abs/cs/0604049v2", "guidislink": true, "updated": "2006-04-13T03:40:27Z", "updated_parsed": [2006, 4, 13, 3, 40, 27, 3, 103, 0], "published": "2006-04-11T23:02:57Z", "published_parsed": [2006, 4, 11, 23, 2, 57, 1, 101, 0], "title": "Low SNR Capacity of Fading Channels with Peak and Average Power\n  Constraints", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0604099%2Ccs%2F0604021%2Ccs%2F0604015%2Ccs%2F0604051%2Ccs%2F0604006%2Ccs%2F0604088%2Ccs%2F0604107%2Ccs%2F0604095%2Ccs%2F0604048%2Ccs%2F0604078%2Ccs%2F0604045%2Ccs%2F0604075%2Ccs%2F0604042%2Ccs%2F0604070%2Ccs%2F0604013%2Ccs%2F0604049%2Ccs%2F0604056%2Ccs%2F0604054%2Ccs%2F0604097%2Ccs%2F0604044%2Ccs%2F0604032%2Ccs%2F0604039%2Ccs%2F0604106%2Ccs%2F0604085%2Ccs%2F0604079%2Ccs%2F0604103%2Ccs%2F0604029%2Ccs%2F0604065%2Ccs%2F0604112%2Ccs%2F0604017%2Ccs%2F0604038%2Ccs%2F0604098%2Ccs%2F0604100%2Ccs%2F0604025%2Ccs%2F0604057%2Ccs%2F0604014%2Ccs%2F0604058%2Ccs%2F0604062%2Ccs%2F0604028%2Ccs%2F0604102%2Ccs%2F0604091%2Ccs%2F0604010%2Ccs%2F0604093%2Ccs%2F0604026%2Ccs%2F0604002%2Ccs%2F0604031%2Ccs%2F0604023%2Ccs%2F0604001%2Ccs%2F0604104%2Ccs%2F0604040%2Ccs%2F0604109%2Ccs%2F0604016%2Ccs%2F0604033%2Ccs%2F0604011%2Ccs%2F0604086%2Ccs%2F0604052%2Ccs%2F0604067%2Ccs%2F0512102%2Ccs%2F0512104%2Ccs%2F0512001%2Ccs%2F0512073%2Ccs%2F0512089%2Ccs%2F0512034%2Ccs%2F0512026%2Ccs%2F0512043%2Ccs%2F0512085%2Ccs%2F0512097%2Ccs%2F0512010%2Ccs%2F0512091%2Ccs%2F0512060%2Ccs%2F0512088%2Ccs%2F0512079%2Ccs%2F0512044%2Ccs%2F0512051%2Ccs%2F0512074%2Ccs%2F0512069%2Ccs%2F0512032%2Ccs%2F0512025%2Ccs%2F0512048%2Ccs%2F0512055%2Ccs%2F0512004%2Ccs%2F0512030%2Ccs%2F0512045%2Ccs%2F0512070%2Ccs%2F0512035%2Ccs%2F0512066%2Ccs%2F0512023%2Ccs%2F0512056%2Ccs%2F0512059%2Ccs%2F0512063%2Ccs%2F0512065%2Ccs%2F0512037%2Ccs%2F0512009%2Ccs%2F0512105%2Ccs%2F0512016%2Ccs%2F0512049%2Ccs%2F0512033%2Ccs%2F0512027%2Ccs%2F0512021%2Ccs%2F0512082%2Ccs%2F0512024&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Low SNR Capacity of Fading Channels with Peak and Average Power\n  Constraints"}, "summary": "Flat-fading channels that are correlated in time are considered under peak\nand average power constraints. For discrete-time channels, a new upper bound on\nthe capacity per unit time is derived. A low SNR analysis of a full-scattering\nvector channel is used to derive a complimentary lower bound. Together, these\nbounds allow us to identify the exact scaling of channel capacity for a fixed\npeak to average ratio, as the average power converges to zero. The upper bound\nis also asymptotically tight as the average power converges to zero for a fixed\npeak power.\n  For a continuous time infinite bandwidth channel, Viterbi identified the\ncapacity for M-FSK modulation. Recently, Zhang and Laneman showed that the\ncapacity can be achieved with non-bursty signaling (QPSK). An additional\ncontribution of this paper is to obtain similar results under peak and average\npower constraints.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0604099%2Ccs%2F0604021%2Ccs%2F0604015%2Ccs%2F0604051%2Ccs%2F0604006%2Ccs%2F0604088%2Ccs%2F0604107%2Ccs%2F0604095%2Ccs%2F0604048%2Ccs%2F0604078%2Ccs%2F0604045%2Ccs%2F0604075%2Ccs%2F0604042%2Ccs%2F0604070%2Ccs%2F0604013%2Ccs%2F0604049%2Ccs%2F0604056%2Ccs%2F0604054%2Ccs%2F0604097%2Ccs%2F0604044%2Ccs%2F0604032%2Ccs%2F0604039%2Ccs%2F0604106%2Ccs%2F0604085%2Ccs%2F0604079%2Ccs%2F0604103%2Ccs%2F0604029%2Ccs%2F0604065%2Ccs%2F0604112%2Ccs%2F0604017%2Ccs%2F0604038%2Ccs%2F0604098%2Ccs%2F0604100%2Ccs%2F0604025%2Ccs%2F0604057%2Ccs%2F0604014%2Ccs%2F0604058%2Ccs%2F0604062%2Ccs%2F0604028%2Ccs%2F0604102%2Ccs%2F0604091%2Ccs%2F0604010%2Ccs%2F0604093%2Ccs%2F0604026%2Ccs%2F0604002%2Ccs%2F0604031%2Ccs%2F0604023%2Ccs%2F0604001%2Ccs%2F0604104%2Ccs%2F0604040%2Ccs%2F0604109%2Ccs%2F0604016%2Ccs%2F0604033%2Ccs%2F0604011%2Ccs%2F0604086%2Ccs%2F0604052%2Ccs%2F0604067%2Ccs%2F0512102%2Ccs%2F0512104%2Ccs%2F0512001%2Ccs%2F0512073%2Ccs%2F0512089%2Ccs%2F0512034%2Ccs%2F0512026%2Ccs%2F0512043%2Ccs%2F0512085%2Ccs%2F0512097%2Ccs%2F0512010%2Ccs%2F0512091%2Ccs%2F0512060%2Ccs%2F0512088%2Ccs%2F0512079%2Ccs%2F0512044%2Ccs%2F0512051%2Ccs%2F0512074%2Ccs%2F0512069%2Ccs%2F0512032%2Ccs%2F0512025%2Ccs%2F0512048%2Ccs%2F0512055%2Ccs%2F0512004%2Ccs%2F0512030%2Ccs%2F0512045%2Ccs%2F0512070%2Ccs%2F0512035%2Ccs%2F0512066%2Ccs%2F0512023%2Ccs%2F0512056%2Ccs%2F0512059%2Ccs%2F0512063%2Ccs%2F0512065%2Ccs%2F0512037%2Ccs%2F0512009%2Ccs%2F0512105%2Ccs%2F0512016%2Ccs%2F0512049%2Ccs%2F0512033%2Ccs%2F0512027%2Ccs%2F0512021%2Ccs%2F0512082%2Ccs%2F0512024&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Flat-fading channels that are correlated in time are considered under peak\nand average power constraints. For discrete-time channels, a new upper bound on\nthe capacity per unit time is derived. A low SNR analysis of a full-scattering\nvector channel is used to derive a complimentary lower bound. Together, these\nbounds allow us to identify the exact scaling of channel capacity for a fixed\npeak to average ratio, as the average power converges to zero. The upper bound\nis also asymptotically tight as the average power converges to zero for a fixed\npeak power.\n  For a continuous time infinite bandwidth channel, Viterbi identified the\ncapacity for M-FSK modulation. Recently, Zhang and Laneman showed that the\ncapacity can be achieved with non-bursty signaling (QPSK). An additional\ncontribution of this paper is to obtain similar results under peak and average\npower constraints."}, "authors": ["Vignesh Sethuraman", "Bruce Hajek"], "author_detail": {"name": "Bruce Hajek"}, "author": "Bruce Hajek", "arxiv_comment": "13 pages, version without proofs submitted to ISIT 2006", "links": [{"href": "http://arxiv.org/abs/cs/0604049v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0604049v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0604049v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0604049v2", "journal_reference": null, "doi": null, "fulltext": "1\n\nLow SNR Capacity of Fading Channels with\nPeak and Average Power Constraints\n\narXiv:cs/0604049v2 [cs.IT] 13 Apr 2006\n\nVignesh Sethuraman, Member, IEEE, and Bruce Hajek, Fellow, IEEE\n\nAbstract\nFlat-fading channels that are correlated in time are considered under peak and average power constraints. For\ndiscrete-time channels, a new upper bound on the capacity per unit time is derived. A low SNR analysis of a fullscattering vector channel is used to derive a complimentary lower bound. Together, these bounds allow us to identify\nthe exact scaling of channel capacity for a fixed peak to average ratio, as the average power converges to zero. The\nupper bound is also asymptotically tight as the average power converges to zero for a fixed peak power.\nFor a continuous time infinite bandwidth channel, Viterbi identified the capacity for M-FSK modulation. Recently,\nZhang and Laneman showed that the capacity can be achieved with non-bursty signaling (QPSK). An additional\ncontribution of this paper is to obtain similar results under peak and average power constraints.\nIndex Terms\n\nLow SNR, channel capacity, correlated fading, flat fading, Gauss Markov fading\n\nI. I NTRODUCTION\nA Rayleigh flat-fading channel with correlation in time is considered. There is no channel state information at\neither the transmitter or the receiver. The fading process is modeled as a stationary ergodic process. An average\npower constraint is imposed at the transmitter. In the low SNR regime, it is known that capacity achieving input\nsignals are bursty [1, 2]. To limit this behavior, a hard peak constraint, in addition to the average power constraint,\nis considered. The focus of this paper is the channel capacity with peak and average power constraints in the low\nSNR regime. For similar work in the high SNR regime, see [3] and references there-in.\nThe capacity of this channel in the low SNR regime has recently been of heightened research interest [4\u20136]. In\nthese works, a lower bound is obtained through constructive methods, and is compared to an upper bound based\non capacity per unit energy [7, 8]. While the bounds are close for channels with memory, an asymptotic analysis\nat low SNR reveals a gap between these bounds.\nThe main result in this work is the complete identification of the asymptotic behavior of the capacity of this\nchannel at low SNR. To this end, a new upper bound on the channel capacity is derived. To obtain a matching\nlower bound, a full-scattering vector channel is considered. For a surprisingly simple choice of input distribution, the\nmutual information rate of this channel, when specialized to flat fading, is shown to coincide with the upper bound.\nIncidentally, it is also established in this work that the bounds in [4\u20136] are not, in general, asymptotically tight at\nlow SNR. Nevertheless, when there is only a peak constraint, a lower bound in [4, 6] is found to be asymptotically\ntight when there is sufficient memory (correlation in time) in the channel. Interestingly, an upper bound derived in\n[3] for tightness at high SNR is found to be tight at low SNR too.\n1 This work was supported in part by the National Science Foundation Grant NSF ITR 00-85929, and the Motorola Center for Communication\nGraduate Fellowship.\n\n\f2\n\nA continuous time version of the flat fading channel is considered.2 The channel capacity for M-FSK modulation\nis calculated in [9]. The input signaling here is bursty in frequency. Zhang and Laneman [4] find the capacity of this\nchannel under a peak constraint, by showing that the rate achieved using a specific constructive scheme matches\nan upper bound obtained using capacity per unit energy. Also, since the constructive scheme can use M-PSK, it\nis not bursty in either time or frequency. Here, similar results are established under both peak and average power\nconstraints.\nThe paper is organized as follows. In Section II, the channel model is defined and some background on the\nchannel capacity is developed. Upper bounds are presented in Section III. The main result, namely, the asymptotic\nbehavior of capacity at vanishing peak and average powers, is stated and proved in Section IV. The material on\ncontinuous time channels is presented in Section V. Interesting directions of future research are outlined in Section\nVI.\n\nII. P RELIMINARIES\nConsider a single-user discrete-time channel without channel state information at either transmitter or receiver.\nThe channel includes additive noise and multiplicative noise (Rayleigh flat-fading) and is specified by\nYk =\nwhere Xk =\n\n\u221a\n\u03c1Zk Hk + Wk ; k \u2208 Z\n\n(1)\n\n\u221a\n\u03c1Zk is the input, H is the fading process, W is an additive noise process, and Y is the output. The\n\nfading and the additive noise processes are mutually independent, and jointly independent of the input. The additive\nnoise W is modeled as an i.i.d. proper complex normal (PCN) process with unit variance. The fading process H\nis also PCN, but is correlated in time with an autocorrelation function {RH (k) : k \u2208 Z}. Further, H is assumed to\nbe ergodic. A peak power constraint |Xi |2 \u2264 \u03c1 for some \u03c1 > 0 is imposed at the transmitter. This translates to\n|Zi |2 \u2264 1\n\n(2)\n\nAn average power constraint, specified through the peak to average ratio \u03b2 \u2265 1 and the peak power constraint \u03c1,\nis also imposed at the transmitter.\n\nE[|Zi |2 ] \u2264\n\n1\n\u03b2\n\n(3)\n\nIn [7], the information-theoretic capacity per unit time for this channel, in the presence of the above defined peak\nand average power constraints on the input, is defined and shown to be equal to the operational capacity. The\ncapacity per unit time of this channel is denoted, in this manuscript, by C(\u03c1, \u03b2).\nConsider the following model of a full-scattering vector channel.\nY1\u00d7n =\nA deterministic signal X1\u00d7n =\n\n\u221a\nb n\u00d7n + W1\u00d7n\n\u03c1Z1\u00d7n H\n\n(4)\n\n\u221a\nb i,j\n\u03c1Z1\u00d7n is the input and Y is the observed output. Here, the random variables H\n\nare elements of the fading process. They are modeled as PCN with unit variance and are allowed to be correlated\nwith each other, but jointly independent of the additive noise term W , which can be thought of as part of an i.i.d.\nPCN process with unit variance. Peak (2) and average power (3) constraints can be applied to this model too. Let\n\u03bc be a distribution on Z1\u00d7n satisfying the peak constraint (2). For low values of peak constraint \u03c1, the mutual\ninformation between the output and the input is well approximated by the leading terms of the Taylor's series\nexpansion.\n2 A flat fading channel model in continuous-time comes with the caveat that such a model is questionable when input symbol durations are\nsmaller than the delay spread of the fading process. Still, this model isn't bad for a first-cut approximation of continuous-time channels.\n\n\f3\n\nLemma 2.1: For small values of \u03c1, I(Z; Y ) is given by\nI(Z; Y ) =\nwhere\n\n\u0002\n\u0003\n\u0001\n\u03c12\nEZ\u223c\u03bc T r(KZ2 ) \u2212 T r(K\u03bc2 ) + o(\u03c12 )\n2\n\nand\n\nHere, o(x) is used in the sense that limx\u21920\n\n(5)\n\nb \u2020 Z \u2020 Z H]\nb\nKZ = EHb [H\n\n(6)\n\nK\u03bc = EZ\u223c\u03bc [KZ ].\n\n(7)\n\no(x)\nx\n\n= 0. The proof of above lemma is given in Appendix I.\n\nb is assumed to be diagonal with entries H\nb i,i = Hi . Then, (4) is simply a vector\nHenceforth, in this manuscript, H\n\nnotation of the first n channel uses of (1).\n\nIt is of interest to calculate the maximum mutual information rate\n1\nsup I(X1n ; Y1n )\nn PX n :\u03c1,\u03b2\n\nCn (\u03c1, \u03b2) =\n\n(8)\n\n1\n\nthat can be achieved in (1) with the constraints (2), (3). Here, X1n refers to the set {Xi : 1 \u2264 i \u2264 n}, and\n\n{PX1n : \u03c1, \u03b2} is the set of distributions on X1n that satisfy a peak power constraint \u03c1, and peak to average ratio \u03b2.\nSince the information-theoretic capacity exists [7],\n\nC(\u03c1, \u03b2) = lim Cn (\u03c1, \u03b2)\n\n(9)\n\nn\u2192\u221e\n\nIII. U PPER B OUNDS\nTwo upper bounds on the channel capacity are presented. Let\nU (\u03c1, \u03b2) = log (1 + \u03c1\u03b8(\u03c1, \u03b2)) \u2212 \u03b8(\u03c1, \u03b2)I(\u03c1)\nwhere\nI(\u03c1) =\n\nZ\n\n2\u03c0\n\nlog(1 + \u03c1SH (\u03c9))\n\n0\n\n\u03b8(\u03c1, \u03b2)\n\n=\n\nd\u03c9\n2\u03c0\n\n1 1\n1\nmin( ,\n\u2212 )\n\u03b2 I(\u03c1) \u03c1\n\n(10)\n\n(11)\n(12)\n\nProposition 3.1: For any peak constraint \u03c1 > 0 and peak to average ratio \u03b2 \u2265 1,\nC(\u03c1, \u03b2) \u2264 U (\u03c1, \u03b2)\n\n(13)\n\nProof: We shall first establish that, for any n \u2208 N,\nCn (\u03c1, \u03b2) \u2264 U (\u03c1, \u03b2).\n\n(14)\n\nThe proposition then follows from (9) and (14).\nConsider Cn (\u03c1, \u03b2) for some n > 0.\nCn (\u03c1, \u03b2)\n\n=\n=\n\n1\nI(X1n ; Y1n )\nn\nPX : \u03c1,\u03b2\nn\n1X\nI(Yi ; X1n |Y1i\u22121 )\nsup\nPX : \u03c1,\u03b2 n i=1\nsup\n\n(15)\n(16)\n\n\f4\n\nby the chain rule of mutual information. Denoting the set {Xi : |i| < \u221e} by X, it can be shown that\nI(Yi ; X1n |Y1i\u22121 ) = I(Yi ; X|Y1i\u22121 ).\n\n(17)\n\nfor 1 \u2264 i \u2264 n. This follows from a generalization of the fact that, conditioned on the current input and past inputs\n\nand outputs, the current output is independent of future inputs due to causality of the channel.\n\nLemma 3.1: For any i \u2208 N, the mutual information I(Yi ; X|Y1i\u22121 ) has the following upper bound.\nI(Yi ; X|Y1i\u22121 ) \u2264 log(1 + \u03c1\u03b8) \u2212 \u03b8I(\u03c1)\n\n(18)\n\nwhere \u03b8 = E[|Zi |2 ].\nThe proof of the above lemma is given in Appendix II. Since the above lemma holds for all input distributions\nsatisfying the power constraints, the following upper bound on Cn (\u03c1, \u03b2) is obtained.\nCn (\u03c1, \u03b2)\n\n=\n\nsup I(Yn ; X|YF )\n\n(19)\n\nmax log(1 + \u03c1\u03b8) \u2212 \u03b8I(\u03c1)\n\n(20)\n\nPX : \u03c1,\u03b2\n\n\u2264\n\n1\n0\u2264\u03b8\u2264 \u03b2\n\nThe proof of (14) is completed by noting that the maximum in the above expression is attained at\n1\n1 1\n\u2212 ).\n\u03b8 = min( ,\n\u03b2 I(\u03c1) \u03c1\n\n(21)\n\nA second upper bound on the capacity per unit time is adapted from [3]:\nC(\u03c1, \u03b2) \u2264 Upred (\u03c1, \u03b2)\nwhere\nUpred (\u03c1, \u03b2) =\n\nmax\n\nPave \u2264\u03c1/\u03b2\n\n(\n\nsup\nPX0 : \u03c1,Pave\n\n(22)\n\nI(X0 ; Y0 ) + log\n\n1 + Pave\nI(\u03c1) \u22121\n\n1 + Pave e\n\n\u03c1\n\n!)\n\n(23)\n\nIt is noted that, while the initial part of the derivation of U (\u03c1, \u03b2) is similar to that of Upred (\u03c1, \u03b2) in [3], the evaluation\nof U (\u03c1, \u03b2) does not require an optimization over input distributions. The following lower bound is quoted from\n[6].\nCl1 (\u03c1, \u03b2) =\n\n1\n\u22121\n\u22121\nI(X0 ; Y0 |X\u2212\u221e\n, Y\u2212\u221e\n).\n\u03b2\n\n(24)\n\nwhere the input X is an i.i.d. process such that, for each i, Xi is constant amplitude\n\n\u221a\n\u03c1 and zero mean. The\n\nfollowing upper bound, quoted from [6, (40)] is derived from the analysis of capacity per unit energy.\nCu (\u03c1, \u03b2) =\nIV. C APACITY\n\n1\n(\u03c1 \u2212 I(\u03c1))\n\u03b2\n\n(25)\n\nASYMPTOTE AT SMALL PEAK AND AVERAGE POWERS\n\nIn this section, a low SNR asymptotic analysis is performed as follows: the peak to average ratio, \u03b2, is fixed,\nand vanishingly small values of the peak power constraint \u03c1 (and, consequently, of the average power constraint)\nare considered. The asymptotic behavior of the capacity per unit time C(\u03c1, \u03b2) is captured by the following result.\nProposition 4.1: For \u03b2 > 0, lim\u03c1\u21920 C(\u03c1, \u03b2)/\u03c12 exists and is given by\n(\n\u03bb2\u221e\nif \u03bb2\u221e \u2264\nC(\u03c1, \u03b2)\n8\nlim\n=\n\u03bb\u221e\n\u03c1\u21920\n\u03c12\n\u2212 1 2 if \u03bb\u221e \u2265\n2\u03b2\n\n2\u03b2\n\n2\n\n1\n\u03b2\n1\n\u03b2\n\n(26)\n\n\f5\n\nwhere \u03bb\u221e is given by\n\u03bb\u221e\n\n=\n\ni=\u2212\u221e\nZ 2\u03c0\n\n|RH (i)|2\n\n(27)\n\nd\u03c9\n(28)\n2\u03c0\n0\nis the power spectral density (if it exists) of the fading process H. (It is assumed here that the integral\n=\n\nHere, SH\n\n\u221e\nX\n\n|SH (\u03c9)|2\n\nin (28) exists and is finite.)\nProof: Let the expression on the RHS of (26) be denoted by f (\u03b2). The following lemma is useful in proving\nProposition 4.1.\nLemma 4.1:\n\nU (\u03c1, \u03b2)\n= f (\u03b2)\n\u03c12\nThe proof of the above lemma is in Appendix III. From Lemma 4.1 and Proposition 3.1, it follows that\nlim\n\n\u03c1\u21920\n\nlim sup\n\u03c1\u21920\n\nC(\u03c1, \u03b2)\n\u2264 f (\u03b2)\n\u03c12\n\n(29)\n\n(30)\n\nThe following lemma completes the proof of Proposition 4.1.\nLemma 4.2:\nlim inf\n\u03c1\u21920\n\nC(\u03c1, \u03b2)\n\u2265 f (\u03b2)\n\u03c12\n\n(31)\n\nThe proof of Lemma 4.2 is given in Appendix IV.\nProposition 4.1 and Lemma 4.1 together prove that U (\u03c1, \u03b2) is asymptotically tight as peak power \u03c1 \u2192 0 for a\n\nfixed \u03b2 > 0. The second upper bound Upred (\u03c1, \u03b2) is also found to be asymptotically tight.\n\nIt can be shown that, for a fixed \u03b2, the bounds in (24) and (25) have the following limits.\nCl1 (\u03c1, \u03b2)\n\u03c12\nCu (\u03c1, \u03b2)\nlim\n\u03c1\u21920\n\u03c12\n\nlim\n\n\u03c1\u21920\n\n=\n=\n\n\u03bb\u221e \u2212 1\n2\u03b2\n\u03bb\u221e\n2\u03b2\n\n(32)\n(33)\n\nThe above limits were derived in [4] for the specific case when \u03b2 = 1. By comparing the above limits with f (\u03b2),\nit is seen that the bounds in [4, 6] are not, in general, asymptotically tight. Nevertheless, when there is only a peak\nconstraint (\u03b2 = 1), the lower bound Cl1 (\u03c1, \u03b2) is tight when \u03bb\u221e \u2265 2; i.e. when there is sufficient memory in the\n\nfading process.\n\nV. C ONTINUOUS\n\nTIME CHANNELS\n\nA continuous-time version of the channel modeled in (1) is considered. Following [7, (20)], the channel model\nis given by:\nY (t) = H(t)X(t) + W (t), 0 \u2264 t \u2264 T\n\n(34)\n\nHere, W (t) is a complex proper Gaussian white noise process with E[W (s)W (t)] = \u03b4(s \u2212 t), and (H(t) : \u2212\u221e <\n\nt < \u221e) is a stationary ergodic proper complex Gaussian process such that E[|H(t)|2 ] = 1. A deterministic signal\n\nX = (X(t) : 0 \u2264 t \u2264 T ),where T is the duration of the signal, is the channel input and Y (t) is the observed\n\noutput.\n\n\f6\n\nAs in [7], both average power and peak power constraints are imposed on the transmitter. The power constraints\nare defined as follows:\n1\nT\n\nZ\n\nT\n\n|X(t)|2 dt\n\n\u2264\n\nPave ,\n\n(35)\n\nsup |X(t)|2\n\n\u2264\n\nPpeak .\n\n(36)\n\n0\n\n0\u2264t\u2264T\n\nThere is no bandwidth constraint, other than that the input codewords are required to be Borel measurable functions\nof t.\nProposition 5.1: The capacity per unit time of the continuous-time channel under power constraints Pave and\nPpeak is given by\nC(Pave , Ppeak ) = Pave \u2212\nwhere\nI(Ppeak ) =\n\nZ\n\nPave\nI(Ppeak )\nPpeak\n\n\u221e\n\nlog(1 + Ppeak SH (\u03c9))\n\n\u2212\u221e\n\nd\u03c9\n.\n2\u03c0\n\n(37)\n\n(38)\n\nwhere SH (\u03c9) denotes the density of the absolutely continuous component of the power spectral measure of H.\nAn outline of the proof is now provided.\nFollowing [7, (12) and Prop. 3.3], the capacity per unit energy Cp (Ppeak ) of this channel gives the following\nupper bound on C(Pave , Ppeak ).\nPave\nI(Ppeak )\n(39)\nPpeak\nTo obtain a matching lower bound, firstly, a discrete-time channel is derived from the continuous-time channel by\nC(Pave , Ppeak ) \u2264 Pave \u2212\n\nconstraining the continuous-time transceiver to using only certain encoding and decoding methods. An information\ntheoretic lower bound on the capacity of the discrete-time channel, presented in [6], is then used. Next, the connection\nbetween mutual information and the mean square error in estimating the input given the output [10] is used to\nevaluate the lower bound. This lower bound is then translated to continuous-time and is shown to coincide with\nthe upper bound, thus giving the capacity per unit time in closed form.\nZhang and Laneman [4] observe that the upper bound (39) is tight when Pave = Ppeak by obtaining a matching\nlower bound using a constructive scheme. The scheme uses an interleaver to convert the channel with correlated\nfading into a set of parallel sub-channels (PSC), with i.i.d. fading in each PSC. While this works for channels with\nfinite memory, i.e. there exists K > 0 such that RH (k) = 0 for all |k| > K, it is unclear how the proof extends\n\nto more general ergodic fading channels with infinite memory. In our proof technique, this problem is avoided by\nobtaining a lower bound using purely information-theoretic methods (see [6]).\nVI. F UTURE\n\nDIRECTIONS\n\nThe low SNR regime is largely motivated by broadband channels, where the total transmit power is constrained\nwhile the available number of degrees of freedom is virtually unlimited. Since broadband channels are, in truth,\nmultipath channels, a specular multipath fading process models such channels in a more realistic manner. We seek\nto investigate the asymptotic capacity of such multipath fading channels. The vector channel model (4) and Lemma\n2.1 apply to these channels.\nAnother direction is to obtain capacity bounds that are good for all SNR. The bound Upred (23) is an example\nof such a bound. This bound is asymptotically tight at both high SNR [3] and at low SNR (for \u03b2 \u2265 2). Deriving\n\nand comparing matching lower bounds to such upper bounds will help in characterizing the capacity of correlated\nfading channels for all values of SNR.\n\n\f7\n\nA PPENDIX I\nP ROOF\n\nOF\n\nL EMMA 2.1\n\nConsider the output of the channel modeled in (1). Then, Y \u2020 Yn\u00d7n is given by\nY \u2020 Yn\u00d7n = \u03c1H \u2020 Z \u2020 ZH + W \u2020 W\n\n(40)\n\nKY = \u03c1KZ + I\n\n(41)\n\nDenoting cov(Y |Z) by KY ,\nwhere KZ is defined in (6).\n\nThe probability density function (pdf) of Y conditioned on Z is given by\nq(Y |Z) =\nSo,\n\nexp(\u2212Y KY\u22121 Y \u2020 )\n\u03c0 n det(KY )\n\nexp(\u2212Y KY\u22121 Y \u2020 + Y Y \u2020 )\nq(Y |Z)\n=\nq(Y |0)\ndet(KY )\n\nFollowing [11], let\n\n(42)\n\n(43)\n\nq(Y |Z)\n\u22121\nq(Y |0)\n\n(44)\n\nKY\u22121 = I \u2212 \u03c1KZ + o(\u03c1)\n\n(45)\n\ndet(KY ) = 1 + \u03c1T r(KZ ) + o(\u03c1)\n\n(46)\n\n\u2206q(Y |Z) =\nSince, for small \u03c1,\n\nand\n\nwe have,\n\u2206q(Y |Z) =\n=\nHere, o(x) is used in the sense that limx\u21920\nbe shown that\n\no(x)\nx\n\nexp(\u03c1Y KZ Y \u2020 + o(\u03c1))\n\u22121\n1 + \u03c1T r(KZ ) + o(\u03c1)\n\u0001\n\u03c1 Y KZ Y \u2020 \u2212 T r(KZ ) + o(\u03c1)\n\n=\n\ne\nh(Y ) =\n=\n\nFurther,\n\n(48)\n\n= 0. Using [11, (13),(14)] and proceeding as in [11, \u00a7VII], it can\n\n\u0013\u0015\u0015\n\u0012\n\u0014 \u0014\nq(Y |Z)\nq(Y |Z)\nlog\n\u2212EX\u223c\u03bc\u01eb E0\nq(Y |0)\nq(Y |0)\nh h\n\u00012 ii\n\u03c12\n\u2212 EZ\u223c\u03bc E0 Y KZ Y \u2020 \u2212 T r(KZ )\n+ o(\u03c12 )\n2 \u0014\n\u0013\u0015\n\u0012\nq(Y |\u03bc\u01eb )\nq(Y |\u03bc\u01eb )\nlog\n\u2212E0\nq(Y |0)\nq(Y |0)\nh\n\u0002\n\u0003\u00012 i\n\u03c12\n\u2212 E0 EZ\u223c\u03bc Y KZ Y \u2020 \u2212 T r(KZ )\n+ o(\u03c12 )\n2\n\ne\nh(Y |Z) =\n\n(47)\n\n\u0003\n\u0002\nE0 Y KZ Y \u2020 =\nh\n\u00012 i\nE0 Y KZ Y \u2020\n=\n\n(49)\n(50)\n(51)\n(52)\n\nT r(KZ )\n\n(53)\n\n(T r(KZ ))2 + T r(KZ2 )\n\n(54)\n\nThis, when substituted in (50) yields\n\n\u0002\n\u0003\n\u03c12\ne\nh(Y |Z) = \u2212 EZ\u223c\u03bc T r(KZ2 ) + o(\u03c12 )\n2\n\n(55)\n\n\f8\n\nIt can be shown in a similar fashion that\n2\n\n\u03c1\ne\nh(Y ) = \u2212 T r(K\u03bc2 ) + o(\u03c12 )\n2\n\n(56)\n\nwhere K\u03bc is defined in (7). Since the mutual information I(Z; Y ) is given by\nI(Z; Y ) = e\nh(Y ) \u2212 e\nh(Y |Z),\n\nfor low SNR, I(Z; Y ) is given by\nI(Z; Y ) =\n\n(57)\n\n\u0002\n\u0003\n\u0001\n\u03c12\nEZ\u223c\u03bc T r(KZ2 ) \u2212 T r(K\u03bc2 ) + o(\u03c12 )\n2\n\n(58)\n\nA PPENDIX II\nP ROOF\n\nOF\n\nL EMMA 3.1\n\nFirst, we shall prove the following lemma.\nLemma 2.1: For any i \u2208 Z and F \u2282 Z \u2212 {i}, the mutual information I(Yi ; X|YF ) has the following upper\n\nbound.\n\n2\n)]\nI(Yi ; X|YF ) \u2264 log(1 + \u03c1\u03b8) \u2212 \u03b8 log(1 + \u03c1\u03c3\u0304i|F\n\n(59)\n\nwhere \u03b8 = E[|Zi |2 ].\nProof:\nI(Yi ; X|YF ) =\n\u2264\n\nI(Yi ; X, YF ) \u2212 I(Yi ; YF )\n\n(60)\n\nI(Yi ; X, YF )\n\n(61)\n\nThe output Yi can be expressed as follows.\nYi\n\n=\n\nHi Xi + Wi\n\n(62)\n\n=\n\nbi + H\ne i )Xi + Wi\n(H\n\n(63)\n\nb i is the following MMSE estimate of Hi :\nwhere H\ne i is the error in estimation.\nand, H\n\nb i = E[Hi |XF , YF ]\nH\n\n(64)\n\ne i = Hi \u2212 H\nbi\nH\n\n(65)\n\n2\ne i |2 ]\n= E[|H\n\u03c3i|F\n\n(66)\n\nLet the variance of the error be denoted by\n\n2\n2\n2\n(XF ) over all X satisfying the peak\nbe the minimum of \u03c3i|F\nIt is noted that \u03c3i|F\nis a function of XF . Let \u03c3\u0304i|F\n2\nis a constant independent of\nconstraint. This minimum is achieved when Xk = \u03c1 for all k \u2208 F . This value of \u03c3\u0304i|F\n\nthe distribution of X (see [12, pg. 440]).\n\n2\nIt follows from (63) that, given X and YF , Yi is PCN with variance 1 + |Xi |2 \u03c3i|F\n. So,\n\nh(Yi |X, YF ) =\n\u2265\n\n2\nlog(\u03c0e) + E[log(1 + |Xi |2 \u03c3i|F\n)]\n\n(67)\n\nlog(\u03c0e) + E[log(1 +\n\n(68)\n\n2\n)]\n|Xi |2 \u03c3\u0304i|F\n\nAlso, the (unconditional) variance of Yi is 1 + E[|Xi |2 ]. So,\nh(Yi ) \u2264 log(\u03c0e) + log(1 + E[|Xi |2 ])\n\n(69)\n\n\f9\n\nSo,\nI(Yi ; X|YF ) \u2264\n1\n\u03b2\n\n(70)\n\nlog(1 + \u03c1E[|Zi | ]) \u2212 E[log(1 +\n\n(71)\n\n2\n\n=\nIf E[|Zi |2 ] = \u03b8, then 0 \u2264 \u03b8 \u2264\n\n2\nlog(1 + E[|Xi |2 ]) \u2212 E[log(1 + |Xi |2 \u03c3\u0304i|F\n)]\n\n2\n)]\n\u03c1|Zi |2 \u03c3\u0304i|F\n\nfrom the average power constraint, and due to the concavity of log, the upper\n\nbound is maximized by\n\n|Zi |2 =\n\n(\n\n1\n\nwith probability\n\n\u03b8\n\n0\n\nwith probability\n\n1\u2212\u03b8\n\n(72)\n\nThis yields the following lemma.\nIt is noted that, if F \u2282 (\u2212\u221e, n \u2212 1], then\n2\n2\n\u03c3\u0304n|F\n\u2264 \u03c3\u0304n|(\u2212\u221e,n\u22121]\n\n(73)\n\n2\n2\nThe causal prediction error \u03c3\u0304n|(\u2212\u221e,n\u22121]\nis clearly independent of n, and can be equivalently denoted by \u03c3\u03040|(\u2212\u221e,\u22121]\n.\n\nFollowing standard results in estimation theory, the causal prediction error is given by\n2\n=\n\u03c30|(\u2212\u221e,\u22121]\n\n1\n(exp(I(\u03c1)) \u2212 1)\n\u03c1\n\n(74)\n\nwhere I(\u03c1) is given by eq:Irho. From Lemma 2.1, it follows that\nI(Yn ; X|YF ) \u2264\n=\n\n2\n)\nlog(1 + \u03c1\u03b8) \u2212 \u03b8 log(1 + \u03c1\u03c30|(\u2212\u221e,\u22121]\n\n(75)\n\nlog(1 + \u03c1\u03b8) \u2212 \u03b8I(\u03c1)\n\n(76)\n\nwhere \u03b8 = E[|Zi |2 ]. This completes the proof of Lemma 3.1.\nFor the interested reader, Lemma 2.1 can be applied to any F \u2282 Z \u2212 {n} in the following manner.\n2\n\u03c3\u0304n|F\n\n2\n\u2264 \u03c3\u0304n|Z\u2212{n}\nZ 2\u03c0\n= 1\u2212\u03c1\n0\n\n(77)\n2\nSH\n(\u03c9)\n\nd\u03c9\n1 + \u03c1SH (\u03c9) 2\u03c0\n\n(78)\n\nA PPENDIX III\nP ROOF\n\nOF\n\nL EMMA 4.1\n\nLemma 4.1 is proved as follows.\nFirst, the asymptotic behaviour of the upper bound U (\u03c1, \u03b2) is analyzed. For small values of \u03c1, the expression\nI(\u03c1), as given in (11), can be approximated as follows.\nZ 2\u03c0\nd\u03c9\nI(\u03c1) =\nlog(1 + \u03c1SH (\u03c9))\n2\u03c0\n0\n\u0013\nZ 2\u03c0 \u0012\n2\n\u03c1 2\n\u03c13 3\nd\u03c9\n=\n\u03c1SH (\u03c9) \u2212 SH (\u03c9) + SH (\u03c9)\n+ o(\u03c13 )\n2\n3\n2\u03c0\n0\n\u03c12\n= \u03c1 \u2212 \u03bb\u221e + o(\u03c12 )\n2\n\n(79)\n(80)\n(81)\n\nwhere \u03bb\u221e is given by (28). Using the above Taylor's series expansion of I(\u03c1), it is easy to show that\nlim\n\n\u03c1\u21920\n\n1\n\u03bb\u221e\n1\n\u2212 =\nI(\u03c1) \u03c1\n2\n\nThe asymptotic behaviour of U (\u03c1, \u03b2) as a function of \u03c1 depends on whether\nfollowing two cases.\n\n(82)\n1\n\u03b2\n\n>\n\n\u03bb\u221e\n2 .\n\nThis is considered in the\n\n\f10\n\nIf\n\n1\n\u03b2\n\n<\n\n\u03bb\u221e\n2 ,\n\nthen there exists a \u03c10 > 0 small enough such that, for all \u03c1 < \u03c10 ,\n1\n1\n1\n\u2264\n\u2212 .\n\u03b2\nI(\u03c1) \u03c1\n\nFor such \u03c1, U (\u03c1, \u03b2) is given by\nU (\u03c1, \u03b2) = log(1 +\n\n(83)\n\n1\n\u03c1\n) \u2212 I(\u03c1)\n\u03b2\n\u03b2\n\n(84)\n\nThen, for fixed \u03b2, the asymptotic behaviour of U (\u03c1, \u03b2) for small values of \u03c1 is given by\n\u0012\n\u0013\nU (\u03c1, \u03b2)\n\u03c1\n1\n1\nlim\n= lim 2 log(1 + ) \u2212 I(\u03c1)\n\u03c1\u21920\n\u03c1\u21920 \u03c1\n\u03c12\n\u03b2\n\u03b2\n\u0012\n\u0013\n1\n1\n\u03bb\u221e \u2212\n=\n2\u03b2\n\u03b2\nSimilarly, when\n\n1\n\u03b2\n\n>\n\n\u03bb\u221e\n2 ,\n\n(85)\n(86)\n\nthen there exists a \u03c10 > 0 such that , for all \u03c1 < \u03c10 ,\n1\n1\n1\n\u2265\n\u2212 .\n\u03b2\nI(\u03c1) \u03c1\n\n(87)\n\nThen, U (\u03c1, \u03b2) is given by\nU (\u03c1, \u03b2) =\n=\nFor small values of \u03c1,\n\nI(\u03c1)\n\u03c1\n\nI(\u03c1)\nI(\u03c1)\n\u22121+\n\u2212 log\n\u03c1\n\u03c1\n\u0012\n\u0013\n\u001a\n\u0012\n\u0013\u001b\nI(\u03c1)\nI(\u03c1)\n\u2212 1 \u2212 log 1 +\n\u22121\n\u03c1\n\u03c1\n\n(88)\n(89)\n\n\u2212 1 behaves as follows.\n\u03bb\u221e\n\u03bd\u221e\nI(\u03c1)\n\u2212 1 = \u2212\u03c1\n\u2212 \u03c12\n+ o(\u03c12 )\n\u03c1\n2\n3\n\n(90)\n\nFrom (89) and (90), and using the fact that, for 0 < x \u226a 1,\nx \u2212 log(1 + x) = x2 /2 + o(x2 ),\n\n(91)\n\nthe asymptotic behaviour of U (\u03c1, \u03b2) in the limit \u03c1 \u2192 0 is given by\n\u0012\n\u00132\nU (\u03c1, \u03b2)\n1\nI(\u03c1)\nlim\n=\nlim\n\u2212\n1\n\u03c1\u21920\n\u03c1\u21920 2\u03c12\n\u03c12\n\u03c1\n2\n\u03bb\u221e\n=\n8\nFrom (86) and (93), it follows that\nlim\n\n\u03c1\u21920\n\n(92)\n(93)\n\nU (\u03c1, \u03b2)\n= f (\u03b2)\n\u03c12\n\n(94)\n\nA PPENDIX IV\nP ROOF\n\nOF\n\nL EMMA 4.2\n\nb is diagonal and H\nb i,i = Hi ), we have\nSetting Z = (Z1 , . . . , Zn ) and Vi = |Zi |2 in Lemma 2.1 (where H\nT r(KZ2 )\n\n=\n\nT r(K\u03bc2 ) =\n\nn\nX\n\ni=1\nn\nX\ni=1\n\n2\n\n|Vi | + 2\n\nn X\nn\nX\n\n|RH (i \u2212 j)|2 Vi Vj\n\ni=1 j>i\nn X\nn\nX\n\nE 2 [Vi ] + 2\n\ni=1 j>i\n\n|RH (i \u2212 j)|2 |E[Zi\u2217 Zj ]|\n\n2\n\n(95)\n(96)\n\n\f11\n\nSubstituting the above in (5),\nI(Z; Y )\n\n=\n\n\u2212\n\n\uf8f1\nn\n\uf8f2X\n\uf8f3\n\ni=1\n\nn\nX\ni=1\n\nn X\nn\nX\n\u0003\n\u0002\n|RH (i \u2212 j)|2 EZ\u223c\u03bc [Vi Vj ]\nEZ\u223c\u03bc |Vi |2 + 2\ni=1 j>i\n\n2\nEZ\u223c\u03bc\n[Vi ] \u2212 2\n\nn X\nn\nX\ni=1 j>i\n\n|RH (i \u2212 j)|2 |EZ\u223c\u03bc [Zi\u2217 Zj ]|\n\nConsider the following distribution, \u03bc\nb, on Z1n . Let\n\n2\n\n\uf8fc\n\uf8fd \u03c12\n\uf8fe 2\n\n+ o(\u03c12 )\n\n|Zi | = I{U\u2264a}\n\n(97)\n\n(98)\n\nwhere U is an auxiliary random variable uniformly distributed in [0, 1], and a \u2208 [0, 1/\u03b2] (to satisfy the average\npower constraint.) Further, let the phases of Zi be independent and uniformly distributed in [0, 2\u03c0], for all i. So,\n\nfor each i, |Zi | \u2208 {0, 1}. Further, for any i, j,\nE[Vi ] = E[Vi Vj ] = a\n\n(99)\n\nAlso, E[Zi\u2217 Zj ] = 0 if i 6= j. The corresponding mutual information rate is obtained by substituting the above in\n\n(97).\n\nwhere\n\n\u0001\n1\n1\na\u03bbn \u2212 a2 + o(\u03c12 )\nI(Z1n ; Y1n ) =\nn\n2\nn\n\n\u03bbn =\nSet\na=\n\n(100)\n\nn\n\n1 XX\n|RH (i \u2212 j)|2\nn i=1 j=1\n(\n\n\u03bbn\n2\n1\n\u03b2\n\nif\n\n\u03bbn\n2\n\n\u2264\n\n(101)\n\n1\n\u03b2\n\n(102)\n\nelse\n\nIt can be checked that, for the above choice of a, the distribution \u03bc\nb does not violate the power constraints. Let the\ncorresponding mutual information rate be denoted by Ln (\u03c1, \u03b2). Then, Ln (\u03c1, \u03b2) is given by:\n\uf8f1\n\u03bb2n 2\n\uf8f2\no(\u03c12 )\nif \u03bb2n \u2264 \u03b21\n8 \u03c1 +\n\u0010\n\u0011\nLn (\u03c1, \u03b2) =\n\uf8f3 \u03bbn \u2212 1 2 \u03c12 + o(\u03c12 ) if \u03bbn \u2265 1\n2\u03b2\n2\u03b2\n2\n\u03b2\n\n(103)\n\nFrom (8) and (9), it follows that\n\nC(\u03c1, \u03b2) \u2265 Ln (\u03c1, \u03b2)\n\nDividing by \u03c12 and taking the limit \u03c1 \u2192 0,\nC(\u03c1, \u03b2)\nlim inf\n\u03c1\u21920\n\u03c12\n\nLn (\u03c1, \u03b2)\nlim\n\u03c1\u21920\n\u03c12\n\uf8f1\n\u03bb2n\n\uf8f2\nif\n8\n\u0010\n\u0011\n=\n\u03bb\n1\nn\n\uf8f3\nif\n2\u03b2 \u2212 2\u03b2 2\n\n(104)\n\n\u2265\n\n(105)\n\u03bbn\n2\n\u03bbn\n2\n\n\u2264\n\u2265\n\n1\n\u03b2\n1\n\u03b2\n\nSince the above lower bound holds for all n, the following lower bound also holds.\nLn (\u03c1, \u03b2)\nC(\u03c1, \u03b2)\n\u2265 lim lim\nlim inf\nn\u2192\u221e \u03c1\u21920\n\u03c1\u21920\n\u03c12\n\u03c12\nIt can be shown (see Appendix V) that\nlim \u03bbn = \u03bb\u221e ,\n\n2\u03b2\n\n2\u03b2\n\n(107)\n\n(108)\n\nn\u2192\u221e\n\nwhere \u03bb\u221e is given by (28). It follows from (106), (107) and (108) that\n\uf8f1\n\u03bb2\u221e\n\uf8f2\nif\n1\n8\n\u0011\n\u0010\nlim inf 2 C(\u03c1, \u03b2) \u2265\n\u03c1\u21920 \u03c1\n\uf8f3 \u03bb\u221e \u2212 1 2\nif\n\n(106)\n\n\u03bb\u221e\n2\n\u03bb\u221e\n2\n\n\u2264\n\u2265\n\n1\n\u03b2\n1\n\u03b2\n\n(109)\n\n\f12\n\nA PPENDIX V\nP ROVING \u03bbn \u2192 \u03bb\u221e\nIt is easy to see that \u03bbn \u2264 \u03bb\u221e for any n. So,\nlim sup \u03bbn \u2265 \u03bb\u221e\n\n(110)\n\nn\u2192\u221e\n\nBy assumption, \u03bb\u221e exists and is finite. So, for \u01eb > 0, there exists N \u2208 N large enough such\nX\n\u01eb\n|RH (i)|2 > \u03bb\u221e \u2212 .\n2\n\n(111)\n\n|i|<N\n\nLet\nK=\n\nX\n\n|i|<N\n\nFor any n > N such that\n\nK\nn\n\n|i||RH (i)|2\n\n(112)\n\n< 2\u01eb ,\n\u03bbn\n\n=\n\nX\n\n|i|<n\n\n=\n\nX\n\n|i|<n\n\n\u2265\n=\n\nX\n\n|i|<n\n\nX\n\n|i|<N\n\n>\n>\n\n|RH (i)|2 (1 \u2212\n\n|i|\n)\nn\n\n|RH (i)|2 \u2212\n\nK\n\u2212\nn\n\n|RH (i)|2 \u2212\n\nK\n\u2212\nn\n\n|RH (i)|2 \u2212\n\nK\nn\n\n(113)\nX\n\nN \u2264|i|<n\n\nX\n\nN \u2264|i|<n\n\n\u01eb\nK\n\u2212\n2\nn\n\u03bb\u221e \u2212 \u01eb\n\u03bb\u221e \u2212\n\n|RH (i)|2\n|RH (i)|2\n\n|i|\nn\n\n(114)\n(115)\n(116)\n(117)\n(118)\n\nSince \u01eb is arbitrary, it follows that\nlim inf \u03bbn \u2265 \u03bb\u221e\n\n(119)\n\nlim \u03bbn = \u03bb\u221e\n\n(120)\n\nn\u2192\u221e\n\nFrom (110) and (119), it follows that\nn\u2192\u221e\n\nR EFERENCES\n[1] M. M\u00e9dard and R. G. Gallager, \"Bandwidth scaling for fading multipath channels,\" IEEE Transactions on Information Theory, vol. 48,\npp. 840\u2013852, Apr. 2002.\n[2] V. G. Subramanian and B. Hajek, \"Broad-band fading channels: Signal burstiness and capacity,\" IEEE Transactions on Information Theory,\nvol. 48, pp. 809\u2013827, Apr. 2002.\n[3] T. Koch and A. Lapidoth, \"The fading number and degrees of freedom in non-coherent mimo fading channels: A peace pipe,\" Proceedings\nof International Symposium on Information Theory, September 2005.\n[4] W. Zhang and N. Laneman, \"How good is phase-shift keying for peak-limited fading channels in the low-snr regime,\" Proceedings of\nAllerton Conference on Communications, Control, and Computing, Monticello, IL, September 2005.\n[5] T. Li, X. Jin, and O. M. Collins, \"Approaching capacity on correlated fading channels with unknown state,\" Proceedings of International\nSymposium on Information Theory, September 2005.\n[6] V. Sethuraman, B. Hajek, and K. Narayanan, \"Capacity bounds for noncoherent fading channels with a peak constraint,\" Proceedings of\nInternational Symposium on Information Theory, September 2005.\n[7] V. Sethuraman and B. Hajek, \"Capacity per unit energy for fading channels with a peak constraint,\" IEEE Transactions on Information\nTheory, vol. 51, pp. 3102\u20133120, September 2005.\n[8] S. Verd\u00fa, \"On channel capacity per unit cost,\" IEEE Transactions on Information Theory, vol. 36, pp. 1019\u20131030, Sept. 1990.\n\n\f13\n\n[9] A. Viterbi, \"Performance of an M-ary orthogonal communication systems using stationary stochastic signals,\" IEEE Transactions on\nInformation Theory, vol. 13, pp. 414\u2013421, July 1967.\n[10] D. Guo, S. S. (Shitz), and S. Verd\u00fa, \"Mutual information and minimum mean-square eerror in Gaussian channels,\" IEEE Transactions on\nInformation Theory, vol. 51, pp. 1261\u20131282, April 2005.\n[11] B. Hajek and V. Subramanian, \"Capacity and reliability function for small peak signal constraints,\" IEEE Trans. on Information Theory,\nvol. 48, pp. 829\u2013839, 2002.\n[12] A. Lapidoth, \"On the asymptotic capacity of stationary Gaussian fading channels,\" IEEE Transactions on Information Theory, vol. 51,\npp. 437\u2013446, Feb. 2005.\n\n\f"}