{"id": "http://arxiv.org/abs/cs/0609088v1", "guidislink": true, "updated": "2006-09-15T23:47:03Z", "updated_parsed": [2006, 9, 15, 23, 47, 3, 4, 258, 0], "published": "2006-09-15T23:47:03Z", "published_parsed": [2006, 9, 15, 23, 47, 3, 4, 258, 0], "title": "Deriving the Normalized Min-Sum Algorithm from Cooperative Optimization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0609130%2Ccs%2F0609021%2Ccs%2F0609056%2Ccs%2F0609166%2Ccs%2F0609088%2Ccs%2F0609103%2Ccs%2F0609162%2Ccs%2F0609126%2Ccs%2F0609070%2Ccs%2F0609167%2Ccs%2F0609031%2Ccs%2F0609054%2Ccs%2F0609105%2Ccs%2F0609165%2Ccs%2F0609002%2Ccs%2F0609049%2Ccs%2F0609164%2Ccs%2F0609067%2Ccs%2F0609080%2Ccs%2F0609076%2Ccs%2F0609158%2Ccs%2F0609039%2Ccs%2F0609009%2Ccs%2F0609136%2Ccs%2F0609008%2Ccs%2F0609069%2Ccs%2F0609041%2Ccs%2F0609149%2Ccs%2F0609122%2Ccs%2F0609097%2Ccs%2F0609139%2Ccs%2F0609007%2Ccs%2F0609037%2Ccs%2F0609124%2Ccs%2F0609053%2Ccs%2F0609116%2Ccs%2F0609141%2Ccs%2F0609029%2Ccs%2F0609001%2Ccs%2F0609140%2Ccs%2F0609028%2Ccs%2F0609163%2Ccs%2F0609159%2Ccs%2F0609134%2Ccs%2F0609035%2Ccs%2F0609017%2Ccs%2F0609022%2Ccs%2F0609061%2Ccs%2F0609016%2Ccs%2F0609077%2Ccs%2F0609047%2Ccs%2F0609094%2Ccs%2F0609074%2Ccs%2F0609108%2Ccs%2F0609086%2Ccs%2F0609102%2Ccs%2F0609084%2Ccs%2F0609052%2Ccs%2F0609095%2Ccs%2F0609125%2Ccs%2F0609065%2Ccs%2F0609082%2Ccs%2F0609051%2Ccs%2F0609112%2Ccs%2F0609046%2Ccs%2F0609010%2Ccs%2F0609093%2Ccs%2F0609045%2Ccs%2F0609120%2Ccs%2F0609104%2Ccs%2F0609115%2Ccs%2F0609150%2Ccs%2F0609129%2Ccs%2F0609018%2Ccs%2F0609092%2Ccs%2F0609033%2Ccs%2F0609146%2Ccs%2F0609087%2Ccs%2F0609106%2Ccs%2F0609160%2Ccs%2F0609138%2Ccs%2F0307008%2Ccs%2F0307041%2Ccs%2F0307035%2Ccs%2F0307048%2Ccs%2F0307003%2Ccs%2F0307034%2Ccs%2F0307021%2Ccs%2F0307010%2Ccs%2F0307073%2Ccs%2F0307072%2Ccs%2F0307062%2Ccs%2F0307004%2Ccs%2F0307054%2Ccs%2F0307018%2Ccs%2F0307006%2Ccs%2F0307064%2Ccs%2F0307066%2Ccs%2F0307031%2Ccs%2F0307001%2Ccs%2F0307027&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Deriving the Normalized Min-Sum Algorithm from Cooperative Optimization"}, "summary": "The normalized min-sum algorithm can achieve near-optimal performance at\ndecoding LDPC codes. However, it is a critical question to understand the\nmathematical principle underlying the algorithm. Traditionally, people thought\nthat the normalized min-sum algorithm is a good approximation to the\nsum-product algorithm, the best known algorithm for decoding LDPC codes and\nTurbo codes. This paper offers an alternative approach to understand the\nnormalized min-sum algorithm. The algorithm is derived directly from\ncooperative optimization, a newly discovered general method for\nglobal/combinatorial optimization. This approach provides us another\ntheoretical basis for the algorithm and offers new insights on its power and\nlimitation. It also gives us a general framework for designing new decoding\nalgorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0609130%2Ccs%2F0609021%2Ccs%2F0609056%2Ccs%2F0609166%2Ccs%2F0609088%2Ccs%2F0609103%2Ccs%2F0609162%2Ccs%2F0609126%2Ccs%2F0609070%2Ccs%2F0609167%2Ccs%2F0609031%2Ccs%2F0609054%2Ccs%2F0609105%2Ccs%2F0609165%2Ccs%2F0609002%2Ccs%2F0609049%2Ccs%2F0609164%2Ccs%2F0609067%2Ccs%2F0609080%2Ccs%2F0609076%2Ccs%2F0609158%2Ccs%2F0609039%2Ccs%2F0609009%2Ccs%2F0609136%2Ccs%2F0609008%2Ccs%2F0609069%2Ccs%2F0609041%2Ccs%2F0609149%2Ccs%2F0609122%2Ccs%2F0609097%2Ccs%2F0609139%2Ccs%2F0609007%2Ccs%2F0609037%2Ccs%2F0609124%2Ccs%2F0609053%2Ccs%2F0609116%2Ccs%2F0609141%2Ccs%2F0609029%2Ccs%2F0609001%2Ccs%2F0609140%2Ccs%2F0609028%2Ccs%2F0609163%2Ccs%2F0609159%2Ccs%2F0609134%2Ccs%2F0609035%2Ccs%2F0609017%2Ccs%2F0609022%2Ccs%2F0609061%2Ccs%2F0609016%2Ccs%2F0609077%2Ccs%2F0609047%2Ccs%2F0609094%2Ccs%2F0609074%2Ccs%2F0609108%2Ccs%2F0609086%2Ccs%2F0609102%2Ccs%2F0609084%2Ccs%2F0609052%2Ccs%2F0609095%2Ccs%2F0609125%2Ccs%2F0609065%2Ccs%2F0609082%2Ccs%2F0609051%2Ccs%2F0609112%2Ccs%2F0609046%2Ccs%2F0609010%2Ccs%2F0609093%2Ccs%2F0609045%2Ccs%2F0609120%2Ccs%2F0609104%2Ccs%2F0609115%2Ccs%2F0609150%2Ccs%2F0609129%2Ccs%2F0609018%2Ccs%2F0609092%2Ccs%2F0609033%2Ccs%2F0609146%2Ccs%2F0609087%2Ccs%2F0609106%2Ccs%2F0609160%2Ccs%2F0609138%2Ccs%2F0307008%2Ccs%2F0307041%2Ccs%2F0307035%2Ccs%2F0307048%2Ccs%2F0307003%2Ccs%2F0307034%2Ccs%2F0307021%2Ccs%2F0307010%2Ccs%2F0307073%2Ccs%2F0307072%2Ccs%2F0307062%2Ccs%2F0307004%2Ccs%2F0307054%2Ccs%2F0307018%2Ccs%2F0307006%2Ccs%2F0307064%2Ccs%2F0307066%2Ccs%2F0307031%2Ccs%2F0307001%2Ccs%2F0307027&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The normalized min-sum algorithm can achieve near-optimal performance at\ndecoding LDPC codes. However, it is a critical question to understand the\nmathematical principle underlying the algorithm. Traditionally, people thought\nthat the normalized min-sum algorithm is a good approximation to the\nsum-product algorithm, the best known algorithm for decoding LDPC codes and\nTurbo codes. This paper offers an alternative approach to understand the\nnormalized min-sum algorithm. The algorithm is derived directly from\ncooperative optimization, a newly discovered general method for\nglobal/combinatorial optimization. This approach provides us another\ntheoretical basis for the algorithm and offers new insights on its power and\nlimitation. It also gives us a general framework for designing new decoding\nalgorithms."}, "authors": ["Xiaofei Huang"], "author_detail": {"name": "Xiaofei Huang"}, "author": "Xiaofei Huang", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ITW2.2006.323788", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cs/0609088v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0609088v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Accepted by IEEE Information Theory Workshop, Chengdu, China, 2006", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0609088v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0609088v1", "journal_reference": null, "doi": "10.1109/ITW2.2006.323788", "fulltext": "Deriving the Normalized Min-Sum Algorithm from\nCooperative Optimization\nXiaofei Huang\n\narXiv:cs/0609088v1 [cs.IT] 15 Sep 2006\n\nSchool of Information Science and Technology, Tsinghua University, Beijing, P.R. China, 100084\nEmail: huangxiaofei@ieee.org\n(Accepted by IEEE Information Theory Workshop, Chengdu, China, 2006)\n\nAbstract- The normalized min-sum algorithm can achieve\nnear-optimal performance at decoding LDPC codes. However,\nit is a critical question to understand the mathematical principle\nunderlying the algorithm. Traditionally, people thought that the\nnormalized min-sum algorithm is a good approximation to the\nsum-product algorithm, the best known algorithm for decoding\nLDPC codes and Turbo codes. This paper offers an alternative\napproach to understand the normalized min-sum algorithm.\nThe algorithm is derived directly from cooperative optimization,\na newly discovered general method for global/combinatorial\noptimization. This approach provides us another theoretical basis\nfor the algorithm and offers new insights on its power and\nlimitation. It also gives us a general framework for designing\nnew decoding algorithms.\n\nI. I NTRODUCTION\nLDPC codes can achieve near capacity performance for\nchannel coding. One popular algorithm for decoding LDPC\ncodes is so called the normalized min-sum algorithm [1], [2]\n(also referred to as the normalized BP-based algorithm). It\nis attractive for hardware/software implementations because\nit reduces the implementation complexity of the sum-product\nalgorithm without losing much of its performance. The sumproduct algorithm [3], [4], is the best known algorithm for\ndecoding both Turbo codes [5] and LDPC codes [6], [7].\nThe min-sum algorithm [8] in its original form is treated\nas a generalization of the Viterbi algorithm [9] for iterative\ndecoding of code realizations on general graphs. When a code\nrealization is cycle-free, the algorithm is an exact solution for\nmaximum likelihood decoding. When there are cycles, it is\nsurprising that the min-sum decoding often works quite well\nin terms of empirical performance. However, we are lack of\ntheoretical understanding about its remarkable performance in\nthis case.\nThe sum-product algorithm [6] can be viewed as a generalization of the belief propagation (BP) algorithm developed in\nAI [10]. The min-sum algorithm is also referred to as the BPbased algorithm [11] where the former can be understood as\nan approximation to the later following the standard approximation of the Max-Log-MAP [12]. Compared with the sumproduct algorithm, the min-sum algorithm sometimes may\nhave a noticeable degradation in performance.\nThe normalized min-sum algorithm was proposed as a\nbetter approximation to the sum-product algorithm than the\noriginal min-sum algorithm. Simulation results show that the\n\nnormalized min-sum algorithm can improve the performance\nof the original min-sum algorithm [1]. It can also achieve a\nnear-optimal performance in many cases at decoding LDPC\ncodes, very close to the sum-product algorithm.\nIt might not be straightforward to understand the (normalized) min-sum algorithm if we argue that it is an approximation to the sum-product algorithm. The sum-product\nalgorithm is only an approximate algorithm for computing the\nmarginal a posteriori distributions. The optimal decoding for a\nchannel code, described in Shannon's information theory [13],\nis based on finding the codeword of the maximum a posteriori\nprobability. The two tasks are closely related to each other,\nbut not equivalent. Furthermore, we still need more theoretical\nunderstanding of the sum-product algorithm even though there\nare some remarkable progresses recently [16], [17], [18].\nThis paper takes different approach to understand the\n(normalized) min-sum algorithm. It will show that the algorithm can be derived as a cooperative optimization algorithm [14]. Cooperative optimization is a newly discovered\nglobal/combinatorial optimization method for attacking hard\noptimization problems. It breaks a hard optimization problem\ninto a number of sub-problems and solves them together in\na cooperative way. It does not struggle with local minima,\nhas global optimality conditions for recognizing global optima, and offers us a complete departure from the classic\noptimization methods. With proper settings, a cooperative\noptimization algorithm has a unique equilibrium and converges\nto it with an exponential rate regardless of initial conditions\nand perturbations.\nDeriving the normalized min-sum algorithm as a cooperative optimization algorithm can offer us new insights about\nthe algorithm. Following this approach, this paper attempts to\nanswer a number of important questions related to the minsum algorithm; 1) why can it find the optimal codeword? 2)\nwhether a found codeword is the optimal one? 3) what is\nits objective function to be optimized? A general framework\nis also presented in this paper for designing new decoding\nalgorithms in a systematic way.\nII. C OOPERATIVE O PTIMIZATION\nCooperative optimization is a general principle for\nfinding the global optimum of a multivariate function\nE(x1 , x2 , . . . , xn ). It utilizes another function in simple forms,\n\n\fP\nsuch as \u03a8(x1 , x2 , . . . , xn ) =\ni \u03a8i (xi ), and iteratively refines the function \u03a8(x) as the lower bound of the multivariate\nfunction E(x). At a given iteration k, if the lower bound\nfunction \u03a8(k) (x) has been tightened enough so that its global\nminimum equals to the global minimum of E(x), i.e.,\nmin \u03a8(k) (x) = min E(x) ,\nx\n\nx\n\nthen the global minimum of E(x) is found which is the same\nas the global minimum of \u03a8(k) (x),\narg min E(x) = arg min \u03a8(k) (x) .\nx\n\nx\n\nThe global minimum of \u03a8(k) (x) of the form\nbe easily found as\n\u2217(k)\n\nxi\n\n(k)\n\n(\u03a8(k) (x)) = arg min \u03a8i (xi ),\n\nP\n\n(k)\n\ni\n\n\u03a8i (xi ) can\n\nfor i = 1, 2, . . . , n .\n\nxi\n\nAssume that the multivariate function E(x), often referred\nto as the energy function or the objective function, can be\ndecomposed as the aggregation of a number of sub-objective\nfunctions,\nX\nEi (x) .\nE(x) =\ni\n\nP (k\u22121)\n(xi ) is a\nAssume further that \u03a8\n(x) of the form i \u03a8i\n(k)\nlower bound function of E(x), \u03a8(k) (x) \u2264 E(x). Let \u03a8i (xi ),\nfor i = 1, 2, . . . , n, be computed as follows\nX\n(k\u22121)\n(k)\n(xj ) , (1)\nwij \u03a8j\n\u03a8i (xi ) = min (1\u2212\u03bbk )Ei (x)+\u03bbk\n(k\u22121)\n\nXi \\xi\n\nj\n\nwhere Xi the set of variables contained in Ei (x) and minXi \\xi\nstands for minimizing with respect to all variables in Xi\nP (k)\nexcluding xi . Then the new function \u03a8(k) (x) = i \u03a8i (xi )\nis also a lower bound function of E(x). In the above equation,\n\u03bbk and wij (1 \u2264 i, j \u2264 n) are coefficients of the linear\n(k)\ncombination of Ei and \u03a8j (x\nPj ). \u03bbk satisfies 0 \u2264 \u03bbk < 1\nand wij s satisfy wij \u2265 0 and i wij = 1 for 1 \u2264 i, j \u2264 n.\nWithout loss of generality, assume that all objective functions including the sub-objective functions Ei (x) are nonnegative functions. Then the cooperative optimization theory tells\nus that the lower bound function \u03a8(x) computed by (1) can\nbe progressively tightened,\n\u03a8(0) (x) \u2264 \u03a8(1) (x) \u2264 . . . \u2264 \u03a8(k) (x) \u2264 E(x) ,\n(0)\n\nwhen we choose the initial condition as \u03a8i (xi ) = 0, for\ni = 1, 2, . . . , n.\nThe difference equations (1) define the dynamics of cooperative optimization. The original minimization problem\nminx E(x) has been divided into n sub-problems of minimization (see (1)). Those sub-problems can be solved in parallel\n(k)\nin implementation. The function \u03a8i (xi ) is the solution at\nsolving the ith sub-problem. The objective function of the ith\nsub-problem, denoted as \u1ebci (x), is a linear combination of the\noriginal sub-objective function Ei (x) and the solutions from\nsolving other sub-problems, i.e.,\nX\n(k)\n(k)\nwij \u03a8j (xj ) .\n\u1ebci (x) = (1 \u2212 \u03bbk ) Ei (x) + \u03bbk\nj\n\nThe cooperation among solving those sub-problems is thus\nachieved by having each sub-problem compromising its solution with the solutions of other sub-problems. \u1ebci (x) is called\nthe modified objective function for the sub-problem i.\nThe coefficient \u03bbk is a parameter for controlling the cooperation at solving the sub-problems and is called the cooperation\nstrength. A high cooperation strength leads to strong cooperation at solving the sub-problems while a lower cooperation\nstrength leads to weak cooperation. The coefficients wij control the propagation of the sub-problem solutions \u03a8i (xi ) in\nthe modified objective functions \u1ebci (x) (details in [14]). They\nare so called the message propagation parameters.\n(k)\nThe function \u03a8i (xi ) can be understood as the soft decision of assigning the variable xi at minimizing \u1ebci (x).\nThe most preferable value for variable xi at iteration k is\n(k)\narg minxi \u03a8i (xi ).\nGiven any variable, say xi , it may be contained in several\nsub-problems. At each iteration, xi has a value in the optimal\nsolution for each of the sub-problems. Those values may\nnot be the same. If all of them are of the same value,\ndenoted as x\u0303i , we say that all the sub-problems reach a\nconsensus assignment for variable xi . If all sub-problems reach\na consensus assignment for each variable at some iteration,\nwe say that a consensus solution is reached at the iteration.\nConsensus solution is an important concept of cooperative optimization for defining global optimality conditions. Normally,\na consensus solution is the global optimum, guaranteed by\ntheory. The following theory [14] offers one global optimality\ncondition based on the concept of consensus solution.\nTheorem 2.1: Assume that the difference equations (1) for\ncooperative optimization reaches its equilibrium, denoted as\n(\u03a8\u221e (xi )) (1 \u2264 i \u2264 n), i.e., (\u03a8\u221e (xi )) is a solution to the\ndifference equations (1). If a consensus solution x\u0303 is found in\nthis case, then it must be the global optimum of E(x), x\u0303 = x\u2217 .\nIt has been shown in [14] that the cooperative optimization\nalgorithm defined by the difference equations (1) has many\nimportant computational properties not possessed by conventional optimization algorithms. Given a constant cooperation\nstrength \u03bbk = \u03bb and the propagation coefficients wij , the algorithm has one and only one equilibrium. It always converges to\nthe unique equilibrium with an exponential rate regardless of\ninitial conditions and perturbations. Mathematical analysis also\nshows that when \u03bb \u2192 1, the lower bound function computed\nby the difference equations (1) tends to have a global minimum\napproaching the global minimum of the original function as\nthe iteration proceeds. Whenever those two global minimums\ntouch each other, the global minimum of the original function\nis found.\nHence, the cooperative optimization is stable (unique equilibrium), fast (exponential convergence rate), and robust (insensitive to initial conditions and perturbations). Unlike conventional optimization methods, it does not struggle with local\nminimums and it knows when to stop search because of the\nglobal optimality condition. Details about these together with\nthe theoretical investigation of cooperative optimization are\nprovided in [14].\n\n\f(k)\n\n(k)\n\nLet \u03a8i (xi )/(1 \u2212 \u03bbk ) \u21d2 \u03a8i (xi ), the difference equations (1) can be rewritten in a different form,\nX\n(k\u22121)\n(k)\n(xj ) ,\n(2)\nwij \u03a8j\n\u03a8i (xi ) = min Ei + \u03bbk\nXi \\xi\n\nj\n\nWith certain way of decomposing E(x), certain settings\nof the cooperation strength \u03bbk and the message propagation\nparameters wij , the normalized min-sum algorithm can be\nderived from the difference equations (1) of cooperative optimization.\nIII. C ONSTRUCTING M ORE P OWERFUL C OOPERATIVE\nO PTIMIZATION A LGORITHMS\nDescribing cooperative optimization in the language of\nmathematics enables us to make a generalization of the basic cooperative optimization algorithm (see Difference Equation (1)). One way to generalize it is to take more complicated\nforms of the lower bound functions other than the simple one\nP (k)\ni \u03a8i (xi ). One of them is to break \u03a8i (xi ) into several\npieces as follows\n\u03a8i (xi ) \u2192 \u03a8i1 (xi ), \u03a8i2 (xi ), . . . , \u03a8iNi (xi ) .\nConsequently, the lower bound function takes the following\nform\nXX\n\u03a8ij (xi ) .\n(3)\nE\u2212 (x1 , x2 , . . . , xn ) =\ni\n\nj\n\nSuch a lower bound function is called the fragmented unary\nlower bound function.\nLet {Eij (x)} be a decomposition of E(x), i.e.,\nXX\nEij (x) = E(x) .\ni\n\nj\n\nAssume that there is one sub-objective function Eij (x) for\neach \u03a8ij (xi ). Correspondingly, the difference equations of the\ncooperative optimization become\nXX\n(k)\n(k\u22121)\n\u03a8ij (xi ) = min Eij + \u03bbk\nwiji\u2032 j \u2032 \u03a8i\u2032 j \u2032 (xi\u2032 ), (4)\nXij \\xi\n\ni\u2032\n\nj\u2032\n\nwhere Xij is a set of variables containing those in Eij (x).\nOur simulation has demonstrated that cooperative optimization algorithms based on the fragmented unary lower bound\nfunction are often more powerful at decoding LDPC codes\nthan those based on the simple form.\nIV. LDPC D ECODING\n\nAS\n\nC OMBINATORIAL O PTIMIZATION\n\nLDPC codes belong to a special class of linear block codes\nwhose parity check matrix H has a low density of ones. For\na LDPC code over GF (q), its parity check matrix H has\nelements hmn defined over GF (q), hmn \u2208 GF (q). Let the\ncode word length be N (the number of symbols), then H\nis a M \u00d7 N matrix, where M is the number of rows. Each\nrow of H introduces one parity check constraint on input data\nx = (x1 , x2 , . . . , xN ), i.e.,\nN\nX\n\nn=1\n\nhmn xn = 0, for m = 1, 2, . . . , M .\n\nPutting the m constraints together, we have HxT = 0.\nLet function fn (xn ) be defined as\nfn (xn ) = \u2212 ln p(xn /yn ) ,\n\n(5)\n\nwhere p(xn /yn ) is the conditional distribution of input data\nsymbol n at value xn given the output data symbol n at value\nyn . fn (0) \u2212 fn(xn ), which is equal to ln(p(xn /yn )/p(0/yn )),\nis the log-likelihood ratio (LLR) of input data symbol n at\nvalue xn versus value 0.\nIn those notations, the maximum likelihood decoding can\nbe formulated as a constrained combinatorial optimization\nproblem,\nmin\n\nx1 ,x2 ,...,xN\n\nn\nX\n\nfn (xn )\n\ns.t. HxT = 0 .\n\n(6)\n\nn=1\n\nThe function to be minimized in (6) is called the objective\nfunction for decoding a LDPC code. The decoding problem\nis, thus, transferred as finding the global minimum of a multivariate objective function.\nLet X be the set of all variables. Given the mth constraint\nbe Hm xT = 0, let Xm be the set of variables corresponding\nto the non-zero elements in Hm , i.e.,\nXm \u2261 {xn |hmn 6= 0} .\nLet fXm (Xm ) be a function defined over Xm as\n\u001a\n0, if Hm xT = 0;\nfXm (Xm ) =\n\u221e, otherwise.\n\n(7)\n\nfXm (Xm ) is called the constraint function representing the\nmth constraint. Using the constraint functions, the decoding\nproblem (6) can be reformulated as an unconstrained optimization problem of the following objective function,\nM\nX\n\nfXm (Xm ) +\n\nm=1\n\nN\nX\n\nfn (xn ) .\n\n(8)\n\nn=1\n\nSuch a combinatorial optimization problem is, in general, NPhard.\nV. D ERIVING\n\nTHE\n\nN ORMALIZED M IN -S UM A LGORITHM\n\nIn the following discussions, we differentiate unary constraints from higher order constraints in notations by using\nsymbol fn (xn ) for unary constraint on variable xn and\nfXm (Xm ) for a constraint of an order higher than one. When\nconstraints are mentioned, they are referred to non-unary\nconstraints and unary constraints will be explicitly declared.\nConventionally, a LDPC code is represented as a Tanner\ngraph, a graphical model useful at understanding code structures and decoding algorithms. A Tanner graph is a bipartite\ngraph with variable nodes on one side and constraint nodes\non the other side. Edges in the graph connect constraint\nnodes to variable nodes. A constraint node connects to those\nvariable nodes that are contained in the constraint. A variable\nnode connects to those constraint nodes that use the variable\nin the constraints. Constraint nodes are also referred to as\ncheck nodes. During each iteration of the min-sum algorithm,\n\n\fmessages are flowed from variables nodes to the check nodes\nfirst, then back to variable nodes from check nodes.\nTo follow the notation used in the literatures of coding, we\nchange the symbols for indices from i and j to m and n.\nLet N (m) be the set of variable nodes that are connected to\nthe check node m. Let M(n) be the set of check nodes that\nare connected to the variable node n. Let symbol '\\' denotes\nthe set minus. N (m) \\ n denotes the set of variable nodes\nexcluding node n that are connected to the check node m.\nM(n) \\ m stands for the set of check nodes excluding the\ncheck node m which are connected to the variable node n.\nTo derive the normalized min-sum algorithm as a cooperative optimization algorithm, we choose the fragmented unary\nfunction (see (3)) as the form of the lower bound function.\nFor each variable n and then for each constraint containing\nthe variable, m \u2208 M(n), we define a component function\n\u03a8nm (xn ). The summation of all those component functions\nis the form of the lower bound function,\nE\u2212 (x1 , x2 , . . . , xN ) =\n\nN\nX\n\nX\n\n\u03a8nm (xn ) .\n\nThis change also does not have any impact on the optimization\nresults. Putting these two changes together, the difference\nequations (10) can be simplified to\n\u03a8(k)\nnm (0) = 0,\n\u03a8(k)\nnm (1) =\n\u2032\n\n(k\u22121)\n\nY\n\nand\n\nsgn(Zn\u2032 m (1))*\n\nn \u2208N (m)\\n\n\nmin\n\nn\u2032 \u2208N (m)\\n\n\n(k\u22121)\n\n|Zn\u2032 m (1)| .\n\n(13)\nEq. (13) is the check node update rule of the normalized minsum algorithm and Eq. (11) is the variable node update rule.\nThe normalized min-sum algorithm is, thus, derived from the\ndifference equations of cooperative optimization.\nDifference Equation (10) defines an optimization algorithm\nmore general than the normalized min-sum algorithm. It is\nsuitable for any kind of variables and any kind of constraints.\nThe normalized min-sum algorithm defined by (13) and (11) is\na special instance of the algorithm, applicable only to boolean\nvariables and parity check constraints. By understanding the\nunderlying principle of the normalized min-sum algorithm, we\nmake it possible to derive more general decoding algorithms.\n\nn=1 m\u2208M(n)\n\nLet the decomposition of the objective function E(x) for\na LDPC decoding problem be {Enm (x)}. Functions Enm (x)\nare sub-objective functions. There is one sub-objective function Enm (x) for each \u03a8nm (xn ). The overall objective function\nis\nX X\nEnm (x) .\nn m\u2208M(n)\n\nDifferent decompositions lead to different cooperative optimization algorithms. The decomposition leading to the normalized min-sum algorithm has the following form\nX\nEnm (x) = fXm (Xm ) +\nfn\u2032 (xn\u2032 ) .\n(9)\nn\u2032 \u2208N (m)\\n\n\nIt contains the mth constraint fXm (Xm ), all the unary constraints on the variables in Xm except the one on xn .\nSubstituting (9) into the difference equations (4) for\nthe fragmented unary lower bound function and choosing\n\u03bbk wnmn\u2032 m\u2032 = \u03b1k (a constant), we have\nX\n(k\u22121) \u2032\nZn\u2032 m (xn ),\n\u03a8(k)\nnm (xn ) = min fXm (Xm ) +\nXm \\xn\n\nn\u2032 \u2208N (m)\\n\n\n(10)\nwhere\n(k\u22121)\nZnm\n(xn ) = fn (xn ) + \u03b1k\n\nX\n\n(k\u22121)\n\n\u03a8nm\u2032 (xn ) . (11)\n\nm\u2032 \u2208M(n\u2032 )\\m\u2032\n\nFor binary LDPC codes, all xn s are binary variables, the\nunary constraint fn (xn ) defined in (5) can be rewritten as\nfn (1) = ln\n\np(xn = 0/yn )\n, and fn (0) = 0 .\np(xn = 1/yn )\n\n(12)\n\nThis change does not have any impact on the objective function (8) of decoding except offsetting the objective function by\n(k)\n(k)\na constant. We can also offset \u03a8nm (xn ) in (10) by \u03a8nm (0).\n\nVI. O BJECTIVE F UNCTION\n\nOF THE\n\nM IN -S UM A LGORITHM\n\nWhen we derived the normalized min-sum algorithm in the\nprevious section, we did not check the objective function it\ntries to minimize. From the cooperative optimization theory,\nthe objective function can be obtained by summing up all subobjective functions. In the case of the normalized min-sum\nalgorithm, the objective function is\n\uf8eb\n\uf8f6\nX\nX X\n\uf8edfXm (Xm ) +\nfn\u2032 (xn\u2032 )\uf8f8 .\nn m\u2208M(n)\n\nn\u2032 \u2208N (m)\\n\n\nIf fXm (Xm )s are parity check constraints (7), we can drop\nfXm (Xm ) in the above objective function and rewrite it as\nX X\nX\nfn\u2032 (xn\u2032 ), s.t. HxT = 0 .\nn m\u2208M(n) n\u2032 \u2208N (m)\\n\n\nThis\nP objective function is, in general, not proportional to\nn fn (xn ) except of a few cases, e.g., regular LDPC codes.\nA regular LDPC code has the property that all variable nodes\nhave the same degree and all the check nodes also have the\nsame degree. Otherwise, it is called an irregular LDPC code.\nLet dv (n) be the degree of the variable node n. Let dc (m)\nbe the degree of the check node m. Using these notations,\nthe objective function of the normalized min-sum algorithm\nbecomes\nX\nAn fn (xn ), where\nn\n\nAn =\n\nX\n\n(dc (m) \u2212 1),\n\nfor n = 1, 2, . . . , N .\n\n(14)\n\nm\u2208M(n)\n\nThe above objective\nP function is proportional to the desired\nobjective function n fn (xn ) if and only if all An s have the\nsame value for all ns.\n\n\fAssume that all constraints in a LDPC code have the same\ndegree, dc , then An in Formula (14) becomes\n(dc \u2212 1)dv (n),\n\nfor n = 1, 2, . . . , N .\n\nHence the weight for each variable in the objective function\nis proportional to the degree of the variable. The objective\nfunction of the normalized min-sum algorithm is biased in\nthis case. It weights bit variables of higher degrees more than\nthe variables of lower degrees. That explains why we have\nexperienced less decoding errors for bit variables of higher\ndegrees in applying the normalized min-sum algorithm at\ndecoding LDPC codes in practice.\nFor a regular LDPC code, dc (m) = dc (a constant) and\ndv (n) = dv (a constant). In this case, An = (dc \u2212 1)dv for\nall n, and the objective\nfunction of the min-sum algorithm\nP\nis proportional to n fn (xn ), which is the desired objective\nfunction for the maximum likelihood decoding of the code.\nVII. G ENERAL F RAMEWORK FOR C ONSTRUCTING\nM IN -S UM A LGORITHMS\nViewing the normalized min-sum algorithm as an instance\nof cooperative optimization lays out several new ways to\ndesign new decoding algorithms. One way is to select different\nforms of the lower bound functions besides the simple form\nand the fragmented form offered in this paper. Another way is\nto explore different decompositions of the objective function\nassociated with a decoding problem. The third way is to use\ndifferent settings of the cooperation strength \u03bbk and message\npropagation parameters wij . The decomposition presented in\nthis paper is simple and direct. It is only one of many possible\nways of decomposing an objective function. Correspondingly,\nrepresenting an objective function using a Tanner graph,\ndifferent decompositions of the objective function correspond\nto different ways of decomposing the Tanner graph into subgraphs. Each LPDC code represented as a Tanner graph\nmay also have its own unique graphical structure. Special\ndecomposition can also be explored to maximize the power\nof cooperative optimization at decoding the code.\nVIII. C ONCLUSIONS\nThis paper has shown that the normalized min-sum algorithm is the simplification of a cooperative optimization algorithm for the special case of boolean variables and parity check\nconstraints. A generalized min-sum algorithm for decoding\nLDPC codes over GF (q) , for any q \u2265 2, and any form of\ncheck constraints are also offered in this paper (see Eq. 10).\nThe normalized min-sum algorithm can be understood as\nfinding the global minimum via a lower bounding technique.\nTo decode a LDPC code, it deploys a function of a simple\nform which is a lower bound function to the objective function\nassociated with the decoding problem. To find the global\noptimum of the original objective function, it progressively\ntightens the lower bound function until the global minimum\nof the lower bound function reaches the global minimum of\nthe original objective function.\n\nA consensus solution found by a cooperative optimization\nalgorithm is in general the global optimum. It offers a general\ncriterion for the normalized min-sum algorithm to identify the\noptimal codeword and to terminate its search process. The\ncriterion is that each variable of the problem has the same best\nassignment in minimizing the modified sub-objective functions\ndefined by the cooperative optimization (see Eq. 10).\nIt has also been shown that the normalized min-sum algorithm has a biased objective function with higher weights\nfor those variables of higher degrees. The objective function\nbecomes the desired, unbiased objective function when a\nLDPC code is regular.\nR EFERENCES\n[1] J. Chen and M. Fossorier, \"Density evolution of two improved BP-based\nalgorithms for LDPC decoding,\" IEEE Communications Letters, vol. 6,\npp. 208\u2013210, 2002.\n[2] J. Chen, \"Reduced complexity decoding algorithms for low-density\nparity check codes and turbo codes,\" Ph.D. dissertation, University of\nHawaii, Dept. of Electrical Engineering, 2003.\n[3] F. R. Kschischang, B. J. Frey, and H. andrea Loeliger, \"Factor graphs and\nthe sum-product algorithm,\" IEEE Transactions on Information Theory,\nvol. 47, no. 2, pp. 498\u2013519, February 2001.\n[4] S. Aji and R. McEliece, \"The generalized distributive law,\" IEEE\nTransactions on Information Theory, vol. 46, pp. 325\u2013343, March 2000.\n[5] C. Berrou, A. Glavieux, and P. Thitimajshima, \"Near shannon limit\nerror-correcting coding and decoding: turbo codes,\" in Proceedings of\nthe 1993 IEEE International Conference on Communication, 1993, pp.\n1064\u20131070.\n[6] R. G. Gallager, \"Low-density parity-check codes,\" Ph.D. dissertation,\nDepartment of Electrical Engineering, M.I.T., Cambridge, Mass., July\n1963.\n[7] D. J. C. MacKay and R. M. Neal, \"Good codes based on very sparse\nmatrices,\" in Cryptography and Coding, 5th IMA Conference, December\n1995.\n[8] N. Wiberg, \"Codes and decoding on general graphs,\" Ph.D. dissertation,\nDepartment of Electrical Engineering, Linkoping University, Linkoping,\nSweden, 1996.\n[9] J. G. D. Forney, \"The Viterbi algorithm,\" Proc. IEEE, vol. 61, pp. 268\u2013\n78, Mar. 1973.\n[10] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of\nPlausible Inference. Morgan Kaufmann, 1988.\n[11] M. Fossorier, M. Mihaljevic, and H. Imai, \"Reduced complexity iterative\ndecoding of low density parity check codes based on belief propagation,\"\nIEEE Transactions on Communications, vol. 47, pp. 673\u2013680, May\n1999.\n[12] J. Hagenauer, E. Offer, and L. Papke, \"Iterative decoding of binary block\nand convolutional codes,\" IEEE Transactions on Information Theory,\nvol. 42, pp. 1064\u20131070, 1996.\n[13] C. E. Shannon, \"A mathematical theory communication,\" Bell Sys. Tech.\nJ., vol. 27, pp. 379\u2013423,623\u2013656, 1948.\n[14] X. Huang, \"Cooperative optimization for solving large scale combinatorial problems,\" in Theory and Algorithms for Cooperative Systems, ser.\nSeries on Computers and Operations Research. World Scientific, 2004,\npp. 117\u2013156.\n[15] --, \"Near perfect decoding of LDPC codes,\" in Proceedings of IEEE\nInternational Symposium on Information Theory (ISIT), 2005, pp. 302\u2013\n306.\n[16] J. Yedidia, W. Freeman, and Y. Weiss, \"Constructing free-energy\napproximations and generalized belief propagation algorithms,\" IEEE\nTransactions on Information Theory, vol. 51, no. 7, pp. 2282\u20132312,\nJuly 2005.\n[17] M. Wainwright, T. Jaakkola, and A. Willsky, \"A new class of upper\nbounds on the log partion function,\" IEEE Transactions on Information\nTheory, vol. 51, no. 7, pp. 2313\u20132335, July 2005.\n[18] T. J. Richardson, M. A. Shokrollahi, and R. L. Urbanke, \"Design of\ncapacity-approaching irregular low-density parity-check codes,\" IEEE\nTransactions on Information Theory, vol. 47, no. 2, pp. 619\u2013637,\nFebruary 2001.\n\n\f"}