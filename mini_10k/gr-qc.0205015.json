{"id": "http://arxiv.org/abs/gr-qc/0205015v1", "guidislink": true, "updated": "2002-05-03T20:27:11Z", "updated_parsed": [2002, 5, 3, 20, 27, 11, 4, 123, 0], "published": "2002-05-03T20:27:11Z", "published_parsed": [2002, 5, 3, 20, 27, 11, 4, 123, 0], "title": "Robust statistics for deterministic and stochastic gravitational waves\n  in non-Gaussian noise. II: Bayesian analyses", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=gr-qc%2F0606131%2Cgr-qc%2F0606071%2Cgr-qc%2F0606065%2Cgr-qc%2F0606109%2Cgr-qc%2F0606112%2Cgr-qc%2F0606102%2Cgr-qc%2F0606122%2Cgr-qc%2F0606034%2Cgr-qc%2F0606033%2Cgr-qc%2F0606075%2Cgr-qc%2F0606021%2Cgr-qc%2F0606060%2Cgr-qc%2F0606054%2Cgr-qc%2F0606128%2Cgr-qc%2F0606120%2Cgr-qc%2F0606085%2Cgr-qc%2F0606089%2Cgr-qc%2F0606110%2Cgr-qc%2F0606079%2Cgr-qc%2F0606136%2Cgr-qc%2F0606004%2Cgr-qc%2F0606124%2Cgr-qc%2F0606025%2Cgr-qc%2F0606061%2Cgr-qc%2F0606015%2Cgr-qc%2F0606104%2Cgr-qc%2F0606103%2Cgr-qc%2F0606016%2Cgr-qc%2F0606084%2Cgr-qc%2F0205071%2Cgr-qc%2F0205046%2Cgr-qc%2F0205014%2Cgr-qc%2F0205059%2Cgr-qc%2F0205058%2Cgr-qc%2F0205066%2Cgr-qc%2F0205004%2Cgr-qc%2F0205032%2Cgr-qc%2F0205013%2Cgr-qc%2F0205021%2Cgr-qc%2F0205126%2Cgr-qc%2F0205015%2Cgr-qc%2F0205100%2Cgr-qc%2F0205010%2Cgr-qc%2F0205008%2Cgr-qc%2F0205129%2Cgr-qc%2F0205099%2Cgr-qc%2F0205086%2Cgr-qc%2F0205074%2Cgr-qc%2F0205035%2Cgr-qc%2F0205062%2Cgr-qc%2F0205080%2Cgr-qc%2F0205040%2Cgr-qc%2F0205037%2Cgr-qc%2F0205041%2Cgr-qc%2F0205053%2Cgr-qc%2F0205110%2Cgr-qc%2F0205031%2Cgr-qc%2F0205048%2Cgr-qc%2F0205113%2Cgr-qc%2F0205117%2Cgr-qc%2F0205102%2Cgr-qc%2F0205104%2Cgr-qc%2F0205092%2Cgr-qc%2F0205108%2Cgr-qc%2F0205087%2Cgr-qc%2F0205038%2Cgr-qc%2F0205109%2Cgr-qc%2F0205044%2Cgr-qc%2F0205012%2Cgr-qc%2F0205006%2Cgr-qc%2F0205097%2Cgr-qc%2F0205106%2Cgr-qc%2F0205023%2Cgr-qc%2F0205057%2Cgr-qc%2F0205072%2Cgr-qc%2F0205009%2Cgr-qc%2F0205073%2Cgr-qc%2F0205028%2Cgr-qc%2F0205088%2Cgr-qc%2F0205132%2Cgr-qc%2F0205045%2Cgr-qc%2F0205101%2Cgr-qc%2F0205121%2Cgr-qc%2F0205065%2Cgr-qc%2F0205128%2Cgr-qc%2F0205076%2Cgr-qc%2F0205030%2Cgr-qc%2F0205024%2Cgr-qc%2F0205125%2Cgr-qc%2F0205001%2Cgr-qc%2F0205112%2Cgr-qc%2F0205111%2Cgr-qc%2F0205061%2Cgr-qc%2F0205122%2Cgr-qc%2F0205115%2Cgr-qc%2F0205090%2Cgr-qc%2F0205131%2Cgr-qc%2F0205042%2Cgr-qc%2F0205033%2Cgr-qc%2F0205003%2Cgr-qc%2F0205098&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Robust statistics for deterministic and stochastic gravitational waves\n  in non-Gaussian noise. II: Bayesian analyses"}, "summary": "In a previous paper (gr-qc/0105100) we derived a set of near-optimal signal\ndetection techniques for gravitational wave detectors whose noise probability\ndistributions contain non-Gaussian tails. The methods modify standard methods\nby truncating sample values which lie in those non-Gaussian tails. The methods\nwere derived, in the frequentist framework, by minimizing false alarm\nprobabilities at fixed false detection probability in weak signal limit. For\nstochastic signals, the resulting statistic consisted of a sum of an\nauto-correlation term and a cross-correlation term; it was necessary to discard\nby hand the auto-correlation term to obtain the correct, generalized\ncross-correlation statistic. In the present paper, we present an alternative\nBayesian derivation of the same signal detection techniques. We compute the\nprobability that a signal is present in the data, in the limit where the\nsignal-to-noise ratio squared per frequency bin is small, where the integrated\nsignal-to-noise ratio is large compared to one, and where the total probability\nin the non-Gaussian tail part of the noise distribution is small. We show that,\nfor each model considered, the resulting probability is to a good approximation\na monotonic function of the detection statistic derived in the previous paper.\nMoreover, for stochastic signals, the new Bayesian derivation automatically\neliminates the problematic auto-correlation term.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=gr-qc%2F0606131%2Cgr-qc%2F0606071%2Cgr-qc%2F0606065%2Cgr-qc%2F0606109%2Cgr-qc%2F0606112%2Cgr-qc%2F0606102%2Cgr-qc%2F0606122%2Cgr-qc%2F0606034%2Cgr-qc%2F0606033%2Cgr-qc%2F0606075%2Cgr-qc%2F0606021%2Cgr-qc%2F0606060%2Cgr-qc%2F0606054%2Cgr-qc%2F0606128%2Cgr-qc%2F0606120%2Cgr-qc%2F0606085%2Cgr-qc%2F0606089%2Cgr-qc%2F0606110%2Cgr-qc%2F0606079%2Cgr-qc%2F0606136%2Cgr-qc%2F0606004%2Cgr-qc%2F0606124%2Cgr-qc%2F0606025%2Cgr-qc%2F0606061%2Cgr-qc%2F0606015%2Cgr-qc%2F0606104%2Cgr-qc%2F0606103%2Cgr-qc%2F0606016%2Cgr-qc%2F0606084%2Cgr-qc%2F0205071%2Cgr-qc%2F0205046%2Cgr-qc%2F0205014%2Cgr-qc%2F0205059%2Cgr-qc%2F0205058%2Cgr-qc%2F0205066%2Cgr-qc%2F0205004%2Cgr-qc%2F0205032%2Cgr-qc%2F0205013%2Cgr-qc%2F0205021%2Cgr-qc%2F0205126%2Cgr-qc%2F0205015%2Cgr-qc%2F0205100%2Cgr-qc%2F0205010%2Cgr-qc%2F0205008%2Cgr-qc%2F0205129%2Cgr-qc%2F0205099%2Cgr-qc%2F0205086%2Cgr-qc%2F0205074%2Cgr-qc%2F0205035%2Cgr-qc%2F0205062%2Cgr-qc%2F0205080%2Cgr-qc%2F0205040%2Cgr-qc%2F0205037%2Cgr-qc%2F0205041%2Cgr-qc%2F0205053%2Cgr-qc%2F0205110%2Cgr-qc%2F0205031%2Cgr-qc%2F0205048%2Cgr-qc%2F0205113%2Cgr-qc%2F0205117%2Cgr-qc%2F0205102%2Cgr-qc%2F0205104%2Cgr-qc%2F0205092%2Cgr-qc%2F0205108%2Cgr-qc%2F0205087%2Cgr-qc%2F0205038%2Cgr-qc%2F0205109%2Cgr-qc%2F0205044%2Cgr-qc%2F0205012%2Cgr-qc%2F0205006%2Cgr-qc%2F0205097%2Cgr-qc%2F0205106%2Cgr-qc%2F0205023%2Cgr-qc%2F0205057%2Cgr-qc%2F0205072%2Cgr-qc%2F0205009%2Cgr-qc%2F0205073%2Cgr-qc%2F0205028%2Cgr-qc%2F0205088%2Cgr-qc%2F0205132%2Cgr-qc%2F0205045%2Cgr-qc%2F0205101%2Cgr-qc%2F0205121%2Cgr-qc%2F0205065%2Cgr-qc%2F0205128%2Cgr-qc%2F0205076%2Cgr-qc%2F0205030%2Cgr-qc%2F0205024%2Cgr-qc%2F0205125%2Cgr-qc%2F0205001%2Cgr-qc%2F0205112%2Cgr-qc%2F0205111%2Cgr-qc%2F0205061%2Cgr-qc%2F0205122%2Cgr-qc%2F0205115%2Cgr-qc%2F0205090%2Cgr-qc%2F0205131%2Cgr-qc%2F0205042%2Cgr-qc%2F0205033%2Cgr-qc%2F0205003%2Cgr-qc%2F0205098&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In a previous paper (gr-qc/0105100) we derived a set of near-optimal signal\ndetection techniques for gravitational wave detectors whose noise probability\ndistributions contain non-Gaussian tails. The methods modify standard methods\nby truncating sample values which lie in those non-Gaussian tails. The methods\nwere derived, in the frequentist framework, by minimizing false alarm\nprobabilities at fixed false detection probability in weak signal limit. For\nstochastic signals, the resulting statistic consisted of a sum of an\nauto-correlation term and a cross-correlation term; it was necessary to discard\nby hand the auto-correlation term to obtain the correct, generalized\ncross-correlation statistic. In the present paper, we present an alternative\nBayesian derivation of the same signal detection techniques. We compute the\nprobability that a signal is present in the data, in the limit where the\nsignal-to-noise ratio squared per frequency bin is small, where the integrated\nsignal-to-noise ratio is large compared to one, and where the total probability\nin the non-Gaussian tail part of the noise distribution is small. We show that,\nfor each model considered, the resulting probability is to a good approximation\na monotonic function of the detection statistic derived in the previous paper.\nMoreover, for stochastic signals, the new Bayesian derivation automatically\neliminates the problematic auto-correlation term."}, "authors": ["Bruce Allen", "Jolien D. E. Creighton", "Eanna E. Flanagan", "Joseph D. Romano"], "author_detail": {"name": "Joseph D. Romano"}, "author": "Joseph D. Romano", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1103/PhysRevD.67.122002", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/gr-qc/0205015v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/gr-qc/0205015v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "revtex, 13 pages, no figures", "arxiv_primary_category": {"term": "gr-qc", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "gr-qc", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/gr-qc/0205015v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/gr-qc/0205015v1", "journal_reference": "Phys.Rev. D67 (2003) 122002", "doi": "10.1103/PhysRevD.67.122002", "fulltext": "Robust statistics for deterministic and stochastic\ngravitational waves in non-Gaussian noise. II: Bayesian analyses.\nBruce Allen and Jolien D. E. Creighton\nDepartment of Physics, University of Wisconsin - Milwaukee, P.O. Box 413, Milwaukee WI 53201\n\n\u00c9anna \u00c9. Flanagan\nNewman Laboratory of Nuclear Studies, Cornell University, Ithaca, NY 14853-5001\n\nJoseph D. Romano\n\narXiv:gr-qc/0205015v1 3 May 2002\n\nDepartment of Physical Sciences, University of Texas at Brownsville, Brownsville TX 78520\nIn a previous paper (paper I), we derived a set of near-optimal signal detection techniques for\ngravitational wave detectors whose noise probability distributions contain non-Gaussian tails. The\nmethods modify standard methods by truncating or clipping sample values which lie in those nonGaussian tails. The methods were derived, in the frequentist framework, by minimizing false alarm\nprobabilities at fixed false detection probability in the limit of weak signals. For stochastic signals,\nthe resulting statistic consisted of a sum of an auto-correlation term and a cross-correlation term;\nit was necessary to discard \"by hand\" the auto-correlation term in order to arrive at the correct,\ngeneralized cross-correlation statistic.\nIn the present paper, we present an alternative derivation of the same signal detection techniques\nfrom within the Bayesian framework. We compute, for both deterministic and stochastic signals, the\nprobability that a signal is present in the data, in the limit where the signal-to-noise ratio squared\nper frequency bin is small, where the signal is nevertheless strong enough to be detected (integrated\nsignal-to-noise ratio large compared to one), and where the total probability in the non-Gaussian\ntail part of the noise distribution is small. We show that, for each model considered, the resulting\nprobability is to a good approximation a monotonic function of the detection statistic derived in\npaper I. Moreover, for stochastic signals, the new Bayesian derivation automatically eliminates the\nproblematic auto-correlation term.\n\nI.\n\nINTRODUCTION AND SUMMARY\n\nMost of the literature on gravitational-wave data analysis assumes that the detector noise is Gaussian. However, significant non-Gaussian tails have been a characteristic feature of the noise distributions in all gravitational wave detectors constructed to date. Standard\ndetection strategies for both deterministic and stochastic signals, which were designed under the assumption of\nGaussian noise, perform more poorly when non-Gaussian\nnoise is present.\nIn a previous paper in this series [1] (henceforth paper\nI), we developed a new set of statistical signal-processing\ntechniques to search for deterministic and stochastic\ngravitational waves in detector data. These techniques\nare robust, meaning that they will work well even if the\ndetector noise is not Gaussian but falls into a broader statistical class that we expect includes realistic detectors.\nThese new methods are similar to the older ones: one\nconstructs matched filters to search for known waveforms\nor cross-correlates the instrument outputs at the different detector sites to search for a stochastic background.\nThe essential difference is that the statistics are modified by truncation: detector samples that fall outside the\ncentral Gaussian-like part of the sample distribution are\nexcluded from (or saturated when constructing) the measurement statistic. For both deterministic and stochastic signals, a robust statistic was found which performs\nbetter than the optimal linear filter in the case where\n\nthe detector noise is non-Gaussian, and almost as well in\nthe Gaussian-noise case. Alternative methods for dealing\nwith non-Gaussian noise for stochastic signals have been\nexplored by Klimenko and Mitselmakher [2].\nIn paper I, we derived the statistics using the frequentist criterion of minimizing false alarm probabilities at\nfixed false detection probabilities in the limit of weak\nsignals. In the present paper, we present an alternative\nderivation of the same signal detection techniques from\nwithin the Bayesian framework.\nWe start in Sec. II by reviewing the foundations of the\ntwo different approaches to determining detection statistics used in paper I and this paper. We review the locally\noptimal criterion used in paper I in Sec. II A. In Sec. II B\nwe explain how Bayesian considerations lead to a unique\nchoice of detection statistic, as discussed by Finn [3]. In\nSecs. III and IV of the paper, we compute, for a variety\nof different models and sets of assumptions, that unique\nBayesian statistic. We show that for each case considered, the Bayesian statistic is equivalent to the statistics\nderived in paper I, in the sense that the false alarm versus false dismissal curves of the two statistics coincide to\na good approximation.\nThe equivalence between the two types of statistic is\nvalid only under certain approximations, discussed below. Under those approximations, the Bayesian statistics which we obtain are equivalent to a particular type\nof maximum likelihood statistic described in Refs. [4, 5].\nThat type of maximum likelihood statistic differs from\n\n\f2\nthe type of maximum likelihood statistic considered previously in gravitational wave data analysis in its treatment of noise parameters.\nSection III deals with known, deterministic signals.\nIn Sec. III A we consider the case of a known signal of\nunknown amplitude, incident on a single detector with\nwhite, Gaussian noise, where the noise variance is assumed to be known. For that case the Bayesian statistic\nis shown to be equivalent to the standard matched filtering statistic. Section III B generalizes this analysis by\nallowing the noise variance to be an unknown parameter, to be measured from the data; the same result is\nobtained. In Secs. III C and III D we consider the cases\nof white and colored non-Gaussian detector noise. For\nboth of these cases we show that the Bayesian statistic is\nequivalent to the corresponding locally optimal statistic\nderived in paper I.\nIn Sec. IV we give a similar analysis of stochastic signals. In Sec. IV A we compute the Bayesian statistic for\nthe case of a white stochastic signal, and of two co-aligned\ndetectors with white Gaussian noise, where the noise variance is assumed to be known. The maximum likelihood\nstatistic for this case was previously computed, in a more\ngeneral context, by Finn and Romano [6]. In this case\nwe recover the result of Finn and Romano: the optimal statistic is not the standard cross-correlation statistic, but instead is a sum of the cross-correlation statistic and and extra auto-correlation terms. In Sec. IV B\nwe show that, under the more realistic assumption where\nthe noise variances are taken to be unknowns to be determined from the data, then the standard cross-correlation\nstatistic is recovered. Finally, in Sec. IV C we consider\ntwo detectors with white, non-Gaussian nosie, and we rederive the generalized cross-correlation statistic of paper\nI. Section V contains a short conclusion and summary.\n\nthe performance of detection statistics in the frequentist\nframework. Suppose one is given a statistic\n\u0393 = \u0393(x),\n\nsay, and that one's detection criterion is that a signal is\npresent if \u0393(x) > \u0393\u2217 and not present otherwise, where\n\u0393\u2217 is a threshold. Then the false dismissal probability\nassociated with this statistic is\nZ\ndx p(x|0),\n(2.4)\n\u03b1(\u0393\u2217 ) =\n\u0393>\u0393\u2217\n\nand the false alarm probability is\nZ\ndx p(x|\u01eb).\n\u03b2(\u0393\u2217 , \u01eb) =\n\nFOUNDATIONS\n\nWe denote the output of a set of gravitational wave\ndetectors by a vector x, with\nx = n + s.\n\n(2.1)\n\nHere n is the detector noise, and s is a possibly present\ngravitational wave signal. We can write\ns = \u01eb\u015d,\n\n(2.2)\n\nwhere \u01eb is a parameter governing the signal strength, and\nthe magnitude of \u015d is fixed. As in paper I, we shall be\nspecializing to weak signals and using an expansion in\npowers of \u01eb about \u01eb = 0 throughout this paper.\nA.\n\nFrequentist signal detection\n\nA key quantity is the probability distribution for x\ngiven \u01eb, p(x|\u01eb). This quantity can be used to compute\n\n(2.5)\n\n\u0393<\u0393\u2217\n\nBy eliminating \u0393\u2217 between Eqs. (2.4) and (2.5) we obtain\nthe false-dismissal versus false-alarm curve\n\u03b2 = \u03b2(\u03b1, \u01eb)\n\n(2.6)\n\nwhich characterizes the performance of the statistic.\nIn paper I we showed that there is a unique statistic\n\u039b(1) (x) which minimizes d\u03b2/d\u01eb at \u01eb = 0 for fixed \u03b1, defined by the expansion\n\u0002\n\u0003\np(x|\u01eb) = p(x|0) 1 + \u01eb\u039b(1) (x) + \u01eb2 \u039b(2) (x) + O(\u01eb3 ) .\n(2.7)\nThis statistic therefore has the best false-dismissal versus\nfalse-alarm curve for weak signals. [If \u039b(1) (x) vanishes\nidentically, then \u039b(2) (x) is the unique statistic that minimizes d2 \u03b2/d\u01eb2 at \u01eb = 0 for fixed \u03b1.] We applied this\nclass of detection statistics (called locally-optimal statistics [8]) to a variety of different gravitational-wave signal\ndetection problems.\nB.\n\nII.\n\n(2.3)\n\nBayesian signal detection\n\nIn the Bayesian framework, the probability P (1) that\na signal is present in the data is given\nP (1)\nP (0)\n= \u039b(x)\n,\n(1)\n1\u2212P\n1 \u2212 P (0)\n\n(2.8)\n\nwhere P (0) is the a priori probability that a signal is\npresent, and \u039b(x) is the likelihood ratio. In the literature on Bayesian statistics \u039b(x) is called a Bayes factor.\nThe Bayesian framework uniquely determines a detection\ncriterion, which is to threshold on the probability P (1)\nthat a signal is present. From Eq. (2.8) it is clear that\nP (1) is a monotonic function of \u039b(x), so one can equivalently threshold on \u039b(x). Thus, an optimal detection\nstatistic is uniquely determined in the Bayesian framework; however, that statistic does depend on choices of\nprior probability distributions.\nWe now describe how likelihood ratio is computed. It\nis given by the formula\nZ\n\u039b(x) = d\u01eb \u039b(x, \u01eb)p(0) (\u01eb)\n(2.9)\n\n\f3\nwhere p(0) (\u01eb) is the prior probability distribution for the\nsignal strength \u01eb, and\n\u039b(x, \u01eb) =\n\np(x|\u01eb)\n.\np(x|0)\n\n(2.10)\n\nSuppose that the noise n is described by a probability\ndistribution pn (n), and the signal s by a signal distribution ps (s|\u01eb). Then it follows from Eq. (2.1) that\nZ\np(x|\u01eb) = ds pn (x \u2212 s) ps (s|\u01eb).\n(2.11)\nThe formula (2.10) can therefore be written as\nZ\npn (x \u2212 s)\nps (s|\u01eb).\n\u039b(x, \u01eb) = ds\npn (n)\n\n(2.12)\n\nThe formula for \u039b(x, \u01eb) becomes more complex when\nthere are unknown signal and/or noise parameters\npresent. Suppose the signal distribution depends on\nsome parameters \u03b8s in addition to the signal amplitude\n\u01eb, which themselves are distributed according to a prior\nprobability distribution p\u03b8s (\u03b8 s |\u01eb). Then the signal distribution ps (s|\u01eb) can be expanded as\nZ\n(2.13)\nps (s|\u01eb) =\nd\u03b8s ps (s|\u01eb, \u03b8s ) p\u03b8s (\u03b8 s |\u01eb),\nwhere ps (s|\u01eb, \u03b8s ) is the distribution for s given both \u01eb and\n\u03b8s . Similarly suppose that the noise distribution contains\nunknown parameters \u03b8n , whose a priori distribution is\np\u03b8n (\u03b8 n ). Then the noise distribution can be expanded as\nZ\n(2.14)\npn (n) =\nd\u03b8 n pn (n|\u03b8 n ) p\u03b8n (\u03b8n ).\nInserting the expansions (2.13) and (2.14) into Eq. (2.12)\ngives the final expression for the likelihood function:\nR\nZ\nd\u03b8 n pn (x \u2212 s|\u03b8n ) p\u03b8n (\u03b8 n )\n\u039b(x, \u01eb) =\nds R \u2032\nd\u03b8n pn (x|\u03b8 \u2032n ) p\u03b8n (\u03b8\u2032n )\nZ\n(2.15)\n\u00d7 d\u03b8s ps (s|\u01eb, \u03b8s ) p\u03b8s (\u03b8 s |\u01eb).\nEquations (2.9) and (2.15) are the foundational equations that we will use throughout this paper to compute\nthe likelihood ratio \u039b(x). A key feature of this formalism\nis that the noise parameters \u03b8 n are treated as unknowns,\nto be measured from the detector data along with the\ngravitational wave signal, rather than being treated as\nknown a priori. That feature underlies the elimination of\nthe auto-correlation terms encountered in paper I in the\ncase of a stochastic gravitational wave background.\nIn the next few sections we will revisit several of the\nsignal detection problems considered in paper I. In each\ncase, we will show that \u039b(x) is, to a good approximation, a monotonic function of the locally-optimal detection statistic derived in paper I. Since a monotonic function of a detection statistic has the same false alarm versus false dismissal curve as the original statistic, it follows that in each case the uniquely determined Bayesian\n\nstatistic \u039b(x) is equivalent to the statistic computed in\npaper I. Note, however, that the equivalence only applies\nat the level of choosing the detection statistic, and not at\nthe level of specifying thresholds. The Bayesian and frequentist approaches lead to different detection thresholds\nfor a given specified significance level; see, for example,\nthe discussion in Sec. III.C of Ref. [7].\nIn deriving the formulae for the likelihood ratio \u039b(x),\nwe shall invoke a number of different approximations. In\nassessing the validity of those approximations, we shall be\nconcerned only with their effect on the false alarm versus\nfalse dismissal curve of the statistic. In other words, the\napproximations might be very inaccurate for computing\nthe value of \u039b(x), but might nevertheless be very accurate in the sense that they have only a small effect on the\nfalse alarm versus false dismissal curve. [We do need to\ncompute accurate numerical values of \u039b(x), since we are\nnot concerned here with computing detection thresholds.]\nWe shall use the notation\n\u039b1 (x) \u2243 \u039b2 (x)\n\n(2.16)\n\nto mean that the false alarm versus false dismissal curves\nof the statistics \u039b1 (x) and \u039b2 (x) are approximately the\nsame.\nIII.\nA.\n\nDETERMINISTIC SIGNALS\n\nSingle detector, white Gaussian noise, known\nvariance\n\nWe first treat the simple case where we are looking for\na signal s, in a single detector, whose values in the time\ndomain are [12]\nsj = \u01eb\u015dj .\n\n(3.1)\n\nWe assume that the quantities \u015dj are known and fixed, so\nthat the only unknown parameter characterizing the signal is its amplitude \u01eb, which can be positive or negative.\nWithout loss of generality we can choose the normalization so that\nX\n|\u015dj |2 = 1.\n(3.2)\nj\n\nWe assume that the detector noise is white and Gaussian\nwith zero mean and unit variance. Then, as shown in\npaper I, the distribution for the data x given \u01eb is\n\u0014\n\u0015\nY 1\n1\n2\n\u221a exp \u2212 (xj \u2212 \u01eb\u015dj ) .\np(x|\u01eb) =\n(3.3)\n2\n2\u03c0\nj\nInserting this formula into Eq. (2.10) gives\n\u0002\n\u0003\n\u039b(x, \u01eb) = exp \u01eb\u01eb\u0302(x) \u2212 \u01eb2 /2 ,\n\n(3.4)\n\nwhere\n\n\u01eb\u0302(x) =\n\nX\nj\n\nxj \u015dj\n\n(3.5)\n\n\f4\nis the standard matched filtering statistic. Combining\nthis with Eq. (2.9) gives for the likelihood ratio\nZ\n2\n2\n\u039b(x) = e\u01eb\u0302(x) /2 d\u01eb p(0) (\u01eb)e\u2212[\u01eb\u2212\u01eb\u0302(x)] /2 .\n(3.6)\nNow the quantity |\u01eb\u0302(x)| is effectively the signal-to-noise\nratio. Let us assume that we are in the relevant regime\nwhere the signal is detectable with high confidence, so\nthat\n\u0002\n\u0003\nexp \u01eb\u0302(x)2 /2 \u226b 1.\n(3.7)\n\nLet us also assume that the prior distribution p(0) (\u01eb) is\nslowly varying and does not strongly constrain the possible values of \u01eb. Then, we can approximately evaluate the\nintegral (3.6) using the Laplace approximation to obtain\n\u221a\n\u0002\n\u0003\n\u039b(x) \u2248 2\u03c0p(0) [\u01eb\u0302(x)] exp \u01eb\u0302(x)2 /2 .\n(3.8)\n\nFinally, we argue that we can neglect the dependence on\nx of the factor p(0) [\u01eb\u0302(x)] in the expression (3.8). The reason is that the prior distribution p(0) (\u01eb) is a slowly varying function of \u01eb, and so this factor has a much weaker dependence on x than the exponential factor in the regime\n(3.7). Therefore, dropping the factor p(0) [\u01eb\u0302(x)] will have\na negligible effect on the false alarm versus false dismissal\ncurve of the statistic. In this approximation we see that\n\u039b(x) is a monotonic function of the standard detection\nstatistic |\u01eb\u0302(x)|,\n\u0002\n\u0003\n\u039b(x) \u2243 exp \u01eb\u0302(x)2 /2 ,\n(3.9)\n\nas claimed [13].\nWe remark that there is a key technical difference\nbetween the above computation and the corresponding\ncomputation in Sec. II.A. of paper I. The Bayesian computation presented here requires expanding the quantity ln \u039b(x, \u01eb) to second order in \u01eb about \u01eb = 0 [Eq.\n(3.4) above], whereas in paper I it sufficed to compute\nln \u039b(x, \u01eb) to linear order in \u01eb [Eqs. (2.3) and (2.5) of paper I]. This difference is a common feature of all of our\nsubsequent computations.\n\nwhere N is the number of data points; this is the conventional estimator of \u03c3. We also define the quantity \u03c1\nby\n\u03c1=\n\n\u01eb\n,\n\u03c3\n\n(3.12)\n\nwhich from the normalization condition (3.2) is the conventional signal to noise ratio. The corresponding estimator is\n\u01eb\n\u03c1\u0302(x) =\n.\n(3.13)\n\u03c3\u0302(x)\nThe conventional matched filtering statistic is\n1\nhx, \u015di ,\n\u03c3\n\n(3.14)\n\nand if we replace the noise variance by its estimator \u03c3\u0302(x)\nwe obtain the statistic\n\u03c1\u03021 (x) \u2261\n\n1\nhx, \u015di .\n\u03c3\u0302(x)\n\n(3.15)\n\nWe shall show below that the likelihood ratio \u039b(x) is\nto a good approximation equivalent to the conventional\nstatistic (3.15).\nThe noise distribution given \u03c3 is taken to be\n\"\n#\nY 1\nn2j\n\u221a\nexp \u2212 2 .\n(3.16)\npn (n|\u03c3) =\n2\u03c3\n2\u03c0\u03c3\nj\nThe full noise distribution is [cf. Eq. (2.14) above]\nZ \u221e\nd\u03c3 p\u03c3 (\u03c3) pn (n|\u03c3),\n(3.17)\npn (n) =\n0\n\nwhere p\u03c3 (\u03c3) is the prior probability distribution for \u03c3.\nInserting Eq. (3.17) into Eq. (3.16) and using the definition (3.11) we obtain\nZ \u221e\npn (n) =\nd\u03c3 p\u03c3 (\u03c3) exp [\u2212N \u039e(\u03c3)/2] ,\n(3.18)\n0\n\nwhere\nB.\n\nSingle detector, white Gaussian noise, unknown\nvariance\n\nWe now add one additional complication to the analysis, by taking the noise variance to be an unknown constant \u03c3. We define an inner product h , i on the space of\nsignals by\nX\nxj yj .\n(3.10)\nhx, yi \u2261\nj\n\nNote that this is not the standard inner product used\nin discussions of matched filtering, which incorporates a\nweighting factor of \u03c3 \u22122 . We define the statistic \u03c3\u0302(x) by\n\u03c3\u0302(x)2 \u2261\n\n1\nhx, xi ,\nN\n\n(3.11)\n\n\u039e(\u03c3) = ln(2\u03c0\u03c3 2 ) +\n\n\u03c3\u0302(n)2\n.\n\u03c32\n\n(3.19)\n\nWe can approximately evaluate the integral (3.18) in the\nlimit where N is large. The function \u039e(\u03c3) can be expanded about its local minimum at \u03c3 = \u03c3\u0302 as\n\u039e(\u03c3) = 1 + ln(2\u03c0\u03c3\u0302 2 ) +\n\n2\n(\u03c3 \u2212 \u03c3\u0302)2 + O[(\u03c3 \u2212 \u03c3\u0302)3 ]. (3.20)\n\u03c3\u0302 2\n\nUsing this expansion we obtain\nr\nN\n\u03c0\n(2\u03c0e)\u2212 2 p\u03c3 [\u03c3\u0302(n)] \u03c3\u0302(n)\u2212(N \u22121)\npn (n) =\nN\n\u0014\n\u0012\n\u0013\u0015\n1\n\u221a\n\u00d7 1+O\n.\n(3.21)\nN\n\n\f5\nWe assume that p\u03c3 (\u03c3) is slowly varying, and so have\nneglected in Eq. (3.21) a fractional error of order\n\u221a the\nfractional change in pn over a interval of width \u03c3\u0302/ N .\nWe next insert the formula (3.21) for the noise distribution into the expression (2.12) for the likelihood ratio,\nusing\nps (s|\u01eb) = \u03b4 N (s \u2212 \u01eb\u015d).\n\n(3.22)\n\nThe result is\n\u0014\n\u0015\u2212(N \u22121)\nZ \u221e\np\u03c3 [\u03c3\u0302(x \u2212 \u01eb\u015d)] \u03c3\u0302(x \u2212 \u01eb\u015d)\n(0)\n\u039b(x) =\nd\u01ebp (\u01eb)\n(3.23)\np\u03c3 [\u03c3\u0302(x)]\n\u03c3\u0302(x)\n0\nZ \u221e\np\u03c3 [\u03c3\u0302(x \u2212 \u01eb\u015d)]\nd\u01ebp(0) (\u01eb)\n=\np\u03c3 [\u03c3\u0302(x)]\n0\n)#\n\"\n(\nN \u22121\nhx, \u015di\n\u01eb2\n. (3.24)\n\u00d7 exp \u2212\nln 1 \u2212 2\u01eb\n+\n2\nhx, xi hx, xi2\nTo obtain the second line we used Eqs. (3.2) and (3.11).\nExpanding the logarithm to second order in \u01eb, we can\nre-express this as\nZ \u221e\np\u03c3 [\u03c3\u0302(x \u2212 \u01eb\u015d)]\n\u039b(x) =\nd\u01ebp(0) (\u01eb)\np\u03c3 [\u03c3\u0302(x)]\n0\nh\ni\n\u00d7 exp \u00e2\u01eb \u2212 b\u0302\u01eb2 + O(\u01eb3 ) ,\n(3.25)\n\nSimilarly we have\nhx, \u015di \u223c \u01eb \u00b1 \u03c3.\n\nWe assume that N \u226b 1 and that \u03c1 >\n\u223c 1. We now consider\ntwo different cases:\n\u2022 When \u03c12 /N \u226a 1, the fluctuations in hx, xi are\nsmall compared to the expected value, and\n\u221a we have\nfrom Eq. (3.29) that hx, xi \u223c N \u03c3 2 \u00b1 N \u03c3 2 . Using this together with Eq. (3.30) shows that the\nfirst term in the argument of the logarithm in Eq.\n(3.24) is\n\u01eb\n\nhx, \u015di\n\u03c12\n\u03c1\n\u223c\n\u00b1\n\u226a 1,\nhx, xi\nN\nN\n\n\u03c12\n\u03c12\n\u01eb2\n\u223c\n\u00b1 3/2 \u226a 1.\nhx, xi\nN\nN\n\n(3.32)\n\nThus the approximation is good in this regime.\n\u2022 When \u03c12 /N \u226b 1, a similar computation gives that\nhx, xi \u223c \u03c12 \u03c3 2 \u00b1 \u03c1\u03c3 2 . The the first term in the\nargument of the logarithm in Eq. (3.24) now scales\nas\n\u01eb\n\n(3.26)\n\nhx, \u015di\n1\n\u223c1\u00b1\nhx, xi\n\u03c1\n\n(3.33)\n\nand similarly the second term scales as\n\nand\n\"\n#\n2\n1\nhx, \u015di\n1\nb\u0302 = (N \u2212 1)\n\u22122\n2 .\n2\nhx, x\u0302i\nhx, x\u0302i\n\n(3.31)\n\nand similarly the second term is\n\nwhere\n\nhx, \u015di\n\u00e2 = (N \u2212 1)\nhx, x\u0302i\n\n(3.30)\n\n1\n\u01eb2\n\u223c1\u00b1 .\nhx, xi\n\u03c1\n\n(3.27)\n\nBefore proceeding further with the computation of the\nlikelihood ratio, we clarify the domain of validity of the\nweak signal expansion (expansion in powers of \u01eb) used in\ngoing from Eq. (3.24) to Eq. (3.25). We will estimate the\nexpected sizes and the scale of statistical fluctuations in\nthe two terms appearing in the argument of the logarithm\nin Eq. (3.24); the expansion will be good when both of\nthese terms and their fluctuations are small compared to\nunity. For the purpose of making these estimates we can\nidentify \u03c3 and \u03c3\u0302, and \u03c1, \u03c1\u0302, and \u03c1\u03021 .\nWe can compute the expected value and variance of the\nstatistic hx, xi using Eqs. (2.1), (2.2) and (3.16), which\ngives\np\nhx, xi \u223c (N \u03c3 2 + \u01eb2 ) \u00b1 2N \u03c3 4 + 4\u01eb2 \u03c3 2 .\n(3.28)\n\nThe notation here is that the first term gives the expected value, and the second term gives an estimate of\nthe statistical fluctuations. We can rewrite this formula\nin terms of the signal-to-noise ratio \u03c1 = \u01eb/\u03c3 as\n#\n\"\nr\n2\n\u03c12\n\u03c12\n2\n(3.29)\n+4 2 .\nhx, xi \u223c N \u03c3 (1 + ) \u00b1\nN\nN\nN\n\n(3.34)\n\nThus, the approximation breaks down in this\nregime.\nWe now return to computing the likelihood ratio \u039b(x).\nWe can approximately evaluate the integral (3.25) using\nthe Laplace approximation to obtain\ns\n\u0014 2\u0015\np\n[\u03c3\u0302(x\n\u2212\n\u01eb\u0302\u015d)]\n\u00e2\u03c0\n\u00e2\n\u03c3\n(0)\n\u039b(x) \u2248 p (\u01eb\u0302)\n,\n(3.35)\nexp\np\u03c3 [\u03c3\u0302(x)]\n4b\u0302\nb\u0302\nwhere\n\u01eb\u0302(x) =\n\n\u00e2(x)\n2b\u0302(x)\n\n.\n\n(3.36)\n\nThis approximation will be good whenever the exponential factor in Eq. (3.35) is large, which it will be in the\nregime where the signal is detectable (see below), and\nwhen the prior probability distribution p(0) (\u01eb) is slowly\nvarying. Using Eqs. (3.11), (3.15), (3.26) and (3.27), we\ncan write the exponential factor as\n\u0015\n\u0014\nN \u22121\n2\ng(\u03c1\u03021 /N ) ,\n(3.37)\nexp\n2\n\n\f6\nwhere the function g is given by g(x) = x/(1 \u2212 2x). Since\nwe are in the regime \u03c12 /N \u226a 1, the argument of the\nfunction g is small, and we can replace g(\u03c1\u030221 /N ) by \u03c121 /N .\nThis gives, using N \u226b 1,\ns\n\u0014\n\u0015\n1\np\u03c3 [\u03c3\u0302(x \u2212 \u01eb\u0302\u015d)] \u00e2\u03c0\n(0)\nexp \u03c1\u03021 (x)2 .\n\u039b(x) \u2248 p (\u01eb\u0302)\np\u03c3 [\u03c3\u0302(x)]\n2\nb\u0302\n(3.38)\nFinally, we argue as before that in the regime\nexp[\u03c12 /2] \u226b 1 where the signal is detectable, the dependence on x of all the other factors in Eq. (3.38) can\nbe neglected in comparison to the exponential factor, assuming that the prior distributions are slowly varying.\nThis gives\n\u039b(x) \u2243 exp[\u03c1\u03021 (x)2 /2],\n\n(3.39)\n\nas claimed.\nOur final answer (3.39) is essentially the same as the\nanswer (3.9) obtained when the noise variance \u03c3 is assumed to be known. Therefore, treating \u03c3 as an unknown\nparameter rather than as a fixed, known parameter does\nnot make much difference in this case. However, we will\nsee below for the case of stochastic signals that treating\nthe properties of the noise distribution as unknowns does\nhave a significant effect on the analysis, and that the correct answer is obtained only when the those properties\nare treated as unknowns.\nWe end this subsection by recapitulating the various\napproximations and assumptions we have invoked:\n\u2022 The large N approximation N \u226b 1.\n\u2022 The assumption that we are in the regime where the\nsignal is detectable, exp(\u03c12 /2) \u226b 1. This is necessary for evaluating the integral over \u01eb to obtain Eq.\n(3.35), and also for the validity in neglecting the\nprefactors in deriving Eq. (3.39). From a practical point of view the assumption exp(\u03c12 /2) \u226b 1 is\nnot a serious restriction, as it does not matter how\nour statistics perform in the regime exp(\u03c12 /2) \u223c 1\nwhere signals are not detectable.\n\u2022 The assumption that the prior probability distributions p(0) (\u01eb) and p\u03c3 (\u03c3) are slowly varying.\n\u2022 We have clarified the \"weak signal\" assumption of\npaper I; it is the assumption is that the signal-tonoise ratio squared per data point is small, \u03c12 /N \u226a\n1. This requirement ensures that the presence of\nthe signal does not significantly bias the estimate\n(3.11) of the noise variance. In practice we can always choose segments of data large enough to satisfy this assumption.\nC.\n\nSingle detector, white non-Gaussian noise\n\nWe now turn to the case where the noise has a known,\nnon-Gaussian distribution. In this subsection we follow\n\nSec. II.A. of paper I, and assume that the noise samples in the time domain are statistically independent but\nidentically distributed, with a known distribution. We\ncan write the noise probability distribution as\nY\ne\u2212f (nj ) .\n(3.40)\npn (n) =\nj\n\nWe assume that the probability distribution e\u2212f (x) has a\ncentral Gaussian region |x| < xb in which\nx2\n,\n2\u03c3 2\n\nf (x) =\n\n(3.41)\n\nwhich contains most of the probability, and a tail region\n|x| \u2265 xb containing a total probability ptail with ptail \u226a 1.\nAs before, the signal is assumed to be known up to\nan overall amplitude parameter. From Eqs. (2.11) and\n(3.22) we obtain the following modified version of Eq.\n(3.3):\nY\nexp [\u2212f (xj \u2212 \u01eb\u015dj )] ,\n(3.42)\np(x|\u01eb) =\nj\n\nand inserting this into Eqs. (2.9) and (2.10) gives\nZ\nY\nexp [\u2212f (xj \u2212 \u01eb\u015dj ) + f (xj )] .\n\u039b(x) = d\u01ebp(0) (\u01eb)\nj\n\n(3.43)\nExpanding to second order in \u01eb gives\nZ\nh\ni\n\u039b(x) = d\u01eb p(0) (\u01eb) exp \u00e2(x)\u01eb \u2212 b\u0302(x)\u01eb2 + O(\u01eb3 ) ,\n(3.44)\n\nwhere\n\u00e2(x) =\n\nX\n\nf \u2032 (xj )\u015dj\n\n(3.45)\n\nj\n\nand\nb\u0302(x) =\n\n1 X \u2032\u2032\nf (xj )\u015d2j .\n2 j\n\n(3.46)\n\nEvaluating the integral over \u01eb using the same types of\narguments as in Sec. III B gives\n\"\n#\n\u00e2(x)2\n\u039b(x) \u2243 exp\n.\n(3.47)\n2b\u0302(x)\nNow the statistic \u00e2(x) is the locally optimal statistic\ncomputed in paper I [Eq. (2.9) of paper I]. Therefore it\nremains to show that we can neglect the x dependence\nof the factor b\u0302(x) in the argument of the exponential in\nEq. (3.47).\nWe can split the sum (3.46) into contributions from\nthe Gaussian region and from the tail. Using the fact\nthat f \u2032\u2032 (x) = 1/\u03c3 2 in the Gaussian region, we obtain\nb\u0302(x) =\n\n1 X \u015d2j\n1 X \u2032\u2032\n+\nf (xj )\u015d2j .\n2\n2 x <x \u03c3\n2\nj\n\nb\n\nxj \u2265xb\n\n(3.48)\n\n\f7\nMost of the values of xj will fall in the central Gaussian\nregion, since xj =Pnj + \u01eb\u015dj , and \u01eb\u015dj \u226a \u03c3 for each individual j [14]. Since j \u015d2j = 1, the first term gives 1/(2\u03c3 2 ),\nup to fractional corrections of order ptail . Similarly the\nsecond term will be bounded above by \u223c ptail /(2\u03c3 2 ),\nsince f \u2032\u2032 (x) will be smaller in the tails than in the central\nGaussian region. We conclude that\n1\n[1 + O(ptail )] .\n(3.49)\n2\u03c3 2\nIt follows in particular that the x-dependent fluctuations\nin b\u0302(x) are smaller than its expected value by a factor of\nptail \u226a 1, and therefore we can neglect the x dependence\nof b\u0302(x) in Eq. (3.47), as required.\nb\u0302(x) =\n\nAs before, the statistic \u00e2(x) coincides with the locally\noptimal statistic derived in paper I [Eq. (2.21) of paper\nI], and it suffices to show that the x dependence of the\nfactor b\u0302(x) can be neglected in Eq. (3.47). We evaluate the sums in Eq. (3.55) by splitting them into Gaussian and tail contributions as before. Since gk\u2032 (x) = 1 in\nthe\nP Gaussian region, the first term in Eq. (3.55) yields\n2 k |s\u0303k |2 /Pk [1 + O(ptail )]. Also the second term is proportional to ptail since gk\u2032\u2032 (x) vanishes in the Gaussian\nregion. Thus we obtain\n\"\n#\n\u0015\nX |s\u0303k |2 \u0014\nb\u0302(x) = 2\n1 + O(ptail ) ,\n(3.56)\nPk\nk\n\nand the rest of the argument follows as before.\nD.\n\nSingle detector, colored non-Gaussian noise\nE.\n\nWe next consider the model of colored, non-Gaussian\nnoise of Sec. II. B. of paper I, where each frequency bin\nis assumed to be statistically independent. This is given\nby\n\u0012\n\u0013\u0015\n\u0014\n[(N \u22121)/2]\nY\n2\n|\u00f1k |2\npn (n) =\n, (3.50)\nexp \u22122gk\n\u03c0Pk\nPk\n\nSignals with unknown parameters\n\nWe now generalize the analysis of the preceding subsections by allowing the signals to depend on additional\nparameters other than the overall amplitude parameter\n\u01eb. We write\ns = \u01eb\u015d(\u03b8s ),\n\nk=1\n\nwhere the volume element is understood to be\n[(N \u22121)/2]\n\nY\n\nd(Re \u00f1k ) d(Im \u00f1k ).\n\n(3.51)\n\nk=1\n\nHere\n\n1 X 2\u03c0ijk/N\ne\nnj\n\u00f1k = \u221a\nN j\n\n(3.52)\n\nare the components of the discrete Fourier transform of\nthe time domain samples nj . The quantities Pk describe\nthe noise spectrum. For each frequency bin k, the function gk (x) is arbitrary except for the normalization conditions\nZ \u221e\nZ \u221e\ndxe\u2212gk (x) =\ndx xe\u2212gk (x) = 1,\n(3.53)\n0\n\n0\n\nand the requirement that gk (x) = x in a central Gaussian\nregion containing most of the probability.\nBy paralleling the analysis of Sec. III C, we again arrive at the formulae (3.44) and (3.47), where now the\nstatistics \u00e2(x) and b\u0302(x) are given by\nX \u0012 |x\u0303k |2 \u0013 Re(x\u0303\u2217 s\u0303k )\nk\n\u00e2(x) = 4\n(3.54)\ngk\u2032\nPk\nPk\nk\n\nand\n\n\u0013\n|x\u0303k |2 |s\u0303k |2\nPk\nPk\nk\n\u0013\n\u0012\n2\nX\n|x\u0303k |2 [Re(x\u0303\u2217k s\u0303k )]\n. (3.55)\n+4\ngk\u2032\u2032\n2\nPk\nPk\n\nb\u0302(x) = 2\n\nX\n\ngk\u2032\n\nk\n\n\u0012\n\n(3.57)\n\nwhere the signal parameters \u03b8s are distributed according\nto the distribution p\u03b8s (\u03b8s |\u01eb). Then from Eq. (2.15) we\ncan write\nZ\nZ\n\u039b(x) = d\u01eb d\u03b8 s p(0) (\u01eb)p\u03b8s (\u03b8 s |\u01eb)\u039b(x, \u01eb, \u03b8s ), (3.58)\nwhere \u039b(x, \u01eb, \u03b8s ) is given by Eq. (2.11) with ps (s|\u01eb) replaced by ps (s|\u01eb, \u03b8s ). In the regime where the signal is\ndetectable, we can repeat the arguments of the preceding\nsubsections to approximately evaluate the integrals as\n\u039b(x) \u2243 max max \u039b(x, \u01eb, \u03b8s ).\n\u01eb\n\u03b8s\n\n(3.59)\n\nThus the result is to take the statistics previously derived and to maximize over the signal parameters. Such\na maximization is the standard thing to do for linear\nmatched filtering; the above argument indicates that it\nis also the appropriate procedure for the more general\nclass of locally optimal statistics.\nIV.\n\nSTOCHASTIC SIGNALS\n\nThe standard method of detecting a stochastic background is to compute a cross-correlation between two\ndifferent instruments [9]; see Ref. [10] for a detailed description. In Sec. IV A below we compute the likelihood\nratio \u039b(x) for the simplest case of a white stochastic signal, and of two co-aligned detectors with white Gaussian noise, where the noise variance is assumed to be\nknown. For this case we do not recover the standard\n\n\f8\ncross-correlation statistic, but instead we obtain a statistic with extra auto-correlation terms. That statistic was\nfirst derived and has been investigated in detail in a more\ngeneral context by Finn and Romano [6]. We then argue\nthat it is unrealistic to take the noise variances to be\nknown parameters. In Sec. IV B we show that, when\nthe noise variances are taken to be unknowns to be determined from the data, then the likelihood ratio \u039b(x)\nis to a good approximation equivalent to the standard\ncross-correlation statistic. This computation is again in\nthe simple context of coincident aligned detectors with\nwhite noise. The computation of Sec. IV B is a simplified\nversion of the computation in Appendix A of Ref. [11].\nWe then turn to non-Gaussian noise models. In paper I, we derived a generalized cross-correlation (GCC)\nstatistic appropriate for non-Gaussian noise, which is a\nmodification of the standard cross-correlation statistic.\nIn Sec. IV C below we re-derive that statistic using the\nBayesian approach.\nA.\n\nTwo coincident co-aligned detectors, white\nGaussian noise, known variances\n\nThe output of the pair of detectors is x = (x1 , x2 ),\nwhere\nx1 = n1 + s\n\n(4.1)\n\nis the output of the first detector, and\nx2 = n2 + s\n\n(4.2)\n\nis the output of the second. We assume, for simplicity,\nthat the noise in each detector is white and Gaussian\nwith unit variance:\n\"\n#\nY 1\nn21j\n\u221a exp \u2212\npn1 (n1 ) =\n,\n(4.3)\n2\n2\u03c0\nj\nwith a similar equation for the second detector. We assume that the stochastic background signal is also white\nand Gaussian with variance \u01eb:\n#\n\"\nY 1\ns2j\n\u221a\n,\n(4.4)\nexp \u2212\nps (s|\u01eb) =\n2\u01eb\n2\u03c0\u01eb\nj\nwhere \u01eb \u2265 0. As in earlier sections, \u01eb parameterizes the\nsignal strength, and we will be using a weak signal expansion of expanding in powers of \u01eb about \u01eb = 0.\nBy inserting the distributions (4.3) and (4.4) into the\nformulae (2.11) and (2.14) we obtain\n\u0014\n\u0015\nN\np(x|\u01eb) = exp \u2212 \u039e(\u01eb, x) ,\n(4.5)\n2\nwhere\n\u039e(\u01eb, x) = 2 ln(2\u03c0) + ln [1 + 2\u01eb]\n(1 + \u01eb)\u03c3\u030222 + (1 + \u01eb)\u03c3\u030212 \u2212 2\u01eb\u01eb\u0302\n+\n,\n1 + 2\u01eb\n\n(4.6)\n\nand where\n\u03c3\u030212 =\n\n1 X 2\nx ,\nN j 1j\n\n\u03c3\u030222 =\n\n1 X 2\nx ,\nN j 2j\n\n(4.7)\n\n(4.8)\n\nand\n\u01eb\u0302 =\n\n1 X\nx1j x2j .\nN j\n\n(4.9)\n\nThe statistic \u01eb\u0302 is the standard cross-correlation statistic.\nBefore proceeding further we discuss the validity of the\nweak signal approximation in the context of a stochastic background signal. Suppose that a stochastic background is present and just barely detectable by crosscorrelating\nbetween the two detectors. Then we have\n\u221a\n\u01eb\u0302 \u223c 1/ N . In the context of ground based detectors like\nLIGO, when this analysis is generalized to colored noise,\nN is replaced by the product T \u2206f , where T is the observation time and \u2206f is the effective bandwidth in the\nusual formula for signal-to-noise ratio [Eq. (1.2) of Ref.\n[11]]. Using the estimates T \u223c 1/3 year and \u2206f \u223c 50 Hz\nwe find\n1\n\u223c 10\u22124 .\n(4.10)\n\u01eb\u0302 \u223c \u221a\nT \u2206f\nTherefore in all our analyses it will be sufficient to work\nto first order in \u03b5\u0302. The approximation would only break\ndown if \u01eb\u0302 \u223c 1, that is, if the stochastic background could\nbe seen in a single detector, which is thought to be very\nunlikely.\nWe define the statistics d\u02c61 and d\u02c62 by\nd\u02c61 (x) = \u03c3\u03021 (x)2 \u2212 1 \u2212 \u01eb\u0302(x)\n\n(4.11)\n\nd\u02c62 (x) = \u03c3\u03022 (x)2 \u2212 1 \u2212 \u01eb\u0302(x).\n\n(4.12)\n\nand\nNow \u03c3\u030212 = 1 + \u01eb, so the quantities d\u02c61 and d\u02c62 will be\nsmall;\n\u221a\n|d\u02c61,2 (x)| <\n(4.13)\n\u223c O(1/ N ) + O(\u01eb\u0302).\n\nWe now insert Eqs. (4.11) and (4.12) into the formula\n(4.6) for the function \u039e. We expand to second order in\n\u01eb, \u01eb\u0302, d\u02c61 , and d\u02c62 , treating these quantities as formally all\nof the same order. We then insert the result into Eqs.\n(2.9), (2.10), and (4.5), which yields\nZ\n\u0003\n\u0002\n\u039b(x) = d\u01ebp(0) (\u01eb) exp \u2212N (\u01eb \u2212 \u01eb\u0302b )2 + N \u01eb\u03022b , (4.14)\nwhere\n\ni\n1 h\u02c6\nd1 (x) + d\u02c62 (x)\n4\n\u0003\n1\u0002\n1\n\u03c3\u03021 (x)2 \u2212 1\n= \u01eb\u0302(x) +\n2\n4\n\u0003\n1\u0002\n+ \u03c3\u03022 (x)2 \u2212 1 .\n4\n\n\u01eb\u0302b (x) = \u01eb\u0302(x) +\n\n(4.15)\n\n\f9\nEvaluating the integral over \u01eb using the same types of\narguments as in previous sections gives, using p(0) (\u01eb) = 0\nfor \u01eb < 0,\nr\n\u03c0 (0)\np (\u01eb\u0302b ) exp[N \u01eb\u03022b ].\n(4.16)\n\u039b(x) \u2248 \u0398(\u01eb\u0302b )\nN\nHere \u0398 is the step function. If follows by the same type\nof arguments as before that \u039b(x) \u2243 \u0398(\u01eb\u0302b ) exp[N \u01eb\u03022b ].\nOur final result identifies the statistic \u01eb\u0302b as the optimal detection statistic; see Ref. [6] for a more general\nversion of this statistic. From Eq. (4.15) this statistic is\nnot the standard cross-correlation statistic \u01eb\u0302, but instead\ncontains the auto-correlation terms \u03c3\u030212 \u2212 1 and \u03c3\u030222 \u2212 1.\nThe interpretation of these terms is that it is possible,\nunder the assumptions of this subsection, to measure the\nstochastic background signal with just one detector. If\nthe detector's noise variance is known, then one can just\nmeasure the variance of the detector's output and subtract the known noise variance to reveal the stochastic\nbackground contribution.\nOf course, in reality, the noise in detectors is not known\npriori, and is measured from the data. In particular,\nthere is no way that the detectors noise can be known\nbeforehand to a fractional accuracy of 10\u22124 . Therefore\nwe have to generalize the preceding analysis by allowing\nthe noise variances to be unknown parameters.\n\nB.\n\nTwo coincident co-aligned detectors, white\nGaussian noise, unknown variances\n\nWe assume that the noise in each detector is white and\nGaussian with variances \u03c31 and \u03c32 . We replace Eq. (4.3)\nwith\n\"\n#\nY\nn21j\n1\n\u221a\npn1 (n1 |\u03c31 ) =\nexp \u2212 2 ,\n(4.17)\n2\u03c31\n2\u03c0\u03c31\nj\n\nand\n\u03c322 + \u01eb = f22 \u03c3\u030222\n\n(4.21)\n\nWe also define the rescaled variables\n\u03b1=\n\n\u01eb\n,\n\u03c3\u03021 \u03c3\u03022\n\n\u03b1\u0302 =\n\n\u01eb\u0302\n.\n\u03c3\u03021 \u03c3\u03022\n\n(4.22)\n\nWe expand \u039e to second order around its local minimum\nat f1 = f2 = 1, \u03b1 = \u03b1\u0302:\n\u039e = 2 ln(2\u03c0\u03c3\u03021 \u03c3\u03022 ) + 2 + ln(1 \u2212 \u03b1\u03022 )\n1 + \u03b1\u03022\n4\u03b1\u0302\n+\n\u2206\u03b12 \u2212\n\u2206\u03b1(\u2206f1 + \u2206f2 )\n(1 \u2212 \u03b1\u03022 )2\n(1 \u2212 \u03b1\u03022 )2\n2\n+\n(\u2206f12 + \u2206f22 + 2\u03b1\u03022 \u2206f12 \u2206f22 ). (4.23)\n(1 \u2212 \u03b1\u03022 )2\nHere \u2206f1 = f1 \u2212 1, \u2206f2 = f2 \u2212 1, and \u2206\u03b1 = \u03b1 \u2212 \u03b1\u0302. At\nfixed \u03b1, \u039e is minimized at\nf1 = f2 = 1 +\n\n\u03b1\u0302\n(\u03b1 \u2212 \u03b1\u0302).\n(1 + \u03b1\u03022 )\n\n(4.24)\n\nWe now perform the Gaussian integral over \u03c31 , \u03c32 or f1 ,\nf2 , which gives\np(x|\u01eb) =\n\np\u03c3 [\u03c3\u03021b (\u01eb), \u03c3\u03022b (\u01eb)] (1 \u2212 \u03b1\u03022 )3/2\nJ (\u01eb)\n(2\u03c0\u03c3\u03021 \u03c3\u03022 )N \u22121 4(1 + \u03b1\u03022 )1/2\n\u0015\n\u0014\nN\n(4.25)\n(\u03b1 \u2212 \u03b1\u0302)2 .\n\u00d7 exp \u2212\n2(1 + \u03b1\u03022 )\n\nHere \u03c3\u03021b (\u01eb) is the value of \u03c31 at the peak (4.24) of the\nintegrand, given from Eqs. (4.20) and (4.24) by\n\u00152\n\u0014\n\u03b1\u0302(\u03b1 \u2212 \u03b1\u0302)\n2\n\u03c3\u03021b\n(\u01eb) = \u2212\u01eb + 1 +\n\u03c3\u030212 ,\n(4.26)\n1 + \u03b1\u03022\nand similarly for \u03c3\u03022b (\u01eb). The factor J (\u01eb) is a Jacobian\nfactor given by\n\u0012\n\u0013\u22121/2 \u0012\n\u0013\u22121/2\n\u01eb\n\u01eb\nJ (\u01eb) = 1 \u2212 2 2\n1\u2212 2 2\n,\n(4.27)\nf1 \u03c3\u03021\nf2 \u03c3\u03022\n\nwith a similar equation for the second detector. The prior\ndistribution for the parameters \u03c31 , \u03c32 will be written as\np\u03c3 (\u03c31 , \u03c32 ). By inserting the distributions (4.17) and (4.4)\nwhere f1 and f2 are given in terms of \u01eb by Eqs. (4.22)\ninto the formulae (2.11) and (2.14) we obtain\n\u0015 and (4.24).\n\u0014\nZ \u221e\nZ \u221e\nWe next insert the result (4.25) for p(x|\u01eb) into Eqs.\nN\np(x|\u01eb) =\nd\u03c31\nd\u03c32 p\u03c3 (\u03c31 , \u03c32 ) exp \u2212 \u039e(\u01eb, \u03c31 , \u03c32 ) , (2.9) and (2.10). The result is\n2\n0\n0\nZ\n(4.18)\np\u03c3 [\u03c3\u03021b (\u01eb), \u03c3\u03022b (\u01eb)] J (\u01eb)\nwhere the function \u039e is now given by\n\u039b(x) =\nd\u01ebp(0) (\u01eb)\np\u03c3 [\u03c3\u03021b (0), \u03c3\u03022b (0)] J (0)\n\u0002\n\u0003\n\u0014\n\u0015\n\u039e(\u01eb, \u03c31 , \u03c32 ) = 2 ln(2\u03c0) + ln \u03c312 \u03c322 + \u01eb(\u03c312 + \u03c322 )\nN\n2\n\u00d7 exp \u2212\n(\u03b1 \u2212 2\u03b1\u03b1\u0302) . (4.28)\n2(1 + \u03b1\u03022 )\n(\u03c312 + \u01eb)\u03c3\u030222 + (\u03c322 + \u01eb)\u03c3\u030212 \u2212 2\u01eb\u01eb\u0302\n+\n. (4.19)\n\u03c312 \u03c322 + \u01eb(\u03c312 + \u03c322 )\nFinally, integrating over \u01eb gives\nr\nTo evaluate the integral over \u03c31 and \u03c32 in Eq. (4.18),\n2\u03c0(1 + \u03b1\u03022 ) (0) p\u03c3 [\u03c3\u03021b (\u01eb\u0302), \u03c3\u03022b (\u01eb\u0302)] J (\u01eb\u0302)\n\u039b(x) =\np (\u01eb\u0302)\nwe make a change of variables to variables f1 , f2 defined\nN\np\u03c3 [\u03c3\u03021b (0), \u03c3\u03022b (0)] J (0)\nby\n\u0014\n\u0015\n2\nN \u03b1\u0302\n\u00d7 exp\n\u0398(\u03b1\u0302),\n(4.29)\n\u03c312 + \u01eb = f12 \u03c3\u030212\n(4.20)\n2(1 + \u03b1\u03022 )\n\n\f10\nand invoking the arguments of the Secs. III A and III B\nabove for neglecting the prefactors gives\n\u0014\n\u0015\nN \u03b1\u03022\n\u039b(x) \u2243 exp\n\u0398(\u03b1\u0302).\n(4.30)\n2(1 + \u03b1\u03022 )\nThus, in the limit \u03b1\u0302 \u226a 1, \u039b(x) is equivalent to the usual\ncross-correlation statistic \u0398(\u03b1\u0302)\u03b1\u0302 defined by Eqs. (4.7),\n(4.8), (4.9), and (4.22).\nWe end this subsection by recapitulating the various\napproximations necessary to obtain the result:\n\u2022 The large N approximation N \u226b 1, necessary for\nthe validity of the Laplace approximation in integrating over \u03c31 , \u03c32 .\n\u2022 The assumption that we are in the regime where\nthe signal is detectable, exp[N \u03b1\u03022 /2] \u226b 1. This is\nnecessary for the evaluation of the integral over \u01eb\nin Eq. (4.28), and for neglecting the prefactors in\nderiving Eq. (4.30).\n\u2022 The assumption, as before, that the prior probability distributions p(0) (\u01eb) and p\u03c3 (\u03c31 , \u03c32 ) are slowly\nvarying.\n\u2022 The weak signal approximation \u03b1\u0302 \u226a 1, which will\nbe satisfied unless the stochastic background contribution to the output of one of the detectors becomes comparable to the noise in that detector. As\ndiscussed above, for signal strengths at the margin\nof detectability, and for several month searches for\na stochastic background with ground based interferometers, we have \u03b1\u0302 \u223c 10\u22124 .\nC.\n\nTwo coincident, co-aligned detectors, white\nnon-Gaussian noise\n\nWe next turn to the non-Gaussian noise model of Sec.\nIII.A. of paper I. The noise in each detector is assumed\nto be white, with each sample statistically independent\nand identically distributed, so that\nY\nexp [\u2212f1 (n1j ) \u2212 f2 (n2j )] .\n(4.31)\npn (n1 , n2 ) =\nj\n\nHowever, it is clear that we cannot assume that the noise\ndistributions e\u2212f1 and e\u2212f2 in each detector are known in\nadvance. Otherwise, as explained in Sec. IV A, the analysis would predict that one can measure the stochastic\nbackground in a single detector by measuring the noise\ndistribution and subtracting from it the \"known\" noise\ndistribution.\nTherefore, in this subsection, we will allow the functions f1 (x) and f2 (x) to be arbitrary except for the normalization conditions\nZ\nZ\n\u2212f1 (x)\ne\ndx = e\u2212f2 (x) dx = 1.\n(4.32)\n\nFormally, there are an infinite number of parameters to\nspecify to determine the functions f1 and f2 . However,\nin practice these distributions will be measured as histograms, determined by a finite set of numbers or parameters. We identify this finite set of parameters with\nthe noise parameters \u03b8n of Eq. (2.14). We rewrite Eq.\n(2.14) as\nZ\nZ\npn (n1 , n2 ) = Df1 Df2 pn (n1 , n2 |f1 , f2 ) pf [f1 , f2 ].\n(4.33)\nHere for simplicity we have used a functional or path integral notation for the integral over the noise parameters\n(even though the integral is only over a finite number of\nparameters). In Eq. (4.33), pf [f1 , f2 ] is the prior probability density functional for the functions f1 and f2 , and\npn (n1 , n2 |f1 , f2 ) is given by the expression on the right\nhand side of Eq. (4.31). The probability distribution\npf [f1 , f2 ] encodes our assumption that the noise distributions will have central Gaussian regions, with unknown\nvariances \u03c31 and \u03c32 , and arbitrary tail regions containing\na small fraction ptail of the total probability.\nWe now insert the distributions (4.4) and (4.33) into\nEqs. (2.11) and (2.14) and expand to second order in \u01eb.\nThe result is\nZ\nY\ne\u2212f1 (x1j )\u2212f2 (x2j )\np(x|\u01eb) =\nDf1 Df2 pf [f1 , f2 ]\nj\n\n\u00d7\n\nY\u0002\nj\n\n\u0003\n1 + \u01ebAj + \u01ebCj + \u01eb2 Ej + O(\u01eb3 ) ,(4.34)\n\nwhere\nAj =\nand\n\n\u0003\n1\u0002 \u2032\nf (x1j )2 + f2\u2032 (x2j )2 \u2212 f1\u2032\u2032 (x1j ) \u2212 f2\u2032\u2032 (x2j ) ,\n2 1\n(4.35)\nCj = f1\u2032 (x1j )f2\u2032 (x2j ).\n\n(4.36)\n\nThe quantity Ej is a sum of terms of the form\n(n)\n(n)\n(n)\n(l)\nf1 (x1j )m , f2 (x2j )m , and f1 (x1j )m f2 (x2j )r for integers n, m, l, r, whose exact form will not be needed here.\nWorking to second order in \u01eb, we can re-express Eq.\n(4.34) as\nZ\nZ\np(x|\u01eb) =\nDf1\nDf2 pf [f1 , f2 ]\n\u00d7 exp [\u2212\u039e(\u01eb, f1 , f2 )] ,\n\n(4.37)\n\nwhere\n\u039e(\u01eb, f1 , f2 ) = \u039e0 [f1 , f2 ] + \u01eb\u039e1 [f1 , f2 ] + \u01eb2 \u039e2 [f1 , f2 ]\n+O(\u01eb3 ),\n(4.38)\nwhere\n\u039e0 [f1 , f2 ] = \u2212\n\nX\nj\n\nf1 (x1j ) + f2 (x2j ),\n\n(4.39)\n\n\f11\n\u039e1 [f1 , f2 ] =\n\nX\nj\n\n(Aj + Cj ),\n\n(4.40)\n\nand\n\u039e2 [f1 , f2 ] =\n\nX\u0014\nj\n\n\u0015\n1\n2\nEj \u2212 (Aj + Cj ) .\n2\n\n(4.41)\n\nWe now note that we can eliminate the autocorrelation terms Aj from Eq. (4.40) by making a change\nof variables. We define the operator P\u01eb that acts on functions via\n1\n1\n(P\u01eb f )(x) = f (x) \u2212 \u01ebf \u2032 (x)2 + \u01ebf \u2032\u2032 (x),\n2\n2\n\n(4.42)\n\nand we define the functions F1 and F2 by\nF1 = P\u01eb f1 ,\n\nF2 = P\u01eb f2 .\n\n(4.43)\n\nUsing Eqs. (4.35), (4.38)\u2013(4.40), (4.42) and (4.43) the\nfunctional \u039e can be rewritten as\n\u039e(\u01eb, f1 , f2 ) = \u039e0 [F1 , F2 ] + \u01eb\u039e\u03031 [F1 , F2 ] + \u01eb2 \u039e\u03032 [F1 , F2 ]\n+O(\u01eb3 ).\n(4.44)\nHere the first order piece \u039e\u03031 consists only of the crosscorrelation term,\nX\n(4.45)\nF1\u2032 (x1j )F2\u2032 (x2j );\n\u039e\u03031 [F1 , F2 ] =\nj\n\nthe corrections to this cross-correlation expression due\nto changing from f1 , f2 to F1 , F2 do not appear at this\n(linear) order in \u01eb and instead contribute to the second\norder term \u039e\u03032 [F1 , F2 ]. The exact form of the functional\n\u039e\u03032 [F1 , F2 ] will not be needed for our arguments below.\nWe define the functions f\u02c61 (x) and f\u02c62 (x) to be the functions corresponding to the measured noise distributions\nat the two detectors. That is, they are step functions\ndefined by the requirement\nZ x\nX\n1\n\u02c6\ne\u2212f1 (u) du =\n1,\n(4.46)\nN\n\u2212\u221e\nj with x1j \u2264x\n\nwith a similar equation for f\u02c62 .\nConsider now the evaluation of the integral (4.37) with\n\u039e given by the expression (4.44). Consider first the \u01eb \u2192 0\nlimit. In this limit one can show from the normalization\nconditions (4.32) that \u039e will be minimized at the measured noise distributions:\nF1 = f\u02c61 ,\n\nF2 = f\u02c62 .\n\n(4.47)\n\nFor nonzero \u01eb, the leading order correction to the \u01eb = 0\nresult will be given by evaluating the function (4.44) at\nthe local minimum (4.47). We thus arrive at\nn\no\np(x|\u01eb) = J (x) pf [P\u01eb\u22121 f\u02c61 , P\u01eb\u22121 f\u02c62 ] exp \u2212\u039e0 [f\u02c61 , f\u02c62 ]\no\nn\n \u0303 [f\u02c6 , f\u02c6 ] .\n(4.48)\nexp \u2212\u01eb\u039e\u03031 [f\u02c61 , f\u02c62 ] \u2212 \u01eb2 \u039e\u0303\n2 1 2\n\nHere J (x) is a width factor whose origin is approximating the integrals over f1 and f2 as the value of the integrand at the peak times the \"width\" of the peak [15].\nThis factor is analogous to the various factors that appear in front of the exponential in Eq. (4.25). It depends\nweakly on x in comparison to the exponential factors.\n \u0303 in Eq. (4.48) will differ\nThe second order functional \u039e\u0303\n2\nfrom the corresponding functional \u039e\u03032 in Eq. (4.44) since\nthe location of the peak of the integrand will receive a\ncorrection of order \u01eb away from the value (4.47) which\nwill give a correction of order O(\u01eb2 ) to the value of the\nintegral.\nWe now insert the formula (4.48) for p(x, \u01eb) into Eqs.\n(2.9) and (2.10). This gives\nZ \u221e\npf [P\u01eb\u22121 f\u02c61 , P\u01eb\u22121 f\u02c61 ]\nd\u01ebp(0) (\u01eb)\n\u039b(x) =\npf [f\u02c61 , f\u02c61 ]\n0\no\nn\n \u0303 [f\u02c6 , f\u02c6 ] .(4.49)\n\u00d7 exp \u2212\u01eb\u039e\u0303 [f\u02c6 , f\u02c6 ] \u2212 \u01eb2 \u039e\u0303\n1\n\n1\n\n2\n\n2\n\n1\n\n2\n\nEvaluating the integral over \u01eb gives\ns\n\u03c0 (0) pf [P\u01eb\u0302\u22121 f\u02c61 , P\u01eb\u0302\u22121 f\u02c61 ]\np (\u01eb\u0302)\n\u039b(x) =\n \u0303\npf [f\u02c61 , f\u02c61 ]\n\u039e\u0303\n2\n(\n)\n\u039e\u03031 [f\u02c61 , f\u02c62 ]2\n\u00d7 exp\n\u0398(\u039e\u03031 ),\n \u0303 [f\u02c6 , f\u02c6 ]\n4\u039e\u0303\n2 1 2\n\n(4.50)\n\nand as before we can neglect the prefactors to give\n)\n(\n\u039e\u03031 [f\u02c61 , f\u02c62 ]2\n\u0398(\u039e\u03031 ).\n(4.51)\n\u039b(x) \u2243 exp\n \u0303 [f\u02c6 , f\u02c6 ]\n4\u039e\u0303\n2 1 2\nNow the statistic \u039e\u03031 [f\u02c61 , f\u02c62 ] defined by Eqs. (4.45) and\n(4.46) coincides with the locally optimal statistic obtained in paper I [Eq. (3.8) of paper I, specialized to\na white stochastic background], except for the following\nmodification. One first measures the noise probability\ndistributions in each detector separately [cf. Eq. (4.46)].\nThen, one computes the generalized cross-correlation\nstatistic (4.45) using the those distributions and the measured data.\nFrom Eq. (4.51), we see that \u039b(x) will be approximately equivalent to the locally optimal statistic\n \u0303 [f\u02c6 , f\u02c6 ] has a weak depen\u039e\u03031 [f\u02c61 , f\u02c62 ] if the statistic \u039e\u0303\n2 1 2\ndence on the data x. To establish this, consider the limit\nptail \u2192 0, where ptail \u226a 1 is the total probability in the\nnoise distribution tails. In that limit our assumptions imply that the noise distributions in the two detectors are\nGaussians with unknown variances \u03c31 , \u03c32 , and therefore\nthe analysis of this subsection reduces to the analysis of\nSec. IV B above. Therefore we can read off the ptail \u2192 0\n \u0303 [f\u02c6 , f\u02c6 ] by comparing Eqs. (4.28)\nlimit of the statistic \u039e\u0303\n2 1 2\nand (4.49) and identifying the coefficients of \u01eb2 in the\narguments of the exponentials. We thus obtain\n \u0303 [f\u02c6 , f\u02c6 ] =\n\u039e\u0303\n2 1 2\n\nN\n1\n[1 + O(ptail )] .\n2(1 + \u03b1\u03022 ) \u03c3\u030212 \u03c3\u030222\n\n(4.52)\n\n\f12\nIn the limit \u03b1\u0302 \u226a 1 we can neglect the \u03b1\u0302 dependence in\nEq. (4.52). We then see that the x-dependent fluctuations in the statistic are suppressed by either the small\nparameter ptail , as\u221aas in Secs. III C and III D above, or by\nthe parameter 1/ N governing the size of the fractional\nfluctuations of the statistics \u03c3\u03021 , \u03c3\u03022 . Thus, we can neglect\n \u0303 [f\u02c6 , f\u02c6 ] in Eq. (4.51), and the apthe x dependence of \u039e\u0303\n2 1 2\nproximate equivalence of \u039b(x) and the locally optimal\nstatistic \u039e\u03031 [f\u02c61 , f\u02c62 ] follows.\nFinally, we remark that we have not analyzed, in this\npaper, the most general situation for stochastic signals\nof separated, non-aligned detectors with colored noise,\nwhich was analyzed in Sec. IV.B of paper I [16]. However, the results we have obtained make it very plausible\nthat, for that more general situation, the Bayesian statistic \u039b(x) should again be equivalent to the generalized\ncross-correlation statistic derived in paper I.\nWe also note that our assumption that the stochastic\nsignal be Gaussian is necessary for our analysis. Modifying the signal by making it be non-Gaussian instead\nof Gaussian would alter Eqs. (4.6), (4.19) and (4.38) at\nO(\u01eb2 ). Therefore, the derivation here does not generalize\nstraightforwardly to non-Gaussian stochastic signals, unlike the corresponding derivations in paper I. In Ref. [5]\nit is shown that one can find detection techniques tailored\nto non-Gaussian stochastic signals that perform better,\nfor such signals, than the methods considered here.\n\nus increased confidence in the utility of those strategies.\nIn addition, the analysis of this paper has clarified the\nregime in which we expect the strategies to work well.\nFor deterministic signals, data segments to be analyzed\nshould be long enough that the signal-to-noise squared\nper data point be small. This requirement is easy to satisfy in practice, as signal-to-noise thresholds are usually\nin the range 5 \u2212 10. In addition, the strategies will only\nbe close to optimal in the regime where signals are strong\nenough to be detectable; this restriction is unimportant\nin practice, as the performance of detection statistics in\nthe regime where signals are far too weak to be detected is\nnot important. Finally, for stochastic signals, the signal\nmust be small compared to the noise in each individual\ndetector, and the total probability in the tail part of the\nnoise distributions must be small.\n\nAcknowledgments\n\nThe derivation in this paper, from a different framework, of the detection strategies obtained in paper I gives\n\nThis research was supported in part by NSF grants\nPHY-9728704, PHY-9722189, PHY-9981795, PHY0071028, NASA grant NASA-JPL 961298, the Sloan\nFoundation, and by the Max Planck Society (Albert Einstein Institute, Potsdam). We thank Steve Drasco for\nhelpful conversations and Tom Loredo for useful comments on the manuscript.\n\n[1] B. Allen, J.D.E. Creighton, \u00c9. \u00c9. Flanagan, J.D Romano,\nRobust statistics for deterministic and stochastic\ngravitational waves in non-Gaussian noise I: Frequentist\nanalyses, gr-qc/0105100.\n[2] S. Klimenko and G. Mitselmakher, A cross correlation\ntechnique in wavelet domain for detection of stochastic\ngravitational waves, unpublished LIGO technical report\nLIGO-T010125-00-D.\n[3] L.S. Finn, Phys. Rev. D 46, 5236 (1992).\n[4] P. J. Bickel and K. A. Doksum, Mathematical statistics:\nbasic ideas and selected topics (Holden-Day, Inc., California, 1977), Sec. 6.4.\n[5] S. Drasco and \u00c9.\u00c9. Flanagan, Detection methods for nonGaussian gravitational wave stochastic backgrounds, in\npreparation.\n[6] L.S. Finn, J.D. Romano, Gravitational-wave data analysis with multiple detectors: Stochastic signals, in preparation; ibid, Detecting stochastic gravitational waves: Performance of maximum-likelihood and cross-correlation\nstatistics, in preparation.\n[7] W.G. Anderson, P.R. Brady, J.D.E. Creighton, and \u00c9.\n\u00c9. Flanagan, Phys. Rev. D 63, 042003 (2001).\n[8] Saleem A. Kassam, Signal detection in non-Gaussian\n\nnoise, (Springer-Verlag, New York, 1988).\n[9] R.F. Michelson, Mon. Not. R. astr. Soc. 227, 933 (1987);\nN.L. Christensen, Phys. Rev. D 46, 5250, (1992).\n[10] B. Allen, in Proceedings of the Les Houches School on\nAstrophysical Sources of Gravitational Waves, edited by\nJean-Alain Marck and Jean-Pierre Lasota (Cambridge\nUniversity Press, Cambridge, England, 1997).\n[11] \u00c9. \u00c9. Flanagan, Phys. Rev. D 48, 2389 (1993). Note\nthat the second term on the RHS of Eq. (B6) should\nread \u221210j1 (\u03b1) rather than \u22122j1 (\u03b1), that the sliding delay function shown in Fig. 2 is incorrect, and that the\nright hand side of Eq. (2.8) should be divided by \u03c0.\n[12] The quantity denoted by \u015dj here was denoted by sj in\npaper I.\n[13] In the case where the prior distribution p(0) (\u01eb) restricts\nthe sign of \u01eb to be positive, one can similarly show that,\nin the relevant regime given by \u01eb\u0302(x) > 0 together with\nthe condition (3.7), \u039b(x) is to a good approximation a\nmonotonic function of the standard detection statistic\n\u0398[\u01eb\u0302(x)] \u01eb\u0302(x), where \u0398 is the step function. Similar comments apply to the statistics (3.39) and (3.47), .\n[14] This follows from the relation \u03c12 /N = \u015d2rms \u01eb2 /\u03c3 2 , where\n\u015drms is the rms average of the quantities \u015dj , and from the\n\nV.\n\nCONCLUSION\n\n\f13\nassumption \u03c12 /N \u226a 1.\n[15] In order that the integrand in Eq. (4.37) be sharply\npeaked, it is necessary to make the number of parameters specifying the functions f1 , f2 , f\u02c61 , and f\u02c62 considerably less than the number N of data points, by modifying\nEq. (4.46) to incorporate a suitable coarse-graining of the\n\nbinning. This modification does not affect our argument.\n[16] For Gaussian noise, the equivalence of \u039b(x) and the standard cross-correlation statistic has been demonstrated\nfor separated, non-aligned detectors with colored noise\nin Appendix A of Ref. [11], in the limit of weak signals.\n\n\f"}