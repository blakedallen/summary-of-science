{"id": "http://arxiv.org/abs/cs/0505057v1", "guidislink": true, "updated": "2005-05-23T07:59:33Z", "updated_parsed": [2005, 5, 23, 7, 59, 33, 0, 143, 0], "published": "2005-05-23T07:59:33Z", "published_parsed": [2005, 5, 23, 7, 59, 33, 0, 143, 0], "title": "Improved Bounds on the Parity-Check Density and Achievable Rates of\n  Binary Linear Block Codes with Applications to LDPC Codes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0405012%2Ccs%2F0405025%2Ccs%2F0405109%2Ccs%2F0405049%2Ccs%2F0405024%2Ccs%2F0405089%2Ccs%2F0405082%2Ccs%2F0405110%2Ccs%2F0405019%2Ccs%2F0405006%2Ccs%2F0405016%2Ccs%2F0405073%2Ccs%2F0405005%2Ccs%2F0405052%2Ccs%2F0405031%2Ccs%2F0405030%2Ccs%2F0405077%2Ccs%2F0405068%2Ccs%2F0405090%2Ccs%2F0405053%2Ccs%2F0405083%2Ccs%2F0405057%2Ccs%2F0405003%2Ccs%2F0405086%2Ccs%2F0405020%2Ccs%2F0405035%2Ccs%2F0405038%2Ccs%2F0405039%2Ccs%2F0405071%2Ccs%2F0405096%2Ccs%2F0405092%2Ccs%2F0405098%2Ccs%2F0405013%2Ccs%2F0405087%2Ccs%2F0405066%2Ccs%2F0405051%2Ccs%2F0405014%2Ccs%2F0405094%2Ccs%2F0405075%2Ccs%2F0405065%2Ccs%2F0405085%2Ccs%2F0405032%2Ccs%2F0405034%2Ccs%2F0405070%2Ccs%2F0405002%2Ccs%2F0405050%2Ccs%2F0405105%2Ccs%2F0405026%2Ccs%2F0405022%2Ccs%2F0405058%2Ccs%2F0405007%2Ccs%2F0405048%2Ccs%2F0405043%2Ccs%2F0405008%2Ccs%2F0405010%2Ccs%2F0405055%2Ccs%2F0405045%2Ccs%2F0505054%2Ccs%2F0505021%2Ccs%2F0505088%2Ccs%2F0505016%2Ccs%2F0505057%2Ccs%2F0505042%2Ccs%2F0505063%2Ccs%2F0505037%2Ccs%2F0505052%2Ccs%2F0505036%2Ccs%2F0505001%2Ccs%2F0505039%2Ccs%2F0505007%2Ccs%2F0505022%2Ccs%2F0505006%2Ccs%2F0505004%2Ccs%2F0505012%2Ccs%2F0505081%2Ccs%2F0505051%2Ccs%2F0505079%2Ccs%2F0505076%2Ccs%2F0505031%2Ccs%2F0505038%2Ccs%2F0505048%2Ccs%2F0505030%2Ccs%2F0505055%2Ccs%2F0505062%2Ccs%2F0505015%2Ccs%2F0505083%2Ccs%2F0505047%2Ccs%2F0505087%2Ccs%2F0505085%2Ccs%2F0505049%2Ccs%2F0505058%2Ccs%2F0505023%2Ccs%2F0505034%2Ccs%2F0505073%2Ccs%2F0505017%2Ccs%2F0505009%2Ccs%2F0505026%2Ccs%2F0505044%2Ccs%2F0505003%2Ccs%2F0505061%2Ccs%2F0505045&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Improved Bounds on the Parity-Check Density and Achievable Rates of\n  Binary Linear Block Codes with Applications to LDPC Codes"}, "summary": "We derive bounds on the asymptotic density of parity-check matrices and the\nachievable rates of binary linear block codes transmitted over memoryless\nbinary-input output-symmetric (MBIOS) channels. The lower bounds on the density\nof arbitrary parity-check matrices are expressed in terms of the gap between\nthe rate of these codes for which reliable communication is achievable and the\nchannel capacity, and the bounds are valid for every sequence of binary linear\nblock codes. These bounds address the question, previously considered by Sason\nand Urbanke, of how sparse can parity-check matrices of binary linear block\ncodes be as a function of the gap to capacity. Similarly to a previously\nreported bound by Sason and Urbanke, the new lower bounds on the parity-check\ndensity scale like the log of the inverse of the gap to capacity, but their\ntightness is improved (except for a binary symmetric/erasure channel, where\nthey coincide with the previous bound). The new upper bounds on the achievable\nrates of binary linear block codes tighten previously reported bounds by\nBurshtein et al., and therefore enable to obtain tighter upper bounds on the\nthresholds of sequences of binary linear block codes under ML decoding. The\nbounds are applied to low-density parity-check (LDPC) codes, and the\nimprovement in their tightness is exemplified numerically. The upper bounds on\nthe achievable rates enable to assess the inherent loss in performance of\nvarious iterative decoding algorithms as compared to optimal ML decoding. The\nlower bounds on the asymptotic parity-check density are helpful in assessing\nthe inherent tradeoff between the asymptotic performance of LDPC codes and\ntheir decoding complexity (per iteration) under message-passing decoding.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0405012%2Ccs%2F0405025%2Ccs%2F0405109%2Ccs%2F0405049%2Ccs%2F0405024%2Ccs%2F0405089%2Ccs%2F0405082%2Ccs%2F0405110%2Ccs%2F0405019%2Ccs%2F0405006%2Ccs%2F0405016%2Ccs%2F0405073%2Ccs%2F0405005%2Ccs%2F0405052%2Ccs%2F0405031%2Ccs%2F0405030%2Ccs%2F0405077%2Ccs%2F0405068%2Ccs%2F0405090%2Ccs%2F0405053%2Ccs%2F0405083%2Ccs%2F0405057%2Ccs%2F0405003%2Ccs%2F0405086%2Ccs%2F0405020%2Ccs%2F0405035%2Ccs%2F0405038%2Ccs%2F0405039%2Ccs%2F0405071%2Ccs%2F0405096%2Ccs%2F0405092%2Ccs%2F0405098%2Ccs%2F0405013%2Ccs%2F0405087%2Ccs%2F0405066%2Ccs%2F0405051%2Ccs%2F0405014%2Ccs%2F0405094%2Ccs%2F0405075%2Ccs%2F0405065%2Ccs%2F0405085%2Ccs%2F0405032%2Ccs%2F0405034%2Ccs%2F0405070%2Ccs%2F0405002%2Ccs%2F0405050%2Ccs%2F0405105%2Ccs%2F0405026%2Ccs%2F0405022%2Ccs%2F0405058%2Ccs%2F0405007%2Ccs%2F0405048%2Ccs%2F0405043%2Ccs%2F0405008%2Ccs%2F0405010%2Ccs%2F0405055%2Ccs%2F0405045%2Ccs%2F0505054%2Ccs%2F0505021%2Ccs%2F0505088%2Ccs%2F0505016%2Ccs%2F0505057%2Ccs%2F0505042%2Ccs%2F0505063%2Ccs%2F0505037%2Ccs%2F0505052%2Ccs%2F0505036%2Ccs%2F0505001%2Ccs%2F0505039%2Ccs%2F0505007%2Ccs%2F0505022%2Ccs%2F0505006%2Ccs%2F0505004%2Ccs%2F0505012%2Ccs%2F0505081%2Ccs%2F0505051%2Ccs%2F0505079%2Ccs%2F0505076%2Ccs%2F0505031%2Ccs%2F0505038%2Ccs%2F0505048%2Ccs%2F0505030%2Ccs%2F0505055%2Ccs%2F0505062%2Ccs%2F0505015%2Ccs%2F0505083%2Ccs%2F0505047%2Ccs%2F0505087%2Ccs%2F0505085%2Ccs%2F0505049%2Ccs%2F0505058%2Ccs%2F0505023%2Ccs%2F0505034%2Ccs%2F0505073%2Ccs%2F0505017%2Ccs%2F0505009%2Ccs%2F0505026%2Ccs%2F0505044%2Ccs%2F0505003%2Ccs%2F0505061%2Ccs%2F0505045&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We derive bounds on the asymptotic density of parity-check matrices and the\nachievable rates of binary linear block codes transmitted over memoryless\nbinary-input output-symmetric (MBIOS) channels. The lower bounds on the density\nof arbitrary parity-check matrices are expressed in terms of the gap between\nthe rate of these codes for which reliable communication is achievable and the\nchannel capacity, and the bounds are valid for every sequence of binary linear\nblock codes. These bounds address the question, previously considered by Sason\nand Urbanke, of how sparse can parity-check matrices of binary linear block\ncodes be as a function of the gap to capacity. Similarly to a previously\nreported bound by Sason and Urbanke, the new lower bounds on the parity-check\ndensity scale like the log of the inverse of the gap to capacity, but their\ntightness is improved (except for a binary symmetric/erasure channel, where\nthey coincide with the previous bound). The new upper bounds on the achievable\nrates of binary linear block codes tighten previously reported bounds by\nBurshtein et al., and therefore enable to obtain tighter upper bounds on the\nthresholds of sequences of binary linear block codes under ML decoding. The\nbounds are applied to low-density parity-check (LDPC) codes, and the\nimprovement in their tightness is exemplified numerically. The upper bounds on\nthe achievable rates enable to assess the inherent loss in performance of\nvarious iterative decoding algorithms as compared to optimal ML decoding. The\nlower bounds on the asymptotic parity-check density are helpful in assessing\nthe inherent tradeoff between the asymptotic performance of LDPC codes and\ntheir decoding complexity (per iteration) under message-passing decoding."}, "authors": ["Gil Wiechman", "Igal Sason"], "author_detail": {"name": "Igal Sason"}, "author": "Igal Sason", "arxiv_comment": "Submitted to IEEE Transactions on Information Theory (May 23rd, 2005)", "links": [{"href": "http://arxiv.org/abs/cs/0505057v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0505057v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0505057v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0505057v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:cs/0505057v1 [cs.IT] 23 May 2005\n\nImproved Bounds on the Parity-Check Density and\nAchievable Rates of Binary Linear Block Codes with\nApplications to LDPC Codes\nGil Wiechman\n\nIgal Sason\n\nTechnion \u2013 Israel Institute of Technology\nHaifa 32000, Israel\n{igillw@tx, sason@ee}.technion.ac.il\n\nJune 27, 2018\nAbstract\nWe derive bounds on the asymptotic density of parity-check matrices and the achievable rates of binary\nlinear block codes transmitted over memoryless binary-input output-symmetric (MBIOS) channels. The\nlower bounds on the density of arbitrary parity-check matrices are expressed in terms of the gap between the\nrate of these codes for which reliable communication is achievable and the channel capacity, and the bounds\nare valid for every sequence of binary linear block codes. These bounds address the question, previously\nconsidered by Sason and Urbanke, of how sparse can parity-check matrices of binary linear block codes\nbe as a function of the gap to capacity. Similarly to a previously reported bound by Sason and Urbanke,\nthe new lower bounds on the parity-check density scale like the log of the inverse of the gap to capacity,\nbut their tightness is improved (except for a binary symmetric/erasure channel, where they coincide with\nthe previous bound). The new upper bounds on the achievable rates of binary linear block codes tighten\npreviously reported bounds by Burshtein et al., and therefore enable to obtain tighter upper bounds on the\nthresholds of sequences of binary linear block codes under ML decoding. The bounds are applied to lowdensity parity-check (LDPC) codes, and the improvement in their tightness is exemplified numerically. The\nupper bounds on the achievable rates enable to assess the inherent loss in performance of various iterative\ndecoding algorithms as compared to optimal ML decoding. The lower bounds on the asymptotic parity-check\ndensity are helpful in assessing the inherent tradeoff between the asymptotic performance of LDPC codes\nand their decoding complexity (per iteration) under message-passing decoding.\n\nIndex Terms: Block codes, channel capacity, error probability, iterative decoding, linear codes,\nlow-density parity-check (LDPC) codes, maximum-likelihood (ML) decoding, thresholds.\n\n1\n\nIntroduction\n\nError correcting codes which employ iterative decoding algorithms are now considered state of the\nart in the field of low-complexity coding techniques. The graphical representation of these codes is\n\n\fused to describe their algebraic structure, and also enables a unified description of their iterative\ndecoding algorithms over various channels. These codes closely approach the capacity limit of many\nstandard communication channels under iterative decoding. By now, there is a large collection of\nfamilies of iteratively decoded codes including low-density parity-check (LDPC), turbo, repeataccumulate and product codes; all of them, demonstrate a rather small gap (in rate) to capacity\nwith feasible complexity. In [6], Khandekar and McEliece have suggested to study the encoding\nand decoding complexities of ensembles of iteratively decoded codes on graphs as a function of\ntheir gap to capacity. They conjectured that if the achievable rate under iterative message-passing\ndecoding is a fraction 1 \u2212 \u03b5 of the channel capacity, then for a wide class of channels, the encoding\ncomplexity scales like ln 1\u03b5 and the decoding complexity scales like 1\u03b5 ln 1\u03b5 . The only exception is the\nbinary erasure channel (BEC) where the decoding complexity behaves like ln 1\u03b5 (same as encoding\ncomplexity) because of the absolute reliability of the messages passed through the edges of the\ngraph (hence, every edge can be used only once during the iterative decoding process).\nLDPC codes are efficiently encoded and decoded due to the sparseness of their parity-check\nmatrices. In his thesis [4], Gallager proved that right-regular LDPC codes (i.e., LDPC codes with a\nconstant degree (aR ) of the parity-check nodes) cannot achieve the channel capacity on a BSC, even\nunder optimal ML decoding. This inherent gap to capacity is well approximated by an expression\nwhich decreases to zero exponentially fast in aR . Richardson et al. [11] have extended this result,\nand proved that the same conclusion holds if aR designates the maximal right degree. Sason and\nUrbanke later observed in [13] that the result still applies when considering the average right degree.\nGallager's bound [4, Theorem 3.3] provides an upper bound on the rate of right-regular LDPC codes\nwhich achieve reliable communications over the BSC. Burshtein et al. have generalized Gallager's\nbound for a general MBIOS channel [1], and the work in [13] relies on their generalization.\nConsider the number of ones in a parity-check matrix which represents a binary linear code,\nand normalize it per information bit (i.e., with respect to the dimension of the code). This quantity\n(which will be later defined as the density of the parity-check matrix) is equal to 1\u2212R\nR times the\naverage right degree of the bipartite graph that represents the code, where R is the rate of the code\nin bits per channel use. In [13], Sason and Urbanke considered how sparse can parity-check matrices\nof binary linear block codes be, as a function of their gap to capacity (where this gap depends in\ngeneral on the channel and on the decoding algorithm). An information-theoretic lower bound\non the asymptotic density of parity-check matrices was derived in [13, Theorem 2.1] where this\nbound applies to every MBIOS channel and every sequence of binary linear block codes achieving a\nfraction 1 \u2212 \u03b5 of the channel capacity with vanishing bit error probability. It holds for an arbitrary\nK1 +K2 ln\n\n1\n\n\u03b5\nwhere K1\nrepresentation of parity-check matrices for these codes, and is of the form\n1\u2212\u03b5\nand K2 are constants which only depend on the channel. Though the logarithmic behavior of\nthis lower bound is in essence correct (due to a logarithmic behavior of the upper bound on the\nasymptotic parity-check density in [13, Theorem 2.2]), the lower bound in [13, Theorem 2.1] is not\ntight (with the exception of the BEC, as demonstrated in [13, Theorem 2.3], and possibly also the\nbinary symmetric channel (BSC)). The derivation of the bounds in this paper was motivated by\nthe desire to improve the results in [1, Theorems 1 and 2] and [13, Theorem 2.1] which are based\non a two-level quantization of the log-likelihood ratio (LLR).\n\nIn [7], Measson and Urbanke derived an upper bound on the maximum-likelihood (ML) thresholds of LDPC ensembles when the codes are transmitted over the BEC. Their general approach\nrelies on EXIT-like functions and the area theorem. This bound coincides with the ML threshold\ndetermined by Montanari et al. using the replica method, showing that the bound is in fact tight.\nIn [8], Montanari presented a new approach for the analysis of codes on graphs under maximum\na posteriori probability (MAP) decoding. His approach is based on statistical mechanics, and the\nresulting expressions are related to the density evolution analysis of belief propagation decoding.\n\n\fMotivated by the heuristic statistical mechanics results, it was conjectured in [8] that the bounds\non the asymptotic achievable rates of LDPC codes are tight.\nWe derive in this paper improved bounds on the achievable rates and the asymptotic paritycheck density of sequences of binary linear block codes. The bounds in [1, 13] and this paper\nare valid for every sequence of binary linear block codes, in contrast to a high probability result\nwhich was previously derived for the binary erasure channel (BEC) from density evolution analysis\n[14]. Shokrollahi proved in [14] that when the codes are communicated over a BEC, the growth\nrate of the average right degree (i.e., the average degree of the parity-check nodes in a bipartite\nTanner graph) is at least logarithmic in terms of the gap to capacity. The statement in [14] is a\nhigh probability result, and hence it is not necessarily satisfied for every particular code from this\nensemble. Further, it assumes a sub-optimal (iterative) decoding algorithm, where the statements\nin [1, 13] and this paper are valid even under optimal ML decoding.\nThe significance of the bounds in this paper is demonstrated in two respects. The new upper\nbounds on the achievable rates of binary linear block codes tighten previously reported bounds\nby Burshtein et al. [1], and therefore enable to obtain tighter upper bounds on the thresholds of\nsequences of binary linear block codes under ML decoding. They are applied to LDPC codes, and\nthe improvement in their tightness is exemplified numerically. Comparing the new upper bounds\non the achievable rates with thresholds provided by a density-evolution analysis gives rigorous\nbounds on the inherent loss in performance due to the sub-optimality of iterative message-passing\ndecoding (as compared to soft-decision ML decoding). The new lower bounds on the asymptotic\nparity-check density tighten the lower bound in [13, Theorem 2.1]. Since the parity-check density\ncan be interpreted as the complexity per iteration under iterative message-passing decoding, then\ntightening the reported lower bound on the parity-check density [13] gives insight on the tradeoff\nbetween the asymptotic performance and decoding complexity of LDPC codes.\nIn this paper, preliminary material is presented in Section 2, and the theorems are introduced\nand proved in Sections 3 and 4. The derivation of the bounds in Section 3 was motivated by\nthe desire to generalize the results in [1, Theorems 1 and 2] and [13, Theorem 2.1]. A two-level\nquantization of the log-likelihood ratio (LLR), in essence replacing the arbitrary MBIOS channel\nby a physically degraded binary symmetric channel (BSC), is modified in Section 3 to a quantized\nchannel which better reflects the statistics of the original channel (though the quantized channel\nis still physically degraded w.r.t. the original channel). The number of quantization levels of the\nLLR for the new channel is an arbitrary integer power of 2, and the calculation of these bounds is\nsubject to an optimization of the quantization levels, as to obtain the tightest bounds within their\nform. In Section 4, we rely on the conditional pdf of the LLR of the MBIOS channel, and operate\non an equivalent channel without quantizing the LLR. This second approach finally leads to bounds\nwhich are uniformly tighter than the bounds we derive in Section 3. It appears to be even simpler\nto calculate the un-quantized bounds in Section 4, as their calculation do not involve the solution\nof any optimization equation (in contrast to the quantized bounds, whose calculation involves a\nnumerical solution of optimization equations w.r.t. the quantization levels of the LLR). Fortunately,\nthe multi-dimensional integral obtained in the derivation of the bounds in Section 4 is transformed\nto a rapidly convergent infinite series of one-dimensional integrals; this issue is crucial in facilitating\nthe calculation of the bounds in Section 4. We note that the significance of both the quantized\nand un-quantized bounds in Sections 3 and 4, respectively, stems from a comparison between these\nbounds which gives insight on the effect of the number of quantization levels of the LLR (even if\nthey are optimally determined) on the achievable rates, as compared to the ideal case where no\nquantization is done. Numerical results are exemplified and explained in Section 5. Finally, we\nsummarize our discussion in Section 6 and present interesting issues which deserve further research.\nFour appendices provide further technical details referring to the proofs in Sections 3 and 4.\n\n\f2\n\nPreliminaries\n\nWe introduce here some definitions and theorems from [1, 13] which serve as a preliminary material\nfor the rest of the paper. Definitions 2.1 and 2.2 are taken from [13, Section 2].\nDefinition 2.1 (Capacity-Approaching Codes). Let {Cm } be a sequence of codes of rate Rm ,\nand assume that for every m, the codewords of the code Cm are transmitted with equal probability\nover a channel whose capacity is C. This sequence is said to achieve a fraction 1 \u2212 \u03b5 of the channel\ncapacity with vanishing bit (block) error probability if limm\u2192\u221e Rm = (1 \u2212 \u03b5)C, and if there exists a\ndecoding algorithm under which the average bit (block) error probability of the code Cm tends to\nzero in the limit where m \u2192 \u221e.\nDefinition 2.2 (Parity-Check Density). Let C be a binary linear code of rate R and block\nlength n, which is represented by a parity-check matrix H. We define the density of H, call it\n\u2206 = \u2206(H), as the normalized number of ones in H per information bit. The total number of ones\nin H is therefore equal to nR\u2206.\nDefinition 2.3 (Log-Likelihood Ratio (LLR)). Let pY |X (*|*) be the conditional pdf of an\narbitrary MBIOS channel. The log-likelihood ratio (LLR) at the output of the channel is\n\u0013\n\u0012\npY |X (y|X = 0)\n.\nLLR(y) , ln\npY |X (y|X = 1)\nThroughout the paper, we assume that all the codewords of a binary linear block code are\nequally likely to be transmitted. Also, we use the notation h2 (*) for the binary entropy function to\nbase 2, i.e., h2 (*) = \u2212x log2 (x) \u2212 (1 \u2212 x) log2 (1 \u2212 x).\nTheorem 2.1 (An Upper Bound on the Achievable Rates for Reliable Communication\nover MBIOS Channels). [1, Theorem 2]: Consider a sequence {Cm } of binary linear block codes\nof rate Rm , and assume that their block length tends to infinity as m \u2192 \u221e. Let Hm be a paritycheck matrix of the code Cm , and assume that dk,m designates the fraction of the parity-check\nequations involving k variables. Let\ndk , lim inf dk,m ,\nm\u2192\u221e\n\nR , lim inf Rm .\nm\u2192\u221e\n\n(1)\n\nSuppose that the transmission of these codes takes place over an MBIOS channel with capacity C\nbits per channel use, and let\nZ\n\u0001\n1 \u221e\nmin f (y), f (\u2212y) dy\n(2)\nw,\n2 \u2212\u221e\nwhere f (y) , pY |X (y|X = 1) designates the conditional pdf of the output of the MBIOS channel.\nThen, a necessary condition for vanishing block error probability as m \u2192 \u221e is\nR \u2264 1 \u2212 X\u001a\nk\n\n1\u2212C\n\u0013\u001b .\n\u0012\n1 \u2212 (1 \u2212 2w)k\ndk h2\n2\n\nTheorem 2.2 (Lower Bounds on the Asymptotic Parity-Check Density with 2-Levels\nQuantization). [13, Theorem 2.1]: Let {Cm } be a sequence of binary linear codes achieving a\nfraction 1 \u2212 \u03b5 of the capacity of an MBIOS channel with vanishing bit error probability. Then, the\nasymptotic density (\u2206m ) of their parity-check matrices satisfies\nlim inf \u2206m >\nm\u2192\u221e\n\nK1 + K2 ln 1\u03b5\n1\u2212\u03b5\n\n(3)\n\n\fwhere\n1\n1\u2212C\n(1 \u2212 C) ln 2 ln\n\u0010 2 \u0011C\nK1 =\n1\n2C ln 1\u22122w\n\n\u0001\n\n,\n\nK2 =\n\n1\u2212C\n\u0011\n\u0010\n1\n2C ln 1\u22122w\n\n(4)\n\nand w is defined in (2). For a BEC with erasure probability p, the coefficients K1 and K2 in (4)\nare improved to\n\u0010\n\u0011\np\np ln 1\u2212p\np\n\u0010\n\u0010\n\u0011 , K2 =\n\u0011.\nK1 =\n(5)\n1\n1\n(1 \u2212 p) ln 1\u2212p\n(1 \u2212 p) ln 1\u2212p\nUsing standard notation, an\nof (n, \u03bb, \u03c1) LDPC\nis characterized by its length n,\nP ensemble\nP\u221e codes\ni\u22121 and \u03c1(x) =\ni\u22121 , where \u03bb (\u03c1 ) is equal to the\nand the polynomials \u03bb(x) = \u221e\n\u03bb\nx\n\u03c1\nx\ni\ni\ni=2 i\ni=2 i\nprobability that a randomly chosen edge is connected to a variable (parity-check) node of degree i.\nThe variables (parity-check sets) are represented by the left (right) nodes of a bipartite graph which\nrepresents an LDPC code.\n\n3\n\nApproach I: Bounds Based on Quantization of the LLR\n\nIn this section, we introduce bounds on the achievable rates and the asymptotic parity-check density\nof sequences of binary linear block codes. The bounds generalize previously reported results in [1]\nand [13] which were based on a symmetric two-level quantization of the LLR. This is achieved by\nextending the concept of quantization to an arbitrary integer power of 2; to this end, our analysis\nrelies on the Galois field GF(2m ). In Section 3.1, we demonstrate the results and their proofs for\nfour-level quantization. In Section 3.2, we extend the results to a symmetric quantization with a\nnumber of levels which is an arbitrary integer power of 2. This order of presentation was chosen since\nmany concepts which are helpful for the generalization in Section 3.2 are written in a simplified\nnotation for the four-level quantization, along with all the relevant lemmas for the general case\nwhich are already introduced in the derivation of the bound with four-level quantization. This also\nshortens considerably the proof for the general quantization in Section 3.2.\n\n3.1\n\nBounds for Four-Levels of Quantization\n\nAs a preparatory step towards developing bounds on the parity-check density and the rate of binary\nlinear block codes, we present a lower bound on the conditional entropy of a transmitted codeword\ngiven the received sequence at the output of an arbitrary MBIOS channel.\nProposition 3.1. Let C be a binary linear block code of length n and rate R. Let x = (x1 , . . . xn )\nand y = (y1 , . . . , yn ) designate the transmitted codeword and received sequence, respectively, when\nthe communication takes place over an MBIOS channel with conditional pdf pY |X (*|*). For an\narbitrary positive l \u2208 R+ , let us define the probabilities p0 , p1 , p2 , p3 as follows:\np0 , Pr{LLR(Y ) > l | X = 0}\n1\nPr{LLR(Y ) = 0 | X = 0}\n2\n1\np2 , Pr{LLR(Y ) \u2208 [\u2212l, 0) | X = 0} + Pr{LLR(Y ) = 0 | X = 0}\n2\np3 , Pr{LLR(Y ) < \u2212l | X = 0}.\n\np1 , Pr{LLR(Y ) \u2208 (0, l] | X = 0} +\n\n(6)\n\n\fFor an arbitrary parity-check matrix of the code C, let dk designate the fraction of the parity-checks\ninvolving k variables. Then, the conditional entropy of the transmitted codeword given the received\nsequence satisfies\nH(X|Y)\n\u2265 1 \u2212 C \u2212 (1 \u2212 R) *\nn\n\uf8f1\n\uf8eb\nk \u0012 \u0013\nX\uf8f2 X\n1\u2212 1\u2212\nk\n*\ndk\n(p1 + p2 )t (p0 + p3 )k\u2212t h2 \uf8ed\n\uf8f3\nt\nt=0\n\nk\n\n2p2 \u0001t\np1 +p2\n\n2\n\n1\u2212\n\n\uf8f6\uf8fc\n\n2p3 \u0001k\u2212t \uf8fd\np0 +p3\n\uf8f8\n\n\uf8fe\n\n.(7)\n\nProof. Considering an MBIOS channel whose conditional pdf is given by pY |X (*|*), we introduce a\nnew physically degraded channel. It is a binary-input, quaternary-output symmetric channel (see\nFig. 1). To this end, let l \u2208 R+ be an arbitrary positive number, and let \u03b1 be a primitive element of\nthe Galois field GF(22 ) (so \u03b12 = 1 + \u03b1). The set of the elements of this field is {0, 1, \u03b1, 1 + \u03b1}. Let\nXi and Yi designate the random variables referring to the input and output of the original channel\npY |X (*|*) at time i (where i = 1, 2, . . . , n). We define the degraded channel as a channel with four\nquantization levels of the LLR. The output of the degraded channel at time i, zi , is calculated from\nthe output yi of the original channel as follows:\n\u2022 If LLR(yi ) > l, then zi = 0.\n\u2022 If 0 < LLR(yi ) \u2264 l, then zi = \u03b1.\n\u2022 If \u2212l \u2264 LLR(yi ) < 0, then zi = 1 + \u03b1.\n\u2022 If LLR(yi ) < \u2212l, then zi = 1.\n\u2022 If LLR(yi ) = 0, then zi is chosen as \u03b1 or 1 + \u03b1 with equal probability ( 12 ).\np0\n\n0\n\n0\n\np1\np2\n\u03b1\np3\np3\n1+\u03b1\np2\n\nZ\n\nX\n\np1\n1\n\n1\n\np0\n\nN\n\nFigure 1: The channel model in the left plot is a physically degraded channel used for the derivation\nof the bound with four levels of quantization. The element \u03b1 denotes a primitive element in GF(22 ).\nThis channel model is equivalent to a channel with an additive noise in GF(22 ) (see right plot).\nFrom the definition of the degraded channel in Fig. 1, this channel has an additive noise and is\nalso binary-input output-symmetric. It follows that the transition probabilities given in (6) can be\nexpressed in an equivalent way by\np0 = Pr(Z = 0 | X = 0) = Pr(Z = 1 | X = 1)\np1 = Pr(Z = \u03b1 | X = 0) = Pr(Z = 1 + \u03b1 | X = 1)\np2 = Pr(Z = 1 + \u03b1 | X = 0) = Pr(Z = \u03b1 | X = 1)\np3 = Pr(Z = 1 | X = 0) = Pr(Z = 0 | X = 1)\n\n\fwhere the symmetry in these transition probabilities holds since the original channel is MBIOS.\nSince C is a binary linear block code of length n and rate R, and the codewords are transmitted\nwith equal probability then\nH(X) = nR.\n(8)\nAlso, since the channel PY |X (*|*) is memoryless, then\nH(Y|X) = nH(Y |X).\n\n(9)\n\nWe designate the output sequences of the original channel and its degraded version by Y and Z,\nrespectively. Since the mapping from Yi to the degraded output Zi (i = 1, 2, * * * , n) is memoryless,\nthen H(Z|Y) = nH(Z|Y ), and\nH(Y) = H(Z) \u2212 H(Z|Y) + H(Y|Z)\n= H(Z) \u2212 nH(Z|Y ) + H(Y|Z)\n\nH(Y|Z) \u2264\n\nn\nX\n\n(10)\n\nH(Yi |Zi )\n\ni=1\n\n= nH(Y |Z)\n= n [H(Y ) \u2212 H(Z) + H(Z|Y )] .\n\n(11)\n\nApplying the above towards a lower bound on the conditional entropy H(X|Y), we get\nH(X|Y) = H(X) + H(Y|X) \u2212 H(Y)\n= nR + nH(Y |X) \u2212 H(Y)\n= nR + nH(Y |X) \u2212 H(Z) \u2212 H(Y|Z) + nH(Z|Y )\n\u2265 nR + nH(Y |X) \u2212 H(Z) \u2212 n [H(Y ) \u2212 H(Z) + H(Z|Y )] + nH(Z|Y )\n= nR \u2212 H(Z) + nH(Z) \u2212 n [H(Y ) \u2212 H(Y |X)]\n= nR \u2212 H(Z) + nH(Z) \u2212 nI(X; Y )\n\u2265 nR \u2212 H(Z) + nH(Z) \u2212 nC\n\n(12)\n\nwhere the second equality relies on (8) and (9), the third equality relies on (10), the first inequality\nrelies on (11), and I(X; Y ) \u2264 C is used for the last transition (where C designates the capacity\nof the non-degraded channel). In order to obtain a lower bound on H(X|Y) from (12), we will\ncalculate the exact entropy of the random variable Z, and find an upper bound on the entropy of\nthe random vector Z. This will finally provide the lower bound in (7).\nSince C is a binary linear block code, then the input X is equally likely to be zero or one. The\noutput Z of the degraded channel in Fig. 1 has the following probability law:\nPr(Z = 0) = Pr(Z = 0, X = 0) + Pr(Z = 0, X = 1)\n= Pr(Z = 0 | X = 0) Pr(X = 0) + Pr(Z = 0 | X = 1) Pr(X = 1)\np0 + p3\n=\n2\nand in a similar manner,\nPr(Z = 1) =\n\np0 + p3\n,\n2\n\nPr(Z = \u03b1) = Pr(Z = 1 + \u03b1) =\n\np1 + p2\n.\n2\n\n\fThe entropy of the random variable Z is therefore equal to\n\u0013\n\u0012\n\u0013\n\u0013\n\u0013\n\u0012\n\u0012\n\u0012\np1 + p2\n2\n2\np0 + p3\n+2\nlog2\nlog2\nH(Z) = 2\n2\np0 + p3\n2\np1 + p2\n\u0013\n\u0013\n\u0012\n\u0012\n1\n1\n= 1 + (p0 + p3 ) log2\n+ (p1 + p2 ) log2\np0 + p3\np1 + p2\n= 1 + h2 (p1 + p2 )\n\n(13)\n\nwhere the last transition follows from the equality p0 + p1 + p2 + p3 = 1. We now derive an upper\nbound on the entropy H(Z). To this end, let\nZi = \u0398i + \u03a6i \u03b1,\n\ni = 1, 2, . . . , n\n\n(14)\n\nwhere \u0398 = (\u03981 , \u03982 , . . . , \u0398n ) and \u03a6 = (\u03a61 , \u03a62 , . . . , \u03a6n ) are random vectors over {0, 1}n . From the\ncomposition of Zi into a pair of two binary components (\u0398i , \u03a6i ), it follows from Fig. 1 that\nPr(\u03a6i = 0) = p1 + p2 ,\n\nPr(\u03a6i = 1) = p0 + p3 = 1 \u2212 (p1 + p2 ),\n\ni = 1, 2, . . . , n.\n\n(15)\n\nBased on (14) and (15), it is easy to verify the following chain of equalities:\nH(Z) = H(Z1 , Z2 , . . . , Zn )\n= H(\u03981 , \u03982 , . . . , \u0398n , \u03a61 , \u03a62 , . . . , \u03a6n )\n= H(\u03a61 , \u03a62 , . . . , \u03a6n ) + H(\u03981 , \u03982 , . . . , \u0398n | \u03a61 , \u03a62 , . . . , \u03a6n )\n= n h2 (p1 + p2 ) + H(\u03981 , \u03982 , . . . , \u0398n | \u03a61 , \u03a62 , . . . , \u03a6n )\n\n(16)\n\nwhere the last equality follows from (15) and since the degraded channel in Fig. 1 is memoryless.\nLet us define the syndrome at the output of the degraded channel as\nS , (\u03981 , \u03982 , . . . , \u0398n ) H T\nwhere H is a parity-check matrix of the binary linear block code C. We note that the calculation\nof the syndrome only takes into account the first components in the composition of the vector Z\nin (14). Note that the transmitted codeword x \u2208 C only affects the \u0398-components of the vector\nZ in (14), and also xH T = 0 for any such a codeword. Let us define L as the index of the vector\n(\u03981 , \u03982 , . . . , \u0398n ) in the coset referring to the syndrome S. Since each coset has exactly 2nR elements\nwhich are equally likely, then H(L) = nR, and\nH(\u03981 , \u03982 , . . . , \u0398n | \u03a61 , \u03a62 , . . . , \u03a6n ) = H(S, L | \u03a61 , \u03a62 , . . . , \u03a6n )\n\u2264 H(L) + H(S | \u03a61 , \u03a62 , . . . , \u03a6n )\n= nR + H(S | \u03a61 , \u03a62 , . . . , \u03a6n ).\n\n(17)\n\nConsidering a parity-check equation involving k variables, let {i1 , i2 , . . . , ik } be the set of indices\nof the variables involved in this parity-check equation. The relevant component of the syndrome S\nwhich refers to this parity-check equation is equal to zero or one if and only if the components of\nthe sub-vector (\u0398i1 , \u0398i2 , . . . , \u0398ik ) differ from the components of the sub-vector (Xi1 , Xi2 , . . . , Xik )\nin an even or odd number of indices, respectively. It is clear from Fig. 1 that for an index i for\n2\n; as a result of the\nwhich \u03a6i = 1, the random variables Xi and \u0398i are different in probability p1p+p\n2\nsymmetry of the channel, this probability is independent of the value of Xi . Similarly, for an index\n3\n, which again\ni for which \u03a6i = 0, the random variables Xi and \u0398i are different in probability p0p+p\n3\nis independent of the value of Xi .\n\n\fGiven that the Hamming weight of the vector (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik ) is t, then the probability that\nthe components of the two random vectors (\u0398i1 , \u0398i2 , . . . , \u0398ik ) and (Xi1 , Xi2 , . . . , Xik ) differ an even\nnumber of times is equal to\n\u0001\n\u0001\nq1 (t, k) q2 (t, k) + 1 \u2212 q1 (t, k) 1 \u2212 q2 (t, k)\n\nwhere q1 (t, k) designates the probability that among the t indices i for which \u03a6i = 1, the random\nvariables Xi and \u0398i differ an even number of times, and q2 (t, k) designates the probability that\namong the k \u2212 t indices i for which \u03a6i = 0, the random variables Xi and \u0398i differ an even number\nof times. Based on the discussion above, it follows that\n\u0011t\n\u0010\n2p\nX \u0012t\u0013 \u0012 p2 \u0013i 1 + 1 \u2212 p1 +p2 2\n=\nq1 (t, k) =\np1 + p2\n2\ni\ni even\n\u0010\n\u0011k\u2212t\n2p\nX \u0012k \u2212 t\u0013 \u0012 p3 \u0013i 1 + 1 \u2212 p0 +p3 3\n=\nq2 (t, k) =\np0 + p3\n2\ni\ni even\n\nso the probability that the two vectors (\u0398i1 , \u0398i2 , . . . , \u0398ik ) and (Xi1 , Xi2 , . . . , Xik ) differ in an even\nnumber of indices is\n\u0011t \u0010\n\u0011k\u2212t\n\u0010\n2p3\n2p2\n1\n\u2212\n1\n+\n1\n\u2212\n\u0001\n\u0001\np1 +p2\np0 +p3\n.\nq1 (t, k) q2 (t, k) + 1 \u2212 q1 (t, k) 1 \u2212 q2 (t, k) =\n2\nWe conclude that given a vector \u03a6 \u2208 {0, 1}k of Hamming weight t\n\uf8eb\n\u0010\n\u0011t \u0010\n\u0011k\u2212t \uf8f6\n2p2\n2p3\n1 \u2212 p0 +p3\n\u0001\n\uf8ec 1 + 1 \u2212 p1 +p2\n\uf8f7\nH Si | (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik ) = \u03a6 = h2 \uf8ed\n\uf8f8.\n2\n\nThis yields that if the calculation of a component Si (i = 1, . . . , n(1 \u2212 R)) in the syndrome S relies\non a parity-check equation involving k variables, then\nH(Si | \u03a61 , \u03a62 , . . . , \u03a6n ) = H(Si | \u03a6i1 , \u03a6i2 , . . . , \u03a6ik )\nX\n\u0001\n=\nPr (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik ) = \u03a6 * H (Si | (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik ) = \u03a6)\n\u03a6\u2208{0,1}k\n\n\uf8eb\n\u0010\nk \u0012 \u0013\n1\n+\n1\u2212\nX\nk\n\uf8ec\n=\n(p1 + p2 )t (p0 + p3 )k\u2212t h2 \uf8ed\nt\n\n2p2\np1 +p2\n\n=\n\nt=0\n\n\uf8eb\n\u0010\n\u0013\n1\n\u2212\n1\u2212\nk\n\uf8ec\n(p1 + p2 )t (p0 + p3 )k\u2212t h2 \uf8ed\nt\n\n1\u2212\n\n2p3\np0 +p3\n\n\u0011k\u2212t \uf8f6\n\n1\u2212\n\n2p3\np0 +p3\n\n\u0011k\u2212t \uf8f6\n\n2\n\nt=0\n\nk \u0012\nX\n\n\u0011t \u0010\n\n2p2\np1 +p2\n\n\u0011t \u0010\n2\n\n\uf8f7\n\uf8f8\n\uf8f7\n\uf8f8\n\nwhere the third equality turns to averaging over the Hamming weight of \u03a6 = (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik ), and\nthe last equality follows from the symmetry of the binary entropy function (where h2 (x) = h2 (1\u2212x)\nfor x \u2208 [0, 1]). Let dk designate the fraction of parity-check equations in the arbitrary parity-check\nmatrix which involve k variables, so their total number is n(1 \u2212 R)dk and\nH(S | \u03a61 , \u03a62 , . . . , \u03a6n )\nn(1\u2212R)\n\n\u2264\n\nX\n\nH(Si | \u03a61 , \u03a62 , . . . , \u03a6n )\n\ni=1\n\n= n(1 \u2212 R)\n\n\uf8f1\nX\uf8f2\nk\n\n\uf8f3\n\ndk\n\nk \u0012 \u0013\nX\nk\nt=0\n\nt\n\n\uf8eb\n\n(p1 + p2 )t (p0 + p3 )k\u2212t h2 \uf8ed\n\n1\u2212 1\u2212\n\n2p2 \u0001t\np1 +p2\n\n2\n\n1\u2212\n\n\uf8f6\uf8fc\n\n2p3 \u0001k\u2212t \uf8fd\np0 +p3\n\uf8f8\n\n\uf8fe\n\n.(18)\n\n\fBy combining (16)\u2013(18), an upper bound on the entropy of the random vector Z follows:\nH(Z) \u2264 nR + nh2 (p1 + p2 )\n\uf8f1\n\uf8eb\nk \u0012 \u0013\nX\uf8f2 X\n1\u2212 1\u2212\nk\n+n(1 \u2212 R)\ndk\n(p1 + p2 )t (p0 + p3 )k\u2212t h2 \uf8ed\n\uf8f3\nt\nt=0\nk\n\n2p2 \u0001t\np1 +p2\n\n2\n\n1\u2212\n\n\uf8f6\uf8fc\n\n2p3 \u0001k\u2212t \uf8fd\np0 +p3\n\uf8f8\n\n\uf8fe\n\n.(19)\n\nThe substitution of (13) and (19) in (12) finally provides the lower bound on the conditional entropy\nH(X | Y) in (7).\nThe following theorem tightens the lower bound on the parity-check density of an arbitrary\nsequence of binary linear block codes given in [13, Theorem 2.1]:\nTheorem 3.1 (\"Four-Level Quantization\" Lower Bound on the Asymptotic Parity-Check\nDensity of Binary Linear Block Codes). Let {Cm } be a sequence of binary linear block codes\nachieving a fraction 1 \u2212 \u03b5 of the capacity of an MBIOS channel with vanishing bit error probability.\nThen, the asymptotic density (\u2206m ) of their parity-check matrices satisfies\nlim inf \u2206m\nm\u2192\u221e\n\nK1 + K2 ln 1\u03b5\n>\n1\u2212\u03b5\n\n(20)\n\nwhere\nK1 = K2 ln\n\n\u0012\n\n1\n1\u2212C\n2 ln(2) C\n\n\u0013\n\n,\n\nK2 = \u2212\n\nC ln\n\n\u0010\n\n1\u2212C\n(p1 \u2212p2 )2\n(p1 +p2 )\n\n+\n\n(p0 \u2212p3 )2\np0 +p3\n\n\u0011\n\n(21)\n\nand p0 , p1 , p2 , p3 are defined in (6) in terms of l \u2208 R+ . The optimal value of l is given implicitly\nby the equation\np22 + e\u2212l p21\np23 + e\u2212l p20\n=\n(22)\n(p1 + p2 )2\n(p0 + p3 )2\nwhere such a solution always exists.1\nProof. Derivation of the lower bound in (20) and (21):\nLemma 3.1. Let C be a binary linear block code of length n and rate R. Let Pb designate\nthe average bit error probability of the code C which is associated with an arbitrary decoding\nalgorithm and channel, and let X and Y designate the transmitted codeword and received sequence,\nrespectively. Then\nH(X | Y)\n\u2264 R h2 (Pb ).\n(23)\nn\nProof. The lemma is proved in Appendix A.1.\nLemma 3.2. h2 (x) \u2264 1 \u2212\n\n2\nln 2\n\n( 21 \u2212 x)2 for 0 \u2264 x \u2264 1.\n\nProof. The lemma is proved in [13, Lemma 3.1].\n1\n\nIt was observed numerically that the solution l of the optimization equation (22) is unique when considering the\nbinary-input AWGN channel. We conjecture that the uniqueness of such a solution is a property which holds for\nMBIOS channels under some mild conditions.\n\n\fReferring to an arbitrary sequence of binary linear block codes {Cm } which achieves a fraction\n1 \u2212 \u03b5 to capacity with vanishing bit error probability, then according to Definition 2.1, there exists\na decoding algorithm (e.g., ML decoding) so that the average bit error probability of the code Cm\ntends to zero as m goes to infinity, and limm\u2192\u221e Rm = (1 \u2212 \u03b5)C. From Lemma 3.1, we obtain that\n| Ym )\n= 0 where Xm and Ym designate the transmitted codeword in the code Cm\nlimm\u2192\u221e H(Xm\nnm\nand the received sequence, respectively , and nm designates the block length of the code Cm . From\nProposition 3.1, we obtain\nH(Xm |Ym )\n\u2265 1 \u2212 C \u2212 (1 \u2212 Rm ) *\nnm\n\uf8f1\n\uf8eb\nk \u0012 \u0013\nX\uf8f2\nX\n1\u2212 1\u2212\nk\n*\ndk,m\n(p1 + p2 )t (p0 + p3 )k\u2212t h2 \uf8ed\n\uf8f3\nt\nt=0\n\nk\n\nBy letting m go to infinity, then\n\uf8f1\n\uf8eb\nk \u0012 \u0013\n1\u2212\n\u0001X\uf8f2 X\nk\n1 \u2212 C \u2212 1 \u2212 (1 \u2212 \u03b5)C\ndk\n(p1 + p2 )t (p0 + p3 )k\u2212t h2 \uf8ed\n\uf8f3\nt\nk\n\nt=0\n\n2p2 \u0001t\np1 +p2\n\n2\n\n1\u2212\n\n\uf8f6\uf8fc\n\n2p3 \u0001k\u2212t \uf8fd\np0 +p3\n\uf8f8\n\n\uf8f6\uf8fc\n\np1 \u2212p2 \u0001t p0 \u2212p3 \u0001k\u2212t \uf8fd\np1 +p2\np0 +p3\n\uf8f8\n\n2\n\n\uf8fe\n\n\uf8fe\n\n.\n\n\u22640\n\nand the upper bound on h2 (*) in Lemma 3.2 gives\n\u0001\n1 \u2212 C \u2212 1 \u2212 (1 \u2212 \u03b5)C *\n#)\n\"\n(\n\u0013 \u0012\n\u0013\n\u0012\nk \u0012 \u0013\nX\nX\np1 \u2212 p2 2t p0 \u2212 p3 2(k\u2212t)\n1\nk\nt\nk\u2212t\n\u2264 0. (24)\n*\n1\u2212\ndk\n(p1 + p2 ) (p0 + p3 )\n2 ln 2 p1 + p2\np0 + p3\nt\nk\n\nt=0\n\nSince p0 + p1 + p2 + p3 = 1 (i.e., the transition probabilities of the channel in Fig. 1 sum to 1),\nthen\n(\n\"\n\u00132t \u0012\n\u00132(k\u2212t) #)\n\u0012\nk \u0012 \u0013\nX\nX\nk\n1\np\n\u2212\np\np\n\u2212\np\n0\n3\n1\n2\ndk\n(p1 + p2 )t (p0 + p3 )k\u2212t 1 \u2212\nt\n2 ln 2 p1 + p2\np0 + p3\nt=0\nk\n( \"\n\u0013t \u0012\n\u0013k\u2212t #)\nk \u0012 \u0013\u0012\nX\n1 X k\n(p0 \u2212 p3 )2\n(p1 \u2212 p2 )2\n=\ndk 1 \u2212\n2 ln 2\np1 + p2\np0 + p3\nt\nt=0\nk\n(\n\u0013t \u0012\n\u0013k\u2212t )\nk \u0012 \u0013\u0012\nX\n1 X\nk\n(p1 \u2212 p2 )2\n(p0 \u2212 p3 )2\n= 1\u2212\ndk\nt\n2 ln 2\np1 + p2\np0 + p3\nt=0\nk\n( \u0012\n\u0013k )\n(p1 \u2212 p2 )2 (p0 \u2212 p3 )2\n1 X\n+\ndk\n= 1\u2212\n2 ln 2\np1 + p2\np0 + p3\nk\n\u0013a\n\u0012\n1\n(p1 \u2212 p2 )2 (p0 \u2212 p3 )2 R\n\u2264 1\u2212\n(25)\n+\n2 ln 2\np1 + p2\np0 + p3\nP\nwhere aR , k kdk designates the asymptotic average right degree of the bipartite graphs which\nrefer to the sequence of linear block codes {Cm }, and the last transition follows from Jensen's\ninequality. Combining (24) and (25) gives\n\u0013a \u0015\n\u0012\n\u0014\n\u0001\n(p1 \u2212 p2 )2 (p0 \u2212 p3 )2 R\n1\n+\n\u2264 0.\n1 \u2212 C \u2212 1 \u2212 (1 \u2212 \u03b5)C 1 \u2212\n2 ln 2\np1 + p2\np0 + p3\nThis yields the following lower bound on the asymptotic average right degree:\n\u0012 \u0013\n1\n\u2032\n\u2032\naR \u2265 K1 + K2 ln\n\u03b5\n\n(26)\n\n\fwhere\nK1\u2032\n\n=\n\n\u0001\n\n1 1\u2212C\n2 ln 2 C\n\u0011\n\u2212 \u0010\n2\n(p0 \u2212p3 )2\n2)\nln (pp11\u2212p\n+\n+p2\np0 +p3\n\nln\n\n,\n\nK2\u2032 = \u2212\n\nln\n\n\u0010\n\n1\n)2\n\n(p1 \u2212p2\np1 +p2\n\n+\n\n(p0 \u2212p3 )2\np0 +p3\n\n\u0011.\n\n(27)\n\nAccording to Definition 2.2, the density (\u2206) of a parity-check matrix is equal to the number of\nedges in the corresponding bipartite graph normalized per information bit, while the average right\ndegree (aR ) is equal to the same number of edges normalized per parity-check node. These different\nscalings of the number of the edges in a bipartite graph therefore imply that\n\u2206=\n\n1\u2212R\naR\nR\n\n(28)\n\nwhere R is the rate of a binary linear block code. By our assumption, the asymptotic rate of the\nsequence of code {Cm } is equal to a fraction 1 \u2212 \u03b5 of the capacity. Therefore, by combining (26) and\n(28) with R = (1 \u2212 \u03b5)C, we obtain a lower bound on the asymptotic parity-check density which is\nof the form\n\u0001\nK1 + K2 ln 1\u03b5\n1\u2212\u03b5\nwhere\n1\u2212C\n\u2032\n* K1,2\n(29)\nK1,2 =\nC\n\u2032\nand K1,2\nare introduced in (27). This completes the proof of the lower bound in (20) with the\ncoefficients K1,2 in (21).\nDerivation of the optimization equation (22): We refer the reader to Appendix A.2, where we\nalso show the existence of such a solution.\nDiscussion: It is required to show that we achieve an improved lower bound on the parity-check\ndensity, as compared to the one in [13, Theorem 2.1]. To this end, it suffices to show that\n(p1 \u2212 p2 )2 (p0 \u2212 p3 )2\n+\n\u2265 (1 \u2212 2w)2 .\np1 + p2\np0 + p3\n\n(30)\n\nFor a proof of this inequality, we refer the reader to Appendix A.3.\nThis therefore proves that the new lower bound is tighter (i.e., larger) than the original bound\nin [13, Theorem 2.1] (which corresponds to a two-level quantization of the LLR, as compared to\nthe new bound which is based on a four-level quantization of the LLR).\nBased on the proof of Theorem 3.1, we prove and discuss an upper bound on the asymptotic\nrate of every sequence of binary linear codes for which reliable communication is achievable. The\nbound refers to soft-decision ML decoding, and it is therefore valid for any suboptimal decoding\nalgorithm. Hence, the following result also provides an upper bound on the achievable rate of\nensembles of LDPC codes under iterative decoding where the transmission takes places over an\nMBIOS channel. The following bound improves the bounds stated in [1, Theorems 1 and 2]:\nCorollary 3.1 (\"Four-Level Quantization\" Upper Bound on the Asymptotic Achievable\nRates of Sequences of Binary Linear Block Codes). Let {Cm } be a sequence of binary linear\nblock codes whose codewords are transmitted with equal probability over an MBIOS channel, and\nsuppose that the block length of this sequences of codes tends to infinity as m \u2192 \u221e. Let dk,m be\nthe fraction of the parity-check nodes of degree k in an arbitrary representation of the code Cm\n\n\fby a bipartite graph.2 Then a necessary condition for this sequence to achieve vanishing bit error\nprobability as m \u2192 \u221e is that the asymptotic rate R of this sequence satisfies\n(\n1\u2212C\n\uf8fc,\n\uf8f1\nR \u2264 1 \u2212 max\n\u0012\n\u0013\n\u0012\n2p3 \u0001k\u2212t \u0013\uf8fd\n2p2 \u0001t\nk\nX\uf8f2 X k\n1\n\u2212\n1\n\u2212\n1\n\u2212\np1 +p2\np0 +p3\ndk\n(p1 + p2 )t (p0 + p3 )k\u2212t h2\n\uf8fe\n\uf8f3\n2\nt\nt=0\nk\n)\n2(p2 + p3 )\n(31)\n\u0011k\nX \u0010\n1\u2212\ndk 1 \u2212 2(p2 + p3 )\nk\n\nwhere p0 , p1 , p2 , p3 are introduced in (6), and dk and R are introduced in (1).\nProof. The first term in the RHS of (31) follows from (7) in Proposition 3.1 and (23) in Lemma 3.1.\nIt follows directly by combining both inequalities, and letting the bit error probability Pb go to\nzero. The second term in the RHS of (31) follows from the proof of [13, Corollary 3.1] which is\nbased on the erasure decomposition Lemma [11].\nConsidering ensembles of LDPC codes, we note that the fraction dk of nodes of degree k is\ncalculated in terms of the degree distribution \u03c1(*) by the equation\ndk = Z\n\n\u03c1k\nk\n1\n\n.\n\n(32)\n\n\u03c1(x) dx\n\n0\n\n3.2\n\nExtension of the Bounds to 2d Quantization Levels\n\nFollowing the method introduced in Section 3.1, we commence by deriving a lower bound on the\nconditional entropy of a transmitted codeword given the received sequence.\nProposition 3.2. Let C be a binary linear block code of length n and rate R. Let x = (x1 , . . . xn )\nand y = (y1 , . . . , yn ) designate the transmitted codeword and received sequence, respectively, when\nthe communication takes place over an MBIOS channel with conditional pdf pY |X (*|*). For an\narbitrary d \u2265 2 and 0 \u2264 l2d\u22121 \u22121 \u2264 . . . \u2264 l2 \u2264 l1 \u2264 l0 , \u221e, let us define the set of probabilities\nd \u22121\n{ps }2s=0\nas follows:\n\uf8f1\nPr{ls+1 < LLR(Y ) \u2264 ls | X = 0}\ns = 0, . . . , 2d\u22121 \u2212 2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 Pr{0 < LLR(Y ) \u2264 l2d\u22121 \u22121 | X = 0} + 1 Pr{LLR(Y ) = 0 | X = 0} s = 2d\u22121 \u2212 1\n2\nps ,\n(33)\n1\n\uf8f4\nPr{\u2212l\n\u2264\nLLR(Y\n)\n<\n0\n|\nX\n=\n0}\n+\nPr{LLR(Y ) = 0 | X = 0} s = 2d\u22121\n\uf8f4\n2d\u22121 \u22121\n2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3 Pr{\u2212l d\ns = 2d\u22121 + 1, . . . , 2d \u2212 1.\n2 \u2212(s+1) \u2264 LLR(Y ) < \u2212l2d \u2212s | X = 0}\n\nFor an arbitrary parity-check matrix of the code C, let dk designate the fraction of the parity-checks\ninvolving k variables. Then, the conditional entropy of the transmitted codeword given the received\n2\n\nFor a sequence of ensembles of binary linear codes {Cm }, we denote by dk,m the probability of picking (with\nuniform distribution) a parity-check node of degree k from a bipartite graph which represents the code Cm .\n\n\fsequence satisfies\n(\nX\nH(X|Y)\n\u2265 1 \u2212 C \u2212 (1 \u2212 R)\ndk\nn\nk\n\n2d\u22121\nY\u22121\n\nX\n\nk0 ,...,k2d\u22121 \u22121\nP\ni ki =k\n\n\u0012\n\n\u0013\nk\n*\nk0 , . . . , k2d\u22121 \u22121\n\n\uf8f9\uf8f6 )\n\uf8eb \uf8ee\n\u0013k i\n2d\u22121\n\u22121 \u0012\nY\n2p2d \u22121\u2212i\n1\n\uf8fb\uf8f8 .\n1\u2212\nh2 \uf8ed \uf8f01 \u2212\n2\npi + p2d \u22121\u2212i\n\n(pi + p2d \u22121\u2212i )ki\n\n(34)\n\ni=0\n\ni=0\n\nProof. Following the proof of Proposition 3.1, we introduce a new physically degraded channel. It\nis a memoryless binary-input 2d -output symmetric channel (see Fig. 1 for d = 2). To this end, let\nl1 \u2265 l2 \u2265 . . . \u2265 l2d\u22121 \u22121 \u2208 R+ be arbitrary non-negative numbers, and denote l0 , \u221e. The output\nalphabet of the degraded channel is defined to be GF(2d ) whose elements form the set\n\uf8fc\n\uf8f1\nd\u22121\n\uf8fd\n\uf8f2X\naj \u03b1j s.t (a0 , a1 , . . . , ad\u22121 ) \u2208 {0, 1}d .\n\uf8fe\n\uf8f3\nj=0\n\n(s)\n\n(s)\n\n(s)\n\nFor s = 0, 1, . . . , 2d\u22121 \u22121 , let us denote the d\u22121-bit binary representation of s by (a1 , a2 , . . . , ad\u22121 )\ni.e.\ns=\n\nd\u22121\nX\n\n(s)\n\naj 2j\u22121 .\n\nj=1\n\nLet Xi and Yi designate the random variables referring to the input and output of the original\nchannel pY |X (*|*) at time i (where i = 1, 2, . . . , n). As a natural generalization of the channel model\nin Fig. 1, we introduce a channel with 2d quantization levels of the LLR. The output of the degraded\nchannel at time i, zi , is calculated from the output yi of the original channel as follows:\n\u2022 If ls+1 < LLR(yi ) \u2264 ls for some 0 \u2264 s < 2d\u22121 \u2212 1, then zi =\n\u2022 If 0 < LLR(yi ) \u2264 l2d\u22121 \u22121 , then zi =\n\nPd\u22121\n\n\u03b1j .\n\nj=1\n\n\u2022 If \u2212l2d\u22121 \u22121 \u2264 LLR(yi ) < 0, then zi = 1 +\n\nPd\u22121\nj=1\n\nPd\u22121\nj=1\n\n(s)\n\naj \u03b1j .\n\n\u03b1j .\n\n\u2022 If \u2212ls \u2264 LLR(yi ) < \u2212ls+1 for some 0 \u2264 s < 2d\u22121 \u2212 1, then zi = 1 +\n\u2022 If LLR(yi ) = 0, then zi is chosen as\n\nPd\u22121\nj=1\n\n\u03b1j or 1 +\n\nPd\u22121\nj=1\n\nPd\u22121\nj=1\n\n(s)\n\naj \u03b1j .\n\n\u03b1j with equal probability ( 12 ).\n\nFrom (33), the transition probabilities of the degraded channel are expressed in an equivalent way\nby\nps = Pr(Z =\n\nd\u22121\nX\n\n(s)\n\naj \u03b1j | X = 0) = Pr(Z = 1 +\n\n(s)\n\naj \u03b1j | X = 1)\n\nj=1\n\nj=1\n\np2d \u22121\u2212s = Pr(Z = 1 +\n\nd\u22121\nX\n\nd\u22121\nX\nj=1\n\n(s)\n\naj \u03b1j | X = 0) = Pr(Z =\n\nd\u22121\nX\n\n(s)\n\naj \u03b1j | X = 1)\n\n(35)\n\nj=1\n\nwhere s = 0, 1, . . . , 2d\u22121 \u2212 1. The symmetry in these equalities holds since the channel is MBIOS.\n\n\fEquations (8)-(12) hold also for the case of 2d -level quantization. Thus, we will calculate the\nentropy of the random variable Z, and an upper bound on the entropy of the random vector Z.\nThis will finally provide the lower bound in (34).\nSince X is equally likely to be zero or one, the output of the degraded channel, which is\nsymmetric, satisfies the following probability law for s = 0, 1, . . . , 2d\u22121 \u2212 1:\nPr(Z = 1 +\n\nd\u22121\nX\n\n(s)\n\naj \u03b1j ) = Pr(Z =\n\nd\u22121\nX\n\n(s)\n\naj \u03b1j )\n\nj=1\n\nj=1\n\n= Pr(Z =\n\nd\u22121\nX\n\n(s)\n\naj \u03b1j | X = 0) Pr(X = 0) + Pr(Z =\n\nd\u22121\nX\n\n(s)\n\naj \u03b1j | X = 1) Pr(X = 1)\n\nj=1\n\nj=1\n\nps + p2d \u22121\u2212s\n.\n2\nThe entropy of Z is therefore\n=\n\nH(Z) = 2\n\n2d\u22121\nX\u22121\ns=0\n\n= 1+\n\nps + p2d \u22121\u2212s\nlog2\n2\n\n2d\u22121\nX\u22121\n\n\u0012\n\n2\nps + p2d \u22121\u2212s\n\n\u0001\n\nps + p2d \u22121\u2212s log2\n\ns=0\n\nwhere the last transition follows from the equality\n\nP2d \u22121\ns=0\n\n\u0012\n\n\u0013\n\n1\nps + p2d \u22121\u2212s\n\n\u0013\n\n(36)\n\nps = 1.\n\nWe now derive an upper bound on the entropy of the random vector Z. To this end, let\nZi = \u0398i +\n\nd\u22121\nX\n\n\u03a6i,j \u03b1j ,\n\ni = 1, 2, . . . , n.\n\n(37)\n\nj=1\n\nDenoting \u03a6i = (\u03a6i,1 , \u03a6i,2 , . . . , \u03a6i,d\u22121 ), we have that \u0398 = (\u03981 , \u03982 , . . . , \u0398n ) is a random vector over\n{0, 1}n , and \u03a6 = (\u03a61 , \u03a62 , . . . , \u03a6n ) is a random vector over {0, 1}(d\u22121)n . From the decomposition\nof Zi in (37), it follows from (35) that\n\u0011\n\u0010\n(s)\n(s)\ni = 1, 2, . . . , n, s = 0, 1, . . . , 2d\u22121 \u2212 1.\n(38)\nPr \u03a6i = (a1 , . . . , ad\u22121 ) = ps + p2d \u22121\u2212s ,\nFrom (37) and (38), and the same chain of equalities leading to (16), it follows that\nH(Z) = n\n\n2d\u22121\nX\u22121\ns=0\n\n\u0001\n\nps + p2d \u22121\u2212s log2\n\n\u0012\n\n1\nps + p2d \u22121\u2212s\n\n\u0013\n\n+H(\u03981 , \u03982 , . . . , \u0398n | \u03a61 , \u03a62 , . . . , \u03a6n ).\n\n(39)\n\nAs in the proof of Proposition 3.1, we define the syndrome as S = \u0398H T where H is a parity-check\nmatrix of the code C. As before, the transmitted codeword x \u2208 C only affects the \u0398-components of\nthe vector Z in (37). In parallel to (17), we obtain\nH(\u03981 , \u03982 , . . . , \u0398n | \u03a61 , \u03a62 , . . . , \u03a6n ) \u2264 nR + H(S | \u03a61 , \u03a62 , . . . , \u03a6n ).\n\n(40)\n\nConsidering a parity-check equation which involves k variables, let {i1 , i2 , . . . , ik } be the set of\nindices of the variables involved in this parity-check equation. The component of the syndrome\nS which refers to this parity-check equation is zero if and only if the components of the subvector (\u0398i1 , \u0398i2 , . . . , \u0398ik ) differ from the components of the sub-vector (Xi1 , Xi2 , . . . , Xik ) in an\n(s) (s)\n(s)\neven number of indices. It is clear from (35) that for an index i, where \u03a6i = (a1 , a2 , . . . , ad\u22121 )\np2d \u22121\u2212s\n.\nfor some s = 0, . . . , 2d\u22121 \u2212 1, the random variables Xi and \u0398i are different in probability ps +p\nd\n2 \u22121\u2212s\n\n\f(s)\n\n(s)\n\n(s)\n\nLemma 3.3. Given that (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik ) has ks elements of the type (a1 , a2 , . . . , ad\u22121 ) (where\ns = 0, . . . , 2d\u22121 \u22121), the probability that the components of (\u0398i1 , \u0398i2 , . . . , \u0398ik ) and (Xi1 , Xi2 , . . . , Xik )\ndiffer in an even number of indices is equal to\n\uf8f9\n\uf8ee\n\u0013k s\n2d\u22121\nY\u22121 \u0012\n2p2d \u22121\u2212s\n1 \uf8f0\n\uf8fb.\n1+\n1\u2212\n2\nps + p2d \u22121\u2212s\ns=0\n\nProof. The lemma is proved in Appendix B.1.\nBased on Lemma 3.3 and the discussion above, it follows that given that the vector (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik )\n(s) (s)\n(s)\nhas ks elements of the type (a1 , a2 , . . . , ad\u22121 ) then\n\uf8eb\n\nH(Si | \u03a6i1 , \u03a6i2 , . . . , \u03a6ik ) = h2 \uf8ed\n\n\uf8ee\n\n1 \uf8f0\n1\u2212\n2\n\n2d\u22121\nY\u22121 \u0012\ns=0\n\n2p2d \u22121\u2212s\n1\u2212\nps + p2d \u22121\u2212s\n\n\u0013k s\n\n\uf8f9\uf8f6\n\n\uf8fb\uf8f8 .\n\nFor a component Si (1 \u2264 i \u2264 n(1 \u2212 R)) of the syndrome S which refers to a parity-check equation\ninvolving k variables\nH(Si | \u03a61 , \u03a62 , . . . , \u03a6n )\n= H(Si | \u03a6i1 , \u03a6i2 , . . . , \u03a6ik )\nX\n\u0001\nPr (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik ) = \u03a6 H (Si | (\u03a6i1 , \u03a6i2 , . . . , \u03a6ik ) = \u03a6)\n=\n\u03a6\u2208{0,1}(d\u22121)k\n\nX\n\n=\n\nk0 ,...,k2d\u22121 \u22121\nP\ns ks =k\n\n\u0012\n\nk\nk0 , . . . , k2d\u22121 \u22121\n\n\u0013 2d\u22121\nY\u22121\n(ps + p2d \u22121\u2212s )ks\ns=0\n\n\uf8eb \uf8ee\n\uf8f9\uf8f6\n\u0013k s\n2d\u22121\nY\u22121 \u0012\n2p2d \u22121\u2212s\n1\n\uf8fb\uf8f8 .\n*h2 \uf8ed \uf8f01 \u2212\n1\u2212\n2\nps + p2d \u22121\u2212s\ns=0\n\nThe number of parity-check equations involving k variables is n(1 \u2212 R)dk , hence\nH(S | \u03a61 , \u03a62 , . . . , \u03a6n )\nn(1\u2212R)\n\n\u2264\n\nX\n\nH(Si | \u03a61 , \u03a62 , . . . , \u03a6n )\n\ni=1\n\n= n(1 \u2212 R)\n\nX\nk\n\n(\n\ndk\n\nX\n\nk0 ,...,k2d\u22121 \u22121\nP\ns ks =k\n\n\u0012\n\nk\nk0 , . . . , k2d\u22121 \u22121\n\n\u0013 2d\u22121\nY\u22121\n(ps + p2d \u22121\u2212s )ks\ns=0\n\n\uf8eb \uf8ee\n\uf8f9\uf8f6 )\n\u0013k s\n2d\u22121\nY\u22121 \u0012\n2p\n1\nd\n2 \u22121\u2212s\n\uf8fb\uf8f8 .\nh2 \uf8ed \uf8f01 \u2212\n1\u2212\n2\np\n+\np\nd \u22121\u2212s\ns\n2\ns=0\n\n(41)\n\n\fBy combining (39)\u2013(41), an upper bound on the entropy of the random vector Z follows:\nH(Z) \u2264 nR + n\n\n2d\u22121\nX\u22121\n\n\u0001\n\nps + p2d \u22121\u2212s log2\n\ns=0\n\n+n(1 \u2212 R)\n\nX\nk\n\n(\n\nX\n\ndk\n\nk0 ,...,k2d\u22121 \u22121\nP\ns ks =k\n\n\u0012\n\n\u0012\n\n1\nps + p2d \u22121\u2212s\n\nk\nk0 , . . . , k2d\u22121 \u22121\n\n\u0013\n\n\u0013 2d\u22121\nY\u22121\n(ps + p2d \u22121\u2212s )ks\ns=0\n\n\uf8f9\uf8f6 )\n\uf8eb \uf8ee\n\u0013k s\n2d\u22121\nY\u22121 \u0012\n2p2d \u22121\u2212s\n1\n\uf8fb\uf8f8 .\n1\u2212\nh2 \uf8ed \uf8f01 \u2212\n2\nps + p2d \u22121\u2212s\n\n(42)\n\ns=0\n\nThe substitution of (36) and (42) in (12) finally provides the lower bound on the conditional entropy\nH(X | Y) in (34).\nDiscussion: The calculation of the lower bound in the RHS of (34) becomes more complex\nas the value of d is increased. However, for optimally chosen quantization levels, we show that\nas the value of d is increased (thus, increasing the number of quantization levels), this bound is\n(d)\n(d)\nmonotonically increasing. To this end, let d \u2265 2 be an arbitrary integer, and let (l1 , . . . , l2d\u22121 \u22121 )\nand their symmetric values around zero denote the optimal choice for a 2d -level quantization. Let\n(d) (d)\n(d)\np0 , p1 , . . . , p2d \u22121 denote the transition probabilities, as defined in (33), which are associated with\nthe optimal 2d quantization levels. In order to show the above monotonicity property, we prove in\nAppendix B.2 that there exist sub-optimal 2d+1 quantization levels  \u0303l1 ,  \u0303l2 , . . . ,  \u0303l2d \u22121 (together with\ntheir symmetric values around zero) so that even with this sub-optimal 2d+1 -level quantization,\nthe bound in the RHS of (34) is already better than the one which is calculated from the optimal\nchoice of a 2d -level quantization.\nTheorem 3.2 (\"2d -Level Quantization\" Lower Bound on the Asymptotic Parity-Check\nDensity of Binary Linear Block Codes). Let {Cm } be a sequence of binary linear block codes\nachieving a fraction 1 \u2212 \u03b5 of the capacity of an MBIOS channel with vanishing bit error probability.\nThen, the asymptotic density of their parity-check matrices satisfies\nlim inf \u2206m >\nm\u2192\u221e\n\nK1 + K2 ln 1\u03b5\n1\u2212\u03b5\n\n(43)\n\nwhere\nK1 = K2 ln\n\n\u0012\n\n1\n1\u2212C\n2 ln(2) C\n\n\u0013\n\n,\n\nK2 = \u2212\n\n\uf8eb\n\n1\u2212C\n\n\uf8f6.\nX (pi \u2212 p2d \u22121\u2212i )2\n\uf8f8\nC ln \uf8ed\npi + p2d \u22121\u2212i\n2d\u22121 \u22121\n\n(44)\n\ni=0\n\nHere, d \u2265 2 is an arbitrary integer and the probabilities {pi } are introduced in (33) in terms\nof l1 \u2265 . . . \u2265 l2d\u22121 \u22121 \u2208 R+ . The optimal vector of quantization levels (l1 , . . . , l2d\u22121 \u22121 ) is given\nimplicitly by the set of 2d\u22121 \u2212 1 equations\np22d \u22121\u2212i + e\u2212li p2i\n(pi + p2d \u22121\u2212i )2\n\n=\n\nwhere such a solution always exists.3\n3\n\nSee the footnote to Theorem 3.1 in p. 10.\n\np22d \u2212i + e\u2212li p2i\u22121\n(pi\u22121 + p2d \u2212i )2\n\n,\n\ni = 1, . . . , 2d\u22121 \u2212 1.\n\n(45)\n\n\fProof. For an arbitrary sequence of binary linear block codes {Cm } which achieves a fraction 1\u2212\u03b5 to\n| Ym )\n=0\ncapacity with vanishing bit error probability, we get from Lemma 3.1 that limm\u2192\u221e H(Xm\nnm\nwhere Xm and Ym designate the transmitted codeword in the code Cm and the received sequence,\nrespectively, and nm designates the block length of the code Cm . From Proposition 3.2, we obtain\nH(Xm |Ym )\n\u2265 1 \u2212 C \u2212 (1 \u2212 Rm ) *\nnm\n(\nX\nX\n*\ndk,m\nk\n\nk0 ,...,k2d\u22121 \u22121\nP\ns ks =k\n\n\u0012\n\nk\nk0 , . . . , k2d\u22121 \u22121\n\n\u0013 2d\u22121\nY\u22121\n(ps + p2d \u22121\u2212s )ks\ns=0\n\n\uf8f9\uf8f6 )\n\uf8eb \uf8ee\n\u0013k s\n2d\u22121\nY\u22121 \u0012\n2p\nd\n1\n2 \u22121\u2212s\n\uf8fb\uf8f8 .\n1\u2212\nh2 \uf8ed \uf8f01 \u2212\n2\nps + p2d \u22121\u2212s\ns=0\n\nSimilarly to the proof of Theorem 3.1, by letting m tend to infinity and using the upper bound on\nh2 (*) from Lemma 3.2, we get\n1 \u2212 C \u2212 1 \u2212 (1 \u2212 \u03b5)C\n\n\u0001X\nk\n\n(\n\ndk\n\nk0 ,...,k2d\u22121 \u22121\nP\ns ks =k\n\n\uf8ee\n\nSince\n\nP\n\nk\n\ndk = 1 and\n\nP2d \u22121\ns=0\n\nX\n\n\uf8ef\n\uf8f01 \u2212\n\n\u0012\n\n\uf8eb\n\n1 \uf8ed\n2 ln 2\n\nk\nk0 , . . . , k2d\u22121 \u22121\n\n2d\u22121\nY\u22121\ns=0\n\n\u0013 2d\u22121\nY\u22121\n(ps + p2d \u22121\u2212s )ks\ns=0\n\n\uf8f62ks \uf8f9 )\nps \u2212 p2d \u22121\u2212s \uf8f8 \uf8fa\n\uf8fb \u2264 0.\nps + p2d \u22121\u2212s\n\nps = 1, the sum in the LHS of (46) is equal to\n\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\nX\n1\nd\n1\u2212\n\uf8f4 k\n2 ln 2\nk \uf8f4\n\uf8f4\n\uf8f3\n\n\uf8fc\n\uf8f4\n\uf8f4\n\u0012\n\u0013 2d\u22121\n\u0013k s \uf8f4\n\u22121 \u0012\n\uf8fd\n2\nX\nY\n(ps \u2212 p2d \u22121\u2212s )\nk\n\uf8f4\nk0 , . . . , k2d\u22121 \u22121\nps + p2d \u22121\u2212s\n\uf8f4\ns=0\nk0 ,...,k2d\u22121 \u22121\n\uf8f4\n\uf8fe\nP\ns\n\nks =k\n\n\uf8f1 \uf8eb\n\uf8f6k \uf8fc\n\uf8f4\n\uf8f4\n2d\u22121\n\u22121\n\uf8f2\n2\nX\nX\n(ps \u2212 p2d \u22121\u2212s ) \uf8f8 \uf8fd\n1\n\uf8ed\n=1\u2212\nd\n\uf8f4 k\n\uf8f4\n2 ln 2\nps + p2d \u22121\u2212s\n\uf8fe\ns=0\nk \uf8f3\n\u22641\u2212\n\n\uf8eb\n\n1 \uf8ed\n2 ln 2\n\n(46)\n\n2d\u22121\nX\u22121\ns=0\n\n\uf8f6aR\n(ps \u2212 p2d \u22121\u2212s )2\n\uf8f8\nps + p2d \u22121\u2212s\n\n(47)\n\nP\nwhere aR , k kdk designates the asymptotic average right degree, and the last transition follows\nfrom Jensen's inequality. Combining (46) and (47) gives\n\uf8eb\n\uf8f6aR \uf8f9\n\uf8ee\n2d\u22121\n\u22121\n2\nX\n\u0001\n(ps \u2212 p2d \u22121\u2212s )\n1 \uf8ed\n\uf8f8 \uf8fb \u2264 0.\n1 \u2212 C \u2212 1 \u2212 (1 \u2212 \u03b5)C \uf8f01 \u2212\n2 ln 2\nps + p2d \u22121\u2212s\ns=0\n\nThis yields the following lower bound on the asymptotic average right degree:\n\u0012 \u0013\n1\naR \u2265 K1\u2032 + K2\u2032 ln\n\u03b5\n\n(48)\n\n\fwhere\nK1\u2032\n\n=\u2212\n\n\uf8eb\n\nln\n\n1 1\u2212C\n2 ln 2 C\n\n2d\u22121 \u22121\n\n\u0001\n\n\uf8f6,\n\nX (ps \u2212 p2d \u22121\u2212s )2\n\uf8f8\nln \uf8ed\nps + p2d \u22121\u2212s\ns=0\n\nK2\u2032 = \u2212\n\n\uf8eb\n\n1\n\n\uf8f6.\nX (ps \u2212 p2d \u22121\u2212s )2\n\uf8f8\nln \uf8ed\nps + p2d \u22121\u2212s\n2d\u22121 \u22121\ns=0\n\nBy combining (28) and (48) with the asymptotic rate R = (1 \u2212 \u03b5)C, we obtain a lower bound on\nthe asymptotic parity-check density which is of the form\n\u0001\nK1 + K2 ln 1\u03b5\n1\u2212\u03b5\nwhere\n1\u2212C\n\u2032\n* K1,2\n.\nK1,2 =\nC\nThis completes the proof of the lower bound in (43) and (44). The derivation of the set of optimization equations in (45) follows along the lines of the derivation of (22). In the general case of\n2d quantization levels, it follows from (44) that we need to maximize\n2d\u22121\nX\u22121\ns=0\n\n(ps \u2212 p2d \u22121\u2212s )2\n.\nps + p2d \u22121\u2212s\n\nTo this end, we set to zero all the partial derivatives w.r.t. ls where s = 1, . . . , 2d\u22121 \u2212 1. Since from\n(33) only ps , ps\u22121 , p2d \u2212s and p2d \u2212s\u22121 depend on ls , then\n\u001b\n\u001a\n(ps\u22121 \u2212 p2d \u2212s )2 (ps \u2212 p2d \u2212s\u22121 )2\n\u2202\n+\n= 0.\n\u2202ls\nps\u22121 + p2d \u2212s\nps + p2d \u2212s\u22121\n\nWe express now the probabilities ps , ps\u22121 , p2d \u2212s and p2d \u2212s\u22121 as integrals of the conditional pdf\na(*) of the LLR, and rely on the symmetry property where a(l) = el a(\u2212l) for l \u2208 R. In a similar\nmanner to the derivation of (22), this gives the set of equations in (45). Their solution provides\nthe quantization levels l1 , . . . , l2d\u22121 \u22121 (where according to Proposition 3.2, the other 2d\u22121 \u2212 1 levels\nare set to be symmetric w.r.t. zero).\nBased on the proof of Theorem 3.2, we derive an upper bound on the asymptotic rate of every\nsequence of binary linear codes for which reliable communication is achievable. The bound refers\nof soft-decision ML decoding, and it is therefore valid for any sub-optimal decoding algorithm.\nCorollary 3.2 (\"2d -Level Quantization\" Upper Bound on the Asymptotic Achievable\nRates of Sequences of Binary Linear Block Codes). Let {Cm } be a sequence of binary linear\nblock codes whose codewords are transmitted with equal probability over an MBIOS channel, and\nsuppose that the block length of this sequences of codes tends to infinity as m \u2192 \u221e. Let dk,m be\nthe fraction of the parity-check nodes of degree k in an arbitrary representation of the code Cm\nby a bipartite graph. Then a necessary condition for this sequence to achieve vanishing bit error\nprobability as m \u2192 \u221e is that the asymptotic rate R of this sequence satisfies\n(\n(\n\u0013 2d\u22121\n\u0012\nY\u22121\nX\nX\nk\n(pi + p2d \u22121\u2212i )ki\ndk\n*\nR \u2264 1 \u2212 max (1 \u2212 C)\nk0 , . . . , k2d\u22121 \u22121\nk\n\n\uf8f9\uf8f6 )\n\uf8eb \uf8ee\n\u22121\n\u0013k i\n2d\u22121\n\u22121 \u0012\nY\n2p2d \u22121\u2212i\n1\uf8f0\n\uf8fb\n\uf8f8\n\uf8ed\n,\n1\u2212\n1\u2212\nh2\n2\npi + p2d \u22121\u2212i\ni=0\n\ni=0\n\nk0 ,...,k2d\u22121 \u22121\nP\n* i ki =k\n\n2\n\nd \u22121\n2X\n\npi\n\ni=2d\u22121\n\n1\u2212\n\nX\nk\n\nd \u22121\n2X\n\u0011k\n\u0010\npi\ndk 1 \u2212 2\n\ni=2d\u22121\n\n)\n\n(49)\n\n\fwhere d \u2265 2 is arbitrary, the probabilities {pi } are introduced in (33), and dk and R are introduced\nin (1).\nProof. The concept of the proof is the same as the proof of Corollary 3.1, except that the first term\nin the RHS of (49) relies on (34).\n\n4\n\nApproach II: Bounds without Quantization of the LLR\n\nSimilarly to the previous section, we derive bounds on the asymptotic achievable rate and the\nasymptotic parity-check density of an arbitrary sequence of binary, linear block codes transmitted\nover an MBIOS channel. As in Section 3, the derivation of these two bounds is based on a lower\nbound on the conditional entropy of a transmitted codeword given the received sequence at the\noutput of an arbitrary MBIOS channel.\nProposition 4.1. Let C be a binary linear code of length n and rate R transmitted over an\nMBIOS channel. Let x = (x1 , x2 , . . . , xn ) and y = (y1 , y2 , . . . , yn ) be the transmitted codeword and\nthe received sequence, respectively. For an arbitrary representation of the code C by a parity-check\nmatrix, let dk designate the fraction of the parity-check equations of degree k. Then the conditional\nentropy of the transmitted codeword given the received sequence satisfies\n\u0012 \u0013 \u0013k !\n\u221e\nX \u0012Z \u221e\nl\n1 X\n1\nH(X|Y)\n\u2212l\n2p\na(l)(1 + e ) tanh\ndk\n\u2265 1 \u2212 C \u2212 (1 \u2212 R) 1 \u2212\ndl\nn\n2 ln(2)\np(2p \u2212 1)\n2\n0\np=1\n\nk\n\n(50)\n\nwhere a(*) denotes the conditional pdf of the LLR given that the transmitted symbol is zero.\nProof. We consider a binary linear block code C of length n and rate R whose transmission takes\nplace over an MBIOS channel. Let us assume that x \u2208 C is the transmitted codeword, and the\n'0' and '1' symbols of the codeword are mapped to +1 and \u22121, respectively. The input alphabet\nto the channel is {+1, \u22121}, and the LLR as a function of the observation y at the output of the\nMBIOS channel gets the form\n\u0013\n\u0012\npY |X (y|X = 1)\n, y \u2208 R.\nLLR(y) = ln\npY |X (y|X = \u22121)\nFor the continuation of the proof, we move from the mapping of the MBIOS channel X \u2192 Y\nto an equivalent representation of the channel X \u2192 Ye , so that the conditional entropies of the\ntransmitted codeword given the received sequences at the outputs of both channels are equal, i.e.,\nH(X|Y ) = H(X|Ye ). The basic idea for showing the equivalence between the original channel and\nthe one which will be introduced shortly is based on the following two facts: Firstly, the LLR forms\na sufficient statistics of the channel, and secondly, denoting the output of an MBIOS channel by\ny then, from the symmetry of the channel, LLR(\u2212y) = \u2212LLR(y). This means that the absolute\nvalue of the LLR doesn't change when the channel output is flipped, but then the sign of the LLR\nalternates.\nFor the characterization of the equivalent channel, let a(*) designate the conditional pdf of\nthe LLR given that the transmitted symbol is 0 (i.e., given that the channel input is 1). For\ni = 1, 2, . . . , n, we randomly generate an i.i.d. sequence {li }ni=1 w.r.t. the conditional pdf a(*), and\n\n\fdefine\n\u03c9i , |li |,\n\n\uf8f1\n\uf8f4\n\uf8f2 +1\n\u22121\n\u03b8i ,\n\uf8f4\n\uf8f3\n\u00b11 w.p.\n\nif li > 0\n1\n2\n\nif li < 0\nif li = 0\n\n.\n\ne = (e\nThe output of the equivalent channel is defined to be the sequence y\ny1 , . . . , yen ) where\nyei = (\u03c6i , \u03c9i ),\n\ni = 1, 2, . . . , n\n\nand \u03c6i = \u03b8i xi . The output of this equivalent channel at time i is therefore the pair (\u03c6i , \u03c9i ) where\n\u03c6i \u2208 {+1, \u22121} and \u03c9i \u2208 R+ . This defines the memoryless mapping\nX \u2192 Ye , (\u03a6, \u03a9)\n\nwhere \u03a6 is a binary random variable which is affected by X, and \u03a9 is a non-negative random\nvariable which represents the absolute value of the LLR and whose pdf is\n(\na(\u03c9) + a(\u2212\u03c9) = (1 + e\u2212\u03c9 ) a(\u03c9) if \u03c9 > 0\n.\n(51)\nf\u03a9 (\u03c9) =\na(0)\nif \u03c9 = 0\nWe note that the transition in case \u03c9 > 0 follows from the symmetry property of a(*), and the\nrandom variable \u03a9 is clearly statistically independent of X. Following the lines which lead to (12),\nwe obtain\ne + nH(Ye ) \u2212 nC.\nH(X|Y) \u2265 nR \u2212 H(Y)\n(52)\n\nIn order to get a lower bound on H(X|Y), we will calculate exactly the entropy of Ye and obtain\ne The calculation of the first entropy is direct\nan upper bound on the entropy of Y.\nH(Ye ) = H(\u03a6, \u03a9)\n\n= H(\u03a9) + H(\u03a6|\u03a9)\n\n= H(\u03a9) + E\u03c9 [H(\u03a6|\u03a9 = \u03c9)]\n\n= H(\u03a9) + 1\n\n(53)\n\nwhere the last transition is due to the fact that given the absolute value of the LLR, its sign is\nequally likely to be positive or negative. We note that H(\u03a9) is not expressed explicitly as it will\ncancel out later.\ne\nWe will now derive an upper bound on H(Y).\n\n\u0001\ne = H (\u03a61 , . . . , \u03a6n ), (\u03a91 , . . . , \u03a9n )\nH(Y)\n\n\u0001\n= H(\u03a91 , . . . , \u03a9n ) + H (\u03a61 , . . . , \u03a6n ), | (\u03a91 , . . . , \u03a9n )\n\u0001\n= nH(\u03a9) + H (\u03a61 , . . . , \u03a6n ), | (\u03a91 , . . . , \u03a9n ) .\n\n(54)\n\nLet us introduce the assignment f : {+1, \u22121} \u2192 {0, 1} where +1 and \u22121 are mapped back to 0\ne i = f (\u03a6i ). Since \u03a6i = \u0398i Xi , then we obtain\nand 1, respectively, and define \u03a6\nei = \u0398\nei + X\nei ,\n\u03a6\n\ni = 1, . . . , n\n\nwhere the last addition is modulo-2. Define the syndrome vector\ne 1, . . . , \u03a6\ne n )H T\nS = (\u03a6\n\n\fwhere H is an arbitrary parity-check matrix of the binary linear block code C, and let L be the\ne 1, . . . , \u03a6\ne n ) in the coset which corresponds to S. Since each coset has exactly\nindex of the vector (\u03a6\nnR\n2 elements which are equally likely then H(L) = nR, and we get\n\u0001\n\u0001\nH (\u03a61 , . . . , \u03a6n ), | (\u03a91 , . . . , \u03a9n ) = H(S, L | (\u03a91 , . . . , \u03a9n )\n\u0001\n\u2264 H(L) + H S | (\u03a91 , . . . , \u03a9n )\n\u0001\n= nR + H S | (\u03a91 , . . . , \u03a9n )\nn(1\u2212R)\n\n\u2264 nR +\n\nX\nj=1\n\n\u0001\nH Sj | (\u03a91 , . . . , \u03a9n )\n\n(55)\n\nSince (e\nx1 , . . . , x\nen ) is the original transmitted codeword in C (i.e., before the conversion of the\nsymbols of the transmitted codeword to \u00b11), then\ne 1, . . . , \u03a6\ne n )H T\nS = (\u03a6\ne 1, . . . , \u0398\ne n )H T + (X\ne1 , . . . , X\nen )H T\n= (\u0398\ne 1, . . . , \u0398\ne n )H T .\n= (\u0398\n\nLet us look at the j-th parity-check equation which involves k variables. Let us assume that the set\nof indices of the active variables in this parity-check equation is {i1 , . . . , ik }. Then, the component\nSj of the syndrome is equal to 1 if and only if there is an odd number of ones in the random vector\ne i1 , . . . , \u0398\ne i ).\n(\u0398\nk\n\nLemma 4.1. If the j-th component of the syndrome S involves k active variables in its parity-check\nequation whose indices are {i1 , i2 , . . . , ik }, then\n\"\n\u0013#\nk \u0012\nY\n\u0001 1\n2e\u2212\u03b1m\n1\u2212\n1\u2212\n.\n(56)\nPr Sj = 1 | (\u03a9i1 , . . . , \u03a9ik ) = (\u03b11 , . . . , \u03b1k ) =\n2\n1 + e\u2212\u03b1m\nm=1\n\nProof. The lemma is proved in Appendix C.1.\nWe therefore obtain from Lemma 4.1 that\n\u0001\nH Sj |(\u03a9i1 , . . . , \u03a9ik ) = (\u03b11 , . . . , \u03b1k ) = h2\n\n\"\n\u0013#!\nk \u0012\nY\n2e\u2212\u03b1m\n1\n1\u2212\n1\u2212\n2\n1 + e\u2212\u03b1m\nm=1\n\nand by taking the statistical expectation over the k random variables \u03a9i1 , . . . , \u03a9ik , we get\n\u0001\nH Sj |(\u03a9i1 , . . . , \u03a9ik )\n\"\n\u0013#! Y\nZ \u221e\nZ \u221e\nk \u0012\nk\nY\n2e\u2212\u03b1m\n1\nf\u03a9 (\u03b1i ) d\u03b11 d\u03b12 . . . d\u03b1k\n1\u2212\n1\u2212\n=\n...\nh2\n2\n1 + e\u2212\u03b1m\n0\n0\nm=1\ni=1\n\u0013k\n\u0012Z \u221e\n\u221e\n\u0010 \u0011\n1 X\n1\n2p \u03b1\n=1\u2212\nd\u03b1\nf\u03a9 (\u03b1) tanh\n2 ln 2\np(2p \u2212 1)\n2\n0\n\n(57)\n\np=1\n\nwhere the equality in the last transition is proved in Appendix C.3. Hence, if dk designates the\nnumber of parity-check equations of degree k, then\nn(1\u2212R)\n\nX\nj=1\n\n\u0001\nH Sj |(\u03a91 , . . . , \u03a9n )\n\n\uf8f9\n\u0013k\n\u0012Z \u221e\n\u221e\n\u0010\u03b1\u0011\nX X\n1\n1\nd\u03b1 \uf8fb .\nf\u03a9 (\u03b1) tanh2p\ndk\n= n(1 \u2212 R) \uf8f01 \u2212\n2 ln 2\np(2p \u2212 1)\n2\n0\n\uf8ee\n\nk\n\np=1\n\n(58)\n\n\fe\nBy combining (51), (54), (55) and (58), we get the following upper bound on H(Y):\ne \u2264 nH(\u03a9) + nR\nH(Y)\n\uf8ee\n\n\u221e\n\n1\n1 X X\ndk\n+n(1 \u2212 R) \uf8f01 \u2212\n2 ln 2\np(2p \u2212 1)\nk\n\np=1\n\n\u0012Z\n\n\u221e\n\na(\u03b1)(1 + e\u2212\u03b1 ) tanh2p\n\n0\n\n\u0010\u03b1\u0011\n2\n\n\uf8f9\n\u0013k\nd\u03b1 \uf8fb .(59)\n\ne given in (59) are substituted in the\nFinally, the equality in (53) and the upper bound on H(Y)\nRHS of (52). This provides the lower bound on the conditional entropy H(X|Y) given in (50), and\ncompletes the proof of this proposition.\nRemark 4.1. For the particular case of a BEC with erasure probability p, C = 1 \u2212 p, and the\nconditional pdf of the LLR is independent of the transmitted symbol. It is equal to\na(l) = p\u22060 (l) + (1 \u2212 p)\u2206\u221e (l)\nwhere \u2206a (*) designates the Dirac delta function at the point a. We obtain from (50)\n\"\n#\nX\nH(X|Y)\n\u2265 p \u2212 (1 \u2212 R) 1 \u2212\ndk (1 \u2212 p)k .\nn\n\n(60)\n\nk\n\nThis lower bound on the conditional entropy for the BEC coincides with the result proved in [13,\nEqs. (33) and (34)]. The result there was obtained by the derivation of an upper bound on the\nrank of HE which is a sub-matrix of H whose columns correspond to the variables erased by the\nBEC.\nDiscussion: Since the proof of Proposition 4.1 relies on the analysis of an equivalent channel,\nrather than a degraded (quantized) channel, it is suggested that the lower bound in the RHS of\n(50) should be tighter than the one in the RHS of (34). In order to prove this property, it is enough\nto show that for any integer d \u2265 2 and any choice of quantization levels l1 , . . . , l2d\u22121 \u22121 , we have\n(\n\u0012 \u0013 \u0013k )\n\u221e\nX \u0012Z \u221e\nl\n1\n1 X\na(l)(1 + e\u2212l ) tanh2p\ndk\ndl\n1\u2212\n2 ln(2)\np(2p \u2212 1)\n2\n0\np=1\nk\n(\n\u0013\n\u0012\nX\nX\nk\n\u2264\ndk\nk0 , . . . , k2d\u22121 \u22121\nk\n\nk0 ,...,k2d\u22121 \u22121\nP\ni ki =k\n\n*\n\n2d\u22121\nY\u22121\n\n(pi + p2d \u22121\u2212i )ki\n\ni=0\n\n\uf8eb \uf8ee\n\uf8f9\uf8f6 )\n\u0013k i\n2d\u22121\n\u22121 \u0012\nY\n2p2d \u22121\u2212i\n1\n\uf8fb\uf8f8\nh2 \uf8ed \uf8f01 \u2212\n1\u2212\n2\npi + p2d \u22121\u2212i\n\n(61)\n\ni=0\n\nwhere p0 , . . . , p2d \u22121 are the transition probabilities associated with l1 , . . . , l2d\u22121 \u22121 , as defined in (33).\nThis inequality is proved in Appendix D.1, using the power series expansion of the binary entropy\nfunction, h2 (*), derived in Appendix C.2.\nTheorem 4.1 (\"Un-Quantized\" Lower Bound on the Asymptotic Parity-Check Density\nof Binary Linear Block Codes). Let {Cm } be a sequence of binary linear codes achieving a\nfraction 1 \u2212 \u03b5 of the capacity C of an MBIOS channel with vanishing bit error probability. Then,\nthe asymptotic density of their parity check matrices satisfies\nK1 (x) + K2 (x) ln 1\u03b5\n1\u2212\u03b5\nx\u2208(0,A]\n\nlim inf \u2206m \u2265 sup\nm\u2192\u221e\n\n(62)\n\n\fwhere\n\u0011\n\u0010\n\u03be (1\u2212C)\nln\nC\n1\u2212C\n\u0001\nK1 (x) =\n,\nC\nln x1\n\nand\n\nA,\n\nZ\n\n\u221e\n0\n\n(1 \u2212 e\u2212l )2\ndl,\na(l)\n1 + e\u2212l\n\nK2 (x) =\n\n\u03be,\n\n\u001a\n\n1\n1\n2 ln(2)\n\n1\n1\u2212C\n\u0001.\nC ln x1\nfor a BEC\n.\notherwise\n\n(63)\n\n(64)\n\nProof. From the lower bound on H(Xn| Y) in Eq. (50) and Lemma 3.1 (see p. 10), we obtain that\nif {Cm } is a sequence of binary linear block codes which achieves a fraction 1 \u2212 \u03b5 of the channel\ncapacity with vanishing bit error probability, then\n\uf8f9\n\uf8ee\n\u0012Z \u221e\n\u0012 \u0013 \u0013k\n\u221e\nX\nX\n\u0001\n1\nl\n1\na(l)(1 + e\u2212l ) tanh2p\ndk\ndl \uf8fb \u2264 0.\n1\u2212C \u2212 1\u2212(1\u2212\u03b5)C \uf8f01 \u2212\n2 ln(2)\np(2p\n\u2212\n1)\n2\n0\np=1\nk\n\n(65)\nSince k kdk = aR is the average right degree, then from the convexity of the exponential function,\nwe obtain by invoking Jensen's inequality that\n\uf8ee\n\uf8f9\n\u0012 \u0013 \u0013aR\n\u0012Z \u221e\n\u221e\nX\n\u0001\n1\nl\n1\n\uf8fb \u2264 0. (66)\n1\u2212C\u2212 1\u2212(1\u2212\u03b5)C \uf8f01 \u2212\na(l)(1 + e\u2212l ) tanh2p\ndl\n2 ln(2)\np(2p \u2212 1)\n2\n0\nP\n\np=1\n\nWe will now derive two different lower bounds on the infinite sum in the RHS of (66), and\ncompare them later. For the derivation of the lower bound in the first approach, let us define the\npositive sequence\n1\n1\n, p = 1, 2, . . .\n(67)\n\u03b1p ,\nln(2) 2p(2p \u2212 1)\nFrom\nP\u221e (C.1) in the appendix, the substitution of x = 0 in both sides of the equality gives that\np=1 \u03b1p = 1, so the sequence {\u03b1p } forms a probability distribution. We therefore obtain that\n\u0012Z \u221e\n\u0012 \u0013 \u0013aR\n\u221e\n1 X\n1\nl\n\u2212l\n2p\na(l)(1 + e ) tanh\ndl\n2 ln(2) p=1 p(2p \u2212 1)\n2\n0\n\u0012 \u0013 \u0013aR\n\u0012Z \u221e\n\u221e\nX\nl\n\u2212l\n2p\ndl\n\u03b1p\na(l)(1 + e ) tanh\n=\n2\n0\np=1\n\uf8eb\n\uf8f6aR\n\u0012 \u0013\nZ \u221e\n\u221e\nX\n(a)\nl\na(l)(1 + e\u2212l )\n\u2265\uf8ed\n\u03b1p tanh2p\ndl\uf8f8\n2\n0\np=1\n\"\n\u0012 \u0013\u0015\u0013# !aR\n\u0012 \u0014\nZ \u221e\nl\n1\n(b)\ndl\n1 \u2212 tanh\n=\na(l)(1 + e\u2212l ) 1 \u2212 h2\n2\n2\n0\n\u0013\u0015 !aR\n\u0014\n\u0012\nZ \u221e\n1\n(c)\ndl\na(l)(1 + e\u2212l ) 1 \u2212 h2\n=\n1 + e\u2212l\n0\n(d)\n\n= C aR\n\n(68)\n\nwhere inequality (a) follows from Jensen's inequality, equality (b) follows from (67) and (C.1),\n2x\nequality (c) follows from the identity tanh(x) = ee2x \u22121\n, and equality (d) follows from the relation\n+1\n\n\fbetween the capacity of an MBIOS channel and the pdf of the absolute value of the LLR (see [12,\nLemma 3.13]).\nFor an alternative derivation of the lower bound of the infinite series, we can truncate the\ninfinite sum in the RHS of (66) and take into account only the first term in this series. This gives\n\u0012Z \u221e\n\u0012 \u0013 \u0013aR\n\u221e\n1\nl\n1 X\n\u2212l\n2p\na(l)(1 + e ) tanh\ndl\n2 ln(2) p=1 p(2p \u2212 1)\n2\n0\n\u0012 \u0013 \u0013aR\n\u0012Z \u221e\n1\nl\n\u2265\na(l)(1 + e\u2212l ) tanh2\ndl\n2 ln(2)\n2\n0\n!aR\n\u0012\nZ \u221e\n\u2212l \u00132\n1\n\u2212\ne\n1\na(l)(1 + e\u2212l )\ndl\n=\n2 ln(2)\n1 + e\u2212l\n0\n=\n\nAaR\n2 ln(2)\n\n(69)\n\nwhere the last transition follows from (64).\nIn order to compare the tightness of the two lower bounds in (68) and (69), we first compare\nthe bases of their exponents (i.e., A and C). To this end, it is easy to verify that\n\u0012\n\u00132\n\u0013\n\u0012\n1 \u2212 e\u2212l\n1\n\u2265 1 \u2212 h2\nl \u2208 [0, \u221e)\n1 + e\u2212l\n1 + e\u2212l\nwith an equality if and only if l = 0 or l \u2192 \u221e. Hence, from (68) and (69), this gives A \u2265 C with\nequality if and only if the MBIOS channel is a BEC. Therefore, up to the multiplicative constant\n1\n2 ln(2) , the second lower bound is tighter than the first one. However, we note that for the BEC,\nthe first bound is tighter. It gives an improvement by a factor of 2 ln(2) \u2248 1.386.\nWe will therefore continue the analysis based on the second bound in (69), and then give the\npotential improvement which follows from the first bound in (68) for a BEC. From (66) and (69),\nwe obtain that\n\u0013\n\u0012\n\u0001\nAaR\n\u2264 0.\n1 \u2212 C \u2212 1 \u2212 (1 \u2212 \u03b5)C 1 \u2212\n2 ln(2)\n\nHence, one can replace A (where 0 < A \u2264 1) in the last inequality by an arbitrary x \u2208 (0, A], and\nobtain that the asymptotic average right degree, aR , satisfies the lower bound\n\u0010\n\u0001\u0011\n1\nln 2 ln(2)\n1 + 1\u2212C\n\u03b5C\n\u0001\n.\naR \u2265\nln x1\nBy dropping the 1 inside the logarithm in the numerator, we obtain that for x \u2208 (0, A]\n\u0012 \u0013\n1\n\u2032\n\u2032\naR > K1 (x) + K2 (x) ln\n\u03b5\n\nwhere K1\u2032 (x) =\n\n\u0010\n1\nln 2 ln(2)\n\n1\u2212C\nC\n\n\u0011\n\nand K2\u2032 (x) =\n\n1\n.\nln( x1 )\n\n(70)\n\nFinally, since the parity-check density and\n\u0001\naR , then we obtain the following lower\naverage right degree are related by the equality \u2206 = 1\u2212R\nR\nbound on the asymptotic parity-check density:\n\u0012\n\u0012 \u0013\u0013\n1\n1 \u2212 (1 \u2212 \u03b5)C\n\u2032\n\u2032\nlim inf \u2206m >\nK1 (x) + K2 (x) ln\nm\u2192\u221e\n(1 \u2212 \u03b5)C\n\u03b5\n\u0001\n1\nK1 (x) + K2 (x) ln \u03b5\n, \u2200 x \u2208 (0, A]\n(71)\n>\n1\u2212\u03b5\nln( x1 )\n\n\f\u2032\nwhere K1,2 (x) , 1\u2212C\nC K1,2 (x). For the BEC, this lower bound can be improved by using the first\nbound in (68). In this case, A = C = 1 \u2212 p where p designates the erasure probability of the BEC,\nso the additive coefficient K1 in the RHS of (62) is improved to\n\u0011\n\u0010\np\nln\n1\u2212p\np\n\u0001 , x \u2208 (0, 1 \u2212 p].\nK1 (x) =\n1 \u2212 p ln x1\n\nThis concludes the proof of this theorem.\n\nRemark 4.2. For a BEC with erasure probability p, the maximization of the RHS of (62) yields\np\np\n(otherwise, if \u03b5 \u2265 1\u2212p\n, the lower\nthat the maximal value is achieved for x = 1 \u2212 p when \u03b5 < 1\u2212p\nbound is useless as it becomes non-positive). Hence, the lower bound on the asymptotic paritycheck density stated in Theorem 4.1 coincides with the bound for the BEC in [13, Eq. (3)]. This\nlower bound was demonstrated in [13, Theorem 2.3] to be tight. This is proved by showing that\nthe sequence of right-regular LDPC ensembles of Shokrollahi [14] is optimal in the sense that it\nachieves (up to a small additive coefficient) the lower bound on the asymptotic parity-check density\nfor the BEC.\nFor a general MBIOS channel (other than the BEC), we show in the proof above that the\npreferable logarithmic growth rate of the lower bound on the parity-check density is achieved by\nusing the bound which follows from (69) (even in the particular case where x = A). However, we\nnote that the lower bound on the parity-check density which follows from (68) is universal w.r.t.\nall MBIOS channels with the same capacity.\nRemark 4.3. The lower bound on the parity-check density in Theorem 4.1 is uniformly tighter\nthan the one in [13, Theorem 2.1] (except for the BSC and BEC where they coincide). For a proof\nof this claim, the reader is referred to Appendix D.2.\nBased on the proof of Theorem 4.1, we prove and discuss an upper bound on the asymptotic rate\nof every sequence of binary linear codes for which reliable communication is achievable. The bound\nrefers of optimal ML decoding, and is therefore valid for any sub-optimal decoding algorithm. Hence\nthe following result also provides an upper bound on the achievable rate of ensembles of LDPC\ncodes under iterative decoding, where the transmission takes places over an MBIOS channel.\nCorollary 4.1 (\"Un-Quantized\" Upper Bound on the Asymptotic Achievable Rates of\nSequences of Binary Linear Block Codes). Let {Cm } be a sequence of binary linear block\ncodes whose codewords are transmitted with equal probability over an MBIOS channel, and assume\nthat the block lengths of these codes tend to infinity as m \u2192 \u221e. Let dk,m be the fraction of the\nparity-check nodes of degree k for arbitrary representations of the codes Cm by bipartite graphs.\nThen a necessary condition on the achievable rate (R) for obtaining vanishing bit error probability\nas m \u2192 \u221e is\nR\u22641\u2212\n\n1\n1\u2212\n2 ln(2)\n\n\u221e\nX\np=1\n\n(\n\nX\n1\ndk\np(2p \u2212 1)\nk\n\n1\u2212C\n\u0012Z \u221e\n0\n\n\u0012 \u0013 \u0013k )\nl\na(l) (1 + e ) tanh\ndl\n2\n\u2212l\n\n(72)\n\n2p\n\nwhere dk and R are introduced in (1).\nProof. This upper bound on the achievable rate follows immediately from Lemma 3.1 (see p. 10)\nand the lower bound on the conditional entropy in Proposition 4.1. The upper bound on R follows\nsince the bit error probability of the sequence of codes {Cm } vanishes as we let m tend to infinity.\n\n\fRemark 4.4. We note that the upper bound on the achievable rate in the RHS of (72) doesn't\ninvolve maximization, in contrast to the bound in the RHS of (49). The second term of the\nmaximization in the latter bound follows from considerations related to the BEC where such an\nexpression is not required in the RHS of (72). The reader is referred to Appendix D.3 for a proof\nof this claim.\nCorollary 4.2 (Lower Bounds on the Bit Error Probability of LDPC Codes). Let C be\na binary linear block code of rate R whose transmission takes place over an MBIOS channel with\ncapacity C. For an arbitrary parity-check matrix H of the code C, let dk designate the fraction of\nparity-check equations that involve k variables. Then, under ML decoding (or any other decoding\nalgorithm), the bit error probability (Pb ) of the code satisfies\n(\n\u0012 \u0013 \u0013k )\n\u221e\nX \u0012Z \u221e\n1\u2212R X\n1\nl\nC\na(l)(1 + e\u2212l ) tanh2p\n. (73)\nh2 (Pb ) \u2265 1 \u2212 +\ndk\ndl\nR 2 ln(2)R p=1 p(2p \u2212 1)\n2\n0\nk\n\nProof. This follows directly by combining (23) and (50).\nWe now introduce the definition of normalized parity-check density from [13], and derive an\nimproved lower bound on the bit error probability (as compared to [13, Theorem 2.5]) in terms of\nthis quantity.\nDefinition 4.1 (Normalized parity-check density [13]). Let C be a binary linear code of rate\nR, which is represented by a parity-check matrix H whose density is \u2206. The normalized density of\nR\u2206\n.\nH, call it t = t(H), is defined to be t = 2\u2212R\nIn the following, we clarify the motivation for the definition of a normalized parity-check density.\nLet us assume that C is a binary linear block code of length n and rate R, and suppose that it can\nbe represented by a bipartite graph which is cycle-free. From [13, Lemma 2.1], since this bipartite\ngraph contains (2 \u2212 R)n \u2212 1 edges, connecting n variable nodes with (1 \u2212 R)n parity-check nodes\n1\nwithout any cycles, then the parity-check density of such a cycle-free code is \u2206 = 2\u2212R\nR \u2212 nR . Hence,\nin the limit where we let n tend to infinity, the normalized parity-check density of a cycle-free code\ntends to 1. For codes which are represented by bipartite graphs with cycles, the normalized paritycheck density is above 1. As shown in [13, Corollary 2.5], the number of fundamental cycles in\na bipartite graph which represents an arbitrary linear block C grows linearly with the normalized\nparity-check density. The normalized parity-check density therefore provides a measure for the\nnumber of cycles in bipartite graphs representing linear block codes. It is well known that cyclefree codes are not good in terms of performance, even under optimal ML decoding [15]; hence, good\nerror-correcting codes (e.g., LDPC codes) should be represented by bipartite graphs with cycles.\nFollowing the lead of [13], providing a lower bound on the asymptotic normalized parity-check\ndensity in terms of their rate and gap to capacity gives a quantitative measure for the number of\nfundamental cycles of bipartite graphs representing good error correcting codes. In the following,\nwe provide such an improved bound as compared to the bound given in [13, Theorem 2.5]. In the\ncontinuation (see Section 5.2), the resulting improvement is exemplified.\nFirst, we note that from Definition 4.1, it follows that the relation between the normalized\nparity-check density and the average right degree is\n\u0013\n\u0012\n1\u2212R\naR\nt=\n2\u2212R\nso the normalized parity-check density grows linearly with the average right degree (which is directly\nlinked to the decoding complexity per iteration of LDPC codes under message-passing decoding)\nwhere the scaling factor depends on the code rate R.\n\n\fP\nSince\nk kdk = aR , then by applying Jensen's inequality to the RHS of (73), we get the\nfollowing lower bound on the bit error probability:\n\uf8fc\n\uf8f1\n\u0012 \u0013 \u0013 (2\u2212R)t \uf8fd\n\u0012Z \u221e\n\u221e\n1\u2212R\nl\n1 \u2212 R X\uf8f2\n1\nC\n.\n(74)\na(l)(1 + e\u2212l ) tanh2p\ndl\nh2 (Pb ) \u2265 1 \u2212 +\n\uf8fe\n\uf8f3 p(2p \u2212 1)\nR 2 ln(2)R\n2\n0\np=1\n\nThis lower bound on the bit error probability is tighter than the bound given in [13, Eq. (23)] because\nof two reasons: Firstly, by combining inequality (69) with the inequality proved in Appendix D.2,\nwe obtain that\n\uf8fc\n\uf8f1\n2(2\u2212R)t\n\u0012Z \u221e\n\u0012 \u0013 \u0013 (2\u2212R)t \uf8fd\n\u221e \uf8f2\nX\n1\u2212R\n1\n(1 \u2212 2w) 1\u2212R\nl\n1\n\u2212l\n2p\n\u2265\n.\na(l)(1 + e ) tanh\ndl\n\uf8fe\n2 ln(2) p=1 \uf8f3 p(2p \u2212 1)\n2\n2 ln 2\n0\n\nSecondly, the further improvement in the tightness of the new bound is obtained by dividing the\nRHS of (74) by R (where R \u2264 1), as compared to the RHS of [13, Eq. (23)].\n\nThe bounds in (73) and (74) become trivial when the RHS of these inequalities are non-positive.\nR\nLet the (multiplicative) gap to capacity be defined as \u03b5 , 1 \u2212 C\n. Analysis shows that the bounds\nin (73) and (74) are useful unless \u03b5 \u2265 \u03b50 (see Appendices D.4 and D.5). For the bound in the RHS\nof (73), \u03b50 gets the form\n(\n\u0012 \u0013 \u0013k )\n\u221e\nX \u0012Z \u221e\nl\n1 X\n1\n(1 \u2212 C)B\n\u2212l\n2p\na(l)(1 + e ) tanh\ndk\n(75)\n, B,\ndl\n\u03b50 =\nC(1 \u2212 B)\n2 ln(2)\np(2p \u2212 1)\n2\n0\np=1\n\nk\n\nand for the bound in the RHS of (74), \u03b50 is the unique solution of the equation\n\uf8f1\n\uf8fc\n\u0012Z \u221e\n\u0012 \u0013 \u0013 (2\u2212(1\u2212\u03b50 )C)t \uf8fd\n\u221e \uf8f2\nX\n1\u2212(1\u2212\u03b5\n)C\n0\nl\n1\n1 \u2212 (1 \u2212 \u03b50 )C\na(l)(1 + e\u2212l ) tanh2p\ndl\n= 0. (76)\n\u2212\u03b50 C +\n\uf8f3 p(2p \u2212 1)\n\uf8fe\n2 ln(2)\n2\n0\np=1\n\nFor a proof of (75) and (76), the reader is referred to Appendices D.4 and D.5, respectively.\nSimilarly to [13, Eq. (25)], we note that \u03b50 in (76) forms a lower bound on the gap to capacity for\nan arbitrary sequence of binary linear block codes achieving vanishing bit error probability over\nan MBIOS channel; the bound is expressed in terms of their asymptotic rate R and normalized\nparity-check density t. It follows from the transition from (73) to (74) that the lower bound on\nthe gap to capacity in (76) is looser as compared to the one given in (75). However, the bound in\n(76) solely depends on the normalized parity-check density, while the bound in (75) requires full\nknowledge of the degree distribution for the parity-check nodes.\n\n\f5\n\nNumerical Results\n\nIn this section we present numerical results for the information-theoretic bounds on the limitations\nof binary linear block codes transmitted over MBIOS channels. These results refer to Theorems 3.1,\n3.2 and 4.1 and Corollaries 3.1, 3.2, 4.1 and 4.2. As expected, they significantly improve the\nnumerical results presented in [1, Section 4] and [13, Section 4]. This improvement is attributed to\nthe fact that, in contrast to [1, 13], in the derivation of the bounds in this paper, we do not perform\na two-level quantization of the LLR which in essence converts the arbitrary MBIOS channel (whose\noutput may be continuous) to a BSC. Throughout this section, we assume transmission of the codes\nover the binary-input AWGN channel.\n\n5.1\n\nThresholds of LDPC Ensembles under ML Decoding\n\nThe following results (see Tables 1\u20133) provide bounds on the thresholds of LDPC ensembles under\nML decoding. They also give an indication on the inherent loss in performance due to the suboptimality of iterative message-passing decoding.\n\nLDPC\nEnsemble\n(3,6)\n(4,6)\n(3,4)\n\nCapacity\nLimit\n+0.187 dB\n\u22120.495 dB\n\u22120.794 dB\n\n2-Levels\nBound [1]\n+0.249 dB\n\u22120.488 dB\n\u22120.761 dB\n\n4-Levels\nBound\n+0.332 dB\n\u22120.472 dB\n\u22120.713 dB\n\n8-Levels\nBound\n+0.361 dB\n\u22120.463 dB\n\u22120.694 dB\n\nUn-Quantized\nLower Bound\n+0.371 dB\n\u22120.463 dB\n\u22120.687 dB\n\nUpper\nBound [5]\n+0.673 dB\n\u22120.423 dB\n\u22120.510 dB\n\nDE\nThreshold\n+1.110 dB\n+1.674 dB\n+1.003 dB\n\nTable 1: Comparison of thresholds for Gallager's ensembles of regular LDPC codes transmitted\nEb\nrefers to\nover the binary-input AWGN channel. The 2-level lower bound on the threshold of N\no\nML decoding, and is based on [1, Theorem 1] (see also [13, Table II]). The 4-level, 8-level and\nun-quantized lower bounds apply to ML decoding, and are based on Corollaries 3.1, 3.2 and 4.1,\nEb\nholds under 'typical pairs' decoding [5] (and\nrespectively. The upper bound on the threshold of N\no\nhence, also under ML decoding), and the DE thresholds are based on density evolution for iterative\nmessage-passing decoding [10].\nThe upper bounds on the achievable rates derived in [1] and Corollaries 3.1, 3.2 and 4.1 provide\nEb\nthresholds under ML decoding. For Gallager's regular LDPC ensembles,\nlower bounds on the N\no\nthe gap between the thresholds under ML decoding and the exact thresholds under the sum-product\ndecoding algorithm (which are calculated using density-evolution analysis) are rather large. For\nEb\nthis reason, we also compare the lower bounds on the N\nthresholds under ML decoding with upper\no\nEb\nEb\nbounds on the No thresholds which rely on \"typical pairs decoding\" [5]; an upper bound on the N\no\nthresholds under an arbitrary sub-optimal decoding algorithm (e.g., \"typical pairs decoding\") also\nforms an upper bound on these thresholds under optimal ML decoding. It is shown in Table 1 that\nthe gap between the thresholds under iterative decoding and the bounds for ML decoding (see the\ncolumns referring to the DE threshold and the upper bound based on \"typical pairs decoding\")\nis rather large. This is attributed to the sub-optimality of belief propagation decoding for regular\nLDPC ensembles. On the other hand, it is also demonstrated in Table 1 that the gap between\nthe upper and lower bounds on the thresholds under ML decoding is much smaller. For example,\naccording to the numerical results in Table 1, the inherent loss in the asymptotic performance due\nto the sub-optimality of belief propagation for Gallager's ensemble of (4, 6) regular LDPC codes\n\n\f(whose design rate is\n\n1\n3\n\nbits per channel use) ranges between 2.097 and 2.137 dB.\n\nFor irregular LDPC ensembles, the calculation of similar upper bounds based on \"typical pairs\"\ndecoding [5] is based on the calculation of the asymptotic growth rate of the distance spectra of such\nensembles. For a given pair of degree distributions (\u03bb, \u03c1), the calculation of the asymptotic growth\nrates of the distance spectrum for the (n, \u03bb, \u03c1) LDPC ensemble (where we let n tend to infinity)\nis tractable (see [2, 17]). However, we avoid calculating these upper bounds in Tables 2 and 3 due\nto the fact that the gap between the DE thresholds under belief propagation and the improved\nEb\nthresholds derived in this paper is already rather small (see Tables 2 and\nlower bounds on the N\no\n3). The rather small gap between the DE thresholds and the un-quantized lower bounds on the\nthresholds under ML decoding also indicate that for the degree distributions which are provided by\nthe LDPC optimizer [16], the asymptotic degradation in performance due to the sub-optimality of\nbelief propagation is marginal (it is observed from Tables 2 and 3 that for several LDPC ensembles,\nthis degradation in the asymptotic performance is at most in the order of hundredthes of a decibel).\n2-Levels\nBound [1]\n\n4-Levels\nBound\n\n8-Levels\nBound\n\nUn-Quantized\nLower Bound\n\nDE\nThreshold\n\n0.24123x4 +\n0.75877x5\n\n0.269 dB\n\n0.370 dB\n\n0.404 dB\n\n0.417 dB\n\n0.809 dB\n\n0.98013x7 +\n0.01987x8\n\n0.201 dB\n\n0.226 dB\n\n0.236 dB\n\n0.239 dB\n\n0.335 dB\n\n0.64854x7 +\n0.34747x8 +\n0.00399x9\n\n0.198 dB\n\n0.221 dB\n\n0.229 dB\n\n0.232 dB\n\n0.310 dB\n\n0.00749x7 +\n0.99101x8 +\n0.00150x9\n\n0.194 dB\n\n0.208 dB\n\n0.214 dB\n\n0.216 dB\n\n0.274 dB\n\n\u03bb(x)\n\n\u03c1(x)\n\n0.38354x + 0.04237x2 +\n0.57409x3\n0.23802x + 0.20997x2 +\n0.03492x3 + 0.12015x4 +\n0.01587x6 +0.00480x13 +\n0.37627x14\n0.21991x + 0.23328x2 +\n0.02058x3 + 0.08543x5 +\n0.06540x6 + 0.04767x7 +\n0.01912x8 +0.08064x18 +\n0.22798x19\n0.19606x + 0.24039x2 +\n0.00228x5 + 0.05516x6 +\n0.16602x7 + 0.04088x8 +\n0.01064x9 +0.00221x27 +\n0.28636x29\n\nTable 2: Comparison of thresholds for rate one-half ensembles of irregular LDPC codes transmitted\nEb\n= 0.187 dB.\nover the binary-input AWGN channel. The Shannon capacity limit corresponds to N\no\nThe 2-level, 4-level, 8-level and un-quantized lower bounds on the threshold refer to ML decoding,\nand are based on [1, Theorem 2], Corollaries 3.1, 3.2 and 4.1, respectively. The degree distributions\nof the ensembles and their DE thresholds are based on density evolution under iterative messagepassing decoding [10], and are taken from [11, Tables 1 and 2].\nEb\n-threshold under ML decoding\nThe plots in Figure 2 compare different lower bounds on the N\n0\nof right-regular LDPC ensembles. The plots refer to a right degree of 6 (left plot) or 10 (right plot).\nThe following lower bounds are depicted in these plots: the Shannon capacity limit, the 2-level\nquantization lower bound in [1, Theorem 1], the 4 and 8-level quantization bounds of the LLR in\nSection 3, and finally, the bound in Section 4 where no quantization of the LLR is performed. It\ncan be observed from the two plots in Figure 2 that the range of code rates where there exists a\nvisible improvement with the new lower bounds depends on the degree of the parity-check nodes.\nIn principle, the larger the value of the right-degree is, then the improvement obtained by these\nbounds is more pronounced starting from a higher rate code rate (e.g., for a right degree of 6 or\n\n\f\u03bb(x)\n0.302468x + 0.319447x2 +\n0.378085x4\n0.244067x + 0.292375x2 +\n0.463558x6\n0.205439x + 0.255432x2 +\n0.0751187x4 +0.1013440x5 +\n0.3626670x11\n\n\u03c1(x)\n\n2-Levels\nBound [1]\n\n4-Levels\nBound\n\n8-Levels\nBound\n\nUn-Quantized\nLower Bound\n\nDE\nThreshold\n\nx11\n\n1.698 dB\n\n1.786 dB\n\n1.815 dB\n\n1.825 dB\n\n2.049 dB\n\nx13\n\n1.664 dB\n\n1.718 dB\n\n1.736 dB\n\n1.742 dB\n\n1.874 dB\n\nx15\n\n1.647 dB\n\n1.680 dB\n\n1.691 dB\n\n1.695 dB\n\n1.763 dB\n\nTable 3: Comparison of thresholds for rate- 34 ensembles of irregular LDPC codes transmitted over\nEb\n= 1.626 dB. The\nthe binary-input AWGN channel. The Shannon capacity limit corresponds to N\no\n2-level, 4-level, 8-level and un-quantized lower bounds on the threshold refer to ML decoding, and\nare based on [1, Theorem 2], Corollaries 3.1, 3.2 and 4.1, respectively. The degree distributions\nof the ensembles and their DE thresholds are based on density evolution under iterative messagepassing decoding [10], and are taken from [16].\n10, the improvement obtained by the new bounds is observed for code rates starting from 0.35 and\n0.55 bits per channel use, respectively).\nRight\u2212Regular LDPC a = 6\n\nRight\u2212Regular LDPC a = 10\n\nR\n\nR\n\n0.65\nShannon Capacity Limit\nLower Bound [1, Theorem 1]\n4\u2212Level Quantization Bound\n8\u2212Level Quantization Bound\nUn\u2212Quantized Bound\n\n0.6\n\n0.75\n\n0.55\n\nShannon Capacity Limit\nLower Bound [1,Theorem 1]\n4 Level Quantization Bound\n8 Level Quantization Bound\nUn\u2212Quantized Bound\n\n0.7\n\nRate\n\nRate\n\n0.5\n\n0.45\n\n0.65\n\n0.6\n\n0.4\n\n0.35\n0.55\n\n0.3\n\n\u22120.75\n\n\u22120.5\n\n\u22120.25\n\n0\n\n0.25\n\n0.5\nEb/No [dB]\n\n0.75\n\n1\n\n1.25\n\n1.5\n\n1.75\n\n0.5\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n1.4\nEb/No [dB]\n\n1.6\n\n1.8\n\n2\n\n2.2\n\nFigure 2: Comparison between different lower bounds on the threshold under ML decoding for\nright-regular LDPC ensembles with aR = 6 (left plot) and aR = 10 (right plot). The transmission\ntakes place over the binary-input AWGN channel.\n\n5.2\n\nLower Bounds on the Bit Error Probability of LDPC Codes\n\nBy combining the lower bound in Proposition 4.1 and Lemma 3.1, we obtain in Corollary 4.2 an\nimproved lower bound on the bit error probability of binary linear block codes, as compared to the\none given in [13, Theorem 2.5]. The plot of Fig. 3 presents a comparison of these lower bounds for\nbinary linear block codes where the bounds rely on (74) and [13, Theorem 2.5]. They are plotted as\na function of the normalized density of an arbitrary parity-check matrix. In our setting, the capacity\nof the channel is 12 bit per channel use, and the bounds are depicted for binary linear block codes\nwhose rate is a fraction 1 \u2212 \u03b5 of the channel capacity. To demonstrate the advantage of the lower\n\n\fbound on the bit error probability in (74) over the lower bound derived in [13, Theorem 2.5], let\nus assume that one wants to design a binary LDPC code which achieves a bit-error probability of\n10\u22126 at a rate which is 99% of the channel capacity. The curve of the lower bound from [13] for\n\u03b5 = 0.01 implies that the normalized density of an arbitrary parity-check matrix which represents\nthe code (see Definition 4.1 in p. 27) should be at least 4.33, while the curve depicting the bound\nfrom (74) strengthens this requirement to a normalized density (of each parity-check matrix) of at\nleast 5.68. Translating this into terms of parity-check density (which is also the complexity per\niteration for message-passing decoding) yields minimal parity-check densities of 13.16 and 17.27,\nmin\n). It is reflected from\nrespectively (the minimal parity-check density is given by \u2206min = (2\u2212R)t\nR\nFig. 3 that as the gap to capacity \u03b5 tends to zero, the lower bound on the normalized density of an\narbitrary parity-check matrix (t), which represents a code which achieves low error probability for\na rate of R = (1 \u2212 \u03b5)C grows significantly.\n0\n\n10\n\nLower Bound on P in Corollary 4.2 (\u03b5 = 0)\nb\nLower Bound on P [13, Theorem 2.5] (\u03b5 = 0)\nb\nLower Bound on P in Corollary 4.2 (\u03b5 = 0.01)\nb\nLower Bound on Pb [13, Theorem 2.5] (\u03b5 = 0.01)\nLower Bound on P in Corollary 4.2 (\u03b5 = 0.05)\nb\nLower Bound on P [13, Theorem 2.5] (\u03b5 = 0.05)\n\n\u22121\n\n10\n\nb\n\n\u22122\n\nBit Error Probability\n\n10\n\n\u22123\n\n10\n\n\u03b5=0\n\u22124\n\n10\n\n\u03b5=0\n\u03b5 = 0.01\n\n\u22125\n\n10\n\n\u03b5 = 0.05\n\n\u03b5 = 0.01\n\n\u03b5 = 0.05\n\n\u22126\n\n10\n\n1\n\n2\n\n3\n\n4\n5\nNormalized Parity\u2212Check Density (t)\n\n6\n\n7\n\n8\n\nFigure 3: Lower bounds on the bit error probability for any binary linear block code transmitted\nover a binary-input AWGN channel whose capacity is 12 bits per channel use. The bounds are\ndepicted in terms of the normalized density of an arbitrary parity-check matrix which represents\nthe code, and the curves correspond to code rates which are a fraction 1 \u2212 \u03b5 of the channel capacity\n(for different values of \u03b5). The bounds depicted in dashed lines are based on [13, Theorem 2.5],\nand the bounds in solid lines are given in Corollary 4.2.\n\n5.3\n\nLower Bounds on the Asymptotic Parity-Check Density\n\nThe lower bound on the parity-check density derived in Theorem 4.1 enables to assess the tradeoff\nbetween asymptotic performance and asymptotic decoding complexity (per iteration) of an iterative\nmessage-passing decoder. This bound tightens the lower bound on the asymptotic parity-check\ndensity derived in [13, Theorem 2.1]. Fig. 4 compares these bounds for codes of rate 12 (left plot)\nb\nand 34 (right plot) where the bounds are plotted as a function of E\nN0 . It can be observed from\nEb\nFig. 4 that as N0 increases, the advantage of the bound in Theorem 4.1 over the bound in [13,\nEb\nis increased, the twoTheorem 2.1] diminishes. This follows from the fact that as the value of N\n0\nlevel quantization of the LLR used in [1] and [13, Theorem 2.1] better captures the true behavior\nof the MBIOS channel. It is also reflected in this figure that as \u03b5 tends to zero (i.e., when the gap\nto capacity vanishes), the slope of the bounds becomes very sharp. This is due to the logarithmic\nbehavior of the bounds.\n\n\f12\n\n9\n\nLower Bound in Theorem 4.1\nLower Bound [13, Theorem 2.1]\n\nLower Bound in Theorem 4.1\nLower Bound [13, Theorem 2.1]\n\n11\n\n8\n\n10\n\n7\n\n9\nParity\u2212Check Density\n\nParity\u2212Check Density\n\n6\n\n8\n\n7\n\n6\n\n5\n\n4\n\n3\n\n5\n2\n\n4\n\n1\n\n3\n\n2\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\nEb/No [dB]\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n1.6\n\n1.8\n\n2\n\n2.2\nEb/No [dB]\n\n2.4\n\n2.6\n\n2.8\n\nFigure 4: Comparison between lower bounds on the asymptotic parity-check density of binary\nlinear block codes where the transmission takes place over a binary-input AWGN channel. The\ndashed line refers to [13, Theorem 2.1], and the solid line refers to Theorem 4.1. The left and right\nplots refer to code rates of 12 and 43 , respectively. The Shannon capacity limit for these code rates\nEb\ncorresponds to N\nof 0.187 dB and 1.626 dB, respectively.\n0\n\n6\n\nSummary and Outlook\n\nWe derive improved lower bounds on the asymptotic density of parity-check matrices and upper\nbounds on the achievable rates of binary linear block codes transmitted over memoryless binaryinput output-symmetric (MBIOS) channels. The improvements are w.r.t. the bounds given in\n[1, 13]. The information-theoretic bounds are valid for every sequence of binary linear block codes,\nin contrast to high probability results which follow from probabilistic analytical tools (e.g., density\nevolution (DE) analysis under iterative decoding). The bounds hold under optimal ML decoding,\nand hence, they hold in particular under any sub-optimal decoding algorithm. We apply the\nbounds to ensembles of low-density parity-check (LDPC) codes. The significance of the bounds is\nthe following: firstly, by comparing the new upper bounds on the achievable rates with thresholds\nprovided by DE analysis, we obtain rigorous bounds on the inherent loss in performance of various\nLDPC ensembles. This degradation in the asymptotic performance is due to the sub-optimality of\niterative message-passing decoding (as compared to optimal ML decoding). Secondly, the paritycheck density can be interpreted as the complexity per iteration under iterative message-passing\ndecoding. Therefore, by tightening the reported lower bound on the asymptotic parity-check density\n[13, Theorem 2.1], the new bounds provide better insight on the tradeoff between the asymptotic\nperformance and the asymptotic decoding complexity of iteratively decoded LDPC codes. Thirdly,\nthe new lower bound on the bit error probability of binary linear block codes tightens the reported\nlower bound in [13, Theorem 2.5].\nThe derivation of the bounds in Section 3 was motivated by the desire to generalize the results in\n[1, Theorems 1 and 2] and [13, Theorem 2.1]. The two-level quantization of the log-likelihood ratio\n(LLR) which in essence replaces the arbitrary MBIOS channel by a physically degraded binary\nsymmetric channel (BSC), is modified in Section 3 to a quantized channel which better reflects\nthe statistics of the original channel (though the quantized channel is still physically degraded\nw.r.t. the original channel). The number of quantization levels at the output of the new channel\nis an arbitrary integer power of 2. The calculation of the bounds in Section 3 is subject to an\noptimization of the quantization levels of the LLR, as to get the tightest bounds within their form.\n\n\fIn Section 4, we rely on the conditional pdf of the LLR at the output of the MBIOS channel, and\noperate on an equivalent channel without quantizing the LLR. This second approach finally leads\nto bounds which are uniformly tighter than the bounds we derive in Section 3. It appears to be\neven simpler to calculate the un-quantized bounds in Section 4, as their calculation do not involve\nthe solution of any optimization equation (in contrast to the quantized bounds, whose calculation\ninvolves a numerical solution of optimization equations w.r.t. the quantization levels of the LLR).\nThe comparison between the quantized and un-quantized bounds gives insight on the effect of the\nnumber of quantization levels of the LLR (even if they are chosen optimally) on the achievable rates,\nas compared to the ideal case where no quantization is done. The results of such a comparison are\nshown in Tables 1\u20133, and indicate that the improvement in the tightness of the bounds when more\nthan 8 levels of quantization are used (in case the quantization levels are optimally determined) is\nmarginal. We also note that practically, the possibility to calculate un-quantized bounds which are\nuniformly better than the quantized bounds was facilitated due to an efficient transformation of the\nmulti-dimensional integral in Appendix C.3 into an infinite series of one-dimensional integrals whose\nconvergence rate is very fast (10 terms are sufficient in practice, as justified in Appendix C.3). Had\nwe used instead an upper bound on h2 (*) like the one in Lemma 3.2 (see p. 10),4 then this would\nloosen the un-quantized bounds, and make them sometimes even worse than the 8-level quantized\nbounds. The ability to express the k-dimensional integral in Appendix C.3 in a closed form was\ntherefore crucial for practically obtaining an un-quantized bound which is uniformly tighter than\nthe quantized bounds for an arbitrary number of quantization levels. As mentioned before, the\nexact calculation of the un-quantized bound also provides insight on the effect of the number of\nquantization levels on the tightness of the quantized bounds (see Tables 1\u20133 in Section 5).\nOur bounds on the thresholds of LDPC ensembles under optimal ML decoding depend only on\nthe degree distribution of their parity-check nodes and their design rate. For a given parity-check\ndegree distribution (\u03c1) and design rate (R), the bounds provide an indication on the inherent gap\nto capacity which is independent of the choice of \u03bb (as long as the pair of degree distributions (\u03bb, \u03c1)\nyield the design rate R). Therefore, our bounds are not expected to be tight for LDPC ensembles\nwith a given pair of degree distributions (\u03bb, \u03c1). The numerical results shown in Tables 1\u20133 indicate,\nhowever, that these bounds are useful for assessing the inherent gap to capacity of various (regular\nand irregular) LDPC ensembles. The comparison of our bounds with the DE thresholds (based on\ndensity evolution) provides an assessment of the degradation in the asymptotic performance due to\nthe sub-optimality of the iterative sum-product decoding algorithm. We note that the gap of LDPC\nensembles to capacity is an inherent phenomenon, due to their finite average-right degree [13]. As\na topic for further research, it is suggested to examine the possibility of tightening the bounds for\nspecific ensembles by explicitly taking into account the exact characterization of \u03bb. We also suggest\nto study a possible generalization of the bounds to non-binary linear block codes. These generalized\nbounds can be applied to the analysis of the ML performance of non-binary LDPC ensembles whose\ntransmission takes place over arbitrary discrete memoryless channels with possibly different types\nof quantization [3].\nThe lower bound on the asymptotic parity-check density in [13, Theorem 2.1] and its improvements in Sections 3 and 4 grow like the log of the inverse of the gap (in rate) to capacity. The result\nin [13, Theorem 2.3] shows that a logarithmic growth rate of the parity-check density is achievable for Gallager's regular LDPC ensemble under ML decoding when transmission takes place over\nan arbitrary MBIOS channel. These results show that for any iterative decoder which is based\non the representation of the codes by Tanner graphs, there exists a tradeoff between asymptotic\nperformance and complexity which cannot be surpassed. Recently, it was shown in [9] that better\ntradeoffs can be achieved by allowing more complicated graphical models; for the particular case of\n4\n\nThis was actually the way we used to calculate the un-quantized bounds at the beginning, due to an initial\ndifficulty in calculating the k dimensional integral in Appendix C.3.\n\n\fthe binary erasure channel (BEC), the encoding and the decoding complexity of properly designed\ncodes on graphs remain bounded as the gap to capacity vanishes. To this end, Pfister, Sason and\nUrbanke consider in [9, Theorems 1 and 2] ensembles of irregular repeat-accumulate codes which\ninvolve punctured bits, and allow in this way a sufficient number of state nodes in the Tanner graph\nrepresenting the codes. This surprising result is considered in [9, Theorem 4], by a derivation of\nan information-theoretic lower bound on the decoding complexity of randomly punctured codes\non graphs whose transmission takes place over MBIOS channels. The approach for the derivation\nof the bounds in [9, Theorem 4] rely on the analysis in [13, Theorem 2.1]. As a topic for further\nresearch, we suggest to tighten the lower bounds in [9, Theorem 4] by relying on the approach used\nto prove the improved lower bound on the parity-check density stated in Theorem 4.1 (as compared\nto the derivation of the bound in [9, Theorem 4] which relies on the analysis in [13, Theorem 2.1]).\n\nAppendix A\nA.1\n\nProof of Lemma 3.1\n(i)\n\nIn order to prove Lemma 3.1, let Pb designate the bit error probability of the i-th symbol in the\nnR\nX\n1\nPb,i is the average bit error probability of the\ncode C (where 1 \u2264 i \u2264 n). Therefore, Pb = nR\ni=1\n\ncode, and\n\nH(X | Y)\nn\n\n(a)\n\n=\n\n(b)\n\n\u2264\n\n(c)\n\n=\n\n(d)\n\n\u2264\n\nn\nX\n\nH(xi | Y, x1 , x2 , . . . , xi\u22121 )\n\nnR\nX\n\nH(xi | Y)\n\nnR\nX\n\nH(xi | Y)\n\nnR\nX\n\nh2 (Pb,i )\n\ni=1\n\nn\n\ni=1\n\ni=nR+1\n\nn\n\nn\n\ni=1\n\nn\nnR\n\nR h2\n\n1 X\nPb,i\nnR\ni=1\n\n=\n\nH(xi | x1 , x2 , . . . , xnR )\n\ni=1\n\n(e)\n\n\u2264\n\n+\n\nn\n\nn\nX\n\n!\n\nR h2 (Pb )\n\nwhere equality (a) follows from the chain rule of the entropy, inequality (b) is due to the fact that\nconditioning reduces the entropy, equality (c) follows since the dimension of the code is nR which\nimplies that there is a set of nR bits of the codeword whose knowledge reveals the entire codeword\n(w.l.g., one can assume that these are the first nR bits), inequality (d) is based on Fano's inequality\nand since the code is binary, and inequality (e) is based on Jensen's inequality and the concavity\nof the binary entropy function.\n\n\fA.2\n\nDerivation of the Optimization Equation in (22) and Proving the Existence\nof its Solution\n\nDerivation of the optimization equation (22): We derive here the optimization equation (22) which\nrefers to the \"four-level quantization\" lower bound on the parity-check density (see p. 10).\nLet a(*) designate the conditional pdf of the LLR at the output of the original MBIOS channel,\ngiven the zero symbol is transmitted. In the following, we express the transition probabilities of\nthe degraded channel in Fig. 1 (see p. 6) in terms of a(*) and the value of l:\nZ \u221e\na(u) du\n(A.1)\np0 = Pr(Z = 0 | X = 0) =\nl\n\np1 = Pr(Z = \u03b1 | X = 0) =\n\nZ\n\nl\n\na(u) du +\n0+\n\np2 = Pr(Z = 1 + \u03b1 | X = 0) =\np3 = Pr(Z = 1 | X = 0) =\n\nZ\n\nZ\n\n1\n2\n\n0\u2212\n\n0+\n\n0\u2212\n\na(u) du +\n\n\u2212l\n\n\u2212l\n\nZ\n\n1\n2\n\na(u) du\n\n(A.2)\n\nZ\n\n(A.3)\n\n0+\n\na(u) du\n0\u2212\n\na(u) du.\n\n(A.4)\n\n\u2212\u221e\n\nWe note that the integration of a(*) from u = 0\u2212 to u = 0+ is meaningful if and only if there is a\nnon-vanishing probability that the value of the LLR at the output of the original channel is zero\n(e.g., a BEC). Otherwise, the contribution of this integral to (A.2) and (A.3) vanishes. Since the\nchannel is MBIOS, the symmetry property [10] gives\na(u) = eu a(\u2212u),\n\n\u2200 u \u2208 R.\n\n(A.5)\n\nBased on the expressions for the coefficients K1 and K2 in the lower bound on the asymptotic\nparity-check density (20), then in order to find the tightest lower bound then we need to maximize\n(p1 \u2212 p2 )2 (p0 \u2212 p3 )2\n+\np1 + p2\np0 + p3\n\n(A.6)\n\nw.r.t. the free parameter l \u2208 R+ . From Eqs. (A.1)\u2013(A.4) and the symmetry property in (A.5)\nZ \u221e\n\u2202\na(u)(1 \u2212 e\u2212u ) du \u21d2\np0 \u2212 p3 =\n(p0 \u2212 p3 ) = \u2212a(l)(1 \u2212 e\u2212l )\n(A.7)\n\u2202l\nl\nZ \u221e\n\u2202\n(p0 + p3 ) = \u2212a(l)(1 + e\u2212l )\n(A.8)\na(u)(1 + e\u2212u ) du \u21d2\np0 + p3 =\n\u2202l\nl\nZ l\n\u2202\n(p1 \u2212 p2 ) = a(l)(1 \u2212 e\u2212l )\n(A.9)\na(u)(1 \u2212 e\u2212u ) du \u21d2\np1 \u2212 p2 =\n\u2202l\n+\n0\nZ l\n\u2202\na(u)(1 + e\u2212u ) du \u21d2\np1 + p2 =\n(p1 + p2 ) = a(l)(1 + e\u2212l )\n(A.10)\n\u2202l\n+\n0\nso the calculation of the partial derivative of (A.6) w.r.t. l gives\n\u001b\n\u001a\n\u2202 (p1 \u2212 p2 )2 (p0 \u2212 p3 )2\n+\n\u2202l\np1 + p2\np0 + p3\n(\"\u0012\n\"\u0012\n\u00132 \u0012\n\u00132 #\n\u00132 \u0012\n\u00132 #)\np2\np3\np\np\n0\n1\n= \u22124 a(l)\n\u2212\n+ e\u2212l\n\u2212\n.\np1 + p2\np0 + p3\np1 + p2\np0 + p3\nSince the first derivative of a function changes its sign at a neighborhood of any local maxima or\nminima point, and since a(*) is always non-negative, then the second multiplicative term above is\n\n\fthe one which changes its sign at a neighborhood of l maximizing (A.6). For this value of l, the\nsecond multiplicative term vanishes, which gives the optimization equation for l in (22).\nProof of existence of a solution to (22): In order to show that a solution to (22) always exists,\nwe will see how the LHS and the RHS of this equation behave as l \u2192 0+ and l \u2192 \u221e. From\n(A.1)\u2013(A.4), it follows that in the limit where l \u2192 \u221e, we get\np1 \u2192 1 \u2212 w \u2212 Pr(LLR(Y ) = \u221e | X = 0),\n\np2 \u2192 w\n\nwhere w is introduced in (2), and therefore\n\u0012\n\u00132\nw\np22 + e\u2212l p21\n=\n.\nlim\nl\u2192\u221e (p1 + p2 )2\n1 \u2212 Pr(LLR(Y ) = \u221e | X = 0)\nSince from the symmetry property\nZ\nZ \u221e\na(\u2212u)du =\np3 =\np3\np0\n\n\u2212u\n\ne\n\n\u2212l\n\na(u)du \u2264 e\n\nl\n\nl\n\nthen the fraction\n\n\u221e\n\nZ\n\n\u221e\n\n(A.11)\n\na(u)du = e\u2212l p0\n\nl\n\ntends to zero as l \u2192 \u221e, so\n+ e\u2212l p20\nlim\nl\u2192\u221e (p0 + p3 )2\np23\n\n\u0010 \u00112\np3\np0\n\n= lim \u0010\nl\u2192\u221e\n1+\n\n+ e\u2212l\n\u00112 = 0.\np3\np0\n\n(A.12)\n\nIt therefore follows from (A.11) and (A.12) that for large enough values of l, the LHS of (22) is\nlarger than the RHS of this equation. On the other hand, in the limit where l \u2192 0+ , we get\nZ +\n1 0\na(u)du\np1 , p2 \u2192\n2 0\u2212\nand therefore\nlim\n\nl\u21920+\n\n1\np22 + e\u2212l p21\n= .\n2\n(p1 + p2 )\n2\n\n(A.13)\n\nIn the limit where l \u2192 0+\np0 \u2192\nwhere \u03b2 , 1 \u2212\n\nZ\n\nZ\n\n\u221e\n\na(u)du,\n\n0+\n\np3 \u2192\n\na(u)du. By denoting u ,\n\n0\u2212\n\nl\u21920+\n\n0\u2212\n\na(u)du,\n\np0 + p3 \u2192 \u03b2\n\n\u2212\u221e\n\n0+\n\nlim\n\nZ\n\nZ\n\n\u221e\n\na(u)du, we get 0 \u2264 u \u2264 \u03b2, and\n\n0+\n\np23 + e\u2212l p20\nu2 + (\u03b2 \u2212 u)2\n1\n=\n\u2265 ,\n2\n2\n(p0 + p3 )\n\u03b2\n2\n\n\u2200 u \u2208 [0, \u03b2].\n\n(A.14)\n\nWe note that the last inequality holds in equality if and only if u = \u03b22 . But if this condition holds,\nthen this implies that\nZ 0\u2212\nZ \u221e\na(u) du\na(u) du =\n\u2212\u221e\n\n0+\n\nwhich from the symmetry property cannot be satisfied unless a(u) = \u03b4(u). The latter condition\ncorresponds to a BEC with erasure probability 1 (whose capacity is equal to zero).\n\nFrom (A.13) and (A.14), we obtain that for small enough (and non-negative) values of l, the\nLHS of (22) is less or equal to the RHS of this equation. Since we also obtained that for large\nenough l, the LHS of (22) is larger than the RHS of this equation, the existence of a solution to\n(22) follows from continuity considerations.\n\n\fA.3\n\nProof of Inequality (30)\n\nWe prove here the inequality (30) (see p. 12) which implies that the \"four-level quantization\" lower\nbound on the parity-check density (see p. 10) is tighter than what can be interpreted as the \"two\nlevels quantization\" bound in [13, Theorem 2.1]. Based on (2), we get\nw = Pr{LLR(Y ) < 0 | X = 0} +\n\n1\nPr{LLR(Y ) = 0 | X = 0}\n2\n\nso from (6), w = p2 + p3 . By invoking Jensen's inequality, we get\n(p1 \u2212 p2 )2 (p0 \u2212 p3 )2\n+\np1 + p2\np0 + p3\n\u0012\n\u0012\n\u0013\n\u0013\np1 \u2212 p2 2\np0 \u2212 p3 2\n= (p1 + p2 )\n+ (p0 + p3 )\np1 + p2\np0 + p3\n\u0013\n\u0012\n\u0013\u0015\n\u0014\n\u0012\np0 \u2212 p3 2\np1 \u2212 p2\n+ (p0 + p3 )\n\u2265 (p1 + p2 )\np1 + p2\np0 + p3\n= (p0 + p1 \u2212 p2 \u2212 p3 )2\n\n= (1 \u2212 2p2 \u2212 2p3 )2\n= (1 \u2212 2w)2 .\np1 \u2212p2\np1 +p2\n\nAn equality is achieved if and only if\n\nZ\n\nl\n\np1 \u2212 p2\n+\n= Z0 l\np1 + p2\n\n=\n\np0 \u2212p3\np0 +p3 .\n\nFrom (A.7)\u2013(A.10), we get\n\na(u)(1 \u2212 e\u2212u ) du\n\u2264\n\n1 \u2212 e\u2212l\n1 + e\u2212l\n\n\u2265\n\n1 \u2212 e\u2212l\n.\n1 + e\u2212l\n\na(u)(1 + e\u2212u ) du\n\n0+\n\nand\nZ\n\n\u221e\n\np0 \u2212 p3\n= Zl \u221e\np0 + p3\n\na(u)(1 \u2212 e\u2212u ) du\na(u)(1 + e\u2212u ) du\n\nl\n\n\u2212p2\n3\nand pp00 \u2212p\nThe two fractions pp11 +p\n+p3 cannot be equal unless the LLR is either equal to l or \u2212l. This\n2\nmakes the four-level quantization of the LLR identical to the two-level quantization used for the\nderivation of the original bound in [1, Theorem 2]. Equality can be also achieved if p1 + p2 = 0 or\np0 + p3 = 0 which converts the channel model in Fig. 1 (see p. 6) to a BSC.\n\n\fAppendix B\nB.1\n\nProof of Lemma 3.3\n\nLemma 3.3 (see p. 16) is proved here by mathematical induction on the value of k.\nk = 1: let s\u0303 denote the value of s for which ks = 1. In this case, we simply need to find the\nprobability that the scalars \u0398 and X differ. From (35)\no\nn\n(s\u0303) (s\u0303)\n(s\u0303)\nPr \u0398 = X | \u03a6 = (a1 , a2 , . . . , ad\u22121 )\nps\u0303\n=\nps\u0303 + p2d \u22121\u2212s\u0303\n\u0013\u0015\n\u0014\n\u0012\n2p2d \u22121\u2212s\u0303\n1\n=\n1+ 1\u2212\n2\nps\u0303 + p2d \u22121\u2212s\u0303\n\uf8f9\n\uf8ee\n\u0013k s\n2d\u22121\n\u22121 \u0012\nY\n2p2d \u22121\u2212s\n1\n\uf8fb.\n= \uf8f01 +\n1\u2212\n2\nps + p2d \u22121\u2212s\ns=0\n\nLet us assume that for every k < k\u2032 the claim holds, and prove it for k = k\u2032 . Let s\u0303 denote the value\n(s\u0303) (s\u0303)\n(s\u0303)\nof s for which \u03a6i1 = (a1 , a2 , . . . , ad\u22121 ). The probability that the components of the two random\nvectors (\u0398i1 , \u0398i2 , . . . , \u0398ik\u2032 ) and (Xi1 , Xi2 , . . . , Xik\u2032 ) differ in an even number of indices is equal to\n\u0001\n\u0001\nq1 (s\u0303) q2 (k\u2032 , s\u0303) + 1 \u2212 q1 (s\u0303) 1 \u2212 q2 (k\u2032 , s\u0303)\nwhere q1 (s\u0303) designates the probability that \u0398i1 = Xi1 , and q2 (k\u2032 , s\u0303) designates the probability that\nthe components of the two random vectors (\u0398i2 , . . . , \u0398ik\u2032 ) and (Xi2 , . . . , Xik\u2032 ) differ in an even\nnumber of indices. Based on the assumption, we get\n\u0013\u0015\n\u0014\n\u0012\n2p2d \u22121\u2212s\u0303\n1\n1+ 1\u2212\nq1 (s\u0303) =\n2\nps\u0303 + p2d \u22121\u2212s\u0303\n\uf8ee\n\uf8f9\n\u0012\n\u0013ks\u0303 \u22121 2d\u22121\n\u0013k s\n\u22121 \u0012\nY\n2p2d \u22121\u2212s\u0303\n2p2d \u22121\u2212s\n1 \uf8ef\n\uf8fa\n*\nq2 (k\u2032 , s\u0303) =\n1\u2212\n\uf8f01 + 1 \u2212\n\uf8fb\n2\nps\u0303 + p2d \u22121\u2212s\u0303\nps + p2d \u22121\u2212s\ns=0\ns6=s\u0303\n\nso the probability that (\u0398i1 , . . . , \u0398ik\u2032 ) and (Xi1 , . . . , Xik\u2032 ) differ in an even number of indices is\n\uf8ee\n\uf8f9\n\u0013k s\n2d\u22121\nY\u22121 \u0012\n\u0001\n\u0001\n2p\n1\nd\n2 \u22121\u2212s\n\uf8fb.\nq1 (s\u0303) q2 (k\u2032 , s\u0303) + 1 \u2212 q1 (s\u0303) 1 \u2212 q2 (k\u2032 , s\u0303) = \uf8f01 +\n1\u2212\n2\nps + p2d \u22121\u2212s\ns=0\n\nThis completes the proof of Lemma 3.3.\n\n\fB.2\n\nProof of the Property Claimed in the Discussion on Proposition 3.2\n\nFollowing the discussion on Proposition 3.2 (see p. 17), we prove the existence of sub-optimal 2d+1\nquantization levels, determined by  \u0303\nl1 , . . . ,  \u0303l2d \u22121 and their symmetric values around zero, so that\nd+1\neven with this sub-optimal 2 -level quantization, the bound in the RHS of (34) is already tighter\nthan the one which follows from the optimal choice of 2d quantization levels. From the RHS of\n(34), it suffices to show that for any integer k \u2265 2\n\uf8f1\nd \u22121\n\u0013 2Y\nX \uf8f2\u0012\n\u0001k\nk\np\u0303i + p\u03032d+1 \u22121\u2212i i\n\uf8f3 k0 , . . . , k2d \u22121\ni=0\n\nk0 ,...,k2d \u22121\nP\ni ki =k\n\n\uf8f9\uf8f6\uf8fc\n\uf8eb \uf8ee\nd \u22121 \u0012\n\u0013k i\n2Y\n\uf8fd\n2p\u0303\nd+1\n1\n2\n\u22121\u2212i\n\uf8fb\uf8f8\n1\u2212\n*h2 \uf8ed \uf8f01 \u2212\n\uf8fe\n2\np\u0303i + p\u03032d+1 \u22121\u2212i\ni=0\n\n\u2264\n\nX\n\nk0 ,...,k2d\u22121 \u22121\nP\ni ki =k\n\n\uf8f1\n\uf8f2\u0012\n\nk\n\uf8f3 k0 , . . . , k2d\u22121 \u22121\n\uf8eb \uf8ee\n\n\uf8ec1 \uf8ef\n*h2 \uf8ed \uf8f01 \u2212\n2\n\n\u0013 2d\u22121\n\u0011ki\nY\u22121 \u0010 (d)\n(d)\npi + p2d \u22121\u2212i\ni=0\n\n2d\u22121\nY\u22121\ni=0\n\n\uf8eb\n\n\uf8ed1 \u2212\n\n\uf8f6 \uf8f9\uf8f6\uf8fc\n\nki\n\uf8f4\n(d)\n\uf8fd\n2p2d \u22121\u2212i\n\uf8f7\n\uf8f8 \uf8fa\n\uf8f8\n\uf8fb\n(d)\n(d)\n\uf8f4\n\uf8fe\npi + p2d \u22121\u2212i\n\n(B.1)\n\nwhere p\u03030 , p\u03031 , . . . , p\u03032d+1 \u22121 denote the transition probabilities, as defined in (33), which are associated\n(d) (d)\n(d)\nwith the above sub-optimal 2d+1 quantization levels. On the other hand, p0 , p1 , . . . , p2d \u22121 denote\nthe transition probabilities in (33) which correspond to the optimal 2d quantization levels.\nTo prove (B.1), we define sub-optimal quantization levels  \u0303l1 , . . . ,  \u0303l2d \u22121 in the following way: For\n(d)\n(d)\n(d)\ni = 1, 2, . . . , 2d\u22121 \u2212 1, we define  \u0303l2i , li , where l1 , . . . , l2d\u22121 \u22121 (and their symmetric values around\nzero) are the optimal 2d quantization levels. The other levels (i.e., l \u0303j where the index j is odd) are\nchosen arbitrarily as long as\n\n\u221e ,  \u0303l0 \u2265  \u0303l1 \u2265 . . . \u2265  \u0303l2d \u22121 \u2265 0.\n\nIn a similar way to (33), let us the denote by p\u03030 , . . . , p\u03032d+1 \u22121 the transition probabilities associated\nwith  \u0303l1 ,  \u0303l2 , . . . ,  \u0303l2d \u22121 . This yields\n\uf8f9\uf8f6\n\uf8eb \uf8ee\nd \u22121 \u0012\nd \u22121\n\u0013k i\n\u0013 2Y\n2Y\nX \u0012\n2p\u03032d+1 \u22121\u2212i\nk\n1\n\uf8fb\uf8f8\n1\u2212\n(p\u0303i + p\u03032d+1 \u22121\u2212i )ki h2 \uf8ed \uf8f01 \u2212\n2\np\u0303i + p\u03032d+1 \u22121\u2212i\nk0 , . . . , k2d \u22121\nk0 ,...,k2d \u22121\nP\ni ki =k\n\n=\n\nX\n\nk0 ,...,k2d \u22121\nP\ni ki =k\n\n\uf8f1\n\uf8f2\u0012\n\nk\n\uf8f3 k0 + k1 , . . . , k2d \u22122 + k2d \u22121\nd \u22121\n2Y\n\n(p\u0303i + p\u03032d+1 \u22121\u2212i )ki\n\ni=0\n\nLet us denote\n\ni=0\n\ni=0\n\n\u0013 2d\u22121\nY\u22121 \u0012\nj=0\n\nk2j + k2j+1\nk2j\n\n\u0013\n\n\uf8f9\uf8f6\uf8fc\n\uf8eb \uf8ee\nd \u22121 \u0012\n\u0013k i\n2Y\n\uf8fd\n2p\u03032d+1 \u22121\u2212i\n1\n\uf8fb\uf8f8 .\n1\u2212\nh2 \uf8ed \uf8f01 \u2212\n\uf8fe\n2\np\u0303i + p\u03032d+1 \u22121\u2212i\ni=0\n\nki\u2032 = k2i + k2i+1 ,\n\ni = 0, 1, . . . , 2d\u22121 \u2212 1\n\n\fthen, the above sum transforms to\n(\u0012\n\u0013\nX\nk\nk0\u2032 , . . . , k2\u2032 d\u22121 \u22121\n\u2032\n\u2032\nk0 ,...,k d\u22121\nP 2\u2032 \u22121\ni ki =k\n\nX\n\nk0 ,k1 ,...,k2d \u22121\n\u2200j: k2j +k2j+1 =kj\u2032\n\n2d\u22121\nY\u22121 \u0012\nj=0\n\nkj\u2032\nk2j\n\n\u0013\n\np\u03032j + p\u03032d+1 \u22121\u22122j\n\n\u0001k2j \u0010\n\np\u03032j+1 + p\u03032d+1 \u22121\u2212(2j+1)\n\n\uf8eb \uf8ee\n\u0013k2j\n2d\u22121\nY\u22121 \u0012\n2p\u03032d+1 \u22121\u22122j\n1\nh2 \uf8ed \uf8f01 \u2212\n1\u2212\n2\np\u03032j + p\u03032d+1 \u22121\u22122j\nj=0\n\n=\n\nX\n\nk0\u2032 ,...,k \u2032 d\u22121\nP 2\u2032 \u22121\ni ki =k\n\n\uf8f1\n\uf8f2\u0012\n\nk\n\uf8f3 k0\u2032 , . . . , k2\u2032 d\u22121 \u22121\n\nX\n\nk0 ,k1 ,...,k2d \u22121\n\u2200j: k2j +k2j+1 =kj\u2032\n\n2d\u22121\nY\u22121 \u0012\nj=0\n\n\u0013 2d\u22121\n\u0011kj\u2032\nY\u22121 \u0010 (d)\n(d)\npj + p2d \u22121\u2212j\n\n\u0011k2j+1\n\n!k2j+1 \uf8f9\uf8f6\uf8fc\n\uf8fd\n\uf8fb\uf8f8\n1\u2212\n\uf8fe\np\u03032j+1 + p\u03032d+1 \u22121\u2212(2j+1)\n2p\u03032d +1\u22121\u2212(2j+1)\n\nj=0\n\n\u0013\nkj\u2032 (p\u03032j + p\u03032d+1 \u22121\u22122j )k2j (p\u03032j+1 + p\u03032d+1 \u22121\u2212(2j+1) )k2j+1\n\u0011kj\u2032\n\u0010\nk2j\n(d)\n(d)\npj + p2d \u22121\u2212j\n\n\uf8eb \uf8ee\n\u0013k2j\n2d\u22121\nY\u22121 \u0012\n2p\u03032d+1 \u22121\u22122j\n1\n1\u2212\nh2 \uf8ed \uf8f01 \u2212\n2\np\u03032j + p\u03032d+1 \u22121\u22122j\nj=0\n\n2p\u03032d+1 \u22121\u2212(2j+1)\n1\u2212\np\u03032j+1 + p\u03032d+1 \u22121\u2212(2j+1)\n\n!k2j+1 \uf8f9\uf8f6\uf8fc\n\uf8fd\n\uf8fb\uf8f8 .\n\uf8fe\n(B.2)\n\n(d)\nDue to the choice of  \u0303lj , we have that pj = p\u03032j + p\u03032j+1 for any j = 0, 1, . . . , 2d \u2212 1. Hence,\n\nX\n\nk0 ,k1 ,...,k2d \u22121\n\u2200j: k2j +k2j+1 =kj\u2032\n(a)\n\n=\n\n2d\u22121\nY\u22121\nj=0\n\n=\n\n2d\u22121\nY\u22121\nj=0\n\n=\n\n2d\u22121\nY\u22121\nj=0\n\n2d\u22121\nY\u22121 \u0012\nj=0\n\nX\n\nk2j ,k2j+1\nk2j +k2j+1 =kj\u2032\n\n\u0012\n\n\u0013\nkj\u2032 (p\u03032j + p\u03032d+1 \u22121\u22122j )k2j (p\u03032j+1 + p\u03032d+1 \u22121\u2212(2j+1) )k2j+1\n\u0011kj\u2032\n\u0010\nk2j\n(d)\n(d)\npj + p2d \u22121\u2212j\n\n(B.3)\n\n\u0013\nkj\u2032 (p\u03032j + p\u03032d+1 \u22121\u22122j )k2j (p\u03032j+1 + p\u03032d+1 \u22121\u2212(2j+1) )k2j+1\n\u0010\n\u0011kj\u2032\nk2j\n(d)\n(d)\npj + p2d \u22121\u2212j\n\n\u0010\n\u0011kj'\n(p\u03032j + p\u03032j+1 ) + (p\u03032d+1 \u22121\u22122j + p\u03032d+1 \u22121\u2212(2j+1) )\n\u0011kj\u2032\n\u0010\n(d)\n(d)\npj + p2d \u22121\u2212j\n\u0011kj\u2032\n\u0010\n(d)\n(d)\npj + p2d \u22121\u2212j\n\u0010\n\u0011kj\u2032 = 1\n(d)\n(d)\npj + p2d \u22121\u2212j\n\nwhere the factorization, in (a), of the sum over the global function (whose variables are k0 , k1 , . . . , k2d \u22121 )\ninto a product of sums of local functions (with variables k2j , k2j+1 ) follows from the concept of factor\ngraphs. Therefore, the expression inside the sum in (B.3) forms a probability distribution.\n\n\fUsing the concavity of h2 (*), we apply Jensen's inequality to (B.2) (which is equal to the RHS\nof (B.1)) and get\n\uf8f9\uf8f6\n\uf8eb \uf8ee\nd \u22121 \u0012\nd \u22121\n\u0013k i\n\u0013 2Y\n2Y\nX \u0012\n2p\u03032d+1 \u22121\u2212i\nk\n1\n\uf8fb\uf8f8\n1\u2212\n(p\u0303i + p\u03032d+1 \u22121\u2212i )ki h2 \uf8ed \uf8f01 \u2212\n2\np\u0303i + p\u03032d+1 \u22121\u2212i\nk0 , . . . , k2d \u22121\n\u2264\n\nX\n\nk0\u2032 ,...,k \u2032 d\u22121\nP 2\u2032 \u22121\ni ki =k\n\nh2\n\n\u0012\n\nk\n\u2032\nk0 , . . . , k2\u2032 d\u22121 \u22121\n\n\"\n2d\u22121\nY\u22121\n1\n1\u2212\n2\nj=0\n\n\u0012\n=\n\nX\n\nk0\u2032 ,...,k \u2032 d\u22121\nP 2\u2032 \u22121\ni ki =k\n\nh2\n\n\u0012\n\nk0\u2032 ,...,k \u2032 d\u22121\nP 2\u2032 \u22121\ni ki =k\n\n\u0012\n\n\uf8eb \uf8ee\n\n\uf8ec1 \uf8ef\nh2 \uf8ed \uf8f01 \u2212\n2\n=\n\nX\n\nk0\u2032 ,...,k \u2032 d\u22121\nP 2\u2032 \u22121\ni ki =k\n\nk2j ,k2j+1\nk2j +k2j+1 =kj\u2032\n\n\u0012\n\nj=0\n\n(d)\n\n\u0011kj\u2032\n\n\u0013\nkj\u2032 (p\u03032j + p\u03032d+1 \u22121\u22122j )k2j (p\u03032j+1 + p\u03032d+1 \u22121\u2212(2j+1) )k2j+1\n\u0011kj\u2032\n\u0010\nk2j\n(d)\n(d)\npj + p2d \u22121\u2212j\n\np\u03032j+1 \u2212 p\u03032d+1 \u22121\u2212(2j+1)\np\u03032j+1 + p\u03032d+1 \u22121\u2212(2j+1)\n\n\u0013 2d\u22121\nY\u22121 \u0010\n\n(d)\n\n(d)\n\npj + p2d \u22121\u2212j\n\nj=0\n\nk2j ,k2j+1\nk2j +k2j+1 =kj\u2032\n\n2d\u22121\nY\u22121\n\n\u0012\n\n\u0013k2j\n\nX\n\nk\n\u2032\nk0 , . . . , k2\u2032 d\u22121 \u22121\n\n(d)\n\npj + p2d \u22121\u2212j\n\nj=0\n\nX\n\nk\n\u2032\nk0 , . . . , k2\u2032 d\u22121 \u22121\n\n\"\n2d\u22121\nY\u22121\n1\n1\u2212\n2\n\nX\n\n\u0013 2d\u22121\nY\u22121 \u0010\n\np\u03032j \u2212 p\u03032d+1 \u22121\u22122j\np\u03032j + p\u03032d+1 \u22121\u22122j\n\nj=0\n\n=\n\ni=0\n\ni=0\n\nk0 ,...,k2d \u22121\nP\ni ki =k\n\n\u0012\n\n!k2j+1 #!\n\n\u0011kj\u2032\n\n#!\n\u0013\nkj\u2032 (p\u03032j \u2212 p\u03032d+1 \u22121\u22122j )k2j (p\u03032j+1 \u2212 p\u03032d+1 \u22121\u2212(2j+1) )k2j+1\n\u0011kj'\n\u0010\nk2j\n(d)\n(d)\npj + p2d \u22121\u2212j\n\n\u0013 2d\u22121\nY\u22121 \u0010\n\n(d)\n\n(d)\n\npj + p2d \u22121\u2212j\n\nj=0\n\n\u0011kj\u2032\n\n\u0010\n\u0011kj\u2032 \uf8f9\uf8f6\n(p\u03032j + p\u03032j+1 ) \u2212 (p\u03032d+1 \u22121\u22122j + p\u03032d+1 \u22121\u2212(2j+1) )\n\uf8fa\uf8f7\n\uf8fb\uf8f8\n\u0011kj\u2032\n\u0010\n(d)\n(d)\npj + p2d \u22121\u2212j\n\nk\nk0\u2032 , . . . , k2\u2032 d\u22121 \u22121\n\n\u0013 2d\u22121\nY\u22121 \u0010\nj=0\n\n(d)\n\n(d)\n\npj + p2d \u22121\u2212j\n\n\u0011kj\u2032\n\n\uf8eb \uf8ee\n\n\uf8ec1 \uf8ef\nh2 \uf8ed \uf8f01 \u2212\n2\n\n2d\u22121\nY\u22121\nj=0\n\n\u0010\n\u0011kj\u2032 \uf8f9\uf8f6\n(d)\n(d)\npj \u2212 p2d \u22121\u2212j\n\uf8fa\uf8f7\n\u0011kj\u2032 \uf8fb\uf8f8\n\u0010\n(d)\n(d)\npj + p2d \u22121\u2212j\n\nwhich therefore proves the sufficient condition for monotonicity, as stated in (B.1).\n\nAppendix C\nWe provide in this appendix further mathematical details related to the proof of Proposition 4.1.\nWe note that Appendix C.2 serves here as a preparatory step for the derivation in Appendix C.3.\n\n\fC.1\n\nProof of Lemma 4.1\n\nIn order to prove Lemma 4.1, we first observe that\ne = 1 | \u03a9 = \u03b1)\nPr(\u0398\n\n= Pr(\u0398 = \u22121 | \u03a9 = \u03b1)\na(\u2212\u03b1)\n=\na(\u03b1) + a(\u2212\u03b1)\ne\u2212\u03b1\n=\n.\n1 + e\u2212\u03b1\nThe proof of the lemma continues by mathematical induction. For k = 1, (56) holds since\n\u0013\u0015\n\u0014\n\u0012\ne\u2212\u03b11\n2e\u2212\u03b11\n1\ne i = 1 | \u03a9i = \u03b11 ).\n=\n= Pr(\u0398\n1\u2212 1\u2212\n1\n1\n\u2212\u03b1\n2\n1+e 1\n1 + e\u2212\u03b11\n\nLet us assume that (56) holds for k\u2032 < k, and prove it also for k. By our assumption and since the\nchannel is memoryless, we get\n\u0001\nPr Sj = 1 | (\u03a9i1 , . . . , \u03a9ik ) = (\u03b11 , . . . , \u03b1k )\n\u0001\ne i = 1 | \u03a9i = \u03b11 ) Pr Even number of ones in (\u03a9i , . . . , \u03a9i )\n= Pr(\u0398\n1\n1\n2\nk\n\u0001\ne i = 0 | \u03a9i = \u03b11 ) Pr Odd number of ones in (\u03a9i , . . . , \u03a9i )\n+ Pr(\u0398\n1\n1\n2\nk\n\"\n\"\n\u0012\n\u0013#\n\u0013#!\nk \u0012\nY\n2e\u2212\u03b11\n1\n2e\u2212\u03b1m\n(a) 1\n1\u2212 1\u2212\n* 1\u2212\n1\u2212\n=\n1\u2212\n2\n1 + e\u2212\u03b11\n2\n1 + e\u2212\u03b1m\nm=2\n\"\n\"\n\u0012\n\u0013#\n\u0013#\nk \u0012\nY\n2e\u2212\u03b11\n1\n2e\u2212\u03b1m\n1\n1+ 1\u2212\n1\u2212\n*\n1\u2212\n+\n2\n1 + e\u2212\u03b11\n2\n1 + e\u2212\u03b1m\nm=2\n\"\n\u0013#\nk \u0012\nY\n2e\u2212\u03b1i\n1\n1\u2212\n1\u2212\n=\n2\n1 + e\u2212\u03b1i\ni=1\n\nwhere equality (a) follows from the assumption that equation (56) holds for k \u2212 1, and the proof\nfollows by mathematical induction.\n\nC.2\n\nPower Series Expansion of the Binary Entropy Function\n\nLemma C.1.\n\n\u221e\n\nh2 (x) = 1 \u2212\n\n1 X (1 \u2212 2x)2p\n,\n2 ln 2\np(2p \u2212 1)\n\n0 \u2264 x \u2264 1.\n\n(C.1)\n\np=1\n\nProof. We prove this by expanding the binary entropy function into a power series around 12 . The\nfirst order derivative is\n\u0001\nln 1\u2212x\n\u2032\nx\nh2 (x) =\nln 2\nand the higher order derivatives get the form\n\u0013\n\u0012\n1\n(n \u2212 2)! (\u22121)n\n(n)\n+\n, n = 2, 3, . . . .\nh2 (x) = \u2212\nln 2\nxn\u22121\n(1 \u2212 x)n\u22121\n\n\fThe derivatives of odd degree therefore vanish at x = 12 , and for an even value of n \u2265 2\n\u0012 \u0013\n(n \u2212 2)! 2n\n(n) 1\n.\nh2\n=\u2212\n2\nln 2\nThis yields the following power series expansion of h2 (*) around x = 12 :\n( (n\u22122)! 2n \u0012\n\u0013n )\nX\n1\nln 2\n* x\u2212\nh2 (x) = 1 \u2212\nn!\n2\nn\u22652 even\n\n= 1\u2212\n= 1\u2212\n\n1\nln 2\n\nX\n\nn\u22652 even\n\u221e\nX\n\n1\n2 ln 2\n\np=1\n\n(2x \u2212 1)n\nn(n \u2212 1)\n\n(2x \u2212 1)2p\np(2p \u2212 1)\n\nand this power series converges for all x \u2208 [0, 1].\nWe note that since the power series in (C.1) has always non-negative coefficients, then its\ntruncation always gives an upper bound on the binary entropy function, i.e.,\nm\n\n1 X (1 \u2212 2x)2p\nh2 (x) \u2264 1 \u2212\n2 ln 2 p=1 p(2p \u2212 1)\n\n\u2200 x \u2208 [0, 1], m \u2208 N.\n\n(C.2)\n\nThe case where m = 1 gives the upper bound in Lemma 3.2 which is used in this paper for\nthe derivation of the lower bounds on the parity-check density. The reason for not using a tighter\nversion of the binary entropy function for this case was because otherwise we would get a polynomial\nequation for aR whose solution cannot be given necessarily in closed form. As shown in Fig. 5,\nthe upper bound on the binary entropy function, h2 (*), over the whole interval [0, 1] is improved\nconsiderably by taking even a moderate value for m (e.g., m = 10 gives already a very tight upper\nbound on h2 (*) which deviates from the exact values only at a small neighborhood near the two\nendpoints of this interval).\n\nC.3\n\nCalculation of the Multi-Dimensional Integral in (57)\n\nBased on Lemma C.1 which provides a power series expansion of h2 (*) near 12 , we obtain\n\u0013!!\nZ \u221e Y\nZ \u221eZ \u221e\nk \u0012\nk\nY\n1 \u2212 e\u2212\u03b1m\n1\n1\u2212\nd\u03b11 d\u03b12 . . . d\u03b1k\n...\nf\u03a9 (\u03b1m ) h2\n2\n1 + e\u2212\u03b1m\n0 m=1\n0\n0\nm=1\n\u00132p\nZ \u221eZ \u221e\nZ \u221e Y\nk \u0012\n\u221e\nk\nY\n1 X\n1 \u2212 e\u2212\u03b1m\n1\n=1\u2212\nd\u03b11 d\u03b12 . . . d\u03b1k\nf\u03a9 (\u03b1m )\n...\n2 ln 2\np(2p \u2212 1) 0\n1 + e\u2212\u03b1m\n0\n0\np=1\n\n1\n=1\u2212\n2 ln 2\n1\n=1\u2212\n2 ln 2\n\n\u221e\nX\np=1\n\u221e\nX\np=1\n\nm=1\n\nm=1\n\n1\np(2p \u2212 1)\n1\np(2p \u2212 1)\n\nZ\n\n0\n\n\u221eZ\n\n\u221e\n\n...\n\n0\n\n\u0012Z\n\n0\n\n\u221e\n\nZ\n\n0\n\nk \u0012\n\u221e Y\n\nf\u03a9 (\u03b1m ) tanh\n\nm=1\n\nf\u03a9 (\u03b1) tanh\n\n2p\n\n\u0010\u03b1\u0011\n2\n\n\u0013k\nd\u03b1 .\n\n2p\n\n\u0010 \u03b1 \u0011\u0013\nm\n\n2\n\nd\u03b11 d\u03b12 . . . d\u03b1k\n\nThis transforms the original k-dimensional integral to an infinite sum of one-dimensional integrals.\nSince we are interested to obtain a tight upper bound on the k-dimensional integral above, and\n\n\f1\n\n0.9\n\n0.8\n\n0.7\nh2(x)\nFirst term of the series\n2 terms of the series\n4 terms of the series\n10 terms of the series\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\nx\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 5: Plot of the binary entropy function to base 2 and some upper bounds which are obtained\nby truncating its power series around x = 12 .\nsince all the terms of the last infinite series are positive, then any truncation of the last infinite\nseries is an upper bound. Based on the discussion in Appendix C.2, we will compute the first 10\nterms of this series which (based on the plot in Fig. 5) will give a very tight upper bound on the\nk-dimensional integral (for all k). Hence, for the calculation of the un-quantized bounds on the\nthresholds of LDPC ensembles, we rely in our computations on the following tight upper bound on\nthe k-dimensional integral for any integer k:\n\u0013!!\nZ \u221eZ \u221e\nZ \u221e Y\nk \u0012\nk\nY\n1 \u2212 e\u2212\u03b1m\n1\n1\u2212\nd\u03b11 d\u03b12 . . . d\u03b1k\nfw (\u03b1m ) h2\n...\n2\n1 + e\u2212\u03b1m\n0\n0 m=1\n0\nm=1\n(\n\u0013k )\n\u0012Z \u221e\n10\n\u0010 \u0011\n1 X\n1\n2p \u03b1\n.\n\u22641\u2212\nd\u03b1\nfw (\u03b1) tanh\n2 ln 2\np(2p \u2212 1)\n2\n0\np=1\n\nThis bound is clearly much tighter than the one we use in Section 4 for the derivation of a lower\nbound on the parity-check density, as for the derivation of the latter bound, we only take into\naccount the first term (among the 10 positive terms) of the sum above.\n\nAppendix D\nD.1\n\nProof of Inequality (61)\n\nLet d \u2265 2 be an arbitrary integer and let \u221e , l0 \u2265 l1 \u2265 . . . \u2265 l2d\u22121 \u22121 \u2265 0 (and their symmetric\nvalues around zero) be arbitrarily chosen quantization levels. To prove inequality (61), we start\nby applying the power series expansion of the binary entropy function (C.1) to the RHS of the\n\n\finequality and get\n(\nX\ndk\nk\n\n\u0012\n\nX\n\nk0 ,...,k2d\u22121 \u22121\nP\ni ki =k\n\n\u0013\nk\n*\nk0 , . . . , k2d\u22121 \u22121\n\n\uf8f9\uf8f6 )\n\uf8eb \uf8ee\n\u0013k i\n2d\u22121\n\u22121 \u0012\nY\n2p2d \u22121\u2212i\n1\n\uf8fb\uf8f8\n1\u2212\n(pi + p2d \u22121\u2212i )ki h2 \uf8ed \uf8f01 \u2212\n2\npi + p2d \u22121\u2212i\ni=0\ni=0\n(\n(\n\u0012\n\u0013\n\u221e\nX\nX\n1 X\nk\n1\n=1\u2212\ndk\nk0 , . . . , k2d\u22121 \u22121\n2 ln(2)\np(2p \u2212 1)\n2d\u22121\nY\u22121\n\np=1\n\nk\n\n*\n\nk0 ,...,k2d\u22121 \u22121\nP\ni ki =k\n\n2d\u22121\nY\u22121\n\n(pi + p2d \u22121\u2212i )ki\n\ni=0\n\n\u0012\n\npi \u2212 p2d \u22121\u2212i\npi + p2d \u22121\u2212i\n\n\u0013ki *2p ))\n\n.\n\nTherefore, a sufficient condition for (61) to hold, is that for any integers k \u2265 2 and p \u2265 1\n\u0012 \u0013 \u0013k\n\u0012Z \u221e\nl\n\u2212l\n2p\ndl\na(l)(1 + e ) tanh\n2\n0\n(\n\u0013 2d\u22121\n\u0012\n\u0013ki *2p )\n\u0012\nX\nY\u22121\np\n\u2212\np\nd\nk\ni\n2 \u22121\u2212i\n\u2265\n(pi + p2d \u22121\u2212i )ki\n. (D.1)\nk0 , . . . , k2d\u22121 \u22121\npi + p2d \u22121\u2212i\ni=0\n\nk0 ,...,k2d\u22121 \u22121\nP\ni ki =k\n\nLet f\u03a9 (*) be the pdf of the absolute value of the LLR, as defined in (51) (it is independent of the\ntransmitted symbol X because of the symmetry of the channel). Dividing the range of integration\n[0, \u221e] in the LHS of (D.1) into the 2d\u22121 \u2212 1 sub-intervals defined by the non-negative quantization\nlevels l1 , . . . , l2d\u22121 \u22121 yields\n\u0012Z \u221e\n\u0012 \u0013 \u0013k\nl\n\u2212l\n2p\ndl\na(l)(1 + e ) tanh\n2\n0\n\u0012Z \u221e\n\u0012 \u0013 \u0013k\nl\n(a)\n2p\nf\u03a9 (l) tanh\n=\ndl\n2\n0\n\uf8f6k\n\uf8eb\n\uf8ec d\u22121 Z\n\u0012 \u0013\n\u0012 \u0013 \uf8f7\nZ l d\u22121\n\uf8f7\n\uf8ec2 X\u22122 li\n2\n\u22121\nl\nl\n\uf8f7\n\uf8ec\ndl +\ndl\uf8f7\nf\u03a9 (l) tanh2p\nf\u03a9 (l) tanh2p\n=\uf8ec\n\uf8f7\n\uf8ec\n2\n2\n+\n0\n\uf8ed i=0 | (li+1 )\n{z\n}\uf8f8\n{z\n} |\n,\u03c6\n\n(p)\n\n\uf8eb\n\n,\uf8ed\n=\n\n2d\u22121\nX\u22121\ni=0\n\nX\n\n\uf8f6k\n\n(p)\n\u03c6i \uf8f8\n\nk0 ,...,k2d\u22121 \u22121\nP\ni ki =k\n\n\u0012\n\n,\u03c6i\n\nk\nk0 , . . . , k2d\u22121 \u22121\n\n\u0013 2d\u22121\nY\u22121 \u0010\ni=0\n\n(p)\n2d\u22121 \u22121\n\n\u0011\n(p) ki\n\n\u03c6i\n\nwhere (a) holds since tanh(0) = 0. The above chain of equalities implies that in order to prove\n(D.1), it suffices to show that for any integer p \u2265 1\n\u0013\n\u0012\npi \u2212 p2d \u22121\u2212i 2p\n(p)\n\u2200 i = 0, 1, * * * , 2d\u22121 \u2212 1 .\n(D.2)\n\u03c6i \u2265 (pi + p2d \u22121\u2212i )\npi + p2d \u22121\u2212i\n\n\fFrom (33), we have that\nZ\n\nli\n\n(li+1 )+\n\nand\n\nf\u03a9 (l)dl = pi + p2d \u22121\u2212i\nZ\n\n0\n\nl2d\u22121 \u22121\n\n\u2200i = 0, . . . , 2d\u22121 \u2212 2\n\nf\u03a9 (l)dl = p2d\u22121 \u22121 + p2d\u22121\n\nwhich implies that for every i = 1, . . . , 2d\u22121 \u2212 1, the function\n\b\ninterval. Therefore, for i \u2208 1, . . . , 2d\u22121 \u2212 1 , we have\n(p)\n\u03c6i\n\n= (pi + p2d \u22121\u2212i )\n\nZ\n\nli\n\n(li+1 )+\n\nZ\n\nf\u03a9 (l)\npi + p2d \u22121\u2212i\n\nf\u03a9 (*)\npi +p2d \u22121\u2212i\n\n\u0012\n\n1 \u2212 e\u2212l\n1 + e\u2212l\n\nli\n\nis a pdf over the respective\n\u00132p\n\ndl\n\n1 \u2212 e\u2212l\nf\u03a9 (l)\ndl\n\u2265 (pi + p2d \u22121\u2212i )\n\u2212l\n(li+1 )+ pi + p2d \u22121\u2212i 1 + e\n!2p\nZ li\na(l)(1 \u2212 e\u2212l )\n(b)\ndl\n= (pi + p2d \u22121\u2212i )\n(li+1 )+ pi + p2d \u22121\u2212i\n!2p\nZ li\na(l) \u2212 a(\u2212l)\n(c)\n= (pi + p2d \u22121\u2212i )\ndl\n(li+1 )+ pi + p2d \u22121\u2212i\n\u0012\n\u0013\npi \u2212 p2d \u22121\u2212i 2p\n(d)\n= (pi + p2d \u22121\u2212i )\npi + p2d \u22121\u2212i\n\n(a)\n\n!2p\n\n(D.3)\n\nwhere (a) follows from the convexity of the function f (x) = x2p and by applying Jensen's equality,\n(b) follows from the definition of f\u03a9 (*) in (51), (c) follows from the symmetry property of the pdf\na(*), and (d) follows from (33). For i = 0, the proof follows along the same lines as (D.3), except\nthat in (b) we also use the fact that tanh(0) = 0, so the two integrals from 0+ to l2d\u22121 \u22121 and from\n0 to l2d\u22121 \u22121 get the same value. This concludes the proof of the sufficient condition (D.2) for the\nsatisfiability of inequality (61).\n\nD.2\n\nProof of the Claim Regarding the Tightness of the Lower Bound on the\nParity-Check Density in Theorem 4.1\n\nWe show here that the lower bound on the parity-check density in Theorem 4.1 is uniformly tighter\nthan the one in [13, Theorem 2.1] (except for the BSC and BEC where they coincide). In order to\nshow this , we first prove the following lemma:\nLemma D.1. For any MBIOS channel, the following inequality holds\nA \u2265 (1 \u2212 2w)2\nwhere w and A are introduced in (2) and (64), respectively.\n\n\fProof. From (2), (51) and (64)\nZ \u221e\n(1 \u2212 e\u2212l )2\ndl\na(l)\nA =\n1 + e\u2212l\n0\n\u0012 \u0013\nZ \u221e\nl\n\u2212l\n2\ndl\na(l) (1 + e ) tanh\n=\n2\n0\u2212\n\u0012 \u0013\nZ \u221e\nl\nf\u03a9 (l) tanh2\n=\ndl\n2\n\u2212\n0\n\u0012 \u0013 !2\nZ \u221e\nl\n\u2265\nf\u03a9 (l) tanh\ndl\n2\n0\u2212\n\u0012\n\u0013 !2\nZ \u221e\n\u2212l\n1\n\u2212\ne\n=\na(l) (1 + e\u2212l ) *\ndl\n1 + e\u2212l\n0\u2212\n!2\nZ \u221e\nZ \u221e\n\u2212l\n=\na(l) dl \u2212\ne a(l) dl\n0\u2212\n\n=\n\nZ\n\n0\u2212\n\n\u221e\n\na(l) dl \u2212\n\n=\n\n\u221e\n\na(\u2212l) dl\n\n0\u2212\n\n0\u2212\n\nZ\n\nZ\n\n\u221e\n\n[a(l) + a(\u2212l)] dl \u2212 2\n\n0\u2212\n\n=\n\n=\n\n1 + Pr(LLR = 0) \u2212 2\n\nZ\n\n!2\n\nZ\n\n\u221e\n\na(\u2212l) dl\n0\u2212\n\n\u221e\n\na(\u2212l) dl\n0\u2212\n\n!2\n\n!2\n\n\u0013 !2\n1\n1 + Pr(LLR = 0) \u2212 2 w + Pr(LLR = 0)\n2\n\u0012\n\n= (1 \u2212 2w)2 .\n\nwhere the single inequality above follows from Jensen's inequality.\nThe proof of our claim now follows directly by replacing the supremum over x \u2208 (0, A], which\nappears in the RHS of (62), with the same expression where we substitute x = (1 \u2212 2w)2 .\n\nD.3\n\nProof for the Claim in Remark 4.4\n\nIn order to prove the claim in Remark 4.4 (see p. 27), it is required to show that\n\n1\n1\u2212\n2 ln(2)\n\u2265\n\n1\u2212\n\nX\n\n\u221e\nX\np=1\n\n(\n\n1\np(2p \u2212 1)\n\n2w\n\ndk (1 \u2212 2w)k\n\nX\nk\n\ndk\n\n1\u2212C\n\u0012Z \u221e\n0\n\n\u0012 \u0013 \u0013k )\nl\na(l) (1 + e\u2212l ) tanh2p\ndl\n2\n(D.4)\n\nk\n\nwhere w is introduced in (2). The reason for showing this in light of the claim in Remark 4.4 is\nthat the RHS of the last inequality follows from considerations related to a BEC, essentially in the\nsame way that the second term of the maximization in the RHS of (49) is derived. By showing\n\n\fthis, we prove that the maximization of the two expressions in the LHS and RHS of (D.4) doesn't\naffect the bound in Corollary 4.1.\nFollowing the steps which lead to (68), we get that for any integer k \u2265 2\n\u221e\n\n1\n1 X\n2 ln(2)\np(2p \u2212 1)\np=1\n\n\u0012Z\n\n\u221e\n\n\u2212l\n\na(l)(1 + e ) tanh\n\n2p\n\n0\n\n\u0012 \u0013 \u0013k\nl\n\u2265 Ck.\ndl\n2\n\nP\nApplying this to (D.4) and denoting \u03a9(x) , k dk xk , we get that a sufficient condition for (D.4)\nto hold is\n1\u2212C\n2w\n\u2265\n.\n(D.5)\n1 \u2212 \u03a9(C)\n1 \u2212 \u03a9(1 \u2212 2w)\nFrom the erasure decomposition lemma, we get that an MBIOS channel is physically degraded as\ncompared to a BEC with an erasure probability p = 2w. By the information processing inequality,\nit follows that C \u2264 1\u22122w. Therefore, in order to prove (D.5), it is enough to show that the function\nf (x) =\n\n1\u2212x\n1 \u2212 \u03a9(x)\n\nis monotonically decreasing for x \u2208 (0, 1). We prove this property by showing that the derivative\nof f (*) is non-positive for x \u2208 (0, 1). As the denominator of the derivative is positive, we may\nequivalently show\n\u03a9\u2032 (x) (1 \u2212 x) \u2212 (1 \u2212 \u03a9(x)) \u2264 0.\nP\nDividing both sides of the inequality by (1 \u2212 x) \u2208 (0, 1) and noting that \u03a9(1) = k dk = 1, we get\nthat it is enough to show\n\u03a9(1) \u2212 \u03a9(x)\n\u2264 0.\n(D.6)\n\u03a9\u2032 (x) \u2212\n1\u2212x\nSince \u03a9(*) is a polynomial and therefore analytic, by the mean-value theorem we get that for some\nx\u0303 \u2208 (x, 1)\n\u03a9(1) \u2212 \u03a9(x)\n= \u03a9\u2032 (x\u0303).\n1\u2212x\nP\nSince \u03a9\u2032 (x) = k kdk xk is monotonically increasing for x \u2265 0, then (D.6) follows for all x \u2208 (0, 1).\nThis in turn proves (D.4).\n\nD.4\n\nProof of Eq. (75)\n\nIn order to prove (75), we first multiply the two sides of (73) by R, and denote R = (1 \u2212 \u03b5)C. This\ngives that the lower bound on the bit error probability in (73) is non-positive if and only if\n(1 \u2212 C)B \u2212 \u03b5C(1 \u2212 B) \u2264 0.\n\n(D.7)\n\n\fUnless the channel is noiseless, we get\n(\n\u0012 \u0013 \u0013k )\n\u221e\nX \u0012Z \u221e\nl\n1\n1 X\na(l)(1 + e\u2212l ) tanh2p\ndk\nB =\ndl\n2 ln(2)\np(2p \u2212 1)\n2\n0+\np=1\nk\n(\n\u0013k )\n\u221e\nX \u0012Z \u221e\n1\n1 X\n\u2212l\na(l)(1 + e )dl\ndk\n<\n2 ln(2) p=1 p(2p \u2212 1)\n+\n0\nk\n\uf8f1\n!k \uf8fc\nZ\n\u221e \uf8f2\n\uf8fd\nX\nX\n1\n1\n=\ndk\na(l)dl\n\uf8fe\n\uf8f3 p(2p \u2212 1)\n2 ln(2)\nR\u2212{0}\np=1\nk\n)\n(\n\u221e\n\u0011k\nX \u0010\n1\n1 X\ndk 1 \u2212 Pr(LLR = 0)\n=\n2 ln(2)\np(2p \u2212 1)\n\u2264\n\n1\n2 ln(2)\n\np=1\n\u221e\nX\np=1\n\nk\n\n1\n= 1.\np(2p \u2212 1)\n\nSince B < 1, the LHS of (D.7) is monotonically decreasing in \u03b5. We therefore deduce that the\ninequality (D.7) holds for \u03b5 \u2265 \u03b50 , where \u03b50 is the solution of\n(1 \u2212 C)B \u2212 \u03b50 C(1 \u2212 B) = 0.\nIt can be readily seen that the solution of the last equation is given by \u03b50 defined in (75).\n\nD.5\n\nProof of Eq. (76)\n\nWe will show both that there exists a unique \u03b50 that satisfies (76), and that the RHS of (74) is\nnon-positive if and only if \u03b5 \u2265 \u03b50 where \u03b50 is that unique solution. As in Appendix D.4, we begin\nby multiplying the two sides of (74) by R and denoting R = (1 \u2212 \u03b5)C. It follows that the bound in\nthe RHS of (74) is trivial (non-positive) if and only if\n\uf8fc\n\uf8f1\n\u0012Z \u221e\n\u0012 \u0013 \u0013 (2\u2212(1\u2212\u03b5)C)t \uf8fd\n\u221e \uf8f2\nX\n1\u2212(1\u2212\u03b5)C\n1 \u2212 (1 \u2212 \u03b5)C\nl\n1\n\u2264 0. (D.8)\n\u2212\u03b5C +\na(l)(1 + e\u2212l ) tanh2p\ndl\n\uf8fe\n\uf8f3 p(2p \u2212 1)\n2 ln(2)\n2\n0\np=1\n\nWe now show that the LHS of the last inequality is monotonically decreasing in \u03b5. Let us denote\n\uf8fc\n\uf8f1\n\u0012Z \u221e\n\u0012 \u0013 \u0013 (2\u2212(1\u2212\u03b5)C)t \uf8fd\n\u221e \uf8f2\nX\n1\u2212(1\u2212\u03b5)C\nl\n1\n1 \u2212 (1 \u2212 \u03b5)C\na(l)(1 + e\u2212l ) tanh2p\ndl\nf (\u03b5) , \u2212\u03b5C +\n\uf8fe\n\uf8f3 p(2p \u2212 1)\n2 ln(2)\n2\n0\np=1\n1\n2 ln(2) p(2p \u2212 1)\n\u0012 \u0013\nZ \u221e\nl\na(l)(1 + e\u2212l ) tanh2p\nap ,\ndl.\n2\n0\n\n\u03b1p ,\n\nBy Dividing the derivative of f w.r.t. \u03b5 by C, we get\nf \u2032 (\u03b5)\nC\n\n=\n\n1\nC\n\n\u2212C +C\n\n\u221e\nX\n\n(2\u2212(1\u2212\u03b5)C)t\n\n\u03b1p ap 1\u2212(1\u2212\u03b5)C\n\np=1\n\n\u221e\n\u0001X\n\n(2\u2212(1\u2212\u03b5)C)t\n1\u2212(1\u2212\u03b5)C\n\n\u0012\n\ntC\nlog(ap ) \u2212\n\u03b1p ap\n+ 1 \u2212 (1 \u2212 \u03b5)C\n(1 \u2212 (1 \u2212 \u03b5)C)2\np=1\n\u0012\n\u221e \u001a\n\u0011\u0013 (2\u2212(1\u2212\u03b5)C)t \u001b\n\u0010\nt\nX\n1\u2212(1\u2212\u03b5)C\nap 1\u2212(1\u2212\u03b5)C\n\u2212 1.\n=\n\u03b1p 1 \u2212 log ap\np=1\n\n\u0013!\n\n\fFrom the symmetry property of a(*) and since tanh(x) \u2264 1 then ap \u2264 1, and it follows that\n(2\u2212(1\u2212\u03b5)C)t\n\nt\n\nap 1\u2212(1\u2212\u03b5)C \u2264 ap1\u2212(1\u2212\u03b5)C . Therefore, the previous equality yields\n\u001b\n\u0013\n\u0012\n\u221e \u001a\nt\nt\n\u0001\nf \u2032 (\u03b5) X\n1\u2212(1\u2212\u03b5)C\n1\u2212(1\u2212\u03b5)C\n\u2212 1.\nap\n\u2264\n\u03b1p 1 \u2212 log ap\nC\np=1\n\nt\n\nFor p \u2208 N, let us denote ap1\u2212(1\u2212\u03b5)C , 1 \u2212 \u03b4p where 0 < \u03b4p < 1. Therefore, we get from the previous\ninequality\nf \u2032 (\u03b5)\nC\n\n\u2264\n\n\u221e n\nX\np=1\n\n\u2264\n\n\u221e\nX\n\no\n\u0001\n\u03b1p 1 \u2212 log(1 \u2212 \u03b4p ) (1 \u2212 \u03b4p ) \u2212 1\n\n\u03b1p \u2212 1 = 0\n\np=1\n\nx\nwhere the second inequality follows from the inequality ln(1 \u2212 x) > \u2212 1\u2212x\nwhich holds for x \u2208 (0, 1).\nThis concludes the proof of the monotonicity of the LHS of (D.8). Observing that\n\nf (0) = (1 \u2212 C)\n\n\u221e\nX\n\n(2\u2212C)t\n\n\u03b1p ap 1\u2212C > 0\n\np=1\n\nand\nf (1) = \u2212C +\n\u2264 \u2212C +\n\n\u221e\nX\n\np=1\n\u221e\nX\n\n\u03b1p a2t\np\n\u03b1p ap\n\np=1\n\n= \u2212C + C = 0\nwhere the first inequality follows since ap \u2264 1 and since t \u2265 1 (t = 1 if and only if the code is\ncycle-free, otherwise t > 1.) The second equality follows from the last three equalities leading\nto (68). From the continuity of the function f (*) w.r.t. \u03b5, we conclude that the monotonicity\nproperty of f , as shown above, ensures a unique solution for (76). From (D.8), it also follows from\nthe monotonicity and continuity properties of f (*) in terms of \u03b5 \u2208 (0, 1) that the RHS of (74) is\nnon-positive if and only if \u03b5 \u2265 \u03b50 where \u03b50 is the unique solution of (76).\n\nAcknowledgment\nThe second author wishes to acknowledge R\u00fcdiger Urbanke for stimulating discussions during the\npreparation of the work in [13] which also motivated the research in this paper.\n\nReferences\n[1] D. Burshtein, M. Krivelevich, S. Litsyn and G. Miller, \"Upper bounds on the rate of LDPC\ncodes,\" IEEE Trans. on Information Theory, vol. 48, no. 9, pp. 2437\u20132449, September 2002.\n[2] D. Burshtein and G. Miller, \"Asymptotic enumeration methods for analyzing LDPC codes,\"\nIEEE Trans. on Information Theory, vol. 50, pp. 1115\u20131131, June 2004.\n\n\f[3] A. Bennatan and D. Burshtein, \"On the application of LDPC codes to arbitrary discretememoryless channels,\" IEEE Trans. on Information Theory, vol. 50, no. 3, pp. 417\u2013438, March\n2004.\n[4] R. G. Gallager, Low-density parity-check codes, Cambridge, MA, USA, MIT Press, 1963.\n[5] H. Jin and R. J. McEliece, \"Typical pairs decoding on the AWGN channel,\" Proceedings 2000\nInternational Symposium on Information Theory and Its Applications, pp. 180\u2013183, Honolulu,\nHawaii, U.S.A., November 5\u20138, 2000.\n[6] A. Khandekar and R. J. McEliece, \"On the complexity of reliable communications on the\nerasure channel,\" Proceedings 2001 International Symposium on Information Theory, p. 1,\nWashington, DC, USA, June 24\u201329, 2001.\n[7] C. Measson and R. Urbanke, \"An upper bound on the ML threshold of LDPC ensembles over\nthe BEC,\" Proceedings of the Forty-First Allerton Conference on Communications, Control\nand Computing, pp. 478\u2013487, Allerton, Monticello, USA, October 1\u20133, 2003.\n[8] A. Montanari, \"Tight bounds for LDPC codes under MAP decoding,\" in Proceedings 2004\nIEEE International Symposium on Information Theory, p. 476, Chicago, IL, USA, June 27\u2013\nJuly 2, 2004.\n[9] H. D. Pfister, I. Sason, and R. Urbanke, \"Capacity-achieving ensembles for the binary erasure\nchannel with bounded complexity,\" IEEE Trans. on Information Theory, vol. 51, no. 7, July\n2005.\n[10] T. Richardson and R. Urbanke, \"The capacity of low-density parity-check codes under messagepassing decoding,\" IEEE Trans. on Information Theory, vol. 47, no. 2, pp. 599\u2013618, February\n2001.\n[11] T. Richardson, A. Shokrollahi and R. Urbanke, \"Design of capacity-approaching irregular lowdensity parity-check codes,\" IEEE Trans. on Information Theory, vol. 47, no. 2, pp. 619\u2013637,\nFebruary 2001.\n[12] T. Richardson and R. Urbanke, Modern Coding Theory, to be published, Cambridge Press,\nJune 2005.\n[13] I. Sason and R. Urbanke, \"Parity-check density versus performance of binary linear block\ncodes over memoryless symmetric channels,\" IEEE Trans. on Information Theory, vol. 49,\nno. 7, pp. 1611\u20131635, July 2003.\n[14] A. Shokrollahi, \"New sequences of time erasure codes approaching channel capacity,\" in Proceedings of the 13th International Symposium on Applied Algebra, Algebraic Algorithms and\nError-Correcting Codes, Lectures Notes in Computer Science 1719, Springer Verlag, pp. 65\u201376,\n1999.\n[15] A. Trachtenberg, T. Etzion and A. Vardy, \"Which codes have cycle-free Tanner graphs ?,\"\nIEEE Trans. on Information Theory, vol. 45, no. 6, pp. 2173\u20132181, September 1999.\n[16] Optimization of degree distributions for ensembles of LDPC codes. [Online]. Available:\nhttp://lthcwww.epfl.ch/research/ldpcopt/index.php.\n[17] LDPC asymptotic weight and stopping set spectrum calculator. [Online]. Available:\nhttp://lthcwww.epfl.ch/\u223ccdi/ldpc/ldpc define.php.\n\n\f"}