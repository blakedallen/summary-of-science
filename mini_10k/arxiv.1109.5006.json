{"id": "http://arxiv.org/abs/1109.5006v1", "guidislink": true, "updated": "2011-09-23T06:42:23Z", "updated_parsed": [2011, 9, 23, 6, 42, 23, 4, 266, 0], "published": "2011-09-23T06:42:23Z", "published_parsed": [2011, 9, 23, 6, 42, 23, 4, 266, 0], "title": "The Shifting Technique for Solving a Nonsymmetric Algebraic Riccati\n  Equation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1109.2833%2C1109.3279%2C1109.0821%2C1109.5792%2C1109.5835%2C1109.4901%2C1109.0271%2C1109.1739%2C1109.5767%2C1109.6545%2C1109.6098%2C1109.0187%2C1109.2747%2C1109.1760%2C1109.6571%2C1109.0509%2C1109.6339%2C1109.1605%2C1109.4134%2C1109.1877%2C1109.5096%2C1109.4371%2C1109.3386%2C1109.3873%2C1109.3305%2C1109.1450%2C1109.1745%2C1109.0404%2C1109.1827%2C1109.1107%2C1109.3827%2C1109.0142%2C1109.3479%2C1109.3889%2C1109.3705%2C1109.0728%2C1109.0153%2C1109.0274%2C1109.1717%2C1109.4293%2C1109.5633%2C1109.0776%2C1109.2968%2C1109.4545%2C1109.2578%2C1109.3576%2C1109.4238%2C1109.1014%2C1109.4800%2C1109.0747%2C1109.1645%2C1109.0926%2C1109.4241%2C1109.4832%2C1109.6149%2C1109.4327%2C1109.3722%2C1109.2202%2C1109.4259%2C1109.2300%2C1109.2366%2C1109.6589%2C1109.5258%2C1109.5187%2C1109.2321%2C1109.0043%2C1109.2000%2C1109.4418%2C1109.0519%2C1109.3662%2C1109.2831%2C1109.2690%2C1109.6174%2C1109.2486%2C1109.0105%2C1109.1464%2C1109.2539%2C1109.6342%2C1109.2248%2C1109.3427%2C1109.1619%2C1109.2897%2C1109.6151%2C1109.5294%2C1109.2883%2C1109.0486%2C1109.2867%2C1109.5888%2C1109.2909%2C1109.0892%2C1109.0325%2C1109.3524%2C1109.6748%2C1109.0184%2C1109.5006%2C1109.0449%2C1109.3768%2C1109.2354%2C1109.0618%2C1109.1279%2C1109.0754&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Shifting Technique for Solving a Nonsymmetric Algebraic Riccati\n  Equation"}, "summary": "This paper analyzes a special instance of nonsymmetric algebraic matrix\nRiccati equations arising from transport theory. Traditional approaches for\nfinding the minimal nonnegative solution of the matrix Riccati equations are\nbased on the fixed point iteration and the speed of the convergence is linear.\nRelying on simultaneously matrix computation, a structure-preserving doubling\nalgorithm (SDA) with quadratic convergence is designed for improving the speed\nof convergence. The difficulty is that the double algorithm with quadratic\nconvergence cannot guarantee to work all the time. Our main trust in this work\nis to show that applied with a suitable shifted technique, the SDA is\nguaranteed to converge quadratically with no breakdown. Also, we modify the\nconventional simple iteration algorithm in the critical case to dramatically\nimprove the speed of convergence. Numerical experiments strongly suggest that\nthe total number of computational steps can be significantly reduced via the\nshifting procedure.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1109.2833%2C1109.3279%2C1109.0821%2C1109.5792%2C1109.5835%2C1109.4901%2C1109.0271%2C1109.1739%2C1109.5767%2C1109.6545%2C1109.6098%2C1109.0187%2C1109.2747%2C1109.1760%2C1109.6571%2C1109.0509%2C1109.6339%2C1109.1605%2C1109.4134%2C1109.1877%2C1109.5096%2C1109.4371%2C1109.3386%2C1109.3873%2C1109.3305%2C1109.1450%2C1109.1745%2C1109.0404%2C1109.1827%2C1109.1107%2C1109.3827%2C1109.0142%2C1109.3479%2C1109.3889%2C1109.3705%2C1109.0728%2C1109.0153%2C1109.0274%2C1109.1717%2C1109.4293%2C1109.5633%2C1109.0776%2C1109.2968%2C1109.4545%2C1109.2578%2C1109.3576%2C1109.4238%2C1109.1014%2C1109.4800%2C1109.0747%2C1109.1645%2C1109.0926%2C1109.4241%2C1109.4832%2C1109.6149%2C1109.4327%2C1109.3722%2C1109.2202%2C1109.4259%2C1109.2300%2C1109.2366%2C1109.6589%2C1109.5258%2C1109.5187%2C1109.2321%2C1109.0043%2C1109.2000%2C1109.4418%2C1109.0519%2C1109.3662%2C1109.2831%2C1109.2690%2C1109.6174%2C1109.2486%2C1109.0105%2C1109.1464%2C1109.2539%2C1109.6342%2C1109.2248%2C1109.3427%2C1109.1619%2C1109.2897%2C1109.6151%2C1109.5294%2C1109.2883%2C1109.0486%2C1109.2867%2C1109.5888%2C1109.2909%2C1109.0892%2C1109.0325%2C1109.3524%2C1109.6748%2C1109.0184%2C1109.5006%2C1109.0449%2C1109.3768%2C1109.2354%2C1109.0618%2C1109.1279%2C1109.0754&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper analyzes a special instance of nonsymmetric algebraic matrix\nRiccati equations arising from transport theory. Traditional approaches for\nfinding the minimal nonnegative solution of the matrix Riccati equations are\nbased on the fixed point iteration and the speed of the convergence is linear.\nRelying on simultaneously matrix computation, a structure-preserving doubling\nalgorithm (SDA) with quadratic convergence is designed for improving the speed\nof convergence. The difficulty is that the double algorithm with quadratic\nconvergence cannot guarantee to work all the time. Our main trust in this work\nis to show that applied with a suitable shifted technique, the SDA is\nguaranteed to converge quadratically with no breakdown. Also, we modify the\nconventional simple iteration algorithm in the critical case to dramatically\nimprove the speed of convergence. Numerical experiments strongly suggest that\nthe total number of computational steps can be significantly reduced via the\nshifting procedure."}, "authors": ["Chun-Yueh Chiang", "Matthew M. Lin"], "author_detail": {"name": "Matthew M. Lin"}, "author": "Matthew M. Lin", "arxiv_comment": "18 pages", "links": [{"href": "http://arxiv.org/abs/1109.5006v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1109.5006v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1109.5006v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1109.5006v1", "journal_reference": null, "doi": null, "fulltext": "THE SHIFTING TECHNIQUE FOR SOLVING A NONSYMMETRIC\nALGEBRAIC RICCATI EQUATION \u2217\n\narXiv:1109.5006v1 [math.NA] 23 Sep 2011\n\nCHUN-YUEH CHIANG\u2020 AND MATTHEW M. LIN\n\n\u2021\n\nAbstract.\nThis paper analyzes a special instance of nonsymmetric algebraic matrix Riccati equations arising from transport theory. Traditional approaches for finding the minimal nonnegative solution of the matrix Riccati equations\nare based on the fixed point iteration and the speed of the convergence is linear. Relying on simultaneously\nmatrix computation, a structure-preserving doubling algorithm (SDA) with quadratic convergence is designed for\nimproving the speed of convergence. The difficulty is that the double algorithm with quadratic convergence cannot guarantee to work all the time. Our main trust in this work is to show that applied with a suitable shifted\ntechnique, the SDA is guaranteed to converge quadratically with no breakdown. Also, we modify the conventional\nsimple iteration algorithm in the critical case to dramatically improve the speed of convergence. Numerical experiments strongly suggest that the total number of computational steps can be significantly reduced via the shifting\nprocedure.\n\nKeywords. nonsymmetric algebraic Riccati equation, transport theory, shifting technique,\ncritical case, structured doubling algorithm, simple iteration method\nAMS subject classifications. 15A24, 65F10\n1. Introduction. The nonsymmeric algebraic Riccati equation (NARE), encountered in\ntransport theory, is given by\nXCX \u2212 XD \u2212 AX + B = 0,\n\n(1.1)\n\nwhere A, B, C and D \u2208 Rn\u00d7n are given by\nA = \u2206 \u2212 eq \u22a4 ,\n\nB = ee\u22a4 ,\n\nC = qq \u22a4 ,\n\nD = \u0393 \u2212 qe\u22a4 .\n\n(1.2)\n\nwhere\ne\nq\n\u2206\n\u0393\n\n=\n=\n=\n=\n\n[1, . . . , 1]\u22a4 \u2208 Rn ,\n[q1 , . . . , qn ]\u22a4 ,\nwith\ndiag([\u03b41 , . . . , \u03b4n ]), with\ndiag([d1 , . . . , dn ]), with\n\nqi =\n\u03b4i =\ndi =\n\nci\n2\u03c9i ,\n1\nc\u03c9i (1+\u03b1) ,\n1\nc\u03c9i (1\u2212\u03b1) .\n\n(1.3)\n\nThe parameters, used to define the above matrices and vectors, satisfy 0 < c \u2264 1, 0 \u2264 \u03b1 < 1 and\nn\nP\nci = 1.\nthe sequences are 0 < \u03c9n < * * * < \u03c92 < \u03c91 < 1, ci > 0, i = 1, 2, . . . , n, so that\ni=1\n\nFor the physical meaning of the NARE (1.1) and its corresponding parameters setup, the\nreader is referred to [13]. Correspondingly, we define the corresponding dual equation of (1.1)\nY BY \u2212 Y A \u2212 DY + C = 0.\n\n(1.4)\n\nTo facilitate our discussion, we need a nonsingular M-matrix or a singular irreducible M-matrix\ngiven by\n\u0014\n\u0015\nD \u2212C\nM=\n(1.5)\n\u2212B A\n\u2217 VERSION\n\nAugust 10, 2018\nfor General Education,National Formosa University,Huwei 632, Taiwan. (chiang@nfu.edu.tw)\n\u2021 Corresponding Author. Department of Mathematics, National Chung Cheng University, Chia-Yi 621, Taiwan.\n(mlin@math.ccu.edu.tw) This research was supported in part by the National Science Council of Taiwan under\ngrant 99-2115-M-194-010-MY2.\n\u2020 Center\n\n1\n\n\f2\nand its relative matrix\nH = JM,\n\n(1.6)\n\nwhere J = diag(In , \u2212In ) with In to be the n by n identity matrix. Our interest in this study\nis to find the minimal nonnegative solution X of (1.1). The existence conditions of the minimal\nnonnegative solution are shown by Juang et al. in [13]. Iterative methods for solving this problem\nare numerous and can be divided into two major categories.\nOne is the method sharing a computational cost of O(n2 ) arithmetic operations (ops) per\nstep, but converges linear or sublinear. The representative method of the first category is the\nsimple iteration method (SI) or vector iteration method, which is first proposed by Lu [15]. This\nmethod is very simple and requires a computational cost of 4n2 ops per step. Recently, three\nmore methods, modified simple iteration (MSI), nonlinear block Jacobi method (NBJ) and the\nnonlinear block Gass-Seidel method (NBGS), based on Lu's method are proposed in [1, 2]. It has\nbeen shown in [10] that if (\u03b1, c) 6= (0, 1), the speed of convergence of the NBGS is faster than\nthe other three. Generally speaking, the iterative methods mentioned above can be classified as\naccelerated variants of the well-known fixed-point iterations. Also, in [10] we know that all these\nfour methods can provide a linear convergence, if (\u03b1, c) 6= (0, 1) and a sublinear convergence, if\n(\u03b1, c) = (0, 1).\nThe other is a method with a cost of O(n3 ) ops but provides quadratic convergence. Despite\nof the complexity, quadratically convergent methods are much to be desired in practice. There\nare several good algorithms that can cause quadratic convergence, for example, the Newton\nmethod [9, 4] and the structure-preserving doubling algorithm (SDA) [11, 8]. However, when\n(\u03b1, c) = (0, 1), both Newton method and the SDA algorithm are not always valid and require\nspecial attention.\nIn this work we fine-tune the customary SDA algorithm and make it always workable and\nquadratical convergent when solving (1.1). The SDA algorithm was first proposed by Guo\net.al. [11] for solving the NARE. In [11, 5], it has been shown to be quadratically convergent,\nif (\u03b1, c) 6= (0, 1) and linearly convergent with rate 1/2, if (\u03b1, c) = (0, 1). The later case is the\nso-called \"critical case\" and is the most challenging problem that we will encounter when solving (1.1). Roughly speaking, the critical case embedded with some type of singularity, i.e., the\nmatrix H has two zero eigenvalues, that will significantly reduce the speed of convergence. In [8],\nGuo et al. propose an efficient method based on a single-shift technique to accelerate the computation of the minimal nonnegative solution so that one singularity can be removed. They also\nshow that the doubling algorithm applied to the shifted equation of (1.1) converges faster than\nthe doubling algorithm applied to (1.1), if no breakdown occurs. The approach of removing two\nzero eigenvalues of H has also been introduced in [8], but again the convergence of the doubling\nalgorithm cannot be guaranteed. Our contribution in this paper, which we think is new in theory,\nis to provide a detailed analysis of changes in the eigenvalue distribution of matrices H and M\nas the shift procedures are employed. Through this discussion, the quadratic convergence of the\nSDA is guaranteed via the duble-shift technique to remove two singularities. Most important of\nall, the minimal nonnegative solution of the duble-shift model is shown to be equal to that of the\noriginal model. We believe such results are the first detailed proofs of the eigenvalue analysis of H\nand M and their corresponding matrices with shift procedures and should be of great significance\nfor solving the NARE.\nThe organization of this paper is as follows. In Section 2, we review some of the main results\nand definitions that will be used for subsequent discussion. In Section 3, we provide a complete\ndiscussion on the shifted modifications for the SDA algorithm. We show that the SDA algorithm\napplied to the double-shift problem is always accessible and the solution obtained from the doubleshift problem is equal to the original NARE problem. In Section 4, advantages of the shifting\n\n\f3\ntechnique applied to the SI algorithm have been thoroughly investigated. In Section 5, we present\na few numerical experiments to show the practicability and effectiveness of the shifting procedure\nand concluding remarks are given in Section 6.\n2. Preliminaries. In this section we briefly review the definitions of Z-matrix and M-matrix\nand discuss further some of their properties which are required in the statements and in the\nproofs discussed in the following sections. We also summarize the popular algorithm, SDA, for\nour numerical experiments as we shall see below.\n2.1. Definition and Theorems. In order to formalize our discussion, we start by introducing the following two definitions.\nDefinition 2.1. A matrix A = (aij ) \u2208 Rn\u00d7n is called a Z-matrix if aij \u2264 0 for all i 6= j.\nNote that for any Z-matrix A, there exists a matrix B \u2208 Rn\u00d7n with B \u2265 0 and some \u03b1 \u2208 R\nsuch that A = \u03b1I \u2212 B where I is the identity matrix. Also, the definition of Z-matrix plays an\nimportant role in defining a given matrix to be an M-matrix.\nDefinition 2.2. A Z-matrix A is called an M-matrix if A = \u03b1I \u2212 B with B \u2265 0 and\n\u03b1 \u2265 \u03c1(B), where \u03c1(B) is the spectral radius of B. It is called a singular M-matrix if \u03b1 = \u03c1(B)\nand a nonsingular M-matrix if \u03b1 > \u03c1(B).\nThere are a great many different conditions, which are mathematically intriguing and important for applications, that discuss the necessary and sufficient conditions for a given Z-matrix\nto be an M-matrix. For our subsequent discussions, we apply the following two well known and\nuseful results in the study of M-matrices.\nTheorem 2.3. [3] If A \u2208 Rn\u00d7n is a Z-matrix, the following statements are equivalent:\n1. A is a nonsingular M-matrix.\n2. \u03c3(A) \u2282 C+ .\n3. Av > 0 holds for some positive vector v \u2208 Rn .\n4. A\u22121 \u2265 0.\nTheorem 2.4. [9] If the matrix (1.5) is a nonsingular M-matrix, then the NARE (1.1) and\nits dual equation (1.4) have minimal nonnegative solutions X and Y , respectively. Also, matrices\nD \u2212 CX and A \u2212 BY are nonsingular M-matrix.\nNote that the conditions we list here are only a selection from many more useful ones. See\n[3, 9, 12, 16] for a longer list of conditions and references to the proofs.\n2.2. SDA Algorithm. In [11], Guo et al. come up with the SDA algorithm for solving\nNARE problems and show that if the matrix M (1.5) is a nonsingular M-matrix (irreducible\nsingular M-matrix [5]), the SDA algorithm is well-defined and quadratically convergent (at least\nlinearly convergent with rate 1/2). Its idea is based on the doubling transformation. For more\ndetails of the doubling transformation, the reader is referred to [11, Theorem 2.1]. The SDA\nalgorithm starts by choosing a positive scalar \u03b3 with\n\u001b\n\u001a\n\u03b3 \u2265 max max aii , max dii .\n1\u2264i\u2264n\n\n1\u2264i\u2264n\n\nLet\nE0 = In \u2212 2\u03b3V\u03b3\u22121 ,\nG0 = 2\u03b3D\u03b3\u22121 CW\u03b3\u22121 ,\n\nF0 = In \u2212 2\u03b3W\u03b3\u22121 ,\nH0 = 2\u03b3W\u03b3\u22121 BD\u03b3\u22121 ,\n\nwhere\nA\u03b3 = A + \u03b3In ,\nW\u03b3 = A\u03b3 \u2212\n\nD\u03b3\n\u22121\nBD\u03b3 C,\n\n= D + \u03b3In ,\nV\u03b3 = D\u03b3 \u2212 CA\u22121\n\u03b3 B.\n\n\f4\nThen, the SDA algorithm presented in [11] is given by\nEk+1 = Ek (In \u2212 Gk Hk )\u22121 Ek ,\n\n(2.3a)\n\nFk+1 = Fk (In \u2212 Hk Gk )\u22121 Fk ,\nGk+1 = Gk + Ek (In \u2212 Gk Hk )\u22121 Gk Fk ,\n\n(2.3b)\n(2.3c)\n\nHk+1 = Hk + Fk (In \u2212 Hk Gk )\u22121 Hk Ek ,\n\n(2.3d)\n\nwhere the sequence Hk and Gk will converge to the minimal nonnegative solutions X of (1.1)\nand Y of (1.4) quadratically.\n2.3. Spectrum Analysis. Recall that in the critical case (\u03b1, c) = (0, 1), the matrix M (1.5)\nis an irreducible singular M-matrix [6] and the corresponding matrix H (1.6) has a double zero\neigenvalue with the geometric multiplicity equal to one. To be specific, the matrix H has 2n real\neigenvalues \u03bdn , . . . , \u03bd1 , \u03bb1 , ..., \u03bbn , which satisfy the following order [13]:\n\u22121\n\u22121\n\u22121\n\u22121\n1\n1\n1\n< \u03bdn <\n<. . .<\n< \u03bd2 <\n< \u03bd1 = 0 = \u03bb1 <\n< \u03bb2 <\n< . . . < \u03bbn <\n.\n\u03c9n\n\u03c9n\u22121\n\u03c92\n\u03c91\n\u03c91\n\u03c92\n\u03c9n\n\n(2.4)\n\nThe phenomenon is called eigenvalue interlacing. Moreover,\n\u03c3(D \u2212 CX) = {\u03bb2 , . . . , \u03bbn , 0},\n\u03c3(A \u2212 BY ) = {0, \u2212\u03bc1 , . . . , \u2212\u03bcn },\n\n(2.5a)\n(2.5b)\n\nif X and Y are the minimal nonnegative solutions of (1.1) and (1.4), respectively [6]. Paralleling the above distribution, the following theorem shows that all eigenvalues of M are real and\nnonnegative. In fact, M has n specific eigenvalues \u03c91i , for i = 1, . . . , n.\nTheorem 2.5. Let M be the matrix defined in (1.5) with (\u03b1, c) = (0, 1). Then M has 2n\nreal eigenvalues, where one part of the eigenvalues of M are 0, \u03c911 , . . . , \u03c91n and the others are\n\u03bc1 , . . . , \u03bcn\u22121 such that the eigenvalues can be arranged in the following order:\n0<\n\n1\n1\n1\n< \u03bc1 <\n< \u03bc2 < * * * < \u03bcn\u22121 <\n.\n\u03c91\n\u03c92\n\u03c9n\n\nProof. Consider the characteristic polynomial of M defined by\n\u0014\n\u0015 \u0014 \u0015\n\u0003\n\u0393 \u2212 \u03bbIn\nq \u0002 \u22a4\n\u2212\nf (\u03bb) \u2261 det(M \u2212 \u03bbIn ) = det(\ne\nq\u22a4 )\ne\n\u2206 \u2212 \u03bbIn\n\u0014\n\u0014\n\u0015\n\u0015\u0014 \u0015\n\u0002 \u22a4\n\u0003 (\u0393 \u2212 \u03bbIn )\u22121\n(\u0393 \u2212 \u03bbIn )\nq\n\u22a4\n= det(\n)(1 \u2212 e\nq\n)\n(\u2206 \u2212 \u03bbIn )\n(\u2206 \u2212 \u03bbIn )\u22121 e\nY\nX\nqj\nqj\n=\n(\u03b3i \u2212 \u03bb)(\u03b4i \u2212 \u03bb)(1 \u2212\n(\n+\n))\n(2.6)\n\u03b3j \u2212 \u03bb \u03b4 j \u2212 \u03bb\n1\u2264i\u2264n\n\n1\u2264j\u2264n\n\nThe last equation (2.6) is called the secular equation of M \u2212 \u03bbI. Notice that \u03b3i = \u03b4i = \u03c91i ,\nci\nqi = 2\u03c9\nfor 1 \u2264 i \u2264 n when (\u03b1, c) = (0, 1). Thus, through a straightforward calculation, we have\ni\nf (\u03bb) =\n\nY\n\n(\n\n1\u2264i\u2264n\n\nX\n1\ncj\n\u2212 \u03bb)2 (1 \u2212\n)\n\u03c9i\n(1 \u2212 \u03c9j \u03bb)\n1\u2264j\u2264n\n\nX\ncj \u03bb\n1\n=\u2212\n( \u2212 \u03bb)2\n1\n\u03c9i\n\u2212\u03bb\n1\u2264j\u2264n \u03c9j\n1\u2264i\u2264n\n\uf8eb\nX\nY 1\nY\ncj\n= \u2212\u03bb\n( \u2212 \u03bb) \uf8ed\n\u03c9i\nY\n\n1\u2264i\u2264n\n\n1\u2264j\u2264n\n\nk6=j,1\u2264k\u2264n\n\n(\n\n\uf8f6\n\n1\n\u2212 \u03bb)\uf8f8 .\n\u03c9k\n\n\f5\nThus, f has roots 0, \u03c911 , * * * , \u03c91n . To complete the proof of the theorem, let\ng(\u03bb) =\n\nX\n\ncj\n\n1\u2264j\u2264n\n\nY\n\n(\n\nk6=j,1\u2264k\u2264n\n\n1\n\u2212 \u03bb).\n\u03c9k\n\nThe sign of g( \u03c91j ) is (\u22121)j\u22121 since the monotonicity of {\u03c9j } , the intermediate value theorem\n1\n) for 1 \u2264 j \u2264 n \u2212 1. Together with the fact that\nindicates that g has at least roots in ( \u03c91j , \u03c9j+1\nthe degree of g is n \u2212 1. The proof of the theorem is thus complete.\nIt should be noted that\n\u0014 \u0015 \u0014 \u0015\nI\nI\n(2.7)\nH n = n (D \u2212 CX).\nX\nX\nFrom the above theorem and (2.7), we know that the minimal nonnegative solution X is related to\nan invariant subspace with nonnegative eigenvalues of H. Also, it is clear that q \u22a4 \u0393\u22121 e+e\u22a4\u2206\u22121 q =\nc = 1. We then have the fact [4] that the matrix H has a right eigenvector v \u22a4 = [v1\u22a4 , v2\u22a4 ], with\nv1 = \u0393\u22121 q and v2 = \u2206\u22121 e, so that\nHv = 0.\n\n(2.8)\n\n\u22a4\n\u22121\nBy applying this right eigenvector v, a left eigenvector u\u22a4 = [u\u22a4\ne and\n1 , u2 ], with u1 = \u0393\n\u22121\nu2 = \u2212\u2206 q of H, corresponding to the eigenvalue 0, can be obtained without any trouble by\ndirectly checking that\n\nu\u22a4 H = 0.\n\n(2.9)\n\nCorresponding to the matrix H, the matrix M = JH has the right and left eigenvectors v and\n\u22a4\nu\u22a4 J. Also, it can be seen that u\u22a4\n1 v1 + u2 v2 = 0. Applying the eigenpair information, we have\nthe following important result given in [6, 8].\nTheorem 2.6. Let M be an irreducible singular M-matrix as defined in (1.5), and let\nX and Y are the minimal nonnegative solutions of (1.1) and (1.4), respectively. Suppose that\ncorresponding to the zero eigenvalue, the right and left eigenvectors of M are v \u22a4 = [v1\u22a4 , v2\u22a4 ] and\n\u22a4\nu\u22a4 = [u\u22a4\n1 , \u2212u2 ]. If (\u03b1, c) = (0, 1), then the following properties are satisfied:\nXv1 = v2 ,\n\n\u22a4\nu\u22a4\n2 X = \u2212u1 ,\n\nand\n\nY v2 = v1 .\n\n(2.10)\n\nIt was shown in [8], that the matrix X is the minimal nonnegative solution of 1.1 if and only\nif X \u22a4 is the minimal nonnegative solution of the equation\nX \u22a4 C \u22a4 X \u22a4 \u2212 X \u22a4 A\u22a4 \u2212 D\u22a4 X \u22a4 + B \u22a4 = 0.\n\n(2.11)\n\nThe same statement can be applied to the dual equation (1.4). Its proof is simply based on taking\nthe transpose on both sides of (1.1).\nCorollary 2.7. The matrix Y is the minimal nonnegative solution of (1.4) if and only if\nY \u22a4 is the minimal nonnegative solution of the equation\nY \u22a4 B \u22a4 Y \u22a4 \u2212 Y \u22a4 D\u22a4 \u2212 A\u22a4 Y \u22a4 + C \u22a4 = 0.\n\n(2.12)\n\nFollowing Corollary (2.7), we want to know that whether there exists a relationship between\nthe left eigenvector of M and the minimal nonnegative solution Y . To begin with, let\n\u0014\n\u0015\nD\u22a4 \u2212B \u22a4\nMt =\n,\n(2.13)\n\u2212C \u22a4 A\u22a4\n\n\f6\n\u22a4 \u22a4\nbe the corresponding M-matrix of (2.12). Note that Mt has a right eigenvector [u\u22a4\n1 , \u2212u2 ] and\n\u22a4 \u22a4\na left eigenvector [v1 , v2 ] corresponding to the eigenvalue 0. Equipped with the notations given\nin (1.2), the matrix Mt is again an irreducible singular M-matrix if (\u03b1, c) = (0, 1). Then, Theorem 2.6 asserts that Y \u22a4 u1 = \u2212u2 . Namely, we have derived the following important relationship\nbetween the left eigenvector u and the minimal solution Y ,\n\u22a4\nu\u22a4\n1 Y = \u2212u2 .\n\n(2.14)\n\nOn the other hand, we know that the convergence rate of the SDA algorithm is determined\nby\np\nkHk \u2212 Xk \u2264 \u03c1(C\u03b3 (D \u2212 CX))\u03c1(C\u03b3 (A \u2212 BY )),\nk\u2192\u221e\np\nk\nlim sup 2 kGk \u2212 Y k \u2264 \u03c1(C\u03b3 (D \u2212 CX))\u03c1(C\u03b3 (A \u2212 BY )),\n\nlim sup\n\n2k\n\n(2.15a)\n(2.15b)\n\nk\u2192\u221e\n\nwhere\n\nC\u03b3 : z \u2192\n\nz\u2212\u03b3\nz+\u03b3\n\n(2.16)\n\nis the Cayley transform and the scalar \u03b3 > 0 [8]. Note that from (2.5), we have \u03c1(C\u03b3 (D \u2212 CX) =\n\u03c1(C\u03b3 (A \u2212 BY )) = 1. It follows that no further conclusion of the convergence rate of the SDA\nalgorithm can be derived from the fact (2.15) except that the linear convergence is guaranteed.\nIn the subsequent section, we want to know that how the shift procedures affect the convergence\nrate.\n3. Properties of the Shifted NARE. In this section, a detailed analysis of the eigenvalue\ndistribution of the matrix M is provided with respect to the the critical case (\u03b1, c) = (0, 1). It\nis shown that under the shifting technique, the matrix M is still an M-matrix and the SDA\nalgorithm is guaranteed to converge. The minimal nonnegative solution in the shifted NARE\nproblems are proved to be equal to the minimal nonnegative solution of (1.1). Last but not least,\nthe SDA algorithm is shown to be accelerated by removing the singularities embedded in the\nmatrix H.\nb be the rank-one modification of the matrix H which is defined by\n3.1. Single Shift. Let H\nb = H + \u03b7vr\u22a4 ,\nH\n\n(3.1)\n\nwhere \u03b7 > 0 is a scalar and r \u2265 0 is a vector satsifying r\u22a4 v = 1. To be specific, we write\nb and M\nc are denoted by\nr\u22a4 = [r1\u22a4 , r2\u22a4 ], where r1 = e, r2 = q. Then, two matrices H\n\"\n#\n\"\n#\nb \u2212C\nb\nb \u2212C\nb\nD\nD\nb=\nc\nH\n,\n(3.2)\nb \u2212A\nb , M = \u2212B\nb A\nb\nB\nwhere\n\nb = D + \u03b7v1 r1\u22a4 , C\nb = C \u2212 \u03b7v1 r2\u22a4 ,\nD\nb = B + \u03b7v2 r\u22a4 , A\nb = A \u2212 \u03b7v2 r\u22a4 .\nB\n1\n2\n\n(3.3)\n\nc given in (3.2) that the matrix M\nc is irreducible.\nIt follows from the specific structure of M\nThe nice feature of this rank-one modification is that one zero eigenvalue of H will be replaced by\nthe scalar \u03b7 > 0. This can be seen by directly applying the following useful lemma shown in [8].\n\n\f7\nLemma 3.1. Let T be a singular matrix with T v = 0 for some nonzero vector v. If r is a\nvector so that r\u22a4 v = 1, then for any scalar r, the eigenvalues of the matrix\nTb = T + \u03b7vr\u22a4 ,\n\nconsist of those of T , except that one zero eigenvalue of T is replaced by \u03b7.\nb are the same except that\nIt can be seen that from Lemma 3.1 the eigenvalues of H and H\none zero eigenvalue is shifted to \u03b7. In the next theorem, we want to show that despite of the rank\nc are equal to those of M .\none modification, the eigenvalues of M\nc be defined in (1.5) and (3.2), respectively. Then, the\nCorollary 3.2. Let M and M\nc are conincident. That is, the eigenvalues of M and M\nc\ncharacteristic polynomials of M and M\nare equal.\nc. We\nProof. This proof can be easily obtained by studying the characteristic polynomial of M\nc, denoted by fb(\u03bb), is defined by\nknow that the characteristic polynomial of M\n\u0014\n\u0393 \u2212 \u03bbIn\nb\nc\nf (\u03bb) \u2261 det(M \u2212 \u03bbI2n ) = det(\n=\n\nn\nY\n\u0002\n1\n( \u2212 \u03bb)2 det(1 + e\u22a4\n\u03c9i\ni=1\n\n=\u2212\n\nn\nn\nY\nX\n1\n( \u2212 \u03bb)2\n\u03c9i\ni=1\nj=1\n\ncj \u03bb\n.\n\u2212\u03bb\n\n\u0015\n\n\u0014\n\n\u0015\n\u0003\n(\u2212In + \u03b7\u0393\u22121 )q \u0002 \u22a4\n+\ne\nq\u22a4 )\n(\u2212In \u2212 \u03b7\u2206\u22121 )e\n\u2206 \u2212 \u03bbIn\n\u0014\n\u0015\u0014\n\u0015\n\u0003 (\u0393 \u2212 \u03bbIn )\u22121\n(\u2212In + \u03b7\u0393\u22121 )q\nq\u22a4\n)\n(\u2206 \u2212 \u03bbIn )\u22121 (\u2212In \u2212 \u03b7\u2206\u22121 )e\n\n(3.4)\n\n1\n\u03c9j\n\nc are precisely those of M .\nFrom (3.4), we know that the eigenvalues of M\nc\nTheorem 3.3. The matrix M defined by equation (3.2) is a Z-matrix if and only if the\nparameter \u03b7, defined in (3.9) satisfy\n0<\u03b7\u2264\nProof. From (3.2),\nNote that\nb =\nD\nb =\nB\n\n1\n.\n\u03c91\n\n(3.5)\n\nc is a Z-matrix if and only if B\nb \u2265 0, C\nb \u2265 0, and D\nb and A\nb are Z-matrices.\nM\nb =\n\u0393 + (\u2212In + \u03b7\u0393\u22121 )qe\u22a4 , C\n\u22121\n\u22a4\nb =\n(I + \u03b7\u2206 )ee > 0,\nA\n\n(In \u2212 \u03b7\u0393\u22121 )qq \u22a4 ,\n\u2206 + (\u2212In \u2212 \u03b7\u2206\u22121 )eq \u22a4 .\n\n(3.6)\n\nc is a Z-matrix is that C\nb \u2265 0, and\nThe sufficient and necessary condition such that the matrix M\nb\nb\nD and A are Z-matrices. This implies that\n\u2212 1 + \u03b7\u03c91 \u2264 0.\n\n(3.7)\n\nc is a Z-matrix if and only if (3.5) is satisfied.\nSince \u03b7 is positive, we have the fact that M\nc is an\nUsing Corollary 3.2 and the given constraint (3.5) in Theorem 3.3, we know that M\nirreducible M-matrix and the SDA algorithm is guaranteed to be applicable. It is known that\nb of the single shifted NARE is equivalent to the minimal\nthe minimal nonnegative solution X\nnonnegative solution X of (1.1) [8]. Thus, we have\np\nk\nb \u2212 CX))\u03c1(C\nb\nb bb\n(3.8)\nlim sup 2 kHk \u2212 Xk \u2264 \u03c1(C\u03b3 (D\n\u03b3 (A \u2212 B Y )) < 1,\nk\u2192\u221e\n\nb \u2212 CX)\nb\nb\u2212 B\nb Yb )) = 1. It concludes that the convergence of the SDA\nsince \u03c1(C\u03b3 (D\n< 1 and \u03c1(C\u03b3 (A\nalgorithm with a single shift is faster than that with no shift. Based on all the properties stated\nabove, it is illuminating to begin the analysis of the double shifting technique.\n\n\f8\n3.2. Double Shift. In order to remove all zero eigenvalues of H, we define the double shifted\nmatrix H,\n\u0014\n\u0015\nD \u2212C\n\u22a4\n\u22a4\nH = H + \u03b7vr + \u03besu =\n,\n(3.9)\nB \u2212A\nwhere \u03b7 > 0, \u03be < 0, p\u22a4 and q \u22a4 such that p\u22a4 v = q \u22a4 u = 1, each size of sub-matrices A, B, C and\nD are n square. This is the so called double shifting technique. Indeed, it can be seen that if we\n\u22a4\nchoose s\u22a4 = [s\u22a4\n1 , s2 ] with s1 = q and s2 = \u2212e and the same vectors r, u and v as defined above,\nthen the vectors p and q satisfy the fact that\nr\u22a4 v = s\u22a4 u = e\u22a4 \u0393\u22121 q + q \u22a4 \u2206\u22121 e = 1.\n\n(3.10)\n\nFrom Lemma 3.1, we know that the double shifting technique will move one zero eigenvalue of H\nto \u03b7 > 0 and the other to \u03be < 0 and keep the nonzero eigenvalues unchanged. With this in mind,\nthe shift technique introduced in formula (3.9) will make the new matrix H nonsingular. Also,\nwe can define a duble shifted NARE in X \u2208 Rn\u00d7n associate with the matrix H as follows:\nX C X \u2212 X D \u2212 A X + B = 0,\n\n(3.11a)\n\nand the dual duble shifted NARE in Y \u2208 Rn\u00d7n\nY B Y \u2212 Y A \u2212 D Y + C = 0,\n\n(3.11b)\n\nwhere\nD = D + \u03b7v1 r1\u22a4 + \u03bes1 u\u22a4\n1,\nB = B + \u03b7v2 r1\u22a4 + \u03bes2 u\u22a4\n1,\n\nC = C \u2212 \u03b7v1 r2\u22a4 \u2212 \u03bes1 u\u22a4\n2,\nA = A \u2212 \u03b7v2 r2\u22a4 \u2212 \u03bes2 u\u22a4\n2.\n\n(3.12)\n\nIn what follows, we show that under suitable assumptions on parameters \u03b7 and \u03be, the matrix\nM defined by\n\u0014\n\u0015\nD \u2212C\nM=\n,\n(3.13)\n\u2212B A\nis a nonsingular M-matrix, that is, the SDA algorithm is well-defined and applicable to the\nNARE (3.11a). We start our proof by showing that this matrix M is a Z-matrix for some\nparameters \u03b7 and \u03be.\nTheorem 3.4. The matrix M defined by equation (3.13) is a Z-matrix if and only if the\nparameters, \u03b7 and \u03be, defined in (3.9) satisfy the following two conditions:\n0<\u03b7<\n\n1\n,\n\u03c91\n\n\u22121 + \u03b7\u03c91\n\u2264 \u03be < 0.\n\u03c91\n\n(3.14a)\n(3.14b)\n\nProof. It follows from (3.13) we know that M is a Z-matrix if and only if B \u2265 0, C \u2265 0, and\nD and A are Z-matrices. Also, from (3.12) we have\nD = \u0393 + (\u2212In + \u03b7\u0393\u22121 )qe\u22a4 + \u03beqe\u22a4 \u0393\u22121 ,\nC = (In \u2212 \u03b7\u0393\u22121 )qq \u22a4 + \u03beqq \u22a4 \u2206\u22121 ,\nB = (I + \u03b7\u2206\u22121 )ee\u22a4 \u2212 \u03beee\u22a4 \u0393\u22121 > 0,\nA = \u2206 + (\u2212In \u2212 \u03b7\u2206\u22121 )eq \u22a4 \u2212 \u03beeq \u22a4 \u2206\u22121 .\n\n\f9\nTherefore, in order to get a Z-matrix M , we only need to consider when C \u2265 0, and D and A are\nZ-matrices. This gives rise to the following three sufficient and necessary conditions:\n\uf8f1\n\uf8f2 \u22121 + \u03b7\u03c91 + \u03be\u03c9n\n\u22121 + \u03b7\u03c91 \u2212 \u03be\u03c91\n\uf8f3\n\u22121 \u2212 \u03b7\u03c9n \u2212 \u03be\u03c91\n\n\u2264\n\u2264\n\u2264\n\n0,\n0,\n0.\n\n(3.15)\n\nIt follows from (3.15) and the initial conditions \u03b7 > 0 and \u03be < 0 that M is a Z-matrix if and\nonly if (3.14a) and (3.14b) are satisfied.\nTo simplify our discussion, we define\n\n\u03a9 = {(\u03b7, \u03be); 0 < \u03b7 <\n\n1 \u22121 + \u03b7\u03c91\n,\n\u2264 \u03be < 0}.\n\u03c91\n\u03c91\n\n(3.16)\n\nOur next approach is to show that the matrix M is indeed an M-matrix. That is, the iterative\nprocesses in SDA algorithm do not break down and convergence quadratically. To begin with, we\nintroduce the following two lemmas.\nLemma 3.5. Let ci and \u03c9i , for i = 1, . . . , n, be defined in (1.1). Given \u03bb \u2208 R and \u03bb 6=\nfor i = 1, . . . , n, we define\n\ng1 (\u03bb) = \u03bb\n\nn\nX\ni=1\n\nci\n,\n1\n\u03c9i \u2212 \u03bb\n\ng2 (\u03bb) =\n\nn\nX\nci \u03c9 i\n,\n1\n\u2212\u03bb\ni=1 \u03c9i\n\ng3 (\u03bb) =\n\nn\nX\n\nci\n.\n\u2212 \u03bb)\n\n\u03c9(1\ni=1 i \u03c9i\n\n1\n\u03c9i ,\n\n(3.17)\n\nThen, the following properties hold:\nn\nP\nci \u03c9 i .\n1. g1 (\u03bb) \u2212 \u03bb2 g2 (\u03bb) = \u03bb\ni=1\n\n2. g1 (\u03bb) \u2212 g3 (\u03bb) = \u22121.\n1\n3. If \u03bb \u2208 ( \u03c91k , \u03c9k+1\n), then g3 (\u03bb) \u2265 g1 (\u03bb) \u03bb\u03c91 k \u2265 g2 (\u03bb) \u03c91k .\n\nProof. The first two properties are following from the direct computation. To see this,\napplying the conditions in (3.17), we have\n\ng1 (\u03bb) \u2212 \u03bb2 g2 (\u03bb) = \u03bb\n=\u03bb\n\nn\nX\n(ci \u2212 \u03bbci \u03c9i )\u03c9i\ni=1\nn\nX\n\n\u03c9i ( \u03c91i \u2212 \u03bb)\n\nci \u03c9 i .\n\ni=1\n\ng1 (\u03bb) \u2212 g3 (\u03bb) =\n\nn\nX\n(\u03bbci \u03c9i \u2212 ci )\ni=1\n\n\u03c9i ( \u03c91i \u2212 \u03bb)\n\n= \u22121.\n\n\f10\n1\nUsing the triangle inequality and \u03bb \u2208 ( \u03c91k , \u03c9k+1\n), we obtain\n\ng3 (\u03bb) =\n\u2265\n\nn\nX\n\nci\n\u2212 \u03bb)\n\n\u03c9(1\ni=1 i \u03c9i\n\nk\nX\ni=1\n\nci\n\u03c9k ( \u03c91i \u2212 \u03bb)\n\n\u2265 g1 (\u03bb)\n\u2265\n\nk\nX\ni=1\n\n\u2265 g2 (\u03bb)\n\n+\n\nn\nX\n\ni=k+1\n\n1\n\u03bb\u03c9k\n\nci\n\u03c9k+1 ( \u03c91i \u2212 \u03bb)\n\nn\nX\nci \u03c9k+1\nci \u03c9 k\n+\n1\n( \u03c9i \u2212 \u03bb) i=k+1 ( \u03c91i \u2212 \u03bb)\n\n!\n\n1\n\u03c9k\n\n1\n.\n\u03c9k\n\nWe have now seen that the relationships among g1 (\u03bb), g2 (\u03bb), and g3 (\u03bb). Let g(\u03bb) to be a\nfunction satisfying\ng(\u03bb) \u2261 \u03bbg1 (\u03bb) + \u03b7\u03beg2 (\u03bb)g3 (\u03bb),\n\n(3.18)\n\n1\nwhere (\u03b7, \u03be) \u2208 \u03a9. Our next approach is to show that for each subinterval ( \u03c91k , \u03c9k+1\n) with k =\n1, . . . , n \u2212 1, there exists a point \u03bb so that g(\u03bb) > 0. This property is a stepping stone for showing\nthat M is an M-matrix.\nLemma 3.6. Let ci and \u03c9i , for i = 1, . . . , n, be defined in (1.1). It then follows that there\n1\nexists a point \u03bbk \u2208 ( \u03c91k , \u03c9k+1\n), for k = 1, . . . , n \u2212 1, so that the function g(\u03bb) of (3.18) is greater\nthan zero.\n1\nProof. Note that g3 (\u03bb) is a continuous function on ( \u03c91k , \u03c9k+1\n), lim + g3 (\u03bb) = \u2212\u221e , and\n\u03bb\u2192 \u03c91\n\nk\n\nlim\n\u03bb\u2192 \u03c9\n\n1\nk+1\n\n1\n) such\ng3 (\u03bb) = +\u221e, for all k = 1, . . . , n \u2212 1. Thus, there exists a point \u03bbk \u2208 ( \u03c91k , \u03c9k+1\n\u2212\n\nthat\ng3 (\u03bbk ) =\n\n4\u03c912\n.\n\u03c9k \u03c9k+1\n\n(3.19)\n\nSince \u03c91 > \u03c92 > * * * > \u03c9n , we have the fact that g3 (\u03bbk ) > 4. It follows from Lemma 3.5 that\ng1 (\u03bbk ) > 0.\nWe first assume that g2 (\u03bbk ) < 0 for this specific \u03bbk , then it is clear that g(\u03bbk ) = \u03bbk g1 (\u03bbk ) +\n\u03b7\u03beg2 (\u03bbk )g3 (\u03bbk ) > 0, since \u03b7\u03be < 0. We now assume that g2 (\u03bbk ) > 0. Combining the inequalities (3.14a) with (3.14b), we have\n\u2212\n\n1\n\u2264 \u03b7\u03be < 0.\n4\u03c912\n\n(3.20)\n\nThen, by (3.18) we get\ng(\u03bbk ) \u2265\n\nn c (\u03bb \u2212\nX\ni k\ni=1\n\n=\n\n1\n\u03c9i\n\n\u2212 \u03bbk\n\nk c (\u03bb \u2212\nX\ni k\ni=1\n\n1\n\u03c9i\n\n\u03c9i\n\u03c9k \u03c9k+1 )\n\n\u03c9i\n\u03c9k \u03c9k+1 )\n\n\u2212 \u03bbk\n\n+\n\nn\nX\nci (\u03bbk \u2212\n\ni=k+1\n\n1\n\u03c9i\n\n\u03c9i\n\u03c9k \u03c9k+1 )\n\n\u2212 \u03bbk\n\n\u2265 0,\n\n(3.21)\n\n\f11\ni\nsince \u03bbk \u2212 \u03c9k \u03c9\u03c9k+1\n<\nk + 1 \u2264 i \u2264 n.\n\n1\n\u03c9k+1\n\n\u03c9k\n\u03c9k \u03c9k+1\n\n\u2212\n\n= 0, for 1 \u2264 i \u2264 k, and \u03bbk \u2212\n\n\u03c9i\n\u03c9k \u03c9k+1\n\n> \u03c9k \u2212\n\n\u03c9k+1\n\u03c9k \u03c9k+1\n\n= 0, for\n\nNow we have enough tools to validate that the given matrix M is indeed an M-matrix. In\nparticular, we can also dig out the eigenvalue distribution of matrix M .\nTheorem 3.7. If (\u03b7, \u03be) \u2208 \u03a9, then the matrix M defined by equation (3.13) is an M-matrix.\nIn particular, M has 2n positive real eigenvalues \u03bb1 , . . . , \u03bb2n satisfying\n0 < \u03bb1 < \u03bb2 <\n\n1\n1\n1\n1\n< \u03bb3 < \u03bb4 <\n< *** <\n< \u03bb2n\u22121 < \u03bb2n <\n\u03c91\n\u03c92\n\u03c9n\u22121\n\u03c9n\n\n(3.22)\n\nProof. Since the matrix H of (3.9) is nonsingular, it is clear that M = JH is nonsingular.\nAlso, Theorem 3.4 implies M is a Z-matrix. In order to show that M is an M-matrix, it suffices to\nshow that all eigenvalues of M have positive real part. Indeed, all eigenvalues of M are positive\nreal numbers and satisfy the interlacing property.\nWe first consider the characteristic polynomial f \u0304(\u03bb) of M defined by\nf \u0304(\u03bb) \u2261 det(M \u2212 \u03bbI2n )\n\u0014\n\u0393 \u2212 \u03bbIn\n= det(\n=\u2212\n\n=\u2212\n\nn\nY\n\n\u2206 \u2212 \u03bbIn\n\ni=1\n\n1\n\u2212 \u03bb)2 g(\u03bb)\n\u03c9i\n\nn\nX\n\nY\n\n(\n\ni=1\n\ncj\n\n1\u2264s\u2264n,s6=i\n\n\u0015\n\n+\n\n\u0014\n\n(\u2212In + \u03b7\u0393\u22121 )q\n(\u2212In \u2212 \u03b7\u2206\u22121 )e\n\nq\ne\n\n\u0015\u0014\n\ne\u22a4\n\u22a4 \u22121\n\u03bee \u0393\n\n\u0015\nq\u22a4\n)\n\u2212\u03beq \u22a4 \u2206\u22121\n(3.23)\n\n\uf8ee\nn\nX\n1\n\u03bb\n\u03be\u03b7\u03c9k\n( \u2212 \u03bb)\nck \uf8f0(\u2212\u03bb2 +\n+\n)\n\u03c9i\n\u03c9k\n\u03c9j\nk=1\n\nY\n\n1\u2264s\u2264n,s6=k\n\n\uf8f9\n1\n\u2212 \u03bb)\uf8fb , (3.24)\n(\n\u03c9s\n\nwhere g(\u03bb) is the function given in (3.18). By direct substitution of \u03c91k in (3.24), we have\nf \u0304( \u03c91k ) > 0, for k = 1, . . . , n. Also, it follows from (3.23) that f \u0304(0) > 0. If we can find a point \u03bb\n1\nsatisfying f \u0304(\u03c9) < 0 in each subinterval ( \u03c91k , \u03c9k+1\n), for k = 1, . . . , n and the interval (0, \u03c911 ), then\nthe intermediate value theorem imply that the distribution of eigenvalues of M arranged in (3.22)\nis valid. This also gives rise to the fact that M is a nonsingular M-matrix.\nNext, we consider the subinterval (0, \u03c911 ). Choosing \u03bb = 2\u03c91 1 , it follows that\ng(\n\nn\n1\n1 X\n)=\n2\u03c91\n2\u03c91 i=1\n\n\u2265 (c1 +\n\n1\n\u03c9i\n\nn\nX\ni=2\n\n\u22650\n\nn\nX\nci\n+\n\u03b7\u03be\n\u2212 2\u03c91 1\ni=1\n\nn\nci \u03c9 i X\nci\n1\n1\n1\n\u03c9i \u2212 2\u03c91 i=1 \u03c9i ( \u03c9i \u2212\n\n1\n2\u03c91 )\nn\nX\n\nn\nX\nci \u03c9i ( \u03c9\u03c91i )\nci \u03c9 i\nci \u03c9 1\n) \u2212 (c1 +\n)(c1 +\n)\n2\u03c91 \u2212 \u03c9i\n2\u03c9i \u2212 \u03c9i\n2\u03c9i \u2212 \u03c9i\ni=2\ni=2\n\n(3.25)\n(3.26)\n\nThe second inequality (3.25) comes from the fact that \u03b7\u03be \u2265 \u2212 4\u03c91 2 . Also, since \u03c9\u03c9k1 < 1 and\n1\nn\nP\nci\u03c91\nc\n=\n1,\nwe\nhave\nthe\nlast\ninequality\n(3.26). For the proof\n<\nc\n,\nfor\ni\n=\n2,\n.\n.\n.\n,\nn,\nand\ni\ni\n2\u03c91 \u2212\u03c9i\ni=1\n\n1\nof each subinterval ( \u03c91k , \u03c9k+1\n), we simply apply the conclusion of Lemma 3.6. Then, (3.23)\n1\nimmediately implies that there exists a point \u03bb \u2208 ( \u03c91k , \u03c9k+1\n) such that f (\u03bb) < 0, for k = 1, . . . , n.\n\nNote that in [8] the minimal nonnegative solution X of (1.1) has been shown to be a solution\nof (3.11a). So far, to the best of our knowledge, no study has investigated the relation between\n\n\f12\nthe solutions X and X. If there does not exist any relation between X and X, the solution\nobtained from the duble-shift algorithm would be exclusively meaningless. Our next result is to\nfind this substantial link through the known fact that M is indeed an M-matrix (3.7).\nTheorem 3.8. Let X and X be the minimal nonnegative solutions of (3.11a) and (1.1),\nrespectively. Then, \u03c3(D \u2212 CX) = {\u03b7, \u03bb2 , . . . , \u03bbn } and X = X.\nProof. Let R(Z) = ZCZ \u2212 ZD \u2212 AZ + B and R(Z) = ZCZ \u2212 ZD \u2212 AZ + B. Observe first\nthat\n\u22a4\nR(X) = R(X) \u2212 \u03b7(Xv1 \u2212 v2 )(r2\u22a4 X + r1\u22a4 ) + \u03be(Xs1 \u2212 s2 )(\u2212u\u22a4\n2 X \u2212 u1 ) = R(X),\n\n(3.27)\n\nwhere the second equality follows directly from Theorem 2.6. This equality amounts to say\nthat the minimal nonnegative solution of (1.1) is also a nonnegative solution of (3.11a) and the\nfollowing equality is satisfied.\n\u0014 \u0015 \u0014 \u0015\nI\nI\nH n = n (D \u2212 CX).\n(3.28)\nX\nX\n\u22a4\nRecall that u\u22a4\n1 + u2 X = 0. Then, we have\n\n\u22a4\n(D \u2212 CX) = D \u2212 CX + \u03b7v1 (r1\u22a4 + r2\u22a4 X) + \u03bes1 (u\u22a4\n1 + u2 X)\n= D \u2212 CX + \u03b7v1 (r1\u22a4 + r2\u22a4 X).\n\nTogether with the fact that\n(D \u2212 CX)v1 = (\u0393 \u2212 qe\u22a4 )\u0393\u22121 q \u2212 qq \u22a4 \u2206\u22121 e = 0,\nand\n(r1\u22a4 + r2\u22a4 X)v1 = e\u22a4 \u0393\u22121 q + q \u22a4 \u2206\u22121 e = 1,\nwe obtain\n(D \u2212 CX)v1 = (D \u2212 CX)v1 = \u03b7v1 .\nThen, Lemma 3.1 and Theorem 2.5 imply that \u03c3(D \u2212 CX) = {\u03b7, \u03bb2 , . . . , \u03bbn }.\nSince M is a nonsingular M-matrix and X is the minimal nonnegative solution of (3.11a),\nTheorem 2.3 and Theorem 2.4 imply that \u03c3(D \u2212 C X) \u2282 C+ . With this in mind, we have\n\u03c3(D \u2212 C X) = \u03c3(D \u2212 CX).\n\n(3.29)\n\n\u0014 \u0015 \u0014 \u0015\nIn\nI\n= n (D \u2212 C X).\nX\nX\n\n(3.30)\n\nNote that\nH\nBy (3.29) and (3.30), it is true that\n\u0014 \u0015\n\u0014 \u0015\nI\nIn\n= span n .\nspan\nX\nX\nThen, there exists a nonsingular matrix S \u2208 Rn\u00d7n such that\n\u0014 \u0015 \u0014 \u0015\nI\nIn\n= n S.\nX\nX\n\n\f13\nIt is clear that this nonsingular matrix S is an identity matrix. So, we conclude that X = X.\nFrom Theorem 3.7 and Theorem 3.8, we know that M is a nonsingular M-matrix. Then, the\nSDA algorithm is guaranteed to converge. Similar to the discussion given in the single shifted\nalgorithm, we have\np\nk\n(3.31)\nlim sup 2 kHk \u2212 Xk \u2264 \u03c1(C\u03b3 (D \u2212 CX))\u03c1(C\u03b3 (A \u2212 BY )) < 1,\nk\u2192\u221e\n\nsince \u03c1(C\u03b3 (D \u2212 CX) < 1 and \u03c1(C\u03b3 (A \u2212 BY )) < 1. This also implies that for any (\u03b7, \u03be) \u2208 \u03a9, the\nSDA algorithm with double shifts converges faster than the SDA algorithm with no shift and is\nquadratically convergent.\n4. Advantages of the Shifting Technique Applied to SI. In [15], Lu shows that the\nminimal nonnegative solution X of (1.1) must be of the form:\nX = T \u25e6 (mn\u22a4 ) = (mn\u22a4 ) \u25e6 T.\n\u0010\n\u0011\n1\nHere, the symbol \u25e6 is the Hadamard product, T = (tij ) = \u03b4i +\u03b3\n, and (m, n) is satisfying the\nj\nvector equation:\nm = m \u25e6 (P n) + e,\nn = n \u25e6 (Qm) + e,\n\n(4.1a)\n(4.1b)\n\nwith\nP = (Pij ) =\n\n\u0012\n\nqj\n\u03b4 i + \u03b3j\n\n\u0013\n\n,\n\nQ = (Qij ) =\n\n\u0012\n\nqj\n\u03b4 j + \u03b3i\n\n\u0013\n\n.\n\n(4.2)\n\nThe SI method for finding the minimal nonnegative solution (m, n) is then given by\nm(k+1) = m(k) \u25e6 (P n(k) ) + e,\nn\n\n(k+1)\n\n=n\n\n(k)\n\n\u25e6 (Qm\n\n(k)\n\n(4.3a)\n\n) + e.\n\n(4.3b)\n\nOur aim in this section is to discuss how the shifted approaches can speed up the SI method.\nTheoretical discussion is also given to analyze the convergence of the SI method with shift. We\nthen rewrite the coefficient matrices (3.12) as\nD = D(\u03b7, \u03be) = \u0393 \u2212 Q1 (\u03b7)E1 (\u03be)\u22a4 ,\nC = C(\u03b7, \u03be) = Q1 (\u03b7)Q2 (\u03be)\u22a4 ,\n\n(4.4a)\n(4.4b)\n\nB = B(\u03b7, \u03be) = E2 (\u03b7)E1 (\u03be)\u22a4 ,\nA = A(\u03b7, \u03be) = \u2206 \u2212 E2 (\u03b7)Q2 (\u03be)\u22a4 ,\n\n(4.4c)\n(4.4d)\n\nwith\n\u0002\nQ1 = Q1 (\u03b7) = \u0002 (In \u2212 \u03b7\u0393\u22121\u0003)q\nE1 = E1 (\u03be) = e \u2212\u03be\u0393\u22121 e ,\n\n\u0003\n\u0002\n\u0003\nq , Q2 = Q2 (\u03be) = \u0002 q \u03be\u2206\u22121 q ,\nE2 = E2 (\u03b7) = (In + \u03b7\u2206\u22121 )e\n\n\u0003\ne ,\n\nand relax the boundary conditions (\u03b7, \u03be) so that (\u03b7, \u03be) \u2208 \u03a9\u0304. Here, \u03a9\u0304 is the closure of the set \u03a9\ndefined in (3.16). Substituting (4.4) into (3.11a), we have\n\u22a4\nZ\u0393 + \u2206Z = (ZQ1 + E2 )(Q\u22a4\n2 Z + E1 ).\n\n(4.5)\n\n\f14\nThis implies that the minimal nonnegative solution Z of (3.11a) can be written as\nZ = T \u25e6 (M N \u22a4 ),\n\n(4.6)\n\n\u22a4\n2\u00d7n\nwith M = ZQ1 + E2 \u2208 Rn\u00d72 , N \u22a4 = Q\u22a4\n.\n2 Z + E1 \u2208 R\nAkin to the iteration given in (4.3), the iteration sequence {Mk , Nk } corresponding to (4.6)\ncan be written as\n\nMk+1 = (T \u25e6 (Mk Nk\u22a4 ))Q1 + E2 ,\nNk+1 = (T \u25e6\n\n(Nk Mk\u22a4 ))Q2\n\n+ E1 ,\n\n(4.7a)\n(4.7b)\n\nwith the initial value\nM0 = 0, N0 = 0.\n\n(4.7c)\n\nLet Zk (\u03b7, \u03be) = Zk = T \u25e6 (Mk Nk\u22a4 ), for all k. Corresponding to (4.5), we then have the classical\nfixed-point iteration,\n\u0001\n\u22a4\nZk+1 \u2261 T \u25e6 (Zk Q1 + E2 )(Q\u22a4\n2 Zk + E1 ) .\n\n(4.8)\n\nOur next theorem is to show that the sequence {Zk } does indeed converge and converge to\nthe minimal nonnegative solution X of (1.1).\nTheorem 4.1. Assume that\nR(X \u2217 ) = X \u2217 CX \u2217 \u2212 X \u2217 D \u2212 AX \u2217 + B \u2264 0,\n\n(4.9)\n\nfor some nonnegative matrix X \u2217 . Then for the fixed-point iteration (4.8) with initial value Z0 = 0,\nwe have\nZ0 < Z1 < * * * < Zk < X \u2217 , for any k \u2265 1.\n\n(4.10)\n\nMoreover, lim Zk (\u03b7, \u03be) = X for any (\u03b7, \u03be) \u2208 \u03a9.\nk\u2192\u221e\n\n\u22a4\nProof. By (4.4), Q1 E1\u22a4 \u2265 0, Q1 Q\u22a4\n2 = C \u2265 0, E2 E1 = B \u2265 0, and E2 Q2 \u2265 0. It follows\nthat (4.10) holds by induction. Since the sequence {Zk } is monotonically increasing and bounded\nabove, we have lim Zk = Z \u2217 , for some Z \u2217 . Hence R(Z \u2217 ) = 0. On the other hand, since Z \u2217 \u2264 X \u2217\nk\u2192\u221e\n\nfor any nonnegative matrix X \u2217 , we have Z \u2217 = X.\nThe convergence property, shown in Theorem 4.1, is of fundamental importance in our subsequence discussion and can induce the possibility of analyzing a number of convergent behaviors\nin the SI method with shift. Note that since Mk and Nk are matrices in Rn\u00d72 , we can define\nh\ni\nh\ni\n(k)\n(k)\n(k)\nMk = m(k)\n,\nN\n=\n,\n(4.11)\nm\nn\nn\nk\n1\n2\n1\n2\n(k)\n\n(k)\n\n(k)\n\n(k)\n\nwhere m1 , m2 , n1 and n2 are n-dimension column vectors. It follows that we have the\nequivalent iteration for Zk , that is,\n\u0010\n\u0011\n(k) (k)\n(k) (k)\nZk = T \u25e6 m1 (n1 )\u22a4 + m2 (n2 )\u22a4 .\n(4.12)\n\n\f15\nThen, we obtain the new algorithm of the SI with shift, given by\n(k+1)\n\nm1\n\n= Zk (In \u2212 \u03b7\u0393\u22121 )q + (In + \u03b7\u0393\u22121 )e,\n\n(4.13a)\n\n(k+1)\nm2\n(k+1)\nn1\n(k+1)\nn2\n\n= Zk q + e,\n\n(4.13b)\n\n= Zk\u22a4 q + e,\n= \u2212\u03be(\u0393\n\n\u22121\n\n(4.13c)\n\ne\u2212\n\nZk\u22a4 \u2206\u22121 q).\n\n(4.13d)\n\nwith the initial value\n(0)\n\n(0)\n\nm1 = 0, m2 = 0,\n(0)\nn1\n\n= 0,\n\n(0)\nn2\n\n(4.14a)\n\n= 0.\n\n(4.14b)\n2\n\nIt is true that this SI iteration with shift is still a method with a cost of O(n ) ops but\nrequires more calculations than the original SI method. However, in order to have a method with\na better behavior, adding some complexity is sometimes a necessary sacrifice. Actually, we can\nsimplify our computation by consider the following iteration,\n\u0014 \u0015\n\u0003 q\n\u0002\n(k+1)\n,\nm2\n= Zk In\ne\n\u0014\n\u0015\n\u0003 \u2212\u0393\u22121 q\n\u0002\n(k+1)\n(k+1)\n,\nm1\n= m2\n+ \u03b7 Zk In\n\u2206\u22121 e\n\u0014 \u0015\n\u0003 q\n\u0002\n(k+1)\n,\nn1\n= Zk\u22a4 In\ne\n\u0014\n\u0015\n\u0003 \u2212\u2206\u22121 q\n\u0002 \u22a4\n(k+1)\n.\nn2\n= \u2212\u03be Zk In\n\u0393\u22121 e\n(k)\n\n(k)\n\n(k)\n\n(k)\n\nIn next theorem, we discuss the convergent property of the sequence (m1 , m2 , n1 , n2 )\nand the convergent speed of the sequence Zk .\n(k)\n(k)\n(k)\n(k)\nTheorem 4.2. Given (\u03b1, c) = (0, 1), the sequence (m1 , m2 , n1 , \u2212n2 ) with initial values(4.14) is strictly monotonically increasing and satisfies the following two conditions:\n(k)\n(k)\n(k)\n(k)\na. e \u2264 m1 \u2264 m, e \u2264 m2 \u2264 m, e \u2264 n1 \u2264 n, 0 \u2264 n2 \u2264 \u2212\u03be\u0393\u22121 e.\n(k)\n(k)\n(k)\n(k)\nb. lim m1 = lim m2 = m, lim n2 = n, lim n2 = 0,\nk\u2192\u221e\n\nk\u2192\u221e\n\nk\u2192\u221e\n\nk\u2192\u221e\n\nwhere m and n are defined on (4.1). In fact, in the critical case, we have m = n and X = X \u22a4 .\nProof. From Theorem 4.1, we know that Z0 < Z1 < . . . < Zk < X and lim Zk = X.\nk\u2192\u221e\n\nSubstituting these two facts to (4.13), we immediately have\ne\ne\ne\n\u2212\u03be\u0393\u22121 e\n\n\u2264\n\u2264\n\u2264\n\u2265\n\n(1)\n\nm2\n(1)\nm1\n(1)\nn1\n(1)\nn2\n\n<\n<\n<\n>\n\n(2)\n\nm2\n(2)\nm1\n(2)\nn1\n(2)\nn2\n\n<\n<\n<\n>\n\n...\n...\n...\n...\n\n<\n<\n<\n>\n\n(k)\n\nm2\n(k)\nm1\n(k)\nn1\n(k)\nn2\n\n\u2264\n\u2264\n\u2264\n\u2265\n\nXq + e = m,\nXq + e = m,\nX \u22a4 q + e = n,\n\u2212\u03be(\u0393\u22121 e \u2212 X \u22a4 \u2206\u22121 q) = 0.\n\n(k)\n\nNote that the order of the sequence{m1 } comes from the fact that Zk q \u2212 \u03b7Zk \u0393\u22121 q > 0 and the\n(k)\nlast equality of the sequence {n2 } comes from Theorem 2.6.\nFrom (4.12), part (b) holds, since lim Zk = X.\nk\u2192\u221e\n\nWhen we studied the shifted procedures, our main purpose is to speed up the convergence.\nIn what follows we discuss the relations of Zk (\u03b7, \u03be) with respect to different \u03b7 and \u03be values and\nshow that the SI with shift converges linear, instead of sublinear.\nTheorem 4.3. Given (\u03b1, c) = (0, 1), the sequence {Zk } has the following two properties:\n\n\f16\na. Zk (0, 0) \u2264 Zk (\u03b7, 0) \u2264 Zk (\u03b7, \u03be), for each k and (\u03b7, \u03be) \u2208 \u03a9.\nb. The sequence {Zk (\u03b7, \u03be)} converges linearly to the minimal nonnegative solution X of\n(1.1) for all (\u03b7, \u03be) \u2208 \u03a9.\nProof. From (4.6), we have\n\u0001\nZ = T \u25e6 ZQ1 (\u03b7)Q2 (\u03be)\u22a4 Z + E2 (\u03b7)Q2 (\u03be)\u22a4 Z + ZQ1 (\u03b7)E1 (\u03be)\u22a4 + E2 (\u03b7)E1 (\u03be)\u22a4\n\u0001\n= T \u25e6 (ZQ1 (0)Q2 (0)\u22a4 Z + E2 (0)Q2 (0)\u22a4 Z + ZQ1 (0)E1 (0)\u22a4 + E2 (0)E1 (0)\u22a4\n\u0001\n\u0001\n+ \u03b7T \u25e6 (\u2212Z\u0393\u22121 q + \u2206\u22121 e)(q \u22a4 Z + e\u22a4 ) \u2212 \u03beT \u25e6 (Zq + e)(\u2212q \u22a4 \u2206\u22121 Z + e\u22a4 \u0393\u22121 ) .\n\nSubsequently, it follows from mathematical induction that part (a) holds.\nFor the proof of part (b), we need to use three well-known results discussed in [9]. First, for\nthe iteration (4.8) and Z0 (\u03b7, \u03be) = 0, we have\np\nlim sup k kZk (\u03b7, \u03be) \u2212 Xk\nk\u2192\u221e\n\u0002\n\u0003\u0001\n= \u03c1 (I \u2297 \u2206 + \u0393 \u2297 I)\u22121 I \u2297 (E2 E1\u22a4 + XC) + (Q1 E1\u22a4 + CX) \u2297 I ,\n(4.15)\n\nwhere \u2297 denotes the Kronecker product (see [9, Theorem 3.2]). Second, let MX = I \u2297 (A\u2212 XC)+\n(D \u2212 CX)\u22a4 \u2297 I. Then, MX is a Z-matrix since both A \u2212 XC and D \u2212 CX are Z-matrices. (see\n[9, Remark 1.1]). Also, MX is a nonsingular matrix since any eigenvalue of MX is the sum of an\neigenvalue of A \u2212 XC and D \u2212 CX. This implies that MX is a nonsingular M-matrix. Third, if\nMX is a nonsingular M-matrix, then\n\u0003\u0001\n\u0002\n(4.16)\n\u03c1 (I \u2297 \u2206 + \u0393 \u2297 I)\u22121 I \u2297 (E2 E1\u22a4 + XC) + (Q1 E1\u22a4 + CX) \u2297 I < 1,\np\nthat is, lim sup k kZk (\u03b7, \u03be) \u2212 Xk < 1. (see [9, Theorem 3.3])\nk\u2192\u221e\n\n5. Numerical Implementation and Comparisons. To illustrate the consequence of the\nprevious sections, numerical experiments, consisting of SDA and SI methods after the shifting\ntechnique, are presented to demonstrate our conclusion. All computations are performed in\nMATLAB/version 2010b on a iMac with an 2.8GHZ Intel Core i5 processor and 16GB main\nmemory, using IEEE double-precision.\nIn the next implementations, the relative error for the SDA is defined by\n\u001a\n\u001b\nkGk \u2212 Gk\u22121 k\u221e kHk \u2212 Hk\u22121 k\u221e\nErrSDA = max\n,\n,\nkGk k\u221e\nkHk k\u221e\nthe relative error for the SI with no shift is defined by\n\u001b\n\u001a\nkm(k) \u2212 m(k\u22121) k\u221e kn(k) \u2212 n(k\u22121) k\u221e\n,\nErrSI = max\n,\nkm(k) k\u221e\nkn(k) k\u221e\nthe relative error for the SI with the shifting procedure is defined by\n\u001a\n\u001b\nkMk \u2212 Mk\u22121 k\u221e kNk \u2212 Nk\u22121 k\u221e\nErrSIS = max\n,\n,\nkMk k\u221e\nkNk k\u221e\nand the relative normalized residual is defined by\nRes =\n\nkXk \u0393 + \u2206Xk \u2212 (Xk q + e)(q \u22a4 Xk + e\u22a4 )k\u221e\n,\nkXk k\u221e k\u0393k\u221e + kXk k\u221e k\u2206k\u221e + (kXk k\u221e kqk\u221e + kek\u221e )(kq \u22a4 k\u221e kXk k\u221e + ke\u22a4 k\u221e )\n\n\f17\n\u22a4\n\nwhere Xk = Gk for the SDA algorithm, Xk = T \u25e6 (m(k) n(k) ) for the SI algorithm with no shift\nand Xk = T \u25e6 (Mk Nk\u22a4 ) for the SI algorithm with shift. All iteration methods are terminated\nwhenever the relative errors or the relative normalized residual residuals are less than n2 \u01eb, where\n\u01eb = 2\u221252 \u223c\n= 2.22 * 10\u221216 be the machine zero.\nExample 5.1. In this example, we compare the methods for finding the minimal nonnegative\nsolution of (1.1) by using the shifting technique. We explain the efficiency of the SDA and SI\napplied to the shifted equations (3.3) and (3.12), respectively. We consider (1.1) with (\u03b1, c) =\n(0, 1). As suggested in [9, 14], the constants ci and \u03c9i are the nodes and weights, which are\nobtained by dividing the interval [0, 1] into n/4 subinterval of equal length and applying to each\nsubinterval the 4-node Gauss\u2013Legendre quadrature.\nIn table 5, we report a comparison of residuals and the number of iterations for the SDA with\nno shift, the SDA with a single shift, the SDA with double shifts , the SI with no shift, the SI with\na single shift, and the SI with double shifts and with size n = 32, 64, 128, and 256. From table 5,\nwe have the following two conclusions.\nFirst, in the critical case (\u03b1, c) = (0, 1), it is known that the SDA algorithm converges linearly.\nAfter applied to the shifted equation, the SDA algorithm converges quadratically. As shown in\nTable 5, the number of steps required in the SDA algorithm with a single shift or double shifts\nare around half of those of the SDA algorithm with no shift. Also, the computed solution of the\nshifted equations is more accurate than the one obtained with no shift. The numerical phenomena\nare in accordance with the theoretical discussion given in [7].\nSecond, we randomly choose \u03b7 and \u03be from the set \u03a9. Indeed, in Table 5, we have (\u03b7, \u03be) =\n\u22121\n( 2\u03c91 1 , 0) for the single-shift problems and (\u03b7, \u03be) = ( 2\u03c91 1 , 2\u03c9\n) for the double-shift problems. We\n1\nsee that even with 10000 steps, the solution obtained from the nonshifted problems can only have\naccuracy up to 10\u22128 . On the other hand, the solution for the shifted problems can have the\naccuracy better than 10\u221210 and a dramatical decrease in the number of steps required in the\ncomputation. Also, the iteration counts listed in Table 5 are in accord with Theorem 4.3.\n\nn\n32\n64\n128\n256\nn\n32\n64\n128\n256\n\nSDA(no shift)\n9.7e-14(27)\n4.2e-13(27)\n1.7e-12(27)\n6.8e-12(27)\nSI(no shift)\n* (>10000)\n*(>10000)\n*(>10000)\n*(>10000)\n\nSDA(single shift)\n4.5e-15(11)\n1.6e-14(12)\n4.2e-14(13)\n1.2e-13(14)\nSI(single shift)\n2.4e-13(164)\n1.0-12(154)\n4.0-12(145)\n1.6-11(136)\n\nSDA(double shifts)\n7.4e-15(11)\n1.9e-14(12)\n6.1e-14(13)\n1.4e-13(14)\nSI(double shifts)\n2.9e-13(40)\n1.3e-12(38)\n5.4e-12(36)\n2.2e-11(34)\n\nTable 5.1\nComparison of the residuals (and in parentheses the number of iterations) of the SDA and SI techniques.\n\n6. Conclusion. The challenging issues of applying the SDA algorithm to the shifted NARE\nproblems are to develop a well-defined sequence, to guarantee the convergence of the sequence,\nand to associate the solutions of the shifted problems with the original one. All these issues\nrelated to the structued NARE (1.1) have been studied in our work. Numerical experiments show\nthe improvement of the speed and accuracy while applying the SDA algorithm to the shifted\nproblems. Note that the bottleneck for applying this algorithm is to compute the inverses of\n(In \u2212 Hk Gk ) and (In \u2212 Gk Hk ), which apparently have an O(n3 ) complexity. Compare with\n\n\f18\nthe Newton method, which has been shown to have O(n2 ) complexity [4], an interesting problem\nworthy of further investigation is to reduce the computational cost by taking the specific structure\nof (1.1) into account.\nOn the other hand, while applying the SI algorithm to the critical case, its convergence is\nvery slow and has almost stopped. Through the shifting technology, a new iteration method\nhas been introduced and preserve the linear convergence. Numerical experiments show that\nwhile considering the SI algorithm, the convergence with double shifts is much faster than the\nconvergence with a single shift or no shift. We believe the results we obtain are new in the field\nand could provide considerable insight into the NARE problems.\nREFERENCES\n[1] Z.-Z. Bai, Y.-H. Gao, and L.-Z. Lu, Fast iterative schemes for nonsymmetric algebraic riccati equations\narising from transport theory, SIAM J. Sci. Comput., 30 (2008), pp. 804\u2013818.\n[2] L. Bao, Y. Lin, and Y. Wei, A modified simple iterative method for nonsymmetric algebraic Riccati equations arising in transport theory, Appl. Math. Comput., 181 (2006), pp. 1499\u20131504.\n[3] A. Berman and R. J. Plemmons, Nonnegative matrices in the mathematical sciences, vol. 9 of Classics in\nApplied Mathematics, Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 1994.\nRevised reprint of the 1979 original.\n[4] D. A. Bini, B. Iannazzo, and F. Poloni, A fast Newton's method for a nonsymmetric algebraic Riccati\nequation, SIAM J. Matrix Anal. Appl., 30 (2008), pp. 276\u2013290.\n[5] C.-Y. Chiang, E. K.-W. Chu, C.-H. Guo, T.-M. Huang, W.-W. Lin, and S.-F. Xu, Convergence analysis\nof the doubling algorithm for several nonlinear matrix equations in the critical case, SIAM J. Matrix\nAnal. Appl., 31 (2009), pp. 227\u2013247.\n[6] C.-H. Guo, Nonsymmetric algebraic Riccati equations and Wiener-Hopf factorization for M -matrices, SIAM\nJ. Matrix Anal. Appl., 23 (2001), pp. 225\u2013242 (electronic).\n[7] C.-H. Guo and N. J. Higham, Iterative solution of a nonsymmetric algebraic Riccati equation, SIAM J.\nMatrix Anal. Appl., 29 (2007), pp. 396\u2013412.\n[8] C.-H. Guo, B. Iannazzo, and B. Meini, On the doubling algorithm for a (shifted) nonsymmetric algebraic\nRiccati equation, SIAM J. Matrix Anal. Appl., 29 (2007), pp. 1083\u20131100.\n[9] C.-H. Guo and A. J. Laub, On the iterative solution of a class of nonsymmetric algebraic Riccati equations,\nSIAM J. Matrix Anal. Appl., 22 (2000), pp. 376\u2013391 (electronic).\n[10] C.-H. Guo and W.-W. Lin, Convergence rates of some iterative methods for nonsymmetric algebraic Riccati\nequations arising in transport theory, Linear Algebra Appl., 432 (2010), pp. 283\u2013291.\n[11] X.-X. Guo, W.-W. Lin, and S.-F. Xu, A structure-preserving doubling algorithm for nonsymmetric algebraic\nRiccati equation, Numer. Math., 103 (2006), pp. 393\u2013412.\n[12] R. A. Horn and C. R. Johnson, Topics in matrix analysis, Cambridge University Press, Cambridge, 1991.\n[13] J. Juang and W.-W. Lin, Nonsymmetric algebraic Riccati equations and Hamiltonian-like matrices, SIAM\nJ. Matrix Anal. Appl., 20 (1999), pp. 228\u2013243 (electronic).\n[14] L.-Z. Lu, Newton iterations for a non-symmetric algebraic Riccati equation, Numer. Linear Algebra Appl.,\n12 (2005), pp. 191\u2013200.\n[15]\n, Solution form and simple iteration of a nonsymmetric algebraic Riccati equation arising in transport\ntheory, SIAM J. Matrix Anal. Appl., 26 (2005), pp. 679\u2013685 (electronic).\n[16] R. S. Varga, Matrix iterative analysis, vol. 27 of Springer Series in Computational Mathematics, SpringerVerlag, Berlin, expanded ed., 2000.\n\n\f"}