{"id": "http://arxiv.org/abs/1104.4530v3", "guidislink": true, "updated": "2013-01-04T05:39:40Z", "updated_parsed": [2013, 1, 4, 5, 39, 40, 4, 4, 0], "published": "2011-04-23T04:23:16Z", "published_parsed": [2011, 4, 23, 4, 23, 16, 5, 113, 0], "title": "Absolute value preconditioning for symmetric indefinite linear systems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.2082%2C1104.3479%2C1104.2630%2C1104.1440%2C1104.5534%2C1104.1993%2C1104.3417%2C1104.3061%2C1104.5300%2C1104.4495%2C1104.4236%2C1104.2173%2C1104.4471%2C1104.1686%2C1104.1202%2C1104.5258%2C1104.2251%2C1104.1973%2C1104.4964%2C1104.2117%2C1104.1025%2C1104.4861%2C1104.1510%2C1104.4126%2C1104.1800%2C1104.2309%2C1104.2416%2C1104.1731%2C1104.1489%2C1104.1994%2C1104.0730%2C1104.4631%2C1104.2021%2C1104.4946%2C1104.0686%2C1104.1830%2C1104.2482%2C1104.1652%2C1104.2230%2C1104.3535%2C1104.5360%2C1104.4530%2C1104.2415%2C1104.0483%2C1104.2052%2C1104.5518%2C1104.5113%2C1104.0424%2C1104.1996%2C1104.0485%2C1104.3904%2C1104.1161%2C1104.1685%2C1104.3584%2C1104.3665%2C1104.1491%2C1104.1560%2C1104.2701%2C1104.3473%2C1104.4026%2C1104.3917%2C1104.3221%2C1104.0451%2C1104.1043%2C1104.0532%2C1104.0197%2C1104.1820%2C1104.3370%2C1104.1637%2C1104.2960%2C1104.2353%2C1104.3605%2C1104.3436%2C1104.4137%2C1104.1265%2C1104.4083%2C1104.2479%2C1104.3152%2C1104.0967%2C1104.3707%2C1104.1488%2C1104.0990%2C1104.1189%2C1104.1986%2C1104.3695%2C1104.2032%2C1104.5434%2C1104.0304%2C1104.3185%2C1104.0394%2C1104.1607%2C1104.4668%2C1104.3649%2C1104.2675%2C1104.3704%2C1104.0768%2C1104.1226%2C1104.2982%2C1104.3383%2C1104.1499%2C1104.5075&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Absolute value preconditioning for symmetric indefinite linear systems"}, "summary": "We introduce a novel strategy for constructing symmetric positive definite\n(SPD) preconditioners for linear systems with symmetric indefinite matrices.\nThe strategy, called absolute value preconditioning, is motivated by the\nobservation that the preconditioned minimal residual method with the inverse of\nthe absolute value of the matrix as a preconditioner converges to the exact\nsolution of the system in at most two steps. Neither the exact absolute value\nof the matrix nor its exact inverse are computationally feasible to construct\nin general. However, we provide a practical example of an SPD preconditioner\nthat is based on the suggested approach. In this example we consider a model\nproblem with a shifted discrete negative Laplacian, and suggest a geometric\nmultigrid (MG) preconditioner, where the inverse of the matrix absolute value\nappears only on the coarse grid, while operations on finer grids are based on\nthe Laplacian. Our numerical tests demonstrate practical effectiveness of the\nnew MG preconditioner, which leads to a robust iterative scheme with minimalist\nmemory requirements.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.2082%2C1104.3479%2C1104.2630%2C1104.1440%2C1104.5534%2C1104.1993%2C1104.3417%2C1104.3061%2C1104.5300%2C1104.4495%2C1104.4236%2C1104.2173%2C1104.4471%2C1104.1686%2C1104.1202%2C1104.5258%2C1104.2251%2C1104.1973%2C1104.4964%2C1104.2117%2C1104.1025%2C1104.4861%2C1104.1510%2C1104.4126%2C1104.1800%2C1104.2309%2C1104.2416%2C1104.1731%2C1104.1489%2C1104.1994%2C1104.0730%2C1104.4631%2C1104.2021%2C1104.4946%2C1104.0686%2C1104.1830%2C1104.2482%2C1104.1652%2C1104.2230%2C1104.3535%2C1104.5360%2C1104.4530%2C1104.2415%2C1104.0483%2C1104.2052%2C1104.5518%2C1104.5113%2C1104.0424%2C1104.1996%2C1104.0485%2C1104.3904%2C1104.1161%2C1104.1685%2C1104.3584%2C1104.3665%2C1104.1491%2C1104.1560%2C1104.2701%2C1104.3473%2C1104.4026%2C1104.3917%2C1104.3221%2C1104.0451%2C1104.1043%2C1104.0532%2C1104.0197%2C1104.1820%2C1104.3370%2C1104.1637%2C1104.2960%2C1104.2353%2C1104.3605%2C1104.3436%2C1104.4137%2C1104.1265%2C1104.4083%2C1104.2479%2C1104.3152%2C1104.0967%2C1104.3707%2C1104.1488%2C1104.0990%2C1104.1189%2C1104.1986%2C1104.3695%2C1104.2032%2C1104.5434%2C1104.0304%2C1104.3185%2C1104.0394%2C1104.1607%2C1104.4668%2C1104.3649%2C1104.2675%2C1104.3704%2C1104.0768%2C1104.1226%2C1104.2982%2C1104.3383%2C1104.1499%2C1104.5075&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We introduce a novel strategy for constructing symmetric positive definite\n(SPD) preconditioners for linear systems with symmetric indefinite matrices.\nThe strategy, called absolute value preconditioning, is motivated by the\nobservation that the preconditioned minimal residual method with the inverse of\nthe absolute value of the matrix as a preconditioner converges to the exact\nsolution of the system in at most two steps. Neither the exact absolute value\nof the matrix nor its exact inverse are computationally feasible to construct\nin general. However, we provide a practical example of an SPD preconditioner\nthat is based on the suggested approach. In this example we consider a model\nproblem with a shifted discrete negative Laplacian, and suggest a geometric\nmultigrid (MG) preconditioner, where the inverse of the matrix absolute value\nappears only on the coarse grid, while operations on finer grids are based on\nthe Laplacian. Our numerical tests demonstrate practical effectiveness of the\nnew MG preconditioner, which leads to a robust iterative scheme with minimalist\nmemory requirements."}, "authors": ["Eugene Vecharynski", "Andrew V. Knyazev"], "author_detail": {"name": "Andrew V. Knyazev"}, "author": "Andrew V. Knyazev", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1137/120886686", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1104.4530v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1104.4530v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1104.4530v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1104.4530v3", "arxiv_comment": null, "journal_reference": "SIAM Journal on Scientific Computing 2013 35:2, A696-A718", "doi": "10.1137/120886686", "fulltext": "ABSOLUTE VALUE PRECONDITIONING FOR SYMMETRIC\nINDEFINITE LINEAR SYSTEMS \u2217\n\narXiv:1104.4530v3 [math.NA] 4 Jan 2013\n\nEUGENE VECHARYNSKI\u2020 AND ANDREW V. KNYAZEV\u2021 \u00a7 \u00b6\nAbstract. We introduce a novel strategy for constructing symmetric positive definite (SPD)\npreconditioners for linear systems with symmetric indefinite matrices. The strategy, called absolute\nvalue preconditioning, is motivated by the observation that the preconditioned minimal residual\nmethod with the inverse of the absolute value of the matrix as a preconditioner converges to the\nexact solution of the system in at most two steps. Neither the exact absolute value of the matrix\nnor its exact inverse are computationally feasible to construct in general. However, we provide\na practical example of an SPD preconditioner that is based on the suggested approach. In this\nexample we consider a model problem with a shifted discrete negative Laplacian, and suggest a\ngeometric multigrid (MG) preconditioner, where the inverse of the matrix absolute value appears\nonly on the coarse grid, while operations on finer grids are based on the Laplacian. Our numerical\ntests demonstrate practical effectiveness of the new MG preconditioner, which leads to a robust\niterative scheme with minimalist memory requirements.\nKey words. Preconditioning, linear system, preconditioned minimal residual method, polar\ndecomposition, matrix absolute value, multigrid, polynomial filtering\nAMS subject classifications. 15A06, 65F08, 65F10, 65N22, 65N55\n\n1. Introduction. Large, sparse, symmetric, and indefinite systems arise in a variety of applications. For example, in the form of saddle point problems, such systems\nresult from mixed finite element discretizations of underlying differential equations of\nfluid and solid mechanics; see, e.g., [3] and references therein. In acoustics, large sparse\nsymmetric indefinite systems are obtained after discretizing the Helmholtz equation\nfor certain media types and boundary conditions. Often the need to solve symmetric\nindefinite problems comes as an auxiliary task within other computational routines,\nsuch as the inner step in interior point methods in linear and nonlinear optimization [3, 26], or solution of the correction equation in the Jacobi-Davidson method [36]\nfor a symmetric eigenvalue problem.\nWe consider an iterative solution of a linear system Ax = b, where the matrix\nA is real nonsingular and symmetric indefinite, i.e., the spectrum of A contains both\npositive and negative eigenvalues. In order to improve the convergence, we introduce a preconditioner T and formally replace Ax = b by the preconditioned system\nT Ax = T b. If T is properly chosen, an iterative method for this system can exhibit\na better convergence behavior compared to a scheme applied to Ax = b. Neither the\npreconditioner T nor the preconditioned matrix T A is normally explicitly computed.\nIf T is not symmetric positive definite (SPD), then T A, in general, is not symmetric with respect to any inner product [29, Theorem 15.2.1]. Thus, the introduction\nof a non-SPD preconditioner replaces the original symmetric problem Ax = b by a\ngenerally nonsymmetric T Ax = T b. Specialized methods for symmetric linear sys\u2217 the current version generated October 30, 2018. This material is based upon work partially\nsupported by the National Science Foundation under Grant No. 1115734. The work is partially\nbased on the PhD thesis of the first coauthor [42].\n\u2020 Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720\n(eugene.vecharynski@gmail.com)\n\u2021 Department of Mathematical and Statistical Sciences; University of Colorado Denver, P.O. Box\n173364, Campus Box 170, Denver, CO 80217-3364, USA (andrew.knyazev[at]ucdenver.edu)\n\u00a7 Mitsubishi Electric Research Laboratories; 201 Broadway Cambridge, MA 02139\n\u00b6 http://www.merl.com/people/?user=knyazev and http://math.ucdenver.edu/ aknyazev/\n~\n\n1\n\n\f2\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\ntems are no longer applicable to the preconditioned problem, and must be replaced by\niterative schemes for nonsymmetric linear systems; e.g., GMRES or GMRES(m) [35],\nBi-CGSTAB [41], and QMR [17].\nThe approach based on the choice of a non-SPD preconditioner, which leads to\nsolving a nonsymmetric problem, has several disadvantages. First, no short-term recurrent scheme that delivers an optimal Krylov subspace method is typically available\nfor a nonsymmetric linear system [15]. In practice, this means that implementations\nof the optimal methods (e.g., GMRES) require an increasing amount of work and\nstorage at every new step, and hence are often computationally expensive.\nSecond, the convergence behavior of iterative methods for nonsymmetric linear\nsystems is not completely understood. In particular, the convergence may not be\ncharacterized in terms of reasonably accessible quantities, such as the spectrum of the\npreconditioned matrix; see the corresponding results for GMRES and GMRES(m)\nin [20, 43]. This makes it difficult to predict computational costs.\nIf T is chosen to be SPD, i.e., T = T \u2217 > 0, then the matrix T A of the preconditioned linear system is symmetric with respect to the T \u22121 \u2013inner product defined by\n(u, v)T \u22121 = (u, T \u22121 v) for any pair of vectors u and v. Here (*, *) denotes the Euclidean\ninner product (u, v) = v \u2217 u, in which the matrices A and T are symmetric. Due to\nthis symmetry preservation, system T Ax = T b can be solved using an optimal Krylov\nsubspace method that admits a short-term recurrent implementation, such as preconditioned MINRES (PMINRES) [14, 28]. Moreover, the convergence of the method\ncan be fully estimated in terms of the spectrum of T A.\nIn light of the above discussion, the choice of an SPD preconditioner for a symmetric indefinite linear system can be regarded as natural and favorable, especially\nif corresponding non-SPD preconditioning strategies fail to provide convergence in a\nsmall number of iterations. We advocate the use of SPD preconditioning.\nThe question of constructing SPD preconditioners for symmetric indefinite systems has been widely studied in many applications. For saddle point problems, the\nblock-diagonal SPD preconditioning has been addressed, e.g., in [16, 37, 44]. In [2],\nit was proposed to use an inverse of the negative Laplacian as an SPD preconditioner\nfor indefinite Helmholtz problems. This approach was further extended in [25] by\nintroducing a shift into the preconditioner. Another strategy was suggested in [18],\nprimarily in the context of linear systems arising in optimization. It is based on the\nso-called Bunch-Parlett factorization [9].\nWe introduce here a different idea of constructing SPD preconditioners that resemble the inverse of the absolute value of the coefficient matrix. Throughout, the\nabsolute value of A is defined as a matrix function |A| = V |\u039b| V \u2217 , where A = V \u039bV \u2217 is\nthe eigenvalue decomposition of A. We are motivated by the observation that PMINRES with |A|\u22121 as a preconditioner converges to the exact solution in at most two\nsteps. We refer to the new approach as the absolute value (AV) preconditioning and\ncall the corresponding preconditioners the AV preconditioners.\nThe direct approach for constructing an AV preconditioner is to approximately\nsolve |A| z = r. However, |A| is generally not available, which makes the application\nof standard techniques, such as, e.g., incomplete factorizations, approximate inverses,\n\u22121\nproblematic. The vector |A| r can also be found using matrix function computations, normally fulfilled by a Krylov subspace method [19, 23] or a polynomial\napproximation [30, 31]. Our numerical experience shows that the convergence, with\nrespect to the outer iterations, of a linear solver can be significantly improved with\n\u22121\nthis approach, but the computational costs of approximating f (A)r = |A| r may be\n\n\fABSOLUTE VALUE PRECONDITIONING\n\n3\n\ntoo high, i.e., much higher than the cost of matrix-vector multiplication with A.\nIntroduction of the general concept of the AV preconditioning is the main theoretical contribution of the present work. As a proof of concept example of the AV\npreconditioning, we use a geometric multigrid (MG) framework. To investigate applicability and practical effectiveness of the proposed idea, we choose a model problem\nresulting from discretization of a shifted Laplacian (Helmholtz operator) on a unit\nsquare with Dirichlet boundary conditions. The obtained linear system is real symmetric indefinite. We construct an MG AV preconditioner that, used in the PMINRES\niteration, delivers an efficient computational scheme.\nLet us remark that the same model problem has been considered in [4], where the\nauthors utilize the coarse grid approximation to reduce the indefinite problem to the\nSPD system. Satisfactory results have been reported for small shifts, i.e., for slightly\nindefinite systems. However, the limitation of the approach lies in the requirement\non the size of the coarse space, which should be chosen sufficiently large. As we show\nbelow, the MG AV preconditioner presented in this paper allows keeping the coarsest\nproblem reasonably small, even if the shift is large.\nNumerical solution of Helmholtz problems is an object of active research; see,\ne.g., [1, 6, 12, 13, 21, 27, 40]. A typical Helmholtz problem is approximated by a complex symmetric (non-Hermitian) system. The real symmetric case of the Helmholtz\nequation, considered in this paper, is less common. However, methods for complex\nproblems are evidently applicable to our particular real case, which allows us to make\nnumerical comparisons with known Helmholtz solvers.\nWe test several of solvers, based on the inverted Laplacian and the standard\nMG preconditioning, to compare with the proposed AV preconditioning. In fact, the\ninverted (shifted) Laplacian preconditioning [2, 25] for real Helmholtz problems can\nbe viewed as a special case of our AV preconditioning. In contrast to preconditioners\nin [18] relying on the Bunch-Parlett factorization, we show that the AV preconditioners\ncan be constructed without any decompositions of the matrix, which is crucial for very\nlarge or matrix-free problems.\nThis paper is organized as follows. In Section 2, we present and justify the general notion of an AV preconditioner. The rest of the paper deals with the question of\nwhether AV preconditioners can be efficiently constructed in practice. In Section 3,\nwe give a positive answer by constructing an example of a geometric MG AV preconditioner for the model problem. The efficiency of this preconditioner is demonstrated\nin our numerical tests in Section 4. We conclude in Section 5.\n2. AV preconditioning for symmetric indefinite systems. Given an SPD\npreconditioner T , we consider solving a linear system with the preconditioned minimal\nresidual method, implemented in the form of the preconditioned MINRES (PMINRES)\nalgorithm [14, 28]. In the absence of round-off errors, at step i, the method constructs\nan approximation x(i) to the solution of Ax = b of the form\n\u0010\n\u0011\n(2.1)\nx(i) \u2208 x(0) + Ki T A, T r(0) ,\nsuch that the residual vector r(i) = b \u2212 Ax(i) satisfies the optimality condition\n(2.2)\n\nkr(i) kT =\n\nmin\nu\u2208AKi (T A,T r (0) )\n\nkr(0) \u2212 ukT .\n\n\u0001\n\b\nHere, Ki T A, T r(0) = span T r(0) , (T A)T r(0) , . . . , (T A)i\u22121 T r(0) is the Krylov subspace generated by the matrix T A and the vector T r(0) , the T -norm is defined by\n\n\f4\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\nkvk2T = (v, v)T for any v, and x(0) is the initial guess. Scheme (2.1)\u2013(2.2) represents\nan optimal Krylov subspace method and the PMINRES implementation is based on\na short-term recurrence. The conventional convergence rate bound for (2.1)\u2013(2.2) can\nbe found, e.g., in [14], and relies solely on the distribution of eigenvalues of T A.\n\u22121\nThe following trivial, but important, theorem regards |A| as an SPD preconditioner for a symmetric indefinite system.\nTheorem 2.1. The preconditioned minimal residual method (2.1)\u2013(2.2) with\n\u22121\npreconditioner T = |A| converges to the solution of Ax = b in at most two steps.\n\u22121\nTheorem 2.1 implies that T = |A| is an ideal SPD preconditioner. Note that the\ntheorem holds not only for the preconditioned minimal residual method (2.1)\u2013(2.2),\nbut for all methods where convergence is determined by the degree of the minimal\npolynomial of T A.\n\u22121\nIn practical situations, the computation of an ideal SPD preconditioner T = |A|\nis prohibitively costly. However, we show that it is possible to construct inexpensive\n\u22121\nSPD preconditioners that resemble |A| and can significantly accelerate the convergence of an iterative method.\nDefinition 2.2. We call an SPD preconditioner T for a symmetric indefinite\nlinear system Ax = b an AV preconditioner if it satisfies\n(2.3)\n\n\u03b40 (v, T \u22121 v) \u2264 (v, |A| v) \u2264 \u03b41 (v, T \u22121 v), \u2200v\n\nwith constants \u03b41 \u2265 \u03b40 > 0, such that the ratio \u03b41 /\u03b40 \u2265 1 is reasonably small.\nLet us remark that Definition 2.2 of the AV preconditioner is informal because\nno precise assumption is made of how small the ratio \u03b41 /\u03b40 should be. It is clear\n\u22121\nfrom (2.3) that \u03b41 /\u03b40 measures how well the preconditioner T approximates |A| ,\nup to a positive scaling. If A represents a hierarchy of mesh problems then it is\ndesirable that \u03b41 /\u03b40 is independent of the problem size. In this case, if A is SPD,\nDefinition 2.2 of the AV preconditioner is consistent with the well known concept of\nspectrally equivalent preconditioning for SPD systems; see [10].\nThe following theorem provides bounds for eigenvalues of the preconditioned matrix T A in terms of the spectrum of T |A|. We note that T and A, and thus T A and\nT |A|, do not in general commute. Therefore, our spectral analysis cannot be based\non a traditional matrix analysis tool, a basis of eigenvectors.\nTheorem 2.3. Given a nonsingular symmetric indefinite A \u2208 Rn\u00d7n and an\nSPD T \u2208 Rn\u00d7n , let \u03bc1 \u2264 \u03bc2 \u2264 . . . \u2264 \u03bcn be the eigenvalues of T |A|. Then eigenvalues\n\u03bb1 \u2264 . . . \u2264 \u03bbp < 0 < \u03bbp+1 \u2264 . . . \u2264 \u03bbn of T A are located in intervals\n(2.4)\n\n\u2212\u03bcn\u2212j+1\n\u03bcj\u2212p\n\n\u2264\n\u2264\n\n\u03bbj\n\u03bbj\n\n\u2264\n\u2264\n\n\u2212\u03bcp\u2212j+1 ,\n\u03bcj ,\n\nj = 1, . . . , p;\nj = p + 1, . . . , n.\n\nProof. We start by observing that the absolute value of the Rayleigh quotient of\nthe generalized eigenvalue problem Av = \u03bb |A| v is bounded by 1, i.e.,\n|(v, Av)| \u2264 (v, |A| v), \u2200v \u2208 Rn .\n\n(2.5)\n\nNow, we recall that the spectra of matrices T |A| and T A are given by the generalized eigenvalue problems |A| v = \u03bcT \u22121 v and Av = \u03bbT \u22121 v, respectively, and introduce\nthe corresponding Rayleigh quotients\n(2.6)\n\n\u03c8(v) \u2261\n\n(v, Av)\n(v, |A| v)\n, \u03c6(v) \u2261\n, v \u2208 Rn .\n\u22121\n(v, T v)\n(v, T \u22121 v)\n\n\fABSOLUTE VALUE PRECONDITIONING\n\n5\n\nLet us fix any index j \u2208 {1, 2, . . . , n}, and denote by S an arbitrary subspace of\nRn such that dim(S) = j. Since inequality (2.5) also holds on S, using (2.6) we write\n\u2212 \u03c8(v) \u2264 \u03c6(v) \u2264 \u03c8(v), v \u2208 S.\n\n(2.7)\n\nMoreover, taking the maxima in vectors v \u2208 S, and after that the minima in subspaces\nS \u2208 S j = {S \u2286 Rn : dim(S) = j}, of all parts of (2.7) preserves the inequalities, so\n(2.8)\n\nmin max(\u2212\u03c8(v)) \u2264 minj max \u03c6(v) \u2264 minj max \u03c8(v).\n\nS\u2208S j v\u2208S\n\nS\u2208S\n\nv\u2208S\n\nS\u2208S\n\nv\u2208S\n\nBy the Courant-Fischer theorem (see, e.g., [24, 29]) for the Rayleigh quotients \u00b1\u03c8(v)\nand \u03c6(v) defined in (2.6), we conclude from (2.8) that\n\u2212\u03bcn\u2212j+1 \u2264 \u03bbj \u2264 \u03bcj .\nRecalling that j has been arbitrarily chosen, we obtain the following bounds on the\neigenvalues of T A:\n\u2212\u03bcn\u2212j+1\n0\n\n(2.9)\n\n\u2264 \u03bbj\n< \u03bbj\n\n<\n\u2264\n\n0,\n\u03bcj ,\n\nj = 1, . . . , p;\nj = p + 1, . . . , n.\n\nNext, in order to derive nontrivial upper and lower bounds for the p negative and\nn \u2212 p positive eigenvalues \u03bbj in (2.9), we use the fact that eigenvalues \u03bej and \u03b6j of the\n\u22121\ngeneralized eigenvalue problems |A| v = \u03beT v and A\u22121 v = \u03b6T v are the reciprocals\nof the eigenvalues of the problems |A| v = \u03bcT \u22121 v and Av = \u03bbT \u22121 v, respectively, i.e.,\n(2.10)\n\n0 < \u03be1 =\n\n1\n1\n1\n\u2264 \u03be2 =\n\u2264 . . . \u2264 \u03ben =\n,\n\u03bcn\n\u03bcn\u22121\n\u03bc1\n\nand\n(2.11)\n\n\u03b61 =\n\n1\n1\n1\n1\n\u2264 . . . \u2264 \u03b6p =\n< 0 < \u03b6p+1 =\n\u2264 . . . \u2264 \u03b6n =\n.\n\u03bbp\n\u03bb1\n\u03bbn\n\u03bbp+1\n\nSimilar to (2.5),\n\u22121\n\n(v, A\u22121 v) \u2264 (v, |A|\n\nv), \u2200v \u2208 Rn .\n\nThus, we can use the same arguments as those following (2.5) to show that relations (2.7) and (2.8), with a fixed j \u2208 {1, 2, . . . , n}, also hold for\n\u22121\n\n(2.12)\n\n\u03c8(v) \u2261\n\n(v, |A| v)\n(v, A\u22121 v)\n, \u03c6(v) \u2261\n, v \u2208 Rn ,\n(v, T v)\n(v, T v)\n\nwhere \u03c8(v) and \u03c6(v) are now the Rayleigh quotients of the generalized eigenvalue\n\u22121\nproblems |A| v = \u03beT v and A\u22121 v = \u03b6T v, respectively. The Courant-Fischer theorem\nfor \u00b1\u03c8(v) and \u03c6(v) in (2.12) allows us to conclude from (2.8) that\n\u2212\u03ben\u2212j+1 \u2264 \u03b6j \u2264 \u03bej .\nGiven the arbitrary choice of j in the above inequality, by (2.10)\u2013(2.11) we get the\nfollowing bounds on the eigenvalues of T A:\n(2.13)\n\n\u22121/\u03bcp\u2212j+1\n0\n\n\u2264 1/\u03bbj\n< 1/\u03bbj\n\n<\n0,\n\u2264 1/\u03bcj\u2212p ,\n\nj = 1, . . . , p;\nj = p + 1, . . . , n.\n\n\f6\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\nCombining (2.9) and (2.13), we obtain (2.4).\nTheorem 2.3 suggests two useful implications given by the corresponding corollaries below. In particular, the following result describes \u039b(T A), i.e., the spectrum of\nthe preconditioned matrix T A, in terms of \u03b40 and \u03b41 in (2.3).\nCorollary 2.4. Given a nonsingular symmetric indefinite A \u2208 Rn\u00d7n , an SPD\nT \u2208 Rn\u00d7n , and constants \u03b41 \u2265 \u03b40 > 0 satisfying (2.3), we have\n[\n(2.14)\n\u039b(T A) \u2282 [\u2212\u03b41 , \u2212\u03b40 ] [\u03b40 , \u03b41 ] ,\nwhere \u039b(T A) is the spectrum of T A.\nProof. Follows directly from (2.3) and (2.4) with j = 1, p, p + 1, n.\nThe next corollary shows that the presence of reasonably populated clusters of\neigenvalues in the spectrum of T |A| guarantees the occurrence of corresponding clusters in the spectrum of the preconditioned matrix T A.\nCorollary 2.5. Given a nonsingular symmetric indefinite A \u2208 Rn\u00d7n and an\nSPD T \u2208 Rn\u00d7n , let \u03bcl \u2264 \u03bcl+1 \u2264 . . . \u2264 \u03bcl+k\u22121 be a sequence of k eigenvalues of\nT |A|, where 1 \u2264 l < l + k \u2212 1 \u2264 n and \u03c4 = |\u03bcl \u2212 \u03bcl+k\u22121 |. Then, if k \u2265 p + 2,\nthe k \u2212 p positive eigenvalues \u03bbl+p \u2264 \u03bbl+p+1 \u2264 . . . \u2264 \u03bbl+k\u22121 of T A are such that\n|\u03bbl+p \u2212 \u03bbl+k\u22121 | \u2264 \u03c4 . Also, if k \u2265 (n \u2212 p) + 2, the k \u2212 (n \u2212 p) negative eigenvalues\n\u03bbn\u2212k\u2212l+2 \u2264 . . . \u2264 \u03bbp\u2212l \u2264 \u03bbp\u2212l+1 of T A are such that |\u03bbn\u2212k\u2212l+2 \u2212 \u03bbp\u2212l+1 | \u2264 \u03c4 .\nProof. Follows directly from bounds (2.4).\nCorollary 2.4 implies that the ratio \u03b41 /\u03b40 \u2265 1 of the constants from (2.3) measures\nthe quality of the AV preconditioner T . Indeed, the convergence speed of the preconditioned minimal residual method is determined by the spectrum of T A, primarily\nby the intervals of the right-hand side of inclusion (2.14). Additionally, Corollary 2.5\nprompts that a \"good\" AV preconditioner should ensure clusters of eigenvalues in the\nspectrum of T |A|. This implies the clustering of eigenvalues of the preconditioned\nmatrix T A, which has a favorable effect on the convergence behavior of a polynomial\niterative method, such as PMINRES.\nIn the next section, we construct an example of the AV preconditioner for a\nparticular model problem. We apply the MG techniques.\n3. MG AV preconditioning for a model problem. Let us consider the\nfollowing real boundary value problem,\n(3.1)\n\n\u2212 \u2206u(x, y) \u2212 c2 u(x, y) = f (x, y), (x, y) \u2208 \u03a9 = (0, 1) \u00d7 (0, 1), u|\u0393 = 0,\n\nwhere \u2206 = \u2202 2 /\u2202x2 + \u2202 2 /\u2202y2 is the Laplace operator and \u0393 denotes the boundary of\n\u03a9. Problem (3.1) is a particular instance of the Helmholtz equation with Dirichlet\nboundary conditions, where c > 0 is a wave number.\nAfter introducing a uniform grid of size h in both directions and using the standard\n5-point finite-difference stencil to discretize continuous problem (3.1), one obtains the\ncorresponding discrete problem\n(3.2)\n\n(L \u2212 c2 I)x = b,\n\nwhere A \u2261 L \u2212 c2 I represents a discrete negative Laplacian L (later called \"Laplacian\"), satisfying the Dirichlet boundary condition shifted by a scalar c2 .\nThe common rule of thumb, see, e.g., [13, 22], for discretizing (3.1) is\n(3.3)\n\nch \u2264 \u03c0/5.\n\n\fABSOLUTE VALUE PRECONDITIONING\n\n7\n\nBelow, we call (3.2) the model problem. We assume that the shift c2 is different from\nany eigenvalue of the Laplacian and is greater than the smallest but less than the\nlargest eigenvalue. Thus, the matrix L \u2212 c2 I is nonsingular symmetric indefinite. In\nthe following subsection, we apply the idea of the AV preconditioning to construct an\nMG AV preconditioner for system (3.2).\nWhile our main focus throughout the paper is on the 2D problem (3.1), in order\nto simplify presentation of theoretical analysis, we also refer to the 1D analogue\n\u2212 u00 (x) \u2212 c2 u(x) = f (x), u(0) = u(1) = 0.\n\n(3.4)\n\nThe conclusions drawn from (3.4), however, remain qualitatively the same for the 2D\nproblem of interest, which we test numerically.\n3.1. Two-grid AV preconditioner. Along with the fine grid of mesh size h\nunderlying problem (3.2), let us consider a coarse grid of mesh size H > h. We\ndenote the discretization of the Laplacian on this grid by LH , and IH represents the\nidentity operator of the corresponding dimension. We assume that the exact fine-level\nabsolute value L \u2212 c2 I and its inverse are not computable, whereas the inverse of\nthe coarse-level operator LH \u2212 c2 IH can be efficiently constructed. In the two-grid\nframework, we use the subscript H to refer to the quantities defined on the coarse\ngrid. No subscript is used for denoting the fine grid quantities.\nWhile L \u2212 c2 I is not available, let us assume that we have its SPD approximation\nB, i.e., B \u2248 |L \u2212 c2 I| and B = B \u2217 > 0. The operator B can be given in the explicit\nmatrix form or through the action on a vector. We suggest the following general\nscheme as a two-grid AV preconditioner for model problem (3.2).\nAlgorithm 3.1 (The two-grid AV preconditioner). Input: r, B \u2248 |L \u2212 c2 I|.\nOutput: w.\n1. Presmoothing. Apply \u03bd smoothing steps, \u03bd \u2265 1:\n(3.5)\n\nw(i+1) = w(i) + M \u22121 (r \u2212 Bw(i) ), i = 0, . . . , \u03bd \u2212 1, w(0) = 0,\n\nwhere M defines a smoother. Set wpre = w(\u03bd) .\n2. Coarse grid correction. Restrict (R) r \u2212 Bwpre to the coarse grid, apply\n\u22121\nLH \u2212 c2 IH\n, and prolongate (P ) to the fine grid. This delivers the coarse\ngrid correction, which is added to wpre :\n\u22121\n\n(3.6)\n\nwH = LH \u2212 c2 IH\n\n(3.7)\n\nwcgc = wpre + P wH .\n\nR (r \u2212 Bwpre ) ,\n\n3. Postsmoothing. Apply \u03bd smoothing steps:\n(3.8)\n\nw(i+1) = w(i) + M \u2212\u2217 (r \u2212 Bw(i) ), i = 0, . . . , \u03bd \u2212 1, w(0) = wcgc ,\n\nwhere M and \u03bd are the same as in step 1. Return w = wpost = w(\u03bd) .\nIn (3.6) we assume that LH \u2212 c2 IH is nonsingular, i.e., c2 is different from\nany eigenvalue of LH . The presmoother is defined by the nonsingular M , while the\npostsmoother is delivered by M \u2217 . Note that the (inverted) absolute value appears only\non the coarse grid, while the fine grid computations are based on the approximation B.\nIt is immediately seen that if B = |L \u2212 c2 I|, Algorithm 3.1 represents a formal\ntwo-grid cycle [8, 39] for system\n(3.9)\n\nL \u2212 c2 I z = r.\n\n\f8\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\nNote that the introduced scheme is rather general in that different choices of approximations B and smoothers M lead to different preconditioners. We address these\nchoices in more detail in the following subsections.\nIt can be verified that the AV preconditioner given by Algorithm 3.1 implicitly\nconstructs a mapping r 7\u2192 w = Ttg r, where the operator is\n(3.10)\n\nTtg = I \u2212 M \u2212\u2217 B\n\n\u0001\u03bd\n\nP LH \u2212 c2 IH\n\n\u22121\n\nR I \u2212 BM \u22121\n\n\u0001\u03bd\n\n+ F,\n\n\u0001\u03bd\n\u03bd\nwith F = B \u22121 \u2212 (I \u2212 M \u2212\u2217 B) B \u22121 I \u2212 BM \u22121 . The fact that the constructed\npreconditioner T = Ttg is SPD follows directly from the observation that the first\nterm in (3.10) is SPD provided that P = \u03b1R\u2217 for some nonzero scalar \u03b1, while the\nsecond term F is SPD if the spectral radii of I \u2212M \u22121 B and I \u2212M \u2212\u2217 B are less than 1.\nThe latter condition requires the pre- and postsmoothing iterations (3.5) and (3.8) to\nrepresent convergent methods for By = r. Note that the above argument essentially\nrepeats the one used to justify symmetry and positive definiteness of a preconditioner\nbased on the standard two-grid cycle for an SPD system; see, e.g., [5, 38].\nIn this paper we consider two different choices of the approximation B. The\nfirst choice is given by B = L, i.e., it is suggested to approximate the absolute value\n|L \u2212 c2 I| by the Laplacian L. The second choice is delivered by B = pm (L \u2212 c2 I),\nwhere pm is a polynomial of degree at most m such that pm (L \u2212 c2 I) \u2248 |L \u2212 c2 I|.\n3.2. Algorithm 3.1 with B = L. If B = L, Algorithm 3.1 can be regarded as\na step of a standard two-grid method [8, 39] applied to the Poisson equation\n(3.11)\n\nLy = r,\n\nmodified by replacing the operator LH by |LH \u2212c2 IH | on the coarse grid. The question\nremains if the algorithm delivers a form of an approximate solve for absolute value\nproblem (3.9), and hence is suitable for AV preconditioning of (3.2). To be able to\n2 \u22121\nanswer this question, we analyze the propagation of the initial error eAV\nr\n0 = |L\u2212c I|\nof (3.9) under the action of the algorithm.\nWe start by relating errors of (3.9) and (3.11).\nLemma 3.1. Given a vector w, consider errors eAV (w) = |L \u2212 c2 I|\u22121 r \u2212 w and\neP (w) = L\u22121 r \u2212 w for (3.9) and (3.11), respectively. Then\n(3.12)\n\neAV (w) = eP (w) + (c2 I \u2212 Wp )L\u22121 |L \u2212 c2 I|\u22121 r,\n\nwhere Wp = 2Vp |\u039bp |Vp\u2217 , Vp is the matrix of eigenvectors of L \u2212 c2 I corresponding to\nthe p negative eigenvalues \u03bb1 \u2264 . . . \u2264 \u03bbp < 0, and |\u039bp | = diag {|\u03bb1 |, . . . , |\u03bbp |}.\nProof. Observe that for any w,\neAV (w) = |L \u2212 c2 I|\u22121 r \u2212 w = |L \u2212 c2 I|\u22121 r + (eP (w) \u2212 L\u22121 r)\n= eP (w) + |L \u2212 c2 I|\u22121 L\u22121 (L \u2212 |L \u2212 c2 I|)r.\nDenoting A = L \u2212 c2 I, we use the expression |A| = A \u2212 2Vp \u039bp Vp\u2217 to get (3.12)\nAlgorithm 3.1 transforms the initial error eP0 = L\u22121 r of equation (3.11) into\n(3.13)\n\neP = S2\u03bd KS1\u03bd eP0 ,\n\nwhere S1 = I \u2212 M \u22121 L and S2 = I \u2212 M \u2212\u2217 L are pre- and postsmoothing operators,\nK = I \u2212 P |LH \u2212 c2 IH |\u22121 RL corresponds to the coarse grid correction step, and\neP = L\u22121 r \u2212 wpost . Denoting the error of absolute value system (3.9) after applying\n\n\f9\n\nABSOLUTE VALUE PRECONDITIONING\n\nAlgorithm 3.1 by eAV = |L \u2212 c2 I|\u22121 r \u2212 wpost and observing that eP0 = |L \u2212 c2 I|L\u22121 eAV\n0 ,\nby (3.12)\u2013(3.13) we obtain\n\u0001\n(3.14)\neAV = S2\u03bd KS1\u03bd |L \u2212 c2 I| + c2 I \u2212 Wp L\u22121 eAV\n0 .\nThe last expression gives an explicit form of the desired error propagation operator,\nwhich we denote by G:\n\u0001\n(3.15)\nG = S2\u03bd KS1\u03bd |L \u2212 c2 I| + c2 I \u2212 Wp L\u22121 .\nBelow, as a smoother, we use a simple Richardson's iteration, i.e., S1 = S2 =\nI \u2212 \u03c4 L, where \u03c4 is an iteration parameter. The restriction R is given by the full\nweighting and the prolongation P by the standard piecewise linear interpolation;\nsee [8, 39].\nAt this point, in order to simplify further presentation, let us refer to the onedimensional analogue (3.4)\n\b of model problem (3.1). In this case, the matrix L is\ntridiagonal: L = tridiag \u22121/h2 , 2/h2 , \u22121/h2 . We assume that n, the number of\ninterior grid nodes, is odd: h = 1/(n + 1). The coarse grid is then obtained by\ndropping the odd-numbered nodes. We denote the size of the coarse grid problem by\nN = (n + 1)/2 \u2212 1; H = 1/(N + 1) = 2h. The tridiagonal matrix LH denotes the\ndiscretization of the 1D Laplacian on the coarse level.\nRecall that the eigenvalues of L are \u03b8j = h42 sin2 j\u03c0h\n2 with corresponding eigenvec\u221a\nn\ntors vj = 2h [sin lj\u03c0h]l=1 . Similarly, the eigenvalues of LH are \u03b8jH = H42 sin2 j\u03c0H\n2 ,\n\u221a\nN\nH\nand the coarse grid eigenvectors are denoted by vj = 2H [sin lj\u03c0H]l=1 . It is clear\nthat operators L \u2212 c2 I and LH \u2212 c2 IH have the same sets of eigenvectors as L and\nH\n2\nLH with eigenvalues tj = \u03b8j \u2212 c2 and tH\nj = \u03b8j \u2212 c , respectively.\nPn\nAV\nLet e0 = j=1 \u03b1j vj be the expansion of the initial error in the eigenbasis of L.\nPn\nSince eAV = GeAV\n=\n0\nj=1 \u03b1j (Gvj ), we are interested in the action of the error\npropagation operator (3.15) on the eigenmodes vj . We first consider the images of vj\nunder the action of the coarse grid correction operator K.\nThe action of the full weighting operator on eigenvectors vj is well known [8]:\n\n(3.16)\n\n\uf8f1 2 H\nj = 1, . . . , N,\n\uf8f2 cj vj ,\n0,\nj = N + 1,\nRvj =\n\uf8f3\nH\n\u2212c2j vn+1\u2212j\n, j = N + 2, . . . , n.\n\nj\u03c0h\nHere, cj = cos j\u03c0h\n2 . With sj = sin 2 , the action of the piecewise linear interpolation\nis written as\n\n(3.17)\n\nP vjH = c2j vj \u2212 s2j vn+1\u2212j , j = 1, . . . , N,\n\nwhere vn+1\u2212j is the so-called complementary mode of vj [8].\nand (3.17) leads to the following expression for Kvj :\n\uf8f1\n!\n\uf8f4\n\u03b8j\n\uf8f4\n4 \u03b8j\n\uf8f4\n1 \u2212 cj H vj + s2j c2j H vn+1\u2212j ,\nj\n\uf8f4\n\uf8f4\n\uf8f4\n|tj |\n|tj |\n\uf8f2\nvj ,\nj\n(3.18) Kvj =\n!\n\uf8f4\n\uf8f4\n\uf8f4\n\u03b8j\n\u03b8j\n\uf8f4\n4\nH\n\uf8f4\nvj + s2j c2j H\nvn+1\u2212j\n, j\n\uf8f4\n\uf8f3 1 \u2212 cj |tH\n|\n|t\n|\nn+1\u2212j\nn+1\u2212j\n\nCombining (3.16)\n\n= 1, . . . , N,\n= N + 1,\n= N + 2, . . . , n.\n\n\f10\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\nSince vj are the eigenvectors of S1 = S2 = I \u2212 \u03c4 L, L \u2212 c2 I, L\u22121 and Wp , (3.15) leads\nto explicit expressions for Gvj .\nTheorem 3.2. Let c2 < \u03b8N +1 = 2/h2 . Then the error propagation operator G\nin (3.15) acts on the eigenvectors vj of 1D Laplacian as follows:\n\uf8f1\n(11)\n(12)\n\uf8f4\n\uf8f2 gj vj + gj vn+1\u2212j , j = 1, . . . , N,\ngj vj ,\nj = N + 1,\nGvj =\n\uf8f4\n\uf8f3 g (21) v + g (22) v\n,\nj\n= N + 2, . . . , n,\nj\nn+1\u2212j\nj\nj\n\n(3.19)\n\nwhere\n(3.20)\n\n(11)\ngj\n\n(3.21)\n\ngj\n\n(12)\n\n= (1 \u2212 \u03c4 \u03b8j )\n\n(3.23)\n\n(21)\ngj\n\n(3.24)\n\ngj\n\n(22)\n\n\u001a\n\n1\u2212\n\n= (1 \u2212 \u03c4 \u03b8j )\u03bd s2j c2j\n\ngj = (1 \u2212 \u03c4 \u03b8j )2\u03bd\n\n(3.22)\n\nand \u03b2j =\n\n2\u03bd\n\n= (1 \u2212 \u03c4 \u03b8j )\n\n2\u03bd\n\nc4j\n\n\u03b8j\n|tH\nj |\n\n!\n\n|tj | c2\n\u03b2j\n+\n\u2212\n,\n\u03b8j\n\u03b8j\n\u03b8j\n\n|tj |\n(1 \u2212 \u03c4 \u03b8n+1\u2212j )\u03bd ,\n|tH\nj |\n\n|tj | c2\n+\n,\n\u03b8j\n\u03b8j\n1\u2212\n\n= (1 \u2212 \u03c4 \u03b8j )\u03bd s2j c2j\n\nc4j\n\n!\n\n\u03b8j\n|tH\nn+1\u2212j |\n\n|tj |\n(1\nH\n|tn+1\u2212j |\n\n|tj | c2\n+\n,\n\u03b8j\n\u03b8j\n\n\u2212 \u03c4 \u03b8n+1\u2212j )\u03bd ;\n\n2(c2 \u2212 \u03b8j ), \u03b8j < c2 ,\n.\n0,\n\u03b8j > c2\n\nTheorem 3.2 implies that for relatively small shifts, Algorithm 3.1 with B = L\nand a proper choice of \u03c4 and \u03bd reduces the error of (3.9) in the directions of almost\nall eigenvectors vj . In a few directions, however, the error may be amplified. These\ndirections are given by the smooth eigenmodes associated with \u03b8j that are close to c2\non the right, as well as with \u03b8j that are distant from c2 on the left. The number of the\nlatter, if any, is small if ch is sufficiently small, and becomes larger as ch increases.\nIndeed, let \u03c4 = h2 /3, so that |1 \u2212 \u03c4 \u03b8j | < 1 for all j and |1 \u2212 \u03c4 \u03b8j | < 1/3 for j > N .\nThis choice of the parameter provides the least uniform bound for |1 \u2212 \u03c4 \u03b8j | that\ncorrespond to the oscillatory eigenmodes [34, p.415]. It is then readily seen that (3.21)\nand (3.24) can be made arbitrarily small within a reasonably small number \u03bd of\nsmoothing steps. Similarly, (3.22) and (3.23) can be made arbitrarily close to c2 /\u03b8j <\n1. If c2 << \u03b8N +1 , then c2 /\u03b8j in (3.22) and (3.23) is close to zero. Thus, Theorem 3.2\nshows that for relatively small shifts, smoothing provides small values of (3.21)\u2013(3.24)\nand, hence, damps of the oscillatory part of the error. Note that the damping occurs\neven though the smoothing is performed with respect to (3.11), not (3.9).\nNow let us consider (3.20). Theorem 3.2 shows that if c2 is close to an eigenvalue\nH\n\u03b8j of the coarse-level Laplacian, i.e., if tH\nj \u2248 0, then the corresponding reduction\ncoefficient (3.20) can be large. This means that Algorithm 3.1 with B = L has a potential difficulty of amplifying the error in the directions of a few smooth eigenvectors.\nSimilar effect is known to appear for standard MG methods applied to Helmholtz type\nproblems; see [7, 13]. Below, we analyze (3.20) in more detail.\n\n\fABSOLUTE VALUE PRECONDITIONING\n\n11\n\nLet \u03b8j > c2 . Then, using the relation \u03b8jH = c2j \u03b8j , we can write (3.20) as\n!\u0012\n\u0013\n1\nc2\nc2\n(11)\n2\u03bd\n2\n1\n\u2212\ngj = (1 \u2212 \u03c4 \u03b8j )\n+\n.\n1 \u2212 cj\n\u03b8j\n\u03b8j\n|1 \u2212 c2 /\u03b8jH |\n(11)\n\nHere, it is easy to see that as c2 /\u03b8jH \u2192 0, gj \u2192 (1 \u2212 \u03c4 \u03b8j )2\u03bd s2j < 1/2, meaning that\nthe smooth eigenmodes corresponding to \u03b8j away from c2 on the right are well damped.\nIf \u03b8j < c2 , then (3.20) takes the form\n!\u0012\n\u0013 \u0012\n\u0013\nc2 /\u03b8j \u2212 c2j \u2212 c4j\nc2\nc2\n(11)\n2\u03bd\ngj = (1 \u2212 \u03c4 \u03b8j )\n\u2212\n1\n+\n2\n\u2212\n.\nc2 /\u03b8j \u2212 c2j\n\u03b8j\n\u03b8j\nSince c2j \u2208 (1/2, 1), for any c2 /\u03b8j > 1, we can obtain the bound\nc2 /\u03b8j \u2212 c2j \u2212 c4j\nc2 /\u03b8j \u2212 3/4\nc2 /\u03b8j \u2212 2\n\u2264\n.\n\u2264\n2\nc2 /\u03b8j \u2212 1\nc2 /\u03b8j \u2212 cj\nc2 /\u03b8j \u2212 1/2\nAdditionally, 3\u22122\u03bd < (1 \u2212 \u03c4 \u03b8j )2\u03bd < 1. Thus,\n(11)\n\nlj < gj\n\n<\n\n3(c2 /\u03b8j ) \u2212 1\n,\n4(c2 /\u03b8j ) \u2212 2\n\nwhere lj = 0 if 1 < c2 /\u03b8j \u2264 2, and lj = 2 \u2212 c2 /\u03b8j if c2 /\u03b8j > 2.\n(11)\nThe inequality implies that |gj | < 1 for 1 < c2 /\u03b8j \u2264 3, i.e., the algorithm\nreduces the error in the directions of several smooth eigenvectors associated with \u03b8j\n(11)\nto the left of c2 . At the same time, we note that as c2 /\u03b8j \u2192 \u221e, gj \u2192 \u221e, i.e., the\n2\nsmooth eigenmodes corresponding to \u03b8j that are distant from c on the left can be\namplified. Clearly, if ch is sufficiently small then the number of such error components\nis not large (or none), and grows as ch increases.\nThe above analysis shows that Algorithm 3.1 with B = L indeed represents a\nsolve for (3.9), where the solution is approximated everywhere, possibly except for\na subspace of a small dimension. In the context of preconditioning, this translates\ninto the fact that the preconditioned matrix has spectrum clustered around 1 and \u22121\nwith a few outliers generated by the amplification of the smooth eigenmodes. If the\nshift is sufficiently small, the number of such outliers is not large, which only slightly\ndelays the convergence of the outer PMINRES iterations and does not significantly\naffect the efficiency of the overall scheme.\n3.3. Algorithm 3.1 with B = pm (L \u2212 c2 I). The analysis of the previous subsection suggests that the quality of Algorithm 3.1 with B = L may deteriorate as ch\nincreases. This result is not surprising, since for larger ch the relation L \u2248 |L \u2212 c2 I|\nbecomes no longer meaningful. Below we introduce a different approach for approximating the fine grid absolute value. In particular, we consider constructing polynomial approximations B = pm (L \u2212 c2 I), where pm (\u03bb) is a polynomial of degree at most\nm > 0, such that pm (L \u2212 c2 I) \u2248 |L \u2212 c2 I|.\nLet us first refer to the ideal particular case, where pm (L \u2212 c2 I) = |L \u2212 c2 I|. This\ncan happen, e.g., if pm (\u03bb) is an interpolating polynomial of f (\u03bb) = |\u03bb| on the spectrum\nof L \u2212 c2 I, m = n \u2212 1. In such a situation, Algorithm 3.1 with B = pm (L \u2212 c2 I)\nresults in the following transformation of the initial error:\n(3.25)\n\neAV = S\u03042\u03bd K\u0304 S\u03041\u03bd eAV\n0 ,\n\n\f12\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\nwhere S\u03041 = I \u2212 M \u22121 |L \u2212 c2 I| and S\u03042 = I \u2212 M \u2212\u2217 |L \u2212 c2 I| are pre- and postsmoothing\noperators, and K\u0304 = I \u2212 P |LH \u2212 c2 IH |\u22121 R|L \u2212 c2 I| corresponds to the coarse grid\ncorrection step. The associated error propagation operator is further denoted by \u1e20,\n\u1e20 = S\u03042\u03bd K\u0304 S\u03041\u03bd .\n\n(3.26)\n\nFor the purpose of clarity, we again consider the 1D counterpart (3.4) of the model\nproblem. As a smoother, we choose Richardson's iteration with respect to absolute\nvalue system (3.9), i.e., S\u03041 = S\u03042 = I \u2212 \u03c4 |L \u2212 c2 I|. It is important to note here that the\neigenvalues |tj | of the absolute value operator are, in general, no longer ascendingly\nordered with respect to j as is the case for \u03b8j 's and tj 's. Moreover, in contrast to\nL and L \u2212 c2 I, the top part of the spectrum of |L \u2212 c2 I| may be associated with\nboth smooth and oscillatory eigenmodes. In particular, this means that Richardson's\niteration may fail to properly eliminate the oscillatory components of the error, which\nis an undesirable outcome of the smoothing procedure. To avoid this, we require that\n|t1 | < tN +1 . It is easy to verify that the latter condition is fulfilled if\n(3.27)\n\nch < 1.\n\nNote that (3.27) automatically holds if discretization rule (3.3) is enforced. Repeating\nthe above argument for the 2D case also leads to (3.27).\nLet the restriction and prolongation operators R and P be the same as in the\nprevious subsection. Similar to (3.18), we obtain an explicit expression for the action\nof the coarse grid correction operator K\u0304 on eigenvectors vj :\n\uf8f1\n!\n\uf8f4\n|tj |\n\uf8f4\n4 |tj |\n\uf8f4\nj = 1, . . . , N,\n1 \u2212 cj H vj + s2j c2j H vn+1\u2212j ,\n\uf8f4\n\uf8f4\n\uf8f4\n|tj |\n|tj |\n\uf8f2\nvj ,\nj = N + 1,\n(3.28) K\u0304vj =\n!\n\uf8f4\n\uf8f4\n\uf8f4\n|tj |\n|tj |\n\uf8f4\n4\nH\n\uf8f4\nvj + s2j c2j H\nvn+1\u2212j\n, j = N + 2, . . . , n.\n\uf8f4\n\uf8f3 1 \u2212 cj |tH\n|t\n|\n|\nn+1\u2212j\nn+1\u2212j\nThe following theorem is the analogue of Theorem 3.2.\nTheorem 3.3. The error propagation operator \u1e20 in (3.26) acts on the eigenvectors vj of the 1D Laplacian as follows:\n\uf8f1\n(11)\n(12)\n\uf8f4\n\uf8f2 \u1e21j vj + \u1e21j vn+1\u2212j , j = 1, . . . , N,\n\u1e21j vj ,\nj = N + 1,\n(3.29)\n\u1e20vj =\n\uf8f4\n\uf8f3 \u1e21 (21) v + \u1e21 (22) v\n,\nj\n= N + 2, . . . , n,\nj\nn+1\u2212j\nj\nj\nwhere\n(3.30)\n\n(11)\n\u1e21j\n\n(3.31)\n\n\u1e21j\n\n(12)\n\n2\u03bd\n\n= (1 \u2212 \u03c4 |tj |)\n\n1\u2212\n\n= (1 \u2212 \u03c4 |tj |)\u03bd s2j c2j\n\nc4j\n\n|tj |\n|tH\nj |\n\n!\n,\n\n|tj |\n(1 \u2212 \u03c4 |tn+1\u2212j |)\u03bd ,\n|tH\nj |\n\n\u1e21j = (1 \u2212 \u03c4 |tj |)2\u03bd ,\n\n(3.32)\n(3.33)\n\n(21)\n\u1e21j\n\n(3.34)\n\n\u1e21j\n\n(22)\n\n= (1 \u2212 \u03c4 |tj |)\n\n2\u03bd\n\n1\u2212\n\n= (1 \u2212 \u03c4 |tj |)\u03bd s2j c2j\n\nc4j\n\n|tj |\n|tH\nn+1\u2212j |\n\n!\n,\n\n|tj |\n(1 \u2212 \u03c4 |tn+1\u2212j |)\u03bd .\n|tH\n|\nn+1\u2212j\n\n\f13\n\nABSOLUTE VALUE PRECONDITIONING\n\nWe conclude from Theorem 3.3 that in the ideal case where pm (L\u2212c2 I) = |L\u2212c2 I|,\nAlgorithm 3.1 with B = pm (L \u2212 c2 I) and a proper choice of \u03c4 and \u03bd reduces the error\nof system (3.9) in the directions of all eigenvectors vj , possibly except for a few that\ncorrespond to \u03b8j close to the shift c2 . Unlike in the case of Algorithm 3.1 with B = L,\nas ch grows, no amplified error components appear in the directions of eigenvectors\nassociated with \u03b8j distant from c2 on the left. This suggests that Algorithm 3.1 with\nB = pm (L \u2212 c2 I) = |L \u2212 c2 I| provides a more accurate solve for (3.9) with larger ch.\nTo see this, let us first assume that \u03c4 = h2 /(3 \u2212 c2 h2 ). Since (3.27) implies that\n|tj | = tj for j > N , this choice is known to give the smallest uniform bound on |1\u2212\u03c4 |tj ||\ncorresponding to the oscillatory eigenmodes vj , which is |1 \u2212 \u03c4 |tj || < 1/(3 \u2212 ch) < 1/2\nwith the last inequality resulting from (3.27). Hence, coefficients (3.31)\u2013(3.34) can be\nreduced within a reasonably small number \u03bd of smoothing steps.\nNext, we note that (3.30), which is not substantially affected by smoothing, can\nbe large if c2 is close to \u03b8jH , i.e., if tH\nj \u2248 0. At the same time, we can write (3.30) as\n(11)\ngj\n\n= (1 \u2212 \u03c4 |tj |)\n\n2\u03bd\n\n1\u2212\n\nc2j\n\nc2 s2j\n1+ H\ntj\n\n!\n,\n\n(11)\n\nwhich shows that |gj | approaches (1\u2212\u03c4 |tj |)2\u03bd s2j < 1/2 as |tH\nj | increases, i.e., smooth\n2\nerror components associated with \u03b8j away from c are well damped.\nThus, if used as a preconditioner, Algorithm 3.1 with B = pm (L\u2212c2 I) = |L\u2212c2 I|\naims at clustering the spectrum of the preconditioned matrix around 1 and \u22121, with\na few possible outliers that result from the amplification of the smooth eigenmodes\nassociated with \u03b8j close to c2 . Unlike in the case where B = L, the increase of ch\ndoes not additionally amplify the smooth error components distant from c2 on the\nleft. Therefore, Algorithm 3.1 with B = pm (L \u2212 c2 I) = |L \u2212 c2 I| can be expected to\nprovide a more accurate preconditioner for larger shifts.\nAlthough our analysis targets the ideal but barely feasible case where pm (L \u2212\nc2 I) = |L \u2212 c2 I|, it motivates the use of polynomial approximations pm (L \u2212 c2 I) \u2248\n|L\u2212c2 I| and provides a theoretical insight into the superior behavior of such an option\nfor larger ch. In the rest of this subsection we describe a method for constructing such\npolynomial approximations. Our approach is based on the finding that the problem\nis easily reduced to constructing polynomial filters.\nWe start by introducing the step function\n\u001a\n1, \u03bb \u2265 \u03b1,\nh\u03b1 (\u03bb) =\n0, \u03bb < \u03b1;\nwhere \u03b1 is a real number, and noting that sign(\u03bb) = 2h0 (\u03bb) \u2212 1, so that\n\u0001\n\u0001\n(3.35)\n|L \u2212 c2 I| = 2h0 (L \u2212 c2 I) \u2212 I L \u2212 c2 I .\nHere h0 (L \u2212 c2 I) = V h0 (\u039b)V \u2217 , where V is the matrix of eigenvectors of L \u2212 c2 I and\nh0 (\u039b) = diag{0, . . . , 0, 1, . . . , 1} is obtained by applying the step function h0 (\u03bb) to the\ndiagonal entries of the matrix \u039b of the associated eigenvalues. Clearly the number of\nzeros on the diagonal of h0 (\u039b) equals the number of negative eigenvalues of L \u2212 c2 I.\nLet qm\u22121 (\u03bb) be a polynomial of degree at most (m \u2212 1), such that qm\u22121 (\u03bb)\napproximates h0 (\u03bb) on the interval [a, b], where a and b are the lower and upper bounds\non the spectrum of L \u2212 c2 I, respectively. In order to construct an approximation\n\n\f14\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\npm (L \u2212 c2 I) of |L \u2212 c2 I|, we replace the step function h0 (L \u2212 c2 I) in (3.35) by the\npolynomial qm\u22121 (L \u2212 c2 I). Thus,\n\u0001\n\u0001\n(3.36)\n|L \u2212 c2 I| \u2248 pm (L \u2212 c2 I) = 2qm\u22121 (L \u2212 c2 I) \u2212 I L \u2212 c2 I .\nThe matrix L \u2212 c2 I is readily available on the fine grid. Therefore, we have\nreduced the problem of evaluating the polynomial approximation pm of the absolute value operator to constructing a polynomial qm\u22121 that approximates the step\nfunction h0 . More specifically, since Algorithm 3.1 can be implemented without the\nexplicit knowledge of the matrix B, i.e., B can be accessed only through its action\non a vector, we need to construct approximations of the form qm\u22121 (L \u2212 c2 I)v to\nh0 (L \u2212 c2 I)v, where v is a given vector.\nThe task of constructing qm\u22121 (L \u2212 c2 I)v \u2248 h0 (L \u2212 c2 I)v represents an instance of\npolynomial filtering, which is well known; see, e.g., [11, 32, 45]. In this context, due to\nthe property of filtering out certain undesirable eigencomponents, the step function\nh0 is called a filter function. The approximating polynomial qm\u22121 is referred to as a\npolynomial filter.\nState-of-the-art polynomial filtering techniques such as [32] would first replace\nthe discontinuous step function h0 (\u03bb) by a smooth approximation on [a, b] and then\napproximate the latter by a polynomial in the least-squares sense. In this paper, we\nfollow a simpler approach based on the direct approximation of h0 (\u03bb) using Chebyshev\npolynomials [30, 31]. The constructed polynomial qm\u22121 allows defining qm\u22121 (L \u2212\nc2 I)v \u2248 h0 (L \u2212 c2 I)v and hence pm (L \u2212 c2 I)v \u2248 |L \u2212 c2 I|v. Below we describe the\nentire procedure.\nLet\n\u0013\n\u0012\nb+a\n2\n\u03bb\u2212\n(3.37)\n\u03be=\nb\u2212a\nb\u2212a\nbe a linear change of variable which maps \u03bb \u2208 [a, b] to \u03be \u2208 [\u22121, 1]. Then note that\nh0 (\u03bb) = h\u03b1 (\u03be), \u03b1 = \u2212\n\nb+a\n.\nb\u2212a\n\nLet Ti (\u03be) = cos(i arccos(\u03be)) be the Chebyshev polynomials of the first kind, \u03be \u2208\n[\u22121, 1]. Then the Chebyshev least-squares approximation q\u0304m\u22121 (\u03be) of the step function\nh\u03b1 (\u03be) is of the form\n(3.38)\n\nq\u0304m\u22121 (\u03be) =\n\nd\nX\n\n\u03b3i Ti (\u03be), \u03be \u2208 [\u22121, 1],\n\ni=0\n\nwhere the expansion coefficients are given by\nZ 1\n1\n1\np\n\u03b3i =\nh\u03b1 (\u03be)Ti (\u03be)d\u03be, i = 0, . . . , m \u2212 1;\nni \u22121 1 \u2212 \u03be 2\nZ 1\n1\np\nwith ni =\nT 2 (\u03be)d\u03be. Calculating the above integrals gives the exact ex2 i\n1\n\u2212\n\u03be\n\u22121\npressions for \u03b3i ,\n\uf8f1\n1\n\uf8f4\n\uf8f4\n(arccos(\u03b1)) ,\ni = 0,\n\uf8f4\n\uf8f2 \u03c0\n(3.39)\n\u03b3i =\n\u0012\n\u0013\n\uf8f4\n2 sin(i arccos(\u03b1))\n\uf8f4\n\uf8f4\n\uf8f3\n, i \u2265 1.\n\u03c0\ni\n\n\fABSOLUTE VALUE PRECONDITIONING\n\n15\n\nSince h0 (\u03bb) = h\u03b1 (\u03be), we define the polynomial approximation qm\u22121 (\u03bb) to h0 (\u03bb)\nas qm\u22121 (\u03bb) \u2261 q\u0304m\u22121 (\u03be), where \u03be is given by (3.37), \u03b1 = \u2212(b + a)/(b \u2212 a), and q\u0304m\u22121 (\u03be)\nis constructed according to (3.38), (3.39). Thus, by (3.36), we get\n|L \u2212 c2 I|v \u2248 pm (L \u2212 c2 I)v = 2\n\nm\u22121\nX\n\n\u03b3i Ti (C)t \u2212 t, t = (L \u2212 c2 I)v,\n\ni=0\n\n\u0001 b+a\n2\nL \u2212 c2 I \u2212\nI. Since the Chebyshev polynomials are generated\nb\u2212a\nb\u2212a\nusing the three-term recurrent relation [30, 31]\nwhere C =\n\nTi (\u03be) = 2\u03beTi\u22121 (\u03be) \u2212 Ti\u22122 (\u03be), T1 (\u03be) = \u03be, T0 (\u03be) = 1, i = 2, 3, . . . ,\nthe procedure for constructing the desired vectors w = pm (L \u2212 c2 I)v \u2248 |L \u2212 c2 I|v can\nbe summarized by the following algorithm.\nAlgorithm 3.2 (Polynomial approximation of the absolute value). Input: v,\nm, [a, b] \u2283 \u039b(L \u2212 c2 I). Output: w = pm (L \u2212 c2 I)v \u2248 |L \u2212 c2 I|v.\n1. Set v \u2190 (L \u2212 c2 I)v.\n\u0001 b+a\n2\nL \u2212 c2 I \u2212\nI. Set \u03b1 \u2190 \u2212(b + a)/(b \u2212 a).\n2. Set C \u2190\nb\u2212a\nb\u2212a\n3. Set v0 \u2190 v, v1 \u2190 Cv, w1 \u2190 \u03b30 v0 + \u03b31 v1 . Throughout, compute \u03b3i by (3.39).\n4. For i = 2, . . . , m \u2212 1 do\n5.\nvi \u2190 2Cvi\u22121 \u2212 vi\u22122\n6.\nwi \u2190 wi\u22121 + \u03b3i vi\n7. EndFor\n8. Return w \u2190 2wm\u22121 \u2212 v.\nAlgorithm 3.2 provides means to replace a matrix-vector product with the unavailable |L \u2212 c2 I| by a procedure that involves a few multiplications with the shifted\nLaplacian L\u2212c2 I. As we further show, the degree m of the approximating polynomial\ncan be kept reasonably low. Moreover, in the MG framework discussed in the next\nsubsection, the algorithm has to be invoked only on sufficiently coarse grids.\n3.4. The MG AV preconditioner. Now let us consider a hierarchy of s + 1\ngrids numbered by l = s, s \u2212 1, . . . , 0 with the corresponding mesh sizes {hl } in\ndecreasing order (hs = h corresponds to the finest grid, and h0 to the coarsest). For\neach level l we define the discretization Ll \u2212 c2 Il of the differential operator in (3.1),\nwhere Ll is the Laplacian on grid l, and Il is the identity of the same size.\nIn order to extend the two-grid AV preconditioner given by Algorithm 3.1 to the\nmultigrid, instead of inverting the absolute value LH \u2212 c2 IH in (3.6), we recursively\napply the algorithm to the restricted vector R(r \u2212 Bwpre ). This pattern is then\nfollowed in the V-cycle fashion on all levels, with the inversion of the absolute value\nof the shifted Laplacian on the coarsest grid. The matrix B on level l is denoted by\nBl . Each Bl is assumed to be SPD and is expected to approximate |Ll \u2212 c2 Il |.\nIn the previous subsections we have considered two choices of B for the two-grid\npreconditioner in Algorithm 3.1. In the MG framework, these choices give Bl = Ll\nand Bl = pml (Ll \u2212 c2 Il ), where pml is a polynomial of degree at most ml on level l.\nThe advantage of the first option, Bl = Ll , is that it can be easily constructed\nand the application of Bl to a vector is inexpensive even if the size of the operator\nis very large. According to our analysis for the 1D model problem in subsection 3.2,\n\n\f16\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\nthe approach is suitable for chl sufficiently small. Typically this is a case for l corresponding to finer grids. However, chl increases with every new level. This may result\nin the deterioration of accuracy of the overall MG preconditioning scheme, unless the\nsize of the coarsest level is kept sufficiently large.\nThe situation is different for the second option Bl = pml (Ll \u2212 c2 Il ). In this case,\napplications of Bl may be expensive on finer grids because they require a sequence\nof matrix-vector multiplications with large shifted Laplacian operators. However, on\ncoarser levels, i.e., for larger chl , this is not restrictive because the involved operators are significantly decreased in size compared to the finest level. Additionally,\nas suggested by the analysis in subsection 3.3, if pml (Ll \u2212 c2 Il ) represent reasonable\napproximations of |Ll \u2212c2 Il | on levels l, one can expect a higher accuracy of the whole\npreconditioning scheme compared to the choice Bl = Ll .\nOur idea is to combine the two options. Let \u03b4 \u2208 (0, 1) be a \"switching\" parameter,\nwhere for finer grids chl < \u03b4. We choose\n\u001a\nLl ,\nchl < \u03b4,\n(3.40)\nBl =\npml (Ll \u2212 c2 Il ), chl \u2265 \u03b4.\nThe polynomials pml (Ll \u2212 c2 Il ) are accessed through their action on a vector and are\nconstructed using Algorithm 3.2 with L \u2212 c2 I \u2261 Ll \u2212 c2 Il and m \u2261 ml .\nSummarizing our discussion, if started from the finest grid l = s, the following\nscheme gives the multilevel extension of the two-grid AV preconditioner defined by\nAlgorithm 3.1. The subscript l is introduced to match quantities to the corresponding\ngrid. We assume that the parameters \u03b4, ml , \u03bdl , and the smoothers Ml are pre-specified.\nAlgorithm 3.3 (AV-MG(rl ): the MG AV preconditioner). Input rl . Output wl .\n1. Set Bl by (3.40).\n2. Presmoothing. Apply \u03bdl smoothing steps, \u03bdl \u2265 1:\n(3.41)\n\n(i+1)\n\nwl\n\n(i)\n\n(i)\n\n(0)\n\n= wl + Ml\u22121 (rl \u2212 Bl wl ), i = 0, . . . , \u03bdl \u2212 1, wl\n\n= 0,\n\n(\u03bd)\n\nwhere Ml defines a smoother on level l. Set wlpre = wl .\n3. Coarse grid correction. Restrict (Rl\u22121 ) rl \u2212Bl wlpre to the grid l\u22121, recursively\napply AV-MG, and prolongate (Pl ) back to the fine grid. This delivers the\ncoarse grid correction added to wlpre :\n\u001a\n\u22121\nL0 \u2212 c2 I0\nR0 (r1 \u2212 B1 w1pre ) , l = 1,\n(3.42)\nwl\u22121 =\nAV-MG (Rl\u22121 (rl \u2212 Bl wlpre )) ,\nl > 1;\nwlcgc = wlpre + Pl wl\u22121 .\n\n(3.43)\n\n4. Postsmoothing. Apply \u03bdl smoothing steps:\n(3.44)\n\n(i+1)\n\nwl\n\n(i)\n\n(i)\n\n(0)\n\n= wl + Ml\u2212\u2217 (rl \u2212 Bl wl ), i = 0, . . . , \u03bdl \u2212 1, wl\n\n= wlcgc ,\n(\u03bd )\n\nwhere Ml and \u03bdl are the same as in step 2. Return wl = wlpost = wl l .\nThe described MG AV preconditioner implicitly constructs a mapping denoted\nby r 7\u2192 w = Tmg r, where the operator T = Tmg has the following structure:\n\u0001\u03bd\n\u0001\u03bd\n(s\u22121)\n(3.45)\nTmg = I \u2212 M \u2212\u2217 B P Tmg\nR I \u2212 BM \u22121 + F,\n\n\fABSOLUTE VALUE PRECONDITIONING\n(s\u22121)\n\nwith F as in (3.10) and Tmg\n(l)\n\n(3.46)\n\nTmg\n(0)\nTmg\n\n=\n=\n\n17\n\ndefined according to the recursion\n\n\u0001\u03bdl\n\u0001\u03bd l\n(l\u22121)\nIl \u2212 Ml\u2212\u2217 Bl Pl Tmg Rl\u22121 Il \u2212 Bl Ml\u22121 + Fl ,\n\u22121\nL0 \u2212 c2 I0\n, l = 1, . . . , s \u2212 1,\n\n\u0001\u03bdl\n\u0001\u03bdl\nwhere Fl = Bl\u22121 \u2212 Il \u2212 Ml\u2212\u2217 Bl Bl\u22121 Il \u2212 Bl Ml\u22121 .\nThe structure of the multilevel preconditioner T = Tmg in (3.45) is the same as\n\u22121\nreplaced by\nthat of the two-grid preconditioner T = Ttg in (3.10), with LH \u2212 c2 IH\n(m\u22121)\nthe recursively defined operator Tmg\nin (3.46). Thus, the symmetry and positive\ndefiniteness of T = Tmg follows from the same property of the two-grid operator\n\u2217\nthrough relations (3.46), provided that Pl = \u03b1l Rl\u22121\nand the spectral radii of Il \u2212\n\u22121\n\u2212\u2217\nMl Bl and Il \u2212 Ml Bl are less than 1 throughout the coarser levels. We remark\nthat preconditioner (3.45)\u2013(3.46) is non-variable, i.e., it preserves the global optimality\nof PMINRES.\nThe simplest possible approach for computing w0 in (3.42) is to explicitly construct |L0 \u2212 c2 I0 |\u22121 through the full eigendecomposition of the coarse-level Laplacian,\nand then apply it to R0 (r1 \u2212 B1 w1pre ). An alternative approach is to determine w0 as\na solution of the linear system (L0 \u2212 c2 I0 + 2V0 |\u039b0 |V0\u2217 )w0 = R0 (r1 \u2212 B1 w1pre ), where\nV0 is the matrix of eigenvectors associated with the negative eigenvalues of L0 \u2212 c2 I0\ncontained in the corresponding diagonal matrix \u039b0 . In the latter case, the full eigendecomposition of L0 is replaced by the partial eigendecomposition targeting negative\neigenpairs, followed by a linear solve.\nTo solve the linear system approximately,\nadditional gains may be obtained by applying the Woodbury formula [19, p.50] to the\nmatrix (L0 \u2212 c2 I0 + 2V0 |\u039b0 |V0\u2217 )\u22121 , provided that systems Cz = r can be efficiently\nsolved for some C \u2248 L0 \u2212 c2 I0 . In this work, we rely only on exact coarse grid solves.\nSince we use Richardson's iteration with respect to pml (Ll \u2212 c2 Il ) as a smoother\non coarser grids, as motivated by the discussion in subsection 3.3, the guidance for\nthe choice of the coarsest grid is given by condition (3.27). More specifically, in the\ncontext of the standard coarsening procedure (hl\u22121 = 2hl ), we select hierarchies of\ngrids satisfying chl < 1 for l = s, . . . , 1, and ch0 > 1. As shown in the next section,\neven for reasonably large c2 , the coarsest-level problems are small.\nThe parameter \u03b4 in (3.40) should be chosen to ensure the balance between computational costs and the quality of the MG preconditioner. In particular, if \u03b4 is\nreasonably large then the choices of Bl are dominated by the option Bl = Ll ,\nwhich is inexpensive but may not be suitable for larger shifts on coarser levels.\nOn the other extreme, if \u03b4 is close to zero then the common choice corresponds to\nBl = pml (Ll \u2212 c2 Il ), which provides a better preconditioning accuracy for larger shifts\nbut may be too computationally intense on finer levels. In our numerical experiments,\nwe keep \u03b4 \u2208 [1/3, 3/4].\nAs we demonstrate in the next section, the degrees ml of the occurring polynomials pml should not be large, i.e., only a few matrix-vector multiplications with Ll \u2212c2 Il\nare required to obtain satisfactory approximations of absolute value operators. For\nproperly chosen \u03b4, these additional multiplications need to be performed on grids that\nare significantly coarser than the finest grid, i.e., the involved matrices Ll \u2212 c2 Il are\norders of magnitude smaller than the original fine grid operator. As confirmed by\nour numerical experiments, the overhead caused by the polynomial approximations\nappears to be marginal and does not affect much the computational cost of the overall\npreconditioning scheme.\n\n\f18\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\n4. Numerical experiments. This section presents a numerical study of the\nMG preconditioner in Algorithm 3.3. Our goal here is twofold. On the one hand, the\nreported numerical experiments serve as a proof of concept of the AV preconditioning\ndescribed in Section 2. In particular, we show that the AV preconditioners can be\nconstructed at essentially the same cost as the standard preconditioning methods (MG\nin our case). On the other hand, we demonstrate that the MG AV preconditioner\nin Algorithm 3.3 combined with the optimal PMINRES iteration, in fact, leads to\nan efficient and economical computational scheme, further called MINRES-AV-MG,\nwhich outperforms several known competitive approaches for the model problem.\nLet us briefly describe the alternative preconditioners used for our comparisons.\nThroughout, we use matlab for our numerical examples.\nThe inverted Laplacian preconditioner. This strategy, introduced in [2],\nis a representative of an SPD preconditioning for model problem (3.2), where the\npreconditioner is applied through solving systems Lw = r, i.e., T = L\u22121 . As has\nbeen previously discussed, for relatively small shifts c2 , the Laplacian L constitutes\na good SPD approximation of |L \u2212 c2 I|. In this sense, the choice T = L\u22121 perfectly\nfits, as a special case, into the general concept of the AV preconditioning presented in\nSection 2. We refer to PMINRES with T = L\u22121 as MINRES-Laplace.\nUsually, one wants to solve the system Lw = r only approximately, i.e., use\nT \u2248 L\u22121 . This can be efficiently done, e.g., by applying the V-cycle of a standard MG\nmethod [8, 39]. In our tests, however, we perform the exact solves using the matlab's\n\"backslash\", so that the reported results reflect the best possible convergence with\nthe inverted Laplacian type preconditioning.\nThe indefinite MG preconditioner. We consider a standard V-cycle for problem (3.2). Formally, it can be obtained from Algorithm 3.3 by setting Bl = Ll \u2212 c2 Il\non all levels and replacing the first equality in (3.42) by the linear solve with L0 \u2212c2 I0 .\nThe resulting MG scheme is used as a preconditioner for restarted GMRES and for\nBi-CGSTAB. We refer to these methods as GMRES(k)-MG and Bi-CGSTAB-MG,\nrespectively; k denotes the restart parameter. A thorough discussion of the indefinite\nMG preconditioning for Helmholtz problems can be found, e.g., in [13].\nTable 4.1\nThe largest problem sizes satisfying chl \u2265 \u03b4 for different values of the shift c2 , \"switching\"\nparameters \u03b4, and the standard coarsening scheme hl\u22121 = 2hl . The last row (\u03b4 = 1) corresponds to\nthe sizes of the coarsest problems for different c2 .\n\n\u03b4 = 1/3\n\u03b4 = 1/2\n\u03b4 = 3/4\n\u03b4=1\n\nc2 = 300\n\nc2 = 400\n\nc2 = 1500\n\nc2 = 3000\n\nc2 = 4000\n\n961\n961\n225\n225\n\n961\n961\n225\n225\n\n3969\n3969\n961\n961\n\n16129\n3969\n3969\n961\n\n16129\n3969\n3969\n961\n\nIn our tests, we consider 2D model problem (3.2) corresponding to (3.1) discretized\non the grid of size h = 2\u22128 (the fine problem size n = 65025). The exact solution x\u2217\nand the initial guess x0 are randomly chosen. The right-hand side b = (L \u2212 c2 I)x\u2217 ,\nwhich allows evaluating the actual errors along the steps of an iterative method. All\nthe occurring MG preconditioners are built upon the standard coarsening scheme (i.e.,\nhl\u22121 = 2hl ), restriction is based on the full weighting, and prolongation on piecewise\nmultilinear interpolation [8, 39].\n\n\f19\n\nABSOLUTE VALUE PRECONDITIONING\n\nLet us recall that Algorithm 3.3 requires setting a parameter \u03b4 to switch between\nBl = L and pml (L\u2212c2 I) on different levels; see (3.40). Assuming standard coarsening,\nTable 4.1 presents the largest problem sizes corresponding to the condition chl \u2265 \u03b4 for\na few values of \u03b4 and c2 . In other words, given \u03b4 and c2 , each cell of Table 4.1 contains\nthe largest problem size for which the polynomial approximation of |Ll \u2212 c2 Il | is\nconstructed. Unless otherwise explicitly stated, we set \u03b4 = 1/3. Note that according to\nthe discussion in subsection 3.4 (condition (3.27)), the row of Table 4.1 corresponding\nto \u03b4 = 1 delivers the sizes n0 of the coarsest problems for different shift values.\nTable 4.1 shows that the coarsest problems remain relatively small even for large\nshifts. The polynomial approximations are constructed for coarser problems of significantly reduced dimensions, which in practical applications are negligibly small\ncompared to the original problem size.\nAs a smoother on all levels of Algorithm 3.3 we use Richardson's iteration, i.e.,\nMl\u22121 \u2261 \u03c4l Il . On the finer levels, where Bl = Ll , we choose \u03c4l = h2l /5 and \u03bdl = 1. On\nthe coarser levels, where Bl = pml (Ll \u2212 c2 Il ), we set \u03c4l = h2l /(5 \u2212 c2 h2l ) and \u03bdl = 5.\nSimilar to the 1D case considered in subsections 3.2 and 3.3, both choices of\n\u03c4l correspond to the optimal smoothing of oscillatory eigenmodes with respect to\nthe 2D operators Ll and |Ll \u2212 c2 Il |, respectively [8, 39]. Since pml (Ll \u2212 c2 Il ) only\napproximates |Ll \u2212 c2 Il | in practice, the choice of \u03c4l on the coarser grids is likely to be\nnot optimal. Therefore, for chl \u2265 \u03b4, we have increased the number of smoothing steps\nto 5. In all tests, the degrees ml of polynomials are set to 10. The intervals containing\n\u039b(Ll \u2212 c2 Il ), required for evaluating pml , are [\u2212c2 , 8h\u22122\n\u2212 c2 ]. The inverted coarse\nl\n2\n\u22121\ngrid absolute value |L0 \u2212 c I0 | is constructed by the full eigendecomposition.\n\nBi\u2212CGSTAB\u2212MG\nGMRES(5)\u2212MG\nMINRES\u2212AV\u2212MG\nMINRES\u2212Laplace\n\n0\n\n10\n\n\u22125\n\n10\n\n0\n\n10\n\n\u22125\n\n10\n\n\u221210\n\n\u221210\n\n10\n\n10\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n0\n\n20\n\n40\n\n60\n\n80\n\nIteration number\n\nIteration number\n\nShift value c2 = 1500 (108 negative eigenvalues)\n\nShift value c2 = 3000 (222 negative eigenvalues)\n\n5\n\nBi\u2212CGSTAB\u2212MG\nGMRES(20)\u2212MG\nMINRES\u2212AV\u2212MG\nMINRES\u2212Laplace\n\n10\n\nEuclidean norm of error\n\nBi\u2212CGSTAB\u2212MG\nGMRES(10)\u2212MG\nMINRES\u2212AV\u2212MG\nMINRES\u2212Laplace\n\n0\n\n10\n\n\u22125\n\n10\n\nEuclidean norm of error\n\nEuclidean norm of error\n\n10\n\nShift value c2 = 400 (26 negative eigenvalues)\n\nEuclidean norm of error\n\nShift value c2 = 300 (19 negative eigenvalues)\n5\n\n0\n\n10\n\n\u22125\n\n10\n\nBi\u2212CGSTAB\u2212MG\nGMRES(150)\u2212MG\nMINRES\u2212AV\u2212MG\nMINRES\u2212Laplace\n\n\u221210\n\n10\n\n0\n\n100\n\n200\n\n300\n\nIteration number\n\n400\n\n500\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\nIteration number\n\nFig. 4.1. Comparison of several preconditioned schemes; n = 65025.\n\n600\n\n700\n\n\f20\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\nIn Figure 4.1, we compare MINRES-AV-MG with the above introduced alternative preconditioned schemes for the model problem. Each plot corresponds to a\ndifferent shift value. The restart parameter k varies for all runs of GMRES(k)-MG,\nincreasing (left to right and top to bottom) as c2 grows from 300 to 3000. In our\ntests, the size n0 of the coarsest problem in Algorithm 3.3 is 225 (Figure 4.1, top) and\n961 (Figure 4.1, bottom); see Table 4.1 with \u03b4 = 1. The same n0 is used for the MG\npreconditioner in the corresponding runs of GMRES(k)-MG and Bi-CGSTAB-MG.\nFigure 4.1 shows that MINRES-AV-MG noticeably outperforms PMINRES with\nthe inverted Laplacian preconditioner. For smaller shifts (c2 = 300, 400), MINRESAV-MG is comparable, in terms of the iteration count, to GMRES(k)-MG and BiCGSTAB-MG; k = 5, 10. For larger shifts (c2 = 1500, 3000), however, MINRESAV-MG provides a superior convergence behavior. In particular, the scheme exhibits\nfaster convergence than GMRES(k)-MG under less demanding storage requirements,\nwhile Bi-CGSTAB-MG fails to converge (k = 50, 150).\nIf the polynomial approximations in Algorithm 3.3 appear only on sufficiently\ncoarse grids and the size n0 of the coarsest problem is relatively small, then the additional costs introduced by the coarser grid computations of the AV MG preconditioner\nare negligible relative to the cost of operations on the finer grids, which are the same\nas in the standard V-cycle for the indefinite problem. This means that the complexity\nof Algorithm 3.3 is similar to that of the MG preconditioner in GMRES(k)-MG and\nBi-CGSTAB-MG.\nTo be more precise, in the tests reported in Figure 4.1 (bottom), a single application of Algorithm 3.3 has required 15\u201320% more time than the indefinite MG\npreconditioner, even though the polynomial approximations in Algorithm 3.3 have\nbeen constructed for problem sizes as large as 16129 if c2 = 3000, which is of the same\norder as the original problem size n. For larger problem sizes, the time difference\nbecomes negligible. For example if h = 2\u22129 (n = 261121), Algorithm 3.3 results in\nonly 5% time increase, and the relative time difference becomes indistinguishable for\nsmaller h. The application of all the MG preconditioners in Figure 4.1 (top) required\nessentially the same time.\nThe above discussion suggests that our numerical comparison, based on the number of iterations, is representative. Additionally, in Table 4.2 we provide the time\ncomparisons for the MG preconditioned schemes. In particular, we measure the actual time required by the runs of MINRES-AV-MG (tAV ), GMRES(k)-MG (tG ), and\nBi-CGSTAB-MG (tB ) in Figure 4.1, and report the speed-ups.\nTable 4.2\nTime comparison of the MG preconditioned schemes in Figure 4.1.\n\ntB /tAV\ntG /tAV\n\nc2 = 300\n\nc2 = 400\n\nc2 = 1500\n\nc2 = 3000\n\n1.1\n1.4\n\n1.1\n1.3\n\n\u2212\n2.6\n\n\u2212\n1.9\n\nWe have observed that the performance of GMRES(k)-MG can be improved by\nincreasing the restart parameter. In Figure 4.1, however, the values k have been\nchosen to balance between storage and convergence behavior. In particular, we set k\nto be sufficiently small, so that the storage required for GMRES(k)-MG is as close as\npossible to that of MINRES-AV-MG, while the convergence of the method is not lost.\nSince Bi-CGSTAB-MG is based on a short-term recurrence, its storage expenses are\n\n\f21\n\nABSOLUTE VALUE PRECONDITIONING\n\nsimilar to that MINRES-AV-MG.\nThe unsatisfactory performance of GMRES(k)-MG in Figure 4.1 can, in part,\nbe attributed to the observation that smoothing based on Richardson's (or, more\ngenerally, Jacobi) iteration becomes increasingly unstable as grids coarsen. In particular, as shown in [13], on the intermediate levels with chl \u2265 1/2 this smoothing\nscheme strongly amplifies the smooth error eigenmodes. A straightforward remedy is\nto invoke the coarse grid solve on the largest grid that fails to satisfy chl < 1/2.\nTable 4.3\nNumber of iterations of MINRES-AV-MG (MINR) and GMRES(k)-MG (GMR(k)) required to\nreduce the initial error by 10\u22128 ; n = 65025. The preconditioner in GMRES(k)-MG uses Richardson's smoothing on levels chl < 1/2 and invokes the coarse grid solve on the level that follows.\nNumbers in the parentheses correspond to the right preconditioned GMRES(k)-MG. Dash denotes\nthat the method failed to converge within 1000 steps.\n\n2\n\nc = 1500\nc2 = 3000\nc2 = 4000\n\nMINR\n\nGMR(5)\n\nGMR(10)\n\nGMR(20)\n\nGMR(25)\n\nGMR(35)\n\n89\n282\n310\n\n29(44)\n\u2212(\u2212)\n\u2212(\u2212)\n\n20(22)\n\u2212(\u2212)\n\u2212(\u2212)\n\n16(18)\n\u2212(\u2212)\n\u2212(\u2212)\n\n16(18)\n223(269)\n\u2212(\u2212)\n\n16(18)\n69(69)\n395(471)\n\nIn Table 4.3, we compare MINRES-AV-MG and GMRES(k)-MG with different\nvalues of the restart parameter. We report the iteration counts required to reduce\nthe initial error by 10\u22128 for systems with c2 = 1500, 3000, and 4000. The indefinite\nMG preconditioner in GMRES(k)-MG is configured to run Richardson's smoothing\non grids chl < 1/2 and perform the coarse grid solve on the level that follows. We\ntest the indefinite MG preconditioner in the left (used so far) and right preconditioned\nversions of GMRES(k).\nThe above described setting of the right preconditioned GMRES(k)-MG represents a special case of the Helmholtz solver introduced in [13]. In this paper, instead\nof the \"early\" coarse grid solve, the MG preconditioning scheme performs further\ncoarsening on levels chl \u2265 1/2 with GMRES used as a smoother. Since the resulting\npreconditioner is nonlinear, (the full) FGMRES [33] is used for outer iterations.\nIt is clear that the results in Table 4.3 provide an insight into the best possible outer iteration counts that can be expected from the restarted version of the\nmethod in [13]. In fact, the same observation is used by the authors of [13] for handtuning the smoothing schedule of their preconditioner to study its best-case performance. Table 4.3 then demonstrates that, regardless of the smoothing schedule and\nthe smoother's stopping criterion, the (F)GMRES based solvers, such as, e.g., the one\nin [13], require increasing storage to maintain the convergence as c2 grows, whereas\nthe robustness of MINRES-AV-MG is not lost under the minimalist memory costs.\nThus, if the shift is large and the amount of storage is limited, so that k is forced\nto be sufficiently small, the (F)GMRES(k) outer iterations may fail to converge within\na reasonable number of steps, even if the coarse grid solve in the MG preconditioner\nis performed \"early\" . We note, however, that if storage is available or the shifts are\nnot too large, the (F)GMRES based methods may represent a valid option.\nWe have also tested the Bunch-Parlett factorization [18] as a coarse grid solve\nin the MG framework. In particular, as a preconditioner for MINRES, we have used\nAlgorithm 3.3 with Bl = Ll on all levels and the coarsest-grid absolute value in (3.42)\nreplaced by the application of the \"perfect\" Bunch-Parlett factorization based preconditioner [18]. We have obtained results that are inferior to the schemes considered\n\n\f22\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\nin this paper for shifts not too small, e.g., for c2 > 200 if h = 2\u22128 . The unsatisfactory behavior may be related to the fact that the inverted Laplacian T = L\u22121 and\n\u22121\nthe ideal absolute value T = L \u2212 c2 I\npreconditioners share the same eigenvectors\nwith A = L \u2212 c2 I, while the preconditioner from [18] does not.\nThe standard MG preconditioners, as in GMRES(k)-MG and Bi-CGSTAB-MG,\nare known to have optimal costs, linearly proportional to n. As discussed above, the\nsame is true for the AV preconditioner in Algorithm 3.3. Therefore, if, in addition, the\nnumber of iterations in the iterative solver preconditioned with Algorithm 3.3 does\nnot depend on the problem size, the overall scheme is optimal.\nTable 4.4\nMesh-independent convergence of PMINRES with the MG AV preconditioner. The numbers in\nparentheses correspond \u03b4 = 3/4. The default value of \u03b4 is 1/3.\n\nc2 = 300\nc2 = 400\nc2 = 1500\nc2 = 3000\n\nh = 2\u22126\n\nh = 2\u22127\n\nh = 2\u22128\n\nh = 2\u22129\n\nh = 2\u221210\n\nh = 2\u221211\n\n31(31)\n37(40)\n67(97)\n228(229)\n\n31(31)\n38(40)\n97(119)\n222(284)\n\n30(32)\n37(40)\n89(109)\n279(332)\n\n30(32)\n37(40)\n88(108)\n256(298)\n\n30(32)\n37(40)\n89(106)\n257(296)\n\n30(30)\n37(39)\n90(107)\n256(298)\n\nWe verify this optimality in Table 4.4, which shows the mesh-independence of the\nconvergence of PMINRES with the MG AV preconditioner. The rows of the table\ncorrespond to the shift values c2 , while the columns match the mesh size h. The cell\nin the intersection contains the numbers of steps performed to achieve the decrease\nby the factor 10\u22128 in the error 2-norm with the choices of the \"switching\" parameter\n\u03b4 = 1/3 and \u03b4 = 3/4.\nAs previously, the size of the coarsest grid has been set according to Table 4.1\nwith \u03b4 = 1. We conclude that the convergence does not slowdown with the decrease\nof h; thus, PMINRES preconditioned by Algorithm 3.3 is optimal. Note that for\nlarger shifts, c2 = 1500 and c2 = 3000, mesh-independent convergence occurs for h\nsufficiently small, when the \"switching\" pattern is stabilized, i.e., Bl = Ll on a few\nfiner grids and Bl = pml (Ll \u2212 c2 Il ) on the coarser grids that follow.\nTable 4.4 shows that as c2 grows, the increase in the iteration count is mild and\nessentially linear. As expected, the smaller value of \u03b4, which leads to the construction\nof the polynomial approximations earlier on finer levels, results in a higher accuracy\nof the AV preconditioner.\nFinally, in Figure 4.2 we plot the eigenvalues of T L \u2212 c2 I and T (L \u2212 c2 I) for\ndifferent shift values using the logarithmic scale; n = 16129. As suggested by Corollary 2.5, clusters of eigenvalues of T L \u2212 c2 I are preserved in the spectrum of the\npreconditioned matrix T (L \u2212 c2 I). Almost all eigenvalues of T (L \u2212 c2 I) are clustered\naround \u22121 and 1, with only a few falling outside of the clusters. We note that the\nclustering and the condition number \u03ba(T |A|) deteriorate as c2 increases from 300 to\n3000, which is compatible with the results in Table 4.4.\nThe spectra computed in Figure 4.2 allow validating numerically the tightness of\nbounds (2.4) in Theorem 2.3 for the MG AV preconditioner. In Table 4.5, we report\nthe number of eigenvalues \u03bbj of T (L \u2212 c2 I) that satisfy either the upper or the lower\nbound up to machine precision. The table shows that the bound is numerically sharp.\n\n\f23\n\nABSOLUTE VALUE PRECONDITIONING\n\nShift value c2\n\n3000\n\nEigenvalues of T|L \u2212 c2 I| and T(L \u2212 c2 I) for different shifts\n3000\n3000\n\n1500\n\n1500\n\n1500\n\n400\n\n400\n\n400\n\n300\n\n0\n\n300 0\n300\n\u22121\n\u221210 \u221210\n\u039b(T(L \u2212 c2 I))<0\n\n1\n\n10 10\n\u039b(T|L \u2212 c2 I|)\n\n0\n\n1\n\n10 10\n\u039b(T(L \u2212 c2 I))>0\n\nFig. 4.2. Spectrum of T L \u2212 c2 I (left), negative eigenvalues of T L \u2212 c2 I\n\u0001\npositive eigenvalues of T L \u2212 c2 I (right); n = 16129.\n\n\u0001\n\n(center), and\n\nTable 4.5\nNumber of eigenvalues \u03bbj that equal the upper/lower bound in (2.4) up to machine precision.\n\nUpper\nLower\n\nc2 = 300\n\nc2 = 400\n\nc2 = 1500\n\nc2 = 3000\n\n0\n0\n\n15\n0\n\n1\n10\n\n115\n0\n\n5. Conclusions. We propose a new approach for SPD preconditioning for symmetric indefinite systems, based on the idea of implicitly constructing approximations\nto the inverse of the system matrix absolute value. A multigrid example of such a\npreconditioner is presented, for a real-valued Helmholtz problem. Our experiments\ndemonstrate that PMINRES with the new MG absolute value preconditioner leads to\nan efficient iterative scheme, which has modest memory requirements and outperforms\ntraditional GMRES based methods if available memory is tight.\nAcknowledgments. The authors thank Michele Benzi, Yvan Notay, and Joe Pasciak for their comments on the draft of the manuscript.\nREFERENCES\n\n[1] T. Airaksinen, E. Heikkola, and A. Pennanen, An algebraic multigrid based shiftedLaplacian preconditioner for the Helmholtz equation, J. Comput. Phys., 226 (2007) pp.\n1196\u20131210.\n[2] A. Bayliss, C. I. Goldstein, and E. Turkel, An iterative method for the Helmholtz equation,\nJ. Comput. Phys., 49 (1983) pp. 443\u2013457.\n[3] M. Benzi, G. H. Golub, and J. Liesen, Numerical solution of saddle point problems, Acta\nNumerica, 14 (2005), pp. 1\u2013137.\n[4] J. H. Bramble, Z. Leyk, and J. E. Pasciak, Iterative schemes for nonsymmetric and indefinite elliptic boundary value problems, Math. Comp., 60 (1993), pp. 1\u201322.\n[5] J. H. Bramble and X. Zhang, The analysis of multigrid methods, in Handb. Numer. Anal.,\nSolution of Equation in Rn (Part 3), Techniques of Scientific Computing (Part 3), Elsevier,\npp. 173\u2013415, 2000.\n[6] M. Bollh\u00f6fer, M. J. Grote, and O. Schenk, Algebraic multilevel preconditioner for the\nHelmholtz equation in heterogeneous media, SIAM J. Sci. Comput., 31 (2009), pp. 3781\u2013\n3805.\n[7] A. Brandt and S. Ta'asan, Multigrid method for nearly singular and slightly indefinite problems, in Lecture Notes in Mathematics, Multigrid Methods II, Springer Berlin / Heidelberg,\npp. 99\u2013121, 1986.\n\n\f24\n\nEUGENE VECHARYNSKI AND ANDREW V. KNYAZEV\n\n[8] W. L. Briggs, V. E. Henson, and S. F. McCormick, A Multigrid Tutorial, SIAM, 2000.\n[9] J. R. Bunch and B. N. Parlett, Direct methods for solving symmetric indefinite systems of\nlinear equations, SIAM J. Numer. Anal., 8 (1971), pp. 639\u2013655.\n[10] E. G. D'yakonov, Optimization in Solving Elliptic Problems, CRC-Press, 1996.\n[11] J. Erhel, F. Guyomarc, and Y. Saad, Least-squares polynomial filters for ill-conditioned\nlinear systems, Report UMSI-2001-32, Minnesota Supercomputer Institute, University of\nMinnesota, Minneapolis, MN, 2001.\n[12] Y. A. Erlangga, C. Vuik, and C. W. Oosterlee, On a class of preconditioners for solving\nthe Helmholtz equation, Appl. Numer. Math, 50 (2004), pp. 409\u2013425.\n[13] H. C. Elman, O. G. Ernst, and D. O'Leary, A multigrid method enhanced by Krylov subspace\niteration for discrete Helmholtz equations, SIAM J. Sci. Comput., 23 (2001), pp. 1291\u20131315.\n[14] H. C. Elman, D. J. Silvester, and A.J. Wathen, Finite Elements and Fast Iterative Solvers\nwith Applications in Incompressible Fluid Dynamics, Oxford University Press, 2005.\n[15] V. Faber and T. Manteuffel, Necessary and sufficient conditions for the existence of a\nconjugate gradient method, SIAM J. Numer. Anal., 21 (1984), pp. 352\u2013362.\n[16] B. Fischer, A. Ramage, D. J. Silvester and A. J. Wathen, Minimum residual methods for\naugmented systems, BIT, 38 (1998), pp. 527\u2013543.\n[17] R. W. Freund and N. M. Nachtigal, QMR: a quasi-minimal residual method for nonHermitian linear systems, Numer. Math., 60 (1991), pp. 315\u2013339.\n[18] P. E. Gill, W. Murray, D. B. Poncele\u00f3n, and M. A. Saunders, Preconditioners for indefinite systems arising in optimization, SIAM J. Matrix Anal. Appl., 13 (1992), pp. 292\u2013311.\n[19] G. H. Golub and C. F. Van Loan, Matrix Computations, The Johns Hopkins University\nPress, 1996.\n[20] A. Greenbaum, V. Pt\u00e1k, and Z. Strako\u0161, Any nonincreasing convergence curve is possible\nfor GMRES, SIAM J. Matrix Anal. Appl., 17 (1996), pp. 465\u2013469.\n[21] E. Haber and S. MacLachlan, A fast method for the solution of the Helmholtz equation, J.\nComp. Phys., 230 (2011), pp. 4403\u20134418.\n[22] I. Harari and T. J. R. Hughes, Finite element method for the Helmholtz equation in an\nexterior domain, Comput. Methods Appl. Mech. Engrg., 87 (1991), pp. 59\u201396.\n[23] N. J. Higham, Functions of Matrices: Theory and Computation, SIAM, Philadelphia, PA,\nUSA, 2008.\n[24] R. A. Horn and C. R. Johnson, Topics in Matrix Analysis, Cambridge University Press, New\nYork, 1994.\n[25] A. L. Laird and M. B. Giles, Preconditioned iterative solution of the 2D Helmholtz equation,\nOxford University Computing Laboratory, Oxford, UK, 02/12, 2002.\n[26] J. Nocedal and S. Wright, Numerical Optimization, Springer, 1999.\n[27] D. Osei-Kuffuor and Y. Saad, Preconditioning Helmholtz linear systems, Appl. Numer.\nMath., 60 (2010), pp. 420\u2013431.\n[28] C. C. Paige and M. A. Saunders, Solution of sparse indefinite systems of linear equations,\nSIAM J. Numer. Anal., 12 (1975), pp. 617\u2013629.\n[29] B. N. Parlett, The Symmetric Eigenvalue Problem, SIAM, 1998.\n[30] M. J. D. Powell, Approximation Theory and Methods, Cambridge University Press, 1981.\n[31] T. J. Rivlin, An Introduction to the Approximation of Functions, Dover, New York, 1981.\n[32] Y. Saad, Filtered conjugate residual-type algorithms with applications, SIAM J. Matrix Anal.\nAppl., 28 (2006), pp. 845\u2013870.\n[33] Y. Saad, A flexible inner-outer preconditioned GMRES algorithm, SIAM J. Sci. Comput., 14\n(1993), pp. 461\u2013469.\n[34] Y. Saad, Iterative Methods for Sparse Linear Systems, SIAM, 2003.\n[35] Y. Saad and M. H. Schultz, GMRES: A generalized minimal residual algorithm for solving\nnonsymmetric linear systems, SIAM J. Sci. Statist. Comput., 7 (1986), pp. 856\u2013869.\n[36] G. L. G. Sleijpen and H. A. van der Vorst, A Jacobi\u2013Davidson iteration method for linear\neigenvalue problems, SIAM J. Matrix Anal. Appl., 17 (1996), pp. 401\u2013425.\n[37] D. J. Silvester and A. J. Wathen, Fast iterative solution of stabilised Stokes systems II:\nUsing general block preconditioners, SIAM J. Numer. Anal. 31 (1994), 1352\u20131367.\n[38] O. Tatebe, The multigrid preconditioned conjugate gradient method, in Sixth Copper Mountain\nConference on Multigrid Methods, pp. 621\u2013634, 1993.\n[39] U. Trottenberg, C. W. Oosterlee, and A. Sch\u00fcller, Multigrid, Academic Press, 2001.\n[40] M. B. van Gijzen, Y. A. Erlangga, and C. Vuik, Spectral analysis of the discrete Helmholtz\noperator preconditioned with a shifted Laplacian, SIAM J. Sci. Comput., 29 (2007), pp.\n1942\u20131958.\n[41] H. A. van der Vorst, Bi-CGSTAB: a fast and smoothly converging variant of Bi-CG for the\nsolution of nonsymmetric linear systems, SIAM J. Sci. Statist. Comput., 13 (1992), pp.\n\n\fABSOLUTE VALUE PRECONDITIONING\n\n25\n\n631\u2013644.\n[42] E. Vecharynski, Preconditioned Iterative Methods for Linear Systems, Eigenvalue and Singular Value Problems, PhD thesis, University of Colorado Denver, 2010.\n[43] E. Vecharynski and J. Langou, Any admissible cycle-convergence behavior is possible for\nrestarted GMRES at its initial cycles, Numer. Linear Algebra Appl., 18 (2011), pp. 499\u2013\n511.\n[44] A. J. Wathen and D. J. Silvester, Fast iterative solution of stabilised Stokes systems I: Using\nsimple diagonal preconditioners, SIAM J. Numer. Anal., 30 (1993), 630\u2013649.\n[45] Y. Zhou, Y. Saad, M. L. Tiago, and J. R. Chelikowsky, Self-consistent-field calculation\nusing Chebyshev-filtered subspace iteration, J. Comp. Phys., 219 (2006), pp. 172\u2013184.\n\n\f"}