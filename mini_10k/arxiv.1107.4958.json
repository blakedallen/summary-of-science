{"id": "http://arxiv.org/abs/1107.4958v1", "guidislink": true, "updated": "2011-07-25T14:20:34Z", "updated_parsed": [2011, 7, 25, 14, 20, 34, 0, 206, 0], "published": "2011-07-25T14:20:34Z", "published_parsed": [2011, 7, 25, 14, 20, 34, 0, 206, 0], "title": "Efficient and Accurate Gaussian Image Filtering Using Running Sums", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.0565%2C1107.0219%2C1107.5993%2C1107.5355%2C1107.3097%2C1107.0917%2C1107.2390%2C1107.4183%2C1107.3879%2C1107.5200%2C1107.3127%2C1107.5812%2C1107.0659%2C1107.3454%2C1107.1353%2C1107.0957%2C1107.0160%2C1107.4922%2C1107.2653%2C1107.1539%2C1107.0671%2C1107.4518%2C1107.2480%2C1107.4877%2C1107.3186%2C1107.0802%2C1107.5520%2C1107.2401%2C1107.2079%2C1107.1428%2C1107.4008%2C1107.0015%2C1107.1036%2C1107.2113%2C1107.5197%2C1107.4861%2C1107.2469%2C1107.4301%2C1107.4371%2C1107.3620%2C1107.5642%2C1107.2253%2C1107.2511%2C1107.0914%2C1107.1219%2C1107.4310%2C1107.4782%2C1107.5756%2C1107.1911%2C1107.0144%2C1107.5050%2C1107.1971%2C1107.5166%2C1107.4958%2C1107.1563%2C1107.3691%2C1107.1491%2C1107.1200%2C1107.0636%2C1107.1566%2C1107.1290%2C1107.3560%2C1107.3087%2C1107.2979%2C1107.4807%2C1107.4010%2C1107.2490%2C1107.4763%2C1107.5402%2C1107.1759%2C1107.2989%2C1107.3743%2C1107.2826%2C1107.5527%2C1107.0743%2C1107.5837%2C1107.2392%2C1107.3606%2C1107.5422%2C1107.5219%2C1107.0347%2C1107.4577%2C1107.4097%2C1107.1656%2C1107.5124%2C1107.1844%2C1107.2917%2C1107.5787%2C1107.0028%2C1107.5623%2C1107.4498%2C1107.1970%2C1107.2428%2C1107.0971%2C1107.2908%2C1107.0299%2C1107.1178%2C1107.0391%2C1107.4077%2C1107.2960%2C1107.2929&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Efficient and Accurate Gaussian Image Filtering Using Running Sums"}, "summary": "This paper presents a simple and efficient method to convolve an image with a\nGaussian kernel. The computation is performed in a constant number of\noperations per pixel using running sums along the image rows and columns. We\ninvestigate the error function used for kernel approximation and its relation\nto the properties of the input signal. Based on natural image statistics we\npropose a quadratic form kernel error function so that the output image l2\nerror is minimized. We apply the proposed approach to approximate the Gaussian\nkernel by linear combination of constant functions. This results in very\nefficient Gaussian filtering method. Our experiments show that the proposed\ntechnique is faster than state of the art methods while preserving a similar\naccuracy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.0565%2C1107.0219%2C1107.5993%2C1107.5355%2C1107.3097%2C1107.0917%2C1107.2390%2C1107.4183%2C1107.3879%2C1107.5200%2C1107.3127%2C1107.5812%2C1107.0659%2C1107.3454%2C1107.1353%2C1107.0957%2C1107.0160%2C1107.4922%2C1107.2653%2C1107.1539%2C1107.0671%2C1107.4518%2C1107.2480%2C1107.4877%2C1107.3186%2C1107.0802%2C1107.5520%2C1107.2401%2C1107.2079%2C1107.1428%2C1107.4008%2C1107.0015%2C1107.1036%2C1107.2113%2C1107.5197%2C1107.4861%2C1107.2469%2C1107.4301%2C1107.4371%2C1107.3620%2C1107.5642%2C1107.2253%2C1107.2511%2C1107.0914%2C1107.1219%2C1107.4310%2C1107.4782%2C1107.5756%2C1107.1911%2C1107.0144%2C1107.5050%2C1107.1971%2C1107.5166%2C1107.4958%2C1107.1563%2C1107.3691%2C1107.1491%2C1107.1200%2C1107.0636%2C1107.1566%2C1107.1290%2C1107.3560%2C1107.3087%2C1107.2979%2C1107.4807%2C1107.4010%2C1107.2490%2C1107.4763%2C1107.5402%2C1107.1759%2C1107.2989%2C1107.3743%2C1107.2826%2C1107.5527%2C1107.0743%2C1107.5837%2C1107.2392%2C1107.3606%2C1107.5422%2C1107.5219%2C1107.0347%2C1107.4577%2C1107.4097%2C1107.1656%2C1107.5124%2C1107.1844%2C1107.2917%2C1107.5787%2C1107.0028%2C1107.5623%2C1107.4498%2C1107.1970%2C1107.2428%2C1107.0971%2C1107.2908%2C1107.0299%2C1107.1178%2C1107.0391%2C1107.4077%2C1107.2960%2C1107.2929&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper presents a simple and efficient method to convolve an image with a\nGaussian kernel. The computation is performed in a constant number of\noperations per pixel using running sums along the image rows and columns. We\ninvestigate the error function used for kernel approximation and its relation\nto the properties of the input signal. Based on natural image statistics we\npropose a quadratic form kernel error function so that the output image l2\nerror is minimized. We apply the proposed approach to approximate the Gaussian\nkernel by linear combination of constant functions. This results in very\nefficient Gaussian filtering method. Our experiments show that the proposed\ntechnique is faster than state of the art methods while preserving a similar\naccuracy."}, "authors": ["Elhanan Elboher", "Michael Werman"], "author_detail": {"name": "Michael Werman"}, "author": "Michael Werman", "links": [{"href": "http://arxiv.org/abs/1107.4958v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.4958v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.4958v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1107.4958v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "1\n\nEfficient and Accurate Gaussian Image Filtering\nUsing Running Sums\n\narXiv:1107.4958v1 [cs.CV] 25 Jul 2011\n\nElhanan Elboher and Michael Werman\n\nAbstract-This paper presents a simple and efficient method\nto convolve an image with a Gaussian kernel. The computation\nis performed in a constant number of operations per pixel\nusing running sums along the image rows and columns. We\ninvestigate the error function used for kernel approximation\nand its relation to the properties of the input signal. Based on\nnatural image statistics we propose a quadratic form kernel error\nfunction so that the output image l2 error is minimized. We apply\nthe proposed approach to approximate the Gaussian kernel by\nlinear combination of constant functions. This results in very\nefficient Gaussian filtering method. Our experiments show that\nthe proposed technique is faster than state of the art methods\nwhile preserving a similar accuracy.\nIndex Terms-Non uniform filtering, Gaussian kernel, integral\nimages, natural image statistics.\n\nI. I NTRODUCTION\n\nI\n\nMAGE filtering is an ubiquitous image processing tool,\nwhich requires fast and efficient computation. When the\nkernel size increases, direct computation of the kernel response\nrequires more operations and the process becomes slow.\nVarious methods have been suggested for fast convolution\nwith specific kernels in linear time (see related work in\nSection II). An important kernel is the Gaussian, which is\nused in many applications.\nIn this paper we present an efficient filtering algorithm\nfor separable non uniform kernels and apply it for very fast\nand accurate Gaussian filtering. Our method is based on one\ndimensional running sums (integral images) along single rows\nand columns of the image. The proposed algorithm is very\nsimple and can be written in a few lines of code. Complexity\nanalysis, as well as experimental results show that it is faster\nthan state of the art methods for Gaussian convolution while\npreserving similar approximation accuracy.\nAn additional contribution of this paper is an analysis\nof the relation between the kernel approximation and the\napproximation of the final output, the filtered image. Usually,\nthe approximation quality is measured in terms of the difference between the kernel and its approximation. However,\nminimizing the kernel error does not necessarily minimize the\nerror on the resulting image. To minimize this error, natural\nimage statistics should be considered.\nIn Section IV we investigate the kernel approximation error\nfunction. Based on natural image statistics we find a quadratic\nform kernel error measurement which minimizes the l2 error\non the output pixel values.\nThe next section reviews related methods for fast non uniform image filtering. Section III presents the filtering algorithm\nE. Elboher and M. Werman are with the Hebrew University of Jerusalem,\nIsrael.\n\nand discusses its computational aspects. Section IV discusses\nthe relation between kernel approximation and natural image\nstatistics. Section V presents our experimental results. We\nconclude in Section VI.\nII. R ELATED W ORK\nIn the following we review the main approaches to accelerate image filtering. We describe in more detail the integral\nimage based approach on which the current work is based.\nGeneral and Multiscale Approaches. Convolving an image of N pixels with an arbitrary kernel of size K can be\ncomputed directly in O(N K) operations, or using the Fast\nFourier Transform in O(N log K) operations [1].\nA more efficient approach is the linear time multiscale\ncomputation using image pyramids [2], [3]. The coarser image\nlevels are filtered with small kernels and the results are interpolated into the finer levels. This approximates the convolution\nof the image with a large Gaussian kernel.\nRecursive Filtering. The recursive method is a very efficient filtering scheme for one dimensional (or separable)\nkernels. The infinite impulse response (IIR) of the desired\nkernel is expressed as a ratio between two polynomials in\nZ space [4]. Then the convolution with a given signal is\ncomputed by difference equations. Recursive algorithms were\nproposed for approximate filtering with Gaussian kernels [5],\n[6], [7], [8], [9], [10], anisotropic Gaussians [11] and Gabor\nkernels [12].\nThe methods of Deriche [5], [6], [7] and Young and van\nVliet [8], [9] are the current state of the art for fast approximate Gaussian filtering. Both methods perform two passes in\nopposite directions, in order to consider the kernel response\nboth of the forward and backward neighbors. The method of\nYoung and van Vliet's requires less arithmetic operations per\npixel. However, unlike Deriche's method the two passes cannot\nbe parallelized.\nTan et al. [13] evaluated the performance of both methods\nfor small standard deviations using normalized RMS error.\nWhile Deriche's impulse response is more accurate, Young\nand van Vliet performed slightly better on a random noise\nimage. No natural images were examined. Section V provides\nfurther evaluation of these methods.\nIntegral Image Based Methods. Incremental methods such\nas box filtering [14] and summed area tables, known also as\nintegral images [15], [16], cumulate the sum of pixel values\nalong the image rows and columns. In this way the sum of\na rectangular region can be computed using O(1) operations\nindependent of its size. This makes it possible to convolve an\nimage very fast with uniform kernels.\n\n\f2\n\nHeckbert [17] generalized integral images for polynomial\nkernels of degree d using d repeated integrations. Derpanis et\nal. [18] applied this scheme for a polynomial approximation\nof the Gaussian kernel. Werman [19] introduced another\ngeneralization for kernels which satisfy a linear homogeneous\nequation (LHE). This scheme requires d repeated integrations,\nwhere d is the LHE order.\nHussein et al. [20] proposed Kernel Integral Images (KII)\nfor non uniform filtering. The required kernel is expressed\nas a linear combination of simple functions. The convolution with each such functions is computed separately using\nintegral images. To demonstrate their approach, the authors\napproximated the Gaussian kernel by a linear combination of\npolynomial functions based on the Euler expansion. Similar\nfiltering schemes were suggested by Marimon [21] who used\na combination of linear functions to form pyramid shaped kernels and by Elboher and Werman [22] who used a combination\nof cosine functions to approximate the Gaussian and Gabor\nkernels and the bilateral filter [23].\nStacked Integral Images. The most relevant method to this\nwork is the Stacked Integral Images (SII) proposed by Bhatia\net al. [24]. The authors approximate non uniform kernels by a\n'stack' of box filters, i.e. constant 2D rectangles, which are all\ncomputed from a single integral image. The simplicity of the\nused function and not using multiple integral images makes\nthe filtering very efficient.\nThe authors of SII demonstrated their method for Gaussian smoothing. However, they approximated only specific\n2D kernels, and found for each of them a local minima of\na non-convex optimization problem. Although the resulting\napproximations can be rescaled, they are not very accurate (see\nSection V). Moreover, the SII framework does not exploit the\nseparability of the Gaussian kernel.\nAs shown in this paper, utilizing the separability property\nwe find an optimal kernel approximation which can be scaled\nto any standard deviation. As shown in Figure 2, this approximation is richer and more accurate. Actually, separable\nfiltering of the row and the columns by k one dimensional\nconstants is equivalent to filtering by 2k \u2212 1 two dimensional\nboxes. The separability property can also be used for an\nefficient computation both in time and space, as described in\nSection III.\nIII. A LGORITHM\nA. Piecewise Constant Kernel Decomposition\nConsider the convolution f \u2217K of a function f with a kernel\nK. For simplicity we first discuss the case in which f and\nK are one dimensional. In the following (Section III-D) we\ngeneralize the discussion to higher dimensions.\nSuppose that the support of the kernel K is r, i.e. that K is\nzero outside of [0, r]. Assume also that we are given a partition\nP = (p0 , p1 , ...pk ), in which 0 \u2264 p0 < p1 < p2 ... < pk\u22121 <\npk \u2264 r. Thus, the kernel K can be approximated by a linear\ncombination of k simple functions Ki , i = 1...k:\n\u001a\nci if pi\u22121 \u2264 t < pi\nKi (t) =\n(1)\n0 otherwise\n\nFig. 1. Approximation of the 1D Gaussian kernel on [\u2212\u03c0, \u03c0] by k = 4\npiecewise constant functions. The dashed lines show the equivalence between\n4 constant 'slices' and 7 constant 'segments' (Equation 4).\n\n(a) Gaussian kernel.\n\n(b) SII [24] approximation.\n\n(c) Proposed approximation (4 con- (d) Proposed approximation (5 constants).\nstants).\nFig. 2.\nComparison of (a) exact Gaussian kernel, (b) Stacked Integral\nImages [24] with 5 2D boxes, and the proposed method with 4 constants (c)\nand 5 constants (d). Our proposed approximation is richer and more accurate\nsince it utilizes the Gaussian separability. Instead of using 2D boxes, we use\n1D segments to filter the rows and then the columns.\n\nUsing the above approximation\nf \u2217K \u2248\n\nk\nX\n\nf \u2217 Ki\n\n(2)\n\ni=1\n\nwhich can be computed very efficiently, as described in\nSection III-C.\nB. Symmetric Kernels\nConsider the case in which K is a symmetric kernel on\n[\u2212r, r]. The approximation can be limited to the range [0, r].\n\n\f3\n\nMethod\nDirect\nFFT\nKII [20]\nCII [22]\nSII [24]\nDeriche [7]\nYoung and\nvan Vliet [8]\nProposed method\n\nAdditions\nh+w\u22122\nO(log(hw))\n53\n12k \u2212 8\n4k + 1\n8k \u2212 2\n4k\n\nMultiplications\nh+w\nO(log(hw))\n18\n8k \u2212 8\nk\n8k\n4k + 4\n\n4k\n\n2k\n\nD. Higher Dimensions and More Computational Aspects\n\nTABLE I\nA RITHMETIC OPERATIONS PER IMAGE PIXEL (S ECTION III-D).\n\nEach constant ci can be used both in the negative interval\n[\u2212pi , \u2212pi\u22121 ] and in the positive one [pi\u22121 , pi ], however, this\nrequires 2k constant function. Actually, the same approximation can be computed using 'weighted slices':\n\u001a\nSi (t) =\n\nif \u2212pi < t < pi\notherwise\n\nwi\n0\n\n(3)\n\nwhere wi = ci \u2212 ci\u22121 . Now the kernel K is approximated\nby sum of k constant functions which are non zero in the\noverlapping intervals [\u2212pi , pi ], with weights wi = ci \u2212 ci\u22121 .\nThe approximation of K remains the same:\nk\nX\n\nSi (t)\n\nX\n\n=\n\ni=1\n\nwi\n\ni:\u2212pi <t<pi\n\nX\n\n=\n\nX\n\n=\n\n(ci \u2212 ci\u22121 )\n\ni:\u2212pi <t<pi\n\nci\n\nk\nX\n\n=\n\nKi (t)\n\ni=1\n\ni:pi\u22121 \u2264|t|<pi\n\n(4)\nThis equality is illustrated in Figure 1.\n\nC. 1D Filtering Algorithm\nIn the following we describe the algorithm for the case of\na symmetric kernel K (Section III-B).\nGiven a 1D discrete function f (x) and k piecewise constant functions Si , we compute the approximated convolution\n(Equation 2) as follows:\n\nWe now describe the case of convolving a 2D image f (x, y)\nwith a separable 2D kernel K. A kernel K is separable if it\ncan be expressed as a convolution of 1D filters Kx \u2217 KyT . The\nconvolution f \u2217 K can be computed by first convolving the\nimage rows with Kx and then the columns of the intermediate\nresult by Ky . Hence, filtering an image with a separable 2D\nkernel only doubles the computational cost of the 1D case.\nThe space complexity is also very low. Since the convolution\nof Kx with each row (and also Ky with each column) is\nindependent, filtering an image of size n \u00d7 m requires only\nO(max(n, m)) additional space over the input and the output\nimages for storing the cumulative sum of a single row or\ncolumn.\nSimilarly to the 2D case, the filtering scheme can be extended to d-dimensional separable kernels using 2dk additions\nand dk multiplications per single pixel. The additional required\nspace is O(maxi (ni )), where ni (i = 1...d) are the sizes of\nthe signal dimensions.\nTable I counts the required operations per image pixel for\n2D image filtering by a h\u00d7w separable kernel. The parameter\nk denotes the number of terms (constants, 2D boxes etc.)\ndepending on the specific method. Notice that experimentally\nour proposed method with 3 constants is more accurate than\nSII [24] with 5 boxes and has an accuracy similar to the\nrecursive methods with 3 coefficients (Section V, Figure 3(b)).\nThis means that our proposed method requires less operations\nthan Deriche [7] and all the integral image based methods,\nand a comparable number of operations to Young and van\nVliet [8].\nThe proposed method is also convenient for parallel computation. The summation step (Equation 5) can be parallelized\nas described in Section 4.3 of [25], while the next step (Equation 6) computes the response of each pixel independently of\nits neighbors. On the other hand, the recursive methods can\nbe parallelized only partially \u2013 e.g. by filtering different rows\nindependently. However, within each row (or column) all the\ncomputations are strongly dependent.\nNotice also that Equation 6 can be computed for a small\npercentage of the image pixels. This can accelerate applications in which the kernel response is computed for sampled\nwindows such as image downscaling. The recursive methods\ndo not have this advantage, since they compute the kernel\nresponse of each pixel using the responses of its neighbors.\n\n1) Compute the cumulative sum of f (x),\nI(x) =\n\nx\nX\n\nIV. K ERNEL A PPROXIMATION\nf (x0 )\n\n(5)\n\nx0 =0\n\n2) Compute the convolution result for each pixel x,\nk\nX\n\nwi (I(x + pi ) \u2212 I(x \u2212 pi \u2212 1))\n\n(6)\n\ni=1\n\nThe total cost of steps 1 and 2 is 2k additions and k\nmultiplications per image pixel. In the 1D case memory access\nis not an issue, as all the elements are usually in the cache.\n\nThe approximation of a one dimensional kernel K(t) by\nk constant functions is determined by the partition P and\nthe constants ci . Finding the best parameters is done by\nminimizing an approximation error function on the desired\nkernel. Indeed, our real purpose is to minimize the error\non the output image, which is not necessarily equivalent.\nSection IV-A relates the kernel approximation error and the\noutput image error using natural image statistics. We define a\nquadratic form error function for the kernel approximation, so\nthat the output image l2 error is minimized. This error function\nis used in Section IV-B to approximate the Gaussian kernel.\n\n\f4\n\nA. Minimal Output l2 Error\nWe denote the input signal by x, the output signal by y, and\nthe kernel weights by w. The approximated output and kernel\nweights are denoted by \u0177 and \u0175 respectively. The upper case\nletter X denotes the Fourier transforms of the input x.\nThe squared l2 error of the final result is given by\nX\nE2 =\n(yi \u2212 \u0177i )2\n(7)\ni\n\nP\n\nSince yi = j wj xi+j , E2 can be expressed in terms of the\ninput signal x and the kernels w, \u0175:\nE2\n\n=\n\nX\u0010X\ni\n\n=\n\nX\n\nwj xi+j \u2212\n\nj\n\nX\n\n\u0175j xi+j\n\n\u00112\n\nj\n\n(wj \u2212 \u0175j )(wk \u2212 \u0175k )xi+j xi+k\n\n(8)\n\ni,j,k\n\n=\n\nX\u0010\n\n(wj \u2212 \u0175j )(wk \u2212 \u0175k )\n\nX\n\nxi+j xi+k\n\n\u0011\n\ni\n\nj,k\n\nThe only term which involves thePinput signal x is the inner\nsum, which we denote as Ajk = i xi+j xi+k .\nNote that Ajk is the the autocorrelation of x at location\nj\u2212k. The error can be therefore expressed as\n\nform (Equation 9) where Ajk = \u03a6j\u2212k results in a very low\nl2 error (or high PSNR) on the output image. Modifying \u03a6\nvalues slightly decreases the accuracy.\nB. Gaussian Approximation\nThe Gaussian kernel is zero mean and its only parameter\nis standard deviation \u03c3. Actually, all the Gaussian kernels are\n2\nnormalized and scaled versions of the standard kernel exp( t2 ).\nTherefore the approximation need to be computed only once\nfor some \u03c30 . For other \u03c3 values the pre-computed solution of\nN (t) is simply rescaled.\nWe approximate the Gaussian within the range [\u2212\u03c0\u03c3, \u03c0\u03c3],\nsince for greater distances from the origin kernel values are\nnegligible. The optimization is done on the positive part\n[0, \u03c0\u03c3]. Then the kernel symmetry is used to getcompute the\nweights wi as described in Section III-B.\nThe partition indices pi and constants ci were found by an\nexhaustive search using 100 samples of the Gaussian kernel\nwith \u03c30 = 100\n\u03c0 . To get the parameters for other \u03c3 values, we\nscale the indices and round them:\n\u0016\n\u0017\n\u03c3\n(\u03c3)\npi =\n* pi\n(12)\n\u03c30\nThe weights are also scaled,\n(\u03c3)\n\nE2 = (w \u2212 \u0175)T A(w \u2212 \u0175)\n\nwhere A's entries are given by the autocorrelation of the signal\nx.\nIn order to make E2 independent of the values of a specific\nx, we make use of a fundamental property of natural images\nintroduced by Field [26] the Fourier spectrum of a natural\nimage in each frequency u is proportional to u1 ,\n1\n|Xu | \u221d\n(10)\nu\nApplying the Convolution Theorem, the autocorrelation of x\nis\n1\n|Xu | \u221d 2\n(11)\nu\nHence, Ajk should be proportional to \u03a6, the inverse Fourier\ntransform of u12 .\nOf course, the definition of \u03a6 is problematic since u12 is\nnot defined for u = 0. Completing the missing value by\nan arbitrary number affects all \u03a6 values. However, additional\ninformation is available which helps to complete the missing\nvalue.\nAssuming that the values of x are uniformly distributed in\n[0, 1], we find that \u03a60 (which equals Rto Ajj , the correlation of a\n1\npixel with itself) is proportional to 0 x2 dx = 13 . In addition,\nassuming that distant pixels are uncorrelated, the boundary\nvalues \u03a6r , \u03a6\u2212r are proportional to \u03bc2x = 14 . This means that\n\u03a6\nthe ratio \u03a60 should be 43 . Determining the missing zero value\nr\nin the Fourier domain as 16.5 results in such a function \u03a6.\nAs shown by our experiments (Section V, Figure 3(b)), approximating the Gaussian kernel using the proposed quadratic\n2\n\nwi\n\n(9)\n\n=\n\npi\n(\u03c3)\n2pi +\n\n1\n\n* wi\n\n(13)\n\nThe parameters for different numbers of constants are presented in Table II. Figure 1 shows the optimal approximation\nwith 4 constant functions. Figure 2(c,d) present the 2D result\nof filtering image rows and columns using the proposed\napproximation.\n#constants (k)\nk=3\nk=4\nk=5\n\npartition indices (pi )\n23, 46, 76\n19, 37, 56, 82\n16, 30, 44, 61, 85\n\nweights (ci )\n0.9495, 0.5502, 0.1618\n0.9649, 0.6700, 0.3376, 0.0976\n0.9738, 0.7596, 0.5031,\n0.2534, 0.0739\n\nTABLE II\nA LGORITHM PARAMETERS .\n\nV. E XPERIMENTAL R ESULTS\nIn order to evaluate the performance Gaussian filtering\nmethods we made an experiment on natural images from\ndifferent scenes. We used a collection of 20 high resolution\nimages (at least 1 megapixel) taken from Flickr under creative\ncommons. Each of the image was filtered with several standard\ndeviation (\u03c3) values. Speed and accuracy are measured in\ncomparison to the exact Gaussian filtering performed by the\n'cvSmooth' function of the OpenCV library [27]. The output\nimage error is measured by the peak signal-to-noise ratio\n(PSNR). This score is defined as \u221210log10 (M SE), M SE\nis the mean squared error and the image values are in [0, 1].\nWe examined our proposed method as well as the state of\nthe art methods of Deriche [7] and Young and van Vliet [8].\n\n\f5\n\n(a) Speedup\n\n(b) Accuracy\nFig. 3.\nExperimental results (Section V). (a) Time speedup is in comparison to exact filtering. Our proposed method with k = 3 is the fastest. Using\nk = 5 is similar to Young and van Vliet, while Deriche and SII are slower. (b) Accuracy of the output image: the highest PSNR (minimal l2 ) is achieved by\nour proposed quadratic form approximation with k = 4 or k = 5 which are slightly better than Deriche. Using k = 3 is still better than all other methods\nincluding Young and van Vliet. Approximating the Gaussian kernel by l2 instead of the proposed quadratic form decreases the approximation accuracy. Kernel\nIntegral Images (KII) gives poor results for small \u03c3 values since integral images of polynomials are numerically unstable for high ratio between \u03c3 and the\nimage size.\n\nWe also examined the integral image based methods of Hussein et al. (KII, [20]), Elboher and Werman (CII, [22]) and\nBhatia et al. (SII, [24]), which use a similar approach to the\nproposed method (Section II). All the compared methods were\nimplemented by us1 except Young and van Vliet's filtering,\nfor which we adopted the code of [11]. This implementation\nis based on the updated design of Young and van Vliet in [12]\nwith corrected boundary conditions [28].\nIn order to examine the effect of using different error func1 The CImg library contains an implemetation of Deriche's method for\nk = 2 coefficients. Due to caching considerations, our implementation is\nabout twice as fast. For the integral image based methods there is no public\nimplementation except CII [22] which was proposed by the authors of the\ncurrent paper.\n\ntions for kernel approximation, we tested our proposed method\nwith two sets of parameters. The first set was computed\nby minimizing the quadratic form error function defined in\nSection IV-A. The second set was computed by minimizing\nthe l2 error. Since the filtering technique is identical the speed\nis the same, the difference is in the output accuracy.\nFigure 3 presents the average results of our experiments.\nThe highest acceleration is achieved by our proposed method\nwith k = 3 constants (Figure 3(a)). Using k = 4 is still faster\nthan all other methods, while k = 5 has equivalent speedup\nas Young and van Vliet's method.\nThe best accuracy is achieved by our quadratic form approximation with k = 4, 5, which is slightly better than Deriche's\nmethod (Figure 3(b)). Using l2 kernel approximation decreases\n\n\f6\n\nthe quality of our proposed method. This demonstrates the\nimportance of using an appropriate kernel approximation.\nHowever, these results are still similar to Young and van Vliet\nand better than other integral image based methods.\nVI. C ONCLUSION\nWe present a very efficient and simple scheme for filtering\nwith separable non uniform kernels. In addition we analyze\nthe relation between kernel approximation, output error and\nnatural image statistics. Computing an appropriate Gaussian\napproximation by the proposed filtering scheme is faster than\nthe current state of art methods, while preserving similar\naccuracy.\nR EFERENCES\n[1] R.C. Gonzalez and R.E. Woods, Digital image processing, Addison\nWisley, 1992.\n[2] P.J. Burt, \"Fast filter transform for image processing,\" Computer\ngraphics and image processing, vol. 16, no. 1, pp. 20\u201351, 1981.\n[3] P. Burt and E. Adelson, \"The laplacian pyramid as a compact image\ncode,\" Communications, IEEE Transactions on, vol. 31, no. 4, pp. 532\u2013\n540, 1983.\n[4] J.G. Proakis and D.G. Manolakis, Digital signal processing: principles,\nalgorithms, and applications, Engelwoods Cliffs, NJ: Prentice Hall,\n1996.\n[5] R. Deriche, \"Using canny's criteria to derive a recursively implemented\noptimal edge detector,\" International journal of computer vision, vol. 1,\nno. 2, pp. 167\u2013187, 1987.\n[6] R. Deriche, \"Fast algorithms for low-level vision,\" IEEE Transactions\non Pattern Analysis and Machine Intelligence, pp. 78\u201387, 1990.\n[7] R. Deriche, \"Recursively implementating the gaussian and its derivatives,\" Research Report 1893,INRIA, France, 1993.\n[8] I.T. Young and L.J. van Vliet, \"Recursive implementation of the gaussian\nfilter,\" Signal processing, vol. 44, no. 2, pp. 139\u2013151, 1995.\n[9] L.J. Van Vliet, I.T. Young, and P.W. Verbeek, \"Recursive gaussian\nderivative filters,\" in Pattern Recognition, 1998. Proceedings. Fourteenth\nInternational Conference on. IEEE, 1998, vol. 1, pp. 509\u2013514.\n[10] J.S. Jin and Y. Gao, \"Recursive implementation of log filtering,\" realtime imaging, vol. 3, no. 1, pp. 59\u201365, 1997.\n[11] J.M. Geusebroek, A.W.M. Smeulders, and J. van de Weijer, \"Fast\nanisotropic gauss filtering,\" Image Processing, IEEE Transactions on,\nvol. 12, no. 8, pp. 938\u2013943, 2003.\n[12] I.T. Young, L.J. van Vliet, and M. van Ginkel, \"Recursive gabor\nfiltering,\" Signal Processing, IEEE Transactions on, vol. 50, no. 11,\npp. 2798\u20132805, 2002.\n[13] S. Tan, J.L. Dale, and A. Johnston, \"Performance of three recursive\nalgorithms for fast space-variant gaussian filtering,\" Real-Time Imaging,\nvol. 9, no. 3, pp. 215\u2013228, 2003.\n[14] MJ McDonnell, \"Box-filtering techniques,\" Computer Graphics and\nImage Processing, vol. 17, no. 1, pp. 65\u201370, 1981.\n[15] F.C. Crow, \"Summed-area tables for texture mapping,\" in Proceedings\nof the 11th annual conference on Computer graphics and interactive\ntechniques. ACM, 1984, pp. 207\u2013212.\n[16] P. Viola and M. Jones, \"Rapid object detection using a boosted cascade\nof simple features,\" in IEEE Computer Society Conference on Computer\nVision and Pattern Recognition. Citeseer, 2001, vol. 1.\n[17] P.S. Heckbert, \"Filtering by repeated integration,\" ACM SIGGRAPH\nComputer Graphics, vol. 20, no. 4, pp. 315\u2013321, 1986.\n[18] K.G. Derpanis, E.T.H. Leung, and M. Sizintsev, \"Fast scale-space feature\nrepresentations by generalized integral images,\" in Image Processing,\n2007. ICIP 2007. IEEE International Conference on. IEEE, 2007, vol. 4,\npp. IV\u2013521.\n[19] M. Werman, \"Fast convolution,\" J. WSCG, vol. 11, no. 1, pp. 528\u2013529,\n2003.\n[20] M. Hussein, F. Porikli, and L. Davis, \"Kernel integral images: A\nframework for fast non-uniform filtering,\" in Computer Vision and\nPattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE,\n2008, pp. 1\u20138.\n[21] D. Marimon, \"Fast non-uniform filtering with Symmetric Weighted Integral Images,\" in Image Processing (ICIP), 2010 17th IEEE International\nConference on. IEEE, 2010, pp. 3305\u20133308.\n\n[22] E. Elboher and M. Werman, \"Cosine integral images for fast spatial and\nrange filtering,\" in IEEE International Conference on Image Processing,\n2011. IEEE, 2011, to apper.\n[23] C. Tomasi and R. Manduchi, \"Bilateral filtering for gray and color\nimages,\" in Computer Vision, 1998. Sixth International Conference on.\nIEEE, 2002, pp. 839\u2013846.\n[24] A. Bhatia, W.E. Snyder, and G. Bilbro, \"Stacked Integral Image,\" in\nRobotics and Automation (ICRA), 2010 IEEE International Conference\non. IEEE, 2010, pp. 1530\u20131535.\n[25] G. Hendeby, J.D. Hol, R. Karlsson, and F. Gustafsson, \"A graphics\nprocessing unit implementation of the particle filter,\" Measurement, vol.\n1, no. 1, pp. x0, 2007.\n[26] D.J. Field et al., \"Relations between the statistics of natural images and\nthe response properties of cortical cells,\" J. Opt. Soc. Am. A, vol. 4, no.\n12, pp. 2379\u20132394, 1987.\n[27] OpenCV v2.1 documentation, \"cvsmooth,\" http://opencv.willowgarage.\ncom/documentation/c/image filtering.html.\n[28] B. Triggs and M. Sdika, \"Boundary conditions for young-van vliet\nrecursive filtering,\" Signal Processing, IEEE Transactions on, vol. 54,\nno. 6, pp. 2365\u20132367, 2006.\n\n\f"}