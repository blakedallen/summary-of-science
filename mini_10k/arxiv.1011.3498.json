{"id": "http://arxiv.org/abs/1011.3498v1", "guidislink": true, "updated": "2010-11-15T20:32:10Z", "updated_parsed": [2010, 11, 15, 20, 32, 10, 0, 319, 0], "published": "2010-11-15T20:32:10Z", "published_parsed": [2010, 11, 15, 20, 32, 10, 0, 319, 0], "title": "Effects of the Generation Size and Overlap on Throughput and Complexity\n  in Randomized Linear Network Coding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.0959%2C1011.6221%2C1011.1123%2C1011.1236%2C1011.1512%2C1011.5018%2C1011.2617%2C1011.4790%2C1011.2410%2C1011.1335%2C1011.5266%2C1011.1696%2C1011.5464%2C1011.5031%2C1011.6539%2C1011.0271%2C1011.3329%2C1011.5427%2C1011.3498%2C1011.0480%2C1011.6610%2C1011.6043%2C1011.4581%2C1011.2344%2C1011.3453%2C1011.6098%2C1011.1453%2C1011.2537%2C1011.4514%2C1011.2772%2C1011.4259%2C1011.3531%2C1011.5786%2C1011.2813%2C1011.3281%2C1011.1222%2C1011.5370%2C1011.1108%2C1011.1025%2C1011.2216%2C1011.0432%2C1011.0603%2C1011.0159%2C1011.1178%2C1011.1047%2C1011.1816%2C1011.1325%2C1011.4631%2C1011.3848%2C1011.5805%2C1011.2267%2C1011.4788%2C1011.4822%2C1011.3472%2C1011.1469%2C1011.5050%2C1011.6346%2C1011.4205%2C1011.3005%2C1011.2797%2C1011.4998%2C1011.5079%2C1011.5966%2C1011.5418%2C1011.5878%2C1011.4972%2C1011.2437%2C1011.1940%2C1011.3682%2C1011.6134%2C1011.4966%2C1011.6619%2C1011.4186%2C1011.5525%2C1011.6062%2C1011.1827%2C1011.2325%2C1011.6118%2C1011.3144%2C1011.0526%2C1011.2236%2C1011.3841%2C1011.3856%2C1011.1637%2C1011.6207%2C1011.4584%2C1011.2037%2C1011.6288%2C1011.3157%2C1011.1447%2C1011.4001%2C1011.5639%2C1011.2599%2C1011.1776%2C1011.4716%2C1011.3610%2C1011.4914%2C1011.0444%2C1011.0125%2C1011.2343%2C1011.1612&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Effects of the Generation Size and Overlap on Throughput and Complexity\n  in Randomized Linear Network Coding"}, "summary": "To reduce computational complexity and delay in randomized network coded\ncontent distribution, and for some other practical reasons, coding is not\nperformed simultaneously over all content blocks, but over much smaller,\npossibly overlapping subsets of these blocks, known as generations. A penalty\nof this strategy is throughput reduction. To analyze the throughput loss, we\nmodel coding over generations with random generation scheduling as a coupon\ncollector's brotherhood problem. This model enables us to derive the expected\nnumber of coded packets needed for successful decoding of the entire content as\nwell as the probability of decoding failure (the latter only when generations\ndo not overlap) and further, to quantify the tradeoff between computational\ncomplexity and throughput. Interestingly, with a moderate increase in the\ngeneration size, throughput quickly approaches link capacity. Overlaps between\ngenerations can further improve throughput substantially for relatively small\ngeneration sizes.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.0959%2C1011.6221%2C1011.1123%2C1011.1236%2C1011.1512%2C1011.5018%2C1011.2617%2C1011.4790%2C1011.2410%2C1011.1335%2C1011.5266%2C1011.1696%2C1011.5464%2C1011.5031%2C1011.6539%2C1011.0271%2C1011.3329%2C1011.5427%2C1011.3498%2C1011.0480%2C1011.6610%2C1011.6043%2C1011.4581%2C1011.2344%2C1011.3453%2C1011.6098%2C1011.1453%2C1011.2537%2C1011.4514%2C1011.2772%2C1011.4259%2C1011.3531%2C1011.5786%2C1011.2813%2C1011.3281%2C1011.1222%2C1011.5370%2C1011.1108%2C1011.1025%2C1011.2216%2C1011.0432%2C1011.0603%2C1011.0159%2C1011.1178%2C1011.1047%2C1011.1816%2C1011.1325%2C1011.4631%2C1011.3848%2C1011.5805%2C1011.2267%2C1011.4788%2C1011.4822%2C1011.3472%2C1011.1469%2C1011.5050%2C1011.6346%2C1011.4205%2C1011.3005%2C1011.2797%2C1011.4998%2C1011.5079%2C1011.5966%2C1011.5418%2C1011.5878%2C1011.4972%2C1011.2437%2C1011.1940%2C1011.3682%2C1011.6134%2C1011.4966%2C1011.6619%2C1011.4186%2C1011.5525%2C1011.6062%2C1011.1827%2C1011.2325%2C1011.6118%2C1011.3144%2C1011.0526%2C1011.2236%2C1011.3841%2C1011.3856%2C1011.1637%2C1011.6207%2C1011.4584%2C1011.2037%2C1011.6288%2C1011.3157%2C1011.1447%2C1011.4001%2C1011.5639%2C1011.2599%2C1011.1776%2C1011.4716%2C1011.3610%2C1011.4914%2C1011.0444%2C1011.0125%2C1011.2343%2C1011.1612&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "To reduce computational complexity and delay in randomized network coded\ncontent distribution, and for some other practical reasons, coding is not\nperformed simultaneously over all content blocks, but over much smaller,\npossibly overlapping subsets of these blocks, known as generations. A penalty\nof this strategy is throughput reduction. To analyze the throughput loss, we\nmodel coding over generations with random generation scheduling as a coupon\ncollector's brotherhood problem. This model enables us to derive the expected\nnumber of coded packets needed for successful decoding of the entire content as\nwell as the probability of decoding failure (the latter only when generations\ndo not overlap) and further, to quantify the tradeoff between computational\ncomplexity and throughput. Interestingly, with a moderate increase in the\ngeneration size, throughput quickly approaches link capacity. Overlaps between\ngenerations can further improve throughput substantially for relatively small\ngeneration sizes."}, "authors": ["Yao Li", "Emina Soljanin", "Predrag Spasojevic"], "author_detail": {"name": "Predrag Spasojevic"}, "author": "Predrag Spasojevic", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TIT.2010.2095111", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1011.3498v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.3498v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "To appear in IEEE Transactions on Information Theory Special Issue:\n  Facets of Coding Theory: From Algorithms to Networks, Feb 2011", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.3498v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.3498v1", "journal_reference": null, "doi": "10.1109/TIT.2010.2095111", "fulltext": "1\n\nEffects of the Generation Size and Overlap\non Throughput and Complexity\nin Randomized Linear Network Coding\n\narXiv:1011.3498v1 [cs.IT] 15 Nov 2010\n\nYao Li, Student Member, IEEE, Emina Soljanin, Senior Member, IEEE, and Predrag Spasojev\u0107, Member, IEEE,\n\nAbstract-To reduce computational complexity and delay in\nrandomized network coded content distribution, and for some\nother practical reasons, coding is not performed simultaneously\nover all content blocks, but over much smaller, possibly overlapping subsets of these blocks, known as generations. A penalty of\nthis strategy is throughput reduction. To analyze the throughput\nloss, we model coding over generations with random generation\nscheduling as a coupon collector's brotherhood problem. This\nmodel enables us to derive the expected number of coded packets\nneeded for successful decoding of the entire content as well\nas the probability of decoding failure (the latter only when\ngenerations do not overlap) and further, to quantify the tradeoff\nbetween computational complexity and throughput. Interestingly,\nwith a moderate increase in the generation size, throughput\nquickly approaches link capacity. Overlaps between generations\ncan further improve throughput substantially for relatively small\ngeneration sizes.\nIndex Terms-network coding, rateless codes, coupon collector's problem\n\nI. I NTRODUCTION\nA. Motivation: Coding over Disjoint and Overlapping Generations\nRandom linear network coding was proposed in [1] for\n\"robust, distributed transmission and compression of information in networks\". Subsequently, the idea found a place\nin a peer-to-peer(P2P) file distribution system Avalanche [2]\nfrom Microsoft. In P2P systems such as BitTorrent, content\ndistribution involves fragmenting the content at its source,\nand using swarming techniques to disseminate the fragments\namong peers. Systems such as Avalanche, instead, circulate\nlinear combinations of content fragments, which can be generated by any peer. The motivation behind such a scheme is\nthat, it is hard for peers to make optimal decisions on the\nscheduling of fragments based on their limited local vision,\nwhereas when fragments are linearly combined at each node,\ntopology diversity is implanted inherently in the data flows\nand can be exploited without further co-ordination.\nManuscript received April 15, 2010; revised August 14, 2010 and November 5, 2010. The material in this work was presented in part at the IEEE\nInternational Symposium on Information Theory (ISIT'10), Austin, Texas,\nJune 2010, and the IEEE International Symposium on Network Coding\n(NetCod'10), Toronto, Canada, June 2010. This work was in part supported\nby NSF CNS Grant No. 0721888.\nY. Li and P. Spasojevi\u0107 are with WINLAB, the Department of Electrical\nand Computer Engineering, Rutgers University, North Brunswick, NJ 08902\nUSA emails: {yaoli,spasojev}@winlab.rutgers.edu.\nE. Soljanin is with the Mathematics of Networking and Communication\nDepartment, Enabling Computing Technologies, Bell Laboratories, AlcatelLucent, Murray Hill, NJ 07974, email: emina@alcatel-lucent.com.\n\nThe introduction of network coding in P2P content distribution systems brings about the issue of computational complexity. Consider distributing a file consisting of N fragments,\neach made up of d symbols from a Galois field GF (q) of\nsize q. It takes O(N d) operations in GF (q) to form a linear\ncombination per coded packet, and O(N 3 + N 2 d) operations,\nor, equivalently, O(N 2 + N d) operations per information\npacket, to decode the information packets by solving linear\nequations. According to the implementers of UUSee [3], a\npeer-to-peer video streaming application using randomized\nlinear coding, even with the most optimized implementation,\ngoing beyond 512 fragments in each generation risks taxing\na low-end CPU, typically used in power-efficient notebook\ncomputers.\nIn an effort to reduce computational complexity, information\npackets are partitioned into disjoint subsets referred to as\ngenerations, and coding is done only within generations. This\napproach scales down the encoding and decoding problem\nfrom the whole file size N to the generation size times the\nnumber of generations. The concept of generation in network\ncoding was first proposed by Chou et al. in [4] to handle\nthe issue of network synchronization. Coding over randomly\nscheduled generations was first theoretically analyzed by Maymounkov et al. in [5]. Random scheduling of generations\nprovides the \"rateless\" property which reduces the need for\nreceiver feedback and offers resilience to various erasure\npatterns over the communication link. In addition, in the peerto-peer content distribution setting, random scheduling is to\nsome degree a good approximation when global co-ordination\namong peers is impractical.\nWith random scheduling of generations, coded packets accumulate faster in some generations than in others, even if all\ngenerations are scheduled equally probably. While waiting for\nthe last generation to become decodable, redundant packets are\naccumulated in other generations. The situation is aggravated\nas the generation size decreases. One way to recover some\nof the throughput loss due to random scheduling without\nlosing the benefits of reduced generation sizes is to allow\ngenerations to help each other in decoding. If the generations\nare allowed to overlap, after some of the \"faster\" generations\nare decoded, the number of unknown variables can be reduced\nin those generations sharing information packets with the\ndecoded ones, which in turn reduces the number of coded\npackets needed to decode those generations, and enhances the\nthroughput as a result. Our goal is to characterize the effects of\ngeneration size and overlaps on the throughput and complexity\n\n\f2\n\nin randomized linear network coding.\nB. Related Work\nThe performance of codes with random scheduling of\ndisjoint generations was first theoretically analyzed in [5] by\nMaymounkov et al., who referred to them as chunked codes.\nChunked codes allow convenient encoding at intermediate\nnodes, and are readily suitable for peer-to-peer file dissemination. In [5], the authors used an adversarial schedule as\nthe network model and characterized the code performance\nunder certain restrictions on the chunk(generation) size when\nthe length of the information to be encoded tends to infinity.\nCoding with overlapping generations was first studied in\n[6] and [7] with the goal to improve throughput. Reference\n[7] studied a \"head-to-toe\" overlapping scheme in which\nonly contiguous generations overlap for a given number of\ninformation packets, and analyzed its asymptotic performance\nover a line network when the length of information goes to\ninfinity. Another overlapping scheme with a grid structure was\nproposed in [6], analyzed for short lengths (e.g., 4 generations)\nand simulated for practical lengths. When properly designed,\nthese codes show improved performance over codes with\ndisjoint generations. In our work, we offer an analysis of\ncoding over disjoint and overlapping generations for finite but\npractically long information lengths.\nC. Organization and Main Contribution\nIn this work, coding with both disjoint and overlapping\ngenerations together with random generation scheduling is\nstudied from a coupon collection [8] perspective. Previously\nexisting results from the classical coupon collector's problem,\nalong with our extensions, enable us to characterize the code\nperformance with finite information lengths, from which the\nasymptotic code performance can further be deduced.\nSection II introduces the general model for coding over\ngenerations, disjoint or overlapping, over a unicast (binary\nerasure) link, and characterizes the computational cost for\nencoding and decoding.\nSection III derives several results concerning linear independence among coded packets from the same generation.\nSuch results serve to link coupon collection to the decoding\nof content that has been encoded into multiple generations.\nIncluded (Claim 1) is a very good upper bound on the\ndistribution of the number of coded packets needed for a\nspecific generation for successful decoding.\nSection IV introduces the coupon collector's brotherhood\nproblem and its variations that can be used to model coding\nover generations. Probability generating functions (Theorems\n2 and 4) and moments (Corollaries 3 and 5) of the number of\nsamplings needed to collect multiple copies of distinct coupons\nare derived for the random sampling of a finite set of coupons\nin Section IV-A. Relevant asymptotic results on expected\nvalues and probability distributions in existing literature are\nrecapitulated in Section IV-B for performance characterization\nof coding over generations in the later part of the work. The\nsection is presented in the coupon collection language and is\n\nin itself of independent interest for general readers interested\nin coupon collecting problems.\nIn Sections V and VI, results from the previous two sections\nare combined to enable the analysis of the effects of generation\nsize and overlaps on the decoding latency/throughput of coding\nover disjoint or overlapping generations.\nSection V studies the effects of generation size on the code\nthroughput over a BEC channel for coding over disjoint generations. Section V-A characterizes the mean and variance of\nthe decoding latency (the number of coded packets transmitted\nuntil successful decoding) for finite information lengths, and\nSection V-B provides a lower bound for the probability of\ndecoding failure. A large gain in throughput is observed when\nthe generation size increases from 1 to a few tens.\nIn Section VI, the random annex code is proposed as\nan effort to improve code throughput by allowing random\noverlaps among generations. Section VI-C lists an algorithm\nproviding precise estimation of the expected decoding latency\nof the random annex code. The algorithm is based on the\nanalysis of the overlapping structure in Section VI-B and the\nresults from the extended collector's brotherhood in Section\nIV. Section VI-D demonstrates the effects of overlap sizes\non code throughput is shown through both numerical computation and simulations. One of our interesting observations\nis that overlaps between generations can provide a tradeoff\nbetween computational complexity and decoding latency. In\naddition, without increasing the generation size (and hence\ncomputational complexity), it is still possible to improve\ncode throughput significantly by allowing overlaps between\ngenerations.\nII. C ODING OVER G ENERATIONS : T HE G ENERAL M ODEL\nIn this section, we describe a general random coding scheme\nover generations. Generations do not have to be disjoint or of\nequal size, and random scheduling of generations does not\nhave to be uniform. We describe the coding scheme over a\nunicast link.\nA. Forming Generations\nThe file being distributed F is represented as a set of N\ninformation packets, p1 , p2 , . . . , pN . Each information packet\nis a d-dimensional column vector of information symbols in\nGalois Field GF (q). Generations are non-empty subsets of F .\nSuppose that n generations, G1 , G2 , . . . , Gn , are formed s.t.\nF = \u222anj=1 Gj . A coding scheme is said to be non-overlapping\nif the generations are disjoint, i.e., \u2200i 6= j, Gi \u2229 Gj = \u2205;\notherwise, the scheme is said to be overlapping. The size\nof each generation Gj is denoted by gj , and its elements\n(j) (j)\n(j)\np1 , p2 , . . . , pgj . For convenience, we will occasionally also\n(j)\n(j) (j)\nuse Gj to denote the matrix with columns p1 , p2 , . . . , pgj .\nB. Encoding\nIn each transmission, the source first selects one of the n\ngenerationsP\nat random. The probability of choosing generation\nn\nGi is \u03c1i ,\ni=1 \u03c1i = 1. Let \u03c1 = (\u03c11 , \u03c12 , . . . , \u03c1n ). Once\ngeneration Gj is chosen, the source chooses a coding vector\n\n\f3\n\ne = [e1 , e2 , . . . , egj ]T , with each of the gj components chosen\nindependently and equally probably from GF (q). A new\npacket p\u0304 is then formed by linearly combining packets from\nPgj\n(j)\nei pi = e*Gj (Gj here denotes a matrix).\nGj by e: p\u0304 = i=1\nThe coded packet p\u0304 is then sent over the communication\nlink to the receiver along with the coding vector e and\nthe generation index j. Figure 1 shows a diagram of the\ncommunication between the source and the receiver. The\ngenerations shown in this example are chosen to be disjoint,\nbut this is not necessary.\n\ntake up \u2308log2 n\u2309 + gj \u2308log2 q\u2309 bits. Meanwhile, the data in\neach coded packet comprise d\u2308log2 q\u2309 bits. The generation\nsize makes a more significant contribution to packet overhead\nand such contribution is non-negligible due to the limited size\n(\u223c a few KB) of transmission packets in practical networks.\nThis gives another reason to keep generations small, besides\nreducing computational complexity.\nE. Computational Complexity\nThe computational complexity for encoding is\nO(d max{gj }) per coded packet for linearly combining\nthe gj information packets in each generation (recall that d is\nthe number of GF (q) symbols in each information packet,\nas defined in Section II-A). For decoding, the largest number\nof unknowns in the systems of linear equations to be solved\nis not more than max{gj }, and therefore the computational\ncomplexity is upper bounded by O((max{gj })2 +d max{gj })\nper information packet.\nF. Decoding Latency\n\nFig. 1.\nA file divided into N = 12 fragments and n = 4 (disjoint)\ngenerations containing h = 3 fragments each is available for distribution\nat the server. A receiver collects random linear combinations of randomly\nscheduled generations.\n\nC. Decoding\nDecoding starts with any generation Gj for which the\nreceiver has collected gj coded packets with linearly independent coding vectors. The information packets making up\nthis generation are decoded by solving a system of gj linear\nequations in GF (q) formed by the coded packets on one side\nand the linear combinations of the information packets by the\ncoding vectors on the other. Since generations are allowed to\noverlap, a decoded information packet may also participate in\nother generations, from the equations of which the information\npacket is then removed as an unknown variable. Consequently,\nin all the generations overlapping with the decoded generations, the number of unknown packets is reduced. As a\nresult, some generations may become decodable even if no\nnew coded packets are received from the source. Again, the\nnewly decoded generations resolve some unknowns of the\ngenerations they overlap with, which in turn may become\ndecodable and so on. We declare successful decoding when\nall N information packets have been decoded.\nThe coding scheme described here is inherently rateless\nand easily extendable to more general network topologies that\nallow coding at intermediate network nodes.\nD. Packet Overhead\nContained in each coded packet are the index of a generation\nGj and a linear combining vector for Gj which together\n\nIn this paper, we focus on the tradeoff between the computational complexity and the decoding latency of these codes over\nunicast links with erasures. Decoding latency here is defined\nas the number of coded packets transmitted until successful\ndecoding of all the information packets, and overhead is the\ndifference between the number of information packets and\nthe decoding latency. We assume a memoryless BEC with a\nconstant erasure rate \u01eb. Since our coding scheme is rateless,\neach coded packet is statistically of the same importance, and\nso the average decoding latency is inversely proportional to\nthe achievable capacity (1 \u2212 \u01eb) of the link. The throughput of\nthe code is inversely proportional to the decoding latency for\ngiven information length.\nIII. C OLLECTING C ODED PACKETS\n\nAND\n\nD ECODING\n\nA generation Gi is not decodable until the number of\nlinearly independent equations collected for Gi reaches the\nnumber of its information packets not yet resolved by decoding\nother generations. The connection between the number of\ncoded packets collected and the linear independence among\nthese coded packets has to be established before we can\npredict the decoding latency of codes over generations using\nthe collector's brotherhood model that will be discussed in the\nnext section.\nLet M (g, x) be the number of coded packets from a generation of size g adequate for collecting x linearly independent\nequations. Then M (g, x) has expected value [9]\nE[M (g, x)] =\n\nx\u22121\nX\nj=0\n\n1\n.\n1 \u2212 q j\u2212g\n\n(1)\n\nApproximating summation by integration, from (1) we get\nZ x\u22121\n1\n1\ndy +\nE[M (g, x)] /\n1 \u2212 q y\u2212g\n1 \u2212 q x\u22121\u2212g\n0\nq x\u22121\u2212g\n1 \u2212 q \u2212g\n=x +\n+\nlog\n.\n(2)\nq\n1 \u2212 q x\u22121\u2212g\n1 \u2212 q x\u22121\u2212g\n\n\f4\n\nLet\n\nA. Generating Functions, Expected Values and Variances\nx\u22121\u2212g\n\n\u03b7g (x) = x +\n\n\u2212g\n\nq\n1\u2212q\n+ logq\n.\n1 \u2212 q x\u22121\u2212g\n1 \u2212 q x\u22121\u2212g\n\n(3)\n\nWe can use \u03b7g (x) to estimate the number of coded packets\nneeded from a certain generation to gather x linearly independent equations.\nIn addition, we have the following Claim 1 which upper\nbounds the tail probability of M (g, g), the number of coded\npackets needed for a certain generation to gather enough\nlinearly independent equations for decoding.\nClaim 1: There exist positive constants \u03b1q,g and \u03b12,\u221e such\nthat, for s \u2265 g,\nProb[M (g, g) > s] = 1 \u2212\n\ng\u22121\nY\n\n(1 \u2212 q k\u2212s )\n\nk=0\n\n< 1 \u2212 exp(\u2212\u03b1q,g q\n\n\u2212(s\u2212g)\n\n) < 1 \u2212 exp(\u2212\u03b12,\u221e q \u2212(s\u2212g) ).\n\nAlso, since 1 \u2212 exp(\u2212x) < x for x > 0,\nProb[M (g, g) > s] < \u03b1q,g q \u2212(s\u2212g) .\n\n(4)\n\nProof: Please refer to Appendix A.\nWe will use Claim 1 in Theorem 8 in Section V to derive an\nupper bound to the expected overhead of coding over disjoint\ngenerations.\nIV. C OUPON C OLLECTOR ' S B ROTHERHOOD AND\nC OLLECTING C ODED PACKETS FROM G ENERATIONS\nThe coupon collector's brotherhood problem [10], [11]\nstudies quantities related to the completion of m sets of n\ndistinct coupons by sampling a set of n distinct coupons\nuniformly at random with replacement. In analogy, coded\npackets belonging to generation j can be viewed as copies of\ncoupon j, and hence the process of collecting coded packets\nwhen generations are scheduled uniformly at random can be\nmodeled as collecting multiple copies of distinct coupons.\nBecause of possible linear dependence among coded packets\nand the overlaps between generations, the numbers of coded\npackets needed for each of the n generations to ensure successful decoding, however, are n random variables. Therefore,\nwe must generalize the coupon collector's brotherhood model\nfrom collecting a uniform number of copies for all coupons to\ncollecting different numbers of copies for different coupons,\nbefore it can be applied to the analysis of the throughput\nperformance of coding over generations. In this section, the\noriginal collector's brotherhood model is generalized in two\nways. And later in this paper, the analysis of the throughput\nperformance of coding over disjoint generations in Section V\nrests on the first generalization, whereas that of coding over\noverlapping generations in Section VI rests on the second\ngeneralization. As our results are of more general interest than\nthe coding-over-generations problem, we will express them in\nthe coupon collection language. For example, the probability\n\u03c1i of scheduling generation Gi (defined in Section II) here\nrefers to the probability of sampling a copy of coupon Gi , for\ni = 1, 2, . . . , n.\n\nFor any m \u2208 N, we define Sm (x) as follows:\nx2\nxm\u22121\nx\n+\n+ ***+\n1!\n2!\n(m \u2212 1)!\nSm (x) =0 (m \u2264 0) and S\u221e (x) = ex .\n\nSm (x) =1 +\n\n(m \u2265 1)\n\n(5)\n(6)\n\nLet the total number of samplings needed to ensure that\nat least mi (\u2265 0) copies of coupon Gi are collected for all\ni = 1, 2, . . . , n be T (\u03c1, m), where m = (m1 , m2 , . . . , mn ).\nThe following Theorem 2 gives \u03c6T (\u03c1,m) (z), the generating\nfunction of the tail probabilities of T (\u03c1, m). This result\nis generalized from [10] and [11], and its proof uses the\nNewman-Shepp symbolic method in [10]. Boneh et al. [12]\ngave the same generalization, but we restate it here for use in\nour analysis of coding over disjoint generations. If for each\nj = 1, 2, . . . , n, the number of coded packets needed from\ngeneration Gj for its decoding is known to be mj (which\ncan be strictly larger than the generation size gj ), T (\u03c1, m)\nthen gives the total number of coded packets needed to ensure\nsuccessful decoding of the entire content when the generations\nare scheduled according to the probability vector \u03c1.\nTheorem 2: (Non-Uniform Sampling) Let\nX\n\u03c6T (\u03c1,m) (z) =\nProb[T (\u03c1, m) > i]z i .\n(7)\ni\u22650\n\nThen,\n\u03c6T (\u03c1,m) (z) =\n(8)\nZ \u221en\nn\no\nY\n\u0003\n\u0002 \u2212\u03c1i x(1\u2212z)\ne\n\u2212 Smi (\u03c1i xz)e\u2212\u03c1i x dx.\ne\u2212x(1\u2212z) \u2212\n0\n\ni=1\n\nProof: Please refer to Appendix B, where we give a\nfull proof of the theorem to demonstrate the Newman-Shepp\nsymbolic method [10], which is also used in the proof of our\nother generalization in Theorem 4.\nThe expected value and the variance of T (\u03c1, m) follow\nfrom the tail probability generating function derived in Theorem 2.\nCorollary 3:\nE[T (\u03c1, m)] = \u03c6T (\u03c1,m) (1)\n)\nZ \u221e(\nn\nY\n\u0003\n\u0002\n1 \u2212 Smi (\u03c1i x)e\u2212\u03c1i x dx,\n1\u2212\n=\n0\n\ni=1\n\nV ar[T (\u03c1, m)] = 2\u03c6\u2032T (\u03c1,m) (1) + \u03c6T (\u03c1,m) (1) \u2212 \u03c62T (\u03c1,m) (1).\nProof: Please refer to Appendix B.\nNote that in Theorem 2 and Corollary 3, mi -s are allowed\nto be 0, thus including the case where only a specific subset\nof the coupons is of interest. Theorem 2 and Corollary 3 are\nalso useful for the analysis of coding over generations when\nthere is a difference in priority among the generations. For\ninstance, in layered coded multimedia content, the generations\ncontaining the packets of the basic layer could be given\na higher priority than those containing enhancement layers\nbecause of a hierarchical reconstruction at the receiver.\nIn the following, we present another generalization of the\ncollector's brotherhood model. Sometimes we are simply\ninterested in collecting a coupon subset of a certain size,\n\n\f5\n\nregardless of the specific content of the subset. This can be\nfurther extended to the following more complicated case: for\neach i = 1, 2, . . . , A(A \u2265 1), ensure that there exists a subset\nof {G1 , G2 , . . . , Gn } such that each of its ki elements has at\nleast mi copies in the collected samples. Such a generalization\nis intended for treatment of coding over equally important\ngenerations, for example, when each generation is a substream\nof multiple-description coded data. In this generalization,\nthe generation scheduling (coupon sampling) probabilities are\nassumed to be uniform, i.e., \u03c11 = \u03c12 = * * * = \u03c1n = 1/n.\nSuppose that for some positive integer A \u2264 n, integers\nk1 , . . . , kA and m1 , . . . , mA satisfy 1 \u2264 k1 < * * * < kA \u2264 n\nand \u221e = m0 > m1 > * * * > mA > mA+1 = 0. We are\ninterested in the total number U (m, k) of coupons that needs\nto be collected, to ensure that the number of distinct coupons\nfor which at least mi copies have been collected is at least\nki , for all i = 1, 2, . . . , A, where m = (m1 , m2 , . . . , mA )\nand k = (k1 , k2 , . . . , kA ). The following Theorem 4 gives the\ngenerating function \u03c6U(m,k) (z) of U (m, k).\nTheorem 4: (Uniform Sampling)\nZ \u221e\nn\n(9)\n\u03c6U(m,k) (z) = n\ne\u2212nx enxz \u2212\n0\n\nX\n\n\u0013\nA \u0012\niij+1 \u2212ij o\nY\nij+1 h\ndx.\nSmj (xz) \u2212 Smj+1 (xz)\nij\n): j=0\n\n(i0 ,i1 ,...,iA+1\ni0 =0,iA+1 =n\nij \u2208[kj ,ij+1 ]\nj=1,2,...,A\n\nProof: Please refer to Appendix B.\nSame as for Corollary 3, we can find E[U (m, k)] =\n\u03c6U(m,k) (1). A computationally wieldy representation of\nE[U (m, k)] is offered in the following Corollary 5 in a\nrecursive form.\nCorollary 5: For k = k1 , k1 + 1, . . . , n, let\n\u03c60,k (x) = [(Sm0 (x) \u2212 Sm1 (x))e\u2212x ]k ;\nFor j = 1, 2, . . . , A, let\n\u03c6j,k (x)\n=\n\nk \u0012 \u0013\nX\n\u0003k\u2212w\nk \u0002\n\u03c6j\u22121,w (x),\n(Smj (x) \u2212 Smj+1 (x))e\u2212x\nw\n\nw=kj\n\nfor k = kj+1 , kj+1 + 1, . . . , n.\nThen,\nE[U (m, k)] = n\n\nZ\n\n\u221e\n\n(1 \u2212 \u03c6A,n (x)) dx.\n\n(10)\n\n0\n\nIt is not hard to find an algorithm that calculates 1\u2212\u03c6A,n (x)\nP\nPn\nin (c1 m1 +c2 (n\u22121)+c3 A\nj=1\nk=kj+1 (k \u2212kj )) basic arithmetic operations, where c1 , c2 and c3 are positive constants. As\nlong as m1 = O(An2 ), we can estimate the amount of work\nfor a single evaluation of 1 \u2212 \u03c6A,n (x) to be O(An2 ). The\nintegral (10) can be computed through the use of an efficient\nquadrature method, for example, Gauss-Laguerre quadrature.\nFor reference, some numerical integration issues for the special\ncase where A = 1 have been addressed in Part 7 of [13] and\nin [12].\nIn Section VI, we will apply Corollary 5 to find out the\nexpected throughput of the random annex code, an overlapping\n\ncoding scheme in which generations share randomly chosen\ninformation packets. The effect of the overlap size on the\nthroughput can be investigated henceforth.\nB. Limiting Mean Value and Distribution\nIn the previous subsection, we considered collecting a finite\nnumber of copies of a coupon set of a finite size. In this part,\nwe present some results from existing literature on the limiting\nbehavior of T (\u03c1, m) as n \u2192 \u221e or m1 = m2 = * * * = mn =\nm \u2192 \u221e, assuming \u03c11 = \u03c12 = * * * = \u03c1n = n1 . By slight abuse\nin notation, we denote T (\u03c1, m) here as Tn (m).\nBy Corollary 3,\nZ \u221e\n\u0002\n\u0003\n1 \u2212 (1 \u2212 Sm (x)e\u2212x )n dx.\n(11)\nE[Tn (m)] = n\n0\n\nThe asymptotics of E[Tn (m)] for large n has been discussed\nin literature [10], [14] and [15], and is summarized in the\nfollowing Theorem 6, (13), and Theorem 7.\nTheorem 6: ([14]) When n \u2192 \u221e,\n\nE[Tn (m)] = n log n+(m\u22121)n log log n+Cm n+o(n), (12)\nwhere Cm = \u03b3 \u2212 log(m \u2212 1)!, \u03b3 is Euler's constant, and\nm \u2208 N.\nFor m \u226b 1, on the other hand, we have [10]\nE[Tn (m)] \u2192 nm.\n\n(13)\n\nWhat is worth mentioning is that, as the number of coupons\nn \u2192 \u221e, for the first complete set of coupons, the number\nof samplings needed is O(n log n), whereas the additional\nnumber of samplings needed for each additional set is only\nO(n log log n).\nIn addition to the expected value of Tn (m), the concentration of Tn (m) around its mean is also of great interest to\nus. This concentration leads to an estimate of the probability\nof successful decoding for a given number of collected coded\npackets. We can specialize Corollary 3 to derive the variance\nof Tn (m), as a measure of probability concentration.\nFurther, since the tail probability generating functions derived in the last subsection are power series of non-negative\ncoefficients and are convergent at 1, they are absolutely\nconvergent on and inside the circle |z| = 1 in the complex\nz-plane. Thus, it is possible to compute the tail probabilities\nusing Cauchy's contour integration formula. However, extra\ncare is required for numerical stability in such computation.\nHere we instead look at the asymptotic case where the\nnumber of coupons n \u2192 \u221e. Erd\u00f6s and R\u00e9nyi have proven\nin [16] the limit law of Tn (m) as n \u2192 \u221e. Here we restate\nLemma B from [14] by Flatto, which in addition expresses\nthe rate of convergence to the limit law. We will later use this\nresult to derive a lower bound for the probability of decoding\nfailure in Theorem 9 in Section V-B.\nTheorem 7: ([14]) Let\n1\nYn (m) = (Tn (m) \u2212 n log n \u2212 (m \u2212 1)n log log n) .\nn\nThen,\n\u0013\n\u0012\n\u0013\n\u0012\nlog log n\ne\u2212y\n+O\n.\nPr[Yn (m) \u2264 y] = exp \u2212\n(m \u2212 1)!\nlog n\n\n\f6\n\nRemark 1: (Remarks 2&3, [14]) The estimation in Theorem\n7 is understood to hold uniformly on any finite interval \u2212a \u2264\ny \u2264 a. i.e., for any a > 0,\n\u0012\n\u0013\nexp(\u2212y)\nlog log n\nProb [Yn (m) \u2264 y] \u2212 exp \u2212\n\u2264 C(m, a)\n,\n(m \u2212 1)!\nlog n\nn \u2265 2 and \u2212a \u2264 y \u2264 a. C(m, a) is a positive constant\ndepending on m and a, but independent of n. For m\n\u0010 = 1, the\n\u0011\nconvergence rate to limit law is much faster: the O logloglogn n\n\u0011\n\u0010\nterm becomes O logn n .\nV. C ODING OVER D ISJOINT G ENERATIONS\nIn this section, we study the performance of coding over\ndisjoint generations. We derive both an upper bound and a\nlower bound for the expected decoding latency (as defined\nin Section II-F). We also derive the variance of the decoding\nlatency.\n\nE[W (\u03c1, g)]\n!\nZ \u221e\nn\nY\n\u0001\n\u2212\u03c1i x\n1\u2212e\nEMi [SMi (\u03c1i x)] dx\n1\u2212\n=\n0\n\n<\n\nZ\n\n\u221e\n\n\u0012\n\n0\n\nLet Mi (i = 1, 2, . . . , n) be the number of collected\ncoded packets from generation Gi when Gi first becomes\ndecodable. Then Mi is at least gi , has the same distribution as\nM (gi , gi ), the number of coded packets needed for a certain\ngeneration to gather enough linearly independent equations\nfor decoding, as defined and studied in Section III. Mi 's are\nindependent random variables. Let the decoding latency over a\nperfect channel be W (\u03c1, g), where g = (g1 , g2 , . . . , gn ). Use\nW\u01eb (\u03c1, g) to denote the decoding latency on a BEC(\u01eb).\nLet Xk (k = 1, 2, . . . ) be i.i.d. geometric random variables\n1\nwith success rate 1\u2212\u01eb. Therefore, E[Xk ] = 1\u2212\u01eb\nand E[Xk2 ] =\n1+\u01eb\n(1\u2212\u01eb)2 . Then\nW (\u03c1,g)\n\nW\u01eb (\u03c1, g) =\n\nX\n\nXi ,\n\ni=1\n\nand therefore,\n1\nE[W (\u03c1, g)],\n(14)\n1\u2212\u01eb\n\u0001\n1\nV ar[W (\u03c1, g)] + \u01ebE[W 2 (\u03c1, g)] .\nV ar[W\u01eb (\u03c1, g)] =\n(1 \u2212 \u01eb)2\n(15)\nE[W\u01eb (\u03c1, g)] =\n\nBy definition, E[W (\u03c1, g)] is lower bounded by E[T (\u03c1, g)],\nthe expected number of coded packets necessary for collecting at least gi coded packets for each generation Gi , and\nE[T (\u03c1, g)] is as given in Corollary 3.\nThe following Theorem 8 gives the exact expression for the\nfirst and second moments of W (\u03c1, g), along with an upper\nbound for E[W (\u03c1, g)] considering the effect of finite finite\nfield size q. Then, the expected value and the variance of\nW\u01eb (\u03c1, g) can be derived from (14) and (15).\nTheorem 8: The expected number of coded packets needed\nfor successful decoding of all N information packets\n\n(16)\n\n1 \u2212 e\u2212\u03c1i x Sgi (\u03c1i x)\n\n(17)\n\ni=1\n\ngi \u03c1i x/q\n\n+ \u03b1q,gi q e\n\n\u0013\n\u0001\u0011\ndx,\n\u2212 \u03b1q,gi q Sgi (\u03c1i x/q)\ngi\n\nE[W 2 (\u03c1, g)]\n(18)\nZ \u221e \u0012\nn\n\u2212\u03c1i x\nX\n1 \u2212 EMi [SMi \u22121 (\u03c1i x)]e\n\u03c1i\n=2\nx 1\u2212\n*\n1 \u2212 EMi [SMi (\u03c1i x)] e\u2212\u03c1i x\n0\ni=1\n\u0013\nn\nY\n\u0003 \u2212\u03c1j x \u0001\n\u0002\ndx\n1 \u2212 EMj SMj (\u03c1j x) e\n*\nj=1\n\n+\n\nZ\n\n0\n\nA. Expected Decoding Latency and Its Variance\n\n1\u2212\n\ni=1\nn \u0010\nY\n\n\u221e\n\n1\u2212\n\nn\nY\n\n1\u2212e\n\n\u2212\u03c1i x\n\ni=1\n\nEMi [SMi (\u03c1i x)]\n\n\u0001\n\n!\n\ndx\n\n\u0001\nPgi \u22121\nln 1 \u2212 q k\u2212gi , i = 1, 2, . . . , n.\nwhere \u03b1q,gi = \u2212 k=0\nProof: Please refer to Appendix C.\nIn the case where generations are of equal size and scheduled uniformly at random, we can estimate the asymptotic\nlower bound for E[W (\u03c1, g)] by the asymptotics of Tn (m)\ngiven in (12) and (13).\nFigure 2(a) shows several estimates of E[W (\u03c1, g)], and Figure 2(b) shows the standard deviation of W (\u03c1, g) calculated\nfrom Theorem 8 and simulation results, when \u03c1i = n1 and\ngi = g for i = 1, 2, . . . , n. The estimates are plotted versus\nthe uniform generation size g for fixed N = ng = 1000.\nFor coding over disjoint generations and a fixed total number of information packets, both the expected value and the\nstandard deviation of the decoding latency drop significantly\nas the generation size g grows to a relatively small value from\nthe case where no coding is used (g = 1). Hence, throughput\nis improved by a moderate increase in the computational cost\nthat scales quadratically with the generation size (see Section\nII-E). On the other hand, we also observe that past a moderate\ngeneration size (\u223c 50 \u2212 100 coded packets for N = 1000),\nthe decrease in decoding latency becomes slower by further\nincreasing the encoding/decoding complexity. We therefore\nargue for a \"sweet spot\" generation size which characterizes\nthe tradeoff between throughput and complexity.\nB. Probability of Decoding Failure\nIn this subsection we assume uniform generation size and\nscheduling probability, i.e., \u03c1i = n1 , gi = g for i = 1, 2, . . . , n.\nFor short, we denote W (\u03c1, g) as Wn (g). From Theorem 7,\nwe obtain the following lower bound to the probability of\ndecoding failure as n \u2192 \u221e:\nTheorem 9: When n \u2192 \u221e, the probability of decoding\nfailure whenht coded packets have been collected\n\u0010 is greater\n\u0011\n\u0001i\nt\n1\ng\u22121\nexp \u2212 n + O logloglogn n .\nthan 1 \u2212 exp \u2212 (g\u22121)! n(log n)\n\n\f7\n\nProof: The probability of decoding failure after acquiring\nt coded packets equals Prob[Wn (g) > t]. Since Wn (g) \u2265\nTn (g),\nProb[Wn (g) > t] \u2265 Prob[Tn (g) > t]\n\u0015\n\u0014\nt\n= 1 \u2212Prob Yn (g) \u2264 \u2212 log n \u2212 (g \u2212 1) log log n .\nn\n\n(a)\n\nThe result in Theorem 9 follows directly from Theorem 7.\nCorollary 10: When g is fixed and n \u2192 \u221e, in order\nto make the probability of decoding failure smaller than \u03b4,\nthe number of coded packets collected has to be at least\n1\nE[Tn (g)] \u2212 n log log 1\u2212\u03b4\n. If \u03b4 = N1c for some constant c,\nthen the number of coded packets necessary for successful\ndecoding has to be at least E[Tn (g)] + cn log(ng).\nTheorem 4.2 in [5] also gives the number of coded packets\nneeded to have the probability of decoding failure below \u03b4 =\n1\nN c , but under the assumption that ln(N/\u03b4) = o(N/n) = o(g).\nIn comparison, Corollary 10 treats the case where g is constant.\nFigure 2(c) shows the estimate of the probability of decoding failure versus T , the number of coded packets collected.\nAs pointed out in Remark 1, for m \u2265 2, the deviation of\nthe CDF of Tn (m) from the limit law for n \u2192 \u221e depends\non m and is on the order of O( logloglogn n ) for m \u2265 2, which\nis quite slow, partly explaining the deviation of the limit law\ncurves from the simulation curves for m = 5 and m = 10 in\nFigure 2(c).\nVI. C ODING OVER OVERLAPPING G ENERATIONS\n\n(b)\n\n(c)\nFig. 2. (a) Estimates of E[W (\u03c1, g)], the expected number of coded packets\nrequired for successful decoding when the total number of information packets\nis N = 1000, and both g and \u03c1 are uniform. Estimates shown: lower bound\nE[T (\u03c1, g)]; upper bound (17); mean of W (\u03c1, g) in simulation; n \u2192 \u221e\nasymptotic (12); m \u226b 1 asymptotics (13); (b) Estimates of the standard\ndeviation of W (\u03c1, g); (c) Estimates of probability of decoding failure versus\nthe number of coded packets collected: Theorem 9 along with simulation\nresults.\n\nEven when generations are scheduled uniformly at random,\nthere will be more coded packets accumulated in some of\nthe generations than in others. The \"slowest\" generation is\nthe bottleneck for file decoding. It is then advisable to design\na mechanism that allows \"faster\" generations to help those\nlagging behind. In this section, we propose the random annex code, a new coding scheme in which generations share\nrandomly chosen packets, as opposed to previously proposed\n\"head-to-toe\" overlapping scheme of [7].\nWe provide a heuristic analysis of the code throughput\nbased on our results for the coupon collection model and an\nexamination of the overlapping structure. Previous work on\ncoding over overlapping generations, [6] and [7], lacks accurate performance analysis for information blocks of moderate\nfinite lengths. On the other hand, the computational effort\nneeded to carry out our analysis scales well with the length\nof information, and the performance predictions coincide with\nsimulation data. In addition, we find that our random annex\ncode outperforms the \"head-to-toe\" overlapping scheme of [7]\nover a unicast link.\nIn this section we conveniently assume that the coded\npackets are sent over a perfect channel, since here we are\ninterested in comparing the performance of different rateless\ncoding schemes.\nA. Forming Overlapping Generations\nWe form n overlapping generations out of a file with N\ninformation packets in two steps as follows:\n\n\f8\n\n1) Partition the file set F of N packets into subsets\nB1 , B2 , . . . , Bn , each containing h consecutive packets.\nThese n = N/h subsets are referred to as base generations. Thus, Bi = {p(i\u22121)h+1 , p(i\u22121)h+2 , . . . , pih } for\ni = 1, 2, . . . , n. N is assumed to be a multiple of h for\nconvenience. In practice, if N is not a multiple of h, set\nn = \u2308N/h\u2309 and assign the last [N \u2212 (n \u2212 1)h] packets\nto the last (smaller) base generation.\n2) To each base generation Bi , add a random annex Ri ,\nconsisting of l packets chosen uniformly at random\n(without replacement) from the N \u2212 h = (n \u2212 1)h\npackets in F \\Bi . The base generation together with its\nannex constitutes the extended generation Gi = Bi \u222aRi ,\nthe size of which is g = h + l. Throughout this\npaper, unless otherwise stated, the term \"generation\" will\nrefer to \"extended generation\" whenever used alone for\noverlapping generations.\nThe generation scheduling probabilities are chosen to be\nuniform, \u03c11 = \u03c12 = * * * = \u03c1n = 1/n. The encoding and\ndecoding procedures run the same as described in the general\nmodel in Section II.\nB. Analyzing the Overlapping Structure\nThe following Claims 11 through 14 present combinatorial\nderivations of quantities concerning the frequency at which\nan arbitrary information packet is represented in different\ngenerations.\nClaim 11: For any packet in a base generation Bk , the\nprobability that it belongs to annex Rr for some r \u2208\n{1, 2, . . . , n}\\{k} is\n\u0012\n\u0013 \u0012\n\u0013\nl\nN \u2212h\u22121\nl\nN \u2212h\n=\n,\n\u03c0=\n/\n=\nN \u2212h\n(n \u2212 1)h\nl\u22121\nl\nwhereas the probability that it does not belong to Rr is \u03c0\u0304 =\n1 \u2212 \u03c0.\nClaim 12: Let X be the random variable representing the\nnumber of generations an information packet participates in.\nThen, X = 1 + Y, where Y is Binom(n \u2212 1, \u03c0).\nE[X] = 1 + (n \u2212 1)\u03c0 = 1 +\n\nl\n,\nh\n\nand\nV ar[X] = (n \u2212 1)\u03c0\u03c0\u0304.\nClaim 13: In each generation of size g = h+l, the expected\nnumber of information packets not participating in any other\ngeneration is h\u03c0\u0304 (n\u22121) \u2248 he\u2212l/h for n \u226b 1; the expected\nnumber of information packets participating in at least two\ngenerations is\ni\nh\nl + h[1 \u2212 \u03c0\u0304 (n\u22121) ] \u2248 l + h 1 \u2212 e\u2212l/h < min{g, 2l}\n\nfor n \u226b 1 and l > 0.\nClaim 14: The probability that two generations overlap is\n\u0001 N \u2212h\u00012\n\u22122h\n1 \u2212 l,l,NN\u22122h\u22122l\n/ l\n. The number of generations overlapping with any one generation Gi is then\n\"\n\u0012\n\u0013 \u0012\n\u00132 #!\nN \u2212 2h\nN \u2212h\nBinom n \u2212 1, 1 \u2212\n/\n.\nl, l, N \u2212 2h \u2212 2l\nl\n\nThe following Theorem 15 quantifies the expected amount\nof help a generation may receive from previously decoded\ngenerations in terms of common information packets. In the\nnext subsection, we use Corollary 5 and Theorem 15 for a\nheuristic analysis of the expected throughput performance of\nthe random annex code.\nTheorem 15: For any I \u2282 {1, 2, . . . , n} with |I| = s, and\nany j \u2208 {1, 2, . . . , n}\\I,\n\u03a9(s) = E[| (\u222ai\u2208I Gi ) \u2229 Gj |] = g * [1 \u2212 \u03c0\u0304 s ] + sh * \u03c0\u03c0\u0304 s (19)\nwhere |B| denotes the cardinality of set B. When n \u2192 \u221e,\n= \u03a9(s), then \u03c9(\u03b2) \u2192\nif \u0002hl \u2192 \u03b1 and ns \u2192 \u0001\u03b2, and let \u03c9(\u03b2)\n\u0003\nh (1 + \u03b1) 1 \u2212 e\u2212\u03b1\u03b2 + \u03b1\u03b2e\u2212\u03b1\u03b2 .\nProof: Please refer to Appendix D.\nC. Expected Throughput Analysis: The Algorithm\nGiven the overlapping structure, we next describe an analysis of the expected number of coded packets a receiver needs\nto collect in order to decode all N information packets of F\nwhen they are encoded by the random annex code. We base\nour analysis on Theorem 15 above, Corollary 5 in Section IV,\nand also (3) in Section III, and use the mean value for every\nquantity involved.\nBy the time when s (s = 0, 1, . . . , n \u2212 1) generations have\nbeen decoded, for any one of the remaining (n\u2212s) generations,\non the average \u03a9(s) of its participating information packets\nhave been decoded, or equivalently, (g \u2212 \u03a9(s)) of them are\nnot yet resolved. If for any one of these remaining generations\nthe receiver has collected enough coded packets to decode\nits unresolved packets, that generation becomes the (s + 1)th\ndecoded; otherwise, if no such generation exists, decoding\nfails.\nThe quantity \u03b7g (x) defined in (3) in Section III estimates the\nnumber of coded packets from a generation of size g adequate\nfor collecting x linearly independent equations. By extending\nthe domain of \u03b7g (x) from integers to real numbers, we can\nestimate that the number of coded packets needed for the (s +\n1)th decoded generation should exceed m\u2032s = \u2308\u03b7g (g \u2212 \u03a9(s))\u2309.\nSince in the random annex code, all generations are randomly\nscheduled with equal probability, for successful decoding, we\nwould like to have at least m\u20320 coded packets belonging to one\nof the generations, at least m\u20321 belonging to another, and so\non. Then Corollary 5 in Section IV can be applied to estimate\nthe total number of coded packets needed to achieve these\nminimum requirements for the numbers of coded packets.\nThe algorithm for our heuristic analysis is listed as follows:\n1) Compute \u03a9(s \u2212 1) for s = 1, . . . , n using Theorem 15;\n2) Compute m\u2032s = \u2308\u03b7g (g \u2212 \u03a9(s \u2212 1))\u2309 for s = 1, 2, . . . , n\nusing (3);\n3) Map m\u2032s (s = 1, 2, . . . , n) into A values mj (j =\n1, 2, . . . , A) so that mj = m\u2032kj\u22121 +1 = m\u2032kj\u22121 +2 = * * * =\nm\u2032kj , for j = 1, 2, . . . , A, k0 = 0 and kA = n;\n4) Evaluate (10) in Corollary 5 with the A, kj s, and mj s\nobtained in Step 3), as an estimate for the expected number of coded packets needed for successful decoding.\nRemark 2: The above Step 3) is viable because \u03a9(s) is\nnondecreasing in s, \u03b7g (x) is non-decreasing in x for fixed g,\nand thus m\u2032s is non-increasing in s.\n\n\f9\n\nAlthough our analysis is heuristic, we will see in the next\nsection that the estimate closely follows the simulated average\nperformance curve of the random annex coding scheme.\nD. Numerical Evaluation and Simulation Results\n1) Throughput vs. Complexity in Fixed Number of Generations Schemes: Our goal here is to find out how the annex size\nl affects the decoding latency of the scheme with fixed base\ngeneration size h and the total number of information packets\nN (and consequently, the number of generations n). Note\nthat the generation size g = h + l affects the computational\ncomplexity of the scheme, and hence we are actually looking\nat the tradeoff between throughput and complexity.\nFigure 3 shows both the analytical and simulation results\nwhen the total number N of information packets is 1000 and\nthe base generation size h is 25. Figure 3(a) shows h + l \u2212\n\u03a9(s) for s = 0, 1, . . . , n with different annex sizes. Recall\nthat \u03a9(s) is the expected size of the overlap of the union of\ns generations with any one of the leftover n \u2212 s generations.\nAfter the decoding of s generations, for any generation not yet\ndecoded, the expected number of information packets that still\nneed to be resolved is then h + l \u2212 \u03a9(s). We observe that the\nh + l \u2212 \u03a9(s) curves start from h + l for s = 0 and gradually\ndescends, ending somewhere above h \u2212 l, for s = n \u2212 1.\nRecall that we measure throughput by decoding latency\n(Section II-F). Figure 3(b) shows the expected performance\nof the random annex code, along with the performance of the\nhead-to-toe overlapping code and the non-overlapping code\n(l = 0). Figure 3(c) shows the probability of decoding failure\nof these codes versus the number of coded packets collected.\n\u2022 Our analysis for the expected decoding latency closely\nmatches the simulation results.\n\u2022 Figure 3(b) shows that by fixing the file size N and the\nbase generation size h, the expected decoding latency\ndecreases roughly linearly with increasing annex size l,\nup to l = 12 for the random annex scheme and up to l = 8\nfor the head-to-toe scheme. Meanwhile, the decoding\ncost per information packet is quadratic in g = h + l.\nBeyond the optimal annex size, throughput cannot be\nfurther increased by raising computational cost.\n\u2022 The random annex code outperforms head-to-toe overlapping at their respective optimal points. Both codes\noutperform the non-overlapping scheme.\n\u2022 As more coded packets are collected, the probability of\ndecoding failure of the random annex code converges to\n0 faster than that of the head-to-toe and that of the nonoverlapping scheme.\nOverlaps provide a tradeoff between computational complexity and decoding latency.\n2) Enhancing Throughput in Fixed Complexity Schemes:\nOur goal here is to see if we can choose the annex size\nto optimize the throughput with negligible sacrifice in complexity. To this end, we fix the extended generation size\ng = h + l and vary only the annex size l. Consequently, the\ncomputational complexity for coding does not increase when\nl increases. Actually, since some of the information packets in\na generation of size g could already be solved while decoding\n\n(a)\n\n(b)\n\n(c)\nFig. 3. N = 1000, h = 25, q = 256: (a) Difference between the generation\nsize and the expected size of overlap with previously decoded generations\n(h + l \u2212 \u03a9(s)); (b) Expected number of coded packets needed for successful\ndecoding versus annex size l; (c) Probability of decoding failure\n\nother generations, the remaining information packets in this\ngeneration can be solved in a system of linear equations of\nfewer than g unknowns, and as a result increasing l might\ndecrease the decoding complexity.\nFigure 4 shows both the analytical and simulation results for\nthe code performance when the total number N of information\npackets is fixed at 1000 and size g of extended generation fixed\nat 25.\n\u2022 Again our analytical results agree with simulation results\nvery well;\n\u2022 It is interesting to observe that, without raising computational complexity, increasing annex size properly can still\ngive non-negligible improvement to throughput;\n\u2022 Figure 4(a) shows a roughly linear improvement of\nthroughput with increasing l, up to l = 10 for the random\n\n\f10\n\n(a)\n\nFig. 5. Optimal expected decoding latency and the optimal overlap size with\nrandom annex codes. N = 1000, q = 16\n\n(b)\nFig. 4. N = 1000, g = h+l = 25, q = 256: (a) Expected number of coded\npackets needed for successful decoding versus annex size l; (b) Probability\nof decoding failure\n\ncomplexity is considerable. Capturing the optimal overlap size\nin terms of other parameters of the code is our object of interest\nin the future.\nA PPENDIX A\nP ROOF OF C LAIM 1\n\nannex scheme and up to l = 6 for the head-to-toe scheme.\nIncreasing l beyond affects throughput adversely;\n\u2022 The random annex code again outperforms head-to-toe\noverlapping at their optimal points. Both codes outperform the non-overlapping scheme;\n\u2022 We again observe that the probability of decoding failure\nof the random annex code converges faster than those of\nthe head-to-toe and the non-overlapping schemes.\nWhen the overlap size increases, we either have larger\ngenerations with unchanged number of generations, or a larger\nnumber of generations with unchanged generation size. In both\ncases the decoding latency would increase if we neglected the\neffect of overlaps during the decoding process. If we make use\nof the overlap in decoding, on the other hand, the larger the\noverlap size, the more help the generations can lend to each\nother in decoding and, hence, reducing the decoding latency.\nTwo canceling effects result in a non-monotonic relationship\nbetween throughput and overlap size.\nThe effect of generation size on the throughput of random\nannex codes is further illustrated in Figure 5. Figure 5 plots the\noptimal expected decoding latency achievable by random annex codes and the corresponding optimal annex size versus the\ngeneration size for N = 1000 and q = 16. The plotted values\nare calculated using the algorithm listed in Section VI-C. We\ncan see from Figure 5 that with the random annex code and a\ngeneration size of 20, the expected throughput is better than\nwhat can be achieved with coding over disjoint generations\nand a generation size of 50. The reduction in computational\n\nFor i = 1, 2, . . . , n and any s \u2265 g, we have\n\b\nln Prob M (g, g) \u2264 s\n= ln\n=\u2212\n\nh\u22121\nY\n\n(1 \u2212 q k\u2212s ) =\n\nk=0\ng\u22121\n\u221e\nXX\nk=0 j=1\n\n=\u2212\n\n\u221e\nX\n1\nj=1\n\nj\n\ng\u22121\n\u221e\nX\n1 X j(k\u2212s)\n1 (k\u2212s)j\nq\n=\u2212\nq\nj\nj\nj=1\nk=0\n\njg\n\nq \u22121\nqj \u2212 1\n\n\u221e\nX\n1\nj=1\n\n>q \u2212(s\u2212g)\n\nln(1 \u2212 q k\u2212s )\n\nk=0\n\nq \u2212js\n\n= \u2212 q \u2212(s\u2212g)\n\ng\u22121\nX\n\nj\n\nq \u2212(j\u22121)(s\u2212g)\n\n1 \u2212 q \u2212jg\nqj \u2212 1\n\n\u221e\nX\n1 1 \u2212 q \u2212jg\n\nj 1 \u2212 qj\n\b\n=q \u2212(s\u2212g) ln Prob M (g, g) \u2264 g\n\b\n>q \u2212(s\u2212g) lim ln Prob M (g, g) \u2264 g\nj=1\n\nh\u2192\u221e,q=2\n\nThe claim is obtained by setting\n\b\n\u03b1q,g = \u2212 ln Prob M (g, g) \u2264 g ,\n\nand\n\n\u03b12,\u221e = \u2212 lim\n\ng\u2192\u221e,q=2\n\n\b\nln Prob M (g, g) \u2264 g .\n\n\f11\n\nP ROOFS\n\nOF\n\nA PPENDIX B\nG ENERALIZED R ESULTS OF C OLLECTOR ' S\nB ROTHERHOOD P ROBLEM\n\nProof of Corollary 3\nNote that\n\u03c6T (\u03c1,m) (z) =\n\nProof of Theorem 2\n\n\u221e\nX\n\nProb[T (\u03c1, m) > t]z t\n\nt=0\n\nOur proof generalizes the symbolic method of [10].\nLet \u03be be the event that the number of copies of coupon Gi\nis at least mi for every i = 1, 2, . . . , n. For integer t \u2265 0,\nlet \u03be(t) be the event that \u03be has occurred after a total of t\n \u0304 be the complementary event. Then,\nsamplings, and let \u03be(t)\n \u0304\nthe tail probability Prob[T (\u03c1, m) > t] = Prob[\u03be(t)]\n= \u03bdt .\nTo derive \u03bdt , we introduce an operator f acting on\nan n-variable polynomial g. f removes all monomials\nwn\n1 w2\nxw\nin g satisfying w1 \u2265 m1 , . . . , wn \u2265 mn .\n1 x2 . . . xn\nNote that f is a linear operator, i.e., if g1 and g2 are two\npolynomials in the same n variables, and a and b two scalars,\nwe have af (g1 ) + bf (g2 ) = f (ag1 + bg2 ).\nEach monomial in (x1 +* * *+xn )t corresponds to one of the\nt\nn possible outcomes of t samplings, with the exponent of xi\nbeing the number of copies of coupon Gi . Since the samplings\nwn\n1 w2\nare independent, the probability of an outcome xw\n1 x2 . . . xn\nw1 w2\nwn\n \u0304\nis \u03c11 \u03c12 . . . \u03c1n . Hence, the probability of \u03be(t) is f ((x1 +\n* * * + xn )t ), when evaluated at xi = \u03c1i for i = 1, 2, . . . n, i.e.,\nt\n\n\u03bdt = f ((x1 + * * * + xn ) )|xi =\u03c1i ,i=1,...,n .\n\n=\n\n=\n\nZ\n\n\u221e\n\n0\n\nE[T (\u03c1, m)] =\n\n\u221e\nX\n\nevaluated at xi = \u03c1i , i = 1, . . . , n.\nWe next find the sum of the monomials in the polynomial\nexpansion of exp(x1 +* * *+xn ) Q\nthat should be removed under\nn\nf . Clearly, this sum should be i=1 (exi \u2212 Smi (xi )), where\nS is defined in (5) and (6)). Therefore,\nf (exp(x1 zy + * * * + xn zy)) |xi =\u03c1i ,i=1,...,n\nn\nY\n(e\u03c1i zy \u2212 Smi (\u03c1i zy)) .\n= ezy \u2212\ni=1\n\n\u03c6T (\u03c1,m) (z) =\n\n\"\n\ne\n\n0\n\nzy\n\n\u2212\n\nn\nY\n\ni=1\n\n(e\n\n\u03c1i zy\n\nj\u22121\nX\n\nzt\n\nt=0\n\njProb[T (\u03c1, m) = j] = \u03c6T (\u03c1,m) (1).\n\nj=1\n\nSimilarly,\n\u03c6\u2032T (\u03c1,m) (z) =\n=\n\n\u221e\nX\n\nt=0\n\u221e\nX\n\ntProb[T (\u03c1, m) > t]z t\u22121\nProb[T (\u03c1, m) = j]\n\n\u221e\nX\n1\nj=1\n\ntz t\u22121\n\nt=0\n\nj=1\n\n\u03c6\u2032T (\u03c1,m) (1) =\n\nj\u22121\nX\n\n2\n\nj(j \u2212 1)Prob[T (\u03c1, m) = j].\n\nHence,\nE[T (\u03c1, m)2 ] =\n\n\u221e\nX\n\nj 2 Prob[T (\u03c1, m) = j]\n\nj=1\n=2\u03c6\u2032T (\u03c1,m) (1)\n\n+ \u03c6T (\u03c1,m) (1),\n\nVar[T (\u03c1, m)] = 2\u03c6\u2032T (\u03c1,m) (1) + \u03c6T (\u03c1,m) (1) \u2212 \u03c62T (\u03c1,m) (1).\n\n0\n\n\u221e\n\nProb[T (\u03c1, m) = j]\n\nand consequently,\n\n1 t \u2212y\ny e dy = 1\nt!\n\nand the linearity of the operator f imply that\nZ \u221eX\nf ((x1 + * * * + xn )t ) t t \u2212y\n\u03c6T (\u03c1,m) (z) =\nz y e dy\nt!\n0\nt\u22650\nZ \u221e \u0010X\n(x1 zy + * * * + xn zy)t \u0011 \u2212y\n=\nf\ne dy\nt!\n0\nt\u22650\nZ \u221e\n=\nf (exp(x1 zy + * * * + xn zy)) e\u2212y dy (21)\n\nZ\n\n\u221e\nX\nj=1\n\nt\u22650\n\nThe identity\n\nProb[T (\u03c1, m) = j]z t\n\nt=0 j=t+1\n\n(20)\n\nHence, (20) and (7) lead to\nX\n\u0001\n\u03c6T (\u03c1,m) (z) =\nf (x1 + * * * + xn )t z t |xi =\u03c1i ,i=1,...,n .\n\n\u221e X\n\u221e\nX\n\n#\n\n\u2212 Smi (\u03c1i zy)) e\u2212y dy\n(22)\n\nWe have\n\u03c6\u2032T (\u03c1,m) (z) =\nZ \u221e \u0012\n\u2212x(1\u2212z)\n\nn\nX\n\ne\u2212\u03c1i x(1\u2212z) \u2212 Smi \u22121 (\u03c1i xz)e\u2212\u03c1i x\n*\ne\u2212\u03c1i x(1\u2212z) \u2212 Smi (\u03c1i xz)e\u2212\u03c1i x\n0\ni=1\nn \u0010\n\u0011\u0013\nY\ne\u2212\u03c1j x(1\u2212z) \u2212 Smj (\u03c1j xz)e\u2212\u03c1j x dx,\n*\nx e\n\n\u2212\n\n\u03c1i\n\nj=1\n\nand from there, we can get \u03c6\u2032T (\u03c1,m) (1) and Var[T (\u03c1, m)].\nProof of Theorem 4\n\nWe again apply the Newman-Shepp symbolic method. Similar\nto the proof of Theorem 2, we introduce an operator f acting\nwn\n1\non an n-variable polynomial g. For a monomial xw\n1 . . . xn ,\nlet ij be the number of exponents wu among w1 , . . . , wn\nsatisfying wu \u2265 kj , for j = 1, . . . , A. f removes all\nwn\n1\nmonomials xw\n1 . . . xn in g satisfying i1 \u2265 k1 , . . . , iA \u2265 kA\nand i1 \u2264 * * * \u2264 iA . f is again a linear operator. One can see\nthat\n\u03c6U(m,k) (z) =\n(23)\nZ \u221e\nf (exp(x1 zy + * * * + xn zy)) e\u2212y dy|x1 =x2 =***=xn = n1 .\n0\n\n\f12\n\nWe choose integers 0 = i0 \u2264 i1 \u2264 * * * \u2264 iA \u2264 iA+1 = n,\nsuch that ij \u2265 kj for j = 1, . . . , A, and then partition indices\n{1, . . . , n} into (A + 1) subsets I1 , . . . , IA+1 , where Ij (j =\n1, . . . , A + 1) has ij \u2212 ij\u22121 elements. Then\nA+1\nY\n\nY\n\n(Smj\u22121 (xi ) \u2212 Smj (xi ))\n\nExpression (18) for E[W 2 (\u03c1, g)] can be derived in the same\nmanner, and then the expression for Var[W (\u03c1, g)] immediately\nfollows.\nA PPENDIX D\nP ROOF OF T HEOREM 15\n\n(24)\n\nj=1 i\u2208Ij\n\nequals the sum of all monomials in exp(x1 + * * * + xn ) with\n(ij \u2212 ij\u22121 ) of the n exponents smaller than mj\u22121 but greater\nthan or equal to mj , for j = 1, . . . , A+1. (Here S is as defined\nby (5-6).) The number\npartitions\nof {1, . . . , n} is equal\n\u0001 of\n\u0001\nQsuch\nA\nn\nij+1\n=\nto n\u2212iA ,...,i\n.\nFinally,\nwe need to sum\nj=0\nij\n2 \u2212i1 ,i1\nthe terms of the form (24) over all partitions of all choices of\ni1 , . . . , iA satisfying kj \u2264 ij \u2264 ij+1 for j = 1, . . . , A:\nf (exp(x1 zy + * * * + xn zy)) |x1 =***=xn = n1 = exp(zy)\u2212\n\u0013\nA \u0012\nX\nY\nzy iij+1 \u2212ij\nzy\nij+1 h\n.\nSmj ( ) \u2212 Smj+1 ( )\nn\nn\nij\n(i ,i ,...,i\n): j=0\n0 1\nA+1\ni0 =0,iA+1 =n\nij \u2208[kj ,ij+1 ]\nj=1,2,...,A\n\n(25)\n\nWithout loss of generality, let I = {1, 2, . . . , s} and j =\ns + 1, and define Rs = \u222asi=1 Ri , Bs = \u222asi=1 Bi , and Gs =\n\u222asi=1 Gi for s = 0, 1, . . . , n \u2212 1. Then, E [| (\u222ai\u2208I Gi ) \u2229 Gj |] =\nE [|Gs \u2229 Gs+1 |]. For any two sets X and Y , we use X + Y\nto denote X \u222a Y when X \u2229 Y = \u2205.\nGs \u2229 Gs+1 =(Bs + Rs \\Bs ) \u2229 (Bs+1 + Rs+1 )\n=Bs \u2229 Rs+1 + Rs \u2229 Bs+1 + (Rs \\Bs ) \u2229 Rs+1 ,\nand therefore\nE[|Gs \u2229 Gs+1 |] =E[|Bs \u2229 Rs+1 |]+\n\n(27)\n\nE[|Rs \u2229 Bs+1 |] + E[|(Rs \\Bs ) \u2229 Rs+1 |].\nUsing Claim 11, we have\nE[|Bs \u2229 Rs+1 |] = sh\u03c0,\n\n(28)\ns\n\nE[|Rs \u2229 Bs+1 |] = h[1 \u2212 (1 \u2212 \u03c0) ],\n\nBringing (25) into (23) gives our result in Theorem 4.\n\n(29)\ns\n\nE[|(Rs \\Bs ) \u2229 Rs+1 |] = (n \u2212 s \u2212 1)h\u03c0[1 \u2212 (1 \u2212 \u03c0) ], (30)\nA PPENDIX C\nP ROOF OF T HEOREM 8\n\nwhere \u03c0 is as defined in Claim 11. Bringing (28)-(30) into\n(27), we obtain (19).\nFurthermore, when n \u2192 \u221e, if l/h \u2192 \u03b1 and s/n \u2192 \u03b2, then\n\nE[W (\u03c1, g)]\n!\nn\nX Y\nPr[Mi = mi ] E[T (\u03c1, m)]\n=\nm\n\n=\n\n\u221e\n\nZ\n\n1\u2212\n\n0\n\n=\n\nZ\n\n\"i=1\n\n\u221e\n\n1\u2212\n\n0\n\nn X\nY\n\nPr[Mi = mi ](1 \u2212 Smi (\u03c1i x)e\n\n\u2212\u03c1i x\n\ni=1 mi\nn\nY\n\n#\n\n) dx\n(26)\n\n1 \u2212 e\u2212\u03c1i x EMi [SMi (\u03c1i x)]\n\ni=1\n\n!\n\u0001\n\ndx.\n\n(26) comes from the distributivity.\nSince\n\u221e\nX\n(\u03c1i x)j\nEMi [SMi (\u03c1i x)] =\nPr[Mi > j],\nj!\nj=0\nby Claim 1,\nEMi [SMi (\u03c1i x)]\n\u221e\nX\n(\u03c1i x)j\n< Sgi (\u03c1i x) +\n\u03b1q,g q \u2212(j\u2212g)\nj!\nj=g\ni\n\n= Sgi (\u03c1i x) + \u03b1q,gi q gi e\u03c1i x/q \u2212 \u03b1q,gi q gi Sgi (\u03c1i x/q),\n\nwhere\n\u03b1q,gi\n\ngX\ni \u22121\n\b\n\u0001\n= \u2212 ln Pr M (gi , gi ) \u2264 gi = \u2212\nln 1 \u2212 q k\u2212gi\nk=0\n\nfor i = 1, 2, . . . , n.\nHence, we have (17).\n\nE[|Gs \u2229 Gs+1 |] =g * [1 \u2212 \u03c0\u0304 s ] + sh * \u03c0\u03c0\u0304 s\nh\n\u0010\n\u03b1 \u0011n\u03b2 i\n+\n\u2192h(1 + \u03b1) 1 \u2212 1 \u2212\nn\u22121\n\u0010\n\u03b1 \u0011n\u03b2\nh\u03b1\u03b2 1 \u2212\nn\u22121\ni\nh\n\u2192h (1 + \u03b1)(1 \u2212 e\u2212\u03b1\u03b2 ) + \u03b1\u03b2e\u2212\u03b1\u03b2\ni\nh\n=h 1 + \u03b1 \u2212 (1 + \u03b1 \u2212 \u03b1\u03b2)e\u2212\u03b1\u03b2\nACKNOWLEDGEMENT\nThe authors would like to thank the anonymous reviewers\nfor helpful suggestions to improve the presentation of the\npaper.\nR EFERENCES\n[1] T. Ho, R. Koetter, M. Medard, D. Karger, and M. Effros, \"The benefits\nof coding over routing in a randomized setting,\" in Proc. IEEE International Symposium on Information Theory (ISIT'03), 2003, p. 442.\n[2] C. Gkantsidis and P. Rodriguez, \"Network coding for large scale\ncontent distribution,\" in Proc. the 24th Annual Joint Conference of the\nIEEE Computer and Communications Societies (INFOCOM'05), vol. 4,\nMiami, FL, Mar. 2005, pp. 2235\u20132245.\n[3] Z. Liu, C. Wu, B. Li, and S. Zhao, \"UUSee: Large-scale operational\non-demand streaming with random network coding,\" in Proc. the 30th\nIEEE Conference on Computer Communications (INFOCOM'10), San\nDiego, California, Mar. 2010.\n[4] P. A. Chou, Y. Wu, and K. Jain, \"Practical network coding,\" in Proc.\n41st Annual Allerton Conference on Communication, Control, and\nComputing, Monticello, IL, Oct. 2003.\n[5] P. Maymounkov, N. Harvey, and D. S. Lun, \"Methods for efficient\nnetwork coding,\" in Proc. 44th Annual Allerton Conference on Communication, Control, and Computing, Monticello, IL, Sept. 2006.\n\n\f13\n\n[6] D. Silva, W. Zeng, and F. Kschischang, \"Sparse network coding with\noverlapping classes,\" in Proc. Workshop on Network Coding, Theory,\nand Applications (NetCod '09), Lausanne, Switzerland, Jun. 2009, pp.\n74\u201379.\n[7] A. Heidarzadeh and A. Banihashemi, \"Overlapped chunked network\ncoding,\" in IEEE Information Theory Workshop (ITW'10), Jan. 2010,\npp. 1\u20135.\n[8] W. Feller, An Introduction to Probability Theory and Its Applications,\n3rd ed. New York: John Wiley & Sons Inc., 1968, vol. 1, pp. 224\u2013225.\n[9] C. Fragouli and E. Soljanin, Network Coding Applications, ser. Foundations and Trends in Networking. Hanover, MA: now Publishers Inc.,\nJan. 2008, vol. 2, no. 2, pp. 146\u2013147.\n[10] D. Newman and L. Shepp, \"The double dixie cup problem,\" The\nAmerican Mathematical Monthly, vol. 67, no. 1, pp. 58\u201361, Jan. 1960.\n[11] D. Foata and D. Zeilberger, \"The collector's brotherhood problem using\nthe Newman-Shepp symbolic method,\" Algebra Universalis, vol. 49,\nno. 4, pp. 387\u2013395, 2003.\n[12] A. Boneh and M. Hofri, \"The coupon-collector problem revisited\n\u2013 a survey of engineering problems and computational methods,\"\nStochastic Models, vol. 13, pp. 39 \u2013 66, 1997. [Online]. Available:\nhttp://www.informaworld.com/10.1080/15326349708807412\n[13] P. Flajolet, D. Gardy, and L. Thimonier, \"Birthday paradox, coupon\ncollectors, caching algorithms and self-organizing search,\" Discrete\nApplied Mathematics, vol. 39, no. 3, pp. 207\u2013229, 1992.\n[14] L. Flatto, \"Limit theorems for some random variables associated with\nurn models,\" The Annals of Probability, vol. 10, no. 4, pp. 927\u2013934,\n1982. [Online]. Available: http://www.jstor.org/stable/2243548\n[15] A. N. Myers and H. S. Wilf, \"Some new aspects of the coupon collector's\nproblem,\" SIAM Rev., vol. 48, no. 3, pp. 549\u2013565, 2006.\n[16] P. Erd\u00f6s and A. R\u00e9nyi, \"On a classical problem of probability theory,\"\nMagyar Tud. Akad. Mat. Kutat\u00f3 Int. K\u00f6zl., vol. 6, pp. 215\u2013220, 1961.\n\n\f"}