{"id": "http://arxiv.org/abs/1002.5004v2", "guidislink": true, "updated": "2010-05-04T14:07:16Z", "updated_parsed": [2010, 5, 4, 14, 7, 16, 1, 124, 0], "published": "2010-02-26T15:11:53Z", "published_parsed": [2010, 2, 26, 15, 11, 53, 4, 57, 0], "title": "Direct reconstruction of dark energy", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.1265%2C1002.3366%2C1002.3585%2C1002.1829%2C1002.2971%2C1002.3172%2C1002.1779%2C1002.4750%2C1002.2012%2C1002.4107%2C1002.1462%2C1002.4785%2C1002.4226%2C1002.4064%2C1002.4336%2C1002.3692%2C1002.3029%2C1002.0224%2C1002.3072%2C1002.3493%2C1002.3488%2C1002.1093%2C1002.0115%2C1002.1112%2C1002.2758%2C1002.0532%2C1002.3679%2C1002.2481%2C1002.4867%2C1002.2185%2C1002.4063%2C1002.0680%2C1002.2846%2C1002.1930%2C1002.4220%2C1002.4415%2C1002.2555%2C1002.0979%2C1002.5026%2C1002.2941%2C1002.3719%2C1002.0752%2C1002.2406%2C1002.0059%2C1002.0369%2C1002.0991%2C1002.3306%2C1002.4923%2C1002.2997%2C1002.0040%2C1002.5004%2C1002.2126%2C1002.2105%2C1002.2601%2C1002.2854%2C1002.0765%2C1002.3162%2C1002.3022%2C1002.1500%2C1002.0354%2C1002.2875%2C1002.3946%2C1002.2274%2C1002.1587%2C1002.4017%2C1002.0558%2C1002.4915%2C1002.1627%2C1002.2129%2C1002.0491%2C1002.4620%2C1002.3648%2C1002.2877%2C1002.0611%2C1002.3808%2C1002.3669%2C1002.4813%2C1002.1692%2C1002.1535%2C1002.1638%2C1002.4875%2C1002.1197%2C1002.2976%2C1002.3369%2C1002.4251%2C1002.1856%2C1002.4087%2C1002.3672%2C1002.2253%2C1002.3584%2C1002.4281%2C1002.2254%2C1002.5020%2C1002.0755%2C1002.1808%2C1002.2112%2C1002.4036%2C1002.1851%2C1002.2816%2C1002.1305%2C1002.1357&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Direct reconstruction of dark energy"}, "summary": "An important issue in cosmology is reconstructing the effective dark energy\nequation of state directly from observations. With so few physically motivated\nmodels, future dark energy studies cannot only be based on constraining a dark\nenergy parameter space. We present a new non-parametric method which can\naccurately reconstruct a wide variety of dark energy behaviour with no prior\nassumptions about it. It is simple, quick and relatively accurate, and involves\nno expensive explorations of parameter space. The technique uses principal\ncomponent analysis and a combination of information criteria to identify real\nfeatures in the data, and tailors the fitting functions to pick up trends and\nsmooth over noise. We find that we can constrain a large variety of w(z) models\nto within 10-20 % at redshifts z<1 using just SNAP-quality data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.1265%2C1002.3366%2C1002.3585%2C1002.1829%2C1002.2971%2C1002.3172%2C1002.1779%2C1002.4750%2C1002.2012%2C1002.4107%2C1002.1462%2C1002.4785%2C1002.4226%2C1002.4064%2C1002.4336%2C1002.3692%2C1002.3029%2C1002.0224%2C1002.3072%2C1002.3493%2C1002.3488%2C1002.1093%2C1002.0115%2C1002.1112%2C1002.2758%2C1002.0532%2C1002.3679%2C1002.2481%2C1002.4867%2C1002.2185%2C1002.4063%2C1002.0680%2C1002.2846%2C1002.1930%2C1002.4220%2C1002.4415%2C1002.2555%2C1002.0979%2C1002.5026%2C1002.2941%2C1002.3719%2C1002.0752%2C1002.2406%2C1002.0059%2C1002.0369%2C1002.0991%2C1002.3306%2C1002.4923%2C1002.2997%2C1002.0040%2C1002.5004%2C1002.2126%2C1002.2105%2C1002.2601%2C1002.2854%2C1002.0765%2C1002.3162%2C1002.3022%2C1002.1500%2C1002.0354%2C1002.2875%2C1002.3946%2C1002.2274%2C1002.1587%2C1002.4017%2C1002.0558%2C1002.4915%2C1002.1627%2C1002.2129%2C1002.0491%2C1002.4620%2C1002.3648%2C1002.2877%2C1002.0611%2C1002.3808%2C1002.3669%2C1002.4813%2C1002.1692%2C1002.1535%2C1002.1638%2C1002.4875%2C1002.1197%2C1002.2976%2C1002.3369%2C1002.4251%2C1002.1856%2C1002.4087%2C1002.3672%2C1002.2253%2C1002.3584%2C1002.4281%2C1002.2254%2C1002.5020%2C1002.0755%2C1002.1808%2C1002.2112%2C1002.4036%2C1002.1851%2C1002.2816%2C1002.1305%2C1002.1357&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "An important issue in cosmology is reconstructing the effective dark energy\nequation of state directly from observations. With so few physically motivated\nmodels, future dark energy studies cannot only be based on constraining a dark\nenergy parameter space. We present a new non-parametric method which can\naccurately reconstruct a wide variety of dark energy behaviour with no prior\nassumptions about it. It is simple, quick and relatively accurate, and involves\nno expensive explorations of parameter space. The technique uses principal\ncomponent analysis and a combination of information criteria to identify real\nfeatures in the data, and tailors the fitting functions to pick up trends and\nsmooth over noise. We find that we can constrain a large variety of w(z) models\nto within 10-20 % at redshifts z<1 using just SNAP-quality data."}, "authors": ["Chris Clarkson", "Caroline Zunckel"], "author_detail": {"name": "Caroline Zunckel"}, "author": "Caroline Zunckel", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1103/PhysRevLett.104.211301", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1002.5004v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1002.5004v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "5 pages, 4 figures. v2 has added refs plus minor changes. To appear\n  in PRL", "arxiv_primary_category": {"term": "astro-ph.CO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "astro-ph.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1002.5004v2", "affiliation": "Princeton and KwaZulu-Natal", "arxiv_url": "http://arxiv.org/abs/1002.5004v2", "journal_reference": "Phys.Rev.Lett.104:211301,2010", "doi": "10.1103/PhysRevLett.104.211301", "fulltext": "Direct reconstruction of dark energy\nChris Clarkson1 and Caroline Zunckel2,3\nCentre for Astrophysics, Cosmology & Gravitation, and, Department of\nMathematics and Applied Mathematics, University of Cape Town, South Africa\n2\nAstrophysics Department, Princeton University, New Jersey, USA\n3\nAstrophysics and Cosmology Research Unit, University of KwaZulu-Natal, South Africa\n\narXiv:1002.5004v2 [astro-ph.CO] 4 May 2010\n\n1\n\nAn important issue in cosmology is reconstructing the effective dark energy equation of state\ndirectly from observations. With so few physically motivated models, future dark energy studies\ncannot only be based on constraining a dark energy parameter space. We present a new nonparametric method which can accurately reconstruct a wide variety of dark energy behaviour with\nno prior assumptions about it. It is simple, quick and relatively accurate, and involves no expensive\nexplorations of parameter space. The technique uses principal component analysis and a combination\nof information criteria to identify real features in the data, and tailors the fitting functions to pick\nup trends and smooth over noise. We find that we can constrain a large variety of w(z) models to\nwithin 10-20% at redshifts z . 1 using just SNAP-quality data.\n\nIntroduction The dark energy crisis in cosmology\nhighlights our incomprehension of what the universe actually consists of. Usually characterised by an effective\nequation of state function w in which we hide our lack of\nunderstanding, an important goal over the coming years\nwill be to try to understand this variable as a function\nof redshift, giving w(z). Theoretically we have little to\ngo on, other than the cosmological constant which has\nw = \u22121 for all time. Alternatives such as quintessence\nor modified gravity theories make predictions about how\nw diverges from \u22121 yet are often forced to parameterise\nfree functions [1]. More radically, proposals which modify\nthe radial distribution of matter on Hubble scales provide\nno a priori constrains on what the effective equation of\nstate (defined by matching up the distance indicator with\nan FLRW model) could be [2]. So, the forward problem\nof parameterising models, matching to data and discarding the fits which are poor or over-parameterised, suffers\nfrom a profound arbitrariness when we try to interpret\nthe errors: using simple smoothly varying functions of z\nseverely limit the departures from cosmological constant\nto a small range of models, but if we add more freedom to\nw the errors grow uncontrollably. Without encapsulating\nthe behaviour of w(z) which may actually exist, what are\nwe really constraining? Would a 'backwards' method be\nbetter? Can we instead reconstruct w from observations\ndirectly?\nThe dark energy equation of state is typically\n(re)constructed using distance measurements as a function of redshift. The luminosity\ndistance may be written\n\u0010\u221a\nR z 0 H0 \u0011\nc(1+z)\nas dL (z) = H \u221a\u2212\u03a9 sin\n\u2212\u03a9k 0 dz H(z0 ) , where H(z)\n0\n\nk\n\nis given by the Friedmann equation, H(z)2 = H02 {\u03a9m (1+\nRz\n0\n)\n0\nz)3 + \u03a9k (1 + z)2 + (1 \u2212 \u03a9m \u2212 \u03a9k ) exp [3 0 1+w(z\n1+z 0 dz ]},\nwhere H0 = H(0) and \u03a9m,k are the normalised density parameters. A common procedure is to postulate\na several parameter form for w(z) and calculate dL (z).\nThe most promising of these approaches uses a princi-\n\npal component analysis to construct the 'optimal' basis functions for w(z) based on the data [3]. An alternative method is to reconstruct w(z) by directly reconstructing the luminosity-distance curve. Writing D(z) =\n(H0 /c)(1 + z)\u22121 dL (z) as the normalised comoving distance, we have [5\u20137]:\nw(z) = {2(1 + z)(1 + \u03a9k D2 )D00 \u2212 [(1 + z)2 \u03a9k D02\n+2(1 + z)\u03a9k DD0 \u2212 3(1 + \u03a9k D2 )]D0 }/\n\n(1)\n\n{3{(1 + z)2 [\u03a9k + (1 + z)\u03a9m ]D02 \u2212 (1 + \u03a9k D2 )}D0 }.\nThus, given a distance-redshift curve D(z), we can reconstruct the dark energy equation of state, assuming\nwe know the density parameters \u03a9m and \u03a9k . Different\nmethods for doing this involve smoothing the data to give\nD(z), or parameterising D(z) by a function; see [4] for a\ncomprehensive review, and [9\u201315] for alternative model\nindependent approaches.\nThe direct reconstruction method is unstable because\nof the two derivatives of the observed function in eq.\n2, requiring the fitting function to accurately capture\nthe slope and concavity of luminosity distance curve.\nThis means that differences between the true underlying\nmodel and the fitted function due to the choice of parameterisation, are amplified drastically when reconstructing\nw. Furthermore, w is constructed from a quotient of\nfunctions which need to balance to obtain the correct w.\nHowever, there must exist a set of 'optimal' basis functions with which achieving this balance becomes possible\nand direct reconstruction feasible; in essence this is the\nsame as specifying 'the correct' parameterisation for dark\nenergy. Can we find such a basis?\nAt first sight it seems not: If we have no inherent intuition of the true w, how can we possibly guess the right\nform for D(z)? Of course, a large polynomial expansion\nwould work but at the expense of ludicrous errors. Furthermore, one can easily achieve a fit that is too good:\na \u03c72 is less than the value the actual underlying model\n\n\f2\n\nFIG. 1: Extracting w = \u22121. The first 10 eigenfunctions for \u039b are shown left. We take two separate reconstructions, using\nM = 4 and M = 5 (N = 10), and generate the errors on \u03bcresid via a monte carlo (with 100 samples each for clarity). The\ntwo reconstructions are combined with equal weight to give the 1-\u03c3 errors. The choice of number of eigenfunctions is not yet\nfavourable; beyond z \u223c 1.2 (not shown) the reconstruction is hopeless for these eigenfunctions.\n\nwould produce. Over-fitting to noisy data translates into\nwild behaviour in w.\nHere we present a method to find well adapted basis functions for fitting D(z) and a simple way to construct the errors on w. We first calculate the residuals of\nthe measured apparent magnitude around some fiducial\nmodel. Assuming some basis functions, we then calculate\nthe principal components of the residuals, and use those\nfixed principal components to provide a measure of the\ntrue D(z). Given the many possibilities, a combination\nof information criteria are used to select which encompass the information present in the data. Folding the\nerrors together appropriately produces a non-parametric\nmethod which reproduces w(z) together with an effective '1-\u03c3' confidence measure. A non-parametric method\ndoesn't produce confidence limits; rather a confidence interval of x% must be expected to trap the correct value\nx% of the time [16].\nBasis functions for distances Let's assume we have\nNd data points distributed at zi for the distance modulus \u03bc = 5 log10 dL (z) + 25, with Gaussian errors \u03c3i which\nare independent. We create \u03bcresid = \u03bcdata \u2212 \u03bcfiducial\nwhere the fiducial model is some predefined model, such\nas flat LCDM, an empty model, or EdS. A good choice is\nthe best-fit LCDM model, being consistent with current\ndata. Our goal is to construct \u03bcresid and two derivatives as accurately as possible. We choose a set of primary basis functions pn (z) such as pn (z) = z n\u22121 and use\nPN\nthe function\nn=1 an pn (z) as the basis to fit \u03bcresid (z)\nto data, for a fixed N . This is a linear fit, so can be\ndone easily, and the covariance matrix C calculated algorithmically without having to explore a complicated\nparameter space, or make assumptions about Gaussian\n\nerrors on the parameters. Now we perform a principal component analysis on the fit to find the best basis of functions as follows. We diagonalise the inverse\ncovariance matrix, and create a matrix of eigenvectors\nE. We then order them according to decreasing eigenvalue; i.e., so that the n'th column of E, en , is the eigenvector with the n'th largest eigenvalue, etc. Write emn\nas the m'th component of thePn'th eigenvector. Define\nN\nthe eigenfunctions Pn (z) =\nm=1 emn pm (z), for n \u2208\n[1, N ] which now form a new family of basis functions\nwhich are suitably adapted to the data. In matrix\nform P = E T p \u21d4 p = EP . These functions are\nnow\nwith respect to the errors on the data:\nPNd orthogonal\n\u22122\ni \u03c3i Pm (zi )Pn (zi ) = 0 if m 6= n. In other words, the\nFisher matrix in this basis is diagonal.\nqPWe can normalise\nNd \u22122\n2\nthe eigenfunctions P\u0302n (z) = Pn (z)/\ni \u03c3i Pn (zi ) , in\nwhich case the Fisher matrix will be the identity matrix. If we now refit to the data with new parameters for\nthese basis functions, the covariance matrix is also (very\nnearly) the identity matrix. For a fixed N , we may be\ninterested in the first M < N eigenfunctions which encompass the dominant features in the data, and throw\naway the higher ones which contain noise-induced oscillations.\nPracticalities and a test case - can we recover \u039b? To\ntest the method, we construct a hypothetical data set in\nline with expectations from the SNAP experiment, consisting of 2000 type 1a supernovae (SNIa) measurements\nevenly distributed in the redshift range z = 0.08 \u2212 1.7,\nand 300 local SNIa in 0.03 \u2212 0.08 [17]. The statistical uncertainty is conservatively estimated as \u03c3mag = 0.15mag\nand we include systematic errors, as a linear drift from\n\u03c3sys = 0 \u2212 0.02 [17]. We assume a true underly-\n\n\f3\n\nFIG. 2: Reconstruction of w = \u22121 using the CIC criteria, Eq. (4), with s = 0.2. All [N, M ] values satisfying Eq. (4) are monte\ncarlo-ed using 200 runs each (left); the results are bundled into a single probability distribution (middle) from which we infer\n1-\u03c3 errors at each redshift, shown left. For this example, the CIC selects for M = 2, 3 for each N . We show two probability\ndistributions for N = 10 (middle), as well as the combination for all N, M used. It is at the stage of bundling up the different\nN, M eigenmodes into one probability distribution where the method becomes non-parametric. The combination of different\neigenmodes leads to slight non-Gaussianity of the distribution. In this example, s can be increased all the way up to 1 which\nshrinks the errors even further, and allows tight constraints on w over the full range of z (right).\n\ning w and some parameters, \u03b6 true = {\u03a9m , \u03a9k , h}true =\n{0.3, 0.0, 0.65}. Our goal is to recover wtrue (z) in a robust way. We assume \u03b6 fiducial = \u03b6 true for now, and take\nwfiducial = \u22121. The actual model used for \u03bcfiducial is\nre-incorporated when evaluating w(z), but an incorrect\nchoice of the parameters for \u03b6 fiducial are subsumed by the\nusual uncertainties on those parameters. If we pick a\nfiducial model which is reasonably close to the true underlying cosmology, the residuals are predominantly noise\nand variations in w, requiring fewer fitting parameters\ngiving typically smaller errors. Errors from the incorrect\nchoice of \u03b6 will be considered elsewhere but are standard.\nHere, we consider only the reconstruction errors for clarity.\n\nThe first M of these eigenmodes are used as the new basis functions for \u03bcresid , and these are fitted to the data.\nIn the normalised basis the errors on\n\u221athe parameters are\nall Gaussian and unity (up to \u223c 1/ Nd ). Since the parameters are uncorrelated, the errors may be propagated\ninto \u03bcresid by a simple monte carlo for each parameter.\nThese curves then give a family of w(z) curves from which\nthe 1-\u03c3 error may be given. Errors on \u03b6 may be folded\nin at this stage and lead to larger error bars, though we\ndon't investigate this here. We show an example of this\nreconstruction procedure in Fig. 1, using M = 4 and 5,\nmixed together to form one set of error bars. There is\nnothing to say that these are good choices of M , an issue\nwe explore now.\n\nWe used na variety of primary basis functions\no such\nn\u22121\nn\u22121\nas pn (z) \u2208 z n\u22121 , [z/(1 + z)]\n, [1/(1 + z)]\n, with\nsimilar results, though we achieve smoother reconstructions as we move left to right in this list; we present\nour results using the middle one. For a fixed N the\neigenfunctions range from smooth with no turning points,\nto very oscillatory; typically, the n'th eigenmode crosses\nzero n \u2212 1 times. Roundoff error can cause problems for\nN & 10, which is signalled by C having non-zero diagonal elements for the normalised basis. (Strictly speaking,\nC differs from the identity matrix slightly, since we use\none realisation\n\u221a of the data: we would expect a change of\nroughly 1/ Nd for the diagonal elements.) For N > 10 a\nsingular valued decomposition could be used for the fits.\nIn Fig. 1 we show the first 10 normalised eigenfunctions\nwhen the underlying model is w = \u22121, with N = 10.\n\nSelection criteria The number of eigenfunctions to\nuse in the final reconstruction is critical as it determines the accuracy and size of the errors bars. Consider\na subset of [N, M ] with N \u2208 [2, 10] and M \u2208 [2, N ].\nEach choice [N, M ] will give a particular reconstruction\nof w(z), together with some errors. In the case where\nM = N , the original error bars are recovered and no information is thrown away. Reducing M is accompanied\nby a reduction in the error, but an increased chance of\ngetting w(z) wrong. How do we select the 'correct' set of\neigenfunctions? Choosing the combination [N, M ] leading to the smallest \u03c72 runs the risk of overfitting to noise.\nThe Risk may be used [3, 16], but requires the knowledge\nof the underlying value of w(z) a priori.\nInstead, we use a mixture of Akaike and Bayesian in-\n\n\f4\n\nFIG. 3: Reconstruction of evolving dark energy. With a choice of s = 0.2 and \u03ba = 5 we can reconstruct w to z \u223c 1 with\n\u223c 10-20% accuracy.\n\nformation criteria [18]:\nAIC = \u03c72min + 2M,\n\nBIC = \u03c72min + M ln Nd ,\n\n(2)\n\nwhere smaller values are assumed to imply a more\nfavoured model. The utility of these criteria over the\nRisk is that they are computed without knowing the underlying solution. We evaluated AIC and BIC values\ncorresponding to each [N, M ] combination for a number\nof test cases and found that minimizing these two criteria\nlead to dramatically different reconstructions that were\nusually not optimal. Typically, models with a very low\nBIC are too smooth with tight error bars, while those\nwith low AIC values are often too oscillatory and have\nlarge errors. A more adaptable requirement uses a combined information criteria which we define as:\nCIC = (1 \u2212 s) AIC + s BIC,\n\n(3)\n\nwhere s takes us from conservative models when s = 1 to\nmore wild models when s = 0. The parameter s thus mediates the competition between a smoother reconstruction (in which BIC is minimized) and one that is more\nfeatured (in which the AIC criterion is smallest).\nBut there is no reason to select one particular reconstruction; the minimum CIC is still no silver bullet. We\nfind a successful strategy is to select different [N, M ]\nchoices which are near the best values of CIC, for a given\ns, and amalgamate them at the monte carlo stage when\nwe compute the errors. We weight each [N, M ] choice\n\nequally. In this way, we reduce inherent bias which exists in any particular choice of [N, M ], even after the\nprinciple component analysis.\nAfter experimenting number of different w(z), we find\nthat the family of [N, M ] reconstructions which satisfy\nCIC < CICmin + \u03ba\n\n(4)\n\nwhere s is in the region of 0.2 and \u03ba = 5 yields very\nsolid results. Typically the BIC produces models with\nfew parameters and small errors and generally disfavours\nlarge variations in w(z) unless strongly warranted by the\ndata; the AIC on the other hand renders more featured\nreconstructions, at the expense of larger errors. We find\ns = 0.2 works well for the models we present below, balancing AIC and BIC. For alternative data sets, s and the\nchoice of \u03ba = 5 can be adjusted (e.g., \u03ba = 10 is more\nrobust).\nThe reconstructed cosmological constant discussed\nabove is shown in Fig 2. In this case where s = 0.2,\nthe reconstruction is good for z . 1, but degrades above\nz ' 1.4. For s & 0.8, however, the fits at high z improve dramatically with the reduced freedom in the fitting functions; this is comparable in complexity to fitting\na constant w, and so the errors are very tight. This result\nimproves significantly on previous non-parametric reconstructions of \u039b using SNAP-like data, such as in [10].\nResults In Fig. 3 we show the method in action for\ntwo very different types of w. One is a standard slow\n\n\f5\nw [20]. This results from using BIC as a selection criterion which yields conservative reconstructions, at the\nrisk of smoothing over real features in the data and underestimating the errors. For realistic constraints from\na given data set, simulated data of the same standard\nshould be examined first to quantify sensible choices for\ns and \u03ba.\n\nFIG. 4: Reconstruction of w using the constitution SNIa.\n\n\b\n\u0002\n\u0001\u0003\n; the\nevolution, given by w = 12 \u22121 + tanh 3 z \u2212 12\nother mimics the effective w(z) one finds in best fit\nvoid models of dark\nwe model by\n\u0002 energy [19], which\n\u0003\nw = 0.2 + 1.8 exp \u2212(z \u2212 0.4)2 /0.15 . We use s = 0.2\nand \u03b6 fiducial = \u03b6 true . We can see that the reconstruction\nis impressive, with errors \u223c 0.1. Above z \u223c 1 the errors\ngrow uncontrollably, leading to weak constraints despite\nthe large number of SNIa in this range. This is because\nconstant errors in \u03bc lead to strongly divergent errors in\nw in a non-parametric setting. We find that similar fits\nfor other w(z) models which have features on the same\nscales in z-space. We find that if the underlying w(z)\ncontains sharp features such that its derivative is large,\nthen a choice of s = 0.2 isn't sufficient, and the CIC\nneeds to be weighted more heavily towards AIC (smaller\ns), which increases the errors. Choosing s = 0.05 with\nNmax = 10 lets us reconstruct a step-like w with a step\nof width \u223c 0.1, or a Gaussian bump of about twice that\nwidth. Thus, we see that s (combined with the choice of\nlargest N , and a choice of \u03ba) sets the resolving scale for\nthe reconstruction and must be treated as a prior, representing one's intuition of the complexity of w(z). For\nexample, we can reconstruct w = \u22121 to high accuracy by\nusing s = 1, shown in Fig. 2 (right). This gives errors on\nw of less than 5% to z \u223c 1. This situation is analogous\nto the choice of eigenfunctions which minimise the risk\naround \u039b in [3].\nAs a final example we consider the w(z) we obtain\nfrom the Constitution data set [20]. To do this, we\nfirst find the best fit LCDM model, which gives \u03b6 =\n{0.32, \u22120.09, .653}, and we use this for both the residual \u03bc and for the model reconstruction. In Fig. 4 we\nshow the constraints on w(z) with different choices of s,\nand using \u03ba = 5. This serves as an illustration of the\nmethod and the effect s has, but errors on \u03b6 have not\nbeen folded in for clarity. Using s = 1 yields tight constraints on a par with those found assuming a constant\n\nDiscussion We have presented a novel method to perform a direct reconstruction of w(z), shown to be capable\nof constraining w to \u223c 0.1 \u2212 0.2 for z . 1 using SNAPquality data. The method capitalizes on the use of principal component analysis to find orthogonal bases with\nwhich to fit the data, and uses a combination of information criteria to construct a family of reasonably good\nfitting functions. Then, by combining the different fitting functions we find the 'real' structure present in the\ndata, and smooth over noise-induced features. All fitting\nis linear, so calculations and errors estimations are easy,\nwith no exploration of parameter space required. Future\nwork will incorporate errors from other parameters, and\nexplore an iterative approach to finding \u03bcresid based on\nthe best fit obtained.\nWe thank Mat Smith for discussions. CC is funded\nby the NRF (South Africa). CZ is funded by the PIRE\ngrant and the NRF (South Africa).\n\n[1] See, e.g., E. J. Copeland, M. Sami and S. Tsujikawa,\nhep-th/0603057 for a review.\n[2] C\u00e9l\u00e9rier, M. N. arXiv:astro-ph/0702416 (2007).\n[3] Huterer D., Starkman G.D., 2003, Phys. Rev. Lett., 90,\n031301\n[4] V. Sahni and A. Starobinsky, Int. J. Mod. Phys. D 15,\n2105 (2006)\n[5] A. A. Starobinsky, JETP Lett. 68, 757 (1998)\n[6] T. Nakamura and T. Chiba, MNRAS 306, 696 (1999)\n[7] D. Huterer and M. S. Turner, Phys. Rev. D 60, 081301\n(1999)\n[8] Saini, T.D et al., 2000, Phys. Rev. Lett., 85, 1162.\n[9] Weller J., Albrecht A., 2002, Phys. Rev., D65, 103512\n[10] U. Alam, V. Sahni, T. D. Saini and A. A. Starobinsky,\nMNRAS 344, 1057 (2003)\n[11] Daly, R.A., Djorgovski, S.G., 2003, Ap.J., 597, 9\n[12] U. Alam, V. Sahni and A. A. Starobinsky, JCAP 0406,\n008 (2004)\n[13] Y. Wang and M. Tegmark, Phys. Rev. Lett. 92, 241302\n(2004)\n[14] R. A. Daly and S. G. Djorgovski, AJ 612, 652 (2004)\n[15] Shafieloo, A. et al., 2006, Mon. Not. Roy. Astron. Soc.,\n366, 1081 Shafieloo, A. Mon. Not. Roy. Astron. Soc., 380,\n1573 (2007).\n[16] Wasserman, L. et. al. arXiv:astro-ph/0112050v1 (2001)\n[17] Aldering G., et al., 2004, astro-ph/0405232\n[18] Liddle, Mon.Not.Roy.Astron.Soc.Lett.377:L74-L78,2007\n[19] S. February et al. arXiv:0909.1479 (2009)\n[20] Hicken, M. et. al. Ap. J. 700,1097-1140 (2009)\n\n\f"}