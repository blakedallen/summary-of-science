{"id": "http://arxiv.org/abs/1103.0941v1", "guidislink": true, "updated": "2011-03-04T16:29:04Z", "updated_parsed": [2011, 3, 4, 16, 29, 4, 4, 63, 0], "published": "2011-03-04T16:29:04Z", "published_parsed": [2011, 3, 4, 16, 29, 4, 4, 63, 0], "title": "Estimating $\u03b2$-mixing coefficients", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1103.5093%2C1103.4113%2C1103.2938%2C1103.0857%2C1103.4683%2C1103.4143%2C1103.1362%2C1103.2087%2C1103.5540%2C1103.2121%2C1103.6149%2C1103.4409%2C1103.0761%2C1103.1728%2C1103.5477%2C1103.3599%2C1103.4673%2C1103.5971%2C1103.1495%2C1103.2262%2C1103.2858%2C1103.3711%2C1103.5625%2C1103.1038%2C1103.4437%2C1103.3884%2C1103.3217%2C1103.1752%2C1103.3686%2C1103.3517%2C1103.5153%2C1103.1455%2C1103.1837%2C1103.3472%2C1103.5357%2C1103.0909%2C1103.4199%2C1103.5487%2C1103.1701%2C1103.4668%2C1103.3996%2C1103.2512%2C1103.5047%2C1103.0488%2C1103.3736%2C1103.2122%2C1103.3784%2C1103.4678%2C1103.2687%2C1103.5458%2C1103.1558%2C1103.5874%2C1103.1722%2C1103.3705%2C1103.2193%2C1103.6197%2C1103.0941%2C1103.3573%2C1103.2690%2C1103.1847%2C1103.4624%2C1103.0372%2C1103.5571%2C1103.4819%2C1103.2808%2C1103.4408%2C1103.1465%2C1103.0574%2C1103.3587%2C1103.0822%2C1103.0180%2C1103.2246%2C1103.1334%2C1103.1673%2C1103.1983%2C1103.5806%2C1103.4774%2C1103.3112%2C1103.5401%2C1103.5964%2C1103.5508%2C1103.4693%2C1103.1403%2C1103.2366%2C1103.3523%2C1103.1231%2C1103.2925%2C1103.2874%2C1103.4441%2C1103.5219%2C1103.5087%2C1103.3273%2C1103.1503%2C1103.2482%2C1103.1905%2C1103.4357%2C1103.0038%2C1103.6008%2C1103.1964%2C1103.3957%2C1103.5757&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Estimating $\u03b2$-mixing coefficients"}, "summary": "The literature on statistical learning for time series assumes the asymptotic\nindependence or ``mixing' of the data-generating process. These mixing\nassumptions are never tested, nor are there methods for estimating mixing rates\nfrom data. We give an estimator for the $\\beta$-mixing rate based on a single\nstationary sample path and show it is $L_1$-risk consistent.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1103.5093%2C1103.4113%2C1103.2938%2C1103.0857%2C1103.4683%2C1103.4143%2C1103.1362%2C1103.2087%2C1103.5540%2C1103.2121%2C1103.6149%2C1103.4409%2C1103.0761%2C1103.1728%2C1103.5477%2C1103.3599%2C1103.4673%2C1103.5971%2C1103.1495%2C1103.2262%2C1103.2858%2C1103.3711%2C1103.5625%2C1103.1038%2C1103.4437%2C1103.3884%2C1103.3217%2C1103.1752%2C1103.3686%2C1103.3517%2C1103.5153%2C1103.1455%2C1103.1837%2C1103.3472%2C1103.5357%2C1103.0909%2C1103.4199%2C1103.5487%2C1103.1701%2C1103.4668%2C1103.3996%2C1103.2512%2C1103.5047%2C1103.0488%2C1103.3736%2C1103.2122%2C1103.3784%2C1103.4678%2C1103.2687%2C1103.5458%2C1103.1558%2C1103.5874%2C1103.1722%2C1103.3705%2C1103.2193%2C1103.6197%2C1103.0941%2C1103.3573%2C1103.2690%2C1103.1847%2C1103.4624%2C1103.0372%2C1103.5571%2C1103.4819%2C1103.2808%2C1103.4408%2C1103.1465%2C1103.0574%2C1103.3587%2C1103.0822%2C1103.0180%2C1103.2246%2C1103.1334%2C1103.1673%2C1103.1983%2C1103.5806%2C1103.4774%2C1103.3112%2C1103.5401%2C1103.5964%2C1103.5508%2C1103.4693%2C1103.1403%2C1103.2366%2C1103.3523%2C1103.1231%2C1103.2925%2C1103.2874%2C1103.4441%2C1103.5219%2C1103.5087%2C1103.3273%2C1103.1503%2C1103.2482%2C1103.1905%2C1103.4357%2C1103.0038%2C1103.6008%2C1103.1964%2C1103.3957%2C1103.5757&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The literature on statistical learning for time series assumes the asymptotic\nindependence or ``mixing' of the data-generating process. These mixing\nassumptions are never tested, nor are there methods for estimating mixing rates\nfrom data. We give an estimator for the $\\beta$-mixing rate based on a single\nstationary sample path and show it is $L_1$-risk consistent."}, "authors": ["Daniel J. McDonald", "Cosma Rohilla Shalizi", "Mark Schervish"], "author_detail": {"name": "Mark Schervish"}, "author": "Mark Schervish", "arxiv_comment": "9 pages, accepted by AIStats. CMU Statistics Technical Report", "links": [{"href": "http://arxiv.org/abs/1103.0941v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1103.0941v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1103.0941v1", "affiliation": "Carnegie Mellon University", "arxiv_url": "http://arxiv.org/abs/1103.0941v1", "journal_reference": null, "doi": null, "fulltext": "Estimating \u03b2-mixing coefficients\n\narXiv:1103.0941v1 [stat.ML] 4 Mar 2011\n\nDaniel J. McDonald\nCarnegie Mellon University\n\nCosma Rohilla Shalizi\nCarnegie Mellon University\nSanta Fe Institute\n\nAbstract\nThe literature on statistical learning for time\nseries assumes the asymptotic independence\nor \"mixing' of the data-generating process.\nThese mixing assumptions are never tested,\nnor are there methods for estimating mixing\nrates from data. We give an estimator for\nthe \u03b2-mixing rate based on a single stationary\nsample path and show it is L1 -risk consistent.\n\n1\n\nIntroduction\n\nRelaxing the assumption of independence is an active\narea of research in the statistics and machine learning\nliterature. For time series, independence is replaced\nby the asymptotic independence of events far apart\nin time, or \"mixing\". Mixing conditions make the dependence of the future on the past explicit, quantifying\nthe decay in dependence as the future moves farther\nfrom the past. There are many definitions of mixing\nof varying strength with matching dependence coefficients (see [8, 6, 3] for reviews), but most of the results\nin the learning literature focus on \u03b2-mixing or absolute\nregularity. Roughly speaking (see Definition 2.1 below\nfor a precise statement), the \u03b2-mixing coefficient at\nlag a is the total variation distance between the actual\njoint distribution of events separated by a time steps\nand the product of their marginal distributions, i.e.,\nthe L1 distance from independence.\nNumerous results in the statistical machine learning\nliterature rely on knowledge of the \u03b2-mixing coefficients. As Vidyasagar [24, p. 41] notes, \u03b2-mixing is\n\"just right\" for the extension of IID results to dependent data, and so recent work has consistently\nfocused on it. Meir [14] derives generalization error\nbounds for nonparametric methods based on model seAppearing in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)\n2011, Ft. Lauderdale, FL, USA. Copyright 2011 by the\nauthors.\n\nMark Schervish\nCarnegie Mellon University\n\nlection via structural risk minimization. Baraud et al.\n[1] study the finite sample risk performance of penalized least squares regression estimators under \u03b2mixing. Lozano et al. [12] examine regularized boosting algorithms under absolute regularity and prove\nconsistency. Karandikar and Vidyasagar [11] consider\n\"probably approximately correct\" learning algorithms,\nproving that PAC algorithms for IID inputs remain\nPAC with \u03b2-mixing inputs under some mild conditions. Ralaivola et al. [19] derive PAC bounds for\nranking statistics and classifiers using a decomposition\nof the dependency graph. Finally, Mohri and Rostamizadeh [15] derive stability bounds for \u03b2-mixing\ninputs, generalizing existing stability results for IID\ndata.\nAll these results assume not just \u03b2-mixing, but known\nmixing coefficients. In particular, the risk bounds\nin [14, 15] and [19] are incalculable without knowledge of the rates. This knowledge is never available.\nUnless researchers are willing to assume specific values for a sequence of \u03b2-mixing coefficients, the results\nmentioned in the previous paragraph are generally useless when confronted with data.To illustrate this deficiency, consider Theorem 18 of [15]:\nTheorem 1.1 (Briefly). Assume a learning algorithm\nis \u03bb-stable. Then, for any sample of size n drawn from\na stationary \u03b2-mixing distribution, and \u01eb > 0\nb > \u01eb) \u2264 \u0393(n, \u03bb, \u01eb, a, b) + \u03b2(a)(\u03bcn \u2212 1)\nP(|R \u2212 R|\n\nwhere n = (a + b)\u03bcn , \u0393 has a particular functional\nb is the difference between the true risk\nform, and R \u2212 R\nand the empirical risk.\n\nIdeally, one could use this result for model selection\nor to control the size of the generalization error of\ncompeting prediction algorithms (support vector machines, support vector regression, and kernel ridge regression are a few of the many algorithms known to\nsatisfy \u03bb-stability). However the bound depends explicitly on the mixing coefficient \u03b2(a). To make matters worse, there are no methods for estimating the\n\u03b2-mixing coefficients. According to Meir [14, p. 7],\n\"there is no efficient practical approach known at this\n\n\fEstimating \u03b2-mixing coefficients\n\nstage for estimation of mixing parameters.\" We begin\nto rectify this problem by deriving the first method for\nestimating these coefficients. We prove that our estimator is consistent for arbitrary \u03b2-mixing processes.\nIn addition, we derive rates of convergence for Markov\napproximations to these processes.\nApplication of statistical learning results to \u03b2-mixing\ndata is highly desirable in applied work. Many common time series models are known to be \u03b2-mixing,\nand the rates of decay are known given the true parameters of the process. Among the processes for\nwhich such knowledge is available are ARMA models [16], GARCH models [4], and certain Markov processes - see [8] for an overview of such results. To\nour knowledge, only Nobel [17] approaches a solution\nto the problem of estimating mixing rates by giving\na method to distinguish between different polynomial\nmixing rate regimes through hypothesis testing.\nWe present the first method for estimating the \u03b2mixing coefficients for stationary time series data. Section 2 defines the \u03b2-mixing coefficient and states our\nmain results on convergence rates and consistency for\nour estimator. Section 3 gives an intermediate result\non the L1 convergence of the histogram estimator with\n\u03b2-mixing inputs. Section 4 proves the main results\nfrom \u00a72. Section 5 concludes and lays out some avenues for future research.\n\n2\n\nEstimation of \u03b2-mixing\n\nIn this section, we present one of many equivalent definitions of absolute regularity and state our main results, deferring proof to \u00a74.\nTo fix notation, let X = {Xt }\u221e\nt=\u2212\u221e be a sequence of\nrandom variables where each Xt is a measurable function from a probability space (\u03a9, F , P) into a measurable space X . A block of this random sequence will\nbe given by Xji \u2261 {Xt }jt=i where i and j are integers,\nand may be infinite. We use similar notation for the\nsigma fields generated by these blocks and their joint\ndistributions. In particular, \u03c3ij will denote the sigma\nfield generated by Xji , and the joint distribution of Xji\nwill be denoted Pji .\n2.1\n\nDefinitions\n\nThere are many equivalent definitions of \u03b2-mixing (see\nfor instance [8], or [3] as well as Meir [14] or Yu [27]),\nhowever the most intuitive is that given in Doukhan\n[8].\nDefinition 2.1 (\u03b2-mixing). For each positive integer a, the the coefficient of absolute regularity, or \u03b2-\n\nmixing coefficient, \u03b2(a), is\n\u03b2(a) \u2261 sup Pt\u2212\u221e \u2297 P\u221e\nt+a \u2212 Pt,a\nt\n\nTV\n\n(1)\n\nwhere || * ||T V is the total variation norm, and Pt,a is\nthe joint distribution of (Xt\u2212\u221e , X\u221e\nt+a ). A stochastic\nprocess is said to be absolutely regular, or \u03b2-mixing,\nif \u03b2(a) \u2192 0 as a \u2192 \u221e.\nLoosely speaking, Definition 2.1 says that the coefficient \u03b2(a) measures the total variation distance between the joint distribution of random variables seaparted by a time units and a distribution under which\nrandom variables separated by a time units are independent. The supremum over t is unnecessary for\nstationary random processes X which is the only case\nwe consider here.\nDefinition 2.2 (Stationarity). A sequence of random variables X is stationary when all its finitedimensional distributions are invariant over time: for\nall t and all non-negative integers i and j, the random\nvectors Xt+i\nand Xt+i+j\nhave the same distribution.\nt\nt+j\nOur main result requires the method of blocking used\nby Yu [26, 27]. The purpose is to transform a sequence\nof dependent variables into subsequence of nearly IID\nones. Consider a sample Xn1 from a stationary \u03b2mixing sequence with density f . Let mn and \u03bcn be\nnon-negative integers such that 2mn \u03bcn = n. Now divide Xn1 into 2\u03bcn blocks of each length mn . Identify\nthe blocks as follows:\nUj = {Xi : 2(j \u2212 1)mn + 1 \u2264 i \u2264 (2j \u2212 1)mn },\nVj = {Xi : (2j \u2212 1)mn + 1 \u2264 i \u2264 2jmn }.\n\nLet U be the entire sequence of odd blocks Uj , and let\nV be the sequence of even blocks Vj . Finally, let U\u2032\nbe a sequence of blocks which are independent of Xn1\nbut such that each block has the same distribution as\na block from the original sequence:\nD\n\nD\n\nUj\u2032 = Uj = U1 .\n\n(2)\n\nThe blocks U\u2032 are now an IID block sequence, so standard results apply. (See [27] for a more rigorous analysis of blocking.) With this structure, we can state our\nmain result.\n2.2\n\nResults\n\nOur main result emerges in two stages. First, we recognize that the distribution of a finite sample depends\nonly on finite-dimensional distributions. This leads to\nan estimator of a finite-dimensional version of \u03b2(a).\nNext, we let the finite-dimension increase to infinity\nwith the size of the observed sample.\n\n\fDaniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish\n\nFor positive integers t, d, and a, define\nt+a+d\u22121\n\u03b2 d (a) = Ptt\u2212d+1 \u2297 Pt+a\n\u2212 Pt,a,d\n\ngers such that 2\u03bcn mn = n and \u03bcn \u2265 d > 0. Then\nTV\n\n,\n\nwhere Pt,a,d is the joint distribution of\nt+a+d\u22121\n(Xtt+d+1 , Xt+a\n). Also, let fbd be the d-dimensional\nhistogram estimator of the joint density of d consecutive observations, and let fba2d be the 2d-dimensional\nhistogram estimator of the joint density of two sets of\nd consecutive observations separated by a time points.\nWe construct an estimator of \u03b2 d (a) based on these two\nhistograms.1 Define\nZ\n1\n\u03b2bd (a) =\nfba2d \u2212 fbd \u2297 fbd\n(4)\n2\nWe show that, by allowing d = dn to grow with n,\nthis estimator will converge on \u03b2(a). This can be seen\nmost clearly by bounding the L1 -risk of the estimator\nwith its estimation and approximation errors:\n|\u03b2bdn \u2212 \u03b2(a)| \u2264 |\u03b2bdn \u2212 \u03b2 dn | + |\u03b2 dn \u2212 \u03b2(a)|.\n\nThe first term is the error of estimating \u03b2 d (a) with a\nrandom sample of data. The second term is the nonstochastic error induced by approximating the infinite\ndimensional coefficient, \u03b2(a), with its d-dimensional\ncounterpart, \u03b2 d (a).\nOur first theorem in this section establishes consistency of \u03b2bdn (a) as an estimator of \u03b2(a) for all \u03b2-mixing\nprocesses provided dn increases at an appropriate rate.\nTheorem 2.4 gives finite sample bounds on the estimation error while some measure theoretic arguments\ncontained in \u00a74 show that the approximation error\nmust go to zero as dn \u2192 \u221e.\nTheorem 2.3. Let Xn1 be a sample from an arbitrary\n\u03b2-mixing process. Let dn = O(exp{W (log n)}) where\nP\n\u2192 \u03b2(a)\nW is the Lambert W function.2 Then \u03b2bdn (a) \u2212\nas n \u2192 \u221e.\n\nA finite sample bound for the approximation error is\nthe first step to establishing consistency for \u03b2bdn . This\nresult gives convergence rates for estimation of the finite dimensional mixing coefficient \u03b2 d (a) and also for\nMarkov processes of known order d, since in this case,\n\u03b2 d (a) = \u03b2(a).\nTheorem 2.4. Consider a sample Xn1 from a stationary \u03b2-mixing process. Let \u03bcn and mn be positive inte1\n\nP(|\u03b2bd (a) \u2212 \u03b2 d (a)| > \u01eb)\n\u001a\n\u001b\n\u001a\n\u001b\n\u03bcn \u01eb21\n\u03bcn \u01eb22\n\u2264 2 exp \u2212\n+ 2 exp \u2212\n2\n2\n\n(3)\n\nWhile it is clearly possible to replace histograms with\nother choices of density estimators (most notably KDEs),\nhistograms in this case are more convenient theoretically\nand computationally. See \u00a75 for more details.\n2\nThe Lambert W function is defined as the (multivalued) inverse of f (w) = w exp{w}.\nThus,\nO(exp{W (log n)}) is bigger than O(log log n) but smaller\nthan O(log n). See for example Corless et al. [5].\n\n+ 4(\u03bcn \u2212 1)\u03b2(mn ),\ni\nhR\nwhere \u01eb1 = \u01eb/2 \u2212 E |fbd \u2212 f d | and \u01eb2 = \u01eb \u2212\nhR\ni\nE |fba2d \u2212 fa2d | .\n\nConsistency of the estimator \u03b2bd (a) is guaranteed only\nfor certain choices of mn and \u03bcn . Clearly \u03bcn \u2192 \u221e\nand \u03bcn \u03b2(mn ) \u2192 0 as n \u2192 \u221e are necessary conditions.\nConsistency also requires convergence of the histogram\nestimators to the target densities. We leave the proof\nof this theorem for section 4. As an example to show\nthat this bound can go to zero with proper choices of\nmn and \u03bcn , the following corollary proves consistency\nfor first order Markov processes. Consistency of the\nestimator for higher order Markov processes can be\nproven similarly. These processes are algebraically \u03b2mixing as shown in e.g. Nummelin and Tuominen [18].\nCorollary 2.5. Let Xn1 be a sample from a first order\nMarkov process with \u03b2(a) = \u03b2 1 (a) = O(a\u2212r ). Then\nP\n\u2192 \u03b2(a).\nunder the conditions of Theorem 2.4, \u03b2b1 (a) \u2212\n\nProof. Recall that n = 2\u03bcn mn . Then,\n\n4(\u03bcn \u2212 1)\u03b2(mn ) = 4\u03bcn \u03b2(mn ) + 4\u03b2(mn )\nn \u2212r\n= K1\nm + K2 m\u2212r\nn\nmn n\n\u21920\nif mn < n1/(1+r) for constants K1 and K2 . In this\ncase, we have that the exponential terms are less than\n)\n(\no\nn\nn\u01eb2j\nexp \u2212K3 1/(1+r) = exp \u2212K3 nr/(1+r) \u01eb2j ,\nn\nfor j = 1, 2 and a constant K3 . Therefore, both exponential terms go to 0 as n \u2192 \u221e.\nProving Theorem 2.4 requires showing the L1 convergence of the histogram density estimator with \u03b2mixing data. We do this in the next section.\n\n3\n\nL1 convergence of histograms\n\nConvergence of density estimators is thoroughly studied in the statistics and machine learning literature.\nEarly papers on the L\u221e convergence of kernel density\nestimators (KDEs) include [25, 2, 21]; Freedman and\nDiaconis [9] look specifically at histogram estimators,\n\n\fEstimating \u03b2-mixing coefficients\n\nand Yu [26] considered the L\u221e convergence of KDEs\nfor \u03b2-mixing data and shows that the optimal IID rates\ncan be attained. Devroye and Gy\u00f6rfi [7] argue that L1\nis a more appropriate metric for studying density estimation, and Tran [22] proves L1 consistency of KDEs\nunder \u03b1- and \u03b2-mixing. As far as we are aware, ours is\nthe first proof of L1 convergence for histograms under\n\u03b2-mixing.\nAdditionally, the dimensionality of the target density\nis analogous to the order of the Markov approximation. Therefore, the convergence rates we give are\nasymptotic in the bandwidth hn which shrinks as n\nincreases, but also in the dimension d which increases\nwith n. Even under these asymptotics, histogram estimation in this sense is not a high dimensional problem.\nThe dimension of the target density considered here is\non the order of exp{W (log n)}, a rate somewhere between log n and log log n.\nTheorem 3.1. If fb is the histogram estimator based\non a (possibly vector valued) sample Xn1 from a \u03b2mixingh sequenceiwith stationary density f , then for all\nR\n\u01eb > E |fb \u2212 f | ,\nP\n\n\u0012Z\n\n|fb \u2212 f | > \u01eb\n\nwhere \u01eb1 = \u01eb \u2212 E\n\nhR\n\n\u0013\n\n\u001b\n\u001a\n\u03bcn \u01eb21\n\u2264 2 exp \u2212\n2\n\n+ 2(\u03bcn \u2212 1)\u03b2(mn )\n\n(5)\n\ni\n|fb \u2212 f | .\n\nTo prove this result, we use the blocking method of Yu\n[27] to transform the dependent \u03b2-mixing into a sequence of nearly independent blocks. We then apply\nMcDiarmid's inequality to the blocks to derive asymptotics in the bandwidth of the histogram as well as the\ndimension of the target density. For completeness, we\nstate Yu's blocking result and McDiarmid's inequality\nbefore proving the doubly asymptotic histogram convergence for IID data. Combining these lemmas allows\nus to derive rates of convergence for histograms based\non \u03b2-mixing inputs.\nLemma 3.2 (Lemma 4.1 in Yu [27]). Let \u03c6 be a measurable function with respect to the block sequence U\nuniformly bounded by M . Then,\n|E[\u03c6] \u2212 \u1ebc[\u03c6]| \u2264 M \u03b2(mn )(\u03bcn \u2212 1),\n\n(6)\n\nwhere the first expectation is with respect to the dependent block sequence, U, and \u1ebc is with respect to the\nindependent sequence, U\u2032 .\nThis lemma essentially gives a method of applying IID\nresults to \u03b2-mixing data. Because the dependence decays as we increase the separation between blocks,\nwidely spaced blocks are nearly independent of each\n\nother. In particular, the difference between expectations over these nearly independent blocks and expectations over blocks which are actually independent can\nbe controlled by the \u03b2-mixing coefficient.\nLemma 3.3 (McDiarmid Inequality [13]). Let\nX1 , . . . , Xn be independent random variables, with Xi\ntaking values in a set AQ\ni for each i. Suppose that the\nmeasurable function f : Ai \u2192 R satisfies\n|f (x) \u2212 f (x\u2032 )| \u2264 ci\n\nwhenever the vectors x and x\u2032 differ only in the ith\ncoordinate. Then for any \u01eb > 0,\n\u001a\n\u001b\n2\u01eb2\nP(f \u2212 Ef > \u01eb) \u2264 exp \u2212 P 2 .\nci\n\nLemma 3.4. For an IID sample X1 , . . . , Xn from\nsome density f on Rd ,\n\u0012 q\n\u0013\nZ\nd\nb\nb\nE |f \u2212 Ef |dx = O 1/ nhn\n(7)\nZ\n|Efb \u2212 f |dx = O(dhn ) + O(d2 h2n ),\n(8)\nwhere fb is the histogram estimate using a grid with\nsides of length hn .\n\nProof of Lemma 3.4. Let pj be the probability of\nfalling into the j th bin Bj . Then,\nE\n\nZ\n\nJ\nX\n\nn\n\npj\n1 X\nI(Xi \u2208 Bj ) \u2212 d\nd\nnh\nh\nn i=1\nj=1\nv \"\n#\nu\nJ\nn\nX\nu X\n1\ntV\n\u2264 hdn\nI(Xi \u2208 Bj )\nd\nnh\nn\nj=1\ni=1\n\n|fb \u2212 Efb| = hdn\n\n= hdn\n\nE\n\nJ\nq\nX\n1\nnpj (1 \u2212 pj )\nnhdn\nj=1\nJ\n\n1 X\n=\u221a\nn j=1\n\nq\npj (1 \u2212 pj )\n\n\u0012 q\n\u0013\nd .\n= O(n\u22121/2 )O(h\u2212d/2\n)\n=\nO\n1/\nnh\nn\nn\nFor the second claim, consider the bin Bj centered at\nc. Let I be the union of all bins Bj . Assume the\nfollowing:\n1. f \u2208 L2 and f is absolutely continuous on I, with\n\u2202\na.e. partial derivatives fi = \u2202y\nf (y)\ni\n2. fi \u2208 L2 and fi is absolutely continuous on I, with\na.e. partial derivatives fik = \u2202y\u2202 k fi (y)\n3. fik \u2208 L2 for all i, k.\n\n\fDaniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish\n\nUsing a Taylor expansion\nd\nX\n\n(xi \u2212 ci )fi (c) + O(d2 h2n ),\n\n\u2202\n\u2202yi f (y).\n\nTherefore, pj is given by\n\nf (x) = f (c) +\n\ni=1\n\nwhere fi (y) =\npj =\n\nZ\n\nBj\n\nfbn = 12 (fbU + fbV ). Now,\nP(g > \u01eb) = P\n\nd\nX\ni=1\n\n(xi \u2212 ci )fi (c) + O(d2 h2n ).\n\nTherefore,\nZ\n\nBj\n\n=\n\nZ\n\nEfbn (x) \u2212 f (x)\n\nBj\n\n\u2264\n=\n\nZ\n\nBj\n\nZ\n\nBj\n\n\u2212\n\n\u2212\n\nd\nX\ni=1\n\nd\nX\ni=1\n\nd\nX\ni=1\n\n(xi \u2212 ci )fi (c) + O(d2 h2n )\n(xi \u2212 ci )fi (c) +\n\nZ\n\nO(d2 h2 )\n\nBj\n\n(xi \u2212 ci )fi (c) + O(d2 h2+d\nn )\n\n2 2+d\n= O(dhd+1\nn ) + O(d hn )\n\nSince each bin is bounded, we can sum over all J bins.\nThe number of bins is J = h\u2212d\nn by definition, so\nZ\n\n|Efbn (x) \u2212 f (x)|dx\n\n\u0001\nd+1\n2 2+d\n= O(h\u2212d\nn ) O(dhn ) + O(d hn )\n= O(dhn ) + O(d2 h2n ).\n\n\u2264 P(gU > \u01eb) + P(gV > \u01eb)\n= 2P(gU \u2212 E[gU ] > \u01eb \u2212 E[gU ])\n\n= 2P(gU \u2212 E[gU\u2032 ] > \u01eb \u2212 E[gU\u2032 ])\n= 2P(gU \u2212 E[gU\u2032 ] > \u01eb1 ),\n\nwhere \u01eb1 = \u01eb \u2212 E[gU\u2032 ]. Here,\nZ\nZ\nb\nb\nE[gU\u2032 ] \u2264 \u1ebc |fU\u2032 \u2212 \u1ebcfU\u2032 |dx + |\u1ebcfbU\u2032 \u2212 f |dx,\n\nso by Lemma 3.4, as long as for \u03bcn \u2192 \u221e, hn \u2193 0 and\n\u03bcn hdn \u2192 \u221e, then for all \u01eb there exists n0 (\u01eb) such that\nfor all n > n0 (\u01eb), \u01eb > E[g] = E[gU\u2032 ]. Now applying\nLemma 3.2 to the expectation of the indicator of the\nevent {gU \u2212 E[gU\u2032 ] > \u01eb1 } gives\n2P(gU \u2212 E[gU\u2032 ] > \u01eb1 ) \u2264 2P(gU\u2032 \u2212 E[gU\u2032 ] > \u01eb1 )\n+ 2(\u03bcn \u2212 1)\u03b2(mn )\nwhere the probability on the right is for the \u03c3-field generated by the independent block sequence U\u2032 . Since\nthese blocks are independent, showing that gU\u2032 satisfies the bounded differences requirement allows for\nthe application of McDiarmid's inequality 3.3 to the\nblocks. For any two block sequences u\u20321 , . . . , u\u2032\u03bcn and\n\u016b\u20321 , . . . , \u016b\u2032\u03bcn with u\u2032l = \u016b\u2032l for all l 6= j, then\ngU\u2032 (u\u20321 , . . . , u\u2032\u03bcn ) \u2212 gU\u2032 (\u016b\u20321 , . . . , \u016b\u2032\u03bcn )\nZ\n=\n|fb(y; u\u20321 , . . . , u\u2032\u03bcn ) \u2212 f (y)|dy\nZ\n\u2212\n|fb(y; \u016b\u20321 , . . . , \u016b\u2032\u03bcn ) \u2212 f (y)|dy\nZ\n\u2264 |fb(y; u\u20321 , . . . , u\u2032\u03bcn ) \u2212 fb(y; \u016b\u20321 , . . . , \u016b\u2032\u03bcn )|dy\n=\n\nWe can now prove the main result of this section.\n\n\u0013\n\n= P(gU + gV > 2\u01eb)\n\nsince the integral of the second term over the bin is\nzero. This means that for the j th bin,\n\n=\u2212\n\n|f \u2212 fbn | > \u01eb\n\n!\nf \u2212 fbV\nf \u2212 fbU\n+\n>\u01eb\n=P\n2\n2\n\u0013\n\u0012 Z\nZ\n1\n1\nb\nb\n|f \u2212 fU | +\n|f \u2212 fV | > \u01eb\n\u2264P\n2\n2\nZ\n\nf (x)dx = hdn f (c) + O(d2 hd+2\nn )\n\npj\nEfbn (x) \u2212 f (x) = d \u2212 f (x)\nhn\n\n\u0012Z\n\n2\n2\nhdn =\n.\nd\n\u03bcn h n\n\u03bcn\n\nTherefore,\nProof of Theorem 3.1. LetR g be the L1 loss of the histogram\nestimator, g = |f \u2212 fbn |. Here fbn (x) =\n1 Pn\ni=1 I(Xi \u2208 Bj (x)) where Bj (x) is the bin connhd\nn\ntaining x. Let fbU , fbV , and fbU\u2032 be histograms based on\nthe block sequences U, V, and U\u2032 respectively. Clearly\n\nP(g > \u01eb) \u2264 2P(gU\u2032 \u2212 E[gU\u2032 ] > \u01eb1 ) + 2(\u03bcn \u2212 1)\u03b2(mn )\n\u001b\n\u001a\n\u03bcn \u01eb21\n+ 2(\u03bcn \u2212 1)\u03b2(mn ).\n\u2264 2 exp \u2212\n2\n\n\fEstimating \u03b2-mixing coefficients\n\n4\n\nProofs\n\nThe proof of Theorem 2.4 relies on the triangle inequality and the relationship between total variation\ndistance and the L1 distance between densities.\nProof of Theorem 2.4. For any probability measures \u03bd\nand \u03bb defined on the same probability space with associated densities f\u03bd and f\u03bb with respect to some dominating measure \u03c0,\nZ\n1\n||\u03bd \u2212 \u03bb||T V =\n|f\u03bd \u2212 f\u03bb |d(\u03c0).\n2\nLet P be the d-dimensional stationary distribution of\nthe dth order Markov process, i.e. P = Ptt\u2212d+1 =\nt+a+d\u22121\nPt+a\nin the notation of equation 3. Let Pa,d be\nthe joint distribution of the bivariate random process\ncreated by the initial process and itself separated by a\ntime steps. By the triangle inequality, we can upper\nb a,d be the\nbound \u03b2 d (a) for any d = dn . Let Pb and P\ndistributions associated with histogram estimators fbd\nand fba2d respectively. Then,\n\u03b2 d (a) = ||P \u2297 P \u2212 Pa,d ||T V\n\n= P \u2297 P \u2212 Pb \u2297 Pb + Pb \u2297 Pb\n\nba,d + P\nb a,d \u2212 Pa,d\n\u2212 P\n\u2264 P \u2297 P \u2212 Pb \u2297 Pb\n\nb a,d \u2212 Pa,d\n+ P\n\u2264 2 P \u2212 Pb\n\nTV\n\nTV\n\nba,d\n+ Pb \u2297 Pb \u2212 P\n\nb a,d\n+ Pb \u2297 Pb \u2212 P\n\nTV\n\nZ\n\n1\n|f d \u2212 fbd | +\n2\n\nZ\n\n|fa2d \u2212 fba2d |.\n\nhR\n\n|fbd \u2212 f d |\n\ni\n\nand \u01eb2 = \u01eb \u2212\n\nThe proof of Theorem 2.3 requires two steps which are\ngiven in the following Lemmas. The first specifies the\nhistogram bandwidth hn and the rate at which dn (the\ndimensionality of the target density) goes to infinity. If\nthe dimensionality of the target density were fixed, we\ncould achieve rates of convergence similar to those for\nhistograms based on IID inputs. However, we wish to\nallow the dimensionality to grow with n, so the rates\nare much slower as shown in the following lemma.\nthe histogram\n\nestimator\n\nin\n\ndn \u223c exp{W (log n)},\n\nhn \u223c n\u2212kn ,\nwith\n\nA similar argument starting from \u03b2 d (a)\n=\n||P \u2297 P \u2212 Pa,d ||T V shows that\nZ\nZ\n1\n\u03b2 d (a) \u2212 \u03b2bd (a) \u2265 \u2212 |f d \u2212 fbd | \u2212\n|fa2d \u2212 fba2d |,\n2\n\u03b2 d (a) \u2212 \u03b2bd (a) \u2264\n\nwhere \u01eb1 = \u01eb/2 \u2212 E\nhR\ni\nE |fba2d \u2212 fa2d | .\n\nTV\n\nb a,d \u2212 Pa,d\n+ P\nTV\nZ\nZ\n1\nd\nd\nb\n|fbd \u2297 fbd \u2212 fba2d |\n= |f \u2212 f | +\n2\nZ\n1\n|fa2d \u2212 fba2d |\n+\n2\nR\nwhere 21 |fbd \u2297 fbd \u2212 fba2d | is our estimator \u03b2bd (a) and the\nremaining terms are the L1 distance between a density\nestimator and the target density. Thus,\nZ\nZ\n1\nd\nd\nd\nd\nb\nb\n|fa2d \u2212 fba2d |.\n\u03b2 (a) \u2212 \u03b2 (a) \u2264 |f \u2212 f | +\n2\n\nso we have that\n\n+ 4(\u03bcn \u2212 1)\u03b2(mn ),\n\nLemma 4.1. For\nLemma 3.4, let\n\nTV\n\nTV\n\nTherefore,\n\u0011\n\u0010\nP \u03b2 d (a) \u2212 \u03b2bd (a) > \u01eb\n\u0012Z\n\u0013\nZ\n1\n\u2264P\n|f d \u2212 fbd | +\n|fa2d \u2212 fba2d | > \u01eb\n2\n\u0013\n\u0012 Z\n\u0013\n\u0012Z\n1\n\u01eb\n\u01eb\n+P\n|fa2d \u2212 fba2d | >\n\u2264P\n|f d \u2212 fbd | >\n2\n2\n2\n\u001a\n\u001b\n\u001a\n\u001b\n2\n2\n\u03bcn \u01eb 1\n\u03bcn \u01eb 2\n\u2264 2 exp \u2212\n+ 2 exp \u2212\n2\n2\n\nkn =\n\nW (log n) + 21 log n\n\u0001.\nlog n 21 exp{W (log n)} + 1\n\nThese choices lead to the optimal rate of convergence.\nProof. Let hn = n\u2212kn for some kn to be determined.\n\u2212d /2\nThen we want n\u22121/2 hn n\n= n(kn dn \u22121)/2 \u2192 0,\n\u2212k\n2 2\ndn hn = dn n\n\u2192 0, and dn hn = d2n n\u22122k \u2192 0 all\nas n \u2192 \u221e. Call these A, B, and C. Taking A and B\nfirst gives\nn(kn dn \u22121)/2 \u223c dn n\u2212kn\n\n1\n(kn dn \u2212 1) log n \u223c log dn \u2212 kn log n\n2\n\u0012\n\u0013\n1\n1\n\u21d2 kn log n\ndn + 1 \u223c log dn + log n\n2\n2\n\u21d2\n\n\u21d2 kn \u223c\n\nlog dn + 12 log n\n\u0001.\nlog n 21 dn + 1\n\n(9)\n\nSimilarly, combining A and C gives\nkn \u223c\n\n2 log dn + 12 log n\n\u0001 .\nlog n 21 dn + 2\n\n(10)\n\n\fDaniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish\n\nEquating (9) and (10) and solving for dn gives\n\u21d2 dn \u223c exp {W (log n)}\nwhere W (*) is the Lambert W function. Plugging back\ninto (9) gives that\nhn = n\u2212kn\n\n0 = R(\u03a9) = Q+ (\u03a9) \u2212 Q\u2212 (\u03a9)\n\n= Q+ (D) + Q+ (Dc ) \u2212 Q\u2212 (D) \u2212 Q\u2212 (Dc )\n\nwhere\nkn =\n\na signed measure on \u03c3 d . Decompose R into positive\nand negative parts as R = Q+ \u2212 Q\u2212 and similarly for\nRd = Q+d \u2212 Q\u2212d . Notice that since Rd is constructed\nusing the marginals of P, then R(E) = Rd (E) for all\nE \u2208 \u03c3 d . Now since R is the difference of probability\nmeasures, we must have that\n\nW (log n) + 21 log n\n\u0001.\nlog n 12 exp {W (log n)} + 1\n\nfor all D \u2208 \u03c3.\n\nDefine Q = Q+ + Q\u2212 . Let \u01eb > 0. Let C \u2208 \u03c3 be such\nthat\nQ(C) = \u03b2(a) = Q+ (C) = Q\u2212 (C c ).\n(13)\n\nIt is also necessary to show that as d grows, \u03b2 d (a) \u2192\n\u03b2(a). We now prove this result.\nLemma 4.2. \u03b2 d (a) converges to \u03b2(a) as d \u2192 \u221e.\nProof. By stationarity, the supremum over t is unnecessary in Definition 2.1, so without loss of generality, let t = 0. Let P0\u2212\u221e be the distribution on\n0\n\u03c3\u2212\u221e\n= \u03c3(. . . , X\u22121 , X0 ), and let P\u221e\na be the distribu\u221e\ntion on \u03c3a+1\n= \u03c3(Xa+1 , Xa+2 , . . .). Let Pa be the\n0\n\u221e\ndistribution on \u03c3 = \u03c3\u2212\u221e\n\u2297 \u03c3a+1\n(the product sigmafield). Then we can rewrite Definition 2.1 using this\nnotation as\n\u03b2(a) = sup |Pa (C) \u2212 [P0\u2212\u221e \u2297 P\u221e\na ](C)|.\n\nSuch a set C is guaranteed by the Hahn decomposition theorem (letting C \u2217 be a set which attains the\nsupremum in (11), we can throw away any subsets\nwith negative R measure) and (12) assuming without\nloss of generality that S\nPa (C) > [P0\u2212\u221e \u2297 P\u221e\na ](C). We\ncan use the field \u03c3f = d \u03c3 d to approximate \u03c3 in the\nsense that, for all \u01eb, we can find A \u2208 \u03c3f such that\nQ(A\u2206C) < \u01eb/2 (see Theorem D in Halmos [10, \u00a713]\nor Lemma A.24 in Schervish [20]). Now,\nQ(A\u2206C) = Q(A \u2229 C c ) + Q(C \u2229 Ac )\n\n= Q\u2212 (A \u2229 C c ) + Q+ (C \u2229 Ac )\n\nby (13) since A \u2229 C c \u2286 C c and C \u2229 Ac \u2286 C. Therefore,\nsince Q(A\u2206C) < \u01eb/2, we have\n\nC\u2208\u03c3\n\na+d\n0\n0\nLet \u03c3\u2212d+1\nand \u03c3a+1\nbe the sub-\u03c3-fields of \u03c3\u2212\u221e\nand\n\u221e\n\u03c3a+1 consisting of the d-dimensional cylinder sets for\nthe d dimensions closest together. Let \u03c3 d be the product \u03c3-field of these two. Then we can rewrite \u03b2 d (a)\nas\n\n\u03b2 d (a) = sup ||Pa (C) \u2212 [P0\u2212\u221e \u2297 P\u221e\na ](C)|.\n\nQ\u2212 (A \u2229 C c ) \u2264 \u01eb/2\n+\n\nAlso,\nQ(C) = Q(A \u2229 C) + Q(Ac \u2229 C)\n\n(11)\n\nAs such \u03b2 d (a) \u2264 \u03b2(a) for all a and d. We can\nrewrite (11) in terms of finite-dimensional marginals:\n\u03b2 (a) = sup |Pa,d (C) \u2212\nC\u2208\u03c3d\n\n[P0\u2212d\n\n\u2297\n\nPa+d\n](C)|,\na\n\nwhere Pa,d\nis the restriction of P to\n\u03c3(X\u2212d , . . . , X0 , Xa , . . . , Xa+d ).\nBecause of the\nnested nature of these sigma-fields, we have\n\u03b2 d1 (a) \u2264 \u03b2 d2 (a) \u2264 \u03b2(a)\nfor all finite d1 \u2264 d2 . Therefore, for fixed a, {\u03b2 d (a)}\u221e\nd=1\nis a monotone increasing sequence which is bounded\nabove, and it converges to some limit L \u2264 \u03b2(a). To\nshow that L = \u03b2(a) requires some additional steps.\nLet R = Pa \u2212 [P0\u2212\u221e \u2297 P\u221e\na ], which is a signed measure on \u03c3. Let Rd = Pa,d \u2212 [P0\u2212d \u2297 Pa+d\n], which is\na\n\n(14)\n\nc\n\nQ (A \u2229 C) \u2264 \u01eb/2.\n\nC\u2208\u03c3d\n\nd\n\n(12)\n\n= Q+ (A \u2229 C) + Q+ (Ac \u2229 C)\n\n\u2264 Q+ (A) + \u01eb/2\n\nsince A\u2229C and Ac \u2229C are contained in C and A\u2229C \u2286\nA. Therefore\nQ+ (A) \u2265 Q(C) \u2212 \u01eb/2.\nSimilarly,\nQ\u2212 (A) = Q\u2212 (A \u2229 C) + Q\u2212 (A \u2229 C c ) \u2264 0 + \u01eb/2 = \u01eb/2\nsince A \u2229 C \u2286 C and Q\u2212 (C) = 0 by (14). Finally,\nQ+d (A) \u2265 Q+d (A) \u2212 Q\u2212d (A) = Rd (A)\n= R(A) = Q+ (A) \u2212 Q\u2212 (A)\n\n\u2265 Q(C) \u2212 \u01eb/2 \u2212 \u01eb/2 = Q(C) \u2212 \u01eb\n= \u03b2(a) \u2212 \u01eb.\n\n\fEstimating \u03b2-mixing coefficients\n\nAnd since \u03b2 d (a) \u2265 Q+d (A), we have that for all \u01eb > 0\nthere exists d such that for all d1 > d,\n\u03b2 d1 (a) \u2265 \u03b2 d (a) \u2265 Q+d (A)\n\u2265 \u03b2(a) \u2212 \u01eb.\nThus, we must have that L = \u03b2(a), so that \u03b2 d (a) \u2192\n\u03b2(a) as desired.\nProof of Theorem 2.3. By the triangle inequality,\n|\u03b2bdn (a) \u2212 \u03b2(a)| \u2264 |\u03b2bdn (a) \u2212 \u03b2 dn (a)| + |\u03b2 dn (a) \u2212 \u03b2(a)|.\n\nThe first term on the right is bounded by the result in Theorem 2.4, where we have shown that dn =\nO(exp{W (log n)}) is slow enough for the histogram\ndn \u2192\u221e\n\nestimator to remain consistent. That \u03b2 dn (a) \u2212\u2212\u2212\u2212\u2192\n\u03b2(a) follows from Lemma 4.2.\n\n5\n\nDiscussion\n\nWe have shown that our estimator of the \u03b2-mixing\ncoefficients is consistent for the true coefficients \u03b2(a)\nunder some conditions on the data generating process.\nThere are numerous results in the statistics and machine learning literatures which assume knowledge of\nthe \u03b2-mixing coefficients, yet as far as we know, this\nis the first estimator for them. An ability to estimate\nthese coefficients will allow researchers to apply existing results to dependent data without the need to\narbitrarily assume their values. Despite the obvious\nutility of this estimator, as a consequence of its novelty,\nit comes with a number of potential extensions which\nwarrant careful exploration as well as some drawbacks.\nThe reader will note that Theorem 2.3 does not provide a convergence rate. The rate in Theorem 2.4 applies only to the difference between \u03b2\u0302 d (a) and \u03b2 d (a).\nIn order to provide a rate in Theorem 2.3, we would\nneed a better understanding of the non-stochastic convergence of \u03b2 d (a) to \u03b2(a). It is not immediately\nclear that this quantity can converge at any welldefined rate. In particular, it seems likely that the\nrate of convergence depends on the tail of the sequence\n{\u03b2(a)}\u221e\na=1 .\n\nthe histograms in our esitmator with KDEs. However,\nKDEs suffer from two major issues. Theoretically,\nwe need an analogue of the double asymptotic results\nproven for histograms in Lemma 3.4. In particular,\nwe need to estimate increasingly higher dimensional\ndensities as n \u2192 \u221e. This does not cause a problem\nof small-n-large-d since d is chosen as a function of n,\nhowever it will lead to increasingly higher dimensional\nintegration. For histograms, the integral is always trivial, but in the case of KDEs, the numerical accuracy\nof the integration algorithm becomes increasingly important. This issue could swamp any efficiency gains\nobtained through the use of kernels. However, this\nquestion certainly warrants further investigation.\nThe main drawback of an estimator based on a density estimate is its complexity. The mixing coefficients\nare functionals of the joint and marginal distributions\nderived from the stochastic process X, however, it is\nunsatisfying to estimate densities and solve integrals in\norder to estimate a single number. Vapnik's main principle for solving problems using a restricted amount of\ninformation is\nWhen solving a given problem, try to avoid\nsolving a more general problem as an intermediate step [23, p. 30].\nThis principle is clearly violated here, but perhaps our\nseed will precipitate a more aesthetically pleasing solution.\nReferences\n[1] Baraud, Y., Comte, F., and Viennet, G. (2001),\n\"Adaptive estimation in autoregression or \u03b2-mixing\nregression via model selection,\" Annals of statistics,\n29, 839\u2013875.\n[2] Bickel, P. and Rosenblatt, M. (1973), \"On Some\nGlobal Measures of the Deviations of Density Function Estimates,\" The Annals of Statistics, 1, 1071\u2013\n1095.\n[3] Bradley, R. C. (2005), \"Basic Properties of Strong\nMixing Conditions. A Survey and Some Open Questions,\" Probability Surveys, 2, 107\u2013144.\n\nSeveral other mixing and weak-dependence coefficients\nalso have a total-variation flavor, perhaps most notably \u03b1-mixing [8, 6, 3]. None of them have estimators,\nand the same trick might well work for them, too.\n\n[4] Carrasco, M. and Chen, X. (2002), \"Mixing\nand Moment Properties of Various GARCH and\nStochastic Volatility Models,\" Econometric Theory,\n18, 17\u201339.\n\nThe use of histograms rather than kernel density estimators for the joint and marginal densities is somewhat surprising and not entirely necessary. As mentioned above, Tran [22] proved that KDEs are consistent for estimating the stationary density of a time\nseries with \u03b2-mixing inputs, so one could just replace\n\n[5] Corless, R., Gonnet, G., Hare, D., Jeffrey, D., and\nKnuth, D. (1996), \"On the Lambert W Function,\"\nAdvances in Computational Mathematics, 5, 329\u2013\n359.\n[6] Dedecker, J., Doukhan, P., Lang, G., Leon R.,\nJ. R., Louhichi, S., and Prieur, C. (2007), Weak\n\n\fDaniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish\n\nDependence: With Examples and Applications, vol.\n190 of Lecture Notes in Statistics, Springer Verlag,\nNew York.\n\n[20] Schervish, M. (1995), Theory of Statistics,\nSpringer Series in Statistics, Springer Verlag, New\nYork.\n\n[7] Devroye, L. and Gy\u00f6rfi, L. (1985), Nonparametric Density Estimation: The L1 View, Wiley, New\nYork.\n\n[21] Silverman, B. (1978), \"Weak and Strong Uniform\nConsistency of the Kernel Estimate of a Density and\nits Derivatives,\" The Annals of Statistics, 6, 177\u2013\n184.\n\n[8] Doukhan, P. (1994), Mixing: Properties and Examples, vol. 85 of Lecture Notes in Statistics, Springer\nVerlag, New York.\n[9] Freedman, D. and Diaconis, P. (1981), \"On the\nMaximum Deviation Between the Histogram and\nthe Underlying Density,\" Probability Theory and\nRelated Fields, 58, 139\u2013167.\n[10] Halmos, P. (1974), Measure Theory, Graduate\nTexts in Mathematics, Springer-Verlag, New York.\n[11] Karandikar, R. L. and Vidyasagar, M. (2009),\n\"Probably Approximately Correct Learning with\nBeta-Mixing Input Sequences,\" submitted for publication.\n[12] Lozano, A., Kulkarni, S., and Schapire, R.\n(2006), \"Convergence and Consistency of Regularized Boosting Algorithms with Stationary BetaMixing Observations,\" Advances in Neural Information Processing Systems, 18, 819.\n[13] McDiarmid, C. (1989), \"On the Method of\nBounded Differences,\" in Surveys in Combinatorics,\ned. J. Siemons, vol. 141 of London Mathematical Society Lecture Note Series, pp. 148\u2013188, Cambridge\nUniversity Press.\n[14] Meir, R. (2000), \"Nonparametric Time Series Prediction Through Adaptive Model Selection,\" Machine Learning, 39, 5\u201334.\n[15] Mohri, M. and Rostamizadeh, A. (2010), \"Stability Bounds for Stationary \u03c6-mixing and \u03b2-mixing\nProcesses,\" Journal of Machine Learning Research,\n11, 789\u2013814.\n[16] Mokkadem, A. (1988), \"Mixing properties of\nARMA processes,\" Stochastic processes and their\napplications, 29, 309\u2013315.\n[17] Nobel, A. (2006), \"Hypothesis Testing for Families of Ergodic Processes,\" Bernoulli, 12, 251\u2013269.\n[18] Nummelin, E. and Tuominen, P. (1982), \"Geometric Ergodicity of Harris Recurrent Markov\nChains with Applications to Renewal Theory,\"\nStochastic Processes and Their Applications, 12,\n187\u2013202.\n[19] Ralaivola, L., Szafranski, M., and Stempfel, G.\n(2010), \"Chromatic PAC-Bayes Bounds for NonIID Data: Applications to Ranking and Stationary\n\u03b2-Mixing Processes,\" Journal of Machine Learning\nResearch, 11, 1927\u20131956.\n\n[22] Tran, L. (1989), \"The L1 Convergence of Kernel\nDensity Estimates under Dependence,\" The Canadian Journal of Statistics/La Revue Canadienne de\nStatistique, 17, 197\u2013208.\n[23] Vapnik, V. (2000), The Nature of Statistical\nLearning Theory, Statistics for Engineering and Information Science, Springer Verlag, New York, 2nd\nedn.\n[24] Vidyasagar, M. (1997), A Theory of Learning and\nGeneralization: With Applications to Neural Networks and Control Systems, Springer Verlag, Berlin.\n[25] Woodroofe, M. (1967), \"On the Maximum Deviation of the Sample Density,\" The Annals of Mathematical Statistics, 38, 475\u2013481.\n[26] Yu, B. (1993), \"Density Estimation in the L\u221e\nNorm for Dependent Data with Applications to the\nGibbs Sampler,\" Annals of Statistics, 21, 711\u2013735.\n[27] Yu, B. (1994), \"Rates of Convergence for Empirical Processes of Stationary Mixing Sequences,\" The\nAnnals of Probability, 22, 94\u2013116.\n\n\f"}