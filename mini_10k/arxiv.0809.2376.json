{"id": "http://arxiv.org/abs/0809.2376v1", "guidislink": true, "updated": "2008-09-14T02:20:21Z", "updated_parsed": [2008, 9, 14, 2, 20, 21, 6, 258, 0], "published": "2008-09-14T02:20:21Z", "published_parsed": [2008, 9, 14, 2, 20, 21, 6, 258, 0], "title": "Quantum State Tomography: 'the best' is the enemy of 'good enough'", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0809.1914%2C0809.2267%2C0809.4087%2C0809.1725%2C0809.2180%2C0809.5204%2C0809.3874%2C0809.5121%2C0809.1549%2C0809.1575%2C0809.2017%2C0809.4577%2C0809.5038%2C0809.4315%2C0809.3604%2C0809.3618%2C0809.0191%2C0809.0979%2C0809.0976%2C0809.1038%2C0809.2472%2C0809.3079%2C0809.2435%2C0809.4907%2C0809.4370%2C0809.4701%2C0809.3379%2C0809.2595%2C0809.0163%2C0809.3973%2C0809.3893%2C0809.2119%2C0809.1460%2C0809.0308%2C0809.1782%2C0809.4563%2C0809.3463%2C0809.3599%2C0809.3862%2C0809.0330%2C0809.2378%2C0809.0618%2C0809.2358%2C0809.4805%2C0809.3056%2C0809.3544%2C0809.3309%2C0809.3414%2C0809.1605%2C0809.3494%2C0809.5108%2C0809.3438%2C0809.2398%2C0809.2117%2C0809.2250%2C0809.2515%2C0809.3116%2C0809.0768%2C0809.2302%2C0809.3863%2C0809.4146%2C0809.0886%2C0809.2052%2C0809.3534%2C0809.1628%2C0809.0595%2C0809.3642%2C0809.2293%2C0809.2318%2C0809.4088%2C0809.1540%2C0809.3372%2C0809.1910%2C0809.2423%2C0809.2838%2C0809.0931%2C0809.3903%2C0809.3420%2C0809.2111%2C0809.4945%2C0809.2376%2C0809.2049%2C0809.4085%2C0809.1960%2C0809.0479%2C0809.1670%2C0809.1309%2C0809.4704%2C0809.3058%2C0809.4969%2C0809.4473%2C0809.2039%2C0809.5031%2C0809.3353%2C0809.2247%2C0809.2528%2C0809.2673%2C0809.3836%2C0809.1778%2C0809.0977%2C0809.2526&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Quantum State Tomography: 'the best' is the enemy of 'good enough'"}, "summary": "In this paper, we examine a variety of strategies for numerical quantum-state\nestimation from data of the sort commonly measured in experiments involving\nquantum state tomography. We find that, in some important circumstances, an\nelaborate and time-consuming numerical optimization to obtain 'the best'\ndensity matrix corresponding to a given data set is not necessary, and that\ncruder, faster numerical techniques may well be 'good enough'.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0809.1914%2C0809.2267%2C0809.4087%2C0809.1725%2C0809.2180%2C0809.5204%2C0809.3874%2C0809.5121%2C0809.1549%2C0809.1575%2C0809.2017%2C0809.4577%2C0809.5038%2C0809.4315%2C0809.3604%2C0809.3618%2C0809.0191%2C0809.0979%2C0809.0976%2C0809.1038%2C0809.2472%2C0809.3079%2C0809.2435%2C0809.4907%2C0809.4370%2C0809.4701%2C0809.3379%2C0809.2595%2C0809.0163%2C0809.3973%2C0809.3893%2C0809.2119%2C0809.1460%2C0809.0308%2C0809.1782%2C0809.4563%2C0809.3463%2C0809.3599%2C0809.3862%2C0809.0330%2C0809.2378%2C0809.0618%2C0809.2358%2C0809.4805%2C0809.3056%2C0809.3544%2C0809.3309%2C0809.3414%2C0809.1605%2C0809.3494%2C0809.5108%2C0809.3438%2C0809.2398%2C0809.2117%2C0809.2250%2C0809.2515%2C0809.3116%2C0809.0768%2C0809.2302%2C0809.3863%2C0809.4146%2C0809.0886%2C0809.2052%2C0809.3534%2C0809.1628%2C0809.0595%2C0809.3642%2C0809.2293%2C0809.2318%2C0809.4088%2C0809.1540%2C0809.3372%2C0809.1910%2C0809.2423%2C0809.2838%2C0809.0931%2C0809.3903%2C0809.3420%2C0809.2111%2C0809.4945%2C0809.2376%2C0809.2049%2C0809.4085%2C0809.1960%2C0809.0479%2C0809.1670%2C0809.1309%2C0809.4704%2C0809.3058%2C0809.4969%2C0809.4473%2C0809.2039%2C0809.5031%2C0809.3353%2C0809.2247%2C0809.2528%2C0809.2673%2C0809.3836%2C0809.1778%2C0809.0977%2C0809.2526&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper, we examine a variety of strategies for numerical quantum-state\nestimation from data of the sort commonly measured in experiments involving\nquantum state tomography. We find that, in some important circumstances, an\nelaborate and time-consuming numerical optimization to obtain 'the best'\ndensity matrix corresponding to a given data set is not necessary, and that\ncruder, faster numerical techniques may well be 'good enough'."}, "authors": ["Max S. Kaznady", "Daniel F. V. James"], "author_detail": {"name": "Daniel F. V. James"}, "author": "Daniel F. V. James", "arxiv_comment": "12 pages, 3 tables, 9 figures. Files: 9 PDF figures and 1 TEX file,\n  all in ZIP archive", "links": [{"href": "http://arxiv.org/abs/0809.2376v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0809.2376v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0809.2376v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0809.2376v1", "journal_reference": null, "doi": null, "fulltext": "Quantum State Tomography: 'the best' is the enemy of 'good enough'.\nMax S. Kaznady and Daniel F. V. James\u2217\nDepartment of Physics and Centre of Quantum Information and Quantum Control\nUniversity of Toronto, 60 St. George Street, Toronto, ON, M5S 1A7, CANADA\n(Dated: November 12, 2018)\nIn this paper, we examine a variety of strategies for numerical quantum-state estimation from\ndata of the sort commonly measured in experiments involving quantum state tomography. We find\nthat, in some important circumstances, an elaborate and time-consuming numerical optimization to\nobtain 'the best' density matrix corresponding to a given data set is not necessary, and that cruder,\nfaster numerical techniques may well be 'good enough'.\n\narXiv:0809.2376v1 [quant-ph] 14 Sep 2008\n\nPACS numbers: 03.67.-a, 03.65.Wj\n\nI.\n\nINTRODUCTION\n\nThe goal of quantum state tomography [1, 2, 3] is to\nestimate, from a series of projective measurements performed on identically prepared quantum systems, the\ndensity matrix of the underlying ensemble of which these\nquantum systems are realizations. This process is necessarily non-deterministic in nature, relying on the frequency of experimental outcomes to estimate probabilities - a process that converges to the actual probabilities\nonly in the infinite limit. Thus the reconstruction of the\nquantum state cannot be exact in any realistic experiment. Furthermore, these measurements can only yield\nestimates of the on-diagonal elements of the density matrix, but not directly any data about the off-diagonal\nelements. It is necessary to perform various unitary operations on the system (or, equivalently to perform projective measurements in a variety of bases) in order to\nobtain such information about the complete state. Indeed, for a system with a discrete spectrum of n-levels,\nthe density matrix is specified by n2 \u2212 1 independent\nreal parameters, and each parameter will require a separate measurement. Even after the required measurements have been performed, the experimenter faces the\nproblem of estimating the density matrix from incomplete and noisy data. The problem is aggravated by the\nconstraints that quantum physics places on the density\nmatrix: It must be a non-negative, unit-trace Hermitian\nmatrix. Today, the approach that is usually taken is to\ndetermine computationally what is the 'best' such positive, unit-trace Hermitian matrix which corresponds to\na particular data set, and what confidence can we place\non such an estimate. The most complicated such tomographic measurement performed to date [5], on an 8\nqubit (256 state) system, realized in a trapped ion experiment, was limited not by the experimental capabilities of\nthe system, but rather by the complexity of the numerical state recovery problem [6]. This computational complexity, while underscoring the awesome computational\n\n\u2217 Electronic\n\naddress: max.kaznady@gmail.com; Electronic address:\ndfvj@physics.utoronto.ca\n\npotential inherent in quantum information, nevertheless\npresents an experimenter, intent on exploring larger and\nlarger Hilbert spaces, with considerable tribulation when\ncharacterising the performance of his or her apparatus.\nIn this paper we examine the problem from an entirely\ncomputational perspective. Specifically, we address the\nconcern that maybe we are being too fastidious in approaching the state reconstruction problem. One can\nobtain a positive, unit-trace Hermitian matrix from tomographic data in a variety of ways. First, and most\nsimply, one could generate a linear reconstruction of the\nnoisy data (which tends to give a non-positive matrix),\nand ensure positivity by setting the negative eigenvalues\nto zero, then re-normalizing to ensure a unit-trace. This\nwe call the \"Quick and Dirty\" (QD) approach. A second\nstrategy is to assume the state must be nearly pure - after\nall, quantum technologies are usually in the business of\ntrying to create pure states - and to simplify the computation by finding the pure state most compatible with the\ndata. We call this the \"Forced Purity\" (FP) approach.\nA third approach is full optimization, i.e. the application\nof some constrained optimization routine, with a specific\nmetric to define the 'distance' between our data set and\na positive density matrix, and search parameter space\nuntil the absolute 'best' (i.e. global minimum) density\nmatrix is obtained. Our goal is specifically to address\nthe question: When is the rigorous optimization required,\nand when will some short-cut technique be good enough?\nThis is a question that can only be addressed by simulation: Since we need to know a priori the underlying\ndensity matrix of the ensemble to compare the recovered\nestimates. Starting with assumed density matrix, we employ a pseudo-random number generator to create some\n'pseudo-experimental data' with appropriate probability\ndistribution. The various approaches to density matrix\nrecovery are applied to it, and the result is compared\nwith the initial density matrix to assess the accuracy\nof the recovery techniques. Our analysis concerns solely\nmultiple correlated two-level systems, e.g. the qubits of\na small scale quantum computer; however, many of the\ntechniques and results we present are readily adaptable\nto more general systems.\nThe paper is organized as follows: In Sec. II we discuss\nthe generic tomography problem for a single qubit, which\n\n\f2\nis generalized to the n-qubit case in Sec. III, describing\nspecifically a number of memory management techniques\nrequired for scalability of the code, and our novel approach to the optimization routine (using gradient-based\nalgorithms and employing the matrix differential calculus). The code itself is described in detail in Sec. IV, and\nour results in Sec. V.\nII.\n\nONE QUBIT\n\nIn this section, we will review the basic concept of\nquantum state tomography by considering the estimation of a state of a single two-level system, or qubit.\nA.\n\nParametrizing the Density Matrix\n\nhence that the desired parameters r\u03bc can be obtained\nfrom the observed quantities n\u03bd , viz.,\nr\u03bd = (N )\u22121\n\n\u03c1\u0302 =\n\nr\u03bc \u03c3\u0302\u03bc ,\n\n(1)\n\nwhere\n(2)\n\nSince T r{\u03c1\u0302} = 1, r0 = 1/2; further, since \u03c1\u0302\u2020 = \u03c1\u0302, the r\u03bd\nare all real parameters.\nThe r\u03bd may be determined experimentally as follows:\nSuppose we perform a measurement, specified by the projector \u03a0\u03020 , on the system, the probability of obtaining a\npositive outcome is T r{\u03c1\u0302\u03a0\u03020 }. Repeating this measurement N times on identically prepared systems, the expected number of times we obtain this outcome will be\n\nSubstituting r\u03bd into Eq. (1), we obtain the density matrix, as a function of measurement outcomes, provided\nthe measurements have no noise or errors in them.\nFollowing the precedent of Ref. [4] we use the standard\nStokes measurement basis for our numerical experiments.\nThese measurement operators are given by:\n\u03a0\u03020 = 12 (|0ih0| + |1ih1|), \u03a0\u03021 = |0ih0|,\n\u03a0\u03022 = |D\u0304ihD\u0304|,\n\u03a0\u03023 = |RihR|,\n\n3\nX\n\nT r{\u03c3\u0302\u03bc \u03a0\u03020 }r\u03bc .\n\n(3)\n\n\u03bc=0\n\nIf one repeated this procedure of multiple measurements for a set of four different measurement operators,\n{\u03a0\u0302\u03bd }, (\u03bd = 0, 1, 2, 3) one obtains a set of linear equations\nn\u03bd = N\n\n3\nX\n\nB\u03bd,\u03bc r\u03bc ,\n\n(7)\n\n(4)\n\n1\n|Ri = \u221a (|0i \u2212 i * |1i),\n2\n\n(8)\n\n1\n|D\u0304i = \u221a (|0i \u2212 |1i).\n2\n\n(9)\n\nA natural metric to compare the recovered density matrix \u03c1\u0302meas with the actual density matrix \u03c1\u0302true is the fidelity [19], defined as:\np\np\nF (\u03c1\u0302meas , \u03c1\u0302true ) = {T r[( \u03c1\u0302meas \u03c1\u0302true \u03c1\u0302meas )1/2 ]}2 . (10)\nHowever, when we invert the measurement data linearly,\nour recovered \"density matrix\" \u03c1\u0302linear is not non-negative\ndefinite and hence we have the specific problem that fidelity turns out to be complex (not to mention the more\ngeneral problem that \u03c1\u0302linear cannot be interpreted as a\ndensity matrix of a physical state). We have to correct\nthe matrix obtained by linear reconstruction to obtain a\nproper density matrix.\n\nB.\n\nn0 = N T r{\u03c1\u0302\u03a0\u03020 }\n= N\n\n(6)\n\nwhere |0i and |1i represent the two states of our qubits,\nand\n\n\u03bc=0\n\nr\u03bd = T r{\u03c3\u0302\u03bd \u03c1\u0302}/2.\n\n(B \u22121 )\u03bd,\u03bc n\u03bc .\n\n\u03bc=0\n\nThe density operator describing the state of a system [7] is a Hermitian, non-negative definite operator of\nunit-trace. The set of Pauli matrices [8] {\u03c3\u03020 , \u03c3\u03021 , \u03c3\u0302 2 , \u03c3\u0302 3 }\nform, for a two dimensional space, a complete orthonormal set of matrices, so that \u03c1\u0302 can be expanded as a linear\ncombination of \u03c3\u0302\u03bc as\n3\nX\n\n3\nX\n\n\"Quick and Dirty\" Reconstruction\n\nAs a simple initial approach to this problem, we can\ndecompose \u03c1\u0302linear into its spectral representation, i.e.\n\u03c1\u0302linear = \u00db D\u0302\u00db \u2020 ,\n\n(11)\n\nwhere D\u0302 is the diagonal matrix of eigenvalues (which\nare real, but not necessarily positive) and \u00db is a unitary\nmatrix. We then set all negative eigenvalues in D\u0302 to zero,\ncall this matrix D\u03020 , and obtain:\n\n\u03bc=0\n\nwhere\n\n\u03c1\u0302QD =\nB\u03bd,\u03bc = T r{\u03c3\u0302\u03bc \u03a0\u0302\u03bd }.\n\n\u00db D\u03020 \u00db \u2020\nT r{D\u03020 }\n\n.\n\n(5)\n\nBy choosing the measurement operators, {\u03a0\u0302\u03bd }, judiciously, one can ensure that B\u03bd,\u03bc is non-singular, and\n\nThis provides a rough initial estimate of the state; one\nof the goals of our analysis in this paper is to assess how\ngood an estimate it is.\n\n\f3\nC.\n\n\"Forced Purity\"\n\nAn alternative approach to the problem of obtaining a\nnon-negative definite density matrix from measured data\nis to assume that the state is pure. Recall that for a pure\nstate |\u03a8i the density matrix can be described by a single\nket as \u03c1\u0302pure = |\u03a8ih\u03a8|. Such a density matrix for n qubits\nhas eigenvalue 0 with degeneracy 2n \u2212 1 and eigenvalue\n1 with degeneracy 1.\nBecause \u03c1\u0302pure is also Hermitian, it can be written in\nits spectral decomposition as\n\u03c1\u0302pure = V\u0302 D\u0302pure V\u0302 \u2020 ,\nwhere D\u0302pure is the diagonal matrix with a single element\nequal to 1, and all other elements being zero; V\u0302 is a\nunitary matrix.\nDuring linear inversion of a pure state, the eigenvalues\nof \u03c1\u0302linear may be negative, but sufficiently close to eigenvalues of \u03c1\u0302pure . The idea of forcing purity on such a state\nis to obtain\n\u03c1\u0302FP =\n\n\u2020\nV\u0302linear D\u02c600 V\u0302linear\n,\nT r{D\u02c600 }\n\nwhere D\u02c600 is the diagonal matrix obtained from D\u0302 by\nsetting the largest eigenvalue equal to 1, and all others\nequal to 0.\nD.\n\nMaximum Likelihood\n\nAny Hermitian 2\u00d72 non-negative unit-trace matrix can\nbe uniquely parametrized using the Cholesky decomposition as:\n\n\u03c1\u0302ideal (t1 , t2 , t3 , t4 ) =\n\nT \u2020T\n,\nT r{T \u2020 T }\n\n(12)\n\nwhere\n\u0012\nT (t1 , t2 , t3 , t4 ) =\n\nt1\n0\nt3 + i * t4 t2\n\n\u0013\n.\n\n(13)\n\nThus a 'physical' density matrix can be specified by the\nfour parameters ~t = {t1 , t2 , t3 , t4 }. The ideal of the maximum likelihood method is to perform\u0001 a search of the ~t parameter space until we find a \u03c1\u0302ideal ~t which is most likely\nto have generated the observed data {n0 , n1 , n2 , n3 }. To\nassess this likelihood, suppose that each datum n\u03bc is\na statistically independent, Poisson-distributed random\nvariable with expectation value n\u0304\u03bc . Further, if n\u0304\u03bc is a\nlarge number, the Poisson distribution is well approximated by the Gaussian distribution, i.e.\nP (n0 , n1 , n2 , n3 ) =\n\n1\nNnorm\n\n\u0015\n3\nY\n(n\u03bd \u2212 n\u0304\u03bd )2\nexp \u2212\n,\n2n\u0304\u03bd\n\u03bd=0\n(14)\n\u0014\n\nwhere Nnorm is the normalization constant.\nIf\neach datum n\u03bc is garnered from N repetitions of\na measurement\ncarried out on a system in state\n\u0001\n\u03c1\u0302ideal ~t , it is reasonable to make the identification\nn\u0304\u03bd (t1 , t2 , t3 , t4 ) = N h\u03c8\u03bd |\u03c1\u0302ideal (t1 , t2 , t3 , t4 )|\u03c8\u03bd i, and the\nlikelihood of a given parameter vector ~t generating the\ndata {n0 , n1 , n2 , n3 } can be obtained by substituting this\nidentity into Eq. (14). We are then in a position to determine the parameter vector for which this probability is maximized, and hence the most likely density matrix. Instead of maximizing Eq. (14), it is equivalent, and\nmathematically more convenient to minimize the following function:\nh\ni2\n3\n~\nn\n\u2212\nN\nT\nr{\n\u03a0\u0302\n\u03c1\u0302\n(\nt\n)}\nX\n\u03bd\n\u03bd ideal\n1\nL(~t) =\n.\n(15)\n2 \u03bd=0\nN T r{\u03a0\u0302\u03bd \u03c1\u0302ideal (~t)}\nIn order to optimize this function efficiently, we need\nto compute its gradient. This is not an easy feat, as the\nclosed analytic form does not simplify well, and finitedifferencing is too inefficient. The situation becomes exponentially worse as we increase the number of qubits.\nIII.\n\nGENERALIZATION TO N-QUBITS\n\nIn the previous section, we outlined the possible routines for performing tomography of a single qubit. We\nnow extend these routines to a higher number of qubits\nand see how the \"Quick and Dirty\" and \"Forced Purity\"\nmethods compare to the elaborate and time-consuming\nMaximum Likelihood Estimation (MLE) routine.\nAt first, the problem looks very simple - any state of\neach qubit is completely characterized by only 4 measurements. Hence, numerically the MLE procedure is rather\neasy to implement - we just need to optimize a function of 4 variables, which is achieved by the simplex or\nPowell optimization algorithm in a fairly short amount\nof time [9], without computing the gradient. However,\ntwo qubits, when correlated, are not characterized by\n8 measurements, but by 4 * 4 = 16 measurements, because we are looking at a system of 2 qubits. If n is the\nnumber of qubits, then we would need to obtain 4n measurement outcomes in some fixed 4n dimensional basis.\nDue to wave function collapse, we can only perform one\nprojection measurement at a time (an outcome is an average over multiple identical projection measurements) and\nfor each projection measurement on one qubit, we have\nto cycle through all possible combinations of projection\nmeasurements for the other qubits.\nLet us introduce the following set of operators which\ngeneralize the Pauli matrices for n qubit systems:\n1\n\u0393\u0302\u03bc = \u221a \u03c3\u0302\u03bc1 \u2297 \u03c3\u0302\u03bc2 \u2297 ... \u2297 \u03c3\u0302\u03bcn ,\n2n\n\n(16)\n\nwhere 0 \u2264 \u03bc\u03be \u2264 3 for all 1 \u2264 \u03be \u2264 n are the digits of\nthe index \u03bc in base-4. For example, if \u03bc = 33 for a 4qubit system, \u0393\u030233 = \u03c3\u03020 \u2297 \u03c3\u03022 \u2297 \u03c3\u03020 \u2297 \u03c3\u03021 , since 33 is equal\n\n\f4\nto 0201 in base-4. For convenience, we have included\na normalization constant, so that T r{\u0393\u0302\u03bc \u0393\u0302\u03bd } = \u03b4\u03bc,\u03bd (in\nkeeping with the convention used in Ref. [4]). Similarly,\nwe write the projection operators for our measurement\nstates as\n\u03a0\u0302\u03bd = \u03a0\u0302\u03bd1 \u2297 \u03a0\u0302\u03bd2 \u2297 ... \u2297 \u03a0\u0302\u03bdn .\n\nQubits\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n(17)\n\nThe Cholesky decomposition of \u03c1\u0302 remains the same, except that T (~t) is a 2n \u00d7 2n matrix specified by 4n parameters ~t = {t1 , t2 , . . . t4n }, i.e.\n\uf8ee\n\nt1\n0\n\uf8ef t2n +1 + i * t2n +2 t2\n\uf8ef\nT (~t) = \uf8ef\n..\n\uf8f0\n.\nt4n \u22121 + i * t4n\n\n0\n0\n..\n.\n\n0\n0\n..\n.\n\nBytes\n128\n2.05 \u00d7 103\n3.28 \u00d7 104\n5.24 \u00d7 105\n8.39 \u00d7 106\n1.34 \u00d7 109\n2.15 \u00d7 109\n3.44 \u00d7 1010\n5.50 \u00d7 1011\n8.80 \u00d7 1012\n1.41 \u00d7 1014\n\nGigaBytes\n1.28 \u00d7 10\u22127\n2.05 \u00d7 10\u22126\n3.28 \u00d7 10\u22125\n5.24 \u00d7 10\u22124\n0.01\n0.13\n2.15\n34.4\n580\n8.80 \u00d7 103\n1.41 \u00d7 105\n\n\uf8f9\n\uf8fa\n\uf8fa Table I: Amount of memory required to store a 4n \u00d7 4n com\uf8fa.\n\uf8fb plex floating point matrix using 32 bits to store the real or\n\n* * * t2n+1 \u22124 + i * t2n+1 \u22123 t2n\n(18)\n\nA. Computational Constraints and\nMemory-efficient Linear Reconstruction\n\nIn order to perform computational numerical tomography in practice, we need to take the following into consideration:\n\n1. Computational efficiency - what is the upper bound\non the number of floating point operations of a certain tomography algorithm.\n\nimaginary part.\n\nIt must also be noted that any type of storage media\nhas to be able to perform read and write operations quite\nfast because this data structure would be accessed quite\nfrequently. This is simply not the case for most conventional hard drives: Using standard personal computers\nof the type typically integrated into quantum optics laboratories, one is in practice limited to about 7 qubits,\nwithout resorting to more powerful computer hardware.\nHowever, a data structure of maximum size of 2n \u00d7 2n\nwould allow to go as high as 15-16 qubits, at which point\nthe density matrix itself would become a storage problem. Thus our goal is to avoid storing matrix B\u03bc,\u03bd into\nmemory. Instead, we can obtain its inverse element by\nelement. This can be achieved as follows: The matrix\nB\u03bc,\u03bd for an n-qubit system is defined by the equation\nB\u03bd,\u03bc = T r{(\u03a0\u0302\u03bd1 \u2297 ... \u2297 \u03a0\u0302\u03bdn )(\u03c3\u0302\u03bc1 \u2297 ... \u2297 \u03c3\u0302\u03bcn )}\n= T r{\u03a0\u0302\u03bd1 \u03c3\u0302\u03bc1 }T r{\u03a0\u0302\u03bd2 \u03c3\u0302\u03bc2 }...T r{\u03a0\u0302\u03bdn \u03c3\u0302\u03bcn }.(19)\n\n2. Amount of memory available - what is the upper bound on the size in computer memory of the\nlargest data structure used by the tomography algorithm.\n\nKronecker tensor products increase the size of resultant\nmatrices exponentially. The goal is to obtain the density matrix which has 2n \u00d7 2n elements. So we cannot\nhave any other data structure in memory which would be\nlarger, otherwise the problem of increasing the number\nof qubits becomes constrained by that particular data\nstructure.\nFor example, consider the approach described in\nRef. [4], in which a 4n \u00d7 4n complex matrix B\u03bc,\u03bd (the nqubit generalization of the matrix defined by Eq. (5)) was\nstored in memory. The table below outlines how much\nmemory is needed to store a 4n \u00d7 4n complex floating\npoint matrix, using 32 bits to store the real or imaginary\npart:\n\nDefining the 4 \u00d7 4 matrix \u03b2\u03bd\u03be ,\u03bc\u03be = T r{\u03a0\u0302\u03bd\u03be \u03c3\u0302\u03bc\u03be } for all\n1 \u2264 \u03be \u2264 n, which can be easily inverted (provided a\nsuitable set of measurements {\u03a0\u0302\u03bc }, (\u03bc = 0, 1, 2, 3) has\nbeen chosen), we find\n\u22121\nB\u03bd,\u03bc\n= \u03b2\u03bd\u22121\n\u03b2 \u22121 ...\u03b2\u03bd\u22121\n,\n1 ,\u03bc1 \u03bd2 ,\u03bc2\nn ,\u03bcn\n\n(20)\n\nwhere as before, (\u03bd1 , \u03bd2 , ...\u03bdn ) are the base-4 digits of the\nindex \u03bd (and similarly for \u03bc).\nThis allows us to calculate the initial linear reconstruction of the density matrix from the observation data\n(n0 , n1 , ....n4n \u22121 ), viz:\n\u03c1\u0302linear =\n\nn\n4X\n\u22121\n\n\u0393\u0302\u03bd r\u03bd ,\n\n(21)\n\n\u03bd=0\n\nwhere\nr\u03bd = (N )\u22121\n\nn\n4X\n\u22121\n\n(B \u22121 )\u03bd,\u03bc n\u03bc ,\n\n\u03bc=1\n\n(22)\n\n\f5\nin a computationally efficient manner. Now the only size\nconstraint on linear reconstruction is the density matrix\nitself. Of course, storing the projection measurement matrices \u03a0\u0302\u03bd is also problematic - a quick solution is to generate these matrices when they become needed - one can\nstore certain tensor combinations which make up \u03a0\u0302\u03bd into\nmemory and only tensor on additional combinations to\nobtain the desired \u03a0\u0302\u03bd .\n\nB.\n\nMaximum Likelihood\n\n(23)\n\nwhere to simplify calculations we assumed that we can\napproximate variance by the measurement outcome average in the denominator. Minimizing this function becomes a severe computational problem. Most gradientfree optimization routines are rather slow and only work\nwell for a low number of dimensions, whereas here we\nhave a number of dimensions which grows exponentially\nwith the number of qubits. We have to use a numerical routine which is more efficient; this usually involves\ncalculating the gradient and/or the Jacobian. The finitedifferencing approach is too slow for computing the gradient (a fact which we verified computationally) because\nevaluating the MLE function is exponentially inefficient.\nHence, we require an analytic closed form for the gradient\nand/or the Jacobian matrix.\nIt should also be noted that if the region of optimization is convex, we are looking at non-linear convex\noptimization problem, for which a number of algorithmic approaches should work. We decided to take the\nsimplest approach possible: Optimize the MLE function\nwith built-in constraints using an algorithm which works\non both convex and non-convex sets. We reduce the computation time by deriving an analytic form for the gradient. An alternative approach is to derive a different MLE\nfunction with an external set of constraints, and launch\nanother convex optimization algorithm similar to linear\nprogramming [15, 16].\n\nInitial Algorithmic Attempts\n\nThe following algorithms were considered to optimize\nL [9, 10], mostly because they are available in libraries\nsuch as GNU Scientific Library (GSL) [20]:\n1. simplex method;\n2. Powell's quadratically convergent method;\n3. Levenberg-Marquardt nonlinear least squares;\n\nRoutines 4 and 5 need to be able to perform gradient\ncomputation and line search in an efficient manner and\nhave to converge to the desired minimum, even if started\nfar away from it.\nLet us start with the line search routines - the following\nalgorithms can be implemented for line search:\n\n2. Newton's method;\n3. Golden Section Search (GSS).\n\nn\n\n1.\n\n5. BFGS algorithm.\n\n1. successive parabolic interpolation;\n\nExtending the Maximum Likelihood Function (MLF)\nfrom Eq. (15) to n qubits we obtain:\n4 \u22121\n1 X [N T r{\u03a0\u0302\u03bd \u03c1\u0302ideal (~t)} \u2212 n\u03bd ]2\n,\nL(~t) =\n2 \u03bd=0\nn\u03bd\n\n4. conjugate-gradient method;\n\nIn order to pick one algorithm out of these three, we need\nto first know if the region of optimization is convex or not,\nand if so then how closely does it resemble a quadratic\nfunction.\n\n2.\n\nMatrix Calculus Derivation\n\nRegardless of the method chosen for the line search in\nSec. III B 1, we still need an efficient way of computing\nthe gradient. As established earlier, finite-differencing requires too many function evaluations and does not satisfy\nour computational efficiency constraints.\nThe goal of this section is to find the gradient of L\nin closed form. This procedure can then be extended to\nfinding second order partial derivatives for the Hessian\nmatrix and differentiation with respect to a constant for\nline search routine.\nWe begin with the gradient derivation. This reduces\nto finding\n\u2202L\n\u2202L \u2202T\n=\n.\n\u2202t\u03bd\n\u2202T \u2202t\u03bd\n\n(24)\n\n\u2202L\nUsing matrix calculus, it suffices to find \u2202T\n, which would\nn\nn\nbe a matrix of size 2 \u00d7 2 in our case. Certain elements\n\u2202L\nof this matrix would represent the values of \u2202t\n:\n\u03bd\n\n\"\n#\nn\n4X\n\u22121\nN\nN T r{\u03a0\u0302\u03bd \u03c1\u0302ideal (~t)} \u2212 n\u03bd\n\u2202L(~t)\n=\n\u2202T\nn\u03bd\nT r{T \u2020 (~t)T (~t)}2 \u03bd=0\n\"\n\u2202T r{\u03a0\u0302\u03bd T \u2020 (~t)T (~t)}\n\u00d7 T r{T \u2020 (~t)T (~t)}\n\u2202T\n\u0015\n\u2202T r{T \u2020 (~t)T (~t)}\n\u2212T r{\u03a0\u0302\u03bd T \u2020 (~t)T (~t)}\n(25)\n\u2202T\nDefining the following real quantities:\nA(~t) = T r{T \u2020 (~t)T (~t)} and\nB\u03bd (~t) = T r{\u03a0\u0302\u03bd T \u2020 (~t)T (~t)},\n\n(26)\n(27)\n\n\f6\nwe find:\nT r{\u03a0\u0302\u03bd \u03c1\u0302ideal (~t)} =\n\nB\u03bd\n.\nA\n\n(28)\n\nFurther we will denote the matrix derivatives of these\nquantities with respect to the Cholesky matrix T as follows:\n\u2202T r{\u03a0\u0302\u03bd T \u2020 (~t)T (~t)}\nB\u03bd0 (~t) =\n,\n\u2202T\n\n2. \"Quick and Dirty\" - quickly fixes \u03c1\u0302linear into \u03c1\u0302QD\nby setting all negative eigenvalues to zero and renormalizing.\n\n(29)\n\n\u2020\n\n\u2202T r{T (~t)T (~t)}\nA0 (T~ ) =\n.\n\u2202T\n\n(30)\n\nBecause matrix calculus is only well-defined for realvalued matrices, let us write\nT = T (~t) = X + i * Y,\n\n1. Linear reconstruction - provides the linear reconstruction of the data by inverting the measurements\ninto a matrix \u03c1\u0302linear , outlined in Sec. III A, which\nhas all characteristics of a density matrix, except\npositive semi-definiteness.\n\n\u03a0\u0302\u03bd = K\u03bd + i * \u039b\u03bd .\n\n(31)\n\n3. \"Forced Purity\" - for pure states, eigenvalues are\nknown. This routine forces eigenvalues of \u03c1\u0302linear or\n\u03c1\u0302QD (does not matter which one) into those of a\npure state, also ensuring unit-trace condition.\n4. MLE - we use the elements of the \"Quick and\nDirty\" density matrix as a starting point for our\noptimization routine. We then launch the BFGS2\nalgorithm supplied with GSL.\n\nThen, using the matrix calculus theorems in Sec. A we\nfind\n\nOur progress while developing these routines is as follows:\n\nA0 = 2X + i * 2Y,\n(32)\nB\u03bd0 = 2XK\u03bd \u2212 2Y \u039b\u03bd + i * (2X\u039b\u03bd + 2Y K\u03bd ). (33)\n\n1. Started with our own simplex method code in Matlab, which only optimized 4 qubits - gradient-based\nalgorithm was needed.\n\nDenoting\n\u0015\n\u0014\nN B\u03bd \u2212 An\u03bd\n,\nC\u03bd =\nAn\u03bd\n\n\u0014\nD\u03bd =\n\nAB\u03bd0\n\n\u2212 B\u03bd A\nA2\n\n0\n\n\u0015\n(34)\n\nwe find the matrix derivative of L can be written in the\ncompact form\nn\n\n4X\n\u22121\n\u2202L\n= L0 (~t) = N\nC\u03bd D\u03bd ,\n\u2202T\n\u03bd=0\n\n(35)\n\nwhere C\u03bd is a scalar and D\u03bd is a 2n \u00d7 2n matrix. In\nfact, the upper-diagonal of L0 (~t) and imaginary part of\nthe diagonal are of no use to us - values of the gradient\nare seeded in the original locations of t\u03bd , so L0 (~t) has to\nbe disassembled into real and imaginary parts and then\nthe gradient vector has to be filled from the resulting\nmatrices.\n\nIV.\n\nDESCRIPTION OF CODE\n\nThe goal is to scale tomography routines up to a higher\nnumber of qubits on a standard single-processor workstation by refining the tomography algorithms to remove the\nnumerical complexity bottleneck from experimental postprocessing. In this section, we describe how the codes\nwere implemented.\n\nA.\n\nState Tomography Routine\n\nFour routines provide tomography and run in the following order:\n\n2. Wrote the conjugate-gradient routine in Matlab\nusing GSS line search routine and using finitedifference gradient, which allowed for 5-6 qubit tomography.\n3. Applied matrix differential calculus to the gradient and obtained a closed form expression, which\nseverely improved Matlab routines for up to 7\nqubits.\n4. Experimented with Newton's method and successive parabolic interpolation line searches which did\nnot work in the end. This led us to suspect that\nthe region of optimization is not convex.\n5. Re-wrote everything in C using GSL and employed\nGSL's BFGS2 algorithm and its collection of line\nsearches - this pushed our routines to 9 qubits\n(MLE limits to 9 qubits, but not \"Forced Purity\").\nAll code is currently implemented in C using GSL, with\nprototype routines also available in Matlab.\n\nB.\n\nCreating Pseudo-Experimental Data\n\nGenerally, if one wants to simulate a physical state\ncharacterized by \u03c1\u0302physical with 100\u000f% experimental state\nerror, then\n\u03c1\u0302physical = (1 \u2212 \u000f) * \u03c1\u0302theoretical + \u000f * \u03c1\u0302random ,\nwhere \u03c1\u0302theoretical is a density matrix of some desired state\nand \u000f, a real-valued constant, simulates experimental\n\n\f7\n\"state error\" - the physical state always differs from the\nintended state by some small amount; a random density\nmatrix is created as follows:\nR = 2 * rand(2n ) \u2212 1 + i * (2 * rand(2n ) \u2212 1),\n\n\u03c1\u0302random\n\nR\u2020 R\n,\n=\nT r{R\u2020 R}\n\n\u03c4 = [max{\u03bb4 \u2212 \u03bb1 \u2212 \u03bb2 \u2212 \u03bb3 , 0}]2 ,\n\n(36)\n\n(37)\n\nwhere rand function creates a 2n \u00d7 2n matrix of pseudorandom values, sampled from Uniform(0,1) distribution\n[21].\nFor instance, the following results in a noisy GHZ state:\n\u03c1\u0302GHZ\n\nfor n qubits.\nThe tangle (i.e. the square of the concurrence [17]) is\ndefined for 2 qubits, as\n\n1\n= (1 \u2212 \u000f) * |100...01ih100...01| + \u000f * \u03c1\u0302random .\n2\n\nThe simulation routine creates a physical density matrix,\nsimulates experimental measurement outcomes and then\nattempts to reconstruct this density matrix. Knowing\nwhat the reconstructed density matrix should be, enables\nus to compare how well each reconstruction routine works\nfor a certain number of qubits.\nThe expected number of positive outcomes is obtained\nusing Eq. (3), viz:\n\nwhere \u03bb's\n\u221a the eigenvalues of the\n\u221a are the square roots of\nmatrix \u03c1\u0302(\u03c3\u0302y \u2297 \u03c3\u0302y )\u03c1\u0302\u2217 (\u03c3\u0302y \u2297 \u03c3\u0302y ) \u03c1\u0302, which is guaranteed\nto be Hermitian [17] and \u03c3\u0302y \u2297 \u03c3\u0302y is the spin-flip matrix\nand \u03c1\u0302\u2217 is the complex conjugate of density matrix \u03c1\u0302. For\nlarger numbers of qubits, it can be used as a lower bound\non the degree of entanglement [18].\n\nA.\n\nWe observed that for 2 qubits, certain states produce\nfidelities of over 90% using \"Quick and Dirty\" routine\nand consistently high fidelities using MLE. We generated\n2 \u00d7 106 pseudo-random density matrices, that filled the\nentire entropy-tangle plane. Random density matrices\nhad to be biased in order to evenly cover the entire plane.\nFor example, to fill the plane below the Werner state line,\nwe used:\n\uf8ee\n\nn\u0304\u03bd = N T r{\u03a0\u0302\u03bd1 \u2297 \u03a0\u0302\u03bd2 \u2297 ... \u2297 \u03a0\u0302\u03bdn \u03c1\u0302physical },\nwhere N is a constant which is equivalent to the number\nof times repeated projective measurements were taken\n[22]. We then add experimental noise to the measurements [23] using:\n\nLinear Entropy vs Tangle Plane\n\n\uf8ef\n\uf8ef\n\u03c1\u0302\u03c4 = \uf8ef\n\uf8f0\n\n1 \u2212 \u03b42\n0\n0\n\u221a\n\u03b4 1 \u2212 \u03b42\n\n0\n0\n0\n0\n\n\u221a\n0 \u03b4 1 \u2212 \u03b42\n0\n0\n0\n0\n0\n\u03b42\n\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa,\n\uf8fb\n\n(38)\n\nwhich biased the tangle in\n\nn\u03bd = P oisson(n\u0304\u03bd ),\nwhere P oisson(\u03bb) generates a random number from a\nPoisson distribution with mean \u03bb using a probability integral transformation.\n\n\u03c1\u0302trial = \u000f2 * \u03c1\u0302random + (1 \u2212 \u000f2 ) * \u03c1\u0302\u03c4 ,\nwhere\n\u221a\n0 \u2264 \u03b4 \u2264 1/ 2.\n\n0 \u2264 \u000f \u2264 1,\nV.\n\nRESULTS\n\nIn this section, we discuss the conclusions to be drawn\nfrom the numerical trials described in the previous sections. In particular we address the question posed in\nthe title of this paper: Do we always need an expensive\nMLE routine to perform tomography or would \"Quick\nand Dirty\" or \"Forced Purity\" methods suffice?\nWe compare the \"Quick and Dirty\" and \"Forced Purity\" routines to the \"MLE\" routine for states with wide\nvariations of entropy and entanglement. We also show\nhow well these routines scale in runtime and how experimental errors affect the reconstructed states as the\nnumber of qubits increases.\nThe linear entropy, which specifies the degree of purity\nof the state, is defined as\n\nVarying \u03b4 changes tangle and varying \u000f changes entropy. We cycled through 100 \u00d7 100 different combinations of \u03b4 and \u000f and for each setting performed 100 trials,\nsampling \u03c1\u0302random from Uniform(-1,1) probability distribution for each trial. We can also move along the Maximally Entangled Mixed State (MEMS) line by varying\n\u03b3,\n\uf8ee\n\uf8f9\ng(\u03b3)\n0\n0 \u03b3/2\n\uf8fa\n\uf8ef\n\uf8ef 0 1 \u2212 2g(\u03b3) 0 0 \uf8fa\n\u03c1\u0302MEMS = \uf8ef\n\uf8fa,\n\uf8f0 0\n0\n0 0 \uf8fb\n\u03b3/2\n0\n0 g(\u03b3)\nwhere\n(\n\n2n\nSlinear (\u03c1\u0302) = n\n[1 \u2212 T r{\u03c1\u0302}]\n2 \u22121\n\ng(\u03b3) =\n\n\u03b3/2, \u03b3 \u2265 2/3\n1/3, \u03b3 < 2/3\n\n\f8\nand\n\u03c1\u0302trial = \u000f2 * \u03c1\u0302random + (1 \u2212 \u000f2 ) * \u03c1\u0302MEMS .\nWe further sampled 1000 \u00d7 1000 different settings of \u000f\nand \u03b3 to fill the area around MEMS line: In this case\nincreasing \u000f increases the distance from the MEMS line.\n\nFigure 1: On the Slinear and \u03c4 plane you can see that the\n2 \u00d7 106 generated states covered the entire plane; above each\npoint is the corresponding fidelity of the recovered state using\nmaximum likelihood. Notice that most fidelity values lie between 90%-99%. The fidelity values create a thin plane, which\nsuggests that the standard deviation is low for various states.\nHowever, some high-entropy states cannot be recovered well\neven with the expensive MLE procedure.\n\nregardless of the value of \u03c4 , share one thing in common:\neigenvalues. More precisely, for n qubits, eigenvalue 0\noccurs with degeneracy 2n \u2212 1 and eigenvalue 1 occurs\nwith degeneracy 1. So, setting negative eigenvalues to\nzero adjusts the eigenvalues closer to the eigenvalues of\na pure state. If we know that the state is pure ahead\nof time, we can just reset the eigenvalues to the known\nvalues after the linear inversion procedure and obtain the\ndensity matrix - this is further explored in Sec. V C.\n\nFigure 3: Again, same 2 \u00d7 106 states, but this time with\n\"Forced Purity\" performed. Notice that for 2 qubits \"Forced\nPurity\" appears to be worse than \"Quick and Dirty\" even for\npure states - this is not the case when the number of qubits\nincreases, as we explore in the next section.\n\nB.\n\nPerformance for N qubits\n\nIn order to extend this assessment to larger numbers\nof qubits, while still varying the amount of entanglement and disorder, we considered a generalized version\nof the Werner state for n qubits. Since this state slices\nthrough the entire plane presented in Sec. V A, we can\nsee how well tomography operates on states with various\ntangle and entropy values by varying the location along\nthe Werner state line. An adjusted Werner state density\nmatrix is given by:\n\u03c1\u0302Werner = |GHZihGHZ| * \u000f +\nFigure 2: These are the same 2 \u00d7 106 states as in the previous\nfigure and the projection on Slinear and \u03c4 plane is identical.\nNotice how the \"Quick and Dirty\" routine also fails to reach\nhigh fidelity values as the states become more mixed. However, for pure states Quick and Dirty is comparable to MLE\nin fidelity values.\n\nThis suggests that for states with low entropies (pure\nstates), Quick and Dirty routine should work in theory.\nThis is not surprising, as from the spectral decomposition, we can see that all states with Slinear = 0 property,\n\n(1 \u2212 \u000f) \u02c6\n* I,\n2n\n\nwhere\n1\n|GHZi = \u221a {|00...0i + |11...1i}\n2\nand I\u02c6 is the 2n \u00d7 2n identity operator, representing a\nmaximally mixed state. When \u000f = 1 we obtain a state\nlocated at Slinear = 0 and \u03c4 = 1 and when \u000f = 0 we\nobtain Slinear = 1 and \u03c4 = 0. We then vary \u000f from 0\nto 1 in 101 increments and for each value of \u000f perform\ntomography 100 times, for a fixed number of qubits.\n\n\f9\nC.\n\nFigure 4: MLE works well for 2 qubits, but for a higher number of qubits there is a significant drop in fidelity. A 10% state\nerror is quite large, but even at this error pure states are reconstructed better than mixed states as you can see around\n\u000f = 1 (highly entangled pure state).\n\n\"Forced Purity\" Tomography for Pure States\n\nIn order for \"Forced Purity\" to work, the measurement\noutcomes have to be sufficiently close to their true values. To address the issue of 'how close', we simulated\npure states, with 11 distinct tangle values evenly spaced\nbetween 0 and 1 and then started with N = 10 - i.e. N\nrepeated projective measurements for each measurement\noutcome. We then increased N by one at each iteration and repeated \"Forced Purity\" tomography for some\nnumber of qubits. As soon as N allowed \"Forced Purity\"\nto perform tomography at 90% fidelity, the routine was\nterminated and the value for N recorded.\nOne of the possible reasons why MLE tomography did\nnot yield high fidelity values for a larger number of qubits\nis that it also required more accurate estimates of the\nmeasurement outcomes. Because MLE produced almost\nequal fidelities to \"Forced Purity\" for pure states, we\nwould expect the same number of N to work for MLE\ntomography.\n\nD.\n\nFigure 5: \"Quick and Dirty\" routine is not performing very\nwell even for pure states, although for a certain number of\nqubits it appears to work. On the contrary, it appears to be\nimproving as the number of qubits is increasing.\n\nRuntime Analysis\n\nSection V C suggests that pure states do not require\nexpensive MLE techniques for tomography. Nonetheless,\nit is interesting to see how MLE scales in runtime compared to Quick and Dirty and \"Forced Purity\" routines.\nHere we present runtimes and fidelity estimates for a pure\nstate with \u03c4 \u2248 0.5 and a slightly mixed state with the\nsame tangle value which lies on the Werner state line.\nWe also show that even the expensive MLE routine decreases in fidelity as we increase the number of qubits.\nFor this analysis we assume that an experiment can be\nperformed a sufficiently large number of times, 106 to be\nexact.\nIn conclusion, \"Forced Purity\" results in lower Fidelity\nvalues for 2 qubits than \"Quick and Dirty\", but then\nincreases in Fidelity and converges to MLE's fidelity for\na higher number of qubits.\nn\nMLE\nIteration Time Q & D\nFP\n2\n40 \u00b1 10\n1.0 \u00b1 0.5\n0.19 \u00b1 0.05 0.17 \u00b1 0.04\n3 1000 \u00b1 300\n12 \u00b1 8\n0.29 \u00b1 0.03 0.26 \u00b1 0.02\n4 (16 \u00b1 3) 103\n180 \u00b1 70\n1.1 \u00b1 0.1 1.1 \u00b1 0.2\n5 (35 \u00b1 6) 104\n(4 \u00b1 1) 103\n9.4 \u00b1 4\n12 \u00b1 4\n6 (6.0 \u00b1 0.3) 106 (60 \u00b1 16) 103 58 \u00b1 2.5\n60 \u00b1 5.4\n\nFigure 6: For \"Forced Purity\" we can confidently say that the\nroutine is improving significantly, as the number of qubits\nis increasing. And for pure entangled states this routine is\nalmost as good as MLE, but runs at a fraction of the time!\n\nTable II: Runtimes in milliseconds for a state at \u03c4 \u2248 0.5 and\nSlinear = 0. \"Iteration Time\" measures how long the line\nsearch routine takes for each iteration of the BFGS routine.\nAbbreviations: MLE: Complete Maximum Likelihood reconstruction; QD: \"Quick and Dirty\" method; FP: \"Forced Purity\".\n\n\f10\nn\nMLE\nIteration Time\nQ&D\nFP\n2\n41 \u00b1 10\n0.77 \u00b1 0.4\n0.060 \u00b1 0.005 0.038 \u00b1 0.006\n3 1200 \u00b1 200\n12 \u00b1 20\n0.17 \u00b1 0.003 0.36 \u00b1 0.6\n4 (19 \u00b1 1) 103\n200 \u00b1 100\n1.3 \u00b1 0.4\n1.1 \u00b1 0.1\n5 (29 \u00b1 2) 104\n2900 \u00b1 900\n7.6 \u00b1 0.6\n8.8 \u00b1 2\n6 (6.7 \u00b1 0.4) 106 (7 \u00b1 2) 104\n65 \u00b1 5\n67 \u00b1 5\nTable III: Runtimes in milliseconds for a state at \u03c4 \u2248 0.5 and\nSlinear to that along the Werner state line; abbreviations same\nas Table II.\n\nFigure 9: Using the same procedure as in Fig. 8 we then\nperformed \"Forced Purity\" on each state. We said that for\nthe Werner state line \"Forced Purity\" routine improves overall\nas the number of qubits increases. This is not the case for\npure states, and as we can clearly see \"Forced Purity\" slightly\ndecoheres, but overall still remains at over 90% fidelity even\nfor 7 qubits.\n\nVI.\nFigure 7: For each number of qubits, this plot shows the\nnumber of times each projection measurement has to be repeated for a state, in order to obtain an accurate estimate\nof the measurement outcome. With a 5% state error, this\nnumber of measurements will allow the Forced Purity routine\nto estimate the state with 90% fidelity. There is an exponential increase in how many times the experiment has to be\nrepeated.\n\nCONCLUSION\n\nWe have demonstrated that if the experiments can\nbe performed a sufficient number of times, then using\n\"Forced Purity\" routine, tomography can be performed\nin a quick and robust manner. However, as the entropy\nof a state increases, a much more expensive MLE routine has to be used to perform tomography, which does\nnot scale well as the number of qubits increases. Quantum computing requires only pure state tomography, for\nwhich we have obtained a scalable and efficient routine\n[24].\n\nAcknowledgments\n\nThe authors would like to thank Robin Blume-Kohout,\nRen\u00e9 Stock and Rob Adamson for stimulating discussions\nand useful comments. This work was supported by the\nU.S. Army Research Office, NSERC and Project OpenSource.\n\nFigure 8: This graph shows the average fidelities of estimated\npure states recovered using the \"Quick and Dirty\" approach\nfor different numbers of qubits at 5% state error. In total\n11 tangle values equally spaced between 0 and 1 were used,\nand 10 recoveries performed for each tangle value. This shows\nthat while \"Quick and Dirty\" appears to work for 2 qubits,\nin the long run it linearly worsens and cannot be used as a\nsuitable tomography algorithm.\n\nAppendix A: MATRIX DIFFERENTIAL\nCALCULUS THEOREMS\n\nThe following theorems were used to derive an analytic\nexpression to the gradient of the MLE function, which is\nmore computationally efficient than the finite-difference\ngradient computation as the MLE function itself is\nexpensive to evaluate.\n\n\f11\nIf M is a real-valued matrix and T = X + i * Y, T \u2208 C,\nthen [11, 12, 13]:\n\u2202M\n\u2202M\n\u2202M\n=\n+i*\n.\n\u2202T\n\u2202X\n\u2202Y\n\nThen using Eq. (A2) and Eq. (A3)\n\u2202T r{\u03a6}\n= 2X + i * Y \u2212 i * Y = 2X\n\u2202X\n\n(A1)\nand\n\nThe following are defined for real square matrices [13]:\n\u2202T r{X T Y }\n\u2202X\n\u2202T r{X T X}\n\u2202X\n\u2202T r{KX T Y }\n\u2202X\n\u2202T r{KX T X}\n\u2202X\n\n= Y,\n\n\u2202T r{\u03a6}\n= i * X \u2212 i * X + 2Y = 2Y.\n\u2202Y\n\n(A2)\n\nThis is consistent with our result in Eq. (A7).\n= 2X,\n\n(A3)\n\n= Y K,\n\n(A4)\n\n= XK + XK T .\n\n(A5)\n\nWe begin by observing that\n\u2020\n\nT r{T (~t)T (~t)} =\n\nn\n4X\n\u22121\n\nt2\u03bd ,\n\n(A6)\n\n\u03bd=0\n\nNext, we set out to compute\n\u03a0\u0302\u03bd = K\u03bd + i * \u039b\u03bd , hence,\n\n\u2202T r{\u03a0\u0302\u03bd \u03a6}\n= XK\u03bd + XK\u03bdT \u2212 Y \u039b\u03bd + Y \u039bT\u03bd\n\u2202X\n\n(A9)\n\n\u2202T r{\u03a0\u0302\u03bd \u03a6}\n= X\u039b\u03bd \u2212 X\u039bT\u03bd + Y K\u03bd + Y K\u03bdT .\n\u2202Y\n\n(A10)\n\n\u2020\n\nand\n\nHence, using matrix calculus, we have the result\n\u2202T r{T \u2020 (~t)T (~t)}\n= 2X + i * 2Y.\n\u2202T\n\nRecall that\n\n\u03a0\u0302\u03bd \u03a6 = (K\u03bd + i * \u039b\u03bd )(X T X + i * X T Y \u2212 i * Y T X + Y T Y )\n= K\u03bd X T X + K\u03bd Y T Y + \u039b\u03bd Y T X \u2212 \u039b\u03bd X T Y +\ni * (K\u03bd X T Y + \u039b\u03bd X T X + \u039b\u03bd Y T Y \u2212 K\u03bd Y T X).\n(A8)\nApplying Eq. (A4) and Eq. (A5) to the real part of\nEq. (A8) we obtain\n\nwhich immediately implies that\n\u2202T r{T (~t)T (~t)}\n= 2t\u03bd .\n\u2202t\u03bd\n\n\u2202T r{\u03a0\u0302\u03bd \u03a6}\n.\n\u2202T\n\n(A7)\n\nEq. (A7) is a compact means of stating the result of\nEq. (A6) using a 2n \u00d7 2n matrix, where the value of the\nderivative is stored in the original position of t\u03bd in the\nCholesky-decomposed matrix T (~t). This is the general\nidea behind all matrix calculus results we have used. We\ncould have also obtained the same result by applying matrix calculus directly. For example, denote\n\nObserve that \u2200\u03bd, \u03a0\u0302\u03bd = \u03a0\u0302\u2020\u03bd yields \u039b\u03bd = \u2212\u039bT\u03bd and K\u03bd =\nK\u03bdT , therefore\n\u2202T r{\u03a0\u0302\u03bd \u03a6}\n= 2XK\u03bd \u2212 2Y \u039b\u03bd ,\n\u2202X\n\u2202T r{\u03a0\u0302\u03bd \u03a6}\n= 2X\u039b\u03bd + 2Y K\u03bd .\n\u2202Y\n\n(A11)\n(A12)\n\n\u03a6 = T \u2020 (~t)T (~t).\n\nSubstituting the above two equations into Eq. (A1) we\nobtain the result described in Sec. III B 2.\n\n[1] U. Leonhardt, Measuring the quantum state of light\n(Cambridge University Press, 1997).\n[2] M. G. A. Paris and J. \u0158eh\u00e1\u010dek (eds.), Quantum State\nEstimation, Lecture Notes in Physics 649 (Springer, Heidelberg, 2004).\n[3] M. Hayashi (ed.), Asymptotic Theory of Quantum Statistical Inference: Selected Papers (World Scientific, Singapore, 2005).\n[4] D. F. V. James, P. G. Kwiat, W. J. Munro, and A. G.\nWhite, Phys. Rev. A 64, 052312 (2001).\n[5] H. H\u00e4ffner, W. Hansel, C. F. Roos, J. Benhelm, D. Chekal-kar, M. Chwalla, T. K\u00f6rber, U. D. Rapol, M. Riebe, P.\nO. Schmidt, C. Becher, O. G\u00fchne, E. D\u00fcr, and R. Blatt,\nNature (London) 438, 643 (2005).\n[6] H. H\u00e4ffner, private communication.\n\n[7] See, for example, C. Cohen-Tannoudji, B. Diu and F.\nLalo\u00eb, Quantum Mechanics (John Wiley: New York,\n1977), Complement DIII.\n[8] We assume the standard form for the Pauli matrices, with\nT r{\u03c3\u0302\u03bc \u03c3\u0302\u03bd } = 2 * \u03b4\u03bc,\u03bd , (\u03bc, \u03bd = 0, 1, 2, 3); see, for example,\nC. Cohen-Tannoudji, et al., op. cit., Complement AIV.\nThe matrix \u03c3\u03020 is the two-dimentional identity operator.\n[9] W. H. Press, Numerical recipes in C: The art of scientific\ncomputing 2nd ed., (Cambridge University Press, 1992).\n[10] Michael T. Heath, Scientific Computing: An Introductory Survey, 2nd Edition (McGraw Hill, Toronto, 2002).\n[11] J. R. Magnus and H. Neudecker, Matrix Differential\nCalculus with Applications in Statistics and Economics\n(John Wiley and Sons Ltd., Toronto, 1988).\n[12] D. A. Turkington, Matrix Calculus & Zero-One Matrices\n\n\f12\n(Cambridge University Press, New York, 2002).\n[13] A. Graham, Kronecker Products and Matrix Calculus:\nWith Applications (Halsted Press, Toronto, 1981).\n[14] We thank to all the users of Matlab File Exchange whose\ncodes we have used to produce better graphics for the\nplots in this paper.\n[15] R. L. Kosut, A. Shabani, and D.A. Lidar, Phys. Rev.\nLett. 100, 020502 (2008).\n[16] R. L. Kosut, I. Walmsley, and H. Rabitz, arXiv:quantph/0411093 (2004).\n[17] W. K. Wootters, Phys. Rev. Lett. 80, 2245 (1998).\n[18] A. Wong and N. Christensen, Phys. Rev. A 63, 044301\n(2001).\n[19] R. Jozsa, J. Mod. Opt. 41, 2315 (1994).\n[20] GNU Scientific Library, http://www. gnu. org/ software/\ngsl/ .\n\n[21] See the Matlab online manual at http://www. mathworks. com .\n[22] N was set to 104 in our tomographic routine, but it can\nbe any positive integer as long as n\u03bd values resemble realistic photon counts, and not fractions less than 1.\n[23] Note that if the measurements are performed with zero\nnoise, then the Linear Reconstruction routine performs\ntomography of the density matrix with Fidelity value of\n1.\n[24] The runtimes of the routines mentioned in this paper can\nbe improved linearly using parallel computation. However, because the complexity increases exponentially with\nthe number of qubits, an efficient routine to performing\ntomography is crucial.\n\n\f"}