{"id": "http://arxiv.org/abs/cond-mat/0411428v1", "guidislink": true, "updated": "2004-11-17T00:24:49Z", "updated_parsed": [2004, 11, 17, 0, 24, 49, 2, 322, 0], "published": "2004-11-17T00:24:49Z", "published_parsed": [2004, 11, 17, 0, 24, 49, 2, 322, 0], "title": "Continuous extremal optimization for Lennard-Jones Clusters", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0411604%2Ccond-mat%2F0411078%2Ccond-mat%2F0411002%2Ccond-mat%2F0411417%2Ccond-mat%2F0411536%2Ccond-mat%2F0411215%2Ccond-mat%2F0411018%2Ccond-mat%2F0411426%2Ccond-mat%2F0411552%2Ccond-mat%2F0411576%2Ccond-mat%2F0411729%2Ccond-mat%2F0411419%2Ccond-mat%2F0411016%2Ccond-mat%2F0411464%2Ccond-mat%2F0411334%2Ccond-mat%2F0411356%2Ccond-mat%2F0411364%2Ccond-mat%2F0411144%2Ccond-mat%2F0411427%2Ccond-mat%2F0411723%2Ccond-mat%2F0411403%2Ccond-mat%2F0411669%2Ccond-mat%2F0411226%2Ccond-mat%2F0411282%2Ccond-mat%2F0411639%2Ccond-mat%2F0411305%2Ccond-mat%2F0411231%2Ccond-mat%2F0411535%2Ccond-mat%2F0411714%2Ccond-mat%2F0411435%2Ccond-mat%2F0411443%2Ccond-mat%2F0411248%2Ccond-mat%2F0411270%2Ccond-mat%2F0411318%2Ccond-mat%2F0411704%2Ccond-mat%2F0411436%2Ccond-mat%2F0411076%2Ccond-mat%2F0411431%2Ccond-mat%2F0411553%2Ccond-mat%2F0411485%2Ccond-mat%2F0411260%2Ccond-mat%2F0411740%2Ccond-mat%2F0411286%2Ccond-mat%2F0411160%2Ccond-mat%2F0411203%2Ccond-mat%2F0411091%2Ccond-mat%2F0411724%2Ccond-mat%2F0411083%2Ccond-mat%2F0411363%2Ccond-mat%2F0411044%2Ccond-mat%2F0411746%2Ccond-mat%2F0411754%2Ccond-mat%2F0411126%2Ccond-mat%2F0411271%2Ccond-mat%2F0411324%2Ccond-mat%2F0411401%2Ccond-mat%2F0411320%2Ccond-mat%2F0411183%2Ccond-mat%2F0411062%2Ccond-mat%2F0411600%2Ccond-mat%2F0411258%2Ccond-mat%2F0411674%2Ccond-mat%2F0411556%2Ccond-mat%2F0411713%2Ccond-mat%2F0411437%2Ccond-mat%2F0411349%2Ccond-mat%2F0411570%2Ccond-mat%2F0411385%2Ccond-mat%2F0411329%2Ccond-mat%2F0411122%2Ccond-mat%2F0411462%2Ccond-mat%2F0411609%2Ccond-mat%2F0411648%2Ccond-mat%2F0411605%2Ccond-mat%2F0411309%2Ccond-mat%2F0411332%2Ccond-mat%2F0411240%2Ccond-mat%2F0411263%2Ccond-mat%2F0411504%2Ccond-mat%2F0411149%2Ccond-mat%2F0411602%2Ccond-mat%2F0411201%2Ccond-mat%2F0411104%2Ccond-mat%2F0411653%2Ccond-mat%2F0411195%2Ccond-mat%2F0411264%2Ccond-mat%2F0411146%2Ccond-mat%2F0411333%2Ccond-mat%2F0411524%2Ccond-mat%2F0411041%2Ccond-mat%2F0411634%2Ccond-mat%2F0411532%2Ccond-mat%2F0411310%2Ccond-mat%2F0411538%2Ccond-mat%2F0411428%2Ccond-mat%2F0411721%2Ccond-mat%2F0411035%2Ccond-mat%2F0411377%2Ccond-mat%2F0411710%2Ccond-mat%2F0411658%2Ccond-mat%2F0411425&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Continuous extremal optimization for Lennard-Jones Clusters"}, "summary": "In this paper, we explore a general-purpose heuristic algorithm for finding\nhigh-quality solutions to continuous optimization problems. The method, called\ncontinuous extremal optimization(CEO), can be considered as an extension of\nextremal optimization(EO) and is consisted of two components, one is with\nresponsibility for global searching and the other is with responsibility for\nlocal searching. With only one adjustable parameter, the CEO's performance\nproves competitive with more elaborate stochastic optimization procedures. We\ndemonstrate it on a well known continuous optimization problem: the\nLennerd-Jones clusters optimization problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0411604%2Ccond-mat%2F0411078%2Ccond-mat%2F0411002%2Ccond-mat%2F0411417%2Ccond-mat%2F0411536%2Ccond-mat%2F0411215%2Ccond-mat%2F0411018%2Ccond-mat%2F0411426%2Ccond-mat%2F0411552%2Ccond-mat%2F0411576%2Ccond-mat%2F0411729%2Ccond-mat%2F0411419%2Ccond-mat%2F0411016%2Ccond-mat%2F0411464%2Ccond-mat%2F0411334%2Ccond-mat%2F0411356%2Ccond-mat%2F0411364%2Ccond-mat%2F0411144%2Ccond-mat%2F0411427%2Ccond-mat%2F0411723%2Ccond-mat%2F0411403%2Ccond-mat%2F0411669%2Ccond-mat%2F0411226%2Ccond-mat%2F0411282%2Ccond-mat%2F0411639%2Ccond-mat%2F0411305%2Ccond-mat%2F0411231%2Ccond-mat%2F0411535%2Ccond-mat%2F0411714%2Ccond-mat%2F0411435%2Ccond-mat%2F0411443%2Ccond-mat%2F0411248%2Ccond-mat%2F0411270%2Ccond-mat%2F0411318%2Ccond-mat%2F0411704%2Ccond-mat%2F0411436%2Ccond-mat%2F0411076%2Ccond-mat%2F0411431%2Ccond-mat%2F0411553%2Ccond-mat%2F0411485%2Ccond-mat%2F0411260%2Ccond-mat%2F0411740%2Ccond-mat%2F0411286%2Ccond-mat%2F0411160%2Ccond-mat%2F0411203%2Ccond-mat%2F0411091%2Ccond-mat%2F0411724%2Ccond-mat%2F0411083%2Ccond-mat%2F0411363%2Ccond-mat%2F0411044%2Ccond-mat%2F0411746%2Ccond-mat%2F0411754%2Ccond-mat%2F0411126%2Ccond-mat%2F0411271%2Ccond-mat%2F0411324%2Ccond-mat%2F0411401%2Ccond-mat%2F0411320%2Ccond-mat%2F0411183%2Ccond-mat%2F0411062%2Ccond-mat%2F0411600%2Ccond-mat%2F0411258%2Ccond-mat%2F0411674%2Ccond-mat%2F0411556%2Ccond-mat%2F0411713%2Ccond-mat%2F0411437%2Ccond-mat%2F0411349%2Ccond-mat%2F0411570%2Ccond-mat%2F0411385%2Ccond-mat%2F0411329%2Ccond-mat%2F0411122%2Ccond-mat%2F0411462%2Ccond-mat%2F0411609%2Ccond-mat%2F0411648%2Ccond-mat%2F0411605%2Ccond-mat%2F0411309%2Ccond-mat%2F0411332%2Ccond-mat%2F0411240%2Ccond-mat%2F0411263%2Ccond-mat%2F0411504%2Ccond-mat%2F0411149%2Ccond-mat%2F0411602%2Ccond-mat%2F0411201%2Ccond-mat%2F0411104%2Ccond-mat%2F0411653%2Ccond-mat%2F0411195%2Ccond-mat%2F0411264%2Ccond-mat%2F0411146%2Ccond-mat%2F0411333%2Ccond-mat%2F0411524%2Ccond-mat%2F0411041%2Ccond-mat%2F0411634%2Ccond-mat%2F0411532%2Ccond-mat%2F0411310%2Ccond-mat%2F0411538%2Ccond-mat%2F0411428%2Ccond-mat%2F0411721%2Ccond-mat%2F0411035%2Ccond-mat%2F0411377%2Ccond-mat%2F0411710%2Ccond-mat%2F0411658%2Ccond-mat%2F0411425&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper, we explore a general-purpose heuristic algorithm for finding\nhigh-quality solutions to continuous optimization problems. The method, called\ncontinuous extremal optimization(CEO), can be considered as an extension of\nextremal optimization(EO) and is consisted of two components, one is with\nresponsibility for global searching and the other is with responsibility for\nlocal searching. With only one adjustable parameter, the CEO's performance\nproves competitive with more elaborate stochastic optimization procedures. We\ndemonstrate it on a well known continuous optimization problem: the\nLennerd-Jones clusters optimization problem."}, "authors": ["Tao Zhou", "Wen-Jie Bai", "Long-Jiu Cheng", "Bing-Hong Wang"], "author_detail": {"name": "Bing-Hong Wang"}, "author": "Bing-Hong Wang", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1103/PhysRevE.72.016702", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cond-mat/0411428v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cond-mat/0411428v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "5 pages and 3 figures", "arxiv_primary_category": {"term": "cond-mat.mtrl-sci", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat.mtrl-sci", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cond-mat/0411428v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cond-mat/0411428v1", "journal_reference": "Physical Review E 72, 016702(2005)", "doi": "10.1103/PhysRevE.72.016702", "fulltext": "Continuous extremal optimization for Lennard-Jones Clusters\nTao Zhou1 ,\u2217 Wen-Jie Bai2 , Long-jiu Cheng2 , and Bing-Hong Wang1\u2020\n\narXiv:cond-mat/0411428v1 [cond-mat.mtrl-sci] 17 Nov 2004\n\n1\n\nNonlinear Science Center and Department of Modern Physics,\nUniversity of Science and Technology of China,\nHefei Anhui, 230026, PR China\n2\nDepartment of Chemistry,\nUniversity of Science and Technology of China,\nHefei Anhui, 230026, PR China\n(Dated: October 23, 2018)\n\nIn this paper, we explore a general-purpose heuristic algorithm for finding high-quality solutions\nto continuous optimization problems. The method, called continuous extremal optimization(CEO),\ncan be considered as an extension of extremal optimization(EO) and is consisted of two components,\none is with responsibility for global searching and the other is with responsibility for local searching.\nWith only one adjustable parameter, the CEO's performance proves competitive with more elaborate\nstochastic optimization procedures. We demonstrate it on a well known continuous optimization\nproblem: the Lennerd-Jones clusters optimization problem.\nPACS numbers: 36.40.-c,02.60.Pn,05.65.+b\n\nI.\n\nINTRODUCTION\n\nThe optimization of system with many degrees of freedom with respect to some cost function is a frequently\nencountered task in physics and beyond. One special\nclass of algorithms used for finding the high-quality solutions to those NP-hard optimization problems is the\nso-called nature inspired algorithms, including simulated\nannealing(SA)[1, 2], genetic algorithms(GA)[3, 4, 5], genetic programming(GP)[6], and so on.\nIn recent years, a novel nature inspired algorithm named extremal optimization(EO) is proposed by\nBoettcher and Percus[7, 8, 9, 10, 11], which is very\nsententious and competitive comparing with some well\nknown algorithms like SA, GA, GP etc.. To make the\nunderlying mechanism of EO more concrete, let's focus\non the natural selection of biological system. In nature,\nhighly specialized, complex structure often emerge when\ntheir most inefficient elements are selectively driven to\nextinction. For example, evolution progresses by selecting against the few most poorly adapted species, rather\nthan by expressly breeding those species best adapted to\ntheir environment. The principle that the least fit elements are progressively eliminated has been applied successfully in the Bak-Sneppen model[12, 13], where each\nindividual corresponding a certain species is characterized by a fitness value, and the least fit one with smallest\nfitness value and its closest dependent species are successively selected for adaptive changes. The extremal optimization algorithm draws upon the Bak-Sneppen mechanism, yielding a dynamic optimization procedure free of\nselection parameters.\n\n\u2217 Electronic\n\u2020 Electronic\n\naddress: zhutou@ustc.edu\naddress: bhwang@ustc.edu.cn\n\nHere we consider a general optimization problem,\nwhere the system consists of N elements, and we wish\nto minimize the cost function C(S) depending on the\nsystem configuration S. The EO algorithm proceeds as\nfollows:\n(1) Choose an initial configuration S of the system at\nwill; set Sbest := S.\n(2) Evaluate the fitness value fi for each individual i and\nrank each individual according to its fitness value so as\nto the least fit one is in the top. Use ki to denote the\nindividual i's rank, clearly, the least fit one is of rank 1.\nChoose one individual j that will be changed with the\nprobability P (kj ), and then, only randomly change the\nstate of j and keep all other individuals' state unaltered.\nAccept the new configuration S \u2032 unconditionally S := S \u2032 ,\nand if C(S) < C(Sbest ) then set Sbest := S.\n(3) Repeat at step (2) as long as desired.\n(4) Return Sbest and C(Sbest ).\nThe efficiency of EO algorithm is sensitive to the probability function P (k). In basic EO, P (1) = 1 and for any\nk(2 \u2264 k \u2264 N ), P (k) = 0. A more efficient algorithm, the\nso-called \u03c4 -EO, can be obtained through a slight modification from basic EO. In \u03c4 -EO, P (k) \u223c k \u2212\u03c4 where \u03c4 > 0.\nOf course, aiming at idiographic optimization problems,\none can design various forms of P (k) to improve the performance of basic EO. For example, Middleton has proposed the jaded extremal optimization(JEO) method for\nIsing spin glass system by reducing the probability of\nflipping previously selected spins, which remarkably improved the efficiency of EO[14].\nThe previous studies indicate that EO algorithm can\noften outperform some far more complicated or finely\ntuned algorithm, such as SA or GA, on some famous NPhard[15] discrete optimization problems, including graph\npartitioning[7, 8, 16], travelling salesman problem[7],\nthree-coloring problem[17, 18], finding lowest energy configuration for Iring spin glass system[14, 17, 19], and so\n\n\f2\n\n(a)\n-181.0\n-128.21\n\n-128.22\n\n-128.23\n\n<H>\n\n-181.5\n-128.24\n\n-128.25\n\n-128.26\n\n-182.0\n\n-128.27\n\n<H>\n\non. However, many practical problems can not be abstracted to discrete form, thus to investigate EO's efficiency on continuous optimization problems[20] is not\nonly of theoretic interest, but also of prominent practical\nworthiness.\nIn this paper, a so-called continuous extremal optimization(CEO) algorithm aiming at continuous optimization problem will be introduced, which can be considered as a mixing algorithm consisting of two components, one is with responsibility for global searching\nand the other is with responsibility for local searching.\nWith only one adjustable parameter, the CEO's performance proves competitive with more elaborate stochastic\noptimization procedures. We demonstrate it on a well\nknown continuous optimization problem: the LennerdJones(LJ) clusters optimization problem.\nThis paper is organized as follows: in section 2, the\nLJ clusters optimization problem will be briefly introduced. In section 3, we will give the algorithm proceeds\nof CEO. Next, we give the computing results about the\nperformance of CEO on LJ clusters optimization problem. Finally, in section 5, the conclusion is drawn and\nthe relevances of the CEO to the real-life problems are\ndiscussed.\n\n1.0\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2.0\n\n-182.5\n\n-183.0\n\n-183.5\n1.0\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2.0\n\n(b)\n0.40\n\n0.35\n\n0.30\n\n0.25\n\nLENNERD-JONES CLUSTERS\nOPTIMIZATION PROBLEM\n\nR\n\nII.\n\n0.85\n\n0.80\n\n0.20\n\n0.75\n\n0.70\n\nV (r) =\n\n1\n1\n\u2212 6\nr12\nr\n\n(1)\n\nwhere r is the distance between two atoms.\nThis po\u221a\ntential has a single minimum at r = 6 2, which is the\nequilibrium distance of two atoms. It can, of course, easily be reduced to an arbitrary LJ-potential by a simple\nrescaling of length and energy units. The ith atom has\nenergy\nEi =\n\n1X\nV(rij )\n2\nj6=i\n\n(2)\n\nR\n\n0.65\n\nContinuous optimization problem is ubiquitous in materials science: many situation involve finding the structure of clusters and the dependence of structure on size is\nparticularly complex and intriguing. In practice, we usually choose a potential function to take the most steady\nstructure since it's considered to be in possession of the\nminima energy. However, in all but the simplest cases,\nthese problem are complicated due to the presence of\nmany local minima. Such problem is encountered in\nmany area of science and engineering, for example, the\nnotorious protein folding problem[21].\nAs one of the simplest models that exhibits such behavior [22] one may consider the problem of finding the\nground-state structure of nanocluster of atoms interacting through a classical Lennerd-Jones pair potential, in\nreduced units given by\n\n0.15\n\n0.60\n\n0.55\n\n0.50\n\n0.45\n\n0.40\n\n0.10\n\n1.0\n\n1.0\n\n1.2\n\n1.2\n\n1.4\n\n1.4\n\n1.6\n\n1.6\n\n1.8\n\n2.0\n\n1.8\n\n2.0\n\nFIG. 1: The details of \u03c4 -CEO for \u03c4 \u2208 [1, 2]. Figure 1a shows\nthe average energies obtained by CEO over 200 runs, and\nfigure 1b exhibits the success rate of hitting the global minima\nin 200 runs[27]. For both figure 1a and 1b, the main plot and\ninset represent the case N = 40 and N = 30 respectively. One\ncan find that, the best \u03c4 corresponding lowest average energy\nand highest success rate is approximate to 1.5.\n\nand the total energy for N atoms is\nX\nEi\nE=\n\n(3)\n\ni\n\nThe optimization task is to find the configuration with\nminimum total potential energy of a system of N atoms,\neach pair interacting by potential of the form (1). Clearly,\na trivial lower bound for the total energy is \u2212N (N \u22121)/2,\nobtained when one assumes that all pairs are at their\nequilibrium separation. For N = 2, 3, 4 the lower bound\ncan actually be obtained in three-dimensional space, corresponding respectively to a dimer, equilateral triangle,\n\n\f3\n\n(a)\n0\n\n-100\n\n-200\n\n-300\n\nE\n\nand regular tetrahedron, with all interatomic distance\nequal to 1. However, from N = 5 onwards it is not possible to place all the atoms simultaneously at the potential\nminimum of all others and the ground-state energy is\nstrictly larger than the trivial lower bound. This system\nhas been studied intensely [23] and is known to have an\nexponential increasing number of local minima, growing\n2\nroughly as e0.36N +0.03N near N = 13, at which point\nthere are already at least 988 minima [23]. If this scaling\ncontinues, more than 10140 local minima exist when N\napproach 100.\n\n-400\n\n-500\n\nThe global minimum enengy\nThe average energy obtained by CEO\n\nIII.\n\nCONTINUOUS EXTREMAL\nOPTIMIZATION\n\n-600\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nN\n\nThe continuous extremal optimization algorithm is\nconsisted of two components, one is the classical EO algorithm with responsibility for global searching, and the\nother is a certain local searching algorithm. We give the\ngeneral form of CEO algorithm by way of the LJ clusters\noptimization problem as follows:\n(1) Choose an initial state of the system, where all\nthe atoms are placed within a spherical container with\nradius[24, 25]\n\n\u221a\n6\n\n1.0\n0.10\n\n0.08\n\n0.8\n\nR\n\n0.06\n\n0.6\n\n0.04\n\n0.02\n\n(4)\n\nwhere re = 2 is the equilibrium distance and N denotes\nthe number of atoms. Set the minimal energy Emin = 0.\n(2) Use a certain local searching algorithm to find the\nlocal minimum from the current configuration of system.\nIf the local minimal energy is lower than Emin , then replace Emin by the present local minimum.\n(3) Rank each atom according to its energy obtained by\nEqu.(2). Here, the atom who has highest energy is the\nleast fit one and is arranged in the top of the queue.\nChoose one atom j that will be changed with the probability P (kj ) where kj denotes the rank of atom j, and\nthen, only randomly change the coordinates of j and keep\nall other atoms' positions unaltered. Accept the new configuration unconditionally.\n(4) Repeat at step (2) as long as desired.\n(5) Return the minimal energy Emin and the corresponding configuration.\nFor an idiographic problem, one can attempt various\nlocal searching algorithms and pitch on the best one.\nIn this paper, for the LJ clusters optimization problem,\nwe choose limited memory BFGS method(LBFGS) qua\nthe local searching algorithm. The BFGS method is an\noptimization technique based on quasi-Newton method\nproposed by Broyden, Fletcher, Goldfard and Shanno.\nLBFGS proposed by Liu and Nocedal[25, 26] is especially effective on problems involving a large number of\nvariables. In this method, an approximation Hk to the\ninverse of the Hessian is obtained by applying M BFGS\nupdates to a diagonal matrix H0 , using information from\n\nR\n\n1\n3N\nradius = re [ + ( \u221a )1/3 ],\n2\n4\u03c0 2\n\n(b)\n\n0.4\n\n0.00\n\n50\n\n60\n\n70\n\nN\n\n80\n\n90\n\n100\n\n0.2\n\n0.0\n\n0\n\n20\n\n40\n\nN\n\n60\n\n80\n\n100\n\nFIG. 2: The performance of CEO algorithm on LJ clusters\noptimization problem. In figure 2a, the red circles represent\nthe average energies obtained by CEO over 200 runs, where\nthe black squares represent the global minima. Figure 2b\nshows the success rate of hitting the global minima in 200\nruns, the inset is the success rate for N > 50 that may be\nunclear in the main plot.\n\nthe previous M steps. The number M determines the\namount of storage required by the routine, which is specified by the user, usually 3 \u2264 M \u2264 7 and in our computation M is fixed as 4.\n\nIV.\n\nCOMPUTING RESULTS\n\nSimilar to \u03c4 -EO, we use \u03c4 -CEO algorithm for the\nLJ clusters optimization problem, where the probability\n2\nfunction of CEO is P (k) \u223c k \u2212\u03c4 . Since there are N2 pairs\nof interactional atoms in a LJ cluster of size N , we require\n\n\f4\n\nCPU Time (ms)\n\n100000\n\n10000\n\n1000\n\nSlope=3.886\n\n100\n\n10\n\nN\n\n100\n\nFIG. 3: The average CPU time(ms) over 200 runs va the size\nof LJ clusters. In the log-log plot, the data can be well fitted\nby a straight line with slope 3.886 \u00b1 0.008, which indicates\nthat the increasing tendency of CPU time T vs cluster size is\napproximate to power-law form as T \u223c N 3.886 .\n\n\u03b1N 2 updates where \u03b1 is a constant and fixed as 100 in\nthe following computation. In order to avoid falling into\nthe same local minimum too many times, before running LBFGS, we should make the system configuration\nfar away from last local minimum, thus we run LBFGS\nevery 20 time steps. That is to say, for a LJ cluster of\nsize N , the present algorithm runs EO 100N 2 times and\nLBFGS 5N 2 times in total.\nWe have carried out the \u03c4 -CEO algorithm so many\ntimes for different \u03c4 and N , and find that the algorithm\nperforms better when \u03c4 is in the interval [1,2]. In figure 1,\nwe report the details for 1 \u2264 \u03c4 \u2264 2, where figure 1a shows\nthe average energies obtained by CEO over 200 runs, and\nfigure 1b exhibits the success rate R of hitting the global\nminima[27]. For both figure 1a and 1b, the main plot and\ninset represent the case N = 40 and N = 30 respectively.\nThe readers should note that, although the difference of\naverage energies between two different \u03c4 is great in the\nplot, it is very very small in fact. For the case N = 40,\nthe best value of \u03c4 is \u03c4 = 1.6 corresponding the lowest\naverage energy and highest success rate, however, the\nperformance of CEO for \u03c4 = 1.5 is almost the same as \u03c4 =\n1.6 in this case but obviously better than \u03c4 = 1.6 in the\ncase N = 30. Therefore, in the following computation,\nwe set \u03c4 = 1.5. We have also compared the performance\nof CEO on larger LJ clusters for \u03c4 = 1.5 and \u03c4 = 1.6, the\ntwo cases are pretty much the same thing and \u03c4 = 1.5 is\na little better.\nWe demonstrate that for all the LJ clusters of size N\nnot more than 100, the global minima can be obtained\nby using CEO algorithm. In figure 2, we report the\nperformance of CEO on LJ clusters optimization problem according to 200 independent runs. In figure 2a,\nthe red circles represent the average energies obtained\n\nby CEO over 200 runs, where the black squares represent the global minima. One can see that the deviation\nfrom global minimum becomes greater and greater when\nthe cluster size getting larger and larger, which indicates\nthat for very large LJ cluster, CEO may be a poor algorithm. Figure 2b shows the success rate of hitting the\nglobal minima in 200 runs, the inset is the success rate\nfor N > 50 that may be unclear in the main plot. For\nboth the case N = 95 and N = 100, the global optimal\nsolutions appears only once in 200 runs.\nAlthough CEO is not a all-powerful algorithm and it\nmay perform poorly for very large LJ clusters, we demonstrate that it is competitive or even superior over some\nmore elaborate stochastic optimization procedures like\nSA[28], GA[29] in finding the most stable structure of LJ\nclusters with minimal energy.\nFinally, we investigate the average CPU time over\n200 runs vs the size of LJ clusters. The computations\nwere carried out in a single PentiumIII processor(1GHZ).\nFrom figure 3, in the log-log plot, the data can be well fitted by a straight line with slope 3.326\u00b10.008, which indicates that the increasing tendency of CPU time T vs cluster size is approximate to power-law form as T \u223c N 3.886 .\nThat means the CEO is a polynomial algorithm of order\nO(N 4 ).\n\nV.\n\nCONCLUSION AND DISCUSSION\n\nIn this paper, we explore a general-purpose heuristic algorithm for finding high-quality solutions to continuous optimization problems. The computing results\nindicate that this simple approach is competitive and\nsometimes can outperform some far more complicated\nor finely tuned nature inspired algorithm including simulated annealing and genetic algorithm, on a well-known\nNP-hard continuous optimization problem for LJ clusters(see reference[28, 29] for comparison). According to\nEO's updating rule, it is clear that EO has very high ability in global searching, thus to combine EO and a strong\nlocal searching algorithm may produce a high efficient\nalgorithm for continuous optimization problems.\nRecently, several novel algorithms aiming at LJ clusters optimization problem have been proposed, such\nas fast annealing evolutionary algorithm[25], conformational space annealing method[30], adaptive immune\noptimization algorithm[31], cluster similarity checking\nmethod[32], and so forth. These algorithms consider\nmore about the special information about LJ clusters\nand perform better than CEO. However, we have not\nfound a compellent evidence indicating that there exists a\ngeneral-purpose algorithm like SA or GA entirely preponderate over CEO on LJ cluster optimization problem. It\nis worthwhile to emphasize that, in this paper, we do not\nwant to prove that the CEO is an all-powerful algorithm,\neven do not want to say that the CEO is a good choice\nfor chemists on LJ cluster optimization problem since a\ngeneral-purpose method often perform poorer than some\n\n\f5\nspecial methods aiming at an idiographic problem. The\nonly thing we want to say is the CEO, an extension of\nnature inspired algorithm EO, is a competitive algorithm\nand needs more attention.\nFurther more, to demonstrate the efficiency of CEO,\nmuch more experiments on various hard continuous optimization problems should be achieved.\n\n(NNSFC) under Grant No. 20325517, and the Teaching\nand Research Award Program for Outstanding Young\nTeacher (TRAPOYT) in higher education institutions of\nthe Ministry of Education (MOE) of China, the State\nKey Development Programme of Basic Research of China\n(973 Project), the NNSFC under Grant No. 10472116,\n70471033 and 70271070, and the Specialized Research\nFund for the Doctoral Program of Higher Education\n(SRFDP No.20020358009).\n\nAcknowledgments\n\nThis work is supported by the outstanding youth fund\nfrom the National Natural Science Foundation of China\n\n[1] S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi, Science\n220, 671(1983).\n[2] E. H. L. Aarts and J. H. M. Korst, Simulated annealing\nand Boltzmann machines (John Wiley and Sons, 1989).\n[3] J. Holland, Adaption in natural and artificial systems\n(The university of Michigan Press, Ann Arbor, MI, 1975).\n[4] D. G. Bounds, Nature 329, 215(1987).\n[5] D. E. Goldberg, Genetic algorithms in search, optimization, and machine learning (Addison-Wesly, MA, 1989).\n[6] W. Banzhaf, P. Nordin, R. Keller and F. Francone, Genetic programming - an introduction (Morgan Kaufmann,\nSan Francisco, CA, 1998).\n[7] S. Boettcher and A. G. Percus, Artificial Intelligence 199,\n275(2000).\n[8] S. Boettcher, J. Phys. A 32, 5201(1999).\n[9] S. Boettcher, Comput. Sci. Eng. 2, 6(2000).\n[10] S. Boettcher, Comput. Sci. Eng. 2, 75(2000).\n[11] S. Boettcher, A. G. Percus and M. Grigni, Lect. Notes.\nComput. Sci. 1917, 447(2000).\n[12] P. Bak and K. Sneppen, Phys. Rev. Lett. 71, 4083(1993).\n[13] K. Sneppen, P. Bak, H. Flyvbjerg and M. H. Jensen,\nProc. Natl. Acad. Sci. 92, 5209(1995).\n[14] A. A. Middleton, Phys. Rev. E 69, 055701(R)(2004).\n[15] M. R. Garey and D. S. Johnson, Computers and intractability: a guide to the theory of NP-completeness\n(Freeman, New York, 1979).\n[16] S. Boettcher and A. G. Percus, Phys. Rev. E 64,\n026114(2001).\n[17] S. Boettcher and A. G. Percus, Phys. Rev. Lett. 86,\n5211(2001).\n[18] S. Boettcher and A. G. Percus, Phys. Rev. E 69,\n\n066703(2004).\n[19] S. Boettcher and M. Grigni, J. Phys. A 35, 1109(2002).\n[20] In discrete optimization problem, the set of all the states\nof system is denumerable, while in continuous optimization problem, the corresponding set is a continuum.\n[21] T. P. Martin, T.Bergmann, H.G\u00f6hilich and T. Lange,\nChem. Phys. Lett. 172,209 (1990).\n[22] L. T. Wille, in: D. Stauffer(Ed.), Annual Reviewsof\nComputational Physics VII, World Scientific, Singnapore, 2000.\n[23] M. R. Hoare, Advan. Chem. Phys. 40 (1979) 49, and\nreferences therein.\n[24] B. Xia, W. Cai, X. Shao, Q. Guo, B. Maigret and Z. Pan,\nJ. Mol. Struct.(Theochem) 546, 33(2001).\n[25] W. Cai, Y. Feng, X. Shao and Z. Pan, J. Mol.\nStruct.(Theochem) 579, 229(2002).\n[26] D. C. Liu and J. Nocedal, Math. Progm. B 45, 503(1989).\n[27] The complete up-to-date list of the global minima of LJ clusters can be download from the web\nhttp://brian.ch.cam.ac.uk/.\n[28] L. T. Wille, Computational Materials Science 17,\n551(2000).\n[29] D. M. Deaven, N. Tit, J. R. Morris and K. M. Ho, Chem.\nPhys. Lett. 256, 195(1996).\n[30] J. Lee, I. -H. Lee and J. Lee, Phys. Rev. Lett. 91,\n080201(2003).\n[31] X. Shao, L. Cheng and W. Cai, J. Chem. Phys. 120,\n11401(2004).\n[32] L. Cheng, W. Cai and X. Shao, Chem. Phys. Lett. 389,\n309(2004).\n\n\f"}