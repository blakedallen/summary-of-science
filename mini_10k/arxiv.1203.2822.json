{"id": "http://arxiv.org/abs/1203.2822v2", "guidislink": true, "updated": "2014-12-12T18:43:33Z", "updated_parsed": [2014, 12, 12, 18, 43, 33, 4, 346, 0], "published": "2012-03-13T14:29:24Z", "published_parsed": [2012, 3, 13, 14, 29, 24, 1, 73, 0], "title": "A Fast Algorithm Finding the Shortest Reset Words", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.3738%2C1203.5548%2C1203.6807%2C1203.2523%2C1203.2641%2C1203.4754%2C1203.5249%2C1203.4477%2C1203.4790%2C1203.4323%2C1203.6385%2C1203.4242%2C1203.1228%2C1203.2263%2C1203.2310%2C1203.2414%2C1203.0513%2C1203.0489%2C1203.3494%2C1203.1276%2C1203.6706%2C1203.5391%2C1203.3026%2C1203.2587%2C1203.6071%2C1203.0183%2C1203.2837%2C1203.6358%2C1203.3804%2C1203.5185%2C1203.4026%2C1203.5089%2C1203.4159%2C1203.4174%2C1203.2860%2C1203.0206%2C1203.3072%2C1203.0820%2C1203.6492%2C1203.2383%2C1203.2629%2C1203.2134%2C1203.2822%2C1203.2744%2C1203.0972%2C1203.4806%2C1203.6820%2C1203.3062%2C1203.3682%2C1203.2156%2C1203.6709%2C1203.6678%2C1203.0616%2C1203.6603%2C1203.1890%2C1203.2009%2C1203.3823%2C1203.4915%2C1203.0704%2C1203.3124%2C1203.1775%2C1203.4543%2C1203.4434%2C1203.1611%2C1203.3548%2C1203.2745%2C1203.1193%2C1203.3622%2C1203.4484%2C1203.6672%2C1203.2351%2C1203.5500%2C1203.5322%2C1203.0862%2C1203.1385%2C1203.0636%2C1203.3748%2C1203.6052%2C1203.0945%2C1203.6199%2C1203.0296%2C1203.3023%2C1203.2103%2C1203.0675%2C1203.0786%2C1203.0122%2C1203.3512%2C1203.3179%2C1203.5316%2C1203.4217%2C1203.4721%2C1203.6232%2C1203.0644%2C1203.5202%2C1203.1918%2C1203.6231%2C1203.4097%2C1203.5587%2C1203.3133%2C1203.4913%2C1203.6259&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Fast Algorithm Finding the Shortest Reset Words"}, "summary": "In this paper we present a new fast algorithm finding minimal reset words for\nfinite synchronizing automata. The problem is know to be computationally hard,\nand our algorithm is exponential. Yet, it is faster than the algorithms used so\nfar and it works well in practice. The main idea is to use a bidirectional BFS\nand radix (Patricia) tries to store and compare resulted subsets. We give both\ntheoretical and practical arguments showing that the branching factor is\nreduced efficiently. As a practical test we perform an experimental study of\nthe length of the shortest reset word for random automata with $n$ states and 2\ninput letters. We follow Skvorsov and Tipikin, who have performed such a study\nusing a SAT solver and considering automata up to $n=100$ states. With our\nalgorithm we are able to consider much larger sample of automata with up to\n$n=300$ states. In particular, we obtain a new more precise estimation of the\nexpected length of the shortest reset word $\\approx 2.5\\sqrt{n-5}$.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.3738%2C1203.5548%2C1203.6807%2C1203.2523%2C1203.2641%2C1203.4754%2C1203.5249%2C1203.4477%2C1203.4790%2C1203.4323%2C1203.6385%2C1203.4242%2C1203.1228%2C1203.2263%2C1203.2310%2C1203.2414%2C1203.0513%2C1203.0489%2C1203.3494%2C1203.1276%2C1203.6706%2C1203.5391%2C1203.3026%2C1203.2587%2C1203.6071%2C1203.0183%2C1203.2837%2C1203.6358%2C1203.3804%2C1203.5185%2C1203.4026%2C1203.5089%2C1203.4159%2C1203.4174%2C1203.2860%2C1203.0206%2C1203.3072%2C1203.0820%2C1203.6492%2C1203.2383%2C1203.2629%2C1203.2134%2C1203.2822%2C1203.2744%2C1203.0972%2C1203.4806%2C1203.6820%2C1203.3062%2C1203.3682%2C1203.2156%2C1203.6709%2C1203.6678%2C1203.0616%2C1203.6603%2C1203.1890%2C1203.2009%2C1203.3823%2C1203.4915%2C1203.0704%2C1203.3124%2C1203.1775%2C1203.4543%2C1203.4434%2C1203.1611%2C1203.3548%2C1203.2745%2C1203.1193%2C1203.3622%2C1203.4484%2C1203.6672%2C1203.2351%2C1203.5500%2C1203.5322%2C1203.0862%2C1203.1385%2C1203.0636%2C1203.3748%2C1203.6052%2C1203.0945%2C1203.6199%2C1203.0296%2C1203.3023%2C1203.2103%2C1203.0675%2C1203.0786%2C1203.0122%2C1203.3512%2C1203.3179%2C1203.5316%2C1203.4217%2C1203.4721%2C1203.6232%2C1203.0644%2C1203.5202%2C1203.1918%2C1203.6231%2C1203.4097%2C1203.5587%2C1203.3133%2C1203.4913%2C1203.6259&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper we present a new fast algorithm finding minimal reset words for\nfinite synchronizing automata. The problem is know to be computationally hard,\nand our algorithm is exponential. Yet, it is faster than the algorithms used so\nfar and it works well in practice. The main idea is to use a bidirectional BFS\nand radix (Patricia) tries to store and compare resulted subsets. We give both\ntheoretical and practical arguments showing that the branching factor is\nreduced efficiently. As a practical test we perform an experimental study of\nthe length of the shortest reset word for random automata with $n$ states and 2\ninput letters. We follow Skvorsov and Tipikin, who have performed such a study\nusing a SAT solver and considering automata up to $n=100$ states. With our\nalgorithm we are able to consider much larger sample of automata with up to\n$n=300$ states. In particular, we obtain a new more precise estimation of the\nexpected length of the shortest reset word $\\approx 2.5\\sqrt{n-5}$."}, "authors": ["Andrzej Kisielewicz", "Jakub Kowalski", "Marek Szyku\u0142a"], "author_detail": {"name": "Marek Szyku\u0142a"}, "author": "Marek Szyku\u0142a", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/978-3-642-38768-5_18", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1203.2822v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1203.2822v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "COCOON 2013. The final publication is available at\n  http://link.springer.com/chapter/10.1007%2F978-3-642-38768-5_18", "arxiv_primary_category": {"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1203.2822v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1203.2822v2", "journal_reference": "In Computing and Combinatorics, volume 7936 of LNCS, pages\n  182-196, 2013", "doi": "10.1007/978-3-642-38768-5_18", "fulltext": "A Fast Algorithm Finding the Shortest Reset Words\nAndrzej Kisielewicz\u22c61,2 , Jakub Kowalski1, and Marek Szyku\u0142a1\n\narXiv:1203.2822v2 [cs.FL] 12 Dec 2014\n\n1\n\nDepartment of Mathematics and Computer Science, University of Wroc\u0142aw, Poland\n2\nInstitute of Mathematics and Computer Science, University of Opole, Poland\nandrzej.kisielewicz@math.uni.wroc.pl, {kot,msz}@ii.uni.wroc.pl\n\nAbstract. In this paper we present a new fast algorithm for finding minimal reset words\nfor finite synchronizing automata, which is a problem appearing in many practical applications. The problem is known to be computationally hard, so our algorithm is exponential\nin the worst case, but it is faster than the algorithms used so far and it performs well on\naverage. The main idea is to use a bidirectional BFS and radix (Patricia) tries to store and\ncompare subsets. Also a number of heuristics are applied. We give both theoretical and\npractical arguments showing that the effective branching factor is considerably reduced.\nAs a practical test we perform an experimental study of the length of the shortest reset\nword for random automata with n \u2264 300 states and 2 input letters. In particular,\nwe\n\u221a\nobtain a new estimation of the expected length of the shortest reset word \u2248 2.5 n \u2212 5.\nKeywords: Synchronizing automaton, synchronizing word, \u010cern\u00fd conjecture\n\n1\n\nIntroduction\n\nWe deal with (complete deterministic) finite automata A = hQ, \u03a3, \u03b4i with the state set Q, the\ninput alphabet \u03a3, and the transition function \u03b4 : Q \u00d7 \u03a3 \u2192 Q. The action of \u03a3 on Q given by\n\u03b4 is denoted simply by concatenation: \u03b4(q, a) = qa. This action extends naturally to the action\nqw of words for any w \u2208 \u03a3 \u2217 . If |Qw| = 1, that is, the image of Q by w consists of a single\nstate, then w is called a reset (or synchronizing) word for A, and A itself is called synchronizing.\n(In other words, w resets (synchronizes) A in the sense that, under the action of w, all the\nstates are sent into the same state). The synchronizing property is very important, because it\nmakes the automaton resistant to errors that could occur in an input word. After detecting an\nerror a synchronizing word can be used to reset the automaton to its initial state. Synchronizing\nautomata have many practical applications. They are used in robotics (for designing so-called\npart orienters) [2], bioinformatics (the reset problem) [3], network theory [12], theory of codes\n[11] etc.\nTheoretical research in the area is mainly motivated by the \u010cern\u00fd conjecture stating that\nevery synchronizing automaton A with n states has a reset word of length \u2264 (n \u2212 1)2 . This\nconjecture was formulated by \u010cern\u00fd in 1964 [4], and is considered the most longstanding open\nproblem in the combinatorial theory of finite automata. So far, the conjecture has been proved\nonly for a few special classes of automata and a general cubic upper bound (n3 \u2212 n)/6 has been\nestablished (see Volkov [25] for an excellent survey of the results, and Trahtman [24] for a recently\nfound new cubic bound). Using computers the conjecture has been verified for small automata\nwith 2 letters and n \u2264 10 states (and with k \u2264 4 letters and n \u2264 7 states [23]; see also [1] for\nn = 9 states). It is known that, in general, the problem is computationally hard, since it involves\nan NP-hard decision problem. Recently, it has been shown that the problem of finding the length\n\u22c6\n\nSupported in part by Polish MNiSZW grant N N201 543038.\n\n\f2\n\nAndrzej Kisielewicz, Jakub Kowalski, and Marek Szyku\u0142a\n\nof the shortest reset word is FPNP[log] -complete, and the related decision problem is both NPand coNP-hard [15].\nOn the other hand, there are several theoretical and experimental results showing that most\nsynchronizing automata have relatively short reset words and those slowly synchronizing (with\nthe shortest reset words of quadratic length) are rather exceptional [1]. An old result by Higgins\n[9] on products in transformation semigroups shows that a random automaton with an alphabet\nof size larger than 2n has, with high probability, a reset word of length \u2264 2n. More recently,\nit was proved that, for every \u01eb > 0, a random automaton with n states over an alphabet of\nsize n0.5+\u01eb , with high probability, is synchronizing and satisfies the \u010cern\u00fd conjecture [21]. In\ncomputing reset words, either exponential algorithms finding the shortest reset words [20,23,13]\nor polynomial heuristics finding relatively short reset words [8,13,17,18,23] are widely used. The\nstandard approach is to construct the power automaton and to compute the shortest path from\nthe whole set state to a singleton [19,23,13,25]. Most naturally, the breadth-first-search method\nis used which starts from the set of all states of the given automaton and forms images applying\nletter transformations until a singleton is reached. Based on these ideas computation packages\nhave been created (TESTAS [22] and recently developed COMPAS [5]). In [18], Roman uses a\ngenetic algorithm to find a reset word of randomly generated automata and thus obtains upper\nbounds on the length of the shortest reset word.\nA new interesting approach for finding the exact length using a SAT-solver has been applied\nrecently by Skvortsov and Tipikin [20]. The problem of determining if an automaton has a reset\nword of length at most l is reduced to the SAT problem and the binary search for the exact length\nis performed. Using this approach, the following experimental study is done. For chosen numbers\nn of states from the interval [1, 100] random automata with 2 input letters are generated, checked\nif they are synchronizing, and if so, the shortest reset word is computed. The results directly\ncontradict the conjecture made by Roman [18] that the mean length of the shortest reset word\nfor a random n-state synchronizing automaton is linear and almost equal to 0.486n. Skvortsov\nand Tipikin argue that their experiment based on a larger set of data shows that this length is\nactually sublinear and \u2248 1.95n0.55.\nIn this paper we present a new algorithm based on a bidirectional breadth-first-search. Implementing this idea requires efficiently solving the problem of storing and comparing resulted\nsubsets of states. To this aim radix tries (also known as Patricia tries [14]) are used. We analyze\nthe algorithm from both theoretical and practical sides. As the first test of efficiency we have\nperformed experiments analogous to those done by Skvortsov and Tipikin. Due to the well performance of the algorithm we were able to generate and check one million automata for each\nn \u2264 100, (compared with 200\u20132000 generated by Skvortsov and Tipikin), and we were able to\ntest much larger automata with up to n = 320 states. Our data confirm the hypothesis that the\nexpected length of the\n\u221a shortest reset word is sublinear, but show that more precise is a smaller\napproximation \u2248 2.5 n \u2212 5. In addition, the larger set of data enables us to estimate the error\nand to show that for our approximation with high probability the error is very small. We also\nverify and discuss other results and claims of [20].\nOur algorithm makes also possible to find a reset word of the shortest length (not only the\nlength). Curiously, it works in polynomial time for known slowly synchronizing automata series\n[1]. So far, most of the empirical research in the area concerns automata with 2 input letters.\nSome researches suggest that automata with more letters may exhibit a different behavior. We\nplan to use the algorithm to perform an extensive research on automata with k > 2 letters.\n\n\fA Fast Algorithm Finding the Shortest Reset Words\n\n2\n\n3\n\nAlgorithm\n\nThe algorithm gets an automaton A = hQ, \u03a3, \u03b4i with n states and k input letters. First, A is\nchecked if it is synchronizing using the well known (and efficient) algorithm [7]. If so, then we\nproceed to search for a synchronizing word of the shortest length. Here, one may perform the\nbreadth-first search (BFS) on the power automaton of A starting from the set Q of all the states\nand computing successive images by the letters of the alphabet \u03a3 (and recording the sequences\nof the letters applied). One may also search in the inverse (backward) direction starting from\nthe singleton sets and computing successive preimages (this search will be refereed to as IBFS).\nBoth the searches have branching factor k (the number of input letters) and need to compute\nO(k l ) sets (or O(nk l ) in IBFS) to find a synchronizing word of the shortest length l. The idea\nbehind bidirectional search is to perform two searches simultaneously and check if they meet.\nThen a synchronizing word may be found in only O(nk l/2 ) steps. However, to implement this\nidea there must be an efficient way to check each new subset to see if it already appears in the\nsearch tree of the other half of the search.\n2.1\n\nGeneral Ideas\n\nFor each search we maintain the current list of subsets that can be obtained from the start in\na given number of steps. Since the lists have a tendency to grow exponentially and to contain\nsubsets obtained on earlier steps, it is more efficient to maintain additional lists of visited subsets\n(for each search) and to use them to remove from the current lists redundant subsets. We have\nchecked experimentally that it is a good strategy to decrease the branching factor.\nTo check if the two searches meet one needs to perform subset checking: after each step, BFS\nor IBFS, we check if a set on the current IBFS list contains a set on the current BFS list. If so,\nit means that there are words u, w \u2208 \u03a3 \u2217 such that the image Qu is a subset of the preimage\n{q}w\u22121 for some q \u2208 Q. Consequently, Quw = {q}, as required.\nSince, in the bidirectional approach, subset checking must be performed anyway, it may be\nalso applied to reduce lists using the following simple observation. If S and T are subsets of Q\nsuch that S \u2286 T , then |T w| = 1 implies |Sw| = 1 for any w \u2208 \u03a3 \u2217 . It follows that, for example, a\nsubset on the IBFS list contains a subset on the BFS list if and only if \u2013 with respect to inclusion\n\u2013 a maximal element on the IBFS list contains a minimal element on the BFS list. Consequently,\nthe only subsets on the BFS lists we need to consider are those minimal with respect to inclusion\nand the only subsets on the IBFS lists we need to consider are those maximal with respect to\ninclusion.\nTo store and check subsets on the lists we apply an efficient data structure known as radix trie\n(Patricia trie) [14]. We show that the subset checking operation (checking whether a given set S\nhas a subset stored in the trie) and the dual superset checking (checking whether a given set S\nhas a superset stored in the trie) are efficient enough for these structures to make a combination\nof the ideas presented above work well in practice.\nThis approach is fast but memory consuming. In order to also make the algorithm work\nefficiently for larger automata, when the memory limit is reached, the bidirectional approach is\nreplaced by a sort of an inverse DFS search not involving the tries of visited subsets anymore. We\nalso apply several technical optimizations and heuristics which yields a considerable speed-up.\nThey are described in Section 3.\n2.2\n\nRadix Tries\n\nA radix trie is a binary tree of the maximal depth n which stores subsets of a given n-set Q in its\nleaves. Having a fixed linear order of elements q1 , . . . , qn \u2208 Q, each subset S of Q encodes a path\n\n\f4\n\nAndrzej Kisielewicz, Jakub Kowalski, and Marek Szyku\u0142a\n\nfrom the root to a leaf in the natural way: after i steps the path goes to the right child whenever\nqi \u2208 S, and goes to the left, otherwise. A radix trie is compressed in the sense that instead in a\nnode at depth n it stores a subset in the first node that determines uniquely the subset in the\nstored collection (no other subset shares the same path as a prefix of the encoding); c.f. [14].\nThe insert operation for radix tries is natural and can be performed in at most n steps. The\nsubset checking operation is performed by a depth-first-search checking if the given set S \u2286 Q\ncontains a subset stored in the visited leaf. An essential advantage is that the search does not\nneed to branch into the right child of a node if the checked subset S does not contain the state\ncorresponding to the current level. The superset checking operation (for IBFS) is done in the\ndual way. These issues are discussed in more detail in 2.3.\nAlgorithm 1 The main part\nInput A = hQ, \u03a3, \u03b4i \u2013 a synchronizing automaton with n = |Q| states and k = |\u03a3| input letters.\nInput maxlen \u2013 maximum length of words to be checked.\n\u22b2 Initialize four radix tries to store and handle subsets of Q:\n1: Tc \u2190 EmptyTrie\n\u22b2 BFS current trie\n2: Tv \u2190 EmptyTrie\n\u22b2 BFS visited trie\n3: Tic \u2190 EmptyTrie\n\u22b2 IBFS current trie\n4: Tiv \u2190 EmptyTrie\n\u22b2 IBFS visited trie\n5: Tc .insert(Q)\n6: Tv .insert(Q)\n7: for all q \u2208 Q do\n8:\nTic .insert({q})\n9:\nTiv .insert({q})\n10: end for\n11: for l \u2190 1 to maxlen do\n12:\nif estimated time of the BFS step is smaller than that of IBFS then\n13:\nBFS_Step(Tc ,Tv )\n\u22b2 Modify BFS tries; minimize Tc using Tv\n14:\nelse\n15:\nIBFS_Step(Tic ,Tiv )\n\u22b2 Modify IBFS tries; minimize Tic using Tiv\n16:\nend if\n17:\nfor all S \u2208 Tic do\n\u22b2 The goal test loop\n18:\nif Tc .contains_subset_of(S) then\n19:\nreturn l\n\u22b2 The length of the shortest reset word\n20:\nend if\n21:\nend for\n22: end for\n23: return \"No synchronizing word of length \u2264 maxlen\"\n\n2.3\n\nDescription\n\nThe main part of the algorithm is given in Algorithm 1. To make it clearer we restrict the task\nto finding the shortest length of a reset word only. Yet, the algorithm can be easily modified to\nreturn also a reset word of such length (see 2.4).\nWe use, in principle, four radix tries Tc , Tv , Tic , Tiv to maintain the BFS current, BFS visited,\nIBFS current, and IBFS visited lists, respectively. After initializing the tries we enter a loop\nconsisting of at most maxlen steps (line 11). In each step we perform a step of the BFS procedure\nor IBFS procedure depending on comparison of estimated expected execution time of both steps,\nwhich we discuss in 3.1.\n\n\fA Fast Algorithm Finding the Shortest Reset Words\n\n5\n\nWith no regard if BFS or IBFS step was performed recently, in lines 17-21 of Algorithm 1, the\nsame goal test loop is performed. For each S in Tic , the procedure Tc .contains_subset_of(S)\nis executed, which checks if Tc contains a subset of S. If so, we claim that l is the shortest length\nof a rest word for A. To prove this we need to analyze the content of the BFS and IBFS steps.\nIn BFS step (Algorithm 2), for each set S \u2032 in the current BFS trie and for each input letter\na we compute the image S = S \u2032 a and insert it to the list L. For each set S \u2208 L we check if\na subset of S is already in the BFS visited trie. If so, we skip it. If not, we insert S into the\nBFS visited trie and in the (newly formed; line 9) BFS current trie Tc . Processing elements of L\n(line 10) in ascending cardinality order is a heuristic aimed in getting more subsets skipped in\nthe checking subset procedure in line 11, and in consequence, to deal with smaller structures. It\nalso guarantees that Tc contains only minimal sets in terms of inclusion (the proof of this fact\nand all other proofs will be given in the extended version of this paper).\nAfter executing lines 10-15 of Algorithm 2 the trie Tv may contains some redundant subsets\n(which are not minimal with respect to inclusion). Therefore in lines 16-18 we have an additional\nprocedure to reduce Tv completely.\nThe procedure Tv .reduce consists of two steps. First, we form a list of elements of Tv using\na DFS-search from the left to the right (smaller subsets first). This guarantees that if S precedes\nT on the list then S does not contain T . Hence the only pairs of comparable elements on the list\nare those with S preceding T and S \u2282 T . In the second step we insert the elements from the list\ninto the empty Tv depending on the result of subset checking performed before each insertion.\nThis guarantees that if a subset S of T is inserted then T will be skipped on the later step. Hence\nthe resulting trie Tv contains no comparable subsets, as required.\nUnfortunately, this procedure applied for such a large trie as Tv (which may be of exponential\nsize in terms of n) may be time-consuming. We found experimentally that if the trie has not\ngrown too large since the last reduction it is more effective to process a larger trie rather than to\nperform reduction. In our implementation we perform it after the first step and then only when\nTv contains at least k times more sets since it had after the last reduction (which is the worse\ncase for one step with branching factor k = |\u03a3|).\nThe IBFS step is dual and completely analogous. In line 10 ascending cardinality order is\nreplaced by descending one, in line 5 we compute preimages instead of images, and in line 11\nsubset checking is replaced by superset checking.\nOne can prove the following\nTheorem 1. Given a synchronizing n-state automaton A = hQ, \u03a3, \u03b4i, Algorithm 1 returns the\nshortest length of a reset word for A or reports that no such a word of length \u2264 maxlen exists.\nProof. In order to prove the correctness of Algorithm 1, we introduce additional notation. Let\nTci denote Tc after performing i steps of BFS, and let Ticj denote Tic after performing j steps of\nj\nIBFS. Similarly, let Tvi denote Tv after performing i steps of BFS, and let Tvc\ndenote Tiv after\nperforming j steps of IBFS. We have the following\nLemma 1. For each set S \u2208 Tci there is a word u of length i, such that Qu = S. Similarly for\neach set T \u2208 Ticj there is a word v of length j, such that {q}v \u22121 = T for some state q \u2208 Q.\nProof. The proof is by induction. For i = 0 the claim is true with the empty word. For i > 0, we\nnote that all new sets S inserted into Tci are obtained by applying a letter a to a set S \u2032 \u2208 Tci\u22121\n(line 5 of Algorithm 2). By induction hypothesis, there is a word u\u2032 of the length i \u2212 1 such that\nQu = S \u2032 . Hence, u\u2032 a has length i and we have Qu\u2032 a = S \u2032 a = S, as required. The proof of the\nsecond statement is dual.\n\n\f6\n\nAndrzej Kisielewicz, Jakub Kowalski, and Marek Szyku\u0142a\n\nAlgorithm 2 BFS step procedure\n1: procedure BFS_Step(Tc ,Tv )\n2:\nL \u2190 EmptyList\n\u22b2 The list of all new images\n3:\nfor all S \u2032 \u2208 Tc do\n4:\nfor all a \u2208 \u03a3 do\n5:\nS \u2190 \u03b4(S \u2032 , a)\n\u22b2 Compute the image of S \u2032 by the letter a\n6:\nL.insert(S)\n7:\nend for\n8:\nend for\n9:\nTc \u2190 EmptyTrie\n10:\nfor all S \u2208 L in ascending cardinality order do\n11:\nif not Tv .contains_subset_of(S) then\n12:\nTv .insert(S)\n13:\nTc .insert(S)\n14:\nend if\n15:\nend for\n16:\nif Tv has grown large since the last reduction then\n17:\nTv .reduce\n18:\nend if\n19: end procedure\n\nLet l be the length of the shortest reset words for A. First we show that the algorithm in\norder to report the length of a reset word in line 19 needs to perform at least l (BFS or IBFS)\nsteps.\nAssume that the algorithm reaches line 19 after i steps of BFS and j steps of IBFS. So there\nare sets S \u2208 Tci and T \u2208 Ticj such that S \u2286 T . By Lemma 1, there are words u, v of lengths i, j,\nrespectively, and a state q \u2208 Q such that Qu = S and {q}v \u22121 = T . Thus, Quv = {q}, and uv is\na reset word of length i + j. Consequently, l \u2264 i + j.\nNow we show that, if l \u2264 maxlen, then the algorithm reaches line 19 after at most l steps. By\ninduction, we prove the following more general statement implying our claim: for each i, j \u2265 0,\n0 \u2264 i + j \u2264 l, after i steps of BFS and j steps of IBFS there are sets S \u2208 Tci and T \u2208 Ticj , and\nthere exists a reset word w = uxv of length l, where |u| = i, |v| = j, |x| = l \u2212 i \u2212 j, such that\nQu = S and {q}v \u22121 = T .\nFor i + j = 0, because of the initialization in lines 5-10, we have that Q \u2208 Tc0 and {q} \u2208 Tic0 ,\nand a reset word of length l is as required. Assume that the statement is true for all i\u2032 + j \u2032 <\ni + j. Assume also, first, that the (i + j)-th performed step is BFS one. Then, by the induction\nassumption there exists a reset word w\u2032 = u\u2032 x\u2032 v of length l and sets S \u2032 \u2208 Tci\u22121 and T \u2208 Ticj such\nthat Qu\u2032 = S \u2032 and {q}v \u22121 = T for some state q \u2208 Q, |u\u2032 | = i \u2212 1, |v| = j.\nSince i + j \u2264 l, |x\u2032 | > 0. Let a be the first letter of x\u2032 and x\u2032 = ax\u2032\u2032 . We need to consider two\ncases, depending on whether S \u2032 a = \u03b4(S \u2032 , a) (created in line 5 of Algorithm 2) is added (in line 13)\ninto Tci or not. If so, then the statement is true, because we have the reset word w = w\u2032 = (ua)x\u2032\u2032 v\nand sets S = S \u2032 a \u2208 Tci and T \u2208 Ticj , with required properties..\nOtherwise the reason for not adding S \u2032 a into Tci must be a set S \u2208 Tvi , such that S \u2286 S \u2032 a\n(line 11). Let u be the word corresponding to S by Lemma 1. Then the word w = ux\u2032\u2032 v (where\nx\u2032 = ax\u2032\u2032 ) is a reset word. If |u| < i (we do not know yet if u \u2208 Tci ), then w is shorter than l,\nbecause |u| + |x\u2032\u2032 | + |v| < i + (l \u2212 (i \u2212 1) \u2212 j \u2212 1) + j = l, which is a contradiction. So, |u| = i,\nwhich means that S has been added into Tvi in the currently performed i-th BFS step. It follows\nthat S has been also added into Tci . Now, w = ux\u2032\u2032 v is the required word for i, j with S \u2208 Tci\nand T \u2208 Ticj , Qu = S, and {q}v \u22121 = T .\n\n\fA Fast Algorithm Finding the Shortest Reset Words\n\n7\n\nFor the second part of the proof we need to assume that the (i + j)-th performed step is\nIBFS one. In this case the proof is, again, analogous. The difference is that by the induction\nassumption, we have now a reset word w\u2032 = ux\u2032 v \u2032 , and we take into consideration the last letter\nof x\u2032 . We leave this part to the reader.\n\u2293\n\u2294\n2.4\n\nFinding a Reset Word\n\nIn order to find a reset word of the found minimal length l, one needs to apply the following\nslight modification to the algorithm described above. The main point is that together with the\nsets stored in the current tries we need to store also the words assigned to these sets. To this\nend, in line 5 of Algorithm 2 (and analogously in the IBFS procedure) we assign to S \u2032 the word\nobtained by concatenating the word assigned earlier to S with the letter a (at the end or at the\nbeginning, respectively). When the goal is reached, the two words are simply merged to form\nthe required reset word. Of course, instead of complete words, with each set we store only a\nletter and a pointer to the previous part of the word. From these the word is reconstructed when\nwe reach the goal. We note that in this way the asymptotic time and space complexity of the\nalgorithm remain the same.\n\n3\n\nHeuristics and Optimizations\n\nIn addition to the main part of the algorithm described in the previous section we use a number\nof heuristics and optimizations. They are justified both by experiments and theoretical arguments. Altogether they can reduce computation time by a factor of at least 25 relative to the\nimplementation without these optimizations. We describe briefly only the most important of\nthem.\n3.1\n\nEstimation of Expected Step Time\n\nTo decide which step will be performed in line 12 of the Algorithm 1 we follow the greedy strategy\nchoosing this step whose execution time, together with the goal test, seems to be smaller at the\nmoment. We use a rough estimation of expected execution time by calculating upper bounds\nfor the expected number of visited nodes in subset checking operations, under some simplifying\nassumptions. Since all other operations in the steps in question are linear in terms of n and the\nsizes of the current lists, subset checking are the most time consuming operations. The base for\nthe estimation is the following theoretical result we have established. (A set S \u2282 X is a random\nsubset of X with Bernoulli distributions in [q, r] if each element x of X is a member of S with\nprobability px \u2208 [q, r].)\nTheorem 2. Let p, q, r \u2208 (0, 1) be such that q \u2264 r and q > pr. Let F be a family of m random\nsubsets of a given set X with Bernoulli distributions in [q, r], and let S be a random subset of X\nwith Bernoulli distributions in [0, p]. Then in the trie constructed for the family F , the expected\nnumber of visited nodes by the subset checking procedure for S is at most\n\u0013\n\u0012\n1+p\n1\nmlogw (1+p) ,\n+\np\nq \u2212 pr\nwhere w =\n\n1+p\n1+pr\u2212q .\n\n\f8\n\nAndrzej Kisielewicz, Jakub Kowalski, and Marek Szyku\u0142a\n\nProof. Let f (S, F ) be the number of visited nodes in the trie constructed for F by subset checking\nprocedure for S.\nConsider the trie constructed for F as a subtrie of the complete trie. Then f (S, F ) can be\nwritten as a sum over the nodes in the complete:\nX\nf (S, F ) =\ng(x, S, F ),\nx\n\nwhere g(x, S, F ) is an indicator function taking 1 if the node x is visited and 0 otherwise. By\nlinearity of expectation,\nX\nX\nE[f (S, F )] =\nE[g(x, S, F )] =\nP(g(x, S, F ) = 1).\nx\n\nx\n\nWe can then group the nodes at the same height:\n\uf8eb\n\u221e\nX\nX\nX\n\uf8ed\nP(g(x, S, F ) = 1) =\nx\n\nh=0\n\nx at height h\n\n\uf8f6\n\nP(g(x, S, F ) = 1)\uf8f8 .\n\nWe will estimate now probability that a node is visited at the height h. Let x be a node in\nthe complete trie with the path from the root with exactly i ones and h \u2212 i zeros. The node\nis visited if and only if (1) the searching procedure for a subset of S would reach the node in\nthe complete trie (containing all possible sets) and (2) the node belongs to the constructed trie.\nThese two events are independent, since (1) depends only on S and (2) only on F . We may\ndefine therefore two indicator functions: g \u2032 (x, S) which takes the value 1 if the first condition\nholds (and 0 otherwise) and g \u2032\u2032 (x, F ) which takes the value 1 if the second condition holds (and\n0 otherwise).\nWe bound the probability that condition (1) holds. It holds if and only if S contains all the\nelements corresponding to ones in the path (otherwise the search does not go into the corresponding branch). Since the probability of containing each element is in [0, p], the probability that the\ncondition (1) holds does not exceed pi . Similarly we bound the probability that condition (2)\nholds. It holds only if there exists a set in F whose first h elements correspond to the path of the\nnode (in fact, this condition is necessary, but not sufficient, because of truncating paths). The\nprobability that a single subset has the required sequence of the first h elements, with exactly i\nones and h \u2212 i zeros, in view of the assumption on Bernoulli distribution in [q, r], can be bounded\nfrom above by ri (1 \u2212 q)h\u2212i . Since F contains m elements, the probability that condition (2) holds\nmay be upper bounded by min{1, mri (1 \u2212 q)h\u2212i }. Summarizing, for a node x with i ones and\nh \u2212 i zeros on the path we have:\nE[g \u2032 (x, S)] = P[g \u2032 (x, S) = 1] = P(S contains the i elements specified by x) \u2264 pi ,\nE[g \u2032\u2032 (x, F )] = P(g \u2032\u2032 (x, F ) = 1) = P(F contains the set encoded by x)\n\u2264 min{1, mri (1 \u2212 q)h\u2212i }.\n\nNow we can group the nodes at the height h, which have the same number of ones on the\npath and we can sum over these groups of the nodes, obtaining:\nX\n\nx at height h\n\n\u2032\n\n\u2032\u2032\n\nP[g (x, S)g (x, F ) = 1] \u2264\n\nh \u0012 \u0013\nX\nh\ni=0\n\ni\n\npi min{1, mri (1 \u2212 q)h\u2212i }.\n\n\fA Fast Algorithm Finding the Shortest Reset Words\n\n9\n\nThis yields a bound that we will use to estimate\n\u221e\nh \u0012 \u0013\nX\nX\nh i\np min{1, mri (1 \u2212 q)h\u2212i }\nE[f (S, F )] \u2264\ni\ni=0\nh=0\n\n!\n\n1+p\nLet t = \u230alogw m\u230b, where w = 1+pr\u2212q\n. We will split up the sum above into two parts: the first\none that sums over the levels from\n0\nto\nt, and the second one that\u0011sums from t + 1 to n.\nPt \u0010Ph h\u0001 i\ni\nh\u2212i\nCase 1. We estimate h=0\n} . For P[g \u2032\u2032 (x, F ) = 1] we use\ni=0 i p min{1, mr (1 \u2212 q)\n\nin this case the trivial bound P[g \u2032\u2032 (x, F ) = 1] \u2264 1. So, we have the bound\nt X\nh \u0012 \u0013\nt\nX\nh i X\n(1 + p)t+1 \u2212 1\n.\np =\n(1 + p)h =\np\ni\ni=0\n\nh=0\n\nh=0\n\nSubstituting t = \u230alogw m\u230b yields\n\n(1 + p)t+1 \u2212 1\n(1 + p)\u230alogw m\u230b+1 \u2212 1\n=\np\np\n(1 + p)(1 + p)logw (m) \u2212 1\n\u2264\np\nlogw (1+p)\n(1 + p)m\n\u22121\n=\np\n(1 + p) logw (1+p)\nm\n<\np\n\u0010P\n\u0011\n\u0001\nP\nh\nh i\ni\nh\u2212i\nCase 2. We estimate nh=t+1\np\nmin{1,\nmr\n(1\n\u2212\nq)\n}\n. For this case we use the\ni=0 i\n\nsecond bound P[g \u2032\u2032 (x, F ) = 1] \u2264 mri (1 \u2212 q)h\u2212i . We obtain\n\nh \u0012 \u0013\nX\nh i i\np mr (1 \u2212 q)h\u2212i = m(1 + pr \u2212 q)h ,\ni\ni=0\n\nand consequently,\n\u221e\nX\n\nh=0\n\nm(1 + pr \u2212 q)h+t+1 \u2264\n=\n\n\u221e\nX\n\nh=0\n\u221e\nX\n\nh=0\n\nm(1 + pr \u2212 q)h+logw m\n\u0010\n\u0011\nm mlogw (1+pr\u2212q) (1 + pr \u2212 q)h\n\n= mlogw (w)+logw (1+pr\u2212q)\n\n\u221e\nX\n\n(1 + pr \u2212 q)h\n\nh=0\n\n(note that (1 + pr \u2212 q) < 1, since by assumption q > pr)\n1\n< mlogw ((1+pr\u2212q)w)\nq \u2212 pr\n1+p\n1\n=\nmlogw (1+pr\u2212q) 1+pr\u2212q\nq \u2212 pr\n1\nmlogw (1+p)\n=\nq \u2212 pr\n\n\f10\n\nAndrzej Kisielewicz, Jakub Kowalski, and Marek Szyku\u0142a\n\nCombining both the cases we obtain\n1 + p logw (1+p) mlogw (1+p)\nE[f (S, F )] <\nm\n+\n=\np\nq \u2212 pr\nas required.\n\n\u0012\n\n1\n1+p\n+\np\nq \u2212 pr\n\n\u0013\n\nmlogw (1+p) ,\n\u2293\n\u2294\n\nIn our empirical observations this optimization reduces computation time by an average of\n70% relative to the implementation performing the BFS and IBFS steps alternatingly. It usually\nleads to perform slightly more BFS steps, since average sizes of subsets decrease much faster in\nBFS than increase in IBFS. By a result of Higgins after applying two BFS steps the average\nsize of subsets not greater than 0.55n (see [9]). Our empirical observations show that the two\nsearches meet when the sizes of subsets are as small as 0.03n. This fact is also the reason why in\nthe goal test we decided to use subset checking of Tc rather than superset checking of Tic (subset\nchecking does not require branching in subtries corresponding to elements not belonging to the\nqueried set).\n3.2\n\nAdding the IDFS Phase\n\nThis is the most important optimization improving not only the performance, but also modifying\nthe general idea. Bidirectional BFS works if we have no limit on memory resources. Since the\nnumber of sets stored in the tries grows exponentially with the number of steps performed, for\nlarge automata, we can easily run out of memory. To deal with this, we change the search strategy\nwhen we reach the memory limit. Rather than to continue BFS searches we switch to depth-first\nsearch, which has restricted memory requirements, and may use the subsets and words computed\nso far. Moreover, assuming the \u010cern\u00fd conjecture, we may impose an initial limit on the depth\nof the search, which allows to make the DFS search complete. After each recursive call, when a\nshorter reset word is found, the limit on the depth of the search is suitably decreased. The search\nis finished when no limit decreasing is possible and all paths of the limited DFS are exhausted.\nThe search returns either the shortest reset word or a counterexample to the \u010cern\u00fd conjecture.\nThe IDFS phase is used also to reduce the computation time of the algorithm (even if we are far\nfrom reaching the memory limit). This will be discussed in subsection 3.5.\nOur experiments show that it is more efficient to apply the inverse DFS, that is, one starting\nfrom the sets in Tic and computing the preimages to find a set containing a member of Tc (rather\nthan the forward DFS starting from the sets in Tc and computing images to find a set contained in\na member of Tic ). An important modification is that we perform search on partial lists of subsets\nmaking use of all available memory rather then on single subsets. This gives an additional boost.\n3.3\n\nReduction of the Automaton\n\nIf the input automaton is not strongly connected, after some steps of BFS it can be reduced to\na smaller automaton without the states not involved in computation anymore. More precisely,\nwe can remove the states which are not reachable from any state in any subset in the current\nBFS list. So, at the beginning, before the main loop of Algorithm 1 (line 11), we perform a few\nsteps of BFS and when the size of Tc is larger than sn, where s is an experimentally established\nconstant, we check if there are unreachable states in Q. This is done by the standard DFS search\non Q. If this is the case, we create a reduced automaton A\u2032 removing the unreachable states, and\nrebuild all the tries to make them compatible with the reduced automaton. Then, the algorithm\nmay continue using the parameters computed so far.\n\n\fA Fast Algorithm Finding the Shortest Reset Words\n\n11\n\nOur experiments show that after the first reduction the automaton is usually strongly connected (and no further reduction of this kind can be done). Yet, this optimization is efficient since\nwe have proved that the fraction of strongly connected automata to all automata with n states\ntends to 0 as n goes to infinity, and that the size of the minimal strongly connected component is\non average less than 1\u22121/ek (provided most automata are synchronizing). From our experiments\nit follows that for synchronizing automata with k = 2 this size is \u2248 0.7987n. Thus, for example,\nautomata with n = 200 states are reduced on average by as much as 40 states.\n3.4\n\nReordering of the States\n\nEfficiency of operations on radix tries depends on the order in which the input automaton's states\nare processed. We found that the subset checking is performed faster if the states occurring more\nfrequently in queried subsets are later in the ordering. This is because radix tries tends to\nhave logarithmic height (cf. [6]), and the states at the end in the ordering are rarely or never\nchecked. As a result, the \"effective size\" of the queried sets is smaller. To establish frequencies of\noccurrences of states, and a preferred initial order based on them, we use a stationary distribution\nof a Markov chain based on the underlying digraph of the automaton. The details will be given\nin the extended version of the paper. This optimization is performed before the bidirectional\nsearch phase.\nThe situation changes completely during the IDFS phase, when the trie Tc is fixed and does\nnot change anymore. The frequencies of occurrences of the subsets in Tc may by computed\nexactly. This leads to a different reordering. Both reorderings have been confirmed as optimal\nby experiments. They show that these optimization reduce computation time by an average of\n27%.\n3.5\n\nUsing Heuristic Algorithms and IDFS Shortcut\n\nIn order to save a step of search computation we may use known heuristic algorithms to find\nquickly a good bound for search depth. Therefore, at the beginning of the algorithm, before\nstarting the bidirectional search, we apply a few polynomial time algorithms finding upper bounds\nfor the length of the shortest reset word. In our implementation we use Eppstein algorithm [7],\nFastSynchro algorithm [13] and our procedure Cut-Off IBFS. The latter is the standard IBFS\nsearch with cutting the branches of the search with smallest subsets. This may spare one step in\nbidirectional search, if the heuristic algorithms find the shortest word.\nYet, more importantly, combined with the IDFS phase, this makes possible to reduce the\ncomputation time by several orders of magnitude. Knowing that bidirectional search is close to\nend it is profitable to switch to IDFS phase: at the end the IDFS works much faster, since we do\nnot need to check visited sets and do not need to reconstruct Tc anymore. We call this optimization the shortcut. Between steps we use an estimate if it is faster to continue the bidirectional\nphase or to switch to IDFS phase. Note that the IDFS has a lower constant factor, but the\nbranching factor is equal to k. So, it slows the search if started too early. For estimation we use\nthe formula in Theorem 2. Our experiments show that this optimization reduces computation\ntime by as much as 89%.\n\n4\n\nComplexity\n\nThe efficiency gain of the algorithm relies mainly on two properties of the majority of automata.\nFirst, the average size of subsets decreases fast during the first BFS steps, but increases slow\n\n\f12\n\nAndrzej Kisielewicz, Jakub Kowalski, and Marek Szyku\u0142a\n\nduring IBFS steps (cf. subsection 3.1). Due to this fact the maintained subsets are usually\nsmall. Second, the branching factors of both BFS and IBFS are less than k, because of skipping\nredundant visited sets. Both of the properties are hard to study in a theoretical way, we however\nhave observed them in series of experiments.\nTo provide a theoretical argument we analyze here the expected running time of the algorithm\nunder some artificial assumptions. We give an upper bound for the bidirectional search only,\nwhich is a rough estimate of the expected time, but shows a significant impact of the automata\nproperties on performance. The following assumptions are made:\n1. The input is a synchronizing automaton with n states on k letters.\n2. The overall branching factor is r in each step of both BFS and IBFS, 1 < r < k. This\ncorresponds to an effective branching factor, which in view of our experiments is considerably\nless than k.\n3. The sets in the tries Tc , Tv and Tic , Tiv have random Bernoulli distribution: in each step,\nthey contain any given state with probability 0 < pc < 1 (for BFS steps) and 0 < pic < 1\n(for IBFS steps). We assume also that pic \u2264 pc .\n4. The steps of BFS and IBFS are performed alternatingly, starting from BFS.\n5. No reductions of the visited tries are made and no IDFS phase is performed.\nWhile the assumptions 2-3 are purely theoretical, they may be treated as an idealization of a\ntypical situation. Using these assumption, denoting by l the length of the shortest reset word of\nthe automaton, we can prove that there exists an integer 0 < d < 1, depending on probabilities\npc , pic , such that the following holds.\nTheorem 3. Under the assumptions (1-5) above, and with l denoting the length of the shortest\nreset word of the automaton, the expected time complexity of the algorithm is O(kn2 rl(1+d)/2) ),\nand the space complexity is O(n(k + n) + nrl/2 ).\nProof. We use RAM computation model in the analysis, with the uniform cost criteria (that is,\neach elementary operation costs one time unit). We consider r, pc , pic as constants and compute\na bound as a function of n and k. Let l be the length of the shortest reset word of the automaton.\nFor simplification, assume that l is even.\nThe initialization phase time may be bounded polynomially by O(kn4 ). This includes computing the inverse automaton O(nk), running the heuristic synchronizing algorithms O(kn4 ),\ncomputing the stationary distribution O(n3 ), changing the order of the states of the automaton\nO(nk + n log n), and initializing the tries O(n2 ).\nUnder the assumption on the branching factor, the number of sets in Tc in i-th BFS step,\nafter performing (i \u2212 1)-BFS steps and (i \u2212 1)-IBFS steps, can be bounded by ri , which is the\nnumber of sets after the step. The number of sets in Tv can be bounded by summing added sets\nPi\ni+1\nduring all the BFS steps: j=0 rj = r r\u22121\u22121 \u2208 O(ri ). Similar bounds hold for Tic and Tiv , but\nthere are n sets at the beginning, so it yields O(nri ).\nRecall that under assumptions of the theorem we may use Theorem 2, and to obtain the\nfollowing estimation for the visited number of nodes in the trie\n\u0013\n\u0012\n1\n1+p\nmlogw (1+p) ,\n+\nExpNvn(m, p, q) =\np\nq \u2212 pq\n1+p\n. Since we we use this formula for various pairs p and q, we shall use an\nwhere w = 1+pq\u2212q\n1+p\nabbreviation w(p, q) = 1+pq\u2212q\n.\nNote that each of computing an image or preimage of a set, checking the size of a set, checking\nif a set is a subset or superset of another set, can be done in O(n) time. Subset checking for one\n\n\fA Fast Algorithm Finding the Shortest Reset Words\n\n13\n\nset can be done in expected time O(nExpNvn(m, p, q)), for suitable m, p, q. This is so, because\nwe must count not only visited nodes but also test if the set is a subset of a stored set (which\ncosts O(n), but is done at most once for a visited node).\nThe expected time of the BFS step includes sorting of sets in L (this is done by counting sort,\nin this case), computing the image of each set by each letter, and checking for visited subsets.\nSo we can bound this by\n\u0001\nO (nri ) + (knri ) + (knri ExpNvn(O(ri ), pc , pc )) .\n\nThe last component in the sum is dominating, which yields\n\u0011\n\u0010\nO knri (ri )logw(pc ,pc ) (1+pc ) .\n\nSimilarly for the bound for the expected time of IBFS step we obtain:\n\u0011\n\u0010\nO knri (nri )logw(pic ,pic ) (1+pic ) .\n\nConsidering the goal test, it is enough to count only the goal test time after the IBFS step\n(multiplied by 2). Considering the goal test, in both cases after the BFS step or IBFS step we\ncan bound the time by\nO(nri ExpNvn(O(ri ), pic , pc )) = O(n2 ri (ri )logw(pic ,pc ) (1+pic ) ).\n\nComputing estimated expected step times after i-th BFS step and i-th IBFS are done in O(nri )\n(having access to list of sets in a trie in linear time), so it may be neglected.\nSumming these all yields under domination of the BFS and IBFS step time and the goal test:\n\u0011\n\u0011\n\u0010\n\u0010\nO knri (ri )logw(pc ,pc ) (1+pc ) + (nri )logw(pic ,pic ) (1+pic ) + n2 ri (ri )logw(pic ,pc ) (1+pic )\n\u0001\n\u2208 O kn2 ri (ri )d\n\u0001\n= O kn2 (ri )1+d\nwhere\n\nd = max((logw(pc ,pc ) (1 + pc )), (logw(pic ,pic ) (1 + pic )), (logw(pic ,pc ) (1 + pic ))).\nThe parameter d depends on the distribution of sets in the tries. Note that 0 < d < 1, so we\ncould bound nd simply by n.\nWe can now sum over the steps and obtain as the final result the time complexity:\nl/2\nX\n\nO kn2 (ri )1+d\n\ni=1\n\n\u2208 O(kn2\n\n\u0012\n\n\u0001\n\n\u0013\nr(1+d)(l/2+1) \u2212 1\n)\nr1+d \u2212 1\n\n\u2208 O(kn2 rl(1+d)/2) )\n\nThe expected space complexity can be bounded by counting stored sets and nodes in the\ntries after the last step. There are O(rl/2 ) sets in each of the tries. Each set requires O(n) space,\nalso it induces at most O(n) nodes in a trie. The initialization phase can be done in O(nk + n2 )\nspace. So we can state up the space bound for O(n(k + n) + nrl/2 ).\n\u2293\n\u2294\n\n\f14\n\nAndrzej Kisielewicz, Jakub Kowalski, and Marek Szyku\u0142a\n\nWe can observe that the expected time is exponential with regard to the length l, but the\nexponent is less than l, since (1 + d)/2 < 1. It is an improvement over the standard BFS\nalgorithm, which has time bound O(knRl ) (assuming we can check visited sets in constant\ntime). Moreover the standard algorithm usually has a larger branching factor R > r, since\nstrict supersets of visited sets are not skipped. The expected space complexity also yields an\nimprovement in comparison to the O(nRl ) space bound for the standard BFS.\nWhile, generally, our algorithm is exponential in the length l of the shortest reset word, surprisingly, it works fast in polynomial time for the known series of slowly synchronizing automata,\nthat is those with l close to the \u010cern\u00fd bound. These are automata Cn (the \u010cern\u00fd automaton),\nWn ,Dn\u2032 ,Dn\u2032\u2032 , and Bn introduced in [1].\nTheorem 4. For the class of the \u010cern\u00fd automata Cn , and the classes a Wn ,Dn\u2032 ,Dn\u2032\u2032 , and Bn\nintroduced in [1] the algorithm works in O(n4 ) time and O(n3 ) space.\nThe proof is based on the exact description of the heuristic mentioned in 3.1, which shows\nthat for each of the mentioned slowly synchronizing automata the algorithm performs mainly\nIBFS steps (rather than BFS), and the IBFS lists keep containing only one or two sets (due to\nreductions of visited subsets).\n\n5\n\nExperiments\n\nWe performed a series of the following experiments for various n \u2264 320. For a given n, we generate\na random automaton A with n states and 2 input letters, check whether A is synchronizing and\nif so, we find the minimal length of a reset word using the algorithm described in Section 2. On\nthe basis of the obtained results we estimate the expected length of the shortest reset word.\n5.1\n\nComputations\n\nIn the experiments we have used the standard model of random automata, where for each state\nand each letter all the possible transitions are equiprobable. A random automaton with n states\nand 2 input letters can be then represented as a sequence of 2n uniformly random natural\nnumbers from [0, n \u2212 1]. To generate high quality random sequences we have used the WELL\nnumber generator [16] (variants 1024 and 19937) seeded by random bytes from /dev/random\ndevice. For comparison, recall that Skvortsov and Tipikin, in their experimental study [20], have\ngenerated and checked the following numbers of random automata: 2000 automata for each\nn \u2208 {1, 2, . . . , 20, 25, 30, . . . , 50}, 500 automata for each n \u2208 {55, 60, 65, 70}, and 200 automata\nfor each n \u2208 {75, 80, . . . , 100}. In our experiment, up to 7 states, we have computed exact results\nchecking all automata. For each 8 \u2264 n \u2264 100 we checked one million automata, and for each\n101 \u2264 n \u2264 260 and n = 265, 270, . . . , 320 we checked 10000 automata. Our computations have\nbeen performed mostly on 16 computers with Intel(R) Core(TM) i7-2600 CPU 3.40GHz 4 cores\nand 16GB of RAM. The algorithm was implemented in C++ and compiled with g++. Distributed\ncomputations were managed by a dedicated server and clients applications written in Python.\nThe average computation time is about 100 or 1000 times faster than the time of Trahtman's\nprogram TESTAS [22,23] for automata with 50 states. The reduction to SAT used in [20] seemed\nto be the fastest recently known algorithm and the reported average time for 50 states automata\nis 2.7 seconds, and for 100 states automata is 70 seconds. Our comparable results are less than\n0.006 and 0.07 seconds, respectively (we have used faster machines, but only about twice as fast).\nThe Table 1 presents a rough comparison. The average times are relatively small because of rare\noccurrences of slowly synchronizing automata. We present also the maximum computation time.\n\n\fA Fast Algorithm Finding the Shortest Reset Words\n\n15\n\nTable 1. Comparison of average and maximum computation time for random automata.\nn\nTESTAS ([22])\nSAT reduction ([20])\nOur average time\nOur maximum time\n\n5.2\n\n50\n1.4 s\n2.7 s\n0.005 s\n0.26 s\n\n100\ntime-out\n70 s\n0.06 s\n3.79 s\n\n150\n\u2013\n\u2013\n0.469 s\n10.12 s\n\n200\n\u2013\n\u2013\n2.88 s\n159.670 s\n\n250\n\u2013\n\u2013\n31.637 s\n5 h 19 min\n\n300\n\u2013\n\u2013\n596.249 s\n7 h 55 min\n\nResults\n\nOur experiment confirms that for the standard random automata model A(n) on the binary\nalphabet the probability that the automaton is synchronizing seems to tend to 1 as the number\nn of states grows. This conjecture is posed in [20], but we have heard it earlier from Peter\nCameron during BCC conference in Exeter 2011. For n = 100, 2250 of one million automata\nturned out to be non-synchronizing (0.225%), and for n = 300, only five of 10000 automata.\nThe graphical representations of our experiments in this respect forms a smooth curve very fast\nconverging to 1. We observe also that random automata mostly are not strongly connected.\nThe main result of our experiments is the estimation of the expected length of the shortest\nreset word. We deal with the infinite sequence of random variables l(n) defined as the length of\nthe shortest reset word for a random synchronizing automaton with n states. We have observed\nthat the approximation E[l(n)] \u2248 1.95n0.55 proposed in [20] is inflated. Based on currently\navailable data, we propose\na new more precise experimental approximation for the expected\n\u221a\nlength E[l(n)] \u2248 2.5 n \u2212 5. A comparison of the estimations with the experimentally obtained\nmean length is given\n\u221a in Figure 1. We observe also that our result suggest that the expected length\nmay belong to \u0398( n).\nIn contrast with the experiments by Skvortsov and Tipikin [20], our experiments allow also to\nobtain a good estimation of the approximation error. Making use of the well-known Hoeffding's\ninequality, we obtain the following:\nTheorem 5. Let M L(n) denotes the mean length of the shortest reset word of the automata in\nthe sample of m randomly generated synchronizing n-state automata. If the ratio of the automata\nwith the length of the shortest reset word larger than Mn to all automata in the sample does not\nexceed r, then with probability at least 1 \u2212 p\nr\nlog(2/p) n3\n+\nr.\n|M L(n) \u2212 E[l(n)]| \u2264 Mn (1 \u2212 r)\n2m\n6\nProof. We make use of the well-known Hoeffding's inequality [10]. Given 0 < p \u2264 1, with probability at least 1 \u2212 p\nr\nlog(2/p)\n|X \u2212 E[X]| \u2264 R\n,\n(1)\n2m\nwhere X = (X1 + . . . + Xm )/m is the empirical mean of random variables X1 , . . . , Xm with the\nsame range R. Since the distribution of l(n) is highly asymmetric, one needs to combine this\ninequality with the statistical fact that the maximal lengths of the shortest reset words obtained\nin the experiment are much smaller than the known bounds and that longer lengths occur rarely.\nLet Mn be the maximal length of the shortest reset word for the n-state automata generated in\nthe experiment and m the size of the sample. First we assume that we sample only automata with\nthe length \u2264 Mn . Denote the corresponding random variable by l\u2032 (n). Applying the Hoeffding's\ninequality, putting X1 = . . . = Xm = l\u2032 (n), and R = Mn we obtain\nr\nlog(2/p)\n\u2032\n.\n|M L(n) \u2212 E[l ]| \u2264 Mn\n2m\n\n\f16\n\nAndrzej Kisielewicz, Jakub Kowalski, and Marek Szyku\u0142a\n\n45\n\nLength of the shortest reset word\n\n40\n35\n30\n25\n20\n15\n10\n\nExperimental mean length\nProposed estimation: 2.5 sqrt(n-5)\n\n5\n\n0.55\n\nSkvortsov, Tipikin estimation: 1.95 n\n0\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n120\n\n140 160 180 200\nNumer of states: n\n\n220\n\n240\n\n260\n\n280\n\n300\n\n320\n\nFig. 1. Experimental mean length of the shortest reset words compared with estimations.\n\nLet l\u2032\u2032 (n) be the length of the shortest reset word for a synchronizing automata with n states\nl(n) \u2265 Mn . Then we obtain\n\u2032\n\n\u2032\u2032\n\n|M L(n) \u2212 E[l]| \u2264 (1 \u2212 r)|M L(n) \u2212 E[l ]| + r|M L(n) \u2212 E[l ]| \u2264 (1 \u2212 r)Mn\n\nr\n\nn3\nlog(2/p)\n+r ,\n2m\n6\n\nas required. We have used the well-known bound n3 /6 for the length of the shortest reset word.\n\u2293\n\u2294\nAssuming the \u010cern\u00fd conjecture in the last term n3 /6 may be replaced by (n \u2212 1)2 (giving\nessentially better estimation). Let us take n = 100, m = 106 and p = 0.0001. Since, with\nprobability q = (1 \u2212 r)m the ratio of the automata with the shortest reset word longer than\nMn is less than r, one may see that for 1/r \u2265 100975, q < 0.0001. Hence, with high probability\n1/r > 100975, and taking into account the experimental value M100 = 41, the error is less than\n1.75 (or 0.19 assuming the \u010cern\u00fd conjecture). This means that with high probability the expected\nlength of the shortest reset word for synchronizing automata with n = 100 states is close to our\nexperimental result M L(100) = 24.34. Comparing this with the results of Skvortsov and Tipikin\n[20], we note that, for automata with 100 states, they also have obtained the expected length\nclose to 24, but the small size of their sample m = 200 does not allow any reasonable estimation\nof the error. Other interesting claims of [20] concerning the variance and approximation of l(n)\nwill be discussed in the extended version of the paper.\n\n\fA Fast Algorithm Finding the Shortest Reset Words\n\n17\n\nReferences\n1. D. Ananichev, V. Gusev, and M. Volkov. Slowly synchronizing automata and digraphs. In Mathematical Foundations of Computer Science 2010, volume 6281 of LNCS, pages 55\u201365. 2010.\n2. D. Ananichev and M. Volkov. Synchronizing monotonic automata. In Developments in Language\nTheory, volume 2710 of LNCS, pages 111\u2013121. 2003.\n3. Yaakov Benenson, Rivka Adar, Tamar Paz-Elizur, Zvi Livneh, and Ehud Shapiro. DNA molecule\nprovides a computing machine with both data and fuel. Proceedings of the National Academy of\nSciences, 100(5):2191\u20132196, 2003.\n4. J. \u010cern\u00fd. Pozn\u00e1mka k homog\u00e9nnym eksperimentom s kone\u010dn\u00fdmi automatami. Matematickofyzik\u00e1lny \u010casopis Slovenskej Akad\u00e9mie Vied, 14(3):208\u2013216, 1964. In Slovak.\n5. K. Chmiel and A. Roman. COMPAS - A computing package for synchronization. In Implementation\nand Application of Automata, volume 6482 of LNCS, pages 79\u201386. 2011.\n6. L. Devroye. A note on the average depth of tries. Computing, 28:367\u2013371, 1982.\n7. D. Eppstein. Reset sequences for monotonic automata. SIAM Journal on Computing, 19:500\u2013510,\n1990.\n8. M. Gerbush and B. Heeringa. Approximating minimum reset sequences. In Implementation and\nApplication of Automata, volume 6482 of LNCS, pages 154\u2013162. 2011.\n9. P. Higgins. The range order of a product of i-transformations from a finite full transformation\nsemigroup. Semigroup Forum, 37:31\u201336, 1988.\n10. W. Hoeffding. Probability inequalities for sums of bounded random variables. J. Amer. Statist.\nAssoc., 58(301):13\u201330, 1963.\n11. J\u00fcrgensen, H. Synchronization. Information and Computation, 206(9\u201310):1033\u20131044, 2008.\n12. J. Kari. Synchronization and stability of finite automata. Journal of Universal Computer Science,\n8(2):270\u2013277, 2002.\n13. R. Kud\u0142acik, A. Roman, and H. Wagner. Effective synchronizing algorithms. Expert Systems with\nApplications, 39(14):11746\u201311757, 2012.\n14. D.R. Morrison. PATRICIA \u2013 practical algorithm to retrieve information coded in alphanumeric.\nJournal of the ACM, 15:514\u2013534, 1968.\n15. J. Olschewski and M. Ummels. The complexity of finding reset words in finite automata. In Mathematical Foundations of Computer Science 2010, volume 6281 of LNCS, pages 568\u2013579. 2010.\n16. F. Panneton, P. L'Ecuyer, and M. Matsumoto. Improved long-period generators based on linear\nrecurrences modulo 2. ACM Transactions on Mathematical Software, 32(1):1\u201316, 2006.\n17. A. Roman. New algorithms for finding short reset sequences in synchronizing automata. In International Enformatika Conference (Prague), pages 13\u201317, 2005.\n18. A. Roman. Genetic algorithm for synchronization. In Language and Automata Theory and Applications, volume 5457 of LNCS, pages 684\u2013695. 2009.\n19. S. Sandberg. Homing and synchronizing sequences. In Model-Based Testing of Reactive Systems,\nvolume 3472 of LNCS, pages 5\u201333. 2005.\n20. E. Skvortsov and E. Tipikin. Experimental study of the shortest reset word of random automata.\nIn Implementation and Application of Automata, volume 6807 of LNCS, pages 290\u2013298. 2011.\n21. E. Skvortsov and Y. Zaks. Synchronizing random automata. Discrete Mathematics and Theoretical\nComputer Science, 12(4):95\u2013108, 2010.\n22. A. N. Trahtman. A package TESTAS for checking some kinds of testability. In Implementation and\nApplication of Automata, volume 2608 of LNCS, pages 228\u2013232. 2003.\n23. A. N. Trahtman. An efficient algorithm finds noticeable trends and examples concerning the C\u0306ern\u00fd\nconjecture. In Mathematical Foundations of Computer Science, volume 4162 of LNCS, pages 789\u2013\n800. 2006.\n24. A. N. Trahtman. Modifying the upper bound on the length of minimal synchronizing word. In\nFundamentals of Computation Theory, volume 6914 of LNCS, pages 173\u2013180. 2011.\n25. M. Volkov. Synchronizing automata and the C\u0306ern\u00fd conjecture. In Language and Automata Theory\nand Applications, volume 5196 of LNCS, pages 11\u201327. 2008.\n\n\f"}