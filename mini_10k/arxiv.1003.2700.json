{"id": "http://arxiv.org/abs/1003.2700v2", "guidislink": true, "updated": "2010-04-01T18:20:40Z", "updated_parsed": [2010, 4, 1, 18, 20, 40, 3, 91, 0], "published": "2010-03-13T12:47:46Z", "published_parsed": [2010, 3, 13, 12, 47, 46, 5, 72, 0], "title": "The role of semantics in mining frequent patterns from knowledge bases\n  in description logics with rules", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1003.4283%2C1003.2115%2C1003.2379%2C1003.4101%2C1003.3052%2C1003.1830%2C1003.1408%2C1003.5169%2C1003.4490%2C1003.4245%2C1003.4708%2C1003.1332%2C1003.2017%2C1003.3883%2C1003.2107%2C1003.3036%2C1003.3069%2C1003.0055%2C1003.5933%2C1003.1001%2C1003.0286%2C1003.4510%2C1003.2296%2C1003.0201%2C1003.0430%2C1003.3875%2C1003.6085%2C1003.2649%2C1003.1506%2C1003.3660%2C1003.2389%2C1003.4454%2C1003.4157%2C1003.4353%2C1003.1687%2C1003.5652%2C1003.4266%2C1003.1206%2C1003.2003%2C1003.0257%2C1003.0168%2C1003.1869%2C1003.3728%2C1003.5103%2C1003.2495%2C1003.4943%2C1003.1444%2C1003.4551%2C1003.3630%2C1003.5461%2C1003.0362%2C1003.1520%2C1003.0077%2C1003.5439%2C1003.3122%2C1003.1728%2C1003.0334%2C1003.2234%2C1003.0143%2C1003.4932%2C1003.1959%2C1003.5323%2C1003.0778%2C1003.1533%2C1003.5283%2C1003.1528%2C1003.3620%2C1003.2681%2C1003.1818%2C1003.6048%2C1003.1699%2C1003.1036%2C1003.3093%2C1003.1042%2C1003.2533%2C1003.1828%2C1003.5319%2C1003.3694%2C1003.4622%2C1003.2700%2C1003.0488%2C1003.3390%2C1003.1410%2C1003.5159%2C1003.1802%2C1003.3312%2C1003.4689%2C1003.3741%2C1003.5950%2C1003.0074%2C1003.2829%2C1003.4350%2C1003.1685%2C1003.2184%2C1003.1950%2C1003.3446%2C1003.5824%2C1003.4173%2C1003.1165%2C1003.6128%2C1003.1113&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The role of semantics in mining frequent patterns from knowledge bases\n  in description logics with rules"}, "summary": "We propose a new method for mining frequent patterns in a language that\ncombines both Semantic Web ontologies and rules. In particular we consider the\nsetting of using a language that combines description logics with DL-safe\nrules. This setting is important for the practical application of data mining\nto the Semantic Web. We focus on the relation of the semantics of the\nrepresentation formalism to the task of frequent pattern discovery, and for the\ncore of our method, we propose an algorithm that exploits the semantics of the\ncombined knowledge base. We have developed a proof-of-concept data mining\nimplementation of this. Using this we have empirically shown that using the\ncombined knowledge base to perform semantic tests can make data mining faster\nby pruning useless candidate patterns before their evaluation. We have also\nshown that the quality of the set of patterns produced may be improved: the\npatterns are more compact, and there are fewer patterns. We conclude that\nexploiting the semantics of a chosen representation formalism is key to the\ndesign and application of (onto-)relational frequent pattern discovery methods.\nNote: To appear in Theory and Practice of Logic Programming (TPLP)", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1003.4283%2C1003.2115%2C1003.2379%2C1003.4101%2C1003.3052%2C1003.1830%2C1003.1408%2C1003.5169%2C1003.4490%2C1003.4245%2C1003.4708%2C1003.1332%2C1003.2017%2C1003.3883%2C1003.2107%2C1003.3036%2C1003.3069%2C1003.0055%2C1003.5933%2C1003.1001%2C1003.0286%2C1003.4510%2C1003.2296%2C1003.0201%2C1003.0430%2C1003.3875%2C1003.6085%2C1003.2649%2C1003.1506%2C1003.3660%2C1003.2389%2C1003.4454%2C1003.4157%2C1003.4353%2C1003.1687%2C1003.5652%2C1003.4266%2C1003.1206%2C1003.2003%2C1003.0257%2C1003.0168%2C1003.1869%2C1003.3728%2C1003.5103%2C1003.2495%2C1003.4943%2C1003.1444%2C1003.4551%2C1003.3630%2C1003.5461%2C1003.0362%2C1003.1520%2C1003.0077%2C1003.5439%2C1003.3122%2C1003.1728%2C1003.0334%2C1003.2234%2C1003.0143%2C1003.4932%2C1003.1959%2C1003.5323%2C1003.0778%2C1003.1533%2C1003.5283%2C1003.1528%2C1003.3620%2C1003.2681%2C1003.1818%2C1003.6048%2C1003.1699%2C1003.1036%2C1003.3093%2C1003.1042%2C1003.2533%2C1003.1828%2C1003.5319%2C1003.3694%2C1003.4622%2C1003.2700%2C1003.0488%2C1003.3390%2C1003.1410%2C1003.5159%2C1003.1802%2C1003.3312%2C1003.4689%2C1003.3741%2C1003.5950%2C1003.0074%2C1003.2829%2C1003.4350%2C1003.1685%2C1003.2184%2C1003.1950%2C1003.3446%2C1003.5824%2C1003.4173%2C1003.1165%2C1003.6128%2C1003.1113&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose a new method for mining frequent patterns in a language that\ncombines both Semantic Web ontologies and rules. In particular we consider the\nsetting of using a language that combines description logics with DL-safe\nrules. This setting is important for the practical application of data mining\nto the Semantic Web. We focus on the relation of the semantics of the\nrepresentation formalism to the task of frequent pattern discovery, and for the\ncore of our method, we propose an algorithm that exploits the semantics of the\ncombined knowledge base. We have developed a proof-of-concept data mining\nimplementation of this. Using this we have empirically shown that using the\ncombined knowledge base to perform semantic tests can make data mining faster\nby pruning useless candidate patterns before their evaluation. We have also\nshown that the quality of the set of patterns produced may be improved: the\npatterns are more compact, and there are fewer patterns. We conclude that\nexploiting the semantics of a chosen representation formalism is key to the\ndesign and application of (onto-)relational frequent pattern discovery methods.\nNote: To appear in Theory and Practice of Logic Programming (TPLP)"}, "authors": ["Joanna Jozefowska", "Agnieszka Lawrynowicz", "Tomasz Lukaszewski"], "author_detail": {"name": "Tomasz Lukaszewski"}, "author": "Tomasz Lukaszewski", "arxiv_comment": "40 pages, 6 figures, 6 tables", "links": [{"href": "http://arxiv.org/abs/1003.2700v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1003.2700v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "68T27", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1003.2700v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1003.2700v2", "journal_reference": null, "doi": null, "fulltext": "arXiv:1003.2700v2 [cs.LO] 1 Apr 2010\n\nTo appear in Theory and Practice of Logic Programming (TPLP)\n\n1\n\nThe role of semantics\nin mining frequent patterns from knowledge bases\nin description logics with rules\nJOANNA J\u00d3ZEFOWSKA, AGNIESZKA LAWRYNOWICZ, TOMASZ LUKASZEWSKI\nInstitute of Computing Science, Poznan University of Technology,\nul. Piotrowo 2, 60-965 Poznan, Poland\nEmail: {jjozefowska, alawrynowicz, tlukaszewski}@cs.put.poznan.pl\nCategory: Regular Paper, Guest editors: Letizia Tanca and Giorgio Orsi\nsubmitted 24 April 2009; revised 15 November 2009; accepted 1 January 2010\n\nAbstract\nWe propose a new method for mining frequent patterns in a language that combines both\nSemantic Web ontologies and rules. In particular we consider the setting of using a language that combines description logics with DL-safe rules. This setting is important for\nthe practical application of data mining to the Semantic Web. We focus on the relation\nof the semantics of the representation formalism to the task of frequent pattern discovery,\nand for the core of our method, we propose an algorithm that exploits the semantics of the\ncombined knowledge base. We have developed a proof-of-concept data mining implementation of this. Using this we have empirically shown that using the combined knowledge\nbase to perform semantic tests can make data mining faster by pruning useless candidate\npatterns before their evaluation. We have also shown that the quality of the set of patterns\nproduced may be improved: the patterns are more compact, and there are fewer patterns.\nWe conclude that exploiting the semantics of a chosen representation formalism is key to\nthe design and application of (onto-)relational frequent pattern discovery methods.\nNote: To appear in Theory and Practice of Logic Programming (TPLP).\nKEYWORDS: frequent pattern discovery, ontologies, Semantic Web, DL-safe rules\n\n1 Introduction\nThe discovery of frequent patterns is a fundamental data mining task. It has been\nstudied for many different forms of input data and the pattern. Within the relational\nsetting it has been investigated since the development of WARMR (Dehaspe and\nToivonen 1999). WARMR uses the Datalog subset of first-order logic (FOL) as the\nrepresentation language for both data and patterns. As such, WARMR, and other\nsubsequently proposed relational frequent pattern miners, FARMER (Nijssen and\nKok 2001; Nijssen and Kok 2003) and c-armr (de Raedt and Ramon 2004), can\nbe classified as Inductive Logic Programming (ILP) (Nienhuys-Cheng and de Wolf\n1997; Dzeroski and Lavrac 2001) methods. These ILP systems have been successfully applied to a number of domains, most notably bioinformatics (King et al.\n\n\f2\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\n2000a; King et al. 2000b; King et al. 2001) and chemoinformatics (Dehaspe et al.\n1998).\nWhile relational frequent pattern mining methods have mostly assumed Datalog\nas the representation language, currently most activity within the field of knowledge representation (KR) assumes the use of logic-based ontology languages such as\ndescription logics (DLs) (Baader et al. 2003). Thanks to its significant support for\nmodelling ontologies, and suitability to the inherently open and incomplete nature\nof the Web environment, description logic has been chosen as the formal foundation of the standard ontology language for the Web, the Web Ontology Language\n(OWL) (McGuinness and van Harmelen 2004). OWL is now considered one of the\nfundamental technologies underpinning the Semantic Web (Berners-Lee et al. 2001),\ncurrently one of the most active application fields of artificial intelligence.\nResearch in KR is focused on developing deductive reasoning procedures, which\nare also traditionally employed to reason with logic-based ontological data. However, to meet the challenges posed by the Semantic Web scale and use cases, such\ndeductive approaches are not enough. Therefore, there is a recent trend in Semantic\nWeb research to propose complementary forms of reasoning that are more efficient\nand noise-tolerant. A promising approach in this area is to use inductive methods to\ncomplement deductive ones. This is in line with the recent trends in ILP research to\nbroaden the scope of the logical formalisms considered to description logics, or hybrid languages combining description logics with logic programs. Since description\nlogic knowledge bases are often equated with ontologies, ILP methods applied to\nsuch knowledge bases have been referred to as \"ontology mining\" methods (Fanizzi\nand d'Amato 2006; Fanizzi et al. 2008; d'Amato et al. 2008), and the ones applied\nto the hybrid knowledge bases to as \"onto-relational mining\" methods (Lisi and\nEsposito 2008). To the best of our knowledge, only one onto-relational frequent\npattern mining method, SPADA (Lisi and Malerba 2004), has been proposed.\nThis paper describes a method for frequent pattern mining in knowledge bases\nrepresented in the formalism of DL-safe rules (Motik et al. 2005) that combine\nSemantic Web ontologies (represented in description logic) and rules (represented\nin disjunctive Datalog). This language meets the requirements of knowledge representation for the Semantic Web and target application domains, and possesses\nproperties suitable for data mining applications. In the core of the method, we\npropose an algorithm that exploits at various steps the semantics of a combined\nknowledge base on which it operates. We show how to realize the proposed method\nin terms of exploiting state-of-the-art reasoning techniques, and present a proofof-concept implementation of the method. For Semantic Web research, the paper\ncontributes to the general understanding of the role of ontologies and semantics in\nhelping to solve knowledge-intensive tasks by exploiting the meaning of the represented knowledge. For ILP data mining research, the method's main novel feature\nis its exploitation of the semantics of the chosen language.\nThe rest of the paper is organized as follows. Section 2 disusses a technical and an\napplication-oriented motivation of the work. In Section 3 we introduce the basics of\nknowledge representation formalisms considered in this paper, and the problem of\nfrequent pattern mining from combined knowledge bases. In Section 4 we present\n\n\fThe role of semantics in mining frequent patterns\n\n3\n\nour method for mining frequent patterns. In Section 5 we present the experimental evaluation of the proposed approach. Section 6 contains the discussion of the\nrelated work. Finally, Section 7 concludes the paper, and outlines future work.\n\n2 Motivation\n2.1 The Setting\nThe problem of combining ontologies with rules is central in the Semantic Web. In\nthe current stack of the Semantic Web languages, rules are placed in the same layer\nas ontologies. There is an ongoing initiative to define an open format for rule interchange on the Semantic Web, the Rule Interchange Format (RIF)1 , that will cover\na wide spectrum of rule types, among them deductive rules represented in Datalog.\nAs we will discuss further in the paper, some important application domains such\nas life sciences require a language that combines description logic with some form\nof Datalog rules.\nSince a straightforward combination of DL and rules may easily lead to the undecidability of reasoning problems, the problem of developing such combinations\nhas received a lot of attention in KR and Semantic Web research. This has resulted\nin several proposals which may be generally divided into the following approaches:\ninteraction of rules and ontologies with strict semantic separation (loose coupling),\ninteraction of rules and ontologies with strict semantic integration (tight coupling),\nand reductions from DLs to logic programming formalisms.\nIn the first approach, adopted by dl-programs (Eiter et al. 2004a; Eiter et al.\n2004b; Eiter et al. 2008), DL and rule components are technically separate, and\ncan be seen as black boxes communicating via \"safe interface\".\nIn the second type of approach, \"safe interaction\", rules and DL knowledge bases\nare combined in a common semantic framework. A straightforward, tight extension\nof DL with first-order implication as proposed for Semantic Web Rule Language\n(SWRL) in (Horrocks et al. 2004), is trivially undecidable. On the other hand,\nDescription Logic Programs (DLP) (Grosof et al. 2003) describe a decidable intersection of description logic and logic programs. In between of these two opposite\napproaches, there is a group of proposals such as AL-log (Donini et al. 1998),\nCARIN (Levy and Rousset 1998), DL-safe rules (Motik et al. 2005) or DL+log\n(Rosati 2006) where to obtain decidability, either DL, or rules or both are typically\nconstrained by various syntactic restrictions, e.g. in the form of a safety condition.\nHowever, the usual syntactic restrictions may also be dropped, through changing\nthe usual perspective of the integration from DLs to the perspective of rule-based\nsystems, as proposed in (Lukasiewicz 2007) for the case of a tightly integrated form\nof disjunctive dl-programs. Finally, the tight integration may also become a full\none, as in hybrid MKNF knowledge bases (Motik and Rosati 2007), where there is\nno separation between vocabularies of a DL and rule component.\n\n1\n\nhttp://www.w3.org/2005/rules/wiki/RIF Working Group\n\n\f4\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nAn interesting representative approach for the works consisting on reducing description logics to logic programming is proposed in (Hustadt et al. 2004; Hustadt\net al. 2007) for an expressive DL language SHIQ. In that approach, the consistency\nchecking and query answering is reduced to the evaluation of a positive disjunctive\nDatalog program, which is obtained by a translation of a description logic knowledge base to first-order logic, followed by an application of superposition techniques,\nand the elimination of function symbols from the resulting set of clauses.\nDespite of the desired expressivity, there are also other important requirements\nfor a language to be used in frequent pattern mining applications, which are dataintensive in nature. In the last decade, the focus of KR research has been mostly on\ndeveloping reasoning techniques for handling complex DL intensional knowledge,\nand on decidability issues. However, new Semantic Web applications require efficient scalable procedures for query answering over ontologies, which now becomes\nan intensively explored area of research. Scalability may be achieved by restricting\nfeatures of a DL language to obtain a lightweight one, but tailored for data-intensive\napplications, as in the case of a tractable family of lanuages called DL-Lite (Calvanese et al. 2007). An interesting recent study into this direction is presented in\n(Cal\u0131\u0300 et al. 2009), where a family of expressive extensions of Datalog is proposed that\ngeneralize the DL-Lite family, e.g. by admitting existentially quantified variables\nin rule heads. The requirement for efficient query answering over large amounts of\ndata (extensional knowledge) is crucial for frequent pattern mining applications.\nTaking into account both criteria, that is sufficiently interesting expressivity required for real applications, and efficient query answering procedures, one combination of DLs and rules with interesting properties is the formalism of DL-safe rules\n(Motik et al. 2005). In this formalism decidability is obtained by restricting the rules\nto DL-safe ones that are applicable only to instances explicitly known by name. As\nit was shown in (Hustadt et al. 2004; Hustadt et al. 2007), the restriction to DLsafety enables the transformation of a DL knowledge base to a disjunctive Datalog\nprogram. This in turn enables the application of well-known reasoning algorithms\nand optimization techniques (such as magic-sets or join-order optimizations) developed for deductive databases in order to handle large data quantities. Some of\nthese methods have recently been extended for disjunctive Datalog (Cumbo et al.\n2004). The algorithm proposed in (Motik et al. 2005) for query answering in DL\nwith DL-safe rules separates reasoning on the intensional part of a knowledge base\nfrom that on the extensional part, which means that the inferences made on the intensional part are not repeated for different instances during query answering. This\nin turn enables better complexity results for the query answering algorithm than\nin case of the other state-of-the-art reasoning techniques developed for expressive\nDLs (Hustadt et al. 2005; Motik and Sattler 2006). It should be noted that, if the\ntranslation does not generate any disjunctive rules, then the algorithm applies the\nleast fixpoint operator used to evaluate non-disjunctive Datalog programs. Since the\nconsequences of the least fixpoint operator can be computed in polynomial time,\nan important feature of the algorithm is that its behaviour becomes tractable while\nit is applied for less expressive languages.\n\n\fThe role of semantics in mining frequent patterns\n\n5\n\nThe discussed features make the chosen DL-safe rules formalism suitable for the\nenvisaged frequent pattern mining applications.\n2.2 Possible applications\nThe primary motivation for our work is for the application of our method to realworld data-mining applications. Arguably the most extensive use of Semantic Web\nKR methods is in the domain of biology. Large amounts of data are increasingly\nbecoming openly available and described using real-life ontologies, represented in\nSemantic Web languages, such as GO (Gene Ontology)2 or BioPax (biological pathway knowledge)3 . This opens up the possibility for interesting large-scale and realworld onto-relational data mining applications.\nBelow we will describe why KR in biology requires a language able to model the\nexistence of unknown entities, disjunctions, and arbitrary composition of relations,\nthat is a language that combines description logic with some form of Datalog rules.\nThese requirements are a domain specific motivation for our language selection.\nInformation stored in biological knowledge bases is inherently incomplete. For\nexample, in functional genomics \"every protein has a function\", but often this\nfunction is unknown. Similarly, it is known that certain genes exist because they\nencode known proteins, but the identity of these genes are unknown (so-called \"locally orphan\" genes). The existence of entities with unknown identity can be easily\nrepresented in description logic4 , while it cannot be represented in Datalog.\nAnother way of modelling incompletness is by use of disjunction, what is not\nexpressible in Datalog. For example, disjunction may be used to describe that one\ninstance of certain tertiary structure units must be present in a protein (\"a classical\ntyrosine phosphatase has at least one low molecular weight phosphotyrosine or one\ntyrosine specific with dual specificity p-domain\"(Stevens et al. 2007)).\nIn general, DLs employ the open-world assumption (OWA) which seems suitable for a domain characterized by information that is incomplete either due to the\nlimits in the current state of knowledge or due to omissions common in curation\nprocesses. OWA is closely related to the monotonic form of reasoning, classically\nassumed in FOL. The monotonicity of reasoning in DLs is in line with the need\nfor the knowledge held in scientific knowledge bases to only comprise information\nthat is generally accepted and experimentally validated, and, which is reasonable\nto assume, will not be falsified. For example, one may state that \"In E. coli K-12,\nthe protein encoded by the gene ECK0647 when in inner membrane, facilitates the\ntransport of glutamate from the periplasm to the cytoplasm.\"5 (Ruttenberg et al.\n2005). This statement does not provide the reference to a particular enzyme and\nany information whether the transport is active or passive. If we subsequently learn\nsuch information, this does not change any positive or negative conclusions. Since\n2\n3\n4\n5\n\nhttp://www.geneontology.org\nhttp://www.biopax.org\nProtein v \u2203hasFunction\ninMembrane u inCytoplasm v \u22a5, ECK 0647 Protein v inCytoplasm, glutamateTransport v\n\u2203participant.ECK 0647 Protein u inMembrane\n\n\f6\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nit is characteristic of the current state of biology that much information is only\nknown to a certain degree, the monotonicity of reasoning in DLs allows scientific\nknowledge bases to be extensible and evolve with scientific knowledge.\nHowever, while possessing features which are not available in Datalog, description\nlogic also has limitations. It does not allow relations of arbitrary arity and arbitrary composition of relations. Assume for example, that we would like to express\nthat \"whenever a metal ion is bound to a phosphatase which catalyses a dephosphorylation of some protein, then this ion regulates the dephosphorylation of this\nprotein\"(Stevens et al. 2007)6 . It requires modelling that a composition of relations\nimplies another relation, which may be expressed in the form of a rule.\nThe above arguments show the need for languages combining description logic\nwith some form of rules in the discussed field, and the DL-safe rules formalism\nmeets all the necessary requirements for expressivity. It is also interesting to note\nhere, that in (Ruttenberg et al. 2005), while discussing problems concerned with\nusing description logic for modelling metabolic pathways, it has been already argued, there should be a way for modelling that certain axioms may be applied only\nif there are known instances of some class. The solution to check this could be by\nsubmitting a query with a DL-safety condition.\nAn interesting sample application of frequent pattern mining in the field of biology may be in functional genomics with the goal of the identification of frequent\npatterns in the amino acid sequence descriptions. The results would be further used\nto generate rules for predicting protein functional class. Such an approach would\nconstitute an onto-relational upgrade of the relational data mining application already proposed in the literature (King et al. 2000b). Another, novel application\nmay be in metabolic pathways analysis. The goal of the application would be to\nidentify common frequent pathways in human and other organisms that cause human diseases. The results of such analysis would allow for targeted drug design.\nDespite of life sciences, frequent pattern mining applications on the Semantic\nWeb data may be valuable in many other domains. Let us take e-business as another example. As rules provide a powerful business logic representation many use\ncases7 provided for RIF are actually in this domain. Most value for e-business that\ncombinations of description logic ontologies and rules may provide is in increasing\ninteroperability. Description logic provide means for expressing common vocabularies and domain knowledge, while rules enable to explicitely express business policies.\nFor example, annotation of product and service offerings with terms from common\nontologies such as GoodRelations8 may enable customers and enterprises an automatic search for suitable suppliers across the Web. Further, employing rules may\nenable to automatically express business relations between offerings and customers,\nand to express business policies such as \"The discount for a customer who buys a\nproduct is 5 percent if the customer is premium and the product is regular.\"9 .\n\n6\n7\n8\n9\n\nregulates(x , y) \u2190 metal ion(x ), isBound(x , z ), phosphatase(z ), catalyses(z , y), dephosphorylation(y)\nhttp://www.w3.org/TR/rif-ucr\nhttp://www.heppnetz.de/projects/goodrelations\ndiscount(x , y, percent5) \u2190 premium(x ), regular (y), buys(x , y)\n\n\fThe role of semantics in mining frequent patterns\n\n7\n\nIn e-business domain, an interesting sample application of frequent pattern mining may be in finding frequent customer buying behaviors to support personalisation, recommendation services and targeted marketing.\nIt should be stressed that for e-business applications, relatively lightweight ontologies may be sufficient, but the need for combining them with rules is essential\nin this domain.\n\n3 Preliminaries\n3.1 Language of knowledge representation\nIn this section we introduce the language of knowledge representation based on\nthe formalism of DL-safe rules. Further in this paper we develop an algorithm for\nfrequent pattern discovery in this language. DL-safe rules combine description logics\nwith disjunctive Datalog, which we briefly recall below.\n\n3.1.1 Description logics\nDescription logics (DLs) (Baader et al. 2003) are a family of knowledge representation languages, specifically suited to represent terminological knowledge in a\nstructured and formalized way. Two kinds of atomic symbols are distinguished in\nany description logic language: atomic concepts (denoted by A) and atomic roles\n(denoted by R and S ). Atomic symbols are elementary descriptions from which we\ninductively build complex descriptions (denoted by C and D) using concept constructors and role constructors. Description logics differ by the set of constructors\nthey admit.\nDLs are equipped with a logic-based model-theoretic semantics. The semantics is\ndefined by interpretations I = (\u2206I , *I ), where the non-empty set \u2206I is the domain\nof the interpretation and the interpretation function *I assigns a set AI \u2286 \u2206I to\nevery atomic concept A and a binary relation R I \u2286 \u2206I \u00d7 \u2206I to every atomic role\nR. The interpretation function is extended to concept descriptions by an inductive\ndefinition. The syntax and semantics of SHIF DL is defined in Table 1.\nA description logic knowledge base, KB, is typically divided into an intensional\npart (terminological one, TBox), and an extensional part (assertional one, ABox).\nThe TBox contains axioms dealing with how concepts and roles are related to each\nother (terminological axioms), while the ABox contains assertions about individuals\n(assertional axioms). A semantics is given to ABoxes by extending interpretations\nI = (\u2206I , *I ) by an additional mapping of each individual name a to an element\na I \u2208 \u2206I . The interpretation I satisfies a set of axioms (a TBox T or/and an ABox\nA) iff it satisfies each element of this set.\n\n3.1.2 Disjunctive Datalog\nDisjunctive Datalog (Eiter et al. 1997) is an extension of Datalog that allows disjunctions of literals in the rule heads.\n\n\f8\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\nTable 1: Syntax and semantics of SHIF.\n\nConstructor\n\nSyntax\n\nSemantics\n\nUniversal concept\nBottom concept\nNegation of arbitrary concepts\nIntersection\nUnion\nValue restriction\nFull existential quantification\n\n>\n\u22a5\n(\u00acC )\n(C u D)\n(C t D)\n(\u2200R.C )\n(\u2203R.C )\n\n\u2206I\n\u2205\n\u2206I \\C I\nC I \u2229 DI\nC I \u222a DI\n{a \u2208 \u2206I |\u2200b : (a, b) \u2208 R I \u2192 b \u2208 C I }\nI\nI\nI\n{a\n\u001a \u2208 \u2206 |\u2203b : (a, b) \u2208 R \u2227 b \u2208 C\n\u001b }\n\nFunctionality\n\n\u22641R\n\nConcept constructors\n\na \u2208 \u2206I |{b|(a, b) \u2208 R I }| \u2264 1\n\nRole constructors\nInverse role\nTransitive role\n\nR\u2212\nTrans(R)\n\n{(a, b) \u2208 \u2206I \u00d7 \u2206I |(b, a) \u2208 R I }\nR I is transitive\n\nDefinition 1 (Disjunctive Datalog rule)\nA disjunctive Datalog rule is a clause of the form\nH1 \u2228 . . . \u2228 Hk \u2190 B1 , . . . , Bn\nwhere Hi and Bj are atoms, and k \u2265 1, n \u2265 0. \u0003\nDefinition 2 (Disjunctive logic program)\nA disjunctive logic program P is a finite collection of disjunctive Datalog rules. \u0003\nWe consider only disjunctive Datalog programs without negative literals in the\nbody, that is, positive programs.\nFor the semantics, only Herbrand models are considered, and the semantics of\nP is defined by the set of all minimal models M of P , denoted by MM(P ). A\nground literal L is called a cautious answer of P , written P |=c L, if L \u2208 M for\nall M \u2208 MM(P ). FOL entailment coincides with cautious entailment for positive\nground atoms on positive programs.\n\n3.1.3 DL-safe rules\nWe use the formalism of DL-safe rules introduced in (Motik et al. 2005). The\ndescription logic SHIF and disjunctive Datalog rules are integrated by allowing\nconcepts and roles to occur in rules as unary and binary predicates, respectively.\nBelow we define DL-safe rules with respect to the description logic SHIF and\ndisjunctive Datalog rules.\n\n\fThe role of semantics in mining frequent patterns\n\n9\n\nDefinition 3 (DL-safe rules)\nLet KB be a knowledge base represened in the SHIF language. A DL-predicate\nis an atomic concept or a simple role from KB . For t1 and t2 being constants or\nvariables, a DL-atom is an atom of the form A(t1 ), where A is an atomic concept in\nKB , or of the form R(t1 , t2 ), where R is a simple role in KB , or of the form t1 = t2 .\nA non-DL-predicate is any other predicate than =, an atomic concept in KB , or a\nrole in KB . A non-DL-atom is an atom with any predicate other than =, an atomic\nconcept in KB , and a role in KB . A (disjunctive) DL-rule is a (disjunctive) rule with\nDL- and non-DL-atoms in the head and in the body. A (disjunctive) DL-program\nP is a finite set of (disjunctive) DL-rules. A combined knowledge base is a pair\n(KB, P). A (disjunctive) DL-rule r is called DL-safe if each variable in r occurs in\na non-DL-atom in the rule body. A (disjunctive) DL-program P is DL-safe if all its\nrules are DL-safe. \u0003\nIn order to define the semantics of a combined knowledge base (KB , P ), the KB\naxioms are mapped into a (disjunctive) Datalog program DD(KB ), which entails\nexactly the same set of ground facts as KB . The details concerning the mapping can\nbe found in (Motik 2006; Motik and Sattler 2006). It is proved that KB is satisfiable\nwith respect to the standard model-theoretic semantics of SHIF iff DD(KB ) is\nsatisfiable in first-order logic (Motik 2006). It is also proved in (Motik 2006) that for\na combined knowledge base (KB , P ) consisting of a SHIF knowledge base KB and\na finite set of DL-safe rules P , (KB , P ) |= \u03b1 iff DD(KB )\u222aP |= \u03b1, for a ground atom\n\u03b1, where \u03b1 is of the form A(a) or R(a, b), and A is an atomic concept. Therefore,\nreasoning in (KB , P ) can be performed using the well-known techniques from the\nfield of deductive databases.\nDL-safety implies that each variable is bound only to constants explicitely introduced in a (KB , P ). Let us consider, for example, a combined knowledge base\n(KB , P ) such that KB contains the concept Person and roles livesAt and worksAt,\nwhile P contains the following rule defining Homeworker as a person who lives and\nworks at the same place:\nHomeworker (x ) \u2190 Person(x ), livesAt(x , y), worksAt(x , y)\n\n(1)\n\nThis rule is not DL-safe. It is because the variables x and y that occur in the\nDL-atoms Person(x ), livesAt(x , y), worksAt(x , y) do not occur in the body in any\nnon-DL-atom. Let us introduce a special non-DL-predicate O such that the fact\nO(a) occurs for each individual a in the ABox. In order to make rule (1) DL-safe,\nwe add non-DL atoms O(x ) and O(y) in the rule body, obtaining:\nHomeworker (x ) \u2190 Person(x ), livesAt(x , y), worksAt(x , y), O(x ), O(y)\n\n(2)\n\nIn order to express a DL-safe rule intuitively, we just append to the original\nrule the phrase: \"where the identity of all objects is known\". The rule (2) can be\nintuitively expressed as follows: \"A Homeworker is a known person who lives at and\nworks at the same known place\".\nA combined knowledge base (KB , P ) may be divided into an intensional part,\nwhich contains knowledge independent of any specific instances, and an extensional\npart, which contains factual knowledge.\n\n\f10\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n3.2 Problem of onto-relational frequent pattern discovery\n\nIn this subsection we formally define the problem of frequent pattern discovery\nfrom knowledge bases represented in the DL-safe rules, as it is addressed in this\npaper. Initial formulation of this problem has been presented in (J\u00f3zefowska et al.\n2005). This subsection specializes it. Let us start with an example of a combined\nknowledge base (KB , P ).\nExample 1 (Example knowledge base (KB , P ))\nGiven is a knowledge base (KB , P ) describing bank services, presented in Table 2.\nFor the clarity of presentation, non-DL-predicates are denoted with prefix p . This\nknowledge base could not be represented in description logic or Datalog alone.\nDefinite Horn rules require all variables to be universally quantified, and therefore\nit is impossible to assert the existence of unknown individuals. For example, it is\nimpossible to assert that each account must have an owner. Moreover, Horn rules\nare unable to represent disjunctions in rule heads, and hence, it is not possible\nto model that the range of the role isOwnerOf is a disjunction of Account and\nCreditCard . In description logic, in turn, it is not possible to define a \"triangle\"\nrelationship that is modelled by the rule defining p familyAccount. \u0003\nThe task addressed in this paper is frequent pattern discovery. The patterns\nbeing found in our approach have the form of conjunctive queries over the combined\nknowledge base (KB , P ). An answer set of a query contains individuals of a userspecified reference concept \u0108. We assume that the queries are positive, i.e. they do\nnot contain any negative literals. Moreover, we assume that the queries are DL-safe.\nThis means that all variables in a query are bound to instances explicitly occurring\nin (KB , P ), even if they are not returned as a part of the query answer. In this\ncontext a query is defined as follows.\nDefinition 4 (Conjunctive DL-safe queries)\nLet (KB , P ) be a combined knowledge base in DL-safe rules with KB represented\nin SHIF. Let x = {x1 , . . . , xm } be a set of undistinguished variables (the variables\nwhose bindings are not a part of the answer) and key be the only distinguished\nvariable (that is the variable whose bindings are returned in the answer). A conjunctive query Q(key, x) over (KB , P ) is a rule using a special predicate name (that\ndoes not belong to the set of names occurring in (KB , P )) in the head, and whose\nbody is a finite conjunction of atoms of the form B (t1 , . . . , tn ), where B is an n-ary\npredicate (either from the KB component or from the disjunctive Datalog program\nP ) and ti , i = 1, . . . , n, is the distinguished variable key or a variable from x. A\nconjunctive query Q(key, x) is DL-safe if each variable occurring in a DL-atom also\noccurs in a non-DL atom in Q(key, x).\nThe inference problems for conjunctive queries are defined as follows:\n\u2022 Query answering: An answer to a query Q(key, x) w.r.t. (KB , P ) is an assignment \u03b8 of an individual to the distinguished variable key such that (KB , P ) |=\n\u2203x : Q(key\u03b8, x).\n\u2022 Query containment: A query Q2 (key, x2 ) is contained in a query Q1 (key, x1 )\nw.r.t. (KB , P ) if (KB , P ) |= \u2200key : [\u2203x2 : Q2 (key, x2 ) \u2192 \u2203x1 : Q1 (key, x1 )].\u0003\n\n\fThe role of semantics in mining frequent patterns\n\n11\n\nTerminology in KB\nClient \u2261 \u2203isOwnerOf\n> v \u2200isOwnerOf .Account t CreditCard\n\u2203isOwnerOf \u2212 v Property\nGold v CreditCard\n\nA client is defined as an owner of something.\nThe range of isOwnerOf is a disjunction of\nAccount and CreditCard .\nHaving an owner means being a property.\nGold is a subclass of CreditCard .\n\nrelative \u2261 relative \u2212\n\nThe role relative is symmetric.\n\nAccount v \u2203isOwnerOf \u2212\n\nEach account has an owner.\n\n> v \u2200hasMortgage.Mortgage\n> v \u2200hasMortgage \u2212 .Account\n> v \u2264 1hasMortgage \u2212\n\nThe range of hasMortgage is Mortgage.\nThe domain of hasMortgage is Account.\nA mortgage can be associated up to one account.\nAccount is disjoint with CreditCard .\n\nAccount \u2261 \u00acCreditCard\n\nRules in P\np familyAccount(x , y, z ) \u2190 Account(x ),\nisOwner (y, x ), isOwner (z , x ),\nrelative(y, z ), O(x ), O(y), O(z )\n\np familyAccount is an account that is\nco-owned by relatives.\n\np sharedAccount(x , y, z ) \u2190\np familyAccount(x , y, z )\n\nFamily account is a shared account.\n\np man(x ) \u2228 p woman(x ) \u2190 Client(x ), O(x ) A client is a man or a woman.\n\nAssertions in (KB, P)\np woman(Anna)\nisOwnerOf (Anna, a1)\nhasMortgage(a1, m1)\nrelative(Anna, Marek )\n\nAnna is a woman.\nAnna is an owner of a1.\nMortgage m1 is associated to account a1.\nAnna is a relative of Marek .\n\nisOwnerOf (Jan, cc1)\nCreditCard (cc1)\n\nJan is an owner of cc1.\ncc1 is a credit card.\n\nisOwnerOf (Marek , a1)\n\nMarek is an owner of a1.\n\nAccount(account2)\n\naccount2 is an account.\n\nO(i) for each explicitly named individual i\n\nEnumeration of all ABox individuals\n\nTable 2: An example of a combined knowledge base.\n\n\f12\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nIn our approach patterns are positive (i.e., without negative literals) conjunctive DL-safe queries over the combined knowledge base (KB , P ) addressing a userspecified reference concept \u0108. The atom with a reference concept as the predicate\ncontains the only distinguished variable key.\nDefinition 5 (Pattern)\nGiven is a combined knowledge base (KB , P ). A pattern Q is a conjunctive, positive\nDL-safe query over (KB , P ) of the following form:\nQ(key) =?\u2212 \u0108 (key), B1 , . . . , Bn , O(key), O(x1 ), . . . , O(xm )\nwhere B1 , . . . , Bn represent atoms of the query, Q(key) denotes that variable key\nis the only distinguished variable, and x1 , . . . , xm represent the undistinguished\nvariables of the query. Q(key) is called the head of Q, denoted head (Q), and\nthe conjunction \u0108 (key), B1 , . . . , Bn , O(key), O(x1 ), . . . , O(xm ) is called the body\nof Q, denoted body(Q). A trivial pattern is the query of the form: Q(key) =\n?\u2212\u0108 (key), O(key). \u0003\nWe assume each query posseses the linkedness property, that is each variable in\nthe body of a query is linked to the variable k ey through a path of atoms.\nDefinition 6 (Linkedness)\nA variable x is linked in a query Q iff x occurs in the head of Q or there is an atom\nB in the body of Q that contains the variable x and a variable y (different from\nx ), and y is linked. \u0003\nExamples of patterns that can be discovered from the knowlegde base introduced\nin Example 1 are presented below.\nExample 2 (Example patterns)\nConsider the knowledge base (KB , P ) from Example 1. Assuming that Client is the\nreference concept \u0108 , the following patterns over (KB , P ), may be built:\nQref (key) =? \u2212 Client(key), O(key)\nQ1 (key) =? \u2212 Client(key), isOwnerOf (key, x ), O(key), O(x )\nQ2 (key) =? \u2212 Client(key), isOwnerOf (key, x ), p familyAccount(x , key, z )\nQ3 (key) =?\u2212Client(key), isOwnerOf (key, x ), isOwnerOf (key, y), O(key), O(x ), O(y)\nQ4 (key) =? \u2212 Client(key), isOwnerOf (key, x ), CreditCard (x ), O(key), O(x )\nwhere Qref is a reference query, counting the number of instances of \u0108 . \u0003\nIn order to define the task of frequent pattern discovery we need to define how\nto calculate the pattern support.\nDefinition 7 (Support)\nLet Q be a query over a combined knowledge base (KB , P ), answerset(\u0108, Q, (KB , P ))\nbe a function that returns the set of all instances of concept \u0108 that satisfy query Q\nwith respect to (KB , P ), and let Qref denote a trivial query for which the answerset\ncontains all instances of the reference concept \u0108 in (KB , P ).\nA support of query Q with respect to the knowledge base (KB , P ) is defined as the\n\n\fThe role of semantics in mining frequent patterns\n\n13\n\nratio between the number of instances of the reference concept \u0108 that satisfy query\nQ w.r.t. (KB , P ) and the total number of instances of the reference concept \u0108 :\nsupport(\u0108, Q, (KB , P )) =\n\n|answerset(\u0108, Q, (KB , P ))|\n|answerset(\u0108, Qref , (KB , P ))|\n\n\u0003\nThe support is calculated as the ratio of the number of bindings of variable key in\nthe given query Q to the number of bindings of variable key in the reference query\nQref . The reference concept \u0108 determines what is counted. Let us now calculate\nthe support of query Q2 from Example 2.\nExample 3\nFor the illustration of the support notion, consider the queries from Example 2.\nThe reference query has 3 items in its answer set that is 3 individuals from (KB , P )\nthat are deduced to be Client due to the axiom defining a client as an owner of\nsomething. Query Q2 , for example, has 2 items in its answer set that is the clients\nthat are co-owners of at least one account with their relatives (Anna, Marek ). The\nsupport of query Q2 is then calculated as: support(\u0108 , Q2 , (KB , P )) = 32 \u22480.66.\u0003\nFinally, we can formulate our task of frequent pattern discovery in a combined\nknowledge base (KB , P ).\nDefinition 8 (Frequent pattern discovery)\nGiven\n\u2022 a combined knowledge base (KB , P ) represented in DL-safe rules, where KB\nis represented in SHIF and P is a positive disjunctive Datalog program,\n\u2022 a set of patterns in the form of queries Q that all contain a reference concept\n\u0108 as a predicate in one of the atoms in the body and where the variable in\nthe atom \u0108 is the only distinguished variable,\n\u2022 a minimum support threshold minsup specified by the user,\nand assuming that queries with support s are frequent in (KB , P ) if s \u2265 minsup,\nthe task of frequent pattern discovery is to find the set of frequent queries. \u0003\nExample 4\nLet us assume the threshold minsup=0.5 and let us consider the queries from Example 2. The set of frequent patterns is then {Qref , Q1 , Q2 , O3 }. \u0003\n4 Solution algorithm\nThe main contribution of this paper is the algorithm for frequent pattern discovery\nin combined knowledge bases represented in DL-safe rules as described in Section\n3. Initial results on the algorithm development have been presented in (J\u00f3zefowska\net al. 2006; J\u00f3zefowska et al. 2008). This section advances them. Our method follows\nthe usual approach where the search starts with the most general patterns and\nrefines them to more specific ones in consecutive steps. Thus, firstly, we define\nthe generality relation and further the refinement operator that computes a set of\nspecializations of a pattern.\n\n\f14\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n4.1 Generality relation\n\nWe use a semantic generality relation in order to fully utilize the information stored\nin the combined knowledge base (KB , P ). As we have defined in Section 3, patterns\nare represented as queries, so it seems natural to define the generality relation as\nthe query containment (or subsumption) relation.\nDefinition 9 (Generality relation)\nGiven two patterns Q1 and Q2 defined as queries over a combined knowledge base\n(KB , P ) (see Definition 5) we say that pattern Q1 is at least as general as pattern\nQ2 under query containment w.r.t. (KB , P ), Q1 \u0017B Q2 , iff query Q2 is contained\nin query Q1 w.r.t. (KB , P ). \u0003\nTheorem 1 lays the foundations for an algorithm to test the pattern subsumption.\nTheorem 1\n[Testing \u0017B ] Let Q1 (key, x1 ) and Q2 (key, x2 ) be two queries and (KB , P ) be a\ncombined knowledge base. Let \u03b8 be a substitution grounding the variables in Q2\nusing new constants not occuring in (KB , P ) (Skolem substitution). Then Q1 \u0017B Q2\nif and only if there exists a ground substitution \u03c3 for Q1 such that\n(i) head (Q2 )\u03b8 = head (Q1 )\u03c3 and\n(ii) (KB , P ) \u222a body(Q2 )\u03b8 |= body(Q1 )\u03c3\nProof\n(\u21d0) Assume there exists a ground substitution \u03c3 for Q1 such that (i ) and (ii ).\nLet a be some individual, Ia be some interpretation of (KB , P ) which is a model\nof (KB , P ) such that a is an answer to the query Q2 in Ia . In order to prove\nthat Q1 \u0017B Q2 we need to prove that a is also an answer to the query Q1 in\nIa . By definition of query answering (Definition 4) there exists a substitution \u03c6\nsuch that a is identical to key\u03c6 and \u2203body(Q2 )\u03c6 is true in Ia . Since Q2 is DL-safe\nthere must exist another substitution, \u03c60 , such that Q2 \u03c60 is ground, a is indentical\nto key\u03c60 and body(Q2 )\u03c60 is true in Ia . Because formula (KB , P ) \u222a body(Q2 )\u03b8 |=\nbody(Q1 )\u03c3 is valid, by the uniform replacement of constants we have head (Q2 )\u03c60 =\nhead (Q1 )\u03c3 and (KB , P )\u222abody(Q2 )\u03c60 |= body(Q1 )\u03c3, so body(Q1 )\u03c3 is also true in Ia .\nBecause head (Q2 )\u03c60 is identical to head (Q1 )\u03c3 this implies a = key\u03c60 is an answer to\nthe query Q1 . This argument follows for any interpretation I satisfying the initial\nconstraints, so Q1 \u0017B Q2 .\n(\u21d2) Assume Q1 \u0017B Q2 . The following arguments show that a ground substitution\n\u03c3 exists. Let a substitution \u03b8 be given as in the theorem. Let I be a model of\n(KB , P ) \u222a Q2 \u03b8. Since key\u03b8 is an answer to Q2 in I, key\u03b8 is also an answer to Q1 in\nI. Moreover, since Q1 is DL-safe, there must exist a ground substitution \u03c6 such that\nhead (Q2 )\u03b8 = head (Q1 )\u03c6, and body(Q1 )\u03c6 is true in I. By the uniform replacement\nof constants we obtain that head (Q2 )\u03b8 = head (Q1 )\u03c3, and body(Q1 )\u03c3 is true in I.\nThis argumentation is valid for any interpretation satisfying the constraints, so the\nthesis follows.\nBelow we prove that appending an atom to a query results in an equally or more\nspecific query which gives an easy way to building specializations of a query.\n\n\fThe role of semantics in mining frequent patterns\n\n15\n\nProposition 1\nLet Q2 be a query over (KB , P ), built from query Q1 by adding an atom. It holds\nthat Q1 \u0017B Q2 .\nProof\nLet us consider query Q1 =? \u2212 \u0108 (key), B1 , . . . , Bn and let us add atom Bn+1 to Q1\nobtaining query Q2 =? \u2212 \u0108 (key), B1 , . . . , Bn , Bn+1 . Let \u03b8 be an answer to query\nQ2 . According to Definition 4, (KB , P ) |= \u2203x : Q2 (key\u03b8, x). But since a query is a\nconjunction of atoms, it follows that also (KB , P ) |= \u2203x : Q1 (key\u03b8, x). Thus, the\nanswer set of query Q2 is a subset of the answer set of query Q1 what completes\nthe proof.\nA crucial property of the generality relation that allows to develop efficient algorithms is monotonicity with regard to support.\nProposition 2\nLet Q1 and Q2 be two queries over the combined knowledge base (KB , P ) that\nboth contain the reference concept \u0108. If Q1 \u0017B Q2 then support(\u0108, Q1 , (KB , P )) \u2265\nsupport(\u0108, Q2 , (KB , P )).\nProof\nIf Q1 \u0017B Q2 then, by Definition 9, query Q2 is contained in query Q1 . Further, from\nDefinition 4 we conclude that since query Q2 is contained in query Q1 then for any\npossible extensional part of (KB , P ), while keeping the same intensional part, the\nanswer set of Q2 is contained in the answer set of Q1 , and in consequence by\nDefinition 7, support(\u0108, Q1 , (KB , P )) \u2265 support(\u0108, Q2 , (KB , P )), what completes\nthe proof.\nThe monotonicity of the query containment with regard to the query support\nmeans that none of the specializations of an infrequent pattern can be frequent.\nThe generality relation \u0017B is a reflexive and transitive binary relation, and so it\nis a quasi-order on the space of patterns. It is known (Nienhuys-Cheng and de Wolf\n1997) that any quasi-ordered space may be searched using refinement operators. In\nthe next section we define the refinement operator used in our algorithm.\n4.2 Refinement operator\nWe define a downward refinement operator that computes a set of specializations of\na query. This set is obtained using both syntax and semantics of the query. Firstly,\na query is appended with a single atom according to the rules given in Definition\n10. In the second step semantic tests are performed which may exclude further\npatterns from consideration.\nIt is convenient to represent the results of the refinement steps on a special trie\nstructure that was introduced in the FARMER method (Nijssen and Kok 2001;\nNijssen and Kok 2003). Trie is a tree with nodes corresponding to the atoms of\nthe query, so that each path from the root to any node corresponds to a query. In\n\n\f16\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nFig. 1: A part of the trie constructed for the (KB , P ) from Example 1, \u0108 =Client,\nminsup = 0.2.\nconsequence every node in a trie defines a query. According to Propositions 1 and\n2 only nodes which correspond to frequent queries need to be expanded further.\nAn example of the trie data structure for a data mining problem defined over\nthe knowledge base from Example 1 is presented in Figure 1. In order to build the\npatterns the following predicates from the knowledge base were selected: Client,\nisOwnerOf , relative, hasMortgage and p woman. Notice that the special purpose\npredicates (the ones of the form O(x )) are omitted from the presentation in the trie.\nThe presence of such predicates indicates that a query is DL-safe. As we assume\nthat all queries within our approach are DL-safe, we can omit the special purpose\npredicates for simplicity. The superscripts in Figure 1 correspond to the two ways\ndescribed in Definition 10 in which atoms are added to the query.\nDefinition 10\nLet T be a trie data structure that imposes an order of atoms in a query. Let Q be\na query, let B be last(Q) that is the last atom in query Q, let Bp be the parent of\nB in T . A variable is called new if it does not occur in any earlier atom of a query.\nAtoms are added to trie T as:\n1. dependent atoms (share at least one variable with last(Q), that was new in\nlast(Q)),\n2. right brothers of a given node in T (these are the copies of atoms that have\nthe same parent Bp as the given atom B and are placed on the right-hand\nside of B in Bp 's child list), new variables are renamed such that they are\nalso new in the copy.\u0003\nThe first rule introduces the dependent atoms that could not be added earlier.\nThe dependent atoms are brothers of each other in the trie. The second rule, the\nright brother copying mechanism, takes care that all possible subsets but only one\npermutation out of the set of dependent atoms is considered.\nLet us now introduce the semantic tests which are performed as the second step\nof the refinement procedure in order to reduce the set of patterns submitted for\nfrequency evaluation. Due to the efficiency reasons semantic tests are performed\n\n\fThe role of semantics in mining frequent patterns\n\n17\n\non a knowledge base (KB , P ) with ground facts like concept and role assertions\nremoved. This reduced knowledge base is denoted by cp(KB , P ). The first test\nconsists in determining the query satisfiability, further ones check for some kinds\nof semantic redundancy as described later in this section.\nThe test for checking satisfiability of query Q(key, x) with regard to knowledge\nbase cp(KB , P ) consists in checking whether cp(KB , P )\u222a{\u2203key, x : Q } is satisfiable\nthat is whether there is a model of cp(KB , P ) in which there is some valuation for\nthe distinguished variable key and undistinguished variables x. The variables are\nskolemized, and assuming that a and b are new constants, Q(a, b) is asserted to\ncp(KB , P ). Then it is checked whether the updated cp(KB , P ) is satisfiable. The\nquery satisfiability test described above is defined in Definition 11 below.\nDefinition 11\nQuery Q is satisfiable w.r.t. a combined knowledge base (KB , P ) iff (KB , P ) \u222a Q\u03b8\nis satisfiable, where \u03b8 is a Skolem substitution. \u0003\nExample 5\nLet us consider the knowledge base (KB , P ) from Example 1 and the query:\nQ(key) =? \u2212 Account(key), CreditCard (key), O(key)\nSince in (KB , P ) the concepts Account and CreditCard are specified as disjoint,\nwe know a priori that it is useless to submit the query Q as it cannot have any\nanswer due to its unsatisfiability. \u0003\nAfter performing the satisfiability test, the queries are further pruned in order to\nobtain only those candidates that are not semantically redundant. We consider two\nkinds of semantic redundancy. The first kind occurs when a query has redundant\natoms, that is atoms that can be deduced from other atoms in the query. The second\nkind occurs when there are frequent queries already found in the earlier steps that\nare semantically equivalent to the newly generated candidate.\nIn order to avoid the first kind of redundancy, the queries are tested for semantic\nfreeness. Only semantically free queries are kept for further processing. The notion\nof the semantic freeness has been introduced in (de Raedt and Ramon 2004). It is\nadapted to our setting as follows.\nDefinition 12 (Semantically free pattern)\nA pattern Q is semantically free or s-free w.r.t a combined knowledge base (KB , P )\nif there is no pattern Q 0 , built from Q by removing any atom, such that Q \u0017B Q 0 .\n\u0003\nExample 6\nGiven is the knowledge base from Example 1 and the following queries to this\nknowledge base:\nQ1 (key) =? \u2212 Account(key), isOwnerOf (x , key), O(key), O(x )\nQ2 (key) =? \u2212 Account(key), isOwnerOf (x , key), Client(x ), O(key), O(x )\n\n\f18\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nQuery Q1 is s-free while query Q2 is not. The reason why the second query is not sfree is that atom Client(x ) can be deduced from the other atoms of this query. More\nspecifically, the atom Client(x ) can be deduced from the atom isOwnerOf (x , key)\nas from the axioms in the knowledge base it follows that any object being asserted\nto the domain of isOwnerOf is a Client. \u0003\nMoreover, the test for semantic freeness is performed on a query with the atom\n\u0108(key) removed. It is motivated by the fact, that some queries could be pruned\nafter the s-freeness test, that we do not necessarily would like to be pruned, just\nbecause of the obligatory presence of the reference concept in each query.\nExample 7\nConsider the knowledge base from Example 1 and Client as a reference concept.\nThen query:\nQ(key) =? \u2212 Client(key), isOwnerOf (key, x ), O(key), O(x )\ndoes not pass the s-freeness test from Definition 12, because the atom Client(key)\ncan be deduced from the second atom of Q. However, the atom Client(key) contains\nthe reference concept, which is obligatory in each query. Consider now query Q 0 ,\nobtained by removing the atom with a reference concept from query Q:\nQ 0 (key) =? \u2212 isOwnerOf (key, x ), O(key), O(x )\nThe modified query, Q 0 , is s-free. Reconsider the queries from Example 2. Queries\nQref , Q1 , Q2 and Q4 are s-free with regard to the modified s-freeness test, while\nquery Q3 is not s-free. \u0003\nA candidate query may be semantically redundant not only due to redundant\natoms. The second kind of redundancy occurs when a candidate query is semantically equivalent to a frequent one already found. Such patterns are also pruned,\nwhich is performed by searching the trie for a pattern equivalent to the given\none. So-called optimal refinement operator assures that no pattern is generated\ntwice. Using the trie data structure and pruning the candidate patterns that are\nsemantically equivalent to the ones already found, make our refinement operator\noptimal. By pruning semantically equivalent patterns we achieve also the property\nof properness of the refinement operator, that is every pattern Q 0 generated by the\nrefinement operator is more specific than the pattern Q being refined (Q 0 is never\nequivalent to Q).\n4.3 The algorithm\nThe approach proposed in this paper follows the common scheme of algorithms for\nfinding frequent patterns which is a \"generate-and-test\" approach. In such approach\ncandidate queries are repeatedly generated and tested for their frequency. In order\nto generate candidates, a refinement operator is applied.\nThe proposed, recursive node expansion algorithm is presented below. A node\n\n\fThe role of semantics in mining frequent patterns\n\n19\n\nbeing expanded is denoted by ni , Q(key, x) denotes a query with x being undistinguished variables, d denotes the depth of the current node in the trie T . The trie\nis generated up to the user-specified MAXDEPTH depth.\nAlgorithm 1\nexpandNode(nd , Q(key, x), d , T , MAXDEPTH)\n1. if d < MAXDEPTH then\n2.\nwhile all possible children of nd not constructed do\n3.\nconstruct child node nd+1 and associated query Qc (key, x) using the trie data\nstructure T and refinement rules from Definition 10\n4.\nif Qc (key, x) is satisfiable wrt (KB , P ) then\n5.\nif Qc (key, x) is semantically free wrt (KB , P ) then\n6.\nif Qc (key, x) is not semantically equivalent wrt (KB , P ) to any frequent\nquery found earlier then\n7.\nevaluate candidate query Qc (key, x)\n8.\nif Qc (key, x) is frequent then\n9.\naddChild(nd , nd+1 ); //add nd+1 as a child of nd\n10.\nT \u2190 T \u222a nd+1 ;\n11.\nfor all children nd+1 of node nd do\n12.\nexpandNode(nd+1 , Qc (key, x), d + 1, T , MAXDEPTH)\n\nCompleteness of search Below we prove the completeness of our method for pattern\nrefinement, that is we prove that the proposed approach to pattern mining generates\nfor each pattern Q from the space of valid patterns a valid pattern Q 0 such that Q 0\nis semantically equivalent to Q. Valid patterns are those, from the ones defined in\nDefinition 5, that are linked and semantically free. In order to prove completeness,\nwe relate to the work on FARMER (Nijssen and Kok 2003), that originally used\ntrie data structure for relational, frequent pattern mining.\nFirst we prove that pruning semantically equivalent patterns (after s-freeness test\nor after the search on the trie) does not exclude adding all possible refinements to\na pattern.\nLemma 1\nLet (KB , P ) be a combined knowledge base, and Q1 , Q2 be two semantically equivalent patterns (Q1 \u2261B Q2 ) over (KB , P ). Then for each variable x in Q1 there exists\na corresponding variable x 0 in Q2 to which the same bindings can be made as to\nthe variable x .\nProof\nBy definition (Definition 5) both patterns have the same distinguished variable key,\nso the thesis follows for x = key. Let us now provide the following argumentation for\nx being an undistinguished variable. Since Q1 \u2261B Q2 then also Q1 \u0017B Q2 . Suppose\n\u03b8 is a Skolem substitution grounding variables in Q2 that satisfies the constraints\nfrom Theorem 1. By definition, the substitution \u03b8 assigns a new individual a to\nvariable key. The individual a is an answer to Q2 , and since Q1 \u0017B Q2 , a is also an\nanswer to Q1 . For Q1 \u0017B Q2 to be valid there must exist a grounding substitution \u03c3\nfor Q1 that satisfies the constraints from Theorem 1. Since Q2 is linked, that is all of\nits variables are linked to the variable key, then also all the constants introduced by\n\n\f20\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\n\u03b8 are linked to the individual a. Since Q1 is linked, then all the constants that bind\nto variables of Q1 to prove the answer a have to be linked to a as well. Since a and\nall the constants introduced to the (KB , P ) by \u03b8 are new, then any other constants\nin the (KB , P ) are not linked to a. In consequence, only the constants introduced\nto the (KB , P ) by \u03b8 can be a part of the substitution \u03c3. Then for each variable\nx in Q1 there must exist a constant b that is assigned to x by the substitution \u03c3\nand has been introduced by \u03b8. That is why there must exist a variable x 0 in Q2\nfor which \u03b8 introduces b, and what follows the same bindings that can be made to\nvariable x in Q1 can be as well made to the corresponding variable x 0 in Q2 . This\nargumentation is valid for any variables x and x 0 , what completes the proof.\nThe following corollary is a consequence of Lemma 1.\nCorollary 1\nLet (KB , P ) be a combined knowledge base, and Q1 , Q2 be two semantically equivalent patterns (Q1 \u2261B Q2 ) over (KB , P ). Then for each variable x in Q1 there exists\na corresponding variable x 0 in Q2 such that any atom B that can be linked to Q1\nthrough the variable x can be also linked to Q2 through the variable x 0 .\nSubsequently we prove that all possible refinements of a pattern are generated.\nLemma 2\nGiven is a trie T , recursively generated by Algorithm 1, a query Q which occurs in\nT , and an atom B \u2208\n/ Q which is a valid refinement of Q. Then either:\n(i) valid query Q 0 = (Q1 , B , Q2 ) exists in trie T , for some subdivision of Q into\nQ1 and Q2 , such that Q = (Q1 , Q2 ) or\n(ii) valid query Q 00 exists in trie T , such that query Q 00 is semantically equivalent\nto query Q 0 .\nProof\nConsider case (i). As B is a valid refinement of Q, there is a prefix (Qp , Bp ) of Q\nsuch that atom B is a dependent atom of Bp . If Bp is the last atom of Q, then it\nis clear that B , as a dependent atom of Bp , is generated as a refinement of Q to\nbe added at the end of the query. Dependent atom B is generated by the first rule\nfrom Definition 10 and checked for its validity (satisfiability and s-freeness). Hence,\nquery Q 0 is generated. Let us assume now that Bp is not the last atom and it has\ndifferent successor Bp+1 in query Q. Atom Bp+1 is also a child of Bp in T . Then\nlet us consider the order of B and Bp+1 in the list of children of Bp in trie T , which\nis one of the following:\n\u2022 B occurs before Bp+1 ; then Bp+1 is a right-hand brother of B . The right brothers\ncopying mechanism, the second rule from Definition 10, will copy Bp+1 as a child\nof B ; the same operations that created Q will create query Q 0 in subsequent steps.\n\u2022 B occurs after Bp+1 ; B is copied as a child of Bp+1 . In order to determine the exact\ninjection place of B , we recursively apply our arguments, taking into account Bp+1\nand B .\n\n\fThe role of semantics in mining frequent patterns\n\n21\n\nIt follows from the above arguments that query Q 0 is always generated. After generation of query Q 0 , it is checked, in line 6 of Algorithm 1, if query Q 0 is semantically\nequivalent to some query Q 00 , already present in the trie T . If it is the case, Q 00 is\nkept in T , and Q 0 is not added to T . Otherwise, the newly generated query Q 0 is\nadded to the trie T . Thus, either query Q 0 exists in the trie T or it is semantically\nequivalent to query Q 00 . This completes the proof.\nFinally we prove the completeness.\nTheorem 2 (Completeness)\nFor every valid, frequent query Q1 in the pattern space, there is semantically equivalent valid query Q2 in the trie T .\nProof\nLet us assume that queries are generated up to the user specified length (MAXDEPTH).\nFor query Q1 of length 1 it is obvious that there is a corresponding query Q2 of the\nform Q(key) =? \u2212 \u0108 (key), O(key) in the root of the trie (atoms of the form O(x )\nare not taken into account as described earlier). For query Q1 of length \u2265 1, the\nproof is by induction on the length of the query. Assume that an equivalent query\nfor Q1 \\last(Q1 ) exists in trie T . From Corollary 1 follows that any refinement that\ncan be made to Q1 \\last(Q1 ) can be also made to any of its equivalent queries.\nIf atom last(Q1 ) is a valid refinement of the equivalent query, Lemma 2 applies.\nHence, the thesis follows by induction.\n4.4 Implementation\nThe proposed method employs several reasoning services run over a combined\nknowledge base (KB , P ) such as: (conjunctive) query answering, deciding knowledge\nbase satisfiability, deciding concept subsumption, classifying the concept hierarchy.\nIn order to perform all these reasoning services, specialized and complex algorithms\nare needed. As the implementation of such reasoning services is out of the scope of\nthis work, to test our ideas we decided to use an external reasoner KAON210 .\nIn the core of KAON2 there is an algorithm for reducing a DL knowledge base\nKB into a disjunctive Datalog program DD(KB ) on which the actual reasoning\nis performed using the techniques of deductive databases. In particular, KAON2\nuses a version of Magic Sets optimization technique, originally defined for nondisjunctive programs and recently extended to disjunctive Datalog, in order to\nidentify the part of the database relevant to the query. And it applies semi-na\u0131\u0308ve,\nbottom-up evaluation strategy, in order to avoid redundant computation of the\nsame conclusions. Employing these techniques makes KAON2 well suited for a frequent pattern mining application. It has been experimentaly shown that in case of\nthe knowledge bases with relatively small intensional part, but large number of instances, KAON2 outperforms the reasoners using the classical tableaux algorithms\nby one to two orders of magnitude (Motik and Sattler 2006; Parsia et al. 2006).\n10\n\nhttp://kaon2.semanticweb.org\n\n\f22\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nQuery\n\ne.g. instance retrieval\nfor named concepts\n\nDL TBox\n\nDL ABox\n\nRules\n\nTransformation to\nDisjunctive Datalog\nprogram\n\nDisjunctive Datalog\nreasoning engine\n\nFig. 2: Reasoning in KAON2.\nFigure 2 presents an overview of the reasoning in KAON2.\nWe implemented the proposed method for pattern mining in a system called\nSEMINTEC11 (Semantically-enabled data mining techniques). Our implementation is written in Java (version 1.5). It uses KAON2's API to manipulate and\nreason on combined knowledge bases. Figure 3 presents the input and output of\nour system and illustrates the interaction with the reasoner. As an input to the\nsystem, the user is expected to provide the following files: setup file (in XML format, with the parameters of the execution such as the logical and physical URI of\nthe knowledge base, reference concept, minimum support threshold etc.) and knowledge base files (in OWL and SWRL12 formats). As an output the system generates\nthe files with: frequent patterns discovered during the execution, statistics of the\nexecution, and a file with a trie that stores patterns, in XML-based GraphML13\nformat. The implementation of SEMINTEC is publicly available14 .\n5 Experimental evaluation\nIn this section, we present an experimental evaluation of the proposed method for\nfrequent pattern mining with the focus on the usefulness of exploiting the semantics\nof the knowledge base at different steps of our algorithm. In particular, the goals of\nthe experiments were to investigate the influence of using intensional background\nknowledge expressed in DL with DL-safe rules on the data mining efficiency (i.e.,\ncomputing time) and the quality of the results (i.e., the number and the form of the\ndiscovered patterns). We wanted to test how our method performs on datasets of\ndifferent sizes and complexities, in order to obtain an idea what kinds of ontologies\n11\n12\n13\n14\n\nhttp://www.cs.put.poznan.pl/alawrynowicz/semintec.htm\nwww.w3.org/Submission/SWRL/\nhttp://graphml.graphdrawing.org\nhttp://www.cs.put.poznan.pl/alawrynowicz/semintec.htm\n\n\fThe role of semantics in mining frequent patterns\n\n23\n\nSEMINTEC\nSetup file\n*.xml\n\n(KB, P)\n\n(KB, P)\ncopy\n\nFrequent\npatterns\n\nStatistics\n(KB, P) Management\n(KAON2 ontology\nmanagement API)\nTesting query\nsatisfiability\n\nTesting query\ns-freeness\n\nTesting query\nequivalency\n\nEvaluating\nquery\n\nTrie\n*.graphml\n\nKnowledge base (KB, P)\n*.owl\n(OWL+SWRL)\n\nReasoner\n(KAON2 reasoner)\n\nTesting (KB, P)\nsatisfiability\n\nAnswering\nconjunctive query\n\nFig. 3: SEMINTEC input/output and interaction with the reasoner.\ncan be handled efficiently. In particular, the experiments were supposed to answer\nthe following questions:\n\u2022 how using the intensional part of the background knowledge for the semantic\ntests of generated patterns influences the execution time and the results of\npattern discovery?\n\u2022 how the complexity of the intensional background knowledge, in particular\nthe types of DL constructors and DL axioms, influences the execution time\nand the results of pattern discovery?\n\u2022 how exploiting concept and role taxonomies influences the execution time of\npattern discovery?\nTest datasets For the tests we used three datasets, whose general characteristics\nis presented in Table 3. The (FINANCIAL)15 dataset was created on the basis of\na dataset from the PKDD'99 Discovery Challenge as a part of our research presented in this paper, and currently is the part of the benchmark suite of KAON2.\nFINANCIAL ontology describes the domain of banking. FINANCIAL dataset\nis relatively simple, as it does not use existential quantifiers or disjunctions. It\ncontains, however, functional roles and disjointness constraints. Thus, it requires\nequality reasoning, which is difficult for deductive databases.\nSWRC ontology, as used in our experiments, was published at the 4th International EON Workshop (EON2006)16 . It was a part of the testbed17 used in the\nontology evaluation session at the workshop. SWRC ontology (\"Semantic Web\n15\n16\n17\n\nFINANCIAL, http://www.cs.put.poznan.pl/alawrynowicz/financial.owl\nhttp://km.aifb.uni-karlsruhe.de/ws/eon2006\nhttp://km.aifb.uni-karlsruhe.de/ws/eon2006/ontoeval.zip\n\n\f24\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\nTable 3: Characteristics of the test datasets.\n\ndataset\n\nDL\n\n#concepts\n\n#obj. roles\n\n#rules\n\n#individuals\n\nFINANCIAL\nrSWRC\nrLUBM\n\nALCIF\nALI(D)\nSHI(D)\n\n60\n55\n43\n\n16\n44\n25\n\n0\n3\n2\n\n17941\n2156\n17174\n\nRules in rSWRC\np knowsAboutTopic(x , z ) \u2190 Person(x ), worksAtProject(x , y), isAbout(y, z )\np coAuthoredByFullProfessor (x ) \u2190 Article(x ), author (x , y), FullProfessor (y)\nfinances(x , z ) \u2190 Organization(x ), finances(x , y), Project(y), isAbout(y, z )\nRules in rLUBM\nGraduateStudent(x ) \u2190 Person(x ), takesCourse(x , y), GraduateCourse(y)\np specialCourse(z ) \u2190 FullProfessor (x ), headOf (x , y), teacherOf (x , z )\n\nfor Research Communities\") represents knowledge about researchers and research\ncommunities. Instance data, published at the EON website, describes the AIFB\nInstitute of the University of Karlsruhe. The TBox of this ontology contains concept inclusion axioms, universal quantification, but no existential quantifiers, and\nno disjunctions, so it is simple. By rSWRC we denote our extension of this dataset\nby the rules presented in Table 3.\nLUBM is a benchmark from the Lehigh University18 , consisting of a university\ndomain ontology and a generator of synthetic data. Existential quantifiers are used,\nbut no disjunctions or number restrictions occur, hence the reduction algorithm of\nKAON2 produces an equality-free Horn program, on which query answering can\nbe performed deterministically. In the experiments we used rLUBM, an extension\nof LUBM ontology by two rules (presented in Table 3) which was proposed by the\nauthors of the DL-safe rules component of Pellet in (Parsia et al. 2006).\nTest setting All tests were performed on a PC with Intel Core2 Duo 2.4GHz processor, 2GB of RAM, running Microsoft Windows Server 2003 Standard Edition\nSP1. The JVM heap size was limited to 1.5GB. We used the version of KAON2\nreleased on 2008-01-14.\n5.1 Results of the experiments\n5.1.1 Analysis whether semantic tests of generated patterns are useful\nThe goal of this experiment was to compare the setting where intensional background knowledge was used for testing generated candidates as well as for evaluat18\n\nLUBM, http://swat.cse.lehigh.edu/projects/lubm/\n\n\fThe role of semantics in mining frequent patterns\n\n25\n\ning them with the setting where the background knowledge was used only during\nthe candidate evaluation. We were interested in efficiency and quality of the results. The bias consisted of restricting the predicates, used to build patterns, only\nto those having any extension (to avoid testing predicates without any assertions),\nand giving new names to all variables in the newly added dependent atoms, except\nthe variables shared with the last atom in a query.\nIn the first setting, SEM, the original algorithm for query expansion was used,\nthat is Algorithm 1. In the second setting, NOSEM, the algorithm was run without\nthe steps for checking pattern satisfiability, s-freeness and equivalence with already\nfound frequent patterns, that is, lines 4-6 from Algorithm 1 were omitted. However,\nthe other parts of the solution such as the trie data structure as well as the techniques for reducing syntactic redundancy based on the trie were left unchanged, and\nused in the second setting as well. Hence, some assumptions made for the kinds of\npatterns expected as the result of the execution of our method were applied for both\nsettings. In particular, syntactically non-redundant copies of atoms, in which output\nvariables were given new names, were not generated as dependent atoms in both settings. Not generating copies of atoms, which is based on the assumption of generating only s-free candidate patterns, greatly influences the time and the results of the\npattern mining, as without the semantic tests for redundancy, one could not avoid\nchains like: Client(x ), isOwnerOf (x , y1 ), isOwnerOf (x , y2 ), isOwnerOf (x , y3 ), * * *.\nThus, we compare our proposed setting with the one which is not strictly naive\nand which lacks the most time consuming operations.\nThe parameters measured during an execution of the experiment, were: (i) running time (runtime), (ii) number of candidate patterns (cand ), (iii) number of frequent patterns (freq). Good results are characterized by low number of candidates\nand frequent patterns, and short running time. Additionally, a ratio of frequent\npatterns to candidate patterns should be as high as possible, that is, as few as\npossible unproductive candidate patterns should be evaluated.\nQualitative analysis Below we present and discuss some patterns discovered during\nthe experimental evaluation. We restrict the analysis to the ontologies with real\n(nonsynthetic) data.\nThe following is one of the longest patterns discovered from the FINANCIAL\ndataset, by our method (SEM setting):\nQSEM 1 (key) = Client(key), hasOwner (x1 , key), hasStatementIssuanceFrequency(x1 , x2 ),\nMonthly(x2 ), hasPermanentOrder (x1 , x3 ), isPermanentOrderFor (x3 , x5 ), Household \u2212\nPayment(x5 ), hasAgeValue(key, x7 ), hasSexValue(key, x8 ), FemaleSex (x8 ), livesIn(key, x10 );\nsupport =0.29\nIt describes \"a client who is an owner of an account with monthly statement issuance frequency, and with a permament order for household payment, who is a\nfemale, lives in some region, and is at some age\". The information that Account is\nhere the domain of hasOwner and Region is the range of livesIn comes from the\nFINANCIAL ontology. One may notice, that the region in which the client lives\n\n\f26\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nand the age at which she is, is not specified in this pattern. Example, shorter patterns discovered, that involve roles hasAgeValue or livesIn and precise their range\nare shown below:\nQSEM 2 (key) = Client(key), hasOwner (x1 , key), hasStatementIssuanceFrequency(x1 , x2 ),\nMonthly(x2 ), hasAgeValue(key, x4 ), From35To50(x4 ); support=0.21\nQSEM 3 (key) = Client(key), livesIn(key, x1 ), NorthMoravia(x1 ); support=0.17\nAn example of a pattern discovered by running NOSEM setting is as follows:\nQNOSEM 1 (key) = Client(key), livesIn(key, x1 ), Region(x1 ); support=1.0\nThe pattern QNOSEM 1 has the semantically redundant atom, Region(x1 ), due to\nthe specification of Region as the range of role livesIn in the FINANCIAL KB .\nLet us now present the example patterns discovered from the rSWRC dataset.\nBy running the SEM setting, the following example patterns have been discovered:\nQSEM 4 (key) = Person(key), author (x1 , key), publication(x2 , x1 ), p knowsAboutTo \u2212\npic(x2 , x3 ); support=0.70\nQSEM 5 (key) = Person(key), author (x1 , key), publication(key, x2 ), Publication(x2 );\nsupport=0.75\nThe meaning of pattern QSEM 4 may seem unclear with regard to the rSWRC\nknowledge base. In the knowledge base neither ranges nor domains of author and\npublication are specified. However, from the rule defining p knowsAboutTopic we\nknow that its first argument represents Person and the second one Topic. Thus,\nwe may conclude that the pattern says that \"some person, who knows about some\ntopic, is related to the publication who is authored by the person represented by\nthe reference concept\". From the intensional part of the (KB , P ) we do not know\nabout the nature of this relation for Person as role publication is missing domain\nand range specifications. By deeper analysis of the knowledge base, we may notice\nthat concept AcademicStaff , that is the subconcept of Person, is subsumed by concept \u2200publication.Publication. Thus for academic staff, a particular type of persons,\npublication range is Publication.\nIn pattern QSEM 5 , a person is related by role publication with some Publication.\nBy the common sense reasoning, this pattern carries redundant information. It is,\nhowever, s-free, as the range of publication is not specified in the (KB , P ).\nWith regard to the NOSEM setting let us discuss the following pattern:\nQNOSEM 2 (key) = Person(key), publication(key, x1 ), Publication(x1 ), InProceedings(x1 );\nsupport=0.47\nSince in the (KB , P ), InProceedings is the subconcept of Publication, atom Publication(x1 )\nis semantically redundant.\n\n\fThe role of semantics in mining frequent patterns\n\n27\n\nTable 4: Results of the experiment on effectiveness of the semantic tests.\n\nMax\nLength\n\nnumber of patterns\nNOSEM\nSEM\ncand\nfreq\ncand\nfreq\n\nreduction\nNOSEM/SEM\ncand\nfreq\n\nruntime[s]\nNOSEM\nSEM\n\nspeedup\nNOSEM/SEM\n\nFINANCIAL, minsup=0.2, reference concept=Client\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n1\n91\n582\n2786\n-\n\n1\n9\n69\n479\n-\n\n1\n15\n71\n253\n569\n1009\n1524\n1963\n2307\n2513\n2608\n2634\n\n1\n7\n27\n68\n131\n214\n303\n376\n421\n440\n444\n444\n\n1.00\n6.07\n8.20\n11.01\n-\n\n1.00\n1.29\n2.56\n7.04\n-\n\n0.5\n42.7\n303.5\n2931.6\n-\n\n0.5\n14.2\n104.5\n569.7\n2166.9\n6042.7\n12204.8\n20200.0\n26346.1\n29614.2\n30309.6\n30821.6\n\n1.07\n3.01\n2.91\n5.15\n-\n\n1.00\n1.00\n1.57\n2.72\n\n0.1\n7.4\n23.0\n169.1\n\n0.1\n16.5\n155.2\n2533.5\n\n1.01\n0.45\n0.15\n0.07\n\n1.00\n1.17\n2.03\n4.07\n\n0.3\n12.5\n82.8\n9713.0\n\n0.3\n16.5\n142.6\n3486.7\n\n1.00\n0.76\n0.58\n2.79\n\nrSWRC, minsup=0.3, reference concept=Person\n1\n2\n3\n4\n\n1\n92\n279\n1556\n\n1\n3\n22\n272\n\n1\n92\n271\n913\n\n1\n3\n14\n100\n\n1.00\n1.00\n1.03\n1.70\n\nrLUBM, minsup=0.3, reference concept=Person\n1\n2\n3\n4\n\n1\n68\n361\n2885\n\n1\n7\n63\n789\n\n1\n67\n269\n1438\n\n1\n6\n31\n194\n\n1.00\n1.01\n1.34\n2.01\n\nQuantitative analysis Table 4 shows the results for a selected support threshold\nfor each dataset. The results are shown up to the lengths of patterns where either\nan execution of the proposed method (SEM ) has not exceeded the threshold of 24\nhours of the running time (rSWRC, rLUBM) or the whole trie was generated in\nthis setting (FINANCIAL). From the presented results one can conclude that with\nregard to the reduction in the number of patterns, there is a gain for all datasets,\nreaching 11.01 times for candidate patterns and 7.04 times for frequent patterns in\ncase of the FINANCIAL dataset.\nWith regard to the running time, in case of FINANCIAL dataset, the speedup\nhas been reached for all maximum lengths of patterns. For longer patterns, the\nNOSEM setting was unable to finish execution in 24 hours, while executing the SEM\nsetting allowed to generate the whole trie of frequent patterns for FINANCIAL\ndataset. In case of rLUBM dataset, the speedup has been reached for the longest,\nmost important, maximum pattern length. For rSWRC, however, the NOSEM setting was significantly better with regard to the running time.\n\n\f28\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nWe also measured the method performance for different minimum support thresholds. The results are reported in Figure 4. The bars representing the numbers of\nfrequent patterns are superimposed on those, representing the numbers of candidate\npatterns. In case of the FINANCIAL dataset the differences between the numbers\nof candidate patterns in the SEM setting in comparison to the NOSEM setting\nare the largest from among those of the tested datasets. The number of candidate\npatterns in the SEM setting constitutes about 9% of that in the NOSEM setting.\nFor the rLUBM dataset this ratio is about 45% on average and for rSWRC is\nabout 59% on average. The number of frequent patterns in the SEM setting is on\naverage equal to about 14% of the number of frequent patterns in the NOSEM setting for the FINANCIAL dataset, about 22% for rLUBM dataset and about 37%\nfor rSWRC dataset. Since in case of the FINANCIAL dataset, the differences in\npattern numbers between the SEM setting and the NOSEM setting are the largest\nfrom among the tested datasets, relatively the biggest number of semantically redundant patterns is pruned away for this dataset, while for the rSWRC dataset\nthis number is the lowest one.\nLet us now discuss the ratio between the number of frequent and the number of\ncandidate patterns in case of the SEM setting. For the FINANCIAL dataset this\nratio is equal 26% on average, for the rLUBM dataset 13% on average, and for\nthe rSWRC 11% on average. Thus, in case of the FINANCIAL dataset relatively\nthe least computation is done to evaluate useless candidate patterns. In case of the\nrSWRC dataset the computational effort is relatively the largest.\nSummarizing, the semantic tests performed during the pattern generation were\nuseful in terms of the number of patterns for all datasets, and in the running\ntime for FINANCIAL and rLUBM datasets but not for rSWRC dataset. They\nwere most useful for the FINANCIAL dataset, where relatively the least number\nof patterns were generated and tested in the SEM setting in comparison to the\nNOSEM setting, and where the ratio between frequent and candidate patterns in\nthe SEM setting was the biggest. The semantic tests were least useful in case of\nthe rSWRC dataset.\nAfter the analysis of the results for rSWRC dataset one may pose the following\nquestion: should the semantic tests on patterns be performed together with checking their frequency or they should be performed afterwards as a postprocessing step\nof pattern mining? For the FINANCIAL and rLUBM datasets it is clear that it\nwas better to perform the tests together with pattern evaluation. The running times\nin SEM setting (at least for the longest patterns in case of rLUBM) are already\nshorter than those in NOSEM setting. For the rSWRC dataset we performed additional test. We took the patterns generated as the output of NOSEM setting\n(MAXLENGTH = 4) and postprocessed them, leaving at the first step only s-free\nones, and at the second step only one representative of each equivalence class of\npatterns. The additional execution time was 907.5s, which together with the execution time of NOSEM setting, 169.1s (as specified in Table 4), gives 1076.6s. This\ntime is shorter than the time of the SEM setting execution which is 2533.5s. That\nis, in case of rSWRC dataset it was faster to perform data mining without semantic\ntests at the first step and then perform the tests as a postprocessing step.\n\n\fThe role of semantics in mining frequent patterns\n\nFINANCIAL\n\n5000\n\nSEM_CAND\n\n5000\n\n4500\n\nNOSEM_FREQ\n\n4000\n\nSEM_FREQ\n\n3500\n\n4000\n\nruntime(s)\n\nnumber of patterns\n\nFINANCIAL\n\nNOSEM_CAND\n\n6000\n\n3000\n\n2000\n\n3000\n2500\n\nNOSEM\n\n2000\n\nSEM\n\n1500\n\n1000\n\n1000\n\n500\n\n0\n\n0\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.1\n\n0.15\n\nMinsup\n\n0.2\n\n0.25\n\n0.3\n\nMinsup\n\n(a) Number of patterns\n\nrSWRC\n\n(b) Running time\n\nrSWRC\n\nNOSEM_CAND\nSEM_CAND\n\n2000\n\n3500\n\nNOSEM_FREQ\n\n3000\n\nSEM_FREQ\n\n2500\nruntime(s)\n\nnumber of patterns\n\n29\n\n1000\n\n2000\nNOSEM\n\n1500\n\nSEM\n\n1000\n500\n0\n\n0\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.2\n\n0.25\n\nMinsup\n\n(c) Number of patterns\n\nrLUBM\n\n0.35\n\n0.4\n\n(d) Running time\n\nrLUBM\n\nNOSEM_CAND\n12000\n\nSEM_CAND\n\n6000\n\n10000\n\nNOSEM_FREQ\n\n5000\n\nSEM_FREQ\n4000\n\nruntime(s)\n\nnumber of patterns\n\n0.3\nMinsup\n\n3000\n\n8000\n6000\n\n2000\n\n4000\n\n1000\n\n2000\n\n0\n\nNOSEM\nSEM\n\n0\n0.2\n\n0.25\n\n0.3\n\n0.35\n\nMinsup\n\n(e) Number of patterns\n\n0.4\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\nMinsup\n\n(f) Running time\n\nFig. 4: Results of the experiment, MAXLENGTH=4, FINANCIAL: \u0108 =Client,\nrSWRC: \u0108 =Person, rLUBM: \u0108 =Person.\n\nWhy rSWRC dataset is especially hard for our approach, while the others are\nnot, is discussed in Section 5.1.2, which provides more insight into the influence\nof using intensional background knowledge during pattern generation. It is also\nnoteworthy, that for the same intensional background knowledge, but for a bigger number of assertions (more probable case for data mining applications) it may\nbe better to perform the semantic tests together with pattern evaluation. As an\n\n\f30\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nFINANCIAL\n\nrSWRC\n\n16000,00\n\n4000,00\n\n14000,00\n\n3500,00\n3000,00\nruntime(s)\n\nruntime(s)\n\n12000,00\n10000,00\n8000,00\n\nSEM\n\n6000,00\n\n2500,00\n2000,00\n1500,00\n\nSEM\n\n1000,00\n\nNOSEM\n\nNOSEM\n\n500,00\n\n4000,00\n\n0,00\nrSWRC\nrSWRC_2\nrSWRC_3\nrSWRC_4\nrSWRC_5\nrSWRC_6\nrSWRC_7\nrSWRC_8\nrSWRC_9\nrSWRC_10\nrSWRC_11\nrSWRC_12\nrSWRC_13\nrSWRC_14\nrSWRC_15\nrSWRC_16\nrSWRC_17\nrSWRC_18\nrSWRC_19\nrSWRC_20\nrSWRC_21\nrSWRC_22\nrSWRC_23\nrSWRC_24\nrSWRC_25\nrSWRC_26\n\n2000,00\n0,00\nFINANCIAL\n\nFINANCIAL_2 FINANCIAL_3 FINANCIAL_4\ndataset size\n\ndataset size\n\n(a) \u0108 =Client, minsup=0.2\n\n(b) \u0108 =Person, minsup=0.3\n\nrLUBM\n50000,00\n45000,00\n\n40000,00\nruntime(s)\n\n35000,00\n30000,00\n25000,00\n\nSEM\n\n20000,00\n\nNOSEM\n\n15000,00\n\n10000,00\n5000,00\n0,00\nrLUBM\n\nrLUBM_2\n\nrLUBM_3\n\nrLUBM_4\n\ndataset size\n\n(c) \u0108 =Person, minsup=0.3\n\nFig. 5: Results of the experiment, MAXLENGTH=4.\n\nempirical proof of this claim we provide the experimental results in the following\nparagraph.\nInfluence of the size of the dataset on the effectiveness We measured how our\nmethod scales in terms of the running time with growing instance data. For this\nreason we used the replication of the axioms in the assertional part of the knowledge\nbase for FINANCIAL and rSWRC. Each assertional part of FINANCIAL n and\nrSWRC n was obtained by replicating the original assertional part n times. For\nrLUBM, we used the results of the execution of the generator of synthetic data\n(downloaded from KAON2's testbed). Each assertional part of rLUBM n was generated automatically for the number n of universities.\nIn the experimental results (Figure 5) one can observe that for all datasets, the\nbigger extensional part of the background knowledge, the relatively better performance of our method, SEM, compared to the method, where no semantic tests are\nperfomed during the pattern generation, NOSEM. That is, the overhead needed\nto compute the semantic tests becomes relatively smaller in comparison with the\ntime needed to evaluate more queries on bigger sets of data. Especially interesting\nfor us are the results on the problematic rSWRC dataset. One can observe that\ntogether with the growth of the assertional part of the background knowledge, the\ntime needed to compute the NOSEM setting increases more then the time needed\n\n\fThe role of semantics in mining frequent patterns\n\nFINANCIAL\n\n31\n\nrSWRC\n900\n\n1000\n\n800\nNumber of patterns\n\nNumber of patterns\n\n1000\n1200\n\n800\ngen\n600\n\nsat\nsfree\n\n400\n\ncand\n\n700\n600\ngen\n\n500\n\nsat\n\n400\n\nsfree\n\n300\n\ncand\n\n200\n\n200\n\n100\n\n0\n\n0\n2\n\n3\n\n4\n\n2\n\n3\n\n4\n\nMax length\n\nMax length\n\n(a) \u0108 =Client, minsup=0.2\n\n(b) \u0108 =Person, minsup=0.3\n\nrLUBM\n1600\n\nNumber of patterns\n\n1400\n1200\n1000\ngen\n\n800\n\nsat\n\n600\n\nsfree\n\n400\n\ncand\n\n200\n0\n2\n\n3\n\n4\n\nMax length\n\n(c) \u0108 =Person, minsup=0.3\n\nFig. 6: Results of the experiment on the influence of the knowledge base expressivity\non effectiveness, MAXLENGTH=4.\nto compute the SEM setting. That's why, we perfomed the tests to show that for\nbigger volumes of data, described by the same intensional knowledge, a time needed\nto compute a trie of patterns in NOSEM setting finally reaches and exceeds the\ntime needed to compute the trie in the SEM setting.\n5.1.2 Influence of the expressivity of the dataset on the effectiveness\nIn this experiment, additionally to the parameters measured in the experiment presented in Section 5.1.1, we collected the following information: (i) the number of\ncandidates generated by the syntactic refinement rules (gen), (ii) the number of\nsatisfiable candidate patterns (sat), (iii) the number of semantically free candidate\npatterns (sfree). The goal was to investigate more deeply, how useful are the semantic tests on different types of datasets. Figure 6 shows the experimental results.\nThe most important remark to be made after the analysis of the results is that\nafter the satisfiability test, in case of the FINANCIAL dataset many, while for the\nother two datasets none of the patterns were pruned away. We conclude that it is\ndue to the disjointness constraints present only in the FINANCIAL dataset, but\n\n\f32\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nnot in the other ones. If two concepts are defined to be disjoint then adding an atom\nwhere some variable is described by one of them, while the other one describing the\nsame variable is already present in a query is useless. For example, as concepts Man\nand Woman are defined to be disjoint in the FINANCIAL dataset, then testing\natom Woman(key) as a refinement of the pattern Q(key) = Client(key), Man(key)\nis useless, and such refinement is pruned due to its unsatisfiability. The important property in the context of the presence of disjointness constraints is also\nthe co-occurrence of the role domain and range specifications. For instance, an\natom with a concept describing a variable that is already described by the range\nof some role already present in a query, may be pruned away if the concept in\nthe role range and the given concept are disjoint. For example, in the FINANCIAL dataset, the range of role hasCreditCard is CreditCard . Then, due to the\ndisjointness of concepts CreditCard and Account, a refinement Account(x1 ) of query\nQ(key) = Client(key), hasCreditCard (key, x1 ) is pruned after the satisfiability test.\nFurthermore, after the analysis of the results, we conclude that the features of\nthe intensional part of the knowledge base of the tested datasets, that helped to\nprune patterns after the s-freeness test were: the organization of concepts and roles\nin taxonomies, the specification of domain and ranges of roles, the specification of\nrole properties, such as role inverse, and concept definitions.\nIn the FINANCIAL dataset the hierarchy of roles is flat, the hierarchy of concepts maximum 4 levels deep. In the rSWRC dataset the hierarchy of roles is also\nflat, and the hierarchy of concepts is maximum 5 levels deep. In the rLUBM dataset\nthe hierarchy of roles is almost flat, with two exceptions, 2 and 3 levels deep, the\nhierarchy of concepts is maximum 5 levels deep.\nIn the FINANCIAL dataset all roles have domain and ranges explicitly specified.\nThere are also some axioms defining inverse roles. In the rSWRC dataset domains\nand ranges are nearly not specified explicitly (except with one exception). There\nare, however, restrictions imposed on ranges of some roles while used with particular concepts in a role domain, for example when concept AcademicStaff is used as\na domain of the role headOfGroup the range of the role can only be ResearchGroup:\nAcademicStaff v \u2200headOfGroup.ResearchGroup. There are many axioms specifying inverse roles in rSWRC. In rLUBM dataset domains and ranges of some roles\nare explicitly specified, there are also some inverse role specifications.\nConcept definitions are only present in the rLUBM dataset. The example concept definition of concept Student as a person taking some course is: Student \u2261\nPerson u \u2203takesCourse.Course. Hence, the atom Student(key) as the refinement\nof query Q(key) = Person(key), takesCourse(x1 ), Course(x1 ) is pruned after the\ns-freeness test.\nIn the context of the query equivalency test (when performed after the s-freeness\ntest), the important features of the intensional part of the tested datasets are: specification of role properties, such as role inverse, and concept definitions.\nLet us consider for example the following queries, Q1 and Q2 , tested for semantic\nuniqueness w.r.t. the FINANCIAL dataset:\nQ1 (key) = Client(key), isOwnerOf (key, x1 ), hasPermanentOrder (x1 , x2 )\n\n\fThe role of semantics in mining frequent patterns\n\n33\n\nTable 5: Results of the experiment. FINANCIAL: minsup=0.2, \u0108 =Client,\nrSWRC: minsup=0.3, \u0108 =Person, rLUBM: minsup=0.3, \u0108 =Person\n\nDataset\n\nFINANCIAL\nrSWRC\nrLUBM\n\nMax\nLength\n12\n4\n4\n\nruntime[s]\nSEM\nSEM+TAX\n30821.6\n2533.5\n3486.7\n\n23146.4\n1594.4\n2232.4\n\nspeedup\nSEM/SEM+TAX\n1.33\n1.59\n1.56\n\nQ2 (key) = Client(key), hasOwner (x1 , key), hasPermanentOrder (x1 , x2 )\nSince the role hasOwner is an inverse of the role isOwnerOf , both patterns have\nexactly the same meaning, and one of them is semantically redundant.\nConcluding, from the analysis of the results and of the tested dataset features,\nit follows that the presence or lack of the disjointness constraints in a dataset, is\na crucial feature. It is also desirable that disjointness constraints co-occur together\nwith the specification of role domains and ranges. Disjointness constraints allow\nto prune unsatisfiable patterns before any query answering procedure execution,\neither on (KB , P ) or on a copy of (KB , P ).\nMoreover, the question posed in Section 5.1.1, why the rSWRC dataset is especially hard for our approach, has been indirectly answered by the analysis presented\nin this section. The rSWRC is not very expressive, as it does not contain disjointness constraints, nor explicit role domain and range specifications, any concept\ndefinitions, and its role hierarchy is flat. Hence, the gain that may be achieved by\nusing intensional background knowledge for this dataset cannot be large.\n\n5.1.3 Using taxonomies of concepts and roles in building pattern refinements\nTable 5 presents the results of the experiment on the use of concept and role taxonomies to build pattern refinements in line with hierarchical information from a\nKB (SEM + TAX ) in addition to the setting SEM . For all the datasets speedup\nhas been achieved, growing with the increasing maximum length of patterns.\n\n6 Related work\nWe start the discussion in this section, from the features of knowledge representation\nlanguages admitted by the (onto-)relational frequent pattern mining approaches.\nFurther we discuss how the related approaches exploit the semantics of their admitted representation languages.\nThe relational frequent pattern miners, WARMR, FARMER, and c-armr, are\ndesigned to operate on knowledge bases represented as logic programs (generally\nin a Datalog variant). Thus, by definition, they are not able to use the knowledge\n\n\f34\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nthat has the form of description logic axioms that could not be rewritten into Datalog. Consider the knowledge base from Example 1. In Datalog it is impossible to\nassert that all accounts must have an owner without explicitly specifying who the\nowner is. Let us show how this may affect the properties of patterns generated by\na method. Consider the following pattern (query):\nQ(x ) =? \u2212 Account(x ), Property(x )\nThe second atom of Q may be considered as redundant, as from the knowledge base\nwe already know that every account is a property, and we know also that if something has an owner, than it is a property. Moreover, it is stated that every account\nhas an owner. For example, account2 is a property, even if there is nowhere written\nso, and the owner of account2 is nowhere specified. In Datalog such deduction, in\norder to catch the redundancy, is impossible.\nDL-safe rules formalism allows modelling the rules with disjunctions of atoms\nin their heads. Hence, despite of admitting an additional component of the knowledge base (in description logic), we also extend the language of logic programs used\nby other methods from Datalog to disjunctive Datalog. Summarizing, WARMR,\nFARMER and c-armr operate only on a part of the one of the two components\nassumed in our approach, namely only on the Datalog part of disjunctive Datalog.\nSPADA (in further versions named AL-QuIn) has been the only approach so\nfar to frequent pattern discovery in combined knowledge bases, more specifically\nthe knowledge bases expressed in AL-log (Donini et al. 1998). AL-log is the combination of Datalog with ALC description logic, and hence our approach supports\nmore expressive language SHIF in the description logic component. The early version of SPADA/AL-QuIn admitted only ALC atomic concepts as the structural\nknowledge (i.e., taxonomies) whereas roles and complex concepts have been disregarded. AL-QuIn does not have this restriction. Rules in SPADA/AL-QuIn are\nrepresented as constrained Datalog clauses. In these clauses, only DL concepts can\nbe used (as constraints in the body). While in DL-safe rules using both concepts\nand roles in DL-atoms is allowed and DL-atoms can be used in rule heads as well.\nDL-safe rules are applicable only to explicitly named objects. The fact that atoms\nwith concept predicates can occur only as constraints in the body of AL-log rules\nhas the similar effect.\nThe actual representation considered in the SPADA/AL-QuIn is a special kind\nof Datalog (where description logic concepts serve as constraints in the Datalog\nclauses) (Lisi 2007). In the core of AL-QuIn, description logic axioms are compiled\ninto Datalog ones and appended to Datalog component (for the details see: (Lisi\n2007)). In turn, the DL-safe rules extend expressive description logics with disjunctive clauses. In the core of the reasoning mechanism proposed for DL-safe rules\nthere is an inverse direction: description logic knowledge base is translated into a\ndisjunctive Datalog program and rules are appended to the result of this translation. Note, that the first approach, consisting on computing consequences of the\nDL component first, and then applying the rules to these consequences is incorrect\nin general. Consider the following knowledge base (KB , P ):\n\n\fThe role of semantics in mining frequent patterns\n\n35\n\nHuman(x ) \u2190 Man(x ), O(x )\nHuman(x ) \u2190 Woman(x ), O(x )\n(Man t Woman)(Pat).\nThe assertion of individual Pat to concept (MantWoman) means that Pat is a Man\nor a Woman, but it is not known whether Pat is a man or a woman. Either of the\nrules from (KB , P ) derives that Pat is a Human, hence (KB , P ) |= Human(Pat).\nIt could not be derived by applying these rules to the consequences of KB , since\nKB 6 |=Man(Pat) and KB 6 |=Woman(Pat). Thus, in order to perform the translation from the description logic to Datalog, AL-QuIn has to assure that all the\nconcepts are named, which is not a restriction in our approach.\nEventually, the language used in SPADA/AL-QuIn corresponds to Datalog,\nwhile that used in our approach to the, more expressive, disjunctive Datalog.\nThe relational data mining methods, WARMR and FARMER, use \u03b8-subsumption\nas the generality measure. The \u03b8-subsumption is a syntactic generality relation and\nas such it is not strong enough to capture semantic redundancies. For the knowledge\nbase from Example 1, WARMR may discover queries like the following one:\nQ(x , y, z ) =? \u2212 p familyAccount(x , y, z ), p sharedAccount(x , y, z )\nIn the knowledge base, p familyAccount is defined as a type of p sharedAccount.\nThis makes the second atom of Q, p sharedAccount(x , y, z ), semantically redundant. The \u03b8-subsumption is to weak to use such taxonomic information. Using the\nsyntactic generality measure causes redundancy not only in a single pattern, but\nalso in a set of patterns. Consider, for example, the following queries that would be\nboth discovered by WARMR:\nQ1 (x , y, z ) =? \u2212 p familyAccount(x , y, z )\nQ2 (x , y, z ) =? \u2212 p familyAccount(x , y, z ), p sharedAccount(x , y, z )\nUnder a semantic generality measure they would be equivalent to each other.\nBoth c-armr and SPADA have been conceived to use a semantic generality measure, but SPADA does not fully exploits it in an algorithm for pattern mining\nto avoid generation of semantically redundant patterns. It is not used either to\nprune patterns semantically redundant, due to redundant literals nor to prune semantically equivalent patterns. That is, similar pattern as described above w.r.t.\nWARMR, would be generated by SPADA:\nq(y) \u2190 p woman(y), p familyAccount(x , y, z ), p sharedAccount(x , y, z )&Client(y)\nAlso, there is no solution in SPADA algorithm to check the redundancy using the\nknowledge linking Datalog and description logic component (like the rule defining\np familyAccount). The following clause may be generated:\nq(x ) \u2190 p familyAccount(x , y, z ), p woman(y) & Account(x ), Client(y)\nwhile from the (KB , P ) already follows the constraint Client on variable y. Since\nthe second argument of p familyAccount describes an owner, and being an owner\nimplies being a client, the atom Client(y) is redundant w.r.t. (KB , P ).\n\n\f36\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nIt is also interesting to note here, that in c-armr not only semantically free (sfree), but also semantically closed (s-closed) patterns are generated. Generation\nof semantically closed patterns is based on the assumption that each s-free clause\nhas a unique s-closed clause (s-closure) and several s-free clauses may have the\nsame s-closure. This is valid for Datalog, as s-closures are computed based on the\ncomputation of the least Herbrand models. In the context of our approach, this assumption is not valid anymore. A combined knowledge base (KB , P ) is translated\ninto a disjunctive Datalog program, where there may be possibly many minimal\nmodels. Additionally, if there are transitive roles defined in a knowledge base, in\norder to be s-closed, a query should contain the transitive closure of atoms with\nsuch role. It may cause that a query contains many, possibly not interesting atoms\nand leads to additional computations. Taking into account the abovementioned issues, we decided to generate only s-free queries.\nRelational frequent pattern mining algorithms usually generate patterns according to a specification in a declarative bias. Declarative bias allows to specify the\nset of atom templates describing the atoms to be used in patterns. Common solution is to take one atom template after the other to build the refinements of a\npattern, in order in which the templates are stored in a declarative bias directives.\nSuch solution is adopted in WARMR, FARMER, c-armr. However, the solution\ndoes not make use of a semantic relationships between predicates in atoms, causing\nredundant computations. Consider the patterns:\nQ1 (y) =? \u2212 p woman(y), p sharedAccount(x , y, z )\nQ2 (y) =? \u2212 p woman(y), p familyAccount(x , y, z )\nAssume that pattern Q1 has been found infrequent. Thus, generating pattern Q2\nis useless, since p familyAccount is more specific than p sharedAccount. However,\nc-armr would generate and test both queries anyway. If some taxonomic information was used to systematically generate refinements, the redundant computation\ncould be avoided. In SPADA, taxonomic information is used only with regard to\nthe concept hierarchies. That is, patterns are refined by replacing more general\nconcepts by more specific ones in the constraints of the constrained Datalog clause.\nAny technique using taxonomic information is not reported with regard to the Datalog predicates. It means, the same scheme of refining patterns, described above, is\napplied in SPADA/AL-QuIn, too.\nFinally, in Table 6, we provide the comparison of the semantic features of the\napproaches to (onto-)relational frequent pattern mining. In the last row we provide\nthe features of our approach, SEMINTEC.\nWith regard to the languages used, all of the approaches are able to operate\non a relational component. Only SPADA/AL-QuIn and our proposed approach,\nSEMINTEC, are the systems designed to take a description logic component into\naccount. Moreover, SPADA/AL-QuIn is only able to use concepts in patterns,\nbut it is not able to use any roles. Additionally, only the representation used in\nSEMINTEC allows disjunctions of atoms in rule heads. With regard to the features of the algorithms, only c-armr, SPADA/AL-QuIn and SEMINTEC use the\n\n\fThe role of semantics in mining frequent patterns\n\n37\n\nTable 6: Semantic features of (onto-)relational frequent pattern mining methods.\n\nx\n\nx\n\nx\n\nx\n\ntaxonomies\ndirectly used in\nrefining patterns\n\nonly one pattern\nfrom each equivalence class\n\nx\n\nx\nx\nx\n\nsemantically free\n(non-redundant)\npatterns\n\nsemantic\ngenerality\nmeasure\n\nx\nx\n\nx\nx\nx\nx\nx\n\nMethod\n\ndisjunctive rules\n\nDatalog\ncomponent\n\nWARMR\nFARMER\nc-armr\nSPADA/AL-QuIn\nSEMINTEC\n\nDL component\n\nKnowledge representation\n\nx\nx\n\nsemantic generality measure, with the consequences described earlier in this section.\nOnly c-armr and SEMINTEC apply a technique to prune semantically redundant\nliterals from patterns, by checking s-freeness property. Summarizing, SEMINTEC\nis the only approach having all the features presented in Table 6.\n7 Conclusions and future work\nIn this paper we have proposed a new method for frequent pattern discovery from\nknowledge bases represented in a combination of SHIF description logic and DLsafe rules. We have focused on the relation of the semantics of the representation\nformalism to the task of frequent pattern discovery, as this is a key aspect to the\ndesign of (onto-)relational frequent pattern discovery methods. For the core of our\nmethod we have proposed an algorithm that applies techniques that exploit the\nsemantics of the combined knowledge base.\nWe have developed a proof-of-concept implementation of this method using the\nstate-of-the-art reasoning techniques. We have empirically shown that using the\nintensional part of the combined knowledge to perform semantic tests on candidate\npatterns can make data mining faster. This is because the semantic tests help to\nprune useless patterns before their evaluation, and they help to avoid the futile\nsearch of large parts of the pattern space. We have also shown that exploiting the\nsemantics of a knowledge base can improve the quality of the set of patterns produced: the patterns are more compact through the removal of redundant atoms,\nand more importantly, there are fewer patterns, as only one pattern is produced\nfrom each semantic equivalence class.\nThe primary motivation for our work is the real-world need of the Semantic Web\nfor data-mining methods. For example large amounts of biological data are now being represented using descriptions logics and rules, and there is a scientific need to\nfind frequent patterns in this data. Our method is a baseline for future work in this\narea that may be twofold. Firstly, after careful investigation of a particular needs\n\n\f38\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nof prominent application domains, the scope of the method may be extended by\nconsidering more expressive languages falling into the framework of DL-safe rules.\nSecondly, we plan to develop optimization techniques and heuristic algorithms for\nwhich the proposed method (complete w.r.t. to the pattern space search) would be\na point of reference.\nAcknowledgements. The authors acknowledge the support of the Polish Ministry of\nScience and Higher Education (grant number N N516 186437). We are grateful to Boris\nMotik for explanations on DL-safe rules formalism, and for providing KAON2 reasoner,\nto Prof. Ross D. King for valuable discussions on KR and data mining in biology, and the\nremarks on earlier versions of the paper, to Jan Ramon for explanations on the concept\nof the semantic freeness of Datalog patterns. We are also very grateful to the anonymous\nreviewers for all their comments.\n\nReferences\nBaader, F., Calvanese, D., McGuinness, D., Nardi, D., and Patel-Schneider, P.,\nEds. 2003. The description logic handbook: Theory, implementation and applications.\nCambridge University Press, Cambridge.\nBerners-Lee, T., Hendler, J., and Lassila, O. 2001. The Semantic Web. Scientific\nAmerican 284, 5, 34\u201343.\nCal\u0131\u0300, A., Gottlob, G., and Lukasiewicz, T. 2009. A general Datalog-based framework\nfor tractable query answering over ontologies. In PODS, J. Paredaens and J. Su, Eds.\nACM, 77\u201386.\nCalvanese, D., Giacomo, G., Lembo, D., Lenzerini, M., and Rosati, R. 2007.\nTractable reasoning and efficient query answering in description logics: The DL-Lite\nfamily. J. Autom. Reason. 39, 3, 385\u2013429.\nCumbo, C., Faber, W., Greco, G., and Leone, N. 2004. Enhancing the magic-set\nmethod for disjunctive Datalog programs. In Proc. of the 20th International Conference\non Logic Programming (ICLP'04). 371\u2013385.\nd'Amato, C., Staab, S., and Fanizzi, N. 2008. On the influence of description logics\nontologies on conceptual similarity. In EKAW, A. Gangemi and J. Euzenat, Eds. Lecture\nNotes in Computer Science, vol. 5268. Springer, 48\u201363.\nde Raedt, L. and Ramon, J. 2004. Condensed representations for inductive logic programming. In Proc. of the Ninth International Conference on Principles of Knowledge\nRepresentation and Reasoning (KR 2004). 438\u2013446.\nDehaspe, L. and Toivonen, H. 1999. Discovery of frequent Datalog patterns. Data\nMining and Knowledge Discovery 3, 1, 7\u201336.\nDehaspe, L., Toivonen, H., and King, R. D. 1998. Finding frequent substructures in\nchemical compounds. In Proc. of the Fourth International Conference on Knowledge\nDiscovery and Data Mining (KDD'1998). AAAI Press, 30\u201336.\nDonini, F., Lenzerini, M., Nardi, D., and Schaerf, A. 1998. AL-log: Integrating\nDatalog and description logics. Journal of Intelligent Information Systems 10, 3, 227\u2013\n252.\nDzeroski, S. and Lavrac, N., Eds. 2001. Relational Data Mining. Springer.\nEiter, T., Gottlob, G., and Mannila, H. 1997. Disjunctive Datalog. ACM Transactions on Database Systems 22, 3, 364\u2013418.\n\n\fThe role of semantics in mining frequent patterns\n\n39\n\nEiter, T., Ianni, G., Lukasiewicz, T., Schindlauer, R., and Tompits, H. 2008.\nCombining answer set programming with description logics for the Semantic Web. Artif.\nIntell. 172, 12-13, 1495\u20131539.\nEiter, T., Lukasiewicz, T., Schindlauer, R., and Tompits, H. 2004a. Combining\nanswer set programming with description logics for the Semantic Web. In Proc. of\nthe International Conference of Knowledge Representation and Reasoning (KR 2004).\n141\u2013151.\nEiter, T., Lukasiewicz, T., Schindlauer, R., and Tompits, H. 2004b. Well-founded\nsemantics for description logic programs in the Semantic Web. In Proceedings of the\n3rd International Workshop on Rules and Rule Markup Languages for the Semantic\nWeb (RuleML-2004). 81\u201397.\nFanizzi, N. and d'Amato, C. 2006. A declarative kernel for ALC concept descriptions. In\nFoundations of Intelligent Systems, 16th International Symposium,, F. Esposito, Z. W.\nRas, D. Malerba, and G. Semeraro, Eds. Lecture Notes in Computer Science, vol. 4203.\nSpringer, Berlin\u2013Heidelberg, Germany, 322\u2013331.\nFanizzi, N., D'Amato, C., and Esposito, F. 2008. Statistical learning for inductive\nquery answering on OWL ontologies. In ISWC'08: Proceedings of the 7th International\nConference on The Semantic Web. Springer-Verlag, Berlin, Heidelberg, 195\u2013212.\nGrosof, B. N., Horrocks, I., Volz, R., and Decker, S. 2003. Description logic\nprograms: Combining logic programs with description logic. In Proc. of the 12th Int.\nWorld Wide Web Conference (WWW 2003). ACM Press, 48\u201357.\nHorrocks, I., Patel-Schneider, P., Boley, H., Tabet, S., Grosof, B., and Dean,\nM. 2004. SWRL: A Semantic Web rule language combining OWL and RuleML. W3C\nMember Submission. Available at http://www.w3.org/Submission/SWRL/.\nHustadt, U., Motik, B., and Sattler, U. 2004. Reducing SHIQ\u2212 description logic\nto disjunctive Datalog programs. In Proc. of the 9th Int. Conf. on the Principles of\nKnowledge Representation and Reasoning (KR 2004). AAAI Press, 152\u2013162.\nHustadt, U., Motik, B., and Sattler, U. 2005. Data complexity of reasoning in very\nexpressive description logics. In Proc. of IJCAI 2005. 466\u2013471.\nHustadt, U., Motik, B., and Sattler, U. 2007. Reasoning in description logics by a\nreduction to disjunctive Datalog. Journal of Automated Reasoning 39, 3, 351\u2013384.\nJ\u00f3zefowska, J., Lawrynowicz, A., and Lukaszewski, T. 2005. Towards discovery of\nfrequent patterns in description logics with rules. In Proc. of International Conference\non Rules and Rule Markup Languages for the Semantic Web (RuleML 2005). Vol. 3791\nof LNCS. Springer, 84\u201397.\nJ\u00f3zefowska, J., Lawrynowicz, A., and Lukaszewski, T. 2006. Frequent pattern discovery in OWL DLP knowledge bases. In Managing Knowledge in a World of Networks,\nProc. of EKAW 2006. Vol. 4248 of LNAI. Springer, 287\u2013302.\nJ\u00f3zefowska, J., Lawrynowicz, A., and Lukaszewski, T. 2008. On reducing redundancy in mining relational association rules from the Semantic Web. In Proc. of the\nSecond International Conference on Web Reasoning and Rule Systems (RR'2008). Vol.\n5341 of LNCS. Springer, 205\u2013213.\nKing, R. D., Karwath, A., Clare, A., and Dehaspe, L. 2000a. Accurate prediction\nof protein class in the M. tuberculosis and E. coli genomes using data mining. Yeast\n(Comparative and Functional Genomics) 17, 4, 283\u2013293.\nKing, R. D., Karwath, A., Clare, A., and Dehaspe, L. 2000b. Genome scale prediction of protein functional class from sequence using data mining. In Proc. of the\nSixth ACM SIGKDD international conference on Knowledge discovery and data mining (KDD'2000). 384\u2013389.\nKing, R. D., Karwath, A., Clare, A., and Dehaspe, L. 2001. The utility of different\n\n\f40\n\nJ. J\u00f3zefowska and A. Lawrynowicz and T. Lukaszewski\n\nrepresentations of protein sequence for predicting functional class. Bioinformatics 17, 5,\n445\u2013454.\nLevy, A. and Rousset, M.-C. 1998. Combining Horn rules and description logics in\nCARIN. Artificial Intelligence 104, 1-2, 165\u2013209.\nLisi, F. 2007. Reasoning with OWL-DL in inductive logic programming. In Proc. of the\nThird International Workshop, OWL: Experiences and Directions (OWLED 2007).\nLisi, F. and Malerba, D. 2004. Inducing multi-level association rules from multiple\nrelations. Machine Learning Journal 55, 2, 175\u2013210.\nLisi, F. A. and Esposito, F. 2008. Foundations of onto-relational learning. In ILP,\nF. Zelezn\u00fd and N. Lavrac, Eds. Lecture Notes in Computer Science, vol. 5194. Springer,\n158\u2013175.\nLukasiewicz, T. 2007. A novel combination of answer set programming with description\nlogics for the Semantic Web. In ESWC, E. Franconi, M. Kifer, and W. May, Eds.\nLecture Notes in Computer Science, vol. 4519. Springer, 384\u2013398.\nMcGuinness, D. and van Harmelen, F. 2004. OWL Web ontology language overview.\nW3C Recommendation. Available at http://www.w3.org/TR/owl-features/.\nMotik, B. 2006. Reasoning in description logics using resolution and deductive databases.\nPh.D. thesis, Universitaet Karlsruhe (TH), Karlsruhe, Germany.\nMotik, B. and Rosati, R. 2007. A faithful integration of description logics with logic\nprogramming. In Proc. of the 20th Int. Joint Conference on Artificial Intelligence (IJCAI\n2007). 477\u2013482.\nMotik, B. and Sattler, U. 2006. A comparison of reasoning techniques for querying\nlarge description logic ABoxes. In Proc. of the 13th International Conference on Logic\nfor Programming Artificial Intelligence and Reasoning (LPAR 2006).\nMotik, B., Sattler, U., and Studer, R. 2005. Query answering for OWL-DL with\nrules. Journal of Web Semantics: Science, Services and Agents on the World Wide\nWeb 3, 1, 41\u201360.\nNienhuys-Cheng, S. and de Wolf, R. 1997. Foundations of inductive logic programming. Vol. 1228 of LNAI. Springer.\nNijssen, S. and Kok, J. 2001. Faster association rules for multiple relations. In Proc. of\nthe 17th Int. Joint Conference on Artificial Intelligence (IJCAI'2001). 891\u2013897.\nNijssen, S. and Kok, J. 2003. Efficient frequent query discovery in FARMER. In Proc.\nof the 7th European Conference on Principles and Practice of Knowledge Discovery in\nDatabases (PKDD 2003). Vol. 2431 of LNAI. 350\u2013362.\nParsia, B., Kolovski, V., and Sirin, E. 2006. Extending the SHOIQ(D) tableaux\nwith DL-safe rules: First results. In Proc. of the Int. Description Logics Workshop (DL\n2006).\nRosati, R. 2006. DL+log: Tight integration of description logics and disjunctive Datalog.\nIn Proc. of KR 2006. 68\u201378.\nRuttenberg, A., Rees, J., and Luciano, J. 2005. Experience using OWL DL for the\nexchange of biological pathway information. In Proc. of OWL-ED 05. Vol. 188 of CEUR.\nStevens, R., Aranguren, M. E., Wolstencroft, K., Sattler, U., Drummond, N.,\nHorridge, M., and Rector, A. L. 2007. Using OWL to model biological knowledge.\nInternational Journal of Man-Machine Studies 65, 7, 583\u2013594.\n\n\f"}