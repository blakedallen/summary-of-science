{"id": "http://arxiv.org/abs/1204.6382v3", "guidislink": true, "updated": "2013-02-14T18:06:54Z", "updated_parsed": [2013, 2, 14, 18, 6, 54, 3, 45, 0], "published": "2012-04-28T08:03:17Z", "published_parsed": [2012, 4, 28, 8, 3, 17, 5, 119, 0], "title": "Uniform convergence and asymptotic confidence bands for model-assisted\n  estimators of the mean of sampled functional data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.5131%2C1204.3185%2C1204.5245%2C1204.3055%2C1204.1305%2C1204.5317%2C1204.0793%2C1204.1318%2C1204.3069%2C1204.4566%2C1204.1936%2C1204.3942%2C1204.1016%2C1204.0494%2C1204.6382%2C1204.3006%2C1204.2101%2C1204.0649%2C1204.4116%2C1204.5984%2C1204.2916%2C1204.5017%2C1204.2457%2C1204.2500%2C1204.5789%2C1204.1872%2C1204.3418%2C1204.4258%2C1204.2864%2C1204.3495%2C1204.0192%2C1204.4736%2C1204.4683%2C1204.6515%2C1204.2039%2C1204.4318%2C1204.4699%2C1204.0650%2C1204.5172%2C1204.5636%2C1204.0169%2C1204.6085%2C1204.1790%2C1204.5637%2C1204.6201%2C1204.3054%2C1204.3053%2C1204.2063%2C1204.2784%2C1204.4131%2C1204.4034%2C1204.4922%2C1204.6209%2C1204.6046%2C1204.2583%2C1204.4679%2C1204.6157%2C1204.1941%2C1204.2145%2C1204.1780%2C1204.4243%2C1204.4165%2C1204.3002%2C1204.5658%2C1204.3578%2C1204.6394%2C1204.3190%2C1204.0287%2C1204.4774%2C1204.3188%2C1204.6151%2C1204.5258%2C1204.3222%2C1204.2170%2C1204.3952%2C1204.3899%2C1204.5023%2C1204.0549%2C1204.5473%2C1204.0957%2C1204.1217%2C1204.4909%2C1204.3650%2C1204.3946%2C1204.4452%2C1204.1015%2C1204.6667%2C1204.5661%2C1204.2580%2C1204.1719%2C1204.2711%2C1204.5920%2C1204.5383%2C1204.0195%2C1204.4929%2C1204.3096%2C1204.4306%2C1204.3480%2C1204.2713%2C1204.1491%2C1204.2327&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Uniform convergence and asymptotic confidence bands for model-assisted\n  estimators of the mean of sampled functional data"}, "summary": "When the study variable is functional and storage capacities are limited or\ntransmission costs are high, selecting with survey sampling techniques a small\nfraction of the observations is an interesting alternative to signal\ncompression techniques, particularly when the goal is the estimation of simple\nquantities such as means or totals. We extend, in this functional framework,\nmodel-assisted estimators with linear regression models that can take account\nof auxiliary variables whose totals over the population are known. We first\nshow, under weak hypotheses on the sampling design and the regularity of the\ntrajectories, that the estimator of the mean function as well as its variance\nestimator are uniformly consistent. Then, under additional assumptions, we\nprove a functional central limit theorem and we assess rigorously a fast\ntechnique based on simulations of Gaussian processes which is employed to build\nasymptotic confidence bands. The accuracy of the variance function estimator is\nevaluated on a real dataset of sampled electricity consumption curves measured\nevery half an hour over a period of one week.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.5131%2C1204.3185%2C1204.5245%2C1204.3055%2C1204.1305%2C1204.5317%2C1204.0793%2C1204.1318%2C1204.3069%2C1204.4566%2C1204.1936%2C1204.3942%2C1204.1016%2C1204.0494%2C1204.6382%2C1204.3006%2C1204.2101%2C1204.0649%2C1204.4116%2C1204.5984%2C1204.2916%2C1204.5017%2C1204.2457%2C1204.2500%2C1204.5789%2C1204.1872%2C1204.3418%2C1204.4258%2C1204.2864%2C1204.3495%2C1204.0192%2C1204.4736%2C1204.4683%2C1204.6515%2C1204.2039%2C1204.4318%2C1204.4699%2C1204.0650%2C1204.5172%2C1204.5636%2C1204.0169%2C1204.6085%2C1204.1790%2C1204.5637%2C1204.6201%2C1204.3054%2C1204.3053%2C1204.2063%2C1204.2784%2C1204.4131%2C1204.4034%2C1204.4922%2C1204.6209%2C1204.6046%2C1204.2583%2C1204.4679%2C1204.6157%2C1204.1941%2C1204.2145%2C1204.1780%2C1204.4243%2C1204.4165%2C1204.3002%2C1204.5658%2C1204.3578%2C1204.6394%2C1204.3190%2C1204.0287%2C1204.4774%2C1204.3188%2C1204.6151%2C1204.5258%2C1204.3222%2C1204.2170%2C1204.3952%2C1204.3899%2C1204.5023%2C1204.0549%2C1204.5473%2C1204.0957%2C1204.1217%2C1204.4909%2C1204.3650%2C1204.3946%2C1204.4452%2C1204.1015%2C1204.6667%2C1204.5661%2C1204.2580%2C1204.1719%2C1204.2711%2C1204.5920%2C1204.5383%2C1204.0195%2C1204.4929%2C1204.3096%2C1204.4306%2C1204.3480%2C1204.2713%2C1204.1491%2C1204.2327&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "When the study variable is functional and storage capacities are limited or\ntransmission costs are high, selecting with survey sampling techniques a small\nfraction of the observations is an interesting alternative to signal\ncompression techniques, particularly when the goal is the estimation of simple\nquantities such as means or totals. We extend, in this functional framework,\nmodel-assisted estimators with linear regression models that can take account\nof auxiliary variables whose totals over the population are known. We first\nshow, under weak hypotheses on the sampling design and the regularity of the\ntrajectories, that the estimator of the mean function as well as its variance\nestimator are uniformly consistent. Then, under additional assumptions, we\nprove a functional central limit theorem and we assess rigorously a fast\ntechnique based on simulations of Gaussian processes which is employed to build\nasymptotic confidence bands. The accuracy of the variance function estimator is\nevaluated on a real dataset of sampled electricity consumption curves measured\nevery half an hour over a period of one week."}, "authors": ["Herv\u00e9 Cardot", "Camelia Goga", "Pauline Lardin"], "author_detail": {"name": "Pauline Lardin"}, "author": "Pauline Lardin", "arxiv_comment": "To appear in the Electronic J. of Statistics", "links": [{"href": "http://arxiv.org/abs/1204.6382v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.6382v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.6382v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.6382v3", "journal_reference": null, "doi": null, "fulltext": "arXiv:1204.6382v3 [math.ST] 14 Feb 2013\n\nElectronic Journal of Statistics\nISSN: 1935-7524\n\nUniform convergence and asymptotic\nconfidence bands for model-assisted\nestimators of the mean of sampled\nfunctional data\nHerv\u00e9 Cardot, and Camelia Goga\nUMR 5584 CNRS Institut de Math\u00e9matiques de Bourgogne,\nUniversit\u00e9 de Bourgogne, UFR Sciences et Techniques,\n9 avenue Alain Savary \u2013 BP 47870, 21078 Dijon Cedex, France.\ne-mail:\nherve.cardot(AT)u-bourgogne.fr; camelia.goga(AT)u-bourgogne.fr; pauline.lardin(AT)laposte.fr\n\nPauline Lardin\nEDF, R&D, ICAME-SOAD, 1 av. du G\u00e9n\u00e9ral de Gaulle,\n92141 Clamart, FRANCE\ne-mail: pauline.lardin(AT)laposte.fr\nAbstract\nWhen the study variable is functional and storage capacities are limited\nor transmission costs are high, selecting with survey sampling techniques a\nsmall fraction of the observations is an interesting alternative to signal compression techniques, particularly when the goal is the estimation of simple\nquantities such as means or totals. We extend, in this functional framework,\nmodel-assisted estimators with linear regression models that can take account of auxiliary variables whose totals over the population are known.\nWe first show, under weak hypotheses on the sampling design and the regularity of the trajectories, that the estimator of the mean function as well\nas its variance estimator are uniformly consistent. Then, under additional\nassumptions, we prove a functional central limit theorem and we assess rigorously a fast technique based on simulations of Gaussian processes which\nis employed to build asymptotic confidence bands. The accuracy of the variance function estimator is evaluated on a real dataset of sampled electricity\nconsumption curves measured every half an hour over a period of one week.\nAMS 2000 subject classifications: Primary 62L20; secondary 60F05.\nKeywords and phrases: Calibration, covariance function, functional linear model, GREG, H\u00e1jek estimator, Horvitz-Thompson estimator, linear\ninterpolation, survey sampling.\n\n1. Introduction\nSurvey sampling techniques, which consist in randomly selecting only a part of\nthe elements of a population, are interesting alternatives to signal compression\nwhen one has to deal with very large populations of quantities that evolve along\ntime. With the development of automatic sensors such very large datasets of\ntemporal data are not unusual anymore and survey sampling techniques offer\n1\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n2\n\na good trade-off between accuracy of the estimators and size of the analyzed\ndata. Examples can be found in different domains such as internet traffic monitoring (see Callado et al. (2009)) or estimation of energy consumption measured\nby individual smart meters. Motivated by the estimation of mean consumption electricity profiles measured every half an hour over one week, Cardot and\nJosserand (2011) have introduced Horvitz-Thompson estimators of the mean\nfunction and have shown, under weak hypotheses on the regularity of the functional trajectories and the sampling design, that one gets uniformly convergent\nestimators. They also prove a functional central limit theorem, in the space of\ncontinuous functions, that can, in part, justify the construction of asymptotic\nconfidence bands. More recently, Cardot et al. (2012b) made a comparison, in\nterms of precision of the mean estimators of electricity load curves and width of\nthe confidence bands, of different sampling approaches that can take auxiliary\ninformation into account. One of the conclusions of this empirical study was\nthat very simple strategies based on simple sampling designs (such as simple\nrandom sampling without replacement) could be improved much if some well\nchosen auxiliary information, whose total is known for the whole population,\nis also taken into account at the estimation stage, with model-assisted estimators. Important variables for the electricity consumption such as temperature\nor geographical location were not available for these datasets so that only one\nauxiliary information, the mean past consumption over the previous period, was\ntaken into account. Its correlation with the current consumption is always very\nhigh (see Figure 1) so that linear regression models are natural candidates for\nassisting the Horvitz-Thompson estimator. More generally, one advantage of\nlinear approaches is that they only require the knowledge of the auxiliary variable totals in the population. More sophisticated nonlinear or nonparametric\napproaches would have required to know the values of the auxiliary variables\nfor all the elements of the population.\nThus, we focus in this paper on linear relationships between the set of auxiliary variables and the response at each instant t of the current period. The\nregression coefficients vary in time (see Faraway (1997) or Ramsay and Silverman (2005)) so that the model-assisted estimator can be seen as a direct\nextension, to a functional or varying-time context, of the generalized regression\n(GREG) estimators studied in Robinson and S\u00e4rndal (1983) and S\u00e4rndal et al.\n(1992). Note also that from another point of view, the model-assisted estimator\ncan be obtained using a calibration technique (Deville and S\u00e4rndal (1992)).\nConfidence bands are then built using a simulation technique developed in\nFaraway (1997), Cuevas et al. (2006) and Degras (2011). We first estimate the\ncovariance function of the mean estimator and then, assuming asymptotic normality, perform simulations of a centered Gaussian process whose covariance\nfunction is the covariance function estimated at the previous step. We can, this\nway, obtain an approximation to the law of the \"sup\" and deduce confidence\nbands for the mean trajectory. In a recent work, Cardot et al. (2012a) have given\na rigorous mathematical justification of this technique for sampled functional\ndata and Horvitz-Thompson estimators for the mean. The required theoretical\ningredients that can justify such a procedure are the functional central limit\n\n\f3\n\n0.88\n0.86\n0.82\n\n0.84\n\nCorrelation\n\n0.90\n\n0.92\n\nH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n0\n\n50\n\n100\n\n150\n\nHours\n\nFigure 1. Correlation between the current consumption at each instant t of the week under\nstudy and the total past consumption of the week before.\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n4\n\ntheorem for the mean estimator, in the space of continuous functions equipped\nwith the sup-norm, as well as a uniformly consistent estimator of the variance\nfunction.\nThe aim of this paper is to study the asymptotic properties of model-assisted\nestimators and to show that we obtain, under classical assumptions, a uniformly\nconsistent estimator of the mean as well as of its variance function. One additional difficulty is that, for model-assisted estimators, the variance function cannot be derived exactly and we can only have asymptotic approximations. Then,\nwe deduce that the confidence bands built via simulations have asymptotically\nthe desired coverage. In Section 2, we introduce notations and we suggest a\nslight modification of the model-assisted estimators which permits control of\nthe variance of the regression coefficient estimator. Under classical assumptions\non the sampling design and on the regularity of the trajectories, we state, in\nSection 3, the uniform convergence of the model assisted-estimators to the mean\nfunction. Under additional assumptions on the design we also prove that we can\nget a consistent estimator of the covariance function and a functional central\nlimit theorem that can justify rigorously that the confidence bands built with\nthe procedure based on Gaussian process simulations attain asymptotically the\ndesired level of confidence. In Section 4, we assess the precision of the variance\nestimator on the real dataset consisting of electricity consumption curves studied in Cardot et al. (2012b) and observe that, in our context, the approximation\nerror is negligible compared to the sampling error. A brief discussion about possible extensions and future investigation is proposed in Section 5. All the proofs\nare gathered in an Appendix.\n2. Notations and estimators\n2.1. The Horvitz Thompson estimator for functional data\nLet us consider a finite population UN = {1, ..., N } of size N supposed to be\nknown, and suppose that, for each unit k of the population UN , we can observe a\ndeterministic curve Yk = (Yk (t))t\u2208[0,T ] . The target is the mean trajectory \u03bcN (t),\nt \u2208 [0, T ], defined as follows:\n\u03bcN (t) =\n\n1 X\nYk (t).\nN\n\n(1)\n\nk\u2208U\n\nWe consider a sample s, with size n, drawn from UN according to a fixed-size\nsampling design pN (s), where pN (s) is the probability of drawing the sample\ns. For simplicity of notations, the subscript N is omitted when there is no\nambiguity. We suppose that the first and second order inclusion probabilities\nsatisfy \u03c0k = P(k \u2208 s) > 0, for all k \u2208 U , and \u03c0kl = P(k&l \u2208 s) > 0 for all\nk, l \u2208 UN , k 6= l. Without auxiliary information, the population mean curve\n\u03bc(t) is often estimated by the Horvitz-Thompson estimator, defined as follows\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n5\n\nfor t \u2208 [0, T ],\n\u03bc\nb(t) =\n\n1 X Yk (t)\n1 X Yk (t)\n=\n1k ,\nN\n\u03c0k\nN\n\u03c0k\nk\u2208s\n\n(2)\n\nk\u2208U\n\nwhere 1k is the sample membership indicator, 1k = 1 if k \u2208 s and 1k = 0\notherwise. For each t \u2208 [0, T ], the estimator \u03bc\nb(t) is design-unbiased for \u03bc(t), i.e.\nEp (b\n\u03bc(t)) = \u03bc(t), where Ep [.] denotes expectation with respect to the sampling\ndesign.\nThe Horvitz-Thompson covariance function of \u03bc\nb between two instants r and\nt, computed with respect to the sampling design, is defined as follows\nCovp (b\n\u03bc(r), \u03bc\nb(t)) =\n\nYk (r) Yl (t)\n1 XX\n(\u03c0kl \u2212 \u03c0k \u03c0l )\n*\nN2\n\u03c0k\n\u03c0l\n\nr, t \u2208 [0, T ].\n\n(3)\n\nk\u2208U l\u2208U\n\nNote that for r = t, we obtain the Horvitz-Thompson variance function.\n2.2. The mean curve estimator assisted by a functional linear model\nLet us suppose now that for each unit k \u2208 U we can also observe p real variables,\nX1 , ..., Xp , and let us denote by xk = (xk1 , ..., xkp )0 , the value of the auxiliary\nvariable vector for each unit k in the population. We introduce an estimator\nbased on a linear regression model that can use these variables in order to\nimprove the accuracy of \u03bc\nb. By analogy to the real case (see e.g. S\u00e4rndal et al.\n(1992)) we suppose that the relationship between the functional variable of\ninterest and the auxiliary variables is modeled by the superpopulation model \u03be\ndefined as follows:\n\u03be:\n\nYk (t) = x0k \u03b2(t) + \u000fkt ,\n\nt \u2208 [0, T ]\n\n(4)\n\nwhere \u03b2(t) = (\u03b21 (t), . . . , \u03b2p (t))0 is the vector of functional regression coefficients, \u000fkt are independent (across units) and centered continuous time processes, E\u03be (\u000fkt ) = 0, with covariance function Cov\u03be (\u000fkt , \u000fkr ) = \u0393(t, r), for (t, r) \u2208\n[0, T ] \u00d7 [0, T ]. This model is a direct extension to several variables of the functional linear model proposed by Faraway (1997).\nP\nIf xk and Yk are known for all units k \u2208 U and if the matrix G = N1 k\u2208U xk x0k\nis invertible,\nit is possible, under the model \u03be, to estimate \u03b2(t) by \u03b2\u0303(t) =\nP\nG\u22121 N1 k\u2208U xk Yk (t), the ordinary least squares estimator. Then, the mean\ncurve \u03bc(t) can be estimated by the generalized difference estimator (see S\u00e4rndal\net al. (1992), Chapter 6) defined as follows for all t \u2208 [0, T ],\n\u03bc\u0303(t) =\n\n1 X x0k \u03b2\u0303(t) \u2212 Yk (t)\n1 X 0\nxk \u03b2\u0303(t) \u2212\nN\nN\n\u03c0k\nk\u2208U\n\n=\n\nk\u2208s\n\n1 X\n1 X \u1ef8k (t) \u2212 Yk (t)\n\u1ef8k (t) \u2212\n,\nN\nN\n\u03c0k\nk\u2208U\n\nk\u2208s\n\n(5)\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n6\n\nwhere \u1ef8k (t) = x0k \u03b2\u0303(t).\nIn practice, we do not know Yk except for k \u2208 s, and it is not possible to\ncompute \u03b2\u0303(t). An estimator of \u03bc(t) is obtained by substituting each total in\nxk x0k\nb = 1 P\n\u03b2\u0303(t) by its Horvitz-Thompson estimator. Thus, if the matrix G\nk\u2208s \u03c0k\nN\nis invertible, \u03b2\u0303(t) is estimated by:\nX xk Yk (t)\nb =G\nb \u22121 1\n\u03b2(t)\n,\nN\n\u03c0k\n\nt \u2208 [0, T ].\n\nk\u2208s\n\nRemark that the denominator N is used in the expression of \u03b2\u0303(t) for asymptotic\npurposes and need not be estimated. The model-assisted estimator \u03bc\nbM A (t) is\nb\nthen defined by replacing \u03b2\u0303(t) by \u03b2(t)\nin (5),\n\u03bc\nbM A (t) =\n\n1 X Ybk (t) \u2212 Yk (t)\n1 Xb\nYk (t) \u2212\n,\nN\nN\n\u03c0k\nk\u2208U\n\nt \u2208 [0, T ],\n\n(6)\n\nk\u2208s\n\n\u00010\nP\nP\nb\nb\nwhere Ybk (t) = x0k \u03b2(t).\nSince k\u2208U Ybk (t) =\nk\u2208U xk \u03b2(t), the only required\ninformation to build \u03bc\nbM A (t) is xk and Yk (t) forPall the units k \u2208 s as well as\nthe population totals of the auxiliary variables, k\u2208U xk .\nRemark 1. If the vector of auxiliary information contains the intercept (constant term), then it can be shown (see S\u00e4rndal (1980)) that the Horvitz-Thompson\nestimator of the estimated residuals Ybk (t) \u2212 Yk (t) is equal to zero for each\nt \u2208 [0, T ]. This means that the model-assisted estimator \u03bc\nbM A reduces in this\ncase to the mean in the population of the predicted values Ybk ,\n\u03bc\nbM A (t) =\n\n1 Xb\nYk (t),\nN\n\nt \u2208 [0, T ].\n\nk\u2208U\n\nMoreover, if only the intercept term is used, namely Yk (t) = \u03b2(t) + \u03b5kt for all\nk \u2208 U, then the estimator \u03bc\u0302M A is simply the well known H\u00e1jek estimator,\nP\n\u03c0 \u22121 Yk (t)\n\u03bc\nbM A (t) = k\u2208s\nP k \u22121 , t \u2208 [0, T ],\nk\u2208s \u03c0k\nwhich is sometimes preferred to the Horvitz-Thompson estimator (see e.g. S\u00e4rndal et al. (1992), Chapter 5.7).\nRemark 2. Estimator \u03bc\nbM A (t) may also be obtained by using a calibration\napproach (Deville and S\u00e4rndal (1992)) which consists in looking for weights\nwks , k \u2208 s, that are as close as possible, according to some distance, to the sampling weights 1/\u03c0k while estimating exactly the population totals of the auxiliary\ninformation,\nX\nX\nwks xk =\nxk .\nk\u2208s\n\nk\u2208U\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n7\n\nConsidering the chi-square distance leads to the following choice of weights\nwks\n\n1\n=\n\u2212\n\u03c0k\n\nX xl\nl\u2208s\n\nand the calibration estimator\n\u03bc\nbM A (t) defined in (6).\n\n\u03c0l\n\nP\n\ns\n\n!0\n\u2212\n\nX\n\nxl\n\nl\u2208U\n\nX xl x0\n\n!\u22121\n\nl\n\nl\u2208s\n\n\u03c0l\n\nxk\n\u03c0k\n\nwks Yk (t)/N for the mean \u03bc(t) is equal to\n\n2.3. A regularized estimator for asymptotics\nThe construction of the estimator \u03bc\nbM A (t) is based on the assumption that\nb is invertible. To show the uniform convergence, we consider a\nthe matrix G\nb which will permit control of the expected norm of its inverse.\nmodification of G\nSuch a trick has already been used for example in Bosq (2000) and Guillas\nb is a p \u00d7 p symmetric and non negative matrix it is possible to\n(2001). Since G\nwrite it as follows\nb =\nG\n\np\nX\n\n0\n\u03b7j,n vjn vjn\n,\n\nj=1\n\nwhere \u03b7j,n is the j th eigenvalue, \u03b71,n \u2265 * * * \u2265 \u03b7p,n \u2265 0, and vjn is the corresponding orthonormal eigenvector. Let us consider a real number a > 0 and\ndefine the following regularized estimator of G,\nba =\nG\n\np\nX\n\n0\nmax(\u03b7j,n , a) vjn vjn\n.\n\nj=1\n\nb a is always invertible and\nIt is clear that G\nb \u22121 k \u2264 a\u22121 ,\nkG\na\n\n(7)\n\nwhere k.k is the spectral norm for matrices. Furthermore, if \u03b7p,n \u2265 a then\nb =G\nb a . If a > 0 is small enough, we show under standard conditions on the\nG\nmoments of the variables X1 , . . . , Xp and on the first and second order inclusion\nb 6= G\nb a ) = P(\u03b7p,n < a) = O(n\u22121 ) (see Lemma A.1 in the\nprobabilities that P(G\nAppendix).\nConsequently, it is possible to estimate the mean function \u03bcN (t) by the following estimator\n\u03bc\nbM A,a (t) =\n\n1 X Ybk,a (t) \u2212 Yk (t)\n1 Xb\nYk,a (t) \u2212\n,\nN\nN\n\u03c0k\nk\u2208U\n\nk\u2208s\n\nb (t) and \u03b2\nb (t) = G\nb \u22121 1 P\nwhere Ybk,a (t) = x0k \u03b2\na\na\na N\nk\u2208s\n\nxk Yk (t)\n.\n\u03c0k\n\nt \u2208 [0, T ],\n\n(8)\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n8\n\n2.4. Discretized observations\nNote finally that with real data, we do not observe Yk (t) at all instants t in [0, T ]\nbut only for a finite set of D measurement times, 0 = t1 < ... < tD = T . In\nfunctional data analysis, when the noise level is low and the grid of discretization\npoints is fine, it is usual to perform a linear interpolation or to smooth the\ndiscretized trajectories in order to obtain approximations of the trajectories at\nevery instant t \u2208 [0, T ] (see Ramsay and Silverman (2005)).\nIf there are no measurement errors, if the trajectories are regular enough\n(but not necessarily differentiable) and if the grid of discretization points is\ndense enough, Cardot and Josserand (2011) showed that linear interpolation\ncan provide sufficiently accurate approximations to the trajectories so that the\napproximation error can be neglected compared to the sampling error for the\nHorvitz-Thompson estimator. Note also that even if the observations are corrupted by noise, it has been shown by simulations in Cardot et al. (2012a) that\nsmoothing does really improve the accuracy of the Horvitz-Thompson estimator\nonly when the noise level is high.\nThus, for each unit k in the sample s, we build the interpolated trajectory\nYk,d (t) = Yk (ti ) +\n\nYk (ti+1 ) \u2212 Yk (ti )\n(t \u2212 ti ) t \u2208 [ti , ti+1 ]\nti+1 \u2212 ti\n\nb (t) as the estimator of \u03b2(t) based on the discretized observaand we define \u03b2\na,d\ntions as follows\nX\nb (t) = G\nb \u22121 1\nxk Yk,d (t)\n\u03b2\na,d\na\nN\nk\u2208s\n\nb\nb\nb (ti ) + \u03b2 a (ti+1 ) \u2212 \u03b2 a (ti ) (t \u2212 ti ).\n=\u03b2\na\nti+1 \u2212 ti\nTherefore, the estimator of the mean population curve \u03bc(t) based on the\ndiscretized observations is obtained by linear interpolation between \u03bc\nbM A,a (ti )\nand \u03bc\nbM A,a (ti+1 ). For t \u2208 [ti , ti+1 ],\n\u03bc\nbM A,d (t) =\n\n1 Xb\n1 X (Ybk,d (t) \u2212 Yk,d (t))\nYk,d (t) \u2212\nN\nN\n\u03c0k\nk\u2208U\n\n=\u03bc\nbM A,a (ti ) +\n\nk\u2208s\n\n\u03bc\nbM A,a (ti+1 ) \u2212 \u03bc\nbM A,a (ti )\n(t \u2212 ti )\nti+1 \u2212 ti\n\nb (t).\nwhere Ybk,d (t) = x0k \u03b2\na,d\n3. Asymptotic properties under the sampling design\nAll the proofs are postponed in an Appendix.\n\n(9)\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n9\n\n3.1. Assumptions\nTo derive the asymptotic properties under the sampling design p(*) of \u03bc\nbM A,d we\nmust suppose that both the sample size and the population size become large.\nMore precisely, we consider the superpopulation framework introduced by Isaki\nand Fuller (1982) with a sequence of growing and nested populations UN with\nsize N tending to infinity and a sequence of samples sN of size nN drawn\nfrom UN according to the sampling design pN (sN ). The first and second order\ninclusion propabilities are respectively denoted by \u03c0kN and \u03c0klN . For simplicity\nof notations and when there is no ambiguity, we drop the subscript N . To prove\nour asymptotic results we need the following assumptions.\nn\n= \u03c0 \u2208 (0, 1).\nA1. We assume that lim\nN \u2192\u221e N\nA2. We assume that min \u03c0k \u2265 \u03bb > 0, min \u03c0kl \u2265 \u03bb\u2217 > 0 and\nk\u2208U\n\nk6=l\n\nlim sup n max |\u03c0kl \u2212 \u03c0k \u03c0l | < C1 < \u221e\nN \u2192\u221e\n\nk6=l\u2208U\n\nA3. There are two positive constants C2 and C3 and 1 \u2265 \u03b2 > 1/2 such that,\nfor all N and for all (r, t) \u2208 [0, T ] \u00d7 [0, T ],\n1 X\nYk (0)2 < C2\nN\nk\u2208U\n\nand\n\n1 X\n{Yk (t) \u2212 Yk (r)}2 < C3 |t \u2212 r|2\u03b2 .\nN\nk\u2208U\n\nA4. We assume that there is a positive constant C4 such that for all k \u2208 U,\nkxk k2 < C4 .\nA5. We assume that, for N > N0 , the matrix G is invertible and that the\nnumber a chosen before satisfies kG\u22121 k < a\u22121 .\nAssumptions A1 and A2 are classical hypotheses in survey sampling and\ndeal with the first and second order inclusion probabilities. They are satisfied\nfor many usual sampling designs with fixed size (see for example H\u00e1jek (1981),\nRobinson and S\u00e4rndal (1983) and Breidt and Opsomer (2000)).\nAssumption A3 is a minimal regularity condition already required in Cardot\nand Josserand (2011). Even if pointwise consistency, for each fixed value of t,\ncan be proved without any condition on the H\u00f6lder coefficient \u03b2, this regularity\ncondition is necessary to get a uniform convergence result. A counterexample\nis given in Hahn (1977) when \u03b2 \u2264 1/2. More precisely it is shown that the\nsample mean i.i.d copies of a uniformly bounded continuous random function\ndefined on a compact interval may not satisfy the Central Limit Theorem in\nthe space of continuous functions. The hypothesis \u03b2 > 1/2 also implies that the\ntrajectories of the residual processes \u000fkt , see (4), are regular enough (but not\nnecessarily differentiable). Assumption A4 could certainly be weakened at the\nexpense of longer proofs. Assumption A5 means that for all u \u2208 R, with u 6= 0,\nwe have u0 Gu \u2265 au0 u. The same kind of assumption is required in Isaki and\nFuller (1982) to get the pointwise convergence in probability whereas Robinson\nand S\u00e4rndal (1983) introduce a much stronger condition (condition A7 in their\narticle) which directly deals with the mean square convergence of the estimator\nof the vector \u03b2 of regression coefficients.\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n10\n\n3.2. Uniform consistency of \u03bc\u0302M A,d\nWe aim at showing that \u03bc\nbM A,d is uniformly consistent for \u03bc, namely that, for\nall \u03b5 > 0,\n!\nP\n\nsup |b\n\u03bcM A,d (t) \u2212 \u03bc(t)| > \u03b5\n\n\u2192 0,\n\nt\u2208[0,T ]\n\nwhen N tends to infinity. The suitable space for proving the uniform convergence\nis the space of continuous functions on [0, T ], denoted by C[0, T ], equipped with\nits natural distance \u03c1; for two elements f, g \u2208 C[0, T ], the distance between f\nand g is \u03c1(f, g) = supt\u2208[0,T ] |f (t) \u2212 g(t)|. It results that the uniform consistency\nof \u03bc\nbM A,d is simply the convergence in probability of \u03bc\nbM A,d to \u03bc in the space\nC[0, T ]. Remark that with assumption A3 the trajectories Yk are continuous for\nall k \u2208 U, and thus the mean curve \u03bc belongs to C[0, T ] as well as its estimator\n\u03bc\nbM A,d , by construction.\nb (t) towards its\nWe first state the uniform consistency of the estimator \u03b2\na,d\npopulation counterpart \u03b2\u0303(t) under conditions on the number and the repartition\nof discretization points.\nProposition 3.1. Let assumptions (A1)-(A5) hold. If the discretization scheme\nsatisfies maxi\u2208{1,..,DN \u22121} |ti+1 \u2212 ti |2\u03b2 = o(n\u22121 ) then there is a constant C > 0\nsuch that, for all n,\n)\n(\n\u221a\nb\n\u2264 C.\nn Ep\nsup \u03b2 (t) \u2212 \u03b2\u0303(t)\na,d\n\nt\u2208[0,T ]\n\nWe can now state a similar type of result for the estimator of the mean function.\nProposition 3.2. Let assumptions (A1)-(A5) hold. If the discretization scheme\nsatisfies maxi\u2208{1,..,DN \u22121} |ti+1 \u2212 ti |2\u03b2 = o(n\u22121 ) then there is a constant C > 0\nsuch that, for all n,\n(\n)\n\u221a\nn Ep\nsup | \u03bc\nbM A,d (t) \u2212 \u03bc(t) | \u2264 C.\nt\u2208[0,T ]\n\nWe deduce from Proposition 3.2 that estimator \u03bc\nbM A,d (t) is asymptotically\nunbiased as well as design consistent. Note that the approximation error (with\nlinear interpolation) is negligible, compared to the sampling variability, under\nthe additional assumption on the repartition of the discretization points. This\nassumption also tolds us that less discretization points are required for smoother\ntrajectories.\nLet us also remark that, for each t,\n\u0012\n\u0013\n\u0011\n1k 0 \u0010 b\n1 X\ne\n1\u2212\nxk \u03b2 a (t) \u2212 \u03b2(t)\n,\n(10)\n\u03bc\nbM A,a (t) \u2212 \u03bc\ne(t) =\nN\n\u03c0k\nk\u2208U\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n11\n\nwhere 1k is the sample membership, so that it is not difficult to prove, under\nprevious assumptions and by using lemma A.4 in the Appendix, that for all\nt \u2208 [0, T ],\n\u221a\nn (b\n\u03bcM A,d (t) \u2212 \u03bc\ne(t)) = op (1).\n(11)\n3.3. Covariance function estimation under the sampling design\nWe undertake in this section a detailed study of the covariance function of\nestimator \u03bc\u0302M A,d . The covariance function is computed with respect to the sampling design p(*) and from relation (9), we can deduce that \u03bc\nbM A,d is a nonlinear\nfunction of Horvitz-Thompson estimators, so the usual Horvitz-Thompson covariance formula given by (3) can not be used anymore. Nevertheless, in light\nof relation (11), the covariance function of \u03bc\nbM A,d between two instants r and\nt may be approximated by the covariance Covp (\u03bc\u0303(r), \u03bc\u0303(t)), which in turn is\nequal to the Horvitz-Thompson covariance applied to the residuals Yk \u2212 \u1ef8k . Let\nus denote by \u03b3M A the approximative covariance function of \u03bc\nbM A,d defined as\nfollows\n1\n\u03b3MA (r, t) = 2 Covp\nN\n\nX Yk (r) \u2212 \u1ef8k (r) X Yk (t) \u2212 \u1ef8k (t)\n,\n\u03c0k\n\u03c0k\nk\u2208s\n\n!\n\nk\u2208s\n\n1 XX\nYk (r) \u2212 \u1ef8k (r) Yl (t) \u2212 \u1ef8l (t)\n= 2\n(\u03c0kl \u2212 \u03c0k \u03c0l )\n,\nN\n\u03c0k\n\u03c0l\n\nr, t \u2208 [0, T ].\n\nk\u2208U l\u2208U\n\n(12)\nThis approximation explains that model-assisted estimators will perform much\nbetter than Horvitz-Thompson estimators if the residuals Yk (t) \u2212 \u1ef8k (t) are\nsmall compared to Yk (t). The covariance function \u03b3MA (r, t) can be estimated by\nthe Horvitz-Thompson variance estimator for the estimated residuals Yk,d (t) \u2212\nYbk,d (t),\n\u03b3\nbMA,d (r, t) =\n\n1 X \u03c0kl \u2212 \u03c0k \u03c0l Yk,d (r) \u2212 Ybk,d (r) Yl,d (t) \u2212 Ybl,d (t)\n*\n*\n,\nN2\n\u03c0kl\n\u03c0k\n\u03c0l\n\nr, t \u2208 [0, T ],\n\nk,l\u2208s\n\n(13)\nb (t).\nwhere Ybk,d (t) = x0k \u03b2\na,d\nTo prove the consistency of the covariance estimator \u03b3\nbMA,d (r, t), let us introduce\nadditional assumptions that involve higher-order inclusion probabilities as well\nas conditions on the fourth order moments of the trajectories.\nA6. We assume that\nlim\n\nmax\n\nN \u2192\u221e (k,l,k0 ,l0 )\u2208D4,n\n\n|Ep {(1kl \u2212 \u03c0kl )(1k0 l0 \u2212 \u03c0k0 l0 )}| = 0\n\nwhere Dt,N is the set of all distinct t-tuples (i1 , ..., it ) from UN and\n1k 1l .\n\n1kl =\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n12\n\nP\nA7. There are two\npositive constants C5 and C6 such that N \u22121 U Yk (0)4 <\nP\nC5 and N \u22121 {Yk (t) \u2212 Yk (r)}4 < C6 |t \u2212 r|4\u03b2 , for all (r, t) \u2208 [0, T ]2\nCondition A6 has already been assumed by Breidt and Opsomer (2000) in a\nnonparametric model-assisted context and in Cardot and Josserand (2011) for\nHorvitz-Thompson estimators. It can be checked that it is fulfilled for simple\nrandom sampling without replacement (SRSWOR) or stratified sampling with\nSRSWOR within each strata. More generally, it is fulfilled for high entropy\nsampling designs. Boistard et al. (2012) prove that it is fulfilled for the rejective\nsampling whereas Cardot et al. (2012c) check that it is true for sampling designs,\nsuch as Sampford sampling, whose Kullback-Leibler divergence with respect to\nrejective sampling, tends to zero when the population size increases.\nProposition 3.3. Assume (A1)-(A7) hold and the sequence of discretization\nschemes satisfy limN \u2192\u221e maxi\u2208{1,..,DN \u22121} |ti+1 \u2212 ti | = 0. Then, as N tends to\ninfinity, we have for all (r, t) \u2208 [0, T ]2 ,\nn Ep {|b\n\u03b3MA,d (r, t) \u2212 \u03b3MA (r, t)|}\n\n\u2192\n\n0\n\nand\n(\nn Ep\n\n)\nsup |b\n\u03b3MA,d (t, t) \u2212 \u03b3MA (t, t)|\n\n\u2192\n\n0.\n\nt\u2208[0,T ]\n\nSince n\u03b3MA (r, t) remains bounded, the previous proposition tells us that\n\u03b3\nbMA,d is consistent pointwise and the variance function estimator is uniformly\nconvergent. Note also that the condition on the number of discretization points\nis much weaker than in Proposition 3.2 because we do not give here rates of\nconvergence. To obtain such rates, we would also need to impose additional\nassumptions on the sampling design.\n3.4. Asymptotic normality and confidence bands\nWe assume a supplementary assumption in order to get the asymptotic normality of the functional estimator \u03bc\u0302MA,d in the space of continuous functions.\nA8. We assume that for each fixed value of t \u2208 [0, 1],\n{\u03b3MA (t, t)}\u22121/2 (\u03bc\u0303(t) \u2212 \u03bc(t)) \u2192 N (0, 1)\nin distribution when N tends to infinity.\nThis assumption is satisfied for usual sampling designs (see e.g. Fuller (2009),\nChapter 2.2). Note that using relation (11), we can write for any fixed value\nt \u2208 [0, T ],\n\u03bc\nbMA,d (t) \u2212 \u03bc(t) = \u03bc\u0303(t) \u2212 \u03bc(t) + op (n\u22121/2 ),\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n13\n\n\u221a\n\u03bcMA,d (t) \u2212 \u03bc(t)) is also pointwise asymptotically Gaussian\nand deduce that n (b\nwhen conditions of Proposition 3.1 hold. Let us state now a much stronger result\nwhich indicates that the convergence to a Gaussian distribution also occurs for\nthe trajectories, in the space of continuous functions (see Billingsley (1968),\nChapter 2).\nProposition 3.4. Assume (A1)-(A5) and (A8) hold. If the discretization scheme\nsatisfies maxi={1,..,DN \u22121} |ti+1 \u2212ti |2\u03b2 = o(n\u22121 ), we have when n tends to infinity\n\u221a\nn {b\n\u03bcMA,d \u2212 \u03bc}\nZ\nwhere\nindicates the convergence in distribution in C[0, T ] with the uniform\ntopology and Z is a Gaussian process taking values in C[0, T ] with mean 0 and\ncovariance function \u03b3Z (r, t) = limn\u2192+\u221e n\u03b3MA (r, t).\nThe \"sup\" functional defined on the space of continuous functions being\n\u221a continuous, the Proposition 3.4 implies that the real random variable supt | n {b\n\u03bcMA,d (t) \u2212 \u03bc(t)} |\nconverges in distribution to supt |Z(t)|. We thus consider confidence bands for\n\u03bc of the form\n\u001a\u0014\n\u0015\n\u001b\n\u03c3\nb(t)\n\u03bc\nbMA,d (t) \u00b1 c \u221a\n, t \u2208 [0, T ]\n(14)\nn\np\n\u03b3MA,d (t, t). Note that the fact that\nwhere c is a suitable number and \u03c3\nb(t) = nb\n\u03bc belongs to the confidence band defined in (14) is equivalent to\n\u221a\nn\nsup\n|b\n\u03bcMA,d (t) \u2212 \u03bc(t)| \u2264 c.\nb(t)\nt\u2208[0,T ] \u03c3\nGiven a confidence level 1 \u2212 \u03b1 \u2208 (0, 1), one way to build such confidence\nband, that is to say one way to find an adequate value for c\u03b1 , is to perform\nsimulations of a centered Gaussian functions Zb defined on [0, T ] with mean\n0 and covariance function nb\n\u03b3MA,d (r, t) and then compute the quantile of order\nb\n\u03c3 (t) . In other words, we look for a constant c\u03b1 , which is\n1\u2212\u03b1 of supt\u2208[0,T ] Z(t)/b\nin fact a random variable since it depends on the estimated covariance function\n\u03b3\nbMA,d , such that\n\u0012\n\u0013\n\u03c3\nb(t)\nb\nP |Z(t)|\n\u2264 c\u03b1 \u221a , \u2200t \u2208 [0, T ] | \u03b3\nbMA,d = 1 \u2212 \u03b1\nn\nThe asymptotic coverage of this simulation based procedure has been rigorously studied for the Horvitz-Thompson estimators of the mean of sampled and\nnoisy trajectories in Cardot et al. (2012a) whereas Cardot et al. (2012b) have\nsuccessfully employed this approach on real load curves with model-assisted estimators. The next proposition, which can be seen as a functional version of\nSlutsky's Lemma, provides a rigorous justification of this latter technique.\nProposition 3.5. Assume (A1)-(A8) hold and the discretization scheme satisfies maxi\u2208{1,..,DN \u22121} |ti+1 \u2212 ti |2\u03b2 = o(n\u22121 ).\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n14\n\nLet Z be a Gaussian process with mean zero and covariance function \u03b3Z (as\nbN ) be a sequence of processes such that for each N ,\nin Proposition 3.4). Let (Z\nconditionally on the estimator \u03b3\nbMA,d defined in (13), ZbN is Gaussian with mean\nzero and covariance nb\n\u03b3MA,d . Suppose that \u03b3Z (t, t) is a continuous function and\ninf t \u03b3Z (t, t) > 0. Then, as N \u2192 \u221e, the following convergence holds uniformly\nin c,\n\u0010\n\u0011\nP |ZbN (t)| \u2264 c \u03c3\nb(t), \u2200t \u2208 [0, T ] \u03b3\nbMA,d \u2192 P (|Z(t)| \u2264 c \u03c3(t), \u2200t \u2208 [0, T ]) ,\nwhere \u03c3\nb(t) =\n\np\np\nnb\n\u03b3MA,d (t, t) and \u03c3(t) = \u03b3Z (t, t).\n\nAs in Cardot et al. (2012a), it is possible to deduce from the previous proposition that the chosen value b\nc\u03b1 = c\u03b1 (b\n\u03b3MA,d ) provides asymptotically the desired\ncoverage since it satisfies\n\u0015\n\u0013\n\u0012\n\u0014\n\u03c3\nb(t)\n, \u2200t \u2208 [0, T ] = 1 \u2212 \u03b1.\nlim P \u03bc(t) \u2208 \u03bc\nbMA,d (t) \u00b1 b\nc\u03b1 \u221a\nN \u2192\u221e\nn\n4. An illustration on electricity consumption curves\nWe consider, as in Cardot et al. (2012b), a population of N = 15069 electricity\nconsumption curves, measured every 30 minutes over a period of one week. Each\nelement k of the population is thus a vector with size 336, denoted by (Yk (t), t \u2208\n{1, . . . , 336}). The auxiliary information X of values xk , k \u2208 U is simply the\nmean consumption of each meter k \u2208 U recorded during the week before the\nsample is drawn. As shown in Figure 1, the real variable X is strongly correlated\nwith the consumption at each instant t of the current period of estimation so\nthat a linear model with a functional response is well adapted for model-assisted\nestimation.\nWe draw samples si of size n, for i = 1, . . . , I = 10000 with simple random\nsampling without replacement (SRSWOR) so that \u03c0k = n/N for k \u2208 {1, . . . , N }.\nWe define, for each sample si , the model-assisted estimator of the mean curve,\n(i)\n\n\u03bc\nbM A,d (t) =\n\n(i)\n1 X b (i)\n1 X Ybk (t) \u2212 Yk (t)\nYk (t) \u2212\nN\nN\nn/N\nk\u2208U\n\n(15)\n\nk\u2208si\n\n(i)\nxk Yk (t)\nb (i) (t), and \u03b2\nb (i) (t) = G\nb \u22121 1 P\nwhere x0k = (1, xk ), Ybk (t) = x0k \u03b2\nk\u2208si n/N\nN\nfor t \u2208 {1, . . . , 336}. Cardot et al. (2012b) noted that, for the same sample\nsize, the mean square error of estimation of the mean curve is divided by four\ncompared to the Horvitz-Thompson estimator with SRSWOR when considering\nthe model-assisted estimator defined in (15). There is only one covariate in this\nb\nstudy and we did not encounter any problem with the invertibility of matrix G,\nthe value of parameter a is thus a = 0.\nPI\n(i)\nWe also define \u03bc\u0302(t) = I1 i=1 \u03bc\u0302M A,d (t), t \u2208 {1, . . . , 336}. The true variance\nfunction of the model-assisted estimator being unknown, we approximate it with\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n15\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\nEmpirical variance\nEstimated variance\nApproximated variance\n\n0\n\n50\n\n100\n\n150\n\nHours\n\nFigure 2. Empirical variance function \u03b3emp , approximated variance \u03b3M A and estimated\nvariance \u03b3\u0302M A,d obtained with a sample of size n = 1500.\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n16\n\na Monte Carlo approach based on the I = 10000 samples drawn with simple\nrandom sampling without replacement. The approximation to the true variance\nfunction is thus given by\nI\n\n1 X (i)\n(i)\n(\u03bc\u0302\n(t) \u2212 \u03bc\u0302(t))(\u03bc\u0302M A,d (r) \u2212 \u03bc\u0302(r))\nI i=1 M A,d\n\n\u03b3emp (r, t) =\n\n(16)\n\nfor (r, t) \u2208 {1, . . . , 336}.\nThe following quadratic loss criterion which measures a relative error is used\nto evaluate, for each sample, the accuracy of the variance estimator defined in\n(13),\n336\n\nEr (\u03b3\u0302M A,d ) =\n\n1 X |\u03b3\u0302M A,d (t, t) \u2212 \u03b3emp (t, t)|2\ndt\n336 t=1\n\u03b3emp (t, t)2\n\n(17)\n\nWe also decompose, over the I = 10000 estimations, the relative mean square\nerror (RMSE) of the estimator into an approximation error (RB(\u03b3\u0302M A,d )2 ) and\na variance term (V R(\u03b3\u0302M A,d )) that can be related to the sampling error,\nI\n\n1 X (i)\nRM SE(\u03b3\u0302M A,d ) =\nE (\u03b3\u0302M A,d )\nI i=1 r\n= RB(\u03b3\u0302M A,d )2 + V R(\u03b3\u0302M A,d )\n(i)\n\nwhere Er (\u03b3\u0302M A,d ) is the value of Er (\u03b3\u0302M A,d ) for the ith sample. The relative\nbias of the estimator \u03b3\u0302M A,d may be written as\n336\n\n1 X\nRB(\u03b3\u0302M A,d ) =\n336 t=1\n2\n\nwhere \u03b3\u0302 M A,d (t, t) =\nSample size\n250\n500\n1500\n\n1\nI\n\nPI\n\ni=1\n\nRM SE(\u03b3\u0302M A,d )\n0.1315\n0.0697\n0.0238\n\n\u03b3\u0302 M A,d (t, t) \u2212 \u03b3emp (t, t)\n\u03b3emp (t, t)\n\n!2\n\n(i)\n\n\u03b3\u0302M A,d (t, t).\nRB(\u03b3\u0302M A,d )2\n0.0027\n0.0016\n0.0003\n\nq5\n0.0264\n0.0166\n0.0076\n\nq25\n0.0455\n0.029\n0.0125\n\nEr (\u03b3\u0302M A,d )\nMedian\nq75\n0.0707\n0.117\n0.0459\n0.0794\n0.0186\n0.028\n\nq95\n0.4945\n0.1945\n0.0569\n\nTable 1\nSummary statistics for Er (\u03b3\u0302M A,d , \u03b3emp ), with I=10000 samples.\n\nThe RMSE as well as the approximation error and statistics (quantiles) for\nEr are given in Table 1. We can note that logically the RMSE decreases as the\nsample size increases and that even for moderate sample sizes, the estimations\nare rather precise. A closer look on how the RMSE is decomposed reveals that\nestimation error is mainly due to the sampling error, via the variance term\nwhereas the approximation error term RB(\u03b3\u0302M A,d )2 is negligible. This fact can\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n17\n\nbe observed in Figure 2 were we plot the true variance function \u03b3emp over the\nconsidered period, its approximation \u03b3M A as well as an estimation \u03b3\nbM A,d with\na sample with size n = 1500, whose error according to criterion (17) is close to\nthe mean error (Er \u2248 0.02).\nWe have also plotted in Figure 3 the difference between the empirical covariance function \u03b3emp and its approximation \u03b3M A and in Figure 4 the difference\nbetween \u03b3M A and its estimation \u03b3\nbM A,d for a sample with size n = 1500 whose\nerror, Er \u2248 0.02, is close to the mean value. Once again, it is clearly seen that\nthe approximation error to the true covariance function (see Figure 3) is much\nsmaller than the sampling error (see Figure 4). We can also remark some strong\nperiodic pattern which reflects the natural daily periodicity in the electricity\nconsumption behavior and that is related to the temporal correlation of the\nunknown residual process \u000fkt defined in (4).\n5. Concluding remarks\nWe have made in this paper an asymptotic study of model-assisted estimators,\nwith linear regression models with functional response, when the target is the\nmean of functional data with discrete observations in time. This work can be\nextended in many directions. For example, one could consider more sophisticated\nregression models than model (4) such as non linear or nonparametric models\nwith functional response by adapting, in a survey sampling context, models\nstudied in the functional data analysis literature (see Chiou et al. (2004), Cardot\n(2007), or Ferraty et al. (2011)). However, one important drawback of such more\nsophisticated approaches is that they would require to know xk for all the units\nk in the population as opposed to only their population totals.\nAn interesting direction for future investigation would be to consider noisy\nand possibly sparse measurements in time. For the Horvitz-Thompson estimator, local polynomials are employed in Cardot et al. (2012a) in order to first\nsmooth the trajectories and it would certainly be possible to adapt the techniques developed in this work to the model-assisted estimation procedure.\nAnother promising direction for future research would be to adapt modelassisted estimators for time-varying samples. When one works with large networks of sensors it can be possible to consider a sequence of samples s(t) that\nevolve along time. A preliminary work (see Degras (2012)), which focuses on\nHorvitz-Thompson estimators and stratified sampling clearly shows that such\ntime-varying samples can outperform sampling designs that are fixed in time.\nAcknowledgements. We thank the two anonymous referees for a careful reading and interesting suggestions that have permitted a great improvement of the\noriginal manuscript.\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n18\n\nFigure 3. (Approximation error) difference between the covariance function and its approximation, \u03b3emp (t, r) \u2212 \u03b3M A (t, r), for a sample with size n = 1500.\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n19\n\nFigure 4. (Sampling error) difference between the approximated covariance function and its\nestimation, \u03b3M A (t, r) \u2212 \u03b3\u0302M A,d (t, r), for a sample with size n = 1500.\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n20\n\nAppendix A: Proofs\nThroughout the proofs we use the letter C to denote a generic constant whose\nvalue may vary from place to place. We also denote by \u03b1k = 1\u03c0kk \u2212 1, k \u2208 U and\nby \u2206kl = \u03c0kl \u2212 \u03c0k \u03c0l , k, l \u2208 U.\nA.1. Some useful Lemmas\nNote that the result showed in the first Lemma is sometimes stated as an assumption (see e.g Robinson and S\u00e4rndal (1983)). It is used to prove the convergence of the estimator of the mean in terms of mean square error.\nLemma A.1. Let assumptions (A1), (A2) and (A4), (A5) hold. Then, there is\na constant C such that\n\u0010\n\u0011\nb \u22121 \u2212 G\u22121 k2 \u2264 C.\nn Ep kG\na\n\nProof . The proof follows the lines of (Bosq (2000), Theorem 8.4) and (Cardot\net al. (2010), Proposition 3.1). Using assumption (A5) and inequality (7), we\nhave\nb \u22121 \u2212 G\u22121 k \u2264 kG\nb \u22121 k.kG\nb a \u2212 Gk.kG\u22121 k\nkG\na\na\nb a \u2212 Gk,\n\u2264 a\u22122 kG\nwhich implies\n\u0010\n\u0011\n\u0010\n\u0011\nb \u22121 \u2212 G\u22121 k2 \u2264 a\u22124 Ep kG\nb a \u2212 Gk2 .\nEp kG\na\n\n(18)\n\n\u0010\n\u0011\nb a \u2212 Gk2 , we use the following decomposition.\nTo bound Ep kG\n\u0010\n\u0011\n\u0010\n\u0011\n\u0010\n\u0011\nb a \u2212 Gk2 = Ep kG\nb a \u2212 Gk2 1 b b + Ep kG\nb a \u2212 Gk2 1 b b\nEp kG\n{Ga =G}\n{Ga 6=G}\n\u0010\n\u0011\n\u0010\n\u0011\n\u0010\n\u0011\nb \u2212 Gk2 + 2Ep kG\nb a \u2212 Gk\nb 2 1 b b + 2Ep kG\nb \u2212 Gk2 1 b b\n\u2264 Ep kG\n{Ga 6=G}\n{Ga 6=G}\n\u0010\n\u0011\n2\n2\nb \u2212 Gk ) + 2Ep kG\nb a \u2212 Gk\nb 1 b b .\n\u2264 3Ep (kG\n(19)\n{Ga 6=G}\nTo bound the first term from the right-side of (19), we use the fact that the\n2\nspectral norm is majored by the trace norm || * ||2 defined by kAk2 = tr(A0 A).\nNext, we show (see also Cardot et al. (2010), proof of Proposition 3.1) that,\nb \u2212 Gk2 = O(n\u22121 ).\nEp kG\n2\n\n(20)\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n21\n\nWe have, with assumptions (A1), (A2) and (A4) that,\n!\nXX\n1\n2\n0\n0\nb \u2212 Gk =\nEp kG\nEp\n\u03b1k \u03b1l tr[xk xk xl xl ]\n2\nN2\nk\u2208U l\u2208U\n1 1 X\n1 XX\n\u2264 2\nkxk x0k k22 + max |\u2206kl | 2 2\nkxk k2 kxl k2\nk6=l\u2208U\nN \u03bb\nN \u03bb\nk\u2208U\nk\u2208U l\u2208U\n\u0013\n\u0012\n1 n 1\n1\n\u2264\n+ n max |\u2206kl | 2 C22\nk6=l\u2208U\nn N\u03bb\n\u03bb\nC\n\u2264 .\nn\nOn the other hand,\n\u0010\n\u0011\nb a \u2212 Gk\nb 2 1 b b \u2264 a2 P(G\nb a 6= G)\nb\nEp kG\n{Ga 6=G}\nsince\np\nX\n0\nb a \u2212 Gk\nb =\n[max(\u03b7j,n , a) \u2212 \u03b7j,n ]vjn vjn\nkG\n\n2\n\n2\n\nj=1\n\nsup | max(\u03b7j,n , a) \u2212 \u03b7j,n |2\n\n\u2264\n\nj=1,...,p\n\n\u2264 a2 .\nMoreover, since a < \u03b7p = G\u22121\n\n\u22121\n\nand by Chebychev inequality, we can bound\n\nb a 6= G)\nb = P(\u03b7p,n < a)\nP(G\n\u0013\n\u0012\n|\u03b7p \u2212 a|\n,\n\u2264 P |\u03b7p,n \u2212 \u03b7p | \u2265\n2\n\u0001\n4\n\u2264\nEp |\u03b7p,n \u2212 \u03b7p |2\n2\n(\u03b7p \u2212 a)\n\u0010\n\u0011\n4\nb \u2212 Gk2 ,\n\u2264\nE\nk\nG\np\n(\u03b7p \u2212 a)2\nbecause it is known that the eigenvalue map is Lipschitzian for symmetric matrices (see Bhatia (1997), Chapter 3). This means that for two p \u00d7 p symmetric\nmatrices A and B, with eigenvalues \u03b71 (A) \u2265 \u03b72 (A) \u2265 * * * \u2265 \u03b7p (A) (resp.\n\u03b71 (B) \u2265 * * * \u2265 \u03b7p (B)), we have\nmax\nj\u2208{1,...,p}\n\n|\u03b7j (A) \u2212 \u03b7j (B)| \u2264 kA \u2212 Bk .\n\nHence, for some constant C\n\u0010\n\u0011\n\u0010\n\u0011\nb a \u2212 Gk2 \u2264 3Ep kG\nb \u2212 Gk2 + 2a2 P(G\nb a 6= G)\nb\nEp kG\n\u2264\n\nC\n.\nn\n\n(21)\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n22\n\n\u0003\n\nCombining (18), (19), (20) and (21), the proof is complete.\n\nLemma A.2. Under assumptions (A1), (A2) and (A4), there is a constant C\nsuch that, for all n,\n\u0012\n\u0013\n1 X 1k\n\u2212 1 xk\nN\n\u03c0k\n\nn Ep\n\n2\n\n\u2264 C.\n\nk\u2208U\n\nProof . Expanding the square norm, we have\nnEp\n\n1 X\n\u03b1k xk\nN\n\n2\n\n= nEp\n\nk\u2208U\n\n1 XX\n\u03b1k \u03b1l x0k xl\nN2\n\n!\n\nk\u2208U l\u2208U\n\nn X X \u2206kl 0\nx xl\n\u2264 2\nN\n\u03c0k \u03c0l k\nk\u2208U l\u2208U\n\u0015\n\u0014\n1\n1 X\nn 1\n+ 2 n max |\u2206kl |\nkxk k2\n\u2264\nN \u03bb \u03bb k6=l\u2208U\nN\nk\u2208U\n\nand the result follows with hypotheses (A1), (A2) and (A4).\n\n\u0003\n\nLemma A.3. Under assumptions (A2)-(A5), we have\ni) k\u03b2\u0303(t) \u2212 \u03b2\u0303(r)k2 \u2264 a\u22122 C3 C4 |t \u2212 r|2\u03b2 .\n2\u03b2\nb (t) \u2212 \u03b2\nb (r)k2 \u2264 a\u22122\nii) k\u03b2\na\na\n\u03bb2 C3 C4 |t \u2212 r| .\nProof For i), we just need to remark that, under hypotheses (A3), (A4) and\n(A5),\n2\n\n1 X\nxk (Yk (t) \u2212 Yk (r)\nk\u03b2\u0303(t) \u2212 \u03b2\u0303(r)k = G\nN\nk\u2208U\n!\n!\n1 X\n1 X\n\u22121 2\n2\n2\n\u2264 kG k\nkxk k\n(Yk (t) \u2212 Yk (r))\nN\nN\n\u22121\n\n2\n\n\u22122\n\n\u2264a\n\nk\u2208U\n2\u03b2\n\nk\u2208U\n\nC4 C3 |t \u2212 r| .\n\nThe proof of point ii) is similar, but also requires the use of lower bounds on\nthe first order inclusion probabilities (assumption (A2)),\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n23\n\n2\n\nX 1k\nb (t) \u2212 \u03b2\nb (r)k2 = G\nb \u22121 1\nk\u03b2\nxk (Yk (t) \u2212 Yk (r)\na\na\na\nN\n\u03c0k\nk\u2208U\n!\n!\n1 b \u22121 2 1 X\n1 X\n2\n2\n\u2264 2 kGa k\nkxk k\n(Yk (t) \u2212 Yk (r))\n\u03bb\nN\nN\nk\u2208U\n\n\u22122\n\n\u2264a\n\nk\u2208U\n\n1\nC4 C3 |t \u2212 r|2\u03b2 .\n\u03bb2\n\u0003\n\nThe following Lemma states the pointwise mean square convergence for any\nfixed value of t \u2208 [0, T ].\nLemma A.4. Suppose that assumptions (A1)-(A5) hold. Then, there is a positive constant \u03b61 such that, for all t \u2208 [0, T ],\n\u0010\n\u0011\nb (t) \u2212 \u03b2\u0303(t)k2 \u2264 \u03b61 .\nnEp k\u03b2\na\nProof . The demonstration is similar to the proof of Lemma A.5 and is thus\nomitted.\n\u0003\nLemma A.5. Suppose that assumptions (A1)-(A5) hold. Then, there is a positive constant \u03b62 such that\n\u0010\n\u0011\nb (t) \u2212 \u03b2\u0303(t) \u2212 \u03b2\nb (r) + \u03b2\u0303(r)k2 \u2264 \u03b62 |t \u2212 r|2\u03b2 .\nnEp k\u03b2\na\na\nProof . A direct decomposition leads to\nb (t) \u2212 \u03b2\u0303(t) \u2212 \u03b2\nb (r) + \u03b2\u0303(r)k2\nnk\u03b2\na\na\nb \u22121 \u2212 G\u22121 )\n\u2264 (G\na\n\n\u0012\n\u0013\n1 X 1k\n1 X 1k\nxk (Yk (t) \u2212 Yk (r)) + G\u22121\n\u2212 1 xk (Yk (t) \u2212 Yk (r))\nN\n\u03c0k\nN\n\u03c0k\nk\u2208U\n\n\u2264\n\n2A21N\n\n+\n\nk\u2208U\n\n2A22N ,\n\n(22)\n\n2\n1k\nb \u22121 \u2212 G\u22121 k2 1 P\nwhere A21N = nkG\nand\na\nk\u2208U \u03c0k xk (Yk (t) \u2212 Yk (r))\nN\n2\nP\nA22N = nkG\u22121 k2 N1 k\u2208U \u03b1k xk (Yk (t)\u2212Yk (r)) . Using now assumptions (A2)(A4) and the Cauchy-Schwarz inequality, we get\n!\n!\nX\nX\n1\n1\n1\nb \u22121 \u2212 G\u22121 k2\nA21N \u2264 nkG\nkxk k2\n(Yk (t) \u2212 Yk (r))2\na\n\u03bb2 N\nN\nk\u2208U\n\n\u2264\n\nb \u22121\nnkG\na\n\n1\nk 2 C3 C4 |t \u2212 r|2\u03b2 .\n\u03bb\n\n\u22121 2\n\n\u2212G\n\nk\u2208U\n\n2\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n24\n\nUsing now Lemma A.1, we can bound\nEp (A21N ) \u2264 C|t \u2212 r|2\u03b2 ,\n\n(23)\n\nfor some constant C. Now, with assumptions (A1)-(A5) and following the same\narguments as in the proof of Lemma A.2, we also have\n\uf8f6\n\uf8eb\n2\nX\n1\nEp (A22N ) \u2264 nkG\u22121 k2 Ep \uf8ed\n\u03b1k xk (Yk (t) \u2212 Yk (r)) \uf8f8\nN\nk\u2208U\n\u0012\n\u0013\nn 1\nn maxk6=l\u2208U |\u2206kl |\n\u2264\nC3 C4 a\u22122 |t \u2212 r|2\u03b2 \u2264 C|t \u2212 r|2\u03b2 . (24)\n+\nN\u03bb\n\u03bb2\nfor some positive constant C. Combining (22), (23) and (24), the result is proved.\n\u0003\n\nA.2. Proof of Proposition 3.1 and Proposition 3.2\nThe proof of Proposition 3.1 is omitted. It is analogous to the proof of Proposition 3.2, which is given below. The different steps are similar to the proof of\nProposition 1 in Cardot and Josserand (2011).\nLet us decompose, for t \u2208 [0, T ],\nsup |b\n\u03bcMA,d (t) \u2212 \u03bc(t)| \u2264 sup |b\n\u03bcMA,d (t) \u2212 \u03bc\nbMA,a (t)| + sup |b\n\u03bcMA,a (t) \u2212 \u03bc(t)|\n\nt\u2208[0,T ]\n\nt\u2208[0,T ]\n\nt\u2208[0,T ]\n\n(25)\nand study each term at the right-hand side of the inequality separately.\nStep 1. The interpolation error supt\u2208[0,T ] |b\n\u03bcMA,d (t) \u2212 \u03bc\nbMA,a (t)|.\nConsider t \u2208 [ti , ti+1 ) and write\n|b\n\u03bcMA,d (t) \u2212 \u03bc\nbMA,a (t)| \u2264 |b\n\u03bcMA,a (ti ) \u2212 \u03bc\nbMA,a (t)| + |b\n\u03bcMA,a (ti+1 ) \u2212 \u03bc\nbMA,a (ti )|.\nUnder assumptions (A2)-(A5) and using Lemma A.3, ii), we get\n|b\n\u03bcMA,a (t) \u2212 \u03bc\nbMA,a (r)| \u2264\n\nX |Yk (t) \u2212 Yk (r)|\n1 X\nb (t) \u2212 \u03b2\nb (r)) + 1\n\u03b1k x0k (\u03b2\na\na\nN\nN\n\u03c0k\nk\u2208U\n\n\u0012\n\n1\n1+\n\u03bb\n\nk\u2208s\n\n\u0013p\n\nX\nb (t) \u2212 \u03b2\nb (r)k + 1 1\n\u2264\nC4 k\u03b2\n(Yk (t) \u2212 Yk (r))2\na\na\n\u03bb N\nk\u2208U\np\n\u0001\n\u2264 (1 + \u03bb\u22121 )C4 a\u22121 + 1 \u03bb\u22121 C3 |t \u2212 r|\u03b2 .\n\n!1/2\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n25\n\nSo, there is a positive constant C such that\n|b\n\u03bcMA,a (t) \u2212 \u03bc\nbMA,a (r)| \u2264 C|t \u2212 r|\u03b2\nand consequently,\n|b\n\u03bcMA,d (t) \u2212 \u03bc\nbMA,a (t)| \u2264 C[|ti \u2212 t|\u03b2 + |ti+1 \u2212 ti |\u03b2 ]\n\u2264 2C|ti+1 \u2212 ti |\u03b2 .\nHence, since by hypothesis, limN \u2192\u221e maxi={1,...,dN \u22121 } |ti+1 \u2212ti |\u03b2 = o(n\u22121/2 ), we\nhave\n\u221a\n\nsup\nt\u2208[0,T ]\n\nn|b\n\u03bcMA,d (t) \u2212 \u03bc\nbMA,a (t)| = o(1).\n\n(26)\n\nStep 2. The estimation error supt\u2208[0,T ] |b\n\u03bcM A,a (t) \u2212 \u03bc(t)|.\nWe use the following decomposition:\nsup |b\n\u03bcMA,a (t) \u2212 \u03bc(t)| \u2264 |b\n\u03bcMA,a (0) \u2212 \u03bc(0)| + supr,t\u2208[0,T ] |b\n\u03bcMA,a (t) \u2212 \u03bc(t) \u2212 \u03bc\nbMA,a (r) + \u03bc(r)|.\n\nt\u2208[0,T ]\n\n(27)\nWriting,\n\u03bc\nbMA,a (0) \u2212 \u03bc(0) =\n\n1 X\n1 X\n\u03b1k Yk (0) \u2212\n\u03b1k Ybk (0)\nN\nN\nk\u2208U\n\nk\u2208U\n\nX xl Yl (0)\n1 X\n1 X\nb \u22121\n=\n\u03b1k Yk (0) \u2212 2\n\u03b1k x0k G\na\nN\nN\n\u03c0l\nk\u2208U\n\nk\u2208U\n\nl\u2208s\n\nwe directly get, with hypotheses A1-A3 and with similar arguments as in the\nproof of Lemma A.2, that for some constant C,\n2\n\nEp (b\n\u03bcMA,a (0) \u2212 \u03bc(0)) \u2264\n\nC\n.\nn\n\n(28)\n\nThe second term at the right-hand side in (27) is dealt with using maximal\ninequalities. More exactly, we use Corollary 2.2.5 in van der Vaart and Wellner\n(2000). Consider for this, the Orlicz norm of some random variable X which is\ndefined as follows\nq\n||X||\u03c8 = Ep (\u03c8(X)).\n2\n2\nFor the particular\npcase \u03c8(u) = u , the Orlicz norm is simply the well-known L\nnorm, ||X||\u03c8 = Ep (X 2 ). Let us introduce for (r, t) \u2208 [0, T ]2 , the semimetric\nd(r, t) defined by\n\u221a\n2\nd2 (r, t) =\nn|b\n\u03bcMA,a (t) \u2212 \u03bc(t) \u2212 \u03bc\nbMA,a (r) \u2212 \u03bc(r)| \u03c8\n\u0001\n= nEp |b\n\u03bcMA,a (t) \u2212 \u03bc(t) \u2212 \u03bc\nbMA,a (r) + \u03bc(r)|2\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n26\n\nand consider D(\u000f, d), the packing number, which is defined as the maximum\nnumber of points in [0, T ] whose distance between each pair is strictly larger\nthan \u000f. Then, Corollary 2.2.5 in van der Vaart and Wellner (2000) states that\nthere is a constant K > 0 such that\n\u221a\n\nsup\n(r,t)\u2208[0,T ]2\n\nZ\n\u2264K\n\nn|b\n\u03bcMA,a (t) \u2212 \u03bc(t) \u2212 \u03bc\nbMA,a (r) \u2212 \u03bc(r)|\n\u03c8\n\nT\n\n\u03c8 \u22121 (D(\u000f, d))d\u000f.\n\n0\n\n(29)\nWe show below that there is a constant C such that d2 (r, t) \u2264 C|t \u2212 r|2\u03b2 and\nthus, since \u03b2 > 1/2, the integral at the right-hand side of (29) is finite.\nLet us first decompose\nd2 (r, t) \u2264 2d21 (r, t) + 2d22 (r, t)\n\n(30)\n\nwhere\nd21 (r, t) = nEp (|b\n\u03bcMA,a (t) \u2212 \u03bc\u0303(t) \u2212 \u03bc\nbMA,a (r) + \u03bc\u0303(r)|2 )\nand\nd22 (r, t) = nEp (|\u03bc\u0303(t) \u2212 \u03bc(t) \u2212 \u03bc\u0303(r) + \u03bc(r)|2 ).\nBy assumptions (A2)-(A4) and Lemma A.5, we can bound, for some constant\nC,\n\uf8f1\n\uf8f2\n\n1 X\nd21 (r, t) \u2264 Ep n\n\u03b1k xk\n\uf8f3 N\n\n2\n\nb (t) \u2212 \u03b2\u0303(t) \u2212 \u03b2\nb (r) + \u03b2\u0303(r)k2\nk\u03b2\na\na\n\nk\u2208U\n\n\u0012\n\u2264\n\n1+\n\n1\n\u03bb\n\n\u00132\n\nC4 \u03b62 |t \u2212 r|2\u03b2 := C|t \u2212 r|2\u03b2 .\n\n\uf8fc\n\uf8fd\n\uf8fe\n(31)\n\nConsidering now d2 (r, t), we have\n\"\nd22 (r, t) = nEp\n\n#\nh\ni 2\n1 X\n\u03b1k Yk (t) \u2212 Yk (r) \u2212 x0k (\u03b2\u0303(t) \u2212 \u03b2\u0303(r))\nN\nk\u2208U\n\n2\n\u2264 2Ep (A2N ) + 2Ep (BN\n)\n\n(32)\n\n\u0010 P\n\u00112\n\u00012\nP\n2\nwhere A2N = n N1 k\u2208U \u03b1k [Yk (t) \u2212 Yk (r)] and BN\n= n N1 k\u2208U \u03b1k x0k (\u03b2\u0303(t) \u2212 \u03b2\u0303(r)) .\nWith hypotheses (A1)-(A3), one can easily obtain that there is a positive constant C such that\nEp (A2N ) \u2264 C|t \u2212 r|2\u03b2\n\n(33)\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n27\n\nand thanks to Lemma A.2 and to Lemma A.3, we can bound\n\uf8ee\n\uf8f9\n2\nX\n1\n2\nEp (BN\n) \u2264 Ep \uf8f0n\n\u03b1k xk \uf8fb k\u03b2\u0303(t) \u2212 \u03b2\u0303(r)k2\nN\nk\u2208U\n\n2\u03b2\n\n\u2264 C|t \u2212 r| .\n\n(34)\n\nCombining now (33) and (34) with (30) and (31), we get that\nd2 (r, t) \u2264 C|t \u2212 r|2\u03b2 ,\n\n(35)\n\nfor some constant C.\nUsing now (35), it is clear that the packing number is bounded as follows:\nD(\u000f, d) = O(\u000f\u22121/\u03b2 ). Consequently, the integral at the right-hand side of (29)\nis finite when \u03b2 > 1/2. Inserting (28) and (29) in (27), the proof of step 2 is\ncomplete.\nA.3. Proof of the consistency of the covariance function\nWe first prove that for any (r, t) \u2208 [0, T ]2 , the estimator \u03b3\nbMA,d (r, t) of the\ncovariance function converges to \u03b3MA (r, t).\nThen we prove the uniform convergence of the variance estimator \u03b3\nbMA,d (t, t)\nby showing its convergence in distribution to zero in the space of continuous\nfunctions. The proof is decomposed into two classical steps (see for example\nTheorem 8.1 in Billingsley (1968)). We first show the pointwise convergence, by\nconsidering the convergence of all finite linear combinations, and then we check\nthat the sequence is tight by bounding the increments.\nStep 1. Pointwise convergence\nWe want to show, that for each (t, r) \u2208 [0, T ]2 , we have when N tends to infinity,\nnEp {| \u03b3\nbMA,d (r, t) \u2212 \u03b3MA (r, t) |} \u2192 0.\nLet us decompose\nn(b\n\u03b3MA,d (r, t) \u2212 \u03b3MA (r, t)) = n(b\n\u03b3MA,d (r, t) \u2212 \u03b3\nbMA,a (r, t)) + n(b\n\u03b3MA,a (r, t) \u2212 \u03b3MA (r, t))\nwhere \u03b3\nbMA,a (r, t) is defined by\n\u03b3\nbMA,a (r, t) =\n\n1 X \u2206kl Yk (r) \u2212 Ybk,a (r) Yl (t) \u2212 Ybl,a (t)\n*\nN2\n\u03c0kl\n\u03c0k\n\u03c0l\nk,l\u2208s\n\nWe study separately the interpolation and the estimation errors.\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n28\n\nInterpolation error\nLet us suppose that t \u2208 [ti , ti+1 ), r \u2208 [ti0 , ti0 +1 ). We have n(b\n\u03b3MA,d (r, t) \u2212\n\u03b3\nbMA,a (r, t)) \u2264 A + B, with\nn X |\u2206kl |\n|(Yk,d (r) \u2212 Yk (r))(Yl,d (t) \u2212 Yl (t))\nA= 2\nN\n\u03c0kl \u03c0k \u03c0l\nk,l\u2208U\n\n+ (Yk,d (r) \u2212 Yk (r))(Yl (t) \u2212 Ybl,d (t)) + (Yk (r) \u2212 Ybk,d (r))(Yl,d (t) \u2212 Yl (t))|\nand\nB=\n\n\u0011\u0010\n\u0011 \u0010\n\u0011\u0010\n\u0011\nn X |\u2206kl | \u0010\nbk,d (r) Yl (t) \u2212 Ybl,d (t) \u2212 Yk (r) \u2212 Ybk,a (r) Yl (t) \u2212 Ybl,a (t)\nY\n(r)\n\u2212\nY\nk\nN2\n\u03c0kl \u03c0k \u03c0l\nk,l\u2208U\n\n=\n\nn X |\u2206kl |\nYk (r)(Ybl,a (t) \u2212 Ybl,d (t)) + Yl (t)(Ybk,a (r) \u2212 Ybk,d (r)) + Ybk,d (r)Ybl,d (t) \u2212 Ybk,a (r)Ybl,a (t) .\nN2\n\u03c0kl \u03c0k \u03c0l\nk,l\u2208U\n\nFor t \u2208 [ti , ti+1 ], we can write\n|Yl,d (t) \u2212 Yl (t)| \u2264 |Yl (ti ) \u2212 Yl (t)| + |Yl (ti+1 ) \u2212 Yl (ti )|\nand\n|Ybl,a (t) \u2212 Ybl,d (t)| \u2264 |Ybl,a (t) \u2212 Ybl,a (ti )| + |Ybl,a (ti+1 ) \u2212 Ybl,d (ti )|\nP\nP\nWe have that N1 l\u2208U (Yl,d (t)\u2212Yl (t))2 \u2264 C[|ti \u2212t|2\u03b2 +|ti+1 \u2212ti |2\u03b2 ] and N1 l\u2208U (Yl (t)\u2212\nYbl,d (t))2 = O(1). Thanks to Lemma A.3, we can bound\n1 1/2\n1 1/2\nC4 a\u22121 C3 |ti \u2212 t|\u03b2 \u2264 C4 a\u22121 C3 |ti+1 \u2212 ti |\u03b2 .\n\u03bb\n\u03bb\nUnder the assumption on the grid of discretization points, one can get after\nsome algebra that\n|Ybl,a (ti ) \u2212 Ybl,a (t)|\n\n\u2264\n\nn|b\n\u03b3MA,d (r, t) \u2212 \u03b3\nbMA,a (r, t)| = o(1).\nEstimation error\nConsider now,\n\u0012\n\u0013\nn X X \u2206kl 1kl\n\u2212\n1\n[Yk (t) \u2212 \u1ef8k (t)][Yl (r) \u2212 \u1ef8l (r)]\nN2\n\u03c0k \u03c0l \u03c0kl\nU\nU\nn X X \u2206kl 1kl\n+ 2\n[Yk (t) \u2212 \u1ef8k (t)][\u1ef8l (r) \u2212 Ybl,a (r)]\nN\n\u03c0k \u03c0l \u03c0kl\nk\u2208U l\u2208U\nn X X \u2206kl 1kl\n+ 2\n[\u1ef8k (t) \u2212 Ybk,a (t)][Yl (r) \u2212 \u1ef8l (r)]\nN\n\u03c0k \u03c0l \u03c0kl\nk\u2208U l\u2208U\nn X X \u2206kl 1kl\n+ 2\n[\u1ef8k (t) \u2212 Ybk,a (t)][\u1ef8l (r) \u2212 Ybl,a (r)]\nN\n\u03c0k \u03c0l \u03c0kl\n\nn(b\n\u03b3MA,a (r, t) \u2212 \u03b3MA (r, t)) =\n\nk\u2208U l\u2208U\n\n:= A1 (r, t) + A2 (r, t) + A3 (r, t) + A4 (r, t).\n\n(36)\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n29\n\nLet us define \u1ebdk (t) = Yk (t) \u2212 \u1ef8k (t) and first show that Ep (A1 (r, t)2 ) \u2192 0 when\nN \u2192 \u221e.\n\uf8ee\nEp (A1 (r, t)2 ) = Ep \uf8f0\n\n2\n\nn\nN4\n\nX\n\nX\n\nk,l\u2208U k0 ,l0 \u2208U\n\n\u2206kl\n\u03c0k \u03c0l\n\n\u0012\n\n1kl\n\u03c0kl\n\n\u0013\n\u22121\n\n\u2206\n\u03c0k 0 \u03c0l 0\nk 0 l0\n\n\u0012\n\n1\n\nk 0 l0\n\n\u03c0k 0 l 0\n\n\uf8f9\n\u0013\n\u2212 1 \u1ebdk (t)\u1ebdl (r)\u1ebdk0 (t)\u1ebdl0 (r)\uf8fb\n\n#\n\u0012\n\u0013\n\u0012\n\u0013\n1 \u2212 \u03c0k 0 1k 0\nn 2 X X 1 \u2212 \u03c0k 1 k\n\u2212 1 \u1ebdk (t)\u1ebdk (r)\u1ebdk0 (t)\u1ebdk0 (r)\n= Ep\n\u22121\nN4\n\u03c0k\n\u03c0k\n\u03c0k 0\n\u03c0k 0\nk\u2208U k0 \u2208U\n\uf8f9\n\uf8ee\n\u0012\n\u0013\n\u0013\n\u0012\n2 X X\n0\n0\n0\n0\n1\n\u2212\n\u03c0\n1\n\u2206\nn\n1\nk\nk\nk l\nk l\n\u22121\n\u2212 1 \u1ebdk (t)\u1ebdk (r)\u1ebdk0 (t)\u1ebdl0 (r)\uf8fb\n+ 2Ep \uf8f0 4\nN\n\u03c0k\n\u03c0k\n\u03c0k 0 \u03c0l 0 \u03c0k 0 l 0\n0\n0\nk\u2208U k 6=l \u2208U\n\uf8ee\n\uf8f9\n\u0013\n\u0012\n\u0013\n2 X\nX \u2206kl \u0012 1kl\n0\n0\n0\n0\nn\n\u2206\n1\nk l\nk l\n+ Ep \uf8f0 4\n\u2212 1 \u1ebdk (t)\u1ebdl (r)\u1ebdk0 (t)\u1ebdl0 (r)\uf8fb\n\u22121\nN\n\u03c0k \u03c0l \u03c0kl\n\u03c0k 0 \u03c0l 0 \u03c0k 0 l 0\n0\n0\n\"\n\nk6=l\u2208U k 6=l \u2208U\n\n:= a1 + a2 + a3 .\n\n(37)\n\nThe hypotheses on the moments of the inclusion probabilities and Lemma A.6\ngive us\n\u0012 2\n\u0013\nn2 maxk6=k0 \u2208U |\u2206kk0 |\nn 1\n+\na1 \u2264\n\u03b64\nN 3 \u03bb3\nN2\n\u03bb4\nas well as\na3 \u2264\n\n(n maxk6=l\u2208U |\u2206kl |)2\nC\n+\nN\n\u03bb4 \u03bb\u22172\n\nmax\n\n(k,l,k0 ,l0 )\u2208D4,n\n\n|Ep {(1kl \u2212 \u03c0kl )(1k0 l0 \u2212 \u03c0k0 l0 )}|\u03b65\n\nso that a1 \u2192 0 and a3 \u2192 0 when N \u2192 \u221e. Then, the Cauchy-Schwarz inequality\nallows us to get that a2 \u2192 0 when N \u2192 \u221e and Ep (A1 (r, t)2 ) \u2192 0 when N \u2192 \u221e.\nLet us show now that Ep (|A4 (r, t)|) \u2192 0 when N \u2192 \u221e. With Lemma A.4,\nand assumptions (A1)-(A5), we have\n1 X X |\u2206kl | 1\nb (t)kk\u03b2\u0303(r) \u2212 \u03b2\nb (r)k\nEp (|A4 (r, t)|) \u2264 nEp\nkxk kkxl kk\u03b2\u0303(t) \u2212 \u03b2\na\na\nN2\n\u03c0k \u03c0l \u03c0kl\nk\u2208U l\u2208U\n\u0014\n\u0015\n1\nn\nn maxk6=l\u2208U |\u2206kl |\n\u2264\n+\nC 4 \u03b61\nn \u03bb2 N\n\u03bb2 \u03bb\u2217\nso that Ep (|A4 (r, t)|) \u2192 0 when N \u2192 \u221e.\n\n!\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n30\n\nIn a similar way, we can bound Ep (|A2 (r, t)|) as follows,\nEp (|A2 (r, t)|) \u2264\n\nn X X |\u2206kl | 1\nEp |\u1ebdk (t)b\u0303\nel (r)|\nN2\n\u03c0k \u03c0l \u03c0kl\nk\u2208U l\u2208U\n\nn X X |\u2206kl | kxl k\nb (r)k)\n|Yk (t) \u2212 \u1ef8k (t)| * Ep (k\u03b2\u0303(r) \u2212 \u03b2\na\nN2\n\u03c0k \u03c0l \u03c0kl\nk\u2208U l\u2208U\n\u0012\u221a\n\u0013\n\u221a\nX\nn\nn maxk6=l\u2208U |\u2206kl |\n1/2 1/2 1\n\u2264\nC4 \u03b61\n+\n|Yk (t) \u2212 \u1ef8k (t)|,\n2\n2\n\u2217\n\u03bb N\n\u03bb \u03bb\nN\n\n\u2264\n\nk\u2208U\n\nwhere b\u0303\nek (t) = \u1ef8k (t) \u2212 Ybk,a (t) = x0k (\u03b2\u0303(t) \u2212 \u03b2\u0302 a (t)). Thus, there is constant C such\nthat,\nC\nEp (|A2 (r, t)|) \u2264 \u221a\nn\nand Ep (|A2 (r, t)|) \u2192 0 when N \u2192 \u221e. We can show in a similar way that\nEp (|A3 (r, t)|) \u2192 0 when N \u2192 \u221e.\nFinally, we have that for all (r, t) \u2208 [0, T ]2 ,\nnEp {| \u03b3\nbMA,a (r, t) \u2212 \u03b3MA (r, t) |} \u2192 0,\n\nwhen N \u2192 \u221e.\n\n(38)\n\nStep 2. Uniform convergence of the variance estimator\nThe pointwise convergence of the variance function proved in the previous step\nclearly implies the convergence of all finite linear combinations : for all p \u2208\n{1, 2, . . .}, for all (c1 , . . . , cp ) \u2208 Rp and for all (t1 , . . . , tp ) \u2208 [0, T ]p , we have\np\nX\n\nc` n (b\n\u03b3MA,a (t` , t` ) \u2212 \u03b3MA (t` , t` )) \u2192 0\n\n(39)\n\n`=1\n\nin probability as N tends to infinity. Thus, we deduce with the Cramer-Wold device that the vector n (b\n\u03b3MA,a (t1 , t1 ) \u2212 \u03b3MA (t1 , t1 ), . . . , \u03b3\nbMA,a (tp , tp ) \u2212 \u03b3MA (tp , tp ))\nconverges in distribution to 0 (in Rp ).\nWe need now to prove that the sequence of random functions \u03b3\nbMA,a (t, t)\nis tight in C[0, T ] by using a bound on its increments. Let us introduce the\nfollowing criterion,\nd2\u03b3 (t, r) = n2 Ep (|b\n\u03b3MA,a (t, t) \u2212 \u03b3MA (t, t) \u2212 \u03b3\nbMA,a (r, r) + \u03b3MA (r, r)|2 ).\nTo conclude we show in the following that d2\u03b3 (t, r) \u2264 C|t \u2212 r|2\u03b2 for a constant C\nand all (r, t) \u2208 [0, T ]2 . Using (36), the distance is decomposed into four parts.\nLet us define \u03c6kl (t, r) = \u1ebdk (t)\u1ebdl (t) \u2212 \u1ebdk (r)\u1ebdl (r) and first consider d2A1 =\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n31\n\nEp (|A1 (t, t) \u2212 A1 (r, r)|2 ). We have\n#\n\"\n\u0012\n\u0013\n\u0012\n\u0013\n1 \u2212 \u03c0k 0 1k 0\nn 2 X X 1 \u2212 \u03c0k 1k\n2\ndA1 = Ep\n\u22121\n\u2212 1 \u03c6kk (t, r)\u03c6k0 k0 (t, r)\nN4\n\u03c0k\n\u03c0k\n\u03c0k 0\n\u03c0k 0\nk\u2208U k0 \u2208U\n\uf8f9\n\uf8ee\n\u0012\n\u0013\n\u0012\n\u0013\n2 X X\n0\n0\n0\n0\n1k l\n1 \u2212 \u03c0k 1 k\n\u2206k l\nn\n\u2212 1 \u03c6kk (t, r)\u03c6k0 l0 (t, r)\uf8fb\n\u22121\n+ 2Ep \uf8f0 4\n0 \u03c0l 0\nN\n\u03c0\n\u03c0\n\u03c0\n\u03c0k 0 l 0\nk\nk\nk\nk\u2208U k0 6=l0 \u2208U\n\uf8ee\n\uf8f9\n\u0012\n\u0013\n\u0012\n\u0013\n2 X\nX\n0\n0\n0\n0\nn\n1k l\n\u2206kl 1kl\n\u2206k l\n+ Ep \uf8f0 4\n\u2212 1 \u03c6kl (t, r)\u03c6k0 l0 (t, r)\uf8fb\n\u22121\n0 \u03c0l 0\nN\n\u03c0\n\u03c0\n\u03c0\n\u03c0\n\u03c0k 0 l 0\nk\nl\nkl\nk\n0\n0\nk6=l\u2208U k 6=l \u2208U\n\n:= b1 + b2 + b3\n\n(40)\n\nThanks to Lemma A.8, we get\n\u0013\n\u0012 2\nn2 maxk6=k0 \u2208U |\u2206kk0 | 1 X\nn 1\n|\u03c6kk (t, r)|2\n+ 2\nb1 \u2264\nN 3 \u03bb3\nN\n\u03bb4\nN\nk\u2208U\n\n\u2264 C|t \u2212 r|2\u03b2\n\n(41)\n\nand\nb3 \u2264\n\nC\n(n maxk6=l\u2208U |\u2206kl |)\n|t \u2212 r|2\u03b2 +\nN\n\u03bb4 \u03bb\u22172\n\n\uf8f62\n\n\uf8eb\n\n2\n\nmax\n\n(k,l,k0 ,l0 )\u2208D4,n\n\n|Ep {(1kl \u2212 \u03c0kl )(1k0 l0 \u2212 \u03c0k0 l0 )}| \uf8ed\n\n\u2264 C|t \u2212 r|2\u03b2 .\n\n1\nN2\n\n(42)\n\nThe Cauchy-Schwarz inequality together with bounds (41) and (42) allows\nus to get b2 \u2264 C|t \u2212 r|2\u03b2 so that\nd2A1 \u2264 C|t \u2212 r|2\u03b2 .\n\n(43)\n\nLet us bound now d2A2 = Ep (|A2 (t, t) \u2212 A2 (r, r)|2 ) and define \u03c6\u0303kl (t, r) =\n\u1ebdk (t)b\u0303\nel (t) \u2212 \u1ebdk (r)b\u0303\nel (r). Thanks to Lemma A.9, we get\n\uf8eb\n\uf8f62\n!2\n2\n2\n2\nX\nX\n2n\nmax\n|\u2206\n|\n1\n2n\n1\nk6=l\u2208U\nkl\nd2A2 \u2264 2 4 Ep\n\u03c6\u0303kk (t, r) +\nEp \uf8ed 2\n|\u03c6\u0303k,l (t, r)|\uf8f8\nN \u03bb\nN\n\u03bb4 \u03bb\u22172\nN\nk\u2208U\n\nk,l\u2208U\n\n2\u03b2\n\n\u2264 C|t \u2212 r| .\n\n(44)\n\nLet us study now the last term, d2A4 = Ep (|A4 (t, t) \u2212 A4 (r, r)|2 ) and define\nb\u0303\n\u03c6kl (t, r) = b\u0303\nek (t)b\u0303\nel (t) \u2212 b\u0303\nek (r)b\u0303\nel (r). Thanks to Lemma A.7, we have\nd2A4\n\n2n2\n\u2264 2 4 Ep\nN \u03bb\n2\u03b2\n\n\u2264 C|t \u2212 r| .\n\n\uf8eb\n\uf8f62\n!2\n1 X b\u0303\n2n2 maxk6=l\u2208U |\u2206kl |2 \uf8ed 1 X b\u0303\n\u03c6kk (t, r) +\nEp\n|\u03c6k,l (t, r)|\uf8f8\nN\n\u03bb4 \u03bb\u22172\nN2\nk\u2208U\n\nk,l\u2208U\n\n(45)\n\nX\nk,l\u2208U\n\n|\u03c6kl (t, r)|\uf8f8\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n32\n\nFinally, we can deduce, with inequalities (36), (43), (44) and (45), that\nd2\u03b3 (t, r) = n2 Ep (|b\n\u03b3MA,a (t, t) \u2212 \u03b3MA (t, t) \u2212 \u03b3\nbMA,a (r, r) + \u03b3MA (r, r)|2 )\n\u2264 C|t \u2212 r|2\u03b2 .\n\n(46)\n\nThe end of the proof is a direct application of Theorem 12.3 of Billingsley\n(1968). Since \u03b2 > 1/2, the sequence n(b\n\u03b3MA,a (t, t)\u2212\u03b3MA (t, t)) is tight in C([0, T ])\nand converges in distribution to 0. The proof is complete with a direct application of the definition of weak convergence in C([0, T ]) considering the bounded\nand continuous \"sup\" functional.\n\u0003\nA.4. Proofs related to the asymptotic normality and the confidence\nbands\nThe steps of the proof of Proposition 3.4 are similar to the steps of the proof\nof Proposition 3.3. We first examine the finite combinations and invoke the\nCramer-Wold device. Then we prove the tightness thanks to inequalities on the\nincrements.\nLet us first deal with the interpolation error, which is negligible under the\nassumption on the grid of discretization points, as shown in (26).\nThen, in light of (10), Lemma A.2 and Lemma A.4, we clearly have that, for\neach value of t,\n\u221a\nn (b\n\u03bcMA,a (t) \u2212 \u03bc\ne(t)) = op (1),\nand consequently, as n tends to infinity,\n\u221a\nn (b\n\u03bcMA,a (t) \u2212 \u03bc(t)) \u2192 N (0, \u03b3Z (t, t)) in distribution,\nwhere the covariance-function of \u03bc\ne, which defined in (12), satisfies limN \u2192\u221e n\u03b3MA =\n\u03b3Z .\nIf we now consider p distinct discretization instants\u0010 0 \u2264 t1 < t2 . . . < tp \u2264 1,\n\u0011 it\n\u221a Pp\np\nis immediate to check that for any vector c \u2208 R , n\n\u03bc(tj ) \u2212 \u03bc(tj )) \u2192\nj=1 cj (e\nN (0, \u03c3c2 ) where\n\u03c3c2 =\n\np X\np\nX\n\ncj c` \u03b3Z (tj , t` ).\n\nj=1 `=1\n\nIndeed, by linearity, there exists a vector of random weights (w1 , . . . , wN ) which\ndoes not depend on time t such that\nX\n\u03bc\ne(t) =\nwk Yk (t),\nk\u2208U\n\nand\n\nPp\n\ne(tj ) =\nj=1 cj \u03bc\n\ntotic variance\n\n\u03c3c2 ,\n\nP\n\nk\u2208U wk\n\n\u0010P\n\np\nj=1 cj Yk (tj )\n\n\u0011\n\nalso satisfies a CLT, with asymp-\n\nunder the moment conditions (A7). Thus, any finite linear\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n33\n\ncombination\nis asymptotically Gaussian and we can conclude that the vec\u221a\n\u03bc(t1 ) \u2212 \u03bc(t1 ), . . . , \u03bc\ne(tp ) \u2212 \u03bc(tp )) is asymptotically Gaussian with the\ntor n (e\nCramer-Wold device.\nIt remains to check the tightness of the functional process\n\u221a and this is a direct\n\u03bcMA,a (t) \u2212 \u03bc(t)) ,\nconsequence of (30) and (35). Indeed, denoting by Zn (t) = n (b\nthere is a constant C such that, for all (r, t) \u2208 [0, T ]2 ,\n\u0010\n\u0011\n2\n2\u03b2\nEp [Zn (t) \u2212 Zn (r)] \u2264 C |t \u2212 r| ,\nand, since \u03b2 > 1/2, the sequence Zn is tight in C[0, T ], in view of Theorem 12.3\nof Billingsley (1968).\n\u0003\nWe prove now Proposition 3.5, the last result of the paper. The proof consists\nin showing the weak convergence of the sequence of distributions (ZbN ) to the\nlaw of Z in C([0, T ]).\nFor any vector of p points 0 \u2264 t1 < . . . < tp \u2264 T, the finite dimensional\nconvergence of the distribution of the Gaussian vector (ZbN (t1 ), . . . , ZbN (tp )) to\nthe distribution of (Z(t1 ), . . . , Z(tp )) is an immediate consequence of the uniform convergence of the covariance function stated in Proposition 3.3. We can\nconclude with Slutsky's Lemma noting that for any (c1 , . . . , cp ) \u2208 Rp ,\np X\np\nX\n\ncj c` \u03b3\nbMA,d (tj , t` ) \u2192\n\nj=1 `=1\n\np X\np\nX\n\ncj c` \u03b3MA (tj , t` ) in probability.\n\n(47)\n\nj=1 `=1\n\nbN ) in C([0, T ]). Given \u03b3\nNow, we need to check the tightness of (Z\nbMA,d , we\n2\nhave for (r, t) \u2208 [0, T ] ,\n\u0015\n\u0014\u0010\n\u00112\nbMA,d = n (b\nEp ZbN (t) \u2212 ZbN (r) | \u03b3\n\u03b3MA,d (t, t) \u2212 2b\n\u03b3MA,d (r, t) + \u03b3\nbMA,d (r, r))\nand after some algebra, we obtain thanks to Assumption (A2) that\n\u0014\u0010\n\u0015\n\u0014\n\u00112\n\u0010\n\u00112 \u0015\nC X\n2\nbN (r) |b\nEp ZbN (t) \u2212 Z\n\u03b3MA,d \u2264\n(Yk,d (t) \u2212 Yk,d (r)) + Ybk,d (t) \u2212 Ybk,d (r)\n.\nN\nk\u2208U\n\n(48)\n2\n\nP\n\nLet us first study the term k\u2208U (Yk,d (t) \u2212 Yk,d (r)) in the previous inequality and without loss of generality suppose that t > r. To check the continuity\nof the trajctories, we only need to consider points r and t that are close to each\nother. If t and r belong to the same interval, say [ti , ti+1 ], then it is easy to\ncheck, with Assumption (A4) that\n(t \u2212 r)2 1 X\n1 X\n2\n2\n(Yk,d (t) \u2212 Yk,d (r)) =\n(Yk (ti+1 ) \u2212 Yk (ti ))\nN\n(ti+1 \u2212 ti )2 N\nk\u2208U\n\nk\u2208U\n\n2\u03b2\n\n\u2264 C(t \u2212 r) .\n\n(49)\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n34\n\nIf we suppose now that r \u2208 [ti\u22121 , ti ] and t \u2208 [ti , ti+1 ], then we have\n\u0012\n\u0013\n|Yk,d (t) \u2212 Yk,d (r)|\n|Yk (ti+1 ) \u2212 Yk (ti )| |Yk (ti ) \u2212 Yk (ti\u22121 )|\n\u2264 max\n,\nt\u2212r\nti+1 \u2212 ti\nti \u2212 ti\u22121\n|Yk (ti+1 ) \u2212 Yk (ti )| |Yk (ti ) \u2212 Yk (ti\u22121 )|\n\u2264\n+\nti+1 \u2212 ti\nti \u2212 ti\u22121\nand using the same decomposition as in (49), we directly get that\nX\n2\n(Yk,d (t) \u2212 Yk,d (r)) \u2264 C(t \u2212 r)2\u03b2 .\nk\u2208U\n\nThe second term at the right-hand side of inequality (48) is dealt with similar\narguments and the decomposition used in the proof of Lemma A.3, so that\n\u0010\n\u00112\nP\n1\nYbk,d (t) \u2212 Ybk,d (r) \u2264 C|t \u2212 r|2\u03b2 .\nN\n\nk\u2208U\n\nThus, the trajectories of the Gaussian process are continuous on [0, T ] whenever \u03b2 > 0 (see e.g Theorem 1.4.1 in Adler and Taylor (2007)) and the sequence\nbN ) converges weakly to Z in C([0, T ]) equipped with the supremum norm.\n(Z\nUsing again Proposition 3.3, we have, uniformly in t, \u03c3\nbZ (t) = \u03c3Z (t) + op (1),\n2\n2\n(t) = \u03b3Z (t, t) is a continu(t) = nb\n\u03b3MA,d (t, t). Since, by hypothesis \u03c3Z\nwhere \u03c3\nbZ\nous function and inf t \u03b3Z (t, t) > 0, we get with Slutsky's lemma that (ZbN /b\n\u03c3Z )\nconverges weakly to Z/\u03c3Z in C([0, T ]). By definition of the weak convergence\nin C([0, T ]) and the continuous mapping theorem, we also deduce that the real\ncN = supt\u2208[0,T ] |ZbN (t)|/b\nrandom variable M\n\u03c3Z (t) converges in distribution to\nM = supt\u2208[0,T ] |Z(t)|/\u03c3Z (t), so that for each c \u2265 0,\n!\nP\n\nsup |ZbN (t)|/b\n\u03c3Z (t) \u2264 c\nt\u2208[0,T ]\n\n!\n\u2192P\n\nsup |Z(t)|/\u03c3Z (t) \u2264 c .\nt\u2208[0,T ]\n\nNote finally, that under the previous hypotheses on \u03b3Z (see e.g. Pitt and\nTran (1979)), the real random variable M = supt\u2208[0,T ] (|Z(t)|/\u03c3Z (t)) has an\nabsolutely continuous and bounded density function so that the convergence\nholds uniformly in c (see e.g. Lemma 2.11 in van der Vaart (1998)).\n\u0003\nA.5. Some useful lemmas\nWe state here without any proof some results that are needed for the study of the\nconvergence of the covariance function. They rely on applications of the CauchySchwarz inequality and on the assumptions on the moments of the trajectories\nand the inclusion probabilities.\nLemma A.6. Assume (A2)-(A5) and (A7) hold. There are two constants \u03b64\nand \u03b65 such that\n1 X\n\u1ebdk (t)2 \u1ebdk (r)2 \u2264 \u03b64\nN\nk\u2208U\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\nand\n\n35\n\n1 XX\n\u1ebdk (t)2 \u1ebdl (r)2 \u2264 \u03b65 ,\nN2\nk\u2208U l\u2208U\n\nwhere \u1ebdk (t) = Yk (t) \u2212 \u1ef8k (t).\nLemma A.7. Assume (A2)-(A5) and (A7) hold. There are two constants \u03b66\nand \u03b67 such that\n!\n1 X b\u0303\n2\n\u03c6kk (t, r)\nEp\n\u2264 \u03b66 |t \u2212 r|2\u03b2\nN\nk\u2208U\n\nand\n\n\uf8f62\nX b\u0303\n1\nEp \uf8ed 2\n\u03c6kl (t, r)\uf8f8 \u2264 \u03b67 [|t \u2212 r|2\u03b2\nN\n\uf8eb\n\nk,l\u2208U\n\nb\u0303\nwhere \u03c6kl (t, r) = b\u0303\nek (t)b\u0303\nel (t) \u2212 b\u0303\nek (r)b\u0303\nel (r) and b\u0303\nek (t) = \u1ef8k (t) \u2212 Ybk,a (t).\nLemma A.8. Assume (A2)-(A5) and (A7) hold. There are two constant constants \u03b68 and \u03b69 such that\n1 X 2\n\u03c6kk (t, r) \u2264 \u03b68 |t \u2212 r|2\u03b2\nN\nk\u2208U\n\nand\n\n\uf8f62\nX\n1\n\uf8ed\n\u03c6kl (t, r)\uf8f8 \u2264 \u03b69 |t \u2212 r|2\u03b2\nN2\n\uf8eb\n\nk,l\u2208U\n\nwhere \u03c6kl (t, r) = \u1ebdk (t)\u1ebdl (t) \u2212 \u1ebdk (r)\u1ebdl (r) and \u1ebdk (t) = Yk (t) \u2212 \u1ef8k (t).\nLemma A.9. Assume (A2)-(A5) and (A7) hold. There are two constants \u03b610\nand \u03b611 such that\n!\n1 X\n2\nEp\n\u03c6\u0303kk (t, r)\n\u2264 \u03b610 |t \u2212 r|2\u03b2\nN\nk\u2208U\n\nand\n\n\uf8f62\nX\n1\nEp \uf8ed 2\n\u03c6\u0303kl (t, r)\uf8f8 \u2264 \u03b611 |t \u2212 r|2\u03b2\nN\n\uf8eb\n\nk,l\u2208U\n\nwhere \u03c6\u0303kl (t, r) = \u1ebdk (t)b\u0303\nel (t) \u2212 \u1ebdk (r)b\u0303\nel (r), \u1ebdk (t) = Yk (t) \u2212 \u1ef8k (t) and b\u0303\nek (t) =\nb\n\u1ef8k (t) \u2212 Yk,a (t).\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n36\n\nReferences\nAdler, R. J. and Taylor, J. E. (2007). Random Fields and Geometry. SpringerVerlag, New York.\nBhatia, R. (1997). Matrix Analysis. Springer-Verlag, New York.\nBillingsley, P. (1968). Convergence of Probability Measures. John Wiley and\nSons.\nBoistard, H., Lopuha\u00e4, H. P., and Ruiz-Gazen, A. (2012). Approximation of\nrejective sampling inclusion probabilites and application to higher order correlation. Electronic J. of Statistics, 6:1967\u20131983.\nBosq, D. (2000). Linear Processes in Function Spaces: Theory and Applications,\nvolume 149 of Lecture notes in Statistics. Springer-Verlag, New York.\nBreidt, F. J. and Opsomer, J. D.(2000). Local polynomial regression estimators\nin survey sampling. Ann. Statist., 28(4):1023\u20131053.\nCallado, A., Kamienski, C., Szab\u00f3, G., Ger\u00f6, B., Kelner, J., Fernandes, S. and\nSadok, D. (2009). A Survey on Internet Traffic Identification and Classification. IEEE Communications Surveys and Tutorials, 11:37\u201352.\nCardot, H. (2007). Conditional functional principal components analysis. Scandinavian J. of Statistics, 34:317\u2013335.\nCardot, H., Chaouch, M., Goga, C., and Labru\u00e8re, C. (2010). Properties of\ndesign-based functional principal components analysis. J. of Statistical Planning and Inference, 140:75\u201391.\nCardot, H., Degras, D., and Josserand, E. (2012a). Confidence bands for\nHorvitz-Thompson estimators using sampled noisy functional data. To appear\nin Bernoulli.\nCardot, H., Dessertaine, A., Goga, C., Josserand, E., and Lardin, P. (2012b).\nComparaison de diff\u00e9rents plans de sondage et construction de bandes\nde confiance pour l'estimation de la moyenne de donn\u00e9es fonctionnelles :\nune illustration sur la consommation \u00e9lectrique. To appear in Techniques\nd'Enqu\u00eates/Survey Methodology.\nCardot, H., Goga, C., and Lardin, P. (2012c). Variance estimation and asymptotic confidence bands for the mean estimator of sampled functional data with\nhigh entropy unequal probability sampling designs. arXiv:1209.6503\nCardot, H. and Josserand, E. (2011). Horvitz-Thompson estimators for functional data: asymptotic confidence bands and optimal allocation for stratified\nsampling. Biometrika, 98:107\u2013118.\nChiou, J., M\u00fcller, H., and Wang, J. (2004). Functional response models. Statistica Sinica, 14:675\u2013693.\nCuevas, A., Febrero, M., and Fraiman, R. (2006). On the use of the bootstrap\nfor estimating functions with functional data. Computational Statistics and\nData Analysis, 51:1063\u20131074.\nDegras, D. (2011). Simultaneous confidence bands for parametric regression\nwith functional data. Statistica Sinica, 21(4):1735\u20131765.\nDegras, D. (2012).\nRotation sampling for functional data.\nhttp://arxiv.org/abs/1204.4494.\nDeville, J. C. and S\u00e4rndal, C. E. (1992). Calibration estimators in survey sam-\n\n\fH. Cardot, C. Goga and P. Lardin/Model-assisted estimators for functional data\n\n37\n\npling. J. Amer. Statist. Assoc., 87:376\u2013382.\nFaraway, J. (1997). Regression analysis for a functional response. Technometrics,\n39(3):254\u2013261.\nFerraty, F., Laksaci, A., Tadj, A., and Vieu, P. (2011). Kernel regression with\nfunctional response. Electronic J. of Statist., 5:159\u2013171.\nFuller, W. A. (2009). Sampling Statistics. John Wiley and Sons, Hoboken, New\nJersey.\nGuillas, S. (2001). Rates of convergence of autocorrelation estimates for autoregressive Hilbertian processes. Statist. and Probability Letters, 55:281\u2013291.\nHahn, M. (1977). Conditions for sample-continuity and the central limit theorem. Annals of Probability, 5:351\u2013360.\nH\u00e1jek, J. (1981). Sampling From a Finite Population. Statistics: Textbooks and\nMonographs. Marcel Dekker, New York.\nIsaki, C. and Fuller, W. (1982). Survey design under the regression superpopulation model. J. Amer. Statist. Assoc., 77:49\u201361.\nPitt, L. D. and Tran, L. T. (1979). Local sample path properties of Gaussian\nfields. Annals of Probability, 7:477\u2013493.\nRamsay, J. O. and Silverman, B. W. (2005). Functional Data Analysis. SpringerVerlag, New York, second edition.\nRobinson, P. and S\u00e4rndal, C. (1983). Asymptotic properties of the generalized\nregression estimator in probability sampling. Sankhya : The Indian Journal\nof Statistics, 45:240\u2013248.\nS\u00e4rndal, C. (1980). On \u03c0 inverse weighting versus best linear unbiased weighting\nin probability sampling. Biometrika, 67:639\u201350.\nS\u00e4rndal, C. E., Swensson, B., and Wretman, J. (1992). Model Assisted Survey\nSampling. Springer series in statistics. Springer-Verlag, New York.\nvan der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press.\nvan der Vaart, A. W. and Wellner, J. A. (2000). Weak Convergence and Empirical Processes. With Applications to Statistics. Springer-Verlag, New York.\n\n\f"}