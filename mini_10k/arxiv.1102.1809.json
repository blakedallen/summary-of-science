{"id": "http://arxiv.org/abs/1102.1809v1", "guidislink": true, "updated": "2011-02-09T08:26:25Z", "updated_parsed": [2011, 2, 9, 8, 26, 25, 2, 40, 0], "published": "2011-02-09T08:26:25Z", "published_parsed": [2011, 2, 9, 8, 26, 25, 2, 40, 0], "title": "Generalized companion matrix for approximate GCD", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1102.1809%2C1102.1406%2C1102.0705%2C1102.0901%2C1102.1909%2C1102.4338%2C1102.0539%2C1102.1217%2C1102.5268%2C1102.0828%2C1102.2064%2C1102.5283%2C1102.5710%2C1102.2868%2C1102.2872%2C1102.0184%2C1102.0289%2C1102.0336%2C1102.0829%2C1102.2996%2C1102.1575%2C1102.1307%2C1102.2237%2C1102.3639%2C1102.3960%2C1102.4066%2C1102.2342%2C1102.5490%2C1102.1986%2C1102.3015%2C1102.1906%2C1102.1375%2C1102.1553%2C1102.0715%2C1102.3672%2C1102.3795%2C1102.3031%2C1102.5084%2C1102.5073%2C1102.5695%2C1102.3988%2C1102.4888%2C1102.3241%2C1102.1557%2C1102.0396%2C1102.0537%2C1102.4624%2C1102.0169%2C1102.1053%2C1102.3090%2C1102.5177%2C1102.0841%2C1102.2001%2C1102.2416%2C1102.5198%2C1102.1947%2C1102.4847%2C1102.0989%2C1102.0315%2C1102.4839%2C1102.4698%2C1102.4107%2C1102.3850%2C1102.1016%2C1102.4812%2C1102.2461%2C1102.4555%2C1102.0772%2C1102.5450%2C1102.2337%2C1102.4780%2C1102.2609%2C1102.2446%2C1102.1258%2C1102.1105%2C1102.4579%2C1102.1380%2C1102.4381%2C1102.0860%2C1102.1577%2C1102.2899%2C1102.4996%2C1102.2565%2C1102.3598%2C1102.2518%2C1102.2718%2C1102.4348%2C1102.1355%2C1102.1130%2C1102.1739%2C1102.5724%2C1102.3402%2C1102.2209%2C1102.4087%2C1102.1502%2C1102.5571%2C1102.2514%2C1102.0283%2C1102.4245%2C1102.2650%2C1102.2441&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Generalized companion matrix for approximate GCD"}, "summary": "We study a variant of the univariate approximate GCD problem, where the\ncoefficients of one polynomial f(x)are known exactly, whereas the coefficients\nof the second polynomial g(x)may be perturbed. Our approach relies on the\nproperties of the matrix which describes the operator of multiplication by gin\nthe quotient ring C[x]=(f). In particular, the structure of the null space of\nthe multiplication matrix contains all the essential information about GCD(f;\ng). Moreover, the multiplication matrix exhibits a displacement structure that\nallows us to design a fast algorithm for approximate GCD computation with\nquadratic complexity w.r.t. polynomial degrees.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1102.1809%2C1102.1406%2C1102.0705%2C1102.0901%2C1102.1909%2C1102.4338%2C1102.0539%2C1102.1217%2C1102.5268%2C1102.0828%2C1102.2064%2C1102.5283%2C1102.5710%2C1102.2868%2C1102.2872%2C1102.0184%2C1102.0289%2C1102.0336%2C1102.0829%2C1102.2996%2C1102.1575%2C1102.1307%2C1102.2237%2C1102.3639%2C1102.3960%2C1102.4066%2C1102.2342%2C1102.5490%2C1102.1986%2C1102.3015%2C1102.1906%2C1102.1375%2C1102.1553%2C1102.0715%2C1102.3672%2C1102.3795%2C1102.3031%2C1102.5084%2C1102.5073%2C1102.5695%2C1102.3988%2C1102.4888%2C1102.3241%2C1102.1557%2C1102.0396%2C1102.0537%2C1102.4624%2C1102.0169%2C1102.1053%2C1102.3090%2C1102.5177%2C1102.0841%2C1102.2001%2C1102.2416%2C1102.5198%2C1102.1947%2C1102.4847%2C1102.0989%2C1102.0315%2C1102.4839%2C1102.4698%2C1102.4107%2C1102.3850%2C1102.1016%2C1102.4812%2C1102.2461%2C1102.4555%2C1102.0772%2C1102.5450%2C1102.2337%2C1102.4780%2C1102.2609%2C1102.2446%2C1102.1258%2C1102.1105%2C1102.4579%2C1102.1380%2C1102.4381%2C1102.0860%2C1102.1577%2C1102.2899%2C1102.4996%2C1102.2565%2C1102.3598%2C1102.2518%2C1102.2718%2C1102.4348%2C1102.1355%2C1102.1130%2C1102.1739%2C1102.5724%2C1102.3402%2C1102.2209%2C1102.4087%2C1102.1502%2C1102.5571%2C1102.2514%2C1102.0283%2C1102.4245%2C1102.2650%2C1102.2441&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study a variant of the univariate approximate GCD problem, where the\ncoefficients of one polynomial f(x)are known exactly, whereas the coefficients\nof the second polynomial g(x)may be perturbed. Our approach relies on the\nproperties of the matrix which describes the operator of multiplication by gin\nthe quotient ring C[x]=(f). In particular, the structure of the null space of\nthe multiplication matrix contains all the essential information about GCD(f;\ng). Moreover, the multiplication matrix exhibits a displacement structure that\nallows us to design a fast algorithm for approximate GCD computation with\nquadratic complexity w.r.t. polynomial degrees."}, "authors": ["Paola Boito", "Olivier Ruatta"], "author_detail": {"name": "Olivier Ruatta"}, "author": "Olivier Ruatta", "arxiv_comment": "Submitted to MEGA 2011", "links": [{"href": "http://arxiv.org/abs/1102.1809v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1102.1809v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "14Q20, 13P05, 68W25, 68W30", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "G.0; G.1.2; G.1.3; I.1.2", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1102.1809v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1102.1809v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:1102.1809v1 [cs.SC] 9 Feb 2011\n\nGeneralized companion matrix for approximate\nGCD\nPaola Boito \u2217\nXLIM-DMI UMR 6172 Universit\u00e9 de Limoges - CNRS\nOlivier Ruatta\u2020\nXLIM-DMI UMR 6172 Universit\u00e9 de Limoges - CNRS\nOctober 24, 2018\nWe study a variant of the univariate approximate GCD problem, where the coefficients of one polynomial f (x)are known exactly, whereas the coefficients of the second\npolynomial g(x)may be perturbed. Our approach relies on the properties of the matrix\nwhich describes the operator of multiplication by gin the quotient ring C[x]/(f ). In\nparticular, the structure of the null space of the multiplication matrix contains all the\nessential information about GCD(f, g). Moreover, the multiplication matrix exhibits\na displacement structure that allows us to design a fast algorithm for approximate\nGCD computation with quadratic complexity w.r.t. polynomial degrees.\n\n1\n\nIntroduction\n\nThe approximate polynomial greatest common divisor (denoted as AGCD) is\na central object of symbolic-numeric computation. The main difficulty of the\nproblem comes from the fact that is no universal notion of AGCD. One can\nfind different approaches and different notions for AGCD. We will not give a\nreview of all the existing work on this subject, but we will recall one of the most\npopular approaches to show how our work brings a different point of view on\nthe problem.\nThe main approach to the computation of an AGCD consists in considering\ntwo univariate polynomials whose coefficients are known with uncertainty. This\nuncertainty can be the result of the fact that the polynomials have floating point\ncoefficients coming from previous computation (and so are subject to round-off\nerrors). The most frequently adopted formulation is related to semi-algebraic\noptimization : given f \u0303 and g\u0303 two approximate polynomials, find two polynomials\nf and g such that kf \u0303\u2212 f k and kg\u0303 \u2212 gk are small (lower than a given tolerance for\ninstance) and such that the degree of gcd(f, g) is maximal. That is, one looks\n\u2217 Email:\n\u2020 Email:\n\npaola.boito@unilim.fr; Web: http://www.unilim.fr/pages perso/paola.boito/index en.html\nolivier.ruatta@unilim.fr\n\n1\n\n\ffor the most singular system close to the input (f \u0303, g\u0303). An \u03b5-gcd is obtained\nif the conditions kf \u0303 \u2212 f k < \u03b5 and kg\u0303 \u2212 gk < \u03b5 are satisfied. One can try to\ncompute the tolerance on the perturbation of the input polynomial thanks to\ndirect computation (for instance from a jump on singular values of particular\nmatrix for instance). This last approach has received a great interest following\nthe work of Zeng using Sylvester like matrices ([14]).\nHere, we consider a slightly different problem. One of the polynomials, say\nf , is known exactly (it is the result of an exact model) and the second one, say g,\nis an approximate polynomial (result of measures or previous approximation for\ninstance). This case occurs in applications such as model checking (to compare\nresults of an exact model and measures). There are many other instances of\nsuch a problem, such as simplification of fractions when one of the polynomial\nis known exactly but the other one is not.\nWe give a example of such a situation. When modeling an electromagnetic\nfilter, one might want to parametrize its behavior with respect to the frequency.\nBut one may need to do so even if there are singularities and to do so one may\nuse Pad\u00e9 approximations of the electromagnetic signal at each point as a function of the frequency. In some cases of interest, one can know all the singularities\nand so compute an exact polynomial called characteristic. Pad\u00e9 approximations\nare computed independently for each point by a numerical process and denominators may have a non trivial gcd with the \"characteristic\" polynomial. The\ndenominators are not known exactly. So, in order to identify unwanted common\nfactors in denominators one has to compute approximate gcds between an exact\nand non exact polynomials.\nThis AGCD problem can also be interpreted as an optimization problem.\nGiven f exactly and g\u0303 approximately, compute a polynomial g close to g\u0303 such\nthat g has a maximal degree gcd with f . Our approach takes advantage of\nthe asymmetry of the problem and of the structure of the quotient algebra\nC[x]/(f (x)) (more accurately, of the displacement rank of the multiplication\noperator in this algebra). So, we address the following problem :\nProblem 1 Let f (x) \u2208 C[x] a given polynomial and g(x) another polynomial. Find\ng\u0303(x) close to g(x) (in a sense that will be explained) such that f (x) and g\u0303(x) have a\ngcd of maximal degree.\n\nThis may be also an interesting approach when one has two polynomials, one\nknown with high confidence and another with worse accuracy. This approach\nmay take advantage of this asymmetry which would not be possible for classical\nframework based on Sylvester or B\u00e9zout matrices.\nIn this paper, we propose an approach and an algorithm to address this\nproblem. The proposed algorithm is \"fast\" since the exponent of its complexity\nis better than the classical linear algebra exponent in the degree of the input\npolynomials.\nOrganisation of the paper: The second section is devoted to some basic result on algebra needed after, the third section gives an algebraic method for gcd\nbased on linear algebra, the fourth section recalls the Barnett formula allowing\n\n2\n\n\fto compute the multiplication matrix without division, the fifth gives the displacement rank structure of the multiplication matrix, the sixth describes the\nfinal algorithm and experiments before finishing with conclusions and perpectives.\n\n2\n\nEuclidian structure and quotient algebra\n\nIn this section, we recall basic algebraic results allowing to understand the\nprinciple of our approach. All material in this section can be found (even in the\nnon reduced case and in the multivariate setting) in [11].\nAssume that K is an algebraically closed field (here we think about C).\nd\nQ\n(x \u2212 \u03b6i ) and that\nLet f (x) and g(x) \u2208 K[x] and assume that f (x) = fd \u2217\ni=1\n\n\u03b6i 6= \u03b6j for all i 6= j in {1, . . . , d}. Let A = K[x]/(f ) and \u03c0 :QK[x] \u2212\u2192 A be\n(x\u2212\u03b6j )\n\nthe natural projection. For i \u2208 {1, . . . , d}, we define Li (x) =\n\nj6=i\n\nQ\n\n(\u03b6i \u2212\u03b6j ) ,\n\nthe ith\n\nj6=i\n\nLagrange polynomial associated to {\u03b61 , . . . , \u03b6d }. Clearly, since deg(Li ) < deg(f )\nwe have \u03c0(Li ) = Li , for all i \u2208 {1, . . . , d}. Let A\u2217 = HomK (A, K) be the usual\ndual space of A. For all i \u2208 {1, . . . , d}, we define 1\u03b6i : A \u2212\u2192 K by 1\u03b6i (p) =\np(\u03b6i ) for all p \u2208 A. The following lemma is obvious form the definition\nof the\n\u001a\n1 if i = j\npolynomials Li that for i and j \u2208 {1, . . . , d}, we have Li (\u03b6j ) =\n.\n0 else\nThis implies that the set {L1 , . . . , Ld } is a basis of A. A well known fact is\nthat the set {1\u03b61 , . . . , 1\u03b6d } form a basis A\u2217 dual of the basis {L1 , . . . , Ld } of A.\nAs a corollary, we have the Lagrange interpolation formula : Each p \u2208 A can\nd\nP\n1\u03b6i (p) \u2217 Li (x). A funny consequence is that if we choose\nbe written p(x) =\ni=1\n\n{L1 , . . . , Ld } as a basis of A, for all g \u2208 K[x], the remainder \u03c0(g) of the euclidian\ndivision of g by f is given by (g(\u03b61 ), . . . , g(\u03b6d )) in the basis {L1 , . . . , Ld }, i.e.\nd\nP\ng(\u03b6i )Li (x). In other word, divide g by f is equivalent to evaluate g at\nr=\ni=1\n\nthe roots of f .\nThe general philosophy of this last proposition will allows us to make a lot\nof proof in a very simple way. For example, it is very easy to see the different\noperation in A using this representation. Let g and h be to elements in A, then\nd\nd\nP\nP\n(g(\u03b6i ) \u2217 h(\u03b6i ))Li (x) in A.\n(g(\u03b6i ) + h(\u03b6i )) \u2217 Li (x) and g \u2217 h =\nwe have g + h =\ni=0\n\ni=1\n\nThis allows us to avoid the use of the section \u03c3. In fact, the Lagrange polynomials L1 , . . . , Ld reveal a deeper structure on the algebra\nA : The polynomials\n\u001a\nLi if i = j\nL1 , . . . , Ld are the idempotents of A, i.e. Li \u2217 Lj =\n.\n0 else\nThanks to this description of the quotient algebra, it is easy to derive algorithms for both polynomial solving and gcd computation even though the\nproblems are of very different nature.\n\n3\n\n\fRemark that we have expressed everything in the monomial basis since it is\nthe most widely used basis to express polynomials but we could use other bases.\nA particular basis is the Chebyshev basis where all results are exactly the same\nsince it is a graduated basis.\n\n3\n\nAn algebraic algorithm for gcd computation\n\nTo first give an idea on how to exploit the section above in order to design\nalgorithm for gcd, We recall a classical method for polynomial solving (see [4]\nfor instance). Proofs are given for the sake of completeness and because very\nsimilar ideas will lead us to the AGCD computation.\n\n3.1\n\nRoots via eigenvalues\n\nLet f (x) =\n\nd\nP\n\nfi xi \u2208 C[x] be a polynomial of degree d. Then we consider the\n\ni=0\n\nmatrix of the multiplication by x in C[x]/(f ). Its matrix in the monomial basis\n1, . . . , xd\u22121 is the following:\n\u0001\n1 x x2 * * * xd\u22121\n\uf8eb\n\uf8f6\n\uf8f6\n\uf8eb\n0 0 0 ***\n\u2212 ffd0\n1\n\uf8ec x \uf8f7 \uf8ec\n1 0 0 ***\n\u2212 ffd1 \uf8f7\n\uf8f7\n\uf8f7 \uf8ec\n\uf8ec\n\uf8f7\nf2\nFrob(f ) = \uf8ec x2 \uf8f7 \uf8ec\n0\n1\n0\n*\n*\n*\n\u2212\n\uf8ec\n\uf8f7\n\uf8f7 \uf8ec\n\uf8ec\nfd\n\uf8f7\n\uf8ec .. \uf8f7 \uf8ec . . . .\n.\n\uf8f7\n\uf8ed . \uf8f8 \uf8ed .. .. ..\n..\n..\n\uf8f8\nfd\u22121\nxd\u22121\n0 0 0 *** \u2212\nfd\n\nwell known as the Froebenius companion matrix associated to f .\nProposition 1 Let f (x) \u2208 C[x]be polynomial of degree d with d distinct roots\nZ(f ) = {z1 , . . . , zd }, then the eigenvalues of Frob(f ) are the roots of f (x), i.e.\nSpec(Frob(f )) = {z1 , . . . , zd }.\nProof It follows directly from the fact that Frob(f ) is the matrix of the multiplication by x in C[x]/(f ). But here we propose to give a direct proof by\ninduction. In fact, we prove by induction that the characteristic polynomial of\nFrob(f ) is f (x) itself (up to a sign and a scalar factor 1/fd ), i.e.:\n\u2212x 1 0 * * *\n0 \u2212x 1 * * *\n.. . .\n..\n..\n.\n.\n.\n.\n0\n0 0 ***\n\n\u2212 ffd0\n\u2212 ffd1\n..\n.\nf\n\u2212x \u2212 d\u22121\nfd\n\n4\n\n= \u2212f (x).\n\n\fsince we have:\n\u2212x 1 0\n0 \u2212x 1\n..\n..\n..\n.\n.\n.\n0\n0 0\n\n***\n***\n..\n.\n***\n\n\u2212 ffd0\n\u2212 ffd1\n..\n.\nf\n\u2212x \u2212 d\u22121\nfd\n\n\u2212x 1 0\n.. ..\n.\n= \u2212x\u2217 ..\n. .\n0 0 0\n\n***\n..\n.\n***\n\n\u2212 ffd1\n..\n.\n\u2212x \u2212 fd\u22121\nfd\n\n\u2212\n\nf0\nf0\n= \u2212(x\u2217f \u0303(x)+ )\nfd\nfd\n\n0\nand by assumption f \u0303(x) = f (x)\u2212f\nand finally we have the wanted result. Refd \u2217x\nmark that this proof allows to avoid the condition that all the roots of f (x) are\ndistinct.\n\u0003\n\nThen, to compute the roots of f (x) one can compute the eigenvalues of\nis Froebenius companion matrix. This is the object of the method proposed\n(reintroduced) by Edelman and Murakami [5] and revisited by Fortune [6] and\nmany others trying to use the displacement structure of the companion matrix.\nIn fact, often, the author realized that the monomial basis of the quotient algebra\nis not the most suitable one and proposed to express the matrix of the same\nlinear application but in other basis. In the case of the Chebyshev basis this\nalgorithm was already known by Barnett [2] and Cardinal later [4].\nIn the next section, we will also take advantage of the structure of the quotient algebra to design an algorithm for gcd computation mainly using linear\nalgebra (eigenvalues are used in theory and never computed).\n\n3.2\n\nStructure of quotient and gcd\n\nLet f (x) and g(x) \u2208 K[x] such that they are both monic. As above, we denote\nA = K[x]/(f ) and d = deg(f ). We denote denote {\u03b61 , . . . , \u03b6d } the set of roots\nof f (x)\u001aand we assume that f (x) is squarefree, i.e. \u03b6i 6= \u03b6j if i 6= j. We define\nA \u2212\u2192 A\nMg :\nwhere \u03c0(p) \u2208 A denote the remainder of p(x) \u2208 K[x]\nh 7\u2212\u2192 \u03c0(gh)\nby division by f (x). We denote Mg the matrix of Mg in the monomial basis\n1, x, . . . , xd\u22121 of A but other bases can be used. A matrix representing the map\nMg is called an extended companion matrix.\nProposition 2 The eigenvalues of Mg are {g(\u03b61 ), . . . , g(\u03b6d )}.\nProof It is a direct corollary of the proposition ?? since if we write the matrix\nof this linear map in the Lagrange basis associated to {\u03b61 , . . . , \u03b6d } is\n\uf8f6\n\uf8eb\ng(\u03b61 ) * * *\n0\n\uf8ec ..\n.. \uf8f7\n..\n\uf8ed .\n.\n. \uf8f8\n0\n\n***\n\nand gives the wanted result.\n\ng(\u03b6d )\n\n\u0003\n\nTrivially, we have:\n5\n\n\fCorollary 1 We have corank(Mg ) = deg(f ) \u2212 rank(Mg ) = deg(gcd(f, g)).\nThe column of index i of Mg is the column vector of the coefficients of\nxi\u22121 \u2217 g(x).\nLet p1 , . . . , pl be a basis of Ker(Mg ) and let P1 (x), . . . , Pl (x) be the corresponding polynomials. First remark that AnnA (g) = {P (x) \u2208 A|P (x)\u2217g(x) = 0}\nis an ideal of A.\nLemma 1 The ideal AnnA (g) is a principal ideal.\nProof Let us define\nY\n\ns(x) =\n\n(x \u2212 \u03b6).\n\n\u03b6\u2208Z(f )\\(Z(f )\u2229Z(g))\n\nFor all h \u2208 AnnA (g) it is clear that Z(h) \u2283 Z(f )\\(Z(f ) \u2229 Z(g)) and then s\ndivide h. Furthermore s \u2208 AnnA (g) since in the Lagrange basis\ns(x) \u2217 g(x) =\n\nd\nX\n\ns(\u03b6i ) \u2217 g(\u03b6i )Li (x) = 0.\n\ni=1\n\nThis shows that AnnA (g) = (s).\n\n\u0003\n\nTo compute s(x), we built the matrix with columns formed by p1 , . . . , pl and\nwe make a triangulation operating only on the columns. This way we obtain\nthe polynomial of minimal degree linear combination of P1 (x), . . . , Pl (x) and it\nis easily seen that this s(x) up to a multiplicative scalar factor.\nLemma 2 The first column of a column echelon form of the matrix Kg built\nfrom a basis of Ker(Mg ) is the generator of AnnA (g), i.e. it is the vector of the\ncoefficients of s(x) up to a scalar multiplication.\nProof Since the columns of a column echelon form of the matrix Kg are\nlinearly independent, they form a basis of AnnA (g) as K-vector space. So s(x)\nis a linear combination of the polynomials associated to those columns. The\npolynomial associated to the column echelon form of Kg have all different degree\n(because it is an echelon form) and so s(x) is a linear combination of those\npolynomial. Because s(x) as the lowest degree possible, it is a scalar multiple\nof the polynomial associated to the first column.\n\u0003\nProposition 3 f (x) \u2227 g(x) =\n\nf (x)\ns(x) .\n\nProof By construction, we have s(x) \u2217 g(x) = 0 mod f (x) and so s(x) divide\n(x)\n(x)\n, g(x)) = gcd(f (x), g(x)) since the roots of fs(x)\nare\nf (x). We also have gcd( fs(x)\n(x)\nthe root of f (x) where g(x) vanishes. Since deg( fs(x)\n) = deg(gcd(f (x), g(x)) we\nhave the wanted result.\n\u0003\n\n6\n\n\fIn all this section, we did not care if the polynomials are known in monomial\nor Chebyshev basis for instance. In fact, in order to have an algebraic algorithm,\nwe only need to be able to perform euclidian division and this is always the case\nif the polynomial basis is graduated (as for monomial, Chebyshev, most of the\northogonal bases).\n\n4\n\nBezoutian and Barnett's formula\n\nA classical matricial formulation of resultant is given by the B\u00e9zout matrix.\nIn this part, we recall the construction of the B\u00e9zout matrix and a special\nfactorization of the multiplication matrix expressed in the monomial basis. This\nfactorization is called Barnett formula (see [2]). The Barnett's formula allows to\nbuild the classical extended companion matrix without using euclidian division\nand only stable numerical computations. Furthermore, this factorization reveals\nthat the extended companion matrix has a special rank structure and we will\nuse this fact later to design a fast algorithm to compute AGCD.\nDefinition 1 Let f and g \u2208 C[x] of degree m and n respectively (with n > m),\nm\u22121\nP\nP\n(y)g(x)\nwe denote \u0398f,g (x, y) = f (x)g(y)\u2212f\n\u03baf,g,j (x)y j . The\n=\n\u03b8i,j xi y j =\nx\u2212y\ni,j\n\u0001 j=0\nB\u00e9zout matrix associated with f and g is Bf,g = \u03b8j,j i,j\u2208{0,...,m\u22121} .\nRemark that since \u0398f,g (x, y) = \u0398f,g (y, x) the matrix Bf,g is symmetric. The\npolynomials \u03baf,g,j (x) are univariate polynomials of degree at most m \u2212 1. One\nparticular case of interest is when f = 1. In this case the B\u00e9zout matrix has a\nHankel structure, i.e. \u03b8i,j = \u03b8i\u22121,j+1 . In this case we denote Hg,i (x) = \u03ba1,g,i (x)\nfor i \u2208 {0, . . . , m \u2212 1} which are called the Horner polynomials.\n\nProposition 4 Let i \u2208 {0, . . . , m \u2212 1}, the polynomial Hg,i (x) = c1,m\u2212i + * * * +\nc1,m xi has degree i and since they have different degree, they form a basis of\nm\u22121\nP\nHg,m\u2212i (x)y i .\nC[x]/(g). Furthermore, \u03981,g (x, y) =\ni=0\n\nCorollary 2 The matrix B1,g is the basis conversion from the Horner basis\nH0 , . . . , Mm\u22121 to the monomial basis 1, x, . . . , xn\u22121 of C[x]/(g).\nThis leads us to the following theorem, known as Barnett formula (see [2]):\nTheorem 1 Let Mf be the multiplication matrix associated to f in C[x]/(g) in\nthe monomial basis, we have:\n\u22121\nMf = Bf,g B1,g\n.\n(y)\nProof We have \u0398f,g (x, y) = f (x) g(y)\u2212g(x)\n+g(x) f (x)\u2212f\nand so f (x) g(x)\u2212g(y)\n\u2261\nx\u2212y\nx\u2212y\nx\u2212y\n\u0398f,g (x, y) in C[x, y]/(g(x)). So, for each i \u2208 {0, . . . , m \u2212 1}, we have \u0398f,g,i (x) \u2261\n\n7\n\n\ff (x)\u03981,g,i (x). This last equality means that Bf,g is the matrix of the multiplication by f (x) in C[x]/(g). The result follows directly from this fact.\n\u0003\nThe Barnett's formula reveals the rank structure of the multiplication matrix. Furthermore, this formula is already known if we choose Chebyshev basis\ninstead of monomial basis to express the polynomials and the matrices have\nexactly the same nature.\n\n5\n\nStructured matrices and asymptotically fast\nalgorithms\n\nIn this section, we briefly recall some basics on displacement structured matrices\nand related algorithms.\n\n5.1\n\nDisplacement structure\n\nGiven an integer n and a complex number \u03b8 with |\u03b8| = 1, define the circulant\nmatrix\n\uf8eb\n\uf8f6\n0\n\u03b8\n\uf8ec 1 0\n\uf8f7\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f7\n..\n\uf8f7 \u2208 Cn\u00d7n .\n.\n1\nZn\u03b8 = \uf8ec\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f7\n..\n..\n\uf8ed\n\uf8f8\n.\n.\n1 0\nNext, define the Toeplitz-like displacement operator as the linear operator\n\u2207T : Cm\u00d7n \u2212\u2192 Cm\u00d7n\n1\n\u2207T (A) = Zm\nA \u2212 AZn\u03b8 .\n\nA matrix A \u2208 Cm\u00d7n is said to be Toeplitz-like if \u2207T (A) is a small rank matrix\n(where \"small\" means small with respect to the matrix size). The number\n\u03b1 = rank(\u2207(A)) is called the displacement rank of A. If A is Toeplitz-like, then\nthere exist (non-unique) displacement generators G \u2208 Cm\u00d7\u03b1 and H \u2208 C\u03b1\u00d7n\nsuch that\n\u2207(A) = GH.\nToeplitz matrices and their inverses are examples of Toeplitz-like matrices. Another useful example is the multiplication matrix Mf , which has Toeplitz-like\ndisplacement rank equal to 2, regardless of its size.\nA similar definition holds for Cauchy-like structure; here the relevant displacement operator is\n\u2207C : Cm\u00d7n \u2212\u2192 Cm\u00d7n\n\u2207C (A) = D1 A \u2212 AD2 ,\nwhere D1 and D2 are diagonal matrices of appropriate size with disjoint spectra.\nSee [10] for a detailed description of displacement structure.\n8\n\n\f5.2\n\nFast solution of displacement structured linear systems\n\nGaussian elimination with partial pivoting (GEPP) is a well-known and reliable algorithm that computes the solution of a linear system. Its arithmetic\ncomplexity for an n \u00d7 n matrix is asymptotically O(n3 ). But if the system matrix exhibits displacement structure, it is possible to apply a variant of GEPP\nwith complexity O(n2 ). The main idea consists in operating on displacement\ngenerators rather than on the whole matrix; see [7] for details.\nStrictly speaking, the GKO algorithm performs GEPP (or, equivalently,\ncomputes the PLU factorization) for Cauchy-like matrices. However, several\nauthors have pointed out (see [7], [9], [12]) that Toeplitz-like matrices can be\nstably and cheaply transformed into Cauchy-like matrices; the same is true for\ndisplacement generators.\nConsider, for instance, the case \u03b8 = 1 and let A be an n\u00d7n Toeplitz-like matrix with generators G and H. Denote by D0 the matrix diag(1, e\u03c0i/n , . . . , e(n\u22121)\u03c0i/n )\nand let F be the Fourier matrix of size n \u00d7 n. Then the matrix F AD0\u22121 F H is\nCauchy-like, of the same displacement rank as A, with respect to the displacement operator defined by D1 = diag(1, e2\u03c0i/n , . . . , e2\u03c0i(n\u22121)/n ) and D2 =\ndiag(e\u03c0i/n , e3\u03c0i/n , . . . , e(2n\u22121)\u03c0i/n ). Its Cauchy-like generators can be computed\nb H = F D0 H H .\nas \u011c = F G and H\nGeneralization to the case of m \u00d7 n rectangular matrices is possible. In this\ncase, the parameter \u03b8 should be chosen so that the spectra of D1 and D2 are\nwell separated (see [1] and [3]).\nWe also point out that the GKO algorithm can be adapted to pivoting techniques other than partial pivoting ([8], [13]). This is especially useful in case of\ninstability due to internal growth of generator entries. A Matlab implementation of the GKO algorithm that takes into account several pivoting strategies is\nfound in the package DRSolve described in [1]. In our implementation, we use\nthe pivoting strategy proposed in [8].\n\n6\n\nA structured approach to AGCD computation\n\nWe propose here an algorithm that exploits the algebraic and displacement\nstructure of the multiplication matrix to compute the AGCD of two given polynomials with real coefficients (as defined in section 1).\n\n6.1\n\nRank estimation\n\nIt has been pointed out in Section 3 that the rank deficiency of the multiplication matrix equals the AGCD degree. Here we use the structured pivoted LU\ndecomposition to estimate the approximate rank of the multiplication matrix.\nRecall that Mg has a Toeplitz-like structure with displacement rank 2; it can\nthen be transformed into a Cauchy-like matrix M\u0302g as described in Section 5.2.\nFast pivoted Gauss elimination yields a factorization M\u0302g = P1 LU P2 , where L\n9\n\n\fis a square, nonsingular, lower triangular matrix with diagonal entries equal to\n1, U is upper triangular and P1,2 are permutation matrices. Inspection of the\ndiagonal entries (or of the row norms) of U allows to estimate the approximate\nrank of M\u0302g and, therefore, of Mg .\n\n6.2\n\nMinimization of a quadratic functional\n\nLet us suppose that:\n\u2022 the polynomial f (x) =\n\nPn\n\nj=0\n\nfj xj is exactly known,\n\nPm\nj\n\u2022 the polynomial g(x) =\nj=0 gj x is approximately known and may be\nperturbed, so that we consider its coefficients as variables,\n\u2022 the AGCD degree is known.\nThen we can reformulate the problem of AGCD computation as the minimization of a quadratic functional. Indeed, recall that the cofactor v(x) with respect\nto f (x) is defined by the \"shortest\" vector (i.e., the vector with the maximum\nnumber of trailing zeros) that belongs to the null space of Mg . We assume v(x)\nto be monic; we denote its degree as k and we have\n\uf8eb \uf8f6\n0\n\uf8eb\n\uf8f6\nv0\n\uf8ec .. \uf8f7\n. \uf8f7\n\uf8ec .. \uf8f7 \uf8ec\n\uf8f7\n\uf8ec . \uf8f7 \uf8ec\n\uf8ec\n\uf8ec\n\uf8f7 \uf8ec .. \uf8f7\n\uf8ec vk\u22121 \uf8f7 \uf8ec . \uf8f7\n\uf8ec\n\uf8f7 \uf8ec . \uf8f7\n\uf8f7=\uf8ec . \uf8f7\n.\nMg v = Mg * \uf8ec\n1\n. \uf8f7\n\uf8ec\n\uf8f7\n\uf8f7\n\uf8ec 0 \uf8f7 \uf8ec\n\uf8f7\n\uf8ec\n.\n\uf8ec\n\uf8f7\n.. \uf8f7\n\uf8ec . \uf8f7 \uf8ec\n\uf8f7\n\uf8ed .. \uf8f8 \uf8ec\n\uf8ec . \uf8f7\n\uf8ed .. \uf8f8\n0\n0\nAlso observe that the entries of Mg are linear functions of the coefficients of\ng(x). Then the equation Mg v = 0 can be rewritten as F (g, v)=0, where the\nfunctional F is defined as\nF : Cm+1 \u00d7 Ck \u2212\u2192 R+\nF (g, v) = kMg vk22 .\nFor a preliminary study of the problem, we have chosen to solve the equation\nF (g, v)=0 by means of Newton's method, applied so as to exploit structure.\nDenote by z = [g0 , . . . , gm , v0 , . . . , vk\u22121 ]T the vector of unknowns; then each\nNewton step has the form\nz (j+1) = z (j) \u2212 J(g (j) , v (j) )\u2020 Mg(j) v (j) .\nIn particular, notice that the Jacobian matrix associated with F is an n \u00d7 (m +\nk + 1) Toeplitz-like matrix of displacement rank 3. This property allows to\n10\n\n\fcompute a solution of the linear system J(g (j) , v (j) )y = Mg(j) v (j) in a fast way;\ntherefore, the arithmetic complexity of each iteration is quadratic w.r.t. the\ndegree of the input polynomials.\nWe propose in the future to take into consideration other optimization methods in the quasi-Newton family, such as BFGS.\n\n6.3\n\nComputation of displacement generators\n\nIn order to perform fast factorization of the multiplication matrix Mg , we need\nto compute Toeplitz-like displacement generators. It turns out that the range\nof \u2207(Mg ) is spanned by the first and last column of the displaced matrix, and\nthe columns of indices from 2 to n \u2212 1 are multiples of the first one. Therefore,\nit suffices to compute a few rows and columns of Mg in order to obtain displacement generators. This can be done in a fast and stable way by using Barnett's\nn\nformula. If we denote as ej the jth vector of the canonical basis of C , then\nthe computation of the j-th column\u0001of Mg can be seen as\nMg (:, j) = B(f, g) * B(1, f )\u22121 ej ,\nthat is, it consists in solving a triangular Hankel linear system and computing\na matrix-vector product. For row computation, recall that the Bezoutian is a\nsymmetric matrix; we have analogously:\n\u0001T\nMg (j, :) = eTj * B(f, g) * B(1, f )\u22121 = B(1, f )\u22121 B(f, g)eTj ,\nso that the computation of a row of Mg amounts to performing a matrix-vector\nproduct and solving a Hankel triangular system.\nA similar approach holds for computation of displacement generators of the\nJacobian matrix J(g, v) associated with the functional F (g, v).\n\n6.4\n\nDescription of the algorithm\n\nInput: coefficients of polynomials f (x) and g(x).\nOutput: a perturbed polynomial g\u0303(x) such that f and g\u0303 have a nontrivial common factor.\n1. Estimate the approximate rank k of Mg by computing a fast pivoted LU\ndecomposition of the associated Cauchy-like matrix.\n2. Again by using fast LU, compute a vector v = [v0 , v1 , . . . ., vk\u22121 , 1, 0, . . . ., 0]\nin the approximate null space of Mg .\n\nT\n\n3. Apply structured Newton with initial guess (g, v) and compute polynomials\ng\u0303 and \u1e7d such that f and g\u0303 have a common factor of degree deg f \u2212 k and \u1e7d\nis the monic cofactor for f .\n\n6.5\n\nNumerical experiments and computational issues\n\nWe have written a preliminary implementation of the proposed method in Matlab (available at the URL http://www.unilim.fr/pages perso/paola.boito/MMgcd.m).\n11\n\n\fThe results of a few numerical experiments are shown below. The polynomials f and g are monic and have random coefficients uniformly distributed over\n[\u22121, 1]. They have an exact GCD of prescribed degree. A perturbation is then\nadded to g. The perturbation vector has random entries uniformly distributed\nover [\u2212\u03b7, \u03b7] and its norm is of the order of magnitude of \u03b7. We show:\n\u2022 the residual F (g\u0303, \u1e7d),\n\u2022 the 2-norm distance between the exact and the computed cofactor v,\n\u2022 the 2-norm distance between the exact and the computed perturbed polynomial g (which is expected to be roughly of the same order of magnitude\nas \u03b7).\nIn the following table we have taken \u03b7 =1e-5.\nn, m, deg f F (g\u0303, \u1e7d)\nkv \u2212 \u1e7dk2 kg \u2212 g\u0303k2\n8, 7, 3\n1.02e-15 1.19e-15 1.40e-5\n15, 14, 5\n1.51e-15 2.26e-15 1.35e-4\n22, 22, 7\n2.07e-13 1.40e-13 2.20e-4\n36, 36, 11\n1.19e-12 5.07e-14 0.0012\nHere are results for \u03b7 =1e-8:\nn, m, deg f\nF (g\u0303, \u1e7d) kv \u2212 \u1e7dk2 kg \u2212 g\u0303k2\n8, 7, 3\n5.49e-15 1.63e-15\n5.85e-8\n28, 27, 13 7.90e-14 8.98e-14\n6.50e-7\n38, 37, 13 4.88e-12 4.26e-12\n2.30e-5\n58, 57, 23 2.03e-12 4.40e-12\n2.54e-4\nThere are several issues in our approach that deserve further investigation.\nLet us mention in particular:\n\u2022 The choice of a threshold (or a more refined technique) for estimating\napproximate rank.\n\u2022 Normalization of polynomials: here we mostly work with monic polynomials, but other normalizations may be considered.\n\u2022 The structured implementation of the optimization step (minimizing F (g, v)).\nWe have used for now a heuristic structured version of the Gauss-Newton\nalgorithm. Observe that each step of classical Gauss-Newton applied to\nour problem has the form z (j+1) = z (j) \u2212 y (j) , where z (j) is the vector containing the coefficients of the j-th iterate polynomials g (j) and v (j) , and y (j)\nis the least-norm solution to the underdetermined system J(g (j) , v (j) )y (j) =\nMg(j) v (j) . Computing this least-norm solution in a structured and fast way\nis a difficult point that will require more work. Our implementation gives\na solution which is not, in general, the least-norm one, even though it\nis typically quite close. Further work will also include a study of other\npossible optimization methods that lend themselves well to a structured\napproach.\n\n12\n\n\f7\n\nConclusions\n\nWe have proposed and implemented a fast structured matrix-based approach to\na variant of the AGCD problem, namely, the problem of computing an approximate greatest common divisor of two univariate polynomials, one of which is\nknown to be exact. To our knowledge, this variant has been so far neglected in\nthe existing literature. It may be also interesting when one polynomial is known\nwith high accuracy and the other is not.\nOur approach is based on the structure of the multiplication matrix and\non the subsequent reformulation of the problem as the minimization of a suitably defined functional. Our choice of the multiplication matrix Mg over other\nresultant matrices (e.g., Sylvester, B\u00e9zout...) is motivated by\n\u2022 the smaller size of Mg , with respect e.g. to the Sylvester matrix,\n\u2022 the strong link between the null space of Mg and the gcd, and in particular\nthe fact that the null space of Mg immediately yields a gcd cofactor,\n\u2022 the displacement structure of Mg ,\n\u2022 the possibility of computing selected rows and columns of Mg in a stable\nand cheap way, thanks to Barnett's formula.\nThis is, however, a preliminary study. Further work will include generalizations\nof the proposed problem and a more thorough analysis of the optimization part\nof the algorithm. Furthermore, this approach can be generalized in several\nintersting way:\n\u2022 using better bases then the monomial one,\n\u2022 it can be extended to some multivariate setting to compute the co-factor\nof a polynomial g in C[x1 , . . . , xn ]/(f1 , . . . , fn ) when f1 , . . . , fn define a\ncomplete intersection since Barnett formula still holds,\n\u2022 to compute the AGCD of f with g1 , . . . , gk where f is known with accuracy\nbut g1 , . . . , gk are inaccurate, one can take g as a linear combination of\ng1 , . . . , gk with our method and succeed with a high probability.\n\nReferences\n[1] A. Aric\u00f2, G. Rodriguez. A fast solver for linear systems with displacement\nstructure. Numer. Algorithms, 2010. DOI: 10.1007/s11075-010-9421-x.\n[2] S. Barnett, Polynomials and linear control systems, Monographs and Textbooks in Pure and Applied Mathematics, 77, Marcel Dekker, Inc., New York,\n1983. xi+452 pp. ISBN: 0-8247-1898-4.\n\n13\n\n\f[3] D. A. Bini, P. Boito, A fast algorithm for approximate polynomial gcd based\non structured matrix computations, in Numerical Methods for Structured Matrices and Applications: Georg Heinig memorial volume, Operator Theory:\nAdvances and Applications, vol. 199, Birkh\u00e4user (2010), 155-173.\n[4] J. P. Cardinal, On two iterative methods for approximating roots of a polynomial , In J. Renegar, M. Shub and S. Smale editors, Proc. SIAM-AMS\nSummer Seminar on Math. of Numerical Analysis, Vol. 32 of Lecture Notes\nin Applied Math., AMS Press (1996), 165-188.\n[5] A. Edelman, H. Murakami, Polynomial roots from companion matrix eigenvalues, Math. Comp. 64 (1995), no. 210, 763-776.\n[6] S. Fortune, An iterated eigenvalue algorithm for approximating roots of univariate polynomials, Journal of Symbolic Computation, 33 (2002), no. 5, 627646.\n[7] I. Gohberg, T. Kailath, and V. Olshevsky, Fast Gaussian elimination with\npartial pivoting for matrices with displacement structure, Math. Comp. 64\n(212), 1557-1576 (1995).\n[8] M. Gu, Stable and Efficient Algorithms for Structured Systems of Linear\nEquations, SIAM J. Matrix Anal. Appl. 19, 279-306 (1998).\n[9] G. Heinig, Inversion of generalized Cauchy matrices and other classes of\nstructured matrices, Linear Algebra in Signal Processing, IMA volumes in\nMathematics and its Applications 69, 95-114 (1994).\n[10] T. Kailath, A. H. Sayed, Displacement Structure: Theory and Applications,\nSIAM Review 37(3), 297-386 (1995).\n[11] B. Mourrain, O. Ruatta, Relations between roots and coefficients, interpolation and application to system solving, Journal of Symbolic Computation,\n33 (2002), no. 5, 679-699.\n[12] V. Y. Pan, On computations with dense structured matrices, Math. of Comput. 55(191), 179-190 (1990).\n[13] M. Stewart, Stable Pivoting for the Fast Factorization of Cauchy-Like Matrices, preprint (1997).\n[14] Z. Zeng, The approximate GCD of inexact polynomials Part I: a univariate\nalgorithm, http://www.neiu.edu/\u223czzeng/uvgcd.pdf.\n\n14\n\n\f"}