{"id": "http://arxiv.org/abs/1204.1564v4", "guidislink": true, "updated": "2012-12-17T16:58:04Z", "updated_parsed": [2012, 12, 17, 16, 58, 4, 0, 352, 0], "published": "2012-04-06T20:57:07Z", "published_parsed": [2012, 4, 6, 20, 57, 7, 4, 97, 0], "title": "Minimal model of associative learning for cross-situational lexicon\n  acquisition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.5231%2C1204.0347%2C1204.1626%2C1204.4427%2C1204.0599%2C1204.1732%2C1204.5685%2C1204.4432%2C1204.2625%2C1204.5531%2C1204.2651%2C1204.0276%2C1204.0789%2C1204.5563%2C1204.0601%2C1204.3895%2C1204.0512%2C1204.0152%2C1204.6303%2C1204.3975%2C1204.0954%2C1204.1077%2C1204.4757%2C1204.5823%2C1204.3160%2C1204.4770%2C1204.6705%2C1204.2248%2C1204.2683%2C1204.1387%2C1204.0465%2C1204.3217%2C1204.3718%2C1204.2126%2C1204.1599%2C1204.6709%2C1204.0303%2C1204.1824%2C1204.3978%2C1204.1332%2C1204.3142%2C1204.3607%2C1204.6211%2C1204.6198%2C1204.0970%2C1204.5662%2C1204.4266%2C1204.6389%2C1204.1658%2C1204.2783%2C1204.1484%2C1204.4455%2C1204.3610%2C1204.1885%2C1204.0400%2C1204.4295%2C1204.0636%2C1204.2530%2C1204.3335%2C1204.2661%2C1204.5328%2C1204.1034%2C1204.0747%2C1204.1551%2C1204.1564%2C1204.4272%2C1204.2779%2C1204.6180%2C1204.1445%2C1204.4275%2C1204.3273%2C1204.4023%2C1204.2045%2C1204.6239%2C1204.1320%2C1204.4697%2C1204.5365%2C1204.6516%2C1204.5710%2C1204.2384%2C1204.6659%2C1204.0836%2C1204.0357%2C1204.1622%2C1204.2876%2C1204.1831%2C1204.2626%2C1204.5799%2C1204.4341%2C1204.0657%2C1204.1141%2C1204.3485%2C1204.0574%2C1204.4962%2C1204.0290%2C1204.3127%2C1204.0300%2C1204.4980%2C1204.3813%2C1204.5581%2C1204.4917&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Minimal model of associative learning for cross-situational lexicon\n  acquisition"}, "summary": "An explanation for the acquisition of word-object mappings is the associative\nlearning in a cross-situational scenario. Here we present analytical results of\nthe performance of a simple associative learning algorithm for acquiring a\none-to-one mapping between $N$ objects and $N$ words based solely on the\nco-occurrence between objects and words. In particular, a learning trial in our\nlearning scenario consists of the presentation of $C + 1 < N$ objects together\nwith a target word, which refers to one of the objects in the context. We find\nthat the learning times are distributed exponentially and the learning rates\nare given by $\\ln{[\\frac{N(N-1)}{C + (N-1)^{2}}]}$ in the case the $N$ target\nwords are sampled randomly and by $\\frac{1}{N} \\ln [\\frac{N-1}{C}] $ in the\ncase they follow a deterministic presentation sequence. This learning\nperformance is much superior to those exhibited by humans and more realistic\nlearning algorithms in cross-situational experiments. We show that introduction\nof discrimination limitations using Weber's law and forgetting reduce the\nperformance of the associative algorithm to the human level.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.5231%2C1204.0347%2C1204.1626%2C1204.4427%2C1204.0599%2C1204.1732%2C1204.5685%2C1204.4432%2C1204.2625%2C1204.5531%2C1204.2651%2C1204.0276%2C1204.0789%2C1204.5563%2C1204.0601%2C1204.3895%2C1204.0512%2C1204.0152%2C1204.6303%2C1204.3975%2C1204.0954%2C1204.1077%2C1204.4757%2C1204.5823%2C1204.3160%2C1204.4770%2C1204.6705%2C1204.2248%2C1204.2683%2C1204.1387%2C1204.0465%2C1204.3217%2C1204.3718%2C1204.2126%2C1204.1599%2C1204.6709%2C1204.0303%2C1204.1824%2C1204.3978%2C1204.1332%2C1204.3142%2C1204.3607%2C1204.6211%2C1204.6198%2C1204.0970%2C1204.5662%2C1204.4266%2C1204.6389%2C1204.1658%2C1204.2783%2C1204.1484%2C1204.4455%2C1204.3610%2C1204.1885%2C1204.0400%2C1204.4295%2C1204.0636%2C1204.2530%2C1204.3335%2C1204.2661%2C1204.5328%2C1204.1034%2C1204.0747%2C1204.1551%2C1204.1564%2C1204.4272%2C1204.2779%2C1204.6180%2C1204.1445%2C1204.4275%2C1204.3273%2C1204.4023%2C1204.2045%2C1204.6239%2C1204.1320%2C1204.4697%2C1204.5365%2C1204.6516%2C1204.5710%2C1204.2384%2C1204.6659%2C1204.0836%2C1204.0357%2C1204.1622%2C1204.2876%2C1204.1831%2C1204.2626%2C1204.5799%2C1204.4341%2C1204.0657%2C1204.1141%2C1204.3485%2C1204.0574%2C1204.4962%2C1204.0290%2C1204.3127%2C1204.0300%2C1204.4980%2C1204.3813%2C1204.5581%2C1204.4917&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "An explanation for the acquisition of word-object mappings is the associative\nlearning in a cross-situational scenario. Here we present analytical results of\nthe performance of a simple associative learning algorithm for acquiring a\none-to-one mapping between $N$ objects and $N$ words based solely on the\nco-occurrence between objects and words. In particular, a learning trial in our\nlearning scenario consists of the presentation of $C + 1 < N$ objects together\nwith a target word, which refers to one of the objects in the context. We find\nthat the learning times are distributed exponentially and the learning rates\nare given by $\\ln{[\\frac{N(N-1)}{C + (N-1)^{2}}]}$ in the case the $N$ target\nwords are sampled randomly and by $\\frac{1}{N} \\ln [\\frac{N-1}{C}] $ in the\ncase they follow a deterministic presentation sequence. This learning\nperformance is much superior to those exhibited by humans and more realistic\nlearning algorithms in cross-situational experiments. We show that introduction\nof discrimination limitations using Weber's law and forgetting reduce the\nperformance of the associative algorithm to the human level."}, "authors": ["Paulo F. C. Tilles", "Jose F. Fontanari"], "author_detail": {"name": "Jose F. Fontanari"}, "author": "Jose F. Fontanari", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.jmp.2012.11.002", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1204.1564v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.1564v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "q-bio.NC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "q-bio.NC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.1564v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.1564v4", "arxiv_comment": null, "journal_reference": "J. Math. Psych. 56, 396-403 (2012)", "doi": "10.1016/j.jmp.2012.11.002", "fulltext": "Minimal model of associative learning for cross-situational lexicon acquisition\nPaulo F. C. Tilles and Jos\u00e9 F. Fontanari\nInstituto de F\u0131\u0301sica de S\u00e3o Carlos, Universidade de S\u00e3o Paulo,\nCaixa Postal 369, 13560-970 S\u00e3o Carlos, S\u00e3o Paulo, Brazil\n\narXiv:1204.1564v4 [q-bio.NC] 17 Dec 2012\n\nAn explanation for the acquisition of word-object mappings is the associative learning in a crosssituational scenario. Here we present analytical results of the performance of a simple associative\nlearning algorithm for acquiring a one-to-one mapping between N objects and N words based solely\non the co-occurrence between objects and words. In particular, a learning trial in our learning\nscenario consists of the presentation of C + 1 < N objects together with a target word, which refers\nto one of the objects in the context.\ni the learning times are distributed exponentially and\nh We find that\nN (N \u22121)\nthe learning rates are given by ln C+(N \u22121)2 in the case the N target words are sampled randomly\n\u0002\n\u0003\nand by N1 ln NC\u22121 in the case they follow a deterministic presentation sequence. This learning\nperformance is much superior to those exhibited by humans and more realistic learning algorithms\nin cross-situational experiments. We show that introduction of discrimination limitations using\nWeber's law and forgetting reduce the performance of the associative algorithm to the human level.\n\nI.\n\nINTRODUCTION\n\nEarly word-learning or lexicon acquisition by children,\nin which the child learns a fixed and coherent lexicon\nfrom language-proficient adults, is still a polemic problem in developmental psychology [1]. The classical associationist viewpoint, which can be traced back to empiricist philosophers such as Hume and Locke, contends that\nthe mechanism of word learning is sensitivity to covariation \u2013 if two events occur at the same time, they become associated \u2013 being part of humans' domain-general\nlearning capability. An alternative viewpoint, dubbed\nsocial-pragmatic theory, claims that the child makes the\nconnections between words and their referents by understanding the referential intentions of others. This idea,\nwhich seems to be originally due to Augustine, implies\nthat children use their intuitive psychology or theory of\nmind [2] to read the adults' minds. Although a variety of\nexperiments with infants demonstrate that they exhibit\na remarkable statistical learning capacity [3], the findings\nthat the word-object mappings are generated both fast\nand errorless by children are difficult to account for by\nany form of statistical learning. We refer the reader to\nthe book by Bloom [1] for a review of this most controversial and fascinating theme.\nRegardless of the mechanisms children use to learn a\nlexicon, the issue of how good humans are at acquiring\na new lexicon using statistical learning in controlled experiments has been tackled recently [4\u20139]. In addition, it\nhas been conjectured that statistical learning may be the\nprincipal mechanism in the development of pidgin [10].\nIn this context (pidgin), however, it is necessary to assume that the agents are endowed with some capacity to\ngrasp the intentions of the others as well as to understand\nnonlinguistic cues, otherwise one cannot circumvent the\nreferential uncertainty inherent in a word-object mapping\n[11].\nThe statistical learning scenario we consider here is\ntermed cross-situational or observational learning, and it\nis based on the intuitive idea that one way that a learner\n\ncan determine the meaning of a word is to find something in common across all observed uses of that word\n[12\u201314]. Hence learning takes place through the statistical sampling of the contexts in which a word appears.\nThere are two competing theories about word learning\nmechanism within the cross-situational scenario, namely,\nhypothesis testing and associative learning (see [9] for a\nreview). The former mechanism assumes that the learner\nbuilds coherent hypotheses about the meaning of a word\nwhich is then confirmed or disconfirmed by evidence [15\u2013\n18], whereas the latter is based essentially on the counting of co-occurrences of word-object statistics [19, 20].\nAlbeit associative learning can be made much more sophisticated than the mere counting of contingencies [9], in\nthis contribution we focus on the simplistic interpretation\nof that learning mechanism, which allows the derivation\nof explicit mathematical expressions to characterize the\nlearner's performance.\nAlthough cross-situational associative learning has\nbeen a very popular lexicon acquisition scenario since it\ncan be easily implemented and studied through numerical simulations (see, e.g., [10, 21\u201323]), there were only\na few attempts to study analytically this learning strategy [24, 25]. These works considered a minimal model of\ncross-situational learning, in which the one-to-one mapping between N objects and N words must be inferred\nthrough the repeated presentation of C + 1 < N objects (the context) together with a target word, which\nrefers to one of the objects in the context. The cooccurrences between objects and words are stored in a\nconfidence matrix, whose integer entries count how many\ntimes an object has co-occurred with a given word during\nthe learning process. The meaning of a particular word\nis then obtained by picking the object corresponding to\nthe greatest confidence value associated to that word, i.e.,\nthe object that has co-occurred more frequently with that\nword. In this paper, we expand on the work of Smith et\nal. [24] and offer analytical expressions for the learning\nrates of this minimal associative algorithm for different\nword sampling schemes, see Eqs. (9), (14) and (17).\n\n\f2\nTo assess the relevance of our findings to the efforts on\nunderstanding how humans perform on cross-situational\nlearning tasks, we use Monte Carlo simulations to compare the performance of the minimal associative algorithm with the performance of humans for short learning times [6] and with the performance of a more elaborated learning algorithm for long times [7]. Our finding\nthat the accuracy of the minimal associative algorithm\nis much higher than that observed in the experiments is\nimputed to the illimited storage and discrimination capability of the algorithm. In fact, introduction of errors\nin the discrimination of confidence values according to\nWeber's law reduces the performance to a level below\nthat of humans. Somewhat surprisingly, introduction of\nforgetting acts synergistically with our prescription for\nWeber's law resulting in an increase of performance that\neventually matches the experimental results.\nThe rest of this paper is organized as follows. In Sect.\nII we describe the learning scenario and in Sect. III we\nintroduce and study analytically the simplest associative\nlearning scheme for counting co-occurrences of words and\nobjects, in which the words are learned independently.\nWe consider first the problem of learning a single word\nand then investigate the effect of using different word\nsampling schemes for learning the complete N -word lexicon. In Sect. IV we compare the performance of the minimal associative algorithm with the performance exhibited\nby adult subjects. To understand the high efficiency of\nthe algorithm we introduce constraints on its storage and\ndiscrimination capabilities and show how the constraint\nparameters can be tunned to describe the experimental\nresults. Finally, in Sect. V we discuss our findings and\npresent some concluding remarks.\n\nII.\n\nCROSS-SITUATIONAL LEARNING\nSCENARIO\n\nWe assume that there are N objects, N words and\na one-to-one mapping between words and objects. To\ndescribe the one-to-one word-object mapping, we use the\nindex i = 1, . . . , N to represent the N distinct objects\nand the index h = 1, . . . , N to represent the N distinct\nwords. Without loss of generality, we define the correct\nmapping as that for which the object represented by i =\n1 is named by the word represented by h = 1, object\nrepresented by i = 2 by word represented by h = 2, and\nso on. Henceforth we will refer to the integers i and h\nas objects and words, respectively, but we should keep\nin mind that they are actually labels to those complex\nentities.\nAt each learning event, a target word, say word h = 1,\nis selected and then C + 1 distinct objects are selected\nfrom the list of N objects. This set of C + 1 objects\nforms a context for the selected word. The correct object\n(i = 1, in this case) must be present in the context. The\nlearner's task is to guess which of the C + 1 objects the\nword refers to. This is then an ambiguous word learning\n\nscenario in which there are multiple object candidates for\nany word.\nThe parameter C is a measure of the ambiguity (and\nso of the difficulty) of the learning task. In particular,\nin the case C = N \u2212 1 the word-object mapping is unlearnable. At first sight one could expect that learning\nwould be trivial for C = 0 since there is no ambiguity, but\nthe learning complexity depends also on the manner the\nobjects are selected to compose the contexts. Typically,\nthe objects are chosen randomly and without replacement from the list of N objects (see, e.g., [23\u201325]), which\nfor C = 0 results in a learning error (i.e., the fraction of\nwrong word-object associations) that decreases exponentially with learning rate \u2212 ln (1 \u2212 1/N ) as the number\nof learning trials t increases. This is so because there\nis a non-vanishing probability that some words are not\nselected in the t trials [25].\nIn order to avoid testing subjects on the meaning of\nwords they never heard, most experimental studies on\nword-learning mechanisms use a deterministic word selection procedure which guarantees that all words are\nuttered before the testing stage, although some words\nmay be spoken more frequently than others [4\u20137]. Hence\nwe consider here, in addition to the random selection procedure, a deterministic selection procedure which guarantees that all N words are selected in t = N trials. For\nthis procedure the case C = 0 is trivial and the learning\nerror becomes zero at t = N . However, since encountering words whose meaning is unknown is not a rare\nevent in the real world (hence the utility of dictionaries), a non-uniform Zipfian random selection of words is\nlikely to be a more realistic sampling scheme for learning\nnatural word-referent associations (see, e.g., [25]).\n\nIII.\n\nMINIMAL ASSOCIATIVE LEARNING\nALGORITHM\n\nHere we consider one of the earliest mathematical\nlearning models \u2013 the linear learning model [26]. The\nbasic assumption of this model is that learning can be\nmodeled as a change in the confidence with which the\nlearner associates the target word to a certain object in\nthe context. More to the point, this confidence is represented by a matrix whose non-negative integer entries\nphi yield a value for the confidence with which word h\nis associated to object i. We assume that at the outset\n(t = 0) all confidences are set to zero, i.e., phi = 0 with\ni, h = 1, . . . , N and whenever object i\u2217 appear in a context in companion with target word h\u2217 the confidence\npi\u2217 h\u2217 increases by one unit. Hence at each learning trial,\nC + 1 confidences are updated. Note that this learning\nalgorithm considers reinforcement only.\nTo determine which object corresponds to word h the\nlearner simply chooses the object index i for which phi\nis maximum. In the case of ties, the learner selects one\nobject at random among those that maximize the confidence phi . Recalling our definition of the correct word-\n\n\f3\nobject mapping in the previous section, the learning algorithm achieves a perfect performance when phh > phi\nfor all h and i 6= h. The learning error E at a given trial\nt is then given by the fraction of wrong word-object associations. Note that we have phi \u2264 phh with i 6= h since\nobject i = h must appear in the contexts of all learning\nevents in which the target word is h (see Sect. II). In\nthis case, the learning error of any single word, say h,\nwhich we denote by \u000fsw , is the reciprocal of the number\nof objects for which phi = phh with i 6= h.\nInterestingly, it can easily be shown that this very simple and general learning algorithm is identical to the algorithm presented in [24] which is based on detecting the\nintersections of context realizations in order to single out\nthe set of confounder objects at a given trial t. This\nequivalence has already been noted in the literature [27]\n(see also [8]). The minimal associative learning algorithm\ncan be immediately adapted to incorporate more realistic features, such as finite memory and imprecision in\nthe comparison of magnitudes, whereas the confounder\nreducing algorithm is restricted to an ideal learning scenario.\nA salient feature of the minimal associative learning\nalgorithm which allows the analytical study of its performance is the fact that words are learned independently. This is easily seen by noting that the confidences\nphi , i = 1, . . . , N are updated only when the target word\nh is selected. This means that, aside from a trivial rescaling of the learning time, our scenario is equivalent to\nthe experimental settings (see Sect. IV) in which C + 1\ntarget words are presented together with a context exhibiting C + 1 objects, with each object associated to\none of the target words [4\u20137]. Taking advantage of this\nfeature, we will first solve a simplified version of the crosssituational learning in which a given target word h (and\nits associated object i = h) appears in all learning trials whereas the C other objects (the confounders) that\nmake up the rest of the context vary in each learning trial.\nOnce the problem of learning a single word is solved (see\nSect. III A), we can easily work out the generalization to\nlearning the whole lexicon (see Sects. III B and III C). We\nwill use \u03c4 to measure the time of the learning trials in\nthe case of single-word learning and t in the whole lexicon\nlearning case.\n\nA.\n\nLearning a single word\n\nBefore any learning event has taken place, the target\nword may be associated to any one of the N objects, so\nthe initial state of the learning error is always equal to\n(N \u2212 1) /N . When the first learning event takes place,\nthe target word may be incorrectly assigned to the C\nother confounder objects shown in the context, so the\nprobability of error at the first trial is always equal to\nC/ (C + 1). In the second trial, there are two possibilities: the probability of error is unchanged because the\nsame context is chosen or the probability of error de-\n\ncreases to the value n/ (n + 1) l with n < C because n\nconfounder objects of the first context appeared again in\nthe second trial. The same reasoning allows us to describe the probability of error in any trial given that this\nprobability is known in the previous trial as described\nnext.\nAs pointed out, the possible error values are n/ (n + 1)\nwith n = 0, 1, ..., C. Labeling these values by the index\nn, the probability of error at trial \u03c4 can be written as\nW (\u03c4 ) = (wC (\u03c4 ) , wC\u22121 (\u03c4 ) , * * * , w1 (\u03c4 ) , w0 (\u03c4 )) .\n\n(1)\n\nThe time evolution of W (\u03c4 ) is given by the Markov chain\nW (\u03c4 + 1) = W (\u03c4 ) T,\n\n(2)\n\nwhere T is a (C + 1) \u00d7 (C + 1) transition matrix whose\nentries Tmn yield the probability that the error at\na certain trial is n/ (n + 1) given that the error was\nm/ (m + 1) in the previous trial. Clearly, Tmn = 0 for\nm < n since the error cannot increase during the learning\nstage in the absence of noise.\nIt is a simple matter to derive Tmn for m \u2265 n [24]. In\nfact, it is given by the probability that in C choices one\nselects exactly n of the m confounder objects from the\nlist of N \u22121 objects. (We recall that the object associated\nto the target word is picked with certainty and so the list\ncomprises N \u2212 1 objects, rather than N , and the number\nof selections is C rather than C + 1.) This is given by\nthe hyper-geometric distribution [28]\n\u0012 \u0013\u0012\n\u0013\nm N \u22121\u2212m\nn\nC \u2212n\n\u0013\n\u0012\nTmn =\n(3)\nN \u22121\nC\nfor m \u2265 n and Tmn = 0 for m < n. Since the\ntransition matrix is triangular, its eigenvalues \u03bbn with\nn = 0, 1, ..., C are the elements of the main diagonal that\ncorrespond to transitions that leave the learning error\nunchanged, i.e.,\n\u0012\n\u0013\nN \u22121\u2212n\nC \u2212n\n\u0013 .\n\u03bbn = Tnn = \u0012\n(4)\nN \u22121\nC\nNote that \u03bb0 = 1 > \u03bbn6=1 > 0 as expected for eigenvalues\nof a transition matrix. In addition, since \u03bbn /\u03bbn+1 =\n(N \u2212 1 \u2212 n) / (C \u2212 n) > 1 the eigenvalues are ordered\nsuch that \u03bb0 > \u03bb1 > . . . > \u03bbN \u22121 .\nRecalling that the probability vector is known at \u03c4 = 1,\nnamely, W1 = (1, 0, . . . , 0) we can write\nW (\u03c4 ) = W (\u03c4 = 1) T \u03c4 \u22121 .\n\n(5)\n\nAlthough it is a simple matter to write T \u03c4 \u22121 in terms of\nthe right and left eigenvectors of T , this procedure does\nnot produce an explicit analytical expression for Wn (\u03c4 )\n\n\f4\n100\n\nthe average learning error for a single word as\n\n10-1\n\n\u000fsw (\u03c4 ) =\n\n\u0395sw\n\n10-2\n10-3\n10-4\n10-5\n\n10\n\n20\n\n30\n\n40\n\u03a4\n\n50\n\n60\n\n70\n\n80\n\n10-1\n10-2\n\u0395sw\n\nn\nWn (\u03c4 ) ,\nn\n+\n1\nn=0\n\n(7)\n\nwhich is valid for \u03c4 > 0 only. For \u03c4 = 0 one has\n\u000fsw (0) = 1 \u2212 1/N . The dependence of \u000fsw on the number of learning trials \u03c4 for different values of N and C is\nillustrated in Fig. 1 using a semi-logarithmic scale. Except for very small \u03c4 , the learning error exhibits a neat\nexponential decay which is revealed by considering only\nthe leading non-vanishing contribution to Wn for large \u03c4 ,\nnamely,\n\u0014\n\u0012\n\u0013\u0015\nC \u03c4 \u22121\nN \u22121\nN \u22121\n\u000fsw (\u03c4 ) \u223c \u03bb1 =\nexp \u2212\u03c4 ln\n. (8)\n2\n2\nC\n\n100\n\nHence the learning rate for single-word learning is\n\n10-3\n10\n\nC\nX\n\n\u03b1sw = ln [(N \u2212 1) /C]\n\n(9)\n\n-4\n\n10-5\n10-6\n10-7\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n\u03a4\n\nFIG. 1: (Color online) The expected single-word learning error \u000fsw as a function of the number of learning trials \u03c4 . The\nsolid curves are the results of Eq. (7) and the filled circles the\nresults of Monte Carlo simulations. The upper panel shows\nthe results for C = 2 and (left to right) N = 100, 50, 30 and\n20, and the lower panel for N = 20 and (left to right) C = 5,\n10, 13, 15 and 16.\n\nin terms of the two parameters of the model C and N ,\nsince we are not able to find analytical expressions for the\neigenvectors. However, Smith et al. [24] have succeeded\nin deriving a closed analytical expression for Wn (\u03c4 ) using\nthe inclusion-exclusion principle of combinatorics [29],\n\u0012 \u0013X\n\u0012\n\u0013\nC\nC\ni\u2212n C \u2212 n\nWn (\u03c4 ) =\n(\u22121)\n\u03bb\u03c4i \u22121 ,\nn i=n\ni\u2212n\n\n(6)\n\nwhere \u03bbi , given by Eq. (4), is the probability that a\nparticular set of i members of the C confounders in the\nfirst learning episode \u03c4 = 1 appear in any subsequent\nepisode. Although the spectral decomposition of T plays\nno role in the derivation of Eq. (6) we choose to maintain\nthe notation \u03bbi for the above mentioned probability.\nRecalling that a situation described by n corresponds\nto the learning error n/ (n + 1) we can immediately write\n\nwhich is zero in the case C = N \u2212 1, i.e., all objects appear in the context and so learning is impossible. In the\ncase C = 0, the learning rate diverges so that \u000fsw = 0\nat the first learning trial \u03c4 = 1 already. Most interestingly, the learning rate increases with increasing N (see\nFig. 1) indicating that the larger the number of objects,\nthe faster the learning of a single word. This apparently counterintuitive result has a simple explanation: a\nlarge list of objects to select from actually decreases the\nchances of choosing the same confounding object during\nthe learning events.\n\nB.\n\nLearning the whole lexicon with random\nsampling\n\nWe turn now to the original learning problem in which\nthe learner has to acquire the one-to-one mapping between the N words and the N objects. In this section\nwe focus in the case the target word at each learning\ntrial is chosen randomly from the list of N words. Since\nall words have the same probability of being chosen, the\nprobability of choosing a particular word is 1/N .\nAt trial t we assume that word 1 appeared k1 times,\nword 2 appeared k2 times, and so on with k1 + k2 + . . . +\nkN = t. The integers ki = 0, . . . , t are random variables\ndistributed by the multinomial\nP (k1 , . . . , kN ) = N \u2212t\n\nt!\n\u03b4t,k1 +...+kN .\nk1 ! * * * kN !\n\n(10)\n\nClearly, if word i appeared ki times in the course of t\ntrials then the expected error associated to it is \u000fsw (ki )\nwith the (word independent) single word error given by\nEq. (7) for ki > 0. With this observation in mind, we\ncan immediately write the expected learning error in the\n\n\f5\n100\n10-1\n\nEr\n\n10-2\n10-3\n10-4\n10-5\n\n10\n\n0\n\n200\n\n400\n\n600\nt\n\n800\n\n1000\n\n1200\n\nwith \u03bbi given by Eq. (4). This is a formidable expression\nwhich can be evaluated numerically for C not too large\nand in Fig. 2 we exhibit the dependence of Er on the\nnumber of learning trials for a selection of values of N\nand C.\nTo obtain the asymptotic time dependence of Er we\nneed to keep in the double sum only the leading order\nterm. Since the summand in Eq. (12) vanishes for n = 0,\nthe largest eigenvalue that appears in that expression is\n\u03bb1 , corresponding to the term i = n = 1, and so this is\nthe term that dominates the sum in the limit t \u2192 \u221e.\nHence Er exhibits the exponential decay\nC\nEr \u223c\n2\u03bb1\n\n0\n\n\u0012\n\n\u03bb1 + N \u2212 1\nN\n\n\u0013t\n=\n\nN \u22121\nexp [\u2212t\u03b1r (C, N )]\n2\n(13)\n\nwhere\n\n10-1\n\n\"\n10-2\nEr\n\n\u03b1r (C, N ) = ln\n\n#\n\nN (N \u2212 1)\nC + (N \u2212 1)\n\n2\n\n(14)\n\n10-3\n10\n\n-4\n\n10-5\n\n0\n\n100\n\n200\n\n300\nt\n\n400\n\n500\n\n600\n\nFIG. 2: (Color online) The expected learning error Er in the\ncase the N words are sampled randomly as a function of the\nnumber of learning trials t. The solid curves are the results\nof Eq. (12) and the filled circles the results of Monte Carlo\nsimulations. The upper panel shows the results for C = 2\nand (left to right) N = 10, 20, . . . , 80 and the lower panel the\nresults for N = 20 and (left to right) C = 1, 2, . . . , 10.\n\ncase the N words are sampled randomly,\nEr (t) =\n\nX\nk1 ,...,kN\n\nP (k1 , . . . , kN )\n\nN\n1 X\n\u000fsw (ki )\nN i=1\n\n\u0013k \u0012\n\u0013t\u2212k\nt \u0012 \u0013\u0012\nX\n1\nt\n1\n=\n1\u2212\n\u000fsw (k) .(11)\nk\nN\nN\nk=0\n\nThe sum over k can be easily carried out provided we\ntake into account the fact that \u000fsw (k) has different prescriptions for the cases k = 0 and k > 0. We find\n\u0012 \u0013X\n\u0013\nC\nC \u0012\ni\u2212n\nX\nn\nC\nC \u2212 n (\u22121)\n\u00d7\nEr (t) =\nn + 1 n i=n i \u2212 n\n\u03bbi\nn=0\n\"\u0012\n\u0013t \u0012\n\u0013t #\n\u03bbi + N \u2212 1\nN \u22121\n\u2212\nN\nN\n\u0012\n\u0013t+1\nN \u22121\n+\n(12)\nN\n\nis the learning rate of our algorithm in the case the N\nwords are sampled randomly. As already mentioned, it is\ninteresting that the unambiguous learning scenario C = 0\nresults in the finite learning rate \u2212 ln (1 \u2212 1/N ) simply\nbecause some words may never be chosen in the course\nof the t learning trials. Interestingly, the learning rate\n\u03b1r exhibits a non-monotone dependence on N for fixed\nC: for N > 2C + 1, it decreases with increasing N (this\nis the parameter selection used to draw the upper panel\nof Fig. 2), and it increases with increasing N otherwise.\nRecalling that for fixed C the minimum value of N is\nN = C + 1 at which \u03b1r = 0, increasing N from this\nminimal value must result in an increase of \u03b1r . The fact\nthat \u03b1r decreases for large N \u2013 an effect of sampling \u2013\nimplies that there is an optimal value N \u2217 = 2C + 1 that\nmaximizes the learning speed for fixed C. Of course, for\nfixed N the learning speed is maximized by C = 0.\n\nC.\n\nLearning the whole lexicon with deterministic\nsampling\n\nTo better understand the effects of the random sampling of the N words we consider here a deterministic\nsampling scheme in which every word is guaranteed to\nbe chosen in the course of N learning trials. Let us begin\nwith the first N learning trials and recall that at time\nt = 0 all words have error \u000fsw (0) = (N \u2212 1) /N . Then\nduring the learning process for t = 1, . . . , N there will\nbe t words with error \u000fsw (1) = C/ (C + 1) and N \u2212 t\nwith error \u000fsw (0) so that the total learning error for the\ndeterministic sampling is\nEd (t) =\n\n1\n[t\u000fsw (1) + (N \u2212 t) \u000fsw (0)] ,\nN\n\nt \u2264 N.\n(15)\n\n\f6\n100\n10-1\n\nEd\n\n10-2\n10-3\n10-4\n10-5\n\n0\n\n100\n\n200\nt\n\n300\n\n400\n\n100\n10-1\n\nin the absence of ambiguity, the learning task should be\ncompleted in N steps. In fact, the learning error decreases linearly with t as given by Eq. (15). Similarly to\nour findings for the random sampling, \u03b1d exhibits a nonmonotonic dependence on N : beginning from \u03b1d = 0 at\nN = C + 1, it increases until reaching a maximum at\nN \u2217 \u2248 eC and then decreases towards zero again as the\nsize of the lexicon further increases.\nIt is interesting to compare the learning rates for the\ntwo sampling schemes, Eqs. (14) and (17). In the leading non-vanishing order for large N and C \u001c N , we find\n\u03b1r \u2248 C/N 2 whereas \u03b1d \u2248 (ln N ) /N . In the more realistic situation in which the context size grows linearly with\nthe lexicon size, i.e., C = \u03b3N with \u03b3 \u2208 [0, 1], for large N\nwe find \u03b1r \u2248 (1 \u2212 \u03b3) /N and \u03b1d \u2248 \u2212 (ln \u03b3) /N . Hence for\nsmall C or \u03b3 \u2248 0, the deterministic sampling of words results in much faster learning than the random sampling.\nFor large C or \u03b3 \u2248 1, however, the two sampling schemes\nproduce equivalent results.\n\nEd\n\n10-2\nIV.\n\n10-3\n10-4\n10-5\n\n0\n\n100\n\n200\n\n300\n\n400\n\nt\nFIG. 3: (Color online) The expected learning error Ed for the\ncase the N words are sampled deterministically as a function\nof the number of learning trials t. The solid curves are the\nresults of Eq. (16) and the filled circles the results of Monte\nCarlo simulations. The upper panel shows the results for C =\n2 and (left to right) N = 10, 20, . . . , 100 and the lower panel\nthe results for N = 20 and (left to right) C = 1, 2, . . . , 10.\n\nThis expression can be easily extended for general t by\nintroducing the single-word learning time \u03c4 = bt/N c,\n1\n[(t \u2212 N \u03c4 ) \u000fsw (\u03c4 + 1) + (N \u03c4 + N \u2212 t) \u000fsw (\u03c4 )]\nN\n(16)\nwhere bxc is the largest integer not greater than x. The\ntime-dependence of the learning error for the deterministic sampling of the N words is shown in Fig. 3. For\nt \u001d N , \u03c4 becomes a continuous variable for any practical purpose, and then we can see that Ed decreases exponentially with increasing t. Clearly, the learning rate\nis determined by the single-word learning error [see Eq.\n(8)] and so replacing \u03c4 by t/N in that equation we obtain\nthe learning rate for the deterministic sampling case\n\u0014\n\u0015\n1\nN \u22121\n\u03b1d (C, N ) =\nln\n.\n(17)\nN\nC\n\nEd (t) =\n\nAs in the single-word learning case, the learning rate diverges for C = 0 in accordance with our intuition that\n\nEFFECTS OF IMPERFECT MEMORY AND\nDISCRIMINABILITY\n\nThe simplicity of the minimal associative learning algorithm analyzed in the previous section is deceiving. In\nfact, the algorithm contains two assumptions that make\nit extremely powerful. The first assumption is illimited\nmemory, since the algorithm stores the confidence values\nfrom the very first to the last learning episode, regardless\nof the number of learning episodes. The second is perfect discriminability, since it always identifies the largest\nconfidence regardless of the closeness to, say, the secondlargest one.\nThe scheme we use to relax the perfect discriminability\nassumption is inspired by Weber's law, which asserts that\nthe discriminability of two perceived magnitudes is determined by the ratio of the objective magnitudes. Accordingly, we assume that the probability that the algorithm\nselects object\nP i as the referent of any given word h is simply phi / j phj , so that referents with similar confidence\nvalues have similar probabilities of being selected. This\ndiffers from the original minimal algorithm for which the\nreferent selection probability is either one or zero, except\nin the case of ties when the probability is divided equally\namong the referents with identical confidence values.\nForgetting or decaying of the confidence values is implemented by subtracting a fixed factor \u03b2 \u2208 [0, 1] from\nthe confidences phi , i = 1, . . . , N whenever word h is absent from a learning episode. The problem with this procedure is that the confidence values may become negative and when this happens we reset them to zero. Another difficulty that may rise is when phi = 0 for all\ni = 1, . . . , N and in this case we reset phi = 1/N for all\ni = 1, . . . , N . These resetting procedures are responsible for the discontinuities observed in the performance\nof the algorithm as we will see next. As in the minimal\nalgorithm, we add 1 to the confidences associated to the\n\n\f7\n0.7\n9\n\n0.6\n1 - X\u0395\\\n\n0.5\n0.4\n0.3\n3\n\n0.2\n0.1\n0.0\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n\u0392\nFIG. 4: (Color online) Expected accuracy for the two frequency condition as function of the forgetting parameter \u03b2 at\nlearning trial t = 27. The curves show the accuracy of the\nset of words sampled 9 and 3 times as indicated in the figure.\nThe horizontal lines and the shaded zones are the experimental results [6]. For \u03b2 \u2248 0.16 we get an excellent agreement\nbetween the model and experiments.\n\n0.7\n9\n\n0.6\n0.5\n1 - X\u0395\\\n\ntarget word and the objects exhibited in the context.\nRelaxation of the perfect memory assumption makes\nthe forgetting parameter \u03b2 dependent on the sampling\nscheme of words, which precludes an analytical approach\nto this problem. As we have to resort to simulations to\nstudy the performance of the modified algorithm anyway, in this section we consider a very specific sampling\nscheme used in experiments with adult subjects to test\nthe effect of varying the frequency of presentation of the\ntarget words on their learning performances [6]. More\nimportantly, use of this sampling scheme allows us to\ncompare quantitatively the performance of the minimal\nas well as of the modified associative learning algorithms\nwith the performances of the adult subjects.\nThe experiment we consider here aims at evaluating\nthe performance of the associative algorithms in learning\na mapping between N = 18 words and N = 18 objects\nafter 27 training episodes [6]. Each episode comprises\nthe presentation of 4 objects together with their corresponding words. Following Ref. [6], we investigate two\nconditions. In the two frequency condition, the 18 words\nare divided into two subsets of 9 words each. In the first\nsubset the 9 words appear 9 times and in the second only\n3 times (see Fig. 4). In the three frequency condition,\nthe 18 words are divided in three subsets of 6 words each.\nIn the first subset, the 6 words appear 3 times, in the second, 6 times and in the third, 9 times (see Fig. 5). In\nthese two conditions, the same word was not allowed to\nappear in two consecutive learning episodes.\nOnce the cross-situational learning scenario is defined,\nwe carry out 104 runs of the modified associative learning\nalgorithm for a fixed value of the forgetting parameter.\nThe results are shown in terms of the average accuracy\n1 \u2212 h\u000fi as function of \u03b2 in Figs. 4 and 5. The horizontal\nstraight lines and the shaded zones around them represent the means and standard deviations of the results of\nexperiments carried out with 33 adult subjects [6].\nBefore discussing the interesting dependence of the accuracy on the forgetting parameter exhibited in Figs. 4\nand 5, a word is in order about the performance of the\noriginal minimal algorithm that is not shown in those\nfigures. In the two frequency condition, the mean accuracy is 0.99 for words in the 9-repetition subset and\n0.90 for those in the 3-repetition subset. In the three frequency condition, the mean accuracy is 0.99 for words\nin the 9- and 6-repetition subsets, and 0.91 for those\nin the 3-repetition subset. These accuracy values are\nwell above those exhibited in Figs. 4 and 5. Moreover,\nadding the forgetting factor to the minimal associative\nalgorithm does not affect its performance, since subtracting the same quantity from all confidence values phi for\na fixed word h does not alter the rank order of these\nconfidences.\nAlthough we intuitively expect that words that appear\nmore frequently would be learned better, this outcome\nactually depends on the value of the forgetting parameter as shown in Figs. 4 and 5. This counterintuitive\nfinding was first observed in the three frequency condi-\n\n6\n\n0.4\n0.3\n\n3\n\n0.2\n0.1\n0.0\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n\u0392\nFIG. 5: (Color online) Expected accuracy for the three frequency condition as function of the forgetting parameter \u03b2 at\nlearning trial t = 27. The curves show the accuracy of the set\nof words sampled 9, 6 and 3 times as indicated in the figure.\nThe horizontal lines and the shaded zones are the experimental results [6]. For \u03b2 \u2248 0.08 we get an excellent agreement\nbetween the model and experiments.\n\ntion experiment on adult subjects [6]. In fact, the results\nof those experiments (i.e., the expected accuracies) can\nbe described very well by choosing \u03b2 = 0.16 in the two\nfrequency condition and \u03b2 = 0.08 in the three frequency\ncondition.\nIt is interesting that the choice of a moderate value for\nthe forgetting parameter \u03b2 may result in a considerable\nimprovement of the performance of the algorithm. This\nis a direct consequence of Weber's law prescription for\nthe discrimination of the confidence values and so there\nis a synergy between discrimination and memory in our\nalgorithm. To see this we note that at a given learning\ntrial the ratio between the probabilities of selecting refer-\n\n\fent i = 1 and referent i = 2 for a word h is r = ph1 /ph2 .\nIf word h does not appear in the next trial then this ratio\nbecomes\n\u0014\n\u0015\nph1 \u2212 \u03b2\n\u03b2\nr0 =\n\u2248r 1+\n(ph1 \u2212 ph2 )\n(18)\nph2 \u2212 \u03b2\nph1 ph2\nso that r0 > r if ph1 > ph2 , thus implying that the forgetting parameter helps the discrimination of the largest\nconfidence. Of course, too large values of \u03b2 deteriorate\nthe performance of the algorithm as shown in the figures.\nWe note that the dents and jumps in the learning curves\nare not statistical fluctuations but consequences of the\ndiscontinuities introduced by the ad hoc regularization\nprocedures discussed before.\nThe above analysis, summarized in part by Figs. 4\nand 5, evinces the better performance of the associative\nalgorithm with perfect storage and discrimination capabilities when compared with humans' performance for a\nfinite number of learning trials (t = 27, in the case). In\naddition, it shows that introduction of imprecision in the\ndiscrimination of confidence values following Weber's law\nprescription together with forgetting brings that performance down to the human level.\nFor the sake of completeness, it would be interesting\nto compare the performance of the minimal associative\nalgorithm with humans' performance in the limit of very\nlong learning times, which was in fact the main focus of\nSect. III. As there are no such experiments \u2013 we guess\nit would be nearly impossible to keep the subjects' attention focused on such boring tasks for too long \u2013 next\nwe compare the performance of the minimal algorithm\nwith the performance of a rather sophisticated learning\nalgorithm which, among other things, models the attention of the learners to regular and novel words [7]. The\nalgorithm is described briefly as follows. At any given\ntrial, the confidence values phi are adjusted according to\nthe update rule\nphi exp [\u03bb (Hh + Hi )]\np0hi = \u03b2\u0302phi + \u03c7 P\nhi phi exp [\u03bb (Hh + Hi )]\n\n(19)\n\nwhere\n\nX\u0395\\\n\n8\n\u00e0\nae\n\u00e7\nae\n100\u00ec\nae\n\u00e0ae\n\u00e7\n\u00e0aeaeaeae\n\u00ec\n\u00ec\n\u00ec\n\u00e0\u00ec\u00ec aeaeae\n\u00e7\n\u00e7 \u00e0\u00e0\u00ec\u00ec\u00ec\nae\u00ec\n\u00e7\n\u00e0 \u00ec\nae\u00ec\n\u00e7\naeae\n\u00e0\n\u00ec\u00ec\n\u00e0\naeae\u00ec\u00ec\n10-1 \u00e7\u00e7\n\u00e0\n\u00e7\naeae\u00ec\u00ec\n\u00e0\n\u00e7\n\u00e0\naeae\u00ec\u00ec\n\u00e7\n\u00e0\n\u00e7\naeae \u00ec\u00ec\n-2\n\u00e0\n\u00e7\n10\naeae \u00ec\u00ec\n\u00e0\n\u00e7\n\u00e0\naeae \u00ec\u00ec\n\u00e7\n\u00e0\naeae \u00ec\u00ec\n\u00e0\n\u00e7\n\u00e0\n\u00e7\naeae \u00ec\u00ec\n\u00e0\n\u00e7\n10-3\naeae \u00ec\u00ec\n\u00e0\n\u00e7\n\u00e0\naeae \u00ec\n\u00e0\n\u00e7\n\u00e0\naeae \u00ec\u00ec\n\u00e7\n\u00e0\naeae \u00ec\u00ec\n\u00e7\n\u00e0\n-4\n\u00e0\n\u00e7\n10\naeae \u00ec\u00ec\n\u00e0\n\u00e7\naeae\n\u00ec\n\u00e0\n\u00e7\n\u00e0\naeae \u00ec\u00ec\n\u00e0\u00e0\n\u00e7\n\u00ec\nae\n\u00e7\n10-5\n0\n200\n400\n600\n800\n1000\nt\n\nFIG. 6: (Color online) Expected learning error for N = 10 and\nC = 2 as function of the number of learning trials t in the case\nwords are sampled randomly. The open circles are results of\nthe minimal associative algorithm whereas the filled symbols\nare the results of the algorithm proposed by Karchergis et\nal. [7]: diamonds (\u03c7 = 3.01, \u03bb = 1.39, \u03b2\u0302 = 0.64), circles\n(\u03c7 = 0.31, \u03bb = 2.34, \u03b2\u0302 = 0.91), and squares (\u03c7 = 0.20, \u03bb =\n0.88, \u03b2\u0302 = 0.96).\n\nIII (i.e., one target word and C +1 objects in the context)\nfor randomly sampled words.\nFigure 6 summarizes our findings for N = 10, C = 2\nand three selection of the parameter set (\u03c7, \u03bb, \u03b2\u0302) used by\nKarchergis et al. to reproduce the experimental results\n[7]. The symbols in this figure represent an average over\n104 independent samples. The expected learning error\ndecreases exponentially with increasing t and the rate of\nlearning (the slope of the learning curves for large t in\nthe semi-log scale) is roughly insensitive to the choice of\nthe parameters of the algorithm. As expected from our\nprevious analysis of short learning times, the minimal\nassociative learning algorithm performs much better than\nthe more realistic algorithm. These conclusions hold true\nfor a vast variety of different selections of N and C, as\nwell as for the deterministic word sampling scheme.\n\nV.\n\nHh = \u2212\n\nX\n\n\u039bhi ln \u039bhi\n\n(20)\n\ni\n\nP\n\nDISCUSSION\n\nwith \u039bhi = phi / i phi , and similarly for Hi with the indexes of the sums running over the set of words [7]. In\nthis equation the entropies Hh and Hi are used as measures of the novelty of word h and object i at the current\nlearning episode. The parameter \u03b2\u0302 governs forgetting, \u03c7\nis the weight distributed among the potential associations\nin the trial, and \u03bb weights the uncertainty (entropies) and\nprior knowledge (phi ). We refer the reader to Ref. [7] for\na detailed explanation of the algorithm as well as for a\ncomparison with experimental results for short learning\ntimes. Here we present its performance in acquiring the\nword-object mapping in the simplified scenario of Sect.\n\nAs the problem of learning a lexicon within a crosssituational scenario was studied rather extensively by\nSmith et al. [24], it is appropriate that we highlight\nour original contributions to the subject in this concluding section. Although we have borrowed from that work\na key result for the problem of learning a single word,\nnamely, Eq. (6), even in this case the focal points of our\nstudies deviate substantially. In fact, throughout the paper our main goal was the determination of the learning\nrates in several learning scenarios, whereas the main interest of Smith et al. was in quantifying the number of\nlearning trials required to learn a word with a fixed given\nprobability [24]. In addition, those authors addressed the\nproblem of the random sampling of words using various\n\n\f9\napproximations, leading to inexact results from where\nthe learning rate \u03b1r , see Eq. (14), cannot be recovered.\nAs a result, the interesting non-monotonic dependence of\n\u03b1r (and \u03b1d , as well) on the size N of the lexicon passed\nunnoticed. The study of the deterministic sampling of\nwords and the introduction and analysis of the effects\nof limited storage and discrimination capabilities on the\noriginal minimal associative algorithm are original contributions of our paper.\nWe note that in the cross-situational scenarios studied\npreviously [24, 25] the set of objects that can be associated to a given word is word-dependent, rather than\nconstant as considered here. In other words, if the target\nword is h then the elements of the context in a learning\nepisode are drawn from a fixed subset of Nh \u2264 N objects. These subsets can freely overlap with each other.\nHere we have assumed Nh = N for h = 1, . . . , N . Of\ncourse, this generalization does not affect the analysis of\nthe single-word learning, except that \u000fsw becomes worddependent since the parameter N is replaced by Nh [see\nEq. (8)] and similarly for the learning rate \u03b1sw [see Eq.\n(9)]. More importantly, since words are learned independently by the minimal associative algorithm, the singleword learning errors contribute additively to the total\nlexicon learning error regardless of the sampling procedure [see Eqs. (11) and (16)]. Hence the asymptotic behavior of the total error is determined by the word that\ntakes the longest to be acquired, i.e., the word with the\nlowest learning rate or equivalently with the smallest subset cardinality Nh . With this in mind we can easily obtain the learning rates for this more general situation,\nnamely, \u03b1r = ln {N (Nm \u2212 1) / [C + (Nm \u2212 1) (N \u2212 1)]}\nand \u03b1d\n= ln [(Nm \u2212 1) /C] /N where Nm\n=\nminh {Nh , h = 1, . . . , N }. As expected, in the case Nm =\nN these expressions reduce to Eqs. (14) and (17).\nThe cross-situational learning scenario considered here,\nas well as those used in experimental studies, does not\naccount for the presence of external noise, such as the effect of out-of-context target words. This situation can be\nmodeled by introducing a probability \u03b3 \u2208 [0, 1] that the\ncorrect object is not part of the context so the target word\ncan be said to be out of context. Since we have assumed\nthat learning is based on the perception of differences in\nthe co-occurrence of objects and target words, in the case\nall N objects have the same probability of being selected\nto form the contexts regardless of the target word, such a\npurely observational learning is clearly unattainable. To\ndetermine the critical value of the noise parameter \u03b3c at\nwhich this situation occurs we simply equate the probability of selecting the correct object with the probability\nof selecting any given confounding object to compose the\ncontext in a learning episode,\n1 \u2212 \u03b3c =\n\n\u03b3c (C + 1)\n(1 \u2212 \u03b3c ) C\n+\n,\nN \u22121\nN \u22121\n\n(21)\n\n[1] P. Bloom, How children learn the meaning of words, MIT\nPress, Cambridge, MA, 2000.\n\nfrom where we get\n\nC +1\n\u03b3c = 1 \u2212\n.\n(22)\nN\nSince in this case all objects and all words are equivalent, in the sense they have the same probability of\nco-occurrence, the average single-word learning error, as\nwells as the total error regardless of the sampling scheme,\nis simply \u000fsw = 1 \u2212 1/N . We refer the reader to Ref.\n[30] for a detailed study of the behavior of the minimal\nassociative learning algorithm near the critical noise parameter using statistical mechanics techniques. Here we\nemphasize that the existence of \u03b3c is not dependent on\nthe algorithm used to learn the word-object mapping.\nRather, it is a limitation of cross-situational learning in\ngeneral.\nThe simplifying feature of our model that allowed an\nanalytical approach, as well as extremely efficient Monte\nCarlo simulations (in all graphs the error bars were\nsmaller than the symbol sizes), is the fact that words\nare learned independently from each other. In this context, the minimal associative algorithm considered here\ncorresponds to the optimal learning strategy. Moreover,\nthe fact that the minimal associative algorithm exhibits\neffectively illimited storage and discrimination capabilities makes its learning performance much superior to\nthat of adult subjects in controlled experiments [6] and\nto that of sophisticated algorithms designed to capture\nthe strategies used by humans in the observational learning task [7]. Interestingly, introduction of errors in the\ndiscrimination of the confidence values using Weber's law\nreduced the performance of the minimal algorithm to the\nlevel reported in the experiments. Perhaps, sophisticated\nlearning strategies such as the mutual exclusivity constraint [15], which directs children to map novel words to\nunnamed referents, have evolved to compensate the limitations imposed by Weber's law to evaluate the frequency\nof co-occurrence of words and referents.\n\nAcknowledgments\n\nThis research was supported by The Southern Office of\nAerospace Research and Development (SOARD), Grant\nNo. FA9550-10-1-0006, and Conselho Nacional de Desenvolvimento Cient\u0131\u0301fico e Tecnol\u00f3gico (CNPq). P.F.C.T.\nwas supported by Funda\u00e7\u00e3o de Amparo \u00e0 Pesquisa do\nEstado de S\u00e3o Paulo (FAPESP).\n\n[2] R. Adolphs, Cognitive neuroscience of human social be-\n\n\f10\nhaviour, Nature Reviews Neuroscience 4 (2003) 165\u2013178.\n[3] E. Bates, J. Elman, Learning rediscovered. Science 274\n(1996) 1849\u20131850.\n[4] C. Yu, L.B. Smith, Statistical Cross-Situational Learning\nto Build Word-to World Mappings, Proceedings of the\n28th Annual Conference of the Cognitive Science Society,\nCognitive Science Society, Austin, TX, 2006, pp. 918\u2013\n923.\n[5] C. Yu, L.B. Smith, Rapid word learning under uncertainty via cross-situational statistics, Psychological Science 18 (2007) 414\u2013420.\n[6] G. Kachergis, C. Yu, R.M. Shiffrin, Frequency and\nContextual Diversity Effects in Cross-Situational Word\nLearning, Proceedings of the 31st Annual Conference of\nthe Cognitive Science Society, Cognitive Science Society,\nAustin, TX, 2009, pp. 755\u2013760.\n[7] G. Kachergis, C. Yu, R.M. Shiffrin, An Associative Model\nof Adaptive Inference for Learning Word-Referent Mappings, Psychonomic Bulletin & Review 19 (2012) 317\u2013\n324.\n[8] K. Smith, A.D.M Smith, R.A. Blythe, Cross-situational\nlearning: An experimental study of word-learning mechanisms. Cognitive Science 35 (2011) 480\u2013498.\n[9] C. Yu, L.B. Smith, Modeling cross-situational wordreferent learning: Prior questions. Psychological Review 119\n(2012) 21\u201339.\n[10] J.F. Fontanari, A. Cangelosi, Cross-situational and supervised learning in the emergence of communication.\nInteraction Studies 12 (2011) 119\u2013133.\n[11] W.V.O. Quine, Word and object, MIT Press, Cambridge,\nMA, 1960.\n[12] S. Pinker, Language learnability and language development, Harvard University Press, Cambridge, MA, 1984.\n[13] L. Gleitman, The structural sources of verb meanings,\nLanguage Acquisition 1 (1990) 1\u201355.\n[14] J.M. Siskind, A computational study of cross-situational\ntechniques for learning word-to-meaning mappings, Cognition 61 (1996) 39\u201391 .\n[15] E.M. Markman, Constraints children place on word\nlearning, Cognitive Science 14 (1990) 57\u201377 .\n[16] F. Xu, J. Tenenbaum, Word learning as Bayesian inference, Psychological Review 114 (2007) 245\u2013272.\n[17] M. Frank, N. Goodman, J. Tenenbaum, A Bayesian\nFramework for Cross-Situational Word-Learning, Ad-\n\n[18]\n\n[19]\n\n[20]\n\n[21]\n\n[22]\n\n[23]\n\n[24]\n\n[25]\n\n[26]\n[27]\n\n[28]\n\n[29]\n[30]\n\nvances in Neural Information Processing Systems 20\n(2008) 457\u2013464.\nS.R. Waxman, S.A. Gelman, Early word-learning entails\nreference, not merely associations, Trends in Cognitive\nSciences 13 (2009) 258\u2013263.\nV.M. Sloutsky, H. Kloos, A.V. Fisher, When looks are\neverything: appearance similarity versus kind information in early induction. Psychological Science 18 (2007)\n179\u2013185.\nC. Yu, A statistical associative account of vocabulary\ngrowth in early word learning, Language Learning and\nDevelopment 4 (2008) 32\u201362.\nA.D.M. Smith, Semantic generalization and the inference\nof meaning, Lecture Notes in Artificial Intelligence 2801\n(2003) 499\u2013506.\nA.D.M. Smith, Intelligent meaning creation in a clumpy\nworld helps communication, Artificial Life 9 (2003) 557\u2013\n574.\nJ.F. Fontanari, V. Tikhanoff, A. Cangelosi, R. Ilin, L.I.\nPerlovsky, Cross-situational learning of object-word mapping using Neural Modeling Fields, Neural Networks 22\n(2009) 579\u2013585.\nK. Smith, A.D.M Smith, R.A. Blythe, P. Vogt, CrossSituational Learning: A Mathematical Approach, Lecture Notes in Computer Science 4211 (2006) 31\u201344.\nR.A. Blythe, K. Smith, A.D.M. Smith, Learning Times\nfor Large Lexicons Through Cross-Situational Learning,\nCognitive Science 34 (2010) 620\u2013642.\nR.R. Bush, F. Mosteller, Stochastic Models for Learning,\nWiley, New York, 1955.\nP. Vogt, A.D.M Smith, Quantifying lexicon acquisition\nunder uncertainty, Proceedings of the Annual Machine\nLearning Conference of Belgium and The Netherlands\n(Benelearn) 2004, Brussels.\nW. Feller, Introduction to Probability Theory and its\nApplications, vol. 1, third ed., John Wiley & Sons, New\nYork, 1968.\nP. Cameron, Combinatorics: Topics, Techniques, Algorithms, Cambridge University Press, Cambridge, 1994.\nP.F.C. Tilles, J.F. Fontanari, Critical behavior in a crosssituational lexicon learning scenario, Europhysics Letters\n99 (2012) 60001.\n\n\f"}