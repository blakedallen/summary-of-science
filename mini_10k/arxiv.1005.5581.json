{"id": "http://arxiv.org/abs/1005.5581v2", "guidislink": true, "updated": "2010-10-29T09:44:44Z", "updated_parsed": [2010, 10, 29, 9, 44, 44, 4, 302, 0], "published": "2010-05-31T03:59:35Z", "published_parsed": [2010, 5, 31, 3, 59, 35, 0, 151, 0], "title": "Multi-View Active Learning in the Non-Realizable Case", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1005.4525%2C1005.0092%2C1005.0621%2C1005.2358%2C1005.2372%2C1005.0744%2C1005.4427%2C1005.4522%2C1005.4672%2C1005.0192%2C1005.3805%2C1005.0281%2C1005.1928%2C1005.3406%2C1005.0186%2C1005.1631%2C1005.3736%2C1005.4463%2C1005.1054%2C1005.2350%2C1005.4978%2C1005.5408%2C1005.1748%2C1005.4269%2C1005.1578%2C1005.0112%2C1005.0530%2C1005.3999%2C1005.0478%2C1005.5129%2C1005.4076%2C1005.1760%2C1005.3874%2C1005.0893%2C1005.4886%2C1005.0869%2C1005.1147%2C1005.4435%2C1005.2952%2C1005.3373%2C1005.2532%2C1005.1901%2C1005.3801%2C1005.2657%2C1005.2813%2C1005.4586%2C1005.3201%2C1005.3347%2C1005.3980%2C1005.3161%2C1005.5581%2C1005.1293%2C1005.1885%2C1005.1851%2C1005.0333%2C1005.2431%2C1005.2777%2C1005.2053%2C1005.3447%2C1005.5619%2C1005.0899%2C1005.3858%2C1005.0464%2C1005.0235%2C1005.4368%2C1005.5058%2C1005.2400%2C1005.3042%2C1005.0127%2C1005.3391%2C1005.3166%2C1005.3637%2C1005.0345%2C1005.3865%2C1005.2586%2C1005.5297%2C1005.3652%2C1005.1517%2C1005.0027%2C1005.4036%2C1005.5028%2C1005.3054%2C1005.0895%2C1005.1137%2C1005.4193%2C1005.0573%2C1005.2179%2C1005.2149%2C1005.0351%2C1005.4571%2C1005.4411%2C1005.3904%2C1005.1929%2C1005.2721%2C1005.4214%2C1005.2625%2C1005.1965%2C1005.3219%2C1005.2012%2C1005.0947%2C1005.4897&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Multi-View Active Learning in the Non-Realizable Case"}, "summary": "The sample complexity of active learning under the realizability assumption\nhas been well-studied. The realizability assumption, however, rarely holds in\npractice. In this paper, we theoretically characterize the sample complexity of\nactive learning in the non-realizable case under multi-view setting. We prove\nthat, with unbounded Tsybakov noise, the sample complexity of multi-view active\nlearning can be $\\widetilde{O}(\\log\\frac{1}{\\epsilon})$, contrasting to\nsingle-view setting where the polynomial improvement is the best possible\nachievement. We also prove that in general multi-view setting the sample\ncomplexity of active learning with unbounded Tsybakov noise is\n$\\widetilde{O}(\\frac{1}{\\epsilon})$, where the order of $1/\\epsilon$ is\nindependent of the parameter in Tsybakov noise, contrasting to previous\npolynomial bounds where the order of $1/\\epsilon$ is related to the parameter\nin Tsybakov noise.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1005.4525%2C1005.0092%2C1005.0621%2C1005.2358%2C1005.2372%2C1005.0744%2C1005.4427%2C1005.4522%2C1005.4672%2C1005.0192%2C1005.3805%2C1005.0281%2C1005.1928%2C1005.3406%2C1005.0186%2C1005.1631%2C1005.3736%2C1005.4463%2C1005.1054%2C1005.2350%2C1005.4978%2C1005.5408%2C1005.1748%2C1005.4269%2C1005.1578%2C1005.0112%2C1005.0530%2C1005.3999%2C1005.0478%2C1005.5129%2C1005.4076%2C1005.1760%2C1005.3874%2C1005.0893%2C1005.4886%2C1005.0869%2C1005.1147%2C1005.4435%2C1005.2952%2C1005.3373%2C1005.2532%2C1005.1901%2C1005.3801%2C1005.2657%2C1005.2813%2C1005.4586%2C1005.3201%2C1005.3347%2C1005.3980%2C1005.3161%2C1005.5581%2C1005.1293%2C1005.1885%2C1005.1851%2C1005.0333%2C1005.2431%2C1005.2777%2C1005.2053%2C1005.3447%2C1005.5619%2C1005.0899%2C1005.3858%2C1005.0464%2C1005.0235%2C1005.4368%2C1005.5058%2C1005.2400%2C1005.3042%2C1005.0127%2C1005.3391%2C1005.3166%2C1005.3637%2C1005.0345%2C1005.3865%2C1005.2586%2C1005.5297%2C1005.3652%2C1005.1517%2C1005.0027%2C1005.4036%2C1005.5028%2C1005.3054%2C1005.0895%2C1005.1137%2C1005.4193%2C1005.0573%2C1005.2179%2C1005.2149%2C1005.0351%2C1005.4571%2C1005.4411%2C1005.3904%2C1005.1929%2C1005.2721%2C1005.4214%2C1005.2625%2C1005.1965%2C1005.3219%2C1005.2012%2C1005.0947%2C1005.4897&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The sample complexity of active learning under the realizability assumption\nhas been well-studied. The realizability assumption, however, rarely holds in\npractice. In this paper, we theoretically characterize the sample complexity of\nactive learning in the non-realizable case under multi-view setting. We prove\nthat, with unbounded Tsybakov noise, the sample complexity of multi-view active\nlearning can be $\\widetilde{O}(\\log\\frac{1}{\\epsilon})$, contrasting to\nsingle-view setting where the polynomial improvement is the best possible\nachievement. We also prove that in general multi-view setting the sample\ncomplexity of active learning with unbounded Tsybakov noise is\n$\\widetilde{O}(\\frac{1}{\\epsilon})$, where the order of $1/\\epsilon$ is\nindependent of the parameter in Tsybakov noise, contrasting to previous\npolynomial bounds where the order of $1/\\epsilon$ is related to the parameter\nin Tsybakov noise."}, "authors": ["Wei Wang", "Zhi-Hua Zhou"], "author_detail": {"name": "Zhi-Hua Zhou"}, "author": "Zhi-Hua Zhou", "arxiv_comment": "22 pages, 1 figure", "links": [{"href": "http://arxiv.org/abs/1005.5581v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1005.5581v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1005.5581v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1005.5581v2", "journal_reference": null, "doi": null, "fulltext": "Multi-View Active Learning in the Non-Realizable Case\nWei Wang, Zhi-Hua Zhou\u2217\n\narXiv:1005.5581v2 [cs.LG] 29 Oct 2010\n\nNational Key Laboratory for Novel Software Technology\nNanjing University, Nanjing 210093, China\n\nAbstract\nThe sample complexity of active learning under the realizability assumption has been well-studied.\nThe realizability assumption, however, rarely holds in practice. In this paper, we theoretically\ncharacterize the sample complexity of active learning in the non-realizable case under multiview setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi1\ne\nview active learning can be O(log\n\u01eb ), contrasting to single-view setting where the polynomial\n\nimprovement is the best possible achievement. We also prove that in general multi-view setting\n\ne 1 ), where the order\nthe sample complexity of active learning with unbounded Tsybakov noise is O(\n\u01eb\n\nof 1/\u01eb is independent of the parameter in Tsybakov noise, contrasting to previous polynomial\nbounds where the order of 1/\u01eb is related to the parameter in Tsybakov noise.\nKey words: active learning, non-realizable case\n\n1. Introduction\n\nIn active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution\ndefined on the learning task and actively queries some labels from an oracle. In this way, the\nactive learner can achieve good performance with much fewer labels than passive learning. The\nnumber of these queried labels, which is necessary and sufficient for obtaining a good leaner, is\nwell-known as the sample complexity of active learning.\nMany theoretical bounds on the sample complexity of active learning have been derived based\non the realizability assumption (i.e., there exists a hypothesis perfectly separating the data in\n\u2217\n\nCorresponding author. Email: zhouzh@nju.edu.cn\n\nPreprint submitted for review\n\nOctober 18, 2018\n\n\fthe hypothesis class) [4, 5, 11, 12, 14, 16]. The realizability assumption, however, rarely holds in\npractice. Recently, the sample complexity of active learning in the non-realizable case (i.e., the\ndata cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise)\nhas been studied [2, 13, 17]. It is worth noting that these bounds obtained in the non-realizable\n2\n\ncase match the lower bound \u03a9( \u03b7\u01eb2 ) [19], in the same order as the upper bound O( \u01eb12 ) of passive\nlearning (\u03b7 denotes the generalization error rate of the optimal classifier in the hypothesis class\nand \u01eb bounds how close to the optimal classifier in the hypothesis class the active learner has to\nget). This suggests that perhaps active learning in the non-realizable case is not as efficient as that\nin the realizable case. To improve the sample complexity of active learning in the non-realizable\ncase remarkably, the model of the noise or some assumptions on the hypothesis class and the\ndata distribution must be considered. Tsybakov noise model [21] is more and more popular in\ntheoretical analysis on the sample complexity of active learning. However, existing result [8]\nshows that obtaining exponential improvement in the sample complexity of active learning with\nunbounded Tsybakov noise is hard.\nInspired by [23] which proved that multi-view setting [6] can help improve the sample complexity\nof active learning in the realizable case remarkably, we have an insight that multi-view setting\nwill also help active learning in the non-realizable case. In this paper, we present the first analysis\non the sample complexity of active learning in the non-realizable case under multi-view setting,\nwhere the non-realizability is caused by Tsybakov noise. Specifically:\n-We define \u03b1-expansion, which extends the definition in [3] and [23] to the non-realizable case,\nand \u03b2-condition for multi-view setting.\n-We prove that the sample complexity of active learning with Tsybakov noise under multi-view\n1\n1\ne\nsetting can be improved to O(log\n\u01eb ) when the learner satisfies non-degradation condition. This\n\nexponential improvement holds no matter whether Tsybakov noise is bounded or not, contrasting\n\nto single-view setting where the polynomial improvement is the best possible achievement for\nactive learning with unbounded Tsybakov noise.\n-We also prove that, when non-degradation condition does not hold, the sample complexity of\ne 1 ), where the order\nactive learning with unbounded Tsybakov noise under multi-view setting is O(\n\u01eb\n1\n\ne notation is used to hide the factor log log( 1 ).\nThe O\n\u01eb\n\n2\n\n\fof 1/\u01eb is independent of the parameter in Tsybakov noise, i.e., the sample complexity is always\ne 1 ) no matter how large the unbounded Tsybakov noise is. While in previous polynomial\nO(\n\u01eb\n\nbounds, the order of 1/\u01eb is related to the parameter in Tsybakov noise and is larger than 1\nwhen unbounded Tsybakov noise is larger than some degree (see Section 2). This discloses\nthat, when non-degradation condition does not hold, multi-view setting is still able to lead to a\nfaster convergence rate and our polynomial improvement in the sample complexity is better than\nprevious polynomial bounds when unbounded Tsybakov noise is large.\nThe rest of this paper is organized as follows. After introducing related work in Section 2 and\npreliminaries in Section 3, we define \u03b1-expansion in the non-realizable case in Section 4. Then we\nanalyze the sample complexity of active learning with Tsybakov noise under multi-view setting\nwith and without the non-degradation condition in Section 5 and Section 6, respectively, and\nverify the improvement in the sample complexity empirically in Section 7. Finally we conclude\nthe paper in Section 8.\n\n2. Related Work\n\nGenerally, the non-realizability of learning task is caused by the presence of noise. For learning\nthe task with arbitrary forms of noise, Balcan et al. [2] proposed the agnostic active learning\nb \u03b722 ).2 Hoping to get tighter bound on\nalgorithm A2 and proved that its sample complexity is O(\n\u01eb\n\nthe sample complexity of the algorithm A2 , Hanneke [17] defined the disagreement coefficient \u03b8,\nwhich depends on the hypothesis class and the data distribution, and proved that the sample\nb 2 \u03b722 ). Later, Dasgupta et al. [13] developed a general\ncomplexity of the algorithm A2 is O(\u03b8\n\u01eb\n\nagnostic active learning algorithm which extends the scheme in [10] and proved that its sample\nb \u03b722 ).\ncomplexity is O(\u03b8\n\u01eb\nRecently, the popular Tsybakov noise model [21] was considered in theoretical analysis on active\nlearning and there have been some bounds on the sample complexity. For some simple cases,\nwhere Tsybakov noise is bounded, it has been proved that the exponential improvement in the\nsample complexity is possible [4, 7, 18]. As for the situation where Tsybakov noise is unbounded,\n2\n\nb notation is used to hide the factor polylog( 1 ).\nThe O\n\u01eb\n\n3\n\n\fonly polynomial improvement in the sample complexity has been obtained. Balcan et al. [4]\nassumed that the samples are drawn uniformly from the the unit ball in Rd and proved that the\n2 \u0001\nsample complexity of active learning with unbounded Tsybakov noise is O \u01eb\u2212 1+\u03bb (\u03bb > 0 depends\non Tsybakov noise). This uniform distribution assumption, however, rarely holds in practice.\n\nCastro and Nowak [8] showed that the sample complexity of active learning with unbounded\n2\u03bc\u03c9+d\u22122\u03c9\u22121 \u0001\nb \u01eb\u2212\n\u03bc\u03c9\n(\u03bc > 1 depends on another form of Tsybakov noise, \u03c9 \u2265 1\nTsybakov noise is O\ndepends on the H\u00f6lder smoothness and d is the dimension of the data). This result is also based\n\non the strong uniform distribution assumption. Cavallanti et al. [9] assumed that the labels of\nexamples are generated according to a simple linear noise model and indicated that the sample\n2(3+\u03bb)\n\u0001\n\u2212\ncomplexity of active learning with unbounded Tsybakov noise is O \u01eb (1+\u03bb)(2+\u03bb) . Hanneke [18]\n\nproved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample\n2 \u0001\nb \u01eb\u2212 1+\u03bb for active learning with unbounded Tsybakov noise. For active learning\ncomplexity O\nwith unbounded Tsybakov noise, Castro and Nowak [8] also proved that at least \u03a9(\u01eb\u2212\u03c1 ) labels are\n\nrequested to learn an \u01eb-approximation of the optimal classifier (\u03c1 \u2208 (0, 2) depends on Tsybakov\nnoise). This result shows that the polynomial improvement is the best possible achievement for\nactive learning with unbounded Tsybakov noise in single-view setting. Wang [22] introduced\nsmooth assumption to active learning with approximate Tsybakov noise and proved that if the\nclassification boundary and the underlying distribution are smooth to \u03be-th order and \u03be > d,\n2d \u0001\nb \u01eb\u2212 \u03be+d ; if the boundary and the distribution are\nthe sample complexity of active learning is O\n\u0001\ninfinitely smooth, the sample complexity of active learning is O polylog( 1\u01eb ) . Nevertheless, this\n\nresult is for approximate Tsybakov noise and the assumption on large smoothness order (or\ninfinite smoothness order) rarely holds for data with high dimension d in practice.\n\n3. Preliminaries\n\nIn multi-view setting, the instances are described with several different disjoint sets of features.\nFor the sake of simplicity, we only consider two-view setting in this paper. Suppose that X =\nX1 \u00d7 X2 is the instance space, X1 and X2 are the two views, Y = {0, 1} is the label space and\nD is the distribution over X \u00d7 Y . Suppose that c = (c1 , c2 ) is the optimal Bayes classifier, where\nc1 and c2 are the optimal Bayes classifiers in the two views, respectively. Let H1 and H2 be\nthe hypothesis class in each view and suppose that c1 \u2208 H1 and c2 \u2208 H2 . For any instance\n4\n\n\fx = (x1 , x2 ), the hypothesis hv \u2208 Hv (v = 1, 2) makes that hv (xv ) = 1 if xv \u2208 Sv and hv (xv ) = 0\notherwise, where Sv is a subset of Xv . In this way, any hypothesis hv \u2208 Hv corresponds to a subset\nSv of Xv (as for how to combine the hypotheses in the two views, see Section 5). Considering that\nx1 and x2 denote the same instance x in different views, we overload Sv to denote the instance\nset {x = (x1 , x2 ) : xv \u2208 Sv } without confusion. Let Sv\u2217 correspond to the optimal Bayes classifier\ncv . It is well-known [15] that Sv\u2217 = {xv : \u03c6v (xv ) \u2265 12 }, where \u03c6v (xv ) = P (y = 1|xv ). Here, we also\noverload Sv\u2217 to denote the instances set {x = (x1 , x2 ) : xv \u2208 Sv\u2217 }. The error rate of a hypothesis Sv\n\u0001\nunder the distribution D is R(hv ) = R(Sv ) = P r(x1 ,x2 ,y)\u2208D y 6= I(xv \u2208 Sv ) . In general, R(Sv\u2217 ) 6=\n\n0 and the excess error of Sv can be denoted as follows, where Sv \u2206Sv\u2217 = (Sv \u2212 Sv\u2217 ) \u222a (Sv\u2217 \u2212 Sv ) and\nd(Sv , Sv\u2217 ) is a pseudo-distance between the sets Sv and Sv\u2217 .\nZ\n\u2217\n|2\u03c6v (xv ) \u2212 1|pxv dxv , d(Sv , Sv\u2217 )\nR(Sv ) \u2212 R(Sv ) =\n\n(1)\n\nSv \u2206Sv\u2217\n\nLet \u03b7v denote the error rate of the optimal Bayes classifier cv which is also called as the noise rate\nin the non-realizable case. In general, \u03b7v is less than 12 . In order to model the noise, we assume\nthat the data distribution and the Bayes decision boundary in each view satisfies the popular\nTsybakov noise condition [21] that P rxv \u2208Xv (|\u03c6v (xv ) \u2212 1/2| \u2264 t) \u2264 C0 t\u03bb for some finite C0 > 0,\n\u03bb > 0 and all 0 < t \u2264 1/2, where \u03bb = \u221e corresponds to the best learning situation and the noise\nis called bounded [8]; while \u03bb = 0 corresponds to the worst situation. When \u03bb < \u221e, the noise is\ncalled unbounded [8]. According to Proposition 1 in [21], it is easy to know that (2) holds.\nd(Sv , Sv\u2217 ) \u2265 C1 dk\u2206 (Sv , Sv\u2217 )\nHere k =\n\n1+\u03bb\n\u03bb ,\n\n\u22121/\u03bb\n\nC1 = 2C0\n\n(2)\n\n\u03bb(\u03bb + 1)\u22121\u22121/\u03bb , d\u2206 (Sv , Sv\u2217 ) = P r(Sv \u2212 Sv\u2217 ) + P r(Sv\u2217 \u2212 Sv ) is also a\n\npseudo-distance between the sets Sv and Sv\u2217 , and d(Sv , Sv\u2217 ) \u2264 d\u2206 (Sv , Sv\u2217 ) \u2264 1. We will use the\nfollowing lamma [1] which gives the standard sample complexity for non-realizable learning task.\n\nLemma 1 Suppose that H is a set of functions from X to Y = {0, 1} with finite VC-dimension\nV \u2265 1 and D is the fixed but unknown distribution over X \u00d7 Y . For any \u01eb, \u03b4 > 0, there is a\npositive constant C, such that if the size of sample {(x1 , y 1 ), . . . , (xN , y N )} from D is N (\u01eb, \u03b4) =\n\u0001\nC\nV + log( 1\u03b4 ) , then with probability at least 1 \u2212 \u03b4, for all h \u2208 H, the following holds.\n\u01eb2\n\u0001\n\u0001\n1 XN\nI h(xi ) 6= y i \u2212 E(x,y)\u2208D I h(x) 6= y | \u2264 \u01eb\n|\ni=1\nN\n\n5\n\n\f4. \u03b1-Expansion in the Non-realizable Case\n\nMulti-view active learning first described in [20] focuses on the contention points (i.e., unlabeled\ninstances on which different views predict different labels) and queries some labels of them. It is\nmotivated by that querying the labels of contention points may help at least one of the two views\nto learn the optimal classifier. Let S1 \u2295 S2 = (S1 \u2212 S2 ) \u222a (S2 \u2212 S1 ) denote the contention points\nbetween S1 and S2 , then P r(S1 \u2295 S2 ) denotes the probability mass on the contentions points.\n\"\u2206\" and \"\u2295\" mean the same operation rule. In this paper, we use \"\u2206\" when referring the excess\nerror between Sv and Sv\u2217 and use \"\u2295\" when referring the difference between the two views S1 and\nS2 . In order to study multi-view active learning, the properties of contention points should be\nconsidered. One basic property is that P r(S1 \u2295 S2 ) should not be too small, otherwise the two\nviews could be exactly the same and two-view setting would degenerate into single-view setting.\nIn multi-view learning, the two views represent the same learning task and generally are consistent\nwith each other, i.e., for any instance x = (x1 , x2 ) the labels of x in the two views are the same.\nHence we first assume that S1\u2217 = S2\u2217 = S \u2217 . As for the situation where S1\u2217 6= S2\u2217 , we will\ndiscuss on it further in Section 5.2. The instances agreed by the two views can be denoted as\n(S1 \u2229 S2 ) \u222a (S1 \u2229 S2 ). However, some of these agreed instances may be predicted different label\nby the optimal classifier S \u2217 , i.e., the instances in (S1 \u2229 S2 \u2212 S \u2217 ) \u222a (S1 \u2229 S2 \u2212 S \u2217 ). Intuitively,\nif the contention points can convey some information about (S1 \u2229 S2 \u2212 S \u2217 ) \u222a (S1 \u2229 S2 \u2212 S \u2217 ),\nthen querying the labels of contention points could help to improve S1 and S2 . Based on this\nintuition and that P r(S1 \u2295 S2 ) should not be too small, we give our definition on \u03b1-expansion in\nthe non-realizable case.\nDefinition 1 D is \u03b1-expanding if for some \u03b1 > 0 and any S1 \u2286 X1 , S2 \u2286 X2 , (3) holds.\n\u0010\n\u0001\u0011\n\u0001\n\u0001\nP r S1 \u2295 S2 \u2265 \u03b1 P r S1 \u2229 S2 \u2212 S \u2217 + P r S1 \u2229 S2 \u2212 S \u2217\n\n(3)\n\nWe say that D is \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 if the above holds for all\nS1 \u2208 H1 \u2229 X1 , S2 \u2208 H2 \u2229 X2 (here we denote by Hv \u2229 Xv the set {h \u2229 Xv : h \u2208 Hv } for v = 1, 2).\n\u0003\n\u0002\nBalcan et al. [3] also gave a definition of expansion, P r(T1 \u2295T2 ) \u2265 \u03b1 min P r(T1 \u2229T2 ), P r(T1 \u2229T2 ) ,\n\nfor realizable learning task under the assumptions that the learner in each view is never \"confident\nbut wrong\" and the learning algorithm is able to learn from positive data only. Here Tv denotes\n6\n\n\fTable 1: Multi-view active learning with the non-degradation condition\n\nInput: Unlabeled data set U = {x1 , x2 , * * * , } where each example xj is given as a pair (xj1 , xj2 )\nProcess:\nQuery the labels of m0 instances drawn randomly from U to compose the labeled data set L\niterate: i = 0, 1, * * * , s\nTrain the classifier hiv P\n(v = 1, 2) by minimizing the empirical risk with L in each view:\nhiv = arg minh\u2208Hv (x1 ,x2 ,y)\u2208L I(h(xv ) 6= y);\nApply hi1 and hi2 to the unlabeled data set U and find out the contention point set Qi ;\nQuery the labels of mi+1 instances drawn randomly from Qi , then add them into L and delete\nthem from U.\nend iterate\nOutput: hs+ and hs\u2212\n\nthe instances which are classified as positive confidently in each view. Generally, in realizable\nlearning tasks, we aim at studying the asymptotic performance and assume that the performance\nof initial classifier is better than guessing randomly, i.e., P r(Tv ) > 1/2. This ensures that\nP r(T1 \u2229 T2 ) is larger than P r(T1 \u2229 T2 ). In addition, in [3] the instances which are agreed by the\ntwo views but are predicted different label by the optimal classifier can be denoted as T1 \u2229 T2 .\nSo, it can be found that Definition 1 and the definition of expansion in [3] are based on the same\nintuition that the amount of contention points is no less than a fraction of the amount of instances\nwhich are agreed by the two views but are predicted different label by the optimal classifiers.\n\n5. Multi-view Active Learning with Non-degradation Condition\n\nIn this section, we first consider the multi-view learning in Table 1 and analyze whether multiview setting can help improve the sample complexity of active learning in the non-realizable case\nremarkably. In multi-view setting, the classifiers are often combined to make predictions and\nmany strategies can be used to combine them. In this paper, we consider the following two\ncombination schemes, h+ and h\u2212 , for binary classification:\n\uf8f1\n\uf8f1\n\uf8f2 1 if hi (x1 ) = hi (x2 ) = 1\n\uf8f2 0 if hi (x1 ) = hi (x2 ) = 0\n2\n1\n2\n1\nhi+ (x) =\nhi\u2212 (x) =\n\uf8f3 0 otherwise\n\uf8f3 1 otherwise\n\n(4)\n\n5.1. The Situation Where S1\u2217 = S2\u2217\nWith (4), the error rate of the combined classifiers hi+ and hi\u2212 satisfy (5) and (6), respectively.\n\n7\n\n\fR(hi+ ) \u2212 R(S \u2217 ) = R(S1i \u2229 S2i ) \u2212 R(S \u2217 ) \u2264 d\u2206 (S1i \u2229 S2i , S \u2217 )\n\n(5)\n\nR(hi\u2212 ) \u2212 R(S \u2217 ) = R(S1i \u222a S2i ) \u2212 R(S \u2217 ) \u2264 d\u2206 (S1i \u222a S2i , S \u2217 )\n\n(6)\n\nHere Svi \u2282 Xv (v = 1, 2) corresponds to the classifier hiv \u2208 Hv in the i-th round. In each round of\nmulti-view active learning, labels of some contention points are queried to augment the training\ndata set L and the classifier in each view is then refined. As discussed in [23], we also assume that\nthe learner in Table 1 satisfies the non-degradation condition as the amount of labeled training\nexamples increases, i.e., (7) holds, which implies that the excess error of Svi+1 is no larger than\nthat of Svi in the region of S1i \u2295 S2i .\n\u0001\nP r Svi+1 \u2206S \u2217 S1i \u2295 S2i \u2264 P r(Svi \u2206S \u2217 S1i \u2295 S2i )\n\n(7)\n\nTo illustrate the non-degradation condition, we give the following example: Suppose the data in\nXv (v = 1, 2) fall into n different clusters, denoted by \u03c01v , . . . , \u03c0nv , and every cluster has the same\nprobability mass for simplicity. The positive class is the union of some clusters while the negative\nclass is the union of the others. Each positive (negative) cluster \u03c0\u03bev in Xv is associated with only 3\npositive (negative) clusters \u03c0\u03c23\u2212v (\u03be, \u03c2 \u2208 {1, . . . , n}) in X3\u2212v (i.e., given an instance xv in \u03c0\u03bev , x3\u2212v\nwill only be in one of these \u03c0\u03c23\u2212v ). Suppose the learning algorithm will predict all instances in\neach cluster with the same label, i.e., the hypothesis class Hv consists of the hypotheses which do\nnot split any cluster. Thus, the cluster \u03c0\u03bev can be classified according to the posterior probability\nP (y = 1|\u03c0\u03bev ) and querying the labels of instances in cluster \u03c0\u03bev will not influence the estimation of\nthe posterior probability for cluster \u03c0\u03c2v (\u03c2 6= \u03be). It is evident that the non-degradation condition\nholds in this task. Note that the non-degradation assumption may not always hold, and we will\ndiscuss on this in Section 6. Now we give Theorem 1.\nTheorem 1 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 ac2 log\n\n1\n\ncording to Definition 1, when the non-degradation condition holds, if s = \u2308 log 18\u01eb \u2309 and mi =\nC2\n16(s+1) \u0001\n256k C\nV + log( \u03b4 ) , the multi-view active learning in Table 1 will generate two classifiers hs+\nC2\n1\n\nand hs\u2212 , at least one of which is with error rate no larger than R(S \u2217 ) + \u01eb with probability at least\n1 \u2212 \u03b4.\nHere, V = max[V C(H1 ), V C(H2 )] where V C(H) denotes the VC-dimension of the hypothesis\n\nclass H, k =\n\n1+\u03bb\n\u03bb ,\n\n\u22121/\u03bb\n\nC1 = 2C0\n\n\u03bb(\u03bb + 1)\u22121\u22121/\u03bb and C2 =\n\n8\n\n5\u03b1+8\n6\u03b1+8 .\n\n\fProof: Let Qi = S1i \u2295 S2i . First we prove that if each view Xv (v = 1, 2) satisfies Tsybakov\nnoise condition, i.e., P rxv \u2208Xv (|\u03c6v (xv ) \u2212 1/2| \u2264 t) \u2264 C3 t\u03bb3 for some finite C3 > 0, \u03bb3 > 0 and all\n0 < t \u2264 1/2, Tsybakov noise condition can also be met in Qi , i.e.,\n\nP rxv \u2208Qi (|\u03c6v (xv )\u22121/2|\u2264t)\nP r(Qi )\n\n\u2264 C4 t\u03bb4\n\nfor some finite C4 > 0, \u03bb4 > 0 and all 0 < t \u2264 1/2. Suppose Tsybakov noise condition cannot\nbe met in Qi , then for C\u2217 =\nP rxv \u2208Qi (|\u03c6v (xv )\u22121/2|\u2264t)\nP r(Qi )\n\nC3\nP r(Qi )\n\nand \u03bb\u2217 = \u03bb3 , there exists some 0 < t\u2217 \u2264 1/2 to satisfy that\n\n> C\u2217 t\u03bb\u2217 \u2217 . So we get\n\nP rxv \u2208Xv (|\u03c6v (xv ) \u2212 1/2| \u2264 t) \u2265 P rxv \u2208Qi (|\u03c6v (xv ) \u2212 1/2| \u2264 t) > C3 t\u03bb\u2217 3 .\nIt is in contradiction with that Xv satisfies Tsybakov noise condition. Thus, we get that Tsybakov\nnoise condition can also be met in Qi . Without loss of generality, suppose that Tsybakov noise\ncondition in all Qi and Xv can be met for the same finite C0 and \u03bb.\nSince m0 =\n\n256k C\nC12\n\n\u0001\n) , according to Lemma 1 we know that d(Sv0 , S \u2217 ) \u2264\nV + log( 16(s+1)\n\u03b4\n\nprobability at least 1 \u2212\n\n\u03b4\n16(s+1) .\n\nWith d(Sv , Sv\u2217 ) \u2265 C1 dk\u2206 (Sv , Sv\u2217 ), we get d\u2206 (Sv0 , S \u2217 ) \u2264\n\nC1\n16k\n1\n16 .\n\nwith\nIt is\n\neasy to find that d\u2206 (S10 \u2229 S20 , S \u2217 ) \u2264 d\u2206 (S10 , S \u2217 ) + d\u2206 (S20 , S \u2217 ) \u2264 1/8 holds with probability at least\n1\u2212\n\n\u03b4\n8(s+1) .\n\nFor i \u2265 0, mi+1 number of labels are queried randomly from Qi . Thus, similarly according to\nLemma 1 we have d\u2206 (S1i+1 \u2229 S2i+1 | Qi , S \u2217 | Qi ) \u2264 1/8 with probability at least 1 \u2212\nTvi+1 = Svi+1 \u2229 Qi and \u03c4i+1 =\n\nP r(T1i+1 \u2295T2i+1 \u2212S \u2217 )\nP r(T1i+1 \u2295T2i+1 )\n\n\u03b4\n8(s+1) .\n\nLet\n\n\u2212 12 , it is easy to get\n\n\u0001\n\u0001\nP r S \u2217 \u2229 (S1i+1 \u2295 S2i+1 )|Qi \u2212 P r S \u2217 \u2229 (S1i+1 \u2295 S2i+1 )|Qi = \u22122\u03c4i+1 P r(S1i+1 \u2295 S2i+1 |Qi ).\nConsidering the non-degradation condition and d\u2206 (S1i \u2229 S2i |Qi , S \u2217 |Qi ) = d\u2206 (Svi |Qi , S \u2217 |Qi ), we\ncalculate that\nd\u2206 (S1i+1 \u2229 S2i+1 |Qi , S \u2217 |Qi )\n\u0011\n\u0011 1 \u0010\n1\u0010\n=\nd\u2206 (S1i+1 |Qi , S \u2217 |Qi ) + d\u2206 (S2i+1 |Qi , S \u2217 |Qi ) + P r S \u2217 \u2229 (S1i+1 \u2295 S2i+1 )|Qi\n2\n2\n\u0011\n1 \u0010 \u2217\n\u2212 P r S \u2229 (S1i+1 \u2295 S2i+1 )|Qi\n2\n\u0011\n1\u0010\nd\u2206 (S1i |Qi , S \u2217 |Qi ) + d\u2206 (S2i |Qi , S \u2217 |Qi ) \u2212 \u03c4i+1 P r(S1i+1 \u2295 S2i+1 |Qi )\n\u2264\n2\n= d\u2206 (S1i \u2229 S2i |Qi , S \u2217 |Qi ) \u2212 \u03c4i+1 P r(S1i+1 \u2295 S2i+1 |Qi ).\n\n9\n\n\fSo we have\nd\u2206 (S1i+1 \u2229 S2i+1 , S \u2217 )\n= d\u2206 (S1i+1 \u2229 S2i+1 |Qi , S \u2217 |Qi )P r(Qi ) + d\u2206 (S1i+1 \u2229 S2i+1 |Qi , S \u2217 |Qi )P r(Qi )\n\u0001\n1\n\u2264\nP r(Qi ) + d\u2206 (S1i \u2229 S2i |Qi , S \u2217 |Qi )P r(Qi ) \u2212 \u03c4i+1 P r (S1i+1 \u2295 S2i+1 ) \u2229 Qi .\n8\nConsidering d\u2206 (S1i \u2229 S2i |Qi , S \u2217 |Qi )P r(Qi ) = P r(S1i \u2229 S2i \u2212 S \u2217 ) + P r(S1i \u2229 S2i \u2212 S \u2217 ), we have\nd\u2206 (S1i+1 \u2229 S2i+1 , S \u2217 )\n\u0001\n1\n\u2264 P r(S1i \u2229 S2i \u2212 S \u2217 ) + P r(S1i \u2229 S2i \u2212 S \u2217 ) + P r(S1i \u2295 S2i ) \u2212 \u03c4i+1 P r (S1i+1 \u2295 S2i+1 ) \u2229 Qi .\n8\nSimilarly, we get\nd\u2206 (S1i+1 \u222a S2i+1 , S \u2217 )\n\u0001\n1\n\u2264 P r(S1i \u2229 S2i \u2212 S \u2217 ) + P r(S1i \u2229 S2i \u2212 S \u2217 ) + P r(S1i \u2295 S2i ) + \u03c4i+1 P r (S1i+1 \u2295 S2i+1 ) \u2229 Qi .\n8\nLet \u03b3i =\n\nP r(S1i \u2295S2i \u2212S \u2217 )\nP r(S1i \u2295S2i )\n\n\u2212 21 , we have\n\nd\u2206 (S1i \u2229 S2i , S \u2217 ) = d\u2206 (S1i \u2229 S2i |Qi , S \u2217 |Qi )P r(Qi ) + d\u2206 (S1i \u2229 S2i |Qi , S \u2217 |Qi )P r(Qi )\n= (1/2 \u2212 \u03b3i )P r(S1i \u2295 S2i ) + P r(S1i \u2229 S2i \u2212 S \u2217 ) + P r(S1i \u2229 S2i \u2212 S \u2217 )\nand d\u2206 (S1i \u222a S2i , S \u2217 ) = (1/2 + \u03b3i )P r(S1i \u2295 S2i ) + P r(S1i \u2229 S2i \u2212 S \u2217 ) + P r(S1i \u2229 S2i \u2212 S \u2217 ).\nAs in each round of the multi-view active learning some contention points of the two views are\nqueried and added into the training set, the difference between the two views is decreasing, i.e.,\nP r(S1i+1 \u2295 S2i+1 ) is no larger than P r(S1i \u2295 S2i ).\nCase 1: If |\u03c4i+1 | \u2264 \u03b3i , with respect to Definition 1, we have\nd\u2206 (S1i+1 \u222a S2i+1 , S \u2217 )\nd\u2206 (S1i \u222a S2i , S \u2217 )\n\n\u2264\n\u2264\n\n1\ni\n8 P r(S1\n\n\u2295 S2i ) + |\u03c4i+1 |P r(S1i+1 \u2295 S2i+1 ) + \u03b11 P r(S1i \u2295 S2i )\n( 12 + \u03b3i )P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n\n( 18 + \u03b3i )P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n5\u03b1 + 8\n\u2264\n;\n1\n1\ni\ni\ni\ni\n8\u03b1 + 8\n( 2 + \u03b3i )P r(S1 \u2295 S2 ) + \u03b1 P r(S1 \u2295 S2 )\n\nCase 2: If \u2212|\u03c4i+1 | > \u03b3i , with respect to Definition 1, we have\nd\u2206 (S1i+1 \u2229 S2i+1 , S \u2217 )\nd\u2206 (S1i \u2229 S2i , S \u2217 )\n\n\u2264\n\u2264\n\n1\ni\n8 P r(S1\n\n\u2295 S2i ) + |\u03c4i+1 |P r(S1i+1 \u2295 S2i+1 ) + \u03b11 P r(S1i \u2295 S2i )\n( 12 + |\u03b3i |)P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n\n5\u03b1 + 8\n;\n8\u03b1 + 8\n10\n\n\fCase 3: If \u03c4i+1 \u2265 \u03b3i and 0 \u2264 \u03b3i \u2264 14 , with respect to Definition 1, we have\nd\u2206 (S1i+1 \u2229 S2i+1 , S \u2217 )\nd\u2206 (S1i \u2229 S2i , S \u2217 )\n\n\u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n( 12 \u2212 \u03b3i )P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n\u03b1+8\n;\n2\u03b1 + 8\n1\ni\n8 P r(S1\n\n\u2264\n\u2264\n\nCase 4: If \u03c4i+1 \u2265 \u03b3i and\n\n1\n4\n\nd\u2206 (S1i+1 \u222a S2i+1 , S \u2217 )\nd\u2206 (S1i \u222a S2i , S \u2217 )\n\n< \u03b3i \u2264 12 , with respect to Definition 1, we have\n\u2264\n\u2264\n\n1\ni\n8 P r(S1\n\n\u2295 S2i ) + \u03c4i+1 P r(S1i+1 \u2295 S2i+1 ) + \u03b11 P r(S1i \u2295 S2i )\n( 12 + \u03b3i )P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n\n5\u03b1 + 8\n;\n6\u03b1 + 8\n\nCase 5: If \u03c4i+1 < \u03b3i and \u2212 41 \u2264 \u03b3i \u2264 0, with respect to Definition 1, we have\nd\u2206 (S1i+1 \u222a S2i+1 , S \u2217 )\nd\u2206 (S1i \u222a S2i , S \u2217 )\n\n\u2264\n\u2264\n\n\u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n( 12 + \u03b3i )P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n\u03b1+8\n;\n2\u03b1 + 8\n1\ni\n8 P r(S1\n\nCase 6: If \u03c4i+1 < \u03b3i and \u2212 12 \u2264 \u03b3i < \u2212 14 , with respect to Definition 1, we have\nd\u2206 (S1i+1 \u2229 S2i+1 , S \u2217 )\nd\u2206 (S1i \u2229 S2i , S \u2217 )\n\n\u2264\n\u2264\n\n1\ni\n8 P r(S1\n\n\u2295 S2i ) + |\u03c4i+1 |P r(S1i+1 \u2295 S2i+1 ) + \u03b11 P r(S1i \u2295 S2i )\n( 12 + |\u03b3i |)P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n\n5\u03b1 + 8\n;\n6\u03b1 + 8\n\nCase 7: If \u03c4i+1 \u2264 \u2212\u03b3i and 0 \u2264 \u03b3i \u2264 12 , with respect to Definition 1, we have\nd\u2206 (S1i+1 \u222a S2i+1 , S \u2217 )\nd\u2206 (S1i \u222a S2i , S \u2217 )\n\n\u2264\n\u2264\n\n\u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n( 12 + \u03b3i )P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n\u03b1+8\n;\n4\u03b1 + 8\n1\ni\n8 P r(S1\n\nCase 8: If \u03c4i+1 > \u2212\u03b3i and \u2212 12 \u2264 \u03b3i \u2264 0, with respect to Definition 1, we have\nd\u2206 (S1i+1 \u2229 S2i+1 , S \u2217 )\nd\u2206 (S1i \u2229 S2i , S \u2217 )\n\n\u2264\n\u2264\n\n\u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n( 12 + |\u03b3i |)P r(S1i \u2295 S2i ) + \u03b11 P r(S1i \u2295 S2i )\n\u03b1+8\n.\n4\u03b1 + 8\n1\ni\n8 P r(S1\n\nd (S i+1 \u2229S i+1 ,S \u2217 )\n\nd (S i+1 \u222aS i+1 ,S \u2217 )\n\n5\u03b1+8\n5\u03b1+8\nor \u2206d 1(S i \u222aS2i ,S \u2217 ) \u2264 6\u03b1+8\nholds.\nThus, after the (i + 1)-th round, either \u2206d 1(S i \u2229S2i ,S \u2217 ) \u2264 6\u03b1+8\n\u2206\n\u2206 1\n1\n2\n2\n\u0010\n\u0011s/2\n\u0010\n\u0011s/2\n5\u03b1+8\nHence, we have d\u2206 (S1s \u2229 S2s , S \u2217 ) \u2264 18 6\u03b1+8\nor d\u2206 (S1s \u222a S2s , S \u2217 ) \u2264 81 5\u03b1+8\nwith probability\n6\u03b1+8\n\n11\n\n\f1\n8\u01eb\n1\nC2\n\n2 log\n\nat least 1 \u2212 \u03b4. When s = \u2308 log\n\n\u2309, where C2 =\n\n5\u03b1+8\n6\u03b1+8\n\nis a constant less than 1, we have either\n\nd\u2206 (S1s \u2229 S2s , S \u2217 ) \u2264 \u01eb or d\u2206 (S1s \u222a S2s , S \u2217 ) \u2264 \u01eb with probability at least 1 \u2212 \u03b4. Thus, considering\nR(hi+ )\u2212R(S \u2217 ) = R(S1i \u2229S2i )\u2212R(S \u2217 ) \u2264 d\u2206 (S1i \u2229S2i , S \u2217 ) and R(hi\u2212 )\u2212R(S \u2217 ) = R(S1i \u222aS2i )\u2212R(S \u2217 ) \u2264\nd\u2206 (S1i \u222a S2i , S \u2217 ), we have either R(hs+ ) \u2264 R(S \u2217 ) + \u01eb or R(hs\u2212 ) \u2264 R(S \u2217 ) + \u01eb.\n\nFrom Theorem 1 we know that we only need to request\n\nPs\n\ni=0 mi\n\n\u0003\n\n1\ns\ne\n= O(log\n\u01eb ) labels to learn h+\n\nand hs\u2212 , at least one of which is with error rate no larger than R(S \u2217 ) + \u01eb with probability at\nleast 1 \u2212 \u03b4. If we choose hs+ and it happens to satisfy R(hs+ ) \u2264 R(S \u2217 ) + \u01eb, we can get a classifier\nwhose error rate is no larger than R(S \u2217 ) + \u01eb. Fortunately, there are only two classifiers and the\nprobability of getting the right classifier is no less than 21 . To study how to choose between hs+\nand hs\u2212 , we give Definition 2 at first.\nDefinition 2 The multi-view classifiers S1 and S2 satisfy \u03b2-condition if (8) holds for some \u03b2 > 0.\n\u0001\n\u0001\nP r {x : x \u2208 S1 \u2295 S2 \u2227 y(x) = 0}\nP r {x : x \u2208 S1 \u2295 S2 \u2227 y(x) = 1}\n\u2212\n\u2265\u03b2\n(8)\nP r(S1 \u2295 S2 )\nP r(S1 \u2295 S2 )\n(8) implies the difference between the examples belonging to positive class and that belonging\nto negative class in the contention region of S1 \u2295 S2 . Based on Definition 2, we give Lemma 2\nwhich provides information for deciding how to choose between h+ and h\u2212 . This helps to get\nTheorem 2.\n2 log( 4 )\n\nLemma 2 If the multi-view classifiers S1s and S2s satisfy \u03b2-condition, with the number of \u03b2 2 \u03b4\n\u0001\nlabels we can decide correctly whether P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} or P r {x : x \u2208\n\u0001\nS1s \u2295 S2s \u2227 y(x) = 0} ) is smaller with probability at least 1 \u2212 \u03b4.\nProof: We apply S1s and S2s to the unlabeled instances set and identify the contention point\nset. Then we query for labels of\n\n2 log( \u03b44 )\n\u03b22\n\ninstances drawn randomly from the contention points set.\nP r({x:x\u2208S1s \u2295S2s \u2227y(x)=1})\nP r(S1s \u2295S2s )\n2 log( 4 )\nwith number of \u03b2 2 \u03b4\n\nWith these labels we estimate the empirical value Pb1 of\n\nvalue Pb2 of\n\nP r({x:x\u2208S1s \u2295S2s \u2227y(x)=0})\nP r(S1s \u2295S2s )\n\n. By Chernoff bound,\n\nand the empirical\nlabels we have the\n\nfollowing two equations with probability at least 1 \u2212 \u03b4.\nh P r {x : x \u2208 S s \u2295 S s \u2227 y(x) = 1}\u0001 \u03b2 P r {x : x \u2208 S s \u2295 S s \u2227 y(x) = 1}\u0001 \u03b2 i\n1\n2\n1\n2\nb\n\u2212 ,\n+\nP1 \u2208\nP r(S1s \u2295 S2s )\n2\nP r(S1s \u2295 S2s )\n2\nh P r {x : x \u2208 S s \u2295 S s \u2227 y(x) = 0}\u0001 \u03b2 P r {x : x \u2208 S s \u2295 S s \u2227 y(x) = 0}\u0001 \u03b2 i\n1\n2\n1\n2\nPb2 \u2208\n\u2212 ,\n+\nP r(S1s \u2295 S2s )\n2\nP r(S1s \u2295 S2s )\n2\n12\n\n\f\u0001\n\u0001\nIf Pb1 \u2264 Pb2 , we get P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} \u2264 P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0} with\n\u0001\nprobability at least 1 \u2212 \u03b4; otherwise, we get P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} > P r {x : x \u2208\n\u0001\nS1s \u2295 S2s \u2227 y(x) = 0} with probability at least 1 \u2212 \u03b4.\n\u0003\n\nTheorem 2 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to Definition 1, when the non-degradation condition holds, if the multi-view classifiers\n1\ne\nsatisfy \u03b2-condition, by requesting O(log\n\u01eb ) labels the multi-view active learning in Table 1 will\n\ngenerate a classifier whose error rate is no larger than R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4.\n\n1\ne\nProof: According to Theorem 1, by requesting O(log\n\u01eb ) labels the multi-view active learning\n\nin Table 1 can get either R(hs+ ) \u2264 R(S \u2217 ) + \u01eb or R(hs\u2212 ) \u2264 R(S \u2217 ) + \u01eb with probability at least\n2 log( 8 )\n\n1 \u2212 2\u03b4 . According to Lemma 2, by requesting \u03b2 2 \u03b4 labels we can decide correctly whether\n\u0001\n\u0001\nP r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} or P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0} is smaller with probability\n\nat least 1 \u2212 2\u03b4 .\n\n\u0001\n\u0001\nCase 1: If P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} \u2264 P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0} , we have\n\nR(hs\u2212 ) \u2264 R(hs+ ). Thus, we get R(hs\u2212 ) \u2264 R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4.\n\n\u0001\n\u0001\nCase 2: If P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} > P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0} , we have\n\nR(hs+ ) < R(hs\u2212 ). Thus, we get R(hs+ ) \u2264 R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4.\n1\ne\nThe total number of labels to be requested is O(log\n\u01eb) +\n\n2 log( \u03b48 )\n\u03b22\n\n1\ne\n= O(log\n\u01eb ).\n\n\u0003\n\n1\ne\nFrom Theorem 2 we know that we only need to request O(log\n\u01eb ) labels to learn a classifier with\n\nerror rate no larger than R(S \u2217 )+\u01eb with probability at least 1\u2212\u03b4. Thus, we achieve an exponential\n\nimprovement in sample complexity of active learning in the non-realizable case under multi-view\nsetting. Sometimes, the difference between the examples belonging to positive class and that\nbelonging to negative class in S1s \u2295 S2s may be very small, i.e., (9) holds.\n\u0001\n\u0001\nP r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1}\nP r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0}\n\u2212\n= O(\u01eb)\nP r(S1s \u2295 S2s )\nP r(S1s \u2295 S2s )\n\n(9)\n\nIf so, we need not to estimate whether R(hs+ ) or R(hs\u2212 ) is smaller and Theorem 3 indicates that\nboth hs+ and hs\u2212 are good approximations of the optimal classifier.\n13\n\n\fTheorem 3 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7H2 according to Definition 1, when the non-degradation condition holds, if (9) is satisfied, by requesting\n1\ns\ne\nO(log\n\u01eb ) labels the multi-view active learning in Table 1 will generate two classifiers h+ and\n\nhs\u2212 which satisfy either (a) or (b) with probability at least 1 \u2212 \u03b4. (a) R(hs+ ) \u2264 R(S \u2217 ) + \u01eb and\n\nR(hs\u2212 ) \u2264 R(S \u2217 ) + O(\u01eb); (b) R(hs+ ) \u2264 R(S \u2217 ) + O(\u01eb) and R(hs\u2212 ) \u2264 R(S \u2217 ) + \u01eb.\nProof: Since P r(S1s \u2295 S2s ) \u2264 1, with the following equation\n\u0001\n\u0001\nP r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1}\nP r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0}\n\u2212\n= O(\u01eb)\nP r(S1s \u2295 S2s )\nP r(S1s \u2295 S2s )\n\u0001\n\u0001\nwe have |P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} \u2212 P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0} | = O(\u01eb). So it is\n\n1\ne\neasy to get |R(hs+ ) \u2212 R(hs\u2212 )| = O(\u01eb). According to Theorem 1, by requesting O(log\n\u01eb ) labels we\n\ncan get either R(hs+ ) \u2264 R(S \u2217 ) + \u01eb or R(hs\u2212 ) \u2264 R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4. Thus,\nwe get that hs+ and hs\u2212 satisfy either (a) or (b) with probability at least 1 \u2212 \u03b4.\n\n\u0003\n\n5.2. The Situation Where S1\u2217 6= S2\u2217\nAlthough the two views represent the same learning task and generally are consistent with each\nother, sometimes S1\u2217 may be not equal to S2\u2217 . Therefore, the \u03b1-expansion assumption in Definition\n1 should be adjusted to the situation where S1\u2217 6= S2\u2217 . To analyze this theoretically, we replace\nS \u2217 by S1\u2217 \u2229 S2\u2217 in Definition 1 and get (10). Similarly to Theorem 1, we get Theorem 4.\n\u0010\n\u0001\n\u0001\n\u0001\u0011\nP r S1 \u2295 S2 \u2265 \u03b1 P r S1 \u2229 S2 \u2212 S1\u2217 \u2229 S2\u2217 + P r S1 \u2229 S2 \u2212 S1\u2217 \u2229 S2\u2217\n\n(10)\n\nTheorem 4 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 ac2 log\n\n1\n\nk\n\nC\ncording to (10), when the non-degradation condition holds, if s = \u2308 log 18\u01eb \u2309 and mi = 256\nV +\nC12\nC2\n\u0001\n) , the multi-view active learning in Table 1 will generate two classifiers hs+ and hs\u2212 ,\nlog( 16(s+1)\n\u03b4\n\nat least one of which is with error rate no larger than R(S1\u2217 \u2229 S2\u2217 ) + \u01eb with probability at least\n\n1 \u2212 \u03b4. (V , k, C1 and C2 are given in Theorem 1.)\nProof: Since Sv\u2217 is the optimal Bayes classifier in the v-th view, obviously, R(S1\u2217 \u2229 S2\u2217 ) is no less\nthan R(Sv\u2217 ), (v = 1, 2). So, learning a classifier with error rate no larger than R(S1\u2217 \u2229 S2\u2217 ) + \u01eb is\nnot harder than learning a classifier with error rate no larger than R(Sv\u2217 ) + \u01eb. Now we aim at\n14\n\n\flearning a classifier with error rate no larger than R(S1\u2217 \u2229 S2\u2217 ) + \u01eb. Without loss of generality,\nwe assume R(Svi ) > R(S1\u2217 \u2229 S2\u2217 ) for i = 0, 1, . . . , s. If R(Svi ) \u2264 R(S1\u2217 \u2229 S2\u2217 ), we get a classifier\nwith error rate no larger than R(S1\u2217 \u2229 S2\u2217 ) + \u01eb. Thus, we can neglect the probability mass on the\nhypothesis whose error rate is less than R(S1\u2217 \u2229 S2\u2217 ) and regard S1\u2217 \u2229 S2\u2217 as the optimal. Replacing\nS \u2217 by S1\u2217 \u2229 S2\u2217 in the discussion of Section 5.1, with the proof of Theorem 1 we get Theorem 4\n\u0003\n\nproved.\n\n1\ne\nTheorem 4 shows that for the situation where S1\u2217 6= S2\u2217 , by requesting O(log\n\u01eb ) labels we can learn\n\ntwo classifiers hs+ and hs\u2212 , at least one of which is with error rate no larger than R(S1\u2217 \u2229 S2\u2217 ) + \u01eb\nwith probability at least 1 \u2212 \u03b4. With Lemma 2, we get Theorem 5 from Theorem 4.\nTheorem 5 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 ac-\n\ncording to (10), when the non-degradation condition holds, if the multi-view classifiers satisfy\n1\ne\n\u03b2-condition, by requesting O(log\n\u01eb ) labels the multi-view active learning in Table 1 will generate\n\na classifier whose error rate is no larger than R(S1\u2217 \u2229 S2\u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4.\n\n1\ne\nProof: According to Theorem 4, by requesting O(log\n\u01eb ) labels the multi-view active learning in\n\nTable 1 can get either R(hs+ ) \u2264 R(S1\u2217 \u2229 S2\u2217 ) + \u01eb or R(hs\u2212 ) \u2264 R(S1\u2217 \u2229 S2\u2217 ) + \u01eb with probability at\n2 log( 8 )\n\nleast 1 \u2212 \u03b42 . According to Lemma 2, by requesting \u03b2 2 \u03b4 labels we can decide correctly whether\n\u0001\n\u0001\nP r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} or P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0} is smaller with probability\n\nat least 1 \u2212 2\u03b4 .\n\n\u0001\n\u0001\nCase 1: If P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} \u2264 P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0} , we have\n\nR(hs\u2212 ) \u2264 R(hs+ ). Thus, we get R(hs\u2212 ) \u2264 R(S1\u2217 \u2229 S2\u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4.\n\n\u0001\n\u0001\nCase 2: If P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 1} > P r {x : x \u2208 S1s \u2295 S2s \u2227 y(x) = 0} , we have\n\nR(hs+ ) < R(hs\u2212 ). Thus, we get R(hs+ ) \u2264 R(S1\u2217 \u2229 S2\u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4.\n1\ne\nThe total number of labels to be requested is O(log\n\u01eb) +\n\n2 log( \u03b48 )\n\u03b22\n\n1\ne\n= O(log\n\u01eb ).\n\n\u0003\n\nGenerally, R(S1\u2217 \u2229S2\u2217 ) is larger than R(S1\u2217 ) and R(S2\u2217 ). When S1\u2217 is not too much different from S2\u2217 ,\ni.e., P r(S1\u2217 \u2295 S2\u2217 ) \u2264 \u01eb/2, we have Corollary 1 which indicates that the exponential improvement\nin the sample complexity of active learning with Tsybakov noise is still possible.\n15\n\n\fCorollary 1 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to (10), when the non-degradation condition holds, if the multi-view classifiers satisfy\n1\ne\n\u03b2-condition and P r(S1\u2217 \u2295 S2\u2217 ) \u2264 \u01eb/2, by requesting O(log\n\u01eb ) labels the multi-view active learning\n\nin Table 1 will generate a classifier with error rate no larger than R(Sv\u2217 ) + \u01eb (v = 1, 2) with\nprobability at least 1 \u2212 \u03b4.\n1\ne\nProof: According to Theorem 5 we know that by requesting O(log\n\u01eb ) labels the multi-view active\n\nlearning in Table 1 will generate a classifier whose error rate is no larger than R(S1\u2217 \u2229 S2\u2217 ) +\n\n\u01eb\n2\n\nwith probability at least 1 \u2212 \u03b4. Considering that\nZ\n\u2217\n\u2217\n\u2217\n|2\u03c6v (xv ) \u2212 1|pxv dxv \u2264 P r(S1\u2217 \u2295 S2\u2217 ),\nR(S1 \u2229 S2 ) \u2212 R(Sv ) =\n(S1\u2217 \u2229S2\u2217 )\u2206Sv\u2217\n\nwe have R(S1\u2217 \u2229 S2\u2217 ) \u2264 R(Sv\u2217 ) + 2\u01eb . Thus, we get that R(S1\u2217 \u2229 S2\u2217 ) +\n\n\u01eb\n2\n\nis no larger than R(Sv\u2217 ) + \u01eb.\n\n\u0003\n\n6. Multi-view Active Learning without Non-degradation Condition\n\nSection 5 considers situations when the non-degradation condition holds, there are cases, however,\nthe non-degradation condition (7) does not hold. In this section we focus on the multi-view active\nlearning in Table 2 and give an analysis with the non-degradation condition waived. Firstly,\nwe give Theorem 6 for the sample complexity of multi-view active learning in Table 2 when\nS1\u2217 = S2\u2217 = S \u2217 .\nTheorem 6 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 ac\u0001\nkC\n2 log 1\nV + log( 16(s+1)\ncording to Definition 1, if s = \u2308 log 18\u01eb \u2309 and mi = 256\n) , the multi-view active\n\u03b4\nC2\n1\n\nC2\n\nlearning in Table 2 will generate two classifiers hs+ and hs\u2212 , at least one of which is with error\n\nrate no larger than R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4. (V , k, C1 and C2 are given in\nTheorem 1.)\n\nProof: After the i-th round in Table 2, the number of training examples in L is\n\nPi\n\nb\nb=0 2 mi\n\n=\n\n(2i+1 \u2212 1)mi . While in the (i + 1)-th round, we randomly query (2i+1 \u2212 1)mi labels from the\nregion of Qi and add them into L. So in the (i + 1)-th round, the number of training examples\n16\n\n\fTable 2: Multi-view active learning without the non-degradation condition\n\nInput: Unlabeled data set U = {x1 , x2 , * * * , } where each example xj is given as a pair (xj1 , xj2 )\nProcess:\nQuery the labels of m0 instances drawn randomly from U to compose the labeled data set L;\nTrain the classifier h0v P\n(v = 1, 2) by minimizing the empirical risk with L in each view:\nh0v = arg minh\u2208Hv (x1 ,x2 ,y)\u2208L I(h(xv ) 6= y);\niterate: i = 1, * * * , s\nApply hi\u22121\nand hi\u22121\nto the unlabeled data set U and find out the contention point set Qi ;\n1\n2\nQuery the labels of mi instances drawn randomly from Qi , then add them into L and delete them\nfrom U;\nQuery the labels of (2i \u2212 1)mi instances drawn randomly from U \u2212 Qi , then add them into L and\ndelete them from U;\nTrain the classifier hiv P\nby minimizing the empirical risk with L in each view:\nhiv = arg minh\u2208Hv (x1 ,x2 ,y)\u2208L I(h(xv ) 6= y).\nend iterate\nOutput: hs+ and hs\u2212\n\nfor Svi+1 (v = 1, 2) drawn randomly from region of Qi is larger than the number of whole training\nexamples for Svi . Since the optimal Bayes classifier cv belongs to Hv , according to the standard\nPAC-model, it is easy to know that d(Svi+1 |Qi , S \u2217 |Qi ) \u2264 d(Svi |Qi , S \u2217 |Qi ) can be met for any \u03c6v ,\nwhere d(Sv |Qi , S \u2217 |Qi ) is defined as\n\u2217\n\n\u2217\n\nd(Sv |Qi , S |Qi ) , R(Sv |Qi ) \u2212 R(S |Qi ) =\n\nZ\n\n(Sv \u2229Qi )\u2206(S \u2217 \u2229Qi )\n\n\u000e\n|2\u03c6v (xv ) \u2212 1|pxv dxv P r(Qi ).\n\nSo, by setting \u03c6v \u2208 {0, 1}, we get d\u2206 (Svi+1 |Qi , S \u2217 |Qi ) \u2264 d\u2206 (Svi |Qi , S \u2217 |Qi ), which implies the\nnon-degradation condition. Thus, with the proof of Theorem 1, we get Theorem6 proved.\n\nTheorem 6 shows that we can request\n\nPs\n\ni\ni=0 2 mi\n\n\u0003\n\ne 1 ) labels to learn two classifiers hs and\n= O(\n+\n\u01eb\n\nhs\u2212 , at least one of which is with error rate no larger than R(S \u2217 ) + \u01eb with probability at least\n\n1 \u2212 \u03b4. To guarantee the non-degradation condition (7), we only need to query (2i \u2212 1)mi more\nlabels in the i-th round. With Lemma 2, we get Theorem 7.\nTheorem 7 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 ace 1 ) labels\ncording to Definition 1, if the multi-view classifiers satisfy \u03b2-condition, by requesting O(\n\u01eb\n\nthe multi-view active learning in Table 2 will generate a classifier whose error rate is no larger\nthan R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4.\n\nProof:\n\ne 1 ) labels the multi-view active learning in\nAccording to Theorem 6, by requesting O(\n\u01eb\n\nTable 2 will generate two classifiers hs+ and hs\u2212 , at least one of which is with error rate no larger\n17\n\n\fthan R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4. Similarly to the proof of Theorem 2, we get\n\u0003\n\nTheorem 7 proved.\n\ne 1 ) labels\nTheorem 7 shows that, without the non-degradation condition, we need to request O(\n\u01eb\n\nto learn a classifier with error rate no larger than R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4. The\norder of 1/\u01eb is independent of the parameter in Tsybakov noise. Similarly to Theorem 3, we\nget Theorem 8 which indicates that both hs+ and hs\u2212 are good approximations of the optimal\nclassifier.\nTheorem 8 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 ace 1 ) labels the multi-view active learning in\ncording to Definition 1, if (9) holds, by requesting O(\n\u01eb\n\nTable 2 will generate two classifiers hs+ and hs\u2212 which satisfy either (a) or (b) with probability at\n\nleast 1 \u2212 \u03b4. (a) R(hs+ ) \u2264 R(S \u2217 ) + \u01eb and R(hs\u2212 ) \u2264 R(S \u2217 ) + O(\u01eb); (b) R(hs+ ) \u2264 R(S \u2217 ) + O(\u01eb) and\nR(hs\u2212 ) \u2264 R(S \u2217 ) + \u01eb.\nProof:\n\ne 1 ) labels the multi-view active learning in\nAccording to Theorem 6, by requesting O(\n\u01eb\n\nTable 2 will generate two classifiers hs+ and hs\u2212 , at least one of which is with error rate no larger\n\nthan R(S \u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4. Similarly to the proof of Theorem 3, we get\n\u0003\n\nTheorem 8 proved.\n\nAs for the situation where S1\u2217 6= S2\u2217 , similarly to Theorem 5 and Corollary 1, we have Theorem 9\nand Corollary 2.\nTheorem 9 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 ace 1 ) labels the\ncording to (10), if the multi-view classifiers satisfy \u03b2-condition, by requesting O(\n\u01eb\nmulti-view active learning in Table 2 will generate a classifier whose error rate is no larger than\nR(S1\u2217 \u2229 S2\u2217 ) + \u01eb with probability at least 1 \u2212 \u03b4.\nProof:\n\nSimilarly to the proof of Theorem 4 and Theorem 6, we know that by requesting\n\ne 1 ) labels the multi-view active learning in Table 2 can get either R(hs+ ) \u2264 R(S \u2217 \u2229 S \u2217 ) + \u01eb or\nO(\n2\n1\n\u01eb\n\nR(hs\u2212 ) \u2264 R(S1\u2217 \u2229 S2\u2217 ) + \u01eb with probability at least 1 \u2212 2\u03b4 . According to Lemma 2, by requesting\n2 log( 8\u03b4 )\n\u03b22\n\nlabels we can decide correctly whether R(hs+ ) or R(hs\u2212 ) is smaller with probability at\n18\n\n\fleast 1 \u2212 2\u03b4 . Thus, we can get a classifiers whose error rate is no larger than R(S1\u2217 \u2229 S2\u2217 ) + \u01eb with\ne 1) +\nprobability at least 1 \u2212 \u03b4. The total number of labels to be requested is O(\n\u01eb\n\n\u0003\n\n2 log( 8\u03b4 )\n\u03b22\n\ne 1 ).\n= O(\n\u01eb\n\nCorollary 2 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7H2 according to (10), if the multi-view classifiers satisfy \u03b2-condition and P r(S1\u2217 \u2295 S2\u2217 ) \u2264 \u01eb/2, by requesting\ne 1 ) labels the multi-view active learning in Table 2 will generate a classifier with error rate no\nO(\n\u01eb\nlarger than R(Sv\u2217 ) + \u01eb (v = 1, 2) with probability at least 1 \u2212 \u03b4.\n\ne 1 ) labels the multi-view active\nProof: According to Theorem 9 we know that by requesting O(\n\u01eb\n\nlearning in Table 2 will generate a classifier whose error rate is no larger than R(S1\u2217 \u2229 S2\u2217 ) +\nwith probability at least 1 \u2212 \u03b4. With the proof of Corollary 1, we get that R(S1\u2217 \u2229 S2\u2217 ) +\nlarger than R(Sv\u2217 ) + \u01eb.\n\n\u01eb\n2\n\n\u01eb\n2\n\nis no\n\u0003\n\n7. Empirical Verification\n\nIn this section we empirically verify that whether multi-view setting can improve the sample\ncomplexity of active learning in the non-realizable case remarkably.\nIn the experiment we use the semi-artificial data set [20] and the course data set [6]. The semiartificial data set has two artificial views which are created by randomly pairing two examples\nfrom the same class and contains 800 examples. In order to control the correlation between\nthe two views, the number of clusters per class can be set as a parameter. We use 1 cluster,\n2 clusters and 4 clusters in the experiments, respectively. The course data set has two natural\nviews: pages view (i.e., the text appearing on the page) and links view (i.e., the anchor text\nattached to hyper-links pointing to the page) and contains 1,051 examples. We randomly use\n25% data as the test set and use the remaining 75% data to generate the unlabeled data set U .\nWe use Random Sampling as the baseline. In each round, we fix the number of examples to be\nqueried in Multi-View Active Learning and that in Random Sampling. Thus, we can study their\nperformances under the same number of queried examples. In the experiments, we query two\n19\n\n\f0.36\n\n0.30\n\nerror rate\n\n0.30\n\nerror rate\n\n0.36\n\nRandom Sampling\nMulti\u2212View Active Learning\n\n0.24\n0.18\n0.12\n\n0.24\n0.18\n0.12\n\n0.06\n60\n\n100\n140\n180\nnumber of queried labels\n\n0.06\n60\n\n220\n\n(a) semi-artificial with 1 cluster\n0.36\n\n100\n140\n180\nnumber of queried labels\n\n220\n\n(b) semi-artificial with 2 clusters\n0.16\n\nRandom Sampling\nMulti\u2212View Active Learning\n\n0.14\n\nerror rate\n\n0.30\n\nerror rate\n\nRandom Sampling\nMulti\u2212View Active Learning\n\n0.24\n0.18\n0.12\n\nRandom Sampling\nMulti\u2212View Active Learning\n\n0.12\n0.10\n0.08\n\n0.06\n60\n\n100\n140\n180\nnumber of queried labels\n\n0.06\n40\n\n220\n\n(c) semi-artificial with 4 clusters\n\n72\n104\nnumber of queried labels\n\n136\n\n(d) course\n\nFigure 1: Multi-view setting improves the sample complexity of active learning in the non-realizable case remarkably.\n\nexamples in each round of the two methods and implement the classifiers with NaiveBayes in\nWEKA. The experiments are repeated for 20 runs and Figure 1 plots the average error rates of\nthe two methods against the number of examples that have been queried. From Figure 1 it can be\nfound that the performance of Multi-View Active Learning is far better than the performance of\nRandom Sampling with the same number of queried examples. In other words, multi-view setting\ncan help improve the sample complexity of active learning in the non-realizable case remarkably.\n\n8. Conclusion\n\nWe present the first study on active learning in the non-realizable case under multi-view setting\nin this paper. We prove that the sample complexity of multi-view active learning with unbounded\n1\ne\nTsybakov noise can be improved to O(log\n\u01eb ), contrasting to single-view setting where only poly-\n\nnomial improvement is proved possible with the same noise condition. In general multi-view\n20\n\n\fsetting, we prove that the sample complexity of active learning with unbounded Tsybakov noise\ne 1 ), where the order of 1/\u01eb is independent of the parameter in Tsybakov noise, contrasting\nis O(\n\u01eb\n\nto previous polynomial bounds where the order of 1/\u01eb is related to the parameter in Tsybakov\n\nnoise. Generally, the non-realizability of learning task can be caused by many kinds of noise, e.g.,\nmisclassification noise and malicious noise. It would be interesting to extend our work to more\ngeneral noise model.\n\nReferences\n[1] M. Anthony and P. L. Bartlett, editors. Neural Network Learning: Theoretical Foundations.\nCambridge University Press, Cambridge, UK, 1999.\n[2] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, pages\n65\u201372, 2006.\n[3] M.-F. Balcan, A. Blum, and K. Yang. Co-training and expansion: Towards bridging theory\nand practice. In NIPS 17, pages 89\u201396. 2005.\n[4] M.-F. Balcan, A. Z. Broder, and T. Zhang. Margin based active learning. In COLT, pages\n35\u201350, 2007.\n[5] M.-F. Balcan, S. Hanneke, and J. Wortman. The true sample complexity of active learning.\nIn COLT, pages 45\u201356, 2008.\n[6] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT,\npages 92\u2013100, 1998.\n[7] R. M. Castro and R. D. Nowak. Upper and lower error bounds for active learning. In\nAllerton Conference, pages 225\u2013234, 2006.\n[8] R. M. Castro and R. D. Nowak. Minimax bounds for active learning. IEEE Transactions\non Information Theory, 54(5):2339\u20132353, 2008.\n[9] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear classification and selective sampling\nunder low noise conditions. In NIPS 21, pages 249\u2013256. 2009.\n\n21\n\n\f[10] D. A. Cohn, L. E. Atlas, and R. E. Ladner. Improving generalization with active learning.\nMachine Learning, 15(2):201\u2013221, 1994.\n[11] S. Dasgupta. Analysis of a greedy active learning strategy. In NIPS 17, pages 337\u2013344. 2005.\n[12] S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS 18, pages\n235\u2013242. 2006.\n[13] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In\nNIPS 20, pages 353\u2013360. 2008.\n[14] S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis of perceptron-based active learning.\nIn COLT, pages 249\u2013263, 2005.\n[15] L. Devroye, L. Gy\u00f6rfi, and G. Lugosi, editors. A Probabilistic Theory of Pattern Recognition.\nSpringer, New York, 1996.\n[16] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by\ncommittee algorithm. Machine Learning, 28(2-3):133\u2013168, 1997.\n[17] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML, pages\n353\u2013360, 2007.\n[18] S. Hanneke. Adaptive rates of convergence in active learning. In COLT, 2009.\n[19] M. K\u00e4\u00e4ri\u00e4inen. Active learning in the non-realizable case. In ACL, pages 63\u201377, 2006.\n[20] I. Muslea, S. Minton, and C. A. Knoblock. Active + semi-supervised learning = robust\nmulti-view learning. In ICML, pages 435\u2013442, 2002.\n[21] A. Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics, 32(1):135\u2013166, 2004.\n[22] L. Wang. Sufficient conditions for agnostic active learnable. In NIPS 22, pages 1999\u20132007.\n2009.\n[23] W. Wang and Z.-H. Zhou. On multi-view active learning and the combination with semisupervised learning. In ICML, pages 1152\u20131159, 2008.\n\n22\n\n\f"}