{"id": "http://arxiv.org/abs/1203.4598v1", "guidislink": true, "updated": "2012-03-20T21:32:33Z", "updated_parsed": [2012, 3, 20, 21, 32, 33, 1, 80, 0], "published": "2012-03-20T21:32:33Z", "published_parsed": [2012, 3, 20, 21, 32, 33, 1, 80, 0], "title": "Adaptive Mixture Methods Based on Bregman Divergences", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.1814%2C1203.2147%2C1203.5522%2C1203.6509%2C1203.3001%2C1203.2567%2C1203.3894%2C1203.5475%2C1203.4544%2C1203.2756%2C1203.2290%2C1203.0487%2C1203.2854%2C1203.3940%2C1203.5893%2C1203.3524%2C1203.5956%2C1203.4598%2C1203.5523%2C1203.5596%2C1203.0108%2C1203.4896%2C1203.0819%2C1203.1903%2C1203.0378%2C1203.4860%2C1203.1372%2C1203.0653%2C1203.6413%2C1203.1893%2C1203.6599%2C1203.6744%2C1203.3282%2C1203.4234%2C1203.1249%2C1203.2387%2C1203.4257%2C1203.2020%2C1203.6751%2C1203.0066%2C1203.2929%2C1203.4571%2C1203.1833%2C1203.4897%2C1203.0037%2C1203.3584%2C1203.2886%2C1203.2896%2C1203.2998%2C1203.5793%2C1203.1011%2C1203.3896%2C1203.4967%2C1203.2153%2C1203.1678%2C1203.6079%2C1203.1111%2C1203.4799%2C1203.1451%2C1203.0938%2C1203.1402%2C1203.1599%2C1203.6459%2C1203.6554%2C1203.1328%2C1203.2017%2C1203.5567%2C1203.5913%2C1203.4751%2C1203.4443%2C1203.2266%2C1203.2320%2C1203.2838%2C1203.2992%2C1203.0898%2C1203.2677%2C1203.1528%2C1203.3390%2C1203.5768%2C1203.1239%2C1203.3250%2C1203.4581%2C1203.6499%2C1203.5028%2C1203.4074%2C1203.3702%2C1203.1486%2C1203.3832%2C1203.0380%2C1203.0948%2C1203.1296%2C1203.3152%2C1203.6244%2C1203.0175%2C1203.3788%2C1203.2880%2C1203.3400%2C1203.3870%2C1203.3107%2C1203.1025%2C1203.1889&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adaptive Mixture Methods Based on Bregman Divergences"}, "summary": "We investigate adaptive mixture methods that linearly combine outputs of $m$\nconstituent filters running in parallel to model a desired signal. We use\n\"Bregman divergences\" and obtain certain multiplicative updates to train the\nlinear combination weights under an affine constraint or without any\nconstraints. We use unnormalized relative entropy and relative entropy to\ndefine two different Bregman divergences that produce an unnormalized\nexponentiated gradient update and a normalized exponentiated gradient update on\nthe mixture weights, respectively. We then carry out the mean and the\nmean-square transient analysis of these adaptive algorithms when they are used\nto combine outputs of $m$ constituent filters. We illustrate the accuracy of\nour results and demonstrate the effectiveness of these updates for sparse\nmixture systems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.1814%2C1203.2147%2C1203.5522%2C1203.6509%2C1203.3001%2C1203.2567%2C1203.3894%2C1203.5475%2C1203.4544%2C1203.2756%2C1203.2290%2C1203.0487%2C1203.2854%2C1203.3940%2C1203.5893%2C1203.3524%2C1203.5956%2C1203.4598%2C1203.5523%2C1203.5596%2C1203.0108%2C1203.4896%2C1203.0819%2C1203.1903%2C1203.0378%2C1203.4860%2C1203.1372%2C1203.0653%2C1203.6413%2C1203.1893%2C1203.6599%2C1203.6744%2C1203.3282%2C1203.4234%2C1203.1249%2C1203.2387%2C1203.4257%2C1203.2020%2C1203.6751%2C1203.0066%2C1203.2929%2C1203.4571%2C1203.1833%2C1203.4897%2C1203.0037%2C1203.3584%2C1203.2886%2C1203.2896%2C1203.2998%2C1203.5793%2C1203.1011%2C1203.3896%2C1203.4967%2C1203.2153%2C1203.1678%2C1203.6079%2C1203.1111%2C1203.4799%2C1203.1451%2C1203.0938%2C1203.1402%2C1203.1599%2C1203.6459%2C1203.6554%2C1203.1328%2C1203.2017%2C1203.5567%2C1203.5913%2C1203.4751%2C1203.4443%2C1203.2266%2C1203.2320%2C1203.2838%2C1203.2992%2C1203.0898%2C1203.2677%2C1203.1528%2C1203.3390%2C1203.5768%2C1203.1239%2C1203.3250%2C1203.4581%2C1203.6499%2C1203.5028%2C1203.4074%2C1203.3702%2C1203.1486%2C1203.3832%2C1203.0380%2C1203.0948%2C1203.1296%2C1203.3152%2C1203.6244%2C1203.0175%2C1203.3788%2C1203.2880%2C1203.3400%2C1203.3870%2C1203.3107%2C1203.1025%2C1203.1889&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We investigate adaptive mixture methods that linearly combine outputs of $m$\nconstituent filters running in parallel to model a desired signal. We use\n\"Bregman divergences\" and obtain certain multiplicative updates to train the\nlinear combination weights under an affine constraint or without any\nconstraints. We use unnormalized relative entropy and relative entropy to\ndefine two different Bregman divergences that produce an unnormalized\nexponentiated gradient update and a normalized exponentiated gradient update on\nthe mixture weights, respectively. We then carry out the mean and the\nmean-square transient analysis of these adaptive algorithms when they are used\nto combine outputs of $m$ constituent filters. We illustrate the accuracy of\nour results and demonstrate the effectiveness of these updates for sparse\nmixture systems."}, "authors": ["Mehmet A. Donmez", "Huseyin A. Inan", "Suleyman S. Kozat"], "author_detail": {"name": "Suleyman S. Kozat"}, "author": "Suleyman S. Kozat", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.dsp.2012.09.006", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1203.4598v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1203.4598v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Submitted to Digital Signal Processing, Elsevier; IEEE.org", "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1203.4598v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1203.4598v1", "journal_reference": null, "doi": "10.1016/j.dsp.2012.09.006", "fulltext": "Adaptive Mixture Methods Based on\nBregman Divergences\n\narXiv:1203.4598v1 [cs.LG] 20 Mar 2012\n\nMehmet A. Donmez a, Huseyin A. Inan a, Suleyman S. Kozat a,\u2217\n\na Department\n\nof Electrical and Computer Engineering, Koc University, Istanbul,\nTel: 90 212 3501840.\n\nAbstract\nWe investigate adaptive mixture methods that linearly combine outputs of m constituent filters running in parallel to model a desired signal. We use \"Bregman divergences\" and obtain certain multiplicative updates to train the linear combination\nweights under an affine constraint or without any constraints. We use unnormalized\nrelative entropy and relative entropy to define two different Bregman divergences\nthat produce an unnormalized exponentiated gradient update and a normalized exponentiated gradient update on the mixture weights, respectively. We then carry\nout the mean and the mean-square transient analysis of these adaptive algorithms\nwhen they are used to combine outputs of m constituent filters. We illustrate the\naccuracy of our results and demonstrate the effectiveness of these updates for sparse\nmixture systems.\nKey words: Adaptive mixture, Bregman divergence, affine mixture, multiplicative\nupdate.\n\n\u2217 Corresponding author.\nEmail addresses: mdonmez@ku.edu.tr (Mehmet A. Donmez),\nhuseyin.inan@boun.edu.tr (Huseyin A. Inan), skozat@ku.edu.tr (Suleyman S.\nKozat).\n\nPreprint submitted to Digital Signal Processing\n\n11 June 2018\n\n\f1\n\nIntroduction\n\nIn this paper, we study adaptive mixture methods based on \"Bregman divergences\" [1, 2] that combine outputs of m constituent filters running in parallel\non the same task. The overall system has two stages [3\u20138]. The first stage contains adaptive filters running in parallel to model a desired signal. The outputs\nof these adaptive filters are then linearly combined to produce the final output\nof the overall system in the second stage. We use Bregman divergences and\nobtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].\nWe use unnormalized [2] and normalized relative entropy [9] to define two\ndifferent Bregman divergences that produce the unnormalized exponentiated\ngradient update (EGU) and the exponentiated gradient update (EG) on the\nmixture weights [9], respectively. We then perform the mean and the meansquare transient analysis of these adaptive mixtures when they are used to\ncombine outputs of m constituent filters. We emphasize that to the best of\nour knowledge, this is the first mean and mean-square transient analysis of the\nEGU algorithm and the EG algorithm in the mixture framework (which naturally covers the classical framework also [13, 14]). We illustrate the accuracy\nof our results through simulations in different configurations and demonstrate\nadvantages of the introduced algorithms for sparse mixture systems.\nAdaptive mixture methods are utilized in a wide range of signal processing\napplications in order to improve the steady-state and/or convergence performance over the constituent filters [11,12,15]. An adaptive convexly constrained\nmixture of two filters is studied in [15], where the convex combination is shown\nto be \"universal\" such that the combination performs at least as well as its\nbest constituent filter in the steady-state [15]. The transient analysis of this\nadaptive convex combination is studied in [16], where the time evolution of\nthe mean and variance of the mixture weights is provided. In similar lines,\n2\n\n\fan affinely constrained mixture of adaptive filters using a stochastic gradient update is introduced in [11]. The steady-state mean square error (MSE)\nof this affinely constrained mixture is shown to outperform the steady-state\nMSE of the best constituent filter in the mixture under certain conditions [11].\nThe transient analysis of this affinely constrained mixture for m constituent\nfilters is carried out in [17]. The general linear mixture framework as well as\nthe steady-state performances of different mixture configurations are studied\nin [12].\n\nIn this paper, we use Bregman divergences to derive multiplicative updates\non the mixture weights. We use the unnormalized relative entropy and the\nrelative entropy as distance measures and obtain the EGU algorithm and the\nEG algorithm to update the combination weights under an affine constraint\nor without any constraints. We then carry out the mean and the mean-square\ntransient analysis of these adaptive mixtures when they are used to combine\nm constituent filters. We point out that the EG algorithm is widely used in\nsequential learning theory [18] and minimizes an approximate final estimation\nerror while penalizing the distance between the new and the old filter weights.\nIn network and acoustic echo cancellation applications, the EG algorithm is\nshown to converge faster than the LMS algorithm [14, 19] when the system\nimpulse response is sparse [13]. Similarly, in our simulations, we observe that\nusing the EG algorithm to train the mixture weights yields increased convergence speed compared to using the LMS algorithm to train the mixture\nweights [11, 12] when the combination favors only a few of the constituent\nfilters in the steady state, i.e., when the final steady-state combination vector\nis sparse. We also observe that the EGU algorithm and the LMS algorithm\nshow similar performance when they are used to train the mixture weights\neven if the final steady-state mixture is sparse.\n\nTo summarize, the main contributions of this paper are as follows:\n3\n\n\f\u2022 We use Bregman divergences to derive multiplicative updates on affinely\nconstrained and unconstrained mixture weights adaptively combining outputs of m constituent filters.\n\u2022 We use the unnormalized relative entropy and the relative entropy to define\ntwo different Bregman divergences that produce the EGU algorithm and the\nEG algorithm to update the affinely constrained and unconstrained mixture\nweights.\n\u2022 We perform the mean and the mean-square transient analysis of the affinely\nconstrained and unconstrained mixtures using the EGU algorithm and the\nEG algorithm.\nThe organization of the paper is as follows. In Section II, we first describe\nthe mixture framework. In Section III, we study the affinely constrained and\nunconstrained mixture methods updated with the EGU algorithm and the\nEG algorithm. In Section IV, we first perform the transient analysis of the\naffinely constrained mixtures and then continue with the transient analysis of\nthe unconstrained mixtures. Finally, in Section V, we perform simulations to\nshow the accuracy of our results and to compare performances of the different adaptive mixture methods. The paper concludes with certain remarks in\nSection VI.\n\n2\n\nSystem Description\n\n2.1 Notation\n\nIn this paper, all vectors are column vectors and represented by boldface\nlowercase letters. Matrices are represented by boldface capital letters. For\npresentation purposes, we work only with real data. Given a vector w, w (i)\n\u25b3\n\ndenotes the ith individual entry of w, w T is the transpose of w, kwk1 =\n4\n\n\fFig. 1. A linear mixture of outputs of m adaptive filters.\nP\n\n\u25b3\n\n(i)\ni |w | is the l1 norm; kwk =\n\n\u221a\n\nw T w is the l2 norm. For a matrix W ,\n\ntr(W ) is the trace. For a vector w, diag(w) represents a diagonal matrix\nformed using the entries of w. For a matrix W , diag(W ) represents a column\nvector that contains the diagonal entries of W . For two vectors v 1 and v 2 ,\n\u25b3\n\nwe define the concatenation [v 1 ; v 2 ] = [v T1 v T2 ]T . For a random variable v, v\u0304\nis the expected value. For a random vector v (or a random matrix V ), v\u0304 (or\nV\u0304 ) represents the expected value of each entry. Vectors (or matrices) 1 and\n0, with an abuse of notation, denote vectors (or matrices) of all ones or zeros,\nrespectively, where the size of the vector (or the matrix) is understood from\nthe context.\n\n2.2 System Description\n\nThe framework that we study has two stages. In the first stage, we have m\nadaptive filters producing outputs \u0177i (t), i = 1, . . . , m, running in parallel to\n5\n\n\fmodel a desired signal y(t) as seen in Fig. 1. The second stage is the mixture\nstage, where the outputs of the first stage filters are combined to improve\nthe steady-state and/or the transient performance over the constituent filters.\nWe linearly combine the outputs of the first stage filters to produce the final\n\u25b3\n\noutput as \u0177(t) = wT (t)x(t), where x(t) = [\u01771 (t), . . . , \u0177m (t)]T and train the\nmixture weights using multiplicative updates (or exponentiated gradient updates) [2]. We point out that in order to satisfy the constraints and derive\nthe multiplicative updates [9], [20], we use reparametrization of the mixture\nweights as w(t) = f (z(t)) and perform the update on z(t) as\n(\n\n\u0010\n\nz(t + 1) = arg min d(z, z(t)) + \u03bc l y(t), f T (z)x(t)\nz\n\n\u0011\n\n)\n\n,\n\n(1)\n\nwhere \u03bc is the learning rate of the update, d(*, *) is an appropriate distance\nmeasure and l(*, *) is the instantaneous loss. We emphasize that in (1), the\nupdated vector z is forced to be close to the present vector z(t) by d(z(t +\n\u0010\n\n\u0011\n\n1), z(t)), while trying to accurately model the current data by l y(t), f T (z)x(t) .\nHowever, instead of directly minimizing (1), a linearized version of (1)\n(\n\n\u0010\n\nz(t + 1) = arg min d(z, z(t)) + l y(t), f T (z(t))x(t)\nz\n\u0010\n\nT\n\n+ \u03bc\u2207z l y(t), f (z)x(t)\n\n\u0011T\n\nz =z (t)\n\n(z \u2212 z(t))\n\n)\n\n\u0011\n\n(2)\n\nis minimized to get the desired update. As an example, if we use the l2 -norm\nas the distance measure, i.e., d(z, z(t)) = kz \u2212 z(t)k2 , and the square error\n\u0010\n\n\u0011\n\nas the instantaneous loss, i.e., l y(t), f T (z)x(t) = [y(t) \u2212 f T (z)x(t)]2 with\nf (z) = z, then we get the stochastic gradient update on w(t), i.e.,\nw(t + 1) = w(t) + \u03bce(t)x(t),\nin (2).\nIn the next section, we use the unnormalized relative entropy\nd1 (z, z(t)) =\n\n(\n\nm\nX\ni=1\n\n\"\n\nz\n\n(i)\n\nz (i)\nln (i)\n+ z (i) (t) \u2212 z (i)\nz (t)\n!\n\n6\n\n#)\n\n(3)\n\n\ffor positively constrained z and z(t), z \u2208\n\nRm+ , z(t) \u2208 Rm+ , and the relative\n\nentropy\nd2 (z, z(t)) =\n\n(\n\nm\nX\ni=1\n\n\"\n\nz (i)\nz (i) ln (i)\nz (t)\n\n!# )\n\n(4)\n\n,\n\nwhere z is constrained to be in an extended simplex such that z (i) \u2265 0,\nPm\n\nk=1 z\n\n(i)\n\n= u for some u \u2265 1 as the distance measures, with appropriately\n\nselected f (*) to derive updates on mixture weights under different constraints.\nWe first investigate affinely constrained mixture of m adaptive filters, and then\ncontinue with the unconstrained mixture using (3) and (4) as the distance\nmeasures.\n\n3\n\nAdaptive Mixture Algorithms\n\nIn this section, we investigate affinely constrained and unconstrained mixtures\nupdated with the EGU algorithm and the EG algorithm.\n\n3.1 Affinely Constrained Mixture\n\nWhen an affine constraint is imposed on the mixture such that w T (t)1 = 1,\nwe get\n\u0177(t) = w(t)T x(t),\ne(t) = y(t) \u2212 \u0177(t),\nw (i) (t) = \u03bb(i) (t), i = 1, . . . , m \u2212 1,\nw (m) (t) = 1 \u2212\n\nm\u22121\nX\n\n\u03bb(i) (t),\n\ni=1\n\n\u25b3\n\nwhere the m \u2212 1 dimensional vector \u03bb(t) = [\u03bb(1) (t), . . . , \u03bb(m\u22121) (t)]T is the unconstrained weight vector, i.e., \u03bb(t) \u2208\n\nRm\u22121. Using \u03bb(t) as the unconstrained\nh\n\ni\n\nweight vector, the error can be written as e(t) = y(t) \u2212 \u0177m (t) \u2212 \u03bbT (t)\u03b4(t),\n\u25b3\n\nwhere \u03b4(t) = [\u01771 (t) \u2212 \u0177m (t), . . . , \u0177m\u22121 (t) \u2212 \u0177m (t)]T . To be able to derive a\n7\n\n\fmultiplicative update on \u03bb(t), we use\n\u03bb(t) = \u03bb1 (t) \u2212 \u03bb2 (t),\nwhere \u03bb1 (t) and \u03bb2 (t) are constrained to be nonnegative, i.e., \u03bbi (t) \u2208\n\n,\nRm\u22121\n+\n\ni = 1, 2. After we collect unconstrained weights in \u03bba (t) = [\u03bb1 (t); \u03bb2 (t)], we\ndefine a function of loss e(t) as\n\u25b3\n\nla (\u03bba (t)) = e2 (t)\nand update positively constrained \u03bba (t) as follows.\n\n3.1.1 Unnormalized Relative Entropy\nUsing the unconstrained relative entropy as the distance measure, we get\n\u03bba (t + 1) = arg min\n\u03bb\n\u0014\n\n( 2(m\u22121) \"\nX\n\n\u03bb(i)\n\n(i)\n\n\u03bb ln\n\n(i)\n\n\u03bba (t)\n\ni=1\n\n!\n\n+\n\n\u03bb(i)\na (t)\n\n\u2212\u03bb\n\nT\n\n\u03bc la (\u03bba (t)) + \u2207\u03bb la (\u03bb)\n(\u03bb \u2212 \u03bba (t))\n\u03bb=\u03bba (t)\n\n(i)\n\n#\n\n\u0015)\n\n+\n\n.\n\nAfter some algebra this yields\n\u03bba(i) (t + 1) = \u03bb(i)\na (t) exp {\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))} , i = 1, . . . , m \u2212 1,\n\u03bba(i) (t + 1) = \u03bb(i)\na (t) exp {\u2212\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))} , i = m, . . . , 2(m \u2212 1),\nproviding the multiplicative updates on \u03bb1 (t) and \u03bb2 (t).\n\n3.1.2 Relative Entropy\nUsing the relative entropy as the distance measure, we get\n\u03bba (t + 1) = arg min\n\u03bb\n\u0014\n\n( 2(m\u22121) \"\nX\n\n\u03bb(i)\n\n(i)\n\n\u03bb ln\n\n(i)\n\n\u03bba (t)\n\ni=1\n\n\u03bc la (\u03bba (t)) + \u2207\u03bb la (\u03bb)\n\n8\n\n!\n\nT\n\n#\n\n+ \u03b3(u \u2212 1 \u03bb) +\n\nT\n\n\u03bb=\u03bba (t)\n\n(\u03bb \u2212 \u03bba (t))\n\n\u0015)\n\n,\n\n\fwhere \u03b3 is the Lagrange multiplier. This yields\n(i)\n\n\u03bba (t) exp {\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))}\n\n(i)\n\n\u03bba (t + 1) = u P\n\nm\u22121\nk=1\n\ni = 1, . . . , m \u2212 1,\n\nh\n\n(k)\n\n(k+m\u22121)\n\n\u03bba (t) exp {\u03bce(t)(\u0177k (t) \u2212 \u0177m (t))} + \u03bba\n\n(t) exp {\u2212\u03bce(t)(\u0177k (t) \u2212 \u0177m (t))}\n\ni,\n\n(i)\n\n\u03bba (t) exp {\u2212\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))}\n\n(i)\n\n\u03bba (t + 1) = u\n\nPm\u22121 h\nk=1\n\n(k)\n\n(k+m\u22121)\n\n\u03bba (t) exp {\u03bce(t)(\u0177k (t) \u2212 \u0177m (t))} + \u03bba\n\n(t) exp {\u2212\u03bce(t)(\u0177k (t) \u2212 \u0177m (t))}\n\ni = m, . . . , 2(m \u2212 1),\n\ni,\n\nproviding the multiplicative updates on \u03bba (t).\n\n3.2 Unconstrained Mixture\n\nWithout any constraints on the combination weights, the mixture stage can\nbe written as\n\u0177(t) = w T (t)x(t),\ne(t) = y(t) \u2212 \u0177(t),\nwhere w(t) \u2208\n\nRm. To be able to derive a multiplicative update, we use a\n\nchange of variables,\nw(t) = w 1 (t) \u2212 w 2 (t),\nwhere w 1 (t) and w2 (t) are constrained to be nonnegative, i.e., wi (t) \u2208\n\nRm+ ,\n\ni = 1, 2. We then collect the unconstrained weights w a (t) = [w1 (t); w2 (t)] and\ndefine a function of the loss e(t) as\n\u25b3\n\nlu (wa (t)) = e2 (t).\n\n3.2.1 Unnormalized Relative Entropy\nDefining cost function similar to (4) and minimizing it with respect to w yields\nwa(i) (t + 1) = wa(i) (t) exp {\u03bce(t)\u0177i (t)} , i = 1, . . . , m,\nwa(i) (t + 1) = wa(i) (t) exp {\u2212\u03bce(t)\u0177i (t)} , i = m + 1, . . . , 2m,\n9\n\n\fproviding the multiplicative update on w a (t).\n\n3.2.2 Relative Entropy\nUsing the relative entropy under the simplex constraint on w, we get the\nupdates\nwa(i) (t + 1) = u\n\nm\nX\n\n\"\n\nm\nX\n\n\"\n\nk=1\n\nwa(i) (t) exp {\u03bce(t)\u0177i (t)}\n\nwa(k) (t) exp {\u03bce(t)\u0177k (t)}\n\n+\n\nwa(k+m) (t) exp {\u2212\u03bce(t)\u0177k (t)}\n\n#,\n\ni = 1, . . . , m,\nwa(i) (t + 1) = u\n\nk=1\n\nwa(i) (t) exp {\u2212\u03bce(t)\u0177i (t)}\n\nwa(k) (t) exp {\u03bce(t)\u0177k (t)}\n\n+\n\nwa(k+m) (t) exp {\u2212\u03bce(t)\u0177k (t)}\n\n#,\n\ni = m + 1 . . . , 2m.\n\nIn the next section, we study the transient analysis of these four adaptive\nmixture algorithms.\n\n4\n\nTransient Analysis\n\nIn this section, we study the mean and the mean-square transient analysis of\nthe adaptive mixture methods. We start with the affinely constrained combination.\n\n4.1 Affinely Constrained Mixture\n\nWe first perform the transient analysis of the mixture weights updated with\nthe EGU algorithm. Then, we continue with the transient analysis of the\nmixture weights updated with the EG algorithm.\n10\n\n\f4.1.1 Unconstrained Relative Entropy\nFor the affinely constrained mixture updated with the EGU algorithm, we\nhave the multiplicative update as\n(i)\n\n(i)\n\n\u03bb1 (t + 1) = \u03bb1 (t) exp {\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))} ,\n(i)\n\n= \u03bb1 (t)\n\n\u221e\nX\n\nk=0\n(i)\n\n(i)\n\n\u0010\n\n\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))\nk!\n\n\u0011k\n\n(5)\n\n,\n\n\u03bb2 (t + 1) = \u03bb2 (t) exp {\u2212\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))} ,\n(i)\n\n= \u03bb2 (t)\n\n\u221e\nX\n\nk=0\n\n\u0010\n\n\u2212 \u03bce(t)(\u0177i (t) \u2212 \u0177m (t))\nk!\n\n\u0011k\n\n(6)\n\n,\n\nfor i = 1, . . . , m \u2212 1. If e(t) and \u0177i (t) \u2212 \u0177m (t) for each i = 1, . . . , m \u2212 1 are\nbounded, then we can write (5) and (6) as\n(i)\n\n(i)\n\n(i)\n\n(i)\n\n\u0010\n\n\u0011\n\n\u03bb1 (t + 1) = \u03bb1 (t) 1 + \u03bce(t)(\u0177i (t) \u2212 \u0177m (t)) + O(\u03bc2) ,\n\u0010\n\n\u0011\n\n\u03bb2 (t + 1) = \u03bb2 (t) 1 \u2212 \u03bce(t)(\u0177i (t) \u2212 \u0177m (t)) + O(\u03bc2) ,\n\n(7)\n(8)\n\nfor i = 1, . . . , m \u2212 1. Since \u03bc is usually relatively small [2], we approximate (7)\nand (8) as\n(i)\n\n(i)\n\n(i)\n\n(i)\n\n\u0010\n\n\u0011\n\n\u03bb1 (t + 1) = \u03bb1 (t) 1 + \u03bce(t)(\u0177i (t) \u2212 \u0177m (t)) ,\n\u0010\n\n\u0011\n\n\u03bb2 (t + 1) = \u03bb2 (t) 1 \u2212 \u03bce(t)(\u0177i (t) \u2212 \u0177m (t)) .\n\n(9)\n(10)\n\nIn our simulations, we illustrate the accuracy of the approximations (9) and\n(10) under the mixture framework. Using (9) and (10), we can obtain updates\non \u03bb1 (t) and \u03bb2 (t) as\n\u0010\n\n\u0010\n\n\u0011\u0011\n\n\u03bb1 (t + 1) = I + \u03bce(t)diag \u03b4(t) \u03bb1 (t),\n\u0010\n\n\u0010\n\n\u0011\u0011\n\n\u03bb2 (t + 1) = I \u2212 \u03bce(t)diag \u03b4(t) \u03bb2 (t).\n\n(11)\n(12)\n\nCollecting the weights in \u03bba (t) = [\u03bb1 (t); \u03bb2 (t)], using the updates (11) and\n(12), we can write update on \u03bba (t) as\n\u0010\n\n\u0010\n\n\u0011\u0011\n\n\u03bba (t + 1) = I + \u03bce(t)diag u(t) \u03bba (t),\n\n11\n\n(13)\n\n\f\u25b3\n\nwhere u(t) is defined as u(t) = [\u03b4(t); \u2212\u03b4(t)].\nFor the desired signal y(t), we can write y(t)\u2212 \u0177m(t) = \u03bbT0 (t)\u03b4(t)+e0 (t), where\n\u25b3\n\n\u03bb0 (t) is the optimum MSE solution at time t such that \u03bb0 (t) = R\u22121 (t)p(t),\n\u25b3\n\nh\n\n\u25b3\n\ni\n\nn\n\nh\n\nR(t) = E \u03b4(t)\u03b4 T (t) , p(t) = E \u03b4(t) y(t) \u2212 \u0177m (t)\n\nio\n\nand e0 (t) is zero-mean\n\nand uncorrelated with \u03b4(t). We next show that the mixture weights conh\n\ni\n\nverge to the optimum solution in the steady-state such that limt\u2192\u221e E \u03bb(t) =\nlimt\u2192\u221e \u03bb0 (t) for properly selected \u03bc.\nSubtracting (12) from (11), we obtain\n\u0010\n\n\u0011\u0010\n\n\u0011\n\n\u03bb(t + 1) = \u03bb(t) + \u03bce(t)diag \u03b4(t) \u03bb1 (t) + \u03bb2 (t) ,\n\u0010\n\n\u0011\n\n\u0010\n\n\u0011\n\n= \u03bb(t) \u2212 \u03bce(t)diag \u03b4(t) \u03bb(t) + 2\u03bce(t)diag \u03b4(t) \u03bb1 (t).\n\n(14)\n\n\u25b3\n\nDefining \u03b5(t) = \u03bb0 (t) \u2212 \u03bb(t) and using e(t) = \u03b4 T (t)\u03b5(t) + e0 (t) in (14) yield\n\u0010\n\n\u0011\n\n\u0010\n\n\u0011\n\n\u0010\n\n\u0011\n\n\u03bb(t + 1) = \u03bb(t) \u2212 \u03bcdiag \u03b4(t) \u03bb(t)\u03b4 T (t)\u03b5(t) \u2212 \u03bcdiag \u03b4(t) \u03bb(t)e0 (t)\n\u0010\n\n\u0011\n\n\u0010\n\n\u0011\n\n+ 2\u03bcdiag \u03b4(t) \u03bb1 (t)\u03b4 T (t)\u03b5(t) + 2\u03bcdiag \u03b4(t) \u03bb1 (t)e0 (t).\n\n(15)\n\nIn (15), subtracting both sides from \u03bb0 (t + 1), we have\n\u0010\n\n\u0011\n\n\u03b5(t + 1) = \u03b5(t) + \u03bcdiag \u03b4(t) \u03bb(t)\u03b4 T (t)\u03b5(t) + \u03bcdiag \u03b4(t) \u03bb(t)e0 (t)\n\u0010\n\n\u0011\n\n\u0010\n\n\u0011\n\n\u2212 2\u03bcdiag \u03b4(t) \u03bb1 (t)\u03b4 T (t)\u03b5(t) \u2212 2\u03bcdiag \u03b4(t) \u03bb1 (t)e0 (t)\nh\n\ni\n\n+ \u03bb0 (t + 1) \u2212 \u03bb0 (t) .\n\n(16)\n\nTaking expectation of both sides of (16) and using\nh\n\n\u0010\n\n\u0011\n\ni\n\nh\n\n\u0010\n\n\u0011\n\ni\n\nE \u03bcdiag \u03b4(t) \u03bb(t)e0 (t) = E \u03bcdiag \u03b4(t) \u03bb(t) E[e0 (t)] = 0,\nh\n\n\u0010\n\n\u0011\n\ni\n\nh\n\n\u0010\n\n\u0011\n\ni\n\nE 2\u03bcdiag \u03b4(t) \u03bb1 (t)e0 (t) = E 2\u03bcdiag \u03b4(t) \u03bb1 (t) E[e0 (t)] = 0,\nand assuming that \u03bb1 (t) and \u03bb2 (t) are independent of \u03b5(t) [17] yield\nh\n\ni\n\nh\n\n\u0010\n\n\u0011\n\ni h\n\nE \u03b5(t + 1) = E I \u2212 \u03bcdiag \u03bb1 (t) + \u03bb2 (t) \u03b4(t)\u03b4 T (t) E \u03b5(t)\nh\n\ni\n\n+ E \u03bb0 (t + 1) \u2212 \u03bb0 (t) .\n\n12\n\ni\n\n(17)\n\n\fAssuming convergence of R(t) and p(t) (which is true for a wide range of\nh\n\nadaptive methods in the first stage [16], [14, 21]), we obtain limt\u2192\u221e E \u03bb0 (t +\ni\n\nh\n\n\u0010\n\n1) \u2212\u03bb0 (t) = 0. If \u03bc is chosen such that the eigenvalues of E I \u2212\u03bcdiag \u03bb1 (t) +\n\u0011\n\ni\n\n\u03bb2 (t) \u03b4(t)\u03b4 T (t) have strictly less than unit magnitude for every t, then\nh\n\ni\n\nlimt\u2192\u221e E \u03bb(t) = limt\u2192\u221e \u03bb0 (t).\n\nFor the transient analysis of the MSE, we have\n\nE[e2 (t)] = E\n\n\u001ah\n\ni2 \u001b\n\n\u2212 2\u03bb\u0304a (t)E\n\n\u001ah\n\ni2 \u001b\n\n\u2212 2\u03bb\u0304a (t)E\n\nnh\n\ni\n\nn\n\no\n\ny(t) \u2212 \u0177m (t)\n\nn\n\nT\n\nnh\n\ni\n\ny(t) \u2212 \u0177m (t) [\u03b4(t); \u2212\u03b4(t)]\no\n\n+ E \u03bbTa (t)[\u03b4(t); \u2212\u03b4(t)][\u03b4(t); \u2212\u03b4(t)]T \u03bba (t) ,\n=E\n\ny(t) \u2212 \u0177m (t)\n\n+ tr E\n=E\n\nh\n\n\u03bba (t)\u03bbTa (t)\n\n\u001ah\n\ny(t) \u2212 \u0177m (t)\n\nT\n\nE u(t)u(t)\n\ni2 \u001b\n\n\u2212\n\nT\n\ni\n\ny(t) \u2212 \u0177m (t) u(t)\n\n!\n\nT\n2\u03bb\u0304a (t)\u03b3(t)\n\no\n\no\n\n,\n\n+ tr E\n\nh\n\n\u03bba (t)\u03bbTa (t)\n\ni\n\n!\n\n\u0393(t) ,\n(18)\n\n\u25b3\n\nn\n\nh\n\nwhere we define \u03b3(t) = E u(t) y(t) \u2212 \u0177m (t)\n\nio\n\n\u25b3\n\nh\n\ni\n\nand \u0393(t) = E u(t)uT (t) .\n\nFor the recursion of \u03bb\u0304a (t) = E[\u03bba (t)], using (13), we get\n\n\u0010\n\n\u0011\n\n\u0010\n\n\u0011\n\n\u03bb\u0304a (t + 1) = \u03bb\u0304a (t) + \u03bcdiag \u03b3(t) \u03bb\u0304a (t) \u2212 \u03bcdiag E[\u03bba (t)\u03bbTa (t)]\u0393(t) .\n\n(19)\n\n(j)\nUsing (32), assuming \u03bba (t) is Gaussian and assuming \u03bb(i)\na (t) and \u03bba (t) are\n\n13\n\n\fi\n\nh\n\nindependent when i 6= j [17], [14], we get a recursion for E \u03bba (t)\u03bbTa (t) as\nh\n\ni\n\nh\n\ni\n\n\u0010\n\n\u0011 h\n\nE \u03bba (t + 1)\u03bbTa (t + 1) = E \u03bba (t)\u03bbTa (t) + \u03bcdiag \u03b3(t) E \u03bba (t)\u03bbTa (t)\n\u0010\n\n\u0011 h\n\n\u2212 \u03bcdiag \u0393(t)\u03bb\u0304a (t) E \u03bba (t)\u03bbTa (t)\nh\n\n2\n\n\u2212 \u03bcE diag (u(t))\n\u0010\n\ni\n\nE\n\nh\n\n\u03bba (t)\u03bbTa (t)\n\n\u0011\n\n\u2212 \u03bcdiag \u03bb\u0304a (t) \u0393(t) E\nh\n\ni\n\ni\n\ni\n\nh\n\n\u0010\n\n\u03bba (t)\u03bbTa (t)\n\u0011\n\n\u2212\ni\n\nT\n\u03bb\u0304a (t)\u03bb\u0304a (t)\n\n\u2212\n\n!\n\nT\n\u03bb\u0304a (t)\u03bb\u0304a (t)\n\nh\n\nT\n\n1\u03bb\u0304a (t)\n\n!\n\ni\n\n\u0010\n\n+ \u03bcE \u03bba (t)\u03bbTa (t) diag \u03b3(t) \u2212 \u03bcE \u03bba (t)\u03bbTa (t) diag \u0393(t)\u03bb\u0304a (t)\n\u2212 \u03bc\u03bb\u0304a (t)1\n\nT\n\nE\n\nh\n\n\u03bba (t)\u03bbTa (t)\n\nh\n\ni\n\n\u2212 \u03bc E \u03bba (t)\u03bbTa (t) \u2212\n\ni\n\n\u2212\n\nT\n\u03bb\u0304a (t)\u03bb\u0304a (t)\n\nT\n\u03bb\u0304a (t)\u03bb\u0304a (t)\n\n\u25b3\n\n\u25b3\n\n!\n\n!\n\nh\n\nE diag2 (u(t))\n\u0010\n\ni\n\n\u0011\n\n\u0011\n\n\u0393(t)diag \u03bb\u0304a (t) .\n\nh\n\ni\n\n(20)\n\ni\n\nDefining q a (t) = \u03bb\u0304a (t) and Qa (t) = E \u03bba (t)\u03bbTa (t) , we express (19) and (20)\nas a coupled recursions in Table 1.\nTable 1\nTime evolution of the mean and the variance of the affinely constrained mixture\nweights updated with the EGU algorithm\n\u0001\n\n\u0001\n\nqa (t + 1) = q a (t) + \u03bcdiag \u03b3(t) qa (t) \u2212 \u03bcdiag Qa (t)\u0393(t) ,\n\nQa (t + 1) =\n\n\u0010\n\n\u0001\n\n\u0001\u0011\n\n\u0001\n\n\u0010\n\n\u0011\n\n\u0003\u0010\n\n\u0002\n\nQa (t) \u2212 \u03bcE diag2 (u(t))\n\nI + \u03bcdiag \u03b3(t) \u2212 \u03bcdiag \u0393(t)q a (t)\n\n\u0010\n\n\u0001\n\n\u0011\n\nT\nQa (t) \u2212 q a (t)q T\na (t) 1q a (t)\n\n\u0001\u0011\n\n\u2212\u03bcdiag qa (t) \u0393(t) Qa (t) \u2212 qa (t)q T\na (t) + Qa (t) \u03bcdiag \u03b3(t) \u2212 \u03bcdiag \u0393(t)q a (t)\n\n\u0010\n\n\u0011 \u0002\n\n\u0010\n\n\u0003\n\n\u0011\n\n\u0001\n\n2\nT\n\u2212\u03bcq a (t)1T Qa (t) \u2212 q a (t)q T\na (t) E diag (u(t)) \u2212 \u03bc Qa (t) \u2212 q a (t)q a (t) \u0393(t)diag q a (t) .\n\nIn Table 1, we provide the mean and the variance recursions for Qa (t) and\nq a (t). To implement these recursions, one needs to only provide \u0393(t) and \u03b3(t).\nNote that \u0393(t) and \u03b3(t) are derived for a wide range of adaptive filters [16],\n[14]. If we use the mean and the variance recursions in (18), then we obtain\nthe time evolution of the final MSE. This completes the transient analysis of\nthe affinely constrained mixture weights updated with the EGU algorithm.\n14\n\n\f4.1.2 Relative Entropy\nFor the affinely constrained combination updated with the EG algorithm, we\nhave the multiplicative updates as\n\n(i)\n\n\u03bb1 (t) exp {\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))}\n\n(i)\n\n\u03bb1 (t + 1) = u\n\nPm\u22121 h\nk=1\n\n(i)\n\n\u03bb2 (t + 1) = u P\n\nm\u22121\nk=1\n\nh\n\n(k)\n\n(k)\n\n\u03bb1 (t) exp {\u03bce(t)(\u0177k (t) \u2212 \u0177m (t))} + \u03bb2 (t) exp {\u2212\u03bce(t)(\u0177k (t) \u2212 \u0177m (t))}\n(i)\n\n\u03bb2 (t) exp {\u2212\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))}\n(k)\n\n(k)\n\n\u03bb1 (t) exp {\u03bce(t)(\u0177k (t) \u2212 \u0177m (t))} + \u03bb2 (t) exp {\u2212\u03bce(t)(\u0177k (t) \u2212 \u0177m (t))}\n\ni,\ni,\n\nfor i = 1, . . . , m \u2212 1. Using the same approximations as in (7), (8), (9) and\n(10), we obtain\n\n(i)\n\n(i)\n\u03bb1 (t\n\n(i)\n\n+ 1) = u P\n\n\u03bb2 (t + 1) = u\n\n\u0001\n\n\u03bb1 (t) 1 + \u03bce(t)(\u0177i (t) \u2212 \u0177m (t))\n\nm\u22121\nk=1\n\nh\n\n(k)\n\nPm\u22121\n\nh\n\n(k)\n\nk=1\n\n\u0001\n\n(k)\n\n\u0001\n\n(k)\n\n\u0001i ,\n\n(21)\n\n\u03bb1 (t) 1 + \u03bce(t)(\u0177k (t) \u2212 \u0177m (t)) + \u03bb2 (t) 1 \u2212 \u03bce(t)(\u0177k (t) \u2212 \u0177m (t))\n(i)\n\n\u0001\n\n\u03bb2 (t) 1 \u2212 \u03bce(t)(\u0177i (t) \u2212 \u0177m (t))\n\ni.\n\n(22)\n\n\u03bb1 (t) 1 + \u03bce(t)(\u0177k (t) \u2212 \u0177m (t)) + \u03bb2 (t) 1 \u2212 \u03bce(t)(\u0177k (t) \u2212 \u0177m (t))\n\nIn our simulations, we illustrate the accuracy of the approximations (21) and\n(22) under the mixture framework. Using (21) and (22), we obtain updates on\n\u03bb1 (t) and \u03bb2 (t) as\n\n\u0010\n\n\u0010\n\n\u0011\u0011\n\n(23)\n\n\u0010\n\n\u0010\n\n\u0011\u0011\n\n(24)\n\nI + \u03bce(t)diag \u03b4(t) \u03bb1 (t)\n\ni\n\u03bb1 (t + 1) = u h T\n,\n1 + \u03bce(t)uT (t) \u03bba (t)\n\nI \u2212 \u03bce(t)diag \u03b4(t)\n\n\u03bb2 (t)\ni\n.\n\u03bb2 (t + 1) = u h T\n1 + \u03bce(t)uT (t) \u03bba (t)\n\nUsing updates (23) and (24), we can write update on \u03bba (t)\n\nh\n\n\u0010\n\n\u0011i\n\nI + \u03bce(t)diag u(t) \u03bba (t)\n\ni\n\u03bba (t + 1) = u h T\n.\n1 + \u03bce(t)uT (t) \u03bba (t)\n\n15\n\n(25)\n\n\fFor the recursion of \u03bb\u0304a (t), using (25), we get\nh\n\n\uf8f1 h\n\uf8f2 I\n\ni\n\n\u0010\n\n\u0011i\n\n\u0010\n\n\u0011i\n\n\uf8fc\n\n+ \u03bce(t)diag u(t) \u03bba (t) \uf8fd\n\ni\nE \u03bba (t + 1) = E u h T\n,\n\uf8f3\n1 + \u03bce(t)uT (t) \u03bba (t) \uf8fe\n\n\u2248u\n=u\n\nE\n\nnh\n\nE\nh\n\nI + \u03bce(t)diag u(t) \u03bba (t)\n\nnh\n\ni\n\n1T + \u03bce(t)uT (t) \u03bba (t)\ni\n\n\u0010\n\n\u0011 h\n\no\n\no\n\n(26)\n\n,\n\ni\n\n\u0010\n\nE \u03bba (t) + \u03bcdiag \u03b3(t) E \u03bba (t) \u2212 \u03bcdiag E[\u03bba (t)\u03bbTa (t)]\u0393(t)\nh\n\ni h\n\ni\n\n\u0010\n\n1T + \u03bc\u03b3 T (t) E \u03bba (t) \u2212 \u03bctr E[\u03bba (t)\u03bbTa (t)]\u0393(t)\n\n\u0011\n\n(27)\n\nwhere in (26) we approximate expectation of the quotient with the quotient\nof the expectations. In our simulations, we also illustrate the accuracy of this\napproximation in the mixture framework. From (25), using the same approximation in (27), assuming \u03bba (t) is Gaussian, assuming \u03bba(i) (t) and \u03bb(j)\na (t) are\ni\n\nh\n\nindependent when i 6= j, we get a recursion for E \u03bba (t)\u03bbTa (t) as\ni\n\nh\n\nE \u03bba (t + 1)\u03bbTa (t + 1) = u2\n\nA(t)\n,\nb(t)\n\n(28)\n\nwhere A(t) is equal to the right hand side of (20) and\nh\n\ni\n\nh\n\ni\n\nb(t) = 1T E \u03bba (t)\u03bbTa (t) 1 + \u03bcpT (t)E \u03bba (t)\u03bbTa (t) 1\n\u2212\n\nT\n\u03bc\u03bb\u0304a (t)R(t)E\n\n\u2212 \u03bc1\n\nT\n\nE\n\nh\n\nh\n\n\u03bba (t)\u03bbTa (t)\n\n\u03bba (t)\u03bbTa (t)\n\nh\n\ni\n\n\u2212\n\ni\n\nT\n\n1 \u2212 \u03bc1\n\nT\n\u03bb\u0304a (t)\u03bb\u0304a (t)\n\ni\n\nE\n\n!\n\nh\n\n\u03bba (t)\u03bbTa (t)\n\nh\n\ni\n\n\u2212\n\nT\n\u03bb\u0304a (t)\u03bb\u0304a (t)\n\n!\n\nR(t)\u03bb\u0304a (t)\n\ni\n\nE diag2 (u(t)) 1T \u03bb\u0304a (t)1\n\nh\n\ni\n\n+ \u03bc1T E \u03bba (t)\u03bbTa (t) p(t) \u2212 \u03bc1T E \u03bba (t)\u03bbTa (t) R(t)\u03bb\u0304a (t)\n\u2212\n\nT\n\u03bc\u03bb\u0304a (t)R(t)\n\n\u2212 \u03bc1\n\nT\n\nT\n\u03bb\u0304a (t)1E\n\nE\nh\n\nh\n\n\u03bba (t)\u03bbTa (t)\n2\n\ndiag (u(t))\n\ni\n\ni\n\nT\n\u03bb\u0304a (t)\u03bb\u0304a (t)\n\n\u2212\n\nE\n\nh\n\n\u03bba (t)\u03bbTa (t)\n\n!\n\ni\n\n1\n\n\u2212\n\nT\n\u03bb\u0304a (t)\u03bb\u0304a (t)\n\n!\n\n1.\n\n(29)\n\nIf we use the mean (27) and the variance (28), (29) recursions in (18), then\nwe obtain the time evolution of the final MSE. This completes the transient\nanalysis of the affinely constrained mixture weights updated with the EG\nalgorithm.\n16\n\n\u0011\n\n,\n\n\f4.2 Unconstrained Mixture\n\nWe use the unconstrained relative entropy and the relative entropy as distance\nmeasures to update unconstrained mixture weights. We first perform transient\nanalysis of the mixture weights updated using the EGU algorithm. Then, we\ncontinue with the transient analysis of the mixture weights updated using the\nEG algorithm. Note that since the unconstrained case is close to the affinely\nconstrained case, we only provide the necessary modifications to get the mean\nand the variance recursions for the transient analysis.\n\n4.2.1 Unconstrained Relative Entropy\nFor the unconstrained combination updated with EGU, we have the multiplicative updates as\n(i)\n\n(i)\n\n(i)\n\n(i)\n\nw1 (t + 1) = w1 (t) exp {\u03bce(t)\u0177i (t)} ,\nw2 (t + 1) = w2 (t) exp {\u2212\u03bce(t)\u0177i (t)} ,\nfor i = 1, . . . , m. Using the same approximations as in (7), (8), (9) and (10),\nwe can obtain updates on w1 (t) and w 2 (t) as\n\u0010\n\n\u0010\n\n\u0011\u0011\n\nw1 (t + 1) = I + \u03bce(t)diag x(t) w 1 (t),\n\u0010\n\n\u0010\n\n(30)\n\n\u0011\u0011\n\nw2 (t + 1) = I \u2212 \u03bce(t)diag x(t) w2 (t).\n\n(31)\n\nCollecting the weights in wa (t) = [w1 (t); w2 (t)], using the updates (30) and\n(31), we can write update on w a (t) as\n\u0010\n\n\u0010\n\n\u0011\u0011\n\nw a (t + 1) = I + \u03bce(t)diag u(t) w a (t),\n\n(32)\n\n\u25b3\n\nwhere u(t) is defined as u(t) = [x(t); \u2212x(t)].\nFor the desired signal y(t), we can write y(t) = wT0 (t)x(t) + e0 (t), where\n\u25b3\n\nw 0 (t) is the optimum MSE solution at time t such that w 0 (t) = R\u22121 (t)p(t),\n17\n\n\f\u25b3\n\nh\n\n\u25b3\n\ni\n\nR(t) = E x(t)xT (t) , p(t) = E {x(t)y(t)} and e0 (t) is zero-mean disturbance\nuncorrelated to x(t). To show that the mixture weights converge to the optih\n\ni\n\nmum solution in the steady-state such that limt\u2192\u221e E w(t) = limt\u2192\u221e w 0 (t),\nwe follow similar lines as in the Section 4.1.1. We modify (14), (15), (16) and\n(17) such that \u03bb will be replaced by w, \u03b4(t) will be replaced by x(t) and\n\u03b5(t) = w 0 (t) \u2212 w(t). After these replacements, we obtain\n\nh\n\ni\n\nh\n\n\u0010\n\n\u0011\n\ni h\n\nE \u03b5(t + 1) = E I \u2212 \u03bcdiag w 1 (t) + w2 (t) x(t)xT (t) E \u03b5(t)\nh\n\ni\n\n+ E w 0 (t + 1) \u2212 w0 (t) .\n\nh\n\ni\n\ni\n\n(33)\n\nSince, we have limt\u2192\u221e E w 0 (t + 1) \u2212 w 0 (t) = 0 for most adaptive filters in\nh\n\nthe first stage [14] and if \u03bc is chosen so that all the eigenvalues of E I \u2212\n\u0010\n\n\u0011\n\ni\n\n\u03bcdiag w1 (t) + w 2 (t) x(t)xT (t) have strictly less than unit magnitude for\nh\n\ni\n\nevery t, then limt\u2192\u221e E w(t) = limt\u2192\u221e w0 (t).\n\n\u25b3\n\n\u25b3\n\nFor the transient analysis of MSE, defining \u03b3(t) = E {u(t)y(t)} and \u0393(t) =\nh\n\ni\n\nE u(t)uT (t) , (18) is modified as\n\nn\n\n2\n\n2\n\no\n\nE[e (t)] = E y (t) \u2212\n\n2w\u0304 Ta (t)\u03b3(t)\n\n+ tr E\n\nh\n\nwa (t)w Ta (t)\n\ni\n\n!\n\n\u0393(t) .\n\n(34)\n\nAccordingly, we modify the mean recursion (19) and the variance recursion\n(20) such that instead of \u03bba (t) we use w a (t). We also modify the Table 1\n\u25b3\n\n\u25b3\n\nh\n\ni\n\nusing q a (t) = w\u0304a (t) and Qa (t) = E wa (t)w Ta (t) . If we use this modified\nmean and variance recursions in (34), then we obtain the time evolution of\nthe final MSE. This completes the transient analysis of the unconstrained\nmixture weights updated with the EGU algorithm.\n18\n\n\f4.2.2 Relative Entropy\nFor the unconstrained combination updated with the EG algorithm, we have\nthe multiplicative updates as\nwa(i) (t + 1) = u\n\nm\nX\n\n\"\n\nm\nX\n\n\"\n\nk=1\n\nwa(i) (t) exp {\u03bce(t)\u0177i (t)}\n\nwa(k) (t) exp {\u03bce(t)\u0177k (t)} + wa(k+m) (t) exp {\u2212\u03bce(t)\u0177k (t)}\n\n#,\n\ni = 1, . . . , m,\nwa(i) (t\n\n+ 1) = u\n\nk=1\n\nwa(i) (t) exp {\u2212\u03bce(t)\u0177i (t)}\n\nwa(k) (t) exp {\u03bce(t)\u0177k (t)}\n\n+\n\nwa(k+m) (t) exp {\u2212\u03bce(t)\u0177k (t)}\n\n#,\n\ni = m + 1 . . . , 2m.\nFollowing similar lines, we modify (23), (24), (25), (27), (28) and (29) such\nh\n\ni\n\nthat we replace \u03b4(t) with x(t), \u03bb with w and u(t) = x(t); \u2212x(t) . Finally,\nwe use the modified mean and variance recursions in (34) and obtain the\ntime evolution of the final MSE. This completes the transient analysis of the\nunconstrained mixture weights updated with the EG algorithm.\n\n5\n\nSimulations\n\nIn this section, we illustrate the accuracy of our results and compare performances of different adaptive mixture methods through simulations. In our\nsimulations, we observe that using the EG algorithm to train the mixture\nweights yields better performance compared to using the LMS algorithm or\nthe EGU algorithm to train the mixture weights for combinations having more\nthan two filters and when the combination favors only a few of the constituent\nfilters. The LMS algorithm and the EGU algorithm perform similarly in our\nsimulations when they are used to train the mixture weights. We also observe\nin our simulations that the mixture weights under the EG update converge to\nthe optimum combination vector faster than the mixture weights under the\n19\n\n\fLMS algorithm.\n\nTo compare performances of the EG and LMS algorithms and illustrate the\naccuracy of our results in (27), (28) and (29) under different algorithmic parameters, the desired signal as well as the system parameters are selected as\nfollows. First, a seventh-order linear filter,\nw o = [0.25, \u22120.47, \u22120.37, 0.045, \u22120.18, 0.78, 0.147]T , is chosen as in [17]. The\nunderlying signal is generated using the data model y(t) = \u03c4 wTo a(t) + n(t),\nwhere a(t) is an i.i.d. Gaussian vector process with zero mean and unit\nvariance entries, i.e., E[a(t)aT (t)] = I, n(t) is an i.i.d. Gaussian noise process with zero mean and variance E[n2 (t)] = 0.3, and \u03c4 is a positive scalar\n\u25b3\n\nto control SNR. Hence, the SNR of the desired signal is given by SNR =\n2\nT u(t))2 ]\n\u03c4 2 kw o k2\no\n10 log( E[\u03c4 (w\n)\n=\n10\nlog(\n). For the first experiment, we have\n0.01\n0.01\nSNR = -10dB. To model the unknown system we use ten linear filters using the LMS update as the constituent filters. The learning rates of these two\nconstituent filters are set to \u03bc1 = 0.002 and \u03bc6 = 0.002 while the learning\nrates for the rest of the constituent filters are selected randomly in [0.1, 0.11].\nTherefore, in the steady-state, we obtain the optimum combination vector\napproximately as \u03bbo = [0.5, 0, 0, 0, 0, 0.5, 0, 0, 0, 0]T , i.e., the final combination\nvector is sparse. In the second stage, we train the combination weights with\nthe EG and LMS algorithms and compare performances of these algorithms.\nFor the second stage, the learning rates for the EG and LMS algorithms are\nselected as \u03bcEG = 0.0008 and \u03bcLMS = 0.005 such that the MSEs of both\nmixtures converge to the same final MSE to provide a fair comparison. We\nselect u = 500 for the EG algorithm. In Fig. 2a, we plot the weight of the\nfirst constituent filter with \u03bc1 = 0.002, i.e. E[\u03bb(1) (t)], updated with the EG\nand LMS algorithms. In Fig. 2b, we plot the MSE curves for the adaptive\nmixture updated with the EG algorithm, the adaptive mixture updated with\nthe LMS algorithm, the first constituent filter with \u03bc1 = 0.002 and the second constituent filter with \u03bc2 \u2208 [0.1, 0.11]. From Fig. 2a and Fig. 2b, we see\n20\n\n\fthat the EG algorithm performs better than the LMS algorithm such that the\ncombination weight under the update of the EG algorithm converges to 0.5\nfaster than the combination weight under the update of the LMS algorithm.\nFurthermore the MSE of the adaptive mixture updated with the EG algorithm\nconverges faster than the MSE of the adaptive mixture updated with the LMS\nalgorithm. In Fig. 2c, to test the accuracy of (27), we plot the theoretical val(10)\nues for \u03bb\u0304(1)\na (t) and \u03bb\u0304a (t) along with simulations. Note in Fig. 2c we observe\n(10)\nthat \u03bb\u0304(1) (t) = \u03bb\u0304(1)\na (t) \u2212 \u03bb\u0304a (t) converges to 0.5 as predicted in our derivations.\n\nIn Fig. 2d, to test the accuracy of (28) and (29), as an example, we plot the\nh\n\ni\n\nh\n\ni\n\n(3)\n2\nand E \u03bb(1)\ntheoretical values of E \u03bb(1)\na (t)\u03bba (t) along with simulations.\na (t)\n\nAs we observe from Fig. 2c and Fig. 2d, there is a close agreement between\nour results and simulations in these experiments. We observe similar results\nfor the other cross terms.\n\nWe next simulate the unconstrained mixtures updated with the EGU and EG\nalgorithms. Here, we have two linear filters and both using the LMS update to\ntrain their weight vectors as the constituent filters. The learning rates for two\nconstituent filters are set to \u03bc1 = 0.002 and \u03bc2 = 0.1 respectively. Therefore, in\nthe steady-state, we obtain the optimum vector approximately as w o = [1, 0].\nWe have SNR = 1 for these simulations. The unconstrained mixture weights\nare first updated with the EGU algorithm. For the second stage, the learning\nrate for the EGU algorithm is selected as \u03bcEGU = 0.01. The theoretical curves\nin the figures are produced using \u0393(t) and \u03b3(t) that are calculated from the\nsimulations, since our goal is to illustrate the validity of derived equations. In\n(2)\n(3)\n(4)\nFig. 3a, we plot the theoretical values of w\u0304 (1)\na (t), w\u0304 a (t), w\u0304 a (t) and w\u0304 a (t)\n\nalong with simulations. In Fig. 3b, as an example, we plot the theoretical valh\n\ni\n\nh\n\ni\n\nh\n\ni\n\nh\n\n(1)\n(2)\n(2)\n(3)\n(3)\n(4)\n2\nues of E w (1)\na (t) , E w a (t)w a (t) , E w a (t)w a (t) and E w a (t)w a (t)\n\ni\n\nalong with simulations. We continue to update the mixture weights with the\nEG algorithm. For the second stage, the learning rate for the EG algorithm\nis selected as \u03bcEG = 0.01. We select u = 3 for the EG algorithm. In Fig. 3c,\n21\n\n\f(2)\n(3)\n(4)\nwe plot the theoretical values of w\u0304(1)\na (t), w\u0304 a (t), w\u0304 a (t) and w\u0304 a (t) along\n\nwith simulations. In Fig. 3d, as an example, we plot the theoretical values of\ni\n\nh\n\nh\n\ni\n\nh\n\ni\n\nh\n\ni\n\n(1)\n(2)\n(2)\n(3)\n(2)\n(4)\n2\nE w (2)\na (t) , E w a (t)w a (t) , E w a (t)w a (t) and E w a (t)w a (t) along\n\nwith simulations. We observe a close agreement between our results and simulations.\nTo test the accurateness of the assumptions in (9) and (10), we plot in Fig.\n4a, the difference\nk exp {\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))} \u2212 {1 + \u03bce(t)(\u0177i (t) \u2212 \u0177m (t)))} k2\n\nq\n\nk exp {\u03bce(t)(\u0177i (t) \u2212 \u0177m (t))} k2 k {1 + \u03bce(t)(\u0177i (t) \u2212 \u0177m (t)))} k2\n\nfor i = 1 with the same algorithmic parameters as in Fig. 2 and Fig. 3. To\ntest the accurateness of the separation assumption in (27), we plot in Fig. 4b,\nthe first parameter of the difference\no\nn\u0002\n\u0001\u0003\n\u0001\u0003\n\u001b\n\u001a \u0002\n2\nE I+\u03bce(t)diag u(t) \u03bba (t)\nI+\u03bce(t)diag u(t) \u03bba (t)\n\u0003\no\nn\u0002\n\u2212\nu\nE u \u0002 T\n\u0003\nT\n1 +\u03bce(t)uT (t) \u03bba (t)\nE 1 +\u03bce(t)uT (t) \u03bba (t)\nv\no\nn\nu\n\u0001\u0003\n\u001b 2 E \u0002I+\u03bce(t)diag u(t)\u0001\u0003\u03bb (t)\n\u001a \u0002\nu\na\nu(t) \u03bba (t)\nt E u I+\u03bce(t)diag\no\n\u0002 T\n\u0003\n\u0003\nu n\u0002 T\n1 +\u03bce(t)uT (t) \u03bba (t)\nE 1 +\u03bce(t)u T (t) \u03bba (t)\n\n2\n\nwith the same algorithmic parameters as in Fig. 2 and Fig. 3. We observe\nthat assumptions are fairly accurate for these algorithms in our simulations.\nIn the last simulations, we compare performances of the EGU, EG and LMS\nalgorithms updating the affinely mixture weights under different algorithmic\nparameters. Algorithmic parameters and constituent filters are selected as in\nFig. 2 under SNR = -5 and 5. For the second stage, under SNR = -5, learning\nrates for the EG, EGU and LMS algorithms are selected as \u03bcEG = 0.0005,\n\u03bcEGU = 0.005 and \u03bcLMS = 0.005 such that the MSEs converge to the same final\nMSE to provide a fair comparison. We choose u = 500 for the EG algorithm.\nIn Fig. 5a, we plot the MSE curves for the adaptive mixture updated with\nthe EG algorithm, the adaptive mixture updated with the EGU algorithm,\nthe adaptive mixture updated with the LMS algorithm, first constituent filter\n22\n\n\fwith \u03bc1 = 0.002 and second constituent filter with \u03bc2 \u2208 [0.1, 0.11] under SNR\n= -5. Under SNR = 5, learning rates for the EG, EGU and LMS algorithms\nare selected as \u03bcEG = 0.002, \u03bcEGU = 0.005 and \u03bcLMS = 0.005. We choose u\n= 100 for the EG algorithm. In Fig. 5b, we plot same MSE curves as in Fig.\n5a. We observe that the EG algorithm performs better than the EGU and\nLMS algorithms such that MSE of the adaptive mixture updated with the\nEG algorithm converges faster than the MSE of adaptive mixtures updated\nwith the EGU and LMS algorithms. We also observe that the EGU and LMS\nalgorithms show similar performances when they are used to train the mixture\nweights.\n\n6\n\nConclusion\n\nIn this paper, we investigate adaptive mixture methods based on Bregman\ndivergences combining outputs of m adaptive filters to model a desired signal.\nWe use the unnormalized relative entropy and relative entropy as distance\nmeasures that produce the exponentiated gradient update with unnormalized\nweights (EGU) and the exponentiated gradient update with positive and negative weights (EG) to train the mixture weights under the affine constraints or\nwithout any constraints. We provide the transient analysis of these methods\nupdated with the EGU and EG algorithms. In our simulations, we compare\nperformances of the EG, EGU and LMS algorithms and observe that the EG\nalgorithm performs better than the EGU and LMS algorithms when the combination vector in steady-state is sparse. We observe that the EGU and LMS\nalgorithms show similar performance when they are used to train the mixture\nweights. We also observe a close agreement between the simulations and our\ntheoretical results.\n23\n\n\fReferences\n\n[1] C. Boukis, D. Mandic, A. G. Constantinides, A class of stochastic gradient\nalgorithms with exponentiated error cost functions, Digital Signal Processing\n19 (2009) 201\u2013212.\n[2] D. P. Helmbold, R. E. Schapire, Y. Singer, M. K. Warmuth, A comparison of\nnew and old algorithms for a mixture estimation problem, Machine Learning\n27 (1997) 97\u2013119.\n[3] J. C. M. Bermudez, N. J. Bershad, J. Y. Tourneret, Stochastic analysis of an\nerror power ratio scheme applied to the affine combination of two lms adaptive\nfilters, Signal Processing 91 (2011) 2615\u20132622.\n[4] J. Arenas-Garcia, M. Martinez-Ramon, A. Navia-Vazquez, A. R. FigueirasVidal, Plant identification via adaptive combination of transversal filters, Signal\nProcessing 86 (2006) 2430\u20132438.\n[5] S. S. Kozat, A. C. Singer, Multi-stage adaptive signal processing algorithms,\nin: Proceedings of SAM Signal Proc. Workshop, 2000, pp. 380\u2013384.\n[6] J. Arenas-Garcia, V. Gomez-Verdejo, M. Martinez-Ramon, A. R. FigueirasVidal, Separate-variable adaptive combination of LMS adaptive filters for plant\nidentification, in: Proc. of the 13th IEEE Int. Workshop Neural Networks Signal\nProcessing, 2003, pp. 239\u2013248.\n[7] J. Arenas-Garcia, M. Martinez-Ramon, V. Gomez-Verdejo, A. R. FigueirasVidal, Multiple plant identifier via adaptive LMS convex combination, in: Proc.\nof the IEEE Int. Symp. Intel. Signal Processing, 2003, pp. 137\u2013142.\n[8] J. Arenas-Garcia, V. Gomez-Verdejo, A. R. Figueiras-Vidal, New algorithms\nfor improved adaptive convex combination of lms transversal filters, IEEE\nTransactions on Instrumentation and Measurement 54 (2005) 2239\u20132249.\n[9] J. Kivinen, M. Warmuth, Exponentiated gradient versus gradient descent for\nlinear predictors 132 (1997) 1\u201364.\n\n24\n\n\f[10] D. P. Helmbold, R. E. Schapire, Y. Singer, M. K. Warmuth, On-line portfolio\nselection using multiplicative updates, Mathematical Finance 8 (4) (1998)\n325347.\n[11] N. J. Bershad, J. C. M. Bermudez, J. Tourneret, An affine combination of two\nLMS adaptive filters: Transient mean-square analysis, IEEE Transactions on\nSignal Processing 56 (5) (2008) 1853\u20131864.\n[12] S. S. Kozat, A. T. Erdogan, A. C. Singer, A. H. Sayed, Steady state\nMSE performance analysis of mixture approaches to adaptive filtering, IEEE\nTransactions on Signal Processing 58 (2010) 4421\u20134427.\n[13] J. Benesty, Y. A. Huang, The LMS, PNLMS, and Exponentiated Gradient\nalgorithms, Proc. Eur. Signal Process. Conf. (EUSIPCO) (2004) 721\u2013724.\n[14] A. H. Sayed, Fundamentals of Adaptive Filtering, John Wiley and Sons, 2003.\n[15] J. Arenas-Garcia, A. R. Figueiras-Vidal, A. H. Sayed, Mean-square performance\nof a convex combination of two adaptive filters, IEEE Transactions on Signal\nProcessing 54 (2006) 1078\u20131090.\n[16] V. H. Nascimento, M. T. M. Silva, J. Arenas-Garcia, A transient analysis for the\nconvex combination of adaptive filters, IEEE Transactions on Signal Processing\n58 (8) (2009) 4064\u20134078.\n[17] S. S. Kozat, A. T. Erdogan, A. C. Singer, A. H. Sayed, Transient analysis of\nadaptive affine combinations, accepted, 2011.\n[18] V. Vovk, A game of prediction with expert advice, Journal of Computer and\nSystem Sciences 56 (1998) 153\u2013173.\n[19] P. A. Naylor, J. Cui, M. Brookes, Adaptive algorithms for sparse echo\ncancellation, Signal Processing 86 (2006) 1182\u20131192.\n[20] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, M. K.\nWarmuth, How to use expert advice, Journal of the ACM 44 (3) (1997) 427\u2013485.\n\n25\n\n\f[21] B. Jelfs, D. P. Mandic, S. C. Douglas, An adaptive approach for the\nidentification of improper complex signals, Signal Processing 92 (2012) 335\u2013\n344.\n\n26\n\n\fMSEs of the constituent filters and adaptive mixtures\n\nWeight of the first constituent filter updated with the EG and LMS alg.\n\u22122.5\n\n0.7\nWeight of the first constituent filter updated with the EG algorithm\nWeight of the first constituent filter updated with the LMS algorithm\n\n0.6\n\n\u22123\n\n\u22123.5\n\n\u22124\n\nMSE (dB)\n\nWeight\n\n0.5\n\n0.4\n\n0.3\n\n\u22124.5\n\n\u22125\n\n\u22125.5\nMSE of the first constituent filter\nMSE of the second constituent filter\nMSE of the adaptive mixture updated with the LMS algorithm\nMSE of the adaptive mixture updated with the EG algorithm \u2212 simulation\nMSE of the adaptive mixture updated with the EG algorithm \u2212 theory\n\n\u22126\n\n0.2\n\n\u22126.5\n0.1\n\n\u22127\n\n0\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n\u22127.5\n\n3000\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n1200\n\n1400\n\nSamples\n\nSamples\n\n(a)\n\n(b)\nThe variance of the mixture weights updated with the EG algotihm\n\nThe mean of the mixture weights updated with the EG algotihm\n788\n\n28.1\n\n786\n\nThe variance of the mixture weight\n\nThe mean of the mixture weight\n\n28\n\nE[ \u03bb(1)\n(t) ] \u2212 theory\na\n\n27.9\n\n(1)\n\nE[ \u03bba (t) ] \u2212 simulation\n(10)\n\nE[ \u03bba (t) ] \u2212 theory\n\n27.8\n\nE[ \u03bb(10)(t) ] \u2212 simulation\na\n\n27.7\n\n784\n\n782\n\n780\n\n778\nE[\u03bb(1)(t)2] \u2212 theory\na\n\n776\n\nE[ \u03bb(1)(t)2 ] \u2212 simulation\na\n\nE[ \u03bb(1)(t) \u03bb(3)(t) ] \u2212 theory\na\n(1)\n\n774\n\na\n(3)\n\nE[ \u03bba (t) \u03bba (t) ] \u2212 simulation\n\n27.6\n\n772\n\n27.5\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n770\n\n3000\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\nSamples\n\nSamples\n\n(c)\n\n(d)\n\nFig. 2. Using 10 LMS filters as constituent filters, where learning rates for 2 constituent filters are \u03bc = 0.002 and for the rest are \u03bc \u2208 [0.1, 0.11]. SNR = -10dB.\nFor the mixture stage, the EG algorithm has \u03bcEG = 0.0008 and the LMS algorithm has \u03bcLMS = 0.005. For the EG algorithm, u = 500. (a) The weight of the\nfirst constituent filter in the mixture, i.e., E[\u03bb(1) (t)]. (b) The MSE curves for adaptive mixture updated with the EG algorithm, the adaptive mixture updated with\nthe LMS algorithm, the first constituent filter and the second constituent filter.\n(1)\n\n(10)\n\n(c) Theoretical values \u03bb\u0304a (t) and \u03bb\u0304a (t) and simulations. (d) Theoretical values\n\u0002 (1)\n\u0003\n\u0002 (1)\n\u0003\n(3)\nE \u03bba (t)2 and E \u03bba (t)\u03bba (t) and simulations.\n\n27\n\n3000\n\n\fThe mean of the unconstrained mixtures updated with the EGU algotihm\n\nThe variance of the mixture weights updated with the EGU algotihm\n\n1.2\n\n1.5\n(1) 2\na\n(1) 2\na\n(1)\n(2)\nw (t) w (t)\na\na\n(1)\n(2)\nw (t) w (t)\na\na\n(2)\nw (t) w(3)(t)\na\na\n(2)\n(3)\nw (t) w (t)\na\na\n(3)\n(4)\nw (t) w (t)\na\na\nw(3)(t) w(4)(t)\na\na\n\nE[ w (t) ] \u2212 theory\n1.1\n\nE[ w (t) ] \u2212 simulation\n\n(1)\na\n(1)\n\nE[ w (t) ] \u2212 theory\n\nE[\n0.9\n\nE[\nE[\n\n0.8\n\nE[\n0.7\n\nE[\nE[\n\n0.6\n\n(2)\nw (t)\na\n(2)\nw (t)\na\n(3)\nw (t)\na\n(3)\nw (t)\na\n(4)\nw (t)\na\n(4)\nw (t)\na\n\nThe variance of the mixture weight\n\nThe mean of the mixture weight\n\nE[\n\nE[ wa (t) ] \u2212 simulation\n\n1\n\n] \u2212 theory\n] \u2212 simulation\n] \u2212 theory\n] \u2212 simulation\n] \u2212 theory\n] \u2212 simulation\n\n0.5\n\n0.4\n\nE[\nE[\n1\n\nE[\nE[\nE[\n\n] \u2212 theory\n] \u2212 simulation\n] \u2212 theory\n] \u2212 simulation\n] \u2212 theory\n] \u2212 simulation\n\n0.5\n\n0.3\n\n0.2\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n3500\n\n0\n\n4000\n\n0\n\n500\n\n1000\n\n1500\n\nSamples\n\n2000\n\n2500\n\n3000\n\n3500\n\n4000\n\nSamples\n\n(a)\n\n(b)\n\nThe mean of the unconstrained mixtures updated with the EG algotihm\n\nThe variance of the mixture weights updated with the EG algotihm\n1.2\n\n1.3\n\n(1)\na\n(1)\nw (t)\na\n(2)\nw (t)\na\n(2)\nw (t)\na\n(3)\nwa (t)\n(3)\nw (t)\na\n(4)\nw (t)\na\nw(4)(t)\na\n\nE[\nE[\n\n1.1\n\nE[\n1\n\nE[\nE[\n\n0.9\n\nE[\n0.8\n\nE[\n\nE[ w (t) ] \u2212 simulation\n\n] \u2212 simulation\n\nThe variance of the mixture weight\n\nThe mean of the mixture weight\n\n1.2\n\n] \u2212 theory\n] \u2212 simulation\n] \u2212 theory\n] \u2212 simulation\n] \u2212 theory\n] \u2212 simulation\n\n0.7\n0.6\n0.5\n\nE[\n\n1\n\nE[\n0.9\n\nE[\nE[\n\n0.8\n\nE[\n0.7\n\nE[\n\n] \u2212 theory\n] \u2212 simulation\n] \u2212 theory\n] \u2212 simulation\n] \u2212 theory\n] \u2212 simulation\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.4\n0.3\n\n(2) 2\na\n(2) 2\na\n(1)\n(2)\nw (t) w (t)\na\na\n(1)\n(2)\nw (t) w (t)\na\na\n(2)\n(3)\nw (t) w (t)\na\na\n(2)\n(3)\nw (t) w (t)\na\na\n(2)\n(4)\nw (t) w (t)\na\na\n(2)\n(4)\nw (t) w (t)\na\na\n\nE[ w (t) ] \u2212 theory\n\n1.1\n\nE[ w (t) ] \u2212 theory\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n0.2\n\n3000\n\n0\n\n500\n\n1000\n\n1500\n\nSamples\n\nSamples\n\n(c)\n\n(d)\n\n2000\n\n2500\n\nFig. 3. Two LMS filters as constituent filters with learning rates \u03bc1 = 0.002 and\n\u03bc2 = 0.1, respectively. SNR = 1dB. For the second stage, the EGU algorithm\nhas \u03bcEGU = 0.01 and the EG algorithm has \u03bcEG = 0.01. For the EG algorithm,\nu = 3. (a) Theoretical values for the mixture weights updated with the EGU\n\u0002 (1)\n\u0003 \u0002 (1)\n\u0003\n(2)\nalgorithm and simulations. (b) Theoretical values E wa (t)2 , E wa (t)w a (t) ,\n\u0002 (2)\n\u0003\n\u0002 (3)\n\u0003\n(3)\n(4)\nE wa (t)wa (t) and E wa (t)wa (t) and simulations. (c) Theoretical mixture\nweights updated with the EG algorithm and simulations. (d) Theoretical values\n\u0003\n\u0002 (2)\n\u0003\n\u0003 \u0002 (2)\n\u0002 (2)\n\u0003 \u0002 (1)\n(3)\n(4)\n(2)\nE wa (t)2 , E wa (t)w a (t) , E w a (t)w a (t) and E wa (t)wa (t) and simulations.\n\n28\n\n3000\n\n\fDifference in dB\n\nDifference in dB\n\n\u221280\n\n\u2212110\n\n\u2212120\n\u221290\n\u2212130\n\n\u2212110\n\n\u2212140\n\nDifference (dB)\n\nDifference (dB)\n\n\u2212100\n\nDifference with the same algorithmic parameters as in Fig. 2\nDifference with the same algorithmic parameters as in Fig. 3\n\n\u2212120\n\nDifference with the same algorithmic parameters as in Fig. 2\nDifference with the same algorithmic parameters as in Fig. 3\n\n\u2212150\n\n\u2212160\n\n\u2212170\n\n\u2212180\n\n\u2212130\n\n\u2212190\n\u2212140\n\u2212200\n\n\u2212150\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n\u2212210\n\n3000\n\n500\n\n1000\n\n1500\n\n2000\n\nSamples\n\nSamples\n\n(a)\n\n(b)\n2\n\ni (t)\u2212\u0177m (t)))}k\nFig. 4. (a) The difference \u221ak exp{\u03bce(t)(\u0177i (t)\u2212\u0177m (t))}\u2212{1+\u03bce(t)(\u0177\n2\n\nk exp{\u03bce(t)(\u0177i (t)\u2212\u0177m (t))}k k{1+\u03bce(t)(\u0177i (t)\u2212\u0177m (t)))}k2\n\n2500\n\nfor i = 1\n\nwith the same algorithmic parameters as in Fig. 2 and Fig.\nn\u0002 3. (b) The first\no\n\u0001\u0003 parame( \u0002\n)\n\u0001\u0003\n2\nE\nI+\u03bce(t)diag u(t)\n\u03bb\n(t)\na\nI+\u03bce(t)diag u(t)\no\n\u0003 \u03bba (t) \u2212u n\u0002\nE u \u0002 T\n\u0003\n1 +\u03bce(t)uT (t) \u03bba (t)\n1T +\u03bce(t)uT (t) \u03bba (t)\nE\nter of the difference v\no\nn\u0002\n\u0001\u0003\nu ( \u0002\n) 2\n\u0001\u0003\nu\nI+\u03bce(t)diag u(t)\n\u03bba (t) 2\nE\nI+\u03bce(t)diag u(t)\n\u03bb\na (t)\nu\n\u0003\no\nu n\u0002\n\u0003\nt E u \u0002 T\n1 +\u03bce(t)uT (t) \u03bba (t)\n1T +\u03bce(t)uT (t) \u03bba (t)\nE\n\nwith the same algorithmic parameters as in Fig. 2 and Fig. 3.\n\n29\n\n3000\n\n\fMSEs of the constituent filters and adaptive mixtures, SNR = \u22125dB\n\nMSEs of the constituent filters and adaptive mixtures, SNR = 5dB\n\n\u22122\n\n1\n\n\u22123\n\n0\n\n\u22121\n\n\u22125\n\n\u22126\n\nMSE (dB)\n\nMSE (dB)\n\n\u22124\n\nMSE of the first constituent filter\nMSE of the second constituent filter\nMSE of the adaptive mixture updated with the LMS algorithm\nMSE of the adaptive mixture updated with the EG algorithm\nMSE of the adaptive mixture updated with the EGU algorithm\n\n\u22127\n\n\u22122\n\n\u22123\n\n\u22124\n\n\u22128\n\n\u22129\n\nMSE of the first constituent filter\nMSE of the second constituent filter\nMSE of the adaptive mixture updated with the LMS algorithm\nMSE of the adaptive mixture updated with the EG algorithm\nMSE of the adaptive mixture updated with the EGU algorithm\n\n\u22125\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n1200\n\n\u22126\n\n1400\n\n200\n\n400\n\n600\n\n800\n\n1000\n\nSamples\n\nSamples\n\n(a)\n\n(b)\n\n1200\n\n1400\n\n1600\n\nFig. 5. Algorithmic parameters and constituent filters are selected as in Fig. 2 under\nSNR = -5dB. For the second stage, the EG algorithm has \u03bcEG = 0.0005, the EGU\nalgorithm has \u03bcEGU = 0.005 and the LMS algorithm has \u03bcLMS = 0.005. For the EG\nalgorithm, u = 500. (a) the MSE curves for the adaptive mixture updated with the\nEG algorithm, the adaptive mixture updated with the EGU algorithm, the adaptive\nmixture updated with the LMS algorithm, the first constituent filter and the second\nconstituent filter. Next, SNR = 5dB. For the second stage, the EG algorithm has\n\u03bcEG = 0.002, the EGU algorithm has \u03bcEGU = 0.005 and the LMS algorithm has\n\u03bcLMS = 0.005. For the EG algorithm, u = 100. (b) the MSE curves for the adaptive\nmixture updated with the EG algorithm, the adaptive mixture updated with the\nEGU algorithm, the adaptive mixture updated with the LMS algorithm, the first\nconstituent filter and the second constituent filter.\n\n30\n\n1800\n\n2000\n\n\f"}