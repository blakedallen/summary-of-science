{"id": "http://arxiv.org/abs/cs/0601107v2", "guidislink": true, "updated": "2006-09-29T12:30:12Z", "updated_parsed": [2006, 9, 29, 12, 30, 12, 4, 272, 0], "published": "2006-01-25T10:33:11Z", "published_parsed": [2006, 1, 25, 10, 33, 11, 2, 25, 0], "title": "Structure of Optimal Input Covariance Matrices for MIMO Systems with\n  Covariance Feedback under General Correlated Fading", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0601050%2Ccs%2F0601066%2Ccs%2F0601047%2Ccs%2F0601029%2Ccs%2F0601054%2Ccs%2F0601104%2Ccs%2F0601053%2Ccs%2F0601041%2Ccs%2F0601028%2Ccs%2F0601114%2Ccs%2F0601037%2Ccs%2F0601007%2Ccs%2F0601064%2Ccs%2F0601019%2Ccs%2F0601081%2Ccs%2F0601014%2Ccs%2F0601086%2Ccs%2F0601071%2Ccs%2F0601057%2Ccs%2F0601070%2Ccs%2F0601134%2Ccs%2F0601109%2Ccs%2F0601107%2Ccs%2F0601077%2Ccs%2F0601090%2Ccs%2F0601048%2Ccs%2F0601098%2Ccs%2F0601106%2Ccs%2F0601043%2Ccs%2F0601022%2Ccs%2F0601016%2Ccs%2F0601055%2Ccs%2F0601030%2Ccs%2F0601115%2Ccs%2F0601017%2Ccs%2F0601046%2Ccs%2F0601005%2Ccs%2F0601008%2Ccs%2F0601065%2Ccs%2F0601102%2Ccs%2F0601117%2Ccs%2F0601033%2Ccs%2F0601036%2Ccs%2F0601052%2Ccs%2F0601035%2Ccs%2F0601078%2Ccs%2F0601089%2Ccs%2F0601063%2Ccs%2F0601108%2Ccs%2F0601013%2Ccs%2F0601135%2Ccs%2F0601127%2Ccs%2F0601001%2Ccs%2F0601080%2Ccs%2F0601111%2Ccs%2F0601009%2Ccs%2F0601124%2Ccs%2F0601125%2Ccs%2F0601042%2Ccs%2F0601097%2Ccs%2F0601073%2Ccs%2F0601034%2Ccs%2F0601039%2Ccs%2F0601061%2Ccs%2F0601032%2Ccs%2F0601006%2Ccs%2F0601113%2Ccs%2F0601002%2Ccs%2F0601059%2Ccs%2F0601024%2Ccs%2F0601100%2Ccs%2F0601094%2Ccs%2F0601133%2Ccs%2F0601027%2Ccs%2F0601083%2Ccs%2F0601031%2Ccs%2F0601091%2Ccs%2F0601056%2Ccs%2F0601126%2Ccs%2F0601110%2Ccs%2F0601049%2Ccs%2F0601131%2Ccs%2F0601120%2Ccs%2F0601068%2Ccs%2F0601038%2Ccs%2F0601105%2Ccs%2F0601062%2Ccs%2F0601122%2Ccs%2F0601121%2Ccs%2F0601082%2Ccs%2F0601118%2Ccs%2F0601123%2Ccs%2F0202031%2Ccs%2F0202022%2Ccs%2F0202017%2Ccs%2F0202023%2Ccs%2F0202010%2Ccs%2F0202003%2Ccs%2F0202020%2Ccs%2F0202011%2Ccs%2F0202025&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Structure of Optimal Input Covariance Matrices for MIMO Systems with\n  Covariance Feedback under General Correlated Fading"}, "summary": "We describe the structure of optimal Input covariance matrices for single\nuser multiple-input/multiple-output (MIMO) communication system with covariance\nfeedback and for general correlated fading. Our approach is based on the novel\nconcept of right commutant and recovers previously derived results for the\nKronecker product models. Conditions are derived which allow a significant\nsimplification of the optimization problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0601050%2Ccs%2F0601066%2Ccs%2F0601047%2Ccs%2F0601029%2Ccs%2F0601054%2Ccs%2F0601104%2Ccs%2F0601053%2Ccs%2F0601041%2Ccs%2F0601028%2Ccs%2F0601114%2Ccs%2F0601037%2Ccs%2F0601007%2Ccs%2F0601064%2Ccs%2F0601019%2Ccs%2F0601081%2Ccs%2F0601014%2Ccs%2F0601086%2Ccs%2F0601071%2Ccs%2F0601057%2Ccs%2F0601070%2Ccs%2F0601134%2Ccs%2F0601109%2Ccs%2F0601107%2Ccs%2F0601077%2Ccs%2F0601090%2Ccs%2F0601048%2Ccs%2F0601098%2Ccs%2F0601106%2Ccs%2F0601043%2Ccs%2F0601022%2Ccs%2F0601016%2Ccs%2F0601055%2Ccs%2F0601030%2Ccs%2F0601115%2Ccs%2F0601017%2Ccs%2F0601046%2Ccs%2F0601005%2Ccs%2F0601008%2Ccs%2F0601065%2Ccs%2F0601102%2Ccs%2F0601117%2Ccs%2F0601033%2Ccs%2F0601036%2Ccs%2F0601052%2Ccs%2F0601035%2Ccs%2F0601078%2Ccs%2F0601089%2Ccs%2F0601063%2Ccs%2F0601108%2Ccs%2F0601013%2Ccs%2F0601135%2Ccs%2F0601127%2Ccs%2F0601001%2Ccs%2F0601080%2Ccs%2F0601111%2Ccs%2F0601009%2Ccs%2F0601124%2Ccs%2F0601125%2Ccs%2F0601042%2Ccs%2F0601097%2Ccs%2F0601073%2Ccs%2F0601034%2Ccs%2F0601039%2Ccs%2F0601061%2Ccs%2F0601032%2Ccs%2F0601006%2Ccs%2F0601113%2Ccs%2F0601002%2Ccs%2F0601059%2Ccs%2F0601024%2Ccs%2F0601100%2Ccs%2F0601094%2Ccs%2F0601133%2Ccs%2F0601027%2Ccs%2F0601083%2Ccs%2F0601031%2Ccs%2F0601091%2Ccs%2F0601056%2Ccs%2F0601126%2Ccs%2F0601110%2Ccs%2F0601049%2Ccs%2F0601131%2Ccs%2F0601120%2Ccs%2F0601068%2Ccs%2F0601038%2Ccs%2F0601105%2Ccs%2F0601062%2Ccs%2F0601122%2Ccs%2F0601121%2Ccs%2F0601082%2Ccs%2F0601118%2Ccs%2F0601123%2Ccs%2F0202031%2Ccs%2F0202022%2Ccs%2F0202017%2Ccs%2F0202023%2Ccs%2F0202010%2Ccs%2F0202003%2Ccs%2F0202020%2Ccs%2F0202011%2Ccs%2F0202025&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We describe the structure of optimal Input covariance matrices for single\nuser multiple-input/multiple-output (MIMO) communication system with covariance\nfeedback and for general correlated fading. Our approach is based on the novel\nconcept of right commutant and recovers previously derived results for the\nKronecker product models. Conditions are derived which allow a significant\nsimplification of the optimization problem."}, "authors": ["Igor Bjelakovic", "Holger Boche"], "author_detail": {"name": "Holger Boche"}, "author": "Holger Boche", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ISIT.2006.261886", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cs/0601107v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0601107v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "5 pages, corrected typos", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0601107v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0601107v2", "journal_reference": "Proc. of the 2006 IEEE International Symposium on Information\n  Theory, ISIT 2006 Seattle, pp. 1041-1045", "doi": "10.1109/ISIT.2006.261886", "fulltext": "arXiv:cs/0601107v2 [cs.IT] 29 Sep 2006\n\nStructure of Optimal Input Covariance Matrices for\nMIMO Systems with Covariance Feedback under\nGeneral Correlated Fading\nIgor Bjelakovi\u0107 and Holger Boche\nHeinrich-Hertz-Chair for Mobile Communications\nBerlin University of Technology\nWerner-von-Siemens-Bau (HFT 6), Einsteinufer 25, 10587 Berlin, Germany\nEmail: {igor.bjelakovic, holger.boche}@mk.tu-berlin.de\n\nAbstract- We describe the structure of optimal Input covariance matrices for single user multiple-input/multiple-output\n(MIMO) communication system with covariance feedback and\nfor general correlated fading. Our approach is based on the novel\nconcept of right commutant and recovers previously derived\nresults for the Kronecker product models. Conditions are derived\nwhich allow a significant simplification of the optimization\nproblem.\n\nI. I NTRODUCTION\nSince the seminal work of Telatar [1] on the Shannon\ncapacity of multi-antenna wireless systems, this area has\nattracted a lot of attention. The deveploment has started with\nthe investigation of the capacity of single-user MIMO systems.\nMany results on the capacity for different types of channel\nstate information at the transmitter and/or receiver are known.\nThe achieved progress in this field was the key element, that\nMIMO systems are already used in existing systems. One\nimportant research topic on MIMO systems is the impact of\ncorrelation of the channel matrix on the achievable capacity\n[2]-[8]. A lot of results are known in this area, but most of the\nworks are using the assumption, that the channel covariance\nmatrix is the Kronecker product of the covariance matrices of\nthe transmit and receive antennas [3], [4]. In the following\npaper the general case is analyzed.\nThe paper is organized as follows: In Section II we review\nshortly the model and formulate the main problem. Additionally we divide the set of variance matrices into two classes\nof separable and entangled positive semidefinite matrices, a\ndefinition borrowed from quantum information theory. This\nseparation shall help us to present our results for the class of\nthe separable matrices which is easier to deal with, followed\nby an extension of results to entangled matrices. Section\nIII starts with a novel concept of right commutant which\nis the key ingredient in our approach. It can be seen as a\ncharacterization of one-sided invariant subspaces for the given\nchannel variance matrix (cf. Lemma 3.1) or, alternatively,\nas description of symmetries of the channel variance matrix\n(cf. Lemma 3.5.1). Our subsequent results in Section III rely\nhardly on that concept, which, combined in a appropriate\n\nway with some simple concavity considerations 1 , turns out\nto be rather powerful tool. For example, we do not need\nany majorization results/considerations which are the basis\nof results in [7], [8]. Our main result, Theorem 3.3, is a\ncharacterization of optimal input variance matrices.\nNotation and Preliminaries We shall denote matrices by\ncapital letters, e.g. H. The hermitian conjugate (adjoint) is\ndenoted by (*)H while (*)t is reserved for the transpose of a\nmatrix. The set of N \u00d7 N matrices with complex entries is\nabbreviated by M(N, C) and A\u2297B denotes the tensor product\n(Kronecker product) of matrices A and B. 1N is the N \u00d7 N\nunity matrix. diag(Q1 , . . . , Qc ) is the shorthand for the matrix\nwhich has the matrices Q1 , . . . , Qc as its diagonal entries and\n0s else, the size of the diagonal blocks will be specified in\neach particular case. tr(A) is the trace of the matrix A and\nH \u223c N (0, \u03a3) means that the complex valued random matrix\nH of prescribed size is normally distributed with mean 0 and\nvariance \u03a3.\nWe shall introduce some simple concepts from the theory of \u2217algebras of matrices which will be helpful in this paper (cf. [9]\nchap. I for more information). A \u2217-algebra A in M(N, C) is\na linear subspace which is closed under matrix multiplication\nand under the action of (*)H -operation. It can be shown [9] that\neach \u2217-algebra of matrices has a multiplicative unit. \u2217-algebras\nappearing in this paper shall have 1N as the unit element with\nrespect to the matrix multiplication. A (orthogonal) projection\nP 6= 0 is called minimal projection in A if P \u2208 A and\nQ \u2264 P for any projection Q \u2208 A implies Q = 0 or P = Q.\nEquivalently, a non-zero projection P \u2208 A is minimal if and\nonly if P AP = CP . By a resolution of identity in A we\nmean a set ofPmutually orthogonal projections {Pi }ci=1 \u2282 A\nc\nthat satisfies i=1 Pi = 1, where 1 denotes the multiplicative\nunit in A.\nIf A \u2208 A \u2282 M(N, C) is hermitian or normal matrix, then\nwe can\nP represent it according to the spectral theorem as\nA =\n\u03bb\u2208\u03c3(A) \u03bbP\u03bb , where \u03c3(A) denotes the spectrum (set\n1 After finishing this paper we learned that Tulino, Lozano and Verd\u00fa\n[14] used the concavity of the capacity in a similar way to characterize\noptimal covariances for channels with independent columns and symmetric\njoint distribution.\n\n\fof eigenvalues) and P\u03bb is the projection onto the eigenspace\ncorresponding to the eigenvalue \u03bb. By defining properties\nof a \u2217-algebra, with A \u2208 A we also have g(A) \u2208 A for\neach complex valued polynomial. It is easily seen that for\neach \u03bb \u2208 \u03c3(A) there is complex valued polynomial g\u03bb with\ng\u03bb (A) = P\u03bb and hence P\u03bb \u2208 A for all \u03bb \u2208 \u03c3(A), a fact which\nwill be useful in the proof of Lemma 3.1 below.\nFinally, we recall a way of viewing a tensor product of\nmatrices as a linear map which will be necessary in the last part\nof the paper: For A \u2208 M(M, C), B \u2208 M(N, C) we consider\nthe tensor product A \u2297 B and an M \u00d7 N matrix H. Then it is\neasily seen using rank one M \u00d7 N matrices that the canonical\naction of A\u2297B on H is given by (A\u2297B)(H) = AHB t . This\naction extends to arbitrary elements of M(M, C) \u2297 M(N, C)\nby linearity, since each \u03a3 \u2208 M(M, C) \u2297 M(N, C) can be\nwritten as a complex linear combination of such elementary\ntensors A \u2297 B.\nII. M ODEL\n\nAND\n\nP ROBLEM F ORMULATION\n\nWe focus on a single point-to-point wireless communication\nsystem using N transmit and M receive antennas. We assume,\nthat the behavior of the channel can be described by the well\nknown narrow-band flat fading channel model, i.e.\ny = Hx + n,\n\nC = max E(log det(1M +\n\nwhere x is the N dimensional transmit vector, y is the M\ndimensional receive vector, H is the M \u00d7 N channel matrix,\nand the M components nk of the noise vector n are assumed\nto be i.i.d. complex circularly symmetric Gaussian distributed\nwith mean 0 and variance \u03c3n2 . For the channel matrix H we\nwill use a more general correlation model than [7], [8] to\npresent our ideas in the most transparent way which allows\na direct comparison with the existing results. Then we shall\nshow that this correlation model already incloses the full\ncomplexity of the general case. The channel matrix in this\nspecial case can be described as follows:\nH=\n\ns\nX\n\n1\n2\n\nt 12\n\nRi Wi Ti ,\n\n(1)\n\nwhere Wi are i.i.d. zero mean , mutually independent complex\nGaussian M \u00d7N matrices and the positive semidefinite M \u00d7M\nresp. N \u00d7 N matrices Ri resp. Ti are related to the variance\n\u03a3 of H by\ns\nX\n\nRi \u2297 Ti ,\n\ntr(Q)\u2264p\nQ\u22650\n\n1\nHQH H )),\n\u03c3n2\n\n(3)\n\nas it is easily seen using the results of [12]. The optimization\nproblem (3) is a convex smooth optimization problem. The\ncapacity C = C(Q\u0302) for an optimal transmit covariance matrix\nQ\u0302 is achieved by transmitting independent complex circular\nGaussian symbols along the eigenvectors of Q\u0302, and the powers\nare allocated according to the eigenvalues of the matrix Q\u0302 [5][8].\nIII. R ESULTS\nFor a given variance matrix \u03a3 \u2208 M(M, C) \u2297 M(N, C) we\ndefine the \"right\" commutant\nC\u03a3 := {A \u2208 M(N, C)|(1M \u2297 A)\u03a3 = \u03a3(1M \u2297 A)},\n\ni=1\n\n\u03a3=\n\northonormal basis in C2 . This non-uniqueness with respect to\ndecompositions corresponds to the freedom of choice in the\nparticular realization of random variables distributed according\nto a given probability distribution.\nPSD matrices acting on CM \u2297 CN that allow a decomposition\nas in (2) with PSD summands are called separable in quantum\ninformation theory. Otherwise we say that they are entangled\n(cf. [11], [10] and references therein). The simplest example\nof an entangled PSD matrix is given by gg H , where g :=\ne1 \u2297 e1 + e2 \u2297 e2 and {e1 , e2 } being canonical basis of C2 .\nA handy sufficient criterion for separability of a given PSD\nmatrix over CM \u2297 CN is given in [10]:\nTheorem 2.1 (Gurvits/Barnum): A PSD matrix \u03a3 is separable if ||\u03a3\u22121M \u22971N ||2 \u2264 1, where ||*||2 denotes\np the Hilbert(A, A)HS :=\nSchmidt\nnorm\non\nmatrices\n(i.e.\n||A||\n:=\n2\np\ntr(AH A)).\nIn the following paper we assume, that the receiver knows\nthe channel perfectly, and the transmitter has only knowledge\nof the channel covariance matrix \u03a3. As a consequence, the\nchannel state information at the transmitter is a deterministic\nfunction of the channel state information at the receiver. Under\nthis condition the ergodic capacity of the considered MIMO\nsystem is given by\n\n(2)\n\ni=1\n\nwhere \u03a3 := E(H \u2297 H) which has components E(Hi,j Hl,m ).\nObserve that, since we are dealing with complex matrices,\nA \u2265 0 implies that A is hermitian.\nRemark: Note that such decompositions into a sum of tensor products of positive semidefinite (PSD) matrices are,\nin general, non-unique: a simple example is given in the\nsymmetric case of two transmit and two receive antennas with\nthe variance matrix \u03a3 =\n\u2297 1 which can be alternatively\nP1\n2\nH\ndecomposed into \u03a3 =\ni=1 1 \u2297 ei ei , {e1 , e2 } being any\n\nand consider any resolution of unity consisting of mutually\nPt\northogonal minimal projections in C\u03a3 , i.e. 1N =\ni=1 Pi\nwith Pi \u2208 C\u03a3 minimal and Pi Pj = \u03b4ij Pi .\nExample 1. If the variance matrix is given by \u03a3 = R \u2297 T then\nwe have C\u03a3 = {A \u2208 M(N, C)|AT = T A}, and each set of\nmutually orthogonal minimal projections in C\u03a3 adding to 1N\nis given by projections onto the one-dimensional subspaces\nspanned by the eigenvectors of T .\nSome simple observations concerning the concept of right\ncommutant are collected for ease in the following\nLemma 3.1: Let \u03a3 be a PSD matrix in M(M, C)\u2297M(N, C)\nthen we have:\n1. C\u03a3 is a subalgebra of M(N, C) containing 1N which is\nclosed under (*)H \u2212operation, i.e. C\u03a3 is a \u2217-algebra.\n2. Let {Pi }ui=1 and {Qj }vj=1 be resolutions of identity\nconsisting of minimal projections in C\u03a3 . Then u = v\n\n\fand there is a permutation \u03c0 of {1, . . . , u} such that\ntr(Pi ) = tr(Q\u03c0(i) ) for all i \u2208 {1, . . . , u}.\n3. If \u03a3 is separable and if {Pj }uj=1 is a resolution of identity\nconsisting of minimal projections in C\u03a3 , then there is a\ndecomposition of \u03a3 into sum of tensor products of PSD\nmatrices\ns\nX\nRi \u2297 Ti ,\n\u03a3=\ni=1\n\nsatisfying Ti Pj = Pj Ti for all i \u2208 {1, . . . , s} and j \u2208\n{1, . . . , u}.\nRemark: Our right commutant C\u03a3 is a close relative of the\nconcept of commutant which is widely used in the theory\nof operator algebras and quantum information theory. And,\nindeed, the proof of the properties stated in Lemma 3.1\nconsist of some standard conclusions, at least for those already\nfamiliar with the usual commutant from the theory of operator\nalgebras. For the ease of reading we include this short proof.\nProof of Lemma 3.1: The first item is easily checked by\ninspection and is standard in the theory of matrix (operator)\nalgebras (cf. [9]). For the second item, note that each Pi Qj Pi\nis hermitian and contained in C\u03a3 . It is well known that then\nall spectral projections of Pi Qj Pi are also contained in C\u03a3 .\nUsing this fact it is easy to deduce a contradiction to the\nassumed minimality of the involved projections unless u = v.\nThe second part is then easily obtained.\nThe third item follows from the relation\nu\nX\n(1M \u2297 Pj )\u03a3(1M \u2297 Pj ),\n\u03a3=\nj=1\n\nPn\ncombined with \u03a3 = l=1 R\u0303l \u2297 T\u0303l where R\u0303l and T\u0303l are PSD,\nwhich is ensured by separability of \u03a3. Indeed, we merely have\nto set\nu\nX\nPj T\u0303i Pj ,\nRi := R\u0303i and Ti :=\nj=1\n\nand we arrive at the desired conclusion of the lemma.\n\u0003\nRemark: As we will show in the following the minimal\nprojections {Pjt }uj=1 shall serve as the starting point of\nblock-diagonalization procedure for optimal input covariance\nmatrices. The second part of Lemma 3.1 ensures that no\nparticularly chosen minimal resolution of identity is preferred,\ni.e. the dimensions of the corresponding ranges of considered\nprojections are equal up to a permutation.\nUnfortunately, there are cases where the algebra C\u03a3 is trivial, i.e. consists of complex multiples of 1N as the following\nexample shows:\nH\nH\nExample 2. Let M = 2 = N and \u03a3 = e1 eH\n1 \u2297 e1 e1 + e2 e2 \u2297\nH\n2\ngg , where {e1 , e2 } denotes the canonical basis in C and\ng = \u221a12 (e1 + e2 ). Let P \u2208 C\u03a3 be a projection, then we have\n(1M \u2297 P )\u03a3 = \u03a3(1M \u2297 P ). Inserting this into the expression\nfor \u03a3 above and multiplying with ei eH\ni \u2297 1N for i = 1, 2 we\nH\nend up with two equations e1 eH\nP\n=\nP e1 eH\n1\n1 and gg P =\nP gg H . A simple calculation shows that P = \u03b11N with\n\u03b1 \u2208 R+ and hence P = 0 or P = 1N .\nIn the following we separate our presentation in two parts; in\n\nthe first we consider the separable variance matrices while in\nthe second no restrictions on channel matrices H are assumed.\nThis separation, although not necessary from the viewpoint\nof mathematics, has the advantage that we can first present\nour ideas in a situation which is close in the spirit to the\nprevious work of Jafar/Wishwanath/Goldsmith [6], [7] and\nJorswieck/Boche [8], and then we show that the result extends\nimmediately to the general case.\nA. Optimal Input Covariance Matrices: Separable Case\nNow, we can describe the optimal input matrix in the case\nwhere \u03a3 is separable and C\u03a3 contains non-trivial minimal\nprojections, i.e. not equal 1N .\nChoose any resolution of identity consisting of minimal mutut\nally orthogonal projections C\u03a3\n(the transpose of C\u03a3 ) , denoted\nc\nby {Pj }j=1 , and a decomposition of \u03a3 with properties given\nin Lemma 3.1.3 with respect to {Pjt }cj=1 , a resolution of\nidentity consisting of minimal projections in C\u03a3 . Then there\nis a unitary U such that Tit = U diag(Ti (1), . . . , Ti (c))U H for\nall i \u2208 {1, . . . , s}, where the matrices Ti (j) map the range of\nPj into itself, i.e. each Tit is block-diagonal in the basis given\nby the unitary matrix U .\nTheorem 3.2: Suppose that the variance matrix \u03a3 of H \u223c\nN (0, \u03a3) is separable and that C\u03a3 6= C * 1N . Then the capacity\nachieving covariance matrix Q can be chosen such that\nQ = U diag(Q1 , . . . , Qc )U H ,\nwhere each Qj maps the range of Pj into itself, j \u2208 {1, . . . , c}.\nProof: Suppose that we are given any capacity achieving\ncovariance matrix Q, i.e.\n\u0013\u0013\n\u0012\n\u0012\nHQH H\n.\nC = C(Q) = E log det 1M +\n\u03c3n2\nDue to our system assumption, the last expression is written\nas\n1\n1\nPs\nPs\nt 21\nt 21\nH 2\n2\ni=1 Ri Wi Ti Q\nl=1 Tl Wl Rl\n)).\nC = E(log det(1M +\n\u03c3n2\nNow, we insert the relation\nTit = U diag(Ti (1), . . . , Ti (c))U H =: U T\u0303i U H ,\nwith Q\u0303 := U H QU fulfilling tr(Q) = tr(Q\u0303) and arrive at\n1\n1\n1\n1\nPs\nH 2\n2\n2\n2\ni,l=1 Ri Wi T\u0303i Q\u0303T\u0303l Wl Rl\nC = E(log det(1M +\n))\n\u03c3n2\n=: C\u0303(Q\u0303)\n(4)\nwhere we have used that the random matrices Wi and Wi U\nhave the same probability distribution since each Wi is i.i.d.\nGaussian and the Wi 's are jointly independent. The transformed matrix Q\u0303 can be written as a block matrix with respect\nto the transformation U induced by the set {Pj }cj=1 of minimal\nt\nprojections in C\u03a3\n:\n\uf8f6\n\uf8eb\nQ11 Q12 . . . Q1c\n\uf8ec Q21 Q22 . . . Q2c \uf8f7\n\uf8f7\n\uf8ec\nQ\u0303 = \uf8ec .\n\uf8f7.\n..\n..\n\uf8f8\n\uf8ed ..\n.\n.\nQc1\n\nQc2\n\n. . . Qcc\n\n\fWe consider the unitary and hermitian matrix\nU1 := diag(1P1 , \u22121P2 , \u22121P3 , . . . , \u22121Pc ),\nwhere 1Pj denotes the matrix acting as the identity on the\nrange of Pj . Then we have U1 T\u0303i U1 = T\u0303i ,\n\uf8f6\n\uf8eb\nQ11\n0\n...0\n\uf8ec 0\nQ22 . . . Q2c \uf8f7\n1\n\uf8f7\n\uf8ec\nQ\u03031 := (Q\u0303 + U1 Q\u0303U1 ) = \uf8ec .\n\uf8f7,\n..\n..\n2\n\uf8ed ..\n. \uf8f8\n.\n0\nQc2 . . . Qcc\nand tr(Q\u0303) = tr(Q\u03031 ).\nDue to the concavity of the functional C\u0303 defined by the last\neqn. in (4) we end up with\n1\n1\nC \u2265 C\u0303(Q\u03031 ) \u2265 C\u0303(Q\u0303) + C\u0303(U1 Q\u0303U1 ) = C\u0303(Q\u0303)\n2\n2\n= C,\n(5)\nwhere we have used U1 T\u0303i U1 = T\u0303i in the first equality. In\nthe next step we consider the unitary and hermitian matrix U2\ngiven by\nU2 := diag(1P1 , 1P2 , \u22121P3 , . . . , \u22121Pc ),\nand can define in a similar way a matrix Q\u03032 := 12 (Q\u03031 +\nU2 Q\u03031 U2 ) and show analogously that C\u0303(Q\u03032 ) = C holds.\nContinuing this procedure we arrive at the claimed conclusion\nof the theorem.\n\u0003\nNote that, as mentioned previously, in the case \u03a3 = R \u2297 T\nthe resolution of identity {Pj }c1 consists of one-dimensional\nprojections, i.e. c = N and we recover the results of [7],\n[8] that the optimal transmission strategy consists of sending\nindependent circularly symmetric gaussian inputs along the\neigenvectors of T .\nB. Optimal Input Covariance Matrices: General Case\nIf we examine carefully our construction in the proof of\ntheorem 3.2 we see that we have needed only the concavity of\nthe capacity functional together with the fact that Uj T\u0303i Uj = T\u0303i\nwhich means that applying Uj does not change the probability\ndistribution of the considered random matrix H. Hence, in\norder to extend our proof to the case of general random\nmatrices H \u223c N (0, \u03a3) we merely have to consider the\nbasis-free versions of hermitian and unitary matrices Uj =\n2(P1 + . . . + Pj ) \u2212 1N , j = 1, . . . , c which realize our blockdiagonalization. Taking into account the first part of Lemma\n3.5 below, that contains the description of the symmetries of\nthe channel at our disposal, we conclude that Theorem 3.2\nextends mutatis mutandis to the general situation. The only\nchange is that we drop the condition of separability we have\nsupposed in the statement of Theorem 3.2:\nTheorem 3.3: Let H \u223c N (0, \u03a3) be a random M \u00d7 N\nchannel matrix and suppose that C\u03a3 6= C1N . Then the capacity\nachieving covariance matrix Q can be chosen such that\n\nt\nin C\u03a3\nand U is any unitary matrix which diagonalizes all Pj\nsimultaneously.\nWe now use Theorem 3.3 for a further analysis of our\noptimization problem. We use the structure\n\nQ = [U 1 , . . . , U c ]diag(Q1 , . . . , Qc )[U 1 , . . . , U c ]H\nof the optimal transmit covariance matrix Q. The block Qi has\nthe dimension li \u00d7 li and the corresponding unitary matrix U i\nc\nP\nli = N . If we use the matrix\nhas the size M \u00d7 li . We have\ni=1\n\nHi = HU i , then we have for the optimal transmit covariance\nmatrix\nc\n1 X\nHl Ql HlH )).\nC = I(Q) = E(log det(1M + 2\n\u03c3n\nl=1\n\nThus the optimal block matrix diag(Q1 , . . . , Qc ) can be calculated as the solution of\nc\n1 X\nmax E(log det(1M + 2\nHl Ql HlH )).\nQl \u22650\n\u03c3n\nc\nP\n\nl=1\n\ntr(Ql )\u2264p\n\nl=1\n\nAs a consequence of this simple observation and Theorem\n3.3 we achieve the following corollary.\nCorollary 3.4: The block matrix diag(Q\u03021 , . . . , Q\u0302c ) is the\noptimal block matrix if and only if, there exists a \u03bc > 0 and\npositive semidefinite matrices \u03a81 , . . . , \u03a8c , such that Q\u0302k \u2265\n0, 1 \u2264 k \u2264 c,\nc\n\nX\n1\nE(tr(HkH (1M +\nHl Q\u0302l HlH )\u22121 Hk ) = \u03bc1lk \u2212 \u03a8k ,\n2\n\u03c3n\nl=1\n\ntr(\u03a8k Q\u0302k ) = 0,\n\nand\n\nc\nX\n\n1 \u2264 k \u2264 c,\n\ntr(Q\u0302l ) = p\n\nl=1\n\nholds.\nP\nRemark: For the classical correlation scenario\n= R\u2297\nT we have again c = N, l1 = . . . = lN = 1, and Q\u0302 =\ndiag(p\u03021 , . . . , p\u0302N ), p\u0302l \u2265 0, where the p\u0302l are the solution of the\nwell known power optimization problem [7], [8].\nThe following Lemma 3.5 gives a further description of the\noptimal transmit covariance matrices.\nLemma 3.5: Consider any M \u00d7 N random channel matrix\nH \u223c N (0, \u03a3) and let U be a unitary N \u00d7 N matrix. Then:\n1. The channel matrices H and HU have equal probability\nt\ndensity functions iff U t \u2208 C\u03a3 , or equivalently U \u2208 C\u03a3\n.\n(1)\n(2)\n2. If Q\nand Q\nare capacity achieving PSD matrices,\ni.e. C(Q(1) ) = C(Q(2) ), with tr(Q(1) ) = p = tr(Q(2) )\nthen\nHQ(1) H H = HQ(2) H H\n\na.s.,\n\n(6)\n\nH\n\nQ = U diag(Q1 , . . . , Qc )U ,\nwhere Qj maps the range of Pj into itself, {Pj }cj=1 denotes\nany resolution of identity consisting of minimal projections\n\nwith respect to the law of H.\nProof: 1. The first statement is easily obtained by using change\nof variables. For reader's convenience we give some crucial\n\n\fsteps: First, the variances \u03a3 of H resp. \u03a3U of HU are related\nby \u03a3U = (1M \u2297U tH )\u03a3(1M \u2297U t ). This can be easily verified\nusing change of variables formula and observing that each\ntensor product A \u2297 B \u2208 M(M, C) \u2297 M(N, C) canonically\ninduces a linear map on M \u00d7 N matrices by assignment\nH 7\u2192 AHB t . Note that the probability density function of\nthe channel matrix can be written as\n1\n\nf (H) = Ke\u2212 2 (H,\u03a3\n\n\u22121\n\nH)HS\n\nThese two relations lead immediately to\n1M +\n\nHQ(2) H H\nHQ(1) H H\n= 1M +\n2\n\u03c3n\n\u03c3n2\n\na.s.\n\n\u0003\n\nRemark: As the proof shows, the second part of our Lemma\n3.5 gives us also a necessary and sufficient condition for\nequality in the concavity of the capacity functional.\nIV. C ONCLUSION\n\n,\n\nwhere (*, *)HS denotes the Hilbert-Schmidt inner product and\nK is the normalization constant. The conclusion of the first\npart of the lemma is now obvious.\n2. According to our assumption and due to the concavity of\nthe capacity functional we may conclude that\n\nWe have described the structure of optimal input covariance\nmatrices using the symmetries of the channel matrix H at\nour disposal. Those symmetries are encoded in the right commutant C\u03a3 . If C\u03a3 6= C1N the original optimization problem\nreduces to independent optimization problems coupled only\nover the trace constraint of Corollary 3.4.\n\n1\n1\n1\n1\nC = C( Q(1) + Q(2) ) = C(Q(1) ) + C(Q(2) ).\n2\n2\n2\n2\n\nACKNOWLEDGMENT\n\nMoreover, since the functional log det(*) is concave we see\nthat for Q\u0303 = 12 (Q(1) + Q(2) )\n!\n\u0013\n\u0012\n1\nHQ(1) H H\nH Q\u0303H H\n=\nlog\ndet\n1\n+\nlog det 1M +\nM\n\u03c3n2\n2\n\u03c3n2\n\u0013\n\u0012\nHQ(2) H H\n1\n+ log det 1M +\n2\n\u03c3n2\nholds almost surely with respect to the probability distribution\nof the channel matrix H. This last equation, in turn, is\nequivalent to\n!\n\u0012\n\u00131\nH Q\u0303H H\nHQ(1) H H 2\ndet 1M +\n= det 1M +\n\u03c3n2\n\u03c3n2\n\u0012\n\u00131\nHQ(2) H H 2\n\u00d7 det 1M +\n(7)\n\u03c3n2\nalmost surely. Now, recall the Minkowski's determinant inequality and the log-concavity of the determinant (cf. [13])\nwhich can be stated as the following chain of inequalities:\ndet(\u03bbA + (1 \u2212 \u03bb)B)\n\n1\n\n\u2265 (\u03bb det(A) M\n1\n\n+\n\n(1 \u2212 \u03bb) det(B) M )M\n\n\u2265\n\ndet(A)\u03bb det(B)1\u2212\u03bb ,\n\n(8)\n\nfor \u03bb \u2208 (0, 1) and A, B \u2208 M(M, C) positive definite. The\nequality appears in the first inequality iff A = \u03b1B with \u03b1 > 0,\nwhile the equality in the second line is obtained iff det(A) =\ndet(B). Hence the overall equality in (8) can appear iff A =\nB. Translating this to our eqn. (7) we see that\n\u0013\n\u0012\nHQ(1) H H\nHQ(2) H H\n1M +\n,\n=\n\u03b1(H)\n1\n+\nM\n\u03c3n2\n\u03c3n2\na.s. with a measurable function \u03b1 which is almost surely\npositive and\n\u0013\n\u0012\n\u0013\n\u0012\nHQ(2) H H\nHQ(1) H H\n=\ndet\n1\n+\na.s.\ndet 1M +\nM\n\u03c3n2\n\u03c3n2\n\nWe would like to thank Eduard Jorswieck for helpful\ndiscussions on this topic . This research was supported by the\nDFG via projects Bj 57/1-1 \"Entropie und Kodierung grosser\nQuanten-Informationssysteme\" and Bo 1734/2-1 \"Optimale\nSendestrategien f\u00fcr MIMO unter partieller Kanalkenntnis\".\nR EFERENCES\n[1] E. Telatar, \"Capacity of multi-antenna Gaussian channels,\" Bell Labs J.\nvol. 10, no. 6, 1999\n[2] G.J. Foschini and M.J. Gans, \"On limits of wireless communications\nin a fading environment when using multiple antennas,\" Wireless Pers.\nCommun. vol. 6, pp. 311-335, 1998\n[3] C.N. Chuah, D.N.C. Tse, J.M. Kahn, \"Capacity scaling in MIMO wireless\nsystems under correlated fading,\" IEEE Trans. Inform. Theory vol. 48, pp.\n637-650, 2002\n[4] D. Shiu, G.J. Foschini, M.J. Gans, J.M. Kahn, \"Fading correlation and\nits effect on the capacity of multi-element antenna systems,\" IEEE Trans.\nCommun. pp. 502-513, 2000\n[5] S.H. Simon, A.L. Moustakas, \"Optimizing MIMO antenna systems with\nchannel covariance feedback,\" IEEE J. Select. Areas Commun. vol. 21,\npp. 406-417, 2003\n[6] S. Jafar, A. Goldsmith, \"Transmitter optimization and optimality of beamforming for multiple antenna systems,\" IEEE Trans. Wireless Commun.\nvol.3, pp. 1165-1175, 2004\n[7] S.A. Jafar, S. Wishwanath, A. Goldsmith, \"Channel Capacity and Beamforming for Multiple Transmit and Receive Antennas with Covariance\nFeedback,\" Proceedings of ICC 2001 2001\n[8] E.A. Jorswieck, H. Boche, \"Channel Capacity and Capacity-Range of\nBeamforming in MIMO Wireless Systems under Correlated Fading with\nCovariance Feedback,\" IEEE Trans. Wireless Commun. Vol. 3, No. 5,\n2004\n[9] M. Takesaki, \"Theory of Operator Algebras I,\" Encyclopedia of Mathematical Sciences, Springer, Berlin 2000\n[10] L. Gurvits, H. Barnum, \"Largest separable balls around maximally\nmixed bipartite quantum state,\" Phys. Rev. A 66, 062311, 2002\n[11] R.F. Werner, \"Quantum states with Einsten-Podolski-Rosen correlations\nadmitting a hidden-variable model,\" Phys. Rev. A 40, 4277, 1989\n[12] G. Caire, S. Shamai, \"On the Capacity of Some Channels with Channel\nState Information,\" IEEE Trans. Inform. Theory vol. 45, no. 6, pp. 20072019, 1999\n[13] R.A. Horn, C.R. Johnson, \"Matrix Analysis,\" Cambridge University\nPress, Cambridge 1999\n[14] A.M. Tulino, A. Lozano, S. Verd\u00fa, \"Capacity-Achieving Input Covariance for Single-User Multi-Antenna Channels,\" IEEE Trans. Wireless\nCommun. vol. 5.no. 3 662-671, 2006\n\n\f"}