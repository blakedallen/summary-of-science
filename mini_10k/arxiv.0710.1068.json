{"id": "http://arxiv.org/abs/0710.1068v1", "guidislink": true, "updated": "2007-10-04T18:36:25Z", "updated_parsed": [2007, 10, 4, 18, 36, 25, 3, 277, 0], "published": "2007-10-04T18:36:25Z", "published_parsed": [2007, 10, 4, 18, 36, 25, 3, 277, 0], "title": "Information and Entropy", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0710.0916%2C0710.5043%2C0710.2013%2C0710.5083%2C0710.4671%2C0710.4817%2C0710.2522%2C0710.2672%2C0710.0994%2C0710.5459%2C0710.2196%2C0710.1050%2C0710.0057%2C0710.5178%2C0710.4775%2C0710.4963%2C0710.5167%2C0710.4013%2C0710.0006%2C0710.1068%2C0710.0658%2C0710.1345%2C0710.4682%2C0710.2977%2C0710.2305%2C0710.4368%2C0710.5071%2C0710.5045%2C0710.2853%2C0710.4182%2C0710.2089%2C0710.1841%2C0710.3358%2C0710.3747%2C0710.5283%2C0710.2273%2C0710.3878%2C0710.5295%2C0710.5775%2C0710.0669%2C0710.0812%2C0710.2555%2C0710.3801%2C0710.4618%2C0710.0325%2C0710.0870%2C0710.1935%2C0710.4624%2C0710.0595%2C0710.4657%2C0710.5600%2C0710.4957%2C0710.1361%2C0710.4734%2C0710.4593%2C0710.0558%2C0710.3531%2C0710.5292%2C0710.2921%2C0710.5871%2C0710.5353%2C0710.1515%2C0710.2774%2C0710.3161%2C0710.1013%2C0710.3963%2C0710.0681%2C0710.1411%2C0710.3968%2C0710.4563%2C0710.1158%2C0710.3009%2C0710.3838%2C0710.1372%2C0710.5371%2C0710.0520%2C0710.4231%2C0710.1080%2C0710.0261%2C0710.4552%2C0710.3548%2C0710.3789%2C0710.5714%2C0710.0959%2C0710.2137%2C0710.0765%2C0710.3260%2C0710.2068%2C0710.5422%2C0710.0816%2C0710.0154%2C0710.0783%2C0710.1298%2C0710.3029%2C0710.0125%2C0710.2355%2C0710.2214%2C0710.3863%2C0710.1317%2C0710.4705%2C0710.3815&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Information and Entropy"}, "summary": "What is information? Is it physical? We argue that in a Bayesian theory the\nnotion of information must be defined in terms of its effects on the beliefs of\nrational agents. Information is whatever constrains rational beliefs and\ntherefore it is the force that induces us to change our minds. This problem of\nupdating from a prior to a posterior probability distribution is tackled\nthrough an eliminative induction process that singles out the logarithmic\nrelative entropy as the unique tool for inference. The resulting method of\nMaximum relative Entropy (ME), which is designed for updating from arbitrary\npriors given information in the form of arbitrary constraints, includes as\nspecial cases both MaxEnt (which allows arbitrary constraints) and Bayes' rule\n(which allows arbitrary priors). Thus, ME unifies the two themes of these\nworkshops -- the Maximum Entropy and the Bayesian methods -- into a single\ngeneral inference scheme that allows us to handle problems that lie beyond the\nreach of either of the two methods separately. I conclude with a couple of\nsimple illustrative examples.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0710.0916%2C0710.5043%2C0710.2013%2C0710.5083%2C0710.4671%2C0710.4817%2C0710.2522%2C0710.2672%2C0710.0994%2C0710.5459%2C0710.2196%2C0710.1050%2C0710.0057%2C0710.5178%2C0710.4775%2C0710.4963%2C0710.5167%2C0710.4013%2C0710.0006%2C0710.1068%2C0710.0658%2C0710.1345%2C0710.4682%2C0710.2977%2C0710.2305%2C0710.4368%2C0710.5071%2C0710.5045%2C0710.2853%2C0710.4182%2C0710.2089%2C0710.1841%2C0710.3358%2C0710.3747%2C0710.5283%2C0710.2273%2C0710.3878%2C0710.5295%2C0710.5775%2C0710.0669%2C0710.0812%2C0710.2555%2C0710.3801%2C0710.4618%2C0710.0325%2C0710.0870%2C0710.1935%2C0710.4624%2C0710.0595%2C0710.4657%2C0710.5600%2C0710.4957%2C0710.1361%2C0710.4734%2C0710.4593%2C0710.0558%2C0710.3531%2C0710.5292%2C0710.2921%2C0710.5871%2C0710.5353%2C0710.1515%2C0710.2774%2C0710.3161%2C0710.1013%2C0710.3963%2C0710.0681%2C0710.1411%2C0710.3968%2C0710.4563%2C0710.1158%2C0710.3009%2C0710.3838%2C0710.1372%2C0710.5371%2C0710.0520%2C0710.4231%2C0710.1080%2C0710.0261%2C0710.4552%2C0710.3548%2C0710.3789%2C0710.5714%2C0710.0959%2C0710.2137%2C0710.0765%2C0710.3260%2C0710.2068%2C0710.5422%2C0710.0816%2C0710.0154%2C0710.0783%2C0710.1298%2C0710.3029%2C0710.0125%2C0710.2355%2C0710.2214%2C0710.3863%2C0710.1317%2C0710.4705%2C0710.3815&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "What is information? Is it physical? We argue that in a Bayesian theory the\nnotion of information must be defined in terms of its effects on the beliefs of\nrational agents. Information is whatever constrains rational beliefs and\ntherefore it is the force that induces us to change our minds. This problem of\nupdating from a prior to a posterior probability distribution is tackled\nthrough an eliminative induction process that singles out the logarithmic\nrelative entropy as the unique tool for inference. The resulting method of\nMaximum relative Entropy (ME), which is designed for updating from arbitrary\npriors given information in the form of arbitrary constraints, includes as\nspecial cases both MaxEnt (which allows arbitrary constraints) and Bayes' rule\n(which allows arbitrary priors). Thus, ME unifies the two themes of these\nworkshops -- the Maximum Entropy and the Bayesian methods -- into a single\ngeneral inference scheme that allows us to handle problems that lie beyond the\nreach of either of the two methods separately. I conclude with a couple of\nsimple illustrative examples."}, "authors": ["Ariel Caticha"], "author_detail": {"name": "Ariel Caticha"}, "author": "Ariel Caticha", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1063/1.2821253", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0710.1068v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0710.1068v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Presented at MaxEnt 2007, the 27th International Workshop on Bayesian\n  Inference and Maximum Entropy Methods (July 8-13, 2007, Saratoga Springs, New\n  York, USA)", "arxiv_primary_category": {"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "gr-qc", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.gen-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0710.1068v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0710.1068v1", "journal_reference": null, "doi": "10.1063/1.2821253", "fulltext": "arXiv:0710.1068v1 [physics.data-an] 4 Oct 2007\n\nInformation and Entropy\u2217\nAriel Caticha\nDepartment of Physics, University at Albany-SUNY,\nAlbany, NY 12222, USA.\n\nAbstract\nWhat is information? Is it physical? We argue that in a Bayesian\ntheory the notion of information must be defined in terms of its effects on\nthe beliefs of rational agents. Information is whatever constrains rational\nbeliefs and therefore it is the force that induces us to change our minds.\nThis problem of updating from a prior to a posterior probability distribution is tackled through an eliminative induction process that singles out\nthe logarithmic relative entropy as the unique tool for inference. The resulting method of Maximum relative Entropy (ME), which is designed for\nupdating from arbitrary priors given information in the form of arbitrary\nconstraints, includes as special cases both MaxEnt (which allows arbitrary constraints) and Bayes' rule (which allows arbitrary priors). Thus,\nME unifies the two themes of these workshops \u2013 the Maximum Entropy\nand the Bayesian methods \u2013 into a single general inference scheme that\nallows us to handle problems that lie beyond the reach of either of the\ntwo methods separately. I conclude with a couple of simple illustrative\nexamples.\n\n1\n\nIntroduction\n\nThe general problem of inductive inference is to update from a prior probability\ndistribution to a posterior distribution when new information becomes available.\nThis raises several basic questions which are the subject of this paper. First,\nwhat is information? It is clear that data \"contains\" or \"conveys\" information,\nbut what does this precisely mean? Is information some sort of physical fluid\nthat can be contained or transported? Is information physical ? Can we measure\namounts of information? Do we need to? What is entropy?\nA second set of questions revolves around our methods to process information. We know that Bayes' rule is the natural way to update probabilities when\nthe new information is in the form of data and we know that Jaynes' method\nof maximum entropy, MaxEnt, is designed to handle information in the form\nof constraints [1]. At first sight these two methods appear unrelated. Are they\n\u2217 Presented at MaxEnt 2007, the 27th International Workshop on Bayesian Inference and\nMaximum Entropy Methods (July 8-13, 2007, Saratoga Springs, New York, USA).\n\n1\n\n\fcompatible with each other? Are there other methods? Moreover, the range\nof applicability of either method is somewhat limited: Bayes' rule can handle\narbitrary priors and data, and it can even handle some constraints, but not arbitrary constraints. On the other hand, MaxEnt can handle arbitrary constraints\neven data, but not arbitrary priors. Can we extend these methods?\nAs discussed in [2] the Shannon-Jaynes interpretation of entropy as a measure of uncertainty or of amount of information is somewhat problematic. The\nissue is not purely academic because the way equations are set up to solve a\nproblem and even the kind of problems that we are willing to consider are affected by the particular meaning attributed to quantities such as entropy or\nprobability. The Shannon-Jaynes interpretation was fairly adequate for their\npurposes, namely, communication theory and statistical mechanics, but it is\nnot at all clear that their entropy with its attendant interpretation was the\nappropriate tool for the very different problem of updating probabilities.\nThe important contribution of Shore and Johnson [3] was the realization that\nany confusion surrounding the meaning of entropy could be, if not resolved, at\nleast evaded by directly axiomatizing the procedure for updating probabilities\ninstead of seeking dubious measures for a vaguely defined notion of information.\nTheir argument, which is based on demanding consistency \u2013 if a problem can\nbe solved in two different ways the two solutions must agree \u2013 is fundamentally sound. However, the detailed assumptions in their derivation have been\ncriticized in [4, 5].\nAnother approach to entropy was proposed by Skilling [6]. Although his axioms were clearly inspired by Shore and Johnson, the method was very different\nin two respects. First, Skilling was not directly concerned with the problem\nof updating probabilities; his method was designed for the determination of\npositive-additive functions such as intensities in an image. In retrospect we see\nthat the application to this particular problem was quite unfortunate because\nwhen the method failed to produce good image reconstructions the natural reaction was a widespread loss of confidence about entropy methods in general.\nThe second difference, which I think is a truly significant contribution, is\nthat Skilling's approach is a systematic method for induction. He spelled out\nin full detail how to construct a general theory from known special cases. The\nfundamental inductive principle is deceptively trivial: 'If a general theory exists\nit must apply to special cases'. The basic idea is that when there exists a special\ncase that happens to be known all candidate theories that fail to reproduce it\nmust be discarded. Thus, the known special cases \u2013 called the axioms of the\ntheory \u2013 constrain the form of the general theory, and the idea is that a sufficient\nnumber of such constraints will determine the general theory completely. Of\ncourse, there is always the unfortunate possibility that the desired general theory\ndoes not exist, but if it does, then the search can be conducted in a systematic\nand orderly way.\nPhilosophers already had a name for such a method: they called it eliminative induction [7]. On the negative side, eliminative induction, like any other\nform of induction, is not guaranteed to work. It failed, for example, in Skilling's\nimage reconstruction problem. On the positive side, eliminative induction adds\n2\n\n\fan interesting twist to Popper's scientific methodology. According to Popper\nscientific theories can never be proved right, they can only be proved false; a\ntheory is corroborated only to the extent that all attempts at falsifying it have\nfailed. Eliminative induction is fully compatible with Popper's notions but the\npoint of view is just the opposite. Instead of focusing on failure to falsify one\nfocuses on success: it is the successful falsification of all rival theories that corroborates the surviving one. The advantage is that one acquires a more explicit\nunderstanding of why competing theories are eliminated.\nThe present paper is the third in a sequence devoted to clarifying the use of\nrelative entropy as a tool for processing information and updating probabilities\n[2, 8]. In [2] we applied Skilling's method to the problem of Shore and Johnson.\nThe answer to the question 'What is entropy?' turns out to be trivial and\nsomewhat surprising: entropy needs no interpretation. We do not need to know\nwhat 'entropy' means, we only need to know how to use it. This explains why\nthe \"correct\" interpretation had been so elusive \u2013 there is none. In [2] and then\nagain in [8] the special cases, the axioms, were increasingly polished to clarify\nhow alternative entropies are ruled out. Furthermore, in [2] we also discussed\nthe question, central to any general method of updating, of the extent to which\nthe distribution of maximum entropy is to be preferred over all others, the extent\nto which distributions with entropies less than the maximum are to be ruled\nout.\nIn this paper we review how eliminative induction leads to a unique candidate for a general theory of inference, the method of Maximum relative Entropy\n(ME), which is designed for updating from arbitrary priors given information in\nthe form of arbitrary constraints. The three axioms used in [8] \u2013 locality, coordinate invariance, and consistency for independent subsystems \u2013 are sufficient\nto single out the logarithmic relative entropy as the unique tool for updating.\nIn particular, we wish to elaborate further on the use of the third axiom \u2013 consistency for independent subsystems \u2013 to eliminate alternative entropies [12].\nThe idea is rather simple. The known special cases covered under axiom\n3 also include situations in which we have a large number N of independent\nidentical systems where all sorts of inferences can be reliably carried out using\nvarious asymptotic techniques (laws of large numbers, large deviation theory,\netc.). The close connection with the method of maximum entropy has been\nrepeatedly emphasized by several authors [9]-[11]. We conclude that the logarithmic relative entropy is the only candidate for a general method for updating\nprobabilities. Alternative entropies can be useful for other purposes \u2013 for example, when studying the information geometry of statistical manifolds \u2013 but\nnot for a general theory of updating.\nIn [8] we showed that the ME method includes both MaxEnt and Bayes'\nrule as special cases and therefore it unifies the two dominant themes of these\nworkshops \u2013 the Maximum Entropy and Bayesian methods \u2013 into a single general\ninference scheme that allows us to handle problems that lie beyond the reach\nof either of the two methods separately. I conclude with a couple of simple\nillustrative examples.\nIn a companion paper [13] we discuss the problem of multiple constraints.\n3\n\n\fShould the constraints be processed simultaneously or sequentially and, if so,\nin what order? There we also give an explicit example in which ME is used to\nsimultaneously process information in the form of data and moment constraints.\n\n2\n\nWhat is information?\n\nIt is not unusual these days to hear that systems \"carry\" or \"contain\" information and that \"information is physical\". This mode of expression can perhaps\nbe traced to the origins of information theory in Shannon's theory of communication. We say that we have received information when among the vast variety\nof messages that could conceivably have been generated by a distant source, we\ndiscover which particular message was actually sent. It is thus that the message\n\"carries\" information. The analogy with physics is straightforward: the set of\nall possible states of a physical system can be likened to the set of all possible\nmessages, and the actual state of the system corresponds to the message that\nwas actually sent. Thus, the system \"conveys\" a message: the system \"carries\"\ninformation about its own state. Sometimes the message might be difficult to\nread, but it is there nonetheless.\nThis language \u2013 information is physical \u2013 useful as it has turned out to be,\ndoes not exhaust the meaning of the word 'information'. The goal of information theory, or better, communication theory, is to characterize the sources of\ninformation, to measure the capacity of the communication channels, and to\nlearn how to control the degrading effects of noise. It is somewhat ironic but\nnevertheless true that this \"information\" theory is unconcerned with the central Bayesian issue of how the message affects the beliefs of a rational agent. A\nfully Bayesian information theory demands an explicit account of the relation\nbetween information and beliefs.\nOur desire to update from one state of belief to another is driven by the\nconviction that not all probability assignments are equally good. One can argue\nthat what makes one probability assignment better than another is that it better\nreflects some objective feature of the world, that it provides a better guide to\nthe \"truth\" \u2013 whatever this might mean. The updating mechanism is supposed\nto allow us to incorporate information about the world into our beliefs.\nThe implication is that when confronted with new information our choices\nas to what we are honestly and rationally allowed to believe should become\ncorrespondingly restricted. This, I propose, is the defining characteristic of\ninformation: Information is whatever constrains rational beliefs. An important\naspect of this notion is that for a rational agent the updating is not optional;\nit is a moral imperative. Information is whatever forces a change of rational\nbeliefs.\nOur definition captures an idea of information that is directly related to\nchanging our minds: information is the driving force behind the process of\nlearning. Note also that although there is no need to talk about amounts of\ninformation, whether measured in units of bits or otherwise, our notion of information allows precise quantitative calculations. Indeed, by information in its\n\n4\n\n\fmost general form, we mean the set of constraints on the family of acceptable\nposterior distributions and this is precisely the kind of information the method\nof maximum entropy has been designed to handle.\nIt may be worthwhile to point out an analogy with Newtonian dynamics.\nThe state of motion of a system is described in terms of momentum \u2013 the\n\"quantity\" of motion \u2013 while the change from one state to another is explained\nin terms of an applied force. Similarly, in Bayesian inference a state of belief is\ndescribed in terms of probabilities \u2013 the \"quantity\" of belief \u2013 and the change\nfrom one state to another is due to information. Just as a force is defined as\nthat which induces a change in motion, so information is that which induces a\nchange of beliefs.\n\n3\n\nUpdating probabilities: the ME method\n\nConsider a variable x which can be discrete or continuous, in one or several\ndimensions. The uncertainty about x is described by a probability distribution\nq(x). Our goal is to update from the prior distribution q(x) to a posterior distribution P (x) when new information \u2013 that is, constraints \u2013 becomes available.\nThe constraints could be given in terms of expected values but this is not necessary. The question is: of all those distributions p(x) within the family defined\nby the constraints, which do we select?\nAs suggested by Skilling [6] to select the posterior it seems reasonable to\nrank the candidate distributions in order of increasing preference. It is clear\nthat to accomplish this goal the ranking must be transitive: if distribution p1 is\npreferred over distribution p2 , and p2 is preferred over p3 , then p1 is preferred\nover p3 . Such transitive rankings are represented by assigning to each p(x) a\nreal number S[p], which we will henceforth call entropy, in such a way that if\np1 is preferred over p2 , then S[p1 ] > S[p2 ]. The selected distribution P (one or\npossibly many, for on the basis of the available information there may be several\nequally preferred distributions) will be that which maximizes the entropy S[p].\nWe are thus led to a method of Maximum Entropy (ME) that is a variational\nmethod involving entropies which are real numbers. These features are imposed\non purpose; they are dictated by the function that the ME method is designed\nto perform.\nNext, to define the ranking scheme, we must decide on the functional form\nof S[p]. First, the purpose of the method is to update from priors to posteriors. The ranking scheme must depend on the particular prior q and therefore\nthe entropy S must be a functional of both p and q. Thus the entropy S[p, q]\nproduces a ranking of the distributions p relative to the given prior q: S[p, q]\nis the entropy of p relative to q. Accordingly S[p, q] is commonly called relative entropy. Since all entropies are relative, even when relative to a uniform\ndistribution, the modifier 'relative' is redundant and will be dropped.\nSecond, since we deal with incomplete information the method, by its very\nnature, cannot be deductive: the method must be inductive. The best we can do\nis use those special cases where we know what the preferred distribution should\n\n5\n\n\fbe to eliminate those entropy functionals S[p, q] that fail to provide the right\nupdate. The known special cases will be called (perhaps inappropriately) the\naxioms of the theory. They play a crucial role: they define what makes one\ndistribution preferable over another.\nThe three axioms below are chosen to reflect the conviction that information\ncollected in the past and codified into the prior distribution is very valuable and\nshould not be frivolously discarded. This attitude is maximally conservative:\nthe only aspects of one's beliefs that should be updated are those for which\nnew evidence has been supplied. Furthermore, since the axioms do not tell us\nwhat and how to update, they merely tell us what not to update, they have\nthe added bonus of maximizing objectivity \u2013 there are many ways to change\nsomething but only one way to keep it the same. Thus, we adopt the\nPrinciple of Minimal Updating (PMU): Beliefs should be updated only\nto the extent required by the new information.\nThe three axioms, a brief motivation for them, and their consequences for the\nfunctional form of the entropy are listed below; more details and proofs are\ngiven in [2] and [8]. As will become immediately apparent the axioms do not\nrefer to merely three cases; any induction from such a weak foundation would\nhardly be reliable. The reason the axioms are convincing and so constraining is\nthat they refer to three infinitely large classes of known special cases.\nAxiom 1: Locality. Local information has local effects.\nSuppose the information to be processed does not refer to a particular subdomain D of the space X of x's. In the absence of any new information about\nD the PMU demands we do not change our minds about D. Thus, we design\nthe inference method so that q(x|D), the prior probability of x conditional on\nx \u2208 D, is not updated. The selected conditional posterior is P (x|D) = q(x|D).\nThe consequence of axiom 1 is that non-overlapping domains of x contribute\nadditively to the entropy. Dropping additive terms and multiplicative factors\nthat do not affect the overall ranking, the entropy functional can be simplified\nto the form\nZ\nS[p, q] = dx F (p(x), q(x), x) ,\n(1)\nwhere F is some unknown function.\nAxiom 2: Coordinate invariance. The system of coordinates carries no\ninformation.\nThe points x can be labeled using any of a variety of coordinate systems. One\ncan always change coordinates but this should not affect the ranking of the\ndistributions. The consequence of axiom 2 is that S[p, q] can be written in\nterms of coordinate invariants such as dx m(x) and p(x)/m(x), and q(x)/m(x):\n\u0012\n\u0013\nZ\np(x) q(x)\nS[p, q] = dx m(x)\u03a6\n.\n(2)\n,\nm(x) m(x)\n(Again, additive terms and multiplicative factors that do not affect the overall\nranking have been dropped.) Thus the unknown function F which had three\narguments has been replaced by two unknown functions, one is a density m(x),\n6\n\n\fand the other is a function \u03a6 with two arguments. Next we determine the\ndensity m(x) by invoking the locality axiom 1 once again.\nAxiom 1 (special case): When there is no new information there is no\nreason to change one's mind.\nWhen no new information is available the domain D in axiom 1 coincides with\nthe whole space X . The conditional probabilities q(x|D) = q(x|X ) = q(x)\nshould not be updated and the selected posterior distribution coincides with\nthe prior, P (x) = q(x). The consequence is that up to normalization m(x) must\nbe the prior distribution q(x), which restricts the entropy to functionals of the\nform\n\u0012\n\u0013\nZ\np(x)\nS[p, q] = dx q(x)\u03a6\n.\n(3)\nq(x)\nAxiom 3: Consistency for independent subsystems. When a system\nis composed of subsystems that are known to be independent it should not matter\nwhether the inference procedure treats them separately or jointly.\nSuppose the information on two independent subsystems 1 and 2 is such that\nthe prior distributions q1 (x1 ) and q2 (x2 ) are respectively updated to P1 (x1 ) and\nP2 (x2 ) when they are treated separately. When treated as a single system the\njoint prior is q1 (x1 )q2 (x2 ) and the family of potential posteriors is p(x1 , x2 ) =\np1 (x1 )p2 (x2 ). The entropy functional must be such that the selected posterior\nis P1 (x1 )P2 (x2 ). The consequence of axiom 3 for this particular case of just two\nsubsystems is that entropies are restricted to the one-parameter family given by\n\u0014\n\u0012\n\u0013\u03b7 \u0015\nZ\np(x)\n1\n1 \u2212 dx p(x)\n.\n(4)\nS\u03b7 [p, q] =\n\u03b7(\u03b7 + 1)\nq(x)\nOnce again, additive terms and multiplicative factors that do not affect the\noverall ranking scheme can be freely chosen. The \u03b7 = 0 case reproduces the\nusual logarithmic relative entropy,\nZ\np(x)\nS[p, q] = \u2212 dx p(x) log\n(5)\nq(x)\n[Use y \u03b7 = exp \u03b7 log y \u2248 1 + \u03b7 log y in eq.(4) and let \u03b7 \u2192 0 to get eq.(5).]\nIn [8] we argued that the index \u03b7 has to be the same for all systems. To see\nwhy consider any two independent systems characterized by \u03b7 1 and \u03b7 2 . Consistency between the joint and separate updates requires that \u03b7 1 = \u03b7 2 therefore\n\u03b7 must be a universal constant. From the success of statistical mechanics as a\ntheory of inference we inferred that the value of this constant must be \u03b7 = 0\nleading to the logarithmic entropy, eq.(5). Here we offer a different argument\nalso based on a broader application of axiom 3:\nAxiom 3 (special case): Consistency for large numbers of independent identical subsystems.\nThe known special cases covered under axiom 3 include situations in which we\nhave a large number N of independent identical systems. In such cases either\nthe weak law of large numbers or large deviation theory in the form of Sanov's\n\n7\n\n\ftheorem are sufficient to make the desired inferences. Entropy considerations\nare not needed.\nLet the x variables be discrete xi with i = 1 . . . m. The identical priors for\nthe individual systems are qi and the available information is that the potential\nposteriors pi are subject, for example, to an expectation\nP value constraint such\nas hai = A, where A is some specified value and hai = ai pi .\nConsider the set of N systems treated jointly. Let the number of systems\nfound in state i be ni , and let fi = ni /N be the corresponding frequency. In\nthe limit of large N the frequencies fi converge\nP (in probability) to the desired\nposterior Pi while the sample average \u0101 =\nai fi converges (also in probability) to the expected value hai = A. The probability of a particular frequency\ndistribution f = {f1 . . . fn } generated by the prior q is multinomial,\nQN (f |q) =\n\nN!\nnm\nq n1 . . . qm\nn1 ! . . . nm ! 1\n\nwith\n\nm\nP\n\nni = N ,\n\n(6)\n\ni=1\n\nand for large N we have\nQN (f |q) \u2248 exp N (S[f, q] + rN ) ,\n\n(7)\n\nwhere S[f, q] given by eq.(5), and where rN is a correction that vanishes as\nN \u2192 \u221e. To find the most probable frequency distribution satisfying the constraint \u0101 = A one maximizes QN (f |q) subject to \u0101 = A, which is equivalent to\nmaximizing the entropy S[f, q] subject to \u0101 = A. The corresponding problem\nfor the individual systems is that of maximizing S\u03b7 [p, q] subject to hai = A. The\ntwo procedures agree only when we choose \u03b7 = 0. Therefore, entropies S\u03b7 with\n\u03b7 6= 0 are not consistent with the laws of large numbers and must be discarded.\nCsiszar [10] and Grendar [11] have argued that the asymptotic argument\nabove provides a valid justification for the ME method of updating. An agent\nwhose prior is q receives the information hai = A which can be reasonably\ninterpreted as a sample average \u0101 = A over a large ensemble of N trials. The\nagent's beliefs are updated so that the posterior P coincides with the most\nprobable f distribution. This is quite compelling but, of course, as a justification\nof the ME method it is restricted to situations where it is natural to think in\nterms of ensembles with large N . This justification is not nearly as compelling\nfor singular events for which large ensembles either do not exist or are too\nunnatural and contrived. From our point of view the asymptotic argument\nabove does not by itself provide a fully convincing justification for the universal\nvalidity of the ME method but it does provide considerable inductive support.\nIt serves as a valuable consistency check that must be passed by any inductive\ninference procedure that claims to be of general applicability.\nThe results are summarized as follows:\nThe ME method: The objective is to update from a prior distribution q to\na posterior distribution given the information that the posterior lies within a\ncertain family of distributions p. The selected posterior P (x) is that which\nmaximizes the entropy S[p, q]. Since prior information is valuable the functional\nS[p, q] has been chosen so that beliefs are updated only to the extent required by\nthe new information. No interpretation for S[p, q] is given and none is needed.\n8\n\n\f4\n\nBayes' rule and its generalizations\n\nThe problem is to update our beliefs about \u03b8 \u2208 \u0398 (\u03b8 represents one or many parameters) on the basis of three pieces of information: (1) the prior information\ncodified into a prior distribution q(\u03b8); (2) the data x \u2208 X (obtained in one or\nmany experiments); and (3) the known relation between \u03b8 and x given by the\nmodel as defined by the sampling distribution or likelihood, q(x|\u03b8). The updating consists of replacing the prior probability distribution q(\u03b8) by a posterior\ndistribution P (\u03b8) that applies after the data has been processed.\nThe crucial element that will allow Bayes' rule to be smoothly incorporated\ninto the ME scheme is the realization that before the data information is available not only we do not know \u03b8, we do not know x either. Thus, the relevant\nspace for inference is not \u0398 but the product space \u0398 \u00d7 X and the relevant joint\nprior is q(x, \u03b8) = q(\u03b8)q(x|\u03b8). We should emphasize that the information about\nhow x is related to \u03b8 is contained in the functional form of the distribution\nq(x|\u03b8) \u2013 for example, whether it is a Gaussian or a Cauchy distribution \u2013 and\nnot in the actual values of the arguments x and \u03b8 which are, at this point, still\nunknown.\nNext we collect data and the observed values turn out to be x\u2032 . We must\nupdate to a posterior that lies within the family of distributions p(x, \u03b8) that\nreflect the fact that x is known,\nR\np(x) = d\u03b8 p(\u03b8, x) = \u03b4(x \u2212 x\u2032 ) .\n(8)\nThis data information constrains but is not sufficient to determine the joint\ndistribution\np(x, \u03b8) = p(x)p(\u03b8|x) = \u03b4(x \u2212 x\u2032 )p(\u03b8|x\u2032 ) .\n(9)\n\nAny choice of p(\u03b8|x\u2032 ) is in principle possible. Additional input is needed and\nit is at this point that we invoke the Principle of Minimal Updating: beliefs\nneed to be revised only to the extent required by the data. Accordingly the\nconditional prior q(\u03b8|x\u2032 ) requires no revision and the selected posterior P (x, \u03b8)\nis such that P (\u03b8|x\u2032 ) = q(\u03b8|x\u2032 ), or\nP (x, \u03b8) = \u03b4(x \u2212 x\u2032 )q(\u03b8|x\u2032 ) .\n\n(10)\n\nThe corresponding marginal posterior probability P (\u03b8) is\nP (\u03b8) =\n\nR\n\ndx P (\u03b8, x) = q(\u03b8|x\u2032 ) = q(\u03b8)\n\nq(x\u2032 |\u03b8)\n,\nq(x\u2032 )\n\n(11)\n\nwhich is recognized as Bayes' rule. This is extremely reasonable: we maintain\nthose beliefs about \u03b8 that are consistent with the data values x\u2032 that turned\nout to be true. Data values that were not observed are discarded because they\nare now known to be false. 'Maintain' is the key word: it reflects the PMU in\naction.\nRemark: Bayes' rule is usually written in the form\nq(\u03b8|x\u2032 ) = q(\u03b8)\n9\n\nq(x\u2032 |\u03b8)\n,\nq(x\u2032 )\n\n(12)\n\n\fand called Bayes' theorem. This formula is very simple; perhaps it is too simple.\nIt is just a restatement of the product rule \u2013 valid for any x\u2032 whether observed\nor not \u2013 and therefore it is a simple consequence of the internal consistency of\nthe prior beliefs. The drawback of this formula is that the left hand side is not a\nposterior but rather a prior (conditional) probability; it obscures the fact that\nan additional principle \u2013 the PMU \u2013 was needed for updating.\nNext we show that Bayes' rule is consistent with, and indeed, is a special\ncase of the ME method [8]. This is not too surprising given that the ME is also\nbased on the PMU. According to the ME method the selected joint posterior\nP (x, \u03b8) is that which maximizes the entropy,\nR\np(x, \u03b8)\nS[p, q] = \u2212 dxd\u03b8 p(x, \u03b8) log\n,\nq(x, \u03b8)\n\n(13)\n\nsubject to the appropriate constraints. Note that the information in the data,\neq.(8), represents an infinite number of constraints on the family p(x, \u03b8): for each\nvalue of x there is one constraint and one Lagrange multiplier \u03bb(x). Maximizing\nS, (13), subject to (8) and normalization,\n\b\n\u0002R\n\u0003 R\n\u0002R\n\u0003\n\u03b4 S + \u03b1 dxd\u03b8 p(x, \u03b8) \u2212 1 + dx \u03bb(x) d\u03b8 p(x, \u03b8) \u2212 \u03b4(x \u2212 x\u2032 ) = 0 ,\n(14)\nyields the joint posterior,\nP (x, \u03b8) = q(x, \u03b8)\n\ne\u03bb(x)\n,\nZ\n\n(15)\n\nwhere Z is a normalization constant, and \u03bb(x) is determined from (8),\nR\n\nd\u03b8 q(x, \u03b8)\n\ne\u03bb(x)\ne\u03bb(x)\n= q(x)\n= \u03b4(x \u2212 x\u2032 ) ,\nZ\nZ\n\n(16)\n\nso that the joint posterior is\nP (x, \u03b8) = q(x, \u03b8)\n\n\u03b4(x \u2212 x\u2032 )\n= \u03b4(x \u2212 x\u2032 )q(\u03b8|x) ,\nq(x)\n\n(17)\n\nfrom which we recover Bayes' rule, eq.(11).\nI conclude with a couple of very simple examples that show how the ME allows generalizations of Bayes' rule. The background for these generalized Bayes\nproblems is the familiar one: We want to make inferences about some variables\n\u03b8 on the basis of information about other variables x. As before, the prior information consists of our prior knowledge about \u03b8 given by the distribution q(\u03b8)\nand the relation between x and \u03b8 is given by the likelihood q(x|\u03b8); thus, the\nprior joint distribution q(x, \u03b8) is known. But now the information about x is\nmuch more limited.\nExample 1.\u2013 The data is uncertain: x is not known. The marginal posterior\np(x) is no longer a sharp delta function but some other known distribution,\np(x) = PD (x). This is still an infinite number of constraints\nR\np(x) = d\u03b8 p(\u03b8, x) = PD (x) ,\n(18)\n10\n\n\fthat are easily handled by ME. Maximizing S, (13), subject to (18) and normalization, leads to\nP (x, \u03b8) = PD (x)q(\u03b8|x) .\n(19)\nThe corresponding marginal posterior,\nP (\u03b8) =\n\nR\n\ndx PD (x)q(\u03b8|x) = q(\u03b8)\n\nR\n\ndx PD (x)\n\nq(x|\u03b8)\n,\nq(x)\n\n(20)\n\nis known as Jeffrey's rule.\nExample 2.\u2013 Now we have even less information: p(x) is not known. All we\nknow about p(x) is an expected value\nR\nhf i = dx p(x)f (x) = F .\n(21)\n\nMaximizing S, (13), subject to (21) and normalization,\n\u0003\n\b\n\u0002R\nR\n\u03b4 S + \u03b1 dxd\u03b8 p(x, \u03b8) \u2212 1 + \u03bb dxd\u03b8 p(x, \u03b8)f (x) \u2212 F = 0 ,\n\n(22)\n\nyields the joint posterior,\n\nP (x, \u03b8) = q(x, \u03b8)\n\ne\u03bbf (x)\n,\nZ\n\n(23)\n\nwhere the normalization constant Z and the multiplier \u03bb are obtained from\nZ=\n\nR\n\ndx q(x)e\u03bbf (x)\n\nand\n\nd log Z\n=F .\nd\u03bb\n\n(24)\n\nThe corresponding marginal posterior is\nR\ne\u03bbf (x)\nq(x|\u03b8) .\nP (\u03b8) = q(\u03b8) dx\nZ\n\n(25)\n\nThe two posteriors (20) and (25) are sufficiently intuitive that one could have\nwritten them down directly without deploying the full machinery of the ME\nmethod, but they do serve to illustrate the essential compatibility of Bayesian\nand Maximum Entropy methods. A less trivial example is given in [13].\n\n5\n\nConclusions\n\nAny Bayesian account of the notion of information cannot ignore the fact that\nBayesians are concerned with the beliefs of rational agents. The relation between information and beliefs must be clearly spelled out. The definition we\nhave proposed \u2013 that information is that which constrains rational beliefs and\ntherefore forces the agent to change its mind \u2013 is convenient for two reasons.\nFirst, the information/belief relation very explicit, and second, the definition is\nideally suited for quantitative manipulation using the ME method.\nThe other main conclusion is that the logarithmic relative entropy is the only\ncandidate for a general method for updating probabilities \u2013 the ME method \u2013\n11\n\n\fwhich includes MaxEnt and Bayes' rule as special cases; it unifies them into a\nsingle theory of inductive inference.\nIt is true that there exist many different ways to define measures of separation, or divergence between distributions and that these \"entropies\" can be\nuseful in a wide variety of ways. In fact, it was precisely this wealth of possibilities that Shore and Johnson intended to avoid. These other \"entropies\" can\nbe useful for other purposes but not for updating; at least not for an updating\ntheory that strives to achieve universal applicability. Let us emphasize that the\nreason the ME method uses the logarithmic entropy as the tool for updating is\nnot that this entropy has been shown to provide the correct measure of distance\n\u2013 there are many other such measures. We do not even claim that inferences\non the basis of the ME method are guaranteed to be correct \u2013 this is induction;\nthere are no guarantees. It is just that all alternative entropies are much worse\nbecause in known cases they give answers that are demonstrably wrong.\nAcknowledgements: I would like to acknowledge valuable discussions with C.\nCafaro, N. Caticha, A. Giffin, K. Knuth, and C. Rodr\u0131\u0301guez.\n\nReferences\n[1] E. T. Jaynes, Phys. Rev. 106, 620 and 108, 171 (1957); R. D. Rosenkrantz\n(ed.), E. T. Jaynes: Papers on Probability, Statistics and Statistical Physics\n(Reidel, Dordrecht, 1983); E. T. Jaynes, Probability Theory: The Logic of\nScience (Cambridge University Press, Cambridge, 2003).\n[2] A. Caticha, \"Relative Entropy and Inductive Inference,\" in Bayesian\nInference and Maximum Entropy Methods in Science and Engineering,\ned. by G. Erickson and Y. Zhai, AIP Conf. Proc. 707, 75 (2004)\n(arXiv.org/abs/physics/0311093).\n[3] J. E. Shore and R. W. Johnson, IEEE Trans. Inf. Theory IT-26, 26 (1980);\nIEEE Trans. Inf. Theory IT-27, 26 (1981).\n[4] S. N. Karbelkar, Pramana \u2013 J. Phys. 26, 301 (1986).\n[5] J. Uffink, Stud. Hist. Phil. Mod. Phys. 26B, 223 (1995).\n[6] J. Skilling, \"The Axioms of Maximum Entropy\" in Maximum-Entropy and\nBayesian Methods in Science and Engineering, G. J. Erickson and C. R.\nSmith (eds.) (Kluwer, Dordrecht, 1988).\n[7] J. Earman, Bayes or Bust?: A Critical Examination of Bayesian Confirmation Theory (MIT Press, Cambridge, 1992).\n[8] A. Caticha and A. Giffin, \"Updating Probabilities\", Bayesian Inference and Maximum Entropy Methods in Science and Engineering, Ali Mohammad-Djafari (ed.), AIP Conf. Proc. 872, 31 (2006)\n(arxiv.org/abs/physics/0608185).\n\n12\n\n\f[9] J. M. van Campenhout and T. M. Cover, IEEE Trans. Inform. Theory,\nIT-27 483 (1981).\n[10] I. Csiszar, \"An extended maximum entropy principle and a Bayesian justification,\" in Bayesian Statistics 2, J. M. Bernardo et al. (eds.) (North\nHolland, Amsterdam, 1985); I. Csiszar, \"MaxEnt, Mathematics, and Information Theory,\" Maximum Entropy and Bayesian Methods in Science and\nEngineering, K. M. Hanson and R. N. Silver (ed.) (Kluwer, 1996).\n[11] M. Grendar, Jr. and M. Grendar, \"What is the question that MaxEnt answers? A probabilistic interpretation,\" Bayesian Inference and Maximum\nEntropy Methods in Science and Engineering, Ali Mohammad-Djafari (ed.),\nAIP Conf. Proc. 568, 83 (2001) (arxiv.org/abs/math-ph/0009020); \"Maximum Entropy and Maximum Entropy methods: Bayesian interpretation\"\n(arxiv.org/abs/physics/0308005).\n[12] A. Renyi, \"On measures of entropy and information,\" Proc. 4th Berkeley\nSymposium on Mathematical Statistics and Probability, Vol. 1, 547-461 (U.\nof California Press, 1961); C. Tsallis, J. Stat. Phys. 52, 479 (1988).\n[13] A. Giffin and A. Caticha, \"Updating Probabilities with Data and Moments\", in these proceedings (arXiv:0708.1593).\n\n13\n\n\f"}