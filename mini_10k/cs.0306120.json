{"id": "http://arxiv.org/abs/cs/0306120v2", "guidislink": true, "updated": "2007-03-09T15:14:15Z", "updated_parsed": [2007, 3, 9, 15, 14, 15, 4, 68, 0], "published": "2003-06-22T08:00:09Z", "published_parsed": [2003, 6, 22, 8, 0, 9, 6, 173, 0], "title": "Reinforcement Learning with Linear Function Approximation and LQ control\n  Converges", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0306070%2Ccs%2F0306009%2Ccs%2F0306097%2Ccs%2F0306127%2Ccs%2F0306047%2Ccs%2F0306022%2Ccs%2F0306038%2Ccs%2F0306085%2Ccs%2F0306062%2Ccs%2F0306094%2Ccs%2F0306010%2Ccs%2F0306080%2Ccs%2F0306076%2Ccs%2F0306102%2Ccs%2F0306130%2Ccs%2F0306107%2Ccs%2F0306131%2Ccs%2F0306136%2Ccs%2F0306045%2Ccs%2F0306052%2Ccs%2F0306029%2Ccs%2F0306128%2Ccs%2F0306055%2Ccs%2F0306110%2Ccs%2F0306091%2Ccs%2F0306090%2Ccs%2F0306061%2Ccs%2F0306124%2Ccs%2F0306030%2Ccs%2F0306065%2Ccs%2F0306036%2Ccs%2F0306093%2Ccs%2F0306037%2Ccs%2F0306088%2Ccs%2F0306082%2Ccs%2F0306120%2Ccs%2F0306066%2Ccs%2F0306075%2Ccs%2F0306024%2Ccs%2F0306084%2Ccs%2F0306032%2Ccs%2F0306060%2Ccs%2F0306039%2Ccs%2F0306048%2Ccs%2F0306072%2Ccs%2F0306064%2Ccs%2F0306017%2Ccs%2F0306014%2Ccs%2F0306083%2Ccs%2F0306112%2Ccs%2F0306031%2Ccs%2F0306028%2Ccs%2F0306106%2Ccs%2F0306098%2Ccs%2F0306074%2Ccs%2F0306049%2Ccs%2F0306095%2Ccs%2F0306096%2Ccs%2F0306135%2Ccs%2F0306057%2Ccs%2F0306113%2Ccs%2F0306108%2Ccs%2F0306034%2Ccs%2F0306081%2Ccs%2F0306086%2Ccs%2F0306105%2Ccs%2F0306033%2Ccs%2F0306114%2Ccs%2F0306125%2Ccs%2F0306011%2Ccs%2F0306005%2Ccs%2F0306043%2Ccs%2F0306021%2Ccs%2F0306089%2Ccs%2F0306092%2Ccs%2F0306053%2Ccs%2F0306059%2Ccs%2F0306035%2Ccs%2F0306058%2Ccs%2F0306115%2Ccs%2F0306025%2Ccs%2F0306018%2Ccs%2F0607100%2Ccs%2F0607082%2Ccs%2F0607049%2Ccs%2F0607021%2Ccs%2F0607020%2Ccs%2F0607036%2Ccs%2F0607059%2Ccs%2F0607096%2Ccs%2F0607045%2Ccs%2F0607057%2Ccs%2F0607136%2Ccs%2F0607129%2Ccs%2F0607128%2Ccs%2F0607113%2Ccs%2F0607023%2Ccs%2F0607043%2Ccs%2F0607145%2Ccs%2F0607005%2Ccs%2F0607014&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Reinforcement Learning with Linear Function Approximation and LQ control\n  Converges"}, "summary": "Reinforcement learning is commonly used with function approximation. However,\nvery few positive results are known about the convergence of function\napproximation based RL control algorithms. In this paper we show that TD(0) and\nSarsa(0) with linear function approximation is convergent for a simple class of\nproblems, where the system is linear and the costs are quadratic (the LQ\ncontrol problem). Furthermore, we show that for systems with Gaussian noise and\nnon-completely observable states (the LQG problem), the mentioned RL algorithms\nare still convergent, if they are combined with Kalman filtering.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0306070%2Ccs%2F0306009%2Ccs%2F0306097%2Ccs%2F0306127%2Ccs%2F0306047%2Ccs%2F0306022%2Ccs%2F0306038%2Ccs%2F0306085%2Ccs%2F0306062%2Ccs%2F0306094%2Ccs%2F0306010%2Ccs%2F0306080%2Ccs%2F0306076%2Ccs%2F0306102%2Ccs%2F0306130%2Ccs%2F0306107%2Ccs%2F0306131%2Ccs%2F0306136%2Ccs%2F0306045%2Ccs%2F0306052%2Ccs%2F0306029%2Ccs%2F0306128%2Ccs%2F0306055%2Ccs%2F0306110%2Ccs%2F0306091%2Ccs%2F0306090%2Ccs%2F0306061%2Ccs%2F0306124%2Ccs%2F0306030%2Ccs%2F0306065%2Ccs%2F0306036%2Ccs%2F0306093%2Ccs%2F0306037%2Ccs%2F0306088%2Ccs%2F0306082%2Ccs%2F0306120%2Ccs%2F0306066%2Ccs%2F0306075%2Ccs%2F0306024%2Ccs%2F0306084%2Ccs%2F0306032%2Ccs%2F0306060%2Ccs%2F0306039%2Ccs%2F0306048%2Ccs%2F0306072%2Ccs%2F0306064%2Ccs%2F0306017%2Ccs%2F0306014%2Ccs%2F0306083%2Ccs%2F0306112%2Ccs%2F0306031%2Ccs%2F0306028%2Ccs%2F0306106%2Ccs%2F0306098%2Ccs%2F0306074%2Ccs%2F0306049%2Ccs%2F0306095%2Ccs%2F0306096%2Ccs%2F0306135%2Ccs%2F0306057%2Ccs%2F0306113%2Ccs%2F0306108%2Ccs%2F0306034%2Ccs%2F0306081%2Ccs%2F0306086%2Ccs%2F0306105%2Ccs%2F0306033%2Ccs%2F0306114%2Ccs%2F0306125%2Ccs%2F0306011%2Ccs%2F0306005%2Ccs%2F0306043%2Ccs%2F0306021%2Ccs%2F0306089%2Ccs%2F0306092%2Ccs%2F0306053%2Ccs%2F0306059%2Ccs%2F0306035%2Ccs%2F0306058%2Ccs%2F0306115%2Ccs%2F0306025%2Ccs%2F0306018%2Ccs%2F0607100%2Ccs%2F0607082%2Ccs%2F0607049%2Ccs%2F0607021%2Ccs%2F0607020%2Ccs%2F0607036%2Ccs%2F0607059%2Ccs%2F0607096%2Ccs%2F0607045%2Ccs%2F0607057%2Ccs%2F0607136%2Ccs%2F0607129%2Ccs%2F0607128%2Ccs%2F0607113%2Ccs%2F0607023%2Ccs%2F0607043%2Ccs%2F0607145%2Ccs%2F0607005%2Ccs%2F0607014&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Reinforcement learning is commonly used with function approximation. However,\nvery few positive results are known about the convergence of function\napproximation based RL control algorithms. In this paper we show that TD(0) and\nSarsa(0) with linear function approximation is convergent for a simple class of\nproblems, where the system is linear and the costs are quadratic (the LQ\ncontrol problem). Furthermore, we show that for systems with Gaussian noise and\nnon-completely observable states (the LQG problem), the mentioned RL algorithms\nare still convergent, if they are combined with Kalman filtering."}, "authors": ["Istvan Szita", "Andras Lorincz"], "author_detail": {"name": "Andras Lorincz"}, "author": "Andras Lorincz", "arxiv_comment": "9 pages", "links": [{"href": "http://arxiv.org/abs/cs/0306120v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0306120v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.6; I.2.8", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0306120v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0306120v2", "journal_reference": null, "doi": null, "fulltext": "arXiv:cs/0306120v2 [cs.LG] 9 Mar 2007\n\nREINFORCEMENT LEARNING WITH LINEAR FUNCTION\nAPPROXIMATION AND LQ CONTROL CONVERGES*\nISTV\u00c1N SZITA AND ANDR\u00c1S L\u0150RINCZ\nAbstract. Reinforcement learning is commonly used with function approximation. However, very few positive results are known about the convergence\nof function approximation based RL control algorithms. In this paper we\nshow that TD(0) and Sarsa(0) with linear function approximation is convergent for a simple class of problems, where the system is linear and the costs are\nquadratic (the LQ control problem). Furthermore, we show that for systems\nwith Gaussian noise and non-completely observable states (the LQG problem),\nthe mentioned RL algorithms are still convergent, if they are combined with\nKalman filtering.\n\n1. Introduction\nReinforcement learning is commonly used with function approximation. However, the technique has little theoretical performance guarantees: for example, it\nhas been shown that even linear function approximators (LFA) can diverge with\nsuch often used algorithms as Q-learning or value iteration [1, 8]. There are positive\nresults as well: it has been shown [10, 7, 9] that TD(\u03bb), Sarsa, importance-sampled\nQ-learning are convergent with LFA, if the policy remains constant (policy evaluation). However, to the best of our knowledge, the only result about the control problem (when we try to find the optimal policy) is the one of Gordon's [4],\nwho proved that TD(0) and Sarsa(0) can not diverge (although they may oscillate\naround the optimum, as shown in [3])1.\nIn this paper, we show that RL control with linear function approximation can\nbe convergent when it is applied to a linear system, with quadratic cost functions\n(known as the LQ control problem). Using the techniques of Gordon [4], we were\nprove that under appropriate conditions, TD(0) and Sarsa(0) converge to the optimal value function. As a consequence, Kalman filtering with RL is convergent for\nobservable systems, too.\nAlthough the LQ control task may seem simple, and there are numerous other\nmethods solving it, we think that this Technical Report has some significance: (i)\nTo our best knowledge, this is the first paper showing the convergence of an RL\ncontrol algorithm using LFA. (ii) Many problems can be translated into LQ form\n[2].\n\n\u2217 Last\n\nupdated: 22 October 2006.\n\n1These are results for policy iteration (e.g. [5]). However, by construction, policy iteration\n\ncould be very slow in practice.\n1\n\n\f2\n\nI. SZITA AND A. L\u0150RINCZ\n\n2. the LQ control problem\nConsider a linear dynamical system with state xt \u2208 Rn , control ut \u2208 Rm , in\ndiscrete time t:\n(1)\n\nxt+1\n\n=\n\nF xt + Gut .\n\nExecuting control step ut in xt costs\nc(xt , ut ) := xTt Qxt + uTt Rut ,\n\n(2)\n\nand after the N th step the controller halts and receives a final cost of xTN QN xN .\nThe task is to find a control sequence with minimum total cost.\nFirst of all, we slightly modify the problem: the run time of the controller\nwill not be a fixed number N . Instead, after each time step, the process will be\nstopped with some fixed probability p (and then the controller incurs the final cost\ncf (xf ) := xTf Qf xf ). This modification is commonly used in the RL literature; it\nmakes the problem more amenable to mathematical treatments.\n2.1. The cost-to-go function. Let Vt\u2217 (x) be the optimal cost-to-go function at\ntime step t, i.e.\n\u0002\n\u0003\n(3)\nVt\u2217 (x) :=\ninf E c(xt , ut ) + c(xt+1 , ut+1 ) + . . . + cf (xf ) xt = x .\nut ,ut+1 ,...\n\nConsidering that the controller is stopped with probability p, Eq. 3 assumes the\nfollowing form\n\u0010\n\u0011\n\u2217\n(4)\nVt\u2217 (x) = p * cf (x) + (1 \u2212 p) inf c(x, u) + Vt+1\n(F x + Gu)\nu\n\nfor any state x. It is an easy matter to show that the optimal cost-to-go function is\ntime-independent and it is a quadratic function of x. That is, the optimal cost-to-go\naction-value function assumes the form\nV \u2217 (x) = xT \u03a0\u2217 x.\n\n(5)\n\nOur task is to estimate the optimal value functions (i.e., parameter matrix \u03a0\u2217 )\non-line. This can be done by the method of temporal differences.\nWe start with an arbitrary initial cost-to-go function V0 (x) = xT \u03a00 x. After\nthis,\n(1) control actions are selected according to the current value function estimate\n(2) the value function is updated according to the experience, and\n(3) these two steps are iterated.\nThe tth estimate of V \u2217 is Vt (x) = xT \u03a0t x. The greedy control action according\nto this is given by\n\u0010\n\u0011\n(6)\nut = arg min c(xt , u) + Vt (F xt + Gu)\nu\n\u0010\n\u0011\n= arg min uT Ru + (F xt + Gu)T \u03a0t (F xt + Gu)\nu\n\n=\n\n\u2212(R + GT \u03a0t G)\u22121 (GT \u03a0t F )xt .\n\nThe 1-step TD error is\n(\ncf (xt ) \u2212 Vt (xt )\nif t = tST OP ,\n\u0001\n(7)\n\u03b4t =\nc(xt , ut ) + Vt (xt+1 ) \u2212 Vt (xt ), otherwise.\n\n\fRL WITH LFA AND LQ CONTROL CONVERGES\n\n3\n\nInitialize x0 , u0 , \u03a00\nrepeat\nxt+1 = F xt + Gut\n\u03bdt+1 := random noise\nut+1 = \u2212(R + GT \u03a0t+1 G)\u22121 (GT \u03a0t+1 F )xt+1 + \u03bdt+1\nwith probability p,\n\u03b4t = xTt Qf xt \u2212 xTt \u03a0t xt\nSTOP\nelse\n\u03b4t = uTt Rut + xTt+1 \u03a0t xt+1 \u2212 xTt \u03a0t xt\n\u03a0t+1 = \u03a0t + \u03b1t \u03b4t xt xTt\nt=t+1\nend\nFigure 1. TD(0) with linear function approximation for LQ control\nand the update rule for the parameter matrix \u03a0t is\n(8)\n\n\u03a0t+1\n\n=\n=\n\n\u03a0t + \u03b1t * \u03b4t * \u2207\u03a0t Vt (xt )\n\n\u03a0t + \u03b1t * \u03b4t * xt xTt ,\n\nwhere \u03b1t is the learning rate.\nThe algorithm is summarized in Fig. 1.\n2.2. Sarsa. The cost-to-go function is used to select control actions, so the actionvalue function Q\u2217t (x, u) is more appropriate for this purpose. The action-value\nfunction is defined as\n\u0002\n\u0003\nQ\u2217t (x, u) :=\ninf\nE c(xt , ut ) + c(xt+1 , ut+1 ) + . . . + cf (xf ) xt = x, ut = u ,\nut+1 ,ut+2 ,...\n\nand analogously to Vt\u2217 , it can be shown that it is time independent and can be\nwritten in the form\n\u0012\n\u0012 \u0013\n\u0013\u0012 \u0013\n\u0001 \u0398\u221711 \u0398\u221712\n\u0001 \u2217 x\nx\n\u2217\nT\nT\nT\nT\nu\nu \u0398\n.\n(9)\nQ (x, u) = x\n= x\n\u0398\u221721 \u0398\u221722\nu\nu\n\nNote that \u03a0\u2217 can be expressed by \u0398\u2217 using the relationship V (x) = minu Q(x, u):\n\u03a0\u2217 = \u0398\u221711 \u2212 \u0398\u221712 (\u0398\u221722 )\u22121 \u0398\u221721\n\n(10)\n\nIf the tth estimate of Q\u2217 is Qt (x, u) = [xT , uT ]T \u0398t [xT , uT ], then the greedy control\naction is given as\n\u0398T21 + \u039821\nxt = \u2212\u0398\u22121\n22 \u039821 xt ,\nu\n2\nwhere subscript t of \u0398 has been omitted to improve readability.\nThe estimation error and the weight update are similar to the state-value case:\n(\ncf (xt ) \u2212 Qt (xt , ut )\nif t = tST OP ,\n\u0001\n(12)\n\u03b4t =\nc(xt , ut ) + Qt (xt+1 , ut+1 ) \u2212 Qt (xt , ut ), otherwise,\n(11)\n\nut\n\n=\n\narg min Qt (x, u) = \u2212\u0398\u22121\n22\n\n\f4\n\nI. SZITA AND A. L\u0150RINCZ\n\nInitialize x0 , u0 , \u03980\nz0 = (xT0 uT0 )T\nrepeat\nxt+1 = F xt + Gut\n\u03bdt+1 := random noise\nut+1 = \u2212(\u0398t )22 (\u0398t )21 xt+1 + \u03bdt+1\nzt+1 = (xTt+1 uTt+1 )T\nwith probability p,\n\u03b4t = xTt Qf xt \u2212 zTt \u0398t zt\nSTOP\nelse\n\u03b4t = uTt Rut + zTt+1 \u0398t zt+1 \u2212 zTt \u0398t zt\n\u0398t+1 = \u0398t + \u03b1t \u03b4t zt zTt\nt=t+1\nend\nFigure 2. Sarsa(0) with linear function approximation for LQ control\n(13)\n\n\u0398t+1\n\n= \u0398t + \u03b1t * \u03b4t * \u2207\u0398t Qt (xt , ut )\n\u0012 \u0013 \u0012 \u0013T\nxt\nxt\n= \u0398t + \u03b1t * \u03b4t *\n.\nut\nut\n\nThe algorithm is summarized in Fig. 2.\n3. Convergence\n\u221a\nTheorem 3.1. If \u03a00 \u2265 \u03a0\u2217 , there exists an L such that kF + GLk \u2264 1/ 1 \u2212 p,\n4 P\nthenP\nthere exists a series of learning rates \u03b1t such that 0 < \u03b1t \u2264 1/ kxt k , t \u03b1t =\n\u221e, t \u03b12t < \u221e, and it can be computed online. For all sequences of learning rates\nsatisfying these requirements, Algorithm 1 converges to the optimal policy.\nThe proof of the theorem can be found in Appendix B.\nThe same line of thought can be carried over for the action-value function\nQ(x, u) = (xT uT )T \u0398(xT uT ), which we do not detail here, giving only the result:\n\u221a\nTheorem 3.2. If \u03980 \u2265 \u0398\u2217 , there exists an L such that kF + GLk \u2264 1/ 1 \u2212 p,\nP\n4\nthenP\nthere exists a series of learning rates \u03b1t such that 0 < \u03b1t \u2264 1/ kxt k , t \u03b1t =\n2\n\u221e, t \u03b1t < \u221e, and it can be computed online. For all sequences of learning rates\nsatisfying these requirements, Sarsa(0) with LFA (Fig. 2) converges to the optimal\npolicy.\n4. Kalman filter LQ control\nNow let us examine the case when we do not know the exact states, but we have\nto estimate them from noisy observations. Consider a linear dynamical system with\nstate xt \u2208 Rn , control ut \u2208 Rm , observation yt \u2208 Rk , noises \u03bet \u2208 Rn and \u03b6t \u2208 Rk\n(which are assumed to be uncorrelated Gaussians with covariance matrix \u03a9\u03be and\n\n\fRL WITH LFA AND LQ CONTROL CONVERGES\n\n5\n\n\u03a9\u03b6 , respectively), in discrete time t:\n(14)\n\nxt+1\n\n=\n\nF xt + Gut + \u03bet\n\n(15)\n\nyt\n\n=\n\nHxt + \u03b6t .\n\nAssume that the initial state has mean x\u03021 , and covariance \u03a31 . Furthermore, assume\nthat executing control step ut in xt costs\nc(xt , ut ) := xTt Qxt + uTt Rut ,\n\n(16)\n\nAfter each time step, the process will be stopped with some fixed probability p, and\nthen the controller incurs the final cost cf (xf ) := xTf Qf xf .\nWe will show that the separation principle holds for our problem, i.e. the control\nlaw and the state filtering can be computed independently from each other. On one\nhand, state estimation is independent of the control selection method (in fact, the\ncontrol could be anything, because it does not affect the estimation error), i.e. we\ncan estimate the state of the system by the standard Kalman filtering equations:\n(17)\n\nx\u0302t+1\n\n=\n\n(18)\n\nKt\n\n=\n\n(19)\n\n\u03a3t+1\n\n=\n\nF x\u0302t + Gut + Kt (yt \u2212 H x\u0302t )\nF \u03a3t H T (H\u03a3t H T + \u03a9e )\u22121\n\n\u03a9w + F \u03a3t F T \u2212 Kt H\u03a3t F T .\n\nOn the other hand, it is easy to show that the optimal control can be expressed\nas the function of x\u0302t . The proof (similarly to the proof of the original separation\nprinciple) is based on the fact that the noise and error terms appearing in the\nexpressions are either linear and have zero mean or quadratic and independent of\nu. In both cases they can be omitted. More precisely, let Wt denote the sequence\ny1 , . . . , yt , u1 , . . . , ut\u22121 , and let et = xt \u2212 x\u0302t . Equation (6) for the filtered case can\nbe formulated as\n\u0011\n\u0010\n(20)\nut = arg min E c(xt , u) + Vt (F xt + Gu + \u03bet ) Wt\nu\n\u0010\n= arg min E xTt Qxt + uT Ru +\nu\n\u0011\n(F xt + Gu + \u03bet )T \u03a0t (F xt + Gu + \u03bet ) Wt .\n\nUsing the fact that E(xTt Qxt |Wt ) and E(\u03betT \u03a0t \u03bet |Wt ) are independent of u and that\nE((F xt + Gu)T \u03a0t \u03bet |Wt ) = 0, furthermore that xt = x\u0302t + et , we get\n\u0011\n\u0010\nut = arg min E uT Ru + (F x\u0302t + F et + Gu)T \u03a0t (F x\u0302t + F et + Gu) Wt\nu\n\nFinally, we know that E(et |Wt ) = 0, because the Kalman filter is an unbiased\nestimator, furthermore E(eTt \u03a0t et |Wt ) is independent of u, which yields\n\u0011\n\u0010\nut = arg min E uT Ru + (F x\u0302t + Gu)T \u03a0t (F x\u0302t + Gu) Wt\nu\n\n= \u2212(R + GT \u03a0t G)\u22121 (GT \u03a0t F )x\u0302t ,\n\ni.e. for the computation of the greedy control action according to Vt we can use\nthe estimated state instead of the exact one. The proof of the separation principle\nfor SARSA(0) is quite similar and therefore is omitted here.\nThe resulting algorithm using TD(0) is summarized in Fig. 3. The algorithm\nusing Sarsa can be derived in a similar manner.\n\n\f6\n\nI. SZITA AND A. L\u0150RINCZ\n\nInitialize x0 , x\u03020 , u0 , \u03a00 , \u03a30\nrepeat\nxt+1 = F xt + Gut + \u03bet\nyt = Hxt + \u03b6t\n\u03a3t+1 = \u03a9\u03be + F \u03a3t F T \u2212 Kt H\u03a3t F T\nKt = F \u03a3t H T (H\u03a3t H T + \u03a9\u03b6 )\u22121\nx\u0302t+1 = F x\u0302t + Gut + Kt (yt \u2212 H x\u0302t )\n\u03bdt+1 := random noise\nut+1 = \u2212(R + GT \u03a0t+1 G)\u22121 (GT \u03a0t+1 F )x\u0302t+1 + \u03bdt+1\nwith probability p,\n\u03b4t = x\u0302Tt Qf x\u0302t \u2212 x\u0302Tt \u03a0t x\u0302t\nSTOP\nelse\n\u03b4t = uTt Rut + x\u0302Tt+1 \u03a0t x\u0302t+1 \u2212 x\u0302Tt \u03a0t x\u0302t\n\u03a0t+1 = \u03a0t + \u03b1t \u03b4t x\u0302t x\u0302Tt\nt=t+1\nend\nFigure 3. Kalman filtering with TD control\n5. Acknowledgments\nThis work was supported by the Hungarian National Science Foundation (Grant\nNo. T-32487). We would like to thank L\u00e1szl\u00f3 Gerencs\u00e9r for calling our attention\nto a mistake in the previous version of the convergence proof.\nAppendix A. The boundedness of kxt k\nWe need several technical lemmas to show that kxt k remains bounded for the\nlinear-quadratic case, and also, E(kxt k) remains bounded for the Kalman filter\ncase. The latter result implies that for the KF case, kxt k remains bounded with\nhigh probability.\nFor any positive semidefinite matrix \u03a0 and any state x, we can define the action\nvector which minimizes the one-step-ahead value function:\n\u0010\n\u0011\nugreedy := arg min uT Ru + (F x + Gu)T \u03a0(F x + Gu)\nu\n\n=\n\n\u2212(R + GT \u03a0G)\u22121 (GT \u03a0F )x.\n\nLet\nL\u03a0 := \u2212(R + GT \u03a0G)\u22121 (GT \u03a0F )\n\ndenote the greedy control for matrix \u03a0, and let\n\nL\u2217 = \u2212(R + GT \u03a0\u2217 G)\u22121 (GT \u03a0\u2217 F )\n\u221a\nbe the optimal policy, furthermore, let q := 1/ 1 \u2212 p.\nLemma A.1. If there exists an L such that kF + GLk < q, then kF + GL\u2217 k < q\nas well.\n\n\fRL WITH LFA AND LQ CONTROL CONVERGES\n\n7\n\nProof. Indirectly, suppose that kF + GL\u2217 k \u2265 q. Then for a fixed x0 , let xt be the\noptimal trajectory\nxt+1 = (F + GL\u2217 )xt .\nThen\nV \u2217 (x0 ) =\n+\n+\n+\nV \u2217 (x0 ) \u2265\n=\n\np cf (x0 )\n(1 \u2212 p)p cf (x1 )\n\n+(1 \u2212 p)c(x0 , L\u2217 x0 )\n\n+(1 \u2212 p)2 c(x1 , L\u2217 x1 )\n\n(1 \u2212 p)2 p cf (x2 ) +(1 \u2212 p)3 c(x2 , L\u2217 x2 )\n...,\n\np cf (x0 ) + (1 \u2212 p)cf (x1 ) + (1 \u2212 p)2 cf (x2 ) + . . .\nX\nT\n(1 \u2212 p)k xT0 (F + GL\u2217 )k Qf (F + GL\u2217 )k x0 .\np\n\n\u0001\n2\n\nWe know that Qf is positive definite, so there exists an \u01eb such that xT Qf x \u2265 \u01eb kxk ,\ntherefore\nX\n2\nV \u2217 (x0 ) \u2265 \u01ebp\n(1 \u2212 p)k (F + GL\u2217 )k x0 .\n\nIf x0 is the eigenvector corresponding to the maximal eigenvalue of F + GL\u2217 , then\nk\n(F + GL\u2217 )x0 = kF + GL\u2217 k x0 , and so (F + GL\u2217 )k x0 = kF + GL\u2217 k x0 . Consequently,\nX\n2k\n2\nV \u2217 (x0 ) \u2265 \u01ebp\n(1 \u2212 p)k kF + GL\u2217 k kx0 k\nX\n1\n2\nkx0 k = \u221e.\n\u2265 \u01ebp\n(1 \u2212 p)k\n(1 \u2212 p)k\n\nOn the other hand, because of kF + GLk < q, it is easy to see that the value of\nfollowing the control law L from x0 is finite, therefore we get V L (x0 ) < V \u2217 (x0 ),\nwhich is a contradiction.\n\u0003\nLemma A.2. For positive definite matrices A and B, if A \u2265 B then A\u22121 B \u2264 1.\n\nProof. Indirectly, suppose that A\u22121 B > 1. Let \u03bbmax be the maximal eigenvalue\nof A\u22121 B, and v be a corresponding eigenvector.\nA\u22121 Bv = \u03bbmax v,\nand according to the indirect assumption,\n\u03bbmax = A\u22121 B > 1.\nA \u2265 B means that for each x, xT Ax \u2265 xT Bx, so this holds specifically for x =\nA\u22121 Bv = \u03bbmax v, too. So, on one hand,\n(\u03bbmax v)T B(\u03bbmax v) = \u03bb2max vT Bv > vT Bv,\nand on the other hand,\n(\u03bbmax v)T A(\u03bbmax v) = (A\u22121 Bv)T A(A\u22121 Bv) = vT (BA\u22121 B)v,\nso,\nvT (BA\u22121 B)v > vT Bv,\nHowever, from A \u2265 B, A\u22121 \u2264 B \u22121 . Multiplying this with B from both sides,\nwe get BA\u22121 B \u2264 B, which is a contradiction.\n\u0003\n\n\f8\n\nI. SZITA AND A. L\u0150RINCZ\n\nLemma A.3. If there exists an L such that kF + GLk < q then for any \u03a0 such\nthat \u03a0 \u2265 \u03a0\u2217 , kF + GL\u03a0 k < q, too.\nProof. We will apply the Woodbury identity [6], stating that for positive definite\nmatrices R and \u03a0,\n(R + GT \u03a0G)\u22121 GT \u03a0 = R\u22121 GT (GR\u22121 GT + \u03a0\u22121 )\u22121\nConsequently,\nF + GL\u03a0\n\nLet\nU\u03a0\n\n= F \u2212 G(R + GT \u03a0G)\u22121 (GT \u03a0F )\n\u0011\n\u0011\u0010\n\u0010\n= F \u2212 GR\u22121 GT (GR\u22121 GT + \u03a0\u22121 )\u22121 F.\n:=\n=\n\nand\nU\u2217\n\n\u0011\u22121\n\u0011\u0010\n\u0010\nI \u2212 GR\u22121 GT GR\u22121 GT + \u03a0\u22121\n\u0011\u22121\n\u0010\n\u03a0\u22121 GR\u22121 GT + \u03a0\u22121\n\n\u0011\u22121\n\u0011\u0010\n\u0010\n:= I \u2212 GR\u22121 GT GR\u22121 GT + (\u03a0\u2217 )\u22121\n\u0011\u22121\n\u0010\n= (\u03a0\u2217 )\u22121 GR\u22121 GT + (\u03a0\u2217 )\u22121\n\nBoth matrices are positive definite, because they are the product of positive definite\nmatrices. With these notations, F + GL\u03a0 = U\u03a0 F and F + GL\u2217 = U \u2217 F .\nIt is easy to show that U\u03a0 \u2264 U \u2217 exploiting the fact that \u03a0 \u2265 \u03a0\u2217 and several wellknown properties of matrix inequalities: if A \u2265 B and C is positive semidefinite,\nthen \u2212A \u2264 \u2212B, A\u22121 \u2264 B \u22121 , A + C \u2265 B + C, A * C \u2265 B * C.\nFrom Lemma A.1 we know that kU \u2217 F k = kF + GL\u2217 k < q, and from the previous\nlemma we know that U\u03c0 (U \u2217 )\u22121 \u2264 1, so\nkF + GL\u03a0 k = kU\u03a0 F k = U\u03a0 (U \u2217 )\u22121 U \u2217 F \u2264 U\u03a0 (U \u2217 )\u22121 kU \u2217 F k \u2264 1 * q\n\u0003\nCorollary A.4. If there exists an L such that kF + GLk \u2264 q, then the state\nsequence generated by the noise-free LQ equations is bounded, i.e., there exists M \u2208\nR such that kxt k \u2264 M .\nProof. This is a simple corollary of the previous lemma: in each step we use a\ngreedy control law Lt , so\nkxt+1 k = k(F + GLt )xt k \u2264 q kxt k\n\u0003\nCorollary A.5. If there exists an L such that kF + GLk \u2264 q, then the state\nsequence generated by the Kalman-filter equations is bounded with high probability,\ni.e., for any e > 0, there exists M \u2208 R such that kxt k \u2264 M with probability 1 \u2212 \u01eb.\nProof.\nE kxt+1 k =\n\u2264\n\nE k(F + GLt )xt + \u03bet k \u2264\nq\nqE kxt k + \u03a9\u03be ,\n\nq\nE k(F + GLt )xt k + \u03a9\u03be\n\n\fRL WITH LFA AND LQ CONTROL CONVERGES\n\n9\n\nso there exists a bound M \u2032 such that E kxt k \u2264 M \u2032 . From Markov's inequality,\nPr(kxt k > M \u2032 /e) < e,\ntherefore, M = M \u2032 /e satisfies our requirements.\n\n\u0003\n\nAppendix B. The proof of the main theorem\nWe will use the following lemma:\nLemma B.1. Let J be a differentiable function, bounded from below by J \u2217 , and let\n\u2207J be Lipschitz-continuous. Suppose the weight sequence wt satisfies\nwt+1 = wt + \u03b1t bt\nfor random vectors bt independent of wt+1 , wt+2 , . . ., and bt is a descent direction\nfor J, i.e. E(bt |wt )T \u2207J(wt ) \u2264 \u2212\u03b4(\u01eb) < 0 whenever J(wt ) > J \u2217 + \u01eb. Suppose also\nthat\nE(kbt k2 |wt ) \u2264 K1 J(wt ) + K2 E(bt |wt )T \u2207J(wt ) + K3\nP\nP\nand finally that the constants \u03b1t satisfy \u03b1t > 0, t \u03b1t = \u221e, t \u03b12t < \u221e. Then\nJ(wt ) \u2192 J \u2217 with probability 1.\nIn our case, the weight vectors are n \u00d7 n dimensional, with wn*i+j := \u03a0ij .\nFor the sake of simplicity, we denote this by w(ij) . Let w\u2217 be the weight vector\ncorresponding to the optimal value function, and let\nJ(w) =\n\n1\nkw \u2212 w\u2217 k2 .\n2\n\nTheorem B.2 (Theorem 3.1). If \u03a00 \u2265 \u03a0\u2217 , there exists an L such that kF + GLk \u2264\n4\nq, then there P\nexists a series of learning rates \u03b1t such that 0 < \u03b1t \u2264 1/ kxt k ,\nP\n2\nt \u03b1t < \u221e, and it can be computed online. For all sequences of\nt \u03b1t = \u221e,\nlearning rates satisfying these requirements, Algorithm 1 converges to the optimal\npolicy.\nProof. First of all, we prove the existence of a suitable learning rate sequence.\nLet\nP\n\u03b1\u2032t bePa sequence of learning rates that satisfy two of the requirements, t \u03b1t = \u221e\nand t \u03b12t < \u221e. Fix a probability 0 < e < 1. By the previous lemma, there exists\na bound M such that kxt k \u2264 M with probability 1 \u2212 e. The learning rates\n4\n\n\u03b1t := min{\u03b1\u2032t , 1/ kxt k }\nwill be satisfactory, and can be computed on the fly. The first\nP and third requirements are trivially satisfied, so we only have to show that t \u03b1t = \u221e. Consider\n4\nthe index set H = {t : \u03b1\u2032t \u2264 1/M 4 } \u222a {t : \u03b1\u2032t \u2264 1/ kxt k }. By the first condition\nonly finitely many indices are excluded. The second condition excludes indices with\n4\n1/M 4 < \u03b1\u2032t < 1/ kxt k , which happens at most with probability e. However,\nX\nX\nX\n\u03b1t \u2265\n\u03b1t =\n\u03b1\u2032t = \u221e.\nt\n\nt\u2208H\n\nt\u2208H\n\nThe last equality holds, because if we take a divergent sum of nonnegative terms,\nand exclude finitely many terms or an index set with density less than 1, then the\nremaining subseries will remain divergent.\n\n\f10\n\nI. SZITA AND A. L\u0150RINCZ\n\nAn update step of the algorithm is \u03b1t \u03b4t xt xTt . To make the proof simpler, we\ndecompose this into a step size \u03b1\u2032t and a direction vector (\u03b1t /\u03b1\u2032t )\u03b4t xt xTt . Denote\nthe scaling factor by\n4\n\nAt := \u03b1t /\u03b1\u2032t = min{1, 1/(\u03b1\u2032t kxt k )}\nClearly, At \u2264 1. In fact, it will be one most of the time, and will damp only the\nsamples that are too big.\nWe will show that\nbt = At \u03b4t xt xTt\nis a descent direction for every t.\nE(bt |wt )T \u2207J(wt )\n\n= At E(\u03b4t |wt )xt xTt (wt \u2212 w\u2217 )\n\n= At E(\u03b4t |wt )xTt (\u03a0t \u2212 \u03a0\u2217 )xt\n= At E(\u03b4t |wt )(Vt (xt ) \u2212 V \u2217 (xt )).\n\nFor the sake of simplicity, from now on we do not note the dependence on wt\nexplicitly.\nWe will show that for all t, E(\u03a0t ) > 0, E(\u03a0t\u22121 ) > E(\u03a0t ) and E(\u03b4t ) \u2264 \u2212pxTt (\u03a0t \u2212\n\u2217\n\u03a0 )xt . We proceed by induction.\n\u2022 t = 0. \u03a00 > \u03a0\u2217 holds by assumption.\n\u2022 Induction step part 1: E(\u03b4t ) \u2264 \u2212p xTt (\u03a0t \u2212 \u03a0\u2217 )xt .\nRecall that\n\u0010\n\u0011\n(21)\nut = arg min c(xt , u) + Vt (F xt + Gu)\nu\n\n= Lt xt ,\n\nwhere\nLt = \u2212(R + GT \u03a0t G)\u22121 (GT \u03a0t F )\nis the greedy control law with respect to Vt . Clearly, by the definition of Lt ,\nc(xt , Lt xt ) + Vt (F xt + GLt xt ) \u2264 c(xt , L\u2217 xt ) + Vt (F xt + GL\u2217 xt ).\nThis yields\n\u0001\n(22) E(\u03b4t ) = p cf (xt ) + (1 \u2212 p) c(xt , Lt xt ) + Vt (F xt + GLt xt ) \u2212 Vt (xt )\n\u0001\n\u2264 p cf (xt ) + (1 \u2212 p) c(xt , L\u2217 xt ) + Vt (F xt + GL\u2217 xt ) \u2212 Vt (xt ).\n\nWe know that the optimal value function satisfies the fixed-point equation\n\u0010\n\u0001\u0011\n(23)\n0 = p cf (xt ) + (1 \u2212 p) c(xt , L\u2217 xt ) + V \u2217 (F xt + GL\u2217 xt ) \u2212 V \u2217 (xt ).\n\nSubtracting this from Eq. (22), we get\n(24)\n(25)\n(26)\n\nE(\u03b4t ) \u2264\n=\n\n\u0001\n(1 \u2212 p) Vt (F xt + GL\u2217 xt ) \u2212 V \u2217 (F xt + GL\u2217 xt )\n\n\u2212(Vt (xt ) \u2212 V \u2217 (xt )).\n\n(1 \u2212 p)xTt (F + GL\u2217 )T (\u03a0t \u2212 \u03a0\u2217 )(F + GL\u2217 )xt\n\n\u2212xTt (\u03a0t \u2212 \u03a0\u2217 )xt .\n\n\fRL WITH LFA AND LQ CONTROL CONVERGES\n\n11\n\n2\n\nLet \u01eb1 = \u01eb1 (p) := 1/(1 \u2212 p) \u2212 kF + GL\u2217 k > 0. Inequality (24) implies\n(27)\n\nE(\u03b4t ) \u2264\n\n(28)\n\n=\n\n(29)\n\n=\n\n1\n\u2212 \u01eb1 (p))xTt (\u03a0t \u2212 \u03a0\u2217 )xt \u2212 xTt (\u03a0t \u2212 \u03a0\u2217 )xt\n1\u2212p\n\u2212(1 \u2212 p)\u01eb1 (p)xTt (\u03a0t \u2212 \u03a0\u2217 )xt .\n(1 \u2212 p)(\n\n\u2212\u01eb2 (p)xTt (\u03a0t \u2212 \u03a0\u2217 )xt ,\n\nwhere we defined \u01eb2 (p) = (1 \u2212 p)\u01eb1 (p)\n\u2022 Induction step part 2: E(\u03a0t+1 ) > \u03a0\u2217 .\n(30) E(\u03b4t ) =\n\u2265\n\n\u0001\np cf (xt ) + (1 \u2212 p) c(xt , Lt xt ) + Vt (F xt + GLt xt ) \u2212 Vt (xt )\n\u0001\np cf (xt ) + (1 \u2212 p) c(xt , Lt xt ) + V \u2217 (F xt + GLt xt ) \u2212 Vt (xt ).\n\nSubtracting eq. 23, we get\n\u0010\n\u0001\nE(\u03b4t ) \u2265 (1 \u2212 p) c(xt , Lt xt ) + V \u2217 (F xt + GLt xt )\n(31)\n\u0001\u0011\n\u2212 c(xt , L\u2217 xt ) + V \u2217 (F xt + GL\u2217 xt ) + V \u2217 (xt ) \u2212 Vt (xt )\n\u2265\n\n2\n\nV \u2217 (xt ) \u2212 Vt (xt ) \u2265 \u2212 k\u03a0t \u2212 \u03a0\u2217 k kxt k .\n\nTherefore\n(32)\n(33)\n\nE(\u03a0t+1 ) \u2212 \u03a0\u2217\n\n(34)\n\n\u2265\n\n\u2265\n\n\u2265\n\n\u03a0t + \u03b1\u2032t At E(\u03b4t )xt xTt \u2212 \u03a0\u2217\n4\n\n(\u03a0t \u2212 \u03a0\u2217 ) \u2212 \u03b1t kxt k k\u03a0t \u2212 \u03a0\u2217 k I\n\n(\u03a0t \u2212 \u03a0\u2217 ) \u2212 k\u03a0t \u2212 \u03a0\u2217 k I > 0.\n\n\u2022 Induction step part 3: \u03a0t > E(\u03a0t+1 ).\n(35)\n\n\u03a0t \u2212 E(\u03a0t+1 )\n\n= \u2212\u03b1\u2032t At E(\u03b4t )xt xTt \u2265 \u03b1t \u01eb2 (p)xTt (\u03a0t \u2212 \u03a0\u2217 )xt * xt xTt ,\n\nbut \u03b1\u2032t \u01eb2 (p) > 0, xTt (\u03a0t \u2212 \u03a0\u2217 )xt > 0 and xt xTt > 0, so their product is positive as\nwell.\nThe induction is therefore complete.\nWe finish the proof by showing that the assumptions of Lemma B.1 hold:\nbt is a descent direction. Clearly, if J(wt ) \u2265 \u01eb, then k\u03a0t \u2212 \u03a0\u2217 k \u2265 \u01eb3 (\u01eb), but\n\u03a0t \u2212 \u03a0\u2217 is positive definite, so \u03a0t \u2212 \u03a0\u2217 \u2265 \u01eb3 (\u01eb)I.\nE(bt |wt )T \u2207J(wt )\n\n= At E(\u03b4t |wt )(Vt (xt ) \u2212 V \u2217 (xt ))\n\n\u2264 \u2212\u01eb2 (p)At xTt (\u03a0t \u2212 \u03a0\u2217 )xt * xTt (\u03a0t \u2212 \u03a0\u2217 )xt\n\n\u2264 \u2212\u01eb2 \u01eb23 At kxt k4\n\n4\n\n\u2264 \u2212\u01eb2 \u01eb23 min{kxt k , 1/\u03b1\u2032t }\n\n\f12\n\nI. SZITA AND A. L\u0150RINCZ\n2\n\nE(kbt k |wt ) is bounded. |E(\u03b4t )| \u2264 |xTt (\u03a0t \u2212 \u03a0\u2217 )xt |. Therefore\n2\n\nE(kbt k |wt ) \u2264\n\u2264\n\n\u2264\n\n\u2264\n\n2\n\n|At |2 |E(\u03b4t )|2 kxt k\n2\n\n8\n\n6\n\nk\u03a0t \u2212 \u03a0\u2217 k * min{1, 1/(\u03b1\u20322\nt kxt k )} * kxt k\n2\n\n6\n\n2\n\nk\u03a0t \u2212 \u03a0\u2217 k * min{kxt k , 1/(\u03b1\u20322\nt kxt k )}\nK * J(wt ).\n\nConsequently, The assumptions of lemma B.1 hold, so the algorithm converges\nto the optimal value function with probability 1.\n\u0003\nReferences\n1. Leemon C. Baird, Residual algorithms: Reinforcement learning with function approximation,\nInternational Conference on Machine Learning, 1995, pp. 30\u201337.\n2. S. J. Bradtke, Reinforcement learning applied to linear quadratic regulation, Advances in\nNeural Information Processing Systems 5, Proceedings of the IEEE Conference in Denver\n(to appear) (San Mateo, CA) (C. L. Giles, S. J. Hanson, and J. D. Cowan, eds.), Morgan\nKaufmann, 1993.\n3. Geoffrey J. Gordon, Chattering in sarsa(lambda) - a CMU learning lab internal report, 1996.\n4.\n, Reinforcement learning with function approximation converges to a region, Advances\nin Neural Information Processing Systems 13 (Todd K. Leen, Thomas G. Dietterich, and\nVolker Tresp, eds.), MIT Press, 2001, pp. 1040\u20131046.\n5. T. J. Perkins and D. Precup, A convergent form of approximate policy iteration, http://www.mcb.mcgill.ca/\u223cperkins/publications/PerPreNIPS02.ps, 2002, Accepted to\nNIPS-02.\n6. K. B. Petersen and M. S. Pedersen, The matrix cookbook, 2005, Version 20051003.\n7. Doina Precup, Richard S. Sutton, and Sanjoy Dasgupta, Off-policy temporal-difference learning with function approximation, Proc. 18th International Conf. on Machine Learning, Morgan\nKaufmann, San Francisco, CA, 2001, pp. 417\u2013424.\n8. R. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge,\n1998.\n9. V. Tadic, On the convergence of temporal-difference learning with linear function approximation, Machine Learning 42 (2001), 241\u2013267.\n10. John N. Tsitsiklis and Benjamin Van Roy, An analysis of temporal-difference learning with\nfunction approximation, Tech. Report LIDS-P-2322, 1996.\n\nDepartment of Information Systems\nE\u00f6tv\u00f6s Lor\u00e1nd University of Sciences\nP\u00e1zm\u00e1ny P\u00e9ter s\u00e9t\u00e1ny 1/C\n1117 Budapest, Hungary\nEmails:\nIstv\u00e1n Szita: szityu@stella.eotvos.elte.hu\nAndr\u00e1s L\u0151rincz: lorincz@inf.elte.hu\nWWW:\nhttp://nipg.inf.elte.hu\nhttp://people.inf.elte.hu/lorincz\n\n\f"}