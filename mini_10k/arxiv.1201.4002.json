{"id": "http://arxiv.org/abs/1201.4002v1", "guidislink": true, "updated": "2012-01-19T10:06:29Z", "updated_parsed": [2012, 1, 19, 10, 6, 29, 3, 19, 0], "published": "2012-01-19T10:06:29Z", "published_parsed": [2012, 1, 19, 10, 6, 29, 3, 19, 0], "title": "Adaptive Policies for Sequential Sampling under Incomplete Information\n  and a Cost Constraint", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.5664%2C1201.3452%2C1201.2861%2C1201.6497%2C1201.2550%2C1201.3898%2C1201.6418%2C1201.6576%2C1201.4746%2C1201.0423%2C1201.4136%2C1201.0246%2C1201.5471%2C1201.4002%2C1201.5680%2C1201.6489%2C1201.3761%2C1201.1221%2C1201.6522%2C1201.6612%2C1201.2002%2C1201.0244%2C1201.2428%2C1201.0513%2C1201.5627%2C1201.2463%2C1201.1798%2C1201.3846%2C1201.4306%2C1201.3396%2C1201.3030%2C1201.1817%2C1201.0306%2C1201.5775%2C1201.5697%2C1201.4700%2C1201.3337%2C1201.5221%2C1201.6114%2C1201.1035%2C1201.3230%2C1201.0510%2C1201.2115%2C1201.5901%2C1201.2343%2C1201.3819%2C1201.2460%2C1201.4309%2C1201.6189%2C1201.0192%2C1201.0430%2C1201.0228%2C1201.0760%2C1201.1183%2C1201.0615%2C1201.1770%2C1201.1895%2C1201.0006%2C1201.1418%2C1201.0575%2C1201.4513%2C1201.2870%2C1201.5693%2C1201.6394%2C1201.0986%2C1201.4810%2C1201.3451%2C1201.0588%2C1201.1250%2C1201.3540%2C1201.5273%2C1201.1691%2C1201.5000%2C1201.2902%2C1201.5012%2C1201.1044%2C1201.0690%2C1201.5842%2C1201.3757%2C1201.5609%2C1201.2911%2C1201.4963%2C1201.0403%2C1201.1740%2C1201.0601%2C1201.1727%2C1201.4033%2C1201.4218%2C1201.0024%2C1201.5073%2C1201.3332%2C1201.3546%2C1201.4827%2C1201.4463%2C1201.3347%2C1201.3507%2C1201.4792%2C1201.2175%2C1201.6539%2C1201.0313%2C1201.2883&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adaptive Policies for Sequential Sampling under Incomplete Information\n  and a Cost Constraint"}, "summary": "We consider the problem of sequential sampling from a finite number of\nindependent statistical populations to maximize the expected infinite horizon\naverage outcome per period, under a constraint that the expected average\nsampling cost does not exceed an upper bound. The outcome distributions are not\nknown. We construct a class of consistent adaptive policies, under which the\naverage outcome converges with probability 1 to the true value under complete\ninformation for all distributions with finite means. We also compare the rate\nof convergence for various policies in this class using simulation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.5664%2C1201.3452%2C1201.2861%2C1201.6497%2C1201.2550%2C1201.3898%2C1201.6418%2C1201.6576%2C1201.4746%2C1201.0423%2C1201.4136%2C1201.0246%2C1201.5471%2C1201.4002%2C1201.5680%2C1201.6489%2C1201.3761%2C1201.1221%2C1201.6522%2C1201.6612%2C1201.2002%2C1201.0244%2C1201.2428%2C1201.0513%2C1201.5627%2C1201.2463%2C1201.1798%2C1201.3846%2C1201.4306%2C1201.3396%2C1201.3030%2C1201.1817%2C1201.0306%2C1201.5775%2C1201.5697%2C1201.4700%2C1201.3337%2C1201.5221%2C1201.6114%2C1201.1035%2C1201.3230%2C1201.0510%2C1201.2115%2C1201.5901%2C1201.2343%2C1201.3819%2C1201.2460%2C1201.4309%2C1201.6189%2C1201.0192%2C1201.0430%2C1201.0228%2C1201.0760%2C1201.1183%2C1201.0615%2C1201.1770%2C1201.1895%2C1201.0006%2C1201.1418%2C1201.0575%2C1201.4513%2C1201.2870%2C1201.5693%2C1201.6394%2C1201.0986%2C1201.4810%2C1201.3451%2C1201.0588%2C1201.1250%2C1201.3540%2C1201.5273%2C1201.1691%2C1201.5000%2C1201.2902%2C1201.5012%2C1201.1044%2C1201.0690%2C1201.5842%2C1201.3757%2C1201.5609%2C1201.2911%2C1201.4963%2C1201.0403%2C1201.1740%2C1201.0601%2C1201.1727%2C1201.4033%2C1201.4218%2C1201.0024%2C1201.5073%2C1201.3332%2C1201.3546%2C1201.4827%2C1201.4463%2C1201.3347%2C1201.3507%2C1201.4792%2C1201.2175%2C1201.6539%2C1201.0313%2C1201.2883&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider the problem of sequential sampling from a finite number of\nindependent statistical populations to maximize the expected infinite horizon\naverage outcome per period, under a constraint that the expected average\nsampling cost does not exceed an upper bound. The outcome distributions are not\nknown. We construct a class of consistent adaptive policies, under which the\naverage outcome converges with probability 1 to the true value under complete\ninformation for all distributions with finite means. We also compare the rate\nof convergence for various policies in this class using simulation."}, "authors": ["Apostolos Burnetas", "Odysseas Kanavetas"], "author_detail": {"name": "Odysseas Kanavetas"}, "author": "Odysseas Kanavetas", "links": [{"href": "http://arxiv.org/abs/1201.4002v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1201.4002v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1201.4002v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1201.4002v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:1201.4002v1 [stat.ML] 19 Jan 2012\n\nAdaptive Policies for Sequential Sampling under\nIncomplete Information and a Cost Constraint\nApostolos Burnetas and Odysseas Kanavetas\nDepartment of Mathematics, University of Athens\nPanepistemiopolis, Athens 15784, Greece\n{aburnetas,okanav}@math.uoa.gr\nNovember 7, 2018\n\nAbstract\nWe consider the problem of sequential sampling from a finite number of independent statistical populations to maximize the expected\ninfinite horizon average outcome per period, under a constraint that\nthe expected average sampling cost does not exceed an upper bound.\nThe outcome distributions are not known. We construct a class of consistent adaptive policies, under which the average outcome converges\nwith probability 1 to the true value under complete information for all\ndistributions with finite means. We also compare the rate of convergence for various policies in this class using simulation.\n\n1\n\nIntroduction\n\nIn this paper we consider the problem of sequential sampling from k independent statistical populations with unknown distributions. The objective\nis to maximize the expected outcome per period achieved over infinite horizon, under a constraint that the expected sampling cost per period does not\nexceed an upper bound. The introduction of a sampling cost introduces a\nnew dimension in the standard tradeoff between experimentation and profit\nmaximization faced in problems of control under incomplete information.\nThe sampling cost may prohibit using populations with high mean outcomes because their sampling cost may be too high. Instead, the decision\nmaker must identify the subset of populations with the best combination\nof outcome versus cost and allocate the sampling effort among them in an\noptimal manner.\n1\n\n\fFrom the mathematical point of view, this class of problems incorporates statistical methodologies into mathematical programming problems.\nIndeed, under complete information, the problem of effort allocation under\ncost constraints is typically formulated in terms of linear or nonlinear programming. However when some of the problem parameters are not known in\nadvance but must be estimated by experimentation, the decision maker must\ndesign adaptive learning and control policies that ensure learning about the\nparameters while at the same time ensuring that the profit sacrificed for the\nlearning process is as low as possible.\nThe model in this paper falls in the general area of multi-armed bandit\nproblems, which was initiated by Robbins (1952), who proposed a simple\nadaptive policy for sequentially sampling from two unknown populations in\norder to maximize the expected outcome per unit time infinite horizon. Lai\nand Robbins (1985) generalize the results by constructing asymptotically\nefficient adaptive policies with optimal convergence rate of the average outcome to the optimal value under complete information and show that the\nfinite horizon loss due to incomplete information increases with logarithmic\nrate. Katehakis and Robbins (1995) prove that simpler index-based efficient\npolicies exist in the case of normal distributions with unknown means, while\nBurnetas and Katehakis (1996) extend the results on efficient policies in the\nnonparametric case of discrete distributions with known support.\nIn a finite horizon Kulkarni and Lugosi (2000) develop a minimax version\nof the Lai and Robbins (1985) results for two populations, while Auer et al.\n(2002) construct policies which also achieve logarithmic regret uniformly\nover time, rather than only asymptotically.\nIn all works mentioned above there is no side constraint in sampling.\nProblems with adaptive sampling and side constraints are scarce in the\nliterature. Wang (1991) considers a multi-armed bandit model with constraints and adopts a Bayesian formulation and the Gittins-index approach.\nThe paper proposes several heuristic policies. Pezeshk and Gittins (1999)\nalso consider the problem of estimating the distribution of a single population with sampling cost under the assumption that the number of users\nwho will benefit from the depends on the outcome of the estimation. Finally, Madani et al. (2004) present computational complexity analysis for\na version of the multi-armed bandit problem with Bernoulli outcomes and\nBeta priors, where there is a total budget for experimentation, which must\nbe allocated to sampling from the different populations.\nAnother approach, which is closer to the one we adopt here is to consider the family of stochastic approximations and reinforcement learning\nalgorithms. The general idea is to select the sampled population following\n2\n\n\fa randomized policy with randomization probabilities that are adaptively\nmodified after observing the outcome in each period. The adaptive scheme\nis based on the stochastic approximation algorithm. Algorithm of this type\nare analyzed in Poznyak et al. (2000) for the more general case where the\npopulation outcomes have Markovian dynamics instead of being i.i.d..\nThe contribution of this paper is the construction of a family of policies\nfor which the average outcome per period converges to the optimal value\nunder complete information for all distributions of individual populations\nwith finite means. In this sense, it generalizes the results of Robbins (1952)\nby including a sampling cost constraint. The paper organized as follows.\nIn Section 2, we describe the model in the complete and incomplete information framework. In Section 3, we construct a class of adaptive sampling\npolicies and prove that it is consistent. In Section 4, we explore the rate of\nconvergence of the proposed policies using simulation. Section 5 concludes.\n\n2\n\nModel description\n\nConsider the following problem in adaptive sampling. There are k independent statistical populations, i = 1, . . . , k. Successive samples from population i constitute a sequence of i.i.d. random variables Xi1 , Xi2 , . . . following\na univariate distribution with density fi (*) with respect to a nondegenerate\nmeasure v. Then the stochastic model is uniquely determined by the vector\nf = (f1 , . . . , fk ) of individual pdf's. Given f let \u03bc(f ) be the vector of expected values, i.e. \u03bci (f ) = E fi (Xi ). The form of f is not known. In each\nperiod the experimenter must select a population to obtain a single sample\nfrom. Sampling from population i incurs cost ci per sample and without loss\nof generality we assume c1 \u2264 c2 \u2264 . . . \u2264 ck , but not all equal. The objective is to maximize the expected average reward per period subject to the\nconstraint that the expected average sampling cost per period over infinite\nhorizon does not exceed a given upper bound C0 . Without loss of generality\nwe assume c1 \u2264 C0 < ck . Indeed if C0 < c1 then the problem is infeasible.\nOn the other hand if C0 \u2265 ck then the cost constraint is redundant. Let\nd = max{j : cj \u2264 C0 }. Then 1 \u2264 d < k and cd \u2264 C0 < cd+1 .\n\n2.1\n\nComplete information framework\n\nWe first analyze the complete information problem. If all fi (*) are known,\nthen the problem can be modeled via linear programming. Consider a randomized sampling policy which at each period selects population j with\n3\n\n\fprobability xj , for j = 1, . . . , k. To find a policy that maximizes the expected reward, we can formulate the following linear program in standard\nform\n\nz\n\n\u2217\n\n= max\n\nk\nX\n\n\u03bcj x j\n\nj=1\nk\nX\n\ncj xj + y = C0\n\n(1)\n\nj=1\nk\nX\n\nxj = 1\n\nj=1\n\nxj \u2265 0, \u2200j.\nNote that z \u2217 depends on f only through the vector \u03bc(f ), i.e. z \u2217 is the same\nfor all collections of pdf with the same \u03bc. Therefore in the remainder we\nwill denote z \u2217 as a function of the unknown mean vector \u03bc.\nIn the analysis we will also use the dual linear program (DLP) of (1),\n\u2217\nzD\n= min g + C0 \u03bb\n\ng + c1 \u03bb \u2265 \u03bc1\n..\n.\ng + ck \u03bb \u2265 \u03bck\ng \u2208 R, \u03bb \u2265 0,\nwith two variables \u03bb and g which correspond to the first and second constraints of (1), respectively.\nThe basic matrix B corresponding to a Basic Feasible Solution (BFS) of\nproblem (1) may take one of two forms:\nIn the first case, the basic variables are xi , xj , for two populations i, j, with\nci \u2264 C0 \u2264 cj , ci < cj , and the basic matrix is\n\u0012\n\u0013\nci cj\nB=\n.\n1 1\nThe BFS is then\nxi =\n\ncj \u2212 C0\nC0 \u2212 ci\n, xj =\n, and xm = 0 for m 6= i, j, y = 0,\nci \u2212 cj\nci \u2212 cj\n4\n\n\fwith\nz(x) = \u03bci xi + \u03bcj xj .\nThe solution is nondegenerate when ci < C0 < cj and degenerate when\nC0 = ci or C0 = cj . In the latter case, it corresponds to sampling from a\nsingle population l = i or l = j, respectively:\nxl = 1, xm = 0 \u2200m 6= l, y = 0,\nwith\nz(x) = \u03bcl .\nThe second case of a BFS corresponds to basic variables xi , y for a population i with ci \u2264 C0 . The basic matrix is\n\u0013\n\u0012\nci 1\n.\nB=\n1 0\nIn this case the BFS corresponds to sampling from population i only\nxi = 1, xm = 0 \u2200m 6= i, y = C0 \u2212 ci ,\nwith\nz(x) = \u03bci .\nThe solution is nondegenerate if ci < C0 , otherwise it is degenerate.\nFrom the above it follows that a BFS is degenerate if xl = 1 for some\nl with cl = C0 . Any basic matrix B that includes xl as a basic variable\ncorresponds to this BFS.\nFor a BFS x let\nb = {i : xi > 0}.\nThen, either b = {i, j} for some i, j with i \u2264 d \u2264 j, or b = {i} for some\ni \u2264 d. There is a one to one correspondence between basic feasible solutions\nand sets b of this form. We use K to denote the set of BFS, or equivalently\nK = {b : b = {i, j}, i \u2264 d \u2264 j or b = {i}, i \u2264 d}.\nSince the feasible region of (1) is bounded, K is finite.\nFor a basic matrix B, let v B = (\u03bbB , g B ) denote the dual vector corresponding to B, i.e., v B = \u03bcB B \u22121 , where \u03bcB = (\u03bci , \u03bcj ), or \u03bcB = (\u03bci , 0),\ndepending on the form of B.\n\n5\n\n\fRegarding optimality, a BFS is optimal if and only if for at least one\ncorresponding basic matrix B the reduced costs (dual slacks) are all nonnegative:\nB\nB\n\u03c6B\na \u2261 c\u03b1 \u03bb + g \u2212 \u03bc\u03b1 \u2265 0, \u03b1 = 1, . . . , k.\nA basic matrix B satisfying this condition is optimal. Note that if an\noptimal BFS is degenerate, then not all basic matrices corresponding to it\nare necessarily optimal.\nIt is easy to show that the reduced costs can be expressed as a linear\nB\nB\ncombinations \u03c6B\n\u03b1 = w \u03b1 \u03bc, where w \u03b1 is an appropriately defined vector that\ndoes not depend on \u03bc.\nWe finally define the set with optimal solutions of (1) for a \u03bc,\ns(\u03bc) = {b \u2208 K : b corresponds to an optimal BFS}.\nAn optimal solution of (1) specifies randomization probabilities that\nguarantee maximization of the average reward subject to the cost constraint.\nNote that an alternative way to implement the optimal solution, without\nrandomization, is to sample periodically from all populations so that the\nproportion of samples from each population j is equal to xj . This characterization of a policy is valid if randomization probabilities are rational.\n\n2.2\n\nIncomplete information framework\n\nIn this paper we assume that the population distributions are unknown.\nSpecifically we make the following assumption.\nAssumption 1 The outcome distributions are independent, and the expected values \u03bc\u03b1 = E(X\u03b1 ) < \u221e, \u03b1 = 1, . . . , k.\nLet F be the set of all f = (f1 , . . . , fk ) which satisfy A.1. Class F is\nthe effective parameter set in the incomplete information framework. Under\nincomplete information, a policy as that in Section 2.1, which depends on\nthe actual value of \u03bc, is not admissible. Instead we restrict our attention to\nthe class of adaptive policies, which depend only on the past observations\nof selections and outcomes.\nSpecifically, let At , Xt , t = 1, 2, ... denote the population selected and\nthe observed outcome at period t. Let ht = (\u03b11 , x1 , ...., \u03b1t\u22121 , xt\u22121 ) be the\nhistory of actions and observations available at period t.\nAn adaptive policy is defined as a sequence \u03c0 = (\u03c01 , \u03c02 , ...) of history\ndependent probability distributions on {1, ..., k}, such that\n6\n\n\f\u03c0t (j, ht ) = P (At = j|ht ).\nGiven the history hn , let Tn (\u03b1) denote the number of times population\n\u03b1 has been sampled during the first n periods\nTn (\u03b1) =\n\nn\nX\n\n1{At = \u03b1}.\n\nt=1\n\nLet Sn\u03c0 be the reward up to period n:\nSn\u03c0 =\n\nn\nX\n\nXt ,\n\nt=1\n\nand Cn\u03c0 be the total cost up to period n:\nCn\u03c0\n\n=\n\nn\nX\n\ncAt .\n\nt=1\n\nThese quantities can be used to define the desirable properties of an\nadaptive policy, namely feasibility and consistency.\nDefinition 1 A policy \u03c0 is called feasible if\nlim sup\nn\u2192\u221e\n\nE \u03c0 (Cn\u03c0 )\n\u2264 C0 , \u2200f \u2208 F.\nn\n\n(2)\n\nDefinition 2 A policy \u03c0 is called consistent if it is feasible and\nSn\u03c0\n= z \u2217 (\u03bc), a.s. \u2200f \u2208 F.\nn\u2192\u221e n\nlim\n\nLet \u03a0F and \u03a0C denote the class of feasible and consistent policies, respectively. The above properties are reasonable requirements for an adaptive\npolicy. The first ensures that the long-run average sampling cost does not\nexceed the budget. The second definition means that the long-run average\noutcome per period achieved by \u03c0 converges with probability one to the\noptimal expected value that could be achieved under full information, for\nall possible population distributions satisfying A.1.\nNote that consistency as defined in Definition 2 is equivalent to the\nnotion of strong consistency of an estimator function.\n\n7\n\n\f3\n\nConstruction of a consistent policy\n\nA key question in the incomplete information framework is whether feasible and, more importantly, consistent policies exist and how they can be\nconstructed.\nIt is very easy to show that feasible policies exist, since the sampling\ncosts are known. Indeed any randomized policy, such as those defined in\nSection 2.1, with randomization probabilities satisfying the constraints of\nLP (1) is feasible for any distribution f . Thus, \u03a0F 6= \u2205.\nOn the other hand, the construction of consistent policies is not trivial.\nA consistent policy must accomplish three goals: First to be feasible, second\nto be able to estimate the mean outcomes from all populations, and third,\nin the long-run, to sample from the nonoptimal populations rarely enough\nso as not to affect the average profit.\nIn this section we establish the existence of a class of consistent policies.\nThe construction follows the main idea of Robbins (1952), based on sparse\nsequences, which is adapted to ensure feasibility.\nWe start with some definitions. For any population j, let \u03bc\u0302j,t , t = 1, 2, . . .\nbe a strongly consistent estimator of \u03bcj , i.e. limt\u2192\u221e \u03bc\u0302j,t = \u03bcj a.s.-fj . Such\nestimators\nexist; for example from Assumption 1, the sample mean X j,t =\n1 Pt\nk=1 Xj,k is strongly consistent.\nt\nFor any n, let \u03bc\u0302n = (\u03bc\u0302j,Tj (n) , j = 1, . . . , k) be the vector estimates of \u03bc\nbased on the history up to period n. Also let \u1e91n = z(\u03bc\u0302n ) denote the optimal\nvalue of the linear program in (1) where the estimates are used in place of\nthe unknown mean vector in the objective. \u1e91n will be referred to as the\nCertainty-Equivalence LP. Note that s(\u03bc\u0302n ) is the set of optimal BFS of \u1e91n .\nThe solution of \u1e91n corresponds to a sampling policy determined by an\n0\noptimal vector x\u0302n , so that \u1e91n = \u03bc\u0302n x\u0302n .\nWe next define a class of sampling policies, which we will show to be\nconsistent. Consider k nonoverlapping sparse sequences of positive integers,\n\u03c4j = {\u03c4j,m , m = 1, 2, . . .}, j = 1, . . . , k,\nsuch that\nlim\n\nm\u2192\u221e\n\n\u03c4j,m\n= \u221e, j = 1, . . . , k.\nm\n\n(3)\n\nNow define policy \u03c0 0 which in period n selects any population j with\nprobability equal to\n\u001a\n1\n, if \u03c4j,m = n for some m \u2265 1\n0\n\u03c0 (j|hn ) =\nx\u0302n,j , otherwise\n8\n\n\fwhere x\u0302n is any optimal BFS of the certainty-equivalence LP \u1e91n .\nThe main idea in \u03c0 0 is that at periods which coincide with the terms\nof sequence \u03c4j , population j is selected regardless of the history. These\ninstances are referred to as forced selections of population j. The purpose\nof forced selections is to ensure that all populations are sampled infinitely\noften, so that the estimate vector \u03bc\u0302n converges to the true mean \u03bc as n \u2192 \u221e.\nOn the other hand, because sequences \u03c4j are sparse, the fraction of forced\nselections periods converges to zero for all j, so that sampling from the\nnonoptimal populations does not affect the average outcome in the longrun.\nIn the remaining time periods, which do not coincide with a sparse sequence term, the sampling policy is that suggested by the certainty equivalence LP, i.e., the experimenter in general randomizes between those populations, which, based on the observed history, appear to be optimal.\nIn the next theorem we prove the main result of the paper, namely that\n\u03c0 0 \u2208 \u03a0C . The proof adapts the main idea of Robbins (1952) to the problem\nwith the cost constraint.\nTheorem 1 Policy \u03c0 0 is consistent.\nBefore we show Theorem 1, we prove an intermediate result which shows\nthat if in some period the certainty equivalence LP yields an optimal solution\nthat is non-optimal under the true distribution f , then the estimate of at\nleast one population mean must be sufficiently different from the true value.\nWe use the supremum norm kxk = maxj |x|.\nLemma 1 For any \u03bc there exists \u000f > 0 such that for any n = 1, 2, . . . if\nb \u2208 s(\u03bc\u0302n ) and b \u2208\n/ s(\u03bc) for some b \u2208 K, then k\u03bc \u2212 \u03bc\u0302n k \u2265 \u000f.\nProof. Since b \u2208\n/ s(\u03bc), we have that for any basic matrix B 0 corresponding\n0\nto BFS b there exists at least one m \u2208 {1, . . . , k} such that \u03c6B\nm (\u03bc) < 0.\nTherefore,\n0\nB0\n\u2212 wB\n(4)\nm \u03bc = \u2212\u03c6m (\u03bc) > 0.\nIn addition, since b \u2208 s(\u03bc\u0302n ), there exists a basic matrix B corresponding\nto b, such for any m \u2208 {1, . . . , k} it is true that \u03c6B\nm (\u03bc\u0302n ) \u2265 0, thus,\nB\nwB\nm \u03bc\u0302n = \u03c6m (\u03bc\u0302n ) \u2265 0.\n\nFor this basic matrix B, it follows from (4) and (5) that\nB\nB\nB\nwB\nm \u03bc\u0302n \u2212 w m \u03bc \u2265 \u2212\u03c6m (\u03bc) = |\u03c6m (\u03bc)| > 0\n\n9\n\n(5)\n\n\fB\n\u21d2 wB\nm (\u03bc\u0302n \u2212 \u03bc) \u2265 |\u03c6m (\u03bc)|\nB\n\u21d2 kkwB\nm kk\u03bc\u0302n \u2212 \u03bck \u2265 |\u03c6m (\u03bc)|\n\n\u21d2 k\u03bc\u0302n \u2212 \u03bck \u2265\n\n|\u03c6B\nm (\u03bc)|\n,\nkkwB\nmk\n\nB\nbecause from the property wB\nm \u03bc < 0 it follows that kw m k > 0.\nNow let\n(\n)\n(\u03bc)|\n|\u03c6B\nm\n\u000f = min min min\n: \u03c6B\nm (\u03bc) < 0 > 0.\nB\u2208b m\u2208{1,...,k}\nkkwB\nk\nb\u2208K,b\u2208s(\u03bc)\n/\nm\n\nwhere the minimization over B \u2208 b is taken over all basic matrices corresponding to BFS b.\nThen k\u03bc\u0302n \u2212 \u03bck \u2265 \u000f.\nProof of Theorem 1.\nFor i = 1, . . . , k let\nSSi (n) =\n\nn\nX\n\n1{\u03c4i,m = t, for some m},\n\nt=1\n\ndenote the number of periods in {1, . . . , n} where a forced selection from\npopulation i is performed.\nAlso let,\n\nYjb (n)\n\n=\n\nn\nX\n\n1{b \u2208 s(\u03bc\u0302t ), b is used in period t, and j is sampled from,\n\nt=1\n\ndue to randomization in b}.\nX\nY b (n) =\nYjb (n),\nj\u2208b\n\nY (n) =\n\nX\n\nY b (n).\n\nb\u2208s(\u03bc)\n\nSince these include all possibilities of selection in a period, it is true that\n\n10\n\n\fn=\n\nk\nX\n\nSSi (n) +\n\ni=1\n\nX\n\nX\n\nY b (n) +\n\nb\u2208s(\u03bc)\n/\n\nY b (n).\n\nb\u2208s(\u03bc)\n\nNow let Wn denote the sum of outcomes in periods where true optimal\nBFS are used:\nWn =\n\nn\nX X\n\nXt * 1{b is used in period t}.\n\nb\u2208s(\u03bc) t=1\n\nTo show the theorem we will prove that\nSSi (n)\n= 0, i = 1, . . . , k\nn\nX Y b (n)\nlim\n= 0, a.s.,\nn\u2192\u221e\nn\nlim\n\nn\u2192\u221e\n\n(6)\n(7)\n\nb\u2208s(\u03bc)\n/\n\nlim\n\nn\u2192\u221e\n\nWn\n= z \u2217 (\u03bc), a.s..\nn\n\n(8)\n\nFirst, (6) holds since \u03c4i,m are sparse for all i. To show (7), in no forced\nselection periods, in order to sample from a BFS b it is necessary but not\nsufficient that b \u2208 s(\u03bc\u0302n ), thus\nb\n\nY (n) \u2264\n\nn\nX\n\n1{b \u2208 s(\u03bc\u0302t )}.\n\nt=1\n\n/ s(\u03bc), it follows from Lemma 1 that\nFor any b \u2208 s(\u03bc\u0302t ) and b \u2208\nk\u03bc\u0302t \u2212 \u03bck \u2265 \u000f.\nTherefore, for b \u2208\n/ s(\u03bc)\n\nb\n\nY (n) \u2264\n\u2264\n\nn\nX\nt=1\nn\nX\n\n1{b \u2208 s(\u03bc\u0302t )}\n1{|\u03bc\u0302t \u2212 \u03bck \u2265 \u000f}\n\nt=1\n\nthus,\n\n11\n\n\fn\n\nY b (n)\n1X\n1{k\u03bc\u0302t \u2212 \u03bck \u2265 \u000f} \u2192 0, n \u2192 \u221e, a.s.,\n\u2264\nn\nn\nt=1\n\nbecause \u03bc\u0302t \u2192 \u03bc, a.s., since \u03bc\u0302t is strongly consistent estimator, thus (7) holds.\nNow to show (8) we rewrite Wn as\nWn\nn\n\n=\n\nn\n1 X X\nXt * 1{b is used in period t}\nn\nb\u2208s(\u03bc) t=1\n\n=\n\nn\n1 X XX\nXt * 1{b is used in period t and j is sampled from}\nn\nb\u2208s(\u03bc) j\u2208b t=1\n\n=\n\n1 X X b\nYj (n) * X j,Y b (n)\nj\nn\nb\u2208s(\u03bc) j\u2208b\n\n=\n\nX Y b (n) X Yjb (n)\n*\n* X j,Y b (n) .\nj\nn\nY b (n)\nj\u2208b\n\nb\u2208s(\u03bc)\n\nFrom this expression it follows that\nWn\n\u2212 z\u2217 =\nn\n=\n\nX Y b (n) X Yjb (n)\n*\n* X j,Y b (n) \u2212 z \u2217\nj\nn\nY b (n)\nj\u2208b\n\nb\u2208s(\u03bc)\n\nX Y b (n)\n* znb \u2212 z \u2217 ,\nn\n\nb\u2208s(\u03bc)\n\nwhere znb =\n\nYjb (n)\nj\u2208b Y b (n)\n\nP\n\nSince Y (n) =\n\n* X j,Y b (n) .\nj\n\nb\nb\u2208s(\u03bc) Y (n), we have\n\nP\n\nWn\n\u2212 z\u2217 =\nn\n=\n\nX Y b (n)\nY (n) \u2217 Y (n) \u2217\n* znb \u2212 z \u2217 +\nz \u2212\nz\nn\nn\nn\n\nb\u2208s(\u03bc)\n\nX Y b (n)\nY (n) \u2217\n* (znb \u2212 z \u2217 ) \u2212 (1 \u2212\n)z .\nn\nn\n\nb\u2208s(\u03bc)\n\nTo show (8) we will prove that\n12\n\n\fY b (n)\nY (n)\n* (znb \u2212 z \u2217 ) \u2192 0 a.s. \u2200b \u2208 s(\u03bc), and\n\u2192 1, a.s..\nn\nn\nRandom variable Y b (n) is increasing in n and 0 \u2264 Y b (n) \u2264 n, thus either\nY b (n) \u2192 \u221e or Y b (n) \u2192 M for some M < \u221e. We define the following events:\nD = {Y b (n) \u2192 \u221e} and Dc = {Y b (n) \u2192 M }.\nNow let P (D) = p and P (Dc ) = 1 \u2212 p. Also let\nY b (n)\n* (znb \u2212 z \u2217 ) = 0}.\nn\u2192\u221e\nn\nThen P (A) = P (A|D) * p + P (A|Dc ) * (1 \u2212 p).\nNow,\nA = { lim\n\nY b (n)\n* (znb \u2212 z \u2217 ) = 0| lim Y b (n) = \u221e)\nn\u2192\u221e\nn\u2192\u221e\nn\nb\n\u2217\nb\n\u2265 P ( lim zn \u2212 z = 0| lim Y (n) = \u221e)\n\nP (A|D) = P ( lim\n\nn\u2192\u221e\n\nn\u2192\u221e\n\n= 1,\nfrom the strong law of large numbers, since\n\nY b (n)\nn\n\n\u2264 1 \u2200 n, and\n\nY b (n)\n* (znb \u2212 z \u2217 ) = 0| lim Y b (n) = M < \u221e) = 1,\nn\u2192\u221e\nn\u2192\u221e\nn\n\nP (A|Dc ) = P ( lim\n\nsince in this case znb \u2212 z \u2217 is bounded for any finite n.\nTherefore, P (A) = 1, thus\nY b (n)\n* (znb \u2212 z \u2217 ) \u2192 0, n \u2192 \u221e, a.s. , \u2200b \u2208 s(\u03bc).\nn\nFinally,\nn\nX Y b (n)\nX\nX Y b (n)\nY (n)\nSSi (n)\n=\n=1\u2212\n\u2212\n\u2192 1, a.s., n \u2192 \u221e.\nn\nn\nn\nn\nb\u2208s(\u03bc)\n\nt=1\n\nb\u2208s(\u03bc)\n/\n\nThus the proof of the theorem is complete.\n\u0004\n\n13\n\n\f4\n\nRate of Convergence - Simulations\n\nFrom the results of the previous section it follows that there exists significant flexibility in the construction of a consistent sampling policy. Indeed,\nany collection of sparse sequences of forced selection periods satisfying (3)\nguarantees that Theorem 1 holds.\nIn this section we refine the notion of consistency and examine how the\nrate of convergence of the average outcome to the optimal value is affected\nby different types of sparse sequences. Furthermore, since the sensitivity\nanalysis will be performed using simulation, it is more appropriate to use\nthe expected value of the deviation as the convergence criterion. We thus\nconsider the expected difference of the average outcome under a consistent\npolicy \u03c0 from the optimal value:\n\u0012\n\u0013\n\u03c0\n\u03c0 Wn\ndn (\u03bc) = E\n\u2212 z \u2217 (\u03bc).\nn\nNote that the almost sure convergence of Wnn to z \u2217 (\u03bc) proved in Theorem 1 does not imply convergence in expectation, unless further technical\nassumptions on the unknown distributions are made. For the purpose of\nour simulation study, we will further assume that the outcomes of any population are absolutely bounded with probability one, i.e., P (|Xj | \u2264 u) = 1,\nfor some u > 0. Under this assumption it is easy to show that Theorem 1\nimplies\nlim d\u03c0n (\u03bc) = 0,\n(9)\nn\u2192\u221e\n\nfor any consistent policy \u03c0 and any vector \u03bc.\nTo explore the rate of convergence in (9), we performed a simulation\nstudy, for a problem with k = 4 populations. The outcomes of population i follow binomial distribution with parameters (N, pi ), where p1 =\n0.3, p2 = 0.5, p3 = 0.9, p4 = 0.8. The vector of expected values is thus\n\u03bc = (1.5, 2.5, 4.5, 4). The cost vector is c = (3, 4, 8, 10) and C0 = 5. Under this set of values the optimal policy under incomplete information is\nx = (0, 3/4, 1/4, 0), y = 0 and z \u2217 (\u03bc) = 3, i.e., it is optimal to randomize\nbetween populations 2 and 3, the expected sampling cost per period is equal\nto 5 and the expected average reward per period is equal to 3.\nFor the above problem we simulated the performance of a consistent\npolicy for sparse sequences of power function form:\n{\u03c4j,m = `j + mb , m = 1, 2, . . . , }, j = 1, . . . , k,\n\n14\n\n\fAverage Reward\n\n3.0\n\n3.2\n\n3.4\n\n3.6\n\n\u03b1\n1.2\n1.5\n2\n3\n5\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n10000\n\nn\n\nFigure 1: Comparison of Convergence Rates for Power Sparse Sequences\nwhere `j are appropriately defined constants which ensure that the sequences\nare not overlapping, and the exponent parameter b is common for all populations. We compared the convergence rate in (9) for five values of b: (1.2,\n1.5, 2, 3, 5). For each value of b the corresponding policy was simulated for\n1000 scenarios of length n = 104 periods each, to obtain an estimate of the\nexpected average outcome per period d\u03c0n (\u03bc). The results of the simulations\nare presented in Figure 1.\nWe observe in Figure 1 that the convergence is slower both for small and\nlarge values of b and faster for intermediate values. Especially for b = 1.2 the\ndifference is relatively large even after 10000 periods. This is explained as follows. For small values of b the forced selections are more frequent. Although\nthis has the desirable effect that the mean estimates for all populations become accurate very soon, it also means that non-optimal populations are\n15\n\n\falso sampled frequently because of forced selections. As a result the average\noutcome may deviate from the true optimal value for a longer time period.\nOn the other hand, for large values of b the sequences \u03c4j all become very\nsparse and thus the forced selections are rare. In this case it takes a longer\ntime for the estimates to converge, and the linear programming problems\nmay produce non-optimal solutions for long intervals.\nIt follows from the above discussion that intermediate values of b are\ngenerally preferable, since they offer a better balance of the two effects, fast\nestimation of all mean values and avoiding non optimal populations. This\nis also evident in the graph, where the value b = 2 seems to be the best in\nterms of speed of convergence.\nTo address the question of accuracy of the comparison of convergence\nrates based on simulation, Figure 2 presents a 95% confidence region for\nthe average outcome curve corresponding to b = 2, based on 1000 simulated\nscenarios. The confidence region is generally very narrow (note that the\nvertical axes have different scale in the two figures), thus the estimate of\nthe expected average outcome is quite accurate. This is also the case for\nthe other curves, therefore the comparison of convergence rates is valid.\nFurthermore, the length of the confidence interval becomes smaller for larger\ntime periods since, as expected, the convergence to the true value is better\nfor longer scenario durations.\nAnother issue arising from Figure 1 is the following. For b = 1.2 the\naverage outcome converges very slowly to z \u2217 , but remains above it for the\nentire scenario duration. Thus it could be argued that, although the convergence is not good, this policy is actually preferable, because it yields higher\naverage outcomes than the other policies. It also seems to contradict the fact\nthat z \u2217 is the maximum average outcome under complete information, since\nthere is a sampling policy that even under incomplete information performs\nbetter.\nThe reason for this discrepancy is related to the form of the cost constraint (2). The constraint requires the infinite-horizon expected cost per\nperiod not to exceed Co . This does not preclude the possibility that one\nor more populations with large sampling costs and large expected outcomes\ncould be used for arbitrarily long intervals before switching to a constrainedoptimal policy for the remaining infinite horizon. Such policies might achieve\naverage rewards higher than z \u2217 for long intervals, however this is achieved\nby \"borrowing\", i.e., violating the cost constraint, also for long time periods.\nSince (2) is only required to hold in the limit, this behavior of a policy is\nallowed.\nAlthough the consistent policies in Section 3 are not designed specifically\n16\n\n\f3.005\n\nAverage Reward\n\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\n\u25cf\n\n3.000\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n2.995\n\n\u25cf\n\n2.985\n\n2.990\n\n\u25cf\n\n2.975\n\n2.980\n\n\u25cf\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n10000\n\nn\n\nFigure 2: Confidence Region for Average Outcome for b = 2\n.\nto take advantage of this observation, they are neither designed to avoid it.\nTherefore, it is possible, as it happens here for b = 2, that a consistent\npolicy may achieve higher than optimal average outcomes for long time\nperiods before it converges to z \u2217 .\nThe above discussion shows that the constraint as expressed in (2), may\nnot be appropriate, if for example the sampling cost is a tangible amount\nthat must be paid each time an observation is taken, and there is a budget\nC0 per period for sampling. In this situation a policy may suggest exceeding\nthe budget for long time periods and still be feasible, something that may\nnot be viable in reality. In such cases it would be more realistic to impose a\nstricter average cost constraint, for example to require that (2) hold for all\nn and not only in the limit.\n17\n\n\f5\n\nConclusion and Extensions\n\nIn this paper we developed a family of consistent adaptive policies for sequentially sampling from k independent populations with unknown distributions under an asymptotic average cost constraint. The main idea in the\ndevelopment of this class of policies is to employ a sparse sequence of forced\nselection periods for each population, to ensure consistent estimation of all\nunknown means and in the remaining time periods employ the solution obtained from a linear programming problem that uses the estimates instead\nof the true values. We also performed a simulation study to compare the\nconvergence rate for different policies in this class.\nThis work can be extended in several directions. First, as it was shown\nin Section 4, the asymptotic form of the cost constraint is in some sense\nweak, since it allows the average sampling cost to exceed the upper bound\nfor arbitrarily long time periods and still be satisfied in the limit. A more\nappropriate, albeit more complex, model would be to require the cost constraint to be satisfied at all time points. The construction of consistent\nand, more importantly, efficient policies under this stricter version of the\nconstraint is work currently in progress.\nAnother extension is towards the direction of Markov process control.\nInstead of assuming distinct independent populations with i.i.d. observations, one might consider an average reward Markovian Decision Process\nwith unknown transition law and/or reward distributions, and one or more\nnonasymptotic side constraints on the average cost. In this case the problem\nis to construct consistent and, more importantly, efficient control policies,\nextending the results of Burnetas and Katehakis (1997) in the constrained\ncase.\nAcknowledgement\nThis research was supported by the Greek Secreteriat of Research and\nTechnology under a Greece/Turkey bilateral research collaboration program.\nThe authors thank Nickos Papadatos and George Afendras for useful discussions on the problem of consistent estimation in a random sequence of\nrandom variables.\n\nReferences\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit. Machine Learning, 47:235\u2013256, 2002.\n\n18\n\n\fA. N. Burnetas and M. N. Katehakis. Optimal adaptive policies for sequential allocation problems. Adv. App. Math., 17:122\u2013142, 1996.\nA. N. Burnetas and M. N. Katehakis. Optimal adaptive policies for markovian decision processes. Math. Oper. Res., 22(1):222\u2013255, 1997.\nM. N. Katehakis and H. Robbins. Sequential choice from several populations.\nProc.Natl.Acad.Sci. USA, 92:8584\u20138585, 1995.\nS. R. Kulkarni and G. Lugosi. Finite-time lower bounds for the two-armed\nbandit problem. IEEE Transactions on Automatic Control, 45:711\u2013714,\n2000.\nT. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules.\nAdv. App. Math., 6:4\u201322, 1985.\nO. Madani, D. Lizotte, and R. Greiner. The budgeted multi-armed bandit\nproblem. Lecture Notes in Artificial Intelligence (Subseries of Lecture\nNotes in Computer Science), 3120:643\u2013645, 2004.\nH. Pezeshk and J. Gittins. Sample size determination in clinical trials.\nStudent, 3(1):19\u201326, 1999.\nA. Poznyak, K. Nazim, and E. Gomez. Self-Learning Control of Finite\nMarkov Chains. CRC Press, New York, 2000.\nH. Robbins. Some aspects of the sequential design of experiments. Bull.\nAmer. Math. Monthly, 58:527\u2013536, 1952.\nY. G. Wang. Gittins indices and constrained allocation in clinical trials.\nBiometrika, 78:101\u2013111, 1991.\n\n19\n\n\f"}