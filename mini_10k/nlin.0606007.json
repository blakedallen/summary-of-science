{"id": "http://arxiv.org/abs/nlin/0606007v3", "guidislink": true, "updated": "2009-04-29T09:29:12Z", "updated_parsed": [2009, 4, 29, 9, 29, 12, 2, 119, 0], "published": "2006-06-01T23:06:12Z", "published_parsed": [2006, 6, 1, 23, 6, 12, 3, 152, 0], "title": "Scale-invariant cellular automata and self-similar Petri nets", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=nlin%2F0211016%2Cnlin%2F0211005%2Cnlin%2F0211036%2Cnlin%2F0211026%2Cnlin%2F0211027%2Cnlin%2F0211018%2Cnlin%2F0211021%2Cnlin%2F0211009%2Cnlin%2F0203053%2Cnlin%2F0203002%2Cnlin%2F0203017%2Cnlin%2F0203038%2Cnlin%2F0203011%2Cnlin%2F0203040%2Cnlin%2F0203056%2Cnlin%2F0203063%2Cnlin%2F0203060%2Cnlin%2F0203054%2Cnlin%2F0203035%2Cnlin%2F0203052%2Cnlin%2F0203057%2Cnlin%2F0203019%2Cnlin%2F0203024%2Cnlin%2F0203021%2Cnlin%2F0203059%2Cnlin%2F0203016%2Cnlin%2F0203005%2Cnlin%2F0203020%2Cnlin%2F0203047%2Cnlin%2F0203036%2Cnlin%2F0203018%2Cnlin%2F0203013%2Cnlin%2F0203039%2Cnlin%2F0203025%2Cnlin%2F0203029%2Cnlin%2F0203004%2Cnlin%2F0203043%2Cnlin%2F0203022%2Cnlin%2F0203055%2Cnlin%2F0203044%2Cnlin%2F0203050%2Cnlin%2F0203023%2Cnlin%2F0203006%2Cnlin%2F0203061%2Cnlin%2F0203026%2Cnlin%2F0203037%2Cnlin%2F0203015%2Cnlin%2F0203058%2Cnlin%2F0203032%2Cnlin%2F0203045%2Cnlin%2F0203033%2Cnlin%2F0203030%2Cnlin%2F0203001%2Cnlin%2F0203014%2Cnlin%2F0203049%2Cnlin%2F0203007%2Cnlin%2F0203051%2Cnlin%2F0203046%2Cnlin%2F0203010%2Cnlin%2F0203031%2Cnlin%2F0203012%2Cnlin%2F0203062%2Cnlin%2F0203009%2Cnlin%2F0203028%2Cnlin%2F0203008%2Cnlin%2F0203027%2Cnlin%2F0203048%2Cnlin%2F0203003%2Cnlin%2F0203041%2Cnlin%2F0203042%2Cnlin%2F0606033%2Cnlin%2F0606026%2Cnlin%2F0606015%2Cnlin%2F0606032%2Cnlin%2F0606009%2Cnlin%2F0606054%2Cnlin%2F0606021%2Cnlin%2F0606072%2Cnlin%2F0606016%2Cnlin%2F0606061%2Cnlin%2F0606070%2Cnlin%2F0606030%2Cnlin%2F0606063%2Cnlin%2F0606031%2Cnlin%2F0606053%2Cnlin%2F0606051%2Cnlin%2F0606045%2Cnlin%2F0606064%2Cnlin%2F0606029%2Cnlin%2F0606040%2Cnlin%2F0606059%2Cnlin%2F0606008%2Cnlin%2F0606052%2Cnlin%2F0606049%2Cnlin%2F0606007%2Cnlin%2F0606039%2Cnlin%2F0606035%2Cnlin%2F0606014%2Cnlin%2F0606017%2Cnlin%2F0606006%2Cnlin%2F0606065&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Scale-invariant cellular automata and self-similar Petri nets"}, "summary": "Two novel computing models based on an infinite tessellation of space-time\nare introduced. They consist of recursively coupled primitive building blocks.\nThe first model is a scale-invariant generalization of cellular automata,\nwhereas the second one utilizes self-similar Petri nets. Both models are\ncapable of hypercomputations and can, for instance, \"solve\" the halting problem\nfor Turing machines. These two models are closely related, as they exhibit a\nstep-by-step equivalence for finite computations. On the other hand, they\ndiffer greatly for computations that involve an infinite number of building\nblocks: the first one shows indeterministic behavior whereas the second one\nhalts. Both models are capable of challenging our understanding of\ncomputability, causality, and space-time.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=nlin%2F0211016%2Cnlin%2F0211005%2Cnlin%2F0211036%2Cnlin%2F0211026%2Cnlin%2F0211027%2Cnlin%2F0211018%2Cnlin%2F0211021%2Cnlin%2F0211009%2Cnlin%2F0203053%2Cnlin%2F0203002%2Cnlin%2F0203017%2Cnlin%2F0203038%2Cnlin%2F0203011%2Cnlin%2F0203040%2Cnlin%2F0203056%2Cnlin%2F0203063%2Cnlin%2F0203060%2Cnlin%2F0203054%2Cnlin%2F0203035%2Cnlin%2F0203052%2Cnlin%2F0203057%2Cnlin%2F0203019%2Cnlin%2F0203024%2Cnlin%2F0203021%2Cnlin%2F0203059%2Cnlin%2F0203016%2Cnlin%2F0203005%2Cnlin%2F0203020%2Cnlin%2F0203047%2Cnlin%2F0203036%2Cnlin%2F0203018%2Cnlin%2F0203013%2Cnlin%2F0203039%2Cnlin%2F0203025%2Cnlin%2F0203029%2Cnlin%2F0203004%2Cnlin%2F0203043%2Cnlin%2F0203022%2Cnlin%2F0203055%2Cnlin%2F0203044%2Cnlin%2F0203050%2Cnlin%2F0203023%2Cnlin%2F0203006%2Cnlin%2F0203061%2Cnlin%2F0203026%2Cnlin%2F0203037%2Cnlin%2F0203015%2Cnlin%2F0203058%2Cnlin%2F0203032%2Cnlin%2F0203045%2Cnlin%2F0203033%2Cnlin%2F0203030%2Cnlin%2F0203001%2Cnlin%2F0203014%2Cnlin%2F0203049%2Cnlin%2F0203007%2Cnlin%2F0203051%2Cnlin%2F0203046%2Cnlin%2F0203010%2Cnlin%2F0203031%2Cnlin%2F0203012%2Cnlin%2F0203062%2Cnlin%2F0203009%2Cnlin%2F0203028%2Cnlin%2F0203008%2Cnlin%2F0203027%2Cnlin%2F0203048%2Cnlin%2F0203003%2Cnlin%2F0203041%2Cnlin%2F0203042%2Cnlin%2F0606033%2Cnlin%2F0606026%2Cnlin%2F0606015%2Cnlin%2F0606032%2Cnlin%2F0606009%2Cnlin%2F0606054%2Cnlin%2F0606021%2Cnlin%2F0606072%2Cnlin%2F0606016%2Cnlin%2F0606061%2Cnlin%2F0606070%2Cnlin%2F0606030%2Cnlin%2F0606063%2Cnlin%2F0606031%2Cnlin%2F0606053%2Cnlin%2F0606051%2Cnlin%2F0606045%2Cnlin%2F0606064%2Cnlin%2F0606029%2Cnlin%2F0606040%2Cnlin%2F0606059%2Cnlin%2F0606008%2Cnlin%2F0606052%2Cnlin%2F0606049%2Cnlin%2F0606007%2Cnlin%2F0606039%2Cnlin%2F0606035%2Cnlin%2F0606014%2Cnlin%2F0606017%2Cnlin%2F0606006%2Cnlin%2F0606065&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Two novel computing models based on an infinite tessellation of space-time\nare introduced. They consist of recursively coupled primitive building blocks.\nThe first model is a scale-invariant generalization of cellular automata,\nwhereas the second one utilizes self-similar Petri nets. Both models are\ncapable of hypercomputations and can, for instance, \"solve\" the halting problem\nfor Turing machines. These two models are closely related, as they exhibit a\nstep-by-step equivalence for finite computations. On the other hand, they\ndiffer greatly for computations that involve an infinite number of building\nblocks: the first one shows indeterministic behavior whereas the second one\nhalts. Both models are capable of challenging our understanding of\ncomputability, causality, and space-time."}, "authors": ["Martin Schaller", "Karl Svozil"], "author_detail": {"name": "Karl Svozil"}, "author": "Karl Svozil", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1140/epjb/e2009-00147-x", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/nlin/0606007v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/nlin/0606007v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "35 pages, 5 figures", "arxiv_primary_category": {"term": "nlin.CG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "nlin.CG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/nlin/0606007v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/nlin/0606007v3", "journal_reference": "The European Physical Journal B 69, 297-311 (2009)", "doi": "10.1140/epjb/e2009-00147-x", "fulltext": "Scale-Invariant Cellular Automata and Self-Similar Petri Nets\nMartin Schaller\u2217\nAlgorithmics, Parkring 10, 1010 Vienna, Austria\n\narXiv:nlin/0606007v3 [nlin.CG] 29 Apr 2009\n\nKarl Svozil\u2020\nInstitut f\u00fcr Theoretische Physik, University of Technology Vienna,\nWiedner Hauptstrasse 8-10/136, A-1040 Vienna, Austria\n\nAbstract\nTwo novel computing models based on an infinite tessellation of space-time are introduced.\nThey consist of recursively coupled primitive building blocks. The first model is a scale-invariant\ngeneralization of cellular automata, whereas the second one utilizes self-similar Petri nets. Both\nmodels are capable of hypercomputations and can, for instance, \"solve\" the halting problem for\nTuring machines. These two models are closely related, as they exhibit a step-by-step equivalence\nfor finite computations. On the other hand, they differ greatly for computations that involve an\ninfinite number of building blocks: the first one shows indeterministic behavior whereas the second\none halts. Both models are capable of challenging our understanding of computability, causality,\nand space-time.\nPACS numbers: 05.90.+m,02.90.+p,47.54.-r\nKeywords: Cellular Automata, pattern formation\n\n\u2217\n\nElectronic address: martin \u0307schaller@acm.org\n\n\u2020\n\nElectronic address: svozil@tuwien.ac.at; URL: http://tph.tuwien.ac.at/~svozil\n\n1\n\n\fI.\n\nINTRODUCTION\n\nEvery physically relevant computational model must be mapped into physical spacetime and vice versa [1, 2, 3]. In this line of thought, Von Neumann's self-reproducing\nCellular Automata [4] have been envisioned by Zuse [5] and other researchers [6, 7, 8, 9]\nas \"calculating space;\" i.e., as a locally connected grid of finite automata [10] capable of\nuniversal algorithmic tasks, in which intrinsic [11] observers are embedded [12]. This model\nis conceptually discreet and noncontinuous and resolves the eleatic \"arrow\" antinomy [13,\n14, 15, 16] against motion in discrete space by introducing the concept of information about\nthe state of motion in between time steps.\nAlas, there is no direct physical evidence supporting the assumption of a tessellation of\nconfiguration space or time. Given enough energy, and without the possible bound at the\nPlanck length of about 10\u221235 m, physical configuration space seems to be potentially infinitely\ndivisible. Indeed, infinite divisibility of space-time has been utilized for proposals of a kind\nof \"Zeno oracle\" [17], a progressively accelerated Turing machine [18, 19, 20] capable of\nhypercomputation [21, 22, 23]. Such accelerated Turing machines have also been discussed\nin the relativistic context [24, 25]. In general, a physical model capable of hypercomputation\nby some sort of \"Zeno squeezing\" has to cope with two seemingly contradictory features: on\nthe one hand, its infinite capacities could be seen as an obstacle of evolution and therefore\nrequire a careful analysis of the principal possibility of motion in finite space and time\nvia an infinity of cycles or stages. On the other hand, the same infinite capacities could\nbe perceived as an advantage, which might yield algorithms beyond the Turing bound of\nuniversal computation, thus extending the Church-Turing thesis.\nThe models presented in this article unify the connectional clarity of von Neumann's\nCellular Automaton model with the requirement of infinite divisibility of cell space. Informally speaking, the scale-invariant cellular automata presented \"contain\" a multitude of\n\"spatially\" and \"temporally\" ever decreasing copies of themselves, thereby using different\ntime scales at different layers of cells. The cells at different levels are also capable to communicate, i.e., exchange information, with these copies, resulting in ever smaller and faster\ncycling cells. The second model is based on Petri nets which can enlarge themselves.\nThe advantage over existing models of accelerated Turing machines - which are just\nTuring machines with a geometrically progression of accelerated time cycles - resides in\n2\n\n\fthe fact that the underlying computational medium is embedded into its environment in\na uniform and homogeneous way. In these new models, the entire universe, and not just\nspecially localized parts therein, is uniformly capable of the same computational capacities.\nThis uniformity of the computational environment could be perceived as one further step\ntowards the formalization of continuous physical systems [26] in algorithmic terms. In this\nrespects, the models seem to be closely related to classical continuum models, which are at\nleast in principle capable of unlimited divisibility and information flows at arbitrary small\nspace and time dimensions. At present however, for all practical purposes, there are finite\nbounds on divisibility and information flow.\nTo obtain a taste of some of the issues encountered in formalizing this approach, note\nthat an infinite sequence of ever smaller and faster cycling cells leads to the following situation. Informally speaking, let a self-similar cellular automaton be a variant of a onedimensional elementary cellular automaton, such that each cell is updated twice as often\nas its left neighbor. The cells of a self-similar cellular automaton can be enumerated as\n. . . , c\u22122 , c\u22121 , c0 , c1 , c2 , . . .. Starting at time 0 and choosing an appropriate time unit, cell ci is\nupdated at times 1/2i , 2/2i , 3/2i , . . .. Remarkably, this definition leads to indeterminism. To\nsee this, let s(i, t) be the state of cell i at time t. Now, the state s(0, 1) depends on s(1, 1/2),\nwhich itself depends on s(2, 1/22 ) and so on, leading to an infinite regress. In general, in\nanalogy to Thomson's paradox [16, 27], this results in an undefined or at least nonunique\nand thus indeterministic behavior of the automaton.\nThis fact relates to the following variant of Zeno's paradox of a runner, according to which\nthe runner cannot even get started [16]. He must first run to the half way point, but before\nthat he must run half way to the half way point and so on indefinitely. Whereas Zeno's\nrunner can find rescue in the limit of convergent real sequences, there is no such relieve for\nthe discrete systems considered.\nLater on, two restrictions on self-similar automata (build from scale-invariant cellular\nautomata) are presented, which are sufficient conditions for deterministic behavior, at least\nfor finite computations. Furthermore, a similar model based on a variant of Petri nets will\nbe introduced, that avoids indeterminism and halts in the infinite limit, thereby coming\nclose to the spirit of Zeno's paradox.\nThe article is organized as follows. Section II defines the Turing machine model used in the\nremainder of the article, and introduces two hypercomputing models: the accelerated and the\n3\n\n\fright-accelerated Turing machine. In section III self-similar as well as scale-invariant cellular\nautomata are presented. Section IV is devoted to the construction of a hypercomputer based\non self-similar cellular automata. There is a strong resemblance between this construction\nand the right-accelerated Turing machine, as defined in section II. A new computing model,\nthe self-similar Petri net is introduced in section V. This model features a step-to-step\nequivalence to self-similar cellular automata for finite computations, but halts in the infinite\ncase. The same construction as in section IV is used to demonstrate that self-similar Petri\nnets are capable of hypercomputation. The final section contains some concluding remarks\nand gives some directions for future research.\n\nII.\n\nTURING MACHINES AND ACCELERATED TURING MACHINES\n\nThe Turing machine is, beside other formal systems that are computationally equivalent,\nthe most powerful model of classical computing [28, 29, 30]. We use the following model of\na Turing machine [10].\nDefinition 1 (Turing Machine). Formally, a Turing machine is a tuple M\n\n=\n\n(Q, \u03a3, \u0393, \u03b4, q0 , B, F ), where Q is the finite set of states, \u0393 is the finite set of tape symbols, \u03a3 \u2282 \u0393 is the set of input symbols, q0 \u2208 Q is the start state, B \u2208 \u0393\\\u03a3 is the blank,\nand F \u2282 Q is the set of final states. The next move function or transition function \u03b4 is a\nmapping from Q \u00d7 \u0393 to Q \u00d7 \u0393 \u00d7 {L, R}, which may be undefined for some arguments.\nThe Turing machine M works on a tape divided into cells that has a leftmost cell but\nis infinite to the right. Let \u03b4(q, a) = (p, b, D). One step (or move) of M in state q and the\nhead of M positioned over input symbol a consists of the following actions: scanning input\nsymbol a, replacing symbol a by b, entering state p and moving the head one cell either to\nthe left (D = L) or to the right (D = R). In the beginning, M starts in state q0 with a\ntape that is initialized with an input word w \u2208 \u03a3\u2217 , starting at the leftmost cell, all other\ncells blank, and the head of M positioned over the first symbol of w. We need sometimes\nthe function \u03b4 split up into three separate functions: \u03b4(q, a) = (\u03b4Q (q, a), \u03b4\u0393 (q, a), \u03b4D (q, a)).\nThe configuration of a Turing machine M is denoted by a string of the form \u03b11 q\u03b12 , where\nq \u2208 Q and \u03b11 , \u03b12 \u2208 \u0393\u2217 . Here q is the current state of M, \u03b11 is the tape content to the left,\nand \u03b12 the tape content to the right of the head including the symbol that is scanned next.\n4\n\n\fLeading and trailing blanks will be omitted, except the head has moved to the left or to\nthe right of the non-blank content. Let \u03b11 q\u03b12 and \u03b11\u2032 p\u03b12\u2032 be two configurations of M. The\nrelation \u03b11 q\u03b12 \u22a2M \u03b11\u2032 p\u03b12\u2032 states that M with configuration \u03b11 q\u03b12 changes in one step to the\nconfiguration \u03b11\u2032 p\u03b12\u2032 . The relation \u22a2\u2217M denotes the reflexive and transitive closure of \u22a2M .\nThe original model of a Turing machine as introduced by Alan Turing contained no\nstatement about the time in which a step of the Turing machine has to be performed.\nIn classical computation, a \"yes/no\"-problem is therefore decidable if, for each problem\ninstance, the answer is obtained in a finite number of steps. Choosing an appropriate\ntime scheduling, the Turing machine can perform infinitely many steps in finite time, which\ntranscends classical computing, thereby leading to the following two hypercomputing models.\nThe concept of an accelerated Turing machine was independently proposed by Bertrand\nRussell, Ralph Blake, Hermann Weyl and others (see Refs. [20, 31]).\nDefinition 2 (Accelerated Turing machine). An accelerated Turing machine is a Turing\nmachine which performs the n-th step of a calculation in 1/2n units of time.\nThe first step is performed in time 1, and each subsequent step in half of the time before.\nSince 1 + 1/2 + 1/4 + 1/8 + . . . = 2, the accelerated Turing machine can perform infinitely\nmany steps in finite time. The accelerated Turing machine is a hypercomputer, since it\ncan, for example, solve the halting problem, see e.g., Ref. [20]. If the output operations\nare not carefully chosen, the state of a cell becomes indeterminate, leading to a variation of\nThomson's lamp paradox. The open question of the physical dynamics in the limit reduces\nthe physical plausibility of the model. The following model of a hypercomputing Turing\nmachine has a different time scheduling, thereby avoiding some of the paradoxes that might\narise from the previous one.\nDefinition 3 (Right-accelerated Turing machine). Let the cells of the tape be numbered\nfrom the left to the right c0 , c1 , c2 , . . .. A right-accelerated Turing machine is a Turing\nmachine that takes 1/2n units of time to perform a step that moves the head from cell cn\nto one of its neighbor cells.\nTheorem 1. There exists a right-accelerated Turing machine that is a hypercomputer.\nProof. Let MU be a universal Turing machine. We construct a Turing machine M U that\nalternates between simulating one step of MU and shifting over the tape content one cell to\n5\n\n\fthe right. We give a sketch of the construction, Ref. [10] contains a detailed description of\nthe used techniques. The tape of M U contains one additional track that is used to mark\nthe cell that is read next by the simulated MU . The finite control of M U is able to store\nsimultaneously the state of the head of MU as well as a tape symbol of MU . We assume that\nthe input of MU is surrounded by two special tape symbols, say $. At the start of a cycle,\nthe head of M U is initially positioned over the left delimiter $. M U scans the tape to the\nright, till it encounters a flag in the additional track that marks the head position of MU .\nAccessing the stored state of MU , M U simulates one step of MU thereby marking either the\nleft or the right neighbor cell as the cell that has to be visited next in the simulation of MU .\nIf necessary, a blank is inserted left to the right delimiter $, thereby extending the simulated\ntape of MU . Afterwards the head of M U moves to the right delimiter $ to start the shift\nover that is performed from the right to the left. M U repeatedly stores the symbols read in\nits finite control and prints them to the cell to the right. After the shift over, the head of\nMU is positioned over the left delimiter $ which finishes one cycle.\nWe now give an upper bound of the cycle time. Let n be the number of cells, from the\nfirst $ to the second one. Without loss of generality we assume that c0 contains the left\n$. M U scans from the left to the right and simulates one step of MU which might require\nto go an additional step to the left. If cell c1 is to be read next, the head of MU cannot\nmove to the right, otherwise it would fall off the tape of MU . Therefore the worst case\noccurs if the cell c2 is marked as cell that MU has to be read next. In this case we obtain\n1 + 1/2 + 1/4 + 1/2 + 1/4 + 1/8 + . . . + 1/2n\u22121 < 3. The head of M U is now either over cell\ncn\u22121 , or over cell cn if a insertion was performed. The shift over visits each cell ci , 1 \u2264 i < n\nthree times, and c0 two times. Therefore the following upper bound of the time of the shift\nover holds: 3(1 + 1/2 + 1/4 + . . . 1/2n ) < 6. We conclude that if the cycle started initally\nin cell cn it took less than time 9/2n . If MU halts on its input, M U finishes the simulation\nin a time less than 9(1 + 1/2 + 1/4 + . . .) = 18. M U therefore solves the halting problem of\nTuring machines. We remark that if MU does not halt, the head of M U vanishes in infinity,\nleaving a blank tape behind.\n\nQ.E.D.\n\nA right-accelerated Turing machine is, in contrast to the accelerated one, in control over\nthe acceleration. This can be used to transfer the result of a computation back to slower\ncells. The construction of an infinite machine, as proposed by Davies [19], comes close to the\n\n6\n\n\fmodel of a right-accelerated Turing machine, and his reasoning shows that a right-accelerated\nTuring machine could be build within a continuous Newtonian universe.\n\nIII.\nA.\n\nSELF-SIMILAR AND SCALE-INVARIANT CELLULAR AUTOMATA\nBasic definitions\n\nCellular automata are dynamical systems in which space and time are discreet. The states\nof cells in a regular lattice are updated synchronously according to a local deterministic\ninteraction rule. The rule gives the new state of each cell as a function of the old states of\nsome \"nearby\" neighbor cells. Each cell obeys the same rule, and has a finite (usually small)\nnumber of states. For a more comprehensive introduction to cellular automata, we refer to\nRefs. [4, 8, 32, 33, 34].\nA scale-invariant cellular automaton operates like an ordinary cellular automaton on a\ncellular space, consisting of a regular arrangement of cells, whereby each cell can hold a\nvalue from a finite set of states. Whereas the cellular space of a cellular automaton consists\nof a regular one- or higher dimensional lattice, a scale-invariant cellular automaton operates\non a cellular space of recursively nested lattices which can be embedded in some Euclidean\nspace as well.\nThe time behavior of a scale-invariant cellular automaton differs from the time behavior\nof a cellular automaton: Cells in the same lattice synchronously change their state [35], but\nas cells are getting smaller in deeper nested lattices, the time steps between state changes\nin the same lattice are assumed to decrease and approach zero in the limit. Thereby, a finite\nspeed of signal propagation between adjacent cells is always maintained. The scale-invariant\ncellular automaton model gains its computing capabilities by introducing a local rule that\nallows for interaction between adjacent lattices [36]. We will introduce the scale-invariant\ncellular automaton model for the one-dimensional case, the extension to higher dimensions\n[37] is straightforward.\nA scale-invariant cellular automaton, like a cellular automaton, is defined by a cellular\nspace, a topology that defines the neighborhood of a cell, a finite set of states a cell can be\nin, a time model that determines when a cell is updated, and a local rule that maps states of\nneighborhood cells to a state. We first define the cellular space of a scale-invariant cellular\n\n7\n\n\fc\n\nc\n\nc\n\nc\n\nc\n\nc\n\nFIG. 1: Space and topological structure of a scale-invariant cellular automaton.\n\nautomaton. To this end, we make use of standard interval arithmetic. For a scalar \u03bb \u2208 R\nand a (half-open) interval [x, y) \u2282 R set: \u03bb + [x, y) = [\u03bb + x, \u03bb + y) and \u03bb[x, y) = [\u03bbx, \u03bby).\nWe denote the unit interval [0, 1) by 1.\nDefinition 4 (Cellular Space and Space Operators). The cellular space C, the set of all\ncells of the scale-invariant cellular automaton, is the set C = {2k (i + 1)|i, k \u2208 Z}. The\nneighborhood of a cell c is determined by the following operators op : C \u2192 C. For a\ncell c = 2k (i + 1) in C let c\u2190 = 2k (i \u2212 1 + 1) be the left neighbor, c\u2192 = 2k (i + 1 + 1)\nthe right neighbor, c\u2191 = 2k+1 (\u230a 2i \u230b + 1) the parent, c\u0582 = 2k\u22121 (2i + 1) the left child, and\nc\u0581 = 2k\u22121 (2i + 1 + 1) the right child of c. The predicate left(c) is true if and only if the cell\nc is the left child of its parent.\nThe cellular space C is the union of all lattices Lk = {2k (i + 1)|i \u2208 Z}, where k is\nan integer. This topology is depicted in Fig. 1. For notational convenience, we introduce\na further operator, this time from C to C \u00d7 C, that maps a cell to its both child cells:\nc\u2193 = (c\u0582 , c\u0581 ). We remark that according to the last definition for each cell either left(c) or\n\u00acleft(c) is true. Later on, we will consider scale-invariant cellular automata where not each\ncell has a parent cell. If c = 2k (i + 1) is such a cell, we set by convention left(c) = 1 if i\nmod 2 = 0, otherwise left(c) = 0.\nAll cells in lattice Lk are updated synchronously at time instances 2k i where i is an integer.\nThe time interval between two cell updates in lattice Lk is again a half-open interval 2k (i+ 1)\nand the cycle time, that is the time between two updates of the cell, is therefore 2k . A simple\nconsequence of this time model is that child cells cycle twice as fast and the parent cell cycle\nhalf as fast as the cell itself.\nDefinition 5 (Time Scale and Time Operators). The time scale T is the set of all possible\n8\n\n\ft\n\nt'\n\nt\n\nt\n\nt\n\nt\n\nt'\n\nt'\n\nt'\n\nt'\n\nFIG. 2: Temporal dependencies of a scale-invariant cellular automaton.\n\ntime intervals, which is in the one-dimensional case equal to the set C: T = {2k (i + 1)|i, k \u2208\nZ}. The temporal dependencies of a cell are expressed by the following time operators\n\u230b + 1),\nop : T \u2192 T . For a time inverval t = 2k (i + 1) let t\u2190 = 2k (i \u2212 1 + 1), t\u2191 = 2k+1 (\u230a i\u22121\n2\nt\u0582 = 2k\u22121(2i \u2212 2 + 1), and t\u0581 = 2k\u22121 (2i \u2212 1 + 1). The predicate coupled (t) is true if and\nonly if the state change of a cell at the beginning of t occurs simultaneously with the state\nchange of its parent cell.\nThe usage of time intervals instead of time instances, has the advantage that a time\ninterval uniquely identifies the lattice where the update occurs. Fig. 2 depicts the temporal\ndependencies of a cell: to the left it shows a coupled state change, to the right an uncoupled\none. We remark that we denoted space and time operators by the same symbols, even if\ntheir mapping is different. In applying these operators, we take in the remainder of this\npaper care, that the context of the operator is always clearly defined.\nAt any time, each cell is in one state from a finite state set Z. The cell state in a given\ntime interval is described by the state function s(c, t), which maps cells and time intervals to\nthe state set. The space-time scale S of the scale-invariant cellular automaton describes the\nset of allowed pairs of cells and time intervals: S = {(c, t)|c \u2208 C, t \u2208 T and |c| = |t|}. Then,\nthe state function s can be expressed as a mapping s : S \u2192 Z. The local rule describes the\nevolution of the state function.\nDefinition 6 (Local Rule). For a cell c and a time interval t, where (c, t) is in S, the\nevolution of the state is given by the local rule f of the scale-invariant cellular automaton\ns(c, t) = f (s(c\u2191 , t\u2191 ), s(c\u2190 , t\u2190 ), s(c, t\u2190 ), s(c\u2192 , t\u2190 ), s(c\u2193 , t\u0582 ), s(c\u2193 , t\u0581 ), left (c), coupled (t))\n\n(1)\n\nIn accordance with the definition, the expanded form of a expression of the kind s(c\u2193 , t\u0582 )\nis (s(c\u0582 , t\u0582 ), s(c\u0581 , t\u0582 )). The local rule f is a mapping from Z 8 \u00d7 {0, 1}2 to Z. Beside the\n9\n\n\fdependencies on the states of the neighbor cells, the new state of the cell further depends on\nwhether the cell is the left or the right child of its parent cell and whether the state change\nis coupled or uncoupled to the state change of its parent cell. Formally, a scale-invariant\ncellular automaton A is denoted by the tuple A = (Z, f ). There are some simplifications\nof the local rule possible, if one allows for a larger state set. For instance, the values of\nthe predicates left and coupled could be stored as substate in the initial configuration. If\nthe local rule accordingly updates the value of coupled , the dependencies on the boolean\npredicates could be dropped from the local rule.\nAs noted in the introduction the application of the local rule in its general form might lead\nto indeterministic behavior. The next subsection introduces two restrictions of the general\nmodel that avoid indeterminism at least for finite computations. A special case of the local\nrule is a rule of the form f (s(c\u2190 , t\u2190 ), s(c, t\u2190 ), s(c\u2192 , t\u2190 )), which is the constituting rule of a\none-dimensional 3-neighborhood cellular automaton. In this case, the scale-invariant cellular\nautomaton splits up in a sequence of infinitely many nonconnected cellular automata. This\nshows that the scale-invariant cellular automaton model is truly an extension of the cellular\nautomaton model and allows us to view a scale-invariant cellular automaton as an infinite\nsequence of interconnected cellular automata.\nWe now examine the signal speed that is required to communicate state changes between\nneighbor cells. To this end, we select the middle point of a cell as the source and the target\nof a signal that propagates the state change of a cell to one of its neighbor cells. A simple\nconsideration shows that the most restricting cases are the paths from the space time points\n(c\u2190 , t\u2190 ), (c\u2191 , t\u2191 ), (c\u0582 , t\u0581 ) to (c, t) if not coupled (t). The simple calculation delivers the\nresults 1, 1, and 12 , respectively, hence a signal speed of 1 is sufficient to deliver the updates\nin the given timeframe. A more general examination takes also the processing time of a cell\ninto account. If a cell in Lk takes time 2k p to process their inputs and if we assume a finite\nsignal speed of v, the cycle time of a cell in Lk must be at least 2k (p + v). In sum, as long\nas the processing time is proportional to the diameter of a cell, we can always find a scaling\nfactor t \u2192 \u03bbt, such that the scale-invariant cellular automaton has cycle times that conform\nto the time scale T .\n\n10\n\n\fB.\n\nSelf-similar cellular automata and indeterminism\n\nThe construction of a hypercomputer in section IV uses a simplified version of a scaleinvariant cellular automaton, which we call a Self-similar Cellular Automaton.\nDefinition 7 (Self-similar Cellular Automaton). A self-similar cellular automaton has the\ncellular space C = {2k 1|k \u2208 Z}, the time scale T = {2k (i + 1)|i, k \u2208 Z}, and the finite state\nset Z. The space-time scale of a self-similar cellular automaton is the set S = {(c, t)|c \u2208\nC, t \u2208 T and |c| = |t|}. The self-similar cellular automaton has the following local rule: for\nall (c, t) \u2208 S\ns(c, t) = f (s(c\u2191 , t\u2191 ), s(c, t\u2190 ), s(c\u0582 , t\u0582 ), s(c\u0582 , t\u0581 ), coupled(t))\n\n(2)\n\nThe local rule f is a mapping from Z 4 \u00d7 {0, 1} to Z. Formally, a self-similar cellular\nautomaton A is denoted by a tuple A = (Z, f ). By restricting the local rule of a scaleinvariant cellular automaton, a self-similar cellular automaton can also be constructed from\na scale-invariant cellular automaton. Consider a scale-invariant cellular automaton whose\nlocal rule does not depend on the cell neighbors c\u2190 , c\u2192 , and c\u0581 . Then, the resulting scaleinvariant cellular automaton contains the self-similar cellular automaton as subautomaton.\nWe introduce the following notation for self-similar cellular automata. We index a cell\n[0, 2k ) by the integer \u2212k, that is a cell with index k has a cyle time of 2\u2212k . We call the cell\nk \u2212 1 the upper neighbor and the cell k + 1 the lower neighbor of cell k. Time instances\ncan be conveniently expressed as a binary number. If not stated otherwise, we use the cycle\ntime of cell 0 as time unit.\nWe noted already in the introduction that the evolution of a scale-invariant cellular\nautomaton might lead to indeterministic behavior. We offer two solutions, one based on a\nspecial quiescent state, the other one based on a dynamically growing lattice.\nDefinition 8 (Short-Circuit Evaluation). A state q in the state set Z is called a quiescent\nstate with regard to the short-circuit evaluation, if f (q, q, ?, ?, ?) = q, where the question\nmark sign \"?\" either represents an arbitrary state or a boolean value, depending on its\nposition. Whenever a cell is in state q, the cell does not access its lower neighbor.\nThe cell remains as long in the quiescent state as long as the upper neighbor is in the\nquiescent state, too. This modus of operandi corresponds to the short-circuit evaluation\n11\n\n\fof logical expressions in programming languages like C or Java. If the self-similar cellular\nautomaton starts in an initial configuration of the form z0 z1 . . . zn qqq . . . at cell 0, the infinite\nregress is interrupted, since cell n + 2 evolves to q without being dependent on cell n + 3.\nDefinition 9 (Dynamically growing self-similar cellular automaton). Let q be a state in the\nstate set Z, called the quiescent state. A dynamically growing self-similar cellular automaton\ninitially starts with the finite set of cells 0, . . . , n and the following boundary condition.\nWhenever cell 0 or the cell with the highest index k is evolved, the state of the missing\nneighbor cell is assumed to be q. The self-similar cellular automaton dynamically appends\ncells to the lower end when needed: whenever the cell with the highest index k enters a state\nthat is different from the quiescent state, a new cell k + 1 is appended, initialized with state\nq, and connected to the cell k. To be more specific: If k is the highest index, and cell k\nevolves at time 2\u2212k i to state z 6= q, a new cell k + 1 in state q is appended. The cell performs\nits first transition at time 2\u2212k (i + 1/2), assuming state q for its missing lower neighbor cell.\nWe note that the same technique could also be applied to append upper cells to the\nself-similar cellular automaton, although in the remainder of this paper we only deal with\nself-similar cellular automata which are growing to the bottom. Both enhancements ensure\na deterministic evaluation either for a configuration where only a finite number of cells is in\na nonquiescent state or for a finite number of cells.\nDefinition 10 (Finite and Final Configuration). A configuration of a self-similar cellular\nautomaton A is called finite if only a finite number of cells is different from the quiescent\nstate. Let C be a finite configuration and C \u2032 the next configuration in the evolution that is\ndifferent to C. C \u2032 is again finite. We denote this relationship by C \u22a2A C \u2032 . The relation \u22a2\u2217A\nis again the reflexive and transitive closure of \u22a2A .\nA self-similar cellular automaton as a scale-invariant cellular automaton cannot halt by\ndefinition and runs forever without stopping. The closest analogue to the Turing machine\nhalting occurs, when the configuration stays constant during evolution. Such a configuration\nthat does not change anymore is called final.\n\n12\n\n\fIV.\n\nCONSTRUCTING A HYPERCOMPUTER\n\nIn this section, we shall construct an accelerated Turing machine based on a self-similar\ncellular automaton. A self-similar cellular automaton which simulates the Turing machine\nM U specified in the proof of Theorem 1 in a step-by-step manner is a hypercomputer, since\nthe resulting Turing machine is a right-accelerated one. We give an alternative construction,\nwhere the shift over to the right is directly embedded in the local rule of the self-similar\ncellular automaton. The self-similar cellular automaton will simultaneously simulate the\nTuring machine and shift the tape content down to faster cycling cells. The advantages of\nthis construction are the smaller state set as well as a resulting faster simulation.\n\nA.\n\nSpecification\n\nLet M = (Q, \u03a3, \u0393, \u03b4, q0 , B, F ) be an arbitrary Turing machine. We construct a self-similar\ncellular automaton AM = (Z, f ) that simulates M as follows. First, we simplify the local\nrule by dropping the dependency on t\u0582 , obtaining\ns(c, t) = f (s(c\u2191 , t\u2191 ), s(c, t\u2190 ), s(c\u0582 , t\u0581 ), coupled(t)).\n\n(3)\n\nThe state set Z of AM is given by\n\u2192\n\u2212\nZ = \u0393 \u222a (\u0393 \u00d7 {\u2192}) \u222a (Q \u00d7 \u0393) \u222a (Q \u00d7 \u0393 \u00d7 {\u2192}) \u222a {\u0003, \u25ed, \u22b3, \u22b3 , \u22b2, \u22b2B , \u22b2\u25ed}.\n\u2192\nWe write \u2212\na for an element (a, \u2192) in \u0393 \u00d7 {\u2192}, hq, ai for an element (q, a) in Q \u00d7 \u0393, and\n\u2212\u2212\u2212\u2192\nhq, ai for an element (q, a, \u2192) in Q \u00d7 \u0393 \u00d7 {\u2192}. To simulate M on the input w = a1 . . . an\n\u2192\n\u2212\nin \u03a3\u2217 , n \u2265 1, AM is initialized with the sequence \u22b3 hq0 , a1 ia2 a3 . . . an \u22b2 starting at cell 0,\nall other cells shall be in the quiescent state \u0003. If w = a1 , AM is initialized with the\n\u2212\n\u2192\nsequence \u22b3 hq0 , a1 iB\u22b2, and if w = \u01eb, the empty word, AM is initialized with the sequence\n\u2192\n\u2212\n\u22b3 hq0 , BiB\u22b2. We denote the initial configuration by C0 , or by C0 (w) if we want to emphasize\nthe dependency on the input word w. The computation is started at time 0, i.e. the first\nstate change of cell k occurs at time 2\u2212k .\n\u2212\u2212\u2212\u2192\nThe elements hq, ai and hq, ai act as head of the Turing machine including the input\nsymbol of the Turing machine that is scanned next. To accelerate the Turing machine,\nwe have to shift down the tape content to faster cycling cells of the self-similar cellular\nautomaton, thereby taking care that the symbols that represent the non-blank content of\n13\n\n\fthe Turing machine tape are kept together. We achieve this by sending a pulse, which is just\na symbol from a subset of the state set, from the left delimiter \u22b3 to the right delimiter \u22b2\nand back. Each zigzag of the pulse moves the tape content one cell downwards and triggers\nat least one move of the Turing machine. Furthermore a blank is inserted to the right of the\nsimulated head if necessary. The pulse that goes down is represented by exactly one element\n\u2192 \u2192 \u2212\u2212\u2212\u2192\n\u2212\nof the form \u22b3 , \u2212\na , hq, ai, \u22b2B , or \u22b2\u25ed, the upgoing pulse is represented by the element \u25ed.\nThe specification of the values for the local rule f for all possible arguments is tedious,\ntherefore we use the following approach. A coupled transition of two neighbor cells can\nperform a simultaneous state change of the two cells. If the state changes of these two\nneighbor cells is independent of their other neighbors, we can specify the state changes as\na transformation of a state pair into another one. Let z1 , z2 , z1\u2032 , z2\u2032 be elements in Z. We\ncall a mapping of the form z1 z2 7\u2192 z1\u2032 z2\u2032 a block transformation. The block transformation\nz1 z2 7\u2192 z1\u2032 z2\u2032 defines a function mapping of the form f (x, z1 , z2 , 0) = f (x, z1 , z2 , 1) = z1\u2032 and\nf (z1 , z2 , y, 1) = z2\u2032 for all x, y in Z. Furthermore, we will also allow block transformations\nthat might be ambiguous for certain configurations. Consider the block transformations\nz1 z2 7\u2192 z1\u2032 z2\u2032 and z2 z3 7\u2192 z2\u2032\u2032 z3\u2032 that might lead to an ambiguity for a configuration that\ncontains z1 z2 z3 . Instead of resolving these ambiguities in a formal way, we will restrict our\nconsideration to configurations that are unambiguous.\nThe evolution of the self-similar cellular automaton AM is governed by the following block\ntransformations:\n1. Pulse moves downwards. Set\n\u2212\u2212\u2212\u2192\n\u2212\n\u2192\n\u22b3 hq, ai 7\u2192 \u22b3 hq, ai;\n\n(4)\n\n\u2192\n\u2212\n\u2212\n\u2192\na b 7\u2192 a b ;\n\n(5)\n\n\u2212\n\u2192\n\u2192\n\u22b3 a 7\u2192 \u22b3 \u2212\na.\n\n(6)\n\nIf \u03b4(q, a) = (p, c, R) set\n\u2212\u2212\u2212\u2192\n\u2212\n\u2192\nb hq, ai 7\u2192 b hq, ai;\n\u2212\u2212\u2212\u2192\n\u2212\u2212\u2212\u2192\nhq, ai b 7\u2192 c hp, bi;\n\u2212\u2212\u2212\u2192\nhq, ai \u22b2 7\u2192 hq, ai \u22b2B .\n\n(9)\n\n\u2212\n\u2192\n\u2192\nb hq, ai 7\u2192 hp, bi \u2212\nc;\n\n(10)\n\n(7)\n(8)\n\nIf \u03b4(q, a) = (p, c, L) set\n\n14\n\n\fSymbol\nState\n0\n1\nX\nq0 (q1 , X, R)\n-\n-\nq1 (q1 , 0, R) (q2 , Y, L)\n-\nq2 (q2 , 0, L)\n-\n(q0 , X, R)\nq3\n-\n-\n-\nq4\n-\n-\n-\n\nY\nB\n(q3 , Y, R)\n-\n(q1 , Y, R)\n-\n(q2 , Y, L)\n-\n(q3 , Y, R) (q4 , B, R)\n-\n-\n\nFIG. 3: The function \u03b4.\n\n\u2212\u2212\u2212\u2192\n\u2192\n\u2212\nhq, ai b 7\u2192 hq, ai b ;\n\n(11)\n\n\u2212\u2212\u2212\u2192\nhq, ai \u22b2 7\u2192 hq, ai \u22b2\u25ed .\n\n(12)\n\n\u2212\n\u2192\na \u22b2 7\u2192 a \u22b2\u25ed;\n\n(13)\n\n\u22b2B \u0003 7\u2192 B \u22b2\u25ed;\n\n(14)\n\n\u22b2\u25ed \u0003 7\u2192\u25ed \u22b2.\n\n(15)\n\na \u25ed7\u2192\u25ed a;\n\n(16)\n\nhq, ai \u25ed7\u2192\u25ed hq, ai;\n\n(17)\n\n\u2192\n\u2212\n\u22b3 \u25ed7\u2192 \u0003 \u22b3 .\n\n(18)\n\nSet\n\n2. Pulse moves upwards. Set\n\nIf to a certain cell no block transformation is applicable the cell shall remain in its previous\nstate. Furthermore, we assume a short-circuit evaluation with regard to the quiescent state:\nf (\u0003, \u0003, ?, ?) = \u0003, whereby the lower neighbor cell is not accessed.\n\nB.\n\nExample\n\nWe illustrate the working of AM by a simple example. Let L be the formal language\nconsisting of strings with n 0's, followed by n 1's: L = {0n 1n |n \u2265 1}. A Turing machine that\naccepts this language is given by M = ({q0 , q1 , q2 , q3 , q4 }, {0, 1}, {0, 1, X, Y, B}, \u03b4, q0, B, {q4 })\n[10] with the transition function depicted in Fig. 3. Note that L is a context-free language,\n\n15\n\n\fbut M will serve for demonstration purposes. The computation of M on input 01 is given\nbelow:\nq0 01 \u22a2 Xq1 1 \u22a2 q2 XY \u22a2 Xq0 Y \u22a2 XY q3 \u22a2 XY Bq4 .\nFig. 4 depicts the computation of AM on the Turing machine input 01. The first column\nof the table specifies the time in binary base. AM performs 4 complete pulse zigzags and\nenters a final configuration in the fifth one after the Turing machine simulation has reached\nthe final state q4 . Fig. 5 depicts the space-time diagram of the computation. It shows the\nposition of the left and right delimiter (gray) and the position of the pulse (black).\n\nC.\n\nProof\n\nWe split the proof that AM is a hypercomputer into several steps. We first show that the\nblock transformations are well-defined and the pulse is preserved during evolution. Afterwards we will prove that AM simulates M correctly and we will show that AM represents\nan accelerating Turing machine.\n\u2212\u2212\u2212\u2192\n\u2192\n\u2212\n\u2192\nLet D = { \u22b3 , \u22b2B , \u22b2\u25ed, \u2212\na , hq, ai} be the set of elements that represent the downgoing\npulse, U = {\u25ed} be the singleton that contains the upgoing pulse, P = D \u222a U, and R = Z\\P\nthe remaining elements. The following lemma states that the block transformations are\nunambiguous for the set of configurations we consider and that the pulse is preserved during\nevolution.\nLemma 1. If the finite configuration C contains exactly one element of P then the application of the block transformations 4 \u2013 18 is unambiguous and at most one block transformation\nis applicable. If a configuration C \u2032 with C \u22a2AM C \u2032 exists, then C \u2032 contains exactly one element of P as well.\nProof. Note that the domains of all block transformations are pairwise disjoint. This ensures\nthat for all pairs z1 z2 in Z \u00d7 Z at most one block transformation is applicable. Block\ntransformations 4 \u2013 14 are all subsets or elements of (D \u00d7R)\u00d7(R\u00d7D), block transformation\n15 is element of (D \u00d7 R) \u00d7 (U \u00d7 R), block transformations 16 and 17 are subsets of (R \u00d7\nU) \u00d7 (U \u00d7 R), and finally block transformation 18 is element of (R \u00d7 U) \u00d7 (R \u00d7 D). Since the\ndomain is either a subset of D \u00d7 R or R \u00d7 U the block transformations are unambiguous if\nC contains at most one element of P . A configuration C \u2032 with C \u22a2AM C \u2032 must be the result\n16\n\n\f0\n1\n\u2212\n\u2192\n0.000000002 \u22b3 hq0 , 0i\n\u2212\u2212\u2212\u2192\n1.000000002 \u22b3 hq0 , 0i\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\nX\n\n1\n\u2212\u2212\u2212\u2192\nhq1 , 1i\n\n\u22b2\n\n1.100000002 \u22b3\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\u0003\n\n1.110000002 \u22b3\n\nX\n\nhq1 , 1i\n\n\u22b2\u25ed\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n1.111000002 \u22b3\n\nX\n\nhq1 , 1i\n\n\u25ed\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n10.000000002 \u22b3\n\nX\n\n\u25ed\n\nhq1 , 1i\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n10.100000002 \u22b3\n11.000000002 \u0003\n\n\u25ed\n\u2212\n\u2192\n\u22b3\n\n11.100000002 \u0003\n11.110000002 \u0003\n\nX\n\nhq1 , 1i\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\nhq1 , 1i\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\nX\n\u2212\n\u2192\nX\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\nhq2 , Xi\n\nhq1 , 1i\n\u2212\n\u2192\nY\n\n\u22b2\n\n\u22b3\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n11.111000002 \u0003\n\n\u22b3\n\nhq2 , Xi\n\nY\n\n\u22b2\u25ed\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n11.111100002 \u0003\n\n\u22b3\n\nhq2 , Xi\n\nY\n\n\u25ed\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n100.000000002 \u0003\n\n\u22b3\n\nhq2 , Xi\n\n\u25ed\n\nY\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n100.010000002 \u0003\n\n\u22b3\n\nY\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u25ed\n\u2212\n\u2192\n\u22b3\n\nhq2 , Xi\n\n100.100000002 \u0003\n\nY\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n100.110000002 \u0003\n\n\u0003\n\n\u22b3\n\nhq2 , Xi\n\u2212\u2212\u2212\u2212\u2192\nhq2 , Xi\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n100.111000002 \u0003\n\n\u0003\n\n\u22b3\n\nX\n\nY\n\u2212\u2212\u2212\u2212\u2192\nhq0 , Y i\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n100.111100002 \u0003\n\n\u0003\n\n\u22b3\n\nX\n\nhq0 , Y i\n\n\u22b2B\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n100.111110002 \u0003\n\n\u0003\n\n\u22b3\n\nX\n\nhq0 , Y i\n\nB\n\n\u22b2\u25ed\n\n\u0003\n\n\u0003\n\n\u0003\n\n100.111111002 \u0003\n\n\u0003\n\n\u22b3\n\nX\n\nhq0 , Y i\n\nB\n\n\u25ed\n\n\u22b2\n\n\u0003\n\n\u0003\n\n101.000000002 \u0003\n\n\u0003\n\n\u22b3\n\nX\n\nhq0 , Y i\n\n\u25ed\n\nB\n\n\u22b2\n\n\u0003\n\n\u0003\n\n101.000100002 \u0003\n\n\u0003\n\n\u22b3\n\nX\n\n\u25ed\n\nhq0 , Y i\n\nB\n\n\u22b2\n\n\u0003\n\n\u0003\n\n101.001000002 \u0003\n\n\u0003\n\n\u22b3\n\nX\n\nhq0 , Y i\n\nB\n\n\u22b2\n\n\u0003\n\n\u0003\n\n101.010000002 \u0003\n\n\u0003\n\n\u0003\n\n\u25ed\n\u2212\n\u2192\n\u22b3\n\n101.011000002 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\nX\n\u2212\n\u2192\nX\n\n101.011100002 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\n101.011110002 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\n101.011111002 \u0003\n\n\u0003\n\n\u0003\n\nhq0 , Y i\n\nB\n\n\u22b2\n\n\u0003\n\n\u0003\n\nB\n\n\u22b2\n\n\u0003\n\n\u0003\n\nX\n\nhq0 , Y i\n\u2212\u2212\u2212\u2212\u2192\nhq0 , Y i\n\n\u0003\n\n\u0003\n\nY\n\nB\n\u2212\u2212\u2212\u2212\u2192\nhq3 , Bi\n\n\u22b2\n\nX\n\n\u22b2\n\n\u0003\n\n\u0003\n\n\u22b3\n\nX\n\nY\n\nhq3 , Bi\n\n\u22b2B\n\n\u0003\n\n\u0003\n\n101.011111102 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\nX\n\nY\n\nhq3 , Bi\n\nB\n\n\u22b2\u25ed\n\n\u0003\n\n101.011111112 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\nX\n\nY\n\nhq3 , Bi\n\nB\n\n\u25ed\n\n\u22b2\n\n101.100000002 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\nX\n\nY\n\nhq3 , Bi\n\n\u25ed\n\nB\n\n\u22b2\n\n101.100001002 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\nX\n\nY\n\n\u25ed\n\nhq3 , Bi\n\nB\n\n\u22b2\n\n101.100010002 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\nX\n\n\u25ed\n\nY\n\nhq3 , Bi\n\nB\n\n\u22b2\n\n101.100100002 \u0003\n\n\u0003\n\n\u0003\n\n\u22b3\n\n101.101000002 \u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n\u25ed\n\u2212\n\u2192\n\u22b3\n\n101.101100002 \u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n101.101110002 \u0003\n\n\u0003\n\n\u0003\n\n\u0003\n\n101.101111002 \u0003\n\n\u0003\n\n\u0003\n\n101.101111102 \u0003\n\n\u0003\n\n\u0003\n\nX\n\nY\n\nhq3 , Bi\n\nB\n\n\u22b2\n\nY\n\nhq3 , Bi\n\nB\n\n\u22b2\n\n\u22b3\n\nX\n\u2212\n\u2192\nX\n\n\u22b3\n\nX\n\nY\n\u2212\n\u2192\nY\n\n\u0003\n\n\u22b3\n\nX\n\n\u0003\n\n\u22b3\n\nX\n\nhq3 , Bi\n\nB\n\n\u22b2\n\nB\n\n\u22b2\n\nY\n\nhq3 , Bi\n\u2212\u2212\u2212\u2212\u2192\nhq3 , Bi\n\nY\n\nB\n\nB\n\u22b2\n\u2212\u2212\u2212\u2212\u2192\nhq4 , Bi \u22b2\n\nFIG. 4: A computation of AM on input 01.\n\nof the application of exactly one block transformation. Since each block transformation\npreserves the pulse, C \u2032 contains one pulse if and only if C contains one.\n\nQ.E.D.\n\nWe introduce a mapping \u03b3 that aims to decode a self-similar cellular automaton configuration into a Turing machine configuration. Let C be a finite configuration. Then \u03b3(C) is\n\n17\n\n\fFIG. 5: Space-time diagram of the computation of AM on input 01.\n\nthe string in (\u0393 \u222a Q)\u2217 that is formed of C as following:\n\u2192\n\u2212\n1. All elements in {\u0003, \u25ed, \u22b3, \u22b3 , \u22b2, \u22b2B , \u22b2\u25ed} are omitted.\n\u2192\n2. All elements of the form \u2212\na are replaced by a and all elements of the form hq, ai or\n\u2212\u2212\u2212\u2192\nhq, ai are replaced by the two symbols q and a.\n3. All other elements of the form a are added as they are.\n4. Leading or trailing blanks of the resulting string are omitted.\nThe following lemma states that AM correctly simulates M.\nLemma 2. Let c1 , c2 be configurations of M. If c1 \u22a2\u2217M c2 , then there exist two finite\nconfigurations C1 , C2 of AM such that \u03b3(C1 ) = c1 , \u03b3(C2 ) = c2 , and C1 \u22a2\u2217AM C2 . Especially if\nthe initial configuration C0 of AM satisfies \u03b3(C0 ) = c1 , then there exists a finite configuration\nC2 of AM , such that \u03b3(C2 ) = c2 and C0 \u22a2\u2217AM C2 .\nProof. If c1 has the form a1 . . . an q we consider without loss of generality a1 . . . an qB.\nTherefore let c1 = a1 . . . ai\u22121 qai . . . an . If i < n or i = n and \u03b4D (q, an ) = L we choose\n\u2192\n\u2212\nC1 = \u22b3 a1 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2. If i = n and \u03b4D (q, an ) = R we insert an additional\n\u2192\n\u2212\nblank: C1 = \u22b3 a1 . . . an\u22121 hq, an iB\u22b2. In any case \u03b3(C1 ) = c1 holds. We show the correctness\nof the simulation by calculating a complete zigzag of the pulse for the start configuration:\n\u2192\n\u2212\n\u22b3 a1 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2. The number of the block transformation that is applied, is\nwritten above the derivation symbol. We split the zigzag up into three phases.\n1. Pulse moves down from the left delimiter to the left neighbor cell of the simulated\nhead.\n\n18\n\n\fFor i > 1 we obtain\n(6)\n(5)\n\u2212\n\u2192\n\u2192\n\u22b3 a1 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2 \u22a2AM \u22b3\u2212\na1 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2 \u22a2AM\n(5)\n(5)\n\u2192\n\u2192\n\u22b3a1 \u2212\na2 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2 \u22a2AM . . . \u22a2AM \u22b3a1 . . . \u2212\na\u2212\ni\u22121 hq, ai iai+1 . . . an \u22b2 .\n\n(19)\n\n\u2192\n\u2212\nIf i = 1 the pulse piggybacked by the left delimiter \u22b3 is already in the left neighbor\ncell of the head and this phase is omitted.\n2. Downgoing pulse passes the head.\nIf in the beginning of the zigzag the head was to the right of the left delimiter then\n(4)\n\u2212\u2212\u2212\u2192\n\u2212\n\u2192\n\u22b3 hq, a1 ia2 . . . an \u22b2 \u22a2AM \u22b3hq, a1 ia2 . . . an \u22b2 .\n\n(20)\n\nIf \u03b4D (q, a1 ) = L no further block transformation is applicable and the configuration\nis final. The case \u03b4D (q, a1 ) = R will be handled later on. We now continue the\nderivation 19. If \u03b4(q, ai ) = (p, b, L) then\n(10)\n\u2192\n\u2212\n\u2192\n\u22b3 a1 . . . \u2212\na\u2212\ni\u22121 hq, ai iai+1 . . . an \u22b2 \u22a2AM \u22b3a1 . . . hp, ai\u22121 i b ai+1 . . . an \u22b2 .\n\n(21)\n\nIf \u03b4(q, ai ) = (p, b, R) then\n(7)\n\u2212\u2212\u2212\u2192\n\u2192hq, a ia . . . a \u22b2 \u22a2\n\u22b3 a1 . . . \u2212\na\u2212\ni\u22121\ni i+1\nn\nAM \u22b3a1 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2 .\n\n(22)\n\nWe distinguish two cases: i < n and i = n. If i < n then\n(8)\n\u2212\u2212\u2212\u2192\n\u2212\u2212\u2212\u2212\u2212\u2192\n\u22b3 a1 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2 \u22a2AM \u22b3a1 . . . ai\u22121 bhp, ai+1 iai+2 . . . an \u22b2 .\n\n(23)\n\nIf the next steps of M are moving the head again to the right, block transformation\n8 will repeatedly applied, till the head changes its direction or till the head is left of\nthe right delimiter \u22b2. If the Turing machine M changes its direction before the right\ndelimiter is reached, we obtain\n(11)\n\u2212\u2212\u2212\u2192\n\u2192\n\u22b3a1 . . . ai\u22121 b1 . . . bj hr, ak iak+1 . . . an \u22b2 \u22a2AM \u22b3a1 . . . ai\u22121 b1 . . . bj hr, ak i\u2212\na\u2212\nk+1 . . . an \u22b2 (24)\n\nor if the direction change happens just before the right delimiter then\n\u2212\u2212\u2212\u2192 (12)\n\u22b3 a1 . . . ai\u22121 b1 . . . bj hr, an i\u22b2 \u22a2AM \u22b3a1 . . . ai\u22121 b1 . . . bj hr, an i \u22b2\u25ed .\n19\n\n(25)\n\n\fIf i = n or if the right-moving head hits the right delimiter the derivation has the\nfollowing form\n(14)\n\u2212\u2212\u2212\u2212\u2192 (9)\n\u22b3 a1 . . . an\u22121 hq, an i\u22b2 \u22a2AM \u22b3a1 . . . an\u22121 hq, an i\u22b2B \u22a2AM \u22b3a1 . . . an\u22121 hq, an iB\u22b2\u25ed, (26)\n\nwhich inserts a blank to the right of the simulated head.\n3. Downgoing pulse is reflected and moves up.\n\u2192\nWe proceed from configurations of the form \u22b3c1 . . . ci\u22121 hp, ci i\u2212\nc\u2212\ni+1 . . . cn \u22b2. Then\n(5)\n(5)\n(13)\n\u2192...c \u22b2 \u22a2\n\u2192\n\u2212\n\u22b3c1 . . . ci\u22121 hp, ci i\u2212\nc\u2212\n.\n.\n.\n\u22a2\n\u22b3c\n.\n.\n.\nc\nhp,\nc\nic\n.\n.\n.\nc\n\u22b2\n\u22a2\ni+1\nn\nAM\nAM\n1\ni\u22121\ni i+1\nn\nAM\n(15)\n(16)\n(16)\n\u22b3c1 . . . ci\u22121 hp, ci ici+1 . . . cn \u22b2\u25ed \u22a2AM \u22b3c1 . . . ci\u22121 hp, ci ici+1 . . . cn \u25ed \u22b2 \u22a2AM . . . \u22a2AM\n(17)\n(16)\n(16)\n\u22b3c1 . . . ci\u22121 hp, ci i \u25ed ci+1 . . . cn \u22b2 \u22a2AM \u22b3c1 . . . ci\u22121 \u25ed hp, ci ici+1 . . . cn \u22b2 \u22a2AM . . . \u22a2AM\n(18)\n\u2192\n\u2212\n\u22b3 \u25ed c1 . . . ci\u22121 hp, ci ici+1 . . . cn \u22b2 \u22a2AM \u22b3 c1 . . . ci\u22121 hp, ci ici+1 . . . cn \u22b2,\n(27)\n\nwhich finishes the zigzag. Note that the continuation of derivations 25 and 26 is\nhandled by the later part of derivation 27. We also remark that the zigzag has shifted\nthe whole configuration one cell downwards.\nAll block transformations except transformations 8 and 10 keep the \u03b3-value of the configuration unchanged. Block transformations 8 and 10 correctly simulate one step in the cal(8)or(10)\nculation of the Turing machine M: if C \u22a2AM C \u2032 , \u03b3(C) = c, and \u03b3(C \u2032 ) = c\u2032 then c \u22a2M c\u2032 .\nLet C1\u2032 be the resulting configuration of the zigzag. We conclude that \u03b3(C1 ) \u22a2\u2217M \u03b3(C1\u2032 ) holds.\nWe have chosen C1 in such a way that at least one step of M is performed, if M does not\nhalt, either by block transformation 8 or 10. If M does not halt the configuration after the\n\u2192\n\u2212\nzigzag is again of the form \u22b3 a1 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2. The case i = n and \u03b4D (q, an ) = R\nis excluded by derivation 26, which inserts a blank to the right of the head, if \u03b4D (q, an ) = R.\nThis means that C1\u2032 has the same form as C1 and that any subsequent zigzag will perform\nat least one step of M as well if M does not halt.\nIn summary, we conclude that AM reaches after a finite number of zigzags a configuration\nC2 such that \u03b3(C2 ) = c2 . On the other hand, if M halts, AM enters a final configuration since\nderivations 21 or 23 are not applicable anymore and the pulse cannot cross the simulated\nhead. Since we have chosen C0 to be of the same form as C1 in the beginning of the proof,\nthe addendum of the lemma regarding the initial configuration is true.\n20\n\nQ.E.D.\n\n\fNext, the time behavior of the self-similar cellular automaton AM will be investigated.\n\u2192\n\u2212\nLemma 3. Let C = \u22b3 a1 . . . ai\u22121 hq, ai iai+1 . . . an \u22b2 be a finite configuration of AM that\nstarts in cell k. If M does not halt, the zigzag of the pulse takes 3 cycles of cell k and AM\n\u2192\n\u2212\nis afterwards in a finite configuration C \u2032 = \u22b3 b1 . . . bj\u22121 hp, bj ibj+1 . . . bm \u22b2 that starts in cell\nk + 1.\nProof. Without loss of generality, we assume that the finite configuration starts in cell 0.\nWe follow the zigzag of the pulse, thereby tracking all times, compare with Fig. 4 and Fig. 5.\nP\nThe pulse reaches at time 1 cell 1, and at time 1i=0 2\u2212i cell 2. In general, the downgoing\nPr\u22121 \u2212i\nP\n\u2212i\npulse reaches cell r in time i=0\n2 . At time n+1\nthe cell n + 2 changes to \u22b2\u25ed which\ni=0 2\nmarks the reversal of direction of the pulse. The next configuration change (\u22b2\u25ed\u0003 7\u2192\u25ed \u22b2)\nP\n\u2212i\noccurs at n+1\n+ 2\u2212(n+1) = 2. The pulse \u25ed reaches cell n + 1 in time 2 + 2\u2212(n+1) and\ni=0 2\n\u2192\n\u2212\nin general cell r in time 2 + 2\u2212r . The final configuration change of the zigzag (\u22b3 \u25ed7\u2192 \u0003 \u22b3 )\nthat marks also the beginning of a new pulse zigzag occurs synchronously in cell 0 and cell\n1 at time 3. We remark that the overall time of the pulse zigzag remains unchanged if the\nsimulated head inserts a blank between the two delimiters.\n\nQ.E.D.\n\nTheorem 2. If M halts on w and AM is initialized with C0 (w) then AM enters a final\nconfiguration in a time less than 6 cycles of cell 0, containing the result of the calculation\nbetween the left and right delimiter. If M does not halt, AM enters after 6 cycles of cell 0\nthe final configuration that consists of an infinite string of the quiescent element: \u0003\u221e .\nProof. AM needs 3 cycles of cell 0 to perform the first zigzag of the pulse. After the 3 cycles\nthe configuration is shifted one cell downwards, starting now in cell 1. The next zigzag takes\n3 cycles of cell 1 which are 3/2 cyles of cell 0, and so on. Each zigzags performs at least one\nstep of the Turing machine M, if M does not halt. We conclude that if M halts, A enters a\nP\ni\nfinal configuration in a time less than \u221e\ni=0 3/2 = 6 cycles of cell 0. If M does not halt, the\nzigzag disappears in infinity after 6 cycles of cell 0 leaving a trail of \u0003's behind.\n\nQ.E.D.\n\nIf M is a universal Turing machine, we immediately obtain the following result, which\nproves that AM is a hypercomputer for certain Turing machines M.\nCorollary 1. Let MU be a universal Turing machine. Then AMU solves the halting problem\nfor Turing machines.\n21\n\n\fProof. Initialize AMU with an encoded Turing machine M and an input word w. Then AM\nenters a final configuration with the result of M on w in less than 6 cycles of cell 0 if and\nonly if M halts.\n\nQ.E.D.\n\nIn the current form of Turing machine simulation the operator has to scan a potentially\nunlimited number of cells to determine whether M has halted or not, which limits its practical value. If M has halted, we would like to propagate at least this fact back to the upper\ncells. The following obvious strategy fails in a subtle way. Add a rule to AM that whenever\nhq, ai has no next move, replaces it by the new symbol H. Add the rule f (?, ?, H, ?) = H to\nAM that propagates H upwards to cell 0. The propagation upwards is only possible if we\n\u2192\n\u2212\nchange also the block transformation 18 to \u22b3 \u25ed7\u2192 \u2666 \u22b3 , thereby introducing a new symbol \u2666\nthat is not subject of the short-circuit evaluation. The last point, even if necessary, causes\nthe strategy to fail, since if AM does not halt, AM is after 6 cycles in the configuration \u2666\u221e\nthat leads to indeterministic behavior of AM . This is in so far problematic, since we can\nnot be sure whether a state H in cell 0 is really the outcome of a halting Turing machine or\nthe result of indeterministic behavior. Instead of enhancing the self-similar cellular automaton model, we will introduce in the next section a computing model that is computational\nequivalent for finite computations, but avoids indeterminism for infinite computations.\n\nV.\n\nSELF-SIMILAR PETRI NETS\n\nThe evolution of a cellular automaton as well as the evolution of a self-similar cellular\nautomaton depends on an extrinsic clock representing a global time that triggers the state\nchanges. Since a self-similar cellular automaton cannot halt, a self-similar cellular automaton\nis forced to perform a state change, even if no state with a causal relationship to the previous\none exists, leading to indeterministic behavior, as described in the introduction. In this\nsection, we present a model based on Petri nets, the self-similar Petri nets, with a close\nresemblance to self-similar cellular automata. Even though Petri nets in general are not\ndeterministic, there exist subclasses that are. As will be shown below, self-similar Petri\nNets are deterministic. They are also capable of hypercomputing, but compared to selfsimilar cellular automata, their behavior differ in the limit. Whereas a self-similar cellular\nautomaton features indeterministic behavior, the self-similar Petri net halts.\n\n22\n\n\fA.\n\nPetri nets\n\nC.A. Petri introduced Petri nets in the 1960s to study asynchronous computing systems.\nThey are now widely used to describe and study information processing systems that are\ncharacterized as being concurrent, asynchronous, distributed, parallel, nondeterministic,\nand/or stochastic. It is interesting to note that very early, and clearly ahead of its time,\nPetri investigated the connections between physical and computational processes, see e.g.,\nRef. [38]. In what follows, we give a brief introduction to Petri nets to define the terminology.\nFor a more comprehensive treatment we refer to the literature; e.g., to Ref. [39].\nDefinition 11 (Petri Net). A Petri net is a directed, weighted, bipartite graph consisting\nof two kinds of nodes, called places and transitions. The weight w(p, t) is the weight of the\narc from place p to transition t, w(t, p) is the weight of the arc from transition t to place\np. A marking assigns to place p a nonnegative integer k, we say that p is marked with k\ntokens. If a place p is connected with a transition t by an arc that goes from p to t, p is an\ninput place of t, if the arc goes from t to p, p is an output place. A Petri net is changed\naccording to the following transition (firing) rule:\n1. a transition t may fire if each input place p of t is marked with at least w(p, t) tokens,\nand\n2. a firing of an enabled transition t removes w(p, t) tokens from each input place p of t,\nand adds w(t, p) tokens to each output place p of t.\nFormally, a Petri net N is a tuple N = (P, T, F, W, M0) where P is the set of places, T is\nthe set of transitions, F \u2286 (P \u00d7 T ) \u222a (T \u00d7 P ) is the set of arcs, W : F \u2192 N is the weight\nfunction, and M0 : P \u2192 N is the initial marking.\nIn graphical representation, places are drawn as circles and transitions as boxes. If a place\nis input place of more than one transition, the Petri net becomes in general indeterministic,\nsince a token in this place might enable more than one transition, but only one can actually\nfire and consume the token. The subclass of Petri nets given in the following definition\navoids these conflicts and is therefore deterministic. In a standard Petri net, tokens are\nindistinguishable. If the Petri net model is extended so that the tokens can hold values, the\nPetri net is called a colored Petri net.\n23\n\n\fCell n\u22121\n\nCell n\n\n1\n1\n\nCell n+1\n\n2\n\n1\n\n1\n\n2\n\n1\n\nCell n+2\n\n2\n\n1\n\n1\n\n2\n\n1\n\n1\n\n2\n\n1\n\n1\n\n2\n\n1\n\n1\n\n1\n2\n\n1\n\nFIG. 6: Underlying graph of a self-similar Petri net.\n\nDefinition 12 (Marked Graph and Colored Petri Net). A marked graph is a Petri Net\nsuch that each place has exactly one input transition and exactly one output transition. A\ncolored Petri net is a Petri net where each token has a value.\n\nB.\n\nSelf-similarity\n\nIt is well-known that cellular automata can be modeled as colored Petri Nets. To do this,\neach cell of the cellular automaton is replaced by a transition and a place for each neighbor.\nThe neighbor transitions send their states as token values to their output places, which are\nthe input places of the transition under consideration. The transition consumes the tokens,\ncalculates the new state, and send its state back to its neighbors. A similar construction\ncan be done for self-similar cellular automata, leading to the class of self-similar Petri nets.\nDefinition 13 (Self-similar Petri Net). A self-similar Petri net is a colored Petri net with\nsome extensions. A self-similar Petri net has the underlying graph partitioned into cells\nthat is depicted in Fig. 6. We denote the transition of cell n by t(n), the place to the left\nof the transition by pl (n), the place to the right of the transition by pr (n) and the central\nplace, in the figure the place above the transition, by pc (n). Let Z be a finite set, the state\nset, q \u2208 Z be the quiescent state, and f be a (partial) function Z 4 \u00d7 {0, 1} \u2192 Z. The\nset V = Z \u222a ({0, 1} \u00d7 Z) is the value set of the tokens. Tokens are added to a place and\nconsumed from the place according to a first-in first-out order. Initially, the self-similar\nPetri net starts with a finite number of cells 0, 1, . . . , n, and is allowed to grow to the right.\nThe notation p \u2190 z defines the following action: create a token with value z and add it to\nplace p. The firing rule for a transition in cell n of a self-similar Petri net extends the firing\nrule of a standard Petri net in the following way:\n\n24\n\n\f1. If the transition t(n) is enabled, the transition removes token Tk l from place pl (n),\ntoken Tk c from pc (n) and tokens Tk r1 , Tk r2 from pr (n). The value of token Tk l shall\nbe of the form (coupled , zl ) in V = {0, 1} \u00d7 Z, the other token values zc , zr1 and zr2\nshall be in Z. If the tokens do not conform, the behavior of the transition is undefined.\n2. The transition calculates z = f (zl , zc , zr1 , zr2 , coupled).\n3. (Left boundary cell) If n = 0 then pl (0) \u2190 (\u00accoupled, q), pc (0) \u2190 z, pl (1) \u2190 (0, z),\npl (1) \u2190 (1, z).\n4. (Inner cell) If n > 0 and n is not the highest index, then: pr (n \u2212 1) \u2190 z, pc (n) \u2190 z,\npl (n + 1) \u2190 (0, z), pl (n + 1) \u2190 (1, z).\n5. (Right boundary cell) If n is the highest index then:\n(a) (Quiescent state) If z = q then pr (n \u2212 1) \u2190 q, pc (n) \u2190 q, pr (n) \u2190 q, pr (n) \u2190 q\n(b) (New cell allocation) If z 6= q then a new cell n + 1 is created and connected to\ncell n. Furthermore: pr (n \u2212 1) \u2190 z, pc (n) \u2190 z, pr (n) \u2190 q, pl (n + 1) \u2190 (0, z),\npl (n + 1) \u2190 (1, z), pc (n + 1) \u2190 q, pr (n + 1) \u2190 q, pr (n + 1) \u2190 q.\nFormally, we denote the self-similar Petri net by a tuple N = (Z, f ).\nA self-similar Petri net is a marked graph and therefore deterministic. The initial markup\nis chosen in such a way that initially only the rightmost transition is enabled.\nDefinition 14. (Initial markup) Let a0 a1 . . . am be an input word in Z m+1 and let N be a\nself-similar Petri net with n cells, whereby n > m + 1. The initial markup of the Petri net\nis as follows:\n\u2022 pl (0) \u2190 (0, q), (pl (i) \u2190 (0, ai\u22121 ), pl (i) \u2190 (1, ai\u22121 )) for 0 < i \u2264 m + 1, (pl (i) \u2190 (0, q),\npl (i) \u2190 (1, q)) for i > m + 1\n\u2022 pc (i) \u2190 ai for i \u2264 m, pc (i) \u2190 q for i > m,\n\u2022 pr (i) \u2190 ai+1 for i < m, pr (i) \u2190 q for i \u2265 m, and pr (n) \u2190 q.\nNote that the place pr (n) is initialized with two tokens. We identify the state of a cell\nwith the value of its pc -token. If pc is empty, because the transition is in the process of\nfiring, the state shall be the value of the last consumed token of pc .\n25\n\n\f0\n\nCell 0\n0\n\nCell 1\n0\n\nCell 2\nCell 3\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n1\n\n0\n\n1\n\nFIG. 7: Token flow in a self-similar Petri net.\n\nFig. 7 depicts the token flow of a self-similar Petri net consisting of 4 cells under the\nassumption that the self-similar Petri net does not grow. Tokens that are created and\nconsumed by the same cell are not shown. The numbers indicate whether the firing is\nuncoupled (0) or coupled (1). The only transition that is enabled in the begin is t(3), since\npr (3) was initialized with 2 tokens. The firing of t(3) bootstraps the self-similar Petri net by\nadding a second token to pr (2), thereby enabling t(2), and so on, until all transitions have\nfired, and the token flow enters periodic behavior.\n\nC.\n\nComparison of self-similar cellular automata and self-similar Petri nets\n\nWe now compare self-similar Petri nets with self-similar cellular automata. We call a\ncomputation finite, if it involves either only a finite number of state updates of a selfsimilar cellular automaton, or a finite number of transition firings of a self-similar Petri net,\nrespectively.\nLemma 4. For finite computations, a dynamically growing self-similar cellular automaton\nA = (Z, f ) and a self-similar Petri net N = (Z, f ) are computationally equivalent on a stepby-step basis if the start with the same number of cells and the same initial configuration.\nProof. Let N be a self-similar Petri net which has n cells initially. For the sake of the proof\nconsider an enhanced self-similar Petri net N \u2032 that is able to timestamp its token. A token\nTk of N \u2032 does not hold only a value, but also a time interval. We refer to the time interval\nof Tk by Tk .t and to the value of Tk by Tk .v. We remark that the timestamps serve only\nto compare the computations of a self-similar cellular automaton and a self-similar Petri net\nand do not imply any time behavior of the self-similar Petri net. The firing rule of N \u2032 works\n26\n\n\fas for N, but has an additional pre- and postprocessing step:\n\u2022 (Preprocessing) Let Tk c , Tk l , Tk r1 , and Tk r2 be the consumed token, where the\nalphabetical subscript denotes the input place and the numerical subscript the order\nin which the tokens were consumed. Calculate t = (Tk c .t)\u2192 , where \u2192 is the inverse\ntime operator of \u2190. If Tk r1 .t 6= t\u0582 or Tk r2 .t 6= t\u0581 or Tk l .t 6= t\u2191 the firing fails and\nthe transition becomes permanently disabled.\n\u2022 (Postprocessing) For each created token Tk , set Tk .t = t.\nThe initial marking must set the t-field, otherwise the first transitions will fail. For the\ninitial tokens in cell k, set Tk l .t = 2\u2212k+11 for both tokens in place pl , Tk c .t = 2\u2212k 1, and\nTk d .t = 2\u2212k\u221211. Set Tk d .t = 2\u2212n\u22121 (1 + 1) for the second token in pr (n). The firings of cell\nk add tokens with timestamps 2\u2212k 1, 2\u2212k (2 + 1), 2\u2212k (3 + 1) . . . to the output place pc (k). If\ntransition t(k) does not fail, the state function for the arguments c = 2\u2212k 1 and t = 2\u2212k (i+ 1)\nis well-defined: s\u2032 (c, t) = z if cell k has produced or was initialized in place pr with a token Tk\nwith Tk .t = t and Tk .v = z. Let s(c, t) be the state function of the scale-invariant cellular\nautomaton A. Due to the initialization, the two state functions are defined for the first n cells\nand first time intervals 2\u2212k 1. Assume that the values of s and s\u2032 differ for some argument\nor that their domains are different. Consider the first time interval t1 where the difference\noccurs: s(c, t1 ) 6= s\u2032 (c, t1 ), or exactly one of s(c, t1 ) or s\u2032 (c, t1 ) is undefined. If there is more\nthan one time interval choose an arbitrary one of these. Since t1 was the first time interval\nwhere the state functions differ, we know that s(c\u2191 , t1\u2191 ) = s\u2032 (c\u2191 , t1\u2191 ), s(c, t1\u2190 ) = s\u2032 (c, t1\u2190 ),\ns(c\u0582 , t1\u0582 ) = s\u2032 (c\u0582 , t1\u0582 ), and s(c\u0582 , t1\u0581 ) = s\u2032 (c\u0582 , t1\u0581 ). We handle the case that the values\nof the state functions are different or that s\u2032 is undefined for (c, t1 ) whereas s is. The other\ncase (s\u2032 defined, but not s) can be handled analogously. If c = 2\u2212k 1, we conclude that\ntokens with timestamps t1\u2191 , t1\u2190 , t1\u0582 , t1\u0581 were sent to cell k, and no other tokens were\nsent afterwards to cell k, since the timestamps are created in chronological order. Hence,\nthe precondition of the firing rule is satisfied and we conclude that s(c, t1 ) = s\u2032 (c, t1 ), which\ncontradicts our assumption. The allocation of new cells introduces some technicalities, but\nthe overall strategy of going back in time and concluding that the conditions for a state\nchange or cell allocation were the same in both models works here also. We complete the\nproof, by the simple observation that N and N \u2032 perform the same computation.\n\n27\n\nQ.E.D.\n\n\fThe proof can be simplified using the following more abstract argumentation. A comparison of Fig. 7 with Fig. 2 shows that each computation step has in both models the same\ncausal dependencies. Since both computers use the same rule to calculate the value of a cell,\nrespectively the value of a token, we conclude that the causal nets [40] of both computations\nare the same for a finite computation, and therefore both computers yield the same output,\nin case the computation is finite.\n\nD.\n\nTimed self-similar Petri nets that hypercompute\n\nA large number of different approaches to introducing time concepts to Petri nets have\nbeen proposed since the first extensions in the mid 1970s. We do not delve into the depths\nof the different models, but instead, define a very simple time schedule for the class of\nself-similar Petri nets.\nDefinition 15 (Timed Self-similar Petri Net). A timed self-similar Petri net is a self-similar\nPetri net that fires as soon as the transition is enabled and where a firing of an enabled\ntransition t(k) takes the time 2\u2212k . In the beginning of the firing, the tokens are removed\nfrom the input places, and at the end of the firing the produced tokes of the firing are\nsimultaneously entered into the output places.\nThis time model can be satisfied if the cells of the timed self-similar Petri net are arranged\nas the cells of a self-similar cellular automaton. Under the assumption of a constant token\nspeed, a firing time that is proportional to the cell length, and an appropriate unit of time\nwe yield again cycle times of 2\u2212k .\nWe now come back to the simulation of Turing machines and construct a hypercomputing\ntimed self-similar Petri net, analogous to the hypercomputing self-similar cellular automaton\nin section IV. Let M = (Q, \u03a3, \u0393, \u03b4, q0 , B, F ) be an arbitrary Turing machine. Let Z be\nthe state set that we used in the simulation of a Turing machine by a self-similar cellular\nautomaton, and let f the local rule that is defined by the block transformations 4 - 18,\nwithout the short-circuit evaluation. By Lemma 4 we know that the timed self-similar Petri\nnet NM = (Z, f ) simulates M correctly for a finite number of Turing machine steps. Hence,\nif M halts on input w, NM enters a final configuration in less than 6 cyles of cell 0. We\nexamine now the case that M does not halt. A pivotal difference between a self-similar\n28\n\n\fcellular automaton and a self-similar Petri net is the ability of the latter one to halt on a\ncomputation. This happens if all transitions of the self-similar Petri net are disabled.\nLemma 5. Let M = (Q, \u03a3, \u0393, \u03b4, q0 , B, F ) be an arbitrary Turing machine and w an input\nword in \u03a3\u2217 . If M does not halt on w, the timed self-similar Petri net NM halts on C0 (w)\nafter 6 cycles of cell 0.\nProof. As long as the number of cells is finite, the boundary condition 5a of the firing rule\nadds by each firing two tokens to the pr -place of the rightmost cell that successively enable\nall other transitions as well. This holds no longer for the infinite case. Let M be a Turing\nmachine, and w an input word, such that M does not halt on w. We consider again the\ntravel of the pulse zigzags down to infinity for the timed self-similar Petri net NM with initial\nconfiguration C0 (w), thereby tracking the marking of the pr -places for times after the zigzag\n\u2192\n\u2212\nhas passed by. The first states of cell 0 are \u22b3 , \u22b3, \u22b3, and \u0003, including the initial one. The\nstate \u0003 is the result of the firing at time 3, exhausting thereby the tokens in place pr (0). At\n\u2192\n\u2212\ntime 3 the left delimiter ( \u22b3 ) of the pulse zigzag is now in cell 1. Cell 1 runs from time 3 on\n\u2192\n\u2212\nthrough the same state sequence \u22b3 , \u22b3, \u22b3, and \u0003, thereby adding in summary 4 tokens to\npr (0). After creating the token with value \u0003, pr (1) is empty as well. We conclude that after\nthe zigzag has passed by a cell, the lower cell sends in summary 4 tokens to the upper cell,\ntill the zigzag has left the lower cell as well. For each cell k these four tokens in pr (k) enable\ntwo firings of cell k thereby adding two tokens to pr (k \u2212 1). These two tokens of pr (k \u2212 1)\nenable again one firing of cell k \u2212 1 thereby adding one token to pr (k \u2212 2). We conclude that\neach cell fires 3 times after the zigzag has passed by and that the final marking of each pr\nis one. Hence, no pr has the necessary two tokens that enable the transition, therefore all\ntransitions are disabled and NM halts at time 6.\n\nQ.E.D.\n\nSince NM halts for nonhalting Turing machines, there are no longer any obstacles that\nprevent the construction of the proposed propagation of the halting state back to upper\ncells. We replace block transformation 4 with the following two and add one new.\nIf \u03b4(q, a) = (p, c, R) set\n\u2212\u2212\u2212\u2192\n\u2212\n\u2192\n\u22b3 hq, ai 7\u2192 \u22b3 hq, ai.\n\n(28)\n\nIf \u03b4(q, a) = (p, c, L) or \u03b4(q, a) is not defined set\n\u2212\n\u2192\n\u22b3 hq, ai 7\u2192 \u22b3 H.\n29\n\n(29)\n\n\fIf \u03b4(q, a) is not defined set\n\u2212\n\u2192\nb hq, ai 7\u2192 b H.\n\n(30)\n\nThe following definition propagates the state H up to cell 0:\nf (?, ?, H, ?) = H.\n\n(31)\n\nWe denote the resulting timed self-similar Petri net by N M . The following theorem makes\nuse of the apparently paradoxical fact, that N M halts if and only if the simulated Turing\nmachine does not halt.\nTheorem 3. Let MU be a universal Turing machine. Then N MU solves the halting problem\nfor Turing machines.\nProof. Consider a Turing machine M and an input word w. Initialize N MU with C0 (hM, wi)\nwhere hM, wi is the encoding of M and w. If M does not halt on w, N MU halts at time\n6 by Lemma 5. If M halts on w, then one cell of N MU enters the state H by block transformation 29 or 30 according to Theorem 2 and Lemma 4 and taking the changes in f into\naccount. The mapping 31 propagates H up to cell 0. An easy calculation shows that cell 0\nis in state H, in time 7 or less.\n\nQ.E.D.\n\nWe have proven that N MU is indeed a hypercomputer without the deficiencies of the scaleinvariant cellular automaton-based hypercomputer. We end this section with two remarks.\nThe timed self-similar Petri net NM sends a flag back to the upper cells, if the simulated\nTuring machine halts. Strictly speaking, this is not necessary, if the operator is able to\nrecognize whether the timed self-similar Petri net has halted or not. On the other hand,\na similar construction is essential, if the operator is interested in the final tape content\nof the simulated Turing machine. Transferring the whole tape content of the simulated\nTuring machine upwards, could be achieved by implementing a second pulse that performs\nan upwards-moving zigzag. The construction is even simpler as the described one, since the\ntape content of the Turing machine becomes static as soon as the Turing machine halts.\nThe halting problem of Turing machines is not the only problem that can be solved by\nself-similar cellular automata, scale-invariant cellular automata, or timed self-similar Petri\nnets, but is unsolvable for Turing machines. A discussion of other problems unsolvable by\nTuring machines and of techniques to solve them within infinite computing machines, can\nbe found in Davies [19].\n30\n\n\fVI.\n\nSUMMARY\n\nWe have presented two new computing models that implement the potential infinite divisibility of physical configuration space. These models are purely information theoretic\nand do not take into account kinetic and other effects. With these provisos, it is possible,\nat least in principle, to use the potential infinite divisibility of space-time to perform hypercomputation, thereby extending the algorithmic domain to hitherto unsolvable decision\nproblems.\nBoth models are composed of elementary computation primitives. The two models are\nclosely related but are very different ontologically. A cellular automaton depends on an\nextrinsic time requiring an external clock and a rigid synchronization of its computing cells,\nwhereas a Petri net implements a causal relationship leading to an intrinsic concept of time.\nScale-invariant cellular automata as well as self-similar Petri nets are built in the same\nway from their primitive building blocks. Each unit is recursively coupled with a sized-down\ncopy of itself, potentially leading to an infinite sequence of ever decreasing units. Their close\nresemblance leads to a step-by-step equivalence of finite computations, yet their ontological difference yields different behaviors for the for the case that the computation involves\nan infinite number of units: a scale-invariant cellular automaton exhibits indeterministic\nbehavior, whereas a self-similar Petri net halts. Two supertasks which operate identically\nin the finite case but differ in their limit is a puzzling observation which might question\nour present understanding of supertasks. This may be considered an analogy to a theorem\n[41] in recursive analysis about the existence of recursive monotone bounded sequences of\nrational numbers whose limit is not a computable number.\nOne striking feature of both models is their scale-invariance. The computational behavior\nof these models is therefore the first example for what might be called scale-invariant or selfsimilar computing, which might be characterized by the property that any computational\nspace-time pattern can be arbitrary squeezed to finer and finer regions of space and time.\nAlthough the basic definitions have been given, and elementary properties of these\nnew models have been explored, a great number of questions remain open for future research. The construction of a hypercomputer was a first demonstration of the extraordinary computational capabilities of these models. Further investigations are necessary\nto determine their limits, and to relate them with the emerging field of hypercomputa31\n\n\ftion [21, 22, 23, 27, 31, 42, 43]. Another line of research would be the investigation of\ntheir phenomenological properties, analogous to the statistical mechanics of cellular automata [8, 44].\n\n[1] R. Landauer, \"Computation, Measurement, Communication and Energy Dissipation,\" in Selected Topics in Signal Processing, S. Haykin, ed. (Prentice Hall, Englewood Cliffs, NJ, 1989),\np. 18.\n[2] H. S. Leff and A. F. Rex, Maxwell's Demon (Princeton University Press, Princeton, 1990).\n[3] C. H. Bennett, \"Logical Reversibility of Computation,\" IBM Journal of Research and Development 17, 525\u2013532 (1973), reprinted in [2, pp. 197-204].\n[4] J. von Neumann, Theory of Self-Reproducing Automata (University of Illinois Press, Urbana,\n1966), a. W. Burks, editor.\n[5] K. Zuse, Rechnender Raum (Friedrich Vieweg & Sohn, Braunschweig, 1969), English translation [45].\n[6] E. Fredkin, \"An informational process based on reversible universal cellular automata,\" Physica D45, 254\u2013270 (1990).\nhttp://dx.doi.org/10.1016/0167-2789(90)90186-S\n[7] T. Toffoli and N. Margolus, \"Invertible cellular automata: A review,\" Physica D 45, 229\u2013253\n(1990).\nhttp://dx.doi.org/10.1016/0167-2789(90)90185-R\n[8] S. Wolfram, A New Kind of Science (Wolfram Media, Inc., Champaign, IL, 2002).\n[9] M. Margenstern and K. Morita, \"A Polynomial Solution for 3-SAT in the Space of Cellular\nAutomata in the Hyperbolic Plane,\" Journal of Universal Computer Science 5, 563\u2013573 (1999).\nhttp://www.jucs.org/jucs 5 9/a polynomial solution for\n[10] J. E. Hopcroft and J. D. Ullman, Introduction to Automata Theory, Languages, and Computation (Addison-Wesley, Reading, MA, 1979).\n[11] K. Svozil, \"Extrinsic-intrinsic concept and complementarity,\" in Inside versus Outside, H. Atmanspacker and G. J. Dalenoort, eds. (Springer-Verlag, Heidelberg, 1994), pp. 273\u2013288.\n[12] T. Toffoli, \"The role of the observer in uniform systems,\" in Applied General Systems Research,\nRecent Developments and Trends, G. J. Klir, ed. (Plenum Press, New York, London, 1978),\n\n32\n\n\fpp. 395\u2013400.\n[13] H. D. P. Lee, Zeno of Elea (Cambridge University Press, Cambridge, 1936).\n[14] G. S. Kirk and J. E. Raven, The Presocratic Philosophers (Cambridge University Press, Cambridge, 1957).\n[15] A. Gr\u00fcnbaum, Modern Science and Zeno's paradoxes (Allen and Unwin, London, 1968), 2nd\nedn.\n[16] W. C. Salmon, Zeno's Paradoxes (Hackett Publishing Company, 1970, 2001).\n[17] H. Weyl, Philosophy of Mathematics and Natural Science (Princeton University Press, Princeton, 1949).\n[18] A. Gr\u00fcnbaum, Philosophical problems of space and time (Boston Studies in the Philosophy of\nScience, vol. 12) (D. Reidel, Dordrecht/Boston, 1974), 2nd edn.\n[19] E. B. Davies, \"Building Infinite Machines,\" The British Journal for the Philosophy of Science\n52, 671\u2013682 (2001).\nhttp://dx.doi.org/10.1093/bjps/52.4.671\n[20] T. Ord, \"The many forms of hypercomputation,\" Applied Mathematics and Computation\n178, 143\u2013153 (2006).\nhttp://dx.doi.org/10.1016/j.amc.2005.09.076\n[21] M. Davis, \"The myth of hypercomputation,\" in Alan Turing: Life and Legacy of a Great\nThinker , C. Teuscher, ed. (Springer, Berlin, 2004), pp. 195\u2013212.\n[22] F. A. Doria and J. F. Costa, \"Introduction to the special issue on hypercomputation,\" Applied\nMathematics and Computation 178, 1\u20133 (2006).\nhttp://dx.doi.org/10.1016/j.amc.2005.09.065\n[23] M. Davis, \"Why there is no such discipline as hypercomputation,\" Applied Mathematics and\nComputation 178, 4\u20137 (2006).\nhttp://dx.doi.org/10.1016/j.amc.2005.09.066\n[24] J. Durand-Lose, \"Abstract Geometrical Computation for Black Hole Computation,\" in Machines, Computations, and Universality, 4th International Conference, MCU 2004, Saint\nPetersburg, Russia, September 21-24, 2004, Revised Selected Papers, M. Margenstern, ed.\n(Springer, 2005), pp. 176\u2013187.\nhttp://dx.doi.org/10.1007/b106980\n[25] I. N\u00e9meti and G. D\u00e1vid, \"Relativistic computers and the Turing barrier,\" Applied Mathemat-\n\n33\n\n\fics and Computation 178, 118\u2013142 (2006), special Issue on Hypercomputation.\nhttp://dx.doi.org/10.1016/j.amc.2005.09.075\n[26] O. Bournez and M. L. Campagnolo, \"A Survey on Continuous Time Computations,\" in New\nComputational Paradigms. Changing Conceptions of What is Computable, S. Cooper, B. L\u00f6we,\nand A. Sorbi, eds. (Springer Verlag, New York, 2008), pp. 383\u2013423.\n[27] O. Shagrir, \"Super-tasks, accelerating Turing machines and uncomputability,\" Theoretical\nComputer Science 317, 105\u2013114 (2004).\nhttp://dx.doi.org/10.1016/j.tcs.2003.12.007\n[28] H. Rogers, Jr., Theory of Recursive Functions and Effective Computability (MacGraw-Hill,\nNew York, 1967).\n[29] P. Odifreddi, Classical Recursion Theory, Vol. 1 (North-Holland, Amsterdam, 1989).\n[30] P. Odifreddi, Classical Recursion Theory, Vol. 2 (North-Holland, Amsterdam, 1999).\n[31] P. H. Potgieter, \"Zeno machines and hypercomputation,\" Theoretical Computer Scienc 358,\n23\u201333 (2006).\nhttp://dx.doi.org/10.1016/j.tcs.2005.11.040\n[32] S. Wolfram, Theory and Application of Cellular Automata (World Scientific, Singapore, 1986).\n[33] H. Gutowitz, \"Cellular Automata: Theory and Experiment,\" Physica D45, 3\u2013483 (1990),\nprevious CA conference proceedings in International Journal of Theoretical Physics 21, 1982;\nas well as in Physica, D10, 1984 and in Complex Systems 2, 1988.\n[34] A. Ilachinski, Cellular Automata: A Discrete Universe (World Scientific Publishing Co., Inc.,\nRiver Edge, NJ, USA, 2001).\n[35] L. G. Morelli and D. H. Zanette, \"Synchronization of stochastically coupled cellular automata,\" Physical Review E 58, R8\u2013R11 (1998).\nhttp://dx.doi.org/10.1103/PhysRevE.58.R8\n[36] B. Feng and M. Ding, \"Block-analyzing method in cellular automata,\" Physical Review E 52,\n3566\u20133569 (1995).\nhttp://dx.doi.org/10.1103/PhysRevE.52.3566\n[37] L. G. Brunnet and H. Chat\u00e9, \"Cellular automata on high-dimensional hypercubes,\" Physical\nReview E 69, 057 201 (2004).\nhttp://dx.doi.org/10.1103/PhysRevE.69.057201\n[38] C. A. Petri, \"State-transition structures in physics and in computation,\" International Journal\n\n34\n\n\fof Theoretical Physics 21, 979\u2013992 (1982).\nhttp://dx.doi.org/10.1007/BF02084163\n[39] T. Murata, \"Petri nets: Properties, analysis and applications,\" Proceedings of the IEEE 77,\n541\u2013580 (1989).\nhttp://dx.doi.org/10.1109/5.24143\n[40] L. A. Levin, \"Causal Nets or What is a Deterministic Computation,\" Information and Control\n51, 1\u201319 (1981).\n[41] E. Specker, \"Nicht konstruktiv beweisbare S\u00e4tze der Analysis,\" The Journal of Smbolic Logic\n14, 145\u2013158 (1949), reprinted in [46, pp. 35\u201348]; English translation: Theorems of Analysis\nwhich cannot be proven constructively.\n[42] C. S. Calude and B. Pavlov, \"Coins, Quantum Measurements, and Turing's Barrier,\" Quantum Information Processing 1, 107\u2013127 (2002).\nhttp://arxiv.org/abs/quant-ph/0112087\n[43] T. Ord, \"Hypercomputation: computing more than the Turing machine,\" (2002).\nhttp://arxiv.org/abs/math/0209332\n[44] S. Wolfram, \"Statistical Mechanics of Cellular Automata,\" Reviews of Modern Physics 55,\n601\u2013644 (1983).\n[45] K. Zuse, Calculating Space. MIT Technical Translation AZT-70-164-GEMIT (MIT (Proj.\nMAC), Cambridge, MA, 1970).\n[46] E. Specker, Selecta (Birkh\u00e4user Verlag, Basel, 1990).\n\n35\n\n\f"}