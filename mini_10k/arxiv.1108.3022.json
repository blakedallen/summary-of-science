{"id": "http://arxiv.org/abs/1108.3022v1", "guidislink": true, "updated": "2011-08-15T15:37:55Z", "updated_parsed": [2011, 8, 15, 15, 37, 55, 0, 227, 0], "published": "2011-08-15T15:37:55Z", "published_parsed": [2011, 8, 15, 15, 37, 55, 0, 227, 0], "title": "Quantum Algorithm for k-distinctness with Prior Knowledge on the Input", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.4000%2C1108.4694%2C1108.4831%2C1108.3362%2C1108.3099%2C1108.6013%2C1108.2910%2C1108.4548%2C1108.3667%2C1108.2189%2C1108.2300%2C1108.6081%2C1108.1736%2C1108.6083%2C1108.3529%2C1108.3651%2C1108.5747%2C1108.4665%2C1108.1010%2C1108.3196%2C1108.2893%2C1108.1316%2C1108.6185%2C1108.2951%2C1108.4253%2C1108.4658%2C1108.1925%2C1108.2555%2C1108.3587%2C1108.4586%2C1108.4315%2C1108.0914%2C1108.0178%2C1108.1509%2C1108.6284%2C1108.5402%2C1108.5144%2C1108.3450%2C1108.2927%2C1108.2710%2C1108.0847%2C1108.5919%2C1108.3022%2C1108.1408%2C1108.2481%2C1108.2206%2C1108.5943%2C1108.5151%2C1108.2772%2C1108.6125%2C1108.3397%2C1108.5014%2C1108.3317%2C1108.2193%2C1108.0389%2C1108.6274%2C1108.2344%2C1108.2499%2C1108.4600%2C1108.1591%2C1108.1581%2C1108.3244%2C1108.3613%2C1108.2089%2C1108.2102%2C1108.1604%2C1108.1468%2C1108.5719%2C1108.1347%2C1108.1017%2C1108.3163%2C1108.4806%2C1108.0938%2C1108.0165%2C1108.3396%2C1108.5540%2C1108.1777%2C1108.2730%2C1108.2514%2C1108.0071%2C1108.1121%2C1108.4393%2C1108.2677%2C1108.5137%2C1108.1355%2C1108.5374%2C1108.3133%2C1108.2957%2C1108.5764%2C1108.3065%2C1108.5855%2C1108.5984%2C1108.4058%2C1108.0115%2C1108.2034%2C1108.3661%2C1108.5448%2C1108.1192%2C1108.1281%2C1108.1239%2C1108.5634&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Quantum Algorithm for k-distinctness with Prior Knowledge on the Input"}, "summary": "It is known that the dual of the general adversary bound can be used to build\nquantum query algorithms with optimal complexity. Despite this result, not many\nquantum algorithms have been designed this way. This paper shows another\nexample of such algorithm.\n  We use the learning graph technique from arXiv:1105.4024 to give a quantum\nalgorithm for $k$-distinctness problem that runs in $o(n^{3/4})$ queries, for a\nfixed $k$, given some prior knowledge on the structure of the input. The best\nknown quantum algorithm for the unconditional problem uses $O(n^{k/(k+1)})$\nqueries.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.4000%2C1108.4694%2C1108.4831%2C1108.3362%2C1108.3099%2C1108.6013%2C1108.2910%2C1108.4548%2C1108.3667%2C1108.2189%2C1108.2300%2C1108.6081%2C1108.1736%2C1108.6083%2C1108.3529%2C1108.3651%2C1108.5747%2C1108.4665%2C1108.1010%2C1108.3196%2C1108.2893%2C1108.1316%2C1108.6185%2C1108.2951%2C1108.4253%2C1108.4658%2C1108.1925%2C1108.2555%2C1108.3587%2C1108.4586%2C1108.4315%2C1108.0914%2C1108.0178%2C1108.1509%2C1108.6284%2C1108.5402%2C1108.5144%2C1108.3450%2C1108.2927%2C1108.2710%2C1108.0847%2C1108.5919%2C1108.3022%2C1108.1408%2C1108.2481%2C1108.2206%2C1108.5943%2C1108.5151%2C1108.2772%2C1108.6125%2C1108.3397%2C1108.5014%2C1108.3317%2C1108.2193%2C1108.0389%2C1108.6274%2C1108.2344%2C1108.2499%2C1108.4600%2C1108.1591%2C1108.1581%2C1108.3244%2C1108.3613%2C1108.2089%2C1108.2102%2C1108.1604%2C1108.1468%2C1108.5719%2C1108.1347%2C1108.1017%2C1108.3163%2C1108.4806%2C1108.0938%2C1108.0165%2C1108.3396%2C1108.5540%2C1108.1777%2C1108.2730%2C1108.2514%2C1108.0071%2C1108.1121%2C1108.4393%2C1108.2677%2C1108.5137%2C1108.1355%2C1108.5374%2C1108.3133%2C1108.2957%2C1108.5764%2C1108.3065%2C1108.5855%2C1108.5984%2C1108.4058%2C1108.0115%2C1108.2034%2C1108.3661%2C1108.5448%2C1108.1192%2C1108.1281%2C1108.1239%2C1108.5634&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "It is known that the dual of the general adversary bound can be used to build\nquantum query algorithms with optimal complexity. Despite this result, not many\nquantum algorithms have been designed this way. This paper shows another\nexample of such algorithm.\n  We use the learning graph technique from arXiv:1105.4024 to give a quantum\nalgorithm for $k$-distinctness problem that runs in $o(n^{3/4})$ queries, for a\nfixed $k$, given some prior knowledge on the structure of the input. The best\nknown quantum algorithm for the unconditional problem uses $O(n^{k/(k+1)})$\nqueries."}, "authors": ["Aleksandrs Belovs", "Troy Lee"], "author_detail": {"name": "Troy Lee"}, "author": "Troy Lee", "arxiv_comment": "21 pages", "links": [{"href": "http://arxiv.org/abs/1108.3022v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1108.3022v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1108.3022v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1108.3022v1", "journal_reference": null, "doi": null, "fulltext": "Quantum Algorithm for k-distinctness with Prior Knowledge\non the Input\n\narXiv:1108.3022v1 [quant-ph] 15 Aug 2011\n\nAleksandrs Belovs\u2217\n\nTroy Lee\u2020\n\nAbstract\nIt is known that the dual of the general adversary bound can be used to build quantum\nquery algorithms with optimal complexity. Despite this result, not many quantum algorithms\nhave been designed this way. This paper shows another example of such algorithm.\nWe use the learning graph technique from [Bel11b] to give a quantum algorithm for kdistinctness problem that runs in o(n3/4 ) queries, for a fixed k, given some prior knowledge on\nthe structure of the input. The best known quantum algorithm for the unconditional problem\nuses O(nk/(k+1) ) queries.\n\n1\n\nIntroduction\n\nThis paper is a sequel of [Bel11b] on applications of span programs, or, more generally, dual of\nthe Adversary Bound, for constructing quantum query algorithms for functions with 1-certificate\ncomplexity bounded by a constant. Also, we use the computational model of a learning graph.\nIn the aforementioned paper, a reduction of a learning graph to a quantum query algorithm was\ndone using the notion of a span program, another computational model, proven to be equivalent\nto quantum query algorithms in the papers of Reichardt et al. [Rei11, LMR+ 11].\nTwo questions remained open from the last paper. Firstly, the logarithmic increase in the\ncomplexity for functions with non-Boolean input; and whether a learning graph that uses values\nof the variables to weight its arcs has more power than the one that doesn't. We fully resolve\nthe first concern by switching from span programs to a more general notion of the dual of the\nadversary bound that possesses the same properties, and, thus, getting a query algorithm with the\nsame complexity as the learning graph, up to a constant factor.\nFor the analysis of the second problem, we have chosen the k-distinctness problem for k > 2.\nThis is the most symmetric problem for which the knowledge of the values of variables can be\nimportant in construction of the learning graph. Let us define this problem here.\nThe element distinctness problem consists in computing the function f : [m]n \u2192 {0, 1} that evaluates to 1 iff there is a pair of equal elements (known as collision) in the input, i.e., f (x1 , . . . , xn ) = 1\niff \u2203i 6= j : xi = xj . The quantum query complexity of the element distinctness problem is well\nunderstood. It is known to be \u0398(n2/3 ), the algorithm given by Ambainis [Amb07] and the lower\nbound shown by Aaronson and Shi [AS04] for the case of large alphabet size \u03a9(n2 ) and by Ambainis [Amb05] in the general case. Even more, the lower bound \u03a9(n2/3 ) holds if one assumes there\nis either no, or exactly one collision in the input.\nThe k-distinctness problem is a direct generalization of the element distinctness problem. Given\nthe same input, function evaluates to 1 iff there is a set of k input elements that are all equal.\nThe situation with the quantum query complexity of the k-distinctness problem is not so clear.\n\u2217 Faculty\n\u2020 Centre\n\nof Computing, University of Latvia, Raina bulv. 19, Riga, LV-1586, Latvia, stiboh@gmail.com.\nfor Quantum Technologies\n\n1\n\n\fAs element distinctness reduces to the k-distinctness problem by repeating each element k \u2212 1\ntimes, the lower bound of \u03a9(n2/3 ) carries over to the k-distinctness problem (this argument is\nattributed to Aaronson in [Amb07]). However, the best known algorithm requires O(nk/(k+1) )\nquantum queries [Amb07].\nAs the above reduction shows, the k-distinctness problem can only become more difficult as k\nincreases. There is another difficulty that arises when k > 2-this the huge diversity in the inputs.\nFor element distinctness, all inputs that are distinct are essentially the same- they are all related\nby an automorphism of the function. Similarly, without loss of generality, one may assume that\nan input which is not distinct has a unique collision, and again all such inputs are related by an\nautomorphism. When k > 2 this is no longer the case. For example, for k = 3 inputs can differ in\nthe number of unique elements.\nMain theorem In this paper, we show how one can fight the first difficulty, but we ignore the\nsecond one. Before explaining how we do so, let us give some additional definitions.\nLet (xi )i\u2208[n] be the input variables for the k-distinctness problem. Assume some subset J \u2286 [n]\nis fixed. A subset I \u2286 J is called t-subtuple with respect to J if\n\u2200i, j \u2208 I : xi = xj ,\n\n\u2200i \u2208 I \u2200j \u2208 J \\ I : xi 6= xj\n\nand\n\n|I| = t,\n\n(1)\n\ni.e., if it is a maximal subset of equal elements and it has size t. For the important special case\nJ = [n], we call them t-tuples. If I is such that only the first condition of (1) is satisfied, we call\nit subset of equal elements.\nWe give a quantum algorithm for the k-distinctness problem that runs in o(n3/4 ) queries for\na fixed k, but with the prior knowledge on the number of t-tuples in the input. Using the same\nreduction as in [Amb07], it is easy to show the complexity of this problem is \u03a9(n2/3 ) as well.\nTheorem 1. Assume we know the number of t-tuples in the input for the k-distinctness problem\n\u221a\nk\u22122\nk\nfor all t = 1, . . . , k \u2212 1 with precision O( 4 n). Then, the problem can be solved in O(n1\u22122 /(2 \u22121) )\nquantum queries. The constant behind the O depends on k, but not on n.\n\u221a\nThe precision in the formulation of the theorem can be loosened, O( 4 n) is the most obvious\nvalue that works for all k's. See Section 4.3 for more details. Concerning the complexity of the\nalgorithm, it is the exact one, and we do not know whether it can be improved.\nOrganization of the Paper The paper is organized as follows. In Section 2, we define basic\nnotions from quantum query complexity and probability theory. In Section 3, we define learning\ngraphs and give a quantum algorithm for computing them. In Section 4, we develop some tools\nand get ready for Section 5, where we prove Theorem 1.\n\n2\n\nPreliminaries\n\nLet [m] denote the set {1, 2, . . . , m} and consider a function f : [m]n \u2287 D \u2192 {0, 1}. We identify\nthe set of input indices of f with [n]. An assignment is a function \u03b1 : [n] \u2283 S \u2192 [m]. One should\nthink of this function as fixing values for input variables in S. We say input x = (xi )i\u2208[n] agrees\nwith assignment \u03b1 if \u03b1(i) = xi for all i \u2208 S. If S \u2286 [n], by xS , we denote the only assignment on\nS that agrees with x.\nAn assignment \u03b1 is called a b-certificate for f if any input from D, consistent with \u03b1, is mapped\nto b by f . The certificate complexity Cx (f ) of function f on input x is defined as the minimal\nsize of a certificate for f that agrees with x. The b-certificate complexity C (b) (f ) is defined as\nmaxx\u2208f \u22121 (b) Cx (f ).\n2\n\n\fWe use [a, b] and ]a, b[ to denote closed and open, respectively, intervals of R; R+ to denote the\nset of non-negative reals. For the real vector space Rm , we use the l\u221e -norm, kxk\u221e = maxi |xi |.\nIn particular, we denote the l\u221e -ball of radius d around x by B(x, d). We use B(d) to denote the\nball with center zero and radius d.\npP\n2\nFor the complex vector spaces Cm , however, we use a more common l2 -norm, kxk =\ni |xi | .\n\n2.1\n\nAdversary bound\n\nIn this paper, we work with query complexity of quantum algorithms, i.e., we measure the complexity of a problem by the number of queries to the input the best algorithm should make. Query\ncomplexity provides a lower bound on time complexity. For many algorithms, query complexity\ncan be analyzed easier than time complexity. For the definition of query complexity and its basic\nproperties, a good reference is [BdW02].\nThe adversary bound, originally introduced by Ambainis [Amb02], is one of the most important\nlower bound techniques for quantum query complexity. In fact, a strengthening of the adversary\nbound, known as the general adversary bound [HL\u016007], has recently been shown to characterize\nquantum query complexity, up to constant factors [Rei11, LMR+ 11].\nWhat we actually use in the paper, is the dual of the general adversary bound. It provides\nupper bounds on the quantum query complexity, i.e., quantum query algorithms. Due to the\nsame results, it also is tight. Despite this tight equivalence, the actual applications of this upper\nbound (in the form of span programs) have been limited, mostly, to formulae evaluation [R\u0160], and,\nrecently, linear algebra problems [Bel11a]. In [Bel11b], it was used to give a variant of an optimal\nalgorithm for the element distinctness problem, and an algorithm for the triangle problem having\nbetter complexity than the one known before. In this paper, we provide yet another application.\nThe (dual of the) general adversary bound is defined as follows.\nDefinition 2. Let f : [m]n \u2192 {0, 1} be a function.\nX\n2\nkux,j k\nAdv\u00b1 (f ) =minimize max\nx\n\nk\u2208N\nux,j \u2208Ck\n\nX\n\nsubject to\n\nj\u2208[n]\n\nj\nxj 6=yj\n\n(2)\n\nhux,j |uy,j i = 1 whenever f (x) 6= f (y).\n\nFor our application, it will be more convenient to use a different formulation of the objective\nvalue.\nClaim 3.\nv\uf8eb\nu\nu\nu\n\u00b1\nAdv (f ) =minimize t\uf8ed max\nk\u2208N\nux,j \u2208Ck\n\nsubject to\n\nx\u2208f \u22121 (1)\n\nX\n\nj\nxj =\n6 yj\n\nX\n\nj\u2208[n]\n\n\uf8f6\uf8eb\n\n2\uf8f8 \uf8ed\n\nkux,j k\n\nmax\n\ny\u2208f \u22121 (0)\n\nX\n\nj\u2208[n]\n\nkuy,j k\n\nhux,j |uy,j i = 1 whenever f (x) 6= f (y).\n\n\uf8f6\n\n2\uf8f8\n\n.\n(3)\n\nProof. The objective value in Eq. (3) is less than that of Eq. (2) by the inequality of arithmetic and\ngeometric means. For the other direction, note that the constraint is invariant under multiplying\nall vectors ux,j where f (x) = 1 by c and all vectors uy,j where f (y) = 0 by c\u22121 . In this way we\ncan ensure that the maximum in Eq. (2) is the same over f \u22121 (0) and f \u22121 (1) and so equal to the\ngeometric mean.\n\n3\n\n\fThe general adversary bound characterizes bounded-error quantum query complexity.\nTheorem 4 ([Rei11, LMR+ 11]). Let f be as above. Then Q1/4 (f ) = \u0398(Adv\u00b1 (f )).\n\n2.2\n\nMartingales and Azuma's Inequality\n\nWe assume the reader is familiar with basic notions of probability theory. In this section, we state\nsome concentration results we will need in the proof of Theorem 1. The results are rather standard,\ncan be found, e.g., in [AS08].\nA martingale is a sequence X0 , . . . , Xm of random variables such that E[Xi+1 | X0 , . . . , Xi ] =\nXi , for all i's.\nTheorem 5 (Azuma's Inequality). Let 0 = X0 , . . . , Xm be a martingale such that |Xi+1 \u2212 Xi | \u2264 1\nfor all i's. Then\n\u221a\n2\nPr[Xm > \u03bb m] < e\u2212\u03bb /2 .\nfor all \u03bb > 0.\nA standard way of defining martingales, known as Doob martingale process, is as follows. Assume f (y1 , . . . , ym ) if a real-valued function, and there is a probability distribution Y on the input\nsequences. The Doob martingale D0 , . . . , Dm is defined as\nDi = E [f (y \u2032 ) | \u2200j \u2264 i : yj\u2032 = yj ]\n\u2032\ny \u2208Y\n\nthat is a random variable dependent on y \u2208 Y . In particular, D0 = E[f ] and Dm = f (y).\nThis is a martingale, and Azuma's inequality states f (y) isn't far away from its expectation with\nhigh probability, if revealing one input variable has little effect on the expectation of the random\nvariable.\n\n3\n3.1\n\nLearning graphs\nDefinitions\n\nBy Theorem 4, to upper bound the quantum query complexity of a function, it suffices to construct\na feasible solution to Eq. (2). Trying to come up with vectors which satisfy all pairwise equality\nconstraints, however, can be quite challenging even for simple functions.\nA learning graph, introduced in [Bel11b], is a computational model that aids in the construction\nof such vectors for a function f : [m]n \u2287 D \u2192 {0, 1} with boolean output. By design, a learning\ngraph ensures that the constraint (3) is satisfied, allowing one to focus on minimizing the objective\nvalue.\nDefinition 6. A learning graph G is a directed acyclic connected graph with vertices labeled by\nsubsets of [n], the input indices. It has arcs connecting vertices S and S \u222a {j} only, where S \u2286 [n]\nand j \u2208 [n] \\ S. Each arc e is assigned a weight function we : [m]S \u2192 R+ , where S is the origin of\ne.\nA learning graph can be thought of as modeling the development of one's knowledge about the\ninput during a query algorithm. Initially, nothing is known, and this is represented by the root\nlabeled by \u2205. When at a vertex labeled by S \u2286 [n], the values of the variables in S have been\nlearned. Following an arc e connecting S to S \u222a {j} can be interpreted as querying the value of\nvariable xj . We say the arc loads element j. When talking about vertex labeled by S, we call S\nthe set of loaded elements.\n4\n\n\fIn order for a learning graph to compute function f correctly, for any x \u2208 f \u22121 (1), there should\nexist a vertex of the learning graph containing a 1-certificate for x. We call vertices containing a\n1-certificate accepting.\nLet e be a weighted arc of the learning graph from S to S \u222a {j}. In the examples of learning\ngraphs given in [Bel11b], it sufficed to assign e a weight we that depended only on the set S and\nelement j, but not the values learned. Here, we follow Remark 4 of [Bel11b] and use a more general\nmodel where we can depend both on S and j, as well as on the values of the variables in S. We\ndenote we (x) = we (xS ). Although, this notation is convenient, it is important to keep in mind\nthat values of the variables outside S do not affect the value of we . The weight 0 of an arc should\nbe thought of as the arc is missing for this particular input.\nBy G(x), we denote the instance of G for input x \u2208 D, i.e., G(x) has the same vertices and arcs\nas G does, only the weight of arc e is a real number we = we (x). Another way to think of a leaning\ngraph, is like a collection of graphs G(x) such that arcs from S to S \u222a {j} in G(x(1) ) and G(x(2) )\n(2)\n(1)\nhave equal weight if xS = xS . The arcs with the latter property are called identical.\nArcs is the main constituent of the learning graph, and we use notation e \u2208 G to denote that e\nis an arc of G. Similarly, we write e \u2208 G(x).\nThe complexity of a learning graph computing f is defined as the geometrical mean of its\npositive\nand negative complexities. The negative complexity N (G(y)) for y \u2208 f \u22121 (0) is defined as\nP\ne\u2208G(y) we . The negative complexity of the learning graph N (G) is defined as maxy\u2208f \u22121 (0) N (G(y)).\nIn order to define positive complexity, we need one additional notion.\nDefinition 7. The flow on G(x) for x \u2208 f \u22121 (1) is a real-valued function pe where e \u2208 G(x). It has\nto satisfy the following properties:\n\u2022 vertex \u2205 is the only source of the flow, and it has intensity 1. In other words, the sum of pe\nover all e's leaving \u2205 is 1;\n\u2022 vertex S is a sink iff it is accepting. That is, if S 6= \u2205 and S does not contain a 1-certificate\nof x for f then, for vertex S, the sum of pe over all in-coming arcs equals the sum of pe over\nall out-going arcs.\nP\nThe complexity of the flow is defined as e\u2208G(x) p2e /we , with convention 0/0 = 0. The positive\ncomplexity P(G(x)) is defined as the smallest complexity of a flow on G(x). The positive complexity\nof the learning graph P(G) is defined as maxx\u2208f \u22121 (1) P(G(x)).\nWe often consider a collection of flows p for all x \u2208 f \u22121 (1). In this case, pe (x) denotes the flow\npe in G(x).\nLet us briefly introduce some additional concepts connected with learning graphs. The i-th\nstep of a learning graph is theP\nset of all arcs ending in a vertex of cardinality i. If E \u2286 G is a set\nof arcs, we use notation pE = e\u2208E pe . Usually, E is a subset of a step. A special case is pS with\nS being a vertex; it is used to denote the flow through vertex S, i.e., the sum of pe over all arcs\nending at S.\nThe following technical result is extracted from [Bel11b]\nLemma 8 (Conditioning). Suppose V is a subset of vertices such that no vertex is a subset\nof another.\nLet pe be a flow from \u2205 and ending at V of intensity 1, W be a subset of V and\nP\n\u2032\n\u2032\np\nt =\nS\u2208W S . Then there exists a flow p with the same properties, such that pS = pS /t for\n\u2032\n2\n\u2032\nS \u2208 W and pS = 0, otherwise. Moreover, the complexity of p is at most 1/t times the complexity\nof p.\nThis lemma is applied as follows. One uses some construction to get a flow that ends at V .\nAfter that, another construction is applied to obtain a flow that starts at W and ends at the proper\nsinks, i.e., accepting vertices. In the second flow, vertices in V \\ W are dead-ends, i.e., no flow\n5\n\n\fshould leave them. Then it is possible to apply Lemma 8 to glue both parts of the flow together\nand get a valid flow.\n\n3.2\n\nReduction to Quantum Query Algorithms\n\nIn this section, we prove that if there is a learning graph for f of complexity C then Q1/4 (f ) = O(C).\nIn the case of f with non-boolean input alphabet this solves an open problem from [Bel11b] by\nremoving a logarithmic factor present there. We do this by showing how a learning graph can be\nused to construct a solution to Eq. (3) of the same complexity, and then appealing to Theorem 4.\nTheorem 9. If there is a learning graph for f : [m]n \u2192 {0, 1} with complexity C then Adv\u00b1 (f ) \u2264\nC.\nProof. Let G be the learning graph, we be the weight function, and p be the optimal flow.\nWe show how to construct the vectors ux,j satisfying (3) from G. Let Ej be the set of arcs\neS,S\u222a{j} between S and S \u222a {j} for some S. Notice that the set of {Ej }j\u2208[n] partition all the arcs\nin the graph. If e = eS,S\u222a{j} , let \u03b1(e) \u2208 [m]S be an assignment of values to the set labeling the\norigin of e.\nL\nS\nThe vectors ux,j will live in a Hilbert space\ne\u2208Ej ,\u03b1(e) He,\u03b1(e) where \u03b1(e) \u2208 [m] is an assignment\nL of values to the positions in S. In our case each He,\u03b1(e) = C. Thus we think of\nux,j = e\u2208Ej ,\u03b1(e) ux,j,e,\u03b1(e) , and now go about designing these vectors.\nFirst of all, if e = eS,S\u222a{j} then ux,j,e,\u03b1(e) = 0 if xS 6= \u03b1(e). Otherwise, if f (y) = 0 then we set\np\np\nuy,j,e,\u03b1(e) = we (y) and if f (x) = 1, we set ux,j,e,\u03b1(e) = pe (x)/ we (x).\nLet us check the objective value. If f (y) = 0 then we have\nXX\nX\nX\n2\nkuy,j k =\nwe (y) =\nwe (y) = N (G(y)).\nj\n\nj\n\ne\u2208Ej\n\ne\u2208G\n\nIf f (x) = 1 then\nX\nj\n\n2\n\nkux,j k =\n\nX pe (x)2\nX X pe (x)2\n=\n= P(G(x)).\nwe (x)\nwe (x)\nj\ne\u2208G\n\ne\u2208Ej\n\nThus the geometric mean of these quantities it is at most C.\nLet us now see that the constraint is satisfied.\nX\nX\nX\nhux,j |uy,j i =\nhux,j,e,xS |uy,j,e,xS i\nj:xj 6=yj eS,S\u222a{j}\nxS =yS\n\nj:xj 6=yj\n\n=\n\nX\nj\n\nX\n\neS,S\u222a{j}\nxS =yS ,xj 6=yj\n\nX\np (x) p\npe\nwe (y) =\nwe (x)\nj\n\nX\n\npe (x) = 1 .\n\neS,S\u222a{j}\nxS =yS ,xj 6=yj\n\nThe second equality from the end holds because we (x) = we (xS ) = we (yS ) = we (y) due to the\nconstruction of the weight function. To see why the last equality holds, note that the set of arcs\neS,S\u222aj where xS = yS and xj 6= yj is the cut induced by the vertex sets {S | xS = yS } and\n{S | xS 6= yS }. Since the source is in the first set, and all the sinks are in the second set, the value\nof the cut is equal to the total flow which is one.\n\n4\n\nGetting Ready\n\nThis section is devoted to the analysis of the applicability of learning graphs for the k-distinctness\nproblem, without constructing the actual learning graph. In Section 4.1, we review the tools\n6\n\n\fof [Bel11b] to the case when the arcs of the learning graph depend on the values of the variables.\nIn Section 4.2, we make the tools of in Section 4.1 easier to apply. In Section 4.3, we describe\nthe conventions on the input variables we assume for the rest of the paper. In Section 4.4, we\ndevelop an important notion of almost symmetric flow that is a generalization of symmetric flow\nused in [Bel11b]. Finally, in Section 4.5, we describe a learning graph that is equivalent to the\nprevious quantum algorithm for the k-distinctness problem.\n\n4.1\n\nSymmetries\n\nIn [Bel11b], the symmetries under consideration were those of the indices of the input variables.\nThis was sufficient because values of the variables did not affect the learning graph. In this paper,\nwe consider a wider group of symmetries, namely Sn \u00d7 Sm , where S is the full symmetric group,\nthat in the first multiplier permutes the indices, and the second one the values of the variables,\ni.e., an input x = (xi )i\u2208[n] gets mapped by \u03c3 = \u03c3i \u00d7 \u03c3o to \u03c3x = (\u03c3o (x\u03c3i i ))i\u2208[n] .\nLet \u03a3 \u2286 Sn \u00d7 Sm be the symmetry group of the problem, i.e., such that f (\u03c3x) = f (x) for all\nx \u2208 D and \u03c3 \u2208 \u03a3. For the k-distinctness problem, \u03a3 equals the whole group Sn \u00d7 Sm .\nWe extend the mapping x 7\u2192 \u03c3x to assignments, as well as vertices and arc of learning graphs in\nan obvious way. For example, an arc e \u2208 G(x) from S to S \u222a {v} is mapped to the arc \u03c3e \u2208 G(\u03c3x)\nfrom \u03c3i S to \u03c3i (S \u222a {v}). Actually, graph G(\u03c3x) may also not contain the latter arc. To avoid such\ninconvenience, we assume G is embedded into the complete graph having all possible arcs of the\nform eS,S\u222a{v} , with the unused arcs having weights and flow equal to 0. Then, it is easy to see any\n\u03c3 \u2208 \u03a3 maps a valid flow on G(x) to a valid flow on G(\u03c3x) in the sense of Definition 7. Of course,\nthe complexity of the latter can be huge, even +\u221e, because it may have a non-zero flow through\nan arc having weight 0. Consider two arcs:\nei \u2208 G(x(i) ) originating in Si and loading vi , for i = 1, 2.\n\n(4)\n\nIn this section, as well as in Section 4.2, we are going to define various equivalence relations between\nthem, mostly, to avoid the increase in the complexity of the flow under transformations from \u03a3.\nNote, in contrary to [Bel11b], we define equivalences between arcs, not transitions, i.e., chains of\narcs.\nEquivalency Arcs e1 and e2 are called equivalent iff there exists \u03c3 \u2208 \u03a3 such that \u03c3i (v1 ) = v2\n(2)\n(1)\nand \u03c3(xS1 ) = xS2 . It is natural to assume equivalent arcs have equal weight. We give a formal\nargument in Proposition 10.\nDenote by E the set of all equivalency classes of G under this relation. Also, we use notation\nEi for all equivalency classes of step i (an equivalency class is fully contained in one step, hence,\nthis is a valid notion). If E \u2208 E, we use notation E(x) to denote the subset of arcs of G(x) that\nbelongs to E.\nFor the k-distinctness, the equivalence is characterized by the structure of the subtuples of\nS. We capture this by the specification \u03b2(S) of the vertex, i.e., by a list of non-negative integers\n(b1 , b2 , . . . , bk\u22121 ) such that S contains exactly bt t-subtuples. (If S contains aPk- or a larger subtuple\nit is an accepting vertex and no arcs are leaving it). In particular, |S| = t tbt . Thus, two arcs\nare equivalent iff the specifications of their origins are equal.\nStrong equivalency The first part of this section describes the equivalence relation for arcs e1\nand e2 with respect to their weight. We would like to get a stronger equivalence that captures the\nflow through an arc. This kind of equivalency has already been used in [Bel11b] without explicit\ndefinition.\n\n7\n\n\fArcs e1 and e2 from (4) are called strongly equivalent iff there exists an element \u03c3 \u2208 \u03a3 such\nthat\n\u03c3(x(1) ) = x(2) ,\n\u03c3i (S1 ) = S2\nand\n\u03c3i (v1 ) = v2 .\n(5)\nAgain, due to symmetry, it is natural to assume the flow through strongly equivalent arcs is\nequal. If, for some positive inputs x(1) and x(2) , there is \u03c3 \u2208 \u03a3 such that the first condition of (5)\nholds, the task of finding a flow for x(2) is reduced to finding a flow for x(1) , that is again a corollary\nof Proposition 10. See also Proposition 11.\nFormal argument We give a proof that, without loss in complexity, we may assume weight and\nflow is constant on equivalent and strongly equivalent arcs, respectively.\nProposition 10. For any learning graph G, it is possible to construct a learning graph G \u2032 and a\nflow p\u2032 on it with the same or smaller complexity, so that equivalent arcs have the same weight and\nstrongly equivalent arcs have the same flow through them.\nProof. The proof is a standard application of symmetry. Let p be an optimal flow for G. We define\nthe weights of arcs in G \u2032 and the flow through it as follows:\n1 X\n1 X\nwe\u2032 (\u03b1) =\nw\u03c3e (\u03c3\u03b1),\nand\np\u2032e (x) =\np\u03c3e (\u03c3x).\n|\u03a3|\n|\u03a3|\n\u03c3\u2208\u03a3\n\n\u03c3\u2208\u03a3\n\n(1)\n\n(2)\n\nIf arcs of (4) are equivalent, there exists \u03c3 \u2032 such that \u03c3 \u2032 e1 = e2 and \u03c3 \u2032 (xS1 ) = xS2 . Hence,\nwe\u2032 2 (x(2) ) =\n\n1 X\n1 X\n(2)\n(1)\nw\u03c3e2 (\u03c3(xS2 )) =\nw\u03c3\u03c3\u2032 e1 (\u03c3\u03c3 \u2032 (xS1 )) = we\u2032 1 (x(1) ),\n|\u03a3|\n|\u03a3|\n\u03c3\u2208\u03a3\n\n\u03c3\u2208\u03a3\n\nsince \u03a3 is a group. The equality of flows is proven in a same way. Let us check the complexity.\nFor a negative input y, we have:\nX 1 X\n1 X\nw\u03c3e (\u03c3y) =\nN (G(\u03c3y)).\nN (G \u2032 (y)) =\n|\u03a3|\n|\u03a3|\n\u03c3\u2208\u03a3\n\ne\u2208G\n\n\u03c3\u2208\u03a3\n\nHence, for at least one \u03c3, N (G \u2032 (y)) \u2264 N (G(\u03c3y)). Thus, N (G \u2032 ) \u2264 N (G).\nFor the positive case, at first note that p\u2032 is a valid flow, as a convex combination of valid flows.\nFor any x \u2208 f \u22121 (1), we have\n!2\n!\u22121\nX 1 X\n1 X\n\u2032\nP(G (x)) \u2264\np\u03c3e (\u03c3x)\nw\u03c3e (\u03c3x)\n|\u03a3|\n|\u03a3|\ne\u2208G\n\n\u03c3\u2208\u03a3\n\n\u03c3\u2208\u03a3\n\nX 1 X p\u03c3e (\u03c3x)2\n1 X\n=\nP(G(\u03c3x)).\n\u2264\n|\u03a3|\nw\u03c3e (\u03c3x)\n|\u03a3|\ne\u2208G\n\n\u03c3\u2208\u03a3\n\n\u03c3\u2208\u03a3\n\n\u00012\nP\n\u03b3\u03c3 z \u03c3 \u2264\nThe second inequality follows from the Jensen's inequality\nfor the square function\n\u03c3\u2208\u03a3\n\u0001\nP\nP\n2\n\u03c3\u2208\u03a3 w\u03c3e (\u03c3x) and z\u03c3 = p\u03c3e (\u03c3x)/\u03b3\u03c3 . Due to the same argu\u03c3\u2208\u03a3 \u03b3\u03c3 z\u03c3 , with \u03b3\u03c3 = w\u03c3e (\u03c3x)/\nment, P(G \u2032 ) \u2264 P(G).\n\n4.2\n\nLoosening equivalencies\n\nAlthough equivalencies defined in the previous section are optimal, they are not always convenient\nto work with. They turn out to be too strong, that results in a vast number of equivalency classes\nthat should be treated separately. In this section, we describe a number of ways to loosen these\nequivalences, thus reducing the number of classes and making them easier to work with.\n8\n\n\fEquivalency Assume the weight function is decomposed as we (\u03b1) = we (\u03b8(\u03b1)), where \u03b8 is some\n\"filter\" that captures the properties of \u03b1 we are interested in. It is good to assume symmetry\npreserves \u03b8, i.e., \u03b8(\u03b11 ) = \u03b8(\u03b12 ) implies \u03b8(\u03c3\u03b11 ) = \u03b8(\u03c3\u03b12 ) for any \u03c3 \u2208 \u03a3. Transitions e1 and e2 are\n(2)\n(1)\ncalled \u03b8-equivalent iff there exists \u03c3 \u2208 \u03a3 such that \u03c3i (v1 ) = v2 and \u03c3(\u03b8(xS1 )) = \u03b8(xS2 ). It is again\nnatural to assume \u03b8-equivalent arcs have the same weight. Two main examples are:\n\u03b81 , the identity. This results in the relation from the previous section. This is the main equivalency used in Section 5;\n\u03b82 , mapping \u03b1 to its domain D(\u03b1). For the k-distinctness, arcs e1 and e2 from (4) are \u03b82 equivalent iff |S1 | = |S2 |. This is the equivalency used in [Bel11b].\nStrong equivalency Unlike equivalency, strong equivalency turns out to be too strong for all\nour applications. We can weaken it by considering G as a learning graph for function f \u0303 that gets\nas input x\u0303, the original input x with some information removed.\nMore precisely, extend the output alphabet [m] with a set of special characters Q. Let \u03b8 : f \u22121 (1) \u2192\n([m] \u222a Q)n be the function that maps x to x\u0303. We extend elements of \u03a3 to ([m] \u222a Q)n by assuming\n\u03c3o (c) = c, if \u03c3i \u00d7 \u03c3o \u2208 \u03a3 and c \u2208 Q.\nThe function f \u0303: ([m] \u222a Q)n \u2192 {0, 1} is defined as f \u0303(\u03b8(x)) = 1 for all x \u2208 f \u22121 (1). Let G\u0303 be\nthe same learning graph as G but calculating f \u0303. Arcs e1 and e2 from (4) are called \u03b8-strongly\nequivalent iff the corresponding arcs \u1ebd1 \u2208 G\u0303(\u03b8(x(1) )) and \u1ebd2 \u2208 G\u0303(\u03b8(x(2) )) are strongly equivalent.\nFor this construction to work, we require a stronger definition of a 1-certificate for f \u0303. We say\nan assignment \u03b1 : [n] \u2287 M \u2192 [m] \u222a Q is a 1-certificate if, for all x \u2208 f \u22121 (1) such that \u03b8(x)M = \u03b1,\nxM is a 1-certificate of x in f . With this definition, any valid flow in G\u0303(\u03b8(x)) is simultaneously a\nvalid flow in G(x).\nWe give three examples of \u03b8's.\n\u03b81 , the identity. This results in the relation from the previous section. We have no example of\nusing this equivalency;\n\u03b82 , the equivalency used in [Bel11b]. Let Q = {*, \u22c6}. For a positive input x, fix some 1-certificate\n\u03b1. Let M be the domain of \u03b1. The elements of M are called marked. Define x\u0303 = \u03b82 (x) as\n(\n\u22c6, i \u2208 M ;\nx\u0303i =\n*, otherwise.\nClearly, \u03b82 (x)M is a 1-certificate. Refer to Section 4.5 for an example of usage of this\nequivalency.\n\u03b83 , defined in Section 4.3. The main equivalency used in Section 5.\nAgain, we assume the flow through \u03b8-strongly equivalent arcs is equal. The equivalencies we\nuse in the paper possess two additional symmetric properties. Firstly, if x(1) , x(2) \u2208 f \u22121 (1) and\n\u03c3 \u2208 \u03a3 are such that \u03c3(\u03b8(x(1) )) = \u03b8(x(2) ) then, for any 1-certificate \u03b1 of \u03b8(x(1) ), \u03c3\u03b1 is a 1-certificate\nof \u03b8(x(2) ). Secondly, any \u03b8-strong equivalency class, having non-zero flow through it, is completely\ncontained in some \u03b8-equivalency class.\nProposition 11. If \u03b8, \u03b8, x(1) , x(2) and a flow pe on G\u0303(\u03b8(x(1) )) satisfy the above conditions and\n\u03b8-equivalent arcs in G have the same weight, then \u03c3pe is a valid flow on G\u0303(\u03b8(x(2) )) with the same\ncomplexity (that, also, is a valid flow on G(x(2) )).\n\n9\n\n\f\u03b8O2 o\n\n\u03b8O1 eLo L\nId\nLLL\n\u2217\nL\n\u03b83 o\n\u03b81\n\n\u03b82 o\n\nFigure 1: Relations between equivalencies. Arrows are from a more strong relation to a weaker one.\nId is the identity relation from Section 3.1, \u03b8's are equivalencies and \u03b8's are strong equivalencies.\nImplication from \u03b83 to \u03b81 only holds for arcs with non-zero flow.\n\n4.3\n\nConventions for k-distinctness\n\nStrong equivalency, as defined in Section 4.1, turns out to be too strong for the k-distinctness\nproblem, because for most of the pairs x(1) , x(2) \u2208 f \u22121 (1) there is no \u03c3 such that \u03c3(x(1) ) = x(2) ,\nand, hence, no arcs from G(x(1) ) and G(x(2) ) can be strongly equivalent, whatever G is. We use\nthe loosening tool of Section 4.2 to define \u03b83 so that there always exists \u03c3 that maps \u03b83 (x(1) ) to\n\u03b83 (x(2) ). Then, by Proposition 11, defining a flow for any positive input x is enough to get a flow\nfor all positive inputs.\nLet the set of special characters be Q = {*}. Fix an arbitrary positive input x. First of all, we\nidentify a subset M of k equal elements (the marked elements in terminology of \u03b82 ). Next, due to\nthe condition in Theorem 1, we may assume there are non-negative integers l1 , . . . , lk\u22121 such that\nin any valid input (either positive, or negative) there are at least lt t-tuples and\nn\u2212\n\nk\u22121\nX\n\n\u221a\ntlt = O( 4 n).\n\nt=1\n\nWe arbitrary select lt t-tuples.\nDenote by At the union of the selected t-tuples. We also use\nSk\u22121\nnotation A\u2265t to denote j=t\nAj . We define x\u0303 = \u03b83 (x) as\n(\nxi , i \u2208 A\u22651 \u222a M ;\nx\u0303i =\n*,\notherwise.\nClearly, assignment x\u0303M is a 1-certificate. In the learning graph for f \u0303, defined using \u03b83 , we will\nhave pe (x) = 0 if the origin of e has at least one * in \u03b83 (x). This convention assures that \u03b81 and\n\u03b83 satisfy the conditions of Proposition 11. Further, we are going to ignore vertices having *'s in\nthem. Figure 1 describes which of the defined equivalence relations imply which.\nLet us use this spot to mention one more convention on the input. Namely,\n\u2200t \u2264 k \u2212 1 : lt = \u03a9(n).\n\n(6)\n\nAny other case can be reduced to this one by extending the input by n t-tuples with elements\noutside the range of the original problem, for each t \u2264 k \u2212 1.\nThe strong equivalency class (with respect to \u03b83 ) of an arc depends solely on the types of its\ninitial and target vertices. The type \u03b2\u0303(S) of vertex S is an (k \u2212 1) \u00d7 k-matrix (bt,s ), where bt,s\nis the number of t-subtuples of S contained in As (or M , if s = k). Most of the time, we will\nimplicitly assume bt,k = 0 for all t's, hence, describe the type of a vertex by an (k \u2212 1) \u00d7 (k \u2212 1)matrix, assuming the removed row contains only zeroes. Note also that the specification (bt ) can\nPk\nbe expressed using the type: bt = s=1 bt,s .\nWe will have to measure distance between types. When doing so, we treat them as vectors.\nI.e., the distance between types \u03b2\u0303(S) = (bs,t ) and \u03b2\u0303(S \u2032 ) = (b\u2032s,t ) is defined as\nk\u03b2\u0303(S) \u2212 \u03b2\u0303(S \u2032 )k\u221e = max |bs,t \u2212 b\u2032s,t |.\ns,t\n\n10\n\n\fNegative complexity Here we estimate how the restriction from the actual number of t-tuples\nin the input to lt ones in A\u22651 affects the negative complexity.\nLemma 12. Consider a set A\u2032\u22651 that is defined similarly to A\u22651 , only it P\nhas l\u2032t t-tuples. Assume\n|lt \u2212 l\u2032t | \u2264 d = o(n) for all t's. Let (bt ) be any specification such that b = t bt = o(n). Then the\nratio of the number of subsets satisfying (bt ) in A\u22651 and A\u2032\u22651 is at most eO(db/n) .\nProof. It is straight-forward to calculate the number of subsets of A\u22651 satisfying specification (bt ).\nIndeed, it equals\n\"\n\u0012\n\u0013\u0012 \u0013bs,s #\nX k\u22121\nY \u0012 ls \u0013\u0012s\u0013b1,s \u0012ls \u2212 b1,s \u0013\u0012s\u0013b2,s\nls \u2212 b1,s \u2212 * * * \u2212 bs\u22121,s\ns\n***\n(7)\nb\nb\nb\n1\n2\ns\n1,s\n2,s\ns,s\ns=1\n(bt,s )\n\nwhere the summation is over all types (bt,s ) that agree with specification (bt ). Eq. (7), with lt\nreplaced by l\u2032t , gives the corresponding number of subsets in A\u2032\u22651 . It is enough to show that each\nmultiplier featuring ls in (7) changes by at most a factor of eO(db/n) . But we have:\n\u0012\n\n\u0013 \u0012\n\u0012 \u0013\u0013O(b)\n\u0013 \u0012\nd\nls \u2212 b1,s \u2212 * * * \u2212 bt\u22121,s\nl\u2032s \u2212 b1,s \u2212 * * * \u2212 bt\u22121,s\n= 1+O\n/\n= eO(db/n) ,\nbt,s\nbt,s\nn\n\nwhere we used that ls = \u0398(n), because of (6).\nSince the complexity mentioned in Theorem 1 is o(n3/4 ), it is natural to assume no vertex of the\nlearning\ngraph has more elements. It's actually the case, as described in Section 5. The precision\n\u221a\nO( 4 n) in the formulation of Theorem 1 has been chosen so that restriction of the flow to A\u22651 \u222a M\ndoes not hurt the negative complexity, as it can be seen from the next\nCorollary 13. Fix any possible negative input y, and any valid specification (bt ) with all entries\no(n3/4 ). Then, the number of subsets of [n] satisfying (bt ) is bounded by a constant times the\nnumber of such subsets included in A\u22651 .\nBecause of this, we may act as if the set of input variables is A\u22651 \u222a M , not [n].\n\n4.4\n\nAlmost symmetric flows\n\nAssume the following scenario. We have chosen which equivalency classes will be present in the\nlearning graph. Also, for each positive input, we have constructed a flow. The task is to weight the\narcs of the learning graph to minimize its complexity. In this section, we define a way of performing\nthis task, if the flow satisfies some requirements.\nFor the k-distinctness problem, all arcs leaving a vertex are equivalent, hence, to specify which\nequivalency classes are present, it is enough to define which vertices have arcs leaving them. For\neach step, we define a set of valid specifications. If a vertex before the step satisfies one of them,\nwe draw all possible arcs out of it. Otherwise, we declare it a dead-end and draw no arcs out of it.\nThe flow is called symmetric in [Bel11b] if, for each equivalency class, the flow through an arc\nof it is either 0, or p, where p does not depend on the input, but may depend on the equivalency\nclass; also it is required that the number of arcs having flow p does not depend on the input as\nwell. This notion was sufficient for the applications in that paper, because \u03b82 -strong equivalence\nwas used, and that is easy to handle. In this paper, we use \u03b83 -strong equivalence, and it is not\nenough with symmetric flows. Thus, we have to generalize this notion.\n\n11\n\n\fDefinition 14. The flow is called almost symmetric if, for each equivalency class E, there exist\nconstants \u03c0(E) and \u03c4 (E) such that, for each positive input x, there exists a subset G(E, x) \u2286 E(x)\nsuch that\n\u0012X\n\u0013\n\u0012\n\u0013\nX\n2\n2\npe (x) = \u03a9\npe (x)\n\u03c4 (E)|G(E, x)| = \u0398\nmax |E(y)| ,\ne\u2208E(x)\ny\u2208f \u22121 (0)\n(8)\ne\u2208G(E,x)\nand \u2200e \u2208 G(E, x) : pe (x) = \u0398(\u03c0(E)).\n\nThe elements inside G(E, x) are called typical arcs. Number \u03c4 (E) is called the speciality of the\nequivalency class (as well, as of any arc in the class). We also define the typical flow through E as\n\u03bc(E) = \u03c0(E) maxx\u2208f \u22121 (1) |G(E, x)|. It is straight-forward to check that\n\u2200x \u2208 f \u22121 (1) : \u03bc(E) = O(pE (x)).\n\n(9)\n\nTheorem 15. If the \u0010flow is almost symmetric,\nthe learning graph can be weighted so that its\n\u0011\np\nP\ncomplexity becomes O\nE\u2208E \u03bc(E) \u03c4 (E) .\n\np\nProof. For each arc e in an equivalency class E, we assign weight we = \u03c0(E)/ \u03c4 (E). Let us\ncalculate the complexity. For each y \u2208 f \u22121 (0), we have the following negative complexity\n!\nX\nX \u03c0(E)\nX\np\np\nwE |E(y)| =\n|E(y)| = O\n\u03c0(E) \u03c4 (E) max |G(E, x)| .\nx\u2208f \u22121 (1)\n\u03c4 (E)\nE\u2208E\nE\u2208E\nE\u2208E\nFor a positive input x \u2208 f \u22121 (1), we have\nX 1 X\npe (x)2 = O\nwE\n\nE\u2208E\n\ne\u2208E(x)\n\nX\n\nE\u2208E\n\n!\np\n\u03c4 (E)\n|G(E, x)|\u03c0(E)2 = O\n\u03c0(E)\n\nX\n\nE\u2208E\n\n!\np\n\u03bc(E) \u03c4 (E) .\n\nBy combining both estimates, we get the statement of the theorem.\nFor each step i, define Ti = maxE\u2208Ei \u03c4 (E). Then Theorem 15 together with (9) and the\nobservation that the total flow through all arcs on any step is at most 1, implies the following\nCorollary 16. If the P\nflow\u221ais \u0001almost symmetric, the learning graph can be weighted so that its\ncomplexity becomes O\nTi where the sum is over all steps.\ni\n\n4.5\n\nPrevious Algorithm for k-distinctness\n\nAs an example of application of Corollary 16, we briefly describe a variant of a learning graph\nfor the k-distinctness problem. It is a direct analog of an algorithm from [Amb07] using learning\ngraphs and a straightforward generalization of the learning graph for element distinctness from\n[Bel11b].\nTo define equivalencies between arcs, we use \u03b82 and \u03b82 from Section 4.2. The learning graph\nconsists of loading r + k elements without any restrictions (as imposed by \u03b82 ), where r is some\nparameter to be specified later. We refer to the first r steps as to the first stage, and to the last k\nsteps as to the second stage.\nClearly, all arcs of the same step are equivalent. Consider strong equivalency. Let x be a\npositive input and let M be a subset of k equal elements in it. We use M as the set of marked\nelements to define \u03b82 . Then, the strong equivalence class of an arc is determined by the number\n\n12\n\n\fof elements in its origin, the number of marked elements among them, and whether the element\nbeing loaded is marked.\nThe flow is organized as follows. On the first stage, only arcs without marked elements are\nused. On the second stage, only arcs loading marked elements are used. Thus, on each step only\none strong equivalency class is used, and the flow among all arcs in it is equal.\nIt is easy to check this is a valid flow for k-distinctness and it is symmetric. Let us calculate\nthe specialities. The first r steps have speciality O(1). The speciality of the i-th step of the second\nstage is O(ni /ri\u22121 ). This is because the fraction of (r + i \u2212 1)-subsets of [n] containing i \u2212 1 marked\nelements is \u0398(ri\u22121 /ni\u22121 ); and k \u2212 i + 1 arc only, out of \u0398(n) originating in such\nis used\n\u0010 vertex,\n\u0011 by\np\nk\nk\u22121\nthe flow. Hence, by Corollary 16, the complexity of the learning graph is O r + n /r\nthat\n\u0001\nk/(k+1)\nk/(k+1)\nis optimized when r = n\nand the optimal value is O n\n.\n\n5\n\nAlgorithm for k-distinctness\n\nThe purpose of this section is to prove Theorem 1. In Section 5.1, we give some intuition behind\nthe learning graph. In Section 5.2, we describe the learning graph, or, more precisely, define valid\nspecifications for each step, as described in Section 4.3. In Section 5.3, we define the flow, and\ngive preliminary estimates of the complexity. Finally, in Section 5.4, we prove the flow defined in\nSection 5.3 is almost symmetric and prove the estimates therein are correct.\n\n5.1\n\nIntuition behind the algorithm\n\nThere is another way to analyze the complexity of the learning graph in Section 4.5.\nLemma 17. Assume convention (6) on the input. The expected number of t-subtuples in an\nr-subset of A\u22651 , chosen uniformly at random, is \u0398(rt /nt\u22121 ).\nProof. Let S be the random subset. Denote n\u2032 = |A\u22651 |. The probability a fixed subset of t equal\n\u0001 n\u2032 \u0001\n\u2032\n\u2212s\nelements from As forms a t-subtuple in S is nr\u2212t\n/ r = \u0398(rt /nt ). The number of such subsets\n\u0001\nP\nis s ls st = \u0398(n). Hence, by linearity of expectation, the expected number is \u0398(rt /nt\u22121 ).\n\nConsider the following informal argument. Let M be the set of marked elements as in Section 4.5. Before the last step, the flow only goes through vertices S having |S \u2229 M | = k \u2212 1. Fix\na vertex S and let M \u2032 = M \u2229 S. One may say, M \u2032 as a (k \u2212 1)-subtuple, is hidden among other\n(k \u2212 1)-subtuples of S. The expected number of such is \u0398(rk\u22121 /nk\u22122 ), total number of (k \u2212 1)tuples is \u0398(n), hence, the fraction of the vertices used by the flow on this step is \u0398(rk\u22121 /nk\u22121 ).\nThus, the speciality of the arc loading the missing marked element is \u0398(nk /rk\u22121 ) that equals the\nestimate in Section 4.5.\nAs such, this is just a more difficult and less strict analysis of the learning graph. But one can\nsee that the speciality of the last steps depends on the number of t-subtuples in the vertices. We\ncannot get a large quantity of them by loading elements blindly without restrictions, but it is quite\npossible, we can deliberately enrich vertices of the learning graph in large subtuples by gradually\nfiltering out vertices containing a small number of them.\n\n5.2\n\nDescription of the Learning graph\n\nWe would like to apply Corollary 16, hence, it is enough to give valid specifications for each step.\nWe do this using a pseudo-code notation in Algorithm 1.\nHere r1 , . . . , rk\u22121 are some parameters with ri+1 = o(ri ), r1 = o(n) and rk\u22121 = \u03c9(1) to be\nspecified later. Also, it will be convenient to denote r0 = n. The commands of the algorithm\n13\n\n\fAlgorithm 1 Learning graph for the k-distinctness problem\n1: for j \u2190 1 to r1 do\n2:\nLoad an element\n3: end for\n4: Declare as dead-ends vertices having more than ct r1t /nt\u22121 t-subtuples for any t = 2, . . . , k \u2212 1\n5: for i \u2190 2 to k \u2212 1 do\n6:\nfor j \u2190 1 to ri do\n7:\nfor l \u2190 1 to i do\n8:\nLoad an element of level l\n9:\nend for\n10:\nend for\n11: end for\n12: Load an element\n// The last element is no subject to any constraints\n\ndefine the specifications as follows. The loop in lines 1-3 says there is no constraint on the first\nr1 steps. Line 4 introduces the original specifications. Here, ct > 0 are some constants we specify\nlater.\nThe loop in Lines 5-11 describes how the specifications change with each step. Assume a step,\ndescribed on Line 8, loads an element of level l. Then, a valid specification (bt ) before the step is\ntransformed into a valid specification (b\u2032t ) after the step as follows\n\uf8f1\n\uf8f4\n\uf8f2bt + 1, t = l;\n\u2032\nbt = bt \u2212 1, t = l \u2212 1;\n\uf8f4\n\uf8f3\nbt ,\notherwise.\n\nIn other words, if there is an arc between vertices of specifications (bt ) and (b\u2032t ) and it load v then\nthere exists an (l \u2212 1)-subtuple Q of S such that Q \u222a {v} is an l-subtuple of S \u222a {v}. In fact, only\nsuch arcs will be used by the flow, as it is described in more detail in Section 5.3.\nHence, for each specification in Lines 5-12, it is possible to trace it back to its original specification. For example, if (bt ) is a specification of the vertex after step in Line 8 with the values of\nthe loop counters i, j and l, the original specification is given by (b\u0303t ) \u2212 (\u03b4tl ), where\n\uf8f1\n(\n\uf8f4\n2 \u2264 t < i;\n\uf8f2bt \u2212 rt ,\n1, t = l;\nl\nb\u0303t = bt \u2212 j + 1, t = i;\nand\n\u03b4t =\n\uf8f4\n0, otherwise.\n\uf8f3\nbt ,\notherwise;\n\nMoreover, the use of the arcs in the flow, as described in the previous paragraph, implies the flow\nthrough all vertices having some fixed original specification is the same for all steps.\nFinally, the step on Line 12 loads the last element, and there is no need for the dead-end\nconditions, because after the last step all vertices have no arcs leaving them.\n\nNaming convention We use the following convention to name the steps of the learning graph.\nThe step on Line 2 is referred as the j-th step of the first stage. The step on Line 8 is referred\nusing triple (i, j, l), except for the case i = k \u2212 1 and j = rk\u22121 . The latter together with the step\non line Line 12 is referred as the steps 1, 2, . . . , k of the last stage. The steps of the form (i, *, *) are\ncalled the i-th stage. Altogether, all steps of the form (*, *, *) are called the preparatory phase.\n\n14\n\n\f5.3\n\nFlow\n\nWe two possible ways to define a flow. The first one is to set the flow through the arcs on each\nstep so that the flow through all vertices on each step is the same. We believe this can be done,\nbut we lack techniques to deal with this kind of arguments.\nInstead of that, we select the second way. For each vertex, we divide the flow evenly among all\npossible arcs. Because of this, the ratio of the maximal and the minimal flow accumulates with each\nstep, and at the end it is quite large. We avoid this complication by applying the concentration\nresults stating that for large n's almost all flow will be concentrated on some typical subset of arcs\nand will be distributed almost evenly on it.\nFirst Stage For the first stage, we use \u03b82 - and \u03b82 -based equivalencies, akin to the first stage of\nthe flow in Section 4.5. Consider the uniform flow, i.e., such that distributes all the in-coming flow\namong all out-going arcs, leading to an element of A\u22651 , equally. Clearly, it is symmetric, and the\n\u0001\n| \u22121\n. The speciality of each step in\nflow through any vertex S \u2286 A\u22651 after the first stage is |Ar\u22651\n1\nthis flow is O(1) because of Corollary 13.\nBut this flow is non-zero for vertices declared as dead-ends in Line 4. We fix this by applying\nLemma 8. We have to choose ct > 0 so that, with probability, say, 1/2, an uniformly picked subset\nof size r1 satisfies a valid specification. And it is possible to do so due to Lemma 17 and Markov's\ninequality.\nAfter performing the conditioning, the complexity of the flow in the first stage increases by at\nmost a constant factor (that can be ignored), and all non-dead-end vertices have the same flow\nthrough them, we denote po .\nPreliminary Estimates For the remaining stages, we use \u03b81 and \u03b83 to define equivalences\nbetween arcs. Here we informally analyze the flow for Lines 5-12 of Algorithm 1, assuming there\nis flow po through all non-dead-end vertices after Line 4. The formal analysis is done in Section 5.4.\nRoughly speaking, the flow is organized as follows. On step (i, j, l), an element, not in M ,\nbelonging to level l is loaded. On any step of the last stage, an element of M is loaded. Let\nus estimate the complexity of the learning graph. Assume for the moment the flow is almost\nsymmetric.\nApproximately n arcs are leaving a vertex on each step. Let (i, j, l) be a step of the preparatory\nphase and assume l > 1. An element of level l is loaded, and there are \u03a9(rl\u22121 ) (l \u2212 1)-subtuples in\nthe vertex that can be extended. Hence, \u03a9(rl\u22121 ) arcs leaving the vertex can be used by the flow.\nThis makes the speciality of the step equal to O(n/rl\u22121 ). This is true for l = 1 as well, because of\nthe convention r0 = n.\nNow turn to the last stage. Let us calculate the speciality of a vertex used by the flow on step\nj > 1 of the last stage. Let V0 be the vertices contained in A\u22651 having a valid specification, and\nVM be the vertices of A\u22651 \u222a M that can be used by the flow. Define relation \u03c6, where S0 \u2208 V0 is\nin relation with SM \u2208 VM if SM can be obtained from S0 by removing one of its (j \u2212 1)-subtuples\nand adding j \u2212 1 elements from M instead. Each S0 has \u03a9(rj\u22121 ) images and each SM has O(n)\npreimages. Hence, |V0 |/|VM | = O(n/rj\u22121 ). Because only O(1), out of \u0398(n) arcs leaving a vertex\nfrom VM , can be used by the flow, we have the speciality of step j of the last stage equal to\nO(n2 /rj\u22121 ). This also is true for j = 1. All this is summarized in Table 1.\nIf we could apply Corollary 16, we would get the complexity\n\u0011\n\u0010\np\np\np\n\u221a\nO r1 + r2 n/r1 + r3 n/r2 + * * * + rk\u22121 n/rk\u22122 + n/ rk\u22121 .\n\n15\n\n\fStep\nSpeciality\nNumber\n\nFirst stage\n1\nr1\n\nPreparatory, (*, *, l)\nn/rl\u22121\nrl\n\nLast stage, j-th\nn2 /rj\u22121\n1\n\nTable 1: Parameters (up to a constant factor) of the stages of the learning graph for the kdistinctness problem.\nDenote \u03c1i = logn ri and assume all the addends are equal. Then\n1\n\u03c1i\u22121\n1\n\u03c1i\n+ \u03c1i \u2212\n= + \u03c1i+1 \u2212 ,\n2\n2\n2\n2\n\ni = 1, . . . , k \u2212 1\n\nwhere we assume \u03c10 = 1 and \u03c1k = 1/2. It is equivalent to \u03c1i \u2212 \u03c1i+1 = (\u03c1i\u22121 \u2212 \u03c1i )/2. Hence,\n1/2 = \u03c10 \u2212 \u03c1k = (2k \u2212 1)(\u03c1k\u22121 \u2212 \u03c1k ).\nThus, the optimal choice of \u03c11 is 1 \u2212 2k\u22122 /(2k \u2212 1).\nWe use these calculations to make our choice of ri = n\u03c1i . It remains to strictly define the flow,\nprove it is almost symmetric and the estimates in Table 1 are correct. Before doing so, we combine\nsome estimates on the values of ri 's in the following\n\u221a\n\u221a\nProposition 18. We have r1 r2 = o(n) and r1 = o(ri ) for any i. Also, any valid specification\non stage i, has \u0398(rj ) j-subtuples for j < i.\nProof. The first equation follows from \u03c11 < 3/4 and \u03c12 < 5/8. The second inequality follows from\n\u03c1i \u2265 1/2 for all i's.\nDue to Line 4 of the algorithm,\nafter the first stage, any valid specification has O(r1i /ni\u22121 )\n\u221a\ni-subtuples. For i > 1, it is o( n) = o(rj ) for any j. Hence, after the first stage there are \u0398(r1 )\n1-subtuples, and this number does not substantially change after that. Similarly, if one doesn't\ntake into account the \u00b11-fluctuations, the number of j-subtuples is changed only on stage j, when\nrj j-subtuples are added.\nValues of the flow Let us describe how the flow is defined. Fix some stage i. A vertex before\na step of the form (i, *, 1) is called a key vertex. Consider a key vertex S with type (bt,s ). The flow\nfrom S is distributed evenly among all succeeding key vertices, where S \u2032 is a succeeding key vertex\nfor S iff S \u2032 \\ S is a subset of equal elements having a value different from any element of S. The\nnumber of succeding key vertices for S is\n\u0013\n\u0012X\nXk\u22121\nk\u22121\nbt,k\u22121\nbt,1 , . . . ,\nN (S) = Di\nt=1\n\nt=1\n\nwhere\n\nDi (z1 , . . . , zk\u22121 ) =\n\nk\u22121\nX\ns=i\n\n(ls \u2212 zs )\n\n\u0012 \u0013\ns\ni\n\nis the number of possible i-subtuples to extend the vertex with, when zs s-tuples have already been\nused.\nMore precisely, let e be an arc of step (i, j, l) originating in a non-dead-end vertex S \u2032 and loading\nan element v. Then the flow through this arc is defined using the values of the flow through key\nvertices before step (i, j, 1) as follows:\n\uf8f1\n\uf8f2 s\u0001 s\u0001\u22121 pS\u2032 \\Q , |Q| = l and s \u2265 i, where Q = {\u03b9 \u2208 S \u2032 \u222a{v} | x\u03b9 = xv }\nlN (S \u2032 \\Q)\ni l\nand s is such that Q is contained in As ;\npe =\n(10)\n\uf8f30,\notherwise.\n16\n\n\fIf the first case in (10) holds, vertex S \u2032 \\ Q is called the key vertex preceeding arc e. Note that it\nis uniquely defined.\n\n5.4\n\nAnalysis of the flow\n\nTypical vertices The point of this section is to prove the flow defined in Section 5.3 is almost\nsymmetric. For this, we should identify the set of typical arcs. Before doing so, we define typical\nvertices.\nLet \u03b2 = (bt ) be a valid specification of the preparatory phase. Select any t \u2208 [k \u2212 1] and let Xt\nbe the collection of all subsets of A\u22651 consisting of bt t-subtuples. In other words, elements of Xt\nsatisfy specification (0, . . . , 0, bt , 0 . . . , 0). Denote et,s = ES\u2208Xt [bt,s (S)], where bt,s (S) = |S \u2229 As |/t\nis the element of \u03b2\u0303(S). Denote \u03b5\u03b2 = (et,s ).\n\u221a\nA type (bt,s ), consistent with \u03b2, is called typical if it is inside B(\u03b5\u03b2 , C r1 ), where C is a constant\n\u221a\nto be specified later, i.e., if for all t and s holds |bt,s \u2212 et,s | \u2264 C r1 . A typical vertex is one of a\ntypical type. Let us state some properties of the typical vertices.\nLemma 19. Let (bt,s ) be the type of any typical vertex on the i-th stage. Then, for all t < i and\ns \u2265 t, we have bt,s = \u03a9(rt ).\n\u221a\nProof. Since r1 = o(rt ), it is enough to show that et,s = \u03a9(rt ). Let S be an element of Xt .\nArbitrarily order its subtuples: S = {s1 , . . . , sbt }. Clearly, the expectation is the same for ordered\nand unordered lists of subtuples, so let us consider the former.\nBy\u0001linearity of expectation, et,s = bt Pr[s1 \u2286 As ]. The number of sequences having s1 in As\nis ls st times the number of ways to pick the remaining bt \u2212 1 t-subtuples out of A\u22651 where one\ns-tuple cannot be used. By (6) and Lemma 12, these numbers are equal for different s \u2265 t, up to\na constant factor. Hence, the probability is \u03a9(1), and since bt = \u03a9(rt ), we have et,s = \u03a9(rt ).\n\u221a\nLemma 20. For any valid specification \u03b2 of the preparatory phase and for any \u03bb > C r1 ,\n2\n\nPr[k\u03b2\u0303(S) \u2212 \u03b5\u03b2 k\u221e > \u03bb] < e\u2212\u03a9(\u03bb\n\n/r1 )\n\n,\n\n(11)\n\nwhere the probability is uniform over all subsets S of A\u22651 satisfying \u03b2.\nWe derive the lemma from the following two pure technical results\nProposition 21. Let H be the disjoint union of (Ht )t\u2208[k] where each Ht is a rectangular array\nof dots, having lt columns and mt rows. Let X be the set of all r-element subsets of H where no\nsubset has more than 1 dot from any column of any Ht . Occupy X with the uniform probability\ndistribution and let ht : X \u220b S 7\u2192 |S \u2229 Ht |. Assume k = O(1), lt = \u0398(n) and r = o(n). Then:\n2\n\nPr [|ht \u2212 E[ht ]| > \u03bb] < e\u2212\u03a9(\u03bb\n\n/r)\n\n,\n\n(12)\n\nfor any \u03bb > 0.\nProof. This is a standard application of Azuma's inequality (Theorem 5). Suppose, we sort the\nelements of each S \u2208 X in any order: S = {s1 , . . . , sr }. Clearly, the probability equals for unsorted\nand for sorted lists. We use both interchangeably in the proof.\nLet Di be the Doob martingale with respect to this sequence. We have to prove that |Di \u2212\nDi\u22121 | = O(1), i.e., the expectation of ht does not change much when a new element of the sequence\nis revealed. To simplify notations, we prove only |D1 \u2212 D0 | = O(1), the remaining inequalities\nbeing similar.\nFor the proof, we define two other classes of probability distributions, all being uniform:\n17\n\n\f\u2022 Yi : r-subsets of H \\ Q, where Q is a fixed column of Hi ;\n\u2022 Zi : (r \u2212 1)-subsets of H \\ Q.\nFor the martingale, it is enough to prove that, for all i:\n|E[ht | X] \u2212 (E[ht | Zi ] + \u03b4i,t )| = O(1),\nwhere \u03b4i,t is the Kronecker delta. We have\nE[ht | X] = Pr[Q \u2229 S 6= \u2205] (E[ht | Zi ] + \u03b4i,t ) + Pr[Q \u2229 S = \u2205] E[ht | Yi ].\n\n(13)\n\nHence, it is enough to prove that\n| E[ht | Zi ] \u2212 E[ht | Yi ]| = O(1).\n\n(14)\n\nDenote l\u2032j = lj \u2212 \u03b4i,j ; and let Kh and Kh\u2032 be the number of elements S of Zi and Yi , respectively,\nhaving ht (S) = h. Note that Kh\u2032 = \u03b3h Kh\u22121 , where h \u2208 [r] and \u03b3h = ((l\u2032t \u2212 h + 1)mt )/h. Thus\nPr\nPr\nPr\n\u2032\nhKh\u22121\nh=1 hKh\nh=1 h\u03b3h Kh\u22121\nP\nP\nPh=1\n[h\n|\nY\n]\n\u2264\n=\n\u2264\n= 1 + E[ht | Zi ],\nE t i\nr\nr\nr\n\u2032\nK\n\u03b3\nK\nh=1\nh=1 h h\u22121\nh=1 Kh\u22121\nh\n\nwhere the second inequality holds because \u03b3h monotonely decreases. Thus, by linearity of expectation, E[ht | Yi ] \u2265 E[ht | Zi ] \u2212 k + 1, for all i, thus proving (14). An application of Azuma's\ninequality finishes the proof of the proposition.\n\nLemma 22. Assume i, j \u2208 [k \u2212 1], at least one of them is not 1, and m = O(1) is an integer. Let \u03bc\n2\n\u221a\nbe a probability distribution on Rm such that \u03bc(Rm \\B(\u03bb)) \u2264 e\u2212C1 \u03bb /ri for any \u03bb \u2265 C2 ri . Assume\nw is a positive real function, defined on the support of \u03bc, such that w(x)/w(y) \u2264 eC3 rj kx\u2212yk\u221e /n\nfor any x, y. Here C1 , C2 , C3 are some positive constants. Then there exists a constant C > 0 such\nthat\nZ\nZ\n\u2212\u03a9(\u03bb2 /r1 )\nw(x) d\u03bc(x) = e\nw(x) d\u03bc(x)\n\u221a\nfor any \u03bb \u2265 C r1 .\n\nRm \\B(\u03bb)\n\nRm\n\nProof. In the proof, C with a subindex denotes a positive constant that may depend on other C's.\n\u221a\nLet \u03bd be a measure on ]C2 ri , +\u221e[ such that \u03bd(]\u03bb, +\u221e[) = \u03bc(Rm \\ B(\u03bb)). The worst case,\n2\nwhen the mass of \u03bc is as far from the origin as possible, is when \u03bd(]\u03bb, +\u221e[) = e\u2212C1 \u03bb /ri . In this\n2\ncase, \u03bd(t) = g(t) dt with g(t) = 2Cri1 t e\u2212C1 t /ri .\n\u221a\nThere exists a point y in the support of \u03bc such that kyk\u221e \u2264 C2 ri . Without loss of generality,\nwe may assume w(y) = 1. Consider\nZ\n\u221a\n2\n\u221a\nw(x) d\u03bc(x) \u2265 \u03bc(B(C2 ri ))\nD=\ninf \u221a w(x) \u2265 (1 \u2212 e\u2212C1 C2 )e\u22122C2 C3 rj ri /n .\n\u221a\nB(C2\n\nx\u2208B(C2\n\nri )\n\nri )\n\n\u221a\nThen, for any \u03bb \u2265 C2 ri ,\n1\nD\n\nZ\n\nRm \\B(\u03bb)\n\nZ\n1 +\u221e C3 rj (t+C2 \u221ari )/n\ng(t) dt\ne\nD \u03bb\n\u0013\n\u0012\nZ +\u221e\n\u221a\nrj ri\nC4 t\nrj t\nt2\n=\ndt.\nexp C5\n+ C3\n\u2212 C1\nri\nn\nn\nri\n\u03bb\n\nw(x) d\u03bc(x) \u2264\n\n18\n\n(15)\n\n\f\u221a\nDenote t\u0303 = t/ r1 . Then the expression in the last exponent can be rewritten as\n\u221a\n\u221a\n\u221a\nrj ri\nrj r1\nrj ri\nr1\nrj t\nt2\nr1\n+ C3\n\u2212 C1 = C5\n+ C3\nt\u0303 \u2212 C1 t\u03032 =\nC5\nn\nn\nri\nn\nn\nri\nri\n\n!\n3/2\nri rj\nri rj\n2\nC5\n+ C3 \u221a t\u0303 \u2212 C1 t\u0303 .\nnr1\nn r1\n\nThe coefficients of the last polynomial can be estimated as follows:\n\u221a\n\u221a\n3/2\nr1 r2\nr1 r2\nri rj\nri rj\n\u2264\n= O(1) and\n= O(1),\n\u221a \u2264\nnr1\nn\nn r1\nn\n\u221a\nby Proposition 18. This means there exist C6 , C7 > 0 such that, for any \u03bb \u2265 C6 r1 , the right\nhand side of (15) is at most\nZ +\u221e\n2\nC4 t \u2212C7 t2 /ri\nC4 \u2212C7 \u03bb2 /ri\ndt =\n= e\u2212\u03a9(\u03bb /r1 ) ,\ne\ne\nr\n2C\ni\n7\n\u03bb\n\u221a\nif \u03bb \u2265 C r1 for C large enough.\nProof of Lemma 20. Let S be the random subset. Denote the\n\u0001 set of t-subtuples of S by St . We\napply Proposition 21 to St with k \u2212 1 Ht 's given by ms = st and r = bt = O(rt ). Thus, if St had\nuniform distribution, Eq. (12) would hold, that would imply (11), because there are O(1) possible\nchoices of s and t.\nBut in S, St does not have uniform distribution. Each St is assigned weight wSt that is\nproportional to the number of subsets of A\u2032\u22651 having specification (b\u2032t ), where A\u2032\u22651 has lj \u2212 hj (St )\nj-tuples in the notations of Proposition 21, b\u2032t = 0 and b\u2032j = bj for j 6= t.\nTake two St and St\u2032 , and assume k\u03b2\u0303(St ) \u2212 \u03b2\u0303(St\u2032 )k\u221e \u2264 d. We apply Lemma 12. There are\ntwo cases. If t > 1, the lemma implies wSt /wSt\u2032 = eO(dr1 /n) . If t = 1 then wSt /wSt\u2032 = eO(dr2 /n) .\nAnyway, either r in (12), or r in the estimation of wSt /wSt\u2032 is not r1 , and, hence, Lemma 22 applies,\nfinishing the proof of the lemma.\nDivergence in the flow After we have defined typical vertices, we are going to show that almost\nall flow goes through them. But before we do so, we show get an estimate of the divergence of the\nflow in the distance of the types.\nLemma 23. Suppose two key vertices S and S \u2032 of the same specification satisfy k\u03b2\u0303(S)\u2212 \u03b2\u0303(S \u2032 )k\u221e \u2264\nd. Then pS /pS \u2032 = eO(dr2 /n) .\nP\nProof. DenoteP(bt ) = \u03b2(S) = \u03b2(S \u2032 ), and b = t bt . Let the original specification of the vertices be\n(ct ), and c = t ct .\nFix some order of subtuples in S and S \u2032 so that the sizes of the i-th subtuple in S and S \u2032 are\nequal for any i. Denote this common value by \u03bd(i). Also, let \u03b4s (i) be 1 if the i-th subtuple of S is\ncontained in As , and 0 otherwise. Define \u03b4 \u2032 for S \u2032 similarly.\nLet \u03a3 be the set of possible sequences of how the subtuples could have been loaded. I.e., for\neach element of \u03a3, the first c subtuples have specification (ct ), and the remaining b\u2212c subtuples are\nin a non-decreasing order with respect to their sizes. Moreover, the order of the first c subtuples is\nirrelevant, i.e., no two distinct elements of \u03a3 have their tails of last b \u2212 c subtuples equal. In these\nnotations,\n\u0013\u22121\n\u0012X\nb\nXj\u22121\nX Y\nj\u22121\n\u03b4k\u22121 (\u03c3i)\n,\n(16)\nD\u03bd(\u03c3j)\n\u03b41 (\u03c3i), . . . ,\npS = po\n\u03c3\u2208\u03a3 j=c+1\n\ni=1\n\ni=1\n\nwhere po and D are defined in Section 5.3. A similar expression works for S \u2032 as well, if one replaces\n\u03b4 by \u03b4 \u2032 .\n19\n\n\fSince the distance between the types of S and S \u2032 is d, one can define the order of the subtuples\nso that \u03b4s (i) = \u03b4s\u2032 (i) for all s's and all, except at most O(d), i's. In this case, for all \u03c3, s and j:\nj\u22121\nX\ni=1\n\n\u03b4s (\u03c3i) \u2212\n\nj\u22121\nX\n\n\u03b4s\u2032 (\u03c3i) = O(d).\n\ni=1\n\nThen the ratio of the D's in (16) is at most 1 + O(d/n). Since there are O(r2 ) multipliers, the\nratio of the products in (16) for the same \u03c3 is at most\n\u0012\n\u0012 \u0013\u0013O(r2 )\nd\n= eO(r2 d/n) .\n1+O\nn\nAnd the same estimate holds for the ratio of sums.\nFinishing the proof Finally, we are about to prove that the statement of Corollary 16 applies\nfor the flow. Call an arc on preparatory or last stage typical if the preceding key vertex is typical\nand the flow through the arc is non-zero. We show that conditions of (8) hold for a fixed value\nof x \u2208 f \u22121 (1). Then the existence of a strong equivalence between any two positive inputs, as\nin Section 4.3, implies that (8) holds for all positive inputs x with the values of \u03c0(E) and \u03c4 (E)\nindependent on x.\n\u0001\n\u0001\n\u0001\nNote that the factor si / l sl N (S \u2032 \\ Q) from (10) is equal for all arcs from a fixed equivalence\nclass of the preparatory stage, up to a constant factor. Thus, the main concern is about pS \u2032 \\Q ,\nthat is flow through a key vertex. The same is true for the last stage as well.\nWe start with the third condition of (8). It is enough to show the flow differs by at most a\nconstant factor for any two typical key vertices of the same specification. The latter follows from\n\u221a\nthe fact the types \u221a\nof typical vertices are at distance O( r1 ), and, hence, by Lemma 23, the ratio\nof the flow is eO(r2 r1 /n) = O(1).\nWe continue with the second condition. Again, it is enough to show its analog for key vertices.\nThe latter is a direct consequence of Lemma 22 applied to the estimates of Lemmas 20 and 23. The\nconstant C in the definition of the typical vertex is that from the last application of Lemma 22.\nFinally, let us calculate the speciality of each step. Because of Corollary 13, we may calculate\nthe speciality as if the set of input variables is reduced to A\u22651 \u222a M . Consider a typical arc e of\nstep (i, j, l). Let S be the origin of e. Note that S is typical (this is a consequence of (14)). If\nl = 1 then we can add any element from an untouched tuple of A\u2265i . Due to (6), there are \u03a9(n)\nsuch elements.\nNow assume l > 1. By the construction of the flow, the non-zero flow is through the arcs that\nload the l-th element for a subtuple from A\u2265i . By Lemma 19, in S, there are \u03a9(rl\u22121 ) (l \u2212 1)subtuples from A\u2265i . In both cases, there are \u03a9(rl\u22121 ) arcs leaving S that are used by the flow. By\nLemma 20, an \u03a9(1) fraction of all vertices is typical, hence, the speciality of an equivalence class\nof step (i, j, l) is O(n/rl\u22121 ).\nFor the last stage, the same argument as in Section 5.3 applies, concluded by a fact an \u03a9(1)\nfraction of all vertices before the last stage is typical.\nThus, the flow is almost symmetric and estimates from Table 1 are correct. This proves Theorem 1.\n\n6\n\nSummary\n\nAn algorithm for k-distinctness problem is constructed in the paper, given the prior knowledge of\nthe structure of the input. Is it true, the problem can be solved in the same number of queries\nwithout any prior knowledge?\n20\n\n\fAlso, the algorithm in Section 4.5 can be used for any function such that its 1-certificate complexity is bounded by k. For the algorithm in Section 5, it is not clear. So, another (stronger) open\nproblem is as follows. Is it true, any function with 1-certificate complexity bounded by constant\ncan be calculated in o(n3/4 ) quantum queries? If so, this would be a far-reaching generalization of\nthe quantum algorithm in [CK11].\n\nAcknowledgements\nAB would like to thank Andris Ambainis for useful discussions. AB has been supported by the\nEuropean Social Fund within the project \"Support for Doctoral Studies at University of Latvia\".\n\nReferences\n[Amb02]\n\nA. Ambainis. Quantum lower bounds by quantum arguments. J. Comput. Syst. Sci.,\n64:750\u2013767, 2002. Earlier version in STOC'00.\n\n[Amb05]\n\nA. Ambainis. Quantum lower bounds for collision and element distinctness with small\nrange. Theory of Computing, 1:37\u201346, 2005.\n\n[Amb07]\n\nA. Ambainis. Quantum walk algorithm for element distinctness. SIAM Journal on\nComputing, 37:210\u2013239, 2007.\n\n[AS04]\n\nS. Aaronson and Y. Shi. Quantum lower bounds for the collision and the element\ndistinctness problems. Journal of the ACM, 51(4):595\u2013605, 2004.\n\n[AS08]\n\nN. Alon and J.H. Spencer. The probabilistic method. Wiley-Interscience series in discrete\nmathematics and optimization. Wiley, 2008.\n\n[BdW02]\n\nH. Buhrman and R. de Wolf. Complexity measures and decision tree complexity: a\nsurvey. Theor. Comput. Sci., 288:21\u201343, October 2002.\n\n[Bel11a]\n\nA. Belovs. Span-program-based quantum algorithm for the rank problem. Technical\nReport arXiv:1103.0842, arXiv, 2011.\n\n[Bel11b]\n\nA. Belovs. Span programs for functions with constant-sized 1-certificates. Technical\nReport arXiv:1105.4024, arXiv, 2011.\n\n[CK11]\n\nA. Childs and R. Kothari. Quantum query complexity of minor-closed graph properties.\nIn Proc. 28th STACS, pages 661\u2013672, 2011.\n\n[HL\u016007]\n\nP. H\u00f8yer, T. Lee, and R. \u0160palek. Negative weights make adversaries stronger. In Proc.\n39th ACM STOC, pages 526\u2013535, 2007.\n\n[LMR+ 11] T. Lee, R. Mittal, B. Reichardt, R. \u0160palek, and M. Szegedy. Quantum query complexity\nof the state conversion problem. In Proc. 52nd IEEE FOCS, 2011.\n[Rei11]\n\nB. Reichardt. Reflections for quantum query algorithms. In Proc. 22nd ACM-SIAM\nSymp. on Discrete Algorithms (SODA), pages 560\u2013569, 2011.\n\n[R\u0160]\n\nB. Reichardt and R. \u0160palek. Span-program-based quantum algorithm for evaluating\nformulas. In Proc. 40th ACM STOC, pages 103-112.\n\n21\n\n\f"}