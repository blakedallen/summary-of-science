{"id": "http://arxiv.org/abs/0811.1825v1", "guidislink": true, "updated": "2008-11-12T06:30:55Z", "updated_parsed": [2008, 11, 12, 6, 30, 55, 2, 317, 0], "published": "2008-11-12T06:30:55Z", "published_parsed": [2008, 11, 12, 6, 30, 55, 2, 317, 0], "title": "A Divergence Formula for Randomness and Dimension", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0811.2483%2C0811.1236%2C0811.1171%2C0811.0407%2C0811.0725%2C0811.4459%2C0811.4707%2C0811.0339%2C0811.4715%2C0811.3664%2C0811.0783%2C0811.4177%2C0811.4618%2C0811.2135%2C0811.1711%2C0811.0045%2C0811.3815%2C0811.4386%2C0811.3792%2C0811.2522%2C0811.2366%2C0811.4027%2C0811.4531%2C0811.4500%2C0811.4050%2C0811.0260%2C0811.0344%2C0811.0710%2C0811.1761%2C0811.3293%2C0811.1580%2C0811.1239%2C0811.3309%2C0811.4258%2C0811.3375%2C0811.2879%2C0811.3641%2C0811.1825%2C0811.1187%2C0811.2162%2C0811.0306%2C0811.1373%2C0811.0718%2C0811.4718%2C0811.0902%2C0811.0418%2C0811.2796%2C0811.3978%2C0811.4019%2C0811.2060%2C0811.2365%2C0811.3143%2C0811.0940%2C0811.3228%2C0811.0523%2C0811.0307%2C0811.4342%2C0811.4344%2C0811.1326%2C0811.1339%2C0811.4744%2C0811.4115%2C0811.1602%2C0811.1026%2C0811.4473%2C0811.4691%2C0811.0912%2C0811.3146%2C0811.0639%2C0811.0096%2C0811.2872%2C0811.0105%2C0811.4764%2C0811.2857%2C0811.3926%2C0811.3981%2C0811.0123%2C0811.0657%2C0811.3043%2C0811.2271%2C0811.3989%2C0811.2426%2C0811.3040%2C0811.3919%2C0811.0682%2C0811.3742%2C0811.0802%2C0811.2767%2C0811.1943%2C0811.1892%2C0811.1591%2C0811.1341%2C0811.4265%2C0811.0392%2C0811.4461%2C0811.4293%2C0811.2258%2C0811.1229%2C0811.0174%2C0811.4338%2C0811.1422&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Divergence Formula for Randomness and Dimension"}, "summary": "If $S$ is an infinite sequence over a finite alphabet $\\Sigma$ and $\\beta$ is\na probability measure on $\\Sigma$, then the {\\it dimension} of $ S$ with\nrespect to $\\beta$, written $\\dim^\\beta(S)$, is a constructive version of\nBillingsley dimension that coincides with the (constructive Hausdorff)\ndimension $\\dim(S)$ when $\\beta$ is the uniform probability measure. This paper\nshows that $\\dim^\\beta(S)$ and its dual $\\Dim^\\beta(S)$, the {\\it strong\ndimension} of $S$ with respect to $\\beta$, can be used in conjunction with\nrandomness to measure the similarity of two probability measures $\\alpha$ and\n$\\beta$ on $\\Sigma$. Specifically, we prove that the {\\it divergence formula}\n\\[\n  \\dim^\\beta(R) = \\Dim^\\beta(R) =\\frac{\\CH(\\alpha)}{\\CH(\\alpha) + \\D(\\alpha ||\n\\beta)} \\] holds whenever $\\alpha$ and $\\beta$ are computable, positive\nprobability measures on $\\Sigma$ and $R \\in \\Sigma^\\infty$ is random with\nrespect to $\\alpha$. In this formula, $\\CH(\\alpha)$ is the Shannon entropy of\n$\\alpha$, and $\\D(\\alpha||\\beta)$ is the Kullback-Leibler divergence between\n$\\alpha$ and $\\beta$. We also show that the above formula holds for all\nsequences $R$ that are $\\alpha$-normal (in the sense of Borel) when\n$\\dim^\\beta(R)$ and $\\Dim^\\beta(R)$ are replaced by the more effective\nfinite-state dimensions $\\dimfs^\\beta(R)$ and $\\Dimfs^\\beta(R)$. In the course\nof proving this, we also prove finite-state compression characterizations of\n$\\dimfs^\\beta(S)$ and $\\Dimfs^\\beta(S)$.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0811.2483%2C0811.1236%2C0811.1171%2C0811.0407%2C0811.0725%2C0811.4459%2C0811.4707%2C0811.0339%2C0811.4715%2C0811.3664%2C0811.0783%2C0811.4177%2C0811.4618%2C0811.2135%2C0811.1711%2C0811.0045%2C0811.3815%2C0811.4386%2C0811.3792%2C0811.2522%2C0811.2366%2C0811.4027%2C0811.4531%2C0811.4500%2C0811.4050%2C0811.0260%2C0811.0344%2C0811.0710%2C0811.1761%2C0811.3293%2C0811.1580%2C0811.1239%2C0811.3309%2C0811.4258%2C0811.3375%2C0811.2879%2C0811.3641%2C0811.1825%2C0811.1187%2C0811.2162%2C0811.0306%2C0811.1373%2C0811.0718%2C0811.4718%2C0811.0902%2C0811.0418%2C0811.2796%2C0811.3978%2C0811.4019%2C0811.2060%2C0811.2365%2C0811.3143%2C0811.0940%2C0811.3228%2C0811.0523%2C0811.0307%2C0811.4342%2C0811.4344%2C0811.1326%2C0811.1339%2C0811.4744%2C0811.4115%2C0811.1602%2C0811.1026%2C0811.4473%2C0811.4691%2C0811.0912%2C0811.3146%2C0811.0639%2C0811.0096%2C0811.2872%2C0811.0105%2C0811.4764%2C0811.2857%2C0811.3926%2C0811.3981%2C0811.0123%2C0811.0657%2C0811.3043%2C0811.2271%2C0811.3989%2C0811.2426%2C0811.3040%2C0811.3919%2C0811.0682%2C0811.3742%2C0811.0802%2C0811.2767%2C0811.1943%2C0811.1892%2C0811.1591%2C0811.1341%2C0811.4265%2C0811.0392%2C0811.4461%2C0811.4293%2C0811.2258%2C0811.1229%2C0811.0174%2C0811.4338%2C0811.1422&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "If $S$ is an infinite sequence over a finite alphabet $\\Sigma$ and $\\beta$ is\na probability measure on $\\Sigma$, then the {\\it dimension} of $ S$ with\nrespect to $\\beta$, written $\\dim^\\beta(S)$, is a constructive version of\nBillingsley dimension that coincides with the (constructive Hausdorff)\ndimension $\\dim(S)$ when $\\beta$ is the uniform probability measure. This paper\nshows that $\\dim^\\beta(S)$ and its dual $\\Dim^\\beta(S)$, the {\\it strong\ndimension} of $S$ with respect to $\\beta$, can be used in conjunction with\nrandomness to measure the similarity of two probability measures $\\alpha$ and\n$\\beta$ on $\\Sigma$. Specifically, we prove that the {\\it divergence formula}\n\\[\n  \\dim^\\beta(R) = \\Dim^\\beta(R) =\\frac{\\CH(\\alpha)}{\\CH(\\alpha) + \\D(\\alpha ||\n\\beta)} \\] holds whenever $\\alpha$ and $\\beta$ are computable, positive\nprobability measures on $\\Sigma$ and $R \\in \\Sigma^\\infty$ is random with\nrespect to $\\alpha$. In this formula, $\\CH(\\alpha)$ is the Shannon entropy of\n$\\alpha$, and $\\D(\\alpha||\\beta)$ is the Kullback-Leibler divergence between\n$\\alpha$ and $\\beta$. We also show that the above formula holds for all\nsequences $R$ that are $\\alpha$-normal (in the sense of Borel) when\n$\\dim^\\beta(R)$ and $\\Dim^\\beta(R)$ are replaced by the more effective\nfinite-state dimensions $\\dimfs^\\beta(R)$ and $\\Dimfs^\\beta(R)$. In the course\nof proving this, we also prove finite-state compression characterizations of\n$\\dimfs^\\beta(S)$ and $\\Dimfs^\\beta(S)$."}, "authors": ["Jack H. Lutz"], "author_detail": {"name": "Jack H. Lutz"}, "author": "Jack H. Lutz", "arxiv_comment": "18 pages", "links": [{"href": "http://arxiv.org/abs/0811.1825v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0811.1825v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0811.1825v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0811.1825v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:0811.1825v1 [cs.CC] 12 Nov 2008\n\nA Divergence Formula for Randomness and Dimension\nJack H. Lutz\u2217\nDepartment of Computer Science\nIowa State University\nAmes, IA 50011, USA\nlutz@cs.iastate.edu\n\nAbstract\nIf S is an infinite sequence over a finite alphabet \u03a3 and \u03b2 is a probability measure on \u03a3, then\nthe dimension of S with respect to \u03b2, written dim\u03b2 (S), is a constructive version of Billingsley\ndimension that coincides with the (constructive Hausdorff) dimension dim(S) when \u03b2 is the\nuniform probability measure. This paper shows that dim\u03b2 (S) and its dual Dim\u03b2 (S), the strong\ndimension of S with respect to \u03b2, can be used in conjunction with randomness to measure the\nsimilarity of two probability measures \u03b1 and \u03b2 on \u03a3. Specifically, we prove that the divergence\nformula\nH(\u03b1)\ndim\u03b2 (R) = Dim\u03b2 (R) =\nH(\u03b1) + D(\u03b1||\u03b2)\nholds whenever \u03b1 and \u03b2 are computable, positive probability measures on \u03a3 and R \u2208 \u03a3\u221e is\nrandom with respect to \u03b1. In this formula, H(\u03b1) is the Shannon entropy of \u03b1, and D(\u03b1||\u03b2) is the\nKullback-Leibler divergence between \u03b1 and \u03b2. We also show that the above formula holds for all\nsequences R that are \u03b1-normal (in the sense of Borel) when dim\u03b2 (R) and Dim\u03b2 (R) are replaced\nby the more effective finite-state dimensions dim\u03b2FS (R) and DimFS \u03b2 (R). In the course of proving\nthis, we also prove finite-state compression characterizations of dim\u03b2FS (S) and DimFS \u03b2 (S).\n\n1\n\nIntroduction\n\nThe constructive dimension dim(S) and the constructive strong dimension Dim(S) of an infinite\nsequence S over a finite alphabet \u03a3 are constructive versions of the two most important classical\nfractal dimensions, namely, Hausdorff dimension [9] and packing dimension [22, 21], respectively.\nThese two constructive dimensions, which were introduced in [13, 1], have been shown to have the\nuseful characterizations\nK(w)\n(1.1)\ndim(S) = lim inf\nw\u2192S |w| log |\u03a3|\nand\nDim(S) = lim sup\nw\u2192S\n\nK(w)\n,\n|w| log |\u03a3|\n\n(1.2)\n\nwhere the logarithm is base-2 [16, 1]. In these equations, K(w) is the Kolmogorov complexity of the\nprefix w of S, i.e., the length in bits of the shortest program that prints the string w. (See section\n\u2217\n\nThis research was supported in part by National Science Foundation Grants 9988483, 0344187, 0652569, and\n0728806 and by the Spanish Ministry of Education and Science (MEC) and the European Regional Development\nFund (ERDF) under project TIN2005-08832-C03-02.\n\n1\n\n\f2.6 or [11] for details.) The numerators in these equations are thus the algorithmic information\ncontent of w, while the denominators are the \"naive\" information content of w, also in bits. We thus\nunderstand (1.1) and (1.2) to say that dim(S) and Dim(S) are the lower and upper information\ndensities of the sequence S. These constructive dimensions and their analogs at other levels of\neffectivity have been investigated extensively in recent years [10].\nThe constructive dimensions dim(S) and Dim(S) have recently been generalized to incorporate\na probability measure \u03bd on the sequence space \u03a3\u221e as a parameter [14]. Specifically, for each\nsuch \u03bd and each sequence S \u2208 \u03a3\u221e , we now have the constructive dimension dim\u03bd (S) and the\nconstructive strong dimension Dim\u03bd (S) of S with respect to \u03bd. (The first of these is a constructive\nversion of Billingsley dimension [2].) When \u03bd is the uniform probability measure on \u03a3\u221e , we have\ndim\u03bd (S) = dim(S) and Dim\u03bd (S) = Dim(S). A more interesting example occurs when \u03bd is the\nproduct measure generated by a nonuniform probability measure \u03b2 on the alphabet \u03a3. In this case,\ndim\u03bd (S) and Dim\u03bd (S), which we write as dim\u03b2 (S) and Dim\u03b2 (S), are again the lower and upper\ninformation densities of S, but these densities are now measured with respect to unequal letter\ncosts. Specifically, it was shown in [14] that\ndim\u03b2 (S) = lim inf\nw\u2192S\n\nand\nDim\u03b2 (S) = lim sup\nw\u2192S\n\nwhere\n\n|w|\u22121\n\nI\u03b2 (w) =\n\nX\n\nlog\n\ni=0\n\nK(w)\nI\u03b2 (w)\n\n(1.3)\n\nK(w)\n,\nI\u03b2 (w)\n\n(1.4)\n\n1\n\u03b2(w[i])\n\nis the Shannon self-information of w with respect to \u03b2. These unequal letter costs log(1/\u03b2(a)) for\na \u2208 \u03a3 can in fact be useful. For example, the complete analysis of the dimensions of individual\npoints in self-similar fractals given by [14] requires these constructive dimensions with a particular\nchoice of the probability measure \u03b2 on \u03a3.\nIn this paper we show how to use the constructive dimensions dim\u03b2 (S) and Dim\u03b2 (S) in conjunction with randomness to measure the degree to which two probability measures on \u03a3 are similar.\nTo see why this might be possible, we note that the inequalities\n0 \u2264 dim\u03b2 (S) \u2264 Dim\u03b2 (S) \u2264 1\nhold for all \u03b2 and S and that the maximum values\ndim\u03b2 (R) = Dim\u03b2 (R) = 1\n\n(1.5)\n\nare achieved whenever the sequence R is random with respect to \u03b2. It is thus reasonable to hope\nthat, if R is random with respect to some other probability measure \u03b1 on \u03a3, then dim\u03b2 (R) and\nDim\u03b2 (R) will take on values whose closeness to 1 reflects the degree to which \u03b1 is similar to \u03b2.\nThis is indeed the case. Our first main theorem says that the divergence formula\ndim\u03b2 (R) = Dim\u03b2 (R) =\n\n2\n\nH(\u03b1)\nH(\u03b1) + D(\u03b1||\u03b2)\n\n(1.6)\n\n\fholds whenever \u03b1 and \u03b2 are computable, positive probability measures on \u03a3 and R \u2208 \u03a3\u221e is random\nwith respect to \u03b1. In this formula, H(\u03b1) is the Shannon entropy of \u03b1, and D(\u03b1||\u03b2) is the KullbackLeibler divergence between \u03b1 and \u03b2. When \u03b1 = \u03b2, the Kullback-Leibler divergence D(\u03b1||\u03b2) is 0, so\n(1.6) coincides with (1.5). When \u03b1 and \u03b2 are dissimilar, the Kullback-Leibler divergence D(\u03b1||\u03b2)\nis large, so the right-hand side of (1.6) is small. Hence the divergence formula tells us that, when\nR is \u03b1-random, dim\u03b2 (R) = Dim\u03b2 (R) is a quantity in [0, 1] whose closeness to 1 is an indicator of\nthe similarity between \u03b1 and \u03b2.\nThe proof of (1.6) serves as an outline of our other, more challenging task, which is to prove\nthat the divergence formula (1.6) also holds for the much more effective finite-state \u03b2-dimension\ndim\u03b2FS (R) and finite-state strong \u03b2-dimension DimFS \u03b2 (R). (These dimensions, defined in section\n2.5, are generalizations of finite-state dimension and finite-state strong dimension, which were\nintroduced in [6, 1], respectively.)\nWith this objective in mind, our second main theorem characterizes the finite-state \u03b2-dimensions\nin terms of finite-state data compression. Specifically, this theorem says that, in analogy with (1.3)\nand (1.4), the identities\n|C(w)|\ndim\u03b2FS (S) = inf lim inf\n(1.7)\nC w\u2192S I\u03b2 (w)\nand\ndim\u03b2FS (S) = inf lim sup\nC\n\nw\u2192S\n\n|C(w)|\nI\u03b2 (w)\n\n(1.8)\n\nhold for all infinite sequences S over \u03a3. The infima here are taken over all information-lossless finitestate compressors (a model introduced by Shannon [20] and investigated extensively ever since) C\nwith output alphabet 0, 1, and |C(w)| denotes the number of bits that C outputs when processing\nthe prefix w of S. The special cases of (1.7) and (1.8) in which \u03b2 is the uniform probability measure\non \u03a3, and hence I\u03b2 (w) = |w| log |\u03a3|, were proven in [6, 1]. In fact, our proof uses these special\ncases as \"black boxes\" from which we derive the more general (1.7) and (1.8).\nWith (1.7) and (1.8) in hand, we prove our third main theorem. This involves the finite-state\nversion of randomness, which was introduced by Borel [3] long before finite-state automata were\ndefined. If \u03b1 is a probability measure on \u03a3, then a sequence S \u2208 \u03a3\u221e is \u03b1-normal in the sense of\nBorel if every finite string w \u2208 \u03a3\u2217 appears with asymptotic frequency \u03b1(w) \u2208 S, where we write\n|w|\u22121\n\n\u03b1(w) =\n\nY\n\n\u03b1(w[i]).\n\ni=0\n\n(See section 2.6 for a precise definition of asymptotic frequency.) Our third main theorem says that\nthe divergence formula\nH(\u03b1)\n(1.9)\ndim\u03b2FS (R) = DimFS \u03b2 (R) =\nH(\u03b1) + D(\u03b1||\u03b2)\nholds whenever \u03b1 and \u03b2 are positive probability measures on \u03a3 and R \u2208 \u03a3\u221e is \u03b1-normal.\nIn section 2 we briefly review ideas from Shannon information theory, classical fractal dimensions, algorithmic information theory, and effective fractal dimensions that are used in this paper.\nSection 3 outlines the proofs of (1.6), section 4 outlines the proofs of (1.7) and (1.8), and section\n5 outlines the proof of (1.9). Various proofs are consigned to a technical appendix.\n\n3\n\n\f2\n2.1\n\nPreliminaries\nNotation and setting\n\nThroughout this paper we work in a finite alphabet \u03a3 = {0, 1, . . . , k \u2212 1}, where k \u2265 2. We write\n\u03a3\u2217 for the set of (finite) strings over \u03a3 and \u03a3\u221e for the set of (infinite) sequences over \u03a3. We write\n|w| for the length of a string w and \u03bb for the empty string. For w \u2208 \u03a3\u2217 and 0 \u2264 i < |w|, w[i] is\nthe ith symbol in w. Similarly, for S \u2208 \u03a3\u221e and i \u2208 N (= {0, 1, 2, . . . }), S[i] is the ith symbol in S.\nNote that the leftmost symbol in a string or sequence is the 0th symbol.\nA prefix of a string or sequence x \u2208 \u03a3\u2217 \u222a \u03a3\u221e is a string w \u2208 \u03a3\u2217 for which there exists a\nstring or sequence y \u2208 \u03a3\u2217 \u222a \u03a3\u221e such that x = wy. In this case we write w \u2291 x. The equation\nlimw\u2192S f (w) = L means that, for all \u01eb > 0, for all sufficiently long prefixes w \u2291 S, |f (w) \u2212 L| < \u01eb.\nWe also use the limit inferior,\nlim inf f (w) = lim inf {f (x) | w \u2291 x \u2291 S } ,\nw\u2192S\n\nw\u2192S\n\nand the limit superior\nlim sup f (w) = lim sup {f (x) | w \u2291 x \u2291 S } .\nw\u2192S\n\n2.2\n\nw\u2192S\n\nProbability measures, gales, and Shannon information\n\nP\nA probability measure on \u03a3 is a function \u03b1 : \u03a3 \u2192 [0, 1] such that a\u2208\u03a3 \u03b1(a) = 1. A probability\nmeasure \u03b1 on \u03a3 is positive if \u03b1(a) > 0 for every \u03b1 \u2208 \u03a3. A probability measure \u03b1 on \u03a3 is rational if\n\u03b1(a) \u2208 Q (i.e., \u03b1(a) is a rational number) for every a \u2208 \u03a3.\nA probability\nmeasure on \u03a3\u221e is a function \u03bd : \u03a3\u2217 \u2192 [0, 1] such that \u03bd(\u03bb) = 1 and, for all w \u2208 \u03a3\u2217 ,\nP\n\u03bd(w) = a\u2208\u03a3 \u03bd(wa). (Intuitively, \u03bd(w) is the probability that w \u2291 S when the sequence S \u2208 \u03a3\u221e\nis \"chosen according to \u03bd.\") Each probability measure \u03b1 on \u03a3 naturally induces the probability\nmeasure \u03b1 on \u03a3\u221e defined by\n|w|\u22121\nY\n\u03b1(w) =\n\u03b1(w[i])\n(2.1)\ni=0\n\n\u03a3\u2217 .\n\nfor all w \u2208\nWe reserve the symbol \u03bc for the uniform probability measure on \u03a3, i.e.,\n\u03bc(a) =\n\n1\nfor all a \u2208 \u03a3,\nk\n\nand also for the uniform probability measure on \u03a3\u221e , i.e.,\n\u03bc(w) = k\u2212|w| for all w \u2208 \u03a3\u2217 .\nIf \u03b1 is a probability measure on \u03a3 and s \u2208 [0, \u221e), then an s-\u03b1-gale is a function d : \u03a3\u2217 \u2192 [0, \u221e)\nsatisfying\nX\nd(w) =\nd(wa)\u03b1(a)s\n(2.2)\na\u2208\u03a3\n\nfor all w \u2208 \u03a3\u2217 . A 1-\u03b1-gale is also called an \u03b1-martingale. When \u03b1 = \u03bc, we omit it from this\nterminology, so an s-\u03bc-gale is called an s-gale, and a \u03bc-martingale is called a martingale.\nWe frequently use the following simple fact without explicit citation.\n4\n\n\fObservation 2.1. Let \u03b1 and \u03b2 be positive probability measures on \u03a3, and let s, t \u2208 [0, \u221e). If\nd : \u03a3\u2217 \u2192 [0, \u221e) is an s-\u03b1-gale, then the function d \u0303 : \u03a3\u2217 \u2192 [0, \u221e) defined by\n\u03b1(w)s\n \u0303\nd(w)\n=\nd(w)\n\u03b2(w)t\nis a t-\u03b2-gale.\nIntuitively, an s-\u03b1-gale is a strategy for betting on the successive symbols in a sequence S \u2208 \u03a3\u221e .\nFor each prefix w \u2291 S, d(w) denotes the amount of capital (money) that the gale d has after betting\non the symbols in w. If s = 1, then the right-hand side of (2.2) is the conditional expectation of\nd(wa), given that w has occurred, so (2.2) says that the payoffs are fair. If s < 1, then (2.2) says\nthat the payoffs are unfair.\nLet d be a gale, and let S \u2208 \u03a3\u221e . Then d succeeds on S if lim supw\u2192S d(w) = \u221e, and d succeeds\nstrongly on S if lim inf w\u2192S d(w) = \u221e. The success set of d is the set S \u221e [d] of all sequences on which\n\u221e [d] of all sequences on which d succeeds\nd succeeds, and the strong success set of d is the set Sstr\nstrongly.\nThe Shannon entropy of a probability measure \u03b1 on \u03a3 is\nX\n1\n,\nH(\u03b1) =\n\u03b1(a) log\n\u03b1(a)\na\u2208\u03a3\n\nwhere 0 log 01 = 0. (unless otherwise indicated, all logarithms in this paper are base-2.) The\nKullback-Leibler divergence between two probability measures \u03b1 and \u03b2 on \u03a3 is\nX\n\u03b1(a)\nD(\u03b1||\u03b2) =\n\u03b1(a) log\n.\n\u03b2(a)\na\u2208\u03a3\n\nThe Kullback-Leibler divergence is used to quantify how \"far apart\" the two probability measures \u03b1\nand \u03b2 are. The Shannon self-information of a string w \u2208 \u03a3\u2217 with respect to a probability measure\n\u03b2 on \u03a3 is\n|w|\u22121\nX\n1\n1\nI\u03b2 (w) = log\n=\n.\nlog\n\u03b2(w)\n\u03b2(w[i])\ni=0\n\nDiscussions of H(\u03b1), D(\u03b1||\u03b2), I\u03b2 (w) and their properties may be found in any good text on information theory, e.g., [5].\n\n2.3\n\nHausdorff, packing, and Billingsley dimensions\n\nGiven a probability measure \u03b2 on \u03a3, each set X \u2286 \u03a3\u221e has a Hausdorff dimension dim(X), a\npacking dimension Dim(X), a Billingsley dimension dim\u03b2 (X), and a strong Billingsley dimension\nDim\u03b2 (X), all of which are real numbers in the interval [0, 1]. In this paper we are not concerned\nwith the original definitions of these classical dimensions, but rather in their recent characterizations\n(which may be taken as definitions) in terms of gales.\nNotation. For each probability measure \u03b2 on \u03a3 and each set X \u2286 \u03a3\u221e , let G \u03b2 (X) (respectively,\nG \u03b2,str (X)) be the set of all s \u2208 [0, \u221e) such that there is a \u03b2-s-gale d satisfying X \u2286 S \u221e [d] (respec\u221e [d]).\ntively, X \u2286 Sstr\nTheorem 2.2 (gale characterizations of classical fractal dimensions). Let \u03b2 be a probability measure\non \u03a3, and let X \u2286 \u03a3\u221e .\n5\n\n\f1. [12] dim(X) = inf G \u03bc (X).\n\n3. [14] dim\u03b2 (X) = inf G \u03b2 (X).\n\n2. [1] Dim(X) = inf G \u03bc,str (X).\n\n4. [14] Dim\u03b2 (X) = inf G \u03b2,str (X).\n\n2.4\n\nRandomness and constructive dimensions\n\nRandomness and constructive dimensions are defined by imposing computability constraints on\ngales.\nA real-valued function f : \u03a3\u2217 \u2192 R is computable if there is a computable, rational-valued\nfunction f\u02c6 : \u03a3\u2217 \u00d7 N \u2192 Q such that, for all w \u2208 \u03a3\u2217 and r \u2208 N,\n|f\u02c6(w, r) \u2212 f (w)| \u2264 2\u2212r .\nA real-valued function f : \u03a3\u2217 \u2192 R is constructive, or lower semicomputable, if there is a computable,\nrational-valued function f\u02c6 : \u03a3\u2217 \u00d7 N \u2192 Q such that\n(i) for all w \u2208 \u03a3\u2217 and t \u2208 N, f\u02c6(w, t) \u2264 f\u02c6(w, t + 1) < f (w), and\n(ii) for all w \u2208 \u03a3\u2217 , f (w) = limt\u2192\u221e f\u02c6(w, t).\nThe first successful definition of the randomness of individual sequences S \u2208 \u03a3\u221e was formulated\nby Martin-L\u00f6f [15]. Many characterizations (equivalent definitions) of randomness are now known,\nof which the following is the most pertinent.\nTheorem 2.3 (Schnorr [17, 18]). Let \u03b1 be a probability measure on \u03a3. A sequence S \u2208 \u03a3\u221e is\nrandom with respect to \u03b1 (or, briefly, \u03b1-random) if there is no constructive \u03b1-martingale that\nsucceeds on S.\nMotivated by Theorem 2.2, we now define the constructive dimensions.\n\u03b2\n\u03b2,str\nNotation. We define the sets Gconstr\n(X) and Gconstr\n(X) to be like the sets G \u03b2 (X) and G \u03b2,constr (X)\nof section 2.3, except that the \u03b2-s-gales are now required to be constructive.\n\nDefinition. Let \u03b2 be a probability measure on \u03a3, let X \u2286 \u03a3\u221e , and let S \u2208 \u03a3\u221e .\n\u03bc\n1. [13] The constructive dimension of X is cdim(X) = inf Gconstr\n(X).\n\u03bc,str\n2. [1] The constructive strong dimension of X is cDim(X) = inf Gconstr\n(X).\n\u03b2\n3. [14] The constructive \u03b2-dimension of X is cdim\u03b2 (X) = inf Gconstr\n(X).\n\u03b2,str\n4. [14] The constructive strong \u03b2-dimension of X is cDim\u03b2 (X) = inf Gconstr\n(X).\n\n5. [13] The dimension of S is dim(S) = cdim({S}).\n6. [1] The strong dimension of S is Dim(S) = cDim({S}).\n7. [14] The \u03b2-dimension of S is dim\u03b2 (S) = cdim\u03b2 ({S}).\n8. [14] The strong \u03b2-dimension of S is Dim\u03b2 (S) = cDim\u03b2 ({S}).\nIt is clear that definitions 1, 2, 5, and 6 above are the special case \u03b2 = \u03bc of definitions 3,\n4, 7, and 8, respectively. It is known that cdim\u03b2 (X) = supS\u2208X dim\u03b2 (S) and that cDim\u03b2 (X) =\nsupS\u2208X Dim\u03b2 (S) [14]. Constructive dimensions are thus investigated in terms of the dimensions\nof individual sequences. Since one does not discuss the classical dimension of an individual sequence (because the dimensions of section 2.3 are all zero for singleton, or even countable, sets),\nno confusion results from the notation dim(S), Dim(S), dim\u03b2 (S), and Dim\u03b2 (S).\n6\n\n\f2.5\n\nNormality and finite-state dimensions\n\nThe preceding section developed the constructive dimensions as effective versions of the classical\ndimensions of section 2.3. We now introduce the even more effective finite-state dimensions.\nNotation. \u2206Q (\u03a3) is the set of all rational-valued probability measure on \u03a3.\nDefinition ([19, 8, 6]). A finite-state gambler (FSG) is a 4-tuple\nG = (Q, \u03b4, q0 , B),\nwhere Q is a finite set of states, \u03b4 : Q \u00d7 \u03a3 \u2192 Q is the transition function; q0 \u2208 Q is the initial state,\nand B : Q \u2192 \u2206Q (\u03a3) is the betting function.\nThe transition structure (Q, \u03b4, q0 ) here works as in any deterministic finite-state automaton.\nFor w \u2208 \u03a3\u2217 , we write \u03b4(w) for the state reached by starting at q0 and processing w according to \u03b4.\nIntuitively, if the above FSG is in state q \u2208 Q, then, for each a \u2208 \u03a3, it bets the fraction B(q)(a)\nof its current capital that the next input symbol is an a. The payoffs are determined as follows.\nDefinition. Let G = (Q, \u03b4, q0 , B) be an FSG.\n1. The martingale of G is the function dG : \u03a3\u2217 \u2192 [0, \u221e) defined by the recursion\ndG (\u03bb) = 1,\ndG (wa) = kdG (w)B(\u03b4(w))(a)\nfor all w \u2208 \u03a3\u2217 and a \u2208 \u03a3.\n2. If \u03b2 is a probability measure on \u03a3 and s \u2208 [0, \u221e), then the s-\u03b2-gale of G is the function\n(s)\ndG,\u03b2 : \u03a3\u2217 \u2192 [0, \u221e) defined by\n\u03bc(w)\n(s)\ndG (w)\ndG,\u03b2 (w) =\n\u03b2(w)s\nfor all w \u2208 \u03a3\u2217 .\n(1)\n\n(1)\n\nIt is easy to verify that dG = dG,\u03bc is a martingale. It follows by Observation 2.1 that dG,\u03b2 is an\ns-\u03b2-gale.\n(s)\n\nDefinition. A finite-state s-\u03b2-gale is an s-\u03b2-gale of the form dG,\u03b2 for some FSG G.\n\u03b2\n\u03b2,str\nNotation. We define the sets GFS\n(X) and GFS\n(X) to be like the sets G \u03b2 (X) and G \u03b2,str (X) of\nsection 2.3, except that the s-\u03b2-gales are now required to be finite-state.\n\nDefinition. Let \u03b2 be a probability measure on \u03a3, and let S \u2208 \u03a3\u221e .\n\u03bc\n1. [6] The finite-state dimension of S is dimFS (S) = inf GFS\n({S}).\n\u03bc,str\n2. [1] The finite-state strong dimension of S is DimFS (S) = inf GFS\n({S}).\n\u03b2\n3. The finite-state \u03b2-dimension of S is dim\u03b2FS (S) = inf GFS\n({S}).\n\u03b2,str\n4. The finite-state strong \u03b2-dimension of S is DimFS \u03b2 (S) = inf GFS\n({S}).\n\n7\n\n\fWe now turn to some ideas based on asymptotic frequencies of strings in a given sequence. For\nnonempty strings w, x \u2208 \u03a3\u2217 , we write\n\u001b\n\u001a\n|x|\n\u2212 1 x[m|w|..(m + 1)|w| \u2212 1] = w\n#\u0003(w, x) = m \u2264\n|w|\nfor the number of block occurrences of w in x. For each sequence S \u2208 \u03a3\u221e , each positive integer n,\nand each nonempty w \u2208 \u03a3<n , the nth block frequency of w in S is\n\u03c0S,n (w) =\n\n#\u0003(w, S[0..n|w| \u2212 1])\n.\nn\n(l)\n\nNote that, for each n and l, the restriction \u03c0S,n of \u03c0S,n to \u03a3l is a probability measure on \u03a3l .\nDefinition. Let \u03b1 be a probability measure on \u03a3, let S \u2208 \u03a3\u221e , and let 0 < l \u2208 N.\n1. S is \u03b1-l-normal in the sense of Borel if, for all w \u2208 \u03a3l , lim \u03c0S,n (w) = \u03b1(w).\nn\u2192\u221e\n\n2. S is \u03b1-normal in the sense of Borel if S is \u03b1-l-normal for all 0 < l \u2208 N.\n3. [3] S is normal in the sense of Borel if S is \u03bc-normal.\n4. S has asymptotic frequency \u03b1, and we write S \u2208 FREQ\u03b1 , if S is \u03b1-1-normal.\nTheorem 2.4 ([19, 4]). For each probability measure \u03b1 on \u03a3 and each S \u2208 \u03a3\u221e , the following three\nconditions are equivalent.\n(1) S is \u03b1-normal.\n(2) No finite-state \u03b1-martingale succeeds on S.\n(3) dim\u03b1FS (S) = 1.\nThe equivalence of (1) and (2) where \u03b1 = \u03bc was proven in [19]. The equivalence of (2) and (3)\nwhen \u03b1 = \u03bc was noted in [4]. The extensions of these facts to arbitrary \u03b1 is routine.\nFor each S \u2208 \u03a3\u221e and 0 < l \u2208 N, the lth normalized lower and upper block entropy rates of S\nare\n1\n(l)\ninf H(\u03c0S,n )\nH\u2212\nl (S) = l log k lim\nn\u2192\u221e\nand\nH+\nl (S) =\n\n1\n(l)\nlim sup H(\u03c0S,n ),\nl log k n\u2192\u221e\n\nrespectively.\nWe use the following result in section 5.\nTheorem 2.5 ([4]). Let S \u2208 \u03a3\u221e .\n1. dimFS (S) = inf 0<l\u2208N H\u2212\nl (S).\n2. DimFS (S) = inf 0<l\u2208N H+\nl (S).\n\n8\n\n\f2.6\n\nKolmogorov complexity and finite-state compression\n\nWe now review known characterizations of constructive and finite-state dimensions that are based\non data compression ideas.\nThe Kolmogorov complexity K(w) of a string w \u2208 \u03a3\u2217 is the minimum length of a program\n\u03c0 \u2208 {0, 1}\u2217 for which U (\u03c0) = w, where U is a fixed universal self-delimiting Turing machine [11].\nTheorem 2.6. Let \u03b2 be a probability measure on \u03a3, and let S \u2208 \u03a3\u221e .\nK(w)\nI\u03b2 (w) .\n\n1. [16] dim(S) = lim inf w\u2192S\n\nK(w)\n|w| log k .\n\n3. [14] dim\u03b2 (S) = lim inf w\u2192S\n\n2. [1] Dim(S) = lim supw\u2192S\n\nK(w)\n|w| log k .\n\n4. [14] Dim\u03b2 (S) = lim supw\u2192S\n\nDefinition ([20]).\n\nK(w)\nI\u03b2 (w) .\n\n1. A finite-state compressor (FSC) is a 4-tuple\nC = (Q, \u03b4, q0 , \u03bd),\n\nwhere Q, \u03b4, and q0 are as in the FSG definition, and \u03bd : Q \u00d7 \u03a3 \u2192 {0, 1}\u2217 is the output\nfunction.\n2. The output of an FSC C = (Q, \u03b4, q0 , \u03bd) on an input w \u2208 \u03a3\u2217 is the string C(w) \u2208 {0, 1}\u2217\ndefined by the recursion\nC(\u03bb) = \u03bb,\nC(wa) = C(w)\u03bd(\u03b4(w), a)\nfor all w \u2208 \u03a3\u2217 and a \u2208 \u03a3.\n3. An information-lossless finite-state compressor (ILFSC) is an FSC for which the function\n\u03a3\u2217 \u2192 {0, 1}\u2217 \u00d7 Q\nw 7\u2192 (C(w), \u03b4(w))\nis one-to-one.\nTheorem 2.7. Let S \u2208 \u03a3\u221e .\n1. [6] dimFS (S) = inf C lim inf w\u2192S\n\n|C(w)|\n|w| log k .\n\n2. [1] DimFS (S) = inf C lim supw\u2192S\n\n3\n\n|C(w)|\n|w| log k .\n\nDivergence formula for randomness and constructive dimensions\n\nThis section proves the divergence formula for \u03b1-randomness, constructive \u03b2-dimension, and constructive strong \u03b2-dimension. The key point here is that the Kolmogorov complexity characterizations of these \u03b2-dimensions reviewed in section 2.6 immediately imply the following fact.\n\n9\n\n\fLemma 3.1. If \u03b1 and \u03b2 are computable, positive probability measure on \u03a3, then, for all S \u2208 \u03a3\u221e ,\nlim inf\n\ndim\u03b2 (S)\nI\u03b1 (w)\nI\u03b1 (w)\n\u2264\n\u2264 lim sup\n,\n\u03b1\nI\u03b2 (w)\ndim (S)\nw\u2192S I\u03b2 (w)\n\nlim inf\n\nI\u03b1 (w)\nI\u03b1 (w)\nDim\u03b2 (S)\n\u2264\n\u2264 lim sup\n.\n\u03b1\nI\u03b2 (w)\nDim (S)\nw\u2192S I\u03b2 (w)\n\nw\u2192S\n\nand\nw\u2192S\n\nThe following lemma is crucial to our argument, both here and in section 5.\nLemma 3.2 (frequency divergence lemma). If \u03b1 and \u03b2 are positive probability measures on \u03a3,\nthen, for all S \u2208 FREQ\u03b1 ,\nI\u03b2 (w) = (H(\u03b1) + D(\u03b1||\u03b2))|w| + o(|w|)\nas w \u2192 S.\nThe next lemma gives a simple relationship between the constructive \u03b2-dimension and the\nconstructive dimension of any sequence that is \u03b1-1-normal.\nLemma 3.3. If \u03b1 and \u03b2 are computable, positive probability measures on \u03a3, then, for all S \u2208\nFREQ\u03b1 ,\ndim(S)\n,\ndim\u03b2 (S) =\nHk (\u03b1) + Dk (\u03b1||\u03b2)\nand\nDim\u03b2 (S) =\n\nDim(S)\n.\nHk (\u03b1) + Dk (\u03b1||\u03b2)\n\nWe now recall the following constructive strengthening of a 1949 theorem of Eggleston [7].\nTheorem 3.4 ([13, 1]). If \u03b1 is a computable probability measure on \u03a3, then, for every \u03b1-random\nsequence R \u2208 \u03a3\u221e ,\ndim(R) = Dim(R) = Hk (\u03b1).\nThe main result of this section is now clear.\nTheorem 3.5 (divergence formula for randomness and constructive dimensions). If \u03b1 and \u03b2 are\ncomputable, positive probability measures on \u03a3, then, for every \u03b1-random sequence R \u2208 \u03a3\u221e ,\ndim\u03b2 (R) = Dim\u03b2 (R) =\n\nH(\u03b1)\n.\nH(\u03b1) + D(\u03b1||\u03b2)\n\nProof. This follows immediately from Lemma 3.3 and Theorem 3.4.\nWe note that D(\u03b1||\u03bc) = log k \u2212 H(\u03b1), so Theorem 3.4 is the case \u03b2 = \u03bc of Theorem 3.5.\n\n10\n\n\f4\n\nFinite-state dimensions and data compression\n\nThis section proves finite-state compression characterizations of finite-state \u03b2-dimension and finitestate strong \u03b2-dimension that are analogous to the characterizations given by parts 3 and 4 of\nTheorem 2.6. Our argument uses the following two technical lemmas, which are proven in the\ntechnical appendix.\nLemma 4.1. Let \u03b2 be a positive probability measure on \u03a3, and let C be an ILFSC. Assume that\nI \u2286 \u03a3\u2217 , s > 0, and \u01eb > 0 have the property that, for all w \u2208 I,\ns\u2265\n\n|C(w)|\n+ \u01eb.\nI\u03b2 (w)\n\n(4.1)\n\nThen there exist an FSG G and a real number \u03b4 > 0 such that, for all sufficiently long strings\nw \u2208 I,\n(s)\n(4.2)\ndG,\u03b2 (w) \u2265 2\u03b4|w| .\nLemma 4.2. Let \u03b2 be a positive probability measure on \u03a3, and let G be an FSG. Assume that\nI \u2286 \u03a3\u2217 , s > 0, and \u01eb > 0 have the property that, for all w \u2208 I,\n(s\u22122\u01eb)\n\ndG,\u03b2\n\n(w) \u2265 1.\n\n(4.3)\n\nThen there is an ILFSC C such that, for all w \u2208 I,\n|C(w)| \u2264 sI\u03b2 (w).\n\n(4.4)\n\nWe now prove the main result of this section.\nTheorem 4.3 (compression characterizations of finite-state \u03b2-dimensions). If \u03b2 is a positive probability measure on \u03a3, then, for each sequence S \u2208 \u03a3\u221e ,\ndim\u03b2FS (S) = inf lim inf\nC\n\nw\u2192S\n\n|C(w)|\n,\nI\u03b2 (w)\n\nand\nDimFS \u03b2 (S) = inf lim sup\nC\n\nw\u2192S\n\n|C(w)|\n,\nI\u03b2 (w)\n\n(4.5)\n\n(4.6)\n\nwhere the infima are taken over all ILFCSs C.\nProof. Let \u03b2 and S be as given. We first prove that the left-hand sides of (4.5) and (4.6) do not\nexceed the right-hand sides. For this, let C be an ILFSC. It suffices to show that\ndim\u03b2FS (S) \u2264 lim inf\nw\u2192S\n\n|C(w)|\nI\u03b2 (w)\n\nand\nDimFS \u03b2 (S) \u2264 lim sup\nw\u2192S\n\n|C(w)|\n.\nI\u03b2 (w)\n\n(4.7)\n\n(4.8)\n\nTo see that (4.7) holds, let s exceed the right-hand side. Then there exist an infinite set I of\nprefixes of S and an \u01eb > 0 such that (4.1) holds for all w \u2208 I. It follows by Lemma 4.1 that there\n11\n\n\f(s)\n\nexist an FSG G and a \u03b4 > 0 such that, for all sufficiently long w \u2208 I, dG,\u03b2 (w) \u2265 2\u03b4|w| . Since I is\n(s)\n\ninfinite and \u03b4 > 0, this implies that S \u2208 S \u221e [dG,\u03b2 ], whence dim\u03b2FS (S) \u2264 s. This establishes (4.7).\nThe proof that (4.8) holds is identical to the preceding paragraph, except that I is now a cofinite\n\u221e [d(s) ].\nset of prefixes of S, so S \u2208 Sstr\nG,\u03b2\nIt remains to be shown that the right-hand sides of (4.5) and (4.6) do not exceed the left-hand\nsides. To see this for (4.5), let s > dim\u03b2FS (S). It suffices to show that there is an ILFSC C such\nthat\n|C(w)|\nlim inf\n\u2264 s.\n(4.9)\nw\u2192S I\u03b2 (w)\nBy our choice of s there exists \u01eb > 0 such that s \u2212 2\u01eb > dim\u03b2FS (S). This implies that there is an\ninfinite set I of prefixes of S such that (4.3) holds for all w \u2208 I. Choose C for G, I, S, and \u01eb as in\nLemma 4.2. Then\n|C(w)|\n|C(w)|\nlim inf\n\u2264 inf\n\u2264s\n(4.10)\nw\u2192S I\u03b2 (w)\nw\u2208I I\u03b2 (w)\nby (4.4), so (4.9) holds.\nThe proof that the right-hand side of (4.6) does not exceed the left-hand side is identical to the\npreceding paragraph, except that the limits inferior in (4.9) and (4.10) are now limits superior, and\nthe set I is now a cofinite set of prefixes of S.\n\n5\n\nDivergence formula for normality and finite-state dimensions\n\nThis section proves the divergence formula for \u03b1-normality, finite-state \u03b2-dimension, and finitestate strong \u03b2-dimension. As should now be clear, Theorem 4.3 enables us to proceed in analogy\nwith section 3.\nLemma 5.1. If \u03b1 and \u03b2 are positive probability measures on \u03a3, then, for all S \u2208 \u03a3\u221e ,\ndim\u03b2FS (S)\nI\u03b1 (w)\nI\u03b1 (w)\n\u2264\n\u2264 lim sup\n,\nI\u03b2 (w)\ndim\u03b1FS (S)\nw\u2192S I\u03b2 (w)\n\n(5.1)\n\nI\u03b1 (w)\nDimFS \u03b2 (S)\nI\u03b1 (w)\n\u2264\n\u2264 lim sup\n.\nI\u03b2 (w)\nDimFS \u03b1 (S)\nw\u2192S I\u03b2 (w)\n\n(5.2)\n\nlim inf\nw\u2192S\n\nand\nlim inf\nw\u2192S\n\nLemma 5.2. If \u03b1 and \u03b2 are positive probability measures on \u03a3, then, for all S \u2208 FREQ\u03b1 ,\ndim\u03b2FS (S) =\nand\nDimFS \u03b2 (S) =\n\ndimFS (S)\n,\nHk (\u03b1) + Dk (\u03b1||\u03b2)\nDimFS (S)\n.\nHk (\u03b1) + Dk (\u03b1||\u03b2)\n\nWe next prove a finite-state analog of Theorem 3.4.\nTheorem 5.3. If \u03b1 is a probability measure on \u03a3, then, for every \u03b1-normal sequence R \u2208 \u03a3\u221e ,\ndimFS (R) = DimFS (R) = Hk (\u03b1).\n12\n\n\fWe now have our third main theorem.\nTheorem 5.4 (divergence theorem for normality and finite-state dimensions). If \u03b1 and \u03b2 are\npositive probability measures on \u03a3, then, for every \u03b1-normal sequence R \u2208 \u03a3\u221e ,\ndim\u03b2FS (R) = DimFS \u03b2 (R) =\n\nH(\u03b1)\n.\nH(\u03b1) + D(\u03b1||\u03b2)\n\nProof. This follows immediately from Lemma 5.2 and Theorem 5.3.\nWe again note that D(\u03b1||\u03b2) = log k \u2212 H(\u03b1), so Theorem 5.3 is the case \u03b2 = \u03bc of Theorem 5.4.\nAcknowledgments. I thank Xiaoyang Gu and Elvira Mayordomo for useful discussions.\n\nReferences\n[1] K. B. Athreya, J. M. Hitchcock, J. H. Lutz, and E. Mayordomo. Effective strong dimension,\nalgorithmic information, and computational complexity. SIAM Journal on Computing, 37:671\u2013\n705, 2007.\n[2] P. Billingsley. Hausdorff dimension in probability theory. Illinois Journal of Mathematics,\n4:187\u2013209, 1960.\n[3] E. Borel. Sur les probabilit\u00e9s d\u00e9nombrables et leurs applications arithm\u00e9tiques. Rend. Circ.\nMat. Palermo, 27:247\u2013271, 1909.\n[4] C. Bourke, J. M. Hitchcock, and N. V. Vinodchandran. Entropy rates and finite-state dimension. Theoretical Computer Science, 349(3):392\u2013406, 2005.\n[5] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, Inc.,\nsecond edition, 2006.\n[6] J. J. Dai, J. I. Lathrop, J. H. Lutz, and E. Mayordomo. Finite-state dimension. Theoretical\nComputer Science, 310:1\u201333, 2004.\n[7] H. Eggleston. The fractional dimension of a set defined by decimal properties. Quarterly\nJournal of Mathematics, Oxford Series 20:31\u201336, 1949.\n[8] M. Feder. Gambling using a finite state machine. IEEE Transactions on Information Theory,\n37:1459\u20131461, 1991.\n[9] F. Hausdorff. Dimension und \u00e4usseres Mass. Mathematische Annalen, 79:157\u2013179, 1919.\nEnglish translation.\n[10] J.\nM.\nHitchcock.\nEffective\nFractal\nDimension\nhttp://www.cs.uwyo.edu/ \u223cjhitchco/bib/dim.shtml (current October, 2008).\n\nBibliography,\n\n[11] M. Li and P. M. B. Vit\u00e1nyi. An Introduction to Kolmogorov Complexity and its Applications.\nSpringer-Verlag, Berlin, 1997. Second Edition.\n\n13\n\n\f[12] J. H. Lutz. Dimension in complexity classes. SIAM Journal on Computing, 32:1236\u20131259,\n2003.\n[13] J. H. Lutz. The dimensions of individual strings and sequences. Information and Computation,\n187:49\u201379, 2003.\n[14] J. H. Lutz and E. Mayordomo. Dimensions of points in self-similar fractals. SIAM Journal on\nComputing, 38:1080\u20131112, 2008.\n[15] P. Martin-L\u00f6f. The definition of random sequences. Information and Control, 9:602\u2013619, 1966.\n[16] E. Mayordomo. A Kolmogorov complexity characterization of constructive Hausdorff dimension. Information Processing Letters, 84(1):1\u20133, 2002.\n[17] C. P. Schnorr. A unified approach to the definition of random sequences. Mathematical Systems\nTheory, 5:246\u2013258, 1971.\n[18] C. P. Schnorr. A survey of the theory of random sequences. In R. E. Butts and J. Hintikka,\neditors, Basic Problems in Methodology and Linguistics, pages 193\u2013210. D. Reidel, 1977.\n[19] C. P. Schnorr and H. Stimm. Endliche Automaten und Zufallsfolgen. Acta Informatica, 1:345\u2013\n359, 1972.\n[20] C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal,\n27:379\u2013423, 623\u2013656, 1948.\n[21] D. Sullivan. Entropy, Hausdorff measures old and new, and limit sets of geometrically finite\nKleinian groups. Acta Mathematica, 153:259\u2013277, 1984.\n[22] C. Tricot. Two definitions of fractional dimension. Mathematical Proceedings of the Cambridge\nPhilosophical Society, 91:57\u201374, 1982.\n\n14\n\n\fA\n\nAppendix \u2013 Various Proofs\n\nProof of Lemma 3.2. Assume the hypothesis, and let S \u2208 FREQ\u03b1 . Then, as w \u2192 S, we have\n|w|\u22121\n\nI\u03b2 (w) =\n\nX\n\n=\n\nX\n\nlog\n\ni=0\n\n1\n\u03b2(w[i])\n\n#(a, w) log\n\na\u2208\u03a3\n\n= |w|\n\n1\n\u03b2(a)\n\nfreqa (w) log\n\nX\n\n(\u03b1(a) + o(1)) log\n\nX\n\n\u03b1(a) log\n\na\u2208\u03a3\n\n= |w|\n\n1\n\u03b2(a)\n\nX\n\na\u2208\u03a3\n\n= |w|\n\na\u2208\u03a3\n\n= |w|\n\nX\u0012\n\na\u2208\u03a3\n\n1\n\u03b2(a)\n\n1\n+ o(|w|)\n\u03b2(a)\n\n1\n\u03b1(a)\n\u03b1(a) log\n+ \u03b1(a) log\n\u03b1(a)\n\u03b2(a)\n\n\u0013\n\n+ o(|w|)\n\n= (H(\u03b1) + D(\u03b1||\u03b2))|w| + o(|w|).\n\nProof of Lemma 3.3. Let \u03b1, \u03b2, and S be as given. By the frequency divergence lemma, we have\nI\u03bc (w)\n|w| log k\n=\nI\u03b2 (w)\n(H(\u03b1) + D(\u03b1||\u03b2))|w| + o(|w|)\nlog k\n=\nH(\u03b1) + D(\u03b1||\u03b2) + o(1)\nlog k\n=\n+ o(1)\nH(\u03b1) + D(\u03b1||\u03b2)\n1\n+ o(1)\n=\nHk (\u03b1) + Dk (\u03b1||\u03b2)\nas w \u2192 S. The present lemma follows from this and Lemma 3.1.\nThe following lemma summarizes the first part of the proof of Theorem 2.7.\nLemma A.1 ([6]). For each ILFSC C there is an integer m \u2208 Z+ such that, for each l \u2208 Z+ , there\nis an FSG G such that, for all w \u2208 \u03a3\u2217 ,\n(1)\n\nlog dG (w) \u2265 |w| log k \u2212 |C(w)| \u2212 m( |w|\nl + l).\nProof of Lemma 4.1. Assume the hypothesis. Let\n\u03b4\u03b2 = min log\na\u2208\u03a3\n\nnoting the following two things.\n15\n\n1\n,\n\u03b2(a)\n\n(A.1)\n\n\f(i) \u03b4\u03b2 > 0, because \u03b2 is positive.\n(ii) For all w \u2208 \u03a3\u2217 ,\nI\u03b2 (w) \u2265 \u03b4\u03b2 |w|.\n\n(A.2)\n\nChoose m for C as in Lemma A.1, let\n\u0018\n\n\u0019\n3m\nl=\n,\n\u01eb\u03b4\u03b2\n\n(A.3)\n\nand choose G for C, m, and l as in Lemma 4.1. Let\n\u03b4 = 23 \u01eb\u03b4\u03b2 ,\nnoting that \u03b4 > 0 and that\n|w| \u2265 l2 =\u21d2 \u01eb\u03b4\u03b2 |w| \u2212 m( |w|\nl + l)\nm\nl (|w|\n2m\n\u2265 \u01eb\u03b4\u03b2 |w| \u2212 l |w|\n= (\u01eb\u03b4\u03b2 \u2212 2m\nl )|w|\n(A.3) 2\n\u2265\n3 \u01eb\u03b4\u03b2 |w|,\n\n= \u01eb\u03b4\u03b2 |w| \u2212\n\ni.e., that\n\n+ l2 )\n\n|w| \u2265 l2 =\u21d2 \u01eb\u03b4\u03b2 |w| \u2212 m( |w|\nl + l) \u2265 \u03b4|w|.\n\n(A.4)\n\nIt follows that, for all w \u2208 I with |w| \u2265 l2 , we have\n(1)\n\n(s)\n\n\u03bc(w)\nlog dG,\u03b2 (w) = log( \u03b2(w)\ns dG (w))\n(1)\n\n= \u2212|w| log k + sI\u03b2 (w) + log dG (w)\n\u2265(A.1) sI\u03b2 (w) \u2212 |C(w)| \u2212 m( |w|\nl + l)\n\u2265(4.1) sI\u03b2 (w) \u2212 m( |w|\nl + l)\n\u2265(A.2) \u01eb\u03b4\u03b2 |w| \u2212 m( |w|\nl + l)\n\u2265(A.4) \u03b4|w|.\nHence (4.2) holds.\nAn FSG G = (Q, \u03a3, \u03b4, \u03b2, q0 ) is nonvanishing if all its bets are nonzero, i.e., if \u03b2(q)(a) > 0 holds\nfor all q \u2208 Q and a \u2208 \u03a3.\nLemma A.2 ([6]). For each FSG G and each \u03b4 > 0, there is a nonvanishing FSG G\u2032 such that,\nfor all w \u2208 \u03a3\u2217 ,\n(1)\n(1)\n(A.5)\ndG\u2032 (w) \u2265 k\u2212\u03b4|w| dG (w).\nThe following lemma summarizes the second part of the proof of Theorem 2.7.\nLemma A.3 ([6]). For each nonvanishing FSG G and each l \u2208 Z+ , there exists an ILFSC C such\nthat, for all w \u2208 \u03a3\u2217 ,\n(1)\n(A.6)\n|C(w)| \u2264 (1 + 2l )|w| log k \u2212 log dG (w).\n16\n\n\fProof of Lemma 4.2. Assume the hypothesis. Let\n\u03b3 = log\n\n1\n\u03b2max\n\n,\n\nwhere\n\u03b2max = max \u03b2(a).\na\u2208\u03a3\n\nNote that \u03b3 > 0 (because \u03b2 is positive) and that, for all w \u2208 \u03a3\u2217 ,\nI\u03b2 (w) \u2265 \u03b3|w|.\nLet\n\u03b4=\n\n(A.7)\n\n\u03b3\u01eb\nlog k\n\n(A.8)\n\nand choose G\u2032 for G and \u03b4 as in Lemma A.2. Let\n\u0018\n\u0019\n2 log k\nl=\n,\n\u03b3\u01eb\n\n(A.9)\n\nand choose C for G\u2032 and l as in Lemma A.3. Then, for all w \u2208 I,\n(1)\n\n|C(w)| \u2264(A.6) (1 + 2l )|w| log k \u2212 log dG\u2032 (w)\n(1)\n\n\u2264(A.9) |w|(\u03b3\u01eb + log k) \u2212 log dG\u2032 (w)\n(1)\n\n\u2264(A.5) |w|(\u03b3\u01eb + log k) \u2212 log(k \u2212\u03b4|w| dG (w))\n(1)\n\n= |w|(\u03b3\u01eb + log k + \u03b4 log k) \u2212 log dG (w)\n(1)\n\n= |w|(2\u03b3\u01eb + log k) \u2212 log dG (w)\n\u0013\n\u0012\n\u03b2(w)s\u22122\u01eb (s\u22122\u01eb)\n(w)\nd\n= |w|(2\u03b3\u01eb + log k) \u2212 log\n\u03bc(w) G,\u03b2\n\u0012\n\u0013\n\u03b2(w)s\u22122\u01eb\n\u2264(A.3) |w|(2\u03b3\u01eb + log k) \u2212 log\n\u03bc(w)\n= |w|(2\u03b3\u01eb + log k) \u2212 log(k|w| \u03b2(w)s\u22122\u01eb )\n= 2\u03b3\u01eb|w| \u2212 log \u03b2(w)s\u22122\u01eb\n= 2\u03b3\u01eb|w| + (s \u2212 2\u01eb)I\u03b2 (w)\n\u2264(A.7) sI\u03b2 (w).\n\nProof of Lemma 5.2. As in the proof of Lemma 3.3, the hypothesis implies that\nI\u03bc (w)\n1\n=\n+ o(1)\nI\u03b2 (w)\nHk (\u03b1) + Dk (\u03b1||\u03b2)\nas w \u2192 S. The present lemma follows from this and Lemma 5.1.\n\n17\n\n\fProof of Theorem 5.3. Assume the hypothesis, and let l \u2208 Z+ . Let \u03b1(l) be the restriction of the\nproduct probability measure \u03bc\u03b1 to \u03a3l , noting that H(\u03b1(l) ) = lH(\u03b1). We first show that\n(l)\n\nlim H(\u03c0R,n ) = H(\u03b1(l) ),\n\nn\u2192\u221e\n\n(A.10)\n\n(l)\n\nwhere \u03c0R,n is the empirical probability measure defined in section 2.5. To see this, let \u01eb > 0. By\nthe continuity of the entropy function, there is a real number \u03b4 > 0 such that, for all probability\nmeasures \u03c0 on \u03a3l ,\nmax |\u03c0(w) \u2212 \u03b1(l) (w)| < \u03b4 =\u21d2 |H(\u03c0) \u2212 H(\u03b1(l) )| < \u01eb.\nw\u2208\u03a3l\n\nSince R is \u03b1-normal, there is, for each w \u2208 \u03a3l , a positive integer nw such that, for all n \u2265 nw ,\n(l)\n\n(l)\n\n|\u03c0R,n (w) \u2212 \u03b1(l) (w)| = |\u03c0R,n (w) \u2212 \u03bc\u03b1 (w)| < \u03b4.\n(l)\n\nLet N = maxw\u2208\u03a3l nw . Then, for all n \u2265 N , we have |H(\u03c0R,n ) \u2212 H(\u03b1(l) )| < \u01eb, confirming (A.10).\nBy Theorem 2.5, we now have\ndimFS (R) = DimFS (R)\n1\n(l)\nlim H(\u03c0R,n )\n= inf\nl\u2208Z+ l log k n\u2192\u221e\n1\nH(\u03b1(l) )\n= inf\nl\u2208Z+ l log k\nH(\u03b1)\n=\nlog k\n= Hk (\u03b1).\n\n18\n\n\f"}