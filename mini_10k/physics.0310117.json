{"id": "http://arxiv.org/abs/physics/0310117v1", "guidislink": true, "updated": "2003-10-23T11:11:11Z", "updated_parsed": [2003, 10, 23, 11, 11, 11, 3, 296, 0], "published": "2003-10-23T11:11:11Z", "published_parsed": [2003, 10, 23, 11, 11, 11, 3, 296, 0], "title": "Additive Entropies of degree-q and the Tsallis Entropy", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0510011%2Cphysics%2F0510236%2Cphysics%2F0510219%2Cphysics%2F0510136%2Cphysics%2F0510134%2Cphysics%2F0510050%2Cphysics%2F0510131%2Cphysics%2F0510162%2Cphysics%2F0510112%2Cphysics%2F0510044%2Cphysics%2F0510045%2Cphysics%2F0510233%2Cphysics%2F0510127%2Cphysics%2F0510087%2Cphysics%2F0510207%2Cphysics%2F0510203%2Cphysics%2F0510256%2Cphysics%2F0510260%2Cphysics%2F0510218%2Cphysics%2F0510192%2Cphysics%2F0510223%2Cphysics%2F0510237%2Cphysics%2F0510155%2Cphysics%2F0510252%2Cphysics%2F0510140%2Cphysics%2F0510068%2Cphysics%2F0310104%2Cphysics%2F0310009%2Cphysics%2F0310074%2Cphysics%2F0310118%2Cphysics%2F0310127%2Cphysics%2F0310090%2Cphysics%2F0310031%2Cphysics%2F0310110%2Cphysics%2F0310014%2Cphysics%2F0310111%2Cphysics%2F0310013%2Cphysics%2F0310121%2Cphysics%2F0310006%2Cphysics%2F0310001%2Cphysics%2F0310040%2Cphysics%2F0310008%2Cphysics%2F0310049%2Cphysics%2F0310069%2Cphysics%2F0310025%2Cphysics%2F0310096%2Cphysics%2F0310022%2Cphysics%2F0310076%2Cphysics%2F0310027%2Cphysics%2F0310087%2Cphysics%2F0310081%2Cphysics%2F0310077%2Cphysics%2F0310041%2Cphysics%2F0310054%2Cphysics%2F0310017%2Cphysics%2F0310093%2Cphysics%2F0310119%2Cphysics%2F0310026%2Cphysics%2F0310007%2Cphysics%2F0310117%2Cphysics%2F0310148%2Cphysics%2F0310100%2Cphysics%2F0310064%2Cphysics%2F0310050%2Cphysics%2F0310157%2Cphysics%2F0310012%2Cphysics%2F0310088%2Cphysics%2F0310036%2Cphysics%2F0310072%2Cphysics%2F0310138%2Cphysics%2F0310020%2Cphysics%2F0310133%2Cphysics%2F0310144%2Cphysics%2F0310106%2Cphysics%2F0310010%2Cphysics%2F0310149%2Cphysics%2F0310082%2Cphysics%2F0310122%2Cphysics%2F0310016%2Cphysics%2F0310143%2Cphysics%2F0310131%2Cphysics%2F0310091%2Cphysics%2F0310094%2Cphysics%2F0310059%2Cphysics%2F0310085%2Cphysics%2F0310018%2Cphysics%2F0310155%2Cphysics%2F0310067%2Cphysics%2F0310145%2Cphysics%2F0310063%2Cphysics%2F0310051%2Cphysics%2F0310161%2Cphysics%2F0310160%2Cphysics%2F0310034%2Cphysics%2F0310033%2Cphysics%2F0310058%2Cphysics%2F0310109%2Cphysics%2F0310048%2Cphysics%2F0310060%2Cphysics%2F0310003%2Cphysics%2F0310021&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Additive Entropies of degree-q and the Tsallis Entropy"}, "summary": "The Tsallis entropy is shown to be an additive entropy of degree-q that\ninformation scientists have been using for almost forty years. Neither is it a\nunique solution to the nonadditive functional equation from which random\nentropies are derived. Notions of additivity, extensivity and homogeneity are\nclarified. The relation between mean code lengths in coding theory and various\nexpressions for average entropies is discussed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0510011%2Cphysics%2F0510236%2Cphysics%2F0510219%2Cphysics%2F0510136%2Cphysics%2F0510134%2Cphysics%2F0510050%2Cphysics%2F0510131%2Cphysics%2F0510162%2Cphysics%2F0510112%2Cphysics%2F0510044%2Cphysics%2F0510045%2Cphysics%2F0510233%2Cphysics%2F0510127%2Cphysics%2F0510087%2Cphysics%2F0510207%2Cphysics%2F0510203%2Cphysics%2F0510256%2Cphysics%2F0510260%2Cphysics%2F0510218%2Cphysics%2F0510192%2Cphysics%2F0510223%2Cphysics%2F0510237%2Cphysics%2F0510155%2Cphysics%2F0510252%2Cphysics%2F0510140%2Cphysics%2F0510068%2Cphysics%2F0310104%2Cphysics%2F0310009%2Cphysics%2F0310074%2Cphysics%2F0310118%2Cphysics%2F0310127%2Cphysics%2F0310090%2Cphysics%2F0310031%2Cphysics%2F0310110%2Cphysics%2F0310014%2Cphysics%2F0310111%2Cphysics%2F0310013%2Cphysics%2F0310121%2Cphysics%2F0310006%2Cphysics%2F0310001%2Cphysics%2F0310040%2Cphysics%2F0310008%2Cphysics%2F0310049%2Cphysics%2F0310069%2Cphysics%2F0310025%2Cphysics%2F0310096%2Cphysics%2F0310022%2Cphysics%2F0310076%2Cphysics%2F0310027%2Cphysics%2F0310087%2Cphysics%2F0310081%2Cphysics%2F0310077%2Cphysics%2F0310041%2Cphysics%2F0310054%2Cphysics%2F0310017%2Cphysics%2F0310093%2Cphysics%2F0310119%2Cphysics%2F0310026%2Cphysics%2F0310007%2Cphysics%2F0310117%2Cphysics%2F0310148%2Cphysics%2F0310100%2Cphysics%2F0310064%2Cphysics%2F0310050%2Cphysics%2F0310157%2Cphysics%2F0310012%2Cphysics%2F0310088%2Cphysics%2F0310036%2Cphysics%2F0310072%2Cphysics%2F0310138%2Cphysics%2F0310020%2Cphysics%2F0310133%2Cphysics%2F0310144%2Cphysics%2F0310106%2Cphysics%2F0310010%2Cphysics%2F0310149%2Cphysics%2F0310082%2Cphysics%2F0310122%2Cphysics%2F0310016%2Cphysics%2F0310143%2Cphysics%2F0310131%2Cphysics%2F0310091%2Cphysics%2F0310094%2Cphysics%2F0310059%2Cphysics%2F0310085%2Cphysics%2F0310018%2Cphysics%2F0310155%2Cphysics%2F0310067%2Cphysics%2F0310145%2Cphysics%2F0310063%2Cphysics%2F0310051%2Cphysics%2F0310161%2Cphysics%2F0310160%2Cphysics%2F0310034%2Cphysics%2F0310033%2Cphysics%2F0310058%2Cphysics%2F0310109%2Cphysics%2F0310048%2Cphysics%2F0310060%2Cphysics%2F0310003%2Cphysics%2F0310021&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Tsallis entropy is shown to be an additive entropy of degree-q that\ninformation scientists have been using for almost forty years. Neither is it a\nunique solution to the nonadditive functional equation from which random\nentropies are derived. Notions of additivity, extensivity and homogeneity are\nclarified. The relation between mean code lengths in coding theory and various\nexpressions for average entropies is discussed."}, "authors": ["B. H. Lavenda", "J. Dunning-Davies"], "author_detail": {"name": "J. Dunning-Davies"}, "author": "J. Dunning-Davies", "links": [{"title": "doi", "href": "http://dx.doi.org/10.3923/jas.2005.315.322", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/physics/0310117v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/physics/0310117v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "13 pages", "arxiv_primary_category": {"term": "physics.class-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.class-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.gen-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/physics/0310117v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/physics/0310117v1", "journal_reference": "Journal of Applied Sciences 5 (2005) 315-322", "doi": "10.3923/jas.2005.315.322", "fulltext": "arXiv:physics/0310117v1 [physics.class-ph] 23 Oct 2003\n\nAdditive Entropies of degree-q and the Tsallis\nEntropy\n\n2\n\nB. H. Lavenda1 and J. Dunning-Davies2\n1\nUniversit\u00e0 degli Studi Camerino 62032 (MC) Italy;\nemail: bernard.lavenda@unicam.it\nDepartment of Physics, University of Hull, Hull HU6 7RX\nEngland; email: j.dunning-davies@hull.ac.uk\nAbstract\nThe Tsallis entropy is shown to be an additive entropy of degree-q that\ninformation scientists have been using for almost forty years. Neither is\nit a unique solution to the nonadditive functional equation from which\nrandom entropies are derived. Notions of additivity, extensivity and homogeneity are clarified. The relation between mean code lengths in coding\ntheory and various expressions for average entropies is discussed.\n\n1\n\nThe 'Tsallis' Entropy\n\nIn 1988 Tsallis [1]published a much quoted paper containing an expression\nfor the entropy which differed from the usual one used in statistical mechanics. Previous to this, the R\u00e9nyi entropy was used as an interpolation formula\nthat connected the Hartley\u2013Boltzmann entropy to the Shannon\u2013Gibbs entropy.\nNotwithstanding the fact that the R\u00e9nyi entropy is additive, it lacks many\nother properties that characterize the Shannon\u2013Gibbs entropy. For example,\nthe R\u00e9nyi entropy is not subadditive, recursive, nor does it possess the branching and sum properties [2]. The so-called Tsallis entropy fills this gap, while\nbeing nonadditive, it has many other properties that resemble the Shannon\u2013\nGibbs entropy. It is no wonder then that this entropy fills an important gap.\nYet, it appears odd, to say the least, that information scientists have left\nsuch a gaping void in their analysis of entropy functions. A closer analysis of\nthe literature reveals that this is not the case and, indeed, a normalized Tsallis\nentropy seems to have first appeared in a 1967 paper by Havrda and Charv\u00e1t\n[3] who introduced the normalized 'Tsallis' entropy\n!\u001e\nn\nX\nq\npi \u2212 1\n(21\u2212q \u2212 1)\n(1)\nSn,q (p1 , . . . , pn ) =\ni=1\n\nPn\nfor a complete set of probabilities, pi , i.e.\ni=1 pi = 1, and parameter q > 0,\nbut q 6= 1. The latter requirement is necessary in order that (1) possess the\n1\n\n\ffundamental property of the entropy; that is, it is a concave function. According\nto Tsallis [4], only for q > 0 is the entropy, (1), said to be expansible [2] [cf. (6)\nbelow].\n\n2\n\nProperties of Additive Entropy of Degree-q\n\nThe properties used to characterize the entropy are [2, 5]:\n1. Concavity\nSn,q\n\nn\nX\n\npi xi\n\ni=1\n\n!\n\n\u2265\n\nn\nX\n\npi Sn,q (xi )\n\n(2)\n\ni=1\n\nwhere the nonnegative n-tuple, (p) = (p1 , . . . , pn ), forms a complete probability distribution. For ordinary means, the n-tuple, (x) = (x1 , . . . , xn ),\nrepresents a set of nonnegative numbers which constitute a set independent variables. What constitutes the main difficulty in proving theorems\non characterizing entropy functions in information theory is that the 'independent variables', (x), are not independent of their 'weights', (p) [6].\nCoding theory, to be discussed in the next section, derives the functional\ndependencies in a very elegant way through optimization. The entropies\nS(xi ) represent the costs of encoding a sequence of lengths xi , whose\nprobabilities are pi . Minimizing the mean length associated with the cost\nfunction, expressed as the weighted mean of the cost function, gives the\noptimal codeword lengths xi as functions of their probabilities, pi . Consequently, the entropies that result when the xi are evaluated at their\noptimal values by expressing them in terms of their probabilities, pi , constitute lower bounds to the mean lengths for the cost function.\n2. Non-negativity\nSn,q (p1 , . . . , pn ) \u2265 0\n\n(3)\n\nSn,q (p1 , . . . , pn ) = Sn,q (p[1] , . . . , p[n] )\n\n(4)\n\n3. Symmetry\nwhere [] denotes any arbitrary permutation of the indices on the probabilities. For the entropy, the symmetry property (4) means that it should\nnot depend upon the order in which the outcomes are labelled.\n4. The sum property\nSn,q (p1 , . . . , pn ) =\n\nn\nX\n\nSn,q (pi )\n\ni=1\n\nwhere Sn,q is a measurable function on ]0, 1[.\n\n2\n\n(5)\n\n\f5. Expansibility\nSn+1,q (0, p1 , . . . , pn ) = Sn,q (p1 , . . . , pn ),\n\n(6)\n\nmeaning that the entropy should not change when an outcome of probability zero is added.\n6. Recursivity of degree-q\n\u0012\n\np1\np2\nSn,q (p1 , . . . , pn ) = Sn\u22121,q (p1 +p2 , p3 , . . . , pn )+(p1 +p2 ) S2,q\n,\np1 + p2 p1 + p2\n(7)\nasserting that if a choice is split into two successive choices, the original\nentropy will be the weighted sum of the individual entropies. Recursivity\nimplies the branching property by requiring at the same time the additivity\nof the entropy as well as the weighting of the different entropies by their\ncorresponding probabilities [7].\nq\n\n7. Normality\nS2,q ( 21 , 12 ) = 1\n\n(8)\n\nS2,q (1, 0) = S2,q (0, 1) = 0\n\n(9)\n\n8. Decisivity\n9. Additivity of degree-q\nSnm,q (p1 q1 , . . . , p1 qm , . . . , pn qm ) = Sn,q (p1 , . . . , pn )\n+\n\n(2\n\n1\u2212q\n\n(10)\n\n\u2212 1)Sn,q (p1 , . . . , pn )Sm,q (q1 , . . . , qm )\n\nfor any two complete sets of probabilities, (p) and (q). As late as 1999,\nTsallis [4] refers to (10) as exhibiting \"a property which has apparently\nnever been focused before, and which we shall refer to as the composability\nproperty.\" Here, composability means something different than in information theory [2], in that it \"concerns the nontrivial fact that the entropy\nS(A + B) of a system composed of two independent subsystems A and B\ncan be calculated from the entropies S(A) and S(B) of the subsystems,\nwithout any need of the microscopic knowledge about A and B, other than\nthe knowledge of some generic universality class, herein the nonextensive\nuniversality class, represented by the entropic index q . . .\"[4].\nHowever, the additive entropy of degree-q, (1), is not the only solution to\nthe functional equation (10) for q 6= 1. The average entropy\n\uf8f1\n!1/q \uf8fc\nn\n\uf8fd\n\uf8f2\nX\nq\nA\npqi\n(11)\n1\u2212\n(p1 , . . . , pn ) =\nSn,q\n\uf8fe\nq\u22121\uf8f3\ni=1\n\nalso satisfies (10), with the only difference that (1 \u2212 q)/q replaces the\ncoefficient in the multiplicative term [8]. Since the weighted mean of\n3\n\n\u0013\n\n,\n\n\fdegree-q is homogeneous, the pseudo-additive entropy (11) is a first-order\nA\nA\nhomogeneous function of (p), Sn,q\n(\u03bbp1 , . . . , \u03bbpn ) = \u03bbSn,q\n(p1 , . . . , pn ). It\ncan be derived by averaging the same solution to the functional equation\n(10), in the case q 6= 1, as that used to derive the Tsallis entropy, except\nwith a different exponent and normalizing factor, under the constraint\nthat the probability distribution is complete [9]. Although the pseudoadditive entropy (11) lacks the property of recursivity, (7), it is monotonic,\ncontinuous, and concave for all positive values of q. Weighted means have\nbeen shown to be measures of the extent of a distribution [10], and (11)\nrelates the entropy to the weighted mean rather than to the more familiar\nlogarithm of the weighted mean, as in the case of the Shannon and R\u00e9nyi\nentropies.\nTsallis, in fact, associates additivity with extensivity in the sense that for\nindependent subsystems\nSnm,q (p1 q1 , . . . , pn qm ) = Sn,q (p1 , . . . , pn ) + Sm,q (q1 , . . . , qm )\n\n(12)\n\nAccording to Tsallis [4], superadditivity, q < 1, would correspond to superextensivity, and subadditivity, q > 1, would correspond to subextensivity. According to Callen [11], extensive parameters have values in a\ncomposite system that are equal to the sum of the values in each of the\nsystems. Anything that is not extensive is labelled intensive, although\nTsallis would not agree [cf. (30) below]. For instance if we consider blackbody radiation in a cavity of volume V , having an internal energy, U , and\nmagnify it \u03bb times, the resulting entropy\n\u03bbS(U, V ) = 34 \u03c3 1/4 (\u03bbU )3/4 (\u03bbV )1/4 ,\n\n(13)\n\nwill be \u03bb times the original entropy, S(U, V ), where \u03c3 is the StefanBoltzmann constant. Whereas extensitivity involves magnifying all the\nextensive variables by the same proportion, additivity in the sense of being\nsuperadditive or subadditive deals with a subclass of extensive variables,\nbecause the condition of extensivity of the entropy imposes that the determinant formed from the second derivatives of the entropy vanish [12].\nThe entropy of black-body radiation, (13), is extensive yet it is subadditive in either of the extensive variables. The property of subadditivity is\nwhat Lorentz used to show how interactions lead to a continual increase\nin entropy [12]. This is a simple consequence of Minkowski's inequality,\n3/4\n\nu1\n\n3/4\n\n+ u2\n\n\u2265 (u1 + u2 )3/4 ,\n\nwhere u = U/V is the energy density. Hence, (sub-or super-) extensivity\nis something very different from (sub-or super-) additivity.\n10. Strong additivity of degree-q\nSmn,q (p1 q11 , . . . , pn q1m , . . . , pn qnm ) = Sn,q (p1 , . . . , pn )\nn\nX\n(14)\npqj Sm,q (qj1 , . . . , qjm )\n+\nj=1\n\n4\n\n\fwhere qij is the conditional probability. Strong additivity of degree-q\ndescribes the situation in which the sets of outcomes of two experiments\nare not independent. Additivity of degree-q, (10), follows from strong\nadditivity by setting q1k = q2k = * * * = qmk = qk , and taking (1) into\nconsideration [2].\nA doubly stochastic matrix (qij ), where m = n, is used in majorization\nto distribute things, like income, more evenly [13], and this leads to an\nincrease in entropy. For if\nn\nX\n\nqj =\n\nqij pi ,\n\n(15)\n\ni=1\n\nand\n\nn\nX\nj=1\n\nqj =\n\nn\nX\ni=1\n\npi\n\nn\nX\n\nqij =\n\nn\nX\n\npi = 1,\n\ni=1\n\nj=1\n\nit follows from the convexity of \u03c8 = x ln x, or\n!\nn\nn\nX\nX\nqij \u03c8(pi ),\nqij pi \u2264\n\u03c8\ni=1\n\ni=1\n\nthat\nSn,1 (q1 , . . . , qn ) = \u2212\n\nn\nX\ni=1\n\nqi ln qi \u2265 \u2212\n\nn X\nn\nX\n\nqij pi ln pi\n\n(16)\n\ni=1 j=1\n\nP\nsince ni=1 qij = 1. We may say that p majorizes q, p \u227b q if and only if\n(15) for some doubly stochastic matrix (qij ) [14]. A more even spread of\nincomes increases the entropy. Here we are at the limits of equilibrium\nthermodynamics because we are invoking a mechanism for the increase\nin entropy, which in the case of incomes means taking from the rich and\ngiving to the poor [15]. This restricts q in the 'Tsallis' entropy to ]0, 1[.\nValues of q in ]1, 2[ show an opposing tendency of balayage or sweeping\nout [16]. Whereas averaging tends to decrease inequality, balayage tends\nto increase it [15].\nYet Tsallis [4] refers to processes with q < 1, i.e. pqi > pi , as rare events,\nand to q > 1, i.e. pqi < pi as frequent events. However, only in the case\nwhere q < 1 will the Shannon entropy, (16) be a lower bound to other\nentropies like, the R\u00e9nyi entropy\n!\nn\nX\n1\nq\nR\nSn,q =\n(17)\npi\nln\n1\u2212q\ni=1\nwhich is the negative logarithm of the weighted mean of piq\u22121 . The R\u00e9nyi\nentropy has the attributes of reducing to the Shannon\u2013Gibbs entropy, (16),\nin the limit as q \u2192 1, and to the Hartley\u2013Boltzmann, entropy\nSn,0 (1/n, . . . , 1/n) = ln n\n5\n\n(18)\n\n\fin the case of equal a priori probabilities pi = 1/n. This leads to the\nproperty of\n11. n-maximal\nSn,q (p1 , . . . , pn ) \u2264 Sn,q\n\n\u0012\n\n1\n1\n,...\nn\nn\n\n\u0013\n\n(19)\n\nfor any given integer n \u2265 2. The right-hand side of (19) should be a\nmonotonic increasing function of n. As we have seen, the tendency of the\nentropy to increase as the distribution becomes more uniform is due to\nthe property of concavity (2). Hence, it would appear that processes with\nq < 1 would be compatible with the second law of thermodynamics, rather\nthan being rare exceptions to it!\n12. Continuity: The entropy is a continuous function of its n variables. Small\nchanges in the probability cause correspondingly small changes in the entropy. Additive entropies of degree-q are small for small probabilities,\ni.e.,\npq + (1 \u2212 p)q \u2212 1\nlim S2,q (p) = lim\n.\np\u21920\np\u21920\n21\u2212q \u2212 1\n\n3\n\nCoding Theory and Entropy Functions\n\nThe analogy between coding theory and entropy functions has long been known\n[18]. If k1 , . . . , kn are the lengths of codewords of a uniquely decipherable code\nwith D symbols then the average codeword length\nn\nX\n\npi ki\n\n(20)\n\ni=1\n\nis bounded from below by the Shannon-Gibbs entropy (16) if the logarithm is\nto the base D. The optimal codeword length is ki = \u2212 ln pi , which represents\nthe information content in event Ei . If D = 21 then pi = 21 contains exactly one\nbit of information.\nOrdinarily, one tries to keep the average codeword length (20) small, but it\ncannot be made smaller than the Shannon-Gibbs entropy. An economical code\nhas frequently occurring messages with large pi and small ki . Rare messages are\nthose with small pi and large ki . The solution ni = \u2212 ln pi has the disadvantage\nthat the codeword length is very great if the probability of the symbol is very\nsmall. A better measure of the codeword length would be\n!\nn\nX\n1\n\u03c4 ki\n(21)\npi D\nlog\n\u03c4\ni=1\nwhere \u03c4 = (1 \u2212 q)/q, thereby limiting q to the interval [0, 1]. As \u03c4 \u2192 \u221e, the\nlimit of (21) is the largest of the ki , independent of pi . Therefore, if q is small\nenough, or \u03c4 large enough, the very large ki 's will contribute very strongly to\n6\n\n\fthe average codeword length (21), thus keeping it from being small even for very\nsmall pi . The optimal codeword length is now\nki = \u2212q ln pi +\n\nn\nX\n\npqi ,\n\ni=1\n\nshowing that the R\u00e9nyi entropy is the lower bound to the average codeword\nlength (21) [18]. Just as the pi = D\u2212ki are the optimum probabilities for the\nShannon-Gibbs entropy, the optimum probabilities for the R\u00e9nyi entropy are\nthe so-called escort probabilities,\npq\nD\u2212ki = Pn i\n\ni=1\n\npqi\n\n.\n\n(22)\n\nAs pi \u2192 0, the optimum value of ki is asymptotic to \u2212q ln pi so that the optimum\nlength is less than \u2212 ln pi for q < 1 and sufficiently small pi . This provides\nadditional support for keeping q within the interval [0, 1] [16].\nAlthough the R\u00e9nyi entropy is additive it does not have other properties\nlisted above; for instance, it is not recursive and does not have the branching\nproperty nor the sum property. It is precisely the 'Tsallis' entropy which fills\nthe gap, while not being additive, it has many of the other properties that\nan entropy should have [19]. Therefore, in many ways the additive entropy of\ndegree-q (1) is closer to the Shannon entropy, (16) than the R\u00e9nyi entropy is.\nThe so-called additive entropies of degree-q can be written as\nSn,q (p1 , . . . , pn ) =\n\nn\nX\n\n(p1 + . . . + pi )q f\n\ni=2\n\n\u0012\n\npi\np1 + . . . + pi\n\n\u0013\n\n,\n\n(23)\n\nwhere the function f is a solution to the functional equation\n\u0012\n\u0013\n\u0012\n\u0013\ny\nx\nq\nq\nf (x) + (1 \u2212 x) f\n= f (y) + (1 \u2212 y) f\n,\n1\u2212x\n1\u2212y\nsubject to f (0) = f (1), which was rederived by Curado and Tsallis [20], and the\nproperty of additivity of degree-q (10) was referred to them as pseudo-additivity,\nomitting the original references. What these authors appeared to have missed\nare the properties of strong additivity, (14) and recursivity of degree-q (7). These\nproperties can be proven by direct calculation using the normalized additive\nentropy of degree-q, (1). Additive entropies of degree-q \u2265 1 are also subadditive.\nMoreover, additive entropies of degree-q satisfy the sum property, (5) where\nSq (pi ) = (pqi \u2212 pi )/(21\u2212q \u2212 1) \u2265 0.\nOnly for q > 0 will (24), and consequently (1), be concave since\nS\u2032\u2032q (pi ) = q(q \u2212 1)piq\u22122 /(21\u2212q \u2212 1) \u2264 0,\n\n7\n\n(24)\n\n\fwhere the prime stands for differentiation with respect to pi . This is contrary\nto the claim that the additive entropy of degree-q is \"extremized for all values\nof q\"[1]. It can easily be shown that the concavity property\n!\nn\nn\nX\nX\npi Sq (xi ),\npi xi \u2265\nSq\ni=1\n\ni=1\n\nimplies the monotonic increase in the entropy (19). Setting pi = 1/n and using\nthe sum property (5) lead to\n\u0012 \u0013\n\u0012\n\u0013\nn \u0010\nn\nX\nX\npi \u0011\n1\n1\n1\nSq (pi ) \u2264 nSq\nSn,q (p1 , . . . , pn ) =\n= nSq\n= Sn,q\n,\n,...,\nn\nn\nn\nn\ni=1\ni=1\nshowing that Sn,q (1/n, . . . , 1/n) is maximal.\nIn order to obtain explicit expressions for the probabilities, Tsallis and collaborators maximized their non-normalized entropy\n!\nn\nX\nq\nT\npi \u2212 1 /(1 \u2212 q)\n(25)\nSn,q (p1 . . . , pn ) =\ni=1\n\nwith respect to certain constraints. Taking their cue from Jaynes' [21] formalism\nof maximum entropy, (25) was to be maximized with respect to the finite norm\n[22]\nZ\n\u221e\n\np(x) dx = 1\n\n\u2212\u221e\n\nand the so-called q average of the second moment [20]\nZ \u221e\nx2 [\u03c3 p(x)]q d(x/\u03c3) = \u03c3 2 .\nx2 q =\n\n(26)\n\n\u2212\u221e\n\nThe latter condition was introduced because the variance of the distribution did\nnot exist, and the weights, (pq ), have been referred to as 'escort' probabilities\n[cf. (22) above]. The resulting distribution is almost identical to Student's\ndistribution\nr\n\u0012\n\u0013\u22121/(q\u22121)\n\u03bc (q \u2212 1)\n(q \u2212 1) 2\n\u0393 (1/(q \u2212 1))\np(x|\u03bc) =\n1+\n\u03bcx\n(27)\n\u03c0 (3 \u2212 q) \u0393 ((3 \u2212 q)/2(q \u2212 1))\n(3 \u2212 q)\nwhere (3 \u2212 q)/(q \u2212 1) is the number of degrees of freedom, and \u03bc is the Lagrange\nmultiplier for the constraint (26) [23].\nThe Gaussian distribution is the only stable law with a finite variance, all\nthe other stable laws have infinite variance. These stable laws have much larger\ntails than the normal law which is responsible for the infinite nature of their\nvariances. Their initial distributions are given by the intensity of small jumps,\nwhere the intensity of jumps having the same sign of x, and greater than x in\nabsolute value is [24]\nc\nF (x) = \u03b2 ,\n(28)\nx\n8\n\n\ffor x > 1. For \u03b2 < 1, the generalized random process, which is of a Poisson\nnature, produces only positive jumps, whose intensity (28) is always increasing.\nNo moments exist, and the fact that\n\u03b2\n\nZ(\u03bb) = e\u2212\u03bb\n\n(29)\n\nwhere \u03bb is both positive and real, follows directly from P\u00f3lya's theorem: If for\neach \u03bb, Z(0) = 1, Z(\u03bb) \u2265 0, Z(\u03bb) = Z(\u2212\u03bb), Z(\u03bb) is decreasing and continuous convex on the right half interval, then Z(\u03bb) is a generating function [25].\nConvexity is easily checked for 0 < \u03b2 \u2264 1, and it is concluded that z(\u03bb) is a\ngenerating function. In other words,\nZ \u221e\n\u0001\n1 \u2212 e\u2212\u03bbx dF (x) = \u0393(1 \u2212 \u03b2)\u03bb\u2212\u03b2\n1 \u2212 Z(\u03bb) = \u2212\n0\n\nexists for a positive argument of the Gamma function, and that implies \u03b2 < 1.\nThis does not hold on the interval 1 < \u03b2 < 2, where it makes sense to talk\nabout a compensated sum of jumps, since a finite mean exists. In the limit\n\u03b2 = 2, positive and negative jumps about the mean value become equally as\nprobable and the Wiener-L\u00e9vy process results, which is the normal limit. If\none introduces a centering term in the expression, \u03bbx, the same expression for\nthe generating function, (29), is obtained to lowest power in \u03bb, as \u03bb \u2192 0 and\nx \u2192 \u221e, such that their product is finite.\nThese stable distributions, 0 < \u03b2 < 1, (and quasi-stable ones, 1 < \u03b2 < 2,\nbecause the effect of partial compensation of jumps introduces an arbitrary additive constant) are related to the process of super-diffusion, where the asymptotic\nbehavior of the generalized Poisson process has independent increments with intensity (28). For strictly stable processes, the super-diffusion packet spreads out\nfaster than the packet of freely moving particles, while a quasi-stable distribution describes the random walk of a particle with a finite mean velocity. It was\nhoped that these tail distributions could be described by an additive entropy\nof degree-q, where the degree of additivity would be related to the exponent\nof the stable, or quasi-stable, distribution. Following the lead of maximum entropy, where the optimal distribution results from maximizing the entropy with\nall that is known about the system, the same would hold true for maximizing\nthe additive entropy of degree-q. However, it was immediately realized that the\nvariance of the distribution does not exist.\nComparing the derivative of the tail density (28) with (27) identifies \u03b2 =\n(3 \u2212 q)/(q \u2212 1), requiring the stable laws to fall in the domain 35 < q < 3 [22].\nHowever, it is precisely in the case in which we are ignorant of the variance\nthat the Student distribution is used to replace the normal since it has much\nfatter tails and only approaches the latter as the number of degrees of freedom\nincreases without limit [24]. Just as the ratio of the difference of the mean\nof a sample and the mean of the distribution to the standard deviation is distributed normally, the replacement of the standard deviation by its estimator is\ndistributed according to the Student's distribution. This distribution (27) was\nnot to be unexpected, because it stands in the same relation to the normal law\n9\n\n\fas the 'Tsallis' entropy, (25), in the limit as the number of degrees of freedom\nis allowed to increase without limit.\nWhereas weighted means of order-q\n\u0012 Pn\nq \u00131/q\ni=1 pi xi\nP\nMq =\nn\ni=1 pi\ndo have physical relevance for different values of q, the so-called q-expectation\nPn\npqi xi\nhxiq := Pi=1\nn\nq ,\ni=1 pi\n\nhas no physical significance for values of q 6= 1. Since the connection between\nstatistical mechanics and thermodynamics lies in the association of average values with thermodynamic variables, the q-expectations would lead to incorrect\naverages. This explains why for Tsallis the internal energy of a composite system is not the same as the internal energies of the subsystems, and makes the\nquestion \"if we are willing to consider the nonadditivity of the entropy, why it is\nso strange to accept the same for the energy?\"[26] completely meaningless. Yet,\nthe zeroth law of thermodynamics, and the derivation of the Tsallis nonintensive\ninverse temperature,\n\u001e\n\u2202Sn,q\n[1 \u2212 (1 \u2212 q)Sn,q ],\n(30)\n\u03b2=\n\u2202Uq\nwhere Uq is the q-expectation of the internal energy, rest on the fact that the\ntotal energy of the composite system is conserved [27].\nIt is as incorrect to speak of 'Tsallis' statistics [28] as it would be to talk of\nR\u00e9nyi statistics. These expressions are mere interpolation formulas leading to\nstatistically meaningful expressions for the entropy in certain well-defined limits.\nWhereas for the R\u00e9nyi entropy the limits q \u2192 1 and q \u2192 0 give the ShannonGibbs and Hartley-Boltzmann entropies, respectively, without assuming equal\nprobabilities, the additive entropy of degree-q reduces to the Shannon entropy in\nthe limit as q \u2192 1, but it must further be assumed that the a priori probabilities\nare equal in order to reduce it to the Hartley-Boltzmann entropy. Hence, only\nthe R\u00e9nyi entropies are true interpolation formulas.\nEither the average of \u2212 ln pi leading to the Shannon entropy, or the negative\nof the weighted average of piq\u22121 , resulting in the R\u00e9nyi entropy will give the\nproperty of additivity [2]. Whereas the Shannon entropy is the negative of the\nlogarithm of the geometric mean of the probabilities,\nSn,1 (p1 , . . . , pn ) = \u2212 ln Gn (p1 , . . . , pn ),\nwhere\n\nGn (p1 . . . , pn ) = \u03a0ni=1 ppi i\n\nis the geometric mean, the R\u00e9nyi entropy is the negative of the logarithm of the\nweighted mean\nR\nSn,q\n= \u2212 ln Mq\u22121 ,\n10\n\n\fwhere\nMq\u22121 =\n\nn\nX\n\npi piq\u22121\n\ni=1\n\npiq\u22121 .\n\n!1/(q\u22121)\n\nis the weighted mean of\nIf the logarithm is to the base 2, the additive\nentropies of degree-q are exponentially related to the R\u00e9nyi entropies of order-q\nby\n\u0011\u000e\n\u0010\nR\nSn,q = 2(1\u2212q)Sn,q \u2212 1 (21\u2212q \u2212 1),\n\nwhich make it apparent that they cannot be additive. But nonadditivity has\nnothing to do with nonextensivity.\nAs a concluding remark it may be of interest to note that undoubtedly the\noldest expression for an additive entropy of degree-2 was introduced by Gini\n[29] in 1912, who used it as an index of diversity or inequality. Moreover,\ngeneralizations of additive entropies of degree-q are well-known. It has been\nclaimed that \"Tsallis changed the mathematical form of the definition of entropy\nand introduced a new parameter q\"[30]. Generalizations that introduce additive\nentropies of degree-q + ri \u2212 1 [31]\n!\u001e\nPn\ni \u22121\npq+r\ni\ni=1\nPn\nSn,q,r1 ,...,rn (p1 , . . . , pn ) =\n\u22121\n(21\u2212q \u2212 1),\nri\ni=1 pi\nwith n + 1 parameters, should give even better results when it comes to curve\nfitting.\n\nReferences\n[1] C. Tsallis, J. Stat. Phys. 52, 470 (1988).\n[2] J. Acz\u00e9l and Z. Dar\u00f3czy, On the Measures of Information and their Characterization (Academic Press, New York, 1975).\n[3] J. Havrda and F. Charv\u00e1t, Kybernetika (Prague) 3, 30 (1967), and first\nacknowledged by Tsallis in C. Tsallis, Solitons and Fractals 6, 539 (1995).\n[4] C. Tsallis, Braz. J. Phys. 29, 1 (1999).\n[5] A. M. Mathai and P. N. Rathie, Basic Concepts in Information Theory and\nStatistics (Wiley, New York, 1975).\n[6] J. Ac\u017ael, Lectures on Functional Equations and their Applications (Academic Press, New York, 1966).\n[7] A. R\u00e9nyi, Probability Theory (North-Holland, Amsterdam, 1970).\n[8] D. E. Boekee and J. C. A. Van der Lubbe, Inform. Contr. 45, 136 (1980).\n[9] S. Arimoto, Inform. Contr. 19, 181 (1971).\n11\n\n\f[10] L. L. Campbell, Z. Wahrscheinlichkeitstheorie verw. Geb. 5, 217 (1966).\n[11] H. B. Callen, Thermodynamics and and Introduction to Thermostatics, 2nd\nedn. (Wiley, New York, 1985).\n[12] B. H. Lavenda, Statistical Physics: A Probabilistic Approach (WileyInterscience, New York, 1991).\n[13] A. W. Marshall and I. Olkin, Inequalities: Theory of Majorization and its\nApplications (Academic Press, San Diego, 1979).\n[14] G. H. Hardy, J. E. Littlewood, and G. P\u00f3lya, Inequalities 2nd. edn. (Cambridge U. P., Cambridge, 1952).\n[15] B. C. Arnold, Majorization and the Lorenz Order: A Brief Introduction\n(Springer-Verlag, Berlin, 1987).\n[16] B. H. Lavenda, Int. J. theoret. Phys. 37, 3119 (1998); B. H. Lavenda and\nJ. Dunning-Davies, in preparation.\n[17] A. R\u00e9nyi, in Proc. Fourth Berkeley Symp. Math. Statist. Prob. (Wiley, New\nYork, 1961), vol. 1, pp. 547\u2013561.\n[18] L. L. Campbell, Inform. Contr. 8, 423 (1966); J. Acz\u00e9l and J. Dhombres,\nFunctional Equations in Several Variables (Cambridge U. P., Cambridge,\n1972); B. H. Lavenda, J. Phys. A 31, 5651 (1998).\n[19] Z. Dar\u00f3czy, Inform. Contr. 16, 36 (1970), and first acknowledged by Tsallis\nin [20].\n[20] E. M. F. Curado and C. Tsallis, J. Phys. A24 L69 (1991); Corrigenda 24,\n3187 (1991); 25, 1019 (1992).\n[21] E. T. Jaynes, Phys. Rev. 106, 620 (1957).\n[22] C. Tsallis, S. V. F. Levy, A. M. C. Souza, and R. Maynard, Phys. Rev.\nLett. 75, 3589 (1995).\n[23] A. M. C. Souza and C. Tsallis, Physica A 236, 52 (1997).\n[24] B. de Finetti, Theory of Probability, vol. 2 (Interscience, New York, 1990).\n[25] K. L. Chung, A Course in Probability Theory (Academic Press, San Diego,\n1974).\n[26] C. Tsallis, R. S. Mendes, and A. R. Plastino, Physica A 261, 534 (1998).\n[27] S. Abe, Physica A 269, 403 (1999).\n[28] Science 300, 249 (2003).\n[29] C. Gini, Studi Economico-Giuridici della Facolt\u00e1 di Giurisprudenza\ndell'Universit\u00e1 di Cagliari AIII, parte II.\n12\n\n\f[30] A. Chou, Science 297, 1268 (2002).\n[31] P. N. Rathie, Kybernetika 7, 125 (1971).\n\n13\n\n\f"}