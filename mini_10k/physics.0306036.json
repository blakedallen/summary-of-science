{"id": "http://arxiv.org/abs/physics/0306036v1", "guidislink": true, "updated": "2003-06-04T13:53:31Z", "updated_parsed": [2003, 6, 4, 13, 53, 31, 2, 155, 0], "published": "2003-06-04T13:53:31Z", "published_parsed": [2003, 6, 4, 13, 53, 31, 2, 155, 0], "title": "Revison Control in the Grid Era - the unmet challenge", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0306153%2Cphysics%2F0306097%2Cphysics%2F0306081%2Cphysics%2F0306114%2Cphysics%2F0306172%2Cphysics%2F0306167%2Cphysics%2F0306181%2Cphysics%2F0306126%2Cphysics%2F0306133%2Cphysics%2F0306012%2Cphysics%2F0306142%2Cphysics%2F0306176%2Cphysics%2F0306066%2Cphysics%2F0306124%2Cphysics%2F0306149%2Cphysics%2F0306127%2Cphysics%2F0306065%2Cphysics%2F0306016%2Cphysics%2F0306175%2Cphysics%2F0306182%2Cphysics%2F0306111%2Cphysics%2F0306085%2Cphysics%2F0306152%2Cphysics%2F0306001%2Cphysics%2F0306042%2Cphysics%2F0306057%2Cphysics%2F0306032%2Cphysics%2F0306185%2Cphysics%2F0306135%2Cphysics%2F0306071%2Cphysics%2F0306138%2Cphysics%2F0306020%2Cphysics%2F0306010%2Cphysics%2F0306080%2Cphysics%2F0306197%2Cphysics%2F0306025%2Cphysics%2F0306095%2Cphysics%2F0306155%2Cphysics%2F0306030%2Cphysics%2F0306190%2Cphysics%2F0306027%2Cphysics%2F0306063%2Cphysics%2F0306091%2Cphysics%2F0306150%2Cphysics%2F0306103%2Cphysics%2F0306022%2Cphysics%2F0306072%2Cphysics%2F0306188%2Cphysics%2F0306023%2Cphysics%2F0306105%2Cphysics%2F0306040%2Cphysics%2F0306183%2Cphysics%2F0306067%2Cphysics%2F0306073%2Cphysics%2F0306192%2Cphysics%2F0306113%2Cphysics%2F0306064%2Cphysics%2F0306171%2Cphysics%2F0306169%2Cphysics%2F0306130%2Cphysics%2F0306087%2Cphysics%2F0306011%2Cphysics%2F0306047%2Cphysics%2F0306120%2Cphysics%2F0306116%2Cphysics%2F0306121%2Cphysics%2F0306074%2Cphysics%2F0306131%2Cphysics%2F0306036%2Cphysics%2F0306100%2Cphysics%2F0306005%2Cphysics%2F0306139%2Cphysics%2F0306045%2Cphysics%2F0306013%2Cphysics%2F0306108%2Cphysics%2F0306002%2Cphysics%2F0306015%2Cphysics%2F0306099%2Cphysics%2F0306004%2Cphysics%2F0306077%2Cphysics%2F0306054%2Cphysics%2F0306156%2Cphysics%2F0306084%2Cphysics%2F0306125%2Cphysics%2F0306101%2Cphysics%2F0306187%2Cphysics%2F0306078%2Cphysics%2F0306033%2Cphysics%2F0306157%2Cphysics%2F0306137%2Cphysics%2F0306049%2Cphysics%2F0306184%2Cphysics%2F0306082%2Cphysics%2F0306191%2Cphysics%2F0306198%2Cphysics%2F0306028%2Cphysics%2F0306196%2Cphysics%2F0306151%2Cphysics%2F0306092%2Cphysics%2F0306170%2Cphysics%2F0306038&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Revison Control in the Grid Era - the unmet challenge"}, "summary": "As we move to distribute High Energy Physics computing tasks throughout the\nglobal Grid, we are encountering ever more severe difficulties installing and\nselecting appropriate versions of the supporting products. Problems show up at\nevery level: the base operating systems and tools, general purpose utilities\nlike root, and specific releases of application code. I will discuss some\nspecific examples, including what we've learned in commissioning the SAM\nsoftware for use by CDF. I will show why revision control can be a truly\ndifficult problem, and will discuss what we've been doing to measure and\ncontrol the situation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0306153%2Cphysics%2F0306097%2Cphysics%2F0306081%2Cphysics%2F0306114%2Cphysics%2F0306172%2Cphysics%2F0306167%2Cphysics%2F0306181%2Cphysics%2F0306126%2Cphysics%2F0306133%2Cphysics%2F0306012%2Cphysics%2F0306142%2Cphysics%2F0306176%2Cphysics%2F0306066%2Cphysics%2F0306124%2Cphysics%2F0306149%2Cphysics%2F0306127%2Cphysics%2F0306065%2Cphysics%2F0306016%2Cphysics%2F0306175%2Cphysics%2F0306182%2Cphysics%2F0306111%2Cphysics%2F0306085%2Cphysics%2F0306152%2Cphysics%2F0306001%2Cphysics%2F0306042%2Cphysics%2F0306057%2Cphysics%2F0306032%2Cphysics%2F0306185%2Cphysics%2F0306135%2Cphysics%2F0306071%2Cphysics%2F0306138%2Cphysics%2F0306020%2Cphysics%2F0306010%2Cphysics%2F0306080%2Cphysics%2F0306197%2Cphysics%2F0306025%2Cphysics%2F0306095%2Cphysics%2F0306155%2Cphysics%2F0306030%2Cphysics%2F0306190%2Cphysics%2F0306027%2Cphysics%2F0306063%2Cphysics%2F0306091%2Cphysics%2F0306150%2Cphysics%2F0306103%2Cphysics%2F0306022%2Cphysics%2F0306072%2Cphysics%2F0306188%2Cphysics%2F0306023%2Cphysics%2F0306105%2Cphysics%2F0306040%2Cphysics%2F0306183%2Cphysics%2F0306067%2Cphysics%2F0306073%2Cphysics%2F0306192%2Cphysics%2F0306113%2Cphysics%2F0306064%2Cphysics%2F0306171%2Cphysics%2F0306169%2Cphysics%2F0306130%2Cphysics%2F0306087%2Cphysics%2F0306011%2Cphysics%2F0306047%2Cphysics%2F0306120%2Cphysics%2F0306116%2Cphysics%2F0306121%2Cphysics%2F0306074%2Cphysics%2F0306131%2Cphysics%2F0306036%2Cphysics%2F0306100%2Cphysics%2F0306005%2Cphysics%2F0306139%2Cphysics%2F0306045%2Cphysics%2F0306013%2Cphysics%2F0306108%2Cphysics%2F0306002%2Cphysics%2F0306015%2Cphysics%2F0306099%2Cphysics%2F0306004%2Cphysics%2F0306077%2Cphysics%2F0306054%2Cphysics%2F0306156%2Cphysics%2F0306084%2Cphysics%2F0306125%2Cphysics%2F0306101%2Cphysics%2F0306187%2Cphysics%2F0306078%2Cphysics%2F0306033%2Cphysics%2F0306157%2Cphysics%2F0306137%2Cphysics%2F0306049%2Cphysics%2F0306184%2Cphysics%2F0306082%2Cphysics%2F0306191%2Cphysics%2F0306198%2Cphysics%2F0306028%2Cphysics%2F0306196%2Cphysics%2F0306151%2Cphysics%2F0306092%2Cphysics%2F0306170%2Cphysics%2F0306038&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "As we move to distribute High Energy Physics computing tasks throughout the\nglobal Grid, we are encountering ever more severe difficulties installing and\nselecting appropriate versions of the supporting products. Problems show up at\nevery level: the base operating systems and tools, general purpose utilities\nlike root, and specific releases of application code. I will discuss some\nspecific examples, including what we've learned in commissioning the SAM\nsoftware for use by CDF. I will show why revision control can be a truly\ndifficult problem, and will discuss what we've been doing to measure and\ncontrol the situation."}, "authors": ["Arthur Kreymer"], "author_detail": {"name": "Arthur Kreymer"}, "author": "Arthur Kreymer", "arxiv_comment": "Talk from the 2003 Computing in High Energy and Nuclear Physics\n  (CHEP03),La Jolla,Ca, USA, March 2003, 5 pages, LaTeX, no eps figures. PSN\n  TUJT003", "links": [{"href": "http://arxiv.org/abs/physics/0306036v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/physics/0306036v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/physics/0306036v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/physics/0306036v1", "journal_reference": "ECONFC0303241:TUJT003,2003", "doi": null, "fulltext": "Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003\n\nRevison Control in the Grid Era - the unmet challenge\nArthur Kreymer\n\narXiv:physics/0306036v1 [physics.data-an] 4 Jun 2003\n\nFermilab, Batavia, IL 60510, USA\n\nAs we move to distribute High Energy Physics computing tasks throughout the global Grid, we are encountering\never more severe difficulties installing and selecting appropriate versions of the supporting products. Problems\nshow up at every level: the base operating systems and tools, general purpose utilities like root, and specific\nreleases of application code. I will discuss some specific examples, including what we've learned in commissioning\nthe SAM software for use by CDF. I will show why revision control can be a truly difficult problem, and will\ndiscuss what we've been doing to measure and control the situation.\n\n1. Concerns\nI'm here to talk about revision control because this\nhas emerged lately as an issue of major concern within\nCDF, especially as we start to integrate our own code\nwith the SAM data handing system previously used\nonly at the Fermilab D0 experiment, and as we start\nusing Grid tools to submit jobs at remote sites.\nWe have considerable experience distributing the\nCDF offline analysis code on a wide variety of hardware and software platforms, and in a number of runtime environments ( online Level 3 triggers, online\nmonitoring, interactive and batch systems. )\nWhen a user comes to us saying \"It's Broken !\", we\nneed to know just what it is that they are running.\nThere can be a surprisingly large number of possibilities if you stop to count them carefully. It is important\nto do so occasionally, and to take steps to keep this\nunder some level of control. I will show several specific\nexamples.\n\n2. CDF Releases\n\nPoint releases are numbered like 4.8.1, and are\notherwise just like base releases. They have specific corrections for operations, and are usually\nnot on the main line of development. For example, 4.8.4 for Production track restruction for\nthe 2003 winter conferences, and 4.9.1hpt2, for\nphysics analysis for the same conferences.\n\u2022 Integration\nIntegration releases are (bi)weekly frozen releases which have much less testing than base\nrelease. We only require that the code compile\nand link for integration releases. They provide\na stable base against which major code development is conducted.\n\u2022 Development\nWe build a release using the latest source code\nfrom the head of the CVS code repository each\nnight, using the code present at midnight. We\nalso perform rudementary code execution validition nightly. This gives early feedback in case of\nbroken code or global problems.\n\u2022 Patch\nInevitably there are slight administrative code\nchanges need in the Level3, online monitoring,\nand production farm operations which do not\nmerit a formal code release, but which need to\nbe reproducible from CVS tags. Patch releases\nare a formalization of the usual developers temporary working area, using a standard creation\nscript and named patch list.\n\nFirst, let me outline what kinds of offline software\nreleases we support, just so that the terminology is\nunderstood.\nThen let's count how many varieties of code we\nmight be dealing with for a single given line of source\ncode, in the development release.\nThis is a worst case, but it's what got me worrying\nabout this problem, and it is interesting to look at the\nnumbers.\n\nYou can find a current list of base, point and integration releases on the web 1 .\n\n2.1. Types of Releases\n\n2.2. Development 2001\n\n\u2022 Base\nThese are fully tested for specified purposes\n(Production, Analysis ) Version numbers are for\nexample 4.8.0 for general purpose use.\n\u2022 Point\nTUJT003\n\nHere we count the different ways in which a given\nline of code might have been compiled in the nightly\ndevelopment release, in 2001 .\n\n1 http://cdfkits.fnal.gov/DIST/logs/rel/Releases.html\n\n1\n\n\f2\n\nComputing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003\n\u2022 5 - OS\nWe were building with Linux 2.0, Linux 2.2,\nIRIX , OSF/1 and SunOS\n\u2022 3 - Optimization\nWe built variously with default, minopt (K1)\nand maxtop (K3)\u2022 3 - History\nSome systems do full clean rebuilds daily. Some\nsystems force a full clean rebuild every Sunday.\nMost do daily incremental builds, and rarely if\never do full rebuilds. These three different histories could leave stale code in some libraries,\nas gmake does not know how to clean up when\nsource code is removed. We have ways of forcing\nglobal rebuilds when we think it is needed. This\nis not usually a problem, but we have to worry\nabout it nevertheless.\n\u2022 3 - Languages\nYou might find code built with with the KAI\nKCC 3, KCC 4 or gcc compilers.\n\u2022 4 - database\nThere was automatically generated code for connecting to text, mSQL, Oracle OCT or Oracle\nOTL. So any calibration database code would\nexist in for different forms.\n\u2022 2 - Library mode\nThe libraries are mostly built for static linking.\nBut when for rapid development, developers often exercise an option to make local shared object libraries.\n\u2022 2 - build style\nThrough early 2001, we had only built libraries\none at a time, using a single processor. Then\nwe transitioned, on the large central systems, to\ndoing parallel rebuilds. This is not trivial, as\npart of the gmake process involved generating\nheaders and code which need to be subsequently\nused by other packages. We think we have this\ncorrectly set up now, but this was less obvious\nin 2001.\nThe net result of these possibilities is that a given\nsingle line of code in the development release could be\nbuilt 2160 different ways.\nIt is clear impossible to exercise and test all of the\npossibilities with our finite resources.\nWe survived because very few of the options were\nexercised by normal users. Most developers ran under\nLinux 2.2 or IRIX, with default optimization, with\nweekly clean rebuilds, KAI 3, using the Oracle OCI\n\nTUJT003\n\ninterface, with static libraries and using the standard distributed libraries built in parallel. So the\nIRIX/Linux choice was the most commmon issue.\nBut remember that the team of people supporting the code do have to be aware of all 2160 variations. There were people exercising all the options\nlisted above, in various combinations. It is essential\nto track down just what a developer is doing before\nstarting to investigate a problem.\n\n2.3. Development 2003\nThe situation was not just a relic of rapid transitions during 2001.\nFewer people work out of the development release,\nnow that Integration releases are available. So these\nchoices are exercised less often by regular developers.\n\u2022 3 - Operating System\nLinux 2.2, Linux 2.4, IRIX\nWe have dropped OSF1 and SunOS, and will\nprobably drop IRIX soon.\n\u2022 3 - Optimization\nDebug (K0), default (K1) and maxopt\n\u2022 5 - Database\ntext, mSQL, Oracle OTL MySQL, ODBC\nIt looks like we have added more options here.\nWe'd like to cut back to text plus one ODBC\ntype API to all the back end databases, reducing\nthis to 2 varieties of code.\n\u2022 3 - History\nclean, weekly clean, stale just as before.\n\u2022 2 - Languages\nKAI KCC 4, gcc\nWe expect to drop KCC, moving to gcc 3.2+\nbefore 2004.\n\u2022 2 - Codegen\nWe are testing a new simplified code generation\nstructure at the moment. Eventually this will\nallow the single database API, making things\nbetter. A this moment, though, it creates one\nmore option to track.\nThere are still a net 810 varieties possible for each\nline of code. Again, most are not exercised. Almost\nall development is now on Linux 2.4, default optimization, Oracle OTL, using the Integration base release,\nwith KCC 4, and the old codegen. The parallel versus\nserial builds are now considered equivalent.\n\n\fComputing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003\n\nTable I Product versions\nProduct\nversion\ncdfdab\nv1 0\ncern\n2000\ncsl offline\nv3 1\ndcap\nv2 24 f0203\ndddebugger\nv3 3 1\ngdb\nv5 0b external\ngsl\nv1 2\ngtools\nv2 4\nherwig\nv6 4a\nisajet\nv7 51a\njava\nv1 2 2b\nkai\nv4 0f a\nlund\nv6 203a\nlevel3 offline\nv2 1\nmsql\nv2 0 10\noracle client\nv8 1 7 lite\npdf\nv8 04\nperl\nv5 005\nperl dbd oracle\nv5 005 817\nroot\nv3 05 00a -q KC 4 0\nstdhep\nv4 09\ntcl\nv8 3 1a\ncdfdb data\nv0 external\ngeant\nv3 21 13 external\nqq\nv9 1b external\n\n2.4. Product versions\nSo far we have just discussed how many ways a single line of code could get built within the CDF offline\nrelease. That release itself depends on many external\nproducts, which we do not build as part of the release.\nWe try to take strong control of these versions, and\nsupport only a single specific version of each for a\ngiven CDF offline code release.\nA current snapshot is given in Table I\nThere are also a small number of products for which\na fixed version is not appropriate, where we need to\nuse the 'current version.\nThese products provide overall infrastructure support (UPS, UPD), or provide services not tied to any\nsingle release (diskcache i, kai key).\nThe development and current product versions are\nupdated by requiring each system to run a nightly\nupdate of a 'development lite' release of the CDF\ncode, containing a few standard maintenance and operations scripts, database access configurations, other\nsmall operations data files.\nTUJT003\n\n3. SAM versions\nHaving this experince in mind, it was natural when\nwe started to integrate the SAM data handling software with the CDF offline code to look at similar issues.\nJust as we did when looking at varieties of code\nin the CDF development releases, we can look at varieties of operating environments for the SAM code.\nWe encountered about 60 configuration files, used by a\nvariety of programs, tailored for each of the following\n:\n\u2022 2 - Experiments\nD0 or CDF\n\u2022 3 - Databases\ndevelopment, integration, or production\n\u2022 5 - Operating systems\nIRIX, Linux, OSF/1, SunOS and generic\n\u2022 - usage\nUser, Station, Stager, CORBA server\nThis makes 120 varieties to consider, clearly excessive.\nAfter some investigation, we found that the D0 experiment does not use these configuration files. CDF\nruns SAM clients only under Linux, and does not want\nthe option of different product versions for any of the\noptions listed above. So in this case, we can reduce the\ncomplexity by simply removing the options entirely.\nI have mentioned this small example mainly because\nthis is what immediately motivated the presentation\nof this talk.\n\n4. Java\nThere is the temptation to look to portable virtual\nenvironments such as Java for a solution to portability\nand revision control problems. So I'll deliver a little\nword of warning that things are not so simple.\nAt this time there are at least five versions of java\nused in the CDF offline environment, each somewhat\nincompatible with the other in function or licensing.\nI'm sure that even more versions are in use in the\nonline system.\n\u2022 v1.1.6 on IRIX\n\u2022 v1.2.2b on Linux\n\u2022 v1.1.7 was used on SunOS and OSF1 before we\nretired them\n\u2022 v1.3 used for dcache data handling software development\n\n3\n\n\f4\n\nComputing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003\n\nTable II CDF Desktop Fermi Linux deployment\nsystems Fermi Linux\n64\n6.1.1\n33\n6.1.2\n121\n7.1.1\n67\n7.3.1a\n\nTable IV CDF offsite\nsystems\n1\n1\n4\n\nkernels\nkernel\n2.2.16-3\n2.2.16-3-ide\n2.2.16-3smp\n\n1 2.2.17-14\n1 2.2.19-6.2.1\n1 2.2.19-6.2.16enterprise\n\nTable III CDF off site Linux systems\nsystems\n1\n6\n2\n7\n11\n5\n3\n\ntype\nRedHat\nFermi\nRedHat\nRedHat\nFermi\nRedHat\nFermi\n\n3 2.4.3-12\n1 2.4.3-12smp\n\nLinux\n6.1\n6.1.1\n6.2\n7.1\n7.1.1\n7.2\n7.3.1\n\n2 2.4.5\n1 2.4.5-4G-rtc\n1 2.4.7-10\n2 2.4.7-10smp\n1 2.4.9-12smp\n1 2.4.9-21smp\n2 2.4.9-34\n3\n2\n3\n1\n1\n\n\u2022 v1.4 used for dcache deployment in production\nat Fermilab\nEven in the very simple offline application,\nwhere we use Java to convert data struction descriptions into C++ calibration database interface code (reading and writing plain text), we\nhave had severe portability problems, particularly on SMP systems. The java application will\ndie with strange and inappropriate diagnostics,\nsomething we've never really been able to find a\ncause for. At the moment we seem to be able to\nwork around this by setting environment variable LD ASSUME KERNEL=2.2.5 ,\n\n5. Operating systems\nIt is tempting when faced with operating system\nrelease issues to demand that everyone run the same\nsystem. But CDF is a worldwide collaboration. Most\nof the systems are not under Fermilab control, and are\nsubject to legitemate local constraints.\nHere is a little snapshot of RedHat releases, and\nkernels in use within CDF on 22 March 2003.\nThe hundreds of centrally managed systems in the\nProduction farms and the Central Analysis Farm\nbatch system all run Fermi Linux 7.3 .\nThe office desktop systems in Table II and offsite\nsystems in Table III are a very mixed bag. Note that\nthe offsite survery only lists systems I can log into, and\ndoes not include any of the large offsite farm systems.\nThere are 21 different Linux kernels in use offsite,\neach on 1 to 4 nodes, as shown in Table IV\nTUJT003\n\n2.4.18\n2.4.18-10smp\n2.4.18-19.7.xsmp\n2.4.18-5bigmem\n2.4.18RUI2S3\n\n2 2.4.19\n\nTherefore, we try very hard, and change the code if\nnecessary, to keep the CDF offline code independent\nof the kernel.\n\n6. root\nEven for a single release of a single product, such as\nroot, there can be many varieties when faced with the\nrequirements of various experiments just at Fermilab.\nWe presently build 15 varieties of each release of root,\nlisted in Table V . We are working hard to reduce\nthis work load, by moving to a common gcc compiler,\nand moving to enable C++ exception handling in the\nCDF code.\nNote that the UPS/UPD tools for selecting and using just the desired version of a product is essential for\nsurvival If a given system can only have one version\nof root in use, shared by all projects, then most users\nwill be unable to use it.\n\n7. File systems\nLikewise, there are many filesystems in use, and one\nhopes that none of the offline physics code depends on\nthese systems. This is not a trivial task.\n\n\fComputing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003\n\nTable V Fermilab\nFlavor\nLinux+2.2\nLinux+2.2\nLinux+2.2\nLinux+2.2\n\nvarieties for root 3.05.03b\nVariety\nGCC 3 1 opt\nGCC 3 1\nKCC 4 0 opt\nKCC 4 0\n\nLinux+2.4\nLinux+2.4\nLinux+2.4\nLinux+2.4\nLinux+2.4\nLinux+2.4\n\nGCC\nGCC\nGCC\nKCC\nKCC\nKCC\n\n3\n3\n3\n4\n4\n4\n\n1\n1\n1\n0\n0\n0\n\nIRIX+6.5\nIRIX+6.5\nIRIX+6.5\nIRIX+6.5\nIRIX+6.5\n\nGCC\nGCC\nGCC\nKCC\nKCC\n\n3\n3\n3\n4\n4\n\n1\n1\n1\n0\n0\n\nexception opt thread\nopt\nexception opt thread\nopt\nexception opt thread\nopt\nexception opt thread\nopt\n\nUnder Linux, one can use ext2, ext3, xfs and reiserfs\ntransparently.\nThere is also limited use of afs, but without Fermilab support. So we try to avoid doing things that\nwould break thisafs usage.\nOn the other hand, file systems like NTFS which\ndo not distinguish upper and lower case characters in\nfile names cannot be used safely, and we will not try\nto 'fix' this in the CDF code.\n\nare in negotiations right now over issues of mutual trust and subsequent use of these certificates for authorization.\n\u2022 Authorization What is the user allowed to do.\nThe classic issues are file access (as in afs), job\nsubmission/login, and file copying. In the Grid\nthis is generalized to a broader range of services,\nhopefully with a single integrated set of authorization tools.\n\u2022 Batch systems The primary initial use of Grid\ntools seems to be to access batch computing facilities. There are a large number of local batch\nsystem, (lsf, pbs, fbsng, etc.),and effort is underway to make this transparent to the end user.\n\u2022 Database access It take more to run a job these\ndays than just handline input, output, and the\nexecuteable environment. Database access to a\ncentral server is ofter requires, raising issues of\nnetwork performance, or cacheing when multiple\ntier servers are involved.\n\u2022 Multi experiments It is quite nontrivial to manage the environment for even one experiment.\nIn the Grid world we are expected to share the\nsame environment. This will require careful attention to detail to see that undesired couplings\ndo not arise.\n\n9. Conclusions\n8. Grid issues\nIt is hard to quantify the complexity of the Grid\nspace, as the tools to be used are just emerging. Some\nof the choices will involve :\n\u2022 Authentication\nWho is the user requesting services ? There are\na variety of methods in use outside the Grid (\nkerberos 4 and 5, ssh, etc.) Various Grid certificate authorities have been commissioned, and\n\nTUJT003\n\nA few simple items of advice seem to have emerged\n\u2022 Count the varieties or configurations, and pay\nattention.\n\u2022 Reduce cross-project couplings\n\u2022 Reduce sensitivity to revisions (file systems, kernels, etc)\n\n5\n\n\f"}