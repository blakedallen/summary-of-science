{"id": "http://arxiv.org/abs/1101.3755v2", "guidislink": true, "updated": "2012-06-19T14:27:35Z", "updated_parsed": [2012, 6, 19, 14, 27, 35, 1, 171, 0], "published": "2011-01-19T19:05:15Z", "published_parsed": [2011, 1, 19, 19, 5, 15, 2, 19, 0], "title": "Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev\n  Inequality", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1101.5008%2C1101.3755%2C1101.3697%2C1101.3701%2C1101.1009%2C1101.2016%2C1101.4599%2C1101.4335%2C1101.3411%2C1101.1488%2C1101.1399%2C1101.6000%2C1101.3133%2C1101.5867%2C1101.5193%2C1101.4132%2C1101.1204%2C1101.5585%2C1101.3709%2C1101.1437%2C1101.3432%2C1101.0564%2C1101.2189%2C1101.5790%2C1101.0175%2C1101.1696%2C1101.5575%2C1101.0488%2C1101.4244%2C1101.1854%2C1101.4001%2C1101.5844%2C1101.5383%2C1101.3967%2C1101.3519%2C1101.3681%2C1101.4186%2C1101.1736%2C1101.1640%2C1101.2361%2C1101.2493%2C1101.5697%2C1101.6067%2C1101.5347%2C1101.4021%2C1101.1592%2C1101.2823%2C1101.5469%2C1101.0229%2C1101.5254%2C1101.5810%2C1101.1962%2C1101.0722%2C1101.2727%2C1101.1225%2C1101.3818%2C1101.1129%2C1101.4553%2C1101.5854%2C1101.2780%2C1101.2881%2C1101.0965%2C1101.4791%2C1101.4234%2C1101.0326%2C1101.2366%2C1101.4740%2C1101.1248%2C1101.5057%2C1101.0500%2C1101.3115%2C1101.4200%2C1101.0534%2C1101.4942%2C1101.2546%2C1101.0877%2C1101.1149%2C1101.1914%2C1101.1833%2C1101.4546%2C1101.3557%2C1101.0903%2C1101.2934%2C1101.1580%2C1101.4193%2C1101.2471%2C1101.0710%2C1101.0121%2C1101.0078%2C1101.0207%2C1101.5022%2C1101.3695%2C1101.4899%2C1101.4897%2C1101.0174%2C1101.4972%2C1101.0493%2C1101.5450%2C1101.0813%2C1101.1999%2C1101.2990&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev\n  Inequality"}, "summary": "Approximating adequate number of clusters in multidimensional data is an open\narea of research, given a level of compromise made on the quality of acceptable\nresults. The manuscript addresses the issue by formulating a transductive\ninductive learning algorithm which uses multivariate Chebyshev inequality.\nConsidering clustering problem in imaging, theoretical proofs for a particular\nlevel of compromise are derived to show the convergence of the reconstruction\nerror to a finite value with increasing (a) number of unseen examples and (b)\nthe number of clusters, respectively. Upper bounds for these error rates are\nalso proved. Non-parametric estimates of these error from a random sample of\nsequences empirically point to a stable number of clusters. Lastly, the\ngeneralization of algorithm can be applied to multidimensional data sets from\ndifferent fields.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1101.5008%2C1101.3755%2C1101.3697%2C1101.3701%2C1101.1009%2C1101.2016%2C1101.4599%2C1101.4335%2C1101.3411%2C1101.1488%2C1101.1399%2C1101.6000%2C1101.3133%2C1101.5867%2C1101.5193%2C1101.4132%2C1101.1204%2C1101.5585%2C1101.3709%2C1101.1437%2C1101.3432%2C1101.0564%2C1101.2189%2C1101.5790%2C1101.0175%2C1101.1696%2C1101.5575%2C1101.0488%2C1101.4244%2C1101.1854%2C1101.4001%2C1101.5844%2C1101.5383%2C1101.3967%2C1101.3519%2C1101.3681%2C1101.4186%2C1101.1736%2C1101.1640%2C1101.2361%2C1101.2493%2C1101.5697%2C1101.6067%2C1101.5347%2C1101.4021%2C1101.1592%2C1101.2823%2C1101.5469%2C1101.0229%2C1101.5254%2C1101.5810%2C1101.1962%2C1101.0722%2C1101.2727%2C1101.1225%2C1101.3818%2C1101.1129%2C1101.4553%2C1101.5854%2C1101.2780%2C1101.2881%2C1101.0965%2C1101.4791%2C1101.4234%2C1101.0326%2C1101.2366%2C1101.4740%2C1101.1248%2C1101.5057%2C1101.0500%2C1101.3115%2C1101.4200%2C1101.0534%2C1101.4942%2C1101.2546%2C1101.0877%2C1101.1149%2C1101.1914%2C1101.1833%2C1101.4546%2C1101.3557%2C1101.0903%2C1101.2934%2C1101.1580%2C1101.4193%2C1101.2471%2C1101.0710%2C1101.0121%2C1101.0078%2C1101.0207%2C1101.5022%2C1101.3695%2C1101.4899%2C1101.4897%2C1101.0174%2C1101.4972%2C1101.0493%2C1101.5450%2C1101.0813%2C1101.1999%2C1101.2990&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Approximating adequate number of clusters in multidimensional data is an open\narea of research, given a level of compromise made on the quality of acceptable\nresults. The manuscript addresses the issue by formulating a transductive\ninductive learning algorithm which uses multivariate Chebyshev inequality.\nConsidering clustering problem in imaging, theoretical proofs for a particular\nlevel of compromise are derived to show the convergence of the reconstruction\nerror to a finite value with increasing (a) number of unseen examples and (b)\nthe number of clusters, respectively. Upper bounds for these error rates are\nalso proved. Non-parametric estimates of these error from a random sample of\nsequences empirically point to a stable number of clusters. Lastly, the\ngeneralization of algorithm can be applied to multidimensional data sets from\ndifferent fields."}, "authors": ["Shriprakash Sinha"], "author_detail": {"name": "Shriprakash Sinha"}, "author": "Shriprakash Sinha", "arxiv_comment": "16 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/1101.3755v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1101.3755v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1101.3755v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1101.3755v2", "journal_reference": null, "doi": null, "fulltext": "Transductive-Inductive Cluster Approximation\nVia Multivariate Chebyshev Inequality\nShriprakash Sinha\n\narXiv:1101.3755v2 [cs.CV] 19 Jun 2012\n\nTu Delft, Dept. of Mediamatics, Faculty of EEMCS, Mekelweg 4,\n2628 CD Delft, The Netherlands\n{Shriprakash.Sinha@gmail.com}\n\nAbstract. Approximating adequate number of clusters in multidimensional data is an open area of research, given a level of compromise made\non the quality of acceptable results. The manuscript addresses the issue by formulating a transductive inductive learning algorithm which\nuses multivariate Chebyshev inequality. Considering clustering problem\nin imaging, theoretical proofs for a particular level of compromise are\nderived to show the convergence of the reconstruction error to a finite\nvalue with increasing (a) number of unseen examples and (b) the number of clusters, respectively. Upper bounds for these error rates are also\nproved. Non-parametric estimates of these error from a random sample\nof sequences empirically point to a stable number of clusters. Lastly, the\ngeneralization of algorithm can be applied to multidimensional data sets\nfrom different fields.\nKeywords: Transductive Inductive Learning, Multivariate Chebyshev\nInequality\n\n1\n\nIntroduction\n\nThe estimation of clusters has been approached either via a batch framework\nwhere the entire data set is presented and different initializations of seed points\nor prototypes tested to find a model of cluster that fits the data like in k-means\n[9] and fuzzy C-means [2] or an online strategy clusters are approximated as\nnew examples of data are presented one at a time using variational Dirichlet\nprocesses [7] and incremental clustering based on randomized algorithms [3].\nIt is widely known that approximation of adequate number of clusters using a\nmultidimensional data set is a open problem and a variety of solutions have been\nproposed using Monte Carlo studies [5], Bayesian-Kullback learning scheme in\nmean squared error setting or gaussian mixture [19], model based approaches [6]\nand information theory [17], to cite a few.\nThis work deviates from the general strategy of defining the number of clusters apriori. It defines a level of compromise, tolerance or confidence in the\nquality of clustering which gives an upper bound on the number of clusters generated. Note that this is not at all similar to defining the number of clusters. It\nonly indicates the level of confidence in the result and the requirement still is to\n\n\f2\n\nSinha\n\nestimate the adequate number of clusters, which may be way below the bound.\nThe current work focuses on dealing with the issue of approximating the number\nof clusters in an online paradigm when the confidence level has been specified.\nIn certain aspects it finds similarity with the recent work on conformal learning\ntheory [18] and presents a novel way of finding the approximation of cluster with\na degree of confidence.\nConformal learning theory [15], which has its foundations in employing a\ntransductive-inductive paradigm deals with the idea of estimating the quality of\npredictions made on the unlabeled example based on the already processed data.\nMathematically, given a set of already processed examples (x1 , y1 ), (x2 , y2 ), ...,\n(xi\u22121 , yi\u22121 ), the conformal predictors give a point prediction \u0177 for the unseen\nexample xi with a confidence level of \u0393 \u03b5 . Thus it estimates the confidence in\nthe quality of prediction using the original label yi after the prediction has been\nmade and before moving on to the next unlabeled example. These predictions are\nmade on the basis of a non-conformity measure which checks how much the new\nexample is different from a bag of already seen examples. A bag is considered\nto be a finite sequence Z (z1 , z2 , ..., zi\u22121 ) of examples, where zi = (xi , yi ). Then\nusing the idea of exchangeability, it is known from [18], under a weaker assumption that for every positive integer i, every permutation \u03c0 of {1, 2, ..., i}, and\nevery measurable set E \u2282 Z i , the probability distribution P {(z1 , z2 , ...) \u2208 Z \u221e\n: (z1 , z2 , ..., zi ) \u2208 E} = P {(z1 , z2 , ...) \u2208 Z \u221e : (z\u03c0(1) , z\u03c0(2) , ..., z\u03c0(i) ) \u2208 E}. A prediction for the new example xi is made if and only if the frequency (p-value) of\nexchanging the new example with another example in the bag is above certain\nvalue.\nThis manuscript finds its motivation from the foregoing theory of online\nprediction using transductive-inductive paradigm. The research work applies the\nconcept of coupling the creation of new clusters via transduction and aggregation\nof examples into these clusters via induction. It finds its similarity with [15] in\nutilizing the idea of prediction region defined by a certain level of confidence. It\npresents a simple algorithm that differs significantly from conformal learning in\nthe following aspect: (1) Instead of working with sequences of data that contain\nlabels, it works on unlabeled sequences. (2) Due the first formulation, it becomes\nimperative to estimate the number of clusters which is not known apriori and\nthe proposed algorithm comes to rescue by employing a Chebyshev inequality.\nThe inequality helps in providing an upper bound on the number of clusters\nthat could be generated on a random sample of sequence. (3) The quality of\nthe prediction in conformal learning is checked based on the p-values generated\nonline. The current algorithms relaxes this restriction in checking the quality\nonline and just estimates the clusters as the data is presented. (4) The foregoing\nstep makes the algorithm a weak learner as it is sequence dependent. To take\nstock of the problem, a global solution to the adequate number of cluster is\napproximated by estimating kernel density estimates on a sample of random\nsequences of a data. Finally, the level of compromise captured by a parameter in\nthe inequality gives an upper bound on the number of clusters generated. In case\nof clustering in static images, for a particular parameter value, theoretical proofs\n\n\fTI Cluster Approximation Via Multivariate Chebyshev Inequality\n\n3\n\nshow that the reconstruction error converges to a finite value with increasing (a)\nnumber of unseen examples and (b) the number of clusters. Empirical kernel\ndensity estimates of reconstruction error over a random sample of sequences\non toy examples indicate the number of clusters that have high probability of\nlow reconstruction error. It is not necessary that labeled data are always present\nto compute the reconstruction error. In that case the proposed algorithm stops\nshort at density estimation of approximated number of clusters from a random\nsequence of examples, with a certain degree of confidence.\nAnother dimension of the proposed work is to use the generalization of multivariate formulation of Chebyshev inequality [1], [10], [11]. It is known that\nChebyshev inequality helps in proving the convergence of random sequences of\ndifferent data. Also the multivariate formulation of the Chebyshev inequality facilitates in providing bounds for multidimensional data which is often afflicted by\nthe curse of dimensionality making it difficult to compute multivariate probabilities. One of the generalizations that exist for multivariate Chebyshev inequality\nis the consideration of probability content of a multivariate normal random vector to lie in an Euclidean n-dimensional ball [14], [13]. This work employs a more\nconservative approach is the employment of the Euclidian n-dimensional ellipsoid which restricts the spread of the probability content [4]. Work by [9] and\n[16] provide motivation in employment of multivariate Chebyshev inequality.\nEfficient implementation and analysis of k-means clustering using the multivariate Chebyshev inequality has been shown in [9]. The current work differs\nfrom k-means in (1) providing an online setting to the problem of clustering (2)\nestimating the number of clusters for a particular sequence representation of the\nsame data via convergence through ellipsoidal multivariate Chebyshev inequality, given the level of confidence, compromise or tolerance in the quality of results\n(3) generating global approximations of number of clusters from non-parametric\nestimates of reconstruction error rates for sample of random sequences representing the same data and (4) not fixing the cluster number apriori. It must be noted\nthat in k-means, the solutions may be different for different initializations for a\nparticular value of k but the value of k as such remains fixed. In the proposed\nwork, with a high probability, an estimate is made regarding the minimum number of clusters that can represent the data with low reconstruction error. This\noutlook broadens the perspective of finding multiple solutions which are upper\nbounded as well as approximating a particular number of cluster which have similar solutions. This similarity in solutions for a particular number of cluster is\nattributed to the constraint imposed by the Chebyshev inequality. A key point\nto be noted is that using increasing levels of compromise or confidence as in\nconformal learners, the proposed work generates a nested set of solutions. Low\nconfidence or compromise levels generate tight solutions and vice versa. Thus\nthe proposed weak learner provides a set of solutions which are robust over a\nsample.\nThis manuscript also extends the work of [16] on employment of multivariate\nChebyshev inequality for image representation. Work in [16] presents a hybrid\nmodel based on Hilbert space filling curve [8] to traverse through the image.\n\n\f4\n\nSinha\n\nSince this curve preserve the local information in the neighbourhood of a pixel,\nit reduces the burden of good image representation in lower dimensions. On the\nother side, it acts as a constraint on processing the image in a particular fashion.\nThe current work removes this restriction of processing images via the space\nfilling curves by considering any pixel sequence that represents the image under\nconsideration. Again, a single sequence may not be adequate enough for the\nlearner to synthesize the image to a recongnizable level. This can be attributed\nto the fact that in an unsupervised paradigm the number of clusters are not\nknown apriori and also the learner would be sequence dependent. To reiterate,\nthe proposed work addresses the issues of \u2022 recognizability, by defining a level of\ncompromise that a user is willing to make via the Chebyshev parameter Cp and\n\u2022 sequence specific solution, by taking random samples of pixel sequences from\nthe same image. The latter helps in estimating a population dependent solution\nwhich would be robust and stable synthesis. Regularization of these error over\napproximated number of clusters for different levels of compromise leads to an\nadequate number of clusters that synthesize the image with minimal deviation\nfrom the original image.\nThus the current work provides a new perspective in approximation of cluster number at a particular confidence level. To test the propositions made, the\nproblem of clustering in images is taken into account. Generalizations of the algorithm can be made and applied to different fields involving multidimensional\ndata sets in an online setting. Let I be an RGB image. A pixel in I is an example xi with N dimensions (here N = 3). It is assumed that examples appear\nrandomly without repetition for the proposed unsupervised learner. Note that\nwhen the sample space (here the image I) has finite number of examples in it\n(here M pixels), then the total number of unique sequences is M!. When M\nis large, M! \u2192 \u221e. Currently, the algorithm works on a subset of unique sequences sampled from M! sequences. The probability of a sequence to occur is\nequally likely (in this case 1/M). RGB images from the Berkeley Segmentation\nBenchmark (BSB) [12] have been taken into consideration for the current study.\n\n2\n\nTransductive-Inductive Learning Algorithm\n\nGiven that the examples (zi = xi ) in a sequence appear randomly, the challenge\nis to (1) learn the association of a particular example to existing clusters or (2)\ncreate a new cluster, based on the information provided by already processed\nexamples. The current algorithm handles the two issues via (1) evaluation of a\nnonconformity measure defined by multivariate Chebyshev's inequality formulation and (2) 1-Nearest Neighbour (NN) transductive learning, respectively. The\nmultivariate formulation of the generalized Chebyshev inequality [4] is applied\nto a new example using a single Chebyshev parameter. This inequality tests the\ndeviation of the new example from the mean of a cluster of examples and gives a\nlower probabilistic bound on whether the example belongs to the cluster under\ninvestigation. If the new random example passes the test, then it is associated\nwith the cluster and the mean and covariance matrix for the cluster is recom-\n\n\fTI Cluster Approximation Via Multivariate Chebyshev Inequality\n\n5\n\nAlgorithm 1 Unsupervised Learner\n1: procedure Unsupervised Learner(img, Cp )\n2:\n[nrows, ncols] \u2190 size(img)\n3:\nM \u2190 nrows \u00d7 ncols\n. Total no. of unseen examples\n4:\nptcntr \u2190 0\n. Number of examples encountered\n5:\nptidx \u2190 {1, 2, ..., M}\n. Total no. of indicies of unseen examples\nInitialize Variables\n6:\nclustercntr \u2190 0\n. Number of clusters\n7:\nCumErrval \u2190 0\n. Cummulative value\n8:\nErr1 \u2190 []\n. Error rate as no. of examples increase\n9:\nErr2 \u2190 []\n. Error rate as no. of clusters increase\n10:\nwhile Card(ptidx ) examples remain unprocessed do\n. ptidx \u2282 {1, 2, ..., M}\n11:\nChoose a random example xi s.t. i \u2208 ptidx\n12:\nptcntr \u2190 ptcntr + 1\n13:\nUpdate ptidx i.e ptidx \u2190 ptidx \u2212 {i}\n14:\nCRIT ERION \u2190 []\n15:\nErrval \u2190 0\n16:\n\u2200q clusters were\nPq \u2208 {0, 1, 2, ..., clustercntr }\n. x means all examples in cluster q\n17:\nErrval \u2190 `k=1 (xk \u2212 Eq (x))2\n18:\nCumErrval \u2190 CumErrval + Errval\n19:\nCompute D \u2190 (xi \u2212 Eq (x))T \u03a3q\u22121 (xi \u2212 Eq (x))\n20:\nIf Dq < Cp\n. Cp is Chebyshev parameter\n21:\nCRIT ERION \u2190 [CRIT ERION ; Dq , q]\n22:\nIf more than one cluster that associates to xi , i.e length(CRIT ERION ) \u2265 1\n23:\nAssociate xi to selected cluster q with minimum Dq\n24:\nErr1 \u2190 [Err1 , Errval /ptcntr ]\n25:\nIf xi is not associated with any cluster, i.e sum(F OU N D) == 0\n26:\nErr2 \u2190 [Err2 , Errval /ptcntr ]\n27:\nclustercntr \u2190 clustercntr + 1\n28:\nUsing 1-NN find xj closest to xi s.t. j \u2208 ptidx\n29:\nUpdate ptidx i.e ptidx \u2190 ptidx \u2212 {j}\n30:\nForm a new cluster {xi , xj }\n31:\nend while\n32: end procedure\n\nputed. In case there exists more than one cluster which qualify for association,\nthen the cluster with lowest deviation to the new example is picked up for association. It is also possible to assign the new example to a random chosen cluster\nfrom the selected clusters to induce noise and then check for the approximations\non the number of cluster. This has not been considered in the current work for\nthe time being. In case of failure to find any association, the algorithm employs\n1-NN transductive algorithm to find a closest neighbour of the current example\nunder processing. This neighbour together with the current example forms a new\ncluster.\nSeveral important implications arise due to the usage of a probabilistic inequality measure as a nonconformal measure. These will be elucidated in detail\nin the later sections. An important point to consider here is the usage of 1-NN\n\n\f6\n\nSinha\n\nalgorithm to create a new cluster. Even though it is known that 1-NN suffers\nfrom the problem of the curse of dimensionality, for problems with small dimensions, it can be employed for transductive learning. The aim of the proposed\nwork is not to address the curse of dimensionality issue. Also, note that in the\ngeneral supervised conformal learning algorithm, a prediction has to be made\nbefore the next random example is processed. This is not the case in the current unsupervised framework of the conformal learning algorithm. In case the\ncurrent random example fails to associate with any of the existing clusters, under the constraint yielded by the Chebyshev parameter, the NN helps in finding\nthe closest example (in feature space) from the remaining unprocessed sample\ndata set, to form a new cluster. Thus the formation of a new cluster depends on\nthe strictness of the Chebyshev parameter Cp . The procedure for unsupervised\nconformal learning is presented in algorithm 1. It does not strictly follow the\nidea of finding confidence on the prediction as labels are not present to be tested\nagainst. The goal here is to reconstruct the clusters from a single pixel sequence\nsuch that they represent the image. The quality of the reconstruction is taken\nup later on when a random sample of pixel sequences are used to estimate the\nprobability density of the reconstruction error rates. Note that in the algorithm,\nEq (x) represents the mean of the examples x in the q th cluster and \u03a3p is the\ncovariance matrix of N D feature examples of the q th cluster.\n\n3\n\nTheoretical Perspective\n\nApplication of the multivariate Chebyshev inequality that yields a probabilistic\nbound enforces certain important implications with regard to the clusters that\nare generated. For the purpose of elucidation of the algorithm, the starfish image\nis taken from [12].\n3.1\n\nMultivariate Chebyshev Inequality\n\nLet X be a stochastic variable in N dimensions with a mean E[X]. Further,\n\u03a3 be the covariance matrix of all observations, each containing N features and\nCp \u2208 R, then the multivariate Chebyshev Inequality in [4] states that:\nP{(X \u2212 E[X])T \u03a3 \u22121 (X \u2212 E[X]) \u2265 Cp } \u2264\n\nN\nCp\n\nP{(X \u2212 E[X])T \u03a3 \u22121 (X \u2212 E[X]) < Cp } \u2265 1 \u2212\n\nN\nCp\n(1)\n\ni.e. the probability of the spread of the value of X around the sample mean\nE[X] being greater than Cp , is less than N /Cp . There is a minor variation for\nthe univariate case stating that the probability of the spread of the value of\nx around the mean \u03bc being greater than Cp \u03c3 is less than 1/Cp2 . Apart from\nthe minor difference, both formulations convey the same message about the\nprobabilistic bound imposed when a random vector or number X lies outside\nthe mean of the sample by a value of Cp .\n\n\fTI Cluster Approximation Via Multivariate Chebyshev Inequality\n\n7\n\nFig. 1. A random sequence of Starfish Image segmented via unsupervised conformal\nlearning algorithm. (Cp , NoClust, TRErr) represent the tuple containing the Chebyshev paramenter (Cp ), number of clusters generated (NoClust) while using Cp and\nthe total reconstruction error of the generated image from the original image (TRErr)(a) (3, 1034, 17.746), (b) (5, 271, 36.32), (c) (7, 159, 54.71), (d) (9, 45, 40.591), (e)\n(11, 31, 62.606), (f) (13, 29, 66.061), (g) (15, 33, 65.424), (h) (17, 24, 64.98).\n\n3.2\n\nAssociation to Clusters\n\nOnce a cluster is initialized (say with xi and xj ), the size of the cluster depends\non the number of examples getting associated with it. The multivariate formalism\n\n\f8\n\nSinha\n\nof the Chebyshev inequality controls the degree of uniformity of feature values\nof examples that constitute the cluster. The association of the example to a\ncluster happens as follows: Let the new random example (say xt ) be considered\nfor checking the association to a cluster. If the spread of example xt from Eq (x)\n(the mean of the q th cluster {xi , xj }), factored by the covariance matrix \u03a3q ,\nis below Cp , then xt is considered as a part of the cluster. Using Chebyshev\ninequality, it boils down to:\nP{(xt \u2212 Eq [xi , xj ])T \u03a3q\u22121 (xt \u2212 Eq [xi , xj ]) \u2265 Cp } \u2264\n\nN\nCp\n\nP{(xt \u2212 Eq [xi , xj ])T \u03a3q\u22121 (xt \u2212 Eq [xi , xj ]) < Cp } \u2265 1 \u2212\n\nN\nCp\n(2)\n\nSatisfaction of this criterion suggests a possible cluster to which xt could be\nassociated. This test is conducted for all the existing clusters. If there are more\nthan one cluster to which xt can be associated, then the cluster which shows\nthe minimum deviation from the new random point is chosen. Once the cluster\nis chosen, its size is extended to by one more example i.e. xt . The cluster now\nconstitutes {xi , xj , xt }. If no association is found at all, a new cluster is initialized\nand the process repeats until all unseen examples have been processed. The\nsatisfaction of the inequality gives a lower probabilistic bound on size of cluster\nby a value of 1\u2212(N /Cp ), if the second version of the Chebyshev formula is under\nconsideration. Thus the size of the clusters grow under a probabilistic constraint\nin a homogeneous manner. For a highly inhomogeneous image, a cluster size\nmay be very restricted or small due to big deviation of pixel intensities from the\ncluster it is being tested with.\nOnce the pixels have been assigned to respective decompositions, all pixels in\na single decomposition are assigned the average value of intensities of pixels that\nconstitute the decomposition. Thus is done under the assumption that decomposed clusters will be homogeneous in nature with the degree of homogeneity\ncontrolled by Cp . Figure 1 shows the results of clustering for varying values of\nCp for the starfish image from [12].\n3.3\n\nImplications\n\nIn [?] various implications have been proposed for using multivariate Chebyshev\ninequality for image representation using space filling curve. In order to extend\non the their work, a few implications are reiterated for further development.\nThe inequality being a criterion, the probability associated with the same gives\na belief based bound on the satisfaction of the criterion. In order to proceed,\nfirst a definition of Decomposition is needed.\nDefinition 1. Let D be a decomposition which contains a set of points x with\na mean of Eq (x). The set expands by testing a new point xt via the Chebyshev\ninequality P{(xt \u2212 Eq (x))T \u03a3q\u22121 (xt \u2212 Eq (x)) < Cp } \u2265 1 \u2212 CNp .\n\n\fTI Cluster Approximation Via Multivariate Chebyshev Inequality\n\n9\n\nThe decomposition may include the point xt depending on the outcome of the\ncriterion. A point to be noted is that, if the new point xt belongs to D, then D\ncan be represented as (xt \u2212 Eq (x))T \u03a3q\u22121 (xt \u2212 Eq (x)).\nLemma 1. Decompositions D are bounded by lower probability bound of 1 \u2212\n(N /Cp ).\nLemma 2. The value of Cp reduces the size of the sample from M to an upper\nbound of M/Cp probabilistically with a lower bound of 1 \u2212 (N /Cp ). Here M is\nthe number of examples in the image.\nLemma 3. As Cp \u2192 N the lower probability bound drops to zero, implying large\nnumber of small decompositions D can be achieved. Vice versa for Cp \u2192 \u221e.\nIt was stated that the image can be reconstructed from pixel sequences at a\ncertain level of compromise. From lemma 2, it can be seen that Cp reduces the\nsample size while inducing a certain amount of error due to loss of information\nvia averaging. This reduction in sample size indicates the level of compromise\nat which the image is to processed. This reduction in sample size or level of\ncompromise is directly related to the construction of probabilistically bounded\ndecompositions also. Since the decompositions are generated via the usage of\nCp in equation 1, the belief of their existence in terms of a lower probability\nbound (from lemma 1) suggests a confidence in the amount of error incurred\nin reconstruction of the image. For a particular pixel, this reconstruction error\ncan be computed by squaring the difference between the value of the intensity\nin the original image and the intensity value assigned after clustering. Since\na somewhat homogeneous decomposition is bounded probabilistically, the reconstruction error of pixels that constitute it are also bounded probabilistically.\nThus for all decompositions, the summation of reconstruction errors for all pixels\nis bounded. The bound indicates the confidence in the generated reconstruction\nerror. Also, by lemma 2, since the number of decompositions or clusters is upper\nbounded, the total reconstruction error is also upper bounded. It now remains\nto be proven that for a particular level of compromise, the error rates converge\nas the number of processed examples and the number of clusters increase.\nIn algorithm 1, three error rates are computed as the random sequence of examples get processed. For each original pixel xi \u2208 RN in the image, let xR\ni be the\nintensity value assigned after clustering. Then the reconstruction error for pixel\nxi is norm-2 ||xi \u2212 xR\ni ||2 . Since a pixel is assigned to a particular decomposition\nDq , it gets a value of the mean of the all pixels that constitute the decomposition\nDq . Thus the reconstruction error for a pixel turns out\nbe ||xi \u2212 Eq (x)||2 . For\nPto\nn\neach cluster q, the reconstruction error is ErrDq = i=1 ||xi \u2212 Eq (x)||2 . Note\nthat the error also indicates how much the examples deviate from the mean of\ntheir respective cluster. As new examples are processed based on the information\npresent from the previous examples, the total error computedP\nat after processing\ncluster\nthe first ptcntr examples in a random sequence is Errval = q=1 cntr ErrDq .\nThe error rate for these ptcntr examples is Err1 = Errval /ptcntr . Finally, an\nerror rate is computed that captures how the deviation of the examples from\n\n\f10\n\nSinha\n\nFig. 2. Error rate Err1 for a particular sequence with increasing number of examples\nwith Cp = 7.\n\ntheir respective cluster means happen, after the formation of a new cluster. This\nerror is denoted by Err2 . The formula for Err2 is the same as Err1 but with\na minute change in conception. The Errval are divided by the total number of\npoint processed after the formation of every new cluster.\nTheorem 1. Let Zi be a random sequence that represents the entire image I.\nIf Zi is decomposed into clusters via the Chebyshev Inequality using the unsupervised learner, then the reconstruction error rate Err1 converges asymptotically\nwith a probabilistically lower bound or confidence level of 1 \u2212 N /Cp or greater.\nProof. It is known that the total\nreconstruction error after ptcntr examples have\nPcluster\nbeen processed, is Errval = q=1 cntr ErrDq . And the error rate is Err1 =\nErrval /ptcntr . It is also known from equation 1 that an example is associated to\na particular decomposition Dq if it satisfies the constraint (xt \u2212 Eq (x))T \u03a3q\u22121\n(xt \u2212Eq (x)) < Cp . Since Cp defines level of compromise on the image via lemma 2\nand the decompositions Dq is almost homogeneous, all examples that constitute\na decomposition have similar attribute values. Due to this similarity between\nthe attribute values, the non-diagonal elements of the covariance matrix in the\ninequality above approach to zero. Thus, \u03a3q\u22121 \u2248 det|\u03a3q\u22121 |I, were I is the identity\nmatrix. The inequality then equates to:\n(xt \u2212 Eq (x))T det|\u03a3q\u22121 |I(xt \u2212 Eq (x)) / Cp\ndet|\u03a3q\u22121 |(xt \u2212 Eq (x))T I(xt \u2212 Eq (x)) / Cp\n(xt \u2212 Eq (x))T I(xt \u2212 Eq (x)) /\n\nCp\ndet|\u03a3q\u22121 |\n\n\fTI Cluster Approximation Via Multivariate Chebyshev Inequality\n\n||xt \u2212 Eq (x))||2 /\n\nCp\ndet|\u03a3q\u22121 |\n\n11\n\n(3)\n\nThus, if xi = xt was the last example to be associated to a decomposition, the\nreconstruction error ||xi \u2212 Eq (x)|| for that example would be upper bounded be\nCp\n. Consequently, the total error after processing ptcntr examples is also\ndet|\u03a3q\u22121 |\nupper bounded, i.e.\nErrval =\n\ncluster\nXcntr\n\nErrDq\n\nq=1\n\n=\n\n/\n\n/\n\ncluster\nXcntr\n\nn\nX\n\nq=1\n\ni=1\n\ncluster\nXcntr\n\nn\nX\n\nq=1\n\ni=1\n\ncluster\nXcntr\n\nn\nX\n\nq=1\n\ni=1\n\n||xi \u2212 Eq (x)||2\nCp\ndet|\u03a3q\u22121 |\nCp\ndet|\u03a3q\u22121 |\n\n(4)\n\nThus the error rate Err1 = Errval /ptcntr is also upper bounded. Different decompositions may have different \u03a3q\u22121 , but in the worst case scenario, if the\ndecomposition with the lowest covariance is substituted for every other decompoC\nclustcntr n\nsitions, then the upper bound on the error is det|\u03a3 \u22121 p |\u00d7pt\n\u03a3q=1\n\u03a3i=1 (1)\nwhich equates to\n\nCp\n.\n\u22121\ndet|\u03a3lowest\n|\n\nlowest\n\ncntr\n\nt\nu\n\nIt is important to note that this error rate converges to a finite value asymptotically as the number of processed examples increases. This is because initially\nwhen the learner has not seen enough examples to learn and solidify the knowledge in terms of a stable mean and variance of decompositions, the error rate\nErr1 increases as new examples are presented. This is attributed to the fact\nthat new clusters are formed more often in the intial stages, due to lack of prior\nknowledge. After a certain time, when large number of examples have been encountered to help solidify the knowledge or stabilize the decompositions, then\naddition of further examples does not increment the error. This stability of clusters is checked via the multivariate formulation of the Chebyshev inequality in\nequation 2. The stability also casues the error rate Err1 to stabilize and thus indicate its convergence in a bounded manner with a probabilistic confidence level.\nThus for any value of ptcntr , there exists an upper bound on reconstruction error,\nwhich stabilizes as ptcntr increases.\nFor Cp = 7, the image (c) in figure 1 shows the clustered image that is generated using the unsupervised conformal learning algorithm. Pixels in a cluster of\nthe generated image have the mean of the cluster as their intensity value or the\nlabel. This holds for all the clusters in the generated image. The total number\nof clusters generated for a particular random sequence was 159. The error rate\nErr1 is depicted in figure 2.\n\n\f12\n\nSinha\n\nFig. 3. Error rate Err2 for a particular sequence with increasing number of clusters\nand Cp = 7.\n\nTheorem 2. Let Zi be a random sequence that represents the entire image I.\nIf Zi is decomposed into clusters via the Chebyshev Inequality using the unsupervised learner, then the reconstruction error rate Err2 converges asymptotically\nwith a probabilistically lower bound or confidence level of 1 \u2212 N /Cp or greater.\nProof. The error rate Err2 is the computation of error after each new cluster is\nformed. The upper bound on Err2 as the number of clusters or decompositions\nincrease follows a proof similar to one presented in theorem 1..\nt\nu\nAgain for the same Cp = 7, the image (c) in figure 1, the error rate Err2 is\ndepicted in figure 3. Intuitively, it can be seen that both the reconstruction error\nrates converge to an approximately similar value.\nThe theoretical proofs and the lemmas suggest that, for a given level of\ncompromise Cp there exists an upper bound on the reconstruction error as well as\nthe number of clusters. But this reconstruction error and the number of clusters\nis dependent on a pixel sequence presented to the learner. Does this mean that\nfor a particular level of compromise one may find values of reconstruction error\nand number of clusters that may never converge to a finite value, when a random\nsample of pixel sequences that represent an image are processed by the learner?\nOr in a more simplified way, is it possible to find a reconstruction error and the\nnumber of clusters at a particular level of compromise that best represents the\nimage? This points to the problem of whether an image can be reconstructed at\na particular level of compromise where there is a high probability of finding a\nlow reconstruction error and the number of clusters, from a sample of sequences.\n\n\fTI Cluster Approximation Via Multivariate Chebyshev Inequality\n\n13\n\nFig. 4. The probability density estimates for (a) Err1 (b) Err2 and (c) the number of\nclusters obtained via the unsupervised conformal learner, generated over 1000 random\nsequences representing the same image with Cp = 10.\n\nThe existence of such a probability value would require the knowledge of the\nprobability distribution of the reconstruction error over increasing (1) number\nof examples and (2) number of clusters generated. In this work, kernel density\nestimation (KDE) is used to estimate the probability distribution of the reconstruction error Err1 and Err2 . To investigate into the quality of the solution\nobtained, the error rates were generated for different random sequences and a\nKDE was evaluated on the observations. The density estimate empirically point\nto the least error rates with high probability. It was found that the error rates\nErr1 , Err2 and the number of clusters, all converge to a particular value, for a\ngiven image.\n\n\f14\n\nSinha\n\nFig. 5. Behaviour of reconstruction error (via KDE) and number of cluster or decompositions (via KDE) based on increasing values of Cp .\n\nFor Cp = 10, the probability density estimates were generated using the density estimates on error rates and the number of clusters obtained on 1000 random\nsequences of the same image. It was found that the error rates Err1 , Err2 and\nthe number of clusters converge to 33.1762, 35.9339 and 38, respectively. Figure\n4 shows the graphs for the same. It can be seen from graphs (a) and (b) in figure\n4, that both Err1 and Err2 converge nearly to the similar values.\nIt can been noted that with increasing value of the parameter Cp , the bound\non the decomposition expands which further leads to generation of lower number\nof clusters required to reconstruct the image. Thus it can be expected that at\nlower levels of compromise, the reconstruction error (via KDE) is low but the\nnumber of clusters (via KDE) is very high and vice versa. Figure 5 shows the\nbehaviour of these reconstruction error and number of clusters generated as\nthe level of compromise increases. High reconstruction error does not necessarily\nmean that the representation of the image is bad. It only suggests the granularity\nof reconstruction obtained. Thus the reconstruction of the image can yield finer\ndetails at low level of compromise and point to segmentations at high level of\ncompromise. Regularization over the level of compromise and the number of\nclusters would lead to a reconstruction which has low reconstruction error as\nwell as adequate number of decompositions that represent an image properly.\nThere are a few points that need to be remembered when applying such\nan online learning paradigm. The reconstructed results come near to original\nimage only at a level of imposed compromise. As the size of dataset or the\nimage increases, the time consumed and the number of computations involved for\nprocessing also increases. To start with, the learner would perform well in clean\nimages than on noisy images. Adaptations need to be made for processing noisy\nimages or the pre-processing would be a necessary step before application of such\nan algorithm. Other inequalities can also be taken into account for multivariate\ninformation online. It would be tough to compare the algorithm with other\n\n\fTI Cluster Approximation Via Multivariate Chebyshev Inequality\n\n15\n\npowerful clustering algorithms as the proposed work presents a weak learner and\nprovides a general solution with no tight bounds on the quality of clustering.\nNevertheless, the current work contributes to estimation of cluster number\nin an unsupervised paradigm using transductive-inductive learning strategy. It\ncan be said that for a fixed Chebyshev parameter, in a bootstrapped sequence\nsampling environment without replacement, the unsupervised learner converges\nto a finite error rate along with the a finite number of clusters. The result in\nterms of clustering and the error rates may not be the most optimal (where the\nmeaning depends on the goal of optimization), but it does give an affirmative\nclue that image decomposition is robust and convergent.\n\n4\n\nConclusion\n\nA simple transductive-inductive learning strategy for unsupervised learning paradigm\nis presented with the usage of multivariate Chebyshev inequality. Theoretical\nproofs of convergence in number of clusters for a particular level of compromise\nshow (1) stability of result over a sequence and (2) robustness of probabilistically\nestimated approximation of cluster number over a random sample of sequences,\nrepresenting the same multidimensional data. Lastly, upper bounds generated\non the number of clusters point to a limited search space.\n\nReferences\n1. PO Berge. A note on a form of tohebycheff's theorem for two variables. Biometrika,\n29(3-4):405, 1938.\n2. J.C. Bezdek. Pattern recognition with fuzzy objective function algorithms. Kluwer\nAcademic Publishers, 1981.\n3. M. Charikar, C. Chekuri, T. Feder, and R. Motwani. Incremental clustering and\ndynamic information retrieval. In Proceedings of the twenty-ninth annual ACM\nsymposium on Theory of computing, pages 626\u2013635. ACM, 1997.\n4. X. Chen. A new generalization of chebyshev inequality for random vectors.\narXiv:math.ST, 0707(0805v1):1\u20135, 2007.\n5. R.C. Dubes. How many clusters are best?-an experiment. Pattern Recognition,\n20(6):645\u2013663, 1987.\n6. C. Fraley and A.E. Raftery. How many clusters? which clustering method? answers\nvia model-based cluster analysis. The computer journal, 41(8):578, 1998.\n7. R. Gomes, M. Welling, and P. Perona. Incremental learning of nonparametric\nbayesian mixture models. In Computer Vision and Pattern Recognition, 2008.\nCVPR 2008. IEEE Conference on, pages 1\u20138. Ieee, 2008.\n8. D. Hilbert. Uber die stetige abbildung einer linie auf ein flachenstuck. Math. Ann.,\n38:459460, 1891.\n9. T. Kanungo, D.M. Mount, N.S. Netanyahu, C.D. Piatko, R. Silverman, and A.Y.\nWu. An efficient-means clustering algorithm: Analysis and implementation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, pages 881\u2013892, 2002.\n10. DN Lal. A note on a form of tchebycheff's inequality for two or more variables.\nSankhy\u0101: The Indian Journal of Statistics (1933-1960), 15(3):317\u2013320, 1955.\n\n\f16\n\nSinha\n\n11. A.W. Marshall and I. Olkin. Multivariate chebyshev inequalities. The Annals of\nMathematical Statistics, pages 1001\u20131014, 1960.\n12. D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring\necological statistics. Proc. 8th Int'l Conf. Computer Vision, 2:416\u2013423, July 2001.\n13. D. Monhor. A chebyshev inequality for multivariate normal distribution. Probability in the Engineering and Informational Sciences, 21(02):289\u2013300, 2007.\n14. D. Monhor and S. Takemoto. Understanding the concept of outlier and its relevance\nto the assessment of data quality: Probabilistic background theory. Earth, Planets,\nand Space, 57(11):1009\u20131018, 2005.\n15. G. Shafer and V. Vovk. A tutorial on conformal prediction. The Journal of Machine\nLearning Research, 9:371\u2013421, 2008.\n16. S. Sinha and G. Horst. Bounded multivariate surfaces on monovariate internal\nfunctions. IEEE International Conference on Image Processing, (18):1037\u20131040,\n2011.\n17. S. Still and W. Bialek. How many clusters? an information-theoretic perspective.\nNeural computation, 16(12):2483\u20132506, 2004.\n18. V. Vovk, A. Gammerman, and G. Shafer. Algorithmic learning in a random world.\nSpringer Verlag, 2005.\n19. L. Xu. How many clusters?: A ying-yang machine based theory for a classical open\nproblem in pattern recognition. In Neural Networks, 1996., IEEE International\nConference on, volume 3, pages 1546\u20131551. IEEE, 1996.\n\n\f"}