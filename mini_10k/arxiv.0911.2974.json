{"id": "http://arxiv.org/abs/0911.2974v3", "guidislink": true, "updated": "2014-04-09T03:44:37Z", "updated_parsed": [2014, 4, 9, 3, 44, 37, 2, 99, 0], "published": "2009-11-16T16:39:33Z", "published_parsed": [2009, 11, 16, 16, 39, 33, 0, 320, 0], "title": "A Dynamic Near-Optimal Algorithm for Online Linear Programming", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0911.3845%2C0911.0721%2C0911.2567%2C0911.2237%2C0911.4346%2C0911.3810%2C0911.2582%2C0911.1260%2C0911.5237%2C0911.2735%2C0911.5117%2C0911.1180%2C0911.0797%2C0911.0707%2C0911.3068%2C0911.5298%2C0911.2885%2C0911.3734%2C0911.4691%2C0911.1772%2C0911.0777%2C0911.4311%2C0911.1977%2C0911.1196%2C0911.1110%2C0911.4936%2C0911.1603%2C0911.1318%2C0911.2225%2C0911.3408%2C0911.5413%2C0911.4465%2C0911.1486%2C0911.4479%2C0911.4995%2C0911.4790%2C0911.4959%2C0911.0025%2C0911.5309%2C0911.0510%2C0911.2736%2C0911.4229%2C0911.5645%2C0911.4457%2C0911.2581%2C0911.1610%2C0911.4304%2C0911.4770%2C0911.5496%2C0911.2367%2C0911.4890%2C0911.3316%2C0911.3943%2C0911.0787%2C0911.2386%2C0911.0382%2C0911.2501%2C0911.1813%2C0911.2035%2C0911.5314%2C0911.0789%2C0911.1986%2C0911.2694%2C0911.1088%2C0911.0313%2C0911.3123%2C0911.3903%2C0911.2250%2C0911.3158%2C0911.1830%2C0911.4854%2C0911.2266%2C0911.2160%2C0911.5426%2C0911.2946%2C0911.1336%2C0911.2470%2C0911.4809%2C0911.0239%2C0911.1734%2C0911.5680%2C0911.2892%2C0911.1836%2C0911.1937%2C0911.5138%2C0911.0301%2C0911.5532%2C0911.4235%2C0911.1034%2C0911.2847%2C0911.2630%2C0911.2762%2C0911.1909%2C0911.2602%2C0911.4221%2C0911.4152%2C0911.0119%2C0911.3331%2C0911.2974%2C0911.2780%2C0911.0433&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Dynamic Near-Optimal Algorithm for Online Linear Programming"}, "summary": "A natural optimization model that formulates many online resource allocation\nand revenue management problems is the online linear program (LP) in which the\nconstraint matrix is revealed column by column along with the corresponding\nobjective coefficient. In such a model, a decision variable has to be set each\ntime a column is revealed without observing the future inputs and the goal is\nto maximize the overall objective function. In this paper, we provide a\nnear-optimal algorithm for this general class of online problems under the\nassumption of random order of arrival and some mild conditions on the size of\nthe LP right-hand-side input. Specifically, our learning-based algorithm works\nby dynamically updating a threshold price vector at geometric time intervals,\nwhere the dual prices learned from the revealed columns in the previous period\nare used to determine the sequential decisions in the current period. Due to\nthe feature of dynamic learning, the competitiveness of our algorithm improves\nover the past study of the same problem. We also present a worst-case example\nshowing that the performance of our algorithm is near-optimal.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0911.3845%2C0911.0721%2C0911.2567%2C0911.2237%2C0911.4346%2C0911.3810%2C0911.2582%2C0911.1260%2C0911.5237%2C0911.2735%2C0911.5117%2C0911.1180%2C0911.0797%2C0911.0707%2C0911.3068%2C0911.5298%2C0911.2885%2C0911.3734%2C0911.4691%2C0911.1772%2C0911.0777%2C0911.4311%2C0911.1977%2C0911.1196%2C0911.1110%2C0911.4936%2C0911.1603%2C0911.1318%2C0911.2225%2C0911.3408%2C0911.5413%2C0911.4465%2C0911.1486%2C0911.4479%2C0911.4995%2C0911.4790%2C0911.4959%2C0911.0025%2C0911.5309%2C0911.0510%2C0911.2736%2C0911.4229%2C0911.5645%2C0911.4457%2C0911.2581%2C0911.1610%2C0911.4304%2C0911.4770%2C0911.5496%2C0911.2367%2C0911.4890%2C0911.3316%2C0911.3943%2C0911.0787%2C0911.2386%2C0911.0382%2C0911.2501%2C0911.1813%2C0911.2035%2C0911.5314%2C0911.0789%2C0911.1986%2C0911.2694%2C0911.1088%2C0911.0313%2C0911.3123%2C0911.3903%2C0911.2250%2C0911.3158%2C0911.1830%2C0911.4854%2C0911.2266%2C0911.2160%2C0911.5426%2C0911.2946%2C0911.1336%2C0911.2470%2C0911.4809%2C0911.0239%2C0911.1734%2C0911.5680%2C0911.2892%2C0911.1836%2C0911.1937%2C0911.5138%2C0911.0301%2C0911.5532%2C0911.4235%2C0911.1034%2C0911.2847%2C0911.2630%2C0911.2762%2C0911.1909%2C0911.2602%2C0911.4221%2C0911.4152%2C0911.0119%2C0911.3331%2C0911.2974%2C0911.2780%2C0911.0433&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A natural optimization model that formulates many online resource allocation\nand revenue management problems is the online linear program (LP) in which the\nconstraint matrix is revealed column by column along with the corresponding\nobjective coefficient. In such a model, a decision variable has to be set each\ntime a column is revealed without observing the future inputs and the goal is\nto maximize the overall objective function. In this paper, we provide a\nnear-optimal algorithm for this general class of online problems under the\nassumption of random order of arrival and some mild conditions on the size of\nthe LP right-hand-side input. Specifically, our learning-based algorithm works\nby dynamically updating a threshold price vector at geometric time intervals,\nwhere the dual prices learned from the revealed columns in the previous period\nare used to determine the sequential decisions in the current period. Due to\nthe feature of dynamic learning, the competitiveness of our algorithm improves\nover the past study of the same problem. We also present a worst-case example\nshowing that the performance of our algorithm is near-optimal."}, "authors": ["Shipra Agrawal", "Zizhuo Wang", "Yinyu Ye"], "author_detail": {"name": "Yinyu Ye"}, "author": "Yinyu Ye", "links": [{"href": "http://arxiv.org/abs/0911.2974v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0911.2974v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "Primary: 68W27, 90B99, Secondary: 90B05, 90B50, 90C05", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0911.2974v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0911.2974v3", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Submitted to Operations Research\nmanuscript (Please, provide the manuscript number!)\n\narXiv:0911.2974v3 [cs.DS] 9 Apr 2014\n\nAuthors are encouraged to submit new papers to INFORMS journals by means of\na style file template, which includes the journal title. However, use of a template\ndoes not certify that the paper has been accepted for publication in the named journal. INFORMS journal templates are for the exclusive purpose of submitting to an\nINFORMS journal and should not be used to distribute the papers in print or online\nor to submit the papers to another publication.\n\nA Dynamic Near-Optimal Algorithm for Online\nLinear Programming\nShipra Agrawal\nMicrosoft Research India, Bangalore, India, shipra@microsoft.com\n\nZizhuo Wang\nDepartment of Industrial and Systems Engineering, University of Minnesota, Minneapolis, MN 55455, zwang@umn.edu\n\nYinyu Ye\nDepartment of Management Science and Engineering, Stanford University, Stanford, CA 94305, yinyu-ye@stanford.edu\n\nA natural optimization model that formulates many online resource allocation problems is the online linear\nprogramming (LP) problem in which the constraint matrix is revealed column by column along with the\ncorresponding objective coefficient. In such a model, a decision variable has to be set each time a column\nis revealed without observing the future inputs and the goal is to maximize the overall objective function.\nIn this paper, we propose a near-optimal algorithm for this general class of online problems under the\nassumptions of random order of arrival and some mild conditions on the size of the LP right-hand-side\ninput. Specifically, our learning-based algorithm works by dynamically updating a threshold price vector at\ngeometric time intervals, where the dual prices learned from the revealed columns in the previous period are\nused to determine the sequential decisions in the current period. Due to the feature of dynamic learning,\nthe competitiveness of our algorithm improves over the past study of the same problem. We also present a\nworst-case example showing that the performance of our algorithm is near-optimal.\nKey words : online algorithms; linear programming; primal-dual; dynamic price update\n\n1. Introduction\nOnline optimization is attracting increasingly wide attention in the computer science, operations\nresearch, and management science communities. In many practical problems, data does not reveal\nitself at the beginning, but rather comes in an online fashion. For example, in online revenue management problems, consumers arrive sequentially, each requesting a subset of goods (e.g., multi-leg\nflights or a period of stay in a hotel) and offering a bid price. On observing a request, the seller needs\nto make an irrevocable decision whether to accept or reject the current bid with the overall objective of maximizing the revenue while satisfying the resource constraints. Similarly, in online routing\n1\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n2\n\nproblems, the network capacity manager receives a sequence of requests from users with intended\nusage of the network, each with a certain utility. And his objective is to allocate the network capacity to maximize the social welfare. A similar format also appears in online auctions, online keyword\nmatching problems, online packing problems, and various other online revenue management and\nresource allocation problems. For an overview of the online optimization literature and its recent\ndevelopment, we refer the readers to Borodin and El-Yaniv (1998), Buchbinder and Naor (2009a)\nand Devanur (2011).\nIn many examples mentioned above, the problem can be formulated as an online linear programming problem (Sometimes, people consider the corresponding integer program. While our\ndiscussion focuses on the linear programming relaxation of these problems, our results naturally\nextend to integer programs. See Section 5.2 for the discussion on this). An online linear programming problem takes a linear program as its underlying form, while the constraint matrix is revealed\ncolumn by column with the corresponding coefficient in the objective function. After observing the\ninput arrived so far, an immediate decision must be made without observing the future data. To\nbe precise, we consider the following (offline) linear program\nPn\nmaximize Pj=1 \u03c0j xj\nn\nsubject to j=1 aij xj \u2264 bi , i = 1, . . . , m\n0 \u2264 xj \u2264 1,\nj = 1, . . . , n,\n\n(1)\n\nm 1\nm\nm\nwhere for all j, \u03c0j \u2265 0, aj = {aij }m\ni=1 \u2208 [0, 1] , and b = {bi }i=1 \u2208 R . In the corresponding online\n\nlinear programming problem, at each time t, the coefficients (\u03c0t , at ) are revealed, and the decision\nvariable xt has to be chosen. Given the previous t \u2212 1 decisions x1 , . . . , xt\u22121 , and input {\u03c0j , aj }tj=1\n\nuntil time t, the tth decision variable xt has to satisfy\nPt\nj=1 aij xj \u2264 bi , i = 1, . . . , m\n0 \u2264 xt \u2264 1.\n\n(2)\n\nThe goal in the online linear programming problem is to choose xt 's such that the objective function\nPn\nt=1 \u03c0t xt is maximized.\nIn this paper, we propose algorithms that achieve good performance for solving the online linear\n\nprogramming problem. In order to define the performance of an algorithm, we first need to make\nsome assumptions regarding the input parameters. We adopt the following random permutation\nmodel in this paper:\nAssumption 1. The columns aj (with the objective coefficient \u03c0j ) arrive in a random order. The\nset of columns (a1 , a2 , ..., an ) can be adversarily picked at the start. However, the arrival order of\n(a1 , a2 , ..., an ) is uniformly distributed over all the permutations.\nAssumption 2. We know the total number of columns n a priori.\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n3\n\nThe random permutation model has been adopted in many existing literature for online problems\n(see Section 1.3 for a comprehensive review of the related literature). It is an intermediate path\nbetween using a worst-case analysis and assuming the distribution of the input is known. The worstcase analysis, which is completely robust to input uncertainty, evaluates an algorithm based on its\nperformance on the worst-case input (see, e.g., Mehta et al. (2005), Buchbinder and Naor (2009b)).\nHowever, this leads to very pessimistic performance bounds for this problem: no online algorithm can achieve better than O(1/n) approximation of the optimal offline solution (Babaioff et al.\n(2008)). On the other hand, although a priori input distribution can simplify the problem to a great\nextent, the choice of distribution is very critical and the performance can suffer if the actual input\ndistribution is not as assumed. Specifically, Assumption 1 is weaker than assuming the columns\nare drawn independently from some (possibly unknown) distribution. Indeed, one can view n i.i.d.\ncolumns as first drawing n samples from the underlying distribution and then randomly permute\nthem. Therefore, our proposed algorithm and its performance would also apply if the input data\nis drawn i.i.d. from some distribution.\nAssumption 2 is required since we need to use the quantity n to decide the length of history for\nlearning the threshold prices in our algorithm. In fact, as shown in Devanur and Hayes (2009), it\nis necessary for any algorithm to get a near-optimal performance.2 However, this assumption can\nbe relaxed to an approximate knowledge of n (within at most 1 \u00b1 \u01eb multiplicative error), without\naffecting our results.\nWe define the competitiveness of online algorithms as follows:\nDefinition 1. Let OPT denote the optimal objective value for the offline problem (1). An online\nalgorithm A is c-competitive in the random permutation model if the expected value of the online\nsolution obtained by using A is at least c factor of the optimal offline solution. That is,\n#\n\" n\nX\n\u03c0t xt \u2265 c * OPT,\nE\u03c3\nt=1\n\nwhere the expectation is taken over uniformly random permutations \u03c3 of 1, . . . , n, and xt is the tth\ndecision made by algorithm A when the inputs arrive in order \u03c3.\nIn this paper, we present a near-optimal algorithm for the online linear program (2) under the\nabove two assumptions and a lower bound condition on the size of b. We also extend our results\nto the following more general online linear optimization problems with multi-dimensional decisions\nat each time period:\n\u2022 Consider a sequence of n non-negative vectors f 1 , f 2 , . . . , f n \u2208 Rk , mn non-negative vectors\n\ng i1 , g i2 , . . . , g in \u2208 [0, 1]k ,\n\ni = 1, . . . , m,\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n4\n\nand K = {x \u2208 Rk : xT e \u2264 1, x \u2265 0} (we use e to denote the all 1 vectors). The offline linear program\n\nis to choose x1 ,...,xn to solve\n\nPn\nmaximize Pj=1 f Tj xj\nn\nsubject to j=1 g Tij xj \u2264 bi , i = 1, ..., m\nxj \u2208 K.\nIn the corresponding online problem, given the previous t \u2212 1 decisions x1 , . . . , xt\u22121 , each time we\n\nchoose a k-dimensional decision xt \u2208 Rk , satisfying:\nPt\nT\nj=1 g ij xj \u2264 bi , i = 1, . . . , m\nxt \u2208 K,\n\nusing the knowledge up to time t. And the objective is to maximize\n\n(3)\nPn\n\nT\nj=1 f j xj\n\nover the entire\n\ntime horizon. Note that Problem (2) is a special case of Problem (3) with k = 1.\n1.1. Specific applications\nIn the following, we show some specific applications of the online linear programming model. The\nexamples are only a few among the wide range of applications of this model.\n1.1.1. Online knapsack/secretary problem The one dimensional version of the online\nlinear programming problem studied in this paper is usually referred as online knapsack or secretary\nproblem. In such problems, a decision maker faces a sequence of options, each with a certain cost\nand a value, and he has to choose a subset of them in an online fashion so as to maximize the total\nvalue without violating the cost constraint. Applications of this problem arise in many contexts,\nsuch as hiring workers, scheduling jobs and bidding in sponsored search auctions.\nRandom permutation model has been widely adopted in the study of this problem, see Kleinberg\n(2005) and Babaioff et al. (2007) and references thereafter. In those papers, either a constant\ncompetitive ratio is obtained for finite-sized problems or a near-optimal algorithm is proposed for\nlarge-sized problems. In this paper, we study an extension of this problem to higher dimension and\npropose a near-optimal algorithm for it.\n1.1.2. Online routing problem Consider a computer network connected by m edges, each\nedge i has a bounded capacity (bandwidth) bi . There are a large number of requests arriving online,\neach asking for certain capacities at \u2208 Rm in the network, along with a utility or price for his/her\n\nrequest. The offline problem for the decision maker is given by the following integer program:\nPn\nmaximize Pt=1 \u03c0t xt\nn\nsubject to t=1 ait xt \u2264 bi i = 1, . . . , m\nxt \u2208 {0, 1}.\n\nDiscussions of this problem can be found in Buchbinder and Naor (2009b), Awerbuch et al. (1993)\nand references therein. Note that this problem is also studied under the name of online packing\nproblem.\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n5\n\n1.1.3. Online adwords problem Selling online advertisements has been the main revenue\ndriver for many internet companies such as Google, Yahoo, etc. Therefore improving the performance of ads allocation systems becomes extremely important for those companies and thus\nhas attracted great attention in the research community in the past decade. In the literature,\nthe majority of the research adopts an online matching model, see e.g., Mehta et al. (2005),\nGoel and Mehta (2008), Devanur and Hayes (2009), Karande et al. (2011), Bahmani and Kapralov\n(2010), Mahdian and Yan (2011), Feldman et al. (2010, 2009a,b). In such models, there are n search\nqueries arriving online. And there are m bidders (advertisers) each with a daily budget bi . Based\non the relevance of each search keyword, the ith bidder will bid a certain amount \u03c0ij on query j\nto display his advertisement along with the search result.3 For the j th query, the decision maker\n(i.e., the search engine) has to choose an m-dimensional vector xj = {xij }m\ni=1 , where xij \u2208 {0, 1}\n\nindicates whether the j th query is allocated to the ith bidder. The corresponding offline problem\ncan be formulated as:\n\nPn\nmaximize Pj=1 \u03c0Tj xj\nn\nsubject to j=1 \u03c0ij xij \u2264 bi , i = 1, . . . , m\nT\nxj e \u2264 1\nxj \u2208 {0, 1}m .\n\n(4)\n\nThe linear programming relaxation of (4) is a special case of the general online linear programming\nproblem (3) with f j = \u03c0j , g ij = \u03c0ij ei where ei is the ith unit vector of all zeros except 1 for the\nith entry.\nIn the literature, the random permutation assumption has attracted great interests recently for\nits tractability and generality. Constant competitive algorithm as well as near-optimal algorithms\nhave been proposed. We will give a more comprehensive review in Section 1.3.\n1.2. Key ideas and main results\nThe main contribution of this paper is to propose an algorithm that solves the online linear programming problem with a near-optimal competitive ratio under the random permutation model.\nOur algorithm is based on the observation that the optimal solution x\u2217 for the offline linear program can be largely determined by the optimal dual solution p\u2217 \u2208 Rm corresponding to the m\n\ninequality constraints. The optimal dual solution acts as a threshold price so that x\u2217j > 0 only if\n\n\u03c0j \u2265 p\u2217 T aj . Our online algorithm works by learning a threshold price vector from some initial\ninputs. The price vector is then used to determine the decisions for later periods. However, instead\nof computing the price vector only once, our algorithm initially waits until \u01ebn steps or arrivals,\nand then computes a new price vector every time the history doubles, i.e., at time \u01ebn, 2\u01ebn, 4\u01ebn, . . .\nand so on. We show that our algorithm is 1 \u2212 O(\u01eb)-competitive in the random permutation model\nunder a size condition of the right-hand-side input. Our main results are precisely stated as follows:\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n6\n\nTheorem 1. For any \u01eb > 0, our online algorithm is 1 \u2212 O(\u01eb) competitive for the online linear\nprogram (2) in the random permutation model, for all inputs such that\n\u0012\n\u0013\nm log (n/\u01eb)\nB = min bi \u2265 \u03a9\n.\ni\n\u01eb2\n\n(5)\n\nAn alternative way to state Theorem 1 is that our algorithm has a competitive ratio of 1 \u2212\n\u0010p\n\u0011\nO\nm log n/B . We prove Theorem 1 in Section 3. Note that the condition in Theorem 1 depends\non log n, which is far from satisfying everyone's demand when n is large. In Kleinberg (2005), the\n\nauthor proves that B \u2265 1/\u01eb2 is necessary to get a 1 \u2212 O(\u01eb) competitive ratio in the B-secretary\nproblem, which is the single dimensional counterpart of the online LP problem with at = 1 for all\nt. Thus, the dependence on \u01eb in Theorem 1 is near-optimal. In the next theorem, we show that a\ndependence on m is necessary for any online algorithm to obtain a near-optimal solution. Its proof\nwill appear in Section 4.\nTheorem 2. For any algorithm for the online linear programming problem (2) in the random\npermutation model, there exists an instance such that its competitive ratio is less than 1 \u2212 \u03a9(\u01eb)\nwhen\nB = min bi \u2264\ni\n\nlog(m)\n.\n\u01eb2\n\nOr equivalently, no algorithm can achieve a competitive ratio better than 1 \u2212 \u03a9\nWe also extend our results to the more general model as introduced in (3) :\n\n\u0010p\n\n\u0011\nlog m/B .\n\nTheorem 3. For any \u01eb > 0, our algorithm is 1 \u2212 O(\u01eb) competitive for the general online linear\nprogramming problem (3) in the random permutation model, for all inputs such that:\n\u0012\n\u0013\nm log (nk/\u01eb)\nB = min bi \u2265 \u03a9\n.\ni\n\u01eb2\n\n(6)\n\nNow we make some remarks on the conditions in Theorem 1 and 3. First of all, the conditions\nonly depend on the right-hand-side input bi 's, and are independent of the size of OPT or the\nobjective coefficients. And by the random permutation model, they are also independent of the\ndistribution of the input data. In this sense, our results are quite robust in terms of the input data\nuncertainty. In particular, one advantage of our result is that the conditions are checkable before\nthe algorithm is implemented, which is unlike the conditions in terms of OPT or the objective\ncoefficients. Even just in terms of bi , as shown in Theorem 2, the dependence on \u01eb is already optimal\nand the dependence on m is necessary. Regarding the dependence on n, we only need B to be\nof order log n, which is far less than the total number of bids n. Indeed, the condition might be\nstrict for some small-sized problems. However, if the budget is too small, it is not hard to imagine\nthat no online algorithm can do very well. On the contrary, in applications with large amount of\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n7\n\ninputs (for example, in the online adwords problem, it is estimated that a large search engine could\nreceive several billions of searches per day, even if we focus on a specific category, the number\ncan still be in the millions) with reasonably large right-hand-side inputs (e.g., the budgets for the\nadvertiser), the condition is not hard to satisfy. Furthermore, the conditions in Theorem 1 and 3\nare just theoretical results, the performance of our algorithm might still be very good even if the\nconditions are not satisfied (as shown in some numerical tests in Wang (2012)). Therefore, our\nresults are of both theoretical and practical interests.\nFinally, we finish this section with the following corollary:\nCorollary 1. In the online linear programming problem (2) and (3), if the largest entry of constraint coefficients does not equal to 1, then both our Theorem 1 and 3 still hold with the conditions\n(5) and (6) replaced by:\n\u0012\n\u0013\nbi\nm log (nk/\u01eb)\n\u2265\u03a9\n, \u2200i,\n\u0101i\n\u01eb2\nwhere, for each row i, \u0101i = maxj {|aij |} of (2), or \u0101i = maxj {kg ij k\u221e } of (3).\n1.3. Related work\nThe design and analysis of online algorithms have been a topic of wide interest in the computer science, operations research, and management science communities. Recently, the random permutation model has attracted growing popularity since it avoids the pessimistic lower\nbounds of the adversarial input model while still capturing the uncertainty of the inputs. Various\nonline algorithms have been studied under this model, including the secretary problem (Kleinberg\n(2005), Babaioff et al. (2008)), the online matching and adwords problem (Devanur and Hayes\n(2009), Feldman et al. (2009b), Goel and Mehta (2008), Mahdian and Yan (2011), Karande et al.\n(2011), Bahmani and Kapralov (2010)) and the online packing problem (Feldman et al. (2010),\nMolinaro and Ravi (2014)). Among these work, two types of results are obtained: one achieves a\nconstant competitive ratio independent of the input parameters; the other focuses on the performance of the algorithm when the input size is large. Our paper falls into the second category. In\nthe following literature review, we will focus ourselves on this category of work.\nThe first result that achieves a near-optimal performance in the random permutation model is\n\u221a\nby Kleinberg (2005), in which a 1 \u2212 O(1/ B) competitive algorithm is proposed for the single\n\u221a\ndimensional multiple-choice secretary problem. The author also proves that the 1 \u2212 O(1/ B) competitive ratio achieved by his algorithm is the best possible for this problem. Our result extends his\np\nwork to mutli-dimensional case with competitiveness 1 \u2212 O( m log n/B). Although the problem\nlooks similar, due to the multi-dimensional structure, different algorithms are needed and different\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n8\n\ntechniques are required for our analysis. Specifically, Kleinberg (2005) recursively applies a randomized version of the classical secretary algorithm while we maintain a price based on the linear\nprogramming duality theory and have a fixed price updating schedule. We also prove that no online\np\nalgorithm can achieve a competitive ratio better than 1 \u2212 \u03a9( log m/B) for the multi-dimensional\n\nproblem. To the best of our knowledge, this is the first result that shows the necessity of dependence on the dimension m, for the best competitive ratio achievable for this problem. It clearly\npoints out that high dimensionality indeed adds to the difficulty of this problem.\nLater, Devanur and Hayes (2009) study a linear programming based approach for the online\nadwords problem. In their approach, they solve a linear program once and utilize its dual solution as threshold price to make future decisions. The authors prove a competitive ratio of 1 \u2212\np\nO( 3 \u03c0max m2 log n/OPT) for their algorithm. In our work, we consider a more general model and\ndevelop an algorithm which updates the dual prices at a carefully chosen pace. By using dynamic\np\nupdates, we achieve a competitive ratio that can depend only on B: 1 \u2212 O( m log n/B). This is\n\nattractive in practice since B can be checked before the problem is solved while OPT can not.\nMoreover, we show that the dependence on B of our result is optimal. Although our algorithm\nshares similar ideas to theirs, the dynamic nature of our algorithm requires a much more delicate\ndesign and analysis. We also answer the important question of how often we should update the\ndual prices and we show that significant improvements can be made by using the dynamic learning\nalgorithm.\nRecently, Feldman et al. (2010) study a more general online packing problem which allows the\ndimension of the choice set to vary at each time period (a further extension of (3)). They propose\na one-time learning algorithm which achieves a competitive ratio that depends both on the rightp\nhand-side B and OPT. And the dependence on B is of order 1 \u2212 O( 3 m log n/B). Therefore,\n\ncomparing to their competitive ratio, our result not only removes the dependence on OPT, but\n\nalso improves the dependence on B by an order. We show that the improvement is due to the use\nof dynamic learning.\nMore recently, Molinaro and Ravi (2014) study the same problem and obtain a competitive ratio\np\nof 1 \u2212 O( m2 log m/B). The main structure of their algorithm (especially the way they obtain\n\nsquare root rather than cubic root) is modified from that in this paper. They further use a novel\ncovering technique to remove the dependence on n in the competitive ratio, at an expense of\n\nincreasing an order of m. In contrast, we present the improvement from the cubic root to square\nroot and how to remove the dependence on OPT.\nA comparison of the results of Kleinberg (2005), Devanur and Hayes (2009), Feldman et al.\n(2010), Molinaro and Ravi (2014) and this work is shown in Table 1.\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n9\n\n\u0010 Competitiveness\n\u221a \u0011\nKleinberg (2005)\n1 \u2212 O 1/ B (only for m = 1)\np\nDevanur and Hayes (2009)\n1 \u2212p\nO( 3 \u03c0max m2 log n/OPT)\nFeldman et al. (2010)\n1 \u2212 O(max{ 3 m logpn/B, \u03c0max m log n/OP T })\nMolinaro and Ravi (2012)\n1 \u2212 O( pm2 log m/B)\nThis paper\n1 \u2212 O( m log n/B)\nTable 1\n\nComparison of existing results\n\nBesides the random permutation model, Devanur et al. (2011) study an online resource allocation\nproblem under what they call the adversarial stochastic input model. This model generalizes the\ncase when the columns are drawn from an i.i.d. distribution, however, it is more stringent than the\nrandom permutation model. In particular, their model does not allow the situations when there\nmight be a number of \"shocks\" in the input series. For this input model, they develop an algorithm\n\u0011\n\u0010\np\np\nthat achieves a competitive ratio of 1 \u2212 O max{ log m/B, \u03c0max log m/OPT} . Their result is\n\nsignificant in that it achieves near-optimal dependence on m. However, the dependence on OPT\n\nand the stronger assumption makes it not directly comparable to our results. And their algorithm\nuses quite different techniques from ours.\nIn the operations research and management science communities, a dynamic and optimal\npricing strategy for various online revenue management and resource allocation problems has\nalways been an important research topic, some literature include Elmaghraby and Keskinocak\n(2003) ,Gallego and van Ryzin (1997, 1994), Talluri and van Ryzin (1998), Cooper (2002) and\nBitran and Caldentey (2003). In Gallego and van Ryzin (1997, 1994) and Bitran and Caldentey\n(2003), the arrival processes are assumed to be price sensitive. However, as commented in Cooper\n(2002), this model can be reduced to a price independent arrival process with availability control\nunder Poisson arrivals. Our model can be further viewed as a discrete version of the availability\ncontrol model which is also used as an underlying model in Talluri and van Ryzin (1998) and discussed in Cooper (2002). The idea of using a threshold - or \"bid\" - price is not new. It is initiated\nin Williamson (1992), Simpson (1989) and investigated further in Talluri and van Ryzin (1998).\nIn Talluri and van Ryzin (1998), the authors show that the bid price control policy is asymptotically optimal. However, they assume the knowledge on the arrival process and therefore the price\nis obtained by \"forecasting\" the future using the distribution information rather than \"learning\"\nfrom the past observations as we do in our paper. The idea of using linear programming to find the\ndual optimal bid price is discussed in Cooper (2002) where asymptotic optimality is also achieved.\nBut again, the arrival process is assumed to be known which makes the analysis quite different.\nThe contribution of this paper is several fold. First, we study a general online linear programming\nframework, extending the scope of many prior work. And due to its dynamic learning capability,\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n10\n\nour algorithm is distribution free\u2013no knowledge on the input distribution is assumed except for the\nrandom order of arrival and the total number of entries. Moreover, instead of learning the price\njust once, we propose a dynamic learning algorithm that updates the prices as more information\nis revealed. The design of such an algorithm answers the question raised in Cooper (2002), that\nis, how often and when should one update the price? We give an explicit answer to this question\nby showing that updating the prices at geometric time intervals -not too sparse nor too often- is\noptimal. Thus we present a precisely quantified strategy for dynamic price update. Furthermore,\nwe provide a non-trivial lower bound for this problem, which is the first of its kind and show for\nthe first time that the dimensionality of the problem adds to its difficulty.\nIn our analysis, we apply many standard techniques from Probably Approximately Correct\nlearning (PAC-learning), in particular, concentration bounds and covering arguments. Our dynamic\nlearning also shares a similar idea as the \"doubling trick\" used in learning problems. However, unlike\nthe doubling trick which is typically applied to an unknown time horizon (Cesa-Bianchi and Lugosi\n(2006)), we show that a geometric pace of price updating in a fixed length of horizon with a careful\ndesign could also enhance the performance of the algorithm.\n1.4. Organization\nThe rest of the paper is organized as follows. In Section 2 and 3, we present our online algorithm\nand prove that it achieves 1 \u2212 O(\u01eb) competitive ratio under mild conditions on the input. To keep\n\nthe discussions clear and easy to follow, we start in Section 2 with a simpler one-time learning\nalgorithm. While the analysis for this simpler algorithm will be useful to demonstrate our proof\ntechniques, the results obtained in this setting are weaker than those obtained by our dynamic\nlearning algorithm, which is discussed in Section 3. In Section 4, we give a detailed proof of Theorem\n2 regarding the necessity of lower bound condition used in our main theorem. In Section 5, we\npresent several extensions of our study. Then we conclude our paper in Section 6.\n\n2. One-time Learning Algorithm\nIn this section, we propose a one-time learning algorithm for the online linear programming problem. We consider the following partial linear program defined only on the input until time s = \u01ebn\n(for the ease of notation, without loss of generality, we assume \u01ebn is an integer throughout our\nanalysis):\n\nPs\nmaximize Pt=1 \u03c0t xt\ns\nsubject to t=1 ait xt \u2264 (1 \u2212 \u01eb) ns bi , i = 1, . . . , m\n0 \u2264 xt \u2264 1,\nt = 1, . . . , s,\n\n(7)\n\nPm\nPs\nminimize Pi=1 (1 \u2212 \u01eb) ns bi pi + t=1 yt\nm\nsubject to i=1 ait pi + yt \u2265 \u03c0t ,\nt = 1, . . . , s\npi , yt \u2265 0,\ni = 1, . . . , m, t = 1, . . . , s.\n\n(8)\n\nand its dual problem:\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n11\n\nLet (p\u0302, \u0177) be the optimal solution to (8). Note that p\u0302 has the natural meaning of the price for each\nresource. For any given price vector p, we define the allocation rule xt (p) as follows:\n\u001a\n0 if \u03c0t \u2264 pT at\nxt (p) =\n1 if \u03c0t > pT at .\n\n(9)\n\nWe now state our one-time learning algorithm:\nAlgorithm OLA (One-time Learning Algorithm):\n1. Initialize xt = 0, for all t \u2264 s. And p\u0302 is defined as above.\nPt\u22121\n2. For t = s + 1, s + 2, . . . , n, if ait xt (p\u0302) \u2264 bi \u2212 j=1 aij xj for all i, set xt = xt (p\u0302); otherwise, set\n\nxt = 0. Output xt .\n\nIn the one-time learning algorithm, we learn a dual price vector using the first \u01ebn arrivals. Then, at\neach time t > \u01ebn, we use this dual price to decide the current allocation, and execute this decision\nas long as it doesn't violate any of the constraints. An attractive feature of this algorithm is that\nit requires to solve only one small linear program, defined on \u01ebn variables. Note that the righthand-side of (7) is modified by a factor 1 \u2212 \u01eb. This modification is to guarantee that with high\nprobability, the allocation xt (p) does not violate the constraints. This trick is also used in Section\n3 when we study the dynamic learning algorithm. In the next subsection, we prove the following\nproposition regarding the competitive ratio of the one-time learning algorithm, which relies on a\nstronger condition than Theorem 1:\nProposition 1. For any \u01eb > 0, the one-time learning algorithm is 1 \u2212 6\u01eb competitive for the online\nlinear program (2) in the random permutation model, for all inputs such that\nB = min bi \u2265\ni\n\n6m log(n/\u01eb)\n.\n\u01eb3\n\n2.1. Competitive Ratio Analysis\nObserve that the one-time learning algorithm waits until time s = \u01ebn, and then sets the solution\nat time t as xt (p\u0302), unless it violates the constraints. To prove its competitive ratio, we follow the\nfollowing steps. First we show that if p\u2217 is the optimal dual solution to (1), then {xt (p\u2217 )} is close\n\nto the primal optimal solution x\u2217 , i.e., learning the dual price is sufficient to determine a close\nprimal solution. However, since the columns are revealed in an online fashion, we are not able to\nobtain p\u2217 during the decision period. Instead, in our algorithm, we use p\u0302 as a substitute. We then\nshow that p\u0302 is a good substitute of p\u2217 : 1) with high probability, xt (p\u0302) satisfies all the constraints of\nP\nthe linear program; 2) the expected value of t \u03c0t xt (p\u0302) is close to the optimal offline value. Before\nwe start our analysis, we make the following simplifying technical assumption in our discussion:\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n12\n\nAssumption 3. The problem inputs are in general position, namely for any price vector p, there\ncan be at most m columns such that pT at = \u03c0t .\nAssumption 3 is not necessarily true for all inputs. However, as pointed out by Devanur and Hayes\n(2009), one can always randomly perturb \u03c0t by arbitrarily small amount \u03b7 through adding a random\nvariable \u03bet taking uniform distribution on interval [0, \u03b7]. In this way, with probability 1, no p can\nsatisfy m + 1 equations simultaneously among pT at = \u03c0t , and the effect of this perturbation on the\nobjective can be made arbitrarily small. Under this assumption, we can use the complementarity\nconditions of linear program (1) to obtain the following lemma.\nLemma 1. xt (p\u2217 ) \u2264 x\u2217t for all t, and under Assumption 3, x\u2217t and xt (p\u2217 ) differs for no more than\nm values of t.\n\nProof. Consider the offline linear program (1) and its dual (let p denote the dual variables associated with the first set of constraints and yt denote the dual variables associated with the constraints\nxt \u2264 1):\n\nPm\nPn\nminimize Pi=1 bi pi + t=1 yt\nm\nsubject to i=1 ait pi + yt \u2265 \u03c0t , t = 1, ..., n\npi , yt \u2265 0,\ni = 1, ..., m, t = 1, ..., n.\n\n(10)\n\nBy the complementarity slackness conditions, for any optimal solution x\u2217 for the primal problem\n(1) and optimal solution (p\u2217 , y \u2217 ) for the dual, we must have:\n!\nm\nX\n\u2217\n\u2217\n\u2217\nand\n(1 \u2212 x\u2217t ) * yt\u2217 = 0\nxt *\nait pi + yt \u2212 \u03c0t = 0\n\nfor all t.\n\ni=1\n\nIf xt (p\u2217 ) = 1, by (9), \u03c0t > (p\u2217 )T at . Thus, by the constraint in (10), yt\u2217 > 0 and finally by the last\ncomplementarity condition, x\u2217t = 1. Therefore, we have xt (p\u2217 ) \u2264 x\u2217t for all t. On the other hand, if\n\n\u03c0t < (p\u2217 )T at , then we must have both xt (p\u2217 ) and x\u2217t = 0. Therefore, xt (p\u2217 ) = x\u2217t if (p\u2217 )T at 6= \u03c0t .\n\nUnder Assumption 3, there are at most m values of t such that (p\u2217 )T at = \u03c0t . Therefore, x\u2217t and\nxt (p\u2217 ) differs for no more than m values of t.\n\n\u0003\n\u2217\n\n\u2217\n\nLemma 1 shows that if an optimal dual solution p to (1) is known, then xt (p )'s obtained by\nour decision policy is close to the optimal offline solution. However, in our online algorithm, we\nuse the sample dual price p\u0302 learned from the first few inputs, which could be different from the\noptimal dual price p\u2217 . The remaining discussion attempts to show that the sample dual price p\u0302\nwill be sufficiently accurate for our purpose. In the following, we will frequently use the fact that\nthe random order assumption can be interpreted as that the first s inputs are uniform random\nsamples without replacement of size s from the n inputs. And we use S to denote the sample set\nof size s, and N to denote the complete input set of size n. We start with the following lemma\nwhich shows that with high probability, the primal solution xt (p\u0302) constructed using the sample\ndual price is feasible:\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n13\n\nLemma 2. The primal solution constructed using the sample dual price is a feasible solution to the\nlinear program (1) with high probability. More precisely, with probability 1 \u2212 \u01eb,\nn\nX\nt=1\n\nait xt (p\u0302) \u2264 bi , \u2200i = 1, . . . , m\n\ngiven B \u2265 6m log(n/\u01eb)\n.\n\u01eb3\nProof. The proof will proceed as follows: Consider any fixed price p and i. We say a sample S is\n\"bad\" for this p and i if and only if p is the optimal dual price to (8) for the sample set S, but\nPn\nt=1 ait xt (p) > bi . First, we show that the probability of bad samples is small for every fixed p\n\nand i. Then, we take a union bound over all distinct prices to prove that with high probability the\nPn\nlearned price p\u0302 will be such that t=1 ait xt (p\u0302) \u2264 bi for all i.\n\nTo start with, we fix p and i. Define Yt = ait xt (p). If p is an optimal dual solution for the sample\n\nlinear program on S, applying Lemma 1 to the sample problem, we have\nX\n\nYt =\n\nt\u2208S\n\nX\nt\u2208S\n\nait xt (p) \u2264\n\nX\nt\u2208S\n\nait x\u0303t \u2264 (1 \u2212 \u01eb)\u01ebbi,\n\nwhere x\u0303 is the primal optimal solution to the sample linear program on S. Now we consider the\nprobability of bad samples for this p and i:\nP\n\nX\nt\u2208S\n\nWe first define Zt =\nP\n\nX\nt\u2208S\n\nP bi Yt\n\nt\u2208N Yt\n\nYt \u2264 (1 \u2212 \u01eb)\u01ebbi ,\n\nX\nt\u2208N\n\n!\n\nYt \u2265 bi .\n\n. It is easy to see that\n\nYt \u2264 (1 \u2212 \u01eb)\u01ebbi ,\n\nX\nt\u2208N\n\nYt \u2265 bi\n\n!\n\n\u2264P\n\nX\nt\u2208S\n\nZt \u2264 (1 \u2212 \u01eb)\u01ebbi ,\n\nX\n\n!\n\nZt = bi .\n\nt\u2208N\n\nFurthermore, we have\nP\n\nX\nt\u2208S\n\nZt \u2264 (1 \u2212 \u01eb)\u01ebbi,\n\nX\nt\u2208N\n\nZt = bi\n\n!\n\n\u2264P\n\nX\n\n\u2264P\n\nX\n\nt\u2208S\n\nt\u2208S\n\nZt \u2212 \u01eb\n\nX\n\nZt \u2212 \u01eb\n\nX\n\n\u2212\u01eb3 bi\n\u2264 2 exp\n2+\u01eb\n\n\u0012\n\nwhere \u03b4 =\n\n\u01eb\n.\nm*nm\n\nt\u2208N\n\nt\u2208N\n\n\u0013\n\n!\n\nZt \u2265 \u01eb2 bi ,\n\nX\n\nZt = bi\n\nZt \u2265 \u01eb2 bi\n\nX\n\nZt = bi\n\nt\u2208N\n\nt\u2208N\n\n!\n\n\u2264\u03b4\n\nThe second to last step follows from the Hoeffding-Bernstein's Inequality for\n\nsampling without replacement (Lemma 10 in Appendix A) by treating Zt , t \u2208 S as the samples\nwithout replacement from Zt , t \u2208 N . We also used the fact that 0 \u2264 Zt \u2264 1 for all t, therefore\nP\nP\n2\n2\n2\nt\u2208N (Zt \u2212 Z\u0304) \u2264\nt\u2208N Zt \u2264 bi (and therefore the \u03c3R in Lemma 10 can be bounded by bi ). Finally,\nthe last inequality is due to the assumption made on B.\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n14\n\nNext, we take a union bound over all distinct p's. We call two price vectors p and q distinct\nif and only if they result in distinct solutions, i.e., {xt (p)} =\n6 {xt (q)}. Note that we only need to\nconsider distinct prices, since otherwise all the Yt 's are exactly the same. Note that each distinct\np is characterized by a unique separation of n points ({\u03c0t , at }nt=1 ) in m + 1-dimensional space by\na hyperplane. By results from computational geometry, the total number of such distinct prices\nis at most nm (Orlik and Terao (1992)). Taking union bound over the nm distinct prices, and\ni = 1, . . . , m, we get the desired result.\n\n\u0003\n\nAbove we showed that with high probability, xt (p\u0302) is a feasible solution. In the following, we\nshow that it is also a near-optimal solution.\nLemma 3. The primal solution constructed using the sample dual price is a near-optimal solution\nto the linear program (1) with high probability. More precisely, with probability 1 \u2212 \u01eb,\nX\nt\u2208N\n\n\u03c0t xt (p\u0302) \u2265 (1 \u2212 3\u01eb)OPT\n\ngiven B \u2265 6m log(n/\u01eb)\n.\n\u01eb3\nProof. The proof is based on two observations. First, {xt (p\u0302)}nt=1 and p\u0302 satisfy all the complementarity conditions, and hence is the optimal primal and dual solution to the following linear\nprogram:\n\nP\n\u03c0x\nmaximize\nPt\u2208N t t\nsubject to t\u2208N ait xt \u2264 b\u0302i , i = 1, . . . , m\n0 \u2264 xt \u2264 1,\nt = 1, . . . , n\nP\nP\nwhere b\u0302i = t\u2208N ait xt (p\u0302) if p\u0302i > 0, and b\u0302i = max{ t\u2208N ait xt (p\u0302), bi }, if p\u0302i = 0.\n\n(11)\n\nSecond, we show that if p\u0302i > 0, then with probability 1 \u2212 \u01eb, b\u0302i \u2265 (1 \u2212 3\u01eb)bi . To show this, let p\u0302\n\nbe the optimal dual solution of the sample linear program on set S and x\u0302 be the optimal primal\nsolution. By the complementarity conditions of the linear program, if p\u0302i > 0, the ith constraint must\nP\nbe satisfied with equality. That is, t\u2208S ait x\u0302t = (1 \u2212 \u01eb)\u01ebbi . Then, by Lemma 1 and the condition\n\nthat B = mini bi \u2265 \u01ebm2 , we have\n\nX\nt\u2208S\n\nait xt (p\u0302) \u2265\n\nX\nt\u2208S\n\nait x\u0302t \u2212 m \u2265 (1 \u2212 2\u01eb)\u01ebbi.\n\nThen, using the Hoeffding-Bernstein's Inequality for sampling without replacement, in a manner\nsimilar to the proof of Lemma 2, we can show that (the detailed proof is given in Appendix A.2)\ngiven the lower bound on B, with probability at least 1 \u2212 \u01eb, for all i such that p\u0302i > 0:\nb\u0302i =\n\nX\nt\u2208N\n\nait xt (p\u0302) \u2265 (1 \u2212 3\u01eb)bi.\n\n(12)\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n15\n\nCombined with the case p\u0302i = 0, we know that with probability 1 \u2212 \u01eb, b\u0302i \u2265 (1 \u2212 3\u01eb)bi for all i. Lastly,\n\nobserving that whenever (12) holds, given an optimal solution x\u2217 to (1), (1 \u2212 3\u01eb)x\u2217 will be a feasible\nsolution to (11). Therefore, the optimal value of (11) is at least (1 \u2212 3\u01eb)OPT, which is equivalently\n\nsaying that\nn\nX\nt=1\n\n\u03c0t xt (p\u0302) \u2265 (1 \u2212 3\u01eb)OPT.\n\u0003\n\nTherefore, the objective value for the online solution taken over the entire period is near-optimal.\nHowever, in the one-time learning algorithm, no decision is made during the learning period S,\nand only the decisions from periods {s + 1, . . . , n} contribute to the objective value. The following\nlemma that relates the optimal value of the sample linear program (7) to the optimal value of the\noffline linear program (1) will bound the contribution from the learning period:\nLemma 4. Let OPT(S) denote the optimal value of the linear program (7) over sample S, and\nOPT(N ) denote the optimal value of the offline linear program (1) over N . Then,\nE[OPT(S)] \u2264 \u01ebOPT(N ).\nProof. Let (x\u2217 , p\u2217 , y \u2217 ) and (x\u0302, p\u0302, \u0177) denote the optimal primal and dual solutions of linear program\n(1) on N , and sample linear program (7) on S, respectively.\nP\nP\n(p\u0302, \u0177) = arg min (1 \u2212 \u01eb)\u01ebbT p + t\u2208S yt\n(p\u2217 , y \u2217 ) = arg min bT p + t\u2208N yt\ns.t.\npT at + yt \u2265 \u03c0t , t \u2208 S\ns.t.\npT at + yt \u2265 \u03c0t , t \u2208 N\np, y \u2265 0.\np, y \u2265 0\nNote that S \u2286 N , thus (p\u2217 , y \u2217 ) is a feasible solution to the dual of the linear program on S.\nTherefore, by the weak duality theorem:\nOPT(S) \u2264 \u01ebbT p\u2217 +\n\nX\n\nyt\u2217 .\n\nt\u2208S\n\nTherefore,\nE[OPT(S)] \u2264 \u01ebbT p\u2217 + E\n\n\"\nX\nt\u2208S\n\n#\n\nyt\u2217 = \u01eb(bT p\u2217 +\n\nX\n\nyt\u2217 ) = \u01ebOPT(N ).\n\n\u0003\n\nt\u2208N\n\nNow, we are ready to prove Proposition 1:\nProof of Proposition 1: Using Lemma 2 and Lemma 3, with probability at least 1 \u2212 2\u01eb, the\nfollowing events happen:\n\nn\nX\nt=1\n\nait xt (p\u0302) \u2264 bi ,\n\ni = 1, . . . , m\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n16\n\nn\nX\nt=1\n\n\u03c0t xt (p\u0302) \u2265 (1 \u2212 3\u01eb)OP T.\n\nThat is, the decisions xt (p\u0302) are feasible and the objective value taken over the entire period\n{1, . . . , n} is near-optimal. Denote this event by E , where P (E ) \u2265 1 \u2212 2\u01eb. We have by Lemma 2, 3\n\nand 4:\nE\n\n\"\n\nn\nX\n\nt=s+1\n\n#\n\n\u03c0t xt = E\n\n\" n\nX\nt=1\n\n\u2265E\n\n\" n\nX\nt=1\n\n\u03c0t xt \u2212\n\ns\nX\n\n\u03c0t xt\n\nt=1\n\n#\n\n#\n\n\u03c0t xt (p\u0302)I(E ) \u2212 E\n\n\" s\nX\n\n\u03c0t xt (p\u0302)\n\nt=1\n\n#\n\n\u2265 (1 \u2212 3\u01eb)P (E )OPT \u2212 \u01ebOPT \u2265 (1 \u2212 6\u01eb)OPT\n\nwhere I(*) is the indicator function, the first inequality is because under E , xt = xt (p\u0302), and the\nsecond last inequality uses the fact that xt (p\u0302) \u2264 x\u0302t which is due to Lemma 1.\n\n\u0003\n\n3. Dynamic Learning Algorithm\nThe algorithm discussed in Section 2 uses the first \u01ebn inputs to learn a threshold price, and then\napplies it in the remaining time horizon. While this algorithm has its own merits, in particular,\nrequires solving only a small linear program defined on \u01ebn variables, the lower bound required on\nB is stronger than that claimed in Theorem 1 by an \u01eb factor.\nIn this section, we propose an improved dynamic learning algorithm that will achieve the result in\nTheorem 1. Instead of computing the price only once, the dynamic learning algorithm will update\nthe price every time the history doubles, that is, it learns a new price at time t = \u01ebn, 2\u01ebn, 4\u01ebn, . . .. To\nbe precise, let p\u0302l denote the optimal dual solution for the following partial linear program defined\non the inputs until time l:\nPl\n\u03c0x\nmaximize\nPlt=1 t t\nsubject to t=1 ait xt \u2264 (1 \u2212 hl ) nl bi , i = 1, . . . , m\n0 \u2264 xt \u2264 1,\nt = 1, . . . , l\n\n(13)\n\nwhere the set of numbers hl are defined as follows:\nhl = \u01eb\n\npn\nl\n\n.\n\nAlso, for any given dual price vector p, we define the same allocation rule xt (p) as in (9). Our\ndynamic learning algorithm is stated as follows:\nAlgorithm DLA (Dynamic Learning Algorithm):\n1. Initialize t0 = \u01ebn. Set xt = 0, for all t \u2264 t0 .\n2. Repeat for t = t0 + 1, t0 + 2, . . .\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n17\n\n(a) Set x\u0302t = xt (p\u0302l ). Here l = 2r \u01ebn where r is the largest integer such that l < t.\nPt\u22121\n(b) If ait x\u0302t \u2264 bi \u2212 j=1 aij xj for all i, then set xt = x\u0302t ; otherwise, set xt = 0. Output xt .\nNote that we update the dual price vector \u2308log2 (1/\u01eb)\u2309 times during the entire time horizon.\nThus, the dynamic learning algorithm requires more computation. However, as we show next, it\nrequires a weaker lower bound on B for proving the same competitive ratio. The intuition behind\n\u221a\nthis improvement is as follows. Note that initially, at l = \u01ebn, hl = \u01eb > \u01eb. Thus, we have larger\nslacks at the beginning, and the large deviation argument for constraint satisfaction (as in Lemma\n2) requires a weaker condition on B. As t increases, l increases, and hl decreases. However, for\nlarger values of l, the sample size is larger, making a weaker condition on B sufficient to prove\nthe same error bound. Furthermore, hl decreases rapidly enough, such that the overall loss on the\nobjective value is not significant. As one will see, the careful choice of the numbers hl plays an\nimportant role in proving our results.\n3.1. Competitive Ratio Analysis\nThe analysis for the dynamic learning algorithm proceeds in a manner similar to that for the onetime learning algorithm. However, stronger results for the price learned in each period need to be\nproved here. In the following, we assume \u01eb = 2\u2212E and let L = {\u01ebn, 2\u01ebn, . . . , 2E\u22121 \u01ebn}.\nLemma 5 and 6 are parallel to Lemma 2 and 3 in Section 2, however require a weaker condition\non B:\nLemma 5. For any \u01eb > 0, with probability 1 \u2212 \u01eb:\n2l\nX\n\nt=l+1\n\ngiven B = mini bi \u2265\n\nait xt (p\u0302l ) \u2264\n\nl\nbi ,\nn\n\nfor all i \u2208 {1, . . . , m}, l \u2208 L\n\n10m log (n/\u01eb)\n.\n\u01eb2\n\nProof. The proof is similar to the proof of Lemma 2 but a more careful analysis is needed.\nWe provide a brief outline here with a detailed proof in Appendix B.1. First, we fix p, i and\nl. This time, we say a permutation is \"bad\" for this p, i and l if and only if p = p\u0302l (i.e., p\nP2l\nl\nl\nis the learned price under the current arrival order) but\nt=l+1 ait xt (p\u0302 ) > n bi . By using the\n\nHoeffding-Bernstein's Inequality for sampling without replacement, we show that the probability\nof \"bad\" permutations is less than \u03b4 =\n\n\u01eb\nm*nm *E\n\nfor any fixed p, i and l under the condition on B.\n\nThen by taking a union bound over all distinct prices, all items i and periods l, the lemma is\nproved.\n\n\u0003\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n18\n\nIn the following, we use LPs (d) to denote the partial linear program that is defined on variables\ntill time s with right-hand-side in the inequality constraints set as d. That is,\nPs\nmaximize Pt=1 \u03c0t xt\ns\nLPs (d) : subject to t=1 ait xt \u2264 di , i = 1, . . . , m\n0 \u2264 xt \u2264 1,\nt = 1, . . . , s.\nAnd let OPTs (d) denote the optimal objective value for LPs (d).\nLemma 6. With probability at least 1 \u2212 \u01eb, for all l \u2208 L:\n2l\nX\nt=1\n\ngiven B = mini bi \u2265\nProof. Let b\u0302i =\n\nP2l\n\nl\n\n\u03c0t xt (p\u0302 ) \u2265 (1 \u2212 2hl \u2212 \u01eb)OPT2l\n\n\u0012\n\n2l\nb\nn\n\n\u0013\n\n10m log (n/\u01eb)\n.\n\u01eb2\n\nj=1\n\nP2l\nb }, otherwise.\naij xj (p\u0302l ) for i such that p\u0302li > 0, and b\u0302i = max{ j=1 aij xj (p\u0302l ), 2l\nn i\n\nl\nThen the solution pair ({xt (p\u0302l )}2l\nt=1 , p\u0302 ) satisfy all the complementarity conditions, thus are optimal\n\nsolutions (primal and dual respectively) to the linear program LP2l (b\u0302):\nP2l\nmaximize\nt=1 \u03c0t xt\nP2l\nsubject to t=1 ait xt \u2264 b\u0302i , i = 1, . . . , m\n0 \u2264 xt \u2264 1,\nt = 1, . . . , 2l.\nThis means\n\n2l\nX\nt=1\n\nl\n\n\u03c0t xt (p\u0302 ) = OPT2l (b\u0302) \u2265 min\n\nNow, we analyze the ratio\n\ni\n\nb\u0302i\n.\n2lbi /n\n\nb\u0302i\nbi 2l\nn\n\n!\n\nOPT2l\n\n\u0012\n\n\u0013\n2l\nb .\nn\n\nBy definition, for i such that p\u0302li = 0, b\u0302i \u2265 2lbi /n. Otherwise, using\n\ntechniques similar to the proof of Lemma 5, we can prove that with probability 1 \u2212 \u01eb, for all i,\nb\u0302i =\n\n2l\nX\nt=1\n\nait xt (p\u0302l ) \u2265 (1 \u2212 2hl \u2212 \u01eb)\n\n2l\nbi .\nn\n\n(14)\n\nA detailed proof of (14) appears in Appendix B.2. And the lemma follows from (14).\n\n\u0003\n\nNext, similar to Lemma 4 in the previous section, we prove the following lemma relating the\noptimal value of the sample linear program to the optimal value of the offline linear program:\nLemma 7. For any l,\n\u0012 \u0013\u0015\n\u0014\nl\nl\nb \u2264 OPT.\nE OPTl\nn\nn\nThe proof of lemma 7 is exactly the same as the proof for Lemma 4 thus we omit its proof.\nNow we are ready to prove Theorem 1.\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n19\n\nProof of Theorem 1: Observe that the output of the online solution at time t \u2208 {l + 1, . . . , 2l} is\n\nxt (p\u0302l ) as long as the constraints are not violated. By Lemma 5 and Lemma 6, with probability at\nleast 1 \u2212 2\u01eb:\n\n2l\nX\n\nt=l+1\n2l\nX\nt=1\n\nait xt (p\u0302l ) \u2264\n\nl\nbi ,\nn\n\nfor all i \u2208 {1, . . . , m}, l \u2208 L\n\n\u03c0t xt (p\u0302l ) \u2265 (1 \u2212 2hl \u2212 \u01eb)OPT2l\n\n\u0012\n\n\u0013\n2l\nb ,\nn\n\nfor all l \u2208 L.\n\nDenote this event by E , where P (E ) \u2265 1 \u2212 2\u01eb. The expected objective value achieved by the online\nalgorithm can be bounded as follows:\n\"\n#\n2l\nX X\nE\n\u03c0t xt\nl\u2208L t=l+1\n\n\u2265E\n\u2265\n\u2265\n\u2265\n\u2265\n\u2265\n\n\"\n2l\nX X\n\nX\nl\u2208L\n\nl\u2208L t=l+1\n\nE\n\n\" 2l\nX\nt=1\n\n\u03c0t xt (p\u0302l )I(E )\nl\n\n#\n\n#\n\n\u03c0t xt (p\u0302 )I(E ) \u2212\n\nX\nl\u2208L\n\nE\n\n\" l\nX\nt=1\n\nl\n\n\u03c0t xt (p\u0302 )I(E )\n\n#\n\n\u0013\n\u0015 X \u0014\n\u0015\n\u0012 \u0013\n\u0012\n\u0014\nl\n2l\nb I(E ) \u2212\nb I(E )\nE OPTl\n(1 \u2212 2hl \u2212 \u01eb)E OPT2l\nn\nn\nl\u2208L\nl\u2208L\n\u0013\n\u0015\n\u0013\n\u0015\n\u0012\n\u0012\n\u0014\nX \u0014\nX\n2l\n2l\nb I(E ) \u2212 \u01eb\nb I(E ) \u2212 E [OPT\u01ebn (\u01ebb)I(E )]\nP (E ) * OPT \u2212\nE OPT2l\n2hl E OPT2l\nn\nn\nl\u2208L\nl\u2208L\n\u0013\u0015\n\u0013\u0015\n\u0014\n\u0012\n\u0012\nX \u0014\nX\n2l\n2l\nb \u2212\u01eb\nE OPT2l\nb \u2212 E [OPT\u01ebn (\u01ebb)]\n(1 \u2212 2\u01eb)OPT \u2212\n2hl E OPT2l\nn\nn\nl\u2208L\nl\u2208L\nX hl l\nXl\n(1 \u2212 2\u01eb)OPT \u2212 4\nOPT \u2212 2\u01eb\nOPT \u2212 \u01ebOPT\nn\nn\nl\u2208L\nl\u2208L\nX\n\n\u2265 (1 \u2212 15\u01eb)OPT.\n\nThe third inequality is due to Lemma 6, the second to last inequality is due to Lemma 7 and the\nlast inequality follows from the fact that\nXl\nX l\nX\n= (1 \u2212 \u01eb), and\nhl = \u01eb\nn\nn\nl\u2208L\nl\u2208L\nl\u2208L\nTherefore, Theorem 1 is proved.\n\nr\n\nl\n\u2264 2.5\u01eb.\nn\n\u0003\n\n4. Worst-case Bound for any Algorithm\nIn this section, we prove Theorem 2, i.e., the condition B \u2265 \u03a9(log m/\u01eb2 ) is necessary for any online\nalgorithm to achieve a competitive ratio of 1 \u2212 O(\u01eb). We prove this by constructing an instance of\n(1) with m items and B units of each item such that no online algorithm can achieve a competitive\nratio of 1 \u2212 O(\u01eb) unless B \u2265 \u03a9(log m/\u01eb2 ).\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n20\n\nIn this construction, we refer to the 0 \u2212 1 vectors at 's as demand vectors, and \u03c0t 's as profit\n\ncoefficients. Assume m = 2z for some integer z. We will construct z pairs of demand vectors such\nthat the demand vectors in each pair are complement to each other, and do not share any item.\n\nHowever, every set of z vectors consisting of exactly one vector from each pair will share at\nleast one common item. To achieve this, consider the 2z possible boolean strings of length z. The\nj th boolean string represents j th item for j = 1, . . . , m = 2z (for illustrative purpose, we index the\nitem from 0 in our later discussion). Let sij denote the value at ith bit of the j th string. Then, we\nconstruct a pair of demand vectors v i , wi \u2208 {0, 1}m , by setting vij = sij , wij = 1 \u2212 sij .\n\n0\n1\n2\n3\n4\n5\n6\n7\n\nDemand vectors\nv3\nv2\nv1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n1\n0\n0\n1\n0\n1\n1\n1\n0\n1\n1\n1\nTable 2\n\nItems\n\nItems\n\nTable 2 illustrates this construction for m = 8 (z = 3):\n\n0\n1\n2\n3\n4\n5\n6\n7\n\nDemand vectors\nw3 w2 w1\n1\n1\n1\n1\n1\n0\n1\n0\n1\n1\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\nIllustration of the worst-case bound\n\nNote that the pair of vectors v i , wi , i = 1, . . . , z are complement to each other. Consider any set of\nz demand vectors formed by picking exactly one of the two vectors v i and w i for each i = 1, . . . , z.\nThen form a bit string by setting sij = 1 if this set has vector v i and 0 if it has vector wi . Then, all\nthe vectors in this set share the item corresponding to the boolean string. For example, in Table\n2, the demand vectors v 3 , w 2 , w 1 share item 4(=\u2032 100\u2032 ), the demand vectors w 3 , v 2 , v 1 share item\n3(=\u2032 011\u2032 ) and so on.\nNow, we construct an instance consisting of\n\u2022 B/z inputs with profit coefficient 4 and demand vector v i , for each i = 1, . . . , z.\n\n\u2022 qi inputs with profit 3 and demand vector wi , for each i, where qi is a random variable following\n\nBinomial(2B/z, 1/2).\np\n\u2022\nB/4z inputs with profit 2 and demand vector wi , for each i.\n\n\u2022 2B/z \u2212 qi inputs with profit 1 and demand vector wi , for each i.\n\nUsing the properties of demand vectors ensured in the construction, we prove the following claim:\nClaim 1. Let ri denote the number of vectors of type w i accepted by any 1 \u2212 \u01eb competitive solution\n\nfor the constructed example. Then, it must hold that\nX\n|ri \u2212 B/z | \u2264 7\u01ebB.\ni\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n21\n\nProof. Let OPT denote the optimal value of the offline problem. And let OPTi denote the profit\nobtained from demands accepted of type i. Let topwi (k) denote the sum of profits of top k inputs\nwith demand vector w i .z Then\nz\nz\nX\nX\nX\ntopwi (B/z).\n(4B/z + topwi (B/z)) = 4B +\nOPTi \u2265\nOPT =\ni=1\n\ni=1\n\ni=1\n\n[ be the objective value of a solution which accepts ri vectors of type w i . First, note that\nLet OPT\nP\ni ri \u2264 B. This is because all w i s share one common item, and there are at most B units of this\nitem available. Let Y be the set {i : ri > B/z }, and X be the remaining i's, i.e. X = {i : ri \u2264 B/z }.\nP\nThen, we show that the total number of accepted v i s cannot be more than B \u2212 i\u2208Y ri + |Y |B/z.\n\nObviously, the set Y cannot contribute more than |Y |B/z v i s. Let S \u2286 X contribute the remaining\nvi s. Now consider the item that is common to all w i s in set Y and v i s in the set S (there is at least\n\none such item by construction). Since only B units of this item are available, the total number of\nP\nvi s contributed by S cannot be more than B \u2212 i\u2208Y ri . Therefore the number of accepted v i s is\nP\nless than or equal to B \u2212 i\u2208Y ri + |Y |B/z.\nP\nP\nDenote P = i\u2208Y ri \u2212 |Y |B/z, M = |X |B/z \u2212 i\u2208X ri . Then, P, M \u2265 0. And the objective value\n[ \u2264\nOPT\n\u2264\n\nz\nX\ni=1\n\nz\nX\ni=1\n\ntopwi (ri ) + 4(B \u2212\n\nX\ni\u2208Y\n\nri + |Y |B/z)\n\ntopwi (B/z) + 3P \u2212 M + 4(B \u2212 P )\n\n= OPT \u2212 P \u2212 M.\nSince OPT \u2264 7B, this means that, P + M must be less than 7\u01ebB in order to get an approximation\n\nratio of 1 \u2212 \u01eb or better.\n\n\u0003\n\nHere is a brief description of the remaining proof. By construction, for every i, there are exactly\n\n2B/z demand vectors w i that have profit coefficients 1 and 3, and among them each has equal\nprobability to take value 1 or 3. Now, from the previous claim, in order to get a near-optimal\nsolution, one must select close to B/z demand vectors of type w i . Therefore, if the total number\nof (3, wi ) inputs are more than B/z, then selecting any (2, wi ) will cause a loss of 1 in profit as\np\ncompared to the optimal profit; and if the total number of (3, wi ) inputs are less than B/z \u2212 B/4z,\nthen rejecting any (2, wi ) will cause a loss of 1 in profit. Using the central limit theorem, at any\nstep, both these events can happen with a constant probability. Thus, every decision for (2, wi )\np\nmight result in a loss with constant probability, which results in a total expected loss of \u03a9( B/z)\n\u221a\nfor every i, that is, a total loss of \u03a9( zB).\np\nIf the number of wi s to be accepted is not exactly B/z, some of these B/z decisions may\n\nnot be mistakes, but as in the claim above, such cases cannot be more than 7\u01ebB. Therefore, the\n\nexpected value of online solution,\n\u221a\nONLINE \u2264 OPT \u2212 \u03a9( zB \u2212 7\u01ebB).\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n22\n\nSince OPT \u2264 7B, in order to get (1 \u2212 \u01eb) approximation factor, we need\n\u03a9(\n\np\nz/B \u2212 7\u01eb) \u2264 7\u01eb \u21d2 B \u2265 \u03a9(z/\u01eb2 ) = \u03a9(log(m)/\u01eb2 ).\n\nThis completes the proof of Theorem 2. A detailed exposition of the steps used in this proof\nappears in Appendix C.\n\n5. Extensions\nWe provide a few extensions of our results in this section.\n5.1. Online multi-dimensional linear program\nWe consider the following more general online linear programs with multi-dimensional decisions\nxt \u2208 Rk at each step, as defined in (3) in Section 1:\nPn\nmaximize Pt=1 f Tt xt\nn\nsubject to t=1 g Tit xt \u2264 bi , i = 1, . . . , m\nT\nxt e \u2264 1, xt \u2265 0, t = 1, . . . , n\nxt \u2208 Rk ,\nt = 1, . . . , n.\n\n(15)\n\nOur online algorithm remains essentially the same (as described in Section 3), with xt (p) now\ndefined as follows:\nxt (p) =\n\n\u001a\n\nP\n0 if for all j, ftj \u2264 i pi gitj\nP\ner otherwise, where r \u2208 arg maxj (ftj \u2212 i pi gitj ).\n\nHere er is the unit vector with 1 at the rth entry and 0 otherwise. And we break ties arbitrarily\nin our algorithm. Using the complementarity conditions of (15), and the lower bound condition on\nB as assumed in Theorem 3, we can prove the following lemmas.4 The proofs are very similar to\nthe proofs for the one-dimensional case, and will be provided in Appendix D.\nLemma 8. Let x\u2217 and p\u2217 be the optimal primal and dual solutions to (15) respectively. Then x\u2217t\nand xt (p\u2217 ) differs for at most m values of t.\nLemma 9. Define p and q to be distinct if and only if xt (p) 6= xt (q) for some t. Then, there are\nat most nm k 2m distinct price vectors.\n\nWith the above lemmas, the proof of Theorem 3 will follow exactly as the proof for Theorem 1.\n5.2. Online integer programs\nFrom the definition of xt (p) in (9), our algorithm always outputs integer solutions. And since the\ncompetitive ratio analysis compares the online solution to the optimal solution of the corresponding\nlinear programming relaxation, the competitive ratio stated in Theorem 1 also holds for the online\ninteger programs. The same observation holds for the general online linear programs introduced in\nSection 5.1 since it also outputs integer solutions.\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n23\n\n5.3. Fast solution for large linear programs by column sampling\nApart from online problems, our algorithm can also be applied for solving (offline) linear programs\nthat are too large to consider all the variables explicitly. Similar to the one-time learning online\nsolution, one could randomly sample of \u01ebn variables, and use the dual solution p\u0302 for this smaller\nprogram to set the values of variables xj as xj (p\u0302). This approach is very similar to the column\ngeneration method used for solving large linear programs Dantzig (1963). Our result provides the\nfirst rigorous analysis of the approximation achieved by the approach of reducing the linear program\nsize by randomly selecting a subset of columns.\n\n6. Conclusions\nIn this paper, we provide a 1 \u2212 O(\u01eb) competitive algorithm for a general class of online linear\nprogramming problems under the assumption of random order of arrival and some mild conditions\non the right-hand-side input. The conditions we use are independent of the optimal objective value,\nthe objective coefficients, and the distributions of input data.\nOur dynamic learning algorithm works by dynamically updating a threshold price vector at\ngeometric time intervals, where the dual prices learned from the revealed columns in the previous\nperiod are used to determine the sequential decisions in the current period. Our dynamic learning\napproach might be useful in designing online algorithms for other problems.\nThere are many questions for future research. One important question is whether the current\nbound on the size of the right-hand-input B is tight? Currently as we show in this paper, there\nis a gap between our algorithm and the lower bound. Through some numerical experiments, we\nfind that the actual performance of our algorithm is close to the lower bound (see Wang (2012)).\nHowever, we are not able to prove it. Filling that gap would be a very interesting direction for\nfuture research.\nAppendix A: Supporting lemmas for Section 2\nA.1. Hoeffding-Bernstein's Inequality for sampling without replacement\nBy Theorem 2.14.19 in van der Vaart and Wellner (1996):\nLemma 10. Let u1 , u2 , ...ur be random samples without replacement from the real numbers\n{c1 , c2 , ..., cR }. Then for every t > 0,\n\nP\n\nr\nX\ni=1\n\n!\n\n\u0012\nui \u2212 rc\u0304 \u2265 t \u2264 2 exp \u2212\n\nwhere \u2206R = maxi ci \u2212 mini ci , c\u0304 = R1\n\nP\n\ni ci ,\n\n2\n= R1\nand \u03c3R\n\nt2\n2\n2r\u03c3R\n+ t\u2206R\n\nPR\n\ni=1 (ci\n\n\u2212 c\u0304)2 .\n\n\u0013\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n24\n\nA.2. Proof of inequality (12)\nWe prove that with probability 1 \u2212 \u01eb, b\u0302i =\n\nP\n\nt\u2208N\n\nait xt (p\u0302) \u2265 (1 \u2212 3\u01eb)bi given\n\nP\n\nt\u2208S\n\nait xt (p\u0302) \u2265 (1 \u2212\n\n2\u01eb)\u01ebbi . The proof is very similar to the proof of Lemma 2. Fix a price vector p and i. Define\nP\nP\na permutation is \"bad\" for p, i if both (a) t\u2208S ait xt (p) \u2265 (1 \u2212 2\u01eb)\u01ebbi and (b) t\u2208N ait xt (p) \u2264\n(1 \u2212 3\u01eb)bi hold.\n\nDefine Yt = ait xt (p). Then, the probability of bad permutations is bounded by:\n!\n!\nX\nX\nX\nX\nX\nX\nZt = (1 \u2212 3\u01eb)bi\nZt \u2212 \u01eb\nZt \u2265 \u01eb2 bi\nYt \u2264 (1 \u2212 3\u01eb)bi \u2264 P\nP\nYt \u2212 \u01eb\nYt \u2265 \u01eb2 bi\nt\u2208S\n\nt\u2208N\n\nwhere Zt =\n\n(1\u22123\u01eb)bi Yt\nP\nt\u2208N Yt\n\nt\u2208S\n\n\u0012\n\u0013t\u2208N\nbi \u01eb3\n\u01eb\n\u2264 2 exp \u2212\n\u2264\n3\nm * nm\n\nt\u2208N\n\nt\u2208N\n\nin the first inequality and the second inequality is because of Lemma 10 and the\n\nlast inequality follows from that bi \u2265 6m log(n/\u01eb)\n. Summing over nm distinct prices and i = 1, . . . , m,\n\u01eb3\nwe get the desired inequality.\n\n\u0003\n\nAppendix B: Supporting lemmas for Section 3\nB.1. Proof of Lemma 5\nP\nConsider t ait x\u0302t for a fixed i. For ease of notation, we temporarily omit the subscript i. Define\nYt = at xt (p). If x and p are the optimal primal and dual solutions for (13) and its dual respectively,\n\nthen we have:\nl\nX\nt=1\n\nYt =\n\nl\nX\nt=1\n\nat xt (p) \u2264\n\nl\nX\n\nl\nat xt \u2264 (1 \u2212 hl )b .\nn\nt=1\n\nHere the first inequality is because of the definition of xt (p) and Lemma 1. Therefore, the probability\nof \"bad\" permutations for this p, i and l is bounded by:\n!\nl\n2l\nX\nbl X\nbl\nYt \u2264 (1 \u2212 hl ) ,\nP\nYt \u2265\nn\nn\nt=1\nt=l+1\n!\n!\nl\nl\n2l\n2l\n2l\nX\nX\nbl X\n1X\n2bl\n2bl\nhl bl X\n\u2264P\nYt \u2264 (1 \u2212 hl ) ,\nYt \u2212\n+P\n. (16)\nYt \u2265\nYt \u2264\nYt \u2265\n,\nn t=1\nn\n2 t=1\n2 n t=1\nn\nt=1\nt=1\nFor the first term, we first define Zt =\nP\n\nl\nX\n\nn\n\n2blY\nP2l t\n\nt=1 Yt\n\n2l\nbl X\n2bl\nYt \u2264 (1 \u2212 hl ) ,\nYt \u2265\nn t=1\nn\nt=1\n\n!\n\n. It is easy to see that\n\u2264P\n\nAnd furthermore, using Lemma 10, we have\n!\n2l\nl\nX\n2bl\nbl X\n\u2264P\nZt =\nP\nZt \u2264 (1 \u2212 hl) ,\nn t=1\nn\nt=1\n\nl\nX\n\n2l\nbl X\n2bl\nZt \u2264 (1 \u2212 hl ) ,\nZt =\nn t=1\nn\nt=1\n\nl\nX\n\n2l\nbl X\n2bl\nZt \u2264 (1 \u2212 hl)\nZt =\nn t=1\nn\nt=1\n\n!\n\n!\n\n.\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n25\n\nl\nX\n\n2l\n2l\n1X\nbl X\n2bl\nZt \u2212\n\u2264P\nZt \u2265 hl\nZt =\n2 t=1\nn t=1\nn\nt=1\n\u0013\n\u0012\n2\n\u03b4\n\u01eb b\n\u2264\n\u2264 2 exp \u2212\n2 + hl\n2\n\nwhere \u03b4 = m*n\u01ebm *E .\n\n!\n\nFor the second term of (16), we can define the same Zt , and we have\n!\n!\nl\n2l\n2l\n2l\n2l\nl\nX\nX\n1X\n2bl\n2bl\nhl bl X\nhl bl X\n1X\nZt \u2212\nYt \u2264\n\u2264P\nZt =\n,\n,\nYt \u2265\nZt \u2265\nYt \u2212\nP\n2 t=1\n2 n t=1\nn\n2 t=1\n2 n t=1\nn\nt=1\nt=1\n!\nl\n2l\n2l\nX\n1X\nhl bl X\n2bl\nZt \u2212\n\u2264P\nZt =\nZt \u2265\n2 t=1\n2 n t=1\nn\nt=1\n\u0012\n\u0013\n2\n\u01eb b\n\u03b4\n\u2264 2 exp \u2212\n\u2264\n8 + 2hl\n2\nwhere the second to last step is due to Lemma 10 and the last step holds because hl \u2264 1 and the\n\ncondition made on B.\n\nLastly, we define two prices to be distinct the same way as we do in the proof of Lemma 2.2.\nThen we take a union bound over all the nm distinct prices, i = 1, . . . , m, and E values of l, the\nlemma is proved.\n\n\u0003\n\nB.2. Proof of inequality (14)\nThe proof is very similar to the proof of Lemma 5. Fix p, l and i \u2208 {1, . . . , m}, we define \"bad\"\npermutations for p, i, l as those permutations such that all the following conditions hold: (a) p = p\u0302l ,\nP2l\nthat is, p is the price learned as the optimal dual solution for (13), (b) pi > 0, and (c) t=1 ait xt (p) \u2264\nb . We will show that the probability of bad permutations is small.\n(1 \u2212 2hl \u2212 \u01eb) 2l\nn i\n\nDefine Yt = ait xt (p). If p is an optimal dual solution for (13), and pi > 0, then by the KKT\n\nconditions the ith inequality constraint holds with equality. Therefore, by Lemma 1, we have:\nl\nX\nt=1\n\nYt =\n\nl\nX\n\nl\nl\nait xt (p) \u2265 (1 \u2212 hl ) bi \u2212 m \u2265 (1 \u2212 hl \u2212 \u01eb) bi ,\nn\nn\nt=1\n\nwhere the last inequality follows from B = mini bi \u2265\n\"bad\" permutations for p, i, l is bounded by:\nP\n\u2264P\n\nm\n,\n\u01eb2\n\nand l \u2265 n\u01eb. Therefore, the probability of\n\n2l\n2l\nl X\nYt \u2264 (1 \u2212 2hl \u2212 \u01eb) bi\nYt \u2265 (1 \u2212 hl \u2212 \u01eb) bi ,\nn t=1\nn\nt=1\n\nl\nX\n\nl\nX\n\n!\n\n2l\n2l\n1X\n2l\nbi l X\nYt \u2212\nYt \u2264 (1 \u2212 2hl \u2212 \u01eb) bi\nYt \u2265 h l\n2\nn\nn\nt=1\nt=1\nt=1\n\nl\nX\n\n!\n\n2l\n2l\n1X\nbi l X\n2l\nZt \u2212\nZt = (1 \u2212 2hl \u2212 \u01eb) bi\nZt \u2265 hl\n\u2264P\n2\nn\nn\nt=1\nt=1\n\u0012 2 \u0013 t=1\n\u01eb bi\n\u2264 2 exp \u2212\n\u2264 \u03b4,\n2\n\n!\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n26\n\nl \u2212\u01eb)2lbi Yt\nP\nwhere Zt = (1\u22122h\nand \u03b4 =\nn 2l Y\nt=1\n\nt\n\n\u01eb\n.\nm*nm *E\n\nwe take a union bound over all the n\nthat with probability 1 \u2212 \u01eb\n\n2l\nX\nt=1\n\nm\n\nThe last inequality follows from the condition on B. Next,\n\ndistinct p's, i = 1, . . . , m, and E values of l, we conclude\n\nait x\u0302t (p\u0302l ) \u2265 (1 \u2212 2hl \u2212 \u01eb)\n\n2l\nbi\nn\n\nfor all i such that p\u0302i > 0 and all l.\n\n\u0003\n\nAppendix C: Detailed steps for Theorem 2\nLet c1 , . . . , cn denote the n customers. For each i, the set Ri \u2286 {c1 , . . . , cn } of customers with bid\nvector w i and bid value 1 or 3 is fixed with |Ri | = 2B/z for all i. Conditional on set Ri the bid\nvalues of customers {cj , j \u2208 Ri } are independent random variables that take value 1 or 3 with equal\nprobability.\nNow consider the tth bid of (2, wi ). In at least 1/2 of the random permutations, the number\nof bids from set Ri before the bid t is less than B/z. Conditional on this event, with a constant\nprobability the bids in Ri before t take values such that the bids after t can make the number\np\nof (3, wi ) bids more than B/z with a constant probability and less than B/z \u2212 B/4z with a\n\nconstant probability. This probability calculation is similar to the one used by Kleinberg (2005) in\nhis proof of the necessity of condition B \u2265 \u03a9(1/\u01eb2 ). For completeness, we derive it in the Lemma\n11 towards the end of the proof.\n\nNow, in the first type of instances (in which the number of (3, wi ) bids are more than B/z),\nretaining a (2, wi ) bid is a \"potential mistake\" of size 1; similarly, in the second type of instances\n(in which the number of (3, wi ) bids are less than B/z), skipping a (2, wi ) bid is a potential\nmistake of size 1. We call it a potential mistake of size 1 because it will cost a profit loss of 1 if the\nonline algorithm decides to pick B/z of w i bids. Among these mistakes, |ri \u2212 B/z | of them may\nbe recovered in each instance by deciding to pick ri 6= B/z of wi bids.\np\n\u221a\nThe total expected number of potential mistakes is \u03a9( Bz) (since there are B/4z of (2, wi )\n\nbids for every i). By Claim 1, no more than a constant fraction of instances can recover more than\n7\u01ebB of the potential mistakes.\nLet ONLINE denote the expected value for the online algorithm over random permutation and\nrandom instances of the problem. Therefore,\n\u221a\nONLINE \u2264 OPT \u2212 \u03a9( zB \u2212 7\u01ebB).\n\nNow, observe that OPT \u2264 7B. This is because by construction every set of demand vectors (consisting of either v i or wi for each i) will have at least 1 item in common, and since there are only B\nunits of this item available, at most 2B demand vectors can be accepted giving a profit of at most\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n27\n\np\n7B. Therefore, ONLINE \u2264 OPT(1 \u2212 \u03a9( z/B \u2212 7\u01eb)), and in order to get (1 \u2212 \u01eb) approximation\nfactor we need\n\n\u03a9(\n\np\nz/B \u2212 7\u01eb) \u2264 O(\u01eb) \u21d2 B \u2265 \u03a9(z/\u01eb2 ).\n\nThis completes the proof of Theorem 2.\n\nLemma 11. Consider 2k random variables Yj , j = 1, . . . , 2k that take value 0/1 independently with\nP2k\nequal probability. Let r \u2264 k.Then with constant probability Y1 , . . . , Yr take value such that j=1 Yj\n\u221a\ncan be greater or less than its expected value k by k/2 with equal constant probability.\n!\n2k\nX\n\u221a\nYj \u2212 k \u2265 \u2308 k/2\u2309 Y1 , . . . Yr \u2265 c\nP\nj=1\n\nfor some constant 0 < c < 1.\nProof of Lemma 11:\n\u221a\nP\n\u2022 Given r \u2264 k, | j\u2264r Yj \u2212 r/2| \u2264 k/4 with constant probability (by central limit theorem).\n\u221a\nP\n\u2022 Given r \u2264 k, | j>r Yj \u2212 (2k \u2212 r)/2)| \u2265 3 k/4 with constant probability.\n\u221a\nP\nGiven the above events | j Yj \u2212 k | \u2265 k/2, and by symmetry both events have equal probability.\n\n\u0003\n\nAppendix D: Online multi-dimensional linear program\nD.1. Proof of Lemma 8\nUsing Lagrangian duality, observe that given optimal dual solution p\u2217 , optimal solution x\u2217 is given\nby:\n\nP\nmaximize f Tt xt \u2212 i p\u2217i g Tit xt\nsubject to eT x \u2264 1, xt \u2265 0.\n\n(17)\n\nTherefore, it must be true that if x\u2217tr = 1, then r \u2208 arg maxj {ftj \u2212 (p\u2217 )T g tj } and ftr \u2212 (p\u2217 )T g tr \u2265 0\nThis means that for t's such that maxj {ftj \u2212 (p\u2217 )T g tj } is strictly positive and arg maxj returns a\n\nunique solution, xt (p\u2217 ) and x\u2217t are identical. By random perturbation argument there can be at\n\nmost m values of t that does not satisfy this condition (for each such t, p satisfies an equation\nftj \u2212 pT g tj = ftl \u2212 pT g tl for some j, l, or ftj \u2212 pT g tj = 0 for some j). This means x\u2217t and xt (p\u2217 )\ndiffers for at most m values of t.\n\n\u0003\n\nD.2. Proof of Lemma 9\nConsider nk 2 expressions\nftj \u2212 pT gtj \u2212 (ftl \u2212 pT gtl ), 1 \u2264 j, l \u2264 k, j 6= l, 1 \u2264 t \u2264 n\nftj \u2212 pT gtj , 1 \u2264 j \u2264 k,\n1 \u2264 t \u2264 n.\nxt (p) is completely determined once we determine the subset of expressions out of these nk 2\nexpressions that are assigned a non-negative value. By theory of computational geometry, there\ncan be at most (nk 2 )m such distinct assignments.\n\n\u0003\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n28\n\nEndnotes\n1. The assumption that aij \u2264 1 is not restrictive as we can normalize the constraint to meet this\nrequirement.\n2. An example to show the knowledge of n is necessary to obtain a near-optimal algorithm is as\nfollows. Suppose there is only one product and the inventory is n. And all ai 's are 1. There might\nbe n or 2n arrivals. And in either case, half of them have value 1 and half of them have value 2.\nNow consider any algorithm. If it accepts less than 2/3 among the first n arrivals, the loss is at\nleast n/3 (or 1/6 of the optimal value) if in fact there are n arrivals in total. On the other hand,\nif it accepts more than 2/3 among the first n arrivals, then it must have accepted more than n/6\nbids with value 1. And if the true number of arrivals is 2n, then it will also have a loss of at least\n1/12 of the true optimal value. Thus if one doesn't know the exact n, there always exists a case\nwhere the loss is a constant fraction of the true optimal value.\n3. Here we assume the search engines use a pay-per-impression scheme. The model can be easily\nadapted to a pay-per-click scheme by multiplying the bid value by the click-through-rate parameters. Also we assume there is only one advertisement slot for each search result.\n4. Here we make an assumption similar to Assumption 3. That is, for any p, there can be at\nP\nmost m arrivals such that there are ties in ftj \u2212 i pi gitj . As argued in the discussions following\nAssumption 3, this assumption is without loss of generality.\n\nAcknowledgments\nThe authors thank the two anonymous referees and the associate editor for their insightful comments and\nsuggestions.\n\nReferences\nAwerbuch, B., Y. Azar, S. Plotkin. 1993. Throughput-competitive on-line routing. FOCS'93: Proceedings of\nthe 34th Annual IEEE Symposium on Foundations of Computer Science. 32\u201340.\nBabaioff, M., N. Immorlica, D. Kempe, R. Kleinberg. 2007. A knapsack secretary problem with applications.\nApproximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, Lecture\nNotes in Computer Science, vol. 4627. 16\u201328.\nBabaioff, M., N. Immorlica, D. Kempe, R. Kleinberg. 2008. Online auctions and generalized secretary\nproblems. SIGecom Exch. 7(2) 1\u201311.\nBahmani, B., M. Kapralov. 2010. Improved bounds for online stochastic matching. ESA'10: Proceedings of\nthe 18th annual European conference on Algorithms: Part I . 170\u2013181.\nBitran, G., R. Caldentey. 2003. An overview of pricing models for revenue management. Manufacturing and\nService Operations Management 5(3) 203\u2013229.\n\n\fAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\n29\n\nBorodin, A., R. El-Yaniv. 1998. Online computation and competitive analysis. Combridge University Press.\nBuchbinder, N., J. Naor. 2009a. The design of competitive online algorihms via a primal-dual approch.\nFoundations and Trends in Theoretical Computer Sciences 3(2-3) 93\u2013263.\nBuchbinder, N., J. Naor. 2009b. Online primal-dual algorithms for covering and packing. Mathematics of\nOperations Research 34(2) 270\u2013286.\nCesa-Bianchi, N., G. Lugosi. 2006. Prediction, learning, and games. Cambridge University Press.\nCooper, W. L. 2002. Asymptotic behavior of an allocation policy for revenue management. Operations\nResearch 50(4) 720\u2013727.\nDantzig, G. 1963. Linear programming and extensions. Princeton University Press.\nDevanur, N. 2011. Online algorithms with stochastic input. SIGecom Exch. 10(2) 40\u201349.\nDevanur, N., T. Hayes. 2009. The adwords problem: online keyword matching with budgeted bidders under\nrandom permutations. EC'09: Proceedings of the 10th ACM conference on Electronic Commerce. 71\u201378.\nDevanur, N., K. Jain, B. Sivan, C. Wilkens. 2011. Near optimal online algorithms and fast approximation algorithms for resource allocation problems. EC'11: Proceedings of the 12th ACM conference on\nElectronic Commerce. 29\u201338.\nElmaghraby, W., P. Keskinocak. 2003. Dynamic pricing in the presence of inventory considerations: research\noverview, current practices and future directions. Management Science 49(10) 1287\u20131389.\nFeldman, J., M. Henzinger, N. Korula, V. Mirrokni, C. Stein. 2010. Online stochastic packing applied to\ndisplay ad allocation. Algorithms\u2013ESA 2010 182\u2013194.\nFeldman, J., N. Korula, V. Mirrokni, S. Muthukrishnan, M. Pal. 2009a. Online ad assignment with free\ndisposal. WINE'09: Proceedings of the 5th Workshop on Internet and Network Economics. 374\u2013385.\nFeldman, J., A. Mehta, V. Mirrokni, S. Muthukrishnan. 2009b. Online stochastic matching: beating 1 1/e. FOCS'09: Proceedings of the 50th Annual IEEE Symposium on Foundations of Computer Science.\n117\u2013126.\nGallego, G., G. van Ryzin. 1994. Optimal dynamic pricing of inventories with stochastic demand over finite\nhorizons. Management Science 40(8) 999\u20131020.\nGallego, G., G. van Ryzin. 1997. A multiproduct dynamic pricing problem and its application to network\nyield management. Operations Research 45(1) 24\u201341.\nGoel, G., A. Mehta. 2008. Online budgeted matching in random input models with applications to adwords.\nSODA'08: Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms. 982\u2013991.\nKarande, C., A. Mehta, P. Tripathi. 2011. Online bipartite matching with unknown distributions. STOC'11:\nProceedings of the 43rd annual ACM symposium on Theory of Computing. 587\u2013596.\nKleinberg, R. 2005. A multiple-choice secretary algorithm with applications to online auctions. SODA'05:\nProceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms. 630\u2013631.\n\n\f30\n\nAgrawal, Wang and Ye: A Dynamic Near-Optimal Algorithm for Online Linear Programming\nArticle submitted to Operations Research; manuscript no. (Please, provide the manuscript number!)\n\nMahdian, M., Q. Yan. 2011. Online bipartite matching with random arrivals: an approach based on strongly\nfactor-revealing LPs. STOC'11: Proceedings of the the 43rd annual ACM symposium on Theory of\nComputing. 597\u2013606.\nMehta, A., A. Saberi, U. Vazirani, V. Vazirani. 2005. Adwords and generalized on-line matching. FOCS'05:\nProceedings of the 46th Annual IEEE Symposium on Foundations of Computer Science. 264\u2013273.\nMolinaro, M., R. Ravi. 2014. Geometry of online packing linear programs. Mathematics of Operations\nResearch 39(1) 46\u201359.\nOrlik, P., H. Terao. 1992. Arrangement of hyperplanes. Grundlehren der Mathematischen Wissenschaften\n[Fundamental Principles of Mathematical Sciences], Springer-Verlag, Berlin.\nSimpson, R. W. 1989. Using network flow techniques to find shadow prices for market and seat inventory\ncontrol. MIT Flight Transportation Laboratory Memorandum M89-1, Cambridge, MA .\nTalluri, K., G. van Ryzin. 1998. An analysis of bid-price controls for network revenue management. Management Science 44(11) 1577\u20131593.\nvan der Vaart, A., J. Wellner. 1996. Weak convergence and empirical processes: with applications to statistics\n(Springer Series in Statistics). Springer.\nWang, Z. 2012. Dynamic learning mechanism in revenue management problems. Ph.D. thesis, Stanford\nUniversity, Palo Alto.\nWilliamson, E. L. 1992. Airline network seat control. Ph. D. Thesis, MIT, Cambridge, MA .\n\n\f"}