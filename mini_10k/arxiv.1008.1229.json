{"id": "http://arxiv.org/abs/1008.1229v1", "guidislink": true, "updated": "2010-08-06T16:29:55Z", "updated_parsed": [2010, 8, 6, 16, 29, 55, 4, 218, 0], "published": "2010-08-06T16:29:55Z", "published_parsed": [2010, 8, 6, 16, 29, 55, 4, 218, 0], "title": "Cosmology, initial conditions, and the measurement problem", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1008.4432%2C1008.2725%2C1008.5210%2C1008.1826%2C1008.2776%2C1008.0487%2C1008.5232%2C1008.0244%2C1008.3574%2C1008.0902%2C1008.5187%2C1008.1891%2C1008.2105%2C1008.1367%2C1008.3547%2C1008.2554%2C1008.3857%2C1008.4540%2C1008.3475%2C1008.2769%2C1008.1016%2C1008.1940%2C1008.2296%2C1008.2384%2C1008.3815%2C1008.3671%2C1008.2619%2C1008.2680%2C1008.1967%2C1008.2967%2C1008.3669%2C1008.3587%2C1008.5142%2C1008.4298%2C1008.2202%2C1008.0212%2C1008.5208%2C1008.2763%2C1008.4978%2C1008.3036%2C1008.0033%2C1008.1503%2C1008.1965%2C1008.2799%2C1008.1399%2C1008.1450%2C1008.2149%2C1008.0188%2C1008.3383%2C1008.0650%2C1008.3880%2C1008.1035%2C1008.3212%2C1008.0672%2C1008.4418%2C1008.3480%2C1008.4975%2C1008.5138%2C1008.0397%2C1008.0608%2C1008.3154%2C1008.4710%2C1008.5012%2C1008.2200%2C1008.3610%2C1008.3107%2C1008.2288%2C1008.1922%2C1008.4688%2C1008.3757%2C1008.3956%2C1008.1713%2C1008.3287%2C1008.4731%2C1008.0535%2C1008.1308%2C1008.4204%2C1008.3848%2C1008.3966%2C1008.0117%2C1008.3507%2C1008.4125%2C1008.1628%2C1008.0480%2C1008.2171%2C1008.2051%2C1008.3328%2C1008.0609%2C1008.2643%2C1008.1602%2C1008.1676%2C1008.3979%2C1008.4018%2C1008.1976%2C1008.4549%2C1008.1362%2C1008.3598%2C1008.1373%2C1008.1229%2C1008.3118%2C1008.4336&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Cosmology, initial conditions, and the measurement problem"}, "summary": "The assumption that a complete description of an early state of the universe\ndoes not privilege any position or direction in space leads to a unified\naccount of probability in cosmology, macroscopic physics, and quantum\nmechanics. Such a description has a statistical character. Deterministic laws\nlink it to statistical descriptions of the cosmic medium at later times, and\nbecause these laws do not privilege any position or direction in space, the\nsame must be true of these descriptions. If the universe is infinite, we can\nidentify the probability that the energy density at a particular instant and a\nparticular point in space (relative to a system of spacetime coordinates in\nwhich the postulated spatial symmetries are manifest) lies in a given range\nwith the fractional volume occupied by points where the energy density lies in\nthis range; and similarly with all other probabilities that figure in the\nstatistical description. The probabilities that figure in a complete\ndescription of the cosmic medium at any given moment thus have an exact and\nobjective physical interpretation. The statistical entropy and the information\nassociated with each cosmological probability distribution are likewise\nobjective properties of the universe, defined in terms of relative frequencies\nor spatial averages.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1008.4432%2C1008.2725%2C1008.5210%2C1008.1826%2C1008.2776%2C1008.0487%2C1008.5232%2C1008.0244%2C1008.3574%2C1008.0902%2C1008.5187%2C1008.1891%2C1008.2105%2C1008.1367%2C1008.3547%2C1008.2554%2C1008.3857%2C1008.4540%2C1008.3475%2C1008.2769%2C1008.1016%2C1008.1940%2C1008.2296%2C1008.2384%2C1008.3815%2C1008.3671%2C1008.2619%2C1008.2680%2C1008.1967%2C1008.2967%2C1008.3669%2C1008.3587%2C1008.5142%2C1008.4298%2C1008.2202%2C1008.0212%2C1008.5208%2C1008.2763%2C1008.4978%2C1008.3036%2C1008.0033%2C1008.1503%2C1008.1965%2C1008.2799%2C1008.1399%2C1008.1450%2C1008.2149%2C1008.0188%2C1008.3383%2C1008.0650%2C1008.3880%2C1008.1035%2C1008.3212%2C1008.0672%2C1008.4418%2C1008.3480%2C1008.4975%2C1008.5138%2C1008.0397%2C1008.0608%2C1008.3154%2C1008.4710%2C1008.5012%2C1008.2200%2C1008.3610%2C1008.3107%2C1008.2288%2C1008.1922%2C1008.4688%2C1008.3757%2C1008.3956%2C1008.1713%2C1008.3287%2C1008.4731%2C1008.0535%2C1008.1308%2C1008.4204%2C1008.3848%2C1008.3966%2C1008.0117%2C1008.3507%2C1008.4125%2C1008.1628%2C1008.0480%2C1008.2171%2C1008.2051%2C1008.3328%2C1008.0609%2C1008.2643%2C1008.1602%2C1008.1676%2C1008.3979%2C1008.4018%2C1008.1976%2C1008.4549%2C1008.1362%2C1008.3598%2C1008.1373%2C1008.1229%2C1008.3118%2C1008.4336&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The assumption that a complete description of an early state of the universe\ndoes not privilege any position or direction in space leads to a unified\naccount of probability in cosmology, macroscopic physics, and quantum\nmechanics. Such a description has a statistical character. Deterministic laws\nlink it to statistical descriptions of the cosmic medium at later times, and\nbecause these laws do not privilege any position or direction in space, the\nsame must be true of these descriptions. If the universe is infinite, we can\nidentify the probability that the energy density at a particular instant and a\nparticular point in space (relative to a system of spacetime coordinates in\nwhich the postulated spatial symmetries are manifest) lies in a given range\nwith the fractional volume occupied by points where the energy density lies in\nthis range; and similarly with all other probabilities that figure in the\nstatistical description. The probabilities that figure in a complete\ndescription of the cosmic medium at any given moment thus have an exact and\nobjective physical interpretation. The statistical entropy and the information\nassociated with each cosmological probability distribution are likewise\nobjective properties of the universe, defined in terms of relative frequencies\nor spatial averages."}, "authors": ["David Layzer"], "author_detail": {"name": "David Layzer"}, "author": "David Layzer", "arxiv_comment": "42 pages, no figures", "links": [{"href": "http://arxiv.org/abs/1008.1229v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1008.1229v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "astro-ph.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1008.1229v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1008.1229v1", "journal_reference": null, "doi": null, "fulltext": "Cosmology, initial conditions, and the measurement problem\nDavid Layzer\nDepartment of Astronomy, Harvard University, 60 Garden Street, Cambridge, MA 02478\n(Submitted 30 June 2010)\nThe assumption that a complete description of an early state of the universe does not\nprivilege any position or direction in space leads to a unified account of probability in\ncosmology, macroscopic physics, and quantum mechanics. Such a description has a\nstatistical character. Deterministic laws link it to statistical descriptions of the cosmic\nmedium at later times, and because these laws do not privilege any position or\ndirection in space, the same must be true of these descriptions. If the universe is\ninfinite, we can identify the probability that the energy density at a particular instant\nand a particular point in space (relative to a system of spacetime coordinates in which\nthe postulated spatial symmetries are manifest) lies in a given range with the\nfractional volume occupied by points where the energy density lies in this range; and\nsimilarly with all other probabilities that figure in the statistical description. The\nprobabilities that figure in a complete description of the cosmic medium at any given\nmoment thus have an exact and objective physical interpretation. The statistical\nentropy and the information associated with each cosmological probability\ndistribution are likewise objective properties of the universe, defined in terms of\nrelative frequencies or spatial averages.\n\nThe initial states of macroscopic systems are characterized by probability\ndistributions of microstates, linked by a deterministic historical account to probability\ndistributions that characterize the early universe. The probability distributions that\ncharacterize the initial states of macroscopic systems are usually deficient in\nparticular kinds of microscopic information -- though the history of a macroscopic\nsystem may include an experimental intervention that creates a hidden form of\nmicroscopic information, as in Hahn\u201fs spin-echo experiment. These general\n\n1\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nconclusions clarify the relation between quantum mechanics and quantum statistical\nmechanics. A macroscopic system\u201fs history determines the statistical ensembles that\nrepresent its macrostates and, absent experimental interventions that produce hidden\nmicroscopic order, justifies what van Kampen has called the \"repeated randomness\nassumptions\" needed to derive master equations and H theorems.\n\nI give a schematic account of quantum measurement that combines von Neumann\u201fs\ndefinition of an ideal measurement and his account of \"premeasurements\" with the\nconclusion that the initial state of a measuring apparatus is fully characterized by a\nhistorically determined probability distribution of microstates that is deficient in\nmicroscopic information. To comply with the postulated requirement that physical\ndescriptions must not privilege our (or any other) position, I interpret the postmeasurement probability distribution of quantum states of the combined system as\ndescribing the state of an ensemble of identical measuring experiments uniformly\nscattered throughout the universe. The resulting account predicts (there is no need to\ninvoke von Neumann\u201fs collapse postulate) that each apparatus in the cosmological\nensemble registers a definite outcome and leaves the quantum system in the\ncorresponding eigenstate of the measured quantity. Because the present account\nconflates the indeterminacy of measurement outcomes with the indeterminacy of the\ncombined system\u201fs position, it reconciles the indeterminacy of quantum measurement\noutcomes with the deterministic character of Einstein\u201fs field equations, thereby\ninvalidating a standard argument for the need to quantize general relativity. A more\ndetailed account of measurement would allow for the effects of decoherence, but as\nJoos and Zeh, Schlosshauer, and others have emphasized, decoherence theory alone\ndoes not explain why quantum measurements have definite outcomes.\n\nFinally, because the initial states of actual macroscopic systems may be objectively\ndeficient in certain kinds of microscopic information, macroscopic processes that are\nsensitive to initial conditions may have objectively indeterminate outcomes. In such\nprocesses, as in quantum measurements, a probability distribution that defines a\n2\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nsingle initial macrostate may evolve (deterministically) into a probability distribution\nthat assigns finite probabilities to two or more macrostates. Examples include chaotic\norbits in celestial mechanics, evolving weather systems, and processes that generate\ndiversity in such biological processes as biological evolution and the immune\nresponse.\n\nPACS numbers: 03.65.Ta, 02.50.-r, 5.30.-d\n\n3\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nI. INTRODUCTION\nInitial conditions characterize systems to which given physical laws apply and the\nconditions under which they apply. Wigner [1] has suggested that a \"minimal set of\ninitial conditions not only does not permit any exact relations between its elements; on\nthe contrary, there is reason to contend that these are, or at some time have been, as\nrandom as the externally imposed gross constraints allow.\" As an illustration of such\ninitial conditions, he cites a version of Laplace\u201fs nebular hypothesis, in which a\nstructureless, spinning gas cloud evolves into a collection of planets revolving in the\nsame sense around a central star in nearly circular, nearly coplanar orbits. The gross\nconstraints in this example include the cloud\u201fs initial mass, angular momentum, and\nchemical composition. \"More generally [Wigner writes], one tries to deduce almost all\n\u201eorganized motion,\u201f even the existence of life, in a similar fashion.\" This paper seeks to\nground this view of initial conditions in a historical account that links the initial\nconditions that characterize physical systems and their environments to the initial\nconditions that characterize a statistically uniform and isotropic model of the universe at\na time shortly after the beginning of the cosmic expansion.\nIn such a universe, particle reaction rates exceed the cosmic expansion rate at\nsufficiently early times [2]. Consequently, local thermal equilibrium prevails at the\ninstantaneous and rapidly changing values of temperature and mass density. Suppose\nthat there is a system of spacetime coordinates relative to which no statistical property of\nthe cosmic medium defines a preferred position or direction. Then the values of a small\nnumber of physical parameters, notably the photon-baryon and lepton-baryon ratios [3],\nsuffice to determine the state of the cosmic medium at early times. This paper explores\nsome consequences of the assumption that this characterization of the early cosmic\nmedium is exact and complete \u2013 that it contains only statistical information and is\ninvariant under spatial translations and rotations. For example, a uniform, unbounded\ndistribution of elementary particles in thermal equilibrium is completely characterized by\nits temperature and mass density.\n\n4\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nThe following discussion does not seek to extend quantum mechanics or general\nrelativity beyond their current domains of validity. Rather, it seeks to show how these\ndomains merge smoothly with the macroscopic domain of statistical physics. I argue that\nmacroscopic systems that interact sufficiently weakly with their surroundings can be\nassigned definite macrostates characterized by probability distributions of microstates\nthat have evolved from probability distributions that characterized the cosmic medium\nearly on. By virtue of a strong version of the cosmological principle, discussed in the\nnext section, these probability distributions represent objective indeterminacy rather than\nignorance. The same strong version of the cosmological principle requires one to\ninterpret the classical fields that figure in relativistic cosmology as random fields, just as\nin the theory of homogeneous turbulence (except that in the cosmological context the\nprobability distributions that characterize the random fields represent objective\nindeterminacy). I will argue (in section III.E) that this way of indirectly linking quantum\nmechanics to general relativity enables one to circumvent a standard argument for the\nneed to quantize gravity. Because the argument in section III.E assumes conditions in\nwhich quantum mechanics and relativistic cosmology are both valid, it doesn\u201ft bear\ndirectly on the need for a theory that applies under more extreme conditions (the very\nearly universe, black holes). On the other hand, an account of initial conditions that\nserves to reconcile quantum mechanics and general relativity under present conditions\ncould conceivably prove to be relevant to the much larger project of unifying the two\ntheories.\n\nII. THE STRONG COSMOLOGICAL PRINCIPLE\nThe cosmological principle states that there exists a system of spacetime coordinates\nrelative to which no statistical property of the physical universe defines a preferred\nposition or direction in space. It embraces Hubble\u201fs \"principle of uniformity,\" which\napplies to the observable distribution of galaxies, and Einstein\u201fs model of the universe as\na uniform, unbounded distribution of dust [35]. Like the assumptions that define\nidealized models of stars and galaxies, it has usually been viewed as a convenient\n5\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nsimplification. Unlike those assumptions, however, the cosmological principle could\nhold exactly. I will refer to the hypothesis that it does hold exactly as the strong\ncosmological principle [4], [5]. It draws support from the following considerations.\nFirst, precise and extensive observations of the cosmic microwave background and of\nthe spatial distribution and line-of-sight velocities of galaxies have so far produced no\nevidence of deviations from statistical homogeneity and isotropy. Astronomical\nobservations provide no positive support for the view that the cosmological principle is\nmerely an approximation or an idealization, like the initial conditions that define models\nof stars and galaxies.\nSecond, the initial conditions that define the universe do not have the same function\nas those that define models of astronomical systems. A theory of stellar structure must\napply to a range of stellar models because stars have a wide range of masses, chemical\ncompositions, spins, and ages. No analogous requirement obtains for models of the\nuniverse.\nFinally, general relativity predicts that local reference frames that are unaccelerated\nrelative to the cosmological coordinate system defined by the cosmological principle are\ninertial [4]. Astronomical evidence supports this prediction. It indicates that local\ninertial reference frames are indeed unaccelerated relative to a coordinate system in\nwhich the cosmic microwave background is equally bright, on average, in all directions\nand in which the spatial distribution of galaxies is statistically homogeneous and\nisotropic. If the distribution of energy and momentum on cosmological scales were not\nstatistically homogeneous and isotropic, there would be no preferred cosmological frame\nand hence no obvious explanation for the observed relation between local inertial frames\nand the frame defined by the cosmic microwave background and the spatial distribution\nand line-of-sight velocities of galaxies.\nCosmological models that conform to the cosmological principle are characterized\nby parameters, such as the photon-to-baryon and lepton-baryon ratios, the curvature\nscale, and the cosmological constant. Some theories for the origin of structure also posit\nprimordial density fluctuations whose spectrum and amplitude are characterized by\nparameters that do not privilege particular spatial positions or directions. Because current\nphysical laws have translational and rotational spatial symmetry (relative to a\n6\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\ncosmological coordinate system in which the description of the cosmic medium is\ninvariant under spatial translations and rotations or relative to a local inertial coordinate\nsystem), the strong cosmological principle follows from the assumption that a model that\nhas these symmetries completely describes the early universe at a single moment of time.\nThe strong cosmological principle is inconsistent with classical microphysics. In a\nuniform, unbounded, infinite distribution of classical particles, every particle is uniquely\nsituated with respect to its neighbors, or even with respect to its nearest neighbor, because\nthe number of pairs of nearest neighbors is countably infinite whereas the number of\npossible (real) values of the ratio between two nearest-neighbor separations is\nuncountably infinite. By contrast, a complete quantum description of a uniform,\nunbounded distribution of non-interacting particles specifies a number density and a set\nof single-particle occupation numbers for each kind of particle. The following discussion\nwill bring to light other links between cosmology and quantum physics.\n\nA. Statistical cosmology\nNon-uniform cosmological models that conform to the strong cosmological principle\nrepresent the distribution of mass, temperature, chemical composition, and other\nmacroscopic variables by random functions of position. Each random function is\ncharacterized by an infinite set of probability distributions each of which is invariant\nunder spatial translations and rotations. For example, the spatial distribution of mass at a\nparticular moment is specified by the probability distribution of the mass density at a\nsingle point, the joint-probability distribution of the mass densities at two given points,\nthe joint-probability distribution of the mass densities at three given points, and so on.\nThe probability distribution of the mass density at the point x \uf03d (x1 , x 2 , x 3 ) is independent\nof x:\nPr{a \uf0a3 \uf072(x) \uf0a3 a \uf02b da} \uf03d p(a)da ,\n\n7\n\n(1)\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nwhere p(a)da , the probability that the mass density lies in the interval (a, a \uf02b da) , is the\nsame at all points. Similarly, the joint-probability distribution of the mass densities at\ntwo distinct points,\nPr{a \uf0a3 \uf072(x1 ) \uf0a3 a \uf02b da,b \uf0a3 \uf072(x2 ) \uf0a3 b \uf02b db} \uf03d p(a,b; x1 \uf02d x2 )dadb ,\n\n(2)\n\ndepends only on the distance x1 \uf02d x2 between the points but not on the direction of the\nvector joining them, and so on.\nThe probabilities that figure in a description that conforms to the strong cosmological\nprinciple can be defined in terms of spatial averages. For example, Pr{a \uf0a3 \uf072(x) \uf0a3 a \uf02b da}\nis the mean, or spatial average, of the function\n\n\uf064 ( \uf072(x);a, a \uf02b da) \uf03d\n\n\uf072 \uf0ce(a, a \uf02b da)\n.\n\uf07b 10ifotherwise\n\n(3)\n\nThat is, the probability that the mass density at a given point lies in a given interval is the\nfractional volume (defined below) occupied by points at which the mass density satisfies\nthis condition. Similarly, the joint probability that the mass densities at two points x,\nx + y lie in given intervals is the fractional volume occupied by points x at which this\ncondition, with y fixed, is satisfied; and so on.\nMore generally, let I denote a function that, like \uf064 in (3), takes the value 1 at points\nwhere a given condition is satisfied and takes the value 0 at points where the condition is\nnot satisfied. The probability that the condition is satisfied is the spatial average of I, or\nfractional volume of the (infinite) region occupied by points where the condition is\nsatisfied. To define this spatial average, let v(V) denote the integral of I over a region of\nvolume V. If the ratio v(V)/V approaches a limit as V increases indefinitely and if this\nlimit doesn\u201ft depend on the shape or location of the regions that figure in the definition,\nwe define it as the spatial average of I.\nIn more detail, assuming for simplicity that space is Euclidean, consider a hierarchy\nof progressively coarser Cartesian grids. Let C k(n ) denote the kth cell belonging to the\n8\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\ngrid of level n; k is an integer. The members of a cell belonging to level 0 of the\nhierarchy of grids are points in space. The members of cells belonging to level n \uf02b1 are\ncells of level n, and each cell of level n \uf02b1 contains 27 cells of level n and is centered on\none of them. Let I(n, k) denote the average value of I in cell C k(n ) . Let \uf065 n denote the\nleast upper bound of the absolute value of the differences \uf0e9\uf0eb I(n, k) \uf02d I(n, k \uf0a2 )\uf0f9\uf0fb for all pairs\n\nk, k \uf0a2 and a fixed n. We now stipulate that \uf065 n \uf0ae 0 as n \uf0ae \uf0a5 . Then the I(n, k) for a\ngiven n approach a common limit as n \uf0ae \uf0a5 , which we identify with the spatial average\nof I. We take the stipulation that \uf065 n \uf0ae 0 as n \uf0ae \uf0a5 to be part of the definition of\nstatistical homogeneity.\nIf we identify point sets in space (at a given moment in cosmic time) with events in a\nsample space, and define the probability of an event as the fractional volume of the set of\npoints where a specified condition is satisfied, the axioms of probability theory become\ntrue statements about point sets in physical space at a given moment of cosmic time,\nprovided our description of the cosmic medium satisfies the strong cosmological\nprinciple (now taken to include the preceding stipulation about spatial averages). Thus\nthe present definition of probability provides a model of axiomatic probability theory.\nThe model goes beyond the axiomatic definition because it captures the intuitive notion\nof indeterminacy associated with probability. For example, the mass density at a given\npoint is objectively indeterminate, because a complete description of the cosmic medium\nthat complies with the strong cosmological principle doesn\u201ft contain the value of the\nmass density at that point.\nIt is instructive to compare the present definition of probability with the definitions\nthat figure in standard accounts of kinetic theory and statistical mechanics. Maxwell and\nBoltzmann assumed that every molecule in a finite sample of an ideal gas is in a definite\nbut unknown molecular state. They identified the probability of a molecular state with\nthe fraction of molecules in that state. Gibbs [6] represented the macroscopic states of a\nsystem in thermodynamic equilibrium by probability distributions of (classical)\nmicrostates. He identified the probability of a range of microstates with the relative\nfrequency of that range in an ensemble, a collection of imaginary replicas of the system,\neach in a definite microstate. But he did not \u2013 as some later authors have done \u2013 assume\n9\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nthat the macroscopic system in a macrostate characterized by a probability distribution of\nmicrostates was actually in one of these microstates. As discussed below, the present\napproach, like Gibbs\u201fs, characterizes macrostates by probability distributions of\nmicrostates, and it does not assume that the system is in one of these microstates. On the\ncontrary, the probability distribution that characterizes a macrostate characterizes it\ncompletely.\n\n1. Statistical entropy\nIn a probabilistic description of the cosmic medium, statistical entropy provides an\nobjective (and, as Shannon showed) essentially unique measure of randomness. The\nstatistical entropy S of a discrete probability distribution \uf07bpi \uf07d is given by [7]:\n\nS \uf028\uf07bpi \uf07d\uf029\uf03d \uf02d\uf0e5 pi log pi .\n\n(4)\n\ni\n\nShannon proved that the three following properties of the function S define it uniquely.\n(i) S is non-negative and vanishes only when one of the pi is equal to 1. (ii) S takes its\nlargest value, log n, just in case there are n non-vanishing probabilities pi , each equal to\n1/n. (iii) S is hierarchically decomposable, in the sense defined by (7) below. The last\nproperty plays a key role in the following discussion. To derive (7) from (4), think of the\nindex \uf061 as the label of a macrostate consisting of microstates (\uf061 i) whose probabilities\npi(\uf061 ) for a fixed value of \uf061 add up to p (\uf061 ) :\n\np(\uf061 ) \uf03d \uf0e5 pi(\uf061 ) .\ni\uf0ce\uf061\n\nThe conditional probability pi \uf061 of the microstate i, given that it belongs to the\nmacrostate \uf061, is defined by the identity\n\n10\n\n(5)\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\npi(\uf061 ) \uf03d pi \uf061 p(\uf061 )\n\n(6)\n\nFrom (4) \u2013 (6) it follows that\n\nS\n\n\uf028\uf07bp \uf07d\uf029\uf03d S \uf028\uf07bp \uf07d\uf029\uf02b \uf0e5 p S \uf028\uf07bp \uf07d\uf029.\n(\uf061 )\ni\n\n(\uf061 )\n\n(\uf061 )\n\n\uf061\n\ni\uf061\n\n(7)\n\n\uf07b \uf07d\nentropy, is the sum of the coarse-grained statistical entropy S \uf028\uf07bp \uf07d\uf029 and a residual\n\nThat is, the statistical entropy of the probability distribution pi(\uf061 ) , often called the Gibbs\n(\uf061 )\n\nstatistical entropy, equal to the weighted average of the statistical entropies associated\nwith the microstructures of the macrostates.\nIf the microstates refer to an isolated system, the Gibbs entropy is constant in time (in\nclassical statistical mechanics on account of Liouville\u201fs theorem, in quantum statistical\nmechanics because microstates evolve deterministically). So if the coarse-grained\nstatistical entropy increases with time, the residual statistical entropy must decrease at the\nsame rate. Gibbs proposed a suitably defined coarse-grained entropy as the \"analogue\"\nof thermodynamic entropy; but, as discussed below, the universal validity of this\ninterpretation has been challenged.\n\n2. Continuous probability distributions\nTo apply Shannon\u201fs definition (4) to a continuous probability distribution {p(x)} , one\npartitions the range of x into discrete intervals \uf044 i and sets pi equal to the integral of\n\np(x) over \uf044 i . The value of S({pi }) depends on how one chooses the intervals \uf044 i . It\nincreases without limit as the size of an interval shrinks to zero.\nBoltzmann and Gibbs defined the statistical counterpart of entropy as an integral,\n\n\uf02d \uf0f2 log p(x) \uf0d7 p(x)dx ,\n\n11\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nwhere p(x) is the probability per unit phase-space volume at a point x in \uf06d- or \uf047-space. In\neffect, they partitioned phase space into cells of equal volume. Since the value of\nlog p(x) depends on how one chooses the unit of phase-space volume, the preceding\nformula does not assign a unique value to S. Like thermodynamic entropy, statistical\nentropy in Boltzmann\u201fs and Gibbs\u201fs theories is defined only up to an additive constant.\nHowever, if we divide each cell in phase space into k equal parts, the integral\ndecreases by an amount log k, so the difference between the values of S for probability\ndistributions defined on the same partition of phase space approaches a finite limit as the\nvolume of a cell shrinks to zero. In quantum statistical mechanics, statistical entropy, as\ndefined by (4), has a well-defined zero point, because bounded systems have discrete sets\nof possible quantum states.\n\n3. Information\nShannon used negative entropy (negentropy), the discrete counterpart of Boltzmann\u201fs H,\nas a measure of information. In the present context it is more convenient to define the\ninformation I of a discrete probability distribution as the amount by which the\ndistribution\u201fs statistical entropy S falls short of the largest value of S that is consistent\nwith prescribed values for such quantities as the mean energy and the mean concentration\nof a chemical constituent [4]:\nI \uf0ba Smax \uf02d S\n\n(8)\n\nThis definition is convenient because, as discussed below, in cosmological contexts the\nlargest allowed value of the statistical entropy per unit mass may increase with time; so\nprocesses that generate statistical entropy may also generate statistical information.\nI is non-negative and vanishes when S \uf03d Smax ; a maximally random probability\ndistribution has zero information. If we interpret statistical entropy as a measure of\n\n12\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nrandomness or disorder, as is customary, we can interpret information as a measure of a\ndeficiency of disorder, or order.\nLike S, I is not uniquely defined for continuous probability distributions; it depends\non how we partition the continuous space of events on which the distribution is defined\ninto cells. But once the space has been partitioned into cells, I, unlike S, approaches a\nfinite limit when we divide each cell into k equal cells and let k increase indefinitely. In\nclassical statistical mechanics, acceptable phase-space grids have cells of equal phasespace volume. As the cell size approaches 0, I approaches a finite limit (whereas S blows\nup).\nIn quantum statistical mechanics, S and I are both well defined. Because a region of\nphase space whose volume V is much larger than the volume v of a quantum cell (h in \uf06dspace, hN in \uf047-space) contains V/v quantum states, classical and quantum estimates of I\nagree whenever the classical estimate applies.\nFinally, follows easily from its definition that information, like statistical entropy, is\nhierarchically decomposable:\n\nI\n\n\uf028\uf07bp \uf07d\uf029\uf03d I \uf028\uf07bp \uf07d\uf029\uf02b \uf0e5 p I \uf028\uf07bp \uf07d\uf029\n(\uf061 )\ni\n\n(\uf061 )\n\n(\uf061 )\n\n\uf061\n\ni\uf061\n\n(9)\n\nwhere each occurrence of I is defined by the appropriate version of (8). The second term\nrepresents residual information. The first term represent information associated with\nwhat Wigner, in the passage quoted at the beginning of this paper, called externally\nimposed gross constraints. His suggestion that initial conditions are as random as the\nexternally imposed gross constraints allow implies that the second term vanishes. One\naim of this paper is to ground this suggestion in a historical account of the initial\nconditions that define macroscopic systems and their surroundings.\n\n13\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nB. The growth of order [4], [5]\nStandard accounts of the early universe assume that at the earliest times when current\ntheories of elementary particles apply, the cosmic medium was a uniform, uniformly\nexpanding gas containing \"a great variety of particles in thermal equilibrium ... .\"\n[2, p. 528] At these early times the relative concentrations of particle kinds and the\ndistributions of particle energies were characterized by maximally random probability\ndistributions. And though the mass density and the temperature were changing rapidly,\nthe rates of particle encounters and reactions were high enough to keep the relative\nconcentrations of all particle species close to the equilibrium values appropriate to the\ninstantaneous values of the temperature and the mass density.\nAs the universe expanded, both its rate of expansion and the rates of particle\nencounters decreased, but the latter decreased faster than the former. As a result, some\nkinds of equilibrium ceased to prevail, and the corresponding probability distributions\nceased to be maximally random. For example, in the standard evolutionary scenario,\nwhich assumes that the cosmic microwave background is primordial, matter and radiation\ndecoupled when their joint temperature fell low enough for hydrogen to recombine.\nThereafter, the matter temperature and the radiation temperature declined at different\nrates. So while matter-radiation interactions tended to equalize the matter and radiation\ntemperatures, generating entropy in the process, the cosmic expansion drove the two\ntemperatures farther apart, generating information. Earlier, the relative concentrations of\nnuclides were frozen in when the rates of nuclear reactions became too slow relative to\nthe rate of cosmic expansion to maintain their equilibrium values. Again, competition\nbetween the cosmic expansion and nuclear reactions generated both entropy and\ninformation.\nLocal, as well as global, gravitational processes tend to disrupt thermodynamic\nequilibrium. The first astronomical systems may have formed when the uniform\ndistribution of mass and temperature became unstable against the growth of density\nfluctuations, or else when primordial density fluctuations reached critical amplitudes. But\nunlike isolated gas samples, newly formed self-gravitating systems did not settle into\nequilibrium states. For example, a gas cloud of stellar mass has negative heat capacity:\n14\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nits temperature rises as it loses energy through radiation. As the cloud continues to\nradiate and contract, the disparities between its mean density and its mean temperature\nand those of and those of its surroundings steadily increase. Within the cloud, gradients\nof temperature and mass density become progressively more marked. In a mature star,\nnuclear reactions in the core gradually alter its chemical composition, thereby producing\nchemical inhomogeneity \u2013 another variety of disequilibrium.\n\n1. Cosmology and the second law of thermodynamics\nI have argued that while local processes drive local conditions toward local\nthermodynamic equilibrium, the cosmic expansion and the contraction of self-gravitating\nastronomical systems drive local conditions away from local thermodynamic equilibrium,\ncreating both information and entropy. This argument raises the questions: In what\ndomain is entropy defined? And in what domain is the Second Law valid?\nIn classical thermodynamics, entropy is initially defined for systems in\nthermodynamic equilibrium. This definition is then extended to systems in local, but not\nglobal, thermodynamic equilibrium and to composite systems whose components are\nindividually in global or local thermodynamic equilibrium. The Boltzmann-GibbsShannon definition of statistical entropy allows us to extend the thermodynamic\ndefinition to any physical state defined by a probability distribution. Can the domain in\nwhich the Second Law is valid be likewise extended?\nGibbs showed that the statistical entropy of the probability distribution of classical\nmicrostates that defines a macrostate of a closed system is constant in time; the same is\ntrue of a probability distribution of classical microstates of a closed system. Boltzmann\u201fs\nH theorem extends the Second Law to arbitrary non-equilibrium states of a sample of an\nideal gas, but his proof of the theorem assumes that the residual statistical information\n(see Eq. 7) of the probability distribution that characterizes the sample\u201fs macrostate\nvanishes at all times. This assumption cannot be weakened. It isn\u201ft enough to assume\nthat the residual information (i.e., information associated with molecular correlations)\nvanishes initially. Boltzmann\u201fs proof and Gibbs\u201fs proof of the constancy of a closed\n15\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nsystem\u201fs statistical entropy show that molecular interactions do not destroy singleparticle information; they convert it into residual information; and Poincar\u00e9\u201fs theorem\nshows that in a closed system residual information residual information eventually makes\nits way back into single-particle information.\nBoltzmann\u201fs assumption that residual statistical information is permanently absent\nexemplifies what van Kampen has called \"repeated randomness assumptions.\" As\ndiscussed in more detail below, he argued that the H theorem exemplifies a wide class of\ntheorems about stochastic processes that rely on versions of this assumption. I argue\nbelow that these assumptions, when valid, are justified by historical arguments. It\nfollows that H theorems \u2013 extensions of the Second Law to macroscopic systems whose\nmacrostates can be characterized by probability distributions of quantum states \u2013 are\nhistorical generalizations, depend on historically justified initial and boundary conditions.\nCan the notion of entropy and the law of entropy growth be extended to selfgravitating systems? Consider a self-gravitating gas cloud. We can regard a sufficiently\nsmall region of the cloud as a gas sample in a uniform external gravitational field, and\nconclude from the Second Law that physical processes occurring within it generate\nentropy. If the cloud\u201fs large-scale structure is not changing too rapidly, we can then\nconclude that local thermodynamic equilibrium prevails. We can, if we wish, define the\nentropy of the cloud as the sum of the entropies of its parts. But this definition does not\ncontain a gravitational contribution. There is no consensus that such a contribution exists\nand no widely accepted view of how it might be defined if it does exist. Presumably it\nwould be non-additive, like the gravitational contribution to the cloud\u201fs energy.\nThe gravitational contribution to the energy of a self-gravitating cloud causes the\ncloud to behave very differently from a closed gas sample in the laboratory. Although\nheat flow, mediated by collisions between gas molecules, tends to reduce temperature\ndifferences between adjacent regions, the cloud does not relax into a state of uniform\ntemperature. (Indeed, such a state doesn\u201ft exist for a cloud of finite mass.) Instead, the\ncloud evolves toward a state of dynamical (quasi-)equilibrium in which the gravitational\nforce acting on each element balances the resultant of the pressure forces on the\nelement\u201fs boundary. The virial theorem, applied to a self-gravitating system in\ndynamical equilibrium, implies that the system has negative heat capacity: its mean\n16\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\ntemperature increases as the cloud loses energy through radiation. As the cloud\ncontracts, the internal gradients of temperature, density, and pressure become steeper.\nThe non-gravitational contribution to the cloud\u201fs entropy decreases.\n\n2. Initial conditions for macroscopic systems\nEvery observer sees the world from his or her own point of view. An account of the\nuniverse and its evolution that incorporates the strong cosmological principle privileges\nnone of these local perspectives. In a sense, it includes all of them. Yet it cannot contain\nor predict initial conditions that characterize individual physical systems, such as the Sun.\nTo make such a prediction the cosmological account would have to contain a sentence\nlike \"Relative to a specified coordinate system, the object whose center of mass is at x at\ntime t has such-and-such properties.\" But it cannot contain such a sentence, because\nthere is no preferred coordinate system. The cosmological account has no way of\nsingling out individual systems.\nOn the other hand, every observer acquires information about individual systems \u2013\ninformation that isn\u201ft in the perspectiveless cosmological account. Where does that\ninformation come from if it wasn\u201ft in the (supposedly) complete cosmological account to\nbegin with? As Szilard [8] pointed out long ago, the entropy generated by an\nobservational process more than offsets the information delivered by the observation.\nPart of the process consists in converting information in information-rich fuel sources\n(batteries, ATP molecules) into qualitatively new forms (improved estimates of the Sun\u201fs\nmass). So we can acquire an indefinite quantity of data that characterize the view from\nEarth. Our description realizes one of infinitely many possible descriptions, centered on\ndifferent points in space. All these descriptions contain the same statistical information.\nThe perspectiveless cosmological account contains only that information.\nEvery macroscopic system and its surroundings are characterized by probability\ndistributions of microstate. Such a probability distribution is determined by its history,\nwhich began in the early universe. But the history of every macroscopic system also\ncontains local contributions. In particular, the history of a system on a laboratory bench\n17\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nincludes an account of how the system and its surroundings were prepared. I will argue\nbelow that macroscopic systems are rarely found in definite microstates (the argument is\ndue to Zeh [32]) but may be, and often are, found or prepared in definite macrostates.\nThis argument lies at the heart of the solution to the measurement problem described\nbelow.\n\nIII. CHANCE IN THE MACROSCOPIC DOMAIN\nIn the 1880s Poincar\u00e9 discovered that some orbits in self-gravitating systems are\nextremely sensitive to initial conditions. In a popular essay published twenty years later\n[9], he suggested that extreme sensitivity to initial conditions is the defining characteristic\nof what is now called deterministic chaos. The outcomes of such processes, Poincar\u00e9\nargued, are predictable in principle but unpredictable in practice.\nPoincar\u00e9\u201fs first example was a cone initially balanced on its tip. Any external\ndisturbance, no matter how small, causes the cone to topple in a direction that is\nunpredictable in practice. The following argument suggests that even in the absence of\nexternal disturbances and even if we neglect Heisenberg\u201fs uncertainty principle, the\ndirection of fall is unpredictable in principle as well as in practice.\nClassical mechanics represents the cone\u201fs possible microstates by points in a fourdimensional phase space whose coordinates are the elevation and azimuth of the cone\u201fs\naxis and the conjugate angular momenta. But according to the arguments of \u00a7II, the\ncone\u201fs initial macrostate is represented not by a single point in this phase space but by a\nprobability distribution of possible microstates. This probability distribution must\ncontain a finite quantity of information; its support must have finite phase-space volume,\nwhich in general will greatly exceed\n\n2\n\n, the limit set by Heisenberg\u201fs uncertainty\n\nrelation. If the support includes the point that represents the initial state of unstable\nequilibrium, it contains classical phase-space trajectories that correspond to all possible\ndirections of fall, and the final orientation of the axis is unpredictable in principle. From\na description of the experimental setup one could in principle infer the probability\ndistribution of classical microstates that characterizes the cone\u201fs initial macrostate.\n18\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nClassical mechanics would allow us to calculate the evolution of each microstate and\nhence to calculate the probability distribution of final microstates; and a description of\nthe apparatus that records the final orientation of the cone\u201fs axis would allow us to\npartition this distribution into a distribution of macrostates. As discussed below, an\nanalogous schema applies to quantum measurements.\nThe probability distributions that characterize the initial states of macroscopic\nsystems depend on their history. Hence there can be no genuine laws about initial\nconditions, only historical generalizations. For example, macroscopic systems cannot\nusually be prepared in definite quantum states; but physicists have succeeded in preparing\nsuperconducting quantum interference devices (SQUIDs) in superpositions of\nmacroscopically distinct quantum states. Again, macroscopic systems are usually\nmicroscopically disordered; but Hahn\u201fs spin-echo experiment showed that this is not\nnecessarily the case.\nNevertheless, the argument of \u00a7II suggests that many natural processes have\nunpredictable outcomes. This conclusion, if correct, would bring the physicists\u201f\nworldview closer not only to the worldview of ordinary experience, in which chance\nseems to play a major role, but also to that of biology. Chance plays a key role in\nevolution: genetic variation, which generates candidates for natural selection, has a\nrandom component, and the histories of individuals, populations, and species are strongly\ninfluenced by apparently unpredictable fluctuations in their environments. Some\ndevelopmental processes \u2013 the immune response, visual perception, and some learning\nstrategies, for example \u2013 likewise rely on processes with unpredictable outcomes [10].\n\nA. Equilibrium statistical mechanics\nMost modern presentations of classical and quantum statistical mechanics follow the\nmathematical approach pioneered by J. W. Gibbs [6], in which probability distributions\nof an undisturbed N-particle system\u201fs microstates represent equilibrium macrostates and\nprobability-weighted averages of microscopic quantities represent thermodynamic\nvariables. Quantum statistical mechanics represents macroscopic states and variables in\n19\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nexactly the same way. Let the probability distribution { p(k\uf061 ) } of quantum states k\ncharacterize a macrostate \uf061, and let O be an observable. The statistical counterpart of O\nis\n\nO\n\n(\uf061 )\n\n\uf0ba \uf0e5 pk(\uf061 ) k O k \uf03d Tr (\uf072 (\uf061 )O)\n\n(10)\n\nk \uf0ce\uf061\n\nwhere\n\n\uf072(\uf061 ) \uf03d \uf0e5 k pk(\uf061 ) k\n\n(11)\n\nk\uf0ce\uf061\n\nO\n\n(\uf061 )\n\nis the value of the macroscopic variable O in the macrostate \uf061. We discuss the\n\nphysical interpretation of (10) in \u00a7III.C below.\nThe statistical counterpart of entropy is statistical entropy:\n\nS(\uf061 ) \uf03d \uf02d\uf0e5 pk(\uf061 ) log pk(\uf061 )\n\n(12)\n\nk\uf0ce\uf061\n\nGibbs called his statistical descriptions \"analogies.\" He showed that the canonical\nprobability distribution,\n\npk \uf03d e\uf02d Ek /kT / \uf0e5 e\n\n\uf02d E j /kT\n\n,\n\n(13)\n\nj\n\nmaximizes the statistical entropy, subject to the constraint that the mean energy has a\nprescribed value, represented by the thermodynamic variable E. The reciprocal of the\nLagrange multiplier associated with this constraint is the absolute (Kelvin) temperature T.\nGibbs showed that the thermodynamic identity TdS \uf03d dE \uf02b PdV obtains in the statistical\ndescription based on the canonical distribution. H also showed that the statistical\ndescription based on the microcanonical distribution, in which the probability distribution\n20\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nis uniform over the (6N \u2013 1)-dimensional region of phase whose points represent\nmicrostates with a given energy, mimics the thermodynamic description less successfully.\nSome authors have tried to derive statistical mechanics from mechanics. These\nauthors favor the microcanonical distribution, because they assume that an undisturbed\nmacroscopic system is in a definite microstate, and hence has a definite energy. They\nreplace Gibbs\u201fs averages over phase space (or, in quantum statistical mechanics,\nmicrostates) by time averages. Although this approach has generated interesting\nmathematics, it is limited to equilibrium statistical mechanics, and even in that context it\nrelies on questionable ad hoc assumptions.\nOther authors have sought to base statistical mechanics on an epistemic argument:\n[M]acroscopic observers, such as we are, are under no circumstances capable of\nobserving, let alone measuring, the microscopic dynamic state of a system which\ninvolves the determination of an enormous number of parameters, of the order of\n1023. ... [Thus] a whole ensemble of possible dynamical states corresponds to the\nsame macroscopic state, compatible with our knowledge. [11]\n\nAs mentioned above, Gibbs proved that the statistical entropy of the canonical\ndistribution exceeds that of any other distribution with the same mean energy. E. T.\nJaynes [12], [13] made this theorem the cornerstone of an elegant formulation of\nstatistical mechanics that interprets statistical entropy as a measure of missing\nknowledge. He argued that \"statistical mechanics [is] a form of statistical inference\nrather than a physical theory.\" Its \"computational rules are an immediate consequence of\nthe maximum-entropy principle,\" which yields \"the best estimates that could have been\nmade on the basis of the information available.\" [11, p. 620] Gibbs, by contrast,\ninterpreted statistical entropy not as a measure of missing knowledge \u2013 he did not identify\nthe macroscopic system he was describing with a definite but unknown member of the\nensemble that characterizes the system\u201fs thermodynamic state \u2013 but as an \"analogue\" of\nthermodynamic entropy. In the present account, the probability distributions that\ncharacterize a macroscopic system and its surroundings characterize them completely and\nobjectively; they have nothing to do with what we know or don\u201ft know about the system\n21\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nand its surroundings; and statistical entropy is not a measure of human ignorance but of\nobjective randomness.\nGibbs also proved that the canonical probability distribution characterizes subsystems\nof an extended isolated system characterized by a microcanonical probability distribution.\nThat is, it characterizes a macroscopic system in a heat bath. Many modern authors (e.g.,\nSchr\u00f6dinger [14]) justify the canonical distribution on these grounds. The foundational\nproblem then shifts its focus from the canonical (or grand canonical) distribution to the\nmicrocanonical distribution and becomes the problem of justifying the ergodic\nhypothesis.\nThe considerations of \u00a7II supply an objective version of Jaynes\u201fs maximum-entropy\nprinciple. Jaynes\u201fs principle encapsulates the sound methodological precept that a\nscientific description of a physical system should not outrun what scientists know, or can\nknow, about the system. In the present account, the probability distribution that\nobjectively characterizes a macrostate contains just the information created by the\nphysical processes that shaped that macrostate. In particular, probability distributions\nthat objectively characterize equilibrium states have zero residual information.\nConsequently, Jaynes\u201fs methodological precept normally leads to the same statistical\ndescription of equilibrium states as a historical physical argument based on an\nassumption about the primordial cosmic medium. But it can happen that an objective\nstatistical description of a macrostate contains residual information that an observer is\nunaware of. In this situation, illustrated by Hahn\u201fs spin-echo experiment, a prediction\nbased on the maximum-entropy principle fails.\n\nB. Non-equilibrium Statistical Mechanics\nAccording to the present approach, a complete description of a macroscopic system\u201fs\ninitial state contains just the information created by the system\u201fs history, including its\nmethod of preparation. This supplies a framework \u2013 but only a framework \u2013 for\naddressing what Van Kampen [15] has called \"the main problem in the statistical\n\n22\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nmechanics of irreversible processes\": What determines the choice of macrostates and\nmacroscopic variables?\nVan Kampen [15, 16] has also emphasized the role of \"repeated randomness\nassumptions\" in theories of stochastic processes:\nThis repeated randomness assumption is drastic but indispensable whenever one\ntries to make a connection between the microscopic world and the macroscopic or\nmesoscopic levels. It appears under the aliases \"Stosszahlansatz,\" \"molecular\nchaos,\" or \"random phase approximation,\" and it is responsible for the appearance\nof irreversibility. Many attempts have been made to eliminate this assumption,\nusually amounting to hiding it under the rug of mathematical formalism. [16, p.\n58]\n\nTo derive his transport equation and his H theorem, Boltzmann had to assume (following\nMaxwell) that the initial velocities of colliding molecules in a closed gas sample are\nstatistically uncorrelated. This assumption cannot hold for non-equilibrium states,\nbecause when the information that characterizes a non-equilibrium state decays, residual\ninformation associated with molecular correlations comes into being at the same rate.\nHowever, because a gas sample\u201fs reservoir of microscopic information is so large,\nmolecular chaos may prevail to a good approximation for periods much shorter than the\nPoincar\u00e9 recurrence time, which is typically much longer than the age of the universe.\nAnother approach to the problem of justifying repeated randomness assumptions\nstarts from the remark that no gas sample is an island unto itself. Can the fact that actual\ngas samples interact with their surroundings justify the assumption that correlation\ninformation is permanently absent in a nominally closed gas sample? And if so, is it\nlegitimate to appeal to environmental \"intervention\"?\nFifty years ago, J. M. Blatt [17] argued that the answer to both questions is yes. The\nwalls that contain a gas sample are neither perfectly reflecting nor perfectly stationary.\nWhen a gas molecule collides with a wall, its direction and its velocity acquire tiny\nrandom contributions. These leave the single-particle probability distribution virtually\nunaltered, but they alter the histories of individual molecules, thereby disrupting multi23\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nparticle correlations. Blatt distinguished between states of true equilibrium, characterized\n(in the vocabulary of the present paper) by an information-free probability distribution,\nand quasi-equilibrium states, in which single-particle information is absent but\ncorrelation (or residual) information is present. With the help of a simple thought\nexperiment, Blatt argued that collisions between molecules of a rarefied gas sample and\nthe walls of its container cause an initial quasi-equilibrium state to relax into true\nequilibrium long before the gas has come into thermal equilibrium with the walls.\nEarlier, Bergmann and Lebowitz [18] constructed and investigated detailed\nmathematical models of the relaxation from quasi-equilibrium to true equilibrium through\nexternal \"intervention.\" More recently, Ridderbos and Redhead [19] have expanded\nBlatt\u201fs case for the interventionist approach. They constructed a simple model of the\nspin-echo experiment [20], in which a macroscopically disordered state evolves into a\nmacroscopically ordered state. They argued that in this experiment (and more generally)\ninteraction between a nominally isolated macroscopic system and its environment\nmediates the loss of correlation information.\nBlatt noted that this \"interventionist\" approach \"has not found general acceptance.\"\nThere is a common feeling that it should not be necessary to introduce the wall of\nthe system in so explicit a fashion. ... Furthermore, it is considered unacceptable\nphilosophically, and somewhat \"unsporting,\" to introduce an explicit source of\nrandomness and stochastic behavior directly into the basic equations. Statistical\nmechanics is felt to be a part of mechanics, and as such one should be able to start\nfrom purely causal behavior. [17, p. 747]\n\nThe historical approach sketched in this paper directly addresses these concerns. It\nimplies that statistical mechanics cannot be derived from quantum (or classical)\nmicrophysics, because, as discussed below, macroscopic systems cannot be idealized as\nclosed systems in definite microstates. It assumes that they can be idealized as closed\nsystems in definite macrostates. And it anchors the statistical assumptions about the\ninitial states of macroscopic systems and their environments on which statistical theories\ndepend in a historical narrative based on a simple cosmological assumption.\n24\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nDerivations of master equations that rely on decoherence ([21] and references cited\nthere]) exemplify and significantly extend the interventionist approach. These\nderivations assume (unrealistically) that macroscopic systems are initially in definite\nquantum states but also (realistically) that they interact with random environments.\nUnder these conditions, environmental interactions transfer information very effectively\nfrom the system to its surroundings. Microscopic information that is wicked away from\nthe system disperses outward, eventually getting lost in interstellar and intergalactic\nspace.\n\nC. The origin of irreversibility\nBecause the microstates of undisturbed systems evolve reversibly, a theory that assigns\nmacroscopic systems (or the universe) definite quantum states cannot provide a\nframework for theories that distinguish in an absolute sense between the two directions of\ntime. The historical approach sketched in this paper offers such a framework because by\nlinking the initial states of macroscopic systems to states of the early universe it equips\ntime with an arrow. Specifically, it predicts that macroscopic systems are usually\nembedded in random environments and that their initial states usually contain only\ninformation associated with the values of macroscopic variables. However, as the spinecho experiment shows, macroscopic systems can be prepared in states that contain\nmicroscopic information.\n\nD. Quantum measurements\n1. Cosmological ensembles\nA description of the universe that comports with the strong cosmological principle should\nnot pick out any particular macroscopic system. We therefore assume that macroscopic\ndescriptions refer not to individual systems but to cosmological ensembles [4], [5] \u2013\n25\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\ninfinite collection of identical systems distributed in a statistically uniform way in space.\nWe interpret the density operator \uf072 (\uf061 ) in Eq. (10) as referring to such an ensemble:\n\nInterpret the quantity Tr (\uf072 (\uf061 )O) as the average value of the macroscopic variable\nO\n\n(\uf061 )\n\nin a cosmological ensemble.\n\nAs in (10), O\n\n(\uf061 )\n\n(14)\n\nis the value of O , the macroscopic counterpart of the observable O, in\n\nthe macrostate \uf061 characterized by the density operator \uf072 (\uf061 ) .\nRule (14) resembles a rule given by Dirac [22, pp. 132, 133]:\nIdentify Tr (\uf072O) with the average result of a large number of measurements of O\nwhen the system \"is in one or other of a number of possible states according to\nsome given probability law.\"\n\n(14*)\n\nDirac\u201fs rule (14*) generalizes his interpretation of the matrix element s O s as the\naverage result of a large number of measurements of O when the system is in state s.\nLike the earlier rule, it treats measurement as a primitive concept. In contrast, rules (10)\nand (14) do not mention measurement; it is trivially true that O\n\n(\uf061 )\n\nis the result of\n\nmeasuring O when the system is in the macrostate characterized by \uf072. Nor do (10) and\n(14) presuppose Dirac\u201fs interpretation of s O s . As discussed in \u00a7D.2, (14) allows us\nto derive this interpretation from a modified form of von Neumann\u201fs account of an ideal\nmeasurement, and thus to bring quantum measurement into the domain of quantum\nstatistical mechanics.\nThe density operator \uf072 (\uf061 ) refers to a cosmological ensemble whose members are\nall in the macrostate \uf061. Suppose \uf061 is the initial quantum state of Poincar\u00e9\u201fs cone,\nwhose axis is as nearly vertical and as nearly stationary as Heisenberg\u201fs uncertainty\nrelations allow. If the cone is undisturbed, its microstates evolve deterministically,\n\n26\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nand the density operator that represents its macrostate evolves into a density operator\nthat represents a number of macroscopically distinguishable macrostates:\n\n\uf072 \uf03d \uf0e5 \uf0e5 \uf061 k pk(\uf061 ) \uf061 k\n\uf061\n\nk\n\n(15)\n\n\uf03d \uf0e5 p (\uf061 ) \uf0e5 \uf061 k p k \uf061 \uf061 k\n\uf061\n\nk\n\n(As earlier, \uf061 k denotes the kth microstate of the macrostate of the macrostate \uf061, pk(\uf061 )\ndenotes the probability of the microstate \uf061 k , pk \uf061 \uf03d pk(\uf061 ) / p(\uf061 ) , and p(\uf061 ) \uf03d \uf0e5 pk \uf061 .)\nk\n\nThen\n\nTr (\uf072O) \uf03d \uf0e5 p(\uf061 ) \uf0e5 pi \uf061 \uf061 i O \uf061 i \uf03d \uf0e5 p(\uf061 ) O\n\uf061\n\n(\uf061 )\n\n(16)\n\n\uf061\n\ni\n\nBy (14), this is the ensemble average of the macroscopic counterpart of the observable O.\nIn particular, if P\uf062 is the projector\n\nP\uf062 \uf03d \uf0e5 \uf062 k \uf062 k ,\n\n(17)\n\nTr (\uf072P\uf062 ) \uf03d p(\uf062 )\n\n(18)\n\nk\n\nSince P\uf062 takes the value 1 in microstates that belong to the macrostate \uf062 and the value 0\nin microstates that belong to other macrostates that figure in the definition (15) of \uf072 ,\nTr ( \uf072 P\uf062 ) is the fraction of ensemble members in the macrostate \uf062 . Eq. (18) says that\n\nthis fraction is p ( \uf062 ) .\nThus the ensemble interpretation accounts for the kind of macroscopic indeterminacy\nexemplified by Poincar\u00e9\u201fs thought experiment: To avoid giving special standing to a\nparticular cone, and hence to its position in the universe, we think of it as a member of a\n\n27\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\ncosmological ensemble. If the probability distribution of microstates that defines the\ncone\u201fs initial macrostate does not contain enough information to predict its final\nmacrostate (defined by the orientation of the cone\u201fs axis), it evolves (deterministically)\ninto a probability distribution that can be partitioned into sub-distributions that represent\nmacroscopically distinguishable macrostates. A complete description of the experimental\nsetup would allow one to assign a probability to each of these macrostates. This\nprobability is its relative frequency in a cosmological ensemble. The outcome at a\nparticular place is unpredictable because all members of a cosmological ensemble are on\nthe same footing.\n\n2. Quantum measurements\nVon Neumann\u201fs account of an ideal measurement [23] assumes that the combined system\n(measured object + measuring apparatus) is initially in a definite quantum state; that\nSchr\u00f6dinger\u201fs equation governs the evolution of this state during a measurement; and that\nif the object is initially in an eigenstate of the measured observable, the measurement\nleaves it in that state. It follows from these assumptions that if the object is initially in a\nsuperposition of eigenstates of the measured observable, the final microstates of the\ncombined system are superpositions of macroscopically distinguishable microstates, each\nassociated with one of the measurement\u201fs possible outcomes (as predicted by the\nstandard interpretation rule). These assumptions define an ideal measurement \u2013 or, more\nprecisely, a \"premeasurement,\" which is followed by a collapse of the superposition onto\none of its components.\nIn a historical account of cosmic structure and evolution that comports with the strong\ncosmological principle, macroscopic systems cannot in general be idealized as being in\ndefinite quantum states. They can, however, be idealized as being in definite\nmacrostates. The macrostate of a macroscopic system, such as a measuring apparatus, is\ndefined by a probability distribution of microstates, which is determined by the system\u201fs\nhistory. Because the history of a macroscopic system stretches back to the early universe,\nthe probability distribution that defines its macrostate contains only the information\n28\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\ncreated by the information-generating processes, including experimental preparation, that\nthe system has undergone. In general, the information that characterizes the initial state\nof a measuring apparatus is that associated with the apparatus\u201fs specifications. Now,\nquantum mechanics does not specify the phases of individual microstates. So if a\nmacroscopic system\u201fs history has not created information about the relative phases of the\nmicrostates that belong to its macrostate, as will always be the case for a measuring\napparatus, a complete description of the macrostate will contain no information about\nrelative phases. They will be random.\nDecoherence calculations, by contrast, begin by assuming, with von Neumann, that\nthe combined system is in a definite quantum state. Interaction between the combined\nsystem (S+M) and an environment E that is assumed to be random transfers relativephase information from the superposition that would represent the state of S+M in the\nabsence of environmental interaction to the superposition that represents the state of\nS+M+E.\nSuppose then that the measuring apparatus is initially in a definite macrostate,\ncharacterized by a probability distribution { pi } of quantum states i\n\nM\n\n. Assume that the\n\ninitial state of the measured system S is a superposition, with coefficients c k , of\neigenstates k\n\nS\n\nof an observable K. As in von Neumann\u201fs account, a measurement of K\n\nproduces an entangled quantum state of the combined system:\n\n\uf0e6\n\uf0f6\n\uf0e7\uf0e8 \uf0e5 ck k S \uf0f7\uf0f8 i\nk\n\nM\n\n\uf0ae \uf0e5 ck k\nk\n\nS\n\nj(k,i)\n\nM\n\n\uf0ba \uf0e5 ck k, j(k,i)\n\n(19)\n\nk\n\nHere j(k,i) labels a quantum state of the measuring apparatus. The argument k indicates\nthat it is correlated to the eigenvalue k of the system S.\nLet O denote an observable that refers to the combined system S + M. By (14), the\nensemble average of the macroscopic counterpart of the observable O in the final state is:\n\n29\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\n\uf0e9\uf0ec\n\uf0fc \uf0ec\n\uf0fc\uf0f9\nTr (\uf072O) \uf03d \uf0e5 pi \uf0ea \uf0ed\uf0e5 ck\uf02a\uf0a2 k \uf0a2, j( k \uf0a2,i) \uf0fd O \uf0ed\uf0e5 ck k, j(k,i) \uf0fd \uf0fa\ni\n\uf0fe \uf0ee k\n\uf0fe\uf0fb\n\uf0eb\uf0ee k\uf0a2\n\uf03d \uf0e5 pi \uf0e5 c c k \uf0a2, j( k \uf0a2,i) O k, j(k,i)\n\n(20)\n\n\uf02a\nk\uf0a2 k\n\nk \uf0a2 ,k\n\ni\n\nNow, as Bohr emphasized (and experimental practice confirms), states of the measuring\napparatus are classical states, completely specified by the values of classical variables; a\ncomplete description of the probability distribution \uf07bpi \uf07d that characterizes the measuring\napparatus\u201fs initial state therefore contains no information about the relative phases of the\nquantum states i. As Bohm [22] pointed out long ago, they have random phases. What\nabout the relative phases of quantum states j(k,i)\n\nM\n\n, j( k \uf0a2,i)\n\nM\n\nbelonging to different\n\npointer states k, k \uf0a2 of the measured observable? Decoherence theory shows that\ninteraction between a macroscopic system and a random environment randomizes the\nrelative phases of such states on very short time scales. Consequently the relative phases\nof off-diagonal matrix elements k \uf0a2, j( k \uf0a2,i) O k, j(k,i) are randomly distributed between\n0 and 2\uf070. Since the number of microstates i >> 1, the result of averaging\nk \uf0a2, j( k \uf0a2,i) O k, j(k,i) over the probability distribution \uf07bpi \uf07d is close to zero:\n\n\uf0e5p\n\ni\n\nk \uf0a2, j(k \uf0a2,i) O k, j(k,i) \uf03d 0 ( k \uf0b9 k\uf0a2 )\n\n(21)\n\ni\n\nSo the right side of (20) reduces, in an excellent approximation, to\n\n\uf0e5 c \uf0e5p\n2\n\nk\n\nk\n\ni\n\ni\n\nk, j(k,i) O k, j(k,i) \uf03d \uf0e5 ck O\n2\n\n(k,k )\n\n,\n\n(22)\n\nk\n\nwhere (k, k) labels the macrostate of the combined system in which the object is in the\nquantum state k and the apparatus is in the correlated pointer state.\nAs in the discussion following Eq. (16), we conclude that in each member of the\ncosmological ensemble of combined systems that represents the outcome of the\nmeasurement, the object is in an eigenstate of the measured observable and the apparatus\n30\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nis in the correlated pointer state. Moreover, the fraction of ensemble members in the state\n2\n\n(k, k) is ck , as Born\u201fs rule prescribes.\n\n2a) Quantum measurements generate coarse-grained entropy.\nIn the preceding description of an ideal measurement, the same probability distribution\n\n\uf07bpi \uf07d of microstates characterizes the initial and final states of the combined system, so\nits statistical entropy (the Gibbs entropy) doesn\u201ft change. But the coarse-grained entropy\nassociated with the probability distribution of macrostates of the combined system\nincreases from 0 (= log 1) to \uf02d\uf0e5 ck log ck . This gain is compensated by an equal\n2\n\n2\n\nj\n\nloss of residual entropy. The loss of residual statistical entropy results mainly from the\nfact that the preceding idealized account allots fewer microstates to each of the final\nmacrostates than to the initial macrostate. For example, if the initial macrostate contains\nn microstates, with n ? 1, its statistical entropy is approximately log n. In the final state,\nthe n microstates are distributed among the final macrostates; so their residual entropy is\napproximately \uf0e5 pa log n\uf061 , where n\uf061 \uf03d p\uf061 n . The total (fine-grained) statistical entropy\n\uf061\n\nof the final probability distribution is thus \uf02d\uf0e5 pa log pa \uf02b\uf0e5 p(\uf061 ) logn\uf061 \uf03d logn .\n\uf061\n\n\uf061\n\n2b) Decoherence and measurement\nMany previous attempts to apply quantum mechanics to the measurement process have\nimplicitly or explicitly invoked the cancellation of matrix elements that connect\nmacroscopically distinguishable quantum states in sums like (20). For example, Bohm\n[22] argued that in an idealized Stern-Gerlach experiment, the conditions for a good\nmeasurement require the phase of the silver atom\u201fs center-of-mass wave function to vary\nby an amount much greater than 2\u03c0 over each region of the detector where the amplitude\nof the wave function is appreciable. Decoherence theory shows how interaction between\n\n31\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\na measuring apparatus and a random (internal or external) environment in effect\nrandomizes the relative phases of quantum states belonging to different pointer states. It\nalso supplies estimates of decoherence times for particular models. The preceding\naccount of measurement appeals to the same process at a crucial step in the argument, the\nstep leading to (21).\nThe preceding account supplements (and differs from) accounts based on\ndecoherence in four significant ways:\n(i) Decoherence theory applies Schr\u00f6dinger\u201fs equation to an undisturbed system\nconsisting of the measured system, the measuring apparatus, and a portion of the\nenvironment. In the present account the measuring apparatus is initially in a definite\nclassical state, characterized by a historically defined probability distribution of quantum\nstates.\n(ii) Decoherence theory postulates that the (external or internal) environment has a\nrandom character. This is a crucial assumption. Its operational meaning is clear, but its\nphysical meaning in the context of measurement theory and, more generally, in quantum\nstatistical mechanics, is unclear. In the present account randomness has a definite and\nobjective meaning: in a particular context it is measured by the statistical entropy of a\nprobability distribution \"descended\" from the probability distributions that (objectively)\ncharacterize the primordial cosmic medium.\n(iii) Because the present account explains environmental randomness, it predicts that\nquantum measurements are irreversible.\n(iv) Decoherence theory explains why local measurements cannot exhibit the effects\nof interference between macroscopically distinguishable quantum states. But, as Joos\nand Zeh emphasized in a seminal paper,\nThe use of the local density matrix allows at most only a partial derivation of classical\nconcepts for two reasons: it already assumes a local description, and it presupposes\nthe probabilistic interpretation leading to the collapse of the state vector at some stage\nof a measurement. ... The difficulties in giving a complete derivation of classical\nconcepts may as well signal the need for entirely novel concepts [25].\n\n32\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nIn particular, decoherence theory does not solve what Schlosshauer [21] calls \"the\nproblem of outcomes;\" it does not predict that measurements have definite outcomes.\nThe present account does make this prediction. In a sense it also explains why quantum\nmeasurements have indeterminate outcomes: a complete description of the universe that\ncomports with the strong cosmological principle cannot tell us where a measurement\noutcome occurs.\n\nE. Quantum measurements and general relativity\nA quantum measurement can result in an unpredictable change in the local structure of\nspacetime. For example, it can cause an unpredictable displacement of a massive object.1\nThis has been said to demonstrate the need for \"treating the spacetime metric in a\nprobabilistic fashion \u2013 i.e., [for] quantizing the gravitational field ...\" [26]\nNow the preceding account of a quantum measurement does not predict where each\nof the possible outcomes of a quantum measurement occurs. Instead it predicts what\nfraction of the membership of a cosmological ensemble experiences each possible\noutcome.\nThe present description of the cosmic medium (\u00a7II) is likewise probabilistic.\nAlthough the stress-energy tensor is a classical field, it is also a realization \u2013 indeed the\nrealization \u2013 of a probabilistic description that does not privilege any position or direction\nin space. The indeterminacy of quantum measurement outcomes is consistent with this\nprobabilistic description of the structure and contents of spacetime. We cannot predict\nthe outcome of a quantum measurement because we cannot know where, in an absolute\nsense, it occurs. The structure of spacetime after a measurement has taken place reflects\n\n1\n\nSuch a displacement affects the structure of spacetime only locally, because it does not\n\nchange the center of mass of the self-gravitating system in which the measurement\noccurs. Its effects therefore diminish rapidly with distance and are eventually lost in the\nnoise associated with random fluctuations in the mass density of the universe.\n\n33\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nthe fact that a measurement has occurred but supplies no information about where it has\noccurred.\nThere are, of course, strong reasons for seeking a quantum theory of gravity. The\npreceding argument merely shows that if the historical account of initial conditions\nsketched in the preceding pages is correct, quantum mechanics and general relativity do\nnot actually clash in the domain where they both hold, at least to an excellent\napproximation. The reason is that contact between them is mediated by statistical\nmacrophysics, whose assumptions, as I have argued, are anchored in an account of\ncosmic evolution.\n\nIV. THE INTERPRETATION OF QUANTUM MECHANICS\nClassical physics extends and refines intuitive and commonsense notions about the\nexternal world. As Dirac [22, preface to the first edition, 1930] pointed out, the\nformalism of quantum mechanics describes a mathematical world we can neither picture\nnor adequately translate into ordinary language. How is that world related to the world\nthat classical physics describes? Although this question falls outside the domain of\nphysics as such, an acceptable answer must comport with the rule that links the\nmathematical formalism of quantum mechanics to the results of possible measurements.\nAnd different versions of this rule have suggested different answers.\nDirac\u201fs version of the rule identifies the matrix element s O s with the average\nresult of a large number of measurements of an observable O when the measured system\nis in the state s. It is unambiguous, and it has proved adequate for all practical purposes.\nHowever, Dirac\u201fs exposition of quantum mechanics stops short of describing the\nmeasurement process itself.\nVon Neumann\u201fs theory of measurement attempted to fill this gap. Von Neumann laid\ndown necessary conditions for an ideal measurement. Assuming that the combined\nsystem is undisturbed and is initially in a definite quantum state, he concluded that the\ncombined system evolves into a superposition of macroscopically distinguishable states,\neach of which represents one of the measurement\u201fs possible outcomes. To reconcile this\n34\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nresult with Dirac\u201fs rule (and with experiment), he postulated that the predicted\nsuperposition promptly collapses onto one of its components.\nSome attempts to avoid the need for such a postulate without altering the theory\u201fs\nformal structure appeal to methodological or philosophical considerations. Some\nprominent examples:\n\u2013 Heisenberg [25] argued that state vectors do not describe objective physical states.\nInstead they describe a \"potential reality\" which measurements \"actualize.\" Some\ncontemporary physicists \u2013 for example, Gottfried and Yan [28, pp. 40, 574] \u2013 have\nembraced this view.\n\u2013 Wigner [29] argued that \"the function of quantum mechanics is not to describe\n\u201ereality,\u201f whatever this term means, but only to furnish statistical correlations between\nsubsequent observations. This assessment reduces the state vector to a calculational\ntool ...\" Peres [30] agreed: \"In fact, there is no evidence whatsoever that every physical\nsystem has at every instant a well defined state [whose] time dependence represents the\nactual evolution of a physical process.\"\n\u2013 Everett [31] hypothesized that the universe is in a definite quantum state whose\nevolution is governed by Schr\u00f6dinger\u201fs equation (or a linear generalization thereof).\nEvery quantum measurement then creates a non-collapsing superposition of equally real\nquantum states. Proponents of this approach hope that the world of classical physics and\nexperience will fit into this proliferating tree-like structure of universe-states, but they\nhave not yet succeeded in showing precisely how.\n\u2013 Zeh [30] argued that the quantum states of macroscopic systems, including the\ncombined system in a quantum measurement, are invariably entangled with states of their\nenvironment. He argued that macroscopic systems, including large molecules, acquire\nclassical properties through their interaction with a random environment, a process later\ndubbed \"decoherence.\" Diverse theories that rely on this process [33] assume that\nquantum mechanics applies to macroscopic systems, including measuring apparatuses\nand their immediate environment, but not necessarily to the universe as a whole. Such\ntheories explain why interference effects between macroscopically distinguishable states\nof a macroscopic system initially in a definite quantum state quickly become\nunobservable. They also explain the observed absence of transitions between\n35\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nmacroscopically distinguishable states of large molecules (superselection rules) [30]. But\nthey do no explain why quantum measurements have definite outcomes.\n\u2013 Zurek [34], one of the founders of decoherence theory, has argued that quantum\nmechanics needs a cosmological postulate, namely, that \"the universe consists of\nquantum systems, and ... that a composite system can be described by a tensor product of\nthe Hilbert spaces of the constituent systems.\" Decoherence-based theories also need to\nassume that macroscopic systems are embedded in random environments. An account of\ncosmic evolution along the lines sketched in this paper would supply these needs.\nIn this paper I have argued that classical physics is neither a province nor a\npresupposition of quantum mechanics. Systems governed by classical laws are\ndistinguished from systems governed by quantum laws by their histories, which\ndetermine the initial conditions that define them. These initial conditions take the form\nof probability distributions of quantum states. Probability distributions that assign\ncomparable probabilities to a very large number of quantum states (i.e., probability\ndistributions with broad support in Hilbert space) characterize systems governed by\nclassical laws. Quantum laws govern systems to which one can assign a definite\nquantum state. Thus quantum physics and classical physics correspond to limiting cases\nof the historically determined initial conditions that define physical systems.\nWhat is distinctive about this characterization of macroscopic states is not its\nmathematical form, which is old and familiar, but the interpretation of the probability\ndistributions of quantum states that figure in it. I have argued that these probability\ndistributions are descended from probability distributions that objectively describe states\nof the early universe; and that, in virtue of the strong cosmological principle, they belong\nto an objective description of the universe and its evolution. This interpretation offers an\nobjective answer to van Kampen\u201fs \"fundamental question: How is it possible that such\nmacroscopic behavior exists, governed by its own equations of motion, regardless of the\ndetails of the microscopic motion?\" [16, p. 56] It also offers an objective interpretation\nof quantum indeterminacy that contrasts with Heisenberg\u201fs view that the unpredictability\nof quantum-measurement outcomes results from \"uncertainties concerning the\nmicroscopic structure of the [measuring] apparatus ... and, since the apparatus is\n\n36\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nconnected with the rest of the world, of the microscopic structure of the whole world.\"\n[25, pp. 53-4, my italics]\nMainstream interpretations of current physical theories offer a picture of the physical\nworld in which the outcomes of physical processes other than quantum measurements\nand measurement-like processes are predictable in principle. The same physical theories,\ninterpreted in light of the strong cosmological principle, suggest a qualitatively different\npicture of the physical world: one in which indeterminacy obtains at all levels of\ndescription. In this picture the universe itself is the unique realization of a statistical\ndescription that initially contains little or no statistical information. Gravitational\nprocesses \u2013 the cosmic expansion, the growth of density fluctuations in the cosmic\nmedium, the contraction of self-gravitating systems \u2013 subsequently create statistical\ninformation, but the statistical information per unit mass remains far smaller than its\nlargest possible value. Thus, as interventionist statistical theories of irreversibility and\ndecoherence theories of the quantum-to-classical transition assume, the structure of the\nuniverse is orderly on macroscopic scales but random on microscopic scales. And\nbecause the initial states of macroscopic systems usually (though not necessarily and not\nalways) contain little or no microscopic information, we can expect the outcomes of\nprocesses that depend sensitively on initial conditions to be objectively unpredictable in\nmany cases.\nThe historical account of initial conditions sketched in this paper connects the\nphysical laws that prevail at different levels of description but establishes no clear order\nof precedence among them. The strong cosmological principle is the source of\nindeterminacy at the macroscopic and astronomical levels, but it presupposes quantum\nmechanics. The strong cosmological principle also enables general relativity\u201fs classical\ndescription of spacetime in the large to coexist peaceably with quantum indeterminacy\n(\u00a7III.E). Macroscopic laws of motion, which involve a small number of macroscopic\nvariables, depend on cosmology, because macroscopic systems are defined by probability\ndistributions that contain little or no microscopic information and are rooted in the\nprobability distributions that characterize the early universe. At the same time, the\npredictions of macroscopic laws depend on the fact that quantum laws govern the\nmicrostructure of macroscopic systems.\n37\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\nThe interconnectedness of the three levels of description allows macroscopic\nmeasuring devices to probe all three levels. But measurement does not play a privileged\nrole in the present description. Nor does this description refer, implicitly or explicitly, to\nhuman knowledge. Thus although it assigns a central role to chance, it is consistent with\nPlanck\u201fs and Einstein\u201fs view that physical theories describe observer-independent\nmathematical regularities behind the phenomena.\n\nACKNOWLEDGMENTS\nAnthony Aguirre and Silvan S. Schweber separately offered detailed, insightful,\nprovocative, and very useful criticisms of several drafts of this paper. This version owes\na great deal to their efforts (though it does not reflect their views on substantive issues\naddressed therein). I am deeply indebted to both of them.\n\nREFERENCES\n[1] E. P. Wigner, \"Events, Laws of Nature, and Invariance Principles\" in The Nobel Prize\nLectures, 1964 (Nobel Foundation, Stockholm, 1964). Reprinted in Symmetries and\nReflections (MIT Press, Cambridge, 1970). The passages quoted in the text are on p. 41\nof the reprint.\n[2] S. Weinberg, Gravitation and Cosmology (Wiley, New York, 1972), Chapter 15,\nespecially Section 6\n[3] A. Aguirre, Astrophysical Journal 521, 17 (1999)\n\n38\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\n[4] D. Layzer, in Astrophysics and General Relativity, Brandeis University Summer\nInstitute of Theoretical Physics, 1968, edited by M. Chr\u00e9tien, S. Deser, J. Goldstein;\nGordon and Breach, New York, 1971\n[5] D. Layzer, Cosmogenesis, Oxford University Press, New York, 1990\n[6] J. W. Gibbs, Elementary Principles in Statistical Mechanics (Scribner, New York,\n1902), p. 5\n[7] C. E. Shannon. Bell Sys. Tech. J. 27, 379, 623 (1948)\n[8] L. Szilard, Z. f. Phys. 53, 840 (1929). English translation in Quantum Theory and\nMeasurement, edited by J. A. Wheeler and W. H. Zurek (Princeton University Press,\nPrinceton NJ, 1983)\n[9] H. Poincar\u00e9 in Science et M\u00e9thode (Flammarion, Paris, 1908). English translation in\nThe World of Mathematics, ed. J. R. Newman (Simon and Schuster, New York, 1956)\n[10] D. Layzer, submitted to Mind and Matter (July, 2010)\n[11] R. Jancel, Foundations of Classical and Quantum Statistical Mechanics (Pergamon,\nOxford, 1963), p. xvii\n[12] E. T. Jaynes, Phys. Rev. 106, 620 (1957)\n[13] E. T. Jaynes, Phys. Rev. 108, 171 (1957)\n[14] E. Schr\u00f6dinger, Statistical Mechanics (Cambridge University Press, Cambridge,\n1948), p. 3\n\n39\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\n[15] N. G. van Kampen in Fundamental Problems in Statistical Mechanics, Proceedings\nof the NUFFIC International Summer Course in Science at Nijenrode Castle, The\nNetherlands, August 1961, compiled by E. G. D. Cohen (North-Holland, Amsterdam,\n1962), especially pp. 182-184.\n[16] N.G. van Kampen, Stochastic Processes in Physics and Chemistry, 3d edition\n(Elsevier, Amsterdam, 2007), especially pp. 55-58.\n[17] J. M. Blatt, Prog. Theor. Phys. 22, 745 (1959)\n[18] P. J. Bergmann and J. L. Lebowitz, Phys. Rev. 99, 578 (1955); J. L. Lebowitz and P.\nJ. Bergmann, Annals of Physics 1, 1 (1959)\n[19] T. M. Ridderbos and M. L. G. Redhead, Found. Phys. 28, 1237 (1988)\n[20] E. L. Hahn, Phys. Rev. 80, 580 (1950)\n\n[21] M. Schlosshauer, Decoherence and the Quantum-to-Classical Transition, corrected\n2d printing (Springer, Berlin, 2008)\n[22] P. A. M. Dirac, The Principles of Quantum Mechanics, 4th ed., revised (Clarendon\nPress, Oxford, 1967)\n[23] J. von Neumann, Mathematische Grundlagen der Quantenmechanik (Springer,\nBerlin, 1932); English translation by R. T. Beyer, Mathematical Foundations of Quantum\nMechanics (Princeton University Press, Princeton, 1955)\n[24] D. Bohm, Quantum Theory (Prentice-Hall, Englewood Cliffs NJ, 1951; reprinted:\nDover, Mineola NY, 1989), p. 602\n[25] E. Joos and H. D. Zeh, Z. Phys. B 59, 223 (1985)\n\n40\n\n\fInitial conditions in cosmology and statistical physics\n\nLayzer\n\n[26] R. M. Wald, General Relativity (University of Chicago Press, Chicago, 1984), p.383\n[27] W. Heisenberg, Physics and Philosophy (Allen and Unwin, London, 1958), Chapter\n3.\n[28] K. Gottfried and Tung-Mow Yan, Quantum Mechanics: Fundamentals, second\nedition (Springer, New York, 2003)\n[29] E. P. Wigner in Quantum Theory and Measurement, eds. J. A. Wheeler and W. H.\nZurek (Princeton University Press, Princeton, 1983), p. 286\n[30] A. Peres, Quantum Theory: Concepts and Methods (Kluwer, Dordrecht, 1993), pp.\n373-4\n[31] H. Everett III, Rev. Mod. Phys. 29, 454 (1957)\n[32] H. D. Zeh, Found. Phys. 1, 69 (1970)\n[33] Review: M. Schlosshauer, Decoherence and the Quantum-to-Classical Transition,\ncorrected 2d printing (Springer, Berlin, 2008)\n[34] W. H. Zurek, Rev. Mod. Phys. 75, 715 (2003), p. 751\n[35] A. Einstein, Sitzungsberichte der Preussischen Akad. d. Wissenschaften, 1917,\ntranslated by W. Perrett and G. B. Jeffery in The Principle of Relativity, a collection of\noriginal memoirs on the special and general theory of relativity, Methuen, London, 1923;\nDover reprint.\n\n41\n\n\f"}