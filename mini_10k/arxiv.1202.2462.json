{"id": "http://arxiv.org/abs/1202.2462v1", "guidislink": true, "updated": "2012-02-11T19:38:41Z", "updated_parsed": [2012, 2, 11, 19, 38, 41, 5, 42, 0], "published": "2012-02-11T19:38:41Z", "published_parsed": [2012, 2, 11, 19, 38, 41, 5, 42, 0], "title": "Aggregation-based Multilevel Methods for Lattice QCD", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.4090%2C1202.5353%2C1202.6197%2C1202.2528%2C1202.4375%2C1202.4523%2C1202.6503%2C1202.6111%2C1202.0886%2C1202.3191%2C1202.3092%2C1202.1862%2C1202.6137%2C1202.0353%2C1202.4072%2C1202.4691%2C1202.1167%2C1202.6025%2C1202.2965%2C1202.4984%2C1202.3134%2C1202.6588%2C1202.4566%2C1202.2066%2C1202.3906%2C1202.5401%2C1202.5212%2C1202.1341%2C1202.1322%2C1202.2462%2C1202.0102%2C1202.6211%2C1202.3596%2C1202.5108%2C1202.6149%2C1202.0281%2C1202.3648%2C1202.1768%2C1202.2930%2C1202.0954%2C1202.3335%2C1202.1153%2C1202.3319%2C1202.0465%2C1202.5034%2C1202.1190%2C1202.1600%2C1202.3884%2C1202.1408%2C1202.4917%2C1202.4860%2C1202.2228%2C1202.4813%2C1202.6223%2C1202.1370%2C1202.1267%2C1202.6534%2C1202.2922%2C1202.2296%2C1202.4591%2C1202.0621%2C1202.2898%2C1202.2189%2C1202.5858%2C1202.4344%2C1202.6150%2C1202.4285%2C1202.3283%2C1202.6474%2C1202.3436%2C1202.2257%2C1202.4696%2C1202.6684%2C1202.6644%2C1202.3295%2C1202.3628%2C1202.4074%2C1202.6317%2C1202.1761%2C1202.2968%2C1202.2750%2C1202.1001%2C1202.1302%2C1202.2119%2C1202.6348%2C1202.2010%2C1202.2543%2C1202.1995%2C1202.4276%2C1202.1025%2C1202.6086%2C1202.0541%2C1202.4109%2C1202.2116%2C1202.3784%2C1202.4517%2C1202.3061%2C1202.6390%2C1202.3830%2C1202.3203%2C1202.5413&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Aggregation-based Multilevel Methods for Lattice QCD"}, "summary": "In Lattice QCD computations a substantial amount of work is spent in solving\nthe Dirac equation. In the recent past it has been observed that conventional\nKrylov solvers tend to critically slow down for large lattices and small quark\nmasses. We present a Schwarz alternating procedure (SAP) multilevel method as a\nsolver for the Clover improved Wilson discretization of the Dirac equation.\nThis approach combines two components (SAP and algebraic multigrid) that have\nseparately been used in lattice QCD before. In combination with a bootstrap\nsetup procedure we show that considerable speed-up over conventional Krylov\nsubspace methods for realistic configurations can be achieved.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.4090%2C1202.5353%2C1202.6197%2C1202.2528%2C1202.4375%2C1202.4523%2C1202.6503%2C1202.6111%2C1202.0886%2C1202.3191%2C1202.3092%2C1202.1862%2C1202.6137%2C1202.0353%2C1202.4072%2C1202.4691%2C1202.1167%2C1202.6025%2C1202.2965%2C1202.4984%2C1202.3134%2C1202.6588%2C1202.4566%2C1202.2066%2C1202.3906%2C1202.5401%2C1202.5212%2C1202.1341%2C1202.1322%2C1202.2462%2C1202.0102%2C1202.6211%2C1202.3596%2C1202.5108%2C1202.6149%2C1202.0281%2C1202.3648%2C1202.1768%2C1202.2930%2C1202.0954%2C1202.3335%2C1202.1153%2C1202.3319%2C1202.0465%2C1202.5034%2C1202.1190%2C1202.1600%2C1202.3884%2C1202.1408%2C1202.4917%2C1202.4860%2C1202.2228%2C1202.4813%2C1202.6223%2C1202.1370%2C1202.1267%2C1202.6534%2C1202.2922%2C1202.2296%2C1202.4591%2C1202.0621%2C1202.2898%2C1202.2189%2C1202.5858%2C1202.4344%2C1202.6150%2C1202.4285%2C1202.3283%2C1202.6474%2C1202.3436%2C1202.2257%2C1202.4696%2C1202.6684%2C1202.6644%2C1202.3295%2C1202.3628%2C1202.4074%2C1202.6317%2C1202.1761%2C1202.2968%2C1202.2750%2C1202.1001%2C1202.1302%2C1202.2119%2C1202.6348%2C1202.2010%2C1202.2543%2C1202.1995%2C1202.4276%2C1202.1025%2C1202.6086%2C1202.0541%2C1202.4109%2C1202.2116%2C1202.3784%2C1202.4517%2C1202.3061%2C1202.6390%2C1202.3830%2C1202.3203%2C1202.5413&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In Lattice QCD computations a substantial amount of work is spent in solving\nthe Dirac equation. In the recent past it has been observed that conventional\nKrylov solvers tend to critically slow down for large lattices and small quark\nmasses. We present a Schwarz alternating procedure (SAP) multilevel method as a\nsolver for the Clover improved Wilson discretization of the Dirac equation.\nThis approach combines two components (SAP and algebraic multigrid) that have\nseparately been used in lattice QCD before. In combination with a bootstrap\nsetup procedure we show that considerable speed-up over conventional Krylov\nsubspace methods for realistic configurations can be achieved."}, "authors": ["Andreas Frommer", "Karsten Kahl", "Stefan Krieg", "Bj\u00f6rn Leder", "Matthias Rottmann"], "author_detail": {"name": "Matthias Rottmann"}, "author": "Matthias Rottmann", "arxiv_comment": "Talk presented at the XXIX International Symposium on Lattice Field\n  Theory, July 10-16, 2011, Lake Tahoe, California", "links": [{"href": "http://arxiv.org/abs/1202.2462v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.2462v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "hep-lat", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "hep-lat", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.2462v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.2462v1", "journal_reference": "PoS(Lattice 2011)046", "doi": null, "fulltext": "Aggregation-based Multilevel Methods for\nLattice QCD\n\narXiv:1202.2462v1 [hep-lat] 11 Feb 2012\n\nA. Frommer\nDepartment of Mathematics, Bergische Universit\u00e4t Wuppertal, 42097 Germany\nE-mail: frommer@math.uni-wuppertal.de\n\nK. Kahl\nDepartment of Mathematics, Bergische Universit\u00e4t Wuppertal, 42097 Germany\nE-mail: kkahl@math.uni-wuppertal.de\n\nS. Krieg\nDepartment of Physics, Bergische Universit\u00e4t Wuppertal, 42097 Germany\n& J\u00fclich Supercomputing Centre, Forschungszentrum J\u00fclich, 52428 Germany\nE-mail: s.krieg@fz-juelich.de\n\nB. Leder\nDepartment of Mathematics, Bergische Universit\u00e4t Wuppertal, 42097 Germany\nE-mail: leder@math.uni-wuppertal.de\n\nM. Rottmann\u2217\nDepartment of Mathematics, Bergische Universit\u00e4t Wuppertal, 42097 Germany\nE-mail: rottmann@math.uni-wuppertal.de\nIn Lattice QCD computations a substantial amount of work is spent in solving the Dirac equation.\nIn the recent past it has been observed that conventional Krylov solvers tend to critically slow\ndown for large lattices and small quark masses. We present a Schwarz alternating procedure\n(SAP) multilevel method as a solver for the Clover improved Wilson discretization of the Dirac\nequation. This approach combines two components (SAP and algebraic multigrid) that have\nseparately been used in lattice QCD before. In combination with a bootstrap setup procedure\nwe show that considerable speed-up over conventional Krylov subspace methods for realistic\nconfigurations can be achieved.\n\nThe XXIX International Symposium on Lattice Field Theory - Lattice 2011\nJuly 10-16, 2011\nSquaw Valley, Lake Tahoe, California\n\u2217 Speaker.\n\nc Copyright owned by the author(s) under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike Licence.\n\nhttp://pos.sissa.it/\n\n\fM. Rottmann\n\nMultilevel Methods\n\n1. Introduction\nThe most costly task in lattice QCD computations is the solution of large sparse linear systems\nof equations\nDz = b,\n(1.1)\nwhere D is a discretization of the Dirac operator. Here we consider the Wilson discretization\nD = D(U) + m which couples only nearest neighbors and depends on a gauge field U and a mass\nparameter m. Usually z is calculated by a Krylov subspace method (e.g. CGN, GCR, BiCGStab).\nThose methods suffer from critical slowing down when approaching the critical mass as well as\nlattice spacing a = 0. Thus it is of utmost importance to develop preconditioners for these methods\nthat remedy these scaling problems.\nIn the recent past preconditioners based on domain decomposition (DD) for the solution\nof (1.1) have been proposed in [1]. Although DD methods excel in supercomputing environments\ndue to their high inherent parallelism they are unable to remedy the scaling problems completely\nunless they are combined with a multilevel approach. Thus we combine the DD approach with an\nalgebraic multigrid hierarchy based on a bootstrap aggregation framework [2, 3]. Our approach\nis similar in construction to the one introduced in [4, 5, 6] where it has been shown that using\nsuch algebraic multigrid approaches can remedy the scaling problems in QCD computations. As\nin [4, 5, 6] we obtain a multilevel hierarchy using non-smoothed aggregation. The difference is\nthat we replace the multigrid smoother by a DD approach, expecting a gain in efficiency on highly\nparallel machines.\nIn section 2 we introduce the concept of DD methods before we explain the construction of\nour algebraic multilevel method in some detail in section 3. Thereafter we give numerical results\nof our method in section 4 and finish with some concluding remarks.\n\n2. Domain Decomposition\nDomain decomposition methods were developed as iterative solvers for linear systems arising\nfrom discretizations of PDEs. The main idea consists of solving the system on the whole domain\nby repeatedly solving smaller systems with less degrees of freedom on smaller subdomains.\nConsider a block decomposition {Li : i = 1, . . . , k} of a lattice L (Figure 1 illustrates a 2D\nexample). The corresponding trivial embeddings and block solvers are denoted by ILi : Li \u2192 L\nT DI ]\u22121 I T . Note that the trivial embedding I is just the restriction of the identity\nand Bi = ILi [IL\nLi\nLi\nLi\ni\non L to Li . Then one iteration of a domain decomposition method consists of solving each of\nthe block systems e \u2190 Bi r, interleaved with a number of residual updates r = b \u2212 Dz. In the two\nextreme cases where one does only one residual update before solving all block systems or when\nthe residual is updated after each block solution, the corresponding error propagators are given by\nk\n\n1 \u2212 ( \u2211 Bi )A\n\nk\n\nand\n\n\u220f (1 \u2212 Bi A) .\n\n(2.1)\n\ni=1\n\ni=1\n\nThese methods go back to H. Schwarz [7] and thus are called additive Schwarz and multiplicative\nSchwarz method, respectively. The block systems of the additive variant can be solved simultaneously while the multiplicative variant is inherently sequential. Its advantage is that it spreads the\ninformation faster on the lattice as a solution of a block system uses previous solutions of other\nblock systems.\n2\n\n\fM. Rottmann\n\nMultilevel Methods\n\nAlgorithm 2.1 Red-Black Schwarz\n1: for c = 1 to 2 do\n2:\nr \u2190 b \u2212 Dz\n3:\nfor all i \u2208 {1, ..., k} with color(i) = c do\n4:\nz \u2190 z + Bi r\n5:\nend for\n6: end for\n\nThe two methods can be combined to exploit advantages of both methods. Coloring the blocks\nsuch that no adjacent blocks have the same color, a residual update on one block no longer influences the residual on blocks of the same color. Thus it suffices to perform the update once for each\ncolor. All blocks of the same color can then be computed simultaneously as described in Algorithm 2.1. Such a DD approach has been applied to solve (1.1) in [1], where it has been named\nSchwarz Alternating Procedure (SAP).\nTypically the solution of the block system e = Bi r is approximated by a few iterations of a\nKrylov subspace method (e.g. GMRES), and the DD method itself is in turn used as a preconditioner for a (flexible) Krylov subspace method. As illustrated in Figure 2 we observe that SAP is\nable to reduce error components belonging to a large part of the spectrum very well but a small part\nbelonging to eigenvectors (EVs) to small eigenvalues (EWs) remains intractable. For larger configurations the number of EWs with small magnitude of the Dirac operator gets larger, which yields\nan explanation why SAP is not able to remedy the scaling problem as the number of intractable\neigenvectors increases as well. Though, the seen behavior of damping large EVs is desirable for an\niterative method to be used as a smoother in a multigrid method and motivated us to use it in this\ncontext.\n\n3. Algebraic Multigrid\nA multigrid method typically consists of a simple iterative method called smoother and complementary coarse grid correction. As motivated in section 2 we deem SAP suitable for the use\nas a smoother since it is cheap to compute and reduces the error efficiently on a large part of the\nspectrum. The main idea of multigrid is to treat the error that is left after a few iterations of the\n\n1.4\nred\u2212black Schwarz\n1.2\n\nL2\n\nL3\n\n1\n||(I\u2212M\u22121 A) vi||2\n\nL1\n\nL4\n\n0.8\n0.6\n0.4\n0.2\n0\n\nL\n\nFigure 1: block decomposed lattice (reduced to\n2D) with 2 colors\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n3500\n\nFigure 2: error component reduction in terms of\nEVs on a 44 lattice with 24 blocks, EWs sorted\nby magnitude\n\n3\n\n\fM. Rottmann\n\nMultilevel Methods\n\nsmoother within a smaller subspace where the troublesome error components can be approximated.\nMore precisely we want to define and solve a \"coarse\" linear system\nDc zc = bc\n\n(3.1)\n\nwith a much smaller operator Dc : Cnc \u2192 Cnc in order to reduce the error on the critical part of (1.1).\nTo this purpose we have to define linear maps R : Cn \u2192 Cnc to restrict information based on the\ncurrent residual r = b \u2212 Dz to the subspace and a linear map P : Cnc \u2192 Cn to interpolate the information that we obtain from solving (3.1) back to Cn where (1.1) is given. This yields a subspace\ncorrection\nz \u2190 z + PD\u22121\nc Rr\n\n(3.2)\n\nwith the corresponding error propagator 1 \u2212 PD\u22121\nc RD. As Dc should resemble the action of D on\nthe troublesome subspace approximated by span(P), the action of Dc is chosen as the action of D\non interpolated vectors which are restricted afterwards. Formally this amounts to a Petrov-Galerkin\nformulation of the coarse operator as Dc = RDP. With this choice of Dc the error propagator of\nthe subspace correction is given by 1 \u2212 P(RDP)\u22121 RD. In order to benefit from such a subspace\ncorrection, solving (3.1) has to be much cheaper than solving (1.1). That is nc should be small\ncompared to n, and Dc should be sparse. As the dimension of the troublesome subspace grows with\nn (cf. [8]) we do not want to fix nc (like in deflation methods) but want to find a sparse description\nof Dc on that subspace.\nOnce Dc is found a basic two level algorithm consists of the alternating application of smoother\nand subspace correction. This procedure can be recursively extended by formulating a two level\nalgorithm of this kind for the computation of (3.1) until we get an operator which is small enough\nto solve (3.1) directly.\nAggregation Based Interpolation: We decided to adjust an aggregation based interpolation,\nthat in turn yields the subspace correction, as the complementary component to the SAP smoother.\nDue to the fact that the coarse grid correction in (3.2) only acts on error components in range(P), it\nshould approximate the subspace spanned by eigenvectors to eigenvalues of small magnitude of D\n(cf. Figure 2). In the multigrid literature, P is built from right, and R from left EVs corresponding\nto EWs of small magnitude of D. Due to the spectral properties of the Wilson Dirac operator it\nis natural to choose R = \u03b35 P. Furthermore, with additional assumptions on its structure we can\nchoose R = P\u2020 according to [5]. Therefore we define the aggregates in such a way that there exists\n\u03b35c such that \u03b35 P = P\u03b35c . Note, that with these assumptions on P the error propagator of the subspace\ncorrection (3.2) is given by z \u2190 z + P(P\u2020 DP)\u22121 P\u2020 r.\nWith this structure of P in mind, we define its entries based on a set of test vectors {v1 , . . . , vN }\nwhose span approximates the troublesome subspace and a set of aggregates {A1 , . . . , As }.The aggregates can be realized as another block decomposition of the lattice. Note that the DD smoother\nand the interpolation do not have to share a common block decomposition. The interpolation P is\nthen given by decomposing the test vectors over the aggregates (cf. Figure 3). Hence P is a linear\nmap from the coarse grid to the fine grid, defined by\nT\nPe j := IA\n\nj\ndN e\n\nv(( j\u22121) mod N)+1\n\n4\n\nfor\n\nj = 1, . . . , N * s\n\n\fM. Rottmann\n\nMultilevel Methods\n\n\uf8eb \uf8f6\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n(v1 , . . . , vN ) = \uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\n\uf8eb\n\n\uf8f6\n\n\uf8f7\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f7 7\u2192 P = \uf8ec\n\uf8f7\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f7\n\uf8ec\n\uf8f8\n\uf8ed\n\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n.. \uf8f7\n. \uf8f7\n\uf8f7\n\uf8f8\n\nFigure 3: construction of the interpolation (operator based point of view)\n\nwhere e j is the j-th unit vector. In order to have P\u2020 P = I, the test vectors are locally orthonormalized\nover the aggregates. Note that due to the aggregation structure of P and R the sparsity/connection\nstructure of Dc resembles the one of D, i.e., the corresponding graphs of the operators are regular\nfour dimensional grids. Thus we can apply (3.2) recursively to (3.1) and obtain a hierarchy of\ncoarser grids and coarser operators. This construction of P is similar to constructions found in\n[4, 5, 6, 8]. More precisely, the structure of the interpolation operators is identical but the test\nvectors vi used to build them and thus the actions of the operators are different.\nBootstrap Setup: A critical part of the construction of an efficient multigrid hierarchy is the\ncomputation of the test vectors used in the definition of P. The setup procedure we employ for this\ntask is divided into two parts. We start with a set of random test vectors v1 , . . . , vN and apply a small\nnumber of smoother iterations S\u03bd to them. During this procedure the smoothed test vectors S\u03bd vi are\nkept orthonormal. After that a temporary operator on the next coarser grid is computed according\nto the aggregation based construction. This procedure is continued recursively on the coarser grids\nuntil we get an operator on the coarsest grid. Herein we restrict smoothed test vectors with P\u2020 to\nthe next coarser grid. In what follows we omit the level indices for the sake of simplicity.\nSince the subspace range(P) should contain a significant part of the eigenvectors to eigenvalues of small magnitude of the fine grid space Cn , the EWs of small magnitude of Dc = P\u2020 DP should\napproximate those of D. For the corresponding EVs the relations P\u2020 P = 1 and P\u2020 (DP\u03c6 \u2212\u03bb P\u03c6) = 0\nimply DP\u03c6 \u2248 \u03bb P\u03c6. Thus the second part of our setup procedure starts with the calculation of N\napproximations to the smallest EVs and EWs {(\u03bbi , \u03c6i )} of P\u2020 DP by means of harmonic Ritz vectors and values. The approximate EVs \u03c6i for i = 1, . . . , N are successively interpolated to the next\nfiner grid and smoothed towards their harmonic Ritz values with some steps of SAP with iteration\nmatrix S(\u03bbi )\u03b9 . In this case the local inverses for the pair (\u03bbi , \u03c6i ) are given by\nT\nT\nB j (\u03bbi ) = IL j [IL\n(D \u2212 \u03bbi )IL j ]\u22121 IL\n.\nj\nj\n\nThe resulting set of vectors and the old test vectors V := {S(\u03bbi )\u03b9 P\u03c6i : i = 1, . . . , N} \u222a {S\u03bd v j : j =\n1, . . . , N} are again reduced to N vectors. In order to preserve the most significant information, the\nN smallest singular values and their corresponding vectors of the 2N \u00d7 2N matrix (DV )\u2020 DV are\ncalculated and the corresponding N linear combinations of the vectors of V are the final vectors V\u0302 .\nThey define the interpolation from the next coarser grid to the current grid and the operator on the\nnext coarser grid. The second part of the setup executes this procedure exactly once, starting on the\ncoarsest grid.\n5\n\n\fM. Rottmann\n\nMultilevel Methods\n\n2\n\n2\nfine grid\ncoarse grid\n\n1.5\n1\n\n1\n\n0.5\n\n0.5\n\n0\n\n0\n\n\u22120.5\n\n\u22120.5\n\n\u22121\n\n\u22121\n\n\u22121.5\n\n\u22121.5\n\n\u22122\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nfine grid\ncoarse grid\n\n1.5\n\n\u22122\n\n7\n\nFigure 4: Spectra after the first setup step, 44 Wilson, 24 aggregate size\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nFigure 5: Spectra after the second setup step, 44\nWilson, 24 aggregate size\n\nFigures 4 and 5 illustrate the influence of the second part of the setup procedure on the coarse\ngrid operator in a two level hierarchy. The smallest eigenvalues of the fine grid operator are much\nbetter represented on the coarse grid. For other setup procedures, see e.g. [4, 2, 3].\n\n4. Results\nOur adaptive DD multilevel solver (\u03b1MG-DD) has been implemented in C using the parallelization interface of MPI. Krylov subspace methods have been implemented within a common\nframework for a fair comparison. For the numerical experiments we have combined our multigrid\nsolver against CGN, i.e. CG on the normal equation. All results were produced with a two level\nmethod. The stopping criterion was to reduce the initial residual norm by a factor of at least 1010 .\nThe solutions of the block systems within SAP were approximated by 3 iterations of GMRES and\nthe coarse grid equation (3.1) was approximately solved with 12 iterations of GMRES. For the\nouter flexible GMRES routine we chose a restart length of 25. All results have been computed on\nJuropa at the J\u00fclich Supercomputing Centre.\n\nsetup\nsolve\ntotal\n\ntimings\niterations\ntimings\ntimings\n\nFGMRES\n+ \u03b1MG-DD\n45.22s\n9\n6.97s\n52.19s\n\nCGN\n4476\n121.82s\n121.82s\n\nsetup\nsolve\ntotal\n\nTable 1: 48 \u00d7 243 Clover Wilson-Dirac, \u03b2 = 5.3\n(1/a = 2.56 GeV), \u03ba = 0.13590 (m\u03c0 = 630 MeV),\ncsw = 1.90952, generated using public code with parameters from L. Del Debbio [9], block-size 34 , coarsening 34 \u00d7 6 \u2192 20, 512 cores (Juropa at JSC).\n\ntimings\niterations\ntimings\ntimings\n\nFGMRES\n+ \u03b1MG-DD\n24.85s\n17\n5.05s\n29.90s\n\nCGN\n9181\n119.04s\n119.04s\n\nTable 2: 64 \u00d7 323 Wilson-Dirac 2HEX smeared\ntree level improved Clover, \u03b2 = 3.5 (1/a =\n2.130 GeV), \u03ba = 0.12646 (m\u03c0 = 300 MeV),\ncsw = 1.0, provided by the BMW collaboration [10, 11], block-size 24 , coarsening 44 \u00d7 6 \u2192\n20, 4096 cores (Juropa at JSC).\n\nIn the cases shown our multigrid solver was 17 and 24 times faster than CGN. Including the\nsetup time we are still twice and four times as fast as CGN. Since the setup has to be done only\nonce the benefits of our approach are larger the more right hand sides there are to be solved.\n\n5. Summary and Outlook\nThe developed method combining DD techniques and algebraic multigrid shows great potential to speed-up calculations of propagators in lattice QCD. Even for single right hand sides our\n6\n\n\fM. Rottmann\n\nMultilevel Methods\n\nmethod outperforms conventional Krylov subspace methods with the potential of an even more\nsignificant speed-up when solving for many right hand sides. This result is mainly due to the introduction of the highly parallel DD smoother and the bootstrap setup into the algebraic multigrid\nmethod. While the first speeds up the setup of the method and the subsequent solution the latter\nsignificantly speeds up the setup of the multigrid hierarchy, which in general is a bottleneck in\nalgebraic multigrid methods especially compared to setup-free Krylov subspace methods. We are\ncurrently working on an optimized version of the code that should be able to run on large-scale\nparallel machines and extend our testing of the method towards larger lattices and lighter quark\nmasses. In the near future we plan to incorporate our algorithm into the production codes of our\ncollaborators within SFB TR55.\nAcknowledgments: This work is funded by Deutsche Forschungsgemeinschaft (DFG) Transregional Collaborative Research Centre 55 (SFB TR55).\n\nReferences\n[1] M. Luscher, \"Solution of the Dirac equation in lattice QCD using a domain decomposition method,\"\nComput.Phys.Commun. 156 (2004) 209\u2013220, arXiv:hep-lat/0310048 [hep-lat].\n[2] A. Brandt, J. Brannick, K. Kahl, and I. Livshits, \"Bootstrap AMG.,\" SIAM J. Sci. Comput. 33 no.~2,\n(2011) 612\u2013632.\n[3] M. Brezina, R. Falgout, S. MacLachlan, T. Manteuffel, S. McCormick, and J. Ruge, \"Adaptive\nsmoothed aggregation (\u03b1SA) multigrid,\" SIAM Review 47, No. 2 (2005) 317\u2013346.\n[4] J. Osborn, R. Babich, J. Brannick, R. Brower, M. Clark, et al., \"Multigrid solver for clover fermions,\"\nPoS LATTICE2010 (2010) 037, arXiv:1011.2775 [hep-lat].\n[5] R. Babich, J. Brannick, R. Brower, M. Clark, T. Manteuffel, et al., \"Adaptive multigrid algorithm for\nthe lattice Wilson-Dirac operator,\" Phys.Rev.Lett. 105 (2010) 201602, arXiv:1005.3043\n[hep-lat].\n[6] J. Brannick, R. Brower, M. Clark, J. Osborn, and C. Rebbi, \"Adaptive Multigrid Algorithm for Lattice\nQCD,\" Phys.Rev.Lett. 100 (2008) 041601, arXiv:0707.4018 [hep-lat].\n[7] H. Schwarz, \"Gesammelte mathematische Abhandlungen,\" Vierteljahrschrift Naturforsch. Ges.\nZ\u00fcrich (1870) 272\u2013286.\n[8] M. Luscher, \"Local coherence and deflation of the low quark modes in lattice QCD,\" JHEP 0707\n(2007) 081, arXiv:0706.2298 [hep-lat].\n[9] L. Del Debbio, L. Giusti, M. L\u00fcscher, R. Petronzio, and N. Tantalo, \"QCD with light Wilson quarks\non fine lattices (I): First experiences and physics results,\" JHEP 0702 (2007) 056,\narXiv:hep-lat/0610059 [hep-lat].\n[10] S. Durr, Z. Fodor, C. Hoelbling, S. Katz, S. Krieg, et al., \"Lattice QCD at the physical point: light\nquark masses,\" Phys.Lett. B701 (2011) 265\u2013268, arXiv:1011.2403 [hep-lat].\n[11] S. Durr, Z. Fodor, C. Hoelbling, S. Katz, S. Krieg, et al., \"Lattice QCD at the physical point:\nSimulation and analysis details,\" JHEP 1108 (2011) 148, arXiv:1011.2711 [hep-lat].\n\n7\n\n\f"}