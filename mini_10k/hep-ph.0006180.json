{"id": "http://arxiv.org/abs/hep-ph/0006180v1", "guidislink": true, "updated": "2000-06-15T21:42:34Z", "updated_parsed": [2000, 6, 15, 21, 42, 34, 3, 167, 0], "published": "2000-06-15T21:42:34Z", "published_parsed": [2000, 6, 15, 21, 42, 34, 3, 167, 0], "title": "The Future of Particle Physics", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=hep-ph%2F0602194%2Chep-ph%2F0602203%2Chep-ph%2F0602004%2Chep-ph%2F0602098%2Chep-ph%2F0602107%2Chep-ph%2F0602012%2Chep-ph%2F0602154%2Chep-ph%2F0602017%2Chep-ph%2F0602113%2Chep-ph%2F0602146%2Chep-ph%2F0602204%2Chep-ph%2F0602110%2Chep-ph%2F0602179%2Chep-ph%2F0602086%2Chep-ph%2F0602089%2Chep-ph%2F0602083%2Chep-ph%2F0602021%2Chep-ph%2F0602001%2Chep-ph%2F0602126%2Chep-ph%2F0602168%2Chep-ph%2F0602232%2Chep-ph%2F0602036%2Chep-ph%2F0006265%2Chep-ph%2F0006216%2Chep-ph%2F0006103%2Chep-ph%2F0006078%2Chep-ph%2F0006071%2Chep-ph%2F0006053%2Chep-ph%2F0006334%2Chep-ph%2F0006297%2Chep-ph%2F0006283%2Chep-ph%2F0006059%2Chep-ph%2F0006118%2Chep-ph%2F0006065%2Chep-ph%2F0006222%2Chep-ph%2F0006033%2Chep-ph%2F0006138%2Chep-ph%2F0006051%2Chep-ph%2F0006020%2Chep-ph%2F0006088%2Chep-ph%2F0006215%2Chep-ph%2F0006296%2Chep-ph%2F0006261%2Chep-ph%2F0006346%2Chep-ph%2F0006167%2Chep-ph%2F0006322%2Chep-ph%2F0006029%2Chep-ph%2F0006156%2Chep-ph%2F0006066%2Chep-ph%2F0006141%2Chep-ph%2F0006341%2Chep-ph%2F0006317%2Chep-ph%2F0006201%2Chep-ph%2F0006085%2Chep-ph%2F0006127%2Chep-ph%2F0006189%2Chep-ph%2F0006284%2Chep-ph%2F0006269%2Chep-ph%2F0006069%2Chep-ph%2F0006018%2Chep-ph%2F0006105%2Chep-ph%2F0006194%2Chep-ph%2F0006180%2Chep-ph%2F0006031%2Chep-ph%2F0006209%2Chep-ph%2F0006264%2Chep-ph%2F0006258%2Chep-ph%2F0006004%2Chep-ph%2F0006271%2Chep-ph%2F0006054%2Chep-ph%2F0006126%2Chep-ph%2F0006096%2Chep-ph%2F0006173%2Chep-ph%2F0006239%2Chep-ph%2F0006310%2Chep-ph%2F0006207%2Chep-ph%2F0006184%2Chep-ph%2F0006247%2Chep-ph%2F0006091%2Chep-ph%2F0006017%2Chep-ph%2F0006275%2Chep-ph%2F0006040%2Chep-ph%2F0006254%2Chep-ph%2F0006159%2Chep-ph%2F0006311%2Chep-ph%2F0006024%2Chep-ph%2F0006232%2Chep-ph%2F0006241%2Chep-ph%2F0006176%2Chep-ph%2F0006129%2Chep-ph%2F0006007%2Chep-ph%2F0006093%2Chep-ph%2F0006149%2Chep-ph%2F0006134%2Chep-ph%2F0006080%2Chep-ph%2F0006050%2Chep-ph%2F0006292%2Chep-ph%2F0006282%2Chep-ph%2F0006073%2Chep-ph%2F0006279%2Chep-ph%2F0006196&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Future of Particle Physics"}, "summary": "After a very brief review of twentieth century elementary particle physics,\nprospects for the next century are discussed. First and most important are\ntechnological limits of opportunities; next, the future experimental program,\nand finally the status of the theory, in particular its limitations as well as\nits opportunities.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=hep-ph%2F0602194%2Chep-ph%2F0602203%2Chep-ph%2F0602004%2Chep-ph%2F0602098%2Chep-ph%2F0602107%2Chep-ph%2F0602012%2Chep-ph%2F0602154%2Chep-ph%2F0602017%2Chep-ph%2F0602113%2Chep-ph%2F0602146%2Chep-ph%2F0602204%2Chep-ph%2F0602110%2Chep-ph%2F0602179%2Chep-ph%2F0602086%2Chep-ph%2F0602089%2Chep-ph%2F0602083%2Chep-ph%2F0602021%2Chep-ph%2F0602001%2Chep-ph%2F0602126%2Chep-ph%2F0602168%2Chep-ph%2F0602232%2Chep-ph%2F0602036%2Chep-ph%2F0006265%2Chep-ph%2F0006216%2Chep-ph%2F0006103%2Chep-ph%2F0006078%2Chep-ph%2F0006071%2Chep-ph%2F0006053%2Chep-ph%2F0006334%2Chep-ph%2F0006297%2Chep-ph%2F0006283%2Chep-ph%2F0006059%2Chep-ph%2F0006118%2Chep-ph%2F0006065%2Chep-ph%2F0006222%2Chep-ph%2F0006033%2Chep-ph%2F0006138%2Chep-ph%2F0006051%2Chep-ph%2F0006020%2Chep-ph%2F0006088%2Chep-ph%2F0006215%2Chep-ph%2F0006296%2Chep-ph%2F0006261%2Chep-ph%2F0006346%2Chep-ph%2F0006167%2Chep-ph%2F0006322%2Chep-ph%2F0006029%2Chep-ph%2F0006156%2Chep-ph%2F0006066%2Chep-ph%2F0006141%2Chep-ph%2F0006341%2Chep-ph%2F0006317%2Chep-ph%2F0006201%2Chep-ph%2F0006085%2Chep-ph%2F0006127%2Chep-ph%2F0006189%2Chep-ph%2F0006284%2Chep-ph%2F0006269%2Chep-ph%2F0006069%2Chep-ph%2F0006018%2Chep-ph%2F0006105%2Chep-ph%2F0006194%2Chep-ph%2F0006180%2Chep-ph%2F0006031%2Chep-ph%2F0006209%2Chep-ph%2F0006264%2Chep-ph%2F0006258%2Chep-ph%2F0006004%2Chep-ph%2F0006271%2Chep-ph%2F0006054%2Chep-ph%2F0006126%2Chep-ph%2F0006096%2Chep-ph%2F0006173%2Chep-ph%2F0006239%2Chep-ph%2F0006310%2Chep-ph%2F0006207%2Chep-ph%2F0006184%2Chep-ph%2F0006247%2Chep-ph%2F0006091%2Chep-ph%2F0006017%2Chep-ph%2F0006275%2Chep-ph%2F0006040%2Chep-ph%2F0006254%2Chep-ph%2F0006159%2Chep-ph%2F0006311%2Chep-ph%2F0006024%2Chep-ph%2F0006232%2Chep-ph%2F0006241%2Chep-ph%2F0006176%2Chep-ph%2F0006129%2Chep-ph%2F0006007%2Chep-ph%2F0006093%2Chep-ph%2F0006149%2Chep-ph%2F0006134%2Chep-ph%2F0006080%2Chep-ph%2F0006050%2Chep-ph%2F0006292%2Chep-ph%2F0006282%2Chep-ph%2F0006073%2Chep-ph%2F0006279%2Chep-ph%2F0006196&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "After a very brief review of twentieth century elementary particle physics,\nprospects for the next century are discussed. First and most important are\ntechnological limits of opportunities; next, the future experimental program,\nand finally the status of the theory, in particular its limitations as well as\nits opportunities."}, "authors": ["James D. Bjorken"], "author_detail": {"name": "James D. Bjorken"}, "author": "James D. Bjorken", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1142/S0217751X01003226", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/hep-ph/0006180v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/hep-ph/0006180v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Invited talk given at the International Conference on Fundamental\n  Sciences: Mathematics and Theoretical Physics, Singapore, 13-17 March 2000", "arxiv_primary_category": {"term": "hep-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "hep-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/hep-ph/0006180v1", "affiliation": "SLAC", "arxiv_url": "http://arxiv.org/abs/hep-ph/0006180v1", "journal_reference": "Int.J.Mod.Phys. A16 (2001) 483-502", "doi": "10.1142/S0217751X01003226", "fulltext": "arXiv:hep-ph/0006180v1 15 Jun 2000\n\nSLAC\u2013PUB\u20138468\nJune 2000\n\nThe Future of Particle Physics\n\n\u2217\n\nJames D. Bjorken\nStanford Linear Accelerator Center\nStanford University, Stanford, California 94309 USA\nE-mail: bjorken@slac.stanford.edu\n\nAbstract\nAfter a very brief review of twentieth century elementary particle physics, prospects\nfor the next century are discussed. First and most important are technological limits\nof opportunities; next, the future experimental program, and finally the status of the\ntheory, in particular its limitations as well as its opportunities.\n\nInvited talk given at the\nInternational Conference on Fundamental Sciences:\nMathematics and Theoretical Physics\nSingapore\n13\u201317 March 2000\n\n\u2217\n\nWork supported by Department of Energy contract DE\u2013AC03\u201376SF00515.\n\n\f1\n\nPreamble\n\nIt is a real pleasure to be back in Singapore and have the opportunity to address this\nmeeting. About a decade ago, I gave a similar talk here, as a last-minute substitute for\nLeon Lederman [1]. Since that time I have for the most part exited theoretical physics\nand entered experimental physics. In particular I have been promoting new detector\ntechniques for exploring the world of strong interactions. This led to my colleague\nCyrus Taylor, a string theorist by training, and I leading a small test/experiment\nin the Fermilab Tevatron Collider. The results of this experiment were modest [2].\nBut the experience was in many ways extremely rewarding and enriching, and in\nother ways frustrating. However, as the new century begins, I have emerged from the\nexperimental trenches and am entering the world of retirement. I now do have the\nopportunity to again take a look at the big picture, after a decade of time off. And\nthe invitation to give this talk provided a most opportune way to organize my own\nthoughts and to try to express them.\n\n2\n\nThe Big Picture\n\nTwentieth century physics featured several great syntheses, The first was the extraordinary synthesis by Planck of thermodynamics with Maxwell electrodynamics,\ngiving birth to quantum theory. It was quickly followed by the synthesis of classical mechanics with electrodynamics by Einstein, giving birth to special relativity.\nAnd in the 1920s, with the decisive synthesis of Newtonian mechanics with the \"old\"\nquantum mechanics by Heisenberg and Schrodinger, there also emerged quantum\nelectrodynamics (or QED), the synthesis of Maxwell electrodynamics with quantum\nmechanics.\nAll this happened in less than three decades. Much of the remaining history for\nthe century belongs to the experiments, which built upon these foundations and in\nparticular drove the development of particle physics. Many particles were discovered. The strong and weak forces themselves needed to be discovered before there\nwas any opportunity to understand their role. Eventually things became understood\nwell enough that by the 1970s there was the possibility of a synthesis of weak and\nelectromagnetic forces into a common structure: the SU(2) \u00d7 U(1) electroweak component of the Standard Model. And the strong force is now successfully described\nby the generalization of QED called quantum chromodynamics (QCD), based on the\nexact SU(3) color symmetry of quarks and their force carriers, the gluons.\nAll these forces (including gravitation) are described at short distances by the same\nclass of fundamental theories, the so-called gauge theories. The force law at short\ndistances is in all cases essentially inverse-square. Standard Model forces are proportional to a variety of conserved charges, while gravity couples to energy-momentum.\nSo as we enter the twenty-first century, there is a strong anticipation that these four\nforces have a common origin and that further synthesis is on the way.\n2\n\n\fThere are quite specific clues to build upon. One clue is the pattern of fundamental building blocks of matter: the quarks and leptons. They are exhibited explicitly\nas building blocks in Fig. 1. This is a construction which has technical meaning\nonly when viewed beyond the Standard Model, when the SU(3) \u00d7 SU(2) \u00d7 U(1)\nStandard Model symmetry group is embedded in the much larger symmetry group of\nten-dimensional rotations, SO(10). A clumsy, reducible representation of the Standard Model group becomes an elegant, single sixteen-dimensional fundamental spinor\nrepresentation of SO(10). In that context the 5-dimensional cube which is depicted\nin Fig. 1 has a definite mathematical meaning.\n\ne+\n\nu\n\u2013\nd\n\nd\n\u2013\nu\n\n\u2013\nN\n\n\u2013\nd\n\n\u2013\nu\n\nd\n\n\u2013\n\u03bde\nd\n\n\u2013\nu\n\nd\n\u2013\nu\n\nu\n\nd\n\nN\n\n\u2013\nu\n\nu\n\u2013\nd\n\nu\n\u2013\nd\n\n\u03bde\n\ne\u2013\n\n\u2013\nd\n\nu\n\u2013\nd\n\ne+\n\nu\n\ne\u2013\n\nd\n\u2013\nu\n5-2000\n8541A1\n\nFigure 1: Building-blocks of the Standard Model. (The solid dots comprise the\nSO(10) 16 for the two-component, left-handed fermion degrees of freedom; the open\ndots comprise the 16 antiparticle representation.)\nA second clue, pointing in the same direction, has to do with the fact that the\ncoupling strengths or \"charges\" associated with the three kinds of Standard Model\n\"gauge\" forces vary slowly with distance scale. It happens that they converge (or very\nnearly converge) to a common value at a very high mass scale, 1015 GeV, give or take\na factor 10, as shown in Fig. 2. It is at this scale that synthesis of these three forces\ncan be expected to occur. This anticipated synthesis is known as Grand Unification.\nEven one or two generations ago, it was the dream of every theorist to come to\nan understanding of the value of the pure number 1/137 of QED, a number which\ncharacterizes the intrinsic strength of the electromagnetic force at the quantum level.\n3\n\n\fThis is no longer the case. The 137 evolves to 128 through \"vacuum polarization\"\neffects; cf. Fig. 2. At this point, at a mass scale of about 100 GeV, the electroweak\nsynthesis takes place. Thereafter it is the coupling strength of a mixture of photon\nand electroweak boson which, together with weak-boson and gluon coupling strengths,\nevolve from values of about 1/60, 1/30, and 1/10 respectively to the common value\nof about 1/40 at the grand-unification scale. So the 137 has been divided by a factor\nof about 3 1/2, and the question is now \"Why 1/40?\"\n\n120\n\nQED\n\n1 80\n\u2013\n\u03b1i\nU(1)\nSU(2)\n\n40\n\nSU(3)\nQCD\n0\n5-2000\n8541A2\n\n100\n\n106\n\n1012\nGeV\n\nFigure 2: The \"fine structure constants\" of the basic forces as function of momentum\nscale. Note the discontinuity in the electromagetic coupling strength at the electroweak scale, due to the replacement of U(1)CM with the Standard Model U(1)Y .\nThere is another synthesis taking place at present, that of cosmology and particle\nphysics. Twentieth century astronomy and astrophysics has its own rich history. But\nwith the emergence of Big Bang cosmology in the last forty years, there has also\nemerged an increasingly strong interdependence of cosmology and particle physics.\nThe high temperatures present during the early epochs of the Big Bang demand\na good understanding of particle physics in any theoretical description, while the\nempirical evidence which constrains theories of the early history of the universe also\nconstrains the theories of particle physics.\nSo there is now an increasingly strong interconnection of the largest distances with\nthe smallest, and of the longest time scales with the shortest. This is illustrated in\nFig. 3, which can truly be called the Big Picture. In that figure every effort has been\n4\n\n\fmade to omit the superfluous. What is left is what I personally believe to be the key\nlandmarks necessary to comprehend when moving on to the next syntheses.\n\neV\n100\n\n1020\n\n10\u201320\n\n5-2000\n8541A3\n\nth\n\nSiz\neo\n\nof\n\nfU\n\nniv\ners\n\ne\n\nGalactic\nBlack Hole\n\nSc\n\nale\n\nng\nal W\nave\nle\n\nGrand Unification Scale\nTh\n\n10\u201320\n\nerm\n\n100\n\nPlanck Scale\n\nct (cm)\n\n1020\n\nElectroweak Scale\nBla\nckb\nod\nyP\nho\nton\ns\nCosmological Constant\n\nNow\n\nInflation?\n\n100\ncm\n\n10\u201320\n\n1020\n\nFigure 3: The Big Picture: important values of distance (or momentum) scales versus\ntime (as measured from the Big Bang).\nNothing of the Big Picture in Fig. 3 existed a century ago. That it exists now\nis a tribute to the extraordinary scientific progress made in that period. Progress in\nphysical science has three components: technological, experimental, and theoretical.\nThese are interconnected, but I believe that the order of importance is as stated.\nWithout technological advances, experimental technique stagnates. And without the\nvalidations and unanticipated discoveries that comes from advances in experiment,\nthe finest creations of theoretical physics languish as exercises in natural philosophy\nor in higher mathematics, and are of little worth as physical theory.\n\n3\n\nTechnology\n\nIn particle physics the name of the game is energy. Throughout the twentieth century\nthere has been exponential growth in the attainable center-of-mass energy available\nfor particle collisions, beginning with a few electron-volts in early vacuum tubes and\nending with the trillions of electron volts in the Fermilab Tevatron Collider [3]. On\naverage this amounts to a doubling time of about 2-1/2 years.\nThis pace is unlikely to be equaled in the coming century. The slowing of the pace\nhas already been apparent for the last decade or two. The new machines are big and\n5\n\n\fexpensive, and take a long time to build and to fully exploit. But this does not mean\nan end to the field. There is plenty of room for further expansion of the possibilities,\nthroughout the coming century.\nModern colliding beam machines are circular storage rings, within which some\ncombination of counter-rotating beams of electrons, positrons, protons, and/or antiprotons collide with each other. The biggest electron-positron collider is the LEP\nring at the CERN laboratory [4] in Geneva, Switzerland, 27 kilometers in circumference, and now running at a cms energy of about 200 GeV. This represents the\nlimit of possibility for this kind of ring. The electrons and positrons emit so much\nsynchrotron radiation, and the rate grows so rapidly with increasing energy, that it\nis impractical to contemplate similar machines at much higher energies. This is not\nthe case for the heaver protons, where ten times that energy is attained in the Fermilab proton-antiproton collider already. And the Large Hadron Collider (LHC), under\nconstruction at CERN in the LEP tunnel itself [5], will increase this value sevenfold.\nSooner or later, a similar synchrotron-radiation limit will be reached, however, especially given that superconducting magnets are needed to bend the protons, and that\nsuperconducting systems and intense beams of x-rays tend not to peacefully coexist.\nA rough estimate of where the ultimate limit for circular proton rings occurs puts\nthe number at a few hundred TeV per beam. This is between one and two orders of\nmagnitude larger than the LHC, and implies that at least one more very big collider\nis technically feasible [6], and perhaps affordable on a world scale sometime in the\nfuture.\nSuch a machine would be very big, probably over a thousand kilometers in circumference. It would require a lot of (underground) real estate, and for a long time it\nhas seemed to me that this region of the planet, with Down Under so close at hand,\nis a natural focus when dreaming about this possibility. Given the premise that the\ntwenty-first century will witness not only an economic but also scientific and cultural\nblossoming within Southeast Asia, such a project in this region might be an appropriate and locally beneficial way of entering this fundamental field of science. The time\nscale for even considering such an initiative is not at all immediate, but could be as\nshort as two or three decades. And as we shall see, there are scientific considerations\nas well to deal with. What is done in the far future depends in an important way on\nwhat will be learned from the experiments performed in the nearer future.\nWhen thinking about higher energy electron-positron collisions, the technique of\nchoice is to accelerate the beams within straight-line, linear accelerators, as done\nin my home institution SLAC at Stanford, where collisions with 50 GeV electrons\nand positrons per beam have been attained [7]. Another factor of 10 to 20 can be\ncontemplated using relatively straightforward extensions of existing techniques. The\nmain issue is to do this in an energy-efficient, economical way. A great deal of study\nfor the \"Next Linear Collider\" (NLC) is being carried out, with a large amount of\ninternational collaboration. And some kind of NLC is a leading candidate, perhaps\nthe leading candidate, for the next large facility beyond the LHC needed to further\npush back the high energy frontier [8].\n6\n\n\fHowever there is a relatively new, competitive idea being studied as well, namely\nto collide muons and antimuons (the unstable, heavier second-generation copies of\nthe electrons and positrons) with each other within circular storage rings (cf. Fig. 4).\nAt first sight this idea looks crazy, but after careful scrutiny it becomes less crazy,\nalbeit difficult and adventurous [9]. One physics advantage of muons over electrons\nand positrons is that resonant production of a Higgs boson is much larger and possibly observable. Another advantage is that muons do not emit so much synchrotron\nradiation. However many of them do decay en route to the collision point, and their\ndecay products create a serious nuisance. An extremely intense source of primary\nmuons is required.\n\nProton\nSynchrotron\n\n\u03bc\u2013\np\u2192\u03c0\u2192\u03bc\n\nMuon\nCooler\n\nStorage\nRing\n\nLinac\n\n\u03bc\u2013\n\u03bc+\n\nLinac\n\n\u03bc+\n\n5-2000\n8541A4\n\nFigure 4: A schematic of a muon collider. A very intense proton source creates a\nbeam which is extracted and targeted. The pions and decay muons are focussed\nand collected. The large phase space occupied by the muon beam is diminished by\n\"ionization cooling\". Then the muons are accelerated in a racetrack linac before being\ninjected into the storage-ring collider.\nBut a general advantage of a muon collider facility is that there are many sidebenefits. One can easily see that each component of the facility can-and should-\nsupport other physics programs. A very intense source of protons of tens of GeV is\nrequired, and this by itself is a good device for studying properties of the hadrons, including e.g. the rare decays of kaons. And the muons are accelerated in a recirculating\nlinear accelerator which is a more powerful and sophisticated version of a facility, CEBAF, now being used to study hadron structure via electron scattering [10]. Perhaps\nthe most promising secondary application is utilization of the intense beams of neutrinos created by the decaying muons. Just in the last few years, with the emergence\nof the evidence for neutrino mass and mixing from non-accelerator experiments, there\nhas been an escalating interest within the particle accelerator community in creating\na \"neutrino factory\" using muon-collider technology. Such a program would provide\nuseful neutrino physics as well as providing proof-of-principle evidence that the idea\nof creating intense, bright muon beams really can be made to work [9].\nSo while consideration of an NLC, with center-of mass energy of up to 1\u20132 TeV,\nwill be high on the options list of future facilities beyond the LHC, the muon colliders,\nwhich might reach to a few TeV in the center of mass were all to go well, will also\n7\n\n\fbe under consideration. It does have to be said that the muon-collider technology\nis new and untested, so that it is reasonable to expect that if that route is followed\nit will be considerably further into the century before one could contemplate high\nenergy muon collisions being attained. In either case, the reason that such lepton\ncolliders are competitive with the much higher energy pp colliders, such as the LHC,\nis the efficient conversion of all the energy in each event into interesting physics.\nThe advantage of higher energy in pp colliders is mitigated by the complexity of the\ncollisions and the relative rarity of the collisions which produce the highest-priority\n\"new physics\".\nIn addition to the energy frontier, there will be many other frontiers remaining\nwithin particle physics. There are good reasons for producing interesting particles\nwith the highest intensities possible. Accelerator facilities which specialize in production of a single kind of particle are already in abundance: we already see B factories,\nZ factories, K factories, and \u03c6 factories. And as mentioned above, we probably will\nsee neutrino factories in the future. Once the Higgs sector, the prime target of experimental physics nowadays, is found, there will be an irresistible urge to create a Higgs\nfactory as well, probably utilizing some kind of lepton-lepton collider. It is likely that\nthe \"factory\" programs will have great longevity.\nAnother frontier is that of non-accelerator facilities, utilizing for example nature's\nown particle beams, be they cosmic rays or neutrinos from the sun. When the ideas\nof grand unification emerged (Fig. 2), there also emerged the prediction of proton\ndecay. This was a real turning point for the field of non-accelerator particle physics,\nbecause thereafter the large-scale investment in huge detectors became commonplace,\nand the level of sophistication relative to previous, traditional cosmic ray research\nescalated considerably. Even today the investment in non-accelerator particle physics\nis a relatively small fraction of the total, so that important measurements may be\nexpected in the future to be approached on a scale larger than done at present. The\nrecent advances in neutrino physics, especially in Japan with SuperKamiokande [11],\nbear witness to the value of making a large, sound investment in a good detector\nwhen the potential benefits warrant it.\nBut the non-accelerator experiments will forever be very difficult and fraught with\nmore uncertainty that the more highly controllable accelerator experiments. So there\nwill be strong motivation to push the high energy frontier well beyond what we have\ntalked about so far. It is here that very difficult barriers appear.\nIf one wants to attain center-of-mass energies well beyond 1000 TeV, there seems\nto be very little choice but to do it with linear acceleration. If we ask to do this within\na reasonable distance, say 100 km, this implies an average acceleration gradient of at\nleast 10 GeV per meter, or 1 eV per Angstrom. High powered lasers can in principle\nprovide forces of such a magnitude. But such forces, if present in matter, are strong\nenough to pull electrons out of metals, or in general destroy the orderly chemical\nstructure of just about any material. There is a general rule, Lawson's theorem, that\nstates that free electromagnetic waves do not make a good accelerator, essentially\nbecause the electric vector is at right angles to the Poynting vector.\n8\n\n\fSo a good accelerator will almost certainly have a material structure close to the\nbeam. or perhaps a plasma within the beam. And this material structure might well\nbe damaged or destroyed by the laser pulse or other source of the accelerating field\nevery time the device is pulsed. I envisage this as the insertion of an accelerating\nstructure into the beam, instead of the present practice of insertion of a beam into\nthe accelerating structure.\nThe notion of an accelerator which is destroyed by the beam during every pulse is\nnot necessarily a hopeless one, especially if the transverse dimensions of the machine\nare kept very small, not much larger than the beam itself. The small transverse\ndimensions are probably a necessity anyway on grounds of the energy budget. One\ncannot afford to fill a large cavity with a lot of energy, as presently done, especially\nwhen it does not directly contribute to the acceleration of the beam. So it could\nbe that the accelerating structure is in the form of a sequence of structured tapes,\nor edges of structured rotating disks, which move through the beam region. The\ntransverse scale might be anything from microns to millimeters.\nAnd there is a new technology, nanotechnology, which although not viable now,\nmight in the future evolve to the point that accurate, practical, and inexpensive (i.e.\ndisposable!) acceleration microstructures might eventually be fabricated. Nevertheless, the trouble list associated with such a line of thinking is impressively formidable.\nA huge obstacle is that of maintaining the brightness of the beam, i.e. keeping the\nbeam size small. The typical unit of periodicity (cell length) of such a \"miniaturized\"\nlinear accelerator might be envisaged to be in the range of millimeters at most. So\nin a 100 km machine, there are at least 108 such cells. And the phase-space volume\nof the beam (emittance) cannot increase on average by much more than 1 part in\n108 per cell or it will grow unacceptably large by the end of the machine. So the reproducibility of these extremely violent acceleration mechanisms must be maintained\nto a very high level of accuracy. I am sure this is only one of a number of similar\nproblems.\nThere is in addition another great difficulty, which is motivation. It is a folk\ntheorem in the trade that a project does best if there is strong physics motivation\nfor it. The sense of urgency creates focus, drives the project forward, and encourages\neveryone to contribute that extra level of creative effort to make it successful. An\nR&D program on very high gradient linear accelerators will naturally be restricted to\nshort prototypes for a long time, and therefore not be highly physics driven. And the\nprimary physics demand is not only for high energy but also for an extremely high\ncollision rate. Typically, for each factor of ten of energy increase, one needs to increase\nthe luminosity, or collision rate, by a factor of order one hundred. But there are\nsecondary physics topics which can be addressed. For example, were 10 GeV electron\nor proton beams to be produced, economically, on a tabletop in the basement of some\nuniversity physics department, some physics uses for them might well be found. This\nis especially true for proton beams, where many physics applications do not require\nthe relatively large intensities that electron-beam physics requires.\n9\n\n\fAt present, there is actually considerable activity in the field of high gradient\nlinear acceleration, not only theoretical but increasingly experimental [12]. It seems\nto me that there should be a high level of attention paid to this problem, even though\nit does not look to be immediately practical, and even though super-high gradients\ndo not seem to be needed for at least a couple of generations of machines, in order\nto push the energy frontier forward. Sometime in the future the field will have to\nface up to this ultimate problem, and doing the homework now would seem to be a\nprudent strategy.\n\n4\n\nExperiments\n\nIn the realm of accelerator-based particle physics, the technology of particle detectors is highly advanced. In the future, as prognosticated in the previous section,\nrefinements and extensions of the existing detector technologies can be expected to\noccur, especially with regard to rate capability, pattern recognition, resolution, and\nmanagement of increasing data volume. But I at least do not perceive any need for\na fundamental change of approach. The most difficult frontier is the providing of\nthe higher energy collisions themselves, not in detection and analyzing the collision\nproducts.\nPerhaps the most interesting experimental frontier is sociological. The size and\ncomplexity of experimental collaborations continues to grow. At present we see, at the\nLHC, collaboration sizes approaching two thousand, with participation of hundreds\nof collaborating institutions around the globe. And the time scale for construction\nand full exploitation of a single detector for a single experimental program rivals the\nfull career length of the participating physicists. When I was here in Singapore the\nlast time, I devoted quite a lot of time to this issue [1]. The social problems are\nstill there. But there remains a large, enthusiastic young generation within the big\ndetector collaborations, and I believe that the system, although very different from\nwhat it was two or three decades ago, still allows a highly creative and stimulating\nscientific environment.\nAnother singular feature of modern experimental particle physics is its professionalism. In the past two decades, the standards applied to all aspects of the experiments,\nespecially with regard to the management and statistical analysis of the data, have\nsoared. This is I think largely due to the size of the collaborations: they are large\nenough to contain experts in all the relevant subspecialties. I think these standards\nare well above what exist in other fields of science, and that participants in big experiments, even if they leave the field of particle physics at some point, are superbly\nprepared to apply that expertise in other scientific or technological applications.\nIndeed, it must not be forgotten that experimental particle physics not only depends upon technological progress, but that it contributes to it. For example, it was\nthe need for efficient communication and for transmission of very large volumes of\ninformation amongst the highly dispersed international community of collaboration\n10\n\n\fmembers that gave birth to the World Wide Web at CERN [13]. It is a perfect example of how research in basic science is not only an intellectual endeavor worthy of\nsupport by society for its own sake, but also an engine that drives economic progress\nin ways that create fundamental changes which are not programmable in advance.\nIt therefore seems to me that it is very important for Singapore and this region\nto not only consider the enhancement of its participation in basic science in the areas of theory, as represented in this meeting, but also to consider participation in\nexperimental and accelerator physics. It is especially easy nowadays to enter the field\nvia the large international experimental collaborations and thereby gain immediate\naccess to the mainstream of the field. The benefits grow in proportion to the investment, and I would urge the creation of conditions that might allow participation of\nespecially the younger generation in particle-physics experimentation.\n\n5\n\nBeyond the New Standard Model\n\nWe now turn to the theoretical situation. I believe, as do many others, that the most\ncrucial problem we face has to do with understanding the mechanism by which the\nquarks, leptons, and electroweak force-carriers (the gauge bosons W and Z) get their\nmass. This problem is characterized by a mass scale at the edge of measurability at\npresent, of order 100 to 1000 GeV. It was well defined already twenty years ago, and\nthe efforts of many experiments, which have included the discovery and measurement\nof the properties of the W , Z, and the sixth, remarkably massive top quark, have\nsimply sharpened the basic issues.\nBut the situation regarding the Standard Model has not remained static, thanks\nto the newest generation of neutrino experiments. They provide strong evidence\nthat neutrinos also have mass and undergo \"mixing\", similar to the way the three\ngenerations of quarks are mixed by the electroweak interactions. This is an extremely\nsignificant discovery, although not really a revolutionary one. It is in fact a very\nwelcome one, one which replace the Old Standard Model by a New Standard Model.\nThe 20 or so free parameters of the Old Standard Model are now increased by at least\n7 (3 neutrino masses and 4 mixing angles) and perhaps 12 new parameters (masses\nfor the 3 N's discussed below plus two other mixing angles), depending upon what\none chooses to include in the count. This by itself may not seem like progress. But\nthe reason that neutrino mass is not all that unwelcome has to do with the Grand\nUnification perspective we mentioned earlier. In the Old Standard Model, there are\n15 spin-1/2 building blocks per generation: the sixteenth degree of freedom N in\nFig. 1 is not included. The natural GUT symmetry group to consider is SU(5), for\nwhich the 15 building blocks fill two multiplets: a 5 and a 10. Neutrino mass, in\nparticular the very small neutrino mass observed, occurs in the SO(10) extension\nwhen the extra degrees of freedom Ni (one for each generation of fermions) possess\nvery large masses of order of the 1015 GeV GUT scale. The ordinary neutrinos mix\nwith the N's, and quantum mechanical level repulsion drives their masses to very\n11\n\n\fsmall values in a natural way. And with the N's present, the pattern of the building\nblocks (Fig. 1) becomes simpler, as mentioned before, being described by a single\n16-dimensional irreducible spinor representation of SO(10). So the New Standard\nModel is to the Old Standard Model as SO(10) is to SU(5): a little bigger, and also\na little prettier. And the transition from \"Old\" to \"New\" should be regarded as a\nvery strong clue in dealing with the question of how to synthesize the strong and\nelectroweak forces at the GUT scale.\nNevertheless, the problem of how the masses of quarks, leptons, W , Z, and neutrinos, are generated still must be faced. There are many ideas on how the problem\nof mass is to be solved. The most economical way requires only one extra particle\nto exist, the famous Higgs boson. In many ways, this simplest of scenarios, called\nthe \"desert\" scenario, works the best. Consistency is maintained all the way to the\nGUT scale of 1015 GeV or so if and only if the mass of the Higgs boson lies in a\nnarrow window: 160 \u00b1 20 GeV. Nevertheless, theorists remain very disquieted by\nthe \"desert\" scenario because there appears a quadratic divergence in the Higgs mass\nwhich must be subtracted away, and it seems very unnatural to set the physical Higgs\nmass to a small value at the electroweak scale of 100 GeV or so, rather than at the\nGUT scale of 1015 GeV.\nHowever, this quadratic divergence problem bears great similarity to the problem of the small value of the cosmological constant. It too suffers from power-law\ndivergences. And the cosmological constant problem cannot be solved with a straightforward appeal to supersymmetry, as is commonplace for the Higgs problem. So a\nserious \"solution\" to this \"hierarchy\" problem is to ignore it, acknowledging that its\nresolution will require a much deeper level of understanding, but recognizing that in\nthe meantime renormalization (subtracting out the infinity) suffices to remove the\nproblem from all known phenomenology.\nBut angst over the \"hierarchy problem\" has in the past 25 years spawned alternative approaches, the most important of which are phenomenological supersymmetry\nand \"technicolor\". Both are characterized by the introduction of many new particles\nas well as new forces [14]. If either is correct, there will be an extraordinarily rich\nexperimental program for the future.\nThe evidence from precision measurements of electroweak processes, especially at\nLEP, seems to favor a Higgs sector of relatively low mass, below 250 GeV [14]. This\nin turn is more in line with the supersymmetry option (\"Minimal Supersymmetric\nStandard Model\", or MSSM), or the \"desert\" scenario than with technicolor, although\nnothing is strictly ruled out. The MSSM is expecially rich in new particles and new\nparameters. Each known particle has its superpartner, differing in spin by 1/2, most\nof which having masses below a few TeV. There are about 100 extra parameters to be\ndetermined through experiment. The Higgs sector is bigger, and it is expected that\nat least one of the Higgs' lies below 130 GeV.\nFrom the experimental point of view, these two alternatives, MSSM and \"desert\",\nstand in stark contrast to one another. If MSSM is correct, then-once the energy scale has increased sufficiently to produce this cornucopia of new particles and\n12\n\n\fyyyy\nyyyy\nyyyy\nyyyyy\n\nphenomena-it will be full employment for experimentalists (as it already has been\nfor the MSSM theorists).\nOn the other hand, suppose none of that is found and only the single Higgs boson\nis seen, with a mass of 160 GeV as predicted by the \"desert\" scenario (cf. Fig. 5).\nIt will be more difficult to motivate new, expensive facilities at higher energies if no\nclear landmarks for new phenomena exist.\n\nHiggs Mass (GeV)\n\n600\n\nHiggs Self-Interaction\nBecome Strong\n\n400\n\n200\n\nmH = 160 \u00b1 20 GeV\n\nVacuum Instability\n\n0\n\n5-2000\n8541A5\n\n106\n\n1012\n\u039b (GeV)\n\n1018\n\nFigure 5: Values of mHiggs and momentum scale for which the Standard Model exists,\ni.e. where electroweak perturbation theory converges. The upper region is forbidden\nbecause the self-interactions of the Higgs particle become strong. The lower region is\nforbidden because the vacuum itself becomes unstable.\n\nBut the MSSM and \"desert\" scenarios are extreme cases. Yet another way of\ndealing with the hierarchy problem is simply not to have a hierarchy at all, but\nto have many new mass scales for new physics between the electroweak and GUT\nscales. In Fig. 6 is plotted versus mass m the number of (2-component) spin 1/2\nfermions possessing mass no larger then m. At the GUT or Planck scale many\ntheories, e.g. superstrings, end up with many hundreds of fermions. For example,\nthree E(8) generations adds up to 744 fermions. At present energies we have \u223c 48. In\nboth the \"desert\" scenario and the MSSM nothing much is supposed to happen across\nall those orders of magnitude. But perhaps there are \"oases\" of new physics all across\nthe desert, which gradually release new degrees of freedom. The hierarchy problem\nbecomes irrelevant. But, just like the \"desert\" scenario for experimentalists, the\n\"oasis\" scenario is a plague for theorists, because the vision of the GUT scale becomes\nobscured by everything which is in between, most of which is beyond experimental\naccess.\n13\n\n\f248 x 3\nGUT\n\nIntegral Number of \"Active\" Fermions\n\n103\n\n102\n\nb t\ns c \u03c4\nd\n\u03bc\nu\n\n101\n\u03bde\n100 \u201312\n10\n\n\u03bd\u03bc\u03bd\u03c4\n10\u20136\n\nMSSM\nDESERT\n\ne\n100\n106\n\u039b (GeV)\n\n1012\n5-2000\n8541A6\n\nFigure 6: Number of (Weyl) spin 1/2 particles (not including antiparticles) with mass\nless than m, versus m, for the \"desert\" scenario and the MSSM. At or beyond the\nGUT scale, many theories anticipate this number to be many hundreds to above a\nthousand.\nThe bottom line is simple. The next generation of experiments is sure to be a\nsingularly important turning point for the field. The importance of the Higgs and\nnew particle searches cannot be overrated.\n\n6\n\nBoundaries of Knowledge and Theories of\nEverything\n\nFor the last two decades, the subfield of theoretical particle physics with the most\nvitality, and with the most powerful intellectual force applied to it, is undeniably that\nof string theory. It is a most ambitious subfield, with the often claimed goal to be\nno less than a \"theory of everything\", one which addresses the \"unsolved\" problem\nof synthesizing quantum mechanics with general relativity [15].\nWhile I have no problem with what people do, I do have a problem with the\nrhetoric. In my opinion, a \"theory of everything\" is not a subfield of physical science,\nwhere a theory requires validation by experiment, but rather is a subfield of \"natural\nphilosophy\", which includes such fields as mathematics, philosophy, and religion,\nand which allows speculations and investigations unfettered by the constraints of\nexperimentation and of the scientific method.\nI also question the assertion that we presently have no quantum field theory of\ngravitation. It is true that there is no closed, internally consistent theory of quantum\ngravity valid at all distance scales, But such theories are hard to come by, and in\n14\n\n\fany case, are not very relevant in practice. But as an open theory, quantum gravity\nis arguably our best quantum field theory, not the worst. Feynman rules for interaction of spin-two gravitons have been written down, and the tree-diagrams (no closed\nloops) provide an accurate description of physical phenomena at all distance scales between cosmological scales, down to near the Planck scale of 10\u221233 cm. The divergent\nloop diagrams can be renormalized at the expense of an in-principle infinite number\nof counterterms appended to the Einstein-Hilbert action. However their effects are\ndemonstrably small until one probes phenomena at the Planck scale of distances and\nenergies [16].\nOne way of characterizing the success of a theory is in terms of bandwidth, defined as the number of powers of ten over which the theory is credible to a majority\nof theorists (not necessarily the same as the domain over which the theory has been\nexperimentally tested). From this viewpoint, quantum gravity, when treated-as described above-as an effective field theory, has the largest bandwidth; it is credible\nover 60 orders of magnitude, from the cosmological to the Planck scale of distances.\nThe runner-up is QCD, which loses credibility at the GUT scale. Above that scale\nQCD arguably gets synthesized into a new improved theory, in a way perhaps similar\nto the way QED gets synthesized at the electroweak scale. Indeed of the three theories, QED as formulated by Dirac and Heisenberg and renormalized by Feynman,\nSchwinger, and Tomonaga, has the worst bandwidth because it is already modified\nin an essential way at the electroweak scale.\nIn the old days QED was considered by Landau and others as an inconsistent\n(closed) theory, because the coupling constant \u03b1 grows at short distances and eventually, at an incredibly short distance, blows up. This would in principle limit the\nbandwidth of QED to a mere 100 orders of magnitude or so. What a disaster! But in\nfact other physics intervenes. It is interesting that this inconsistency does not happen\nin QCD. The strong coupling constant only blows up in the infrared, where-with\nthe help of experimental evidence-it is concluded that the theory remains consistent.\nNevertheless, while QCD does enjoy the (unique) status of an experimentally relevant\nand logically consistent quantum field theory with infinite bandwidth, in practice it\nprobably does not matter that this is the case, because almost certainly new physics\nwill intervene at the GUT scale, if not sooner.\nWhile quantum gravity may have splendid bandwidth, it still remains the case\nthat at the Planck scale the effective field theory formalism totally falls apart. It is\nhere of course that one finds the arena appropriate to the Theories of Everything.\nIt is to be sure a valid and important arena. And it will be wonderful indeed if a\nsuccessful theory can be put together at that scale. The trouble with doing so is that\nthere is precious little guidance from experiment. What has always been typical for\nprogress in physical science is a painful, slow progression, one step at a time, with\nguidance from experiment at most of the steps. Success with a theory of everything\nwould be something very different and extraordinary-that of a theory found almost\ncompletely using arguments of symmetry and/or esthetics.\n15\n\n\fBut no matter what, there are already many beneficial consequences of the theoryof-everything program. In its present state I see it as a theory of theories\u2013the study\nof deep connections between different beautiful theories, some of which might conceivably be relevant to real physical phenomena. These connections will I am sure be\ncovered in other talks. Certainly the theoretical phase-space of ideas has been greatly\nenlarged, for example in the considerations of supersymmetries, of extra space-time\ndimensions, of black hole phenomena, and even of more efficient ways for calculating\nFeynman diagrams. I would be very surprised if the future, improved theory does not\ncontain some of the ideas spawned by the superstring revolution, even if superstrings\nhave nothing to do with anything.\n\n7\n\nMacroscopic Quantum Gravity\n\nGiven the argument that quantum gravity is a good theory because it has large\nbandwidth, I now worry about whether I believe it. The issue is only whether the\nquantum-gravity phenomena not covered by perturbative Feynman-diagram calculations (which I consider safe territory) rest on solid foundations. This boils down to\nthe question of black hole horizons and Hawking radiation, a subject which involves\na nontrivial application of quantum gravity at distance scales large compared to the\nPlanck scale. (The physics near a true gravitational singularity will also be nontrivial,\nbut will be characterized presumably by physics at the Planck scale.) The potential\nproblem that concerns me is that the black hole horizon (Schwarzschild radius) is a\nregion characterized by very large complexity and very large bandwidth.\nBy this I mean the following. In Schwarzschild coordinates, place stationary\nobservers a very small distance h above the horizon. Their job is to survey the local\nenvironment, as well as communicate with neighbors.\nAs the horizon is approached,\n\u221a\nthe spacing of such observers must decrease as h; otherwise they will not be able\nto send light signals to neighboring stations; the photons will fall into the black hole\nbefore getting there. So the number of such stations must be proportional to the black\nhole area, and depend inversely upon the height above the horizon. This result no\ndoubt has something to do with black hole entropy, and is what I mean by increasing\ncomplexity as the horizon is approached.\nThe increase of bandwidth is related to the fact that, because of gravitational\nredshift, the surveyor's clock rate decreases toward zero as the horizon is approached,\nimplying infrared sensitivity. The surveyor will see a divergent ratio of frequency of\nthe light from his own Cesium clock, relative to light received from a distant Cesiumclock frequency standard. In addition the surveyor gets hot; he feels himself immersed\nin local Hawking radiation, with Hawking temperature which again diverges as the\nhorizon is approached.\nIf our existing theoretical formalism has finite bandwidth, then the divergence\nin these quantities, which implies infinite bandwidth, may signal a sensitivity to\nnew-physics phenomena at the horizon. Without an understanding of what the new\n16\n\n\fphysics is, there is no necessity that something discontinuous happens to, say, the\nnature of the vacuum as the horizon is crossed, but only that there is a reasonable\npossibility that this might occur. I realize that this viewpoint cuts strongly against\nconventional wisdom, because freely falling observers are not supposed to \"see\" the\ninfinite bandwidth phenomena that the Schwarzschild surveyors see. And it is also a\nnecessity that the horizon which is considered here is not the event horizon used by\nPenrose and Hawking in their global analyses, but rather an apparent, or \"redshift\"\nhorizon. (See Fig. 7 for a description of the distinction I have in mind.)\n\nyy\nyy\nyy\nyy\nyy\nyy\n(a)\n\nt\n\nGravitational\nSingularity\nRedshift\nHorizon\n\nInfalling Shell\nof Matter\n\nEvent\nHorizon\nr\n\n(b)\n\n+\n\n\u2013\n\n5-2000\n8541A07\n\nFigure 7: (a) Space-time history of a black hole created by a spherically symmetric\nshell of infalling matter, in Schwarzschild coordinates. Shown is the Schwarzschild\nradius and the Penrose-Hawking event horizon, within which no light ray can emerge\nto spatial infinity. Also shown is the \"red-shift\", or \"apparent\" horizon which is paircreated by the infalling matter and which separates the interior region with g00 < 0\nfrom the exterior region with g00 > 0. (b) The same picture, in a Penrose diagram.\n\n17\n\n\fWhile I am not yet sure of my ground and recognize that most experts do not\nshare my doubts, I still find the Schwarzschild horizon a potential frontier, something\nakin to the frontiers posed by the Planck and cosmological distance scales. But maybe\nthat is just my own shortcoming.\n\n8\n\nThe Fate of QCD\n\nAs we have mentioned already, quantum chromodynamics (QCD), the theory of the\nstrong force, is arguably the most comprehensive of the quantum field theories in\nuse today, and in principle is a consistent closed theory with infinite bandwidth. In\npractice, there are two major branches of QCD, short-distance and long-distance. The\nformer is dominated by a Feynman-diagram approach. It is basically perturbative in\nnature, although in practice large sets of diagrams need to be summed in order to\nattain the needed accuracy. And the short-distance limit has a host of applications\nand is of immediate relevance for all new-physics searches at the high energy frontier.\nFor all these reasons there has been and will continue to be a large investment of\neffort in this area [17].\nThe large-distance limit, \"soft QCD\", has to do with hadron structure and vacuum\nstructure. The distance scale ranges from above 10\u221213 cm to a little below 10\u221214 cm,\nessentially the size scale associated with ordinary hadrons. Here perturbation theory\ncannot be reliably used, and therefore the theory is much more challenging. Many\nopen questions remain, the most prominent being a full description of the phenomenon\nof quark confinement.\nThe vacuum structure of QCD is especially rich. At moderate distances there is\nthe challenge of understanding the role of instantons. At large distances, there exists\na \"chiral condensate\", emergent from the spontaneous breaking of the approximate\nstrong-interaction chiral symmetry. And the QCD phase diagram needs explication;\nat present there is a lot of progress in the theory [18]. And the exploration of heavy\nion collisions at CERN and soon at the RHIC ion-ion collider at Brookhaven [19]\nmakes the experimental situation also a dynamic one.\nAt short distances hadrons are described in terms of the pointlike quark-gluon,\n\"parton\" degrees of freedom. Much is know about the momentum spectrum of these\npartons, viewed in reference frames where the parent hadron has very high momentum. But precious little is understood about the correlations between the partons.\nFor example, how they are distributed in the transverse plane is still a serious issue,\nand even the multiplicity distribution of the partons is not established. And the nature of the low-momentum tail of the distribution, the \"wee\" partons of Feynman,\nremains an active and unresolved issue.\nOrdinary collisions at the very high energies of the LHC are another frontier. So\nmany \"wee\" partons participate simultaneously in the typical central proton-proton\ncollision that the complexity of the dynamics, e.g. the number of relevant constituent\ncollisions per proton-proton collision, exceeds what will be dealt with in gold-gold\n18\n\n\fcollisions at RHIC. Diffractive processes comprise another difficult and unresolved\nsubfield, one which increases in prominence with increasing energy. It is a \"shadowy\"\ntopic which probably involves large-distance QCD concepts in an essential way[20].\nEven the venerable subject of hadron spectroscopy ought to have a rich future.\nA modern \"electronic bubble chamber\", built to exceed the classic bubble chamber's\nacceptance and resolution, could improve the statistics and data quality of all the\nold resonance-physics topics by a millionfold. Many anticipated resonant states of\nhadrons, especially those made primarily of gluons, remain to be discovered and\ncarefully studied.\nI prefer to label this whole subfield \"Hadron Physics\", in analogy to atomic,\nmolecular, and nuclear physics. The goals of those subfields are quite analogous to\nthe goals of hadron physics-namely to study the internal structure of their building\nblocks, as well as to study their \"vacua\", namely the extensive, condensed-matter\nstructures built from those constituents.\nThese subfields, especially atomic and nuclear physics, used to be within the\nmainstream of elementary particle physics. As the energy scale of interest to particle\nphysics increased, each evolved into a distinct discipline. And I think this is already\nhappening to hadron physics. Relatively little attention is paid nowadays by mainstream particle physicists to the subfield of hadron physics. And a great deal of the\nsubject matter is now appropriated by the nuclear physics community, even though\nwhat they are doing can hardly be called nuclear physics.\nWhile this evolution is basically a natural one, there is a new problem which was\nnot faced in the previous examples. Much of the experimental program of hadron\nphysics needs to be done via high energy collisions, within the facilities built and\nexploited by particle physicists. It is not easy for hadron-physics initiatives to be\nrecognized and to receive the necessary priority rating, whether from laboratory managements, program committees, funding agencies, or the existing peer review system,\nwhen a program dedicated to hadron structure is put in direct competition with major high-energy physics initiatives. I believe that in order for such hadron-physics\ninitiatives to be viable, there must be institutional changes at all these levels, which\nrecognize the complementary nature of hadron-physics research, and which provide\na certain degree of guaranteed access to the high-energy facilities in return for an\nappropriate contribution to the operating costs of the facilities [21].\n\n9\n\nRemarks\n\nIn summary, elementary particle physics in the next century should continue to be\nfull of progress and full of vitality. While there is a slowing of the pace, it is still the\ncase that within the next decade we should already witness a major turning point,\nnamely a much better understanding of the problem of mass and of the mechanism\nof electroweak symmetry breaking. The way in which that problem is answered will\nhave much to do in shaping the nature of the experimental program beyond.\n19\n\n\fThe vitality of theory nowadays is focused on the superstring ideas, which continue\nto generate new ways of looking at the fundamental problem of going beyond the\nstandard model. The only problem with this activity is its remoteness from data.\nThis in itself causes me no problem. But if it leads to an indifference of theorists\ntoward the data-driven side of the field, then I will have a problem.\nWhen new data appears at a relatively slow rate, ideas and interpretations tend\nto ossify, and it sometimes becomes harder not only to think in new ways, but also\nto maintain a diversity of approach, which is always an essential element of the scientific endeavor. The existence of the Standard Model does not imply the existence of\na standardized anticipation of the future. The only thing that deserves institutionalization is doubt. This problem of maintaining diversity of approach afflicts both\nexperiment and theory, and if I have any concern about how the field is developing,\nit is about this point I worry the most.\nIn this respect, the second-tier initiatives such as the \"factories\", and new generations of non-accelerator facilities are important to encourage. And I would add QCD\nand hadron-physics initiatives to this list as well. There is very much to do, most of\nwhich is relatively accessible provided the resources are made available. And last but\nnot least, the long range problem of reaching extremely high energies should not be\nneglected, implying good support for advanced accelerator R&D.\n\n10\n\nAcknowledgments\n\nIt is a pleasure to thank the organizers of this conference for the excellent program\nand fine hospitality extended to us all. I also thank Pisin Chen, Leon Lederman,\nChris Quigg, and Leonard Susskind for critical comments and advice.\n\nReferences\n[1] J. D. Bjorken (SLAC\u2013PUB\u20135361), Proceedings of the 25th International Conference on High Energy Physics, Singapore, August (1990), published in Singapore,\nH. E. Phys. 1990, 329.\n[2] T. Brooks et al., Phys. Rev. D55, 5667 (1997); D61, 32002 (2000), hepph/9609375 and hep-ex/9906026.\n[3] See http://www.fnal.gov/pub/accelerator.html.\n[4] See http://cern.web.cern.ch.\n[5] See http://cern.web.cern.ch/lhc.\n[6] M. Tigner, Proceedings of the 23rd International Conference on High Energy\nPhysics, ed. S. Loken, Singapore (World Scientific), vol. 2, p. 152; also J. Bjorken,\n20\n\n\fFERMILAB\u2013CONF\u20138455\u2013THY (1982); \"Techniques and Concepts of High Energy Physics II\", ed. T. Ferbel, Plenum Press, 1983 (NATO Advanced Study\nInstitute, Series B; Physics. v. 99).\n[7] See http://www.slac.stanford.edu/welcome/slc.html.\n[8] See http://www-project.slac.stanford.edu/nlc/home.html.\n[9] For example, see http://www.fnal.gov/pub/hepdescript.html (Research\nProgram/Accelerator Physics/Muon Collider R&D).\n[10] See http://www.jlab.org/index.html.\n[11] See http://www-sk.icrr.u-tokyo.ac.jp, or\nhttp://www.phys.washington.edu/\u223csuperk/index.html.\n[12] For an overview of recent work, see Advanced Accelerator Concepts, ed. S. Chattopadhyay, J. McCullough, P. Dahl (1997); AIP Conference Proveedings. vol.\n398 (American Institute of Physics.\n[13] See in particular the original proposal to CERN written by Tim Berners-Lee:\nhttp://www.w3.org/People/Berners-Lee/Overview.html.\n[14] For a recent review, see C. Quigg, hep-ph/9905369.\n[15] G. Veneziano, these proceedings. A splendid popularization of the argument is\ngiven by Brian Greene, \"The Elegant Universe\", W. Norton (New York), 1999.\n[16] J. Donoghue, Phys. Rev. D50, 3874 (1994); for a review see C. P. Burgess, hepph/9812470.\n[17] G. Sterman, these proceedings.\n[18] For a review, see F. Wilczek, hep-ph/0003183.\n[19] See http://www.rhic.bnl.gov/ .\n[20] An account of the QCD issues relevant to the LHC program can be found\nin the letter of intent of the FELIX Collaboration submitted to CERN:\nhttp://www.cern.ch/FELIX/ .\n[21] Steven Heppelman and I have made some suggestions on what might be done;\ncf. http://sheppel.phys.psu.edu/FundQCD/.\n\n21\n\n\f"}