{"id": "http://arxiv.org/abs/1002.4041v1", "guidislink": true, "updated": "2010-02-22T03:01:49Z", "updated_parsed": [2010, 2, 22, 3, 1, 49, 0, 53, 0], "published": "2010-02-22T03:01:49Z", "published_parsed": [2010, 2, 22, 3, 1, 49, 0, 53, 0], "title": "Improving Term Extraction Using Particle Swarm Optimization Techniques", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.2374%2C1002.4158%2C1002.2832%2C1002.2008%2C1002.0170%2C1002.1451%2C1002.0716%2C1002.4382%2C1002.1006%2C1002.4041%2C1002.1624%2C1002.4403%2C1002.5030%2C1002.0659%2C1002.1547%2C1002.3272%2C1002.1724%2C1002.1642%2C1002.4254%2C1002.2306%2C1002.3174%2C1002.0268%2C1002.2143%2C1002.0083%2C1002.4871%2C1002.0241%2C1002.1171%2C1002.1428%2C1002.2132%2C1002.0903%2C1002.1134%2C1002.0022%2C1002.3048%2C1002.3834%2C1002.1960%2C1002.2955%2C1002.2880%2C1002.0531%2C1002.2680%2C1002.3748%2C1002.2229%2C1002.3230%2C1002.0740%2C1002.4725%2C1002.0001%2C1002.3465%2C1002.3044%2C1002.1596%2C1002.3094%2C1002.4485%2C1002.1402%2C1002.2256%2C1002.1667%2C1002.1880%2C1002.3610%2C1002.3512%2C1002.2390%2C1002.1015%2C1002.4039%2C1002.3551%2C1002.2131%2C1002.2842%2C1002.0425%2C1002.4756%2C1002.0933%2C1002.1537%2C1002.0448%2C1002.3887%2C1002.0856%2C1002.3678%2C1002.1568%2C1002.1490%2C1002.4474%2C1002.0550%2C1002.2834%2C1002.1887%2C1002.0075%2C1002.3813%2C1002.4074%2C1002.4702%2C1002.3274%2C1002.0732%2C1002.2019%2C1002.0104%2C1002.3964%2C1002.2815%2C1002.0590%2C1002.2404%2C1002.0813%2C1002.1329%2C1002.2467%2C1002.1075%2C1002.4205%2C1002.1730%2C1002.0509%2C1002.1611%2C1002.0461%2C1002.3332%2C1002.4246%2C1002.4894%2C1002.4402&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Improving Term Extraction Using Particle Swarm Optimization Techniques"}, "summary": "Term extraction is one of the layers in the ontology development process\nwhich has the task to extract all the terms contained in the input document\nautomatically. The purpose of this process is to generate list of terms that\nare relevant to the domain of the input document. In the literature there are\nmany approaches, techniques and algorithms used for term extraction. In this\npaper we propose a new approach using particle swarm optimization techniques in\norder to improve the accuracy of term extraction results. We choose five\nfeatures to represent the term score. The approach has been applied to the\ndomain of religious document. We compare our term extraction method precision\nwith TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental\nresults show that our propose approach achieve better precision than those four\nalgorithm.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.2374%2C1002.4158%2C1002.2832%2C1002.2008%2C1002.0170%2C1002.1451%2C1002.0716%2C1002.4382%2C1002.1006%2C1002.4041%2C1002.1624%2C1002.4403%2C1002.5030%2C1002.0659%2C1002.1547%2C1002.3272%2C1002.1724%2C1002.1642%2C1002.4254%2C1002.2306%2C1002.3174%2C1002.0268%2C1002.2143%2C1002.0083%2C1002.4871%2C1002.0241%2C1002.1171%2C1002.1428%2C1002.2132%2C1002.0903%2C1002.1134%2C1002.0022%2C1002.3048%2C1002.3834%2C1002.1960%2C1002.2955%2C1002.2880%2C1002.0531%2C1002.2680%2C1002.3748%2C1002.2229%2C1002.3230%2C1002.0740%2C1002.4725%2C1002.0001%2C1002.3465%2C1002.3044%2C1002.1596%2C1002.3094%2C1002.4485%2C1002.1402%2C1002.2256%2C1002.1667%2C1002.1880%2C1002.3610%2C1002.3512%2C1002.2390%2C1002.1015%2C1002.4039%2C1002.3551%2C1002.2131%2C1002.2842%2C1002.0425%2C1002.4756%2C1002.0933%2C1002.1537%2C1002.0448%2C1002.3887%2C1002.0856%2C1002.3678%2C1002.1568%2C1002.1490%2C1002.4474%2C1002.0550%2C1002.2834%2C1002.1887%2C1002.0075%2C1002.3813%2C1002.4074%2C1002.4702%2C1002.3274%2C1002.0732%2C1002.2019%2C1002.0104%2C1002.3964%2C1002.2815%2C1002.0590%2C1002.2404%2C1002.0813%2C1002.1329%2C1002.2467%2C1002.1075%2C1002.4205%2C1002.1730%2C1002.0509%2C1002.1611%2C1002.0461%2C1002.3332%2C1002.4246%2C1002.4894%2C1002.4402&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Term extraction is one of the layers in the ontology development process\nwhich has the task to extract all the terms contained in the input document\nautomatically. The purpose of this process is to generate list of terms that\nare relevant to the domain of the input document. In the literature there are\nmany approaches, techniques and algorithms used for term extraction. In this\npaper we propose a new approach using particle swarm optimization techniques in\norder to improve the accuracy of term extraction results. We choose five\nfeatures to represent the term score. The approach has been applied to the\ndomain of religious document. We compare our term extraction method precision\nwith TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental\nresults show that our propose approach achieve better precision than those four\nalgorithm."}, "authors": ["Mohammad Syafrullah", "Naomie Salim"], "author_detail": {"name": "Naomie Salim"}, "author": "Naomie Salim", "links": [{"href": "http://arxiv.org/abs/1002.4041v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1002.4041v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1002.4041v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1002.4041v1", "arxiv_comment": null, "journal_reference": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "fulltext": "JOURNAL OF COMPUTING, VOLUME 2, ISSUE 2, FEBRUARY 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n116\n\nImproving Term Extraction Using Particle\nSwarm Optimization Techniques\nMohammad Syafrullah and Naomie Salim\nAbstract-Term extraction is one of the layers in the ontology development process which has the task to extract all the terms\ncontained in the input document automatically. The purpose of this process is to generate list of terms that are relevant to the\ndomain of the input document. In the literature there are many approaches, techniques and algorithms used for term extraction.\nIn this paper we propose a new approach using particle swarm optimization techniques in order to improve the accuracy of term\nextraction results. We choose five features to represent the term score. The approach has been applied to the domain of\nreligious document. We compare our term extraction method precision with TFIDF, Weirdness, GlossaryExtraction and\nTermExtractor. The experimental results show that our propose approach achieve better precision than those four algorithm.\nIndex Terms-Term Extraction, Particle Swarm Optimization, Feature Selection, Text Mining.\n\n---------- \uf075 ----------\n\n1 INTRODUCTION\n\nR\n\necently many experiments have been conducted for\nterm extraction task. Literatures provide many examples of term extraction methods. Most of these are\nbased on linguistic method, terminology and NLP method, and the others based on statistical/information retrieval method [1].\nIn linguistic method, most of existing approaches use\nshallow text processing techniques such as tokenizer,\npart-of-speech (POS) tagger and syntactic analyzer (parser). Text-to-Onto is one of the systems that use linguistic\nmethod that they called SMES (Saarbrucken Message Extraction System) in their system architecture to produce\nlist of terms from the data input [2]. Another system\ncalled SVETLAN, use syntactic analyzer Sylex to find list\nof terms from the input text [3].\nIn terminology and NLP method many researchers\nhave invented their new techniques [4], [5], [6]. In the\nwork of [4], they use statistical measurement of frequency\noccurrence (C-value/ NC-value method), for the automatic extraction of multi-word terms, from English medical corpus. Park et. al [7], [8] introduced term cohesion\nwhich is used to calculate the cohesion of the multi-word\nterms. The measure is proportional to the co-occurrence\nfrequency and the length of the term. Panel and Lin [5]\npresent a language independent statistical corpus-based\nterm extraction algorithm. In their algorithm, first, they\ncollect bigram frequencies from a corpus and extract twoword candidates. After collecting features for each twoword candidate, they use mutual information (mi) and\nlog likelihood ratio (LLR) to extend them to multi-word\nterms. All those experiment done both with English and\nChinese corpora.\n\nIn statistical method, statistical analysis will be performed on data gathered from the input, and this analysis\nwill identifies the term from the data input based on the\nstatistical rank. Most of the statistical methods for term\nextraction are based on information retrieval method for\nterm indexing [9], [10]. Another works in this method can\nbe found in the literature, such as the notion of \"weirdness\" in [11], domain pertinence in [12], [13] and domain\nspecificity in [7], [8].\nTerminology and NLP approach is more emphasize on\nthe internal analysis for the term extraction within the\ncorpus, while in the statistical method is more underline\non the comparison of frequencies between domain specific and general corpora (external analysis).\n\n2 RELATED WORKS\n\nKea is one of the extraction systems which are using statistical method. It uses TFIDF and first occurrence in the\ndocument as its features to determine the weight of each\nkeyphrase. Kea's extraction algorithm has two stages, first\nis training stage (using Bayesian learning) which has the\ntask to create a model for identifying keyphrases, using\ntraining documents. The second one is extraction stage\nwhich will choose keyphrases from a test document, using the model that has been made in the previous stage\n[14].\nTurney [15] treats the problem of keyphrase extraction\nas supervised learning task. He presented two approaches to the task of learning to extract keyphrases from text.\nThe first approach was to apply the C4.5 and the second\none was using genetic algorithm. Turney's program is\ncalled Extractor. One form of this extractor is called Ge----------------\nnEx, which is use Genitor genetic algorithm to maximize\n\uf0b7 Mohammad Syafrullah is with the Faculty of Computer Science and Inforthe performance (fitness) on the training process. Genitor\nmation Systems, Universiti Teknologi Malaysia, 81310, Skudai, Johor, Mais used to tune Extractor, but is no longer needed once the\nlaysia.\n\uf0b7 Naomie Salim is with the Faculty of Computer Science and Information\ntraining process is complete.\nSystems, Universiti Teknologi Malaysia, 81310, Skudai, Johor, Malaysia.\n\nGlossary Extraction [7],[8] is a glossary extraction tool\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 2, FEBRUARY 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\n117\n\nthat use two features which are domain specificity and\nterm cohesion for calculating the term weight. Glossary\nExtraction algorithm, has the two important parts which\nare identification of candidate glossary items and glossary\nitem ranking and selection. After obtaining candidate\nglossary items, the algorithm will rank them before selecting the final set. In the paper [7], [8], they claim that their\nmethod can improve the document-relevancy ranking\ncompared with log likelihood ratio and mutual information.\nThe term extraction algorithm called Kea++ is the improvement of the original keyphrase extraction algorithm\nKea. Medelyan and Witten [16] called their new approach\nas index term extraction, because they combine the advantages of both keyphrase extraction and term assignment into a single scheme. Their preliminary evaluations\nshows that the Kea++ significantly outperforms compared with Kea extraction algorithm.\nAnother term extraction systems called Term Extractor\n[12], [13], use three features to compute their term weight.\nDomain pertinence is used to perform a contrastive analysis between domain of interest documents and other\ndomains documents. Domain consensus is used to measure the distribution of terms in a domain of interest, while\nthe definition of lexical cohesion similar to that already\nintroduced in [7], [8].\n\n(2)\nstart\ninitialize particles with\nrandom position\nand velocity vectors\n\nupdate gbest=pbest\nyes\n\np=particle's position\n\nfitness(pbest)\nbetter than\nfitness(gbest)?\n\nyes\nfitness(p) better\nthan fitness(pbest)?\n\nupdate pbest=p\n\nno\n\nno\n\nno\n\nall particle has\nbeen evaluated?\n\nyes\nupdate particle velocity using eq. (1)\nupdate particle position using eq. (2)\n\n3 PARTICLE SWARM OPTIMIZATION\nParticle swarm optimization first introduced by Kennedy\nand Eberhart [17], [18], [19], as an optimization technique\nbased on the movement and intelligence of the swarm. It\ninspired by social behavior and dynamics of movement of\nbirds and fish. PSO uses a number of particles that constitute a swarm moving around in the search space to find\nthe best solution. Each particle is treated as a point in the\nsearch space which adjusts its flying according to its own\nflying experience and other particles flying experience.\nInitially, the PSO algorithm randomly selects candidate solutions (particles) within the search space. During\neach iteration of the algorithm, each particle is evaluated\nby the objective function being optimized, determining\nthe fitness of the solution. A new velocity value for each\nparticle is calculated using the following equation:\n(1)\nThe index of the particle is represented by i. So, (t) is\nthe velocity of particle i at time t and (t) is the position\nof particle i at time t. Parameters w, c1, and c2 are usersupplied coefficients. The values r1 and r2 are random\nvalues regenerated for each velocity update. Value (t) is\nthe individual best candidate solution for particle i at time\nt, and g (t) is the swarm's global best candidate solution at\ntime t. Once the velocity for each particle is calculated,\neach particle's position is updated by applying the new\nvelocity to the particle's previous position using equation\n(2). This process is then repeated until some stopping\ncondition is met. Figure 1 describes the flowchart of PSO\nalgorithm\n\nno\n\nall particle has\nbeen updated?\n\nyes\n\nno\n\nstopping criteria\nsatisfied?\n\nyes\noptimal solution=gbest\n\nstop\n\nFig. 1. Flowchart of particle swarm optimization algorithm.\n\n4 TERM EXTRACTION USING PARTICLE SWARM\nOPTIMIZATION\nIn this section, we propose a new approach of term extraction, which takes into account several kinds of features, including domain relevance, domain consensus,\nterm cohesion, first occurrence and length of noun\nphrase, to produce a list of terms.\nTwo steps are employed in our propose approach.\nFirst, terms are ranked to emphasize the most relevant\nfrom domain of input document; second, the score function is trained by the particle swarm optimization to obtain a suitable combination of feature weights.\n\n4.1 Methodology\nThe goal of term extraction is to generate list of terms that\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 2, FEBRUARY 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\nare relevant to the domain of the input domain. Our proposed approach consists of the following steps:\n1. Read the input document.\n2. Preprocessing step consist of three sub tasks:\nSyntactic parser does a syntactic analysis on every input\nsentence from input document, and produces a list of\nsyntactic information (Noun Phrase-NP). Stop words\nshould be filtered from each of the list of NP. Finally, the\nlist of NP should be stemmed to produce list of clean\nNP, as the term candidate.\n3. Each term candidate is associated with vector that\ncontains five features.\n4. The five features are used to calculate the term score\nand then rank the terms based on their score.\n\n118\n\nments we have adopted five features. These five features\nare calculated for each candidate term and used both in\ntraining and extraction stage. The features used are: domain relevance, domain consensus, term cohesion, first\noccurrence and length of noun phrase.\nf1: Domain relevance-domain relevance can be given according to the amount of information captured in the target document with respect to contrastive documents. Let\nDi is the domain of interest (a set of relevant documents)\nand {D1... Dn} is sets of documents in another domain,\ndomain relevance of a term t in class Di is computed as\n[12], [13]:\n(3)\n\nOur propose term extraction approach has two stages:\n1. Training stages: This stage has the task to create a model\nfor identifying terms using training documents.\nFeatures are extracted from training documents and\nused to train the swarm optimization model.\n2. Extraction stages: This stage will choose terms from a\ntest document (this document is different than that were\nused for training), using the model that has been made\nin the training stage.\nFigure 2 shows our proposed term extraction model. Both\nstages choose a set of term candidate from their input documents, and then calculate the values of certain features for\neach candidate.\ntraining stage\n\nextraction stage\n\ntraining\ndocuments\n\ntest\ndocuments\n\npreprocessing\n\npreprocessing\n\nfeature\ncalculation\n\nswarm\noptimization\n\ncontrastive\ndocuments\n\nfeature\ngeneration\n\nwhere (P (t|Dk)) estimated as:\n(4)\nf2: Domain consensus-domain consensus measures the\ndistributed use of a term in a domain Dk. Domain consensus is expressed as follows [12], [13]:\n(5)\nwhere:\n(6)\nf3: Term cohesion-term cohesion is used to calculate the\ncohesion of the multi-word terms. The measure is proportional to the cooccurrence frequency and the length of the\nterm [7], [8]:\n\nlist of\ncorrect terms\n(gold standard)\n\nmodel\n\n(7)\n\nterm\nranking\n\nlist of terms\n\nFig. 2. The training and extraction stage processes\n\nf4: First occurrence-the main idea behind this feature is\nthat important terms tend to occur at the beginning of\ndocuments. First occurrence is calculated as the number\nof words that precede its first appearance, divided by the\nnumber of words in the document. The resulting feature\nis a number between 0 and 1 representing the proportion\nof the documents before the term's first appearance [14].\nf5: Length of noun phrase-candidate length is also a useful feature in extraction as well as in candidate selection,\nbecause the majority of terms are one or two words in\nlength. Length of noun phrase score is calculated as its\nfrequency times its length (in words) [20].\n\n4.3 Term Generation\n4.2 Feature Definition\nIn order to characterize the noun phrases in the docu-\n\nFor a term t, a weighted score function, as shown in the\nfollowing equation, is used to integrate all the feature\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 2, FEBRUARY 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\nscores mentioned in the previous section, where wi indicates the weight of fi.\n(8)\nMoreover, the particle swarm optimization is used to\nobtain an appropriate set of feature weights. We have set\nthe particle swarm optimization variables as follows:\nnumber of particles=40, maximum number of iterations=500, c1=2, c2=2 and w= (0.5 + (random/2)). During\neach iteration of the algorithm, each particle is evaluated\nusing the fitness function as in (9). By applying particle\nswarm optimization, a suitable combination of feature\nweights could be found.\n\n(9)\nwhere |extracted| is a number of terms extracted by\nthe system and |ti\ngoldstandard| is the number of\nterms that is a member of the gold standard (reference of\ncorrect terms).\n\n4.4 Datasets\n1.\n\n2.\n\n3.\n\n119\n\nprecision(feature)\nf1\nf2\nf3\nf4\nf5\n\n25\n0.800\n0.880\n0.880\n0.800\n0.880\n\nnumber of terms\n50\n150\n0.820\n0.607\n0.760\n0.673\n0.780\n0.673\n0.740\n0.650\n0.740\n0.600\n\n250\n0.552\n0.596\n0.596\n0.610\n0.584\n\nWe compare the precision of our propose method with\nfour other known algorithms. The result show that our\npropose method based on particle swarm optimization\ncan improve the precision of the extracted terms. Table 2\nand Figure 3 show the comparison of the precision between swarm model and the four other algorithms\n(TFIDF, Weirdness, GlossaryExtraction and TermExtractor).\nTABLE 2\nCOMPARISON OF THE TERM EXTRACTION PRECISION\n(SWARM MODEL, TFIDF, WEIRDNESS,\nGLOSSARYEXTRACTION AND TERMEXTRACTOR)\nprecision(algorithm)\nTFIDF\nWeirdness\nGlossaryExtraction\nTermExtractor\nSwarm Model\n\n25\n0.840\n0.760\n0.840\n0.840\n0.960\n\nnumber of terms\n50\n150\n0.800\n0.607\n0.660\n0.607\n0.740\n0.633\n0.800\n0.647\n0.860\n0.673\n\n250\n0.560\n0.588\n0.592\n0.564\n0.616\n\nQuran (focus on verses about prayer): we use English\ntranslation to the meaning of the Quran (focus on\nverses about prayer) as the input document in the\nexperiment. We separate the documents into a training documents and test documents.\nReuters-21578: the documents in the Reuters-21578\ncollection appeared on the Reuters newswire in 1987.\nIn 1990, the documents were made available by Reuters for research purposes. We converted all the documents into 22 plain text file (reut2-000.txt until\nreut2-021.txt) and use it as contrastive documents.\nGold Standard: list of the Quran terms (focus on\nverses about prayer).\n\n5 EXPERIMENTAL RESULTS\nIn the extraction stage, we evaluate the precision of our\npropose methods at 4 points: top 25, top 50, top 150 and\ntop 250 terms using the following equation:\n\n(10)\n\nWe compare the terms extracted by the system with the\ngold standard that we have prepare before. Table 1 shows\nthe term extraction precision for each feature for different\nnumber of terms evaluated.\nTABLE 1\nTERM EXTRACTION PRECISION FOR EACH FEATURE\n\nFig. 3. Comparison of the term extraction precision (Swarm Model,\nTFIDF, Weirdness, GlossaryExtraction and TermExtractor)\n\n6 CONCLUSION\nWe have presented a particle swarm optimization technique to improve term extraction precision. We choose\nfive features to represent the term score: domain relevance, domain consensus, term cohesion, first occurrence\nand length of noun phrase. In the experiments, we use a\ntranslation of the meaning of the Quran (focus on verses\nof prayer) as an input document, both for training and\ntesting phases. We separate the documents between training documents and test documents. Particles swarm optimization is trained using the training documents to determine the appropriate weight of each feature to produce\nthe best score for each term. We conduct tests with the\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 2, FEBRUARY 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\ntest document using the weight of each feature which is\ngenerated from the training stage to calculate the final\nscore for each term to be extracted. Our experimental results show the use of particle swarm optimization technique can improve the precision of the extracted terms\ncompared with four other known algorithms (TFIDF,\nWeirdness, GlossaryExtraction and TermExtractor).\n\nACKNOWLEDGMENT\nThis project is sponsored by the Ministry of Science,\nTechnology and Innovation under grant vote number:\n79303.\n\nREFERENCES\n[1]\n[2]\n\n[3]\n\n[4]\n\n[5]\n[6]\n\n[7]\n\n[8]\n\n[9]\n\n[10]\n[11]\n\n[12]\n\n[13]\n\n[14]\n\nP. Cimiano, Ontology Learning and Population from Text: Algorithms,\nEvaluation and Applications. Springer, 2006.\nA. Maedche and S. Staab, \"Mining Ontologies from Text\", Knowledge\nEngineering and Knowledge Management Methods, Models, and\nTools. Springer Berlin / Heidelberg, 2003.\nG. Chalendar and B. Grau, \"SVETLAN' Or How to Classify Words\nUsing Their Context\", Knowledge Engineering and Knowledge\nManagement Methods, Models, and Tools. Springer Berlin /\nHeidelberg, 2003.\nK. Frantzi, S. Ananiadou and H. Mima, \"Automatic recognition of\nmulti-word terms: The C-value/NC-value method\", International\nJournal on Digital Libraries, vol. 3, no. 2, pp. 115-130, 2000.\nP. Panel and D. Lin, \"A Statistical Corpus-Based Term Extractor\",\nLecture Notes in Artificial Intelligence. Springer, pp. 36-46, 2001.\nP. Ryu and K. Choi, \"An Information-Theoretic Approach to\nTaxonomy Extraction for Ontology Learning\", Ontology Learning from\nText: Methods, Evaluation and Applications. IOS Press, 2005.\nY. Park, R. J. Byrd, and B. K. Boguraev, \"Automatic Glossary Extraction:\nBeyond Terminology Identification\", Proceedings of the Nineteenth\nInternational Conference on Computational Linguistics, pp. 772\u2013778,\n2002.\nL. Kozakov, Y. Park, T. Fin, Y. Drissi, Y. Doganata and T. Cofino.\n\"Glossary extraction and utilization in the information search and\ndelivery system for IBM Technical Support\", IBM System Journal, pp.\n546-563, vol. 43, no. 3, 2004.\nG. Salton and C. Buckley, \"Term-Weighting Approaches in Automatic\nText Retrieval\", Information Processing & Management, pp. 515-523,\n1988.\nB. Yates and R. Neto, Modern Information Retrieval. Addison-Wesley,\n1999.\nK. Ahmad, L. Gillam and L. Tostevin, \"University of Surrey\nparticipation in TREC 8: Weirdness Indexing for Logical Document\nExtrapolation and Retrieval (WILDER)\", In the Eighth Text Retrieval\nConference (TREC-8), pp. 717-724, 1999.\nR. Navigli and P. Velardi, \"Learning Domain Ontologies from\nDocument Warehouses and Dedicated Web Sites\", Computational\nLinguistics, vol. 30, no. 2, pp. 151-170, 2004.\nF. Sclano and P. Velardi, \"Termextractor: a web application to learn the\nshared terminology of emergent web communities\", Proceedings of the\n3rd International Conference on Interoperability for Enterprise Software\nand Applications (I-ESA), 2007.\nI. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. NevillManning, \"KEA: Practical Automatic Keyphrase Extraction\",\nProceedings of the. Fourth ACM Conference on Digital Libraries, pp.\n\n254-256, 1999.\n[15] P.D. Turney, \"Learning algorithm for keyphrase extraction\", Journal of\nInformation Retrieval, vol. 2, no. 4, pp. 303\u2013336, 2000.\n[16] O. Medelyan and I. H. Witten, \"Thesaurus-based index term extraction\nfor agricultural documents\", Proceeding of the 6th Agricultural\nOntology Service (AOS) workshop at EFITA/WCCA, 2005.\n[17] J. Kennedy and R.C. Eberhart, \"Particle swarm optimization\",\nProceedings of the IEEE international conference on neural networks\nIV, pp. 1942\u20131948, 1995.\n[18] R.C. Eberhart and J. Kennedy, \"A new optimizer using particle swarm\ntheory\", Proceeding 6th International Symposium on Micro Machine\nand Human Science, pp. 39-43, 1995.\n[19] R.C. Eberhart and Y.H. Shi, \"Comparison between genetic algorithms\nand particle swarm optimization\", Proceedings of Annual Conference\non Evolutionary Programming, pp. 611-616, 1998.\n[20] K. Barker and N. Corrnacchia, \"Using noun phrase heads to extract\ndocument keyphrases\", Proceedings of the 13th Biennial Conference of\nthe Canadian Society on Computational Studies of Intelligence:\nAdvances in Artificial Intelligence, pp. 40-52, 2000.\nMohammad Syafrullah is a Ph.D candidate, Faculty of Computer\nScience and Information System in Universiti Teknologi Malaysia. He\nreceived his bachelor degree in Computer Science from Budi Luhur\nUniversity, Indonesia in 1997. He received his master degree in\nComputer Science from Swiss German University, Indonesia in\n2005. His current research interest includes Ontology Learning,\nData Mining and Soft Computing.\nDr. Naomie Salim is an Associate Professor presently working as a\nDeputy Dean of Research & Postgraduate Studies in the Faculty of\nComputer Science and Information System in Universiti Teknologi\nMalaysia. She received her bachelor degree in Computer Science\nfrom Universiti Teknologi Malaysia in 1989. She received her master\ndegree in Computer Science from University of West Michigan in\n1992. In 2002, she received her Ph.D (Computational Informatics)\nfrom University of Sheffield, United Kingdom. Her current research\ninterest includes Information Retrieval, Distributed Database and\nChemoinformatic.\n\n120\n\n\f"}