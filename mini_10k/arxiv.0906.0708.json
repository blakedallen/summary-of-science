{"id": "http://arxiv.org/abs/0906.0708v2", "guidislink": true, "updated": "2009-06-05T16:24:57Z", "updated_parsed": [2009, 6, 5, 16, 24, 57, 4, 156, 0], "published": "2009-06-03T13:55:17Z", "published_parsed": [2009, 6, 3, 13, 55, 17, 2, 154, 0], "title": "A corrected AIC for the selection of seemingly unrelated regressions\n  models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.4564%2C0906.5008%2C0906.1915%2C0906.4539%2C0906.2521%2C0906.4453%2C0906.5483%2C0906.3654%2C0906.1491%2C0906.5182%2C0906.3235%2C0906.0466%2C0906.5595%2C0906.0826%2C0906.3822%2C0906.4302%2C0906.4124%2C0906.4347%2C0906.2337%2C0906.2293%2C0906.0526%2C0906.3615%2C0906.5423%2C0906.4402%2C0906.3169%2C0906.4863%2C0906.1498%2C0906.3694%2C0906.1792%2C0906.2765%2C0906.0070%2C0906.2652%2C0906.2980%2C0906.3355%2C0906.1685%2C0906.5566%2C0906.2601%2C0906.3238%2C0906.1649%2C0906.5042%2C0906.0445%2C0906.2305%2C0906.5610%2C0906.1056%2C0906.2668%2C0906.4712%2C0906.2044%2C0906.3262%2C0906.4146%2C0906.5339%2C0906.4189%2C0906.4853%2C0906.4363%2C0906.1905%2C0906.1069%2C0906.2047%2C0906.4703%2C0906.0376%2C0906.1916%2C0906.5552%2C0906.0862%2C0906.1895%2C0906.3539%2C0906.0708%2C0906.1573%2C0906.3476%2C0906.2095%2C0906.1131%2C0906.1057%2C0906.2045%2C0906.0182%2C0906.4625%2C0906.1454%2C0906.5273%2C0906.1316%2C0906.1252%2C0906.0742%2C0906.1323%2C0906.3622%2C0906.4492%2C0906.3673%2C0906.3044%2C0906.2862%2C0906.2518%2C0906.0975%2C0906.0251%2C0906.5200%2C0906.1591%2C0906.5434%2C0906.4464%2C0906.4725%2C0906.4054%2C0906.2953%2C0906.1176%2C0906.0033%2C0906.4637%2C0906.2309%2C0906.0750%2C0912.2502%2C0912.3688%2C0912.1352&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A corrected AIC for the selection of seemingly unrelated regressions\n  models"}, "summary": "A bias correction to Akaike's information criterion (AIC) is derived for\nseemingly unrelated regressions models. The correction is of particular use\nwhen the sample size is not much larger than the number of fitted parameters. A\nsmall-sample simulation study indicates that the bias-corrected AIC (AICc)\nprovides better model choices than other model selection criteria.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.4564%2C0906.5008%2C0906.1915%2C0906.4539%2C0906.2521%2C0906.4453%2C0906.5483%2C0906.3654%2C0906.1491%2C0906.5182%2C0906.3235%2C0906.0466%2C0906.5595%2C0906.0826%2C0906.3822%2C0906.4302%2C0906.4124%2C0906.4347%2C0906.2337%2C0906.2293%2C0906.0526%2C0906.3615%2C0906.5423%2C0906.4402%2C0906.3169%2C0906.4863%2C0906.1498%2C0906.3694%2C0906.1792%2C0906.2765%2C0906.0070%2C0906.2652%2C0906.2980%2C0906.3355%2C0906.1685%2C0906.5566%2C0906.2601%2C0906.3238%2C0906.1649%2C0906.5042%2C0906.0445%2C0906.2305%2C0906.5610%2C0906.1056%2C0906.2668%2C0906.4712%2C0906.2044%2C0906.3262%2C0906.4146%2C0906.5339%2C0906.4189%2C0906.4853%2C0906.4363%2C0906.1905%2C0906.1069%2C0906.2047%2C0906.4703%2C0906.0376%2C0906.1916%2C0906.5552%2C0906.0862%2C0906.1895%2C0906.3539%2C0906.0708%2C0906.1573%2C0906.3476%2C0906.2095%2C0906.1131%2C0906.1057%2C0906.2045%2C0906.0182%2C0906.4625%2C0906.1454%2C0906.5273%2C0906.1316%2C0906.1252%2C0906.0742%2C0906.1323%2C0906.3622%2C0906.4492%2C0906.3673%2C0906.3044%2C0906.2862%2C0906.2518%2C0906.0975%2C0906.0251%2C0906.5200%2C0906.1591%2C0906.5434%2C0906.4464%2C0906.4725%2C0906.4054%2C0906.2953%2C0906.1176%2C0906.0033%2C0906.4637%2C0906.2309%2C0906.0750%2C0912.2502%2C0912.3688%2C0912.1352&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A bias correction to Akaike's information criterion (AIC) is derived for\nseemingly unrelated regressions models. The correction is of particular use\nwhen the sample size is not much larger than the number of fitted parameters. A\nsmall-sample simulation study indicates that the bias-corrected AIC (AICc)\nprovides better model choices than other model selection criteria."}, "authors": ["J. L. van Velsen"], "author_detail": {"name": "J. L. van Velsen"}, "author": "J. L. van Velsen", "arxiv_comment": "9 pages including 1 figure and 3 tables; v2: revtex4, typos corrected", "links": [{"href": "http://arxiv.org/abs/0906.0708v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0906.0708v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0906.0708v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0906.0708v2", "journal_reference": null, "doi": null, "fulltext": "A corrected AIC for the selection of seemingly unrelated regressions models\nJ. L. van Velsen\nDutch Ministry of Justice, Research and Documentation Centre (WODC),\nP. O. Box 20301, 2500 EH The Hague, The Netherlands\u2217\n\narXiv:0906.0708v2 [stat.ME] 5 Jun 2009\n\nA bias correction to Akaike's information criterion (AIC) is derived for seemingly unrelated regressions models. The correction is of particular use when the sample size is not much larger than\nthe number of fitted parameters. A small-sample simulation study indicates that the bias-corrected\nAIC (AICc) provides better model choices than other model selection criteria.\n\nI.\n\nINTRODUCTION\n\nThe selection of a model from a set of fitted candidate models requires objective data-driven criteria. One such\ncriterion often used in practice is Akaike's information criterion (AIC), which was designed to be an asymptotically\nunbiased estimator of the expected Kullback-Leibler information of a fitted model [1]. In finite samples, AIC has a\nnon-vanishing bias that depends on the number of fitted parameters. This limits its effectiveness as a model selection\ncriterion, particularly in instances where the sample size is not much larger than the number of fitted parameters of\nthe most complex candidate model. For such instances, Hurvich and Tsai [2] extended the bias-corrected AIC (AICc)\noriginally suggested by Sugiura [3] for linear regression models, to non-linear regression models and autoregressive\nmodels. Also, Hurvich and Tsai [2] demonstrated the small-sample superiority of AICc over AIC as a model selection\ncriterion. Since then, AICc has been extended to many other models, such as autoregressive moving average models\n[4], vector autoregressive models [5] and multivariate linear regression models [6].\nThe objective of this work is to define AICc for seemingly unrelated regressions models. These are models of\nmultiple response variables that follow a joint distribution [7, 8]. In contrast to the multivariate linear regression\nmodel of Ref. [6], the response variables of a seemingly unrelated regressions model do not need to depend on the\nsame covariates. Seemingly unrelated regressions models play a central role in econometrics [9] but also appear in\nother contexts [10, 11, 12].\nThe remainder of this paper is organized as follows. In Sec. II, the bias of AIC is calculated in seemingly unrelated\nregressions models with the assumption that the candidate model is either correctly specified or overspecified. The\nsame assumption is required for AIC to be asymptotically unbiased [13] and has been used to calculate its bias in\nfinite samples in other models [4, 5, 6]. Expanded in inverse powers of the sample size N , the bias of AIC (BAIC )\ntakes the form BAIC = \u2212N \u22121 \u03b2(\u03a30 ) + o(N \u22121 ), where the positive coefficient \u03b2(\u03a30 ) = O(1) depends on the unknown\ntrue p \u00d7 p covariance matrix \u03a30 of the p response variables. In Sec. II, a lower bound \u03b2 \u2217 > 0 of min\u03a9 \u03b2(\u03a9), where\nthe minimization is over all p \u00d7 p symmetric positive definite matrices \u03a9, is found in terms of the number of fitted\nparameters and AICc is defined as AICc = AIC + N \u22121 \u03b2 \u2217 . The performance of AICc as a model selection criterion\nis simulated in Sec. III and compared to that of AIC and the Bayesian information criterion (BIC) of Schwarz [14].\nFinally, we give some concluding remarks in Sec. IV. Details about the calculation of \u03b2(\u03a30 ), its lower bound \u03b2 \u2217 and\nthe simulation study are given in, respectively, Appendices A, B and C. Appendix C also holds additional simulation\nresults.\nII.\n\nAIC AND AICC\n\nWe consider the seemingly unrelated regressions model\nY = ZB + U.\n\n(1)\n\nHere, Y is an N \u00d7 p matrix of p response variables on N subjects, Z is a known N \u00d7 M matrix of N values of M\ncovariates, each row of the N \u00d7 p matrix U has independent Np (0, \u03a3) distribution, and B is an M \u00d7 p matrix holding\nK \u2264 M p regression coefficients and (M p \u2212 K) zeroes. The restriction Bij = 0 means that response variable yj of the\nj-th column of Y does not depend on covariate zi of the i-th column of Z. The entries of the elements of the j-th\n\n\u2217 Electronic\n\naddress: j.l.van.velsen@minjus.nl\n\n\f2\ncolumn of B that are not restricted to zero, are collected in the set Jj . Each column of the matrix B holds at least\none regression coefficient, which means that Jj is non-empty for all j. Throughout this work, we assume that the\nM \u00d7 M matrix Z T Z is positive definite, that p and M do not scale with N , and that limN \u2192\u221e N \u22121 Z T Z is finite and\npositive definite.\nSuppose that Y is not generated by the model of Eq. (1), but by the model\nY = Z0 B0 + E.\n\n(2)\n\nHere, Z0 is an N \u00d7 M0 matrix of N values of M0 unknown true covariates, B0 is an M0 \u00d7 p matrix of unknown\ncoefficients and each row of the N \u00d7 p matrix E has independent Np (0, \u03a30 ) distribution with unknown covariance\nmatrix \u03a30 . The entries of the non-vanishing elements of the j-th column of B0 are collected in the set J0j . A measure\nof the discrepancy between the candidate (or approximating) model of Eq. (1) and the data-generating model of Eq.\n(2) is the Kullback-Leibler information\n\u2206(B, \u03a3) = E0 {\u22122L(B, \u03a3)} = N p ln 2\u03c0 + N ln Det\u03a3 + Tr(Z0 B0 \u2212 ZB)T (Z0 B0 \u2212 ZB)\u03a3\u22121 + N Tr\u03a30 \u03a3\u22121 ,\n\n(3)\n\nwhere E0 denotes expectation under the data-generating model and L(B, \u03a3) is the log-likelihood function of the\ncandidate model,\n\u2212 2L(B, \u03a3) = N p ln 2\u03c0 + N ln Det\u03a3 + Tr(Y \u2212 ZB)T (Y \u2212 ZB)\u03a3\u22121 .\n\n(4)\n\nAIC is an estimator of the expected Kullback-Leibler information E0 {\u2206(B\u0302, \u03a3\u0302)}, where B\u0302 and \u03a3\u0302 are the maximum\nlikelihood estimators of, respectively, B and \u03a3. It is defined as the sum of \u22122L(B\u0302, \u03a3\u0302) and twice the number of fitted\nparameters,\nAIC(\u03a3\u0302) = N ln Det\u03a3\u0302 + N p(ln 2\u03c0 + 1) + 2K + p(p + 1).\n\n(5)\n\nIn Appendix A, with the assumption that the candidate model is either correctly specified or overspecified (Z0 = Z\nand J0i \u2286 Ji for all i), we demonstrate that\nBAIC = E0 {AIC(\u03a3\u0302)} \u2212 E0 {\u2206(B\u0302, \u03a3\u0302)} = \u2212N \u22121 \u03b2(\u03a30 ) + o(N \u22121 ),\n\n(6)\n\nwhere \u03b2(\u03a30 ) = O(1) takes the form\n\u03b2(\u03a30 ) = 6K(p + 1) + 2Tr(TrS P0 )2 \u2212 3TrP0 P0TS \u2212 3Tr(TrR P0 )(TrR P0T ) + p(p + 1)2 .\n\n(7)\n\nHere, the N p \u00d7 N p oblique projection matrix P0 is given by\n\u22121 T\nP0 = X{X T (\u03a3\u22121\nX (\u03a3\u22121\n0 \u2297 11N )X}\n0 \u2297 11N ),\n\n(8)\n\nwhere X is an N p \u00d7 K block-diagonal matrix of p blocks of N \u00d7 |Ji | matrices Xi holding the |Ji | columns of Z\ncorresponding to zj with j \u2208 Ji ,\n\uf8eb\n\uf8f6\nX1 0 0\n\uf8ec\n\uf8f7\nX = \uf8ed 0 ... 0 \uf8f8 .\n(9)\n0 0 Xp\nIn Eq. (7), the operators 'TrS ' and 'TrR ' denote partial traces over, respectively, the N subjects and the p response\nPN\nvariables. Given an N p \u00d7 N p matrix A, TrS A is the p \u00d7 p matrix defined componentwisely as (TrS A)ij = n=1 Ain,jn ,\nwhere Ain,jm is multi-index notation for A(i\u22121)N +n,(j\u22121)N +m . Similarly, TrR A is the N \u00d7 N matrix with elements\nPp\n(TrR A)nm = i=1 Ain,im . Finally, in Eq. (7), 'TS ' denotes the partial transpose of subjects: (ATS )in,jm = Aim,jn .\nIf Ji = Jj for all i and j, \u03b2(\u03a30 ) collapses to\n\u03b2 \u2217 = 3K(p + 1) + 2K 2 p\u22121 + p(p + 1)2 ,\n\n(10)\n\nwhich equals the coefficient of the first term of the expansion of \u2212BAIC of Ref. [6] in inverse powers of N . In Appendix\nB, we demonstrate that\n\u03b2 \u2217 \u2264 min \u03b2(\u03a9),\n\u03a9\n\n(11)\n\n\f3\nwhere the minimization is over all p \u00d7 p symmetric positive definite matrices \u03a9 and the equality sign is attained if\nand only if Ji = Jj for all i and j. We define AICc as\nAICc(\u03a3\u0302) = AIC(\u03a3\u0302) + N \u22121 \u03b2 \u2217 .\n\n(12)\n\nBAICc = E0 {AICc(\u03a3\u0302)} \u2212 E0 {\u2206(B\u0302, \u03a3\u0302)} = \u2212N \u22121 {\u03b2(\u03a30 ) \u2212 \u03b2 \u2217 } + o(N \u22121 )\n\n(13)\n\nBecause 0 < \u03b2 \u2217 \u2264 min\u03a9 \u03b2(\u03a9),\n\nsatisfies\nlim N BAIC < lim N BAICc \u2264 0.\n\nN \u2192\u221e\n\nIII.\n\nN \u2192\u221e\n\n(14)\n\nA SIMULATION STUDY\n\nWe compare the performance of AIC, AICc and BIC in the selection of seemingly unrelated regressions models.\nFor this purpose, 1000 samples of sizes N = 15, N = 20 and N = 50 are created from the data-generating model (2)\nwith p = 2. For each sample and each criterion, the fitted candidate model with the smallest value of the criterion\nis selected from a set of candidate models. The matrix Z holds the values of 10 covariates and its 10N elements are\nfixed after drawing them independently from N (0, 1). We consider 25 candidate models specified by J1 = {1, . . . , i}\nand J2 = {6, . . . , 5 + j}, where i and j are integers ranging from 1 to 5. For the data-generating model, we set Z0 = Z\nand take J01 = {1, 2} and J02 = {6, 7}. The 4 non-vanishing elements of B0 equal unity and the covariance matrix\n\u03a30 has parametrization \u03a30 = (1 \u2212 \u03c1)11p + \u03c1jp , where jp is the p \u00d7 p matrix of ones and |\u03c1| < 1. The samples are\nconstructed based on 1000 independent drawings of E, where each row of E is independently drawn from Np (0, \u03a30 ).\nThe candidate models are fitted with the constrained maximization (CM) algorithm [15, 16]:\n\u03a3\u0302n+1 = N \u22121 (Y \u2212 Z B\u0302n )T (Y \u2212 Z B\u0302n ),\n\n\u22121 T\nwhere vec(Z B\u0302n ) = X{X T(\u03a3\u0302\u22121\nX (\u03a3\u0302\u22121\nn \u2297 11N )X}\nn \u2297 11N )vec(Y ).\n\n(15)\n\nHere, \u03a3\u0302n+1 and B\u0302n are estimators of, respectively, \u03a3 and B, n is a positive integer and 'vec' is the column-wise\nvectorization operator. The algorithm is started with \u03a3\u03021 = 11p and terminated if |Det\u03a3\u0302n+1 \u2212 Det\u03a3\u0302n | \u2264 \u03b4Det\u03a3\u0302n , with\n\u03b4 = 1 * 10\u22127 . If the log-likelihood function L(B, \u03a3) is globally concave, then \u03a3\u0302n+1 and B\u0302n converge to, respectively,\n\u03a3\u0302 and B\u0302 and the numerical error of ln Det\u03a3\u0302 is of the order of magnitude of \u03b4. If L(B, \u03a3) is multi-modal, the CM\nalgorithm does not necessarily converge to the global maximum, but may end up in a local maximum or a saddle\npoint [17, 18]. Although multi-modality is rare, we choose several other initial estimators \u03a3\u03021 and calculate ln Det\u03a3\u0302\nwith a numerical error of about 10\u03b4 (see Appendix C for details). This means that the difference between two values\nof a criterion has a numerical error of 20N \u03b4.\nThe frequencies of selecting the 25 candidate models with the three criteria are given in Table I for \u03c1 = 0.5 and\nN = 15. The correct model (i = j = 2) is more often selected with AICc than with AIC and BIC. To see how\nthe improvement of AICc on AIC is related to the bias correction, we have plotted E0 {\u2206(\u03a3\u0302, B\u0302)}, E0 {AICc(\u03a3\u0302)} and\nE0 {AIC(\u03a3\u0302)} as a function of i (with j = 2) in Fig. 1. The expected Kullback-Leibler information has a minimum\nat i = 2 and increases rapidly with i for i > 2. This increase is more precisely followed by E0 {AICc(\u03a3\u0302)} than by\nE0 {AIC(\u03a3\u0302)}, which explains why AIC more often selects models that are too complex. In Appendix C, the frequencies\nof selecting the correct model with the three criteria are given for \u03c1 = 0.2, 0.5, 0.8 and N = 15, 20, 50. The frequencies\ndo not depend much on \u03c1 and, as expected, the improvement of AICc on AIC decreases as N increases. For N = 20,\nAICc and BIC perform equally well, while for N = 50, the asymptotically consistent BIC outperforms AICc. In\nAppendix C, we also demonstrate that \u03b4 is sufficiently small and that the results of Table I are not affected by\nnumerical errors.\n\n\f4\nTABLE I: Frequencies of selecting the 25 candidate models with AIC, AICc and BIC in 1000 samples of size N = 15 for \u03c1 = 0.5.\ni\n1\n2\n3\n4\n5\n\nAIC\nj\n1 2\n0 1\n3 241\n1 53\n1 49\n4 63\n\n3\n0\n75\n20\n25\n31\n\n4\n0\n60\n16\n36\n46\n\nAICc\nj\n5 1 2\n0 0 2\n66 9 488\n34 5 64\n52 3 49\n123 4 37\n\n3\n0\n83\n14\n12\n12\n\n4\n1\n50\n14\n18\n17\n\n5\n0\n40\n12\n21\n45\n\nBIC\nj\n1 2\n0 2\n7 385\n4 59\n4 47\n1 50\n\n3\n0\n78\n15\n19\n20\n\n4\n0\n53\n18\n26\n29\n\n5\n0\n58\n23\n30\n72\n\nExpected criterion\n\n40\n\n30\n\n20\n\n10\n\n0\n1\n\n2\n\n3\n\n4\n\n5\n\ni\nFIG. 1: Expected Kullback-Leibler information (triangles), AICc (squares) and AIC (circles) as a function of i with j = 2 for\nN = 15 and \u03c1 = 0.5. The expected criteria are estimated with the same 1000 samples as the ones of Table I. The standard\nerror of the expected AIC (and AICc) is about 0.3 for all i and that of the expected Kullback-Leibler information ranges from\n0.3 (i = 1) to 1.8 (i = 5).\n\nIV.\n\nDISCUSSION\n\nIn the simulation study of Sec. III, the data-generating model is finite dimensional and one of the candidate models\nis correctly specified. The case of an infinite dimensional data-generating model is not considered here. Although in\nthis case the assumption of correct specification or overspecification does not hold for any candidate model, Hurvich\nand Tsai [19] demonstrated that for linear regression models in small samples, AICc is much less biased than AIC for\nmost choices of the data-generating model. A similar study can be done for seemingly unrelated regressions models.\nAlso, for an infinite dimensional data-generating model, AIC and AICc are asymptotically efficient [20, 21] and, based\non the results of Ref. [19], it can be surmised that in small samples, AICc is more efficient than AIC and BIC for\nmost choices of the data-generating model.\nAPPENDIX A: BIAS OF AIC\n\nIn this Appendix, we demonstrate that BAIC = \u2212N \u22121 \u03b2(\u03a30 ) + o(N \u22121 ), where \u03b2(\u03a30 ) = O(1) is given by Eq. (7).\nFirst, we calculate \u03b3\u0302 in the expansion\nAIC(\u03a3\u0302) \u2212 \u2206(B\u0302, \u03a3\u0302) = \u2212\u03b3\u0302 + op (N \u22121 ).\n\n(A1)\n\nTaking the expectation under the data-generating model of both sides of Eq. (A1) yields BAIC = \u2212E0 (\u03b3\u0302) + o(N \u22121 ).\nSecond, we calculate E0 (\u03b3\u0302) and find \u03b2(\u03a30 ) from E0 (\u03b3\u0302) = N \u22121 \u03b2(\u03a30 ) + o(N \u22121 ).\n\n\f5\n1.\n\nThe first term of the expansion of Eq. (A1)\n\nlim\n\nn\n\nWe consider the expansion\nn\u2192\u221e\n\no\nAIC(\u03a3\u0302n+1 ) \u2212 \u2206(B\u0302n , \u03a3\u0302n+1 ) = \u2212\u03b7\u0302 + op (N \u22121 ),\n\n(A2)\n\nwhere the estimators \u03a3\u0302n+1 and B\u0302n of, respectively, \u03a3 and B at the n-th step of the constrained maximization (CM)\nalgorithm, are given by Eq. (15). Depending on the initial estimator \u03a3\u03021 , Drton and Richardson [17] demonstrated\nthat the CM algorithm may end up in a local maximum or a saddle point of L(B, \u03a3), rather than in the global\nmaximum L(B\u0302, \u03a3\u0302). It turns out, however, that \u03b7\u0302 does not depend on \u03a3\u03021 , which implies \u03b3\u0302 = \u03b7\u0302.\nBecause the candidate model is either correctly specified or overspecified, the left-hand side of Eq. (A2) can be\nwritten as\nn\no\nlim AIC(\u03a3\u0302n+1 ) \u2212 \u2206(B\u0302n , \u03a3\u0302n+1 ) = 2K + p(p + 1)\nn\u2192\u221e\n\uf8f1 \uf8eb\n\uf8fc\n\uf8f6\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8fd\n\uf8f7\n\uf8ec\n\u2212 N lim Tr \uf8ed\u03a30 \u2212 N \u22121 TrS \u01eb\u01ebT + N \u22121 TrS \u01eb\u01ebT P\u0302nT + N \u22121 TrS P\u0302n \u01eb\u01ebT \uf8f8 \u03a3\u0302\u22121\n, (A3)\nn+1\n{z\n} |\n{z\n} | {z\n|\nn\u2192\u221e \uf8f4\n{z\n} |\n}\uf8f4\n\uf8f3\n\uf8fe\n\u22121\n\u22121/2\nOp (N\n\n)\n\nOp (N \u22121 )\n\nOp (N\n\n)\n\nOp (1)\n\nwhere \u01eb = vec(E), 'vec' is the column-wise vectorization operator,\n\u03a3\u0302n+1 = N \u22121 TrS (11N p \u2212 P\u0302n )\u01eb\u01ebT (11N p \u2212 P\u0302n )T\n\n\u22121 T\nand P\u0302n = X{X T (\u03a3\u0302\u22121\nX (\u03a3\u0302\u22121\nn \u2297 11N )X}\nn \u2297 11N ).\n\n(A4)\n\n(By writing it as N Tr\u03a3\u0302n+1 \u03a3\u0302\u22121\nn+1 , the part N p of AIC is absorbed in the second line of Eq. (A3).) The order symbols\nbelow the horizontal curly braces in Eq. (A3) refer to the elements of the corresponding matrices. From now on,\nwhen an order symbol refers to a matrix, all of its elements are of the indicated order. (The N p \u00d7 N p matrix P\u0302n is\nOp (N \u22121 ) because N \u22121 Z T Z = O(1).)\nThe matrix \u03a3\u0302\u22121\nn has expansion\n\u22121\n\u03a3\u0302\u22121\nn = \u03a30\n\nQ\nX\nj=0\n\noj\nn\n+ op (N \u2212Q/2 ),\n(\u22121)j (\u03a3\u0302n \u2212 \u03a30 )\u03a3\u22121\n0\n\n(A5)\n\nwhere Q is a non-negative integer. The expansion of Eq. (A5) holds because p = O(1). Similarly, because K = O(1),\n\u22121\nthe matrix {X T (\u03a3\u0302\u22121\nhas expansion\nn \u2297 11N )X}\n\u22121\n=\n{X T (\u03a3\u0302\u22121\nn \u2297 11N )X}\n\u2032\n\n{X\n\nT\n\n(\u03a3\u22121\n0\n\n\u2297 11N )X}\n\n\u22121\n\nQ\nX\nj=0\n\nij\nh\n\u2032\n\u22121\n\u22121\nT\n\u22121\n+ op (N \u2212Q /2\u22121 ),\n(\u22121)j X T {(\u03a3\u0302\u22121\nn \u2212 \u03a30 ) \u2297 11N }X{X (\u03a30 \u2297 11N )X}\n\n(A6)\n\nwhere Q\u2032 is a non-negative integer. By combining Eq. (A5) with Q = 2, Eq. (A6) with Q\u2032 = 2 and limn\u2192\u221e \u03a3\u0302n =\nN \u22121 TrS (11N p \u2212 P0 )\u01eb\u01ebT (11N p \u2212 P0 )T + op (N \u22121 ), where P0 = O(N \u22121 ) is given by Eq. (8), we obtain\nlim P\u0302n = P0 + P\u0302\u22123/2 + P\u0302\u22122 + op (N \u22122 ),\n\nn\u2192\u221e\n\n(A7)\n\nwhere the matrices P\u0302\u22123/2 = Op (N \u22123/2 ) and P\u0302\u22122 = Op (N \u22122 ) are given by\nP\u0302\u22123/2 = P0 {(N \u22121 TrS \u01eb\u01ebT \u2212 \u03a30 ) \u2297 11N }(P0T \u2212 11N p )(\u03a3\u22121\n0 \u2297 11N )\n\n(A8)\n\nand\nP\u0302\u22122 = N \u22121 P0 {(TrS \u01eb\u01ebT P0T + TrS P0 \u01eb\u01ebT \u2212 TrS P0 \u01eb\u01ebT P0T ) \u2297 11N }(11N p \u2212 P0T )(\u03a3\u22121\n0 \u2297 11N )\n\u2212 P\u0302\u22123/2 {(N \u22121 TrS \u01eb\u01ebT \u2212 \u03a30 ) \u2297 11N }(\u03a3\u22121\n0 \u2297 11N )(11N p \u2212 P0 ).\n\n(A9)\n\n\f6\nBy combining Eq. (A5) with Q = 3 and limn\u2192\u221e P\u0302n = P0 + P\u0302\u22123/2 + op (N \u22123/2 ), we obtain\n\u22121\nlim \u03a3\u0302\u22121\nn+1 = \u03a30\n\nn\u2192\u221e\n\n3\nX\nj=0\n\n(\u22121)j\n\n\u0011j\ni\n\u0010h\n+ op (N \u22123/2 ).\nN \u22121 TrS {11N p \u2212 (P0 + P\u0302\u22123/2 )}\u01eb\u01ebT {11N p \u2212 (P0 + P\u0302\u22123/2 )}T \u2212 \u03a30 \u03a3\u22121\n0\n\n(A10)\nSubstituting the expansions of Eqs. (A7,A10) in the right-hand side of Eq. (A3), expressing it as the right-hand\nside of Eq. (A2), and noting that \u03b3\u0302 = \u03b7\u0302 (because \u03b7\u0302 does not depend on \u03a3\u03021 ), yields\n\u03b3\u0302 = N Tr(\u03a30 \u2212 N \u22121 TrS \u01eb\u01ebT )\u03a30\u22121 + \u03b3\u03020 + \u03b3\u0302\u22121/2 + \u03b3\u0302\u22121 ,\n\n(A11)\n\nwhere \u03b3\u03020 = Op (1), \u03b3\u0302\u22121/2 = Op (N \u22121/2 ) and \u03b3\u0302\u22121 = Op (N \u22121 ) are given by\n\u22121\n2\n\u03b3\u03020 = \u22122K \u2212 p(p + 1) + Tr(TrS \u01eb\u01ebT P0T + TrS P0 \u01eb\u01ebT )\u03a3\u22121\nTrS \u01eb\u01ebT )\u03a3\u22121\n0 + N Tr{(\u03a30 \u2212 N\n0 } ,\n\n\u22121\nT T\nT\n\u22121\n3\n\u03b3\u0302\u22121/2 = 2Tr(\u03a30 \u2212 N \u22121 TrS \u01eb\u01ebT )\u03a3\u22121\nTrS \u01eb\u01ebT )\u03a3\u22121\n0 (TrS \u01eb\u01eb P0 + TrS P0 \u01eb\u01eb )\u03a30 + N Tr{(\u03a30 \u2212 N\n0 }\n\u22121\n\u22121\nT T\nT\nT T\n\u2212 Tr(\u03a30 \u2212 N \u22121 TrS \u01eb\u01ebT )\u03a3\u22121\n0 (TrS P0 \u01eb\u01eb P0 )\u03a30 + Tr(TrS \u01eb\u01eb P\u0302\u22123/2 + TrS P\u0302\u22123/2 \u01eb\u01eb )\u03a30\n\n(A12)\n\n(A13)\n\nand\n\u22121\nT T\nT\nT T\n\u03b3\u0302\u22121 = N \u22121 Tr(TrS \u01eb\u01ebT P0T + TrS P0 \u01eb\u01ebT )\u03a3\u22121\n0 (TrS \u01eb\u01eb P0 + TrS P0 \u01eb\u01eb \u2212 TrS P0 \u01eb\u01eb P0 )\u03a30\n\u22121\n\u22121\nT\n\u22121\nT\nT T\n+ 2Tr(TrS \u01eb\u01eb P\u0302\u22123/2 + TrS P\u0302\u22123/2 \u01eb\u01eb )\u03a30 (\u03a30 \u2212 N TrS \u01eb\u01eb )\u03a30\n\u22121\nT\n4\n+ TrS P\u0302\u22122 \u01eb\u01ebT )\u03a3\u22121\nTrS \u01eb\u01ebT )\u03a3\u22121\n+ Tr(TrS \u01eb\u01ebT P\u0302\u22122\n0 + N Tr{(\u03a30 \u2212 N\n0 }\n\u22121\n\u22121\nT T\nT T\n\u22121\nT\n\u2212 Tr(TrS P\u0302\u22123/2 \u01eb\u01eb P0 + TrS P0 \u01eb\u01eb P\u0302\u22123/2 )\u03a30 (\u03a30 \u2212 N TrS \u01eb\u01eb )\u03a30\n\n(A14)\n\n\u22121\n2\n+ 3Tr(TrS \u01eb\u01ebT P0T + TrS P0 \u01eb\u01ebT )\u03a3\u22121\nTrS \u01eb\u01ebT )\u03a3\u22121\n0 {(\u03a30 \u2212 N\n0 }\n\u22121\n\u22121 2\n\u22121\nT\nT T\n\u2212 2Tr(TrS P0 \u01eb\u01eb P0 )\u03a30 {(\u03a30 \u2212 N TrS \u01eb\u01eb )\u03a30 } .\n\n2.\n\nExpectation under the data-generating model\n\nThe elements of the N p-dimensional Gaussian columnvector \u01eb = vec(E) have vanishing mean and two-point average\nE0 (\u01ebin \u01ebjm ) = (\u03a30 )ij \u03b4nm ,\nwhere \u01ebin is multi-index notation for \u01ebN (i\u22121)+n = Ein and \u03b4nm is a Kronecker delta.\nN \u22121 TrS \u01eb\u01ebT )\u03a3\u22121\n0 } = 0, E0 (\u03b3\u0302) takes the form\n\n(A15)\nBecause E0 {N Tr(\u03a30 \u2212\n\nE0 (\u03b3\u0302) = E0 (\u03b3\u03020 ) + E0 (\u03b3\u0302\u22121/2 ) + E0 (\u03b3\u0302\u22121 ).\n\n(A16)\n\nApplying Wick's theorem, which states\nQg that the average of a product of 2g elements of \u01eb, where g is a positive integer,\nequals the sum of products of all i=1 (2i \u2212 1) possible pairings of two-point averages, we obtain\nE0 (\u03b3\u03020 ) = 0,\n\n(A17)\n\nE0 (\u03b3\u0302\u22121/2 ) = N \u22121 [\u22126K(p + 1) + 3TrP0 P0TS + 3Tr(TrR P0 )(TrR P0T ) \u2212 {p2 + 3p + p(p + 1)2 }]\n\n(A18)\n\nand\nE0 (\u03b3\u0302\u22121 ) = N \u22121 {12K(p+1)+2Tr(TrS P0 )2 \u22126TrP0 P0TS \u22126Tr(TrR P0 )(TrR P0T )+p2 +3p+2p(p+1)2}+o(N \u22121 ). (A19)\nSubstituting Eqs. (A17,A18,A19) in Eq. (A16), yields\nE0 (\u03b3\u0302) = N \u22121 \u03b2(\u03a30 ) + o(N \u22121 ),\nwhere \u03b2(\u03a30 ) = O(1) is given by Eq. (7).\n\n(A20)\n\n\f7\nAPPENDIX B: PROOF OF EQ. (11)\n\nIn this Appendix, we demonstrate\n\u2212 3K(p + 1) + 2K 2 p\u22121 \u2264 min\n\u03a9\n\n\u0014n\n\no\n2Tr(TrS P0 )2 \u2212 3TrP0 P0TS \u2212 3Tr(TrR P0 )(TrR P0T )\n\n\u03a30 =\u03a9\n\n\u0015\n\n,\n\n(B1)\n\nwhere the minimization is over all p \u00d7 p symmetric positive definite matrices \u03a9 and the equality sign is attained if and\nonly if Ji = Jj for all i and j. By adding 6K(p + 1) + p(p + 1)2 on both sides of Eq. (B1), we obtain \u03b2 \u2217 \u2264 min\u03a9 \u03b2(\u03a9)\nof Eq. (11).\n\u22121/2\n1/2\nUsing TrS P0 = \u03a30 (TrS A)\u03a30 , where\n\u22121/2\n\nA = (\u03a30\n\n1/2\n\n\u2297 11N )P0 (\u03a30\n\n\u22121/2\n\n\u2297 11N ) = (\u03a30\n\n\u22121/2\n\n\u22121 T\n\u2297 11N )X{X T (\u03a3\u22121\nX (\u03a30\n0 \u2297 11N )X}\n\n\u2297 11N ),\n\n(B2)\n\nwe find that Tr(TrS P0 )2 can be written as\nTr(TrS P0 )2 = Tr(TrS A)2 .\n\n(B3)\n\n\u0001\nmin Tr C 2 Tr C = K = K 2 p\u22121 ,\n\n(B4)\n\nFrom\nC\n\nwhere the minimization is over all p \u00d7 p symmetric matrices C, we obtain\ni\nh\b\ni\nh\b\nmin Tr(TrS P0 )2 \u03a30 =\u03a9 = min Tr(TrS A)2 \u03a30 =\u03a9 \u2265 K 2 p\u22121 .\n\u03a9\n\n\u03a9\n\n(B5)\n\nThe minimum of Eq. (B4) is attained if and only if Cii = Kp\u22121 and Cij = 0 for all i 6= j. This corresponds to\nTrS A = Kp\u22121 11p , which can be reached if Ji = Jj for all i and j or if \u03a30 = 11p and |Ji | = |Jj | for all i and j.\nBecause\n1/2\n\nP0TS = (\u03a30\n\n\u22121/2\n\n\u2297 11N )ATS (\u03a30\n\n\u2297 11N ),\n\n(B6)\n\nTrP0 P0TS equals the inner product of ATS and A:\nTrP0 P0TS = TrATS AT .\n\n(B7)\n\nThe squared length TrAAT of A equals K (A is an orthogonal projection matrix of rank K). The squared length of\nATS equals that of A and we have\n\u001b\n\u001a\u0010\no \b\nn\n\u0011\n\u0001\n1/2\nTS\n= K.\n(B8)\n= max TrATS AT \u03a30 =\u03a9 \u2264 TrAAT TrATS (ATS )T\nTrP0 P0\nmax\n\u03a9\n\n\u03a30 =\u03a9\n\n\u03a9\n\nThe upper bound of K in Eq. (B8) is attained if and only if A = ATS , which can be reached if Ji = Jj for all i and\nj or if \u03a30 = 11p .\nUsing TrR P0 = TrR A, we find\nTr(TrR P0 )(TrR P0T )\n\nT\n\n= Tr(TrR A)(TrR A) =\n\np\np X\nX\n\nTraii aT\njj ,\n\n(B9)\n\ni=1 j=1\n\nwhere aij is the ij-th N \u00d7 N submatrix of A. The sum of the squared lengths of the aii 's is bounded by\np\nX\n\nTraii aT\nii \u2264\n\np\np X\nX\n\nT\nTraij aT\nij = TrAA = K\n\n(B10)\n\ni=1 j=1\n\ni=1\n\nThe upper bound pK of\n\uf8eb\n\uf8f6\np\np X\np\nX\nX\n\uf8f8 \u2264 pK,\nTrcii cT\nmax \uf8ed\nTrcii cT\njj\nii \u2264 K\n{cii }\n\ni=1 j=1\n\ni=1\n\n(B11)\n\n\f8\nPp\nwhere the maximization is over p symmetric N \u00d7 N matrices cii , is attained if and only if cii = cjj and i=1 Trcii cT\nii =\nK. Translated to A, this means that aij = 0 for all i 6= j and the aii 's are identical orthogonal projection matrices of\nrank Kp\u22121 . This can be reached if and only if Ji = Jj for all i and j. It follows that\nh\b\ni\n(B12)\nmax Tr(TrR P0 )(TrR P0T ) \u03a30 =\u03a9 \u2264 pK,\n\u03a9\n\nwhere the equality sign is attained if and only if Ji = Jj for all i and j.\nBy combining the bounds of Eqs. (B5,B8,B12), we obtain Eq. (B1).\nAPPENDIX C: DETAILS ABOUT THE SIMULATION STUDY\n\nIn this Appendix, we give the algorithm used to calculate the maximum likelihood estimators. Also, additional\nsimulation results are presented and \u03b4 is demonstrated to be sufficiently small.\n1.\n\nCalculating the maximum likelihood estimators\n\nThe CM algorithm is run with \u03a3\u03021 = 11p and, after convergence is achieved (|Det\u03a3\u0302n+1 \u2212 Det\u03a3\u0302n | \u2264 \u03b4Det\u03a3\u0302n ), we set\n\u03a3\u0302temp = \u03a3\u0302n+1 and B\u0302temp = B\u0302n . Then, another \u03a3\u03021 is constructed by drawing a p \u00d7 p matrix from Wp (11p , p), where\n'W ' denotes a Wishart distribution, and dividing it by p. With the randomly created \u03a3\u03021 , the CM algorithm is run\nup to convergence (possibly with another number of iterations than in the previous run) and if the newly calculated\n\u03a3\u0302new = \u03a3\u0302n+1 and B\u0302new = B\u0302n satisfy\nL(B\u0302new , \u03a3\u0302new ) > L(B\u0302temp , \u03a3\u0302temp ) and |Det\u03a3\u0302new \u2212 Det\u03a3\u0302temp| > 10\u03b4Det\u03a3\u0302temp ,\n\n(C1)\n\nwe set \u03a3\u0302temp = \u03a3\u0302new and B\u0302temp = B\u0302new . The above is repeated until \u03a3\u0302temp and B\u0302temp remain unchanged for 10\ndifferent randomly created \u03a3\u03021 's in a row. When the algorithm is terminated, L(B\u0302temp , \u03a3\u0302temp) is considered to be the\nglobal maximum of L(B, \u03a3) and we set B\u0302 = B\u0302temp and \u03a3\u0302 = \u03a3\u0302temp . Table II holds the number of jumps of \u03a3\u0302temp (and\nB\u0302temp ) in 1000 samples of size N = 15 for \u03c1 = 0.5. (These are the same samples as the ones of Sec. III.) It turns out\nthat multi-modality is indeed rare: The candidate model with i = j = 5 has a multi-modal L(B, \u03a3) in at most 1.6%\nof the 1000 samples. Table II also holds the number of additional \u03a3\u03021 's in the 1000 samples. There are about 2 to 3\nadditional \u03a3\u03021 's per jump and a maximum of 5 additional \u03a3\u03021 's per jump.\nTABLE II: Number of jumps of \u03a3\u0302temp and number of additional \u03a3\u03021 's in 1000 samples of size N = 15 for \u03c1 = 0.5.\ni\n1\n2\n3\n4\n5\n\n2.\n\njumps\nj\n1 2 3\n0 0 0\n0 0 0\n0 0 1\n0 2 1\n0 2 5\n\n4\n0\n1\n2\n2\n5\n\nadditional \u03a3\u03021 's\nj\n5 1 2 3 4 5\n0 0 0 0 0 0\n1 0 0 0 5 2\n1 0 0 5 2 5\n7 0 4 1 4 18\n16 0 3 10 10 38\n\nAdditional simulation results\n\nThe frequencies of selecting the correct model with AIC, AICc and BIC in 1000 samples of sizes N = 15, 20, 50 are\ngiven in Table III for \u03c1 = 0.2, 0.5, 0.8. In the simulation, the values of the covariates in the samples of sizes N = 15 and\nN = 20 are the same as, respectively, the first 15 and 20 values of the covariates in the samples of size N = 50. Table\nIII also holds the number of times that the difference between the second smallest and smallest value of a criterion\nis less than 200N \u03b4 (10 times the numerical error of the difference). These numbers are of order unity such that \u03b4 is\nsufficiently small. The average (over 1000 samples) of the difference between the second smallest and smallest value\nof a criterion is not given in Table III, but it ranges from 3.8 (for AIC with N = 15 and \u03c1 = 0.2) to 52.4 (for BIC\n\n\f9\nwith N = 50 and \u03c1 = 0.8). In all 1000 samples of sizes N = 20 and N = 50, there are no jumps of \u03a3\u0302temp . In the\nsamples of size N = 15, the number of jumps of \u03a3\u0302temp and the number of additional \u03a3\u03021 's do not depend much on \u03c1.\nTABLE III: Frequencies f and \u03bd in 1000 samples of, respectively, selecting the correct model and the difference between the\nsecond smallest and smallest value of a criterion being smaller than 200N \u03b4.\nN \u03c1\n15 0.2\n0.5\n0.8\n20 0.2\n0.5\n0.8\n50 0.2\n0.5\n0.8\n\nAIC\n249\n241\n233\n366\n353\n324\n493\n528\n503\n\nf\nAICc\n500\n488\n473\n570\n604\n553\n590\n616\n594\n\n\u03bd\nBIC AIC AICc BIC\n397\n0\n1\n0\n385\n0\n0\n0\n365\n1\n0\n0\n577\n0\n0\n0\n611\n0\n2\n0\n556\n0\n0\n0\n835\n0\n1\n0\n832\n0\n1\n0\n848\n0\n0\n0\n\n[1] Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In 2nd International Symposium on Information Theory, Ed. B. N. Petrov and F. Csaki, pp. 267-281. Budapest: Akademia Kiado.\n[2] Hurvich, C. M. and Tsai, C. -L. (1989). Regression and time series model selection in small samples. Biometrika 76,\n297-307.\n[3] Sugiura, N. (1978). Further analysis of the data by Akaike's information criterion and the finite corrections. Comm. Statist.\nA7, 13-26.\n[4] Hurvich, C. M., Shumway, R. and Tsai, C. -L. (1990). Improved estimators of Kullback-Leibler information for autoregressive model selection in small samples. Biometrika 77, 709-719.\n[5] Hurvich, C. M. and Tsai, C. -L. (1993). A corrected Akaike information criterion for vector autoregressive model selection.\nJ. Time Ser. Anal. 14, 271-279.\n[6] Bedrick, E. J. and Tsai, C. -L. (1994). Model selection for multivariate regression in small samples. Biometrics 50, 226-231.\n[7] Zellner, A. (1962). An efficient method of estimating seemingly unrelated regressions and tests for aggregation bias. J. Am.\nStatist. Assoc. 57, 348-368.\n[8] Srivastava, V. K. and Giles, D. E. A. (1987). Seemingly Unrelated Regression Equations Models. New York: Marcel Dekker.\n[9] Goldberger, A. S. (1991). A Course in Econometrics, p. 323. Cambridge, Massachusetts: Harvard University Press.\n[10] Verbyla, A. P. and Venables, W. N. (1988). An extension of the growth curve model. Biometrika 75, 129-138.\n[11] Rochon, J. (1996). Analyzing bivariate repeated measures for discrete and continuous outcome variables. Biometrics 52,\n740-750.\n[12] Andersson, S. A., Madigan, D. and Perlman, M. D. (2001). Alternative Markov properties for chain graphs, Scand. J.\nStatist. 28, 33-85.\n[13] Linhart, H. and Zucchini, W. (1986). Model Selection, p. 245. New York: Wiley.\n[14] Schwarz, G. (1978). Estimating the dimension of a model. Ann. Statist. 6, 461-464.\n[15] Meng, X.-L. and Rubin, D. B. (1993). Maximum likelihood estimation via the ECM algorithm: A general framework.\nBiometrika 80, 267-278.\n[16] Oberhofer, W. and Kmenta, J. (1974). A general procedure for obtaining maximum likelihood estimates in generalized\nregression models. Econometrica 42, 579-590.\n[17] Drton, M. and Richardson, T. S. (2004). Multimodality of the likelihood in the bivariate seemingly unrelated regressions\nmodel. Biometrika 91, 383-392.\n[18] Drton, M. (2006). Computing all roots of the likelihood equations of seemingly unrelated regressions. J. Symb. Comput.\n41, 245-254.\n[19] Hurvich, C. M. and Tsai, C. -L. (1991). Bias of the corrected AIC criterion for underfitted regression and time series\nmodels. Biometrika 78, 499-509.\n[20] Shibata, R. (1980). Asymptotically efficient selection of the order of the model for estimating parameters of a linear process.\nAnn. Statist. 8, 147-164.\n[21] Shibata, R. (1981). An optimal selection of regression variabales. Biometrika 68, 45-54.\n\n\f"}