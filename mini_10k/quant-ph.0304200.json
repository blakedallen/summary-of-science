{"id": "http://arxiv.org/abs/quant-ph/0304200v1", "guidislink": true, "updated": "2003-04-30T09:12:23Z", "updated_parsed": [2003, 4, 30, 9, 12, 23, 2, 120, 0], "published": "2003-04-30T09:12:23Z", "published_parsed": [2003, 4, 30, 9, 12, 23, 2, 120, 0], "title": "On the properties of information gathering in quantum and classical\n  measurements", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=quant-ph%2F0304092%2Cquant-ph%2F0304020%2Cquant-ph%2F0304180%2Cquant-ph%2F0304012%2Cquant-ph%2F0304022%2Cquant-ph%2F0304179%2Cquant-ph%2F0304111%2Cquant-ph%2F0304101%2Cquant-ph%2F0304047%2Cquant-ph%2F0304045%2Cquant-ph%2F0304006%2Cquant-ph%2F0304147%2Cquant-ph%2F0304063%2Cquant-ph%2F0304169%2Cquant-ph%2F0304013%2Cquant-ph%2F0304004%2Cquant-ph%2F0304057%2Cquant-ph%2F0304038%2Cquant-ph%2F0304070%2Cquant-ph%2F0304009%2Cquant-ph%2F0304138%2Cquant-ph%2F0304064%2Cquant-ph%2F0304008%2Cquant-ph%2F0304157%2Cquant-ph%2F0304177%2Cquant-ph%2F0304081%2Cquant-ph%2F0304048%2Cquant-ph%2F0304146%2Cquant-ph%2F0304075%2Cquant-ph%2F0304182%2Cquant-ph%2F0304124%2Cquant-ph%2F0304118%2Cquant-ph%2F0304058%2Cquant-ph%2F0304044%2Cquant-ph%2F0304025%2Cquant-ph%2F0304096%2Cquant-ph%2F0304069%2Cquant-ph%2F0304073%2Cquant-ph%2F0304171%2Cquant-ph%2F0304024%2Cquant-ph%2F0304097%2Cquant-ph%2F0304173%2Cquant-ph%2F0304115%2Cquant-ph%2F0304143%2Cquant-ph%2F0304200%2Cquant-ph%2F0304007%2Cquant-ph%2F0304204%2Cquant-ph%2F0304175%2Cquant-ph%2F0304176%2Cquant-ph%2F0304154%2Cquant-ph%2F0304001%2Cquant-ph%2F0304148%2Cquant-ph%2F0304133%2Cquant-ph%2F0304071%2Cquant-ph%2F0304003%2Cquant-ph%2F0304078%2Cquant-ph%2F0304193%2Cquant-ph%2F0304141%2Cquant-ph%2F0304061%2Cquant-ph%2F0304080%2Cquant-ph%2F0304072%2Cquant-ph%2F0304202%2Cquant-ph%2F0304109%2Cquant-ph%2F0304102%2Cquant-ph%2F0304131%2Cquant-ph%2F0304110%2Cquant-ph%2F0304035%2Cquant-ph%2F0304113%2Cquant-ph%2F0304029%2Cquant-ph%2F0304049%2Cquant-ph%2F0304040%2Cquant-ph%2F0304201%2Cquant-ph%2F0304034%2Cquant-ph%2F0304178%2Cquant-ph%2F0304136%2Cquant-ph%2F0304105%2Cquant-ph%2F0304203%2Cquant-ph%2F0304206%2Cquant-ph%2F0304160%2Cquant-ph%2F0304056%2Cquant-ph%2F0304153%2Cquant-ph%2F0304114%2Cquant-ph%2F0304149%2Cquant-ph%2F0304031%2Cquant-ph%2F0304156%2Cquant-ph%2F0304002%2Cquant-ph%2F0409010%2Cquant-ph%2F0409111%2Cquant-ph%2F0409109%2Cquant-ph%2F0409082%2Cquant-ph%2F0409013%2Cquant-ph%2F0409162%2Cquant-ph%2F0409158%2Cquant-ph%2F0409074%2Cquant-ph%2F0409105%2Cquant-ph%2F0409202%2Cquant-ph%2F0409066%2Cquant-ph%2F0409014%2Cquant-ph%2F0409150%2Cquant-ph%2F0409019%2Cquant-ph%2F0409199&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On the properties of information gathering in quantum and classical\n  measurements"}, "summary": "The information provided by a classical measurement is unambiguously\ndetermined by the mutual information between the output results and the\nmeasured quantity. However, quantum mechanically there are at least two notions\nof information gathering which can be considered, one characterizing the\ninformation provided about the initial preparation, useful in communication,\nand the other characterizing the information about the final state, useful in\nstate-preparation and control. Here we are interested in understanding the\nproperties of these measures, and the information gathering capacities of\nquantum and classical measurements. We provide a partial answer to the question\n`in what sense does information gain increase with initial uncertainty?' by\nshowing that, for classical and quantum measurements which are symmetric with\nrespect to reversible transformations of the state space, the information gain\nregarding the initial state always increases with the observer's initial\nuncertainty. In addition, we calculate the capacity of all unitarily covariant\nand commutative permutation-symmetric measurements for obtaining classical\ninformation. While it is the von Neumann entropy of the effects which appears\nin the latter capacity, it is the subentropy which appears in the expression\nfor the former.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=quant-ph%2F0304092%2Cquant-ph%2F0304020%2Cquant-ph%2F0304180%2Cquant-ph%2F0304012%2Cquant-ph%2F0304022%2Cquant-ph%2F0304179%2Cquant-ph%2F0304111%2Cquant-ph%2F0304101%2Cquant-ph%2F0304047%2Cquant-ph%2F0304045%2Cquant-ph%2F0304006%2Cquant-ph%2F0304147%2Cquant-ph%2F0304063%2Cquant-ph%2F0304169%2Cquant-ph%2F0304013%2Cquant-ph%2F0304004%2Cquant-ph%2F0304057%2Cquant-ph%2F0304038%2Cquant-ph%2F0304070%2Cquant-ph%2F0304009%2Cquant-ph%2F0304138%2Cquant-ph%2F0304064%2Cquant-ph%2F0304008%2Cquant-ph%2F0304157%2Cquant-ph%2F0304177%2Cquant-ph%2F0304081%2Cquant-ph%2F0304048%2Cquant-ph%2F0304146%2Cquant-ph%2F0304075%2Cquant-ph%2F0304182%2Cquant-ph%2F0304124%2Cquant-ph%2F0304118%2Cquant-ph%2F0304058%2Cquant-ph%2F0304044%2Cquant-ph%2F0304025%2Cquant-ph%2F0304096%2Cquant-ph%2F0304069%2Cquant-ph%2F0304073%2Cquant-ph%2F0304171%2Cquant-ph%2F0304024%2Cquant-ph%2F0304097%2Cquant-ph%2F0304173%2Cquant-ph%2F0304115%2Cquant-ph%2F0304143%2Cquant-ph%2F0304200%2Cquant-ph%2F0304007%2Cquant-ph%2F0304204%2Cquant-ph%2F0304175%2Cquant-ph%2F0304176%2Cquant-ph%2F0304154%2Cquant-ph%2F0304001%2Cquant-ph%2F0304148%2Cquant-ph%2F0304133%2Cquant-ph%2F0304071%2Cquant-ph%2F0304003%2Cquant-ph%2F0304078%2Cquant-ph%2F0304193%2Cquant-ph%2F0304141%2Cquant-ph%2F0304061%2Cquant-ph%2F0304080%2Cquant-ph%2F0304072%2Cquant-ph%2F0304202%2Cquant-ph%2F0304109%2Cquant-ph%2F0304102%2Cquant-ph%2F0304131%2Cquant-ph%2F0304110%2Cquant-ph%2F0304035%2Cquant-ph%2F0304113%2Cquant-ph%2F0304029%2Cquant-ph%2F0304049%2Cquant-ph%2F0304040%2Cquant-ph%2F0304201%2Cquant-ph%2F0304034%2Cquant-ph%2F0304178%2Cquant-ph%2F0304136%2Cquant-ph%2F0304105%2Cquant-ph%2F0304203%2Cquant-ph%2F0304206%2Cquant-ph%2F0304160%2Cquant-ph%2F0304056%2Cquant-ph%2F0304153%2Cquant-ph%2F0304114%2Cquant-ph%2F0304149%2Cquant-ph%2F0304031%2Cquant-ph%2F0304156%2Cquant-ph%2F0304002%2Cquant-ph%2F0409010%2Cquant-ph%2F0409111%2Cquant-ph%2F0409109%2Cquant-ph%2F0409082%2Cquant-ph%2F0409013%2Cquant-ph%2F0409162%2Cquant-ph%2F0409158%2Cquant-ph%2F0409074%2Cquant-ph%2F0409105%2Cquant-ph%2F0409202%2Cquant-ph%2F0409066%2Cquant-ph%2F0409014%2Cquant-ph%2F0409150%2Cquant-ph%2F0409019%2Cquant-ph%2F0409199&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The information provided by a classical measurement is unambiguously\ndetermined by the mutual information between the output results and the\nmeasured quantity. However, quantum mechanically there are at least two notions\nof information gathering which can be considered, one characterizing the\ninformation provided about the initial preparation, useful in communication,\nand the other characterizing the information about the final state, useful in\nstate-preparation and control. Here we are interested in understanding the\nproperties of these measures, and the information gathering capacities of\nquantum and classical measurements. We provide a partial answer to the question\n`in what sense does information gain increase with initial uncertainty?' by\nshowing that, for classical and quantum measurements which are symmetric with\nrespect to reversible transformations of the state space, the information gain\nregarding the initial state always increases with the observer's initial\nuncertainty. In addition, we calculate the capacity of all unitarily covariant\nand commutative permutation-symmetric measurements for obtaining classical\ninformation. While it is the von Neumann entropy of the effects which appears\nin the latter capacity, it is the subentropy which appears in the expression\nfor the former."}, "authors": ["Kurt Jacobs"], "author_detail": {"name": "Kurt Jacobs"}, "author": "Kurt Jacobs", "arxiv_comment": "13 pages, revtex4", "links": [{"href": "http://arxiv.org/abs/quant-ph/0304200v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/quant-ph/0304200v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/quant-ph/0304200v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/quant-ph/0304200v1", "journal_reference": null, "doi": null, "fulltext": "On the properties of information gathering in quantum and classical measurements\nKurt Jacobs\n\narXiv:quant-ph/0304200v1 30 Apr 2003\n\nCentre for Quantum Computer Technology, Centre for Quantum Dynamics,\nSchool of Science, Griffith University, Nathan 4111, Brisbane, Australia\nThe information provided by a classical measurement is unambiguously determined by the mutual\ninformation between the output results and the measured quantity. However, quantum mechanically\nthere are at least two notions of information gathering which can be considered, one characterizing the information provided about the initial preparation, useful in communication, and the other\ncharacterizing the information about the final state, useful in state-preparation and control. Here\nwe are interested in understanding the properties of these measures, and the information gathering\ncapacities of quantum and classical measurements. We provide a partial answer to the question 'in\nwhat sense does information gain increase with initial uncertainty?' by showing that, for classical\nand quantum measurements which are symmetric with respect to reversible transformations of the\nstate space, the information gain regarding the initial state always increases with the observer's\ninitial uncertainty. In addition, we calculate the capacity of all unitarily covariant and commutative\npermutation-symmetric measurements for obtaining classical information. While it is the von Neumann entropy of the effects which appears in the latter capacity, it is the subentropy which appears\nin the expression for the former.\nPACS numbers: 03.67.-a,03.65.Ta,89.70.+c,02.50.Tt\n\nI.\n\nINTRODUCTION\n\nIn this article we will be concerned with, among other\nthings, the question 'what is the capacity of a given measurement to provide information?' In considering this\nquestion, it is important to note that for quantum measurements the answer is not unique, and depends on what\nthe information is about. Thus, we will also be concerned\nwith the question of how information gain can be quantified, and in doing so we will present two kinds of information gain useful for different applications. Finally,\nfor a given measure of the information gain, we will be\ninterested in asking what properties it has, in particular\nhow it depends upon the observer's initial uncertainty.\nSince we will be concerned with both classical and\nquantum measurements, we begin in Section II by discussing the relationship between the two, and in particular how classical measurements may be described as a\nsubset of quantum measurements, providing us with a\nunified description of both. In Section III we describe\ntwo physically motivated operations which can be used\nto combine measurements, and introduce some notation.\nIn Section IV we discuss two kinds of information gathering, information about the initial ensemble, and information about the final state, and consider some questions regarding their respective properties under operations on measurements. In Section V we pause to consider quantifying disturbance, and discuss how one can\ndefine two measures of disturbance corresponding loosely\nto the two notions of information gain considered in the\nSection IV. In Section VI we consider the information\ngain regarding the initial preparation, and show that\nthere is a precise sense in which it can be said to increase with the observer's initial uncertainty. This motivates the definition of two classes of measurements, the\npermutation-symmetric measurements and the unitarily\n\ncovariant measurements. Finally, we calculate the capacity of these classes to obtain information about the initial\npreparation.\nII.\n\nCLASSICAL AND QUANTUM\nMEASUREMENTS\n\nThe purpose of this section is to describe the relationship between classical and quantum measurements, and\ndiscuss the special role of bare quantum measurements,\nthe class with which we will be primarily concerned\nthroughout. Quantum measurements [1, 2]Pare described\nby sets of operators, {\u03a9n }, which satisfy n \u03a9\u2020n \u03a9n = 1.\nFor efficient measurements each operator in the set corresponds to a possible outcome of the measurement [3].\n(Inefficient measurements, on the other hand, are merely\nefficient measurements in which the result is not known\nwith certainty - however, we will only be concerned with\nefficient measurements in what follows). Due to the polar decomposition theorem [4], we can write each of the\noperators \u03a9n as a product of a unitary and a positive\noperator. As a result, the state following a measurement\nmay be written as\n\u03c1n =\n\nUn Qn \u03c1Qn Un\u2020\n,\nTr[Q2n \u03c1]\n\n(1)\n\nwhere Un is a unitary, and Qn is positive. So long as\nthe observer has the necessary Hamiltonian resources,\nshe can always perform any unitary operation after the\nmeasurement, and conditional upon the result. Such an\naction is often referred to as feedback [5, 6, 7]. Because of\nthis, we can conclude two things from the above expression for the final state resulting from the outcome n. The\nfirst is that, using unitary feedback, one can simulate any\nquantum measurement by making a measurement whose\n\n\f2\noperators \u03a9n are purely positive. The second is the converse; that, given any measurement, one can always use\nunitary feedback to simulate a measurement in which the\noperators are purely positive. However, there is a deeper\nsignificance to the polar form given in Eq.(1), but to see\nthis we must turn to classical measurements.\nClassical measurement theory, which existed, naturally, long before quantum theory, describes measurements performed on classical objects and derives directly\nfrom Bayesian statistical inference. [8, 9, 10]. While\nclassical measurements are not usually written using the\nsame formalism as quantum measurements, they can be,\nand this is to be expected because quantum measurements must reduce to classical measurements under certain conditions; quantum measurement theory reduces\nto the theory of Bayesian statistical inference when both\nthe density matrix and the measurement operators are\nall diagonal in the same basis.\nTo describe classical measurements with the same formalism as quantum measurements, one writes the observer's initial classical state of knowledge (being a probability density) about a classical variable x, as a positive\ndiagonal matrix, \u03c1c . Once one has done this, a classical\nmeasurement may be described\nP by a set of positive diagonal operators Pn , such that n Pn2 = 1. The state that\nresults from the nth outcome is\nPn \u03c1Pn\n\u03c1n =\n.\nTr[Pn2 \u03c1]\n\n(2)\n\nFor an explanation of how this form is derived from the\ntheory of Bayesian inference, the reader is referred to\nRef. [11]. Thus, classical measurements are described by\na subset of quantum POVM's, in which all the operators\nmust be positive, mutually commuting and commuting\nwith the 'density operator' describing the initial classical\nstate of knowledge of the observer.\nHowever, it is useful to note that we can add to the\nclassical formalism deterministic transformations. In the\nclassical case, the only deterministic operations which\ncan be performed are permutations of the states of the\nrandom variable. In our formalism, these are implemented by multiplying the classical (diagonal) density\nmatrix on the left and right by permutation matrices.\n(Permutation matrices are such that every element is zero\nor unity, and there is only one non-zero element in each\nrow and column.) If we allow the observer to perform a\ndeterministic transformation after the measurement, and\nconditional upon the outcome, then we have\n\u03c1n =\n\nTn Pn \u03c1Pn Tn\u2020\n.\nTr[Pn2 \u03c1]\n\n(3)\n\nWhen we augment classical measurement with classical\ndeterministic transformations, one could refer to the result as a classical operation. While Eq.(3) is sufficient\nfor our purposes, it does not give the most general form\nfor a classical operation. To do this one would include,\nin addition to the above, a stochastic map describing an\n\nirreversible randomization of the system. We note that a\ndiscussion of the relationship between quantum and classical measurements, using a different formalism, is given\nin [12].\nThere is now a close similarity between the above expression for classical operations, and the expression for\nquantum operations given in Eq.(1). In fact, this similarity is not just superficial; there is a sense in which\nwe can think of the quantum positive operators Qn as\ncharacterizing the unadorned act of measurement, as the\nPn do in classical operations. To see this we consider the\nvon Neumann entropy of the average state of the system after the measurement. Classically, if we allow only\nmeasurement, we have\nX\nn\n\np n \u03c1n =\n\nX\n\nPn \u03c1Pn =\n\nn\n\nX\n\nPn2 \u03c1 = \u03c1,\n\n(4)\n\nn\n\nso that the entropy of the final average state is the same\nas the entropy of the initial state. Denoting the difference\nbetween the initial and final entropy as\n\"\n#\nX\nDo (\u03c1) \u2261 S\n(5)\npn \u03c1n \u2212 S[\u03c1],\nn\n\n(a notation which will be motivated later) then for classical measurements we have Do = 0. However, if we allow conditional deterministic transformations (feedback),\nthen it becomes possible to arrange that Do < 0. That\nis, feedback allows us to reduce the final average entropy\nbelow the initial entropy, which is the essence of feedback\ncontrol.\nA similar result exists for quantum measurements,\nwhich is due to a theorem of Ando [13]. Ando's theorem\nstates that for quantum measurements in which all the\noperators are positive (\u03a9n = Qn , \u2200n), Do (\u03c1) \u2265 0. In addition, when one allows the use of deterministic feedback\n(unitary operators), as in the classical case one can arrange that Do (\u03c1) < 0. This therefore provides an initial\nquantitative motivation for thinking of the positive operators which appear in the polar decomposition for quantum operations as the equivalent of the measurement operators in classical operations. Moreover, we expect that\nother relations exist in which quantum operations with\npositive operators parallel classical measurements, and\nwe give a conjecture regarding a second example of such\na relation in Section IV.\nIn view of the above argument we will refer to quantum\nmeasurements in which all the measurement operators\nare positive as bare measurements. An important feature\nof bare measurements is that, due to the Polar decomposition theorem, if one is interested purely in the process of\nacquiring information, it is enough to study these alone,\nsince post-measurement unitary transformations do not\naffect this process. We note that such measurements have\nbeen referred to previously by various authors as, pure\nmeasurements [14], measurements without feedback [15],\nand square root measurements [16].\n\n\f3\nIII.\n\nOPERATIONS ON MEASUREMENTS\n\nIt is useful at this point to introduce a few definitions.\nIn what follows we will denote measurements by calligraphic letters. If we write,\nM \u2261 {\u03a9n : n = 1, . . . , M }\n\n(6)\n\nwe will mean that M denotes the measurement described\nby the operators {\u03a9n }. The operators {\u03a9n } are often\nreferred to as the Kraus operators [2], although in what\nfollows we will refer to them simply as the measurement\noperators. We will follow Kraus's terminology and refer\nto the operators En = \u03a9\u2020n \u03a9n as the effects.\nThere are at least two ways in which a physical procedure may be used to combine two measurements together\nto form a new measurement. One we will refer to as mixing [17], and the other as concatenation.\nMixing: In this case the observer chooses between two\nmeasurements, M \u2261 {\u03a9M\nm : m = 1, . . . , M } and N =\n{\u03a9N\nn : n = 1, . . . , N } on a random basis, choosing to use\nM with probability p, and the other with probability\n1 \u2212 p. The resulting measurement is given by\np\n\u221a\nN\n(7)\nQ = { p\u03a9M\nm } \u222a { 1 \u2212 p\u03a9n }\n\u2261 pM + (1 \u2212 p)N .\n(8)\nThis is a kind of 'addition' operation for measurements.\nConcatenation: In this case the observer performs two\nmeasurements, one after the other. If M is made first and\nN second, then this is equivalent to making the single\nmeasurement Q, where\nM\nQ = {\u03a9N\n(9)\nn \u03a9m : n = 1, . . . , N ; m = 1, . . . , M }\n\u2261 M \u25e6 N.\n(10)\n\nOne might think of this as a kind of 'multiplication' operation for measurements.\nIV.\n\nTWO INFORMATION-THEORETIC\nCAPACITIES\n\nWe are interested here in quantifying the amount of information which a measurement provides about the system being measured. In classical terms one might think\nof concepts such as the 'accuracy' of a measurement, or\nthe 'resolving power'. It is worth noting that, in general, the amount of information that the measurement\nprovides depends, in a way we will make precise later,\non the state of the system being measured. However, in\nwhat follows we will be specifically interested in quantities which depend only on the measurement itself. Such\nquantities can be obtained by optimizing over the measured state, or simply by fixing it, if a natural choice\nexists.\nThe first thing to consider when speaking about the\ninformation obtained in a measurement is what this information is about. For quantum systems, there are two\n\nchoices, since one can consider the information provided\nabout the initial preparation, or the information provided about the state which results from measurement.\nIn classical measurements there is only one choice, because these two things are the same.\nOne kind of information, that about the initial preparation, is useful in communicating classical information.\nIf Alice encodes a message in a quantum system, then the\nability of a measurement to extract information about\nthe initial preparation tells us how much classical information Bob can obtain from Alice by employing the measurement. Thus, a measure of this kind of information is\na classical capacity.\nAlternatively, one can ask how much information one\nobtains about the final state - that is, the state that we\nare left with once we have made the measurement. How\nmuch one knows about a quantum state can be characterized by the von Neumann entropy of this state. Thus, a\nmeasure of the amount of information which the measurement extracts about a state can be obtained by taking\nthe average decrease in the von Neumann Entropy resulting from the measurement. This motivates the definition\nof the purification capacity (and the related concept of\nthe measurement strength [14, 15]). We now deal with\neach of these measures of information in turn.\n\nA.\n\nClassical Capacity\n\nThere is now a considerable body of work on the capacity of quantum channels for transmitting classical information (see, e.g. [18, 19, 20, 21, 22, 23, 24, 25]). Here,\nhowever, we are interested in examining the capacity of\na measurement to obtain classical information encoded\nin a quantum state. In defining the capacity of a channel, one must maximize over all measurement strategies\nand encodings, in order to find the amount of information\nthat can be communicated by the channel. To obtain the\ncapacity of a measurement, instead one assumes a perfect channel, fixes the measurement and optimizes over\nall possible encodings, thus providing a quantity which\ncharacterizes the measurement rather than the channel.\nIn contrast to classical channels [26], in considering\nthe classical capacity for quantum channels, there is a\ndistinction between the 'single shot' capacity and the\nasymptotic capacity. The single shot capacity tells us\nhow much classical information can be transmitted by a\nsingle use of the channel. If the channel is a single qubit\nbeing transmitted from sender to receiver, for example,\nthen the single shot capacity tells us how much information can be obtained by measuring just the one qubit.\nThe asymptotic capacity is the amount of information\nthat can be transmitted, per qubit, if we are allowed\nto measure multiple qubits, in the limit of an infinity\nnumber of qubits. In general, the asymptotic capacity is\ngreater than the single shot capacity [27, 28, 29]. However, King and Ruskai [30] have recently shown that this\nis only true if joint measurements are made across multi-\n\n\f4\nple uses of the channel. Thus, as a property of a measurement on a single system, the notion of classical capacity is\nwell defined without reference to the number of systems\nbeing measured.\nThe amount of classical information that is provided\nby a measurement, M, about an encoding, \u03b5, and which\nwe will denote by \u2206Ii (M, \u03b5), is the mutual information\nbetween the encoding and the output results. An encoding is the set of states chosen as the alphabet with which\nto write the information, along with the probability with\nwhich each state will be selected. We will denote an element of the set of all possible encodings by \u03b5, and the\nset of states and probabilities corresponding to this encoding as {\u03c1i : i = 1, . . . n}, and {P (i) : i = 1, . . . n},\nrespectively. The state of the prepared system from the\npointP\nof view of the observer making the measurement is\n\u03c1 = i P (i)\u03c1i . With these definitions, the information\nprovided by the measurement is given by\nX\nQ(j)H[P (i|j)],\n(11)\n\u2206Ii (M, \u03b5) = H[P (i)] \u2212\nj\n\nwhere\nQ(j) = Tr[\u03a9\u2020j \u03a9j \u03c1]\n\n(12)\n\nis the a priori probability density of the output results,\nTr[\u03a9\u2020j \u03a9j \u03c1i ]P (i)\nQ(j|i)P (i)\nP (i|j) = P\n= P\n\u2020\ni Q(j|i)P (i)\ni Tr[\u03a9j \u03a9j \u03c1i ]P (i)\n\n(13)\n\nM (i|j, k) = M (i|j) + M (i|k) \u2212 M (j|k).\n\n(17)\n\nThe point here is that, while two measurements may be\nindependent, the outcomes of the measurements, when\nmade in sequence on the same system, are not, in general,\nuncorrelated. Since M (j|k) \u2265 0, we have M (i|j, k) \u2264\nM (i|j) + M (i|k). Since the classical capacity of M and\nN are the respective supremums of M (i|j) and M (i|k)\nover initial encodings, we have\nC(M \u25e6 N ) = sup M (i|j, k)\n\u03b5\n\n\u2264 sup[M (i|j) + M (i|k)]\n\u03b5\u2032\n\nis the observer's state of knowledge of the initial preparation on receiving the outcome j, and H is the entropy.\nThe quantity Q(j|i) introduced above is the probability\ndensity of the output results given that the initial state\nis \u03c1i . We will continue to use this notation throughout;\nthat is, we will denote the probability densities of the\ninitial states by P , and the densities of outcomes by Q.\nNote that the mutual information is simply the average\nreduction in the entropy of the distribution of the initial\npreparation, P (i), due to the measurement. It is useful\nto note that the mutual information can also be written\nin the reverse form with input i and output j swapped.\nThat is\nX\nP (i)H[Q(j|i)].\n(14)\n\u2206Ii (M, \u03b5) = H[Q(j)] \u2212\ni\n\nNaturally the classical capacity of a measurement should\nbe defined as the supremum of \u2206Ii (M, \u03b5) over all initial\nencoding, and is therefore given by\nC(M) = sup \u2206Ii (M, \u03b5).\n\nThis is because, since the observer chooses between measurements M and N on a random basis, the preparer cannot know which measurement will be used, and therefore\nmust use the same encoding irrespective of the measurement made. If the optimal encoding for both measurements is the same, then we have equality. Otherwise one\nmeasurement necessarily extracts less information than\nis optimal, hence the inequality.\nThe behavior under concatenation is straightforward\nto obtain for classical measurements. If we denote the\nmutual information between two sets of random variables\nX and Y as M (X, Y ), then the capacity of the concatenated measurement is the supremum of M (i|j, k) over\ninitial encodings, where i labels the elements of the encoding, and j and k label the outcomes for the respective\nmeasurements M and N . Rearranging the expression for\nM (i|j, k), one finds that\n\n(15)\n\n\u2264 sup M (i|j) + sup M (i|k)]\n\u03b5\u2032\n\n\u03b5\u2032\u2032\n\n= C(M) + C(N ).\n\n(18)\n\nThe first inequality is only saturated when M (j|k) = 0\nand the second is only saturated when the optimal encoding is the same for both measurements.\nFor quantum measurements at first it is tempting to\nsuggest that C might also be subadditive. However, if we\nallow all classes of measurements, in particular measurements in which the operators \u03a9n are not purely positive,\nbut have a unitary component, then it is not hard to find\nexamples which break the inequality. That is,\n\u2203 M, N , C(M \u25e6 N ) > C(M) + C(N ).\n\n(19)\n\nNevertheless, it is important to point out that the same\ncan be said for the mutual information for classical operations if we allow conditional deterministic transformations. As a result we are prepared to conjecture that for\nbare measurements the purification capacity satisfies the\nclassical relation, Eq.(18).\n\n\u03b5\n\nIt is natural now to ask how C behaves under the operations of mixing and concatenation. For mixing it is\nclear that\nC(pM + (1 \u2212 p)N ) \u2264 pC(M) + (1 \u2212 p)C(N ).\n\n(16)\n\nB.\n\nPurification Capacity\n\nWhile the classical capacity was motivated by considering the transmission of classical information, the purification capacity is motivated by considering the problem\n\n\f5\nof state preparation and more generally quantum feedback control [14, 15]. In this case, instead of being interested in the amount of information one obtains about\nthe initial preparation, one is interested in how well one\nknows the final state that results from the measurement.\nThis is because the more pure the final state, the more\none can subsequently control the system. That is, one\nis interested in the amount by which, on average, the\nmeasurement purifies the state of the system.\nFor the purposes of control, the von Neumann entropy\nis a sensible measure of the observer's uncertainty of a\nstate, since it measures the minimum possible entropy\nof the outcomes of a future measurement. That is, if the\nobserver has the ability to perform any measurement, the\nvon Neumann entropy measures the unavoidable residual\nunpredictability of the measurement result. The outcome\nof a measurement on a state with zero von Neumann\nentropy can, in the best case, be predicted perfectly.\nTo characterize the amount of information that a measurement provides about the final state one can therefore\naverage the von Neumann entropies of all the possible final states over the measurement outcomes. Subtracting\nthis from the von Neumann entropy of the initial state\ngives us a measure of the increase in the observer's information about the state of the system provided by the\nmeasurement:\nX\nP (i)S(\u03c1i ),\n(20)\n\u2206If (M, \u03c1) = S(\u03c1) \u2212\ni\n\nwhere M is the measurement and \u03c1 is the initial state.\nNielsen has shown that, as one would intuitively expect,\n\u2206If is non-negative [31]. (See also [15] for a remarkably\nsimple proof of the this result discovered by Fuchs.)\nIn order to obtain a quantity which depends only on\nthe measurement, two possibilities are to fix the initial\nstate, or to maximize \u2206If over all possible initial states.\nIf we choose the second option, then we have a quantity\nwhich is the final-state equivalent of the classical capacity. We will refer to this as the purification capacity, and\ndenote it by K. Thus we define\nK(M) = sup \u2206If (M, \u03c1).\n\n(21)\n\n\u03c1\n\nIf instead we wish to fix the initial state, which was\nthe procedure adopted in [14], then it is important to\nchoose a state which is invariant under unitary transformations, so that the resulting quantity, which was referred to in [14] as the measurement strength, satisfies the\nintuitive notion that it should be invariant under unitary\ntransformations of the measurement. In Refs. [14, 15]\nthe measurement strength, which we will denote by KI ,\nwas defined by fixing the initial state to be the maximal\nuncertainty state \u03c1 = I/N . This gives\nX\nKI (M) = ln(N ) \u2212\nPn S[E \u0303n ],\n(22)\nn\n\nwhere E \u0303n = En /Tr[En ], and we have used the fact that\nthe eigenvalues of \u03a9n \u03a9\u2020n are the same as those of En [13,\n32]. By the definition of K we have KI (M) \u2264 K(M).\n\nIt is now natural to ask how the purification capacity\nand the measurement strength behave under the operations of mixing and concatenation. Under mixing one\nhas trivially that KI is linear, ie.\nKI (pM + (1 \u2212 p)N ) = pKI (M) + (1 \u2212 p)KI (N ), (23)\nand that K is concave:\nK(pM + (1 \u2212 p)N ) \u2264 pKI (M) + (1 \u2212 p)KI (N ). (24)\nWhile it is not immediately obvious, for bare measurements KI is invariant under a change in the order in\nwhich measurements are concatenated. That is,\nKI (M \u25e6 N ) = KI (N \u25e6 M) , M, N \u2208 B,\n\n(25)\n\nwhere B denotes the set of all bare measurements. This\nfollows almost immediately from the fact that, for any\ntwo operators A and B, AB has the same eigenvalues as\nBA [13, 32].\nFor general quantum measurements, it is not hard to\nshow that neither the measurement strength nor the purification capacity satisfy the classical relation, given by\nEq.(18) (Note that for classical measurements K = C.)\nHowever, we conjecture that for bare measurements both\nthe strength and purification capacity satisfy the classical\nrelation, e.g.\nK(M \u25e6 N ) \u2264 K(M) + K(N ) , M, N \u2208 B.\nV.\n\n(26)\n\nTWO MEASURES OF DISTURBANCE\n\nAlthough disturbance is not the primary focus of this\narticle, we feel that it is worth pointing out here that,\ncorresponding to the two measures of information discussed in the previous section, there are two natural\ninformation-theoretic measures of the disturbance caused\nby a measurement. The first, which we might refer to as\ndisturbance to the input, measures the amount by which\nclassical information encoded in a quantum state is degraded by a measurement. That is, it tells how much\nless classical information we can extract from the system about the initial preparation after having made the\nmeasurement. For this purpose we will assume that the\nobserver who wishes to extract the information does not\nhave access to the results of the disturbing measurement.\nThe reason for using this definition is that it results in\na sharp contrast between classical and quantum measurements; for classical measurements this disturbance\nis zero, but can be non-zero for quantum measurements.\nThe second kind of disturbance, which we might refer to as disturbance to the output, is the difference between the entropy of the initial state, and the entropy\nof the state given by averaging all the final states. This\nmeasures the amount of noise that the measurement is\nfeeding into the system. This is understood most easily\nby thinking of a noise-driven classical system. The random changes that the quantum system experiences as a\n\n\f6\nresult of the random outcomes of the measurement are\nlike the random kicks experienced by a classical system\nunder the influence of a noisy force. The strength of this\nnoise can be characterized by the entropy increase of the\nprobability distribution for the state of the system given\nthat the observer has no knowledge of which kick will occur. This quantum measure of disturbance is simply the\nequivalent calculation for a quantum system. This kind\nof disturbance is relevant for quantum feedback control,\nand was considered in [14, 15]. The disturbance caused\nby a classical measurement under this definition is zero,\nwhereas, for bare quantum measurements, it is greater\nthan or equal to zero [13]. Thus, both measures provide\na precise concept of the general notion that quantum\nmeasurements can produce a disturbance [33], whereas\nclassical measurements do not.\nNote that the definitions of disturbance which have\nbeen most studied in the literature to date, for example\nin the work by Fuchs [34], Banaszek [35] and Barnum [16],\ncharacterize disturbance to the input. It is therefore our\nfirst measure which is most closely related to these notions of disturbance.\n\nA.\n\nDisturbance to the Input\n\nGiven an encoding, \u03b5, the optimal amount of information which an observer Alice can obtain about a message\nencoded by Bob using that encoding is called the accessible information [36]. If a third observer, Eve, makes\na measurement, M, on the system in which the information is encoded, and does not relay the result of the\nmeasurement to the Alice, then from the point of view of\nthe first observer the state of the system is transformed\nby\nX\n\u03c1 \u2192 \u03c1\u2032 =\n\u03a9n \u03c1\u03a9\u2020n ,\n(27)\nn\n\nwhere the \u03a9n are the measurement operators for M. As\na result, the encoding is transformed from \u03b5 to \u03b5\u2032 where\nX\n\u03b5\u2032 = {pi ,\n\u03a9n \u03c1i \u03a9\u2020n : i = 1, * * * , N }.\n(28)\nn\n\nThe disturbance caused by the measurement M to the\nensemble \u03b5 can then be characterized by the difference\nbetween the accessible information of \u03b5 and that of \u03b5\u2032 ,\nwhich we can write as\nDi (\u03b5) = Iaccess (\u03b5) \u2212 Iaccess (\u03b5\u2032 ).\n\n(29)\n\nTo obtain a measure of disturbance which characterizes\nthe measurement alone, we can do one of at least two\nthings. The first is simply to choose \u03b5 to the be the\noptimal encoding for a perfect channel, which, for an N dimensional system, is an N - dimensional orthonormal\nbasis. In this case the first term is simply the capacity of\nthe perfect N -dimensional quantum channel. The second\n\nterm, however, is not the maximal amount of information\nwhich can be transmitted from Alice to Bob given that\nEve makes the measurement M, since the optimal encoding \u03b5 for the perfect channel, may not be the optimal\nencoding for the channel described by the measurement\noperators \u03a9n . Thus, a second approach to characterizing\nthe disturbance of the measurement is to choose the second term in the expression for Di (\u03b5) to be the accessible\ninformation when \u03b5\u2032 is such as to make this optimal. The\nresult is simply the single shot capacity of the channel described by the measurement operators for M. Thus, the\ninput disturbance is essentially a channel capacity. If we\nwrite the capacity of the perfect channel as Ccap (I), and\nthat of the channel described by M as Ccap (M), then\nthe input disturbance of M is\nDi = Ccap (I) \u2212 Ccap (M).\n\n(30)\n\nFor classical measurements on classical systems Di = 0.\nFor quantum measurements Di \u2265 0. To see that Di is\nnon-negative one merely needs to note that the accessible information is a quantity which is maximized over all\nmeasurements. As a result, the measurement M made\nby Eve can be included as part of Alice's possible strategies for obtaining information in the case of the perfect\nchannel. Thus, the case in which Eve makes her measurement is just a special case of the perfect channel in\nwhich Alice, as part of her strategy, first makes Eve's\nmeasurement and then throws away the information before making any further measurements. Therefore, the\ncapacity of the channel described by M cannot be more\nthan that of the perfect channel, and hence the disturbance Di is a non-negative quantity.\n\nB.\n\nDisturbance to the Output\n\nThe above measure of disturbance is concerned with\nthe reduction in the ability of an observer to obtain information about an initial preparation, given that another\nobserver has made a measurement on the system to which\nthe first observer has no access. Now we consider the reduction in an observer's knowledge of the final state of\nthe system, given that a second observer has made a measurement to which the first has no access. If the initial\nstate is \u03c1, then the final state of the system after a measurement M, from the point of view of the first observer,\nis\nX\n\u03c1\u2032 =\n\u03a9n \u03c1\u03a9\u2020n .\n(31)\nn\n\nThe reduction in the observer's knowledge of the state of\nthe system, \u03c1, caused by M, may be characterized by the\nincrease in the von Neumann entropy of \u03c1\u2032 over \u03c1. That\nis,\nDo (\u03c1) = S[\u03c1\u2032 ] \u2212 S[\u03c1].\n\n(32)\n\n\f7\nThis quantity is zero for classical measurements, and\nAndo has shown that it is non-negative for all bare measurements [13].\nNo doubt there is more than one way to use the above\nmeasure to obtain a quantity which characterizes the\nmeasurement alone, and it may well be that different\nchoices may be motivated by different applications. Nevertheless, one reasonable procedure is to minimize Do (\u03c1)\nover all initial states. That is, to define the output disturbance of a measurement as\n\"\n#\nX\n\u2020\nDo \u2261 inf S\n\u03a9n \u03c1\u03a9n \u2212 S[\u03c1].\n(33)\n\u03c1\n\nn\n\nThis measures the least disturbance that can be achieved\nwith the given measurement if one is free to select the\ninitial state. Under this definition one has the desirable\nresult that all commutative measurements have Do = 0.\nVI.\n\nINFORMATION GATHERING AND\nSTATE-SPACE SYMMETRY\nA.\n\nClassical Measurements\n\nSince classical measurements have already been introduced in Section II, we will merely pause to clarify our\ndefinitions: By a classical measurement, we will mean a\nmeasurement which can be performed on a classical system. Thus a classical measurement is not only one in\nwhich all the measurement operators commute, but also\none in which is it understood that the density operator\nis also required to be diagonal in the same basis as these\noperators. We will refer to measurements in which the\nonly restriction is that the operators commute as commutative quantum measurements. The reason for this\ndistinction is that it is possible to enhance the properties of measurements with unitary transformations [37].\nSince such transformations require transforming the density matrix so that it is diagonal in other bases, one must\nmake a distinction between classical measurements and\ncommutative measurements; the latter can be enhanced\nin ways in which the former cannot.\nFor classical measurements, there is no difference between the information about the initial encoding, and\nthat about the state which exists after the measurement,\nbecause the measurement does not interfere with the initial preparation; both \u2206Ii (M, \u03b5) and \u2206If (M, \u03c1) reduce\nto the classical mutual information, which we will write\nsimply as \u2206I(M, \u03c1). Thus K = C, and the measurement\nstrength KI is the mutual information between the output results and the initial preparation when the encoding\nis the uniform ensemble over the all the available initial\nstates.\nNow, the mutual information is the average decrease in\nthe Shannon entropy of the observer's state of knowledge\nas a result of the measurement, and depends, in general\nupon not only the measurement but also the initial state.\n\nHow exactly, does it depend on the initial state? Since\nthe entropy of the state cannot decrease below zero, one\nmight suggest that the information gain in the measurement decreases as the uncertainty in the observer's initial\nstate of knowledge decreases: the more you know the less\nyou find out. However, from the fact that the mutual information is not always maximized when the ensemble\nhas maximal entropy, we know that it is possible for the\nreverse to be true for at least some measurements and\nsome states. So is there a sense in which a reduction in\ninformation gain with increasing knowledge is a fundamental property of measurement?\nTo answer this question, we consider a class of measurements which we will refer to as permutation-symmetric\nmeasurements. We define these measurements as those\nwhich are symmetric under the permutation of two classical basis states. That is, when any two states |ii and\n|ji are swapped, the set of measurement operators remains unchanged - the various measurement operators\nmerely interchange among themselves. The motivation\nfor such a definition is the observation that the information extraction capability (or accuracy, or resolution) of a\nmeasurement can, in general, vary across the state space,\nand it is this that causes some measurements to provide\na larger average amount of information when applied to\nstates with less that maximal entropy. The definition of\npermutation-symmetric measurements ensures that they\nhave the same resolving power over all of state space.\nWe now show that these measurements have the property which we seek. First it is helpful to introduce some\nnotation. We will denote the vector of eigenvalues of a\nmatrix A by \u03bb(A). If we write \u03c1 \u227a \u03c3 then we will mean\nthat \u03bb(\u03c1) \u227a \u03bb(\u03c3) [38].\nTheorem:\nIf M is classical and permutationsymmetric, then for any two classical states \u03c1 and \u03c3,\n\u03c1 \u227a \u03c3 implies that \u2206I(M, \u03c1) > \u2206I(M, \u03c3). (Note that\na more concise way of saying this is that \u2206I(M, \u03c1) is\nSchur-concave in \u03c1 [32]).\nProof: To begin we note that given a diagonal operator \u03a9 of dimension N , the sum of the N ! operators,\n{\u03a9m } which are the different permutations of \u03a9, is proportional to the identity. This means that the operators\n{\u03b1\u03a9m } form a valid measurement for some \u03b1. In what\nfollows when we refer to an operator \u03a9 which is used to\ngenerate a permutation-symmetric measurement by taking all its permutations, we will always define\n\u03a9 so that\np\nTr[E] = Tr[\u03a92 ] = 1, in which case \u03b1 = 1/ (N \u2212 1)!. We\nwill refer to a measurement generated by a single operator \u03a9 as an irreducible permutation-symmetric measurement (IPM). Clearly all permutation-symmetric measurements can be written as mixtures (in the sense of\nthe mixing operation described in section III) of IPM's.\nNow consider the mutual information for an IPM. This\nmay be written as\nX\n\u2206I(M, \u03c1) = H[Q\u03c1 (m)] \u2212\nP\u03c1 (n)H(Q\u03c1 (m|n)), (34)\nn\n\nwhere Q\u03c1 (m) is the distribution of the measurement out-\n\n\f8\ncomes, P\u03c1 (n) is the distribution of the initial states (being the diagonal of \u03c1), and Q\u03c1 (m|n) is the distribution\nof measurement outcomes given that the initial state is\nn. The first thing to note is that due to the permutation\nsymmetry, H(Q\u03c1 (m|n)) is the same for all m. Thus, the\nsecond term is independent of the initial state \u03c1, and we\nneed merely show that\nH[Q\u03c1 (n)] > H[Q\u03c3 (n)] when \u03c1 \u227a \u03c3.\n\n(35)\n\nFor this it is sufficient to show that Q\u03c1 \u227a Q\u03c3 if \u03c1 \u227a\n\u03c3. That is, that the operation that transforms \u03c1 to Q\u03c1\npreserves majorization. The operation that transforms \u03c1\nto Q\u03c1 may be written as\nQ\u03c1 = A\u03c1v ,\n\n(36)\n\nwhere \u03c1v is the vector consisting of the diagonal of \u03c1,\nand A is a matrix whose rows are the diagonals of the\noperators \u03a9m \u03a9\u2020m . The rows of A therefore consist of all\nthe permutations of any given row. At this point we\ncan invoke a theorem of Chong [32, 39], who has shown\nthat the operation of multiplication by a matrix preserves\nmajorization if the matrix is such that the rows form a\npermutation invariant set. That is, if all permutations\nof any row of the matrix are also rows of the matrix.\nWe will refer to matrices of this form in what follows as\nChong matrices. This proves the result for IPM's. Since\nevery permutation-symmetric measurement is a mixture\nof IPM's, and \u2206I is linear under mixing, the result holds\nfor all permutation-symmetric measurements. \u0003\nThat is, once we have eliminated the state-space dependence of the resolution of measurements, a fundamental property remains. This is that, the more one knows,\nthe less is the amount that one will learn by applying a\ngiven measurement.\nAs a corollary of this, we have that the classical capacity of a classical irreducible permutation-symmetric measurement is attained by the uniform distribution, and is\ntherefore given by\nC = ln N \u2212 S(E),\n\n(37)\n\nwhere S is the von Neumann entropy, and E \u2261 \u03a92 .\nIn addition, it is worth noting that since both classical channels and classical measurements are defined by\na complete set of conditional probabilities, classical measurements and classical channels are one and the same\nthing. Thus, Eq.(37) also gives the capacity of irreducible\npermutation-symmetric classical channels. The capacity\nof a general permutation-symmetric measurement\nM=\n\nN\nX\n\nn=1\n\npn Nn ,\n\n(38)\n\nwhere each of the measurements Nn is generated by the\noperator En is thus\nC = ln N \u2212\n\nN\nX\n\nn=1\n\npn S(En ).\n\n(39)\n\nB.\n\nUnitarily Covariant Measurements\n\nIn the previous subsection we found a class of classical measurements for which the information provided by\nthe measurement always increased with the initial uncertainty. In this section we turn our attention to quantum\nmeasurements. In this case, it is the Unitarily Covariant Measurements (UCM's), to be defined below, which\nare the equivalent of the classical permutation-symmetric\nmeasurements, in that they are invariant under all reversible transformations of the state-space. However,\nin considering the information gathering properties of\nUCM's we need to be a little more precise about what\nwe mean by 'information about the initial state'.\nIn the previous section we assumed that the information was always encoded in individual (or pure) states,\nrather than probability densities of states (or mixtures).\nWhat this assumption meant is that \u2206Ii , which measures\nthe information provided about the encoded message,\nalso measured the information obtained about which\nstate the system was initially in. Note that, since classical systems are always in some 'pure' state, it is not\nambiguous to ask what state the system is in, even if the\ninitial encoding is in mixed states. However, if the initial\nencoding does use mixed states, \u2206Ii no longer measures\nthe information that the measurement provides about the\nwhat state the system is in.\nWhen we consider obtaining a quantum equivalent of\nthe theorem of the previous section, then we need to use\nthe equivalent notion of information - that is, information about what pure state the system is in. Now, in\nquantum mechanics, in general the system does not have\nto be in any pure state, since an alternative exists - it\ncould be entangled with another system. Thus, we need\nto be clear that the situation we are concerned with here\nis that in which the system is known to be in some pure\nstate. The information we are concerned with is the information provided about what that state is, and this is\nwhat \u2206Ii (M, \u03b5) measures so long as \u03b5 is an ensemble of\npure states. In what follows our analysis will therefore\nbe restricted to ensembles of pure states.\nTo begin, the unitarily covariant measurements [16, 40]\nare defined as being those measurements which are invariant under every unitary transformation of the measurement operators; that is, when all the measurement\noperators are transformed by a given unitary, all the measurement operators merely transform among themselves.\nThus, every UCM has a continuum of measurement operators, labeled by the unitary transforms U .\nNow, we wish to show that the UCM's are a class of\nquantum measurements for which the information gain\nabout the initial pure state ensemble, \u2206Ii (M, \u03b5), always increases with the initial uncertainty. Now we must\nask, 'the initial uncertainty of what?'. Since, in general,\n\u2206Ii (M, \u03b5) is a function of the initial ensemble, one might\nexpect that we would have to consider some uncertainty\nproperty of the ensemble, rather than the initial state, \u03c1.\nHowever, it turns out that this is not the case:\n\n\f9\nTheorem: The information \u2206Ii (M, \u03b5), obtained about\na pure-state ensemble \u03b5 by a unitarily covariant measurement M, depends\nPon the ensemble only through the density matrix \u03c1 = i P (i)|\u03c8i ih\u03c8i | where \u03b5 = {P (i), |\u03c8i i}.\nProof: The expression for the information gain is\n\u2206I(M, \u03b5) = H[Q\u03c1 (U )]\nX\n\u2212\nP\u03c1 (|\u03c8m i)H(Q\u03c1 (U ||\u03c8m i)).\n\n(40)\n\nthe eigenvectors of \u03c1 to be the same as the eigenvectors of\nE. Denoting these eigenvectors by {|ji}, and the eigenvalues of \u03c1 and E as {\u03bbj } and {Ei }, respectively, we have\nX\n\u03bbj |jihj|)]\nQ\u03c1 (U ) = Tr[U EU \u2020 (\nj\n\n=\n\nj\n\nm\n\nSince M is unitarily covariant, H(Q\u03c1 (U ||\u03c8m i)) is the\nsame for all initial states |\u03c8m i, and therefore the second\nterm is the same for all initial ensembles. As a result,\n\u2206I(M, \u03c1) depends only on the first term, which depends\nonly on \u03c1. \u0003\nIn view of the above result, we now show that under\nunitarily covariant measurements [40], \u2206Ii (M, \u03c1(\u03b5)) is\nShur-concave in \u03c1.\nTheorem: If M is unitarily covariant, then for any\ntwo states \u03c1 and \u03c3, \u03c1 \u227a \u03c3 implies that \u2206Ii (M, \u03c1(\u03b5)) >\n\u2206Ii (M, \u03c3(\u03b5\u2032 )), where \u03b5 and \u03b5\u2032 are ensembles of pure\nstates.\nProof: First we note that given a positive operator \u03a9,\nwe have\nZ\nU EU \u2020 d\u03bc(U ) = Tr[E]I,\n(41)\nwhere E = \u03a92 and d\u03bc(U ) is the (unitarily covariant)\nHaar measure over unitary operators U [40]. Thus, to\ngenerate a covariant measurement from an operator \u03a9,\nwe first scale \u03a9 so that Tr[E] = 1. We will refer to a UC\nmeasurement generated from a single operator \u03a9 as an\nirreducible UC measurement. Now, Eq.(41) also tells us\nthat for any covariant POVM, each subset of the measurement operators which is closed under unitary transformations form themselves a POVM. As a result we can\nrestrict ourselves to measurements in which all the measurement operators are obtained from each other by a\nunitary transform; all other unitarily covariant measurements can be obtained from measurements of that form\nby mixing.\nNow consider the information gain\n\u2206I(M, \u03c1) = H[Q\u03c1 (U )]\nX\n\u2212\nP\u03c1 (|\u03c8m i)H(Q\u03c1 (U ||\u03c8m i)).\n\n\"\nX X\n\n(42)\n\nm\n\nNow, since the measurement is unitarily covariant,\nH(Q\u03c1 (U ||\u03c8m i)) is independent of the state |\u03c8m i. As\na result, the second term in the expression for \u2206I is independent of the ensemble probabilities P\u03c1 (|\u03c8m i), and\ntherefore of the initial state \u03c1. Thus we need merely consider the first term, H[Q\u03c1 (U )]. Now since\nZ\nH[Q\u03c1 (U )] =\nf (Tr[U EU \u2020 \u03c1])d\u03bc(U )\n(43)\n(where f (x) = \u2212x ln x), H is invariant under a unitary\ntransformation of \u03c1, and as a result we can always choose\n\n#\n\nEi Mij (U ) \u03bbj ,\n\ni\n\n(44)\n\nwhere Mij (U ) \u2261 |Uij |2 is a doubly stochastic matrix.\nThe quantity in the square brackets has two indices:\nthe column index, j, is discrete and the 'row' index, U ,\nis continuous. The sum over j is a discrete-to-continuous\nversion of matrix multiplication. This continuous nature\nwill make the following discussion more involved than\nthat in previous section, but the complication is primarily\ntechnical.\nFirst we make the problem discrete by choosing a finite set of points on which to sample the continuous set\nof unitary transformations U . Let us denote the set of\npoints as {Uk : k = 1, . . . , L}, and the sampled values of\nQ at those points, as Q\u03c1 (Uk ). The set of these values is\nnow a vector with L elements. The discrete transformation may now be written as\n#\n\"\nX X\nEi Mij (Uk ) \u03bbj .\n(45)\nQ\u03c1 (Uk ) =\nj\n\ni\n\nNow, for every sample point Uk , we have a doubly\nstochastic matrix, Mij (Uk ). The fact that the continuum\ntransformation includes every value of U (that is, every\nunitary transformation), means that, given any sampling\n{Uk : k = 1, . . . , L}, we can extend this sampling to include every column permutation of every one of the matrices Mij (Uk ). (By a column permutation we mean an\noperation in which one or more of the columns of M are\nswapped.) That is, we can choose a new set, {Ul :},\nsuch that it contains {Uk : k = 1, . . . , L} as a subset,\nand the set {Mij (Ul ) : l = 1, . . . , L\u2032 } is such that for\nevery element M , every column permutation is also an\nelement. The transformation for this extended set is now\nmajorization preserving, since\n#\n\"\nX X\nEi Mij (Ul ) \u03bbj\n(46)\nQ\u03c1 (Ul ) =\n=\n\nj\n\ni\n\nX\n\nM\u0303lj \u03bbj ,\n\n(47)\n\nj\n\nand M\u0303lj is a Chong matrix. This is because the operation of summing over the index i in the first line above\nsimply involves summing each of the columns of each\nstochastic matrix Mij , to create one row of the matrix\nM\u0303lj . Since every column permutation of each Mij exists,\nthe resulting matrix, M\u0303lj , is such that every permutation\nof each row of M\u0303lj appears as another row of M\u0303lj , which\nis Chong's condition.\n\n\f10\nOur goal is naturally to take the limit as L tends to\ninfinity, to obtain the continuum result that we want.\nHowever, before we do so we must take into account the\nmeasure. In calculating the entropy of the density Q\u03c1 (U ),\nwe integrate over the Haar measure. A discrete approximation of this integral for our vector Q\u03c1 (Ul ) is\n\u2032\n\nH[Q\u03c1 (Ul )] =\n\nL\nX\n\n\u2206(Ul )f [Q\u03c1 (Ul )],\n\n(48)\n\nl=1\n\nwhere as before f (x) = \u2212x log x, and \u2206(Ul ) is the Haar\nvolume associated with each sample point Ul . It is important for our purposes that in the above equation we associate the same Haar volume with all sample points which\nare obtained from each other by a permutation. (We will\nrefer to the L sets containing N ! points, all permutations\nof each other, as the L permutation-invariant sets.) To\nsee that this is possible, for any value of L, one first notes\nthat the group of permutations forms a finite subgroup\n(containing N ! elements) of the group of all unitary transformations. As such, it can be used to divide the set of\nall unitaries up into N ! subsets, where each is the image\nof the others under the action of a permutation. Since\nthe Haar measure is invariant under unitary transformations, each of these subsets has not only the same Haar\nvolume, but any region defined within one has the same\nvolume when that region is mapped to another. Because\nof this last property, we can do the following: To ensure\nthat all the points within a given permutation-invariant\nset have the same associated Haar volume, we can (for\nexample) chose our original L points all to lie within one\nof the N ! factorial subsets, choosing the Haar volumes\nassociated with each in whatever way we see fit. When\nwe extend the set of points to include all permutations,\nthe regions associated with the points in the first subset\nare mapped to each of the other N ! \u2212 1 subsets. Each\nof the new points thus has the same Haar volume as the\noriginal point of which it is the image, which means that\nall the points in a given permutation-invariant set have\nan associated region with the same Haar volume.\nThus, we can break the vector of points Ul into L subvectors, where the points in each are the points of one\nof the L permutation-invariant sets, and which consequently all have the same associated Haar volume. The\nimportance of this is that each of these subvectors is obtained from the \u03bbj 's by a Chong matrix. We can now\nwrite the above summation as,\nH[Q\u03c1 (Ul )] =\n\nL\nX\n\nn=1\n\n\u2206(Un )\n\n\"\nX\n\n#\n\nf [Q\u03c1 (Uln )] ,\n\nln\n\n(49)\n\nwhich is simply a weighted sum of the entropies of each of\nthe subvectors. Since each of the subvectors is obtained\nfrom the vector \u03bb(\u03c1) by a transformation of Chong form,\nthe entropy of each is Shur-concave in \u03bb(\u03c1), and hence\nH[Q\u03c1 (Ul )] > H[Q\u03c3 (Ul )]\n\n(50)\n\nif \u03c1 \u227a \u03c3. Now, finally, we can take the limit L \u2192 \u221e,\nso that we recover the Haar integral, and H[Q\u03c1 (Ul )] \u2192\nH[Q\u03c1 (U )]. Since we know that Eq.(50) is true for each\nL in the sequence, it is true in the limit, and we obtain our result for all ICM's. Since all unitarily covariant measurements can be obtained ICM's by mixing (i.e.\naveraging), the result follows for all unitarily covariant\nmeasurements. \u0003\nThe following corollary is an immediate consequence:\nCorollary: The classical capacity of a unitarily covariant measurement is obtained by the uniform ensemble\nover any basis.\nIn addition to the above result, we conjecture that the\nfinal information, \u2206If (M, \u03c1), is also Shur-concave in \u03c1\nfor all unitarily covariant measurements.\n\nC.\n\nClassical Capacities for Symmetric and\nCovariant Measurements\n\nTo obtain the classical capacity of a quantum measurement M one must optimize \u2206Ii (M, \u03b5) over all initial\nencodings. However, it turns out the complexity of this\nprocedure is significantly reduced for commutative measurements. This is because, as we now show, the classical capacity of a commutative quantum measurement\nis the same as that of the equivalent classical measurement. (A commutative and a classical measurement will\nbe said to be equivalent when the measurement operators\nof the commutative measurement are, upon diagonalization, the same as those of the classical measurement.)\nThis gives us the classical capacity of all Commutative\nPermutation-Symmetric (CPS) measurements in terms of\nthe previous results for classical permutation-symmetric\nmeasurements.\nTheorem: The classical capacity of a commutative\nmeasurement is the same as that of the equivalent classical measurement.\nProof: Let M be a commutative measurement, and\nlet us denote the basis in which the measurement operators of M are diagonal as {|ii}. Note that when we\nuse the states {|ii} to encode information, the behavior of the commutative measurement is exactly the same\nas the equivalent classical measurement. Consider now\nthe conditional probability, Q(j||\u03c8i), for outcome\nPj given\nthat the initial state is the arbitrary state |\u03c8i = i \u03b1i |ii.\nThis is\nX\n|\u03b1i |2 Q(j||ii). (51)\nQ(j||\u03c8i) = Tr[Ej |\u03c8ih\u03c8|] =\ni\n\nBut this is precisely the same expressionP\nwe would have\nobtained if we had used the mixture \u03c1 = i |\u03b1i |2 |iihi| as\nthe encoding state, instead of the pure state |\u03c8i. Since\nthe expression for the classical capacity can be written\nentirely in terms of the conditional probabilities Q(j||\u03c8i i)\n(along with the probabilities P (i)), this means that encoding using any state which is not one of the eigenstates\n\n\f11\n|ii renders the same information as encoding using a mixture of the eigenstates. Since using a mixture is never\nbetter than using a single state, the capacity is achieved\nby encoding using the eigenstates. In this case the commutative measurement reduces to the equivalent classical measurement, and their respective capacities are the\nsame. \u0003\nThus the classical capacity of CPS measurements is\ngiven by Eqs.(37) and (39).\nIt was shown in the previous section that an ensemble which achieves the classical capacity for all unitarily\ncovariant measurements is the uniform distribution over\nany basis. Using this we calculate the resulting classical capacity for irreducible UC measurements in Appendix B, which is\nC = ln N \u2212 Q(E) \u2212\n\nN\nX\n1\n,\nk\n\n(52)\n\nk=2\n\nwhere Q(E) is the subentropy of the operator E, as defined by Jozsa, Robb and Wootters [41]. The capacity in\nnats of a general UC measurement,\nM=\n\nN\nX\n\nn=1\n\npn Nn ,\n\n(53)\n\nwhere each of the irreducible UC measurements Nn is\ngenerated by the operator En , is thus\nC = ln N \u2212\n\nN\nX\n\nn=1\n\npn Q(En ) \u2212\n\nN\nX\n1\n.\nk\n\n(54)\n\nk=2\n\nThe capacity in nats of the complete unitarily covariant\nmeasurements (of which there is only one for each dimension N ), being a special case of the above formula,\nis\nC = ln N \u2212\n\nN\nX\n1\n.\nk\n\n(55)\n\nBare: Measurements in which every measurement operator is a positive operator. Alternative terms which\nhave been used for these kind of measurements are pure\nmeasurements [14], measurements without feedback [15],\nand square root measurements [16].\nClassical: Measurements which can be performed on\nsystems which lie within the domain of classical physics.\nThese are measurements in which the all the measurement operators are positive, mutually commuting, and\nalso commute with the density matrix describing the\nstate being measured.\nCommutative: Measurements in which the all the\nmeasurement operators are mutually commuting.\nComplete: Measurements in which all the effects are\nproportional to rank-1 projectors [41]. An alternative\nname for these measurements is maximal strength.\nFinite-Strength: Measurements in which all the measurement operators are of full rank [15].\nInfinite-Strength: Measurements in which at least\none of the measurement operators has at least one zero\neigenvector.\nIncomplete: Measurements in which at least one of\nthe effects is higher than rank one.\nPermutation-Symmetric: Measurements in which,\nif any two of the basis states are permuted, the measurement remains unchanged. That is, under a permutation\nof basis states, the measurement operators merely transform among themselves.\nUnitarily Covariant: Measurements in which, if a\nunitary transformation is applied to the measurement operators, the measurement remains unchanged. That is,\nunder a unitary transformation, all the measurement operators transform among themselves.\nvon Neumann: Measurements in which the all\nthe measurement operators are commuting (orthogonal)\nrank-1 projectors.\n\nAPPENDIX B: CLASSICAL CAPACITY OF\nUNITARILY COVARIANT MEASUREMENTS\n\nk=2\n\nAcknowledgments\n\nThe author would like to thank Gerard Jungman and\nHoward Barnum for helpful discussions, and Howard\nWiseman for helpful comments on the manuscript. The\nauthor is also grateful both to Vlatko Vedral for hospitality during a visit to Imperial College, and Lucien Hardy\nfor hospitality during a visit to the Perimeter Institute\nwhere part of this work was carried out.\n\nAPPENDIX A: TERMINOLOGY\n\nThe following names are used in the body of the paper\nto designate various classes of measurements:\n\nTo calculate the classical capacity it is most convenient to use the form for the mutual information given in\nEq.(11). For Unitarily covariant measurements the classical capacity is attained by an ensemble consisting of the\nuniform distribution over any basis, so we have\n\uf8f9\n\uf8ee\nX\nQ(j)H[P (i|j)]\uf8fb\nC(M) = sup \uf8f0H[P (i)] \u2212\n\u03b5\n\n= ln N \u2212\n\n= ln N \u2212\n\nj\n\nZ\n\nH[P (i|U )]d\u03bc(U )\n\nZ\n\nH[N Q(U ||ii)P (i)]d\u03bc(U )\nZ\n= ln N + N Q(U ||\u03c8i) ln Q(U ||\u03c8i)d\u03bc(U )\n= ln N \u2212 N H[Q(U ||\u03c8i)],\n\n(B1)\n\n\f12\nwhere we have used the fact that Q(U ||ii) is independent\nof |ii, and it should be noted that in the second to last\nline, |\u03c8i simply represents any pure state, and d\u03bc(U ) is\nthe unitarily covariant Haar measure over unitary transformations. To continue,\n\nThus the entropy of the conditional probability density\nis\n\nQ(U ||\u03c8i) = Tr[U EU \u2020 |\u03c8ih\u03c8|] = h\u03c8|U EU \u2020 |\u03c8i\n=\n\nN\nX\nj=1\n\nEj |h\u03c8|U |Ej i|2 .\n\n(B2)\n\nH[Q(U ||\u03c8i)] = \u2212\n= \u2212\n\nZ X\nN\nj=1\n\nZ X\nN\nj=1\n\n2\n\nEj |h\u03c8|U |Ej i| ln\n2\n\nEj |h\u03c8|Ej i| ln\n\nN\nX\n\nk=1\n\nN\nX\n\nk=1\n\nEk |h\u03c8|U |Ek i|\n\nEk |h\u03c8|Ek i|\n\n2\n\n!\n\n2\n\n!\n\nd\u03bc(U )\n\nd\u03bc(|\u03c8i),\n\n(B3)\n\nwhere d\u03bc(|\u03c8i) is the unitarily covariant measure over pure states [42]. Writing Pi = |h\u03c8|Ej i|2P\n, this becomes an integral\nover the uniform measure on the probability simplex [11, 43], being the volume defined by i Pi \u2264 1. That is\nN H[Q(U ||\u03c8i)] = \u2212N !\n\nZ\n\n0\n\n1\n\nZ\n\n0\n\n1\u2212PZ\n1 1\u2212\n\n***\n\n0\n\nP N \u22122\nn=1\n\nP(N \u22121)\nwhere PN = 1 \u2212 n=1 Pn . Evaluating this integral\nis non-trivial, but it has been solved by Jones [42], and\ntwo alternative methods are given by Jozsa, Robb and\nWootters [41]. The solution is\n\uf8f6\n\uf8eb\nN\nX\nY Ek\n\uf8f8\n\uf8ed(Ek ln Ek )\nN H[Q(U ||\u03c8i)] = \u2212\nEk \u2212 El\nk=1\nl6=k\n\u0013\n\u0012\n1\n1 1\n+ + ***+\n+\n2 3\nN\nN\nX\n1\n,\n= Q(E) +\nk\n\n(B5)\n\nk=2\n\nwhere Q(E) is the subentropy of the operator E, as defined by Jozsa, Robb and Wootters. It might appear that\nthis blows up when any of the eigenvalues of E are equal\n- however this is not the case; the value of H[Q(U ||\u03c8i)]\nin the limit as En \u2192 Em , for any n and m, remains fi-\n\n[1] B. Schumacher, Phys. Rev. A 54, 2614 (1996).\n[2] K. Kraus, States, Effects and Operations: Fundamental\nNotions of Quantum Theory, Lecture Notes in Physic\nVol. 190 (Springer-Verlag, Berlin, 1983).\n[3] M.A. Nielsen and I.L. Chuang, Quantum Computation\nand Quantum Information, (Cambridge University Press,\nCambridge, 2000).\n\nN\nPn X\nj=1\n\nEj Pj ln\n\nN\nX\n\nk=1\n\nEk Pk\n\n!\n\ndPN \u22121 * * * dP1 ,\n\n(B4)\n\nnite [41]. In fact, Q(E) \u2264 S(E), \u2200E. Combining Eq.(B5)\nwith Eq.(B1), gives the capacity (in nats) of all covariant\n\u221a\nmeasurements generated from a single operator \u03a9 = E.\nIt is shown in Ref. [41] that the maximum value of the\nright hand side of Eq.(B5) is ln N , which is obtained when\nE = I/N . Thus the capacity\n\u221a of the covariant measurement generated by \u03a9 = I/ N is zero as required.\nFor complete measurements, E1 = 1 and Ej = 0 for\nj > 1, and the integral in Eq.(B4) reduces to\nZ 1\nH[Q(U ||\u03c8i)] = \u2212(N \u2212 1)\n(1 \u2212 P )N \u22122 P ln (P ) dP\n0\n\u0012\n\u0013\n1 1 1\n1\n=\n.\n(B6)\n+ + *** +\nN 2 3\nN\nCombining this with Eq.(B1) gives the classical capacity\n(in nats) for the complete unitarily covariant measurements.\n\n[4] R. Schatten, Norm Ideals of Completely Continuous Operators, (Springer-Verlag, Berlin, 1960).\n[5] H. M. Wiseman and G. J. Milburn, Phys. Rev. Lett.\n70, 548 (1993); H. M. Wiseman, Phys. Rev. A 49, 2133\n(1994); 49, 5159(E) (1994); 50 4428(E) (1994).\n[6] A. Doherty and K. Jacobs, Phys. Rev. A 60, 2700 (1999);\nA.C. Doherty, S. Habib, K. Jacobs, H. Mabuchi and S.M.\n\n\f13\nTan, Phys. Rev. A 62 012105 (2000).\n[7] V.P. Belavkin, Rep. Math. Phys. 43, 405 (1999).\n[8] T. Bayes, Phil. Trans. Roy. Soc., 330 (1763).\n[9] E.T. Jaynes, in E.T. Jaynes : Papers on Probability, Statistics, and Statistical Physics, edited by R.D.\nRosenkrantz, (Dordrecht, Holland, 1983).\n[10] See, e.g. S.J. Press, Bayesian statistics : principles, models, and applications, (Wiley, New York, 1989).\n[11] K. Jacobs, Quant. Information Processing 1, 73 (2002).\n[12] L. Hardy, Quantum Theory From Five Reasonable Axioms, Eprint: quant-ph/0101012.\n[13] T. Ando, Lin. Alg. App. 118, 163 (1989).\n[14] A.C. Doherty, K. Jacobs and G. Jungman, Phys. Rev. A\n63, 062306 (2001).\n[15] C.A. Fuchs and K. Jacobs, Phys. Rev. A 63, 062305\n(2001).\n[16] H. Barnum, Information-disturbance tradeoff in quantum\nmeasurement on the uniform ensemble and on the mutually unbiased bases, Eprint: quant-ph/0205155.\n[17] G.M. D'Ariano, P. LoPresti, 'Classical and quantum\nnoise in measurements and transformations', Eprint:\nquant-ph/0301110.\n[18] A.S. Holevo, Probl. Pereda. Inf. 9, 3 (1973) [Probl. Inf.\nTransm. 9, 110 (1973)].\n[19] B. Schumacher and M.D. Westmoreland, Phys. Rev. A\n56, 131 (1997).\n[20] A.S. Holevo, IEEE Trans. Inf. Theory 44, 269 (1998).\n[21] C.A. Fuchs, Phys.Rev.Lett. 79, 1162 (1997) .\n[22] C. King, M. Nathanson and M.-B. Ruskai, Phys. Rev.\nLett. 88, 057901 (2002).\n[23] P.W. Shor, J. Math. Phys. 43, 4334 (2002).\n[24] C. King, The capacity of the quantum depolarizing channel, Eprint: quant-ph/0204172.\n[25] M. Hayashi and H. Nagaoka, General formulas\nfor capacity of classical-quantum channels, Eprint:\nquant-ph/0206186.\n[26] C.E. Shannon and W. Weaver, The Mathematical Theory\nof Communication, (Illini Books, Chicago, 1963).\n\n[27] A.S. Holevo, Prob. inf. Transm. 15, 247 (1979).\n[28] M. Sasaki, K. Kato, M. Izutsu, O. Hirota, Phys. Lett.\n236, 1 (1997), Eprint: quant-ph/9705043.\n[29] J.R. Buck, S.J. van Enk, C.A. Fuchs, Phys.Rev. A61\n(2000) 032309, Eprint: quant-ph/9903039.\n[30] C. King and M.B. Ruskai, J. Math. Phys. 42, 87-98\n(2001), Eprint: quant-ph/0004062.\n[31] M.A. Nielsen, Phys. Rev. A 63, 022114 (2001).\n[32] A.W. Marshall and I. Olkin, Inequalities: Theory of Majorization and Its Applications, (Academic Press, New\nYork, 1979).\n[33] N. Bohr, Phys. Rev. 48, 696\u2013702 (1935); M. Jammer,\nThe Philosophy of Quantum Mechanics: The Interpretations of Quantum Mechanics in Historical Perspective,\n(Wiley, New York, 1974).\n[34] C.A. Fuchs, Fortschr. Phys. 46, 535 (1998).\n[35] K. Banaszek, Phys. Rev. Lett. 86, 1366 (2001); K. Banaszek, Information gain versus disturbance for a single\nqubit, Eprint: quant-ph/0006062; K. Banaszek, I. Devetak, Phys. Rev. A 64, 052307 (2001).\n[36] B. Schumacher, in Complexity, Entropy and the Physics\nof Information, edited by W.H. Zurek (Addison-Wesley,\nRedwood City, CA, 1990).\n[37] K. Jacobs, Phys. Rev. A 67, 030301(R) (2003).\n[38] The notation v \u227a u, for two vectors u and v, means that\n'v is majorized by u'. A brief (but adequate) introduction\nto the theory of majorization is given in Ref. [31]. A full\nintroduction is given in Ref. [32].\n[39] K.-M. Chong, Trans. Amer. Math. Soc. 200, 437 (1974).\n[40] G. Cassinelli, E. De Vito, A. Toigo, Positive operator\nvalued measures covariant with respect to an irreducible\nrepresentation, Eprint: quant-ph/0302187\n[41] R. Jozsa, D. Robb and W.K. Wootters, Phys. Rev. A 49,\n668 (1994).\n[42] K.R.W Jones, Phys. Rev. A 50, 3682 (1994).\n[43] S. Sykora, J. Stat. Phys. 11, 17 (1974).\n\n\f"}