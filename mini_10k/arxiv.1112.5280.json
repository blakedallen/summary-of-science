{"id": "http://arxiv.org/abs/1112.5280v1", "guidislink": true, "updated": "2011-12-22T11:13:24Z", "updated_parsed": [2011, 12, 22, 11, 13, 24, 3, 356, 0], "published": "2011-12-22T11:13:24Z", "published_parsed": [2011, 12, 22, 11, 13, 24, 3, 356, 0], "title": "LatticeQCD using OpenCL", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1112.6052%2C1112.5039%2C1112.4397%2C1112.1216%2C1112.2889%2C1112.3012%2C1112.3895%2C1112.3963%2C1112.6138%2C1112.1787%2C1112.6156%2C1112.3474%2C1112.0420%2C1112.1472%2C1112.0790%2C1112.1148%2C1112.2146%2C1112.0769%2C1112.0395%2C1112.4744%2C1112.2161%2C1112.1336%2C1112.0388%2C1112.5755%2C1112.5870%2C1112.3589%2C1112.6407%2C1112.4144%2C1112.6026%2C1112.2701%2C1112.6312%2C1112.5926%2C1112.1667%2C1112.4193%2C1112.3862%2C1112.0573%2C1112.5389%2C1112.0319%2C1112.2246%2C1112.4726%2C1112.5285%2C1112.5110%2C1112.0575%2C1112.1263%2C1112.1033%2C1112.2995%2C1112.2887%2C1112.2921%2C1112.1716%2C1112.4014%2C1112.1491%2C1112.3803%2C1112.4155%2C1112.4738%2C1112.0895%2C1112.5586%2C1112.0161%2C1112.2556%2C1112.5041%2C1112.1179%2C1112.1044%2C1112.6391%2C1112.3757%2C1112.1709%2C1112.3280%2C1112.1487%2C1112.2129%2C1112.0279%2C1112.4205%2C1112.4301%2C1112.3024%2C1112.3960%2C1112.2214%2C1112.5428%2C1112.5896%2C1112.4833%2C1112.2590%2C1112.1635%2C1112.0547%2C1112.2233%2C1112.6409%2C1112.4986%2C1112.1488%2C1112.5438%2C1112.5683%2C1112.4325%2C1112.5505%2C1112.4101%2C1112.3028%2C1112.6148%2C1112.2581%2C1112.5280%2C1112.1823%2C1112.2100%2C1112.4428%2C1112.2260%2C1112.1294%2C1112.3850%2C1112.4576%2C1112.4164%2C1112.3656&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "LatticeQCD using OpenCL"}, "summary": "We report on our implementation of LatticeQCD applications using OpenCL. We\nfocus on the general concept and on distributing different parts on hybrid\nsystems, consisting of both CPUs (Central Processing Units) and GPUs (Graphic\nProcessing Units).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1112.6052%2C1112.5039%2C1112.4397%2C1112.1216%2C1112.2889%2C1112.3012%2C1112.3895%2C1112.3963%2C1112.6138%2C1112.1787%2C1112.6156%2C1112.3474%2C1112.0420%2C1112.1472%2C1112.0790%2C1112.1148%2C1112.2146%2C1112.0769%2C1112.0395%2C1112.4744%2C1112.2161%2C1112.1336%2C1112.0388%2C1112.5755%2C1112.5870%2C1112.3589%2C1112.6407%2C1112.4144%2C1112.6026%2C1112.2701%2C1112.6312%2C1112.5926%2C1112.1667%2C1112.4193%2C1112.3862%2C1112.0573%2C1112.5389%2C1112.0319%2C1112.2246%2C1112.4726%2C1112.5285%2C1112.5110%2C1112.0575%2C1112.1263%2C1112.1033%2C1112.2995%2C1112.2887%2C1112.2921%2C1112.1716%2C1112.4014%2C1112.1491%2C1112.3803%2C1112.4155%2C1112.4738%2C1112.0895%2C1112.5586%2C1112.0161%2C1112.2556%2C1112.5041%2C1112.1179%2C1112.1044%2C1112.6391%2C1112.3757%2C1112.1709%2C1112.3280%2C1112.1487%2C1112.2129%2C1112.0279%2C1112.4205%2C1112.4301%2C1112.3024%2C1112.3960%2C1112.2214%2C1112.5428%2C1112.5896%2C1112.4833%2C1112.2590%2C1112.1635%2C1112.0547%2C1112.2233%2C1112.6409%2C1112.4986%2C1112.1488%2C1112.5438%2C1112.5683%2C1112.4325%2C1112.5505%2C1112.4101%2C1112.3028%2C1112.6148%2C1112.2581%2C1112.5280%2C1112.1823%2C1112.2100%2C1112.4428%2C1112.2260%2C1112.1294%2C1112.3850%2C1112.4576%2C1112.4164%2C1112.3656&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We report on our implementation of LatticeQCD applications using OpenCL. We\nfocus on the general concept and on distributing different parts on hybrid\nsystems, consisting of both CPUs (Central Processing Units) and GPUs (Graphic\nProcessing Units)."}, "authors": ["Matthias Bach", "Owe Philipsen", "Christopher Pinke", "Christian Sch\u00e4fer", "Lars Zeidlewicz"], "author_detail": {"name": "Lars Zeidlewicz"}, "author": "Lars Zeidlewicz", "arxiv_comment": "7 pages, 13 figures, proceedings of the XXIX International Symposium\n  on Lattice Field Theory - Lattice 2011, July 10-16, 2011, Squaw Valley, Lake\n  Tahoe, California", "links": [{"href": "http://arxiv.org/abs/1112.5280v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1112.5280v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "hep-lat", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "hep-lat", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1112.5280v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1112.5280v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:1112.5280v1 [hep-lat] 22 Dec 2011\n\nLatticeQCD using OpenCL\n\nOwe Philipsen, Christopher Pinke\u2217, Christian Sch\u00e4fer, Lars Zeidlewicz\nInstitut f\u00fcr Theoretische Physik - Johann Wolfgang Goethe-Universit\u00e4t\nMax-von-Laue-Str. 1, 60438 Frankfurt am Main\nE-mail: philipsen, pinke, cschaefer, zeidlewicz\n@th.physik.uni-frankfurt.de\n\nMatthias Bach\nFrankfurt Institute for Advanced Studies / Institut f\u00fcr Informatik - Johann Wolfgang\nGoethe-Universit\u00e4t\nRuth-Moufang-Str. 1, 60438 Frankfurt am Main\nE-mail: bach@compeng.uni-frankfurt.de\nWe report on our implementation of LatticeQCD applications using OpenCL. We focus on the\ngeneral concept and on distributing different parts on hybrid systems, consisting of both CPUs\n(Central Processing Units) and GPUs (Graphic Processing Units).\n\nXXIX International Symposium on Lattice Field Theory\nJuly 10 - 16 2011\nSquaw Valley, Lake Tahoe, California\n\u2217 Speaker.\n\nc Copyright owned by the author(s) under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike Licence.\n\nhttp://pos.sissa.it/\n\n\fLatticeQCD using OpenCL\n\nChristopher Pinke\n\n1. Introduction\nGraphic Processing Units (GPUs) offer a computing architecture well suited for LatticeQCD applications. Consequently, there is an on-going softwareand algorithm development in order to incorporate\nGPUs effectively into lattice simulations. See for example [1, 2, 3, 4]. These applications are developed\nand carried out predominantly on NVIDIA hardware,\nconsistently using the NVIDIA exclusive CUDA language [5] for the interaction with the GPU. Using\nGPUs is attractive because they have good price-perflop (e.g. \u2248 2,0 A\nC/ Gflop on a NVIDIA GTX 580\nand even \u2248 0,4 A\nC/ Gflop on an AMD 6970 [6]) and\nperformance-per-watt ratios.\nRecently, a new cluster was introduced at FrankFigure 1: LOEWE-CSC\nfurt University, the \"LOEWE-CSC\" [7]. It is dedicated\nto high-performance computing, but contrary to existing clusters it solely consists of AMD hardware. A sketch of its infrastructure is shown in Fig. 1. A striking feature is its heterogeneous\narchitecture: The majority of its compute nodes each hold two 12-core AMD Magny-Cours Central Processing Units (CPUs) and one AMD RADEON 5870 GPU. The LOEWE-CSC is ranked\n22 in the Top500 list of supercomputers [8] and rank 10 in the Green500 list of energy-efficient\nsupercomputers (with 718 Mflops/Watt) [9].\nHowever, presently existing GPU appplications are mostly suitable for NVIDIA hardware.\nOther than using graphic application programming interfaces (APIs) like OpenGL [10], the only\ntool available to use AMD GPUs for general purposes is OpenCL [11]. This is an open standard for\nparallel programming. Furthermore, it is explicitly designed for heterogeneous (or hybrid) systems,\nthus being well suited for the LOEWE-CSC as well as other, non-GPU platforms. Implementations\nof OpenCL can be found both from AMD (AMD Accelerated Parallel Processing (APP) [13],\nformerly ATI Stream SDK) and NVIDIA (as part of CUDA).\nThe first lattice simulations in OpenCL were performed in [1] with staggered fermions. On\nNVIDIA hardware, a significantly lower performance (25% on C1060 and 60% on S2050) of\nOpenCL was reported compared to CUDA for Hybrid Monte Carlo (HMC) updates. An AMD\nGPU was also considered. Here, OpenCL performance is better than on Nvidia hardware, but still\nbelow CUDA results (50% less performance on an AMD 5870 in OpenCL than on a S2050 in\nCUDA).\n\n2. OpenCL\nIn the following, we will present the general ideas of OpenCL and explain important terms.\nFor more information see [12]. The generic concept of an OpenCL application consists of a\n\"host\" program and several \"compute devices\" (see Fig. 2). They live together on a so called\n\"platform\". The host controls memory management and calculations carried out on the devices,\n2\n\n\fLatticeQCD using OpenCL\n\nChristopher Pinke\n\nwhile each device may either be a GPU, a CPU or any other kind of supported compute device. A single device then consists of a bunch of \"compute units\" (on a multicore CPU this\nwould be a single core) which in turn can consist of one or more \"processing elements\" that carry\nout the actual computations. This of course reflects the architecture of GPUs. On CPUs a compute unit (i.e. a single core) and a processing element may also be the same,\nalthough this depends on the specific model and OpenCL\nimplementation used. It should be noted that on a CPU it\nis possible to split up a multicore CPU into several devices,\neffectively grouping the cores suited for specific tasks.\nFigure 2: OpenCL Concept\nFurthermore, execution commands for OpenCL functions (\"kernels\") are scheduled in one or more \"command queues\" via the host. The queue launches\nexecution of kernels on a specific device and also handles memory commands. Synchronization on\nthis level is possible only within a command queue.\nCentral objects of any OpenCL application are the kernels. These have to be written in the\nOpenCL C programming language, which is based on a subset of C99, the C standard. Optionally,\n\"native kernels\" can be included from libraries. Kernels are executed using an up to 3-dimensional\nindex space, where each index can be mapped on an instance of the kernel, which in turn are called\n\"work items\". Several work items make up a \"work group\", which allows for synchronization\nbetween work items. Kernels can access memory on various levels, ranging from \"global\" (e.g.\nthe main memory on the GPU) to \"private\" (e.g. General Purpose Registers (GPRs) of a GPU\nstream-core). Besides the nomenclature, the setup is very similiar to CUDA.\nIt is important to emphasize here that OpenCL allows for data- as well as for task parallel\napplications, meaning that it is on the one hand possible to perform SIMD computations and on\nthe other hand to perform different (possibly independent) tasks in a parallel fashion, providing\nOpenMP [14] or UNIX's pthreads functionality automatically.\nIn order to have a hardware-independent programming model, the actual OpenCL program is\ncompiled and built at runtime of the (host) application. This is done using an OpenCL inherent\ncompiler.\n\n3. Implementation\nThe physical problems we are currently interested in are investigations of the quark gluon\nplasma (QGP) and the thermal transition of QCD with dynamical fermions [15, 16, 17] as well\nas in pure gauge theory (PGT). These have yet been carried out mainly relying on the tmlqcd\nprogram suite [18] and an application written with QDP++ [19]. Several features shall thus be\nprovided by the OpenCL application: On the PGT side we need a SU(3) heatbath algorithm [20],\nwhereas on the fermionic side we require an HMC algorithm including standard features (even-odd\npreconditioning, 2MN integrator, multiple integration timescales) for N f = 2 twisted-mass Wilson\nfermions [18, 21]. Also, ILDG-compatible I/O is required [22].\nIn order to account for the fundamentally different concept of OpenCL we decided to write\nan all new program. The implementation of the desired features mentioned above resulted in four\nexecutables, providing the possiblity to generate gauge configurations as well as calculate phys3\n\n\fLatticeQCD using OpenCL\n\nChristopher Pinke\n\nical observables of interest for pure gauge theory and dynamical fermions. All calculations are\nperformed in OpenCL. In the following we will go to some details describing the concrete implementation.\nBasic\nKappa\n\nTransportcoefficients\n\nRandom\nHeatbath\n\nSpinor LA\n\nHeatbath\nBasic\n\nCorrelators\n\nFermions\n\nHMC\n\nHMC\n\nInverter\n\n(a) OpenCL module classes\n\n(b) Gaugefield classes\n\nFigure 3: Structure of opencl modules- and gaugefield classes\n\nThe host program was set up in C++ in order to implement independent program parts easily\nusing C++ classes and to have extension capabilities in a natural way.\nThe central object is the class gaugefield, which incorporates the initialization of OpenCL\nand holds an application-specific number of opencl_device-objects. As the name indicates,\nthe latter class contains all compute device-related parts, such as kernels or memory objects, and\neventually executes the kernels on a specific device. The class gaugefield is also dedicated to\nsynchronize the physical gauge field when it is used on several devices.\nThe specific physical problems were implemented as child classes of gaugefield and\nopencl_module, containing the problem-related functionality (see Fig. 3). Furthermore, for\neach physical problem a number of \"tasks\" can be defined which then again can contain a number of device objects to carry out this task. For example, the inverter executable essentially\nperforms two tasks, the inversion of the fermion matrix and the calculation of correlators. This\nconcept will prove useful when looking at hybrid applications.\nAs was mentioned in section 2, the OpenCL environment has to go through certain initialization processes before the actual calculations can be carried out. A schematic flow of this process is\nshown in Fig. 4. The major part of the initialization is spent on generating the kernels. This is done\nin a couple of steps: First, the files needed for the kernel code are collected, after that an OpenCL\n\"program\" is compiled and linked using the OpenCL compiler. This program can then be used to\nbuild kernels referring to functions declared with __kernel within the previously read-in source\ncode.\nC++-Hostprogram\nWe found it convenient to build every kernel as a stand alone program\nGaugefield\nsince the kernel is the object of interInit Context\nGet Platform\nest. The compiler generates binary files\n.\n.\n.\nTask 0\nTask 1\nCompile/Inputwhich can be used to extract informaParameters\nInit Queue\ntions about the kernel, e.g. GPR usage,\n...\nOpenCL device 0\nOpenCL device 1\nwhich is useful for benchmarking and\nCollect Sourcefiles/Compileoptions\nInit Buffers/Kernels\noptimization. They can also be reused\nFurther Execution. . .\nfor kernel generation at a later program\nrun. This speeds up the initialization\nFigure 4: Schematic flow of program initialization\ntime significantly.\n4\n\n\fLatticeQCD using OpenCL\n\nChristopher Pinke\n\nOne can influence the whole process at this point by means of the simulation parameters.\nThe latter are typically read in at runtime of the host, so one can e.g. switch between CPU and\nGPU simply via the input file. Since the kernels are compiled only at runtime, one can pass the\nsimulation parameters (e.g. NT, NS, \u03b2 , . . . ) to them as compile time parameters. This is a nice way\nof avoiding many kernel arguments as well as to \"hard code\" the parameters into the kernel code.\n80\n\n110\nflops\nbandwidth\n\n80\n\n75\n\n90\n85\n\n65\n\n80\n60\n\n75\n70\n\n55\n\n65\n60\n\n50\n55\n\n70\n\n60\n\ntime [s]\n\n70\n\nglobal bandwidth utilization [GB/s]\n\n100\n95\n\nflops [Gflops]\n\n90\n\n105\n\n50\n\n40\n\n30\n\n20\n\n50\n\n45\n\n10\n45\n\n40\n\n164\n\n243*8\n\n244\n\n40\n323*12\n\n0\n\nlattice volume\n\n164\n\n243*8\n\n244\n\n323*12\n\nlattice volume\n\nFigure 5: Dslash performance on AMD 5870 on various even-odd preconditioned lattices (statistics are of\nO(10k) for each volume).\n\nIn fermionic applications, the most time-consuming part is the inversion of the fermion matrix\nand the non-diagonal part of the Dirac matrix (\"dslash\"), respectively. On GPUs, this problem is\nalways bandwidth-limited and tuning is required to achieve a satisfiying amount of the maximum\nbandwidth (on an AMD 5870 this is e.g. 154 GB/s). Our (even-odd preconditioned) dslash implementation currently performs at 45 - 60 GFlops in double precision calculations (with a bandwidth\nutilization of up to 105 GB/s) over a wide range of lattice sizes (See Fig. 5). We will give more performance results for AMD hardware (also considering memory optimizations like RECONSTRUCT\nTWELVE [3]) in a future publication.\n\n4. Hybrid strategies\nHaving a hybrid system as the LOEWE-CSC at hand, the question arises how one can use this\ninfrastructure effectively. Both GPU and CPU hold several advantages, qualifying them for certain\ntasks. GPUs outperform CPUs when it comes to floating point operations, whereas a CPU can in\ngeneral operate a bigger amount of memory and a bigger cache, just to name a few. OpenCL can\nbe used quite easily to distribute computations over a hybrid system.\nA typical scenario in lattice simulations\nDevice 0\nDevice 1\nis the iterative calculation of some observSync Gaugefield\nables out of a sequence of gauge configurations. The simplest case one can have\nGaugefield Update\nCalc Obs\nis an observable that does not require any\nsynchronization in between its calculation.\nGiven 2 devices, one can distribute two genFigure 6: Simplest hybrid strategy.\neralized tasks among them, as depicted in\nFig. 6. One task calculates the observable while in the meantime the other provides the ingre5\n\n\fLatticeQCD using OpenCL\n\nChristopher Pinke\n\ndients required for the next iteration's calculation. Synchronization between the devices is carried\nout at the start of each iteration.\nWe implemented this concept for two observables, the N f = 2 mesonic flavour doublet correlators with quantum number \u0393,\n\u0001\nC\u0393 = \u2212 Tr Su\u2020 (x0 , x)\u03b35 \u0393Su (x0 , x)\u0393\u03b35 ,\n(4.1)\nfor which one has to provide the propagator Su (x0 , x) \u223c M \u22121 b(x0 ) (where M is the fermion matrix\nand b a point source at site x0 ), and the second order transport coefficent \u03ba of the Quark Gluon\nPlasma [23]. \u03ba can be extracted from the retarted propagator GR at zero Matsubara frequency,\n\u03ba\n(4.2)\nGR (\u03c9 = 0,~q) = G(0) \u2212 |~q |2 + O(|~q |3 ) ,\n2\nwhich can be calculated on a given gauge configuration by the euclidean correlator GE according\nto\nGR (\u03c9 = 0,~q) = GE (\u03c9 = 0,~q) = N \u2211 eq3 (x3 \u2212y3 ) hT12 (x)T12 (y)i .\n(4.3)\nx,y\n\nT\u03bc\u03bd is a discretization of the energy-momentum tensor using clover-plaquettes [24].\nCP U\n\nGP U\nCP U\n\nSync Su (x0 , x)\n\nSync gauge configuration\n\nLoad gauge configuration\n\nCalc C\u0393\n\nGP U\n\nCalc \u03ba\n\nPerform inversion\n\nUpdate gauge configuration\n\n(b) Transportcoefficient \u03ba\n\n(a) Mesonic correlator C\u0393\n\nFigure 7: Hybrid strategies for two observables of interest.\n\nThe concrete implementations can be seen in Fig. 7, where we considered the case of one\nCPU and one GPU device in the system (note that e.g. on one LOEWE-CSC node, one CPU\ndevice has by default 2*12 = 24 cores). The assignments of the different tasks to the devices came\nquite naturally, since \u03ba 's calculation requires much more memory ressources than the heatbath\nalgorithm and the inversion of the fermion matrix is carried out faster on the GPU.\n\n5. Conclusions\nWe presented to some detail our implementation of LatticeQCD applications using OpenCL.\nThis is a quite different approach compared to other applications around, which mainly operate on\nNVIDIA hardware using CUDA. Since OpenCL is a hardware-independent programming model,\nany hardware can be used, which offer an optimal price-per-flop ratio. Especially, parallel calculations can be performed quite simply in OpenCL, allowing for applications suited for hybrid\narchitectures. We gave two examples using GPU and CPU devices at the same time effectively.\nWe are currently benchmarking and optimizing our code to exploit the compute powers of the\nLOEWE-CSC and AMD hardware in general, providing an alternative to NVIDIA hardware bound\napplications.\n6\n\n\fLatticeQCD using OpenCL\n\nChristopher Pinke\n\nAcknowledgments\nO. P. and C. P. are supported by the German BMBF grant FAIR theory: the QCD phase diagram at vanishing and finite baryon density, 06MS9150. L.Z. is supported by the DFG grant phase\ntransition and screening masses in N f = 2 QCD, PH 158/3-1. M. B., O. P, and C. S. are supported\nby the Helmholtz International Center for FAIR within the LOEWE program of the State of Hesse.\nM.B. is supported by the GSI Helmholtzzentrum f\u00fcr Schwerionenforschung. C.P. acknowledges\ntravel support by the Helmholtz Graduate School HIRe for FAIR.\n\nReferences\n[1] C. Bonati, G. Cossu, M. D'Elia and P. Incardona, arXiv:1106.5673 [hep-lat].\n[2] R. Babich, M. A. Clark and B. Joo, arXiv:1011.0024 [hep-lat].\n[3] M. A. Clark, R. Babich, K. Barros, R. C. Brower and C. Rebbi, Comput. Phys. Commun. 181 (2010)\n1517 [arXiv:0911.3191 [hep-lat]].\n[4] R. Babich, M. A. Clark, B. Joo, G. Shi, R. C. Brower and S. Gottlieb, arXiv:1109.2935 [hep-lat].\n[5] http://developer.nvidia.com/category/zone/cuda-zone\n[6] http://geizhals.at/\n[7] http://compeng.uni-frankfurt.de/index.php?id=86\n[8] http://www.top500.org/list/2011/06/100\n[9] http://www.green500.org/lists/2011/06/top/list.php\n[10] http://www.opengl.org/\n[11] http://www.khronos.org/opencl/\n[12] Khronos Working Group, The OpenCL Specification, http://www.khronos.org/registry/cl/.\n[13] http://developer.amd.com/SDKS/AMDAPPSDK\n[14] http://openmp.org/wp/\n[15] E. M. Ilgenfritz, K. Jansen, M. P. Lombardo, M. Muller-Preussker, M. Petschlies, O. Philipsen and\nL. Zeidlewicz, Phys. Rev. D 80 (2009) 094502 [arXiv:0905.3112 [hep-lat]].\n[16] O. Philipsen and L. Zeidlewicz, Phys. Rev. D 81 (2010) 077501 [arXiv:0812.1177 [hep-lat]].\n[17] F. Burger et al., arXiv:1102.4530 [hep-lat].\n[18] K. Jansen and C. Urbach, Comput. Phys. Commun. 180 (2009) 2717 [arXiv:0905.3331 [hep-lat]].\n[19] http://usqcd.jlab.org/usqcd-docs/qdp++/\n[20] M. Creutz, Phys. Rev. D21 (1980) 2308-2315.; N. Cabibbo and E. Marinari, Phys. Lett. B 119 (1982)\n387.; A. D. Kennedy, B. J. Pendleton, Phys. Lett. B156 (1985) 393-399.\n[21] A. Shindler, Phys. Rept. 461 (2008) 37 [arXiv:0707.4093 [hep-lat]].\n[22] http://cssm.sasr.edu.au/ildg/\n[23] P. Romatschke and D. T. Son, Phys. Rev. D 80 (2009) 065021 [arXiv:0903.3946 [hep-ph]].\n[24] Y. Maezawa, H. Abuki, T. Hatsuda and T. Koide, PoS LATTICE2010 (2010) 201 [arXiv:1012.2222\n[hep-lat]].\n\n7\n\n\f"}