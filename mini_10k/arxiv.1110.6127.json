{"id": "http://arxiv.org/abs/1110.6127v1", "guidislink": true, "updated": "2011-10-27T16:05:29Z", "updated_parsed": [2011, 10, 27, 16, 5, 29, 3, 300, 0], "published": "2011-10-27T16:05:29Z", "published_parsed": [2011, 10, 27, 16, 5, 29, 3, 300, 0], "title": "Optimal Forwarding in Delay Tolerant Networks with Multiple Destinations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.5520%2C1110.3771%2C1110.3516%2C1110.6394%2C1110.2108%2C1110.3356%2C1110.0559%2C1110.0252%2C1110.5109%2C1110.6164%2C1110.5878%2C1110.6232%2C1110.0766%2C1110.0195%2C1110.5436%2C1110.6489%2C1110.1663%2C1110.1455%2C1110.0197%2C1110.3058%2C1110.0769%2C1110.4990%2C1110.3195%2C1110.1337%2C1110.5613%2C1110.2720%2C1110.4947%2C1110.5107%2C1110.5133%2C1110.5425%2C1110.4186%2C1110.0273%2C1110.1831%2C1110.0675%2C1110.2764%2C1110.5423%2C1110.5402%2C1110.3769%2C1110.1277%2C1110.5731%2C1110.5801%2C1110.3086%2C1110.6127%2C1110.2203%2C1110.4397%2C1110.4806%2C1110.6817%2C1110.3823%2C1110.2921%2C1110.6688%2C1110.4768%2C1110.5517%2C1110.6741%2C1110.3784%2C1110.2124%2C1110.2132%2C1110.1814%2C1110.0245%2C1110.1389%2C1110.5454%2C1110.5559%2C1110.3430%2C1110.4388%2C1110.2136%2C1110.1070%2C1110.4028%2C1110.5210%2C1110.3088%2C1110.0277%2C1110.4076%2C1110.5717%2C1110.6851%2C1110.4498%2C1110.4606%2C1110.5293%2C1110.3321%2C1110.1569%2C1110.3669%2C1110.2403%2C1110.4590%2C1110.3360%2C1110.2516%2C1110.4925%2C1110.2799%2C1110.0085%2C1110.5787%2C1110.3594%2C1110.2428%2C1110.6720%2C1110.3838%2C1110.1766%2C1110.1642%2C1110.3731%2C1110.5025%2C1110.4237%2C1110.5111%2C1110.1599%2C1110.5935%2C1110.2717%2C1110.3589%2C1110.0937&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Optimal Forwarding in Delay Tolerant Networks with Multiple Destinations"}, "summary": "We study the trade-off between delivery delay and energy consumption in a\ndelay tolerant network in which a message (or a file) has to be delivered to\neach of several destinations by epidemic relaying. In addition to the\ndestinations, there are several other nodes in the network that can assist in\nrelaying the message. We first assume that, at every instant, all the nodes\nknow the number of relays carrying the packet and the number of destinations\nthat have received the packet. We formulate the problem as a controlled\ncontinuous time Markov chain and derive the optimal closed loop control (i.e.,\nforwarding policy). However, in practice, the intermittent connectivity in the\nnetwork implies that the nodes may not have the required perfect knowledge of\nthe system state. To address this issue, we obtain an ODE (i.e., a\ndeterministic fluid) approximation for the optimally controlled Markov chain.\nThis fluid approximation also yields an asymptotically optimal open loop\npolicy. Finally, we evaluate the performance of the deterministic policy over\nfinite networks. Numerical results show that this policy performs close to the\noptimal closed loop policy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.5520%2C1110.3771%2C1110.3516%2C1110.6394%2C1110.2108%2C1110.3356%2C1110.0559%2C1110.0252%2C1110.5109%2C1110.6164%2C1110.5878%2C1110.6232%2C1110.0766%2C1110.0195%2C1110.5436%2C1110.6489%2C1110.1663%2C1110.1455%2C1110.0197%2C1110.3058%2C1110.0769%2C1110.4990%2C1110.3195%2C1110.1337%2C1110.5613%2C1110.2720%2C1110.4947%2C1110.5107%2C1110.5133%2C1110.5425%2C1110.4186%2C1110.0273%2C1110.1831%2C1110.0675%2C1110.2764%2C1110.5423%2C1110.5402%2C1110.3769%2C1110.1277%2C1110.5731%2C1110.5801%2C1110.3086%2C1110.6127%2C1110.2203%2C1110.4397%2C1110.4806%2C1110.6817%2C1110.3823%2C1110.2921%2C1110.6688%2C1110.4768%2C1110.5517%2C1110.6741%2C1110.3784%2C1110.2124%2C1110.2132%2C1110.1814%2C1110.0245%2C1110.1389%2C1110.5454%2C1110.5559%2C1110.3430%2C1110.4388%2C1110.2136%2C1110.1070%2C1110.4028%2C1110.5210%2C1110.3088%2C1110.0277%2C1110.4076%2C1110.5717%2C1110.6851%2C1110.4498%2C1110.4606%2C1110.5293%2C1110.3321%2C1110.1569%2C1110.3669%2C1110.2403%2C1110.4590%2C1110.3360%2C1110.2516%2C1110.4925%2C1110.2799%2C1110.0085%2C1110.5787%2C1110.3594%2C1110.2428%2C1110.6720%2C1110.3838%2C1110.1766%2C1110.1642%2C1110.3731%2C1110.5025%2C1110.4237%2C1110.5111%2C1110.1599%2C1110.5935%2C1110.2717%2C1110.3589%2C1110.0937&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study the trade-off between delivery delay and energy consumption in a\ndelay tolerant network in which a message (or a file) has to be delivered to\neach of several destinations by epidemic relaying. In addition to the\ndestinations, there are several other nodes in the network that can assist in\nrelaying the message. We first assume that, at every instant, all the nodes\nknow the number of relays carrying the packet and the number of destinations\nthat have received the packet. We formulate the problem as a controlled\ncontinuous time Markov chain and derive the optimal closed loop control (i.e.,\nforwarding policy). However, in practice, the intermittent connectivity in the\nnetwork implies that the nodes may not have the required perfect knowledge of\nthe system state. To address this issue, we obtain an ODE (i.e., a\ndeterministic fluid) approximation for the optimally controlled Markov chain.\nThis fluid approximation also yields an asymptotically optimal open loop\npolicy. Finally, we evaluate the performance of the deterministic policy over\nfinite networks. Numerical results show that this policy performs close to the\noptimal closed loop policy."}, "authors": ["Chandramani Singh", "Eitan Altman", "Anurag Kumar", "Rajesh Sundaresan"], "author_detail": {"name": "Rajesh Sundaresan"}, "author": "Rajesh Sundaresan", "arxiv_comment": "16 pages, 7 figures", "links": [{"href": "http://arxiv.org/abs/1110.6127v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1110.6127v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SY", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1110.6127v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1110.6127v1", "journal_reference": null, "doi": null, "fulltext": "1\n\nOptimal Forwarding in Delay Tolerant Networks\nwith Multiple Destinations\n\narXiv:1110.6127v1 [cs.NI] 27 Oct 2011\n\nChandramani Singh, Student Member, IEEE, Eitan Altman, Fellow, IEEE, Anurag Kumar, Fellow, IEEE,\nand Rajesh Sundaresan, Senior Member, IEEE\n\nAbstract-We study the trade-off between delivery delay and\nenergy consumption in a delay tolerant network in which a\nmessage (or a file) has to be delivered to each of several\ndestinations by epidemic relaying. In addition to the destinations,\nthere are several other nodes in the network that can assist in\nrelaying the message. We first assume that, at every instant, all\nthe nodes know the number of relays carrying the packet and\nthe number of destinations that have received the packet. We\nformulate the problem as a controlled continuous time Markov\nchain and derive the optimal closed loop control (i.e., forwarding\npolicy). However, in practice, the intermittent connectivity in the\nnetwork implies that the nodes may not have the required perfect\nknowledge of the system state. To address this issue, we obtain an\nODE (i.e., a deterministic fluid) approximation for the optimally\ncontrolled Markov chain. This fluid approximation also yields\nan asymptotically optimal open loop policy. Finally, we evaluate\nthe performance of the deterministic policy over finite networks.\nNumerical results show that this policy performs close to the\noptimal closed loop policy.\n\nI. I NTRODUCTION\nDelay tolerant networks (DTNs) [1] are sparse wireless ad\nhoc networks with highly mobile nodes. In these networks,\nthe link between any two nodes is up when these are within\neach other's transmission range, and is down otherwise. In\nparticular, at any given time, it is unlikely that there is a\ncomplete route between a source and its destination.\nWe consider a DTN in which a short message (also referred\nto as a packet) needs to be delivered to multiple (say M )\ndestinations. There are also N potential relays that do not\nthemselves \"want\" the message but can assist in relaying\nit to the nodes that do. At time t = 0, N0 of the relays\nhave copies of the packet. All nodes are assumed to be\nmobile. In such a network, a common technique to improve\npacket delivery delay is epidemic relaying [2]. We consider a\ncontrolled relaying scheme that works as follows. Whenever a\nnode (relay or destination) carrying the packet meets a relay\nthat does not have a copy of the packet, then the former has\nthe option of either copying or not copying. When a node that\nhas the packet meets a destination that does not, the packet\ncan be delivered.\nThis is an extended version of a paper that appeared in WiOpt 2011.\nThis work was supported by the Indo-French Centre for the Promotion of\nAdvanced Research (IFCPAR) Project 4000-IT-1, by DAWN (an Associates\nprogram of INRIA, France), and by the Department of Science and Technology, Government of India.\nChandramani Singh, Anurag Kumar and Rajesh Sundaresan are with the\nDepartment of Electrical Communication Engineering Indian Institute of Science Bangalore, India (email: {chandra, anurag, rajeshs}@ece.iisc.ernet.in).\nEitan Altman is with INRIA, Sophia-Antipolis, France (email: Eitan.Altman@sophia.inria.fr).\n\nWe want to minimize the delay until a significant fraction\n(say \u03b1) of the destinations receive the packet; we refer to this\nduration as delivery delay. Evidently, delivery delay can be\nreduced if the number of carriers of the packet is increased\nby copying it to relays. Such copying can not be done\nindiscriminately, however, as every act of copying between\ntwo nodes incurs a transmission cost. Thus, we focus on the\nproblem of the control of packet forwarding.\nRelated work: Analysis and control of DTNs with a singlesource and a single-destination has been widely studied.\nGroenevelt et al. [3] modeled epidemic relaying and twohop relaying using Markov chains. They derived the average\ndelay and the number of copies generated until the time of\ndelivery. Zhang et al. [4] developed a unified framework based\non ordinary differential equations (ODEs) to study epidemic\nrouting and its variants.\nNeglia and Zhang [5] were the first to study the optimal\ncontrol of relaying in DTNs with a single destination and\nmultiple relays. They assumed that all the nodes have perfect\nknowledge of the number of nodes carrying the packet. Their\noptimal closed loop control is a threshold policy - when a relay\nthat does not have a copy of the packet is met, the packet\nis copied if and only if the number of relays carrying the\npacket is below a threshold. Due to the assumption of complete\nknowledge, the reported performance is a lower bound for the\ncost in a real system.\nAltman et al. [6] addressed the optimal relaying problem for\na class of monotone relay strategies which includes epidemic\nrelaying and two-hop relaying. In particular, they derived static\nand dynamic relaying policies. Altman et al. [7] considered\noptimal discrete-time two-hop relaying. They also employed\nstochastic approximation to facilitate online estimation of\nnetwork parameters. In another paper, Altman et al. [8] considered a scenario where active nodes in the network continuously\nspend energy while beaconing. Their paper studied the joint\nproblem of node activation and transmission power control.\nThese works ([6], [7], [8]) heuristically obtain fluid approximations for DTNs and study open loop controls. Li et al. [9]\nconsidered several families of open loop controls and obtain\noptimal controls within each family.\nDeterministic fluid models expressed as ordinary differential\nequations have been used to approximate large Markovian\nsystems. Kurtz [10] obtained sufficient conditions for the convergence of Markov chains to such fluid limits. Darling [11]\nand subsequently, Darling and Norris [12] generalized Kurtz's\nresults. Darling [11] considers the scenario when the Markovian system satisfies the conditions in [10] only over a subset.\n\n\f2\n\nHe shows that the scaled processes converge to a fluid limit\nuntil they exit from this subset. Darling and Norris [12]\ngeneralize the conditions for convergence, e.g., uniform convergence of the mean drifts of Markov chains and Lipschitz\ncontinuity of the limiting drift function, prescribed in [10].\nGast and Gaujal [13] address the scenario where the limiting\ndrift functions are not Lipschitz continuous. They prove that\nunder mild conditions, the stochastic system converges to the\nsolution of a differential inclusion. Gast et al. [14] study\nan optimization problem on a large Markovian system. They\nshow that solving the limiting deterministic problem yields an\nasymptotically optimal policy for the original problem.\nOur Contributions: We formulate the problem as a controlled continuous time Markov chain (CTMC) [15], and\nobtain the optimal policy (Section III). The optimal policy\nrelies on complete knowledge of the network state at every\nnode, but availability of such information is constrained by\nthe same connectivity problem that limits packet delivery.\nIn the incomplete information setting, the decisions of the\nnodes would have to depend upon their beliefs about the\nnetwork state. The nodes would need to update their beliefs\ncontinuously with time, and also after each meeting with\nanother node. Such belief updates would involve maintaining\na complex information structure and are often impractical\nfor nodes with limited memory and computation capability.\nMoreover, designing closed loop controls based on beliefs is\na difficult task [16], even more so in our context with multiple\ndecision makers and all of them equipped with distinct partial\ninformation.\nIn view of the above difficulties, we adopt the following\napproach. We show that when the number of nodes is large,\nthe optimally controlled network evolution is well approximated by a deterministic dynamical system (Section IV).\nThe existing differential equation approximation results for\nMarkovian systems [10], [11] do not directly apply, as, in the\noptimally controlled Markov chain that arises in our problem,\nthe mean drift rates are discontinuous and do not converge\nuniformly. We extend the results to our problem setting in\nour Theorem 4.1 in Section IV. Note that the differential\ninclusion based approach of Gast and Gaujal [13] is not\ndirectly applicable in our case, as it needs uniform convergence\nof the mean drift rates. The limiting deterministic dynamics\nthen suggests a deterministic control that is asymptotically\noptimal for the finite network problem, i.e., the cost incurred\nby the deterministic control approaches the optimal cost as the\nnetwork size grows. We briefly consider the analogous control\nof two-hop forwarding [17] in Section V. Our numerical\nresults illustrate that the deterministic policy performs close\nto the complete information optimal closed loop policy for a\nwide range of parameter values (Section VI).\nIn a nutshell, the ODE approach is quite common in the\nmodeling of such problems. Its validity in situations without\ncontrol is established by Kurtz [10], Darling and Norris [12],\netc. We aim in this paper at rigorously showing the validity\nof this limit under control in a few DTN problems.\n\nII. T HE S YSTEM M ODEL\nWe consider a set of K := M + N mobile nodes. These\ninclude M destinations and N relays. At t = 0, a packet is\ngenerated and immediately copied to N0 relays (e.g., via a\nbroadcast from an infrastructure network). Alternatively, these\nN0 nodes can be thought of as source nodes.\n1) Mobility model: We model the point process of the meeting instants between pairs of nodes as independent Poisson\npoint processes, each with rate \u03bb. Groenevelt et al. [3] validate\nthis model for a number of common mobility models (random\nwalker, random direction, random waypoint). In particular,\nthey establish its accuracy under the assumptions of small\ncommunication range and sufficiently high speed of nodes.\n2) Communication model: Two nodes may communicate\nonly when they come within transmission range of each other,\ni.e., at meeting instants. The transmissions are assumed to be\ninstantaneous. We assume that that each transmission of the\npacket incurs unit energy expenditure at the transmitter.\n3) Relaying model: We assume that a controlled epidemic\nrelay protocol is employed.\nThroughout, we use the terminology relating to the spread\nof infectious diseases. A node with a copy of the packet is said\nto be infected. A node is said to be susceptible until it receives\na copy of the packet from another infected node. Thus at t = 0,\nN0 nodes are infected while M + N \u2212 N0 are susceptible.\nA. The Forwarding Problem\nThe packet has to be disseminated to all the M destinations.\nHowever, the goal is to minimize the duration until a fraction\n\u03b1 (\u03b1 < 1) of the destinations receive the packet.\nAt each meeting epoch with a susceptible relay, an infected\nnode (relay or destination) has to decide whether to copy the\npacket to the susceptible relay or not. Copying the packet\nincurs unit cost, but promotes early delivery of the packet to\nthe destinations. We wish to find the trade-off between these\ncosts by minimizing\nE{Td + \u03b3Ec }\n\n(1)\n\nwhere Td is the time until which at least M\u03b1 := d\u03b1M e\ndestinations receive the packet, Ec is the total energy consumed in copying, and \u03b3 is the parameter that relates energy\nconsumption cost to delay cost. Varying \u03b3 helps studying the\ntrade-off between the delay and the energy costs.\nIII. O PTIMAL E PIDEMIC F ORWARDING\nWe derive the optimal forwarding policy under the assumption that, at any instant of time, all the nodes have full\ninformation about the number of relays carrying the packet\nand the number of destinations that have received the packet.\nThis assumption will be relaxed in the next section.\nA. The MDP Formulation\nLet tk , k = 1, 2, . . . denote the meeting epochs of the\ninfected nodes (relays or destinations) with the susceptible\nnodes. Let t0 := 0 and define \u03b4k := tk \u2212 tk\u22121 for k \u2265 1.\n\n\f3\n\nek\u22121\nuk\u22121\n\nek\nuk\n\nnk\u22121\nmk\u22121\n\nek+1\nuk+1\n\nnk\nmk\n\nnk+1\nmk+1\n\n\u03b4k\n\n\u03b4k+1\n\nwhere the expectation is taken with respect to the random\ndisturbance (\u03b4k+1 , ek+1 ). It can be observed that\n\uf8f1\n\uf8f4\n\uf8f2\u03b3uk if sk is such that mk \u2265 M\u03b1\ng(sk , uk ) = \u03b3 if sk = (M\u03b1 \u2212 1, n, d) and uk = 1\n\uf8f4\n\uf8f3\n\u03b3uk + Cd (sk , uk ) otherwise,\nwhere\n\ntk\u22121\n\ntk\n\ntk+1\n\nFig. 1. Evolution of the controlled Markov chain {sk }. Note that (mk , nk )\nis embedded at tk \u2212, i.e., just before the meeting epoch.\n\nLet m(t) and n(t) be the numbers of infected destinations\nand relays, respectively, at time t. In particular, m(0) = 0\nand n(0) = N0 , and the forwarding process stops at time t if\nm(t) = M . We use mk and nk to mean M (tk \u2212) and N (tk \u2212)\nwhich are the numbers of infected destinations and relays,\nrespectively, just before the meeting epoch tk . Let ek describe\nthe type of the susceptible node that an infected node meets\nat tk ; ek \u2208 E := {d, r} where d and r stand for destination\nand relay, respectively. The state of the system at a meeting\nepoch tk is given by the tuple\nsk := (mk , nk , ek ).\nSince the forwarding process stops at time t if m(t) = M , the\nstate space is [M \u2212 1] \u00d7 [N0 : N ] \u00d7 E.1\n\nLet uk be the action of the infected node at meeting epoch\ntk , k = 1, 2, . . . . The control space is U \u2208 {0, 1}, where 1 is\nfor copy and 0 is for do not copy. The embedding convention\ndescribed above is shown in Figure 1.\n\nWe treat the tuple (\u03b4k+1 , ek+1 ) as the random disturbance\nat epoch tk . Note that for k = 1, 2, . . . , the time between successive decision epochs, \u03b4k , is independent and exponentially\ndistributed with parameter (mk + nk )(M + N \u2212 mk \u2212 nk )\u03bb.\nFurthermore, with \"w.p.\" standing for \"with probability\", we\nhave\n(\nM \u2212mk\nd w.p. pmk ,nk (d) := M +N\n\u2212mk \u2212nk ,\nek =\n\u2212nk\nr w.p. pmk ,nk (r) := M +NN\u2212m\n.\nk \u2212nk\n1) Transition structure: From the description of the system\nmodel, the state at time k + 1 is given by sk+1 = (mk +\nuk , nk , ek+1 ) if ek = d, and sk+1 = (mk , nk + uk , ek+1 )\nif ek = r. Recall that ek+1 is a component in the random\ndisturbance. Thus the next state is a function of the current\nstate, the current action and the current disturbance as required\nfor an MDP .\n2) Cost Structure: For a state-action pair (sk , uk ) the\nexpected single stage cost is given by\n\b\ng(sk , uk ) = \u03b3uk + E \u03b4k+1 1{mk+1 <M\u03b1 } ,\n\n1 We use notation [a] = {0, 1, . . . , a} and [a : b] = {a, a + 1, . . . , b} for\nb \u2265 a + 1 and a, b \u2208 Z+ .\n\nCd (sk , uk ) =\n\n1\n(mk + nk + uk )(M + N \u2212 mk \u2212 nk \u2212 uk )\u03bb\n\nis the mean time until the next decision epoch. The quantity\n\u03b3 is expended whenever uk = 1, i.e., the action is to copy.\n3) Policies: A policy \u03c0 is a sequence of mappings {u\u03c0k , k =\n0, 1, 2, . . . }, where u\u03c0k : [M \u2212 1] \u00d7 [N0 : N ] \u00d7 E \u2192 U. The\ncost of an admissible policy \u03c0 for an initial state s = (m, n, e)\nis\n\u221e\nn\no\nX\nJ\u03c0 (s) =\nE g(sk , u\u03c0k (sk )) s0 = s .\nk=0\n\nLet \u03a0 be the set of all admissible policies. Then the optimal\ncost function is defined as\nJ(s) = min J\u03c0 (s).\n\u03c0\u2208\u03a0\n\nA policy \u03c0 is called stationary if u\u03c0k are identical, say u, for\nall k. For brevity we refer to such a policy as the stationary\npolicy u. A stationary policy u\u2217 \u2261 {u\u2217 , u\u2217 , . . . } is optimal if\nJu\u2217 (s) = J(s) for all states s.\n4) Total Cost: We now translate the optimal cost-to-go from\nthe first meeting instant into optimal total cost. Recall that\nat the first decision instant t1 , the state s1 is (0, N0 , r) or\n(0, N0 , d) depending on whether the susceptible node that is\nmet is a relay or a destination. The objective function (1) can\nthen be restated as\n\u0012\nN \u2212 N0\n1\n+\nE\u03c0 {Td + \u03b3Ec } =\n\u03bbN0 (M + N \u2212 N0 )\nM + N \u2212 N0\n\u0013\nM\nJ\u03c0 (0, N0 ,r) +\nJ\u03c0 (0, N0 , d) ,\n(2)\nM + N \u2212 N0\n\nwhere the subscript \u03c0 shows dependence on the underlying\n1\npolicy. In the right hand side, the first term \u03bbN0 (M +N\n\u2212N0 ) is\nthe average delay until the first decision instant which has to\nbe borne under any policy.\nB. Optimal Policy\n\nSince the cost function g(*) is nonnegative, Proposition 1.1\nin [15, Chapter 3] implies that the optimal cost function will\nsatisfy the following Bellman equation. For s = (m, n, e),\nJ(s) = min A(s, u)\nu\u2208{0,1}\n\nwhere A(s, u) = g(s, u) + E (J(s0 )|s, u) .\nHere s0 denotes the next state which depends on s, u and the\nrandom disturbance in accordance with the transition structure\ndescribed above. The expectation is taken with respect to the\nrandom disturbance. Furthermore, since the action space is\nfinite, there exists a stationary optimal policy u\u2217 such that, for\nall s, u\u2217 (s) attains minimum in the above Bellman equation\n\n\f4\n\n(see [15, Chapter 3]). In the following we characterize this\nstationary optimal policy.\n\nNext, focus on a reduced state space [M\u03b1 \u2212 1] \u00d7 [N0 :\nN ] \u00d7 {r}. Consider the following one step look ahead\npolicy [15, Section 3.4]. At a meeting with a susceptible relay,\nsay when the state is (m, n, r), compare the following two\naction sequences.\n1) 0s: stop, i.e., do not copy to this relay or to any susceptible relays met in the future,\n2) 1s: copy to this relay and then stop.\nThe costs to go corresponding to the action sequences 0s and\n1s are, respectively,\nJ0s (m, n, r) = (M \u2212 m)\u03b3 +\n\nMX\n\u03b1 \u22121\nj=m\n\nJ1s (m, n, r) = (M \u2212 m + 1)\u03b3 +\n\n1\nand\n\u03bb(n + j)(M \u2212 j)\n\nMX\n\u03b1 \u22121\nj=m\n\n1\n.\n\u03bb(n + j + 1)(M \u2212 j)\n\nThe stopping set SS is defined to be\nSS := {(m, n, r) : \u03a6(m, n) \u2264 0}\n\n(3)\n\nwhere\n\u03a6(m, n) := J0s (m, n, r) \u2212 J1s (m, n, r)\n=\n\nMX\n\u03b1 \u22121\nj=m\n\n1\n\u2212\u03b3\n\u03bb(n + j)(n + j + 1)(M \u2212 j)\n\n(4)\n\nfor all (m, n) \u2208 [M\u03b1 \u2212 1] \u00d7 [N0 : N ]. The one step look ahead\npolicy is to copy to relay when (m, n, r) \u2208\n/ SS , and to stop\ncopying otherwise.2\nOne step look ahead policies have been shown to be optimal\nfor stopping problems under certain conditions (see [18, Section 4.4] and [15, Section 3.4]). Let us reemphasize that our\nproblem is not a stopping problem because an action 0 now is\nnot equivalent to stop as the resulting state is not a terminal\nstate; a susceptible relay that is met in the future may be\ncopied even if the one met now is not. However, we exploit\nthe cost structure to prove that when an infected node meets a\nsusceptible relay, it can restrict attention to two actions: 1 (i.e.,\ncopy now) and stop (i.e., do not copy now and never copy\nagain). Subsequently, we also show that the above one step\nlook ahead policy (see (3)) is optimal.\nTheorem 3.1: The optimal policy u\u2217 : [M \u2212 1] \u00d7 [N0 :\n2 We\n\nuse the standard convention that a sum over an empty index set is\n0. Thus \u03a6(m, n) = \u2212\u03b3 if m \u2265 M\u03b1 . Consequently, for the states [M\u03b1 :\nM \u2212 1] \u00d7 [N0 : N ] \u00d7 {r}, one step-look ahead policy prescribes stop. This\nis consistent with our earlier discussion.\n\n40\nn (infected relays)\n\nFirst, observe that it is always optimal to copy to a destination, that is, the optimal policy satisfies u\u2217 (m, n, d) = 1 for\nall (m, n) \u2208 [M \u2212 1] \u00d7 [N0 : N ]. Moreover, once a fraction \u03b1\nof the destinations have obtained the packet, no further delay\ncost is incurred, and so further copying to relays does not help:\nu\u2217 (m, n, r) = 0 for all (m, n) \u2208 [M\u03b1 : M \u2212 1] \u00d7 [N0 : N ].\n\n50\n\n30\n20\n10\n0\n0\n\n5\n\n10\n\n15\n\nm (infected destinations)\nFig. 2. An illustration of the optimal policy. The symbols 'X' mark the\nstates in which the optimal action (at meeting with a relay) is to copy\n\nN ] \u00d7 E \u2192 U satisfies\n\uf8f1\n\uf8f4\n\uf8f21, if e = d,\nu\u2217 (m, n, e) = 1, if e = r and \u03a6(m, n) > 0,\n\uf8f4\n\uf8f3\nstop if e = r and \u03a6(m, n) \u2264 0.\nProof: Though the optimal policy is a simple stopping\npolicy, the proof of its optimality is far from obvious. See\nAppendix A.\nWe illustrate the optimal policy using an example. Let\nM = 15, N = 50, N0 = 10, \u03b1 = 0.8, \u03bb = 0.001 and\n\u03b3 = 1. The \"\u00d7\" in Figure 2 are the states where the optimal\naction (at meeting with a relay) is to copy. For example, if only\n5 destinations have the packet, then relays are copied to if and\nonly if there are 24 or less infected relays. If 7 destinations\nalready have the packet and there are 19 infected relays, then\nno further copying to relays is done.\n\nIV. A SYMPTOTICALLY O PTIMAL E PIDEMIC F ORWARDING\nIn states [M\u03b1 \u2212 1] \u00d7 [N0 : N ] \u00d7 {r}, the optimal action,\nwhich is governed by the function \u03a6(m, n), requires perfect\nknowledge of the network state (m, n). This may not be available to the decision maker due to intermittent connectivity. In\nthis section, we derive an asymptotically optimal policy that\ndoes not require knowledge of network's state but depends\nonly on the time elapsed since the generation of the packet.\nSuch a policy is implementable if the packet is time-stamped\nwhen generated and the nodes' clocks are synchronized.\n\nA. Asymptotic Deterministic Dynamics\nOur analysis closely follows Darling [11]. It is straightforward to show that the equations that follow are the conditional\nexpected drift rates of the optimally controlled CTMC. For\n(m(t), n(t)) \u2208 [M \u2212 1] \u00d7 [N0 : N ], using the optimal policy\n\n\f5\n\nt \u2265 0,\n\nin Theorem 3.1, we get\ndE(m(t)|(m(t), n(t)))\n= \u03bb(m(t) + n(t))(M \u2212 m(t)),\ndt\n\n(5a)\n\ndE(n(t)|(m(t), n(t)))\n= \u03bb(m(t) + n(t))(N \u2212 n(t))\ndt\n1{\u03a6(m(t),n(t))>0} . (5b)\n\ndx(t)\n= f1 (x(t), y(t)) := \u039b(x(t) + y(t))(X \u2212 x(t)), (9a)\ndt\ndy(t)\n= f2 (x(t), y(t)) := \u039b(x(t) + y(t))(Y \u2212 y(t))\ndt\n1{\u03c6(x(t),y(t))>0}\n(9b)\n\nwhere3\nZ\n\nRecalling that K = M + N , the total number of nodes,\nwe study large K asymptotics. Towards this, we consider a\nsequence of problems indexed by K. The parameters of the\nKth problem are denoted using the superscript K. Normalized\nversions of these parameters, and normalized versions of the\nsystem state are denoted as follows:\n\uf8fc\nNK \uf8f4\nMK\n, Y =\n,\uf8f4\nX=\n\uf8f4\nK\nK \uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\u03b1M K\nN0K \uf8f4\n\uf8fd\n, Y0 =\n,\uf8f4\nX\u03b1 =\nK\nK\n(6)\n\u039b K\n\u0393 \uf8f4\n\uf8f4\nK\n\uf8f4\n\u03bb = , \u03b3 = ,\uf8f4\nK\nK \uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nn(t)\nm(t)\nK\nK\nand y (t) =\n.\uf8fe\nx (t) =\nK\nK\nRemarks 4.1: The pairwise meeting rate and the copying\ncost must both scale down as K increases. Otherwise, the\ndelivery delay will be negligible and the total transmission cost\nwill be enormous for any policy, and no meaningful analysis\nis possible.\nFor each K, we define scaled two-dimensional integer\nlattice\n\u001a\u0012\n\u0013\n\u001b\ni j\nK\nK\nK\nK\n\u2206 =\n,\n: (i, j) \u2208 [M \u2212 1] \u00d7 [N0 : N ] .\nK K\n(xK (t), y K (t)) \u2208 \u2206K . Also, for (xK (t), y K (t)) \u2208 \u2206K , using\nthe notation in (6), the drift rates in (5a)-(5b) can be rewritten\nas follows.\ndE(xK (t)|(xK (t), y K (t)))\ndt\n= f1K (xK (t), y K (t))\n:= \u039b(xK (t) + y K (t))(X \u2212 xK (t)), (7a)\n\ndE(y K (t)|(xK (t), y K (t)))\ndt\n= f2K (xK (t), y K (t))\n\n:= \u039b(xK (t) + y K (t))(Y \u2212 y K (t))1{\u03c6K (xK (t),yK (t))>0} ,\n(7b)\n\n\u03c6K (x, y) :=\n\nX\nj=Kx\n\n1\nj\nK\u039b(y + K )(y +\n\nz=x\n\ndz\n\u2212 \u0393.\n\u039b(y + z)2 (X \u2212 z)\n\n(10)\n\nFinally, we redefine the delivery delay Td (see (1)) to be\n\u03c4 K = inf{t \u2265 0 : xK (t) \u2265 X\u03b1 },\n\nand \u03c4 = inf{t \u2265 0 : x(t) \u2265 X\u03b1 }.\n\n(11)\n(12)\n\nNote that \u03c4 K is a stopping time for the random process\n(xK (t), y K (t)), whereas \u03c4 is a deterministic time instant.\nSince f1K (x, y) is bounded away from zero, \u03c4 K < \u221e with\nprobability 1. Similarly, on account of f1 (x, y) being bounded\naway from zero, \u03c4 < \u221e.\nKurtz [10] and Darling [11] studied convergence of CTMCs\nto the solutions of ODEs. The following are the hypotheses for\nthe version of the limit theorem that appears in Darling [11].\n\u0001\n(i) limK\u2192\u221e P k(xK (0), y K (0) \u2212 (x(0), y(0))k > \u000f = 0;\n(ii) In the scaled process (xK (t), y K (t), the jump rates are\nO(K) and drifts are O(K \u22121 );\n(iii) (f1K (x, y), f2K (x, y)) converges to (f1 (x, y), f2 (x, y))\nuniformly in (x, y);\n(iv) (f1 (x, y), f2 (x, y)) is Lipschitz continuous.\nObserve that, in our case, only the first two hypotheses are\nsatisfied. In particular, f2K (x, y) does not converge uniformly\nto f2 (x, y), and f2 (x, y) is not Lipschitz over [0, X\u03b1 ]\u00d7[Y0 , Y ].\nHence, the convergence results do not directly apply in our\ncontext. Thankfully, there is some regularity we can exploit\nwhich we now summarize as easily checkable facts.\n(a) \u03c6K (x, y) converges uniformly to \u03c6(x, y);\n(b) the drift rates f1 (x, y) and f2 (x, y) are bounded from\nbelow and above;\n(c) f1 (x, y) is Lipschitz and f2 (x, y) is locally Lipschitz; and\n(d) for all small enough \u03bd \u2208 R, and all (x, y) on the graph of\n\"\u03c6(x, y) = \u03bd\", the direction in which the ODE progresses,\n(f1 (x, y), f2 (x, y)), is not tangent to the graph.\nWe then prove the following result which is identical to [11,\nTheorem 2.8].\nTheorem 4.1: Assume that \u03b1 < 1 and Y0 > 0. Then, for\nevery \u000f, \u03b4 > 0,\n\u0012\n\u0013\nK\nK\nlim P sup k(x (t), y (t) \u2212 (x(t), y(t))k > \u000f = 0,\nK\u2192\u221e\n0\u2264t\u2264\u03c4\n\u0001\nlim P |\u03c4 K \u2212 \u03c4 | > \u03b4 = 0.\nK\u2192\u221e\n\nwhere, for (x, y) \u2208 \u2206K ,\ndKX\u03b1 e\u22121\n\nX\u03b1\n\n\u03c6(x, y) =\n\nj+1\nK )(X\n\n\u2212\n\nj\nK)\n\n\u2212 \u0393.\n\n(8)\nWe also define (x(t), y(t)) \u2208 [0, X] \u00d7 [Y0 , Y ] as functions\nsatisfying the following ODEs: x(0) = 0, y(0) = Y0 , and for\n\nProof: See Appendix B.\nWe illustrate Theorem 4.1 using an example. Let X =\n0.2, Y = 0.8, \u03b1 = 0.8, Y0 = 0.2, \u039b = 0.05 and \u0393 = 50.\nIn Figure 3, we plot (x(t), y(t)) and sample trajectories of\n3 We use the convention that an integral assumes the value 0 if its lower\nlimit exceeds the upper limit. So, \u03c6(x, y) = \u2212\u0393 if x \u2265 X\u03b1 .\n\n\f6\n\nthe optimal policy u\u2217 as the network grows. Let us restate (2)\nas\n\u0012\n1\nY \u2212 Y0\nEK\n{T\n+\n\u03b3E\n}\n=\n+\nd\nc\n\u03c0\nK\u039bY0 (1 \u2212 Y0 )\n1 \u2212 Y0\n\u0013\nX\nJ\u03c0 (0, Y0 , d) .\nJ\u03c0 (0, Y0 , r) +\n1 \u2212 Y0\n\n0.2\n\nx(t) and xK(t)\n\nx(t)\n0.15\n\nK=100\nK=200\n\n0.1\n\nK=500\n\nWe have used superscript K to show the dependence of\ncost on the network size. We then establish the following\nasymptotically optimality result.\nTheorem 4.2:\n\n0.05\n0\n0\n\n20\n\nt\n\n40\n\n60\n\nK\n\u2217\nlim EK\nu\u2217 {Td + \u03b3Ec } = lim Eu\u221e {Td + \u03b3Ec } = \u03c4 + \u0393y(\u03c4 ).\n\nK\u2192\u221e\n\n0.8\n\ny(t) and yK(t)\n\ny(t)\n0.6\n\nK=100\nK=200\n\n0.4\n\nK=500\n\n0.2\n0\n0\n\n20\n\nt\n\n40\n\n60\n\nFig. 3.\nSimulation results: The top and bottom sub-plots respectively\nshow the fractions of infected destinations and relays as a function of time.\n(xK (t), y K (t)) are obtained from a simulation of the controlled CTMC, and\n(x(t), y(t)) from the ODEs. The marker 'X' indicates the states at which\ncopying to relays is stopped whereas 'O' indicates the states at which a\nfraction \u03b1 of destinations have the packet.\n\n(xK (t), y K (t)) for K = 100, 200 and 500. We indicate the\nstates at which the optimal policy stops copying to relays,\ni.e., \u03a6K (xK (t), y K (t)) goes below 0 (see Theorem 3.1) and\nthe states at which the fraction of infected destinations crosses\nX\u03b1 . We also show the corresponding states in the fluid model.\nThe plots show that for large K, the fluid model captures the\nrandom dynamics of the network very well.\nB. Asymptotically Optimal Policy\nObserve that \u03c6(x, y) is decreasing in x and y, both of\nwhich are nondecreasing with t. Consequently \u03c6(x(t), y(t))\ndecreases with t. We define\n\u03c4 \u2217 := inf{t \u2265 0 : \u03c6(x(t), y(t)) \u2264 0}.\n\n(13)\n\nThe limiting deterministic dynamics suggests the following\npolicy u\u221e for the original forwarding problem.4\n\uf8f1\n\uf8f4\n\uf8f21 if e = d,\n\u221e\nu (m, n, e) = 1 if e = r and t \u2264 \u03c4 \u2217 ,\n\uf8f4\n\uf8f3\n0 if e = r and t > \u03c4 \u2217 .\nWe show that the policy u\u221e is asymptotically optimal in the\nsense that its expected cost approaches the expected cost of\n4 Observe that the policy u\u221e does not require knowledge of m and n. The\ninfected node readily knows the type of the susceptible node (d or r) at the\ndecision epoch.\n\nK\u2192\u221e\n\nProof: See Appendix C.\nRemarks 4.2: Observe that we do not compare the limiting\nvalue of the optimal costs with the optimal cost on the (limiting) deterministic system. In general, these two may differ.5\nHowever, the deterministic policy u\u221e can be applied on the\nfinite K-node system. The content of the above theorem is\nthat given any \u000f > 0, cost of the policy u\u221e is within \u000f of the\noptimal cost on the K-node system for all sufficiently large\nK.\nDistributed Implementation: The asymptotically optimal\npolicy can be implemented in a distributed fashion. Assume\nthat all the nodes are time synchronized.6 Suppose that the\npacket is generated at the source at time t0 (we assumed t0 = 0\nfor the purpose of analysis). Given the system parameters\nM, N, \u03b1, N0 , \u03bb and \u03b3, the source first extracts X, Y, X\u03b1 , Y0 , \u039b\nand \u0393 as in (6). Then, it calculates \u03c4 \u2217 (see (13)), and stores\nt0 + \u03c4 \u2217 as a header in the packet.\nThe packet is immediately copied to N0 relays, perhaps by\nmeans of a broadcast from an infrastructure \"base station\".\nWhen an infected node meets a susceptible relay, it compares\nt0 + \u03c4 \u2217 with the current time. The susceptible relay is not\ncopied to if the current time exceeds t0 + \u03c4 \u2217 . However, all\nthe infected nodes continue to carry the packet, and to copy\nto susceptible destinations as and when they meet.\nRemarks 4.3: Consider a scenario, where the interest is\nin copying packet to only a fraction \u03b1 of the destinations.\nObserve that for every \u000f > 0,\n\u0012\n\u0013\nm(\u03c4 )\n\u2212 \u03b1 > \u000f = 0.\nlim P\nK\u2192\u221e\nM\nThus, in large networks, copying to destinations can also be\nstopped at time \u03c4 (see (12)) while ensuring that with large\nprobability the fraction of infected destinations is close to \u03b1.\nConsequently, all the relays can delete the packet and free\ntheir memory at \u03c4 . This helps when packets are large and\nrelay (cache) memory is limited.\nV. O PTIMAL T WO -H OP F ORWARDING\nInstead of epidemic relaying one can consider two-hop\nrelaying [17]. Here, the N0 source nodes can copy the packet\n5 In\n\nour case these two indeed match. See Appendix D for a proof.\npractice, due to variations in the clock frequency, the clocks at\ndifferent nodes will drift from each other. But the time differences are\nnegligible compared to the delays caused by intermittent connectivity in the\nnetwork. Moreover, when an infected node meets a susceptible node, clock\nsynchronization can be performed before the packet is copied.\n6 In\n\n\f7\n\n0.8\n\nto any of the N \u2212 N0 relays or M destinations. The infected\ndestinations can also copy the packet to any of the susceptible\nrelays or destinations. However, the relays are allowed to\ntransmit the packet only to the destinations. Here also a similar\noptimization problem as in Section II-A arises.\n\n0.6\n\ny\n\nNow, the decision epochs tk , k = 1, 2, . . . are the meeting\nepochs of the infected nodes (sources, relays or destinations)\nwith the susceptible destinations and the meeting epochs of the\nsources or infected destinations with the susceptible relays. We\ncan formulate an MDP with state\nsk := (mk , nk , ek ).\nat instant tk where mk , nk and ek are as defined in Section III-A. The state space is [M\u03b1 \u2212 1] \u00d7 [N0 : N ] \u00d7 E. The\ncontrol space is U \u2208 {0, 1}, where 1 is for copy and 0 is for\ndo not copy. We also get a transition structure identical to that\nin Section III-A.\nFor a state action pair (sk , uk ) the expected single stage\ncost is given by\n\b\ng(sk , uk ) = \u03b3uk + E \u03b4k+1 1{mk+1 <M\u03b1 }\n\uf8f1\n\uf8f4\n\uf8f2\u03b3uk if sk is such that mk \u2265 M\u03b1\n= \u03b3 if sk = (M\u03b1 \u2212 1, n, d) and uk = 1\n\uf8f4\n\uf8f3\n\u03b3uk + Cd (sk , uk ) otherwise,\nwhere\nCd (sk , uk ) =\n1\n(mk + nk + uk )(M \u2212 mk \u2212 uk 1{sk =d} )\u03bb\n\n\u0001\n+ (mk + uk 1{sk =d} + N0 )(N \u2212 nk \u2212 uk 1{sk =r} )\u03bb\n\nis the mean time until the next decision epoch. As before, the\nquantity \u03b3uk accounts for the transmission energy.\nLet u\u2217 : [M\u03b1 \u2212 1] \u00d7 [N0 : N ] \u00d7 E \u2192 U be a stationary\noptimal policy. As in Section III-B, the optimal policy satisfies\nu\u2217 (m, n, d) = 1 for all (m, n) \u2208 [M \u2212 1] \u00d7 [N0 : N ], and\nu\u2217 (m, n, r) = 0 for all (m, n) \u2208 [M\u03b1 : M \u2212 1] \u00d7 [N0 : N ].\nThus, we focus on a reduced state space [M\u03b1 \u2212 1] \u00d7 [N0 :\nN ] \u00d7 {r}. As before, we look for the one step look ahead\npolicy which turns out to be the same as that for epidemic\nrelaying. Finally, Theorem 3.1 holds for two-hop relaying as\nwell (see the proof in Appendix A).\nNext, we turn to the asymptotically optimal control for twohop relaying. The following are the conditional expected drift\nrates. For (m(t), n(t)) \u2208 [M\u03b1 \u2212 1] \u00d7 [N0 : N ],\ndE(m(t)|(m(t), n(t)))\n= \u03bb(m(t) + n(t))(M \u2212 m(t)),\ndt\ndE(n(t)|(m(t), n(t)))\n= \u03bb(m(t) + N0 )(N \u2212 n(t))\ndt\n1{\u03a6(m(t),n(t))>0} .\n\nWe employ the same scaling and notations as in (6). The drift\n\nepidemic\ntwo\u2212hop\n\n0.4\n\n0.2\n\n0\n0\n\n0.05\n\n0.1\nx\n\n0.15\n\n0.2\n\nFig. 4. An illustration of the epidemic and two hop trajectories. The plots\nalso show the graph of '\u03c6(x, y) = 0'.\n\nrates in terms of (xK (t), y K (t)) \u2208 [0, X\u03b1 ] \u00d7 [Y0 , Y ] are\n\ndE(xK (t)|(xK (t), y K (t)))\n= f1K (xK (t), y K (t))\ndt\n:= \u039b(xK (t) + y K (t))(X \u2212 xK (t)),\n\ndE(y K (t)|(xK (t), y K (t)))\n= f2K (xK (t), y K (t))\ndt\n:= \u039b(xK (t) + Y0 )(Y \u2212 y K (t))1{\u03c6K (xK (t),yK (t))>0} ,\n\nNow, x(t), y(t) are defined as functions satisfying x(0) =\n0, y(0) = Y0 and for t \u2265 0,\ndx(t)\n= f1 (x(t), y(t)) := \u039b(x(t) + y(t))(X \u2212 x(t)),\ndt\ndy(t)\n= f2 (x(t), y(t)) := \u039b(x(t) + Y0 )(Y \u2212 y(t))\ndt\n1{\u03c6(x(t),y(t))>0}\n\nThe analysis in Section IV applies to two-hop relaying as\nwell. In particular, Theorems 4.1 and 4.2 hold. However, for\nthe identical system parameters (M, N, \u03b1, \u03bb and \u03b3) and initial\nstate (N0 ), the value of the time-threshold \u03c4 \u2217 will be larger\non account of the slower rates of infection of relays and\ndestinations.\nWe illustrate the comparison between epidemic and twohop relaying using an example. Let X = 0.2, Y = 0.8, \u03b1 =\n0.8, Y0 = 0.2, \u039b = 0.05 and \u0393 = 50. In Figure 4, we plot\nthe graph of \"\u03c6(x, y) = 0\", and also the 'y versus x' trajectories corresponding to epidemic and two-hop relayings. In\nFigure 5, we plot the trajectories of (x(t), y(t)) corresponding\nto epidemic and two-hop relayings. As anticipated, the value\nof the time-threshold \u03c4 \u2217 is larger for two-hop relaying than\nepidemic relaying. Moreover, the number of transmissions is\nless while the deliverly delay is more under the controlled\ntwo-hop relaying.\nVI. N UMERICAL R ESULTS\nWe now show some numerical results to demonstrate the\ngood performance of the deterministic control in epidemic\nforwarding in a DTN with multiple destinations. Let X =\n0.2, Y = 0.8, \u03b1 = 0.8, Y0 = 0.2 and \u03b3 = 0.5. We vary\n\u03bb from 0.00005 to 0.05 and use K = 50, 100 and 200. In\nFigure 6, we plot the total number of copies to relays and\n\n\f0.2\n\nepidemic\ntwo\u2212hop\n\nx(t)\n\n0.15\n\n0.1\n\n0.05\n\n0\n0\n\n0.8\n\n20\n\nt\n\n40\n\n60\n\nepidemic\ntwo\u2212hop\n\nmean delivery delay\n\ny(t)\n\ndeterministic policy\noptimal policy\n\n150\n100\n\nK = 200\nK = 100\n\n50\n0\n\nK = 50\n\u03bb\n\n4\n\n0.4\n\n0.2\n\n20\n\n200\n\n10\n\n0.6\n\n0\n0\n\nmean number of copies to relays\n\n8\n\nt\n\n40\n\n60\n\nFig. 5. The top and bottom sub-plots respectively show the fractions of\ninfected destinations and relays as a function of time. The marker 'X' indicates\nthe states at which copying to relays is stopped, and 'O' indicates the states\nat which \u03b1 fraction of destinations have been copied.\n\nthe delivery delays corresponding to both the optimal and\nthe asymptotically optimal deterministic policies. Evidently,\nthe deterministic policy performs close to the optimal policy\non both the fronts. We observe that, for a fixed K, both the\nmean delivery delay and the mean number of copies to relays\ndecrease as \u03bb increases. We also observe that, for a fixed \u03bb,\nthe mean delivery delay decreases as the network size grows.\nFinally, for smaller values of \u03bb, the mean number of copies to\nrelays increases with the network size, and for larger values\nof \u03bb, the opposite happens.\nVII. C ONCLUSION\nWe studied the epidemic forwarding in DTNs, formulated\nthe problem as a controlled continuous time Markov chain,\nand obtained the optimal policy (Theorem 3.1). We then\ndeveloped an ordinary differential equation approximation for\nthe optimally controlled Markov chain, under a natural scaling,\nas the population of nodes increases to \u221e (Theorem 4.1).\nThis o.d.e. approximation yielded a forwarding policy that\ndoes not require global state information (and, hence, is\nimplementable), and is asymptotically optimal (Theorem 4.2).\nThe optimal forwarding problem can also be addressed\nfollowing the result of Gast et al. [14]. They study a general discrete time Markov decision process (MDP) [15].\nHowever, they do not solve the finite problem citing the\ndifficulties associated with obtaining the asymptotics of the\n\n3\n\n10\n\n2\n\n10\n\ndeterministic policy\noptimal policy\nK = 50\nK = 100\nK = 200\n\n1\n\n10\n\n0\n\n10\n\n\u03bb\n\nFig. 6. The top and bottom sub-plots, respectively, show the total number\nof copies to relays and the delivery delays corresponding to both the optimal\nand the deterministic policies.\n\noptimally controlled process (see [14, Section 3.3]). Instead,\nthey consider the fluid limit of the MDP, and analyze optimal\ncontrol over the deterministic limiting problem. They then\nshow that the optimal reward of the MDP converges to the\noptimal reward of its mean field approximation, given by the\nsolution to a Hamilton-Jacobi-Bellman (HJB) equation [18,\nSection 3.2]. On the other hand, our approach is more direct.\nWe have a continuous time controlled Markov chain at our\ndisposal We explicitly characterize the optimal policy for the\nfinite (complete information) problem, and prove convergence\nof the optimally controlled Markov chain to a fluid limit. An\nasymptotically optimal deterministic control is then suggested\nby the limiting deterministic dynamics, and does not require\nsolving HJB equations. Our notion of asymptotic optimality\nis also stronger in the sense that we apply both the optimal\npolicy and the deterministic policy to the finite problem, and\nshow that the corresponding costs converge.\nThere are several directions in which this work can be\nextended. In the same DTN framework, there could be a\ndeadline on the delivery time of the packet (or message); the\ngoal of the optimal control could be to maximize the fraction\nof destinations that receive the packet before the deadline\nsubject to an energy constraint. Our work in this paper assumes\nthat network parameters such as M, N, \u03bb etc., are known; it\nwill be important to address the adaptive control problem when\nthese parameters are unknown.\n\n\f9\n\nA PPENDIX A\nP ROOF OF T HEOREM 3.1\nWe first prove that for the optimal policy it is sufficient to\nconsider two actions 1 (i.e., copy now) and stop (i.e., do not\ncopy now and never copy again). More precisely, under the\noptimal policy, if a susceptible relay that is met is not copied,\nthen no susceptible relay is copied in the future as well. Let\nus fix a N0 \u2264 n \u2264 N \u2212 1. Let m\u2217n be the maximum j such\nthat u\u2217 (j, n, r) = 1.7 We show that u\u2217 (j, n, r) = 1 for all\n0 \u2264 j < m\u2217n ; see Figure 2 for an illustration of this fact. The\nproof is via induction.\nProposition A.1: If u\u2217 (j, n, r) = 1 for all m+1 \u2264 j \u2264 m\u2217n ,\nthen u\u2217 (m, n, r) = 1.\nProof: Define\n\u03c8(m, n) := J0s (m, n, r) \u2212 J(m, n, r),\n\nBoth the action sequences that give rise to the two cost terms in\nthe definition of \u03b80 (m, n), do not copy to the susceptible relay\nthat was just met. Let j be the number of infected destinations\nat the next decision epoch when a susceptible relay is met; j\ncan be m, m + 1, ..., M . All interim decision epochs must\nbe meetings with susceptible destinations, and both policies\ncopy at these meetings. Hence, both policies incur the same\ncost until this epoch, and differ by \u03c8(j, n) in the costs to\ngo (from this epoch onwards). Averaging the difference over\nj, and noting that \u03c8(j, n) = 0 for j > M\u03b1 \u2212 1, we get8\n!\nj\u22121\nMX\n\u03b1 \u22121\nY\npl,n (d) pj,n (r)\u03c8(j, n).\n(14)\n\u03b80 (m, n) =\nl=m\n\nSince A((m, n, r), 0) \u2265 J(m, n, r), it follows that \u03c8(m, n) \u2265\n\u03b80 (m, n), and so\n!\nj\u22121\nMX\n\u03b1 \u22121\nY\npl,n (d) pj,n (r)\u03c8(j, n)\n\u03c8(m, n) \u2265\nl=m\n\nwhere\nMX\n\u03b1 \u22121\n\nj\u22121\nY\n\nj=m+1\n\nl=m+1\n\n\u03b81 (m+1, n) =\n\n!\npl,n+1 (d) pj,n+1 (r)\u03c8(j, n+1).\n\nThus it suffices to show that\n\u03c8(m, n + 1) \u2265 \u03b81 (m + 1, n).\n\n\u03c8(j, n) = J0s (j, n, r) \u2212 min{A((j, n, r), 0), A((j, n, r), 1)}\n= max{\u03b80 (j, n), \u03a6(j, n) + \u03b81 (j, n)}.\n\nMX\n\u03b1 \u22121\n\nj\u22121\nY\n\nj=m+1\n\nl=m+1\n\n!\npl,n (d) pj,n (r)\u03c8(j, n)\n\n\u03c8(j, n) = \u03a6(j, n) + \u03b81 (j, n).\nFinally, \u03c8(j, n) = 0 for all m\u2217n < j \u2264 M\u03b1 \u2212 1 as the optimal\npolicy does not copy in these states. Hence, from (14),\n\u03b80 (m, n)\n= pm,n (r) max{\u03b80 (m, n), \u03a6(m, n) + \u03b81 (m, n)} + pm,n (d)\n!\nm\u2217\nj\u22121\nn\nX\nY\n\u0001\n\u00d7\npl,n (d) pj,n (r) \u03a6(j, n) + \u03b81 (j, n)\nj=m+1\n\nl=m+1\n\n< pm,n (r) max {\u03b80 (m, n), \u03a6(m, n) + \u03b81 (m, n)} + pm,n (d)\n!\nm\u2217\nj\u22121\nn\nY\n\u0001 X\n\u00d7 \u03a6(m, n) + \u03b81 (m, n)\npl,n (d) pj,n (r)\nl=m+1\n\n\u2264 pm,n (r) max {\u03b80 (m, n), \u03a6(m, n) + \u03b81 (m, n)}\n\u0001\n+ pm,n (d) \u03a6(m, n) + \u03b81 (m, n)\n\b\n\u0001\n= max pm,n (r)\u03b80 (m, n) + pm,n (d) \u03a6(m, n) + \u03b81 (m, n) ,\n\u03a6(m, n) + \u03b81 (m, n)} ,\n\nwhich implies upon rearrangement\n\u03c8(m, n) \u2265\n\nMX\n\u03b1 \u22121\n\nj\u22121\nY\n\nj=m+1\n\nl=m+1\n\n!\npl,n (d) pj,n (r)\u03c8(j, n)\n\n(15)\n\nNext, we establish the following lemma.\nLemma A.1: \u03b81 (m, n) \u2265 \u03b81 (m + 1, n).\nProof: Note that both the action sequences that lead to\nthe two cost terms in the definition of \u03b81 (m, n) copy at state\n(m, n, r). Subsequently, both incur equal costs until a decision\nthat, for a given n, m\u2217n could be 0, in that case we do not copy to\nany more relays.\n8 We use the standard convention that a product over an empty index set is\n1, which happens when j = m.\n7 Note\n\n(16)\n\nMoreover, from the induction hypothesis, the optimal policy\ncopies at states (j, n, r) for all m + 1 \u2264 j \u2264 m\u2217n . Hence, for\nm + 1 \u2264 j \u2264 m\u2217n ,\n\nj=m+1\n\n= pm,n (r)\u03c8(m, n)\n+ pm,n (d)\n\nl=m\n\nNext, observe that for all m \u2264 j \u2264 m\u2217n ,\n\nand \u03b81 (m, n) := J1s (m, n, r) \u2212 A((m, n, r), 1).\n\nj=m\n\nj=m\n\n= pm,n+1 (r)\u03c8(m, n + 1) + pm,n+1 (d)\u03b81 (m + 1, n)\n\nwhich is same as (15) with n replaced by n + 1.\n\n\u03b80 (m, n) := J0s (m, n, r) \u2212 A((m, n, r), 0),\n\nj=m\n\nepoch when an infected node meets a susceptible relay. Also,\nat any such state (j, n + 1, r), j \u2265 m, the costs to go differ\nby \u03c8(j, n + 1). Hence,\n!\nj\u22121\nMX\n\u03b1 \u22121\nY\n\u03b81 (m, n) =\npl,n+1 (d) pj,n+1 (r)\u03c8(j, n + 1)\n\n(17)\n\nwhere the first (strict) inequality holds because \u03a6(m, n) is\nstrictly decreasing (see (4)) and \u03b81 (m, n) is decreasing (see\nLemma A.1) in m for fixed n. The second inequality follows\nbecause the summation term is a probability which is less than\n1. Now suppose that \u03b80 (m, n) \u2265 \u03a6(m, n) + \u03b81 (m, n). Then\n\b\n\u0001\nmax pm,n (r)\u03b80 (m, n) + pm,n (d) \u03a6(m, n) + \u03b81 (m, n) ,\n\u03a6(m, n) + \u03b81 (m, n)}\n= pm,n (r)\u03b80 (m, n) + pm,n (d) \u03a6(m, n) + \u03b81 (m, n)\n\u2264 \u03b80 (m, n)\nwhich contradicts (17). Thus, we conclude that\n\u03b80 (m, n) < \u03a6(m, n) + \u03b81 (m, n).\n\n\u0001\n\n\f10\n\nThis further implies that \u03c8(m, n) = \u03a6(m, n) + \u03b81 (m, n)\n(see (16)), and so that u\u2217 (m, n, r) = 1.\nWe now return to the proof of Theorem 3.1. We show that\nthe one-step look ahead policy is optimal for the resulting\nstopping problem. To see this, observe that \u03a6(m, n) is decreasing in m for a given n and also decreasing in n for a\ngiven m. Thus, if (m, n, r) \u2208 SS , i.e, \u03a6(m, n) \u2264 0 (see (3)),\nand the susceptible relay that is met is copied, the next\nstate (m, n + 1, r) also belongs to the stopping set SS . In\nother words, SS is also an absorbing set [15, Section 3.4]).\nConsequently, the one-step look ahead policy is an optimal\npolicy.\n\nwhere the first and the last inequalities follow from the\n0\ndefinitions of fy (z) and fmax\nrespectively. On the other hand,\n1\n(y + z)(y + z +\n\nHence\nZ\n\n1\nz+ K\n\nfy (v)dv \u2212\n\nz\n\n\u2264\n\u2264\n\nA PPENDIX B\nP ROOF OF T HEOREM 4.1\n\n1\nK )(X\n\n\u2264\n\nZ\n\ny+z\n1\ny+z+ K\nY0\n\u2265 fy (z)\n1 .\nY0 + K\n\n= fy (z)\n\n1\nK(y + z)(y + z +\n\n1\nz+ K\n\nfy (v)dv \u2212\n\nz\n\nZ\n\n\u2212 z)\n\nfy (z) KY0\nK 1 + KY0\n\n1\nz+ K\n\nz\n0\nfmax\nK2\n\n(fy (v) \u2212 fy (z))dv +\n+\n\n1\nK )(X\n\n\u2212 z)\n\nfy (z)\nK(1 + KY0 )\n\nfmax\n.\nK(1 + KY0 )\n\n(19)\n\nCombining (18) and (19),\n1\nK(y + z)(y + z +\n\nWe start with a preliminary result and a few definitions.\nK\n\nProposition B.1: Let \u03b1 < 1 and Y0 > 0. Let \u03c6 and \u03c6\nbe as given in (8) and (10), respectively. Then, the functions\n\u03c6K (*) converge to \u03c6(*) uniformly, i.e., for every \u03bd > 0, there\nexists a K\u03bd such that\nsup\n(x,y)\u2208\u2206K\n\nK\n\n|\u03c6 (x, y) \u2212 \u03c6(x, y)| < \u03bd\n\nClearly, the family {fy } is positive and uniformly upper\nbounded. Indeed,\n1\nfy (z) \u2264 fmax := 2\n.\nY0 (X \u2212 X\u03b1 )\nFurther,\n\n\u0012\n\n1\n2\n\u2212\nX \u2212z\ny+z\n\nfy (v)dv\nz\n\nNow fix a (x, y) \u2208 \u2206K . Setting z = j/K, and summing over\nj \u2208 [Kx : dKX\u03b1 e \u2212 1], we get\n1\n1\nj\nj\n\u039b K(y + K )(y + j+1\nK )(X \u2212 K )\nj=Kx\nZ X\u03b1\nZ j+1\nK\n1\nfy (v)dv +\nfy (v)dv\n\u2212\nj\n\u03b1e\n\u039b dKX\nK\nK\n\u0012 0\n\u0013\nK(X\u03b1 \u2212 x) fmax\nfmax\nfmax\n\u2264\n+\n+\n2\n\u039b\nK\nK(1 + KY0 )\nK\u039b\n0\nX\u03b1 fmax\nX\u03b1 fmax\nfmax\n\u2264\n+\n+\n.\nK\u039b\n(1 + KY0 )\u039b\nK\u039b\n\u2264\n\nX\n\nThe obtained upper bound on the right-hand side is independent of (x, y) \u2208 \u2206K , and vanishes as K \u2192 \u221e. Thus, for\nevery \u03bd > 0, there exists a K\u03bd such that\n\n\u0013\n,\n\nfrom which it can be seen that\ndfy (z)\n0\n\u2264 fmax\ndz\n\nsup\n(x,y)\u2208\u2206K\n\n0\nwhere fmax\nis a suitably defined constant. So the family {fy }\nis uniformly Lipschitz. Now, for (z, y) \u2208 [0, X\u03b1 ] \u00d7 [Y0 , Y ],\nZ z+ K1\n1\n\u2212\nfy (v)dv\n1\nK(y + z)(y + z + K\n)(X \u2212 z)\nz\nZ z+ K1\nfy (z)\n\u2212\nfy (v)dv\n\u2264\nK\nz\nZ z+ K1\n\u2264\n(fy (z) \u2212 fy (v))dv\n\n\u2264\n\n1\nz+ K\n\ndKX\u03b1 e\u22121\n\nProof: For a y \u2208 [Y0 , Y ], define fy : [0, X\u03b1 ] \u2192 R+ as\nfollows.\n1\n.\nfy (z) =\n(y + z)2 (X \u2212 z)\n\nz\n0\nfmax\nK2\n\n0\nfmax\nfmax\n.\n+\nK2\nK(1 + KY0 )\n\n\u2212 z)\n\nZ\n\n|\u03c6K (x, y) \u2212 \u03c6(x, y)|\n\nfor all K \u2265 K\u03bd .\n\ndfy (z)\n1\n=\ndz\n(y + z)2 (X \u2212 z)\n\n\u2264\n\n1\nK )(X\n\n\u2212\n\n(18)\n\n|\u03c6K (x, y) \u2212 \u03c6(x, y)| < \u03bd\n\nfor all K \u2265 K\u03bd .\n\nIn the following, to facilitate a parsimonious description, we\nuse the notation z K (t) = (xK (t), y K (t)), z(t) = (x(t), y(t))\nand Z = [0, X\u03b1 ] \u00d7 [Y0 , Y ]. Let us define, for a \u03bd \u2208 R,\nS\u03bd = {z \u2208 Z : \u03c6(z) > \u03bd},\n\n\u03c4\u03bd = inf{t \u2265 0 : z(t) \u2208\n/ S\u03bd },\n\nand a stopping time\n\u03c4\u03bdK = inf{t \u2265 0 : z K (t) \u2208\n/ S\u03bd },\n\n\f11\n\nTABLE I\nVARIABLES AND THEIR DESCRIPTION\n\nthe time when z K (t) exits the limiting set S\u03bd . Observe that\n1\n1\n\u2202\u03c6\n=\u2212\n\u2264\u2212\n\u2202x\n\u039b(x + y)2 (X \u2212 x)\n\u039b(X\u03b1 + Y )2 X\n\n(20)\n\nand f1K (x, y) defined in (7a) is positive and is also bounded\naway from zero. These imply that \u03c4\u03bdK < \u221e with probability\n1. Similarly, \u03c4\u03bd < \u221e. The following assertion is a corollary\nof Proposition B.1.\nCorollary B.1: Let K\u03bd be as in Proposition B.1. For K \u2265\nK\u03bd ,\n\u03c6K (z) > 0 for all z \u2208 S\u03bd ,\n\nand \u03c6K (z) \u2264 0 for all z \u2208\n/ S\u2212\u03bd .\nWe define the uncontrolled dynamics (i.e., the one in which\nthe susceptible relays are always copied) as a Markov process\nz\u0304 K (t) = (x\u0304K (t), \u0233 K (t)), t \u2265 0 for which z\u0304 K (0) = z K (0).\nLet z\u0304(t) = (x\u0304(t), \u0233(t)), t \u2265 0 be the corresponding limiting\ndeterministic dynamics. Formally, z\u0304(0) = z(0), and for t \u2265 0,\n\ndx\u0304(t)\n= \u039b(x\u0304(t) + \u0233(t))(X \u2212 x\u0304(t)),\ndt\nd\u0233(t)\n= \u039b(x\u0304(t) + \u0233(t))(Y \u2212 \u0233(t)).\ndt\nThe quantities on the right-hand side of the above equations\nare at most \u039b, and so\n\u221a\ndz\u0304\n\u2264 2\u039b.\ndt\n\nAlso observe that the processes z\u0304 K (t) and z\u0304(t) satisfy the\nhypotheses of Darling [11] (see Section IV-A), and thus\nconvergence of z\u0304 K (t) to z\u0304(t) follows.\nWe also define a Markov process z\u0303 K (t) = (x\u0303K (t), \u1ef9 K (t)),\nt \u2265 \u03c4\u03bd for which z\u0303 K (\u03c4\u03bd ) = z K (\u03c4\u03bd ) and\ndE(x\u0303K (t)|(x\u0303K (t), \u1ef9 K (t))\n= \u039b(x\u0303K (t) + \u1ef9 K (t))(X \u2212 x\u0304K (t))\ndt\ndE(y K (t)|(xK (t), y K (t))\n=0\ndt\n\nIn other words, z\u0303 K (t) is the process in which relays are\nnot copied from \u03c4\u03bd onwards. Similarly, we define z\u0303(t) =\n(x\u0303(t), \u1ef9(t)), t \u2265 \u03c4\u03bd as the solution of the corresponding\ndifferential equations. In other words, z\u0303(\u03c4\u03bd ) = z(\u03c4\u03bd ), and for\nt \u2265 \u03c4\u03bd ,\ndx\u0303(t)\n= f1 (x\u0303(t), \u1ef9(t)) := \u039b(x\u0303(t) + \u1ef9(t))(X \u2212 x\u0303(t)),\ndt\nd\u1ef9(t)\n= f2 (x\u0303(t), \u1ef9(t)) := 0\ndt\nWe define\nK\n\u03c4\u0303\u2212\u03bd\n= inf{t \u2265 \u03c4\u03bd : z\u0303 K (t) \u2208\n/ S\u2212\u03bd },\n\n\u03c4\u0303\u2212\u03bd = inf{t \u2265 \u03c4\u03bd : z\u0303(t) \u2208\n/ S\u2212\u03bd }.\nSince\n\ndx\u0303\n\u039bY0 (X \u2212 X\u03b1 ) \u2264\n\u2264 \u039b,\ndt\nthe lower bound implies that there is a strictly positive increase\nin x\u0303 after time \u03c4\u03bd . Since \u03a6(x, y) decreases with increasing x\n\nvariables\nK\n\ndescription\n\nz (t)\n\ncontrolled dynamics with discontinuity at \u03c4 K\n\nz(t)\n\nz K (t)'s fluid limit with discontinuity at \u03c4 \u2217\n\n\u03c4\u03bdK\n\ninstant when z K (t) exits S\u03bd\n\n\u03c4\u03bd\nK\n\nz\u0304 (t)\n\ninstant when z(t) exits S\u03bd\n\nuncontrolled dynamics with no discontinuity\n\nz\u0304(t)\n\nz\u0304 K (t)'s fluid limit with no discontinuity\n\nz\u0303 K (t)\n\nidentical to z K (t) until \u03c4\u03bd at which copying to\nrelays is stopped\n\nz\u0303(t)\n\nz\u0303 K (t)'s fluid limit with discontinuity at \u03c4\u03bd\n\nK\n\u03c4\u0303\u2212\u03bd\n\ninstant when z\u0303 K (t) exits S\u2212\u03bd\ninstant when z\u0303(t) exits S\u2212\u03bd\n\n\u03c4\u0303\u2212\u03bd\n\nat a rate bounded away from 0 (see 20), z\u0303(t) must exit S\u2212\u03bd\nwithin a short additional duration. Thus, we have that\n\u03c4\u0303\u2212\u03bd \u2212 \u03c4\u03bd \u2264 b\u03bd\nfor a suitably chosen b < \u221e.\nTo aid the reader, we summarize the variables used in\nTable I. We also illustrate sample trajectories of a controlled\nCTMC and the corresponding ODE via an example (Figure 7).\nWe choose M = 40, N = 160, \u03b1 = 0.8, N0 = 40, \u03bb =\n0.00025 and \u03b3 = 0.25. We plot the graphs of '\u03c6(x, y) = \u03bd'\nand '\u03c6(x, y) = \u2212\u03bd' for \u03bd = 0.2. We also show the trajectories\n\"y K vs xK \", \"y vs x\", \"\u1ef9 vs x\u0303\" and the epochs \u03c4\u03bd , \u03c4\u2212\u03bd and\n\u03c4\u0303\u2212\u03bd .\nWe prove the assertion in Theorem 4.1 in three steps:\n(a) over [0, \u03c4\u03bd ], (b) over [\u03c4\u03bd , \u03c4\u0303\u2212\u03bd ] and (c) over [\u03c4\u0303\u2212\u03bd , \u03c4 ].\nHowever, we also need the following lemmas in our proof..\nLemma B.1: For every \u000f > 0, there exists a \u03c4\u0304\u000f such that for\n\n0.8\n\u03c6(x,y)=\u2212\u03bd\n\n0.7\n\n~\n\u03c4\u2212\u03bd\u03c4\u2212\u03bd\n\n0.6\n0.5\n\n\u03c6(x,y)=\u03bd\n\n0.4\n\n\u03c4\u03bd\n\n0.3\n0.2\n\ny vs x\n\nyK vs xK\ny vs x\n\n0.1\n0\n0\n\n0.05\n\n0.1\n\n0.15\n\nFig. 7. An illustration of the trajectories of the controlled CTMC and the\ncorresponding ODE, and the associated variables.\n\n\f12\n\nall t \u2265 0, 0 \u2264 u \u2264 \u03c4\u0304\u000f ,\n\nrem 2.8] we have, for all \u000f, \u03b4 > 0,\n\u0012\n\u0013\nK\nK\nP\nsup kz (t \u2227 \u03c4\u03bd ) \u2212 z(t)k > \u000f = O(K \u22121 )\n\n\u0001\nP kz\u0304 (t + u) \u2212 z\u0304 (t)k > \u000f = O(K \u22121 ).\nK\n\nK\n\n0\u2264t\u2264\u03c4\u03bd\nand P(|\u03c4\u03bdK\n\nProof: Observe that\nK\n\nK\n\nkz\u0304 (t + u) \u2212 z\u0304 (t)k\n\n+ kz\u0304(t) \u2212 z\u0304(t + u)k\n\u221a\nK\nK\n\u2264 kz\u0304 (t) \u2212 z\u0304(t)k + kz\u0304 (t + u) \u2212 z\u0304(t + u)k + 2\u039bu\n\nHence, for all t \u2265 0, u \u2265 0,\n\u0010\n\u221a\n\u000f\u0011\nP kz\u0304 K (t + u) \u2212 z\u0304 K (t)k > 2\u039bu +\n2\n\u0010\n\u000f\u0011\nK\nK\n\u2264 P kz\u0304 (t) \u2212 z\u0304(t)k + kz\u0304 (t + u) \u2212 z\u0304(t + u)k >\n2\n\u0012\n\u0013\n\u000f\n\u2264P\nsup kz\u0304 K (s) \u2212 z\u0304(s)k >\n4\nt\u2264s\u2264t+u\n\nProof: Fix a \u03b4 > 0. Then,\nP(kz\u0304 K (u) \u2212 z\u0304 K (u \u2227 uK )k > \u000f)\n\nP kz\u0304 K (u) \u2212 z\u0304 K (u \u2227 uK )k > \u000f u \u2212 uK \u2264 \u03b4\n\n\u0001\n\n\u2264 O(K \u22121 ) + P kz\u0304 K (u) \u2212 z\u0304 K (u \u2227 uK )k > \u000f u \u2212 uK \u2264 \u03b4\n\u0001\n\u2264 O(K \u22121 ) + P kz\u0304 K (u) \u2212 z\u0304 K (u \u2212 \u03b4)k > \u000f u \u2212 uK \u2264 \u03b4\n\n\u0001\n\nwhere the last inequality holds because z\u0304 K (t) is a monotone\nincreasing function. Setting \u03b4 = \u03c4\u0304\u000f (see Lemma B.1),\nP(kz\u0304 K (u) \u2212 z\u0304 K (u \u2227 uK )k > \u000f)\n\n\u2264 O(K \u22121 ) + P kz\u0304 K (u) \u2212 z\u0304 K (u \u2212 \u03c4\u0304\u000f )k > \u000f u \u2212 uK \u2264 \u03c4\u0304\u000f\n\u0001\n= O(K \u22121 ) + P kz\u0304 K (u) \u2212 z\u0304 K (u \u2212 \u03c4\u0304\u000f )k > \u000f\n\nsup kz K (t) \u2212 z(t)k \u2264\n\nsup kz K (t \u2227 \u03c4\u03bdK ) \u2212 z(t)k\n\n0\u2264t\u2264\u03c4\u03bd\n\n0\u2264t\u2264\u03c4\u03bd\n\nLemma B.2: Suppose u is a fixed\nuK is a random\n\u0001 time and\nK\n\u22121\ntime that satisfies P |u \u2212 u | > \u03b4 = O(K ) for every \u03b4 >\n0. Then, for every \u000f > 0,\n\u0001\nP kz\u0304 K (u) \u2212 z\u0304 K (u \u2227 uK )k > \u000f = O(K \u22121 )\n\n\u0001\n\nwe obtain\n\n+ sup kz K (t) \u2212 z K (t \u2227 \u03c4\u03bdK )k.\n\nwhere the last equality follows from [11, Theorem 2.8]. Setting\n\u03c4\u0304\u000f = 2\u221a\u000f2\u039b , for all t \u2265 0, 0 \u2264 u \u2264 \u03c4\u0304\u000f\n\u0001\nP kz\u0304 K (t + u) \u2212 z\u0304 K (t)k > \u000f\n\u0010\n\u221a\n\u000f\u0011\n\u2264 P kz\u0304 K (t + u) \u2212 z\u0304 K (t)k > 2\u039bu +\n2\n= O(K \u22121 ).\n\nP kz\u0304 K (u) \u2212 z\u0304 K (u \u2227 uK )k > \u000f u \u2212 uK > \u03b4\n\nkz K (t)\u2212z(t)k \u2264 kz K (t\u2227\u03c4\u03bdK )\u2212z(t)k+kz K (t)\u2212z K (t\u2227\u03c4\u03bdK )k,\n\n0\u2264t\u2264\u03c4\u03bd\n\n= O(K \u22121 )\n\n+ P(u \u2212 uK \u2264 \u03b4)\n\n(21b)\n\nSince, for all t \u2265 0,\n\n\u2264 kz\u0304 K (t) \u2212 z\u0304(t)k + kz\u0304 K (t + u) \u2212 z\u0304(t + u)k\n\n= P(u \u2212 uK > \u03b4)\n\n\u2212 \u03c4\u03bd | > \u03b4) = O(K \u22121 ).\n\n(21a)\n\n\u0001\n\nIf the left side is larger than \u000f, at least one of the two terms on\nthe right side is larger than \u000f/2, and so by the union bound,\nwe get\n\u0012\n\u0013\nP\nsup kz K (t) \u2212 z(t)k > \u000f\n0\u2264t\u2264\u03c4\u03bd\n\u0013\n\u0012\n\u000f\n\u2264P\nsup kz K (t \u2227 \u03c4\u03bdK ) \u2212 z(t)k >\n2\n0\u2264t\u2264\u03c4\u03bd\n\u0012\n\u0013\n\u000f\n+P\nsup kz K (t) \u2212 z K (t \u2227 \u03c4\u03bdK )k >\n2\n0\u2264t\u2264\u03c4\u03bd\n\u0010\n\u000f\u0011\n\u2264 O(K \u22121 ) + P kz K (\u03c4\u03bd ) \u2212 z K (\u03c4\u03bd \u2227 \u03c4\u03bdK )k >\n(22)\n2\nwhere the first term in the last inequality follows from (21a).\nAlso, from corollary B.1, for K \u2265 K\u03bd , \u03c6K (z K (\u03c4\u03bdK )\u2212) > 0,\ni.e., the process z K (t) follows uncontrolled dynamics until\n\u03c4\u03bdK . Thus, for K \u2265 K\u03bd , z K (\u03c4\u03bdK ) = z\u0304 K (\u03c4\u03bdK ) and\nkz K (\u03c4\u03bd ) \u2212 z K (\u03c4\u03bd \u2227 \u03c4\u03bdK )k \u2264 kz\u0304 K (\u03c4\u03bd ) \u2212 z\u0304 K (\u03c4\u03bd \u2227 \u03c4\u03bdK )k\nsample path wise. The inequality is an equality if \u03c4\u03bd \u2264 \u03c4\u03bdK ;\nboth sides equal 0 in this case. Otherwise, it is an inequality\nbecause the possible change in dynamics of z K (t) after \u03c4\u03bdK\nmakes it increase (in both its components) at a slower pace\nthan the uncontrolled z\u0304 K (t). Thus\n\u0010\n\u000f\u0011\nP kz K (\u03c4\u03bd ) \u2212 z K (\u03c4\u03bd \u2227 \u03c4\u03bdK )k >\n2\n\u0010\n\u000f\u0011\nK\nK\nK\n\u2264 P kz\u0304 (\u03c4\u03bd ) \u2212 z\u0304 (\u03c4\u03bd \u2227 \u03c4\u03bd )k >\n2\n\u2264 O(K \u22121 )\nwhere the last inequality follows from (21b) and Lemma B.2.\nUsing this in (22) we get\n\u0012\n\u0013\nK\nP\nsup kz (t) \u2212 z(t)k > \u000f \u2264 O(K \u22121 ) + O(K \u22121 )\n0\u2264t\u2264\u03c4\u03bd\n\n= O(K \u22121 )\n\n\u2264 O(K \u22121 ) + O(K \u22121 )\n= O(K \u22121 )\n\nwhere the last inequality follows from Lemma B.1.\nFollowing is the proof of Theorem 4.1.\n(a) First, we prove the convergence of z K (t) to z(t) over\n[0, \u03c4\u03bd ]. Fix a \u03bd > 0. Then Corollary B.1 implies that z K (t)\nconverges to z(t) in the region S\u03bd . Following [11, Theo-\n\n(b) Now we prove the convergence of z K (t) to z(t) over\n[\u03c4\u03bd , \u03c4\u0303\u2212\u03bd ]. Observe that, for t \u2208 [\u03c4\u03bd , \u03c4\u0303\u2212\u03bd ],\nkz K (t) \u2212 z(t)k\n\n\u2264 kz K (\u03c4\u03bd ) \u2212 z(\u03c4\u03bd )k + kz K (t) \u2212 z K (\u03c4\u03bd )k + kz(t) \u2212 z(\u03c4\u03bd )k.\n\n\f13\n\nHence,\nsup\n\u03c4\u03bd \u2264t\u2264\u03c4\u0303\u2212\u03bd\n\nkz K (t) \u2212 z(t)k\n\n\u2264 kz K (\u03c4\u03bd ) \u2212 z(\u03c4\u03bd )k +\n+\n\nsup\n\u03c4\u03bd \u2264t\u2264\u03c4\u0303\u2212\u03bd\n\nsup\n\u03c4\u03bd \u2264t\u2264\u03c4\u0303\u2212\u03bd\n\nkz K (t) \u2212 z K (\u03c4\u03bd )k\nkz(t) \u2212 z(\u03c4\u03bd )k\n\n= kz K (\u03c4\u03bd ) \u2212 z(\u03c4\u03bd )k + kz K (\u03c4\u0303\u2212\u03bd ) \u2212 z K (\u03c4\u03bd )k\n+ kz(\u03c4\u0303\u2212\u03bd ) \u2212 z(\u03c4\u03bd )k\n\nK\n\n\u2264 kz (\u03c4\u03bd ) \u2212 z(\u03c4\u03bd )k + kz K (\u03c4\u0303\u2212\u03bd ) \u2212 z K (\u03c4\u03bd )k +\n\n\u221a\n\n2\u039bb\u03bd\n\nwhere the equality follows because the z ( t) and z(t) are\nnondecreasing.\n\u221a The last inequality holds because kdz/dtk \u2264\nkdz\u0304/dtk \u2264 2\u039b and \u03c4\u0303\u2212\u03bd \u2212 \u03c4\u03bd \u2264 b\u03bd. Moreover,\n!\n\u221a\n\u000f\nK\nP\nsup kz (t) \u2212 z(t)k > 2\u039bb\u03bd +\n2\n\u03c4\u03bd \u2264t\u2264\u03c4\u0303\u2212\u03bd\n\u0010\n\u0011\n\u000f\n\u2264 P kz K (\u03c4\u03bd ) \u2212 z(\u03c4\u03bd )k >\n4\n\u0010\n\u000f\u0011\nK\n+ P kz (\u03c4\u0303\u2212\u03bd ) \u2212 z K (\u03c4\u03bd )k >\n4\n\u0010\n\u000f\u0011\n= O(K \u22121 ) + P kz K (\u03c4\u0303\u2212\u03bd ) \u2212 z K (\u03c4\u03bd )k >\n4\nwhere the equality follows from the result of part (a). We\nnow redefine the Markov process z\u0304 K (t) = (x\u0304K (t), \u0233 K (t)) for\nt \u2265 \u03c4\u03bd , to be the uncontrolled dynamics with initial condition\nz\u0304 K (\u03c4\u03bd ) = z K (\u03c4\u03bd ). Again, it can be easily observed that\nkz K (\u03c4\u0303\u2212\u03bd ) \u2212 z K (\u03c4\u03bd )k \u2264 kz\u0304 K (\u03c4\u0303\u2212\u03bd ) \u2212 z\u0304 K (\u03c4\u03bd )k.\nThus\nP\n\nsup\n\u03c4\u03bd \u2264t\u2264\u03c4\u0303\u2212\u03bd\n\nkz K (t) \u2212 z(t)k >\n\n\u221a\n\n\u000f\n2\u039bb\u03bd +\n2\n\n!\n\nsup\n\u03c4\u03bd \u2264t\u2264\u03c4\u0303\u2212\u03bd\n\n\u2264P\n\nkz K (t) \u2212 z(t)k > \u000f\n\nsup\n\u03c4\u03bd \u2264t\u2264\u03c4\u0303\u2212\u03bd\n\nkz K (t) \u2212 z(t)k >\n\n\u221a\n\n\u000f\n2\u039bb\u03bd +\n2\n\nimplying that the probability that z K (t) has changed its\ndynamics by \u03c4\u0303\u2212\u03bd approaches 1 as K approaches \u221e. In\nthese realizations, the dynamics of z K (t) and z(t) match for\nt \u2265 \u03c4\u0303\u2212\u03bd . We restrict ourselves to only these realizations. We\nalso have from part (b) that, for every \u000f > 0,\n\u0001\nP kz K (\u03c4\u0303\u2212\u03bd ) \u2212 z(\u03c4\u0303\u2212\u03bd )k > \u000f = O(K \u22121 )\nOnce more using [11, Theorem 2.8], for any \u000f, \u03b4 > 0\n!\nP\n\nsup\n\u03c4\u0303\u2212\u03bd \u2264t\u2264\u03c4\n\nkz K (t) \u2212 z(t)k > \u000f\n\n= O(K \u22121 )\n\n\u0001\nand P |\u03c4 K \u2212 \u03c4 | > \u03b4 = O(K \u22121 ).\nA PPENDIX C\nP ROOF OF T HEOREM 4.2\nFor the optimal policy u\u2217 , the total expected cost\nK\nK\nEK\n+ \u0393(X + y K (\u03c4 K ))}\nu\u2217 {Td + \u03b3Ec } = Eu\u2217 {\u03c4\n\nsince Td = \u03c4 K by definition (see (11)); we use the subscript u\u2217\nto show dependence of the probability law on the underlying\npolicy. Under the deterministic policy u\u221e , copying to relays\nis stopped at the deterministic time instant \u03c4 \u2217 < \u03c4 , implying\ny K (\u03c4 \u2217 ) = y K (\u03c4 ). Thus, the total expected cost\nK\nK\nEK\n+ \u0393(X + y K (\u03c4 ))}.\nu\u221e {Td + \u03b3Ec } = Eu\u221e {\u03c4\n\n\u0010\n\u000f\u0011\n\u2264 O(K \u22121 ) + P kz\u0304 K (\u03c4\u0303\u2212\u03bd ) \u2212 z\u0304 K (\u03c4\u03bd )k >\n4\n\u0010\n\u000f\u0011\n\u22121\nK\nK\n\u2264 O(K ) + P kz\u0304 (\u03c4\u03bd + b\u03bd) \u2212 z\u0304 (\u03c4\u03bd )k >\n4\n\u000f\nSet \u03bd = min{ 2\u221a2\u039bb , \u03c4\u0304 4b\u000f }, and apply Lemma B.1 to get\n!\nP\n\nK\nK\nFurthermore, we have that \u03c4\u2212\u03bd/2\n\u2264 \u03c4\u0303\u2212\u03bd/2\nsample path wise.\nK\nThe inequality holds because z (t) may continue to increase\n(in both its components) at a higher pace than z\u0303 K (t) even after\n\u03c4\u03bd . Thus\n\u0011\n\u0010\nK\n> \u03c4\u0303\u2212\u03bd = O(K \u22121 ),\nP \u03c4\u2212\u03bd/2\n\n!\n\n\u2264 O(K \u22121 ) + O(K \u22121 )\n= O(K \u22121 ).\n\n(c) Finally, we prove the convergence of z K (t) to z(t)\nover [\u03c4\u0303\u2212\u03bd , \u03c4 ]. Reconsider the process z\u0303 K (t), t \u2265 \u03c4\u03bd and\nthe associated function z\u0303(t). Recall that, for any \u03bd > 0,\nK\nz\u0303 K (t) and z\u0303(t) exit S\u2212\u03bd at \u03c4\u0303\u2212\u03bd\nand \u03c4\u0303\u2212\u03bd respectively. Clearly,\n\u03c4\u0303\u2212\u03bd/2 < \u03c4\u0303\u2212\u03bd ; say \u03c4\u0303\u2212\u03bd \u2212 \u03c4\u0303\u2212\u03bd/2 = \u03b4\u03bd . Also, using [11,\nTheorem 2.8],\n\u0010\n\u0011\nK\nP \u03c4\u0303\u2212\u03bd/2\n\u2212 \u03c4\u0303\u2212\u03bd/2 > \u03b4\u03bd = O(K \u22121 )\n\u0010\n\u0011\nK\ni.e., P \u03c4\u0303\u2212\u03bd/2\n> \u03c4\u0303\u2212\u03bd = O(K \u22121 )\n\nAlso observe that for (xK (t), y K (t)) under u\u221e , the corresponding fluid limits are the same deterministic dynamics\n(x(t), y(t)) defined in Section IV-A (i.e., solutions of (9a)(9b)). (xK (t), y K (t)) and (x(t), y(t)) satisfy the hypotheses\nassumed in Darling [11] over the intervals [0, \u03c4 \u2217 ] and [\u03c4 \u2217 , \u221e).\nThus [11, Theorem 2.8] applies, and we conclude 9\n\u0012\n\u0013\nK\nK\nK\nlim Pu\u221e sup k(x (t), y (t)) \u2212 (x(t), y(t))k > \u000f = 0,\nK\u2192\u221e\n0\u2264t\u2264\u03c4\n\u0001\nK\nlim PK\n|\u03c4\n\u2212 \u03c4 | > \u03b4 = 0.\n\u221e\nu\nK\u2192\u221e\n\nFurthermore, it can be easily shown that under both the\ncontrols u\u2217 and u\u221e , the delivery delays \u03c4 K have second\nmoments that are bounded uniformly over all K. To see this,\nconsider a policy u0 that never copies to relays. Clearly,\nK 2\nK\nK 2\nEK\nu\u2217 (\u03c4 ) <Eu0 (\u03c4 ) ,\nK 2\nK\nK 2\nEK\nu\u221e (\u03c4 ) <Eu0 (\u03c4 )\n\nfor each K. Then is suffices to show that\nK 2\nsup EK\nu0 (\u03c4 ) < \u221e.\n\n(23)\n\nK\n\n9 Applying\n\u2217\n[11,\nTheorem\n2.8]\nover\nyields\n\u0001[0, \u03c4 ]\nlimK\u2192\u221e P k(xK (\u03c4 \u2217 ), y K (\u03c4 \u2217 ) \u2212 (x(\u03c4 \u2217 ), y(\u03c4 \u2217 ))k > \u000f\n= 0 which\n\u2217\nis a necessary condition to apply [11, Theorem 2.8] over [\u03c4 , \u221e).\n\n\f14\n\nNote that\n\nprobability. Once more using [19, Theorem 9.5.1], we get\n\nK\nM\u03b1\n\u22121\n\n\u03c4K =\n\nX\n\nK\nlim EK\nu\u221e y (\u03c4 ) = y(\u03c4 ).\n\n\u03b4\u0304m\n\nK\u2192\u221e\n\nm=0\n\nwhere \u03b4\u0304m is the time duration for which m(t) = m; \u03b4\u0304m , m =\n0, 1, . . . are independent, and \u03b4\u0304m is exponentially distributed\nwith mean \u03bbK (m+N K1)(M K \u2212m) under policy u0 . Thus\n0\n\nK\nM\u03b1\n\u22121\n\nK\n=\nEK\nu0 \u03c4\n\nX\nm=0\nK\nM\u03b1\n\u22121\n\n\u2264\n=\n\nX\nm=0\n\n\u2212 M\u03b1K )\n\nFinally, combining (24) and (28), we get that\nK\u2192\u221e\n\ndx(t)\n= \u039b(x(t) + y(t))(X \u2212 x(t)),\n(29a)\ndt\ndy(t)\n= \u039b(x(t) + y(t))(Y \u2212 y(t))u(t)\n(29b)\ndt\nwhere u(t) \u2208 [0, 1] is the control at time t. Our objective is\nto minimize\nZ T\n\u0393y(T ) + T = \u0393y(T ) +\n1.dt\n(30)\n\n1\n\nX\n\n\u00012\n\u03bbK (m + N0K )(M K \u2212 m)\nM\u03b1K\n\u2264\n\u00012\n\u03bbK N0K (M K \u2212 M\u03b1K )\nX\u03b1\n=\nK\u039b2 Y02 (X \u2212 X\u03b1 )2\n\u21920\nm=0\n\n0\n\nwhere T is the terminal time when x(T ) = X\u03b1 ; dependence\nof T on the underlying control is understood, and is not shown\nexplicitly.\nTheorem D.1: The optimal policy for the deterministic system (29a)-(29b) with cost (30) is\n\nas K \u2192 \u221e. These results together imply (23).\nFollowing [19, Remark 9.5.1], under both u\u2217 and u\u221e , \u03c4 K\nare uniformly integrable. Since, \u03c4 K , under both u\u2217 and u\u221e ,\nconverge to \u03c4 in probability and hence in distribution, [19,\nTheorem 9.5.1] yields\nK\nK\nlim EK\n= lim EK\n= \u03c4.\nu\u2217 \u03c4\nu\u221e \u03c4\n\nK\u2192\u221e\n\n(24)\n\nK\u2192\u221e\n\n\u2217\n\nK\n\nK\n\nNext, it is easy to show that under the control u , y (\u03c4 )\nconverges to y(\u03c4 ) in probability. To see this, observe that\n|y K (\u03c4 K )\u2212y(\u03c4 )| \u2264 |y K (\u03c4 K )\u2212y K (\u03c4 )|+|y K (\u03c4 )\u2212y(\u03c4 )|. (25)\nFrom Theorem 4.1, y K (\u03c4 ) and \u03c4 K converge to y(\u03c4 ) and \u03c4\nrespectively, in probability. The latter result, along with the\narguments similar to those in the proof of Lemma B.2, implies\nthat\n\u0001\nP |y K (\u03c4 K ) \u2212 y K (\u03c4 )| > \u000f = O(K \u22121 )\nfor every \u000f > 0. Using these facts in (25), we conclude that\n\u0001\nP |y K (\u03c4 K ) \u2212 y(\u03c4 )| > \u000f = O(K \u22121 ).\nfor every \u000f > 0. Since y K (\u03c4 K ) is bounded, and hence\nuniformly integrable, [19, Theorem 9.5.1] implies that\nK K\nlim EK\nu\u2217 y (\u03c4 ) = y(\u03c4 ).\n\nK\u2192\u221e\n\n(28)\n\nT HE H AMILTONIAN F ORMULATION AND T HE S OLUTION\nIn this section we consider the limiting deterministic (fluid)\nsystem and study its optimal control. The limiting controlled\nsystem is: x(0) = 0, y(0) = Y0 , and for t \u2265 0,\n\nSimilarly,\n=\n\nK\u2192\u221e\n\nA PPENDIX D\n\n1\n\u03bbK N0K (M K \u2212 M\u03b1K )\n\nK\nM\u03b1\n\u22121\n\nK K\nK\nK\nlim EK\nu\u2217 y (\u03c4 ) = lim Eu\u221e y (\u03c4 ).\n\nK\u2192\u221e\n\nK\u2192\u221e\n\nX\u03b1\n=\n\u039bY0 (X \u2212 X\u03b1 )\n<\u221e\n\nK\nVarK\nu0 \u03c4\n\nCombining (26) and (27)\n\n\u2217\nK\nlim EK\nu\u2217 {Td + \u03b3Ec } = lim Eu\u221e {Td + \u03b3Ec } = \u03c4 + \u0393y(\u03c4 ).\n\n1\n\u03bbK (m + N0K )(M K \u2212 m)\n\nM\u03b1K\nK\nK\n\u03bb N0 (M K\n\n(27)\n\n(26)\n\nSimilarly, under the control u\u221e also, y K (\u03c4 ) is bounded, and\nhence is uniformly integrable. It also converges to y(\u03c4 ) in\n\nu\u2217 (t) = 1[0,\u03c4 \u2217 ] (t)\nwith \u03c4 \u2217 as in (13). Furthermore, the optimal cost is \u03c4 +\u0393y(\u03c4 \u2217 )\nwith \u03c4 as in (12).\nProof: Following [18, Section 3.3.1], we define the\nHamiltonian for the system\nH(x, y, u, p1 , p2 )\n= 1 + p1 \u039b(X \u2212 x)(x + y) + p2 \u039b(Y \u2212 y)(x + y)u\n= 1 + \u039b(x + y)[p1 (X \u2212 x) + p2 (Y \u2212 y)u]\n\n(31)\n\nwhere pi : R+ \u2192 R, i = 1, 2 are the cojoint functions\nassociated with x(t) and y(t) respectively. Let u\u2217 (t), t \u2265 0,\nbe an optimal control trajectory. Let T \u2217 be the corresponding terminal time, and let (x\u2217 (t), y \u2217 (t)), t \u2208 [0, T \u2217 ] be the\ncorresponding state trajectory.\na) Adjoint equations: By [18, Section 3.3.1, Proposition 3.1], the functions pi (t) are solutions of the following\nadjoint equations:\ndp1 (t)\n\u2202\n=\u2212\nH(x, y \u2217 , u\u2217 , p1 , p2 )\n=\ndt\n\u2202x\nx=x\u2217\n\u2212 \u039b[p1 (t)(X \u2212 2x\u2217 (t) \u2212 y \u2217 (t)) + p2 (t)(Y \u2212 y \u2217 (t))u\u2217 (t)],\n(32)\ndp2 (t)\n\u2202\n=\u2212\nH(x\u2217 , y, u\u2217 , p1 , p2 )\n=\ndt\n\u2202y\ny=y \u2217\n\u2212 \u039b[p1 (t)(X \u2212 x\u2217 (t)) + p2 (t)(Y \u2212 x\u2217 (t) \u2212 2y \u2217 (t))u\u2217 (t)].\n(33)\n\n\f15\n\nb) Boundary condition: Observe that the terminal cost is\n\u0393y (T \u2217 ). Thus, by [18, Section 3.3.1, Proposition 3.1],\n\u2217\n\n\u2202\n(\u0393y)\np2 (T \u2217 ) =\n\u2202y\n\n= \u0393.\n\n(34)\n\ny=y \u2217 (T \u2217 )\n\nc) Minimum principle: Moreover, the optimal control u\u2217\nsatisfies\nu\u2217 (t) = arg min H(x\u2217 (t), y \u2217 (t), u, p1 (t), p2 (t))\nu\u2208[0,1]\n\nfor all t \u2208 [0, T \u2217 ]. From (31), it is immediate that the optimal\npolicy is a bang-bang policy.\n\u001a\n1, if p2 (t) \u2264 0\nu\u2217 (t) =\n(35)\n0, if p2 (t) > 0\nIn particular, our observation (34) implies that u\u2217 (T \u2217 ) = 0.\nd) Free terminal time condition: Since the terminal time\nis free, we also have from [18, Section 3.4.3] that\nH(x\u2217 (t), y \u2217 (t), u\u2217 (t), p1 (t), p2 (t)) = 0\nfor all t \u2208 [0, T \u2217 ]. In particular, equality at t = T \u2217 implies (see (31))\n1 + \u039b(X\u03b1 + y(T \u2217 ))[p1 (T \u2217 )(X \u2212 X\u03b1 )] = 0.\nSince X \u2212 X\u03b1 > 0, we must have\n\np1 (T \u2217 ) < 0.\n\n(36)\n\nWe will find this observation useful later.\nOur characterization of the optimal control consists of two\nsteps. First we show that the optimal control trajectory is of\nthreshold type, i.e.,\n\u001a\n1, if t \u2208 [0, t\u2217 ]\nu\u2217 (t) =\n(37)\n0, if t \u2208 (t\u2217 , T \u2217 ].\nThis is done in the next subsection. In the subsequent subsection, we obtain the threshold t\u2217 .\nA. Optimal control is of threshold type\nWe show that p2 (t) is negative for t \u2208 [0, t\u2217 ] and strictly\npositive for t \u2208 (t\u2217 , T \u2217 ] for some t\u2217 \u2265 0. It then follows\n1 (t)\nin (32). We\nfrom (35) that u\u2217 (t) is as in (37). Recall dpdt\nconsider two scenarios.\n1) Case 1: Let X \u2212 2X\u03b1 \u2212 y \u2217 (T \u2217 ) \u2265 0. Since x\u2217 (t) and\n\u2217\ny (t) both are non-decreasing in t, we have\nX \u2212 2x\u2217 (t) \u2212 y \u2217 (t) \u2265 0 for all t \u2208 [0, T \u2217 ].\nMoreover, from (35),\np2 (t)u\u2217 (t) \u2264 0 for all t \u2208 [0, T \u2217 ]\nwith equality at t = T \u2217 . Thus, from (32),\ndp1 (t)\n\u22650\ndt\nfor all t \u2208 [t0 , T \u2217 ] at which p1 (t) < 0. But, using the\nobservation p1 (T \u2217 ) < 0 (see (36)), it immediately follows\nthat\ndp1 (t)\n\u2265 0 for all t \u2208 [0, T \u2217 ],\ndt\n\nand so, p1 (t) < 0 for all t \u2208 [0, T \u2217 ]. Now, from (33),\n\ndp2 (t)\n>0\ndt\nfor all t \u2208 [0, T \u2217 ] at which p2 (t) \u2265 0. Again, using the\nobservation p2 (T \u2217 ) = \u0393 > 0 (see (34)), it follows that either\np2 (t) > 0 for all t \u2208 [0, T \u2217 ], or there exists a t\u2217 \u2208 [0, T \u2217 ]\nsuch that p2 (t\u2217 ) = 0, and\n\u001a\n< 0, if t \u2208 [0, t\u2217 )\np2 (t)\n> 0, if t \u2208 (t\u2217 , T \u2217 ].\n2) Case 2: Let X \u2212 2X\u03b1 \u2212 y \u2217 (T \u2217 ) < 0. Observe that\nX \u2212 2x\u2217 (t) \u2212 y \u2217 (t) is decreasing in t. Thus, tracing back from\nt = T \u2217 , there exists a t1 such that X \u2212 2x\u2217 (t1 ) \u2212 y \u2217 (t1 ) = 0;\nwe set t1 = 0 if X \u2212 2x\u2217 (t) \u2212 y \u2217 (t) < 0 for all t \u2208 [0, T \u2217 ].\nClearly, X \u2212 2x\u2217 (t) \u2212 y \u2217 (t) \u2264 0 for all t \u2208 [t1 , T \u2217 ].\nWe claim that p1 (t) < 0 for all t \u2208 [t1 , T \u2217 ]. Suppose not,\ni.e., there exists a t2 \u2208 [t1 , T \u2217 ] such that p1 (t2 ) \u2265 0. Then,\nfrom (32),\n\ndp1 (t)\n\u2265 0 for all t \u2208 [t2 , T \u2217 ],\ndt\nand so, p1 (t) increases with t in this interval. But this\ncontradicts the assertion in (36) that p1 (T \u2217 ) < 0. Hence the\nclaim holds.\nNow, X \u2212 2x\u2217 (t1 ) \u2212 y \u2217 (t1 ) = 0, and p1 (t1 ) < 0. An\nargument similar to that in Case 1 yields that\ndp1 (t)\n\u2265 0 for all t \u2208 [0, t1 ],\ndt\nand so, p1 (t) < 0 for all t \u2208 [0, T \u2217 ]; recall that it is readily\nseen that p1 (t) < 0 for all t \u2208 [t1 , T \u2217 ]. Consequently, as in\nCase 1, either p2 (t) > 0 for all t \u2208 [0, T \u2217 ], or there exists a\nt\u2217 \u2208 [0, T \u2217 ] such that p2 (t\u2217 ) = 0, and\n\u001a\n< 0, if t \u2208 [0, t\u2217 )\np2 (t)\n> 0, if t \u2208 (t\u2217 , T \u2217 ].\nTo summarize, in both the cases there exits a t\u2217 \u2208 [0, T \u2217 ]\nsuch that\n\u001a\n< 0, if t \u2208 [0, t\u2217 )\np2 (t)\n> 0, if t \u2208 (t\u2217 , T \u2217 ].\nB. Optimum Threshold\nWe now characterize the optimal threshold t\u2217 . Consider a\nthreshold policy\n\u001a\n1, if t \u2208 [0, t\u0304 ]\nu(t) =\n0, if t \u2208 (t\u0304, T ].\nLet the corresponding state trajectory be (xt\u0304 (t), y t\u0304 (t)), t \u2265 0,\nand let the terminal time be T (t\u0304). Let x\u0304 := xt\u0304 (t\u0304) and \u0233 :=\ny t\u0304 (t\u0304) be the values at the threshold time t\u0304. Clearly,\ndx\u0304\n= \u039b(x\u0304 + \u0233)(X \u2212 x\u0304).\ndt\u0304\nThe associated cost is\nC(t\u0304) = T (t\u0304) + \u0393\u0233,\n\n(38)\n\n(39)\n\n\f16\n\nand10\nt\u2217 = arg min C(t\u0304).\nt\u0304\u22650\n\nFor any t\u0304 \u2265 0 and t \u2208 (t\u0304, \u221e),\ny t\u0304 (t) = \u0233,\nand\n\ndxt\u0304 (t)\n= \u039b(xt\u0304 (t) + \u0233)(X \u2212 xt\u0304 (t)),\ndt\n\nand so\nT (t\u0304) = t\u0304 +\n\n1\n\u039b\n\nZ\n\nX\u03b1\n\ndz\n.\n(z + \u0233)(X \u2212 z)\n\nx\u0304\n\nIts substitution in (39) yields\n1\nC(t\u0304) = t\u0304 + \u0393\u0233 +\n\u039b\n\nZ\n\nX\u03b1\n\nx\u0304\n\ndz\n.\n(z + \u0233)(X \u2212 z)\n\nUsing Leibniz rule of differentiation, we get\n\" Z\nd\u0233\n1 d\u0233 X\u03b1\ndz\ndC(t\u0304)\n=1+\u0393\n\u2212\ndt\u0304\ndt\u0304\n\u039b dt\u0304 x\u0304 (z + \u0233)2 (X \u2212 z)\n\u0015\ndx\u0304\n1\n+\ndt\u0304 (x\u0304 + \u0233)(X \u2212 x\u0304)\n\"\n#\nZ X\u03b1\nd\u0233\n1\ndz\n=\n\u0393\u2212\ndt\u0304\n\u039b x\u0304 (z + \u0233)2 (X \u2212 z)\nwhere the last equality uses (38). Defining\nZ\ndz\n1 X\u03b1\n,\ng(t\u0304) := \u0393 \u2212\n\u039b x\u0304 (z + \u0233)2 (X \u2212 z)\nwe get\n\ndC(t\u0304)\nd\u0233\n=\ng(t\u0304).\ndt\u0304\ndt\u0304\nd\u0233\n1\nNote that \u2202\u2202gx\u0304 > X\u2212X\n, \u2202\u2202g\u0233 \u2265 0, dx\u0304\ndt\u0304 > \u039bY0 (X \u2212X\u03b1 ), dt\u0304 \u2265 0,\n\u03b1\nand so g(t\u0304) is also strictly increasing in t\u0304 with slope bounded\naway from 0. Thus, the optimal threshold is given by\n\u001a\n0\nif g(0) > 0,\n\u2217\nt =\ng \u22121 (0) otherwise\n\nwhich is identical to \u03c4 \u2217 in (13).\nRemarks D.1: Combined with Theorem 4.2, we now have\nthat the limit of the optimal cost (of the finite problem) equals\nthe optimal cost of the limiting system. This does not hold in\ngeneral (see Remark 4.2).\nR EFERENCES\n[1] K. Fall, \"A delay-tolerant network architecture for challenged internets,\"\nin SIGCOMM 03, Karlsruhe, Germany, 2003, pp. 27\u201334.\n[2] A. Vahdat and D. Becker, \"Epidemic routing for partially-connected\nad hoc networks,\" Duke University, Technical Report CS-2000-06, July\n2000.\n[3] R. Groenevelt, P. Nain, and G. Koole, \"The message delay in mobile\nad hoc networks,\" Performance Evaluation, vol. 62, no. 1\u20134, pp. 210 \u2013\n228, October 2005.\n[4] X. Zhang, G. Neglia, J. Kurose, and D. Towsley, \"Performance modeling\nof epidemic routing,\" Computer Networks, vol. 51, no. 10, pp. 2867\u2013\n2891, July 2007.\n[5] G. Neglia and X. Zhang, \"Optimal delay-power tradeoff in sparse delay\ntolerant networks: a preliminary study,\" in SIGCOMM Workshop on\nChallenged Networks, Pisa, Italy, September 2006, pp. 237\u2013244.\n10 We\n\ncan restrict to only those t\u0304 such that x\u0304 := xt\u0304 (t\u0304) \u2264 X\u03b1 .\n\n[6] E. Altman, T. Basar, and F. D. Pellegrini, \"Optimal monotone forwarding policies in delay tolerant mobile ad-hoc networks,\" Performance\nEvaluation, vol. 67, no. 4, pp. 299\u2013317, April 2010.\n[7] E. Altman, G. Neglia, F. D. Pellegrini, and D. Miorandi, \"Decentralized\nstochastic control of delay tolerant networks,\" in Proceedings of IEEE\nINFOCOM, Rio de Janeiro, Brazil, April 2009.\n[8] E. Altman, A. P. Azad, F. D. Pellegrini, and T. Basar, \"Optimal activation\nand transmission control in delay tolerant networks,\" in Proceedings of\nIEEE INFOCOM, San Diego, CA, USA, March 2010.\n[9] Y. Li, Y. Jiang, D. Jin, L. Su, L. Zeng, and D. Wu, \"Energy-efficient\noptimal opportunistic forwarding for delay-tolerant networks,\" IEEE\nTransactions on Vehicular Technology, vol. 59, no. 9, pp. 4500\u20134512,\nNovember 2010.\n[10] T. G. Kurtz, \"Solutions of ordinary differential equations as limits of\npure jump markov processes,\" Journal of Applied Probability, vol. 7,\npp. 49\u201358, 1970.\n[11] R. W. R. Darling, \"Fluid limits of pure jump markov processes: a\npractical guide,\" arXiv:math/0210109v3, 2002.\n[12] R. W. R. Darling and J. R. Norris, \"Differential equation approximations\nfor markov chains,\" Probability Surveys, vol. 5, pp. 37\u201379, 2008.\n[13] N. Gast and B. Gaujal, \"Mean field limit of non-smooth systems and\ndifferential inclusions,\" INRIA, Tech. Rep., 2010.\n[14] N. Gast, B. Gaujal, and J.-Y. L. Boudec, \"Mean field for markov decision\nprocesses: from discrete to continuous optimization,\" INRIA, Tech. Rep.,\n2010.\n[15] D. P. Bertsekas, Dynamic Programming and Optimal Control, Vol. 2,\n3rd ed. Belmont, Massachusetts: Athena Scientific, 2007.\n[16] C. Singh, A. Kumar, and R. Sundaresan, \"Delay and energy optimal\ntwo-hop relaying in delay tolerant networks,\" in 8th International\nSymposium on Modeling and Optimization in Mobile, Ad Hoc and\nWireless Networks (WiOpt), Avignon, France, May\u2013June 2010, pp. 256\u2013\n265.\n[17] M. Grossglauser and D. N. C. Tse, \"Mobility increases the capacity\nof ad-hoc wireless networks,\" IEEE/ACM Transactions on Networking,\nvol. 10, no. 4, pp. 477\u2013486, August 2002.\n[18] D. P. Bertsekas, Dynamic Programming and Optimal Control, Vol. 1,\n3rd ed. Belmont, Massachusetts: Athena Scientific, 2005.\n[19] K. B. Athreya and S. N. Lahiri, Measure Theory and Probability Theory.\nSpringer, 2006.\n\n\f"}