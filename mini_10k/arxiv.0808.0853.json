{"id": "http://arxiv.org/abs/0808.0853v1", "guidislink": true, "updated": "2008-08-06T15:06:46Z", "updated_parsed": [2008, 8, 6, 15, 6, 46, 2, 219, 0], "published": "2008-08-06T15:06:46Z", "published_parsed": [2008, 8, 6, 15, 6, 46, 2, 219, 0], "title": "Divergences Test Statistics for Discretely Observed Diffusion Processes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0808.1274%2C0808.2477%2C0808.3170%2C0808.1183%2C0808.0041%2C0808.1672%2C0808.2576%2C0808.3189%2C0808.3931%2C0808.0772%2C0808.2107%2C0808.2023%2C0808.3153%2C0808.0197%2C0808.1938%2C0808.2800%2C0808.4098%2C0808.2741%2C0808.1383%2C0808.4088%2C0808.0173%2C0808.1235%2C0808.2876%2C0808.0434%2C0808.0106%2C0808.2959%2C0808.2451%2C0808.0619%2C0808.2957%2C0808.1318%2C0808.2674%2C0808.3091%2C0808.1730%2C0808.3832%2C0808.3378%2C0808.1581%2C0808.3705%2C0808.2097%2C0808.1138%2C0808.3511%2C0808.2076%2C0808.0086%2C0808.2056%2C0808.2058%2C0808.0071%2C0808.2704%2C0808.1349%2C0808.0389%2C0808.0853%2C0808.2036%2C0808.3069%2C0808.1962%2C0808.2084%2C0808.0523%2C0808.1640%2C0808.1135%2C0808.2694%2C0808.2854%2C0808.2229%2C0808.2357%2C0808.2732%2C0808.1084%2C0808.2930%2C0808.1391%2C0808.2929%2C0808.3870%2C0808.2378%2C0808.1588%2C0808.3328%2C0808.3857%2C0808.3652%2C0808.1113%2C0808.1206%2C0808.1071%2C0808.2070%2C0808.1341%2C0808.1573%2C0808.0673%2C0808.1632%2C0808.3129%2C0808.3164%2C0808.1877%2C0808.3161%2C0808.2200%2C0808.0817%2C0808.0546%2C0808.3591%2C0808.4159%2C0808.0451%2C0808.2339%2C0808.0222%2C0808.3291%2C0808.0589%2C0808.2670%2C0808.1401%2C0808.3455%2C0808.0347%2C0808.2574%2C0808.0335%2C0808.2168%2C0808.0528&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Divergences Test Statistics for Discretely Observed Diffusion Processes"}, "summary": "In this paper we propose the use of $\\phi$-divergences as test statistics to\nverify simple hypotheses about a one-dimensional parametric diffusion process\n$\\de X_t = b(X_t, \\theta)\\de t + \\sigma(X_t, \\theta)\\de W_t$, from discrete\nobservations $\\{X_{t_i}, i=0, ..., n\\}$ with $t_i = i\\Delta_n$, $i=0, 1, >...,\nn$, under the asymptotic scheme $\\Delta_n\\to0$, $n\\Delta_n\\to\\infty$ and\n$n\\Delta_n^2\\to 0$. The class of $\\phi$-divergences is wide and includes\nseveral special members like Kullback-Leibler, R\\'enyi, power and\n$\\alpha$-divergences. We derive the asymptotic distribution of the test\nstatistics based on $\\phi$-divergences. The limiting law takes different forms\ndepending on the regularity of $\\phi$. These convergence differ from the\nclassical results for independent and identically distributed random variables.\nNumerical analysis is used to show the small sample properties of the test\nstatistics in terms of estimated level and power of the test.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0808.1274%2C0808.2477%2C0808.3170%2C0808.1183%2C0808.0041%2C0808.1672%2C0808.2576%2C0808.3189%2C0808.3931%2C0808.0772%2C0808.2107%2C0808.2023%2C0808.3153%2C0808.0197%2C0808.1938%2C0808.2800%2C0808.4098%2C0808.2741%2C0808.1383%2C0808.4088%2C0808.0173%2C0808.1235%2C0808.2876%2C0808.0434%2C0808.0106%2C0808.2959%2C0808.2451%2C0808.0619%2C0808.2957%2C0808.1318%2C0808.2674%2C0808.3091%2C0808.1730%2C0808.3832%2C0808.3378%2C0808.1581%2C0808.3705%2C0808.2097%2C0808.1138%2C0808.3511%2C0808.2076%2C0808.0086%2C0808.2056%2C0808.2058%2C0808.0071%2C0808.2704%2C0808.1349%2C0808.0389%2C0808.0853%2C0808.2036%2C0808.3069%2C0808.1962%2C0808.2084%2C0808.0523%2C0808.1640%2C0808.1135%2C0808.2694%2C0808.2854%2C0808.2229%2C0808.2357%2C0808.2732%2C0808.1084%2C0808.2930%2C0808.1391%2C0808.2929%2C0808.3870%2C0808.2378%2C0808.1588%2C0808.3328%2C0808.3857%2C0808.3652%2C0808.1113%2C0808.1206%2C0808.1071%2C0808.2070%2C0808.1341%2C0808.1573%2C0808.0673%2C0808.1632%2C0808.3129%2C0808.3164%2C0808.1877%2C0808.3161%2C0808.2200%2C0808.0817%2C0808.0546%2C0808.3591%2C0808.4159%2C0808.0451%2C0808.2339%2C0808.0222%2C0808.3291%2C0808.0589%2C0808.2670%2C0808.1401%2C0808.3455%2C0808.0347%2C0808.2574%2C0808.0335%2C0808.2168%2C0808.0528&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper we propose the use of $\\phi$-divergences as test statistics to\nverify simple hypotheses about a one-dimensional parametric diffusion process\n$\\de X_t = b(X_t, \\theta)\\de t + \\sigma(X_t, \\theta)\\de W_t$, from discrete\nobservations $\\{X_{t_i}, i=0, ..., n\\}$ with $t_i = i\\Delta_n$, $i=0, 1, >...,\nn$, under the asymptotic scheme $\\Delta_n\\to0$, $n\\Delta_n\\to\\infty$ and\n$n\\Delta_n^2\\to 0$. The class of $\\phi$-divergences is wide and includes\nseveral special members like Kullback-Leibler, R\\'enyi, power and\n$\\alpha$-divergences. We derive the asymptotic distribution of the test\nstatistics based on $\\phi$-divergences. The limiting law takes different forms\ndepending on the regularity of $\\phi$. These convergence differ from the\nclassical results for independent and identically distributed random variables.\nNumerical analysis is used to show the small sample properties of the test\nstatistics in terms of estimated level and power of the test."}, "authors": ["Alessandro De Gregorio", "Stefano Iacus"], "author_detail": {"name": "Stefano Iacus"}, "author": "Stefano Iacus", "links": [{"href": "http://arxiv.org/abs/0808.0853v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0808.0853v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0808.0853v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0808.0853v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:0808.0853v1 [math.ST] 6 Aug 2008\n\nDivergences Test Statistics for Discretely\nObserved Diffusion Processes\nAlessandro De Gregorio\nDepartment of Statistics, Probability and Applied Statistics\nP.le Aldo Moro 5, 00185 Rome-Italy\nalessandro.degregorio@uniroma1.it\n\nStefano M. Iacus\nDepartment of Economics, Business and Statistics\nVia Conservatorio 7, 20124 Mlan-Italy\nstefano.iacus@unimi.it\n\nOctober 27, 2018\nAbstract\nIn this paper we propose the use of \u03c6-divergences as test statistics to verify simple hypotheses about a one-dimensional parametric diffusion process\ndXt = b(Xt , \u03b8)dt + \u03c3(Xt , \u03b8)dWt , from discrete observations {Xti , i =\n0, . . . , n} with ti = i\u2206n , i = 0, 1, . . . , n, under the asymptotic scheme\n\u2206n \u2192 0, n\u2206n \u2192 \u221e and n\u22062n \u2192 0. The class of \u03c6-divergences is wide and\nincludes several special members like Kullback-Leibler, R\u00e9nyi, power and\n\u03b1-divergences. We derive the asymptotic distribution of the test statistics\nbased on \u03c6-divergences. The limiting law takes different forms depending\non the regularity of \u03c6. These convergence differ from the classical results for\nindependent and identically distributed random variables. Numerical analysis is used to show the small sample properties of the test statistics in terms\nof estimated level and power of the test.\n\nkeywords: diffusion processes, empirical level, hypotheses testing, \u03c6-divergences,\n\u03b1-divergences\n\n1\n\n\f1\n\nIntroduction\n\nWe consider the problem of parametric testing using \u03c6-divergences. Let X be\na r.v. and f (X, \u03b8) and g(X, \u03b8), \u03b8 \u2208 \u0398 two families of probability densities\non the same measurable space. The \u03c6-divergences are defined as D\u03c6 (f, g) =\nE\u03b8 \u03c6 (f (X)/g(X)), where E\u03b8 is the expected value with respect to P\u03b8 , the true\nlaw of the observations. Because we focus the attention on the use of divergences\nfor hypotheses testing, we will use a simplified notation: let \u03b8 and \u03b80 two points\nin the interior of \u0398 and define the divergence as\n\u0012\n\u0013\np(X, \u03b8)\nD\u03c6 (\u03b8, \u03b80 ) = E\u03b80 \u03c6\n(1.1)\np(X, \u03b80 )\nIn equation (1.1) the density {p(X, \u03b8), \u03b8 \u2208 \u0398} is a same family of probability\ndensities and \u03c6(*) is a function with the minimal property that \u03c6(1) = 0. Examples\nof divergences of the form D\u03b1 (\u03b8, \u03b80 ) = D\u03c6\u03b1 (\u03b8, \u03b80 ) are the \u03b1-divergences, defined\nby means of the following function\n1+\u03b1\n\n4(1 \u2212 x 2 )\n\u03c6\u03b1 (x) =\n,\n1 \u2212 \u03b12\n\n\u22121 < \u03b1 < 1\n\nNote that D\u03b1 (\u03b80 , \u03b8) = D\u2212\u03b1 (\u03b8, \u03b80 ). The class of \u03b1-divergences has been widely\nstudied in statistics (see, e.g., Csisz\u00e1r, 1967 and Amari, 1985) and it is a family of\ndivergences which includes several members of particular interest. For example,\nin the limit as \u03b1 \u2192 \u22121, D\u22121 (\u03b8, \u03b80 ) reduces to the well-known Kullback-Leibler\nmeasure\n\u0013\n\u0012\np(X, \u03b8)\nD\u22121 (\u03b8, \u03b80 ) = \u2212E\u03b80 log\np(X, \u03b80 )\nwhile as \u03b1 \u2192 0, the Hellinger distance (see, e.g., Beran, 1977, Simpson, 1989)\nemerges\n\u00112\np\n1 \u0010p\np(X, \u03b8) \u2212 p(X, \u03b80 )\nD0 (\u03b8, \u03b80 ) = E\n2\nAs noticed in Chandra and Taniguchi (2006), the \u03b1-divergence is also equivalent\nto the R\u00e9nyi's divergence (R\u00e9nyi, 1961) defined, for \u03b1 \u2208 (0, 1), as\n\u0013\u03b1\n\u0012\n1\np(X, \u03b8)\nR\u03b1 (\u03b8, \u03b80 ) =\nlog E\u03b80\n1\u2212\u03b1\np(X, \u03b80 )\nfrom which is easy to see that in the limit as \u03b1 \u2192 1, R\u03b1 reduces to the KullbackLeibler divergence. The transformation \u03c8(R\u03b1 ) = (exp{(\u03b1 \u2212 1)R\u03b1 \u2212 1}/(1 \u2212 \u03b1)\nreturns the power-divergence studied in Cressie and Read (1984). Liese and Vajda (1987) provide extensive study of a modified version of R\u03b1 and Morales et al.\n2\n\n\f(1997) consider divergences with convex \u03c6(*) for independent and identically distributed (i.i.d) observations; for example the power-divergences D\u03c6\u03bb (\u03b8, \u03b80 ) with\n\u03c6\u03bb (x) =\n\nx\u03bb \u2212 \u03bb(x \u2212 1) \u2212 1\n\u03bb(\u03bb \u2212 1)\n\n(1.2)\n\nand \u03bb \u2208 R \u2212 {0, 1}.\nIn this paper we focus our attention on the \u03c6-divergences D\u03c6 (\u03b8, \u03b80 ), defined\nas in (1.1), for one-dimensional diffusion process {Xt , t \u2208 [0, T ]}, solution of the\nfollowing stochastic differential equation\ndXt = b(\u03b1, Xt )dt + \u03c3(\u03b2, Xt )dWt ,\n\nX0 = x0 ,\n\n(1.3)\n\nwhere Wt is a Brownian motion, \u03b8 = (\u03b1, \u03b2) \u2208 \u0398\u03b1 \u00d7 \u0398\u03b2 = \u0398, where \u0398\u03b1 and \u0398\u03b2\nare respectively compact convex subset of Rp and Rq . We assume that the process\nXt is ergodic for every \u03b8 with invariant law \u03bc\u03b8 . Furthermore Xt is observed at\ndiscrete times ti = i\u2206n , i = 0, 1, 2, ..., n, where \u2206n is the length of the steps.\nWe indicate the observations with Xn = {Xti }06i6n . The asymptotic is \u2206n \u2192\n0, n\u2206n \u2192 \u221e and n\u22062n \u2192 0 as n \u2192 \u221e.\nWe study the properties of the estimated \u03c6-divergence D\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ), for\ndiscretely observed diffusion processes, defined as\n!\nfn (Xn , \u03b8\u0303n (Xn ))\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) = \u03c6\nfn (Xn , \u03b80 )\nwhere fn (*, *) is the approximated likelihood proposed by Dacunha-Castelle and\nFlorens-Zmirou (1986) and \u03b8\u0303n (Xn ) is any consistent, asymptotically normal and\nefficient estimator of \u03b8. We prove that, for \u03c6(*) functions which satisfying three\ndifferent regularity conditions, the statistic D\u03c6 converge weakly to three different\nfunctions of the \u03c72p+q random variable. This result differs from the case of i.i.d.\nsetting.\nUp to our knowledge the only result concerning the use of divergences for\ndiscretely observed diffusion process is due to Rivas et al. (2005) where they\nconsider the model of Brownian motion with drift dXt = adt + bdWt where\na and b are two scalars. In that case, the exact likelihood of the observations\nis available in explicit form and is the gaussian law. Conversely, in the general\nsetup of this paper, the likelihood of the process in (1.3) is known only for three\nparticular stochastic differential equations, namely the Ornstein-Uhlembeck diffusion, the geometric Brownian motion and the Cox-Ingersoll-Ross model. In all\nother cases, the likelihood has to be approximated. We choose the approximation due to Dacunha-Castelle and Florens-Zmirou (1986) and, to derive a proper\nestimator, we use the local gaussian approximation proposed by Yoshida (1992)\n3\n\n\falthough our result holds for any consistent and asymptotically Gaussian estimator. This approach has been suggested by the work on Akaike Information Criteria\nby Uchida and Yoshida (2005).\nFor continuous time observations from diffusion processes, Vajda (1990) considered the model dX(t) = \u2212b(t)Xt dt + \u03c3(t)dWt ; K\u00fcchler and S\u00f8rensen (1997)\nand Morales et al. (2004) contain several results on the likelihood ratio test statistics and R\u00e9nyi statistics for exponential family of diffusions. Explicit derivations of the R\u00e9nyi information on the invariant law of ergodic diffusion processes\nhave been presented in De Gregorio and Iacus (2007). For small diffusion processes, with continuous time observations, information criteria have been derived\nin Uchida and Yoshida (2004) using Malliavin calculus.\nThe problem of testing statistical hypotheses from general diffusion processes\nis still a developing stream of research. Kutoyants (2004) and Dachian and Kutoyants (2008) consider the problem of testing statistical hypotheses for ergodic\ndiffusion models in continuous time; Kutoyants (1984) and Iacus and Kutoyants\n(2001) consider parametric and semiparametric hypotheses testing for small diffusion processes; Negri and Nishiyama (2007a, b) propose a non parametric test\nbased on score marked empirical process for both continuous and discrete time\nobservation from small diffusion processes further extended to the ergodic case in\nMasuda et al. (2008). Lee and Wee (2008) considered the parametric version of\nthe same test statistics for a simplified model.\nA\u0131\u0308t-Sahalia (1996, 2008), Giet and Lubrano (2008) and Chen et al. (2008)\nproposed tests based on the several distances between parametric and nonparametric estimation of the invariant density of discretely observed ergodic diffusion\nprocesses. The present paper complements the above references.\nThe paper is organized as follows. Section 2 introduces notation and regularity assumptions. Section 3 states the main result. Section 4 contains numerical\nexperiments to test the small sample performance of the proposed test statistics in\nterms of empirical level and empirical power under some alternatives. The proofs\nare contained in Section 5.\n\n2\n\nAssumptions on diffusion model\n\nWe consider the family of one-dimensional diffusion processes {Xt , t \u2208 [0, T ]},\nsolution to\ndXt = b(\u03b1, Xt )dt + \u03c3(\u03b2, Xt )dWt , X0 = x0 ,\n(2.1)\nwhere Wt is a Brownian motion. Let \u03b8 = (\u03b1, \u03b2) \u2208 \u0398\u03b1 \u00d7 \u0398\u03b2 = \u0398, where\n\u0398\u03b1 and \u0398\u03b2 are respectively compact convex subset of Rp and Rq . Furthermore\nwe assume that the drift function b : R\u00d7\u0398\u03b1 \u2192 R and the diffusion coefficient\n\u03c3 : R \u00d7 \u0398\u03b2 \u2192 R are known apart from the parameters \u03b1 and \u03b2. We assume that\n4\n\n\fthe process Xt is ergodic for every \u03b8 with invariant law \u03bc\u03b8 . The process Xt is\nobserved at discrete times ti = i\u2206n , i = 0, 1, 2, ..., n, where \u2206n is the length of\nthe steps. We indicate the observations with Xn = {Xti }06i6n . The asymptotic is\n\u2206n \u2192 0, n\u2206n \u2192 \u221e and n\u22062n \u2192 0 as n \u2192 \u221e.\nIn the definition of the \u03c6-divergence (1.1) the likelihood of the process is need,\nbut as noted in the Introduction, this is usually not know. There are several ways to\napproximate the likelihood of a discretely observed diffusion process (for a review\nsee, e.g., Chap. 3, Iacus, 2008). In this paper, we use the approximation proposed\nby Dacunha-Castelle and Florens-Zmirou (1986) although our result hold true\n(with some adaptations of the proofs) for other approximations, like, e.g. the\none based on Hermite polynomial expansion by A\u0131\u0308t-Sahalia (2002). To write it\nin explicit way, we use the same setup as in Uchida and Yoshida (2005). We\nintroduce the following functions\nZ x\ndu\nb(\u03b1, x)\n\u03c3 \u2032 (\u03b2, x)\ns(x, \u03b2) =\n, B(x, \u03b8) =\n\u2212\n\u03c3(\u03b2, x)\n2\n0 \u03c3(\u03b2, u)\ne \u03b8) = B(s\u22121 (\u03b2, x), \u03b8),\nB(x,\n\ne\ne 2 (x, \u03b8) + B\ne \u2032 (x, \u03b8)\nh(x, \u03b8) = B\n\nThe following set of assumptions ensure the good behaviour of the approximated\nlikelihood and the existence of a weak solution of (2.1)\nAssumption 2.1. [Regularity on the process]\ni) There exists a constant C such that\n|b(\u03b10 , x) \u2212 b(\u03b10 , y)| + |\u03c3(\u03b20 , x) \u2212 \u03c3(\u03b20 , y)| \u2264 C|x \u2212 y|.\nii) inf \u03b2,x \u03c3 2 (\u03b2, x) > 0.\niii) The process X is ergodic for every \u03b8 with invariant probability measure \u03bc\u03b8 .\nAll polynomial moments of \u03bc\u03b8 are finite.\niv) For all m \u2265 0 and for all \u03b8, supt E|Xt |m < \u221e.\nv) For every \u03b8, the coefficients b(\u03b1, x) and \u03c3(\u03b2, x) are twice differentiable with\nrespect to x and the derivatives are polynomial growth in x, uniformly in \u03b8.\nvi) The coefficients b(\u03b1, x) and \u03c3(\u03b2, x) and all their partial derivatives respect\nto x up to order 2 are three times differentiable respect to \u03b8 for all x in\nthe state space. All derivatives respect to \u03b8 are polynomial growth in x,\nuniformly in \u03b8.\nAssumption 2.2. [Regularity for the approximation]\n5\n\n\fi) e\nh(x, \u03b8) = O(|x|2) as x \u2192 \u221e.\n\nii) inf x e\nh(x, \u03b8) > \u2212\u221e for all \u03b8.\n\niii) sup\u03b8 supx |e\nh3 (x, \u03b8)| \u2264 M < \u221e.\n\ne j (x, \u03b8)| =\niv) There exists \u03b3 > 0 such that for every \u03b8 and j = 1, 2, |B\ne \u03b8)|\u03b3 ) as |x| \u2192 \u221e.\nO(|B(x,\n\nAssumption 2.3. [Identifiability] The coefficients b(\u03b1, x) = b(\u03b10 , x) and \u03c3(\u03b2, x) =\n\u03c3(\u03b20 , x) for \u03bc\u03b80 a.s. all x then \u03b1 = \u03b10 and \u03b2 = \u03b20 .\nUnder Assumptions 2.1 and 2.2 Dacunha-Castelle and Florens-Zmirou (1986)\nintroduced the following approximation of transition density f of the process X\nfrom y to x at lag t\n\u001a\n\u001b\n1\nS 2 (x, y, \u03b2)\nexp \u2212\nf (x, y, t, \u03b8) = \u221a\n+ H(x, y, \u03b8) + tg\u0303(x, y, \u03b8)\n2t\n2\u03c0t\u03c3(y, \u03b2)\n(2.2)\nand its logarithm\nS 2 (x, y, \u03b2)\n1\n+ H(x, y, \u03b8) + tg\u0303(x, y, \u03b8)\nl(x, y, t, \u03b8) = \u2212 log(2\u03c0t) \u2212 log \u03c3(y, \u03b2) \u2212\n2\n2t\nwhere\nS(x, y, \u03b2) =\nH(x, y, \u03b8) =\n1\ng\u0303(x, y, \u03b8) = \u2212\n2\n\n\u001a\n\nZ\n\ny\n\nx\n\n\u001a\n\nZ\n\nx\n\ny\n\ndu\n\u03c3(u, \u03b2)\n\n1 \u03c3 \u2032 (\u03b2, u)\nb(\u03b1, u)\n\u2212\n\u03c3 2 (\u03b2, u) 2 \u03c3(\u03b2, u)\n\n\u001b\n\ndu\n\n1\nC(x, \u03b8) + C(y, \u03b8) + B(x, \u03b8)B(y, \u03b8)\n3\n\n\u001b\n\n1\n1\nC(x, \u03b8) = B 2 (x, \u03b8) + B \u2032 (x, \u03b8)\u03c3(x, \u03b2)\n3\n2\nThe approximated likelihood and log-likelihood functions of the observations Xn\nbecome respectively\nfn (Xn , \u03b8) =\n\nn\nY\n\nf (\u2206n , Xti\u22121 , Xti , \u03b8)\n\nn\nX\n\nl(\u2206n , Xti\u22121 , Xti , \u03b8)\n\ni=1\n\nln (Xn , \u03b8) =\n\ni=1\n\n6\n\n\f3\n\nConstruction of the test statistics and results\n\nConsider the divergence defined in (1.1) and let \u03c6(*) be such that \u03c6(1) = 0 and,\nwhen they exist, define C\u03c6 = \u03c6\u2032 (1) and K\u03c6 = \u03c6\u2032\u2032 (1). We consider three different\nsetup\nAssumption 3.1. C\u03c6 6= 0 is a finite constant depending only on \u03c6 and independent\nof \u03b8;\nAssumption 3.2. C\u03c6 = 0 and K\u03c6 6= 0 is a finite constant depending only on \u03c6\nand independent of \u03b8;\nAssumption 3.3. C\u03c6 6= 0 and K\u03c6 6= 0 are finite constants depending only on \u03c6\nand independent of \u03b8;\nRemark 3.1. The above Assumptions are not so strong. In fact, for example\nthe \u03b1-divergences D\u03c6\u03b1 (\u03b8, \u03b80 ) satisfy the Assumptions 3.1 and 3.3, while for the\npower-divergences D\u03c6\u03bb (\u03b8, \u03b80 ) it's easy to verify that C\u03c6 = \u03c6\u2032 (1) = 0.\nClearly, the quantity D\u03c6 (\u03b8, \u03b80 ) measures the discrepancy between \u03b8 and the\ntrue value of the parameter \u03b80 and is an ideal candidate to construct a test statistics.\nLet \u03b8\u0303n (Xn ) be any consistent estimator of \u03b80 and such that\nd\n\n\u0393\u22121/2 (\u03b8\u0303n (Xn ) \u2212 \u03b80 ) \u2192 N(0, I(\u03b80 )\u22121 )\n\n(3.1)\n\nwhere I(\u03b80 ) is the positive definite and invertible Fisher information matrix at \u03b80\nequal to\n\u0012 kj\n\u0013\n(Ib (\u03b80 ))k,j=1,...,p\n0\nI(\u03b80 ) =\n0\n(I\u03c3kj (\u03b80 ))k,j=1,...,q\n\nwhere\n\nIbkj (\u03b80 )\nI\u03c3kj (\u03b80 )\n\n=\n\nZ\n\n=2\n\nZ\n\n1\n\u03c3 2 (\u03b20 , x)\n\n\u2202b(\u03b10 , x) \u2202b(\u03b10 , x)\n\u03bc\u03b80 (dx)\n\u2202\u03b1k\n\u2202\u03b1j\n\n1\n\u03c3 2 (\u03b20 , x)\n\n\u2202\u03c3(\u03b20 , x) \u2202\u03c3(\u03b20 , x)\n\u03bc\u03b80 (dx)\n\u2202\u03b2k\n\u2202\u03b2j\n\nWe indicate with \u0393 the (p + q) \u00d7 (p + q) matrix\n\u0012 1\n\u0013\nI\n0\np\nn\u2206n\n\u0393=\n1\n0\nI\nn q\nand Ip is the p \u00d7 p identity matrix. Using the approximated likelihood fn (Xn , \u03b8)\nand fn (Xn , \u03b80 ), the \u03c6-divergence in (1.1) becomes\n\u0013\n\u0012\nfn (Xn , \u03b8)\n(3.2)\nD\u03c6 (\u03b8, \u03b80 ) = E\u03b80 \u03c6\nfn (Xn , \u03b80 )\n7\n\n\fTo construct a test statistics we replace \u03b8 by the estimator \u03b8\u0303n (Xn ) and, having\nonly one single observation of Xn , i.e. only one observed trajectory, we estimate\n(3.2) with\n!\nfn (Xn , \u03b8\u0303n (Xn ))\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) = \u03c6\n(3.3)\nfn (Xn , \u03b80 )\nPlease notice that, conversely to the i.i.d. case, there is no integral in the definition\nof (3.3). We will discuss this point after the presentation of the Theorem 3.1.\nThe proposed test for testing H0 : \u03b8 = \u03b80 versus H1 : \u03b8 6= \u03b80 is realized as\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) = 0 versus D\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) 6= 0.\nTheorem 3.1. Under H0 : \u03b8 = \u03b80 , Assumptions 2.1-2.3, convergence (3.1), we\nhave that\ni) if function \u03c6(*) satisfies Assumption 3.1, then\nd\n\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) \u2192 C\u03c6 \u03c72p+q\n\n(3.4)\n\nii) if function \u03c6(*) satisfies Assumption 3.2, then\nd\n\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) \u2192\nwhere\n\np\n\nK\u03c6\nZp+q\n2\n\n(3.5)\n\nZp+q = \u03c72p+q .\n\niii) if function \u03c6(*) satisfies Assumption 3.3, then\nd 1\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) \u2192 (C\u03c6 \u03c72p+q + (C\u03c6 + K\u03c6 )Zp+q )\n2\n\n(3.6)\n\nRemark 3.2. It's clear that for C\u03c6 = 0 from (3.6) we immediately reobtain the\nconvergence result (3.5).\nRemark 3.3. If we consider the limits as \u03b1 \u2192 \u22121 for \u03c6\u03b1 (x) of the \u03b1-divergences,\ni.e. we consider the Kullback-Leibler divergence, we have\n\u03c6(x) = lim \u03c6\u03b1 (x) = \u2212 log(x)\n\u03b1\u2192\u22121\n\nfor which C\u03c6 = \u22121 and K\u03c6 = 1. In that case, (3.6) reduces to the standard result\nfor the likelihood ratio test statistics.\nThe convergence in Theorem 3.1 may appear somewhat strange if one thinks\nabout the usual results on \u03c6-divergences for i.i.d. observations. The main difference in diffusion models, is that our estimate of the divergence has not the usual\n8\n\n\fform of an expected value, i.e. it estimates the expected value with one observation only. This is why, in the i.i.d case, the first term in the Taylor expansion of\nD\u03c6 vanishes being the expected value of the score function, while in our case it\nremains only the score function which, as usual, converges to a Gaussian random\nvariable. For the same reason, in the second term of the Taylor expansion, in the\ni.i.d. case appears the expected value of the second order derivative which converges to the Fisher information and, in our case, we have not the expected value,\nhence the convergence to the square of the \u03c72 emerges.\nIf one wants to emulate the standard results for the i.i.d. case, it is still possible\nto work on the invariant density of the diffusion process. In that case, the \u03c6divergence takes the usual form of the i.i.d. case because the invariant density\nhave the explicit form. Indeed, let\n\u001a Z x\n\u001b\nb(y, \u03b8)\n1\ns(x, \u03b8) = exp \u22122\ndy , m(x, \u03b8) = 2\n2\n\u03c3 (x, \u03b8)s(x, \u03b8)\nx\u0303 \u03c3 (y, \u03b8)\nbe the scale and speed functions of the diffusion,\nwith x\u0303 some value in the state\nR\nspace of the diffusion process. Let M = m(x, \u03b8)dx, then \u03c0(x, \u03b8) = m(x, \u03b8)/M\nis the invariant density of the diffusion process. In this case, it is possible to define\nthe \u03c6-divergence as\n!\nZ\n\u03c0(x, \u03b8\u0303n )\n\u03c0(x, \u03b80 )dx\nD\u03c6 (\u03b8\u0303n , \u03b80 ) = \u03c6\n\u03c0(x, \u03b80 )\nand the standard results follows.\nRemark 3.4. In our application, to derive and estimator, we consider further the\nlocal gaussian approximation of the same transition density (see, Yoshida, 1992)\ngn (Xn , \u03b8) =\n\nn\nX\n\ngn (\u2206n , Xti\u22121 , Xti , \u03b8)\n\n(3.7)\n\ni=1\n\nwhere\n1\n[y \u2212 x \u2212 tb(\u03b1, x)]2\ng(t, x, y, \u03b8) = \u2212 log(2\u03c0t) \u2212 log \u03c3(\u03b2, x) \u2212\n2\n2t\u03c3 2 (\u03b2, x)\nThe approximate maximum likelihood estimator \u03b8\u0302n (Xn ) based on (3.7) is then\ndefined as\n\u03b8\u0302n (Xn ) = arg sup gn (Xn , \u03b8)\n(3.8)\n\u03b8\n\nn\u22062n\n\nUnder the condition\n\u2192 0 (see Theorem 1 in Kessler, 1997) the estimator \u03b8\u0302n (Xn ) in (3.8) satisfies (3.1). Hence, the result of Theorem 3.1 applies for\n\u03b8\u0303n (Xn ) = \u03b8\u0302n (Xn ).\n9\n\n\fRemark 3.5. In Theorem 3.1 there is no need to impose C\u03c6 = 0 and K\u03c6 = 1 as,\ne.g. in Morales et al. (1997). Of course, in our case the constants C\u03c6 and K\u03c6\nenter in the asymptotic distribution of the test statistics. The convergence result\nis also interesting because, contrary to the i.i.d case, the rate of convergence of\nthe estimators of \u03b8 for\nand diffusion coefficients are different and are\n\u221a the drift \u221a\nrespectively equal to n\u2206n and n.\nRemark 3.6. As remarked in Uchida and Yoshida (2001), it is always better to derive approximate ML estimators and the test statistics on different approximations\nof the true likelihood to avoid circularities.\n\n4\n\nNumerical analysis\n\nAlthough asymptotic properties have been obtained, what really matters in application is the behaviour of the test statistics under fine sample setup. We study the\nempirical performance of the test for small samples in terms of level of the test\nand power under some alternatives. In the analysis we consider the estimator (3.8)\nand the following quantities\n\u2022 estimated \u03b1-divergences\nD\u03b1 (\u03b8\u0302n (Xn ), \u03b80 ) = \u03c6\u03b1\n\nfn (Xn , \u03b8\u0302n (Xn ))\nfn (Xn , \u03b80 )\n\n!\n\n1+\u03b1\n\n2\nwith \u03c6\u03b1 (x) = 4(1 \u2212 x 2 )/(1 \u2212 \u03b12 ), with C\u03b1 = \u03b1\u22121\nand K\u03c6 = 1. We\nconsider \u03b1 \u2208 {\u22120.99, \u22120.90, \u22120.75, \u22120.50, \u22120.25, \u22120.10};\n\n\u2022 estimated power-divergences\nD\u03bb (\u03b8\u0302n (Xn ), \u03b80 ) = \u03c6\u03bb\n\nfn (Xn , \u03b8\u0302n (Xn ))\nfn (Xn , \u03b80 )\n\n!\n\nwith \u03c6\u03bb (x) = (x\u03bb+1 \u2212 x \u2212 \u03bb(x \u2212 1))/(\u03bb(\u03bb + 1)), with C\u03bb = 0, K\u03bb = 1.\nWe consider \u03bb \u2208 {\u22120.99, \u22121.20, \u22121.50, \u22121.75, \u22122.00, \u22122.50};\n\u2022 likelihood ratio statistic\nDlog (\u03b8\u0302n (Xn ), \u03b80 ) = \u2212 log\n\n10\n\nfn (Xn , \u03b8\u0302n (Xn ))\nfn (Xn , \u03b80 )\n\n!\n\n\fFor D\u03b1 and D\u03bb , the threshold of the rejection region of the test are calculated using\nformula (3.6) as the empirical quantiles of (3.6) of 100000 simulations of the random variable \u03c72p+q . For Dlog is again used formula (3.6) but exact quantiles of the\nrandom variable \u03c72p+q are used. Because the interest is in testing D\u03c6 = 0 against\nD\u03c6 6= 0, whenever fn (Xn , \u03b8\u0303n (Xn )) > fn (Xn , \u03b80 ) we exchange the numerator and\nthe denominator to avoid negative signs in the test statistics. Usually, this is not\ngoing to happen if \u03c6 is convex and \u03c6\u2032 (1) = 0 (see, e.g. Morales et al., 1997).\nWe evaluate the empirical level of the test calculated as the number of times\nthe test rejects the null hypothesis under the true model, i.e.\n\u03b1\u0302n =\n\nM\n1 X\n1{D\u03c6 >c\u03b1 }\nM i=1\n\nwhere 1A is the indicator function of set A, M = 10000 is the number of simulations and c\u03b1 is the (1 \u2212 \u03b1)% quantile of the proper distribution. Similarly we\ncalculate the power of the test under alternative models as\nM\n1 X\n1{D\u03c6 >c\u03b1 }\n\u03b2\u0302n =\nM i=1\n\nIn our experiments we consider the two families of stochastic processes borrowed\nfrom finance\n\u2022 the Vasicek (VAS) model\ndXt = \u03ba(\u03b1 \u2212 Xt )dt + \u03c3Xt dWt\nwhere, in finance, \u03c3 is interpreted as volatility, \u03b1 is the long-run equilibrium value of the process and \u03ba is the speed of reversion. Let (\u03ba0 , \u03b10 , \u03c302 ) =\n(0.85837, 0.089102, 0.0021854), we consider three different sets of hypotheses for the parameters\nmodel\n\u03b8 = (\u03ba, \u03b1, \u03c3 2 )\nVAS0\n(\u03ba0 , \u03b10 , \u03c302 )\nVAS1 (4 * \u03ba0 , \u03b10 , 4 * \u03c302 )\nVAS2\n( 14 \u03ba0 , \u03b10 , 14 * \u03c302 )\nThe interesting facts are that VAS0 , VAS1 and VAS2 have all the same stationary distributions N(\u03b10 , \u03c302 /(2\u03ba0 )), a Gaussian transition density\n\u0012\n\u0013\n2\n\u22122\u03bat\n)\n\u2212\u03bat \u03c30 (1 \u2212 e\nN \u03b10 + (x0 \u2212 \u03b10 )e ,\n2\u03ba0\n11\n\n\fand covariance function given by\nCov(Xs , Xt ) =\n\n\u03c302 \u2212\u03ba(s+t) \u22122\u03ba(s\u2227t)\u22121 \u0001\ne\ne\n2\u03ba0\n\nand both show a strong dependency of the covariance as a function of \u03ba,\nwhich makes this model interesting in comparison with the i.i.d. setting;\n\u2022 the Cox-Ingersoll-Ross (CIR) model\ndXt = \u03ba(\u03b1 \u2212 Xt )dt + \u03c3\n\np\n\nXt dWt\n\nLet (\u03ba0 , \u03b10 , \u03c302 ) = (0.89218, 0.09045, 0.032742), we consider different sets\nof hypotheses for the parameters\nmodel\n\u03b8 = (\u03ba, \u03b1, \u03c3 2 )\nCIR0\n(\u03ba0 , \u03b10 , \u03c302 )\nCIR1 ( 21 * \u03ba0 , \u03b10 , 12 * \u03c302 )\nCIR2 ( 41 * \u03ba0 , \u03b10 , 14 * \u03c302 )\nThis model has a transition density of \u03c72 -type, hence local gaussian approximation is less likely to hold for non negligible values of \u2206n .\nThe parameters of the above models, have been chosen according to Pritsker\n(1998) and Chen et al. (2008), in particular VAS0 corresponds to the model estimated by A\u0131\u0308t-Sahalia (1996) for real interest rates data.\nWe study the level and the power of the three family of test statistics for different values of \u2206n \u2208 {0.1, 0.001} and n \u2208 {50, 100, 500}. For the same trajectory,\nhence we simulate 1000 observations and we extract only that last n observations.\nDisregarding the first part of the trajectory ensures that the process is in the stationary state.\nThe results of these simulations are reported in the Tables 1-9. We point out\nthat in the Tables 2, 4, 7 and 9, in the column \"model (\u03b1, n)\" the \u03b1 corresponds to\nthe true level of the test used to calculate c\u03b1 . The other \u03b1's in the first row of the\ntables correspond to the \u03b1 in \u03c6\u03b1 -divergences.\nSummary of the analysis for the Vasicek model It turns out that \u03b1-divergences\nare not very good in terms of estimated level of the test, but their power function\nbehaves as expected. It also emerges that for \u03bb = \u22120.99, the power divergence\ncannot identify as wrong model VAS1 for small sample size n = 50 and \u2206n =\n0.001 (Table 3, row 2), although this is not the case for the power-divergences and\nthe likelihood ratio test (Tables 2 and 1, row 2).\n12\n\n\fIn general power divergences for \u03bb in {\u22120.99, \u22121.20, \u22121.50, \u22121.75, \u22122.00}\nhave always very small estimated level and high power under the selected alternatives. The \u03b1-divergences, do not behave very good and, the way they are defined,\nonly approximate the likelihood ratio for \u03b1 = \u22120.99.\nThe power divergences are, on average, better than the likelihood ratio test in\nterms of both empirical level \u03b1\u0302 and power \u03b2\u0302 under the selected alternatives.\nSummary of the analysis for the CIR model The same average considerations\napply to the case of CIR model. The difference is that, for small sample size,\nall test statistics have low power under the alternative CIR1 while CIR2 doesn't\npresent particular problems.\n\n5\n\nProofs\n\nThe following important Lemmas are useful to prove the Theorem 3.1.\nLemma 5.1 (Kessler, 1997). Under the assumptions 2.1-2.3, as n\u22062n \u2192 0 the\nfollowing hold true\np\n\n1\n\n\u0393 2 \u2207\u03b8 gn (Xn , \u03b80 ) \u2192 N(0, I(\u03b80 ))\n\n(5.1)\n\nLemma 5.2 (Uchida and Yoshida, 2005). Under the assumptions 2.1-2.3, as\nn\u22062n \u2192 0 the following hold true\n1\n\n1\n\n\u0393 2 \u2207\u03b8 ln (Xn , \u03b80 ) = \u0393 2 \u2207\u03b8 gn (Xn , \u03b80 ) + op (1)\n\n(5.2)\n\nLemma 5.3 (Uchida and Yoshida, 2005). Under the assumptions 2.1-2.3, as\nn\u22062n \u2192 0 the following hold true\n1\n\n1\n\np\n\n\u0393 2 \u22072\u03b8 ln (Xn , \u03b80 )\u0393 2 \u2192 \u2212I(\u03b80 )\n\n(5.3)\n\nProof of Theorem 3.1. We start by applying delta method. We denote the gradient\nvector by \u2207\u03b8 = [\u2202/\u2202\u03b8i ], i = 1, . . . , p + q and similarly the Hessian matrix by\n\u22072\u03b8 = [\u2202 2 /\u2202\u03b8i \u2202\u03b8j ], i, j = 1, . . . , p + q.\ni) We can write that\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) = D\u03c6 (\u03b80 , \u03b80 ) + [\u2207\u03b8 D\u03c6 (\u03b80 , \u03b80 )]T (\u03b8\u0303n (Xn ) \u2212 \u03b80 ) + op (1)\n= [\u2207\u03b8 D\u03c6 (\u03b80 , \u03b80 )]T (\u03b8\u0303n (Xn ) \u2212 \u03b80 ) + op (1)\nbecause D\u03c6 (\u03b80 , \u03b80 ) = 0. Noting that for k = 1, ..., p + q\n\u0014 \u0012\n\u0013\u0015\n\u0012\n\u0013\n\u2202\nfn (*, \u03b8)\n1\nfn (*, \u03b8) \u2202fn (*, \u03b8)\n\u2032\n\u03c6\n=\n\u03c6\n\u2202\u03b8k\nfn (*, \u03b80 )\nfn (*, \u03b80 )\nfn (*, \u03b80 )\n\u2202\u03b8k\n13\n\n\fby Assumption 3.1 follows that\n\u2207\u03b8 D\u03c6 (\u03b80 , \u03b80 ) = C\u03c6 \u2207\u03b8 ln (Xn , \u03b8)|\u03b8=\u03b80 = C\u03c6 \u2207\u03b8 ln (Xn , \u03b80 )\nand therefore\nh 1\niT\n1\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) = C\u03c6 \u0393 2 \u2207\u03b8 ln (Xn , \u03b80 ) \u0393\u2212 2 (\u03b8\u0303n (Xn ) \u2212 \u03b80 ) + op (1)\n\n(5.4)\n\nFrom (5.4) by means of Lemma 5.2-5.1 and Slutsky's Theorem immediately follows\nd\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) \u2192 C\u03c6 \u03c72p+q\nii) Since for k, j = 1, ..., p + q\n\u0014 \u0012\n\u0013\u0015\n\u0012\n\u0013\n\u22022\nfn (*, \u03b8)\n1\nfn (*, \u03b8) \u2202fn (*, \u03b8) \u2202fn (*, \u03b8)\n\u2032\u2032\n\u03c6\n=\n\u03c6\n\u2202\u03b8k \u2202\u03b8j\nfn (*, \u03b80 )\nfn2 (*, \u03b80 )\nfn (*, \u03b80 )\n\u2202\u03b8k\n\u2202\u03b8j\n\u0012\n\u0013 2\n1\nfn (*, \u03b8) \u2202 fn (*, \u03b8)\n+\n\u03c6\u2032\nfn (*, \u03b80 )\nfn (*, \u03b80 )\n\u2202\u03b8k \u2202\u03b8j\n\nfollows that\n1 \u22121/2\n[\u0393\n(\u03b8\u0303n (Xn ) \u2212 \u03b80 )]T \u03931/2 \u22072\u03b8 D\u03c6 (\u03b80 , \u03b80 )\u03931/2\n2\n\u00d7\u0393\u22121/2 (\u03b8\u0303n (Xn ) \u2212 \u03b80 ) + op (1)\nK\u03c6 \u22121/2\n=\n[\u0393\n(\u03b8\u0303n (Xn ) \u2212 \u03b80 )]T \u03931/2 \u2207\u03b8 ln (Xn , \u03b80 )[\u03931/2 \u2207\u03b8 ln (Xn , \u03b80 )]T\n2\n\u00d7\u0393\u22121/2 (\u03b8\u0303n (Xn ) \u2212 \u03b80 ) + op (1)\n\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) =\n\nFrom (5.4) by means of Lemma 5.2-5.1 and Slutsky's Theorem immediately follows\nK\u03c6\nZp+q\n2\nIt's easy to verify that the density function of the r.v. Zp+q is equal to\nd\n\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) \u2192\np+q\n\n(1/2) 2 \u221a p+q\n\u22121 \u2212\u221az/2 1\n2\n\u0001\n\u221a ,\nfZp+q (z) =\nz\ne\n2 z\n\u0393 p+q\n2\n\nz>0\n\n(5.5)\n\niii) By previous considerations we have that\n\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) = [\u2207\u03b8 D\u03c6 (\u03b80 , \u03b80 )]T (\u03b8\u0303n (Xn ) \u2212 \u03b80 )\n1\n+ [\u0393\u22121/2 (\u03b8\u0303n (Xn ) \u2212 \u03b80 )]T \u03931/2 \u22072\u03b8 D\u03c6 (\u03b80 , \u03b80 )\u03931/2\n2\n\u00d7\u0393\u22121/2 (\u03b8\u0303n (Xn ) \u2212 \u03b80 ) + op (1)\n(5.6)\n14\n\n\fwhere\n\u22072\u03b8 D\u03c6 (\u03b80 , \u03b80 )\n\n1\n\u22072 f (Xn , \u03b80 )\nf (Xn , \u03b80 ) \u03b8\n= (K\u03c6 + C\u03c6 )\u2207\u03b8 ln (Xn , \u03b80 )[\u2207\u03b8 ln (Xn , \u03b80 )]T + C\u03c6 \u22072\u03b8 ln (Xn , \u03b80 ) (5.7)\n= K\u03c6 \u2207\u03b8 ln (Xn , \u03b80 )[\u2207\u03b8 ln (Xn , \u03b80 )]T + C\u03c6\n\nPlugging in (5.6) the quantity (5.7) we derive, applying again Lemma 5.1-5.3, the\nfollowing result\nd\n\nD\u03c6 (\u03b8\u0303n (Xn ), \u03b80 ) \u2192\n\n\u0003\n1\u0002\nC\u03c6 \u03c72p+q + (C\u03c6 + K\u03c6 )Zp+q\n2\n\nConclusions\nIt seems that, as in the i.i.d. case, also for discretely observed diffusion processes\nthe \u03c6-divergences may compete or improve the performance of the standard likelihood ratio statistics. In particular, the power divergences are in general quite\ngood in terms of estimated level and power of the test even for moderate sample\nsizes (e.g. n \u2265 100 in our simulations).\nThe package sde for the R statistical environment (R Development Core\nTeam, 2008) and freely available at http://cran.R-Project.org contains the function sdeDiv which implements the \u03c6-divergence test statistics.\n\nReferences\n[1] A\u0131\u0308t-Sahalia, Y. (1996) Testing continuous-time models of the spot interest\nrate, Rev. Financial Stud., 70(2), 385-426.\n[2] A\u0131\u0308t-Sahalia, Y. (2002) Maximum-likelihood estimation for discretelysampled diffusions: a closed-form approximation apporach, Econometrica,\n70, 223-262.\n[3] A\u0131\u0308t-Sahalia, Y., Fan, J., Peng, H. (2006) Nonparametric transition based tests for jump di?usions. Working paper. Available at\nhttp://www.princeton.edu/yacine/research.htm\n[4] Amari, S.(1985) Differential-Geometrical Methods in Statistics, Lecture\nNotes in Statist., Vol. 28, New York, Springer.\n15\n\n\f[5] Beran, R.J. (1977) Minimum Hellinger estimates for parametric models, Annals of Statistics, 5, 445-463.\n[6] Burbea, J. (1984) The convexity with respect to gaussian distribution of divergences of order \u03b1, Utilitas Math., 26, 171-192.\n[7] Chandra, S.A., Taniguchi, M. (2006) Minimum \u03b1-divergence estimation for\narch models, Journal of Time Series Analysis, 27(1), 19-39.\n[8] Chen, S.X., Gao, J., Cheng, Y.T. (2008) A test for model specification of\ndiffusion processes, Ann. Stat., 36(1), 167-198.\n[9] Cressie, N., Read, T.R.C. (1984) Multinomial goodness of fit tests, J. Roy.\nStatist. Soc. Ser B, 46, 440-464.\n[10] Csisz\u00e1r, I. (1967) On topological properties of f -divergences, Studia Scientific Mathematicae Hungarian, 2, 329-339.\n[11] Dachian, S., Kutoyants, Yu. A. (2008) On the goodness-of-t tests for some\ncontinuous time processes, in Statistical Models and Methods for Biomedical and Technical Systems, 395-413. Vonta, F., Nikulin, M., Limnios, N. and\nHuber-Carol C. (Eds), Birkhuser, Boston.\n[12] Dacunha-Castelle, D., Florens-Zmirou, D. (1986) Estimation of the coefficients of a diffusion from discrete observations, Stochastics, 19, 263-284.\n[13] De Gregorio, A., Iacus, S.M. (2007) R\u00e9nyi information for ergodic diffusion\nprocesses. Available at http://arxiv.org/abs/0711.1789\n[14] Giet, L., Lubrano, M. (2008) A minimum Hellinger distance estimator for\nstochastic differential equations: An application to statistical inference for\ncontinuous time interest rate models, Computational Statistics and Data\nAnalysis, 52, 2945-2965.\n[15] Iacus, S.M. (2008) Simulation and Inference for Stochastic Differential\nEquations, Springer, New York.\n[16] Iacus, S.M., Kutoyants, Y. (2001) Semiparametric hypotheses testing for\ndynamical systems with small noise, Mathematical Methods of Statistics,\n10(1), 105-120.\n[17] Kutoyants, Y. (1994) Identification of Dynamical Systems with Small Noise,\nKluwer, Dordrecht.\n\n16\n\n\f[18] Kutoyants, Y. (2004) Statistical Inference for Ergodic Diffusion Processes,\nSpringer-Verlag, London.\n[19] K\u00fcchler, U., S\u00f8rensen, M. (1997) Exponential Families of Stochastic Processes, Springer, New York.\n[20] Lee, S., Wee, I.-S. (2008) Residual emprical process for diffusion processes,\nJournal of Korean Mathematical Society, 45(3), 2008.\n[21] Liese, F., Vajda, I. (1987) Convex Statistical Distances, Tuebner, Leipzig.\n[22] Morales, D., Pardo, L., Vajda, I. (1997) Some New Statistics for Testing\nHypotheses in Parametric Models, Journal of Multivariate Analysis, 67, 137168.\n[23] Morales, D., Pardo, L., Pardo,, M.C., Vajda, I. (2004) R\u00e9nyi statistics in\ndirected families of exponential experiments, Statistics, 38(2), 133-147.\n[24] Negri, I., Nishiyama, Y. (2007a) Goodness of t test for ergodic diffusion\nprocesses, to appear in Ann. Inst. Statist. Math.\n[25] Negri, I. and Nishiyama, Y. (2007b). Goodness of t test for small diffusions\nbased on discrete observationsm, Research Memorandum 1054, Inst. Statist.\nMath., Tokyo.\n[26] Masuda, H., Negri, I., Nishiyama, Y. (2008) Goodness of fit test for ergodic\ndiffusions by discrete time observations: an innovation martingale approach,\nResearch Memorandum 1069, Inst. Statist. Math., Tokyo.\n[27] Pritsker, M. (1998) Nonparametric density estimation and tests of continuous\ntime interest rate models, Rev. financial Studies, 11, 449-487.\n[28] R Development Core Team (2008) R: A language and environment for statistical computing, R Foundation for Statistical Computing, Vienna, Austria.\nISBN 3-900051-07-0, URL http://www.R-project.org\n[29] R\u00e9nyi, A. (1961) On measures of entropy and information, Proceedings of\nthe Fourth Berkeley Symposium on Probability and Mathematical Statistics,\nVol 1, University of California, Berkeley, 547-461.\n[30] Rivas, M.J., Santos, M.T., Morales, D. (2005) R\u00e9nyi test statistics for partially observed diffusion processes, Journal of Statistical Planning and Inference, 127, 91-102.\n\n17\n\n\f[31] Simpson, D.G. (1989) Hellinger deviance tests: Efficinecy, breakdown\npoints, and examples, J. Amer. Statist. Assoc., 84, 107-113.\n[32] Uchida, M., Yoshida, N. (2004) Information Criteria for Small Diffusions\nvia the Theory of Malliavin-Watanabe, Statistical Inference for Stochastic\nProcesses, 7, 35-67.\n[33] Uchida, M., Yoshida, N. (2005) AIC for ergodic diffusion processes from\ndiscrete observations, preprint MHF 2005-12, march 2005, Faculty of Mathematics, Kyushu University, Fukuoka, Japan.\n[34] Yoshida, N. (1992) Estimation for diffusion processes from discrete observation, J. Multivar. Anal., 41(2), 220\u2013242.\n\n18\n\n\fmodel (n)\nVAS0 (50)\nVAS1 (50)\nVAS2 (50)\n\n\u03b1 = 0.01 \u03b1 = 0.05\n0.01\n0.04\n1.00\n1.00\n1.00\n1.00\n\nVAS0 (100)\nVAS1 (100)\nVAS2 (100)\n\n0.01\n1.00\n1.00\n\n0.04\n1.00\n1.00\n\nVAS0 (500)\n0.01\n0.07\nVAS1 (500)\n1.00\n1.00\nVAS2 (500)\n1.00\n1.00\nmodel (n)\n\u03b1 = 0.01 \u03b1 = 0.05\nVAS0 (50)\n0.01\n0.04\nVAS1 (50)\n1.00\n1.00\nVAS2 (50)\n1.00\n1.00\nVAS0 (100)\nVAS1 (100)\nVAS2 (100)\n\n0.01\n1.00\n1.00\n\n0.04\n1.00\n1.00\n\nVAS0 (500)\nVAS1 (500)\nVAS2 (500)\n\n0.00\n1.00\n1.00\n\n0.02\n1.00\n1.00\n\nTable 1: Numbers represent probability of rejection under the true generating\nmodel, with c\u03b1 calculated under H0 . Therefore, the values are \u03b1\u0302 under model \"0\"\nand \u03b2\u0302 otherwise. Estimates calculated on 10000 experiments. Likelihood ratio,\nfor \u2206n = 0.001 (up) and \u2206n = 0.1 (bottom).\n\n19\n\n\fmodel (\u03b1, n)\nVAS0 (0.01, 50)\nVAS1 (0.01, 50)\nVAS2 (0.01, 50)\n\n\u03b1 = \u22120.99 \u03b1 = \u22120.90 \u03b1 = \u22120.75 \u03b1 = \u22120.50 \u03b1 = \u22120.25 \u03b1 = \u22120.10\n0.01\n0.10\n0.39\n0.62\n0.73\n0.77\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.04\n1.00\n1.00\n\n0.12\n1.00\n1.00\n\n0.39\n1.00\n1.00\n\n0.62\n1.00\n1.00\n\n0.73\n1.00\n1.00\n\n0.77\n1.00\n1.00\n\nVAS0 (0.01, 100)\nVAS1 (0.01, 100)\nVAS2 (0.01, 100)\n\n0.01\n1.00\n1.00\n\n0.10\n1.00\n1.00\n\n0.39\n1.00\n1.00\n\n0.63\n1.00\n1.00\n\n0.74\n1.00\n1.00\n\n0.78\n1.00\n1.00\n\nVAS0 (0.05, 100)\nVAS1 (0.05, 100)\nVAS2 (0.05, 100)\n\n0.04\n1.00\n1.00\n\n0.11\n1.00\n1.00\n\n0.40\n1.00\n1.00\n\n0.63\n1.00\n1.00\n\n0.74\n1.00\n1.00\n\n0.78\n1.00\n1.00\n\nVAS0 (0.01, 500)\nVAS1 (0.01, 500)\nVAS2 (0.01, 500)\n\n0.02\n1.00\n1.00\n\n0.18\n1.00\n1.00\n\n0.61\n1.00\n1.00\n\n0.83\n1.00\n1.00\n\n0.90\n1.00\n1.00\n\n0.92\n1.00\n1.00\n\nVAS0 (0.05, 500)\nVAS1 (0.05, 500)\nVAS2 (0.05, 500)\n\n0.07\n1.00\n1.00\n\n0.20\n1.00\n1.00\n\n0.61\n1.00\n1.00\n\n0.83\n1.00\n1.00\n\n0.90\n1.00\n1.00\n\n0.92\n1.00\n1.00\n\n20\n\nVAS0 (0.05, 50)\nVAS1 (0.05, 50)\nVAS2 (0.05, 50)\n\nTable 2: Numbers represent probability of rejection under the true generating model, with c\u03b1 calculated under H0 . Therefore,\nthe values are \u03b1\u0302 under model \"0\" and \u03b2\u0302 otherwise. Estimates calculated on 10000 experiments. \u03b1-divergences, for \u2206n =\n0.001.\n\n\fmodel (\u03b1, n)\nVAS0 (0.01, 50)\nVAS1 (0.01, 50)\nVAS2 (0.01, 50)\n\n\u03bb = \u22120.99 \u03bb = \u22121.20 \u03bb = \u22121.50 \u03bb = \u22121.75 \u03bb = \u22122.00 \u03bb = \u22122.50\n0.00\n0.00\n0.00\n0.01\n0.02\n0.04\n0.00\n0.99\n1.00\n1.00\n1.00\n1.00\n0.40\n1.00\n1.00\n1.00\n1.00\n1.00\n0.00\n0.67\n0.99\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.03\n1.00\n1.00\n\n0.06\n1.00\n1.00\n\nVAS0 (0.01, 100)\nVAS1 (0.01, 100)\nVAS2 (0.01, 100)\n\n0.00\n0.23\n0.88\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.02\n1.00\n1.00\n\n0.04\n1.00\n1.00\n\nVAS0 (0.05, 100)\nVAS1 (0.05, 100)\nVAS2 (0.05, 100)\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.03\n1.00\n1.00\n\n0.06\n1.00\n1.00\n\nVAS0 (0.01, 500)\nVAS1 (0.01, 500)\nVAS2 (0.01, 500)\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.03\n1.00\n1.00\n\n0.08\n1.00\n1.00\n\nVAS0 (0.05, 500)\nVAS1 (0.05, 500)\nVAS2 (0.05, 500)\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.03\n1.00\n1.00\n\n0.06\n1.00\n1.00\n\n0.12\n1.00\n1.00\n\n21\n\nVAS0 (0.05, 50)\nVAS1 (0.05, 50)\nVAS2 (0.05, 50)\n\nTable 3: Numbers represent probability of rejection under the true generating model, with c\u03b1 calculated under H0 . Therefore,\nthe values are \u03b1\u0302 under model \"0\" and \u03b2\u0302 otherwise. Estimates calculated on 10000 experiments. Power-divergences for\n\u2206n = 0.001\n\n\fmodel (\u03b1, n)\nVAS0 (0.01, 50)\nVAS1 (0.01, 50)\nVAS2 (0.01, 50)\n\n\u03b1 = \u22120.99 \u03b1 = \u22120.90 \u03b1 = \u22120.75 \u03b1 = \u22120.50 \u03b1 = \u22120.25 \u03b1 = \u22120.10\n0.01\n0.15\n0.55\n0.78\n0.86\n0.88\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.05\n1.00\n1.00\n\n0.17\n1.00\n1.00\n\n0.55\n1.00\n1.00\n\n0.78\n1.00\n1.00\n\n0.86\n1.00\n1.00\n\n0.88\n1.00\n1.00\n\nVAS0 (0.01, 100)\nVAS1 (0.01, 100)\nVAS2 (0.01, 100)\n\n0.01\n1.00\n1.00\n\n0.13\n1.00\n1.00\n\n0.48\n1.00\n1.00\n\n0.71\n1.00\n1.00\n\n0.80\n1.00\n1.00\n\n0.83\n1.00\n1.00\n\nVAS0 (0.05, 100)\nVAS1 (0.05, 100)\nVAS2 (0.05, 100)\n\n0.04\n1.00\n1.00\n\n0.15\n1.00\n1.00\n\n0.48\n1.00\n1.00\n\n0.71\n1.00\n1.00\n\n0.80\n1.00\n1.00\n\n0.83\n1.00\n1.00\n\nVAS0 (0.01, 500)\nVAS1 (0.01, 500)\nVAS2 (0.01, 500)\n\n0.00\n1.00\n1.00\n\n0.06\n1.00\n1.00\n\n0.25\n1.00\n1.00\n\n0.54\n1.00\n1.00\n\n0.69\n1.00\n1.00\n\n0.74\n1.00\n1.00\n\nVAS0 (0.05, 500)\nVAS1 (0.05, 500)\nVAS2 (0.05, 500)\n\n0.02\n1.00\n1.00\n\n0.07\n1.00\n1.00\n\n0.25\n1.00\n1.00\n\n0.54\n1.00\n1.00\n\n0.69\n1.00\n1.00\n\n0.74\n1.00\n1.00\n\n22\n\nVAS0 (0.05, 50)\nVAS1 (0.05, 50)\nVAS2 (0.05, 50)\n\nTable 4: Numbers represent probability of rejection under the true generating model, with c\u03b1 calculated under H0 . Therefore,\nthe values are \u03b1\u0302 under model \"0\" and \u03b2\u0302 otherwise. Estimates calculated on 10000 experiments. \u03b1-divergences, for \u2206n = 0.1\n\n\fmodel (\u03b1, n)\nVAS0 (0.01, 50)\nVAS1 (0.01, 50)\nVAS2 (0.01, 50)\n\n\u03bb = \u22120.99 \u03bb = \u22121.20 \u03bb = \u22121.50 \u03bb = \u22121.75 \u03bb = \u22122.00 \u03bb = \u22122.50\n0.00\n0.00\n0.00\n0.01\n0.02\n0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.02\n1.00\n1.00\n\n0.03\n1.00\n1.00\n\n0.09\n1.00\n1.00\n\nVAS0 (0.01, 100)\nVAS1 (0.01, 100)\nVAS2 (0.01, 100)\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.05\n1.00\n1.00\n\nVAS0 (0.05, 100)\nVAS1 (0.05, 100)\nVAS2 (0.05, 100)\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.03\n1.00\n1.00\n\n0.08\n1.00\n1.00\n\nVAS0 (0.01, 500)\nVAS1 (0.01, 500)\nVAS2 (0.01, 500)\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.02\n1.00\n1.00\n\nVAS0 (0.05, 500)\nVAS1 (0.05, 500)\nVAS2 (0.05, 500)\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.04\n1.00\n1.00\n\n23\n\nVAS0 (0.05, 50)\nVAS1 (0.05, 50)\nVAS2 (0.05, 50)\n\nTable 5: Numbers represent probability of rejection under the true generating model, with c\u03b1 calculated under H0 . Therefore,\nthe values are \u03b1\u0302 under model \"0\" and \u03b2\u0302 otherwise. Estimates calculated on 10000 experiments. Power-divergences for\n\u2206n = 0.1\n\n\fmodel (n)\nCIR0 (50)\nCIR1 (50)\nCIR2 (50)\nCIR0 (100)\nCIR1 (100)\nCIR2 (100)\n\n\u03b1 = 0.01 \u03b1 = 0.05\n0.02\n0.11\n0.59\n0.84\n1.00\n1.00\n0.03\n0.96\n1.00\n\n0.11\n0.99\n1.00\n\nCIR0 (500)\n0.02\n0.09\nCIR1 (500)\n1.00\n1.00\nCIR2 (500)\n1.00\n1.00\nmodel (n)\n\u03b1 = 0.01 \u03b1 = 0.05\nCIR0 (50)\n0.01\n0.04\nCIR1 (50)\n0.78\n0.93\nCIR2 (50)\n1.00\n1.00\nCIR0 (100)\nCIR1 (100)\nCIR2 (100)\n\n0.01\n0.99\n1.00\n\n0.04\n1.00\n1.00\n\nCIR0 (500)\nCIR1 (500)\nCIR2 (500)\n\n0.00\n1.00\n1.00\n\n0.02\n1.00\n1.00\n\nTable 6: Numbers represent probability of rejection under the true model, with\nrejection region calculated under H0 . Likelihood ratio, for \u2206n = 0.001 (up) and\n\u2206n = 0.1 (bottom).\n\n24\n\n\fmodel (\u03b1, n)\nCIR0 (0.01, 50)\nCIR1 (0.01, 50)\nCIR2 (0.01, 50)\n\n\u03b1 = \u22120.99 \u03b1 = \u22120.90 \u03b1 = \u22120.75 \u03b1 = \u22120.50 \u03b1 = \u22120.25 \u03b1 = \u22120.10\n0.03\n0.28\n0.69\n0.83\n0.89\n0.90\n0.63\n0.95\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.12\n0.86\n1.00\n\n0.31\n0.96\n1.00\n\n0.69\n1.00\n1.00\n\n0.83\n1.00\n1.00\n\n0.89\n1.00\n1.00\n\n0.90\n1.00\n1.00\n\nCIR0 (0.01, 100)\nCIR1 (0.01, 100)\nCIR2 (0.01, 100)\n\n0.03\n0.97\n1.00\n\n0.28\n1.00\n1.00\n\n0.69\n1.00\n1.00\n\n0.85\n1.00\n1.00\n\n0.89\n1.00\n1.00\n\n0.91\n1.00\n1.00\n\nCIR0 (0.05, 100)\nCIR1 (0.05, 100)\nCIR2 (0.05, 100)\n\n0.12\n0.99\n1.00\n\n0.31\n1.00\n1.00\n\n0.69\n1.00\n1.00\n\n0.85\n1.00\n1.00\n\n0.89\n1.00\n1.00\n\n0.91\n1.00\n1.00\n\nCIR0 (0.01, 500)\nCIR1 (0.01, 500)\nCIR2 (0.01, 500)\n\n0.03\n1.00\n1.00\n\n0.22\n1.00\n1.00\n\n0.59\n1.00\n1.00\n\n0.79\n1.00\n1.00\n\n0.86\n1.00\n1.00\n\n0.88\n1.00\n1.00\n\nCIR0 (0.05, 500)\nCIR1 (0.05, 500)\nCIR2 (0.05, 500)\n\n0.09\n1.00\n1.00\n\n0.24\n1.00\n1.00\n\n0.59\n1.00\n1.00\n\n0.79\n1.00\n1.00\n\n0.86\n1.00\n1.00\n\n0.89\n1.00\n1.00\n\n25\n\nCIR0 (0.05, 50)\nCIR1 (0.05, 50)\nCIR2 (0.05, 50)\n\nTable 7: Numbers represent probability of rejection under the true model, with rejection region calculated under H0 . \u03b1divergences, for \u2206n = 0.001\n\n\fmodel (\u03b1, n)\nCIR0 (0.01, 50)\nCIR1 (0.01, 50)\nCIR2 (0.01, 50)\n\n\u03bb = \u22120.99 \u03bb = \u22121.20 \u03bb = \u22121.50 \u03bb = \u22121.75 \u03bb = \u22122.00 \u03bb = \u22122.50\n0.00\n0.00\n0.00\n0.02\n0.05\n0.13\n0.00\n0.01\n0.28\n0.55\n0.71\n0.87\n0.00\n0.81\n1.00\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n\n0.00\n0.09\n0.97\n\n0.01\n0.48\n1.00\n\n0.04\n0.70\n1.00\n\n0.09\n0.81\n1.00\n\n0.19\n0.92\n1.00\n\nCIR0 (0.01, 100)\nCIR1 (0.01, 100)\nCIR2 (0.01, 100)\n\n0.00\n0.00\n0.00\n\n0.00\n0.21\n1.00\n\n0.00\n0.83\n1.00\n\n0.02\n0.95\n1.00\n\n0.05\n0.98\n1.00\n\n0.14\n0.99\n1.00\n\nCIR0 (0.05, 100)\nCIR1 (0.05, 100)\nCIR2 (0.05, 100)\n\n0.00\n0.00\n0.24\n\n0.00\n0.53\n1.00\n\n0.01\n0.93\n1.00\n\n0.05\n0.98\n1.00\n\n0.09\n0.99\n1.00\n\n0.20\n1.00\n1.00\n\nCIR0 (0.01, 500)\nCIR1 (0.01, 500)\nCIR2 (0.01, 500)\n\n0.00\n0.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.02\n1.00\n1.00\n\n0.04\n1.00\n1.00\n\n0.10\n1.00\n1.00\n\nCIR0 (0.05, 500)\nCIR1 (0.05, 500)\nCIR2 (0.05, 500)\n\n0.00\n0.95\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.04\n1.00\n1.00\n\n0.07\n1.00\n1.00\n\n0.15\n1.00\n1.00\n\n26\n\nCIR0 (0.05, 50)\nCIR1 (0.05, 50)\nCIR2 (0.05, 50)\n\nTable 8: Numbers represent probability of rejection under the true model, with rejection region calculated under H0 . Powerdivergences for \u2206n = 0.001\n\n\fmodel (\u03b1, n)\nCIR0 (0.01, 50)\nCIR1 (0.01, 50)\nCIR2 (0.01, 50)\n\n\u03b1 = \u22120.99 \u03b1 = \u22120.90 \u03b1 = \u22120.75 \u03b1 = \u22120.50 \u03b1 = \u22120.25 \u03b1 = \u22120.10\n0.01\n0.14\n0.54\n0.77\n0.85\n0.87\n0.80\n0.98\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.05\n0.94\n1.00\n\n0.16\n0.98\n1.00\n\n0.54\n1.00\n1.00\n\n0.77\n1.00\n1.00\n\n0.85\n1.00\n1.00\n\n0.87\n1.00\n1.00\n\nCIR0 (0.01, 100)\nCIR1 (0.01, 100)\nCIR2 (0.01, 100)\n\n0.01\n0.99\n1.00\n\n0.13\n1.00\n1.00\n\n0.49\n1.00\n1.00\n\n0.71\n1.00\n1.00\n\n0.79\n1.00\n1.00\n\n0.82\n1.00\n1.00\n\nCIR0 (0.05, 100)\nCIR1 (0.05, 100)\nCIR2 (0.05, 100)\n\n0.04\n1.00\n1.00\n\n0.15\n1.00\n1.00\n\n0.49\n1.00\n1.00\n\n0.71\n1.00\n1.00\n\n0.79\n1.00\n1.00\n\n0.82\n1.00\n1.00\n\nCIR0 (0.01, 500)\nCIR1 (0.01, 500)\nCIR2 (0.01, 500)\n\n0.00\n1.00\n1.00\n\n0.06\n1.00\n1.00\n\n0.28\n1.00\n1.00\n\n0.54\n1.00\n1.00\n\n0.69\n1.00\n1.00\n\n0.74\n1.00\n1.00\n\nCIR0 (0.05, 500)\nCIR1 (0.05, 500)\nCIR2 (0.05, 500)\n\n0.02\n1.00\n1.00\n\n0.08\n1.00\n1.00\n\n0.28\n1.00\n1.00\n\n0.54\n1.00\n1.00\n\n0.69\n1.00\n1.00\n\n0.74\n1.00\n1.00\n\n27\n\nCIR0 (0.05, 50)\nCIR1 (0.05, 50)\nCIR2 (0.05, 50)\n\nTable 9: Numbers represent probability of rejection under the true model, with rejection region calculated under H0 . \u03b1divergences, for \u2206n = 0.1\n\n\fmodel (\u03b1, n)\nCIR0 (0.01, 50)\nCIR1 (0.01, 50)\nCIR2 (0.01, 50)\n\n\u03bb = \u22120.99 \u03bb = \u22121.20 \u03bb = \u22121.50 \u03bb = \u22121.75 \u03bb = \u22122.00 \u03bb = \u22122.50\n0.00\n0.00\n0.00\n0.01\n0.02\n0.06\n0.00\n0.06\n0.52\n0.75\n0.86\n0.94\n0.00\n0.99\n1.00\n1.00\n1.00\n1.00\n0.00\n0.00\n0.06\n\n0.00\n0.23\n1.00\n\n0.00\n0.70\n1.00\n\n0.02\n0.85\n1.00\n\n0.04\n0.92\n1.00\n\n0.09\n0.96\n1.00\n\nCIR0 (0.01, 100)\nCIR1 (0.01, 100)\nCIR2 (0.01, 100)\n\n0.00\n0.00\n0.00\n\n0.00\n0.56\n1.00\n\n0.00\n0.96\n1.00\n\n0.01\n0.99\n1.00\n\n0.02\n1.00\n1.00\n\n0.05\n1.00\n1.00\n\nCIR0 (0.05, 100)\nCIR1 (0.05, 100)\nCIR2 (0.05, 100)\n\n0.00\n0.00\n0.97\n\n0.00\n0.83\n1.00\n\n0.00\n0.99\n1.00\n\n0.02\n1.00\n1.00\n\n0.03\n1.00\n1.00\n\n0.08\n1.00\n1.00\n\nCIR0 (0.01, 500)\nCIR1 (0.01, 500)\nCIR2 (0.01, 500)\n\n0.00\n0.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.02\n1.00\n1.00\n\nCIR0 (0.05, 500)\nCIR1 (0.05, 500)\nCIR2 (0.05, 500)\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.00\n1.00\n1.00\n\n0.01\n1.00\n1.00\n\n0.02\n1.00\n1.00\n\n0.04\n1.00\n1.00\n\n28\n\nCIR0 (0.05, 50)\nCIR1 (0.05, 50)\nCIR2 (0.05, 50)\n\nTable 10: Numbers represent probability of rejection under the true model, with rejection region calculated under H0 .\nPower-divergences for \u2206n = 0.1\n\n\f"}