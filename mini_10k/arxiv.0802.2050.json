{"id": "http://arxiv.org/abs/0802.2050v1", "guidislink": true, "updated": "2008-02-14T16:40:17Z", "updated_parsed": [2008, 2, 14, 16, 40, 17, 3, 45, 0], "published": "2008-02-14T16:40:17Z", "published_parsed": [2008, 2, 14, 16, 40, 17, 3, 45, 0], "title": "FINE: Fisher Information Non-parametric Embedding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0802.4186%2C0802.0528%2C0802.1458%2C0802.2096%2C0802.1875%2C0802.0905%2C0802.1243%2C0802.2733%2C0802.0622%2C0802.0956%2C0802.0499%2C0802.2292%2C0802.0723%2C0802.1674%2C0802.2151%2C0802.0716%2C0802.1939%2C0802.1908%2C0802.1698%2C0802.3218%2C0802.1844%2C0802.2911%2C0802.3310%2C0802.2724%2C0802.1927%2C0802.0877%2C0802.1315%2C0802.2419%2C0802.2745%2C0802.1279%2C0802.1024%2C0802.0791%2C0802.0235%2C0802.2162%2C0802.2903%2C0802.3800%2C0802.1106%2C0802.3658%2C0802.0887%2C0802.2648%2C0802.1606%2C0802.1129%2C0802.3127%2C0802.1139%2C0802.1828%2C0802.2409%2C0802.2390%2C0802.3514%2C0802.2538%2C0802.2937%2C0802.4159%2C0802.3864%2C0802.1557%2C0802.4448%2C0802.3885%2C0802.4281%2C0802.0733%2C0802.3001%2C0802.2586%2C0802.0208%2C0802.3189%2C0802.1668%2C0802.2972%2C0802.4423%2C0802.1193%2C0802.0267%2C0802.1326%2C0802.1291%2C0802.4259%2C0802.0928%2C0802.2792%2C0802.4110%2C0802.3388%2C0802.1728%2C0802.4272%2C0802.1957%2C0802.4223%2C0802.0280%2C0802.1276%2C0802.0024%2C0802.3300%2C0802.3389%2C0802.1659%2C0802.4245%2C0802.1494%2C0802.4094%2C0802.0304%2C0802.4361%2C0802.3670%2C0802.1789%2C0802.3437%2C0802.2050%2C0802.0139%2C0802.2782%2C0802.0879%2C0802.2053%2C0802.2789%2C0802.0563%2C0802.0121%2C0802.2766%2C0802.1016&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "FINE: Fisher Information Non-parametric Embedding"}, "summary": "We consider the problems of clustering, classification, and visualization of\nhigh-dimensional data when no straightforward Euclidean representation exists.\nTypically, these tasks are performed by first reducing the high-dimensional\ndata to some lower dimensional Euclidean space, as many manifold learning\nmethods have been developed for this task. In many practical problems however,\nthe assumption of a Euclidean manifold cannot be justified. In these cases, a\nmore appropriate assumption would be that the data lies on a statistical\nmanifold, or a manifold of probability density functions (PDFs). In this paper\nwe propose using the properties of information geometry in order to define\nsimilarities between data sets using the Fisher information metric. We will\nshow this metric can be approximated using entirely non-parametric methods, as\nthe parameterization of the manifold is generally unknown. Furthermore, by\nusing multi-dimensional scaling methods, we are able to embed the corresponding\nPDFs into a low-dimensional Euclidean space. This not only allows for\nclassification of the data, but also visualization of the manifold. As a whole,\nwe refer to our framework as Fisher Information Non-parametric Embedding\n(FINE), and illustrate its uses on a variety of practical problems, including\nbio-medical applications and document classification.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0802.4186%2C0802.0528%2C0802.1458%2C0802.2096%2C0802.1875%2C0802.0905%2C0802.1243%2C0802.2733%2C0802.0622%2C0802.0956%2C0802.0499%2C0802.2292%2C0802.0723%2C0802.1674%2C0802.2151%2C0802.0716%2C0802.1939%2C0802.1908%2C0802.1698%2C0802.3218%2C0802.1844%2C0802.2911%2C0802.3310%2C0802.2724%2C0802.1927%2C0802.0877%2C0802.1315%2C0802.2419%2C0802.2745%2C0802.1279%2C0802.1024%2C0802.0791%2C0802.0235%2C0802.2162%2C0802.2903%2C0802.3800%2C0802.1106%2C0802.3658%2C0802.0887%2C0802.2648%2C0802.1606%2C0802.1129%2C0802.3127%2C0802.1139%2C0802.1828%2C0802.2409%2C0802.2390%2C0802.3514%2C0802.2538%2C0802.2937%2C0802.4159%2C0802.3864%2C0802.1557%2C0802.4448%2C0802.3885%2C0802.4281%2C0802.0733%2C0802.3001%2C0802.2586%2C0802.0208%2C0802.3189%2C0802.1668%2C0802.2972%2C0802.4423%2C0802.1193%2C0802.0267%2C0802.1326%2C0802.1291%2C0802.4259%2C0802.0928%2C0802.2792%2C0802.4110%2C0802.3388%2C0802.1728%2C0802.4272%2C0802.1957%2C0802.4223%2C0802.0280%2C0802.1276%2C0802.0024%2C0802.3300%2C0802.3389%2C0802.1659%2C0802.4245%2C0802.1494%2C0802.4094%2C0802.0304%2C0802.4361%2C0802.3670%2C0802.1789%2C0802.3437%2C0802.2050%2C0802.0139%2C0802.2782%2C0802.0879%2C0802.2053%2C0802.2789%2C0802.0563%2C0802.0121%2C0802.2766%2C0802.1016&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider the problems of clustering, classification, and visualization of\nhigh-dimensional data when no straightforward Euclidean representation exists.\nTypically, these tasks are performed by first reducing the high-dimensional\ndata to some lower dimensional Euclidean space, as many manifold learning\nmethods have been developed for this task. In many practical problems however,\nthe assumption of a Euclidean manifold cannot be justified. In these cases, a\nmore appropriate assumption would be that the data lies on a statistical\nmanifold, or a manifold of probability density functions (PDFs). In this paper\nwe propose using the properties of information geometry in order to define\nsimilarities between data sets using the Fisher information metric. We will\nshow this metric can be approximated using entirely non-parametric methods, as\nthe parameterization of the manifold is generally unknown. Furthermore, by\nusing multi-dimensional scaling methods, we are able to embed the corresponding\nPDFs into a low-dimensional Euclidean space. This not only allows for\nclassification of the data, but also visualization of the manifold. As a whole,\nwe refer to our framework as Fisher Information Non-parametric Embedding\n(FINE), and illustrate its uses on a variety of practical problems, including\nbio-medical applications and document classification."}, "authors": ["Kevin M. Carter", "Raviv Raich", "William G. Finn", "Alfred O. Hero"], "author_detail": {"name": "Alfred O. Hero"}, "author": "Alfred O. Hero", "arxiv_comment": "30 pages, 21 figures", "links": [{"href": "http://arxiv.org/abs/0802.2050v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0802.2050v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0802.2050v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0802.2050v1", "journal_reference": null, "doi": null, "fulltext": "1\n\nFINE: Fisher Information Non-parametric\nEmbedding\nKevin M. Carter1 , Raviv Raich2 , William G. Finn3 , and Alfred O. Hero III1\n1\n\nDepartment of EECS, University of Michigan, Ann Arbor, MI 48109\n\narXiv:0802.2050v1 [stat.ML] 14 Feb 2008\n\n2\n3\n\nSchool of EECS, Oregon State University, Corvallis, OR 97331\n\nDepartment of Pathology, University of Michigan, Ann Arbor, MI 48109\n\n{kmcarter,wgfinn,hero}@umich.edu, raich@eecs.oregonstate.edu\n\nAbstract\nWe consider the problems of clustering, classification, and visualization of high-dimensional data\nwhen no straightforward Euclidean representation exists. Typically, these tasks are performed by first\nreducing the high-dimensional data to some lower dimensional Euclidean space, as many manifold\nlearning methods have been developed for this task. In many practical problems however, the assumption\nof a Euclidean manifold cannot be justified. In these cases, a more appropriate assumption would be\nthat the data lies on a statistical manifold, or a manifold of probability density functions (PDFs). In this\npaper we propose using the properties of information geometry in order to define similarities between data\nsets using the Fisher information metric. We will show this metric can be approximated using entirely\nnon-parametric methods, as the parameterization of the manifold is generally unknown. Furthermore,\nby using multi-dimensional scaling methods, we are able to embed the corresponding PDFs into a lowdimensional Euclidean space. This not only allows for classification of the data, but also visualization of\nthe manifold. As a whole, we refer to our framework as Fisher Information Non-parametric Embedding\n(FINE), and illustrate its uses on a variety of practical problems, including bio-medical applications and\ndocument classification.\n\nI. I NTRODUCTION\nThe fields of statistical learning and machine learning are used to study problems of inference, which\nis to say gaining knowledge through the construction of models in order to make decisions or predictions\nbased on observed data [1]. Statistical learning examines problems such as observing natural associations\nbetween data sets (clustering), and predicting to which class of known groupings an unlabeled data set\nbelongs (classification), based on some model defined by a priori knowledge of the data. Machine learning\nAcknowledgement: This work is partially funded by the National Science Foundation, grant No. CCR-0325571.\n\nFebruary 12, 2013\n\nDRAFT\n\n\f2\n\nintroduces a non-parametric approach to these learning tasks via model-free learning from examples.\nRecent work on manifold learning aims at the high dimension regime, in which examples are governed by\ngeometrical constraints effectively reducing the dimension of the problem from a high extrinsic dimension\nto a low intrinsic dimension. On the other hand, information geometry aims at understanding the structure\nof statistical models and introduces a geometric perspective to inference problems [2].\nWe are interested in the cross section of the three fields; using the principles of each to solve problems\nthat do not fit within the framework of any of the individual fields. Often data does not exhibit a low\nintrinsic dimension in the data domain as one would have in manifold learning. A straightforward strategy\nis to express the data in terms of a low-dimensional feature vector for which the curse of dimensionality\nis alleviated. This initial processing of data as real-valued feature vectors in Euclidean space, which is\noften carried out in an ad hoc manner, has been called the \"dirty laundry\" of machine learning [3]. This\nprocedure is highly dependent on having a good model for the data and in the absence of such model\nmay be highly suboptimal. When a statistical model is available, the process of obtaining a feature vector\ncan be done optimally by extracting the model parameters for a given data set and thus characterizing\nthe data through its lower dimensional parameter vector. We are interested in extending this approach to\nthe case in which the data follows an unknown parametric statistical model.\nWhile the problem of learning in a Euclidean space is well defined, there are many problems in which\nthe data cannot be appropriately represented by a Euclidean manifold, and the model parameters are\nunspecified and must be learned through the data. In flow cytometry, pathologists study blood samples\ncontaining many cells taken from a patient. Each individual cell is analyzed with different fluorescent\nmarkers, resulting in a large, high-dimensional data set. This is assumed to be a realization of some\noverriding parametric model, but the model parameters are unknown. Pathologists desire the ability to\nappropriately classify patients with differing ailments that may express similar responses to these markers.\nFor the purposes of analysis and visualization, it is then necessary to reduce the dimensionality of these\nsets. The problem of document classification is one in which the data is clearly non-Euclidean, as each\nset is a collection of words from a dictionary. It is still desired to distinguish between documents by\nforming clusters of different similarities. A standard method is to form a probability distribution over\na dictionary and use methods of information geometry to determine a similarity between data sets [4].\nApplications of statistical manifolds have also been presented in the cases of face recognition [5], texture\nsegmentation [6], image analysis [7], and shape analysis [8].\nA common theme to all of the problems presented above is that the model from which the data is\ngenerated is unknown. In this paper, we present a framework to handle such problems. Specifically, we\nfocus on the case where the data is high-dimensional and no lower dimensional Euclidean manifold\ngives a sufficient description. In many of these cases, a lower dimensional statistical manifold can be\nused to assess the data for various learning tasks. We refer to our framework as Fisher Information Non-\n\nFebruary 12, 2013\n\nDRAFT\n\n\f3\n\nparametric Embedding (FINE), and it includes characterization of data sets in terms of a non-parametric\nstatistical model, a geodesic approximation of the Fisher information distance as a metric for evaluating\nsimilarities between data sets, and a dimensionality reduction procedure to obtain a low-dimensional\nEuclidean embedding of the original high-dimensional data set for the purposes of both classification and\nvisualization.\nStatistical manifolds in both the parametric and non-parametric settings have been well discussed [9],\n[10]. Our work differs in that we assume the manifold is derived from some natural parameterization, only\nthat set of parameters is unknown. There has been much work presented on the use of statistical manifolds\n[4], [7], [11], [12] and information geometry [13], [14] in learning problems, all proposing alternatives\nto using Euclidean geometry for data modeling. These methods focus on clustering and classification,\nand do not explicitly address the problems of dimensionality reduction (embedding each set into a lowdimensional Euclidean space) and visualization. Additionally, they focus on parameter estimation as a\nnecessity for their methods, as opposed to our work which is performed in a non-parametric setting. We\nprovide a start-to-finish framework which enables analysis of high-dimensional data through non-linear\nembedding into a low-dimensional space by information, not Euclidean, geometry. Our methods require\nno explicit model assumptions; only than that the given data is a realization from an unknown model\nwith some natural parameterization.\nRecent work by Lee et al. [15] similar to our own [16], [17] has demonstrated the use of statistical\nmanifolds for dimensionality reduction. While each work has been developed independently and originally\npresented at nearly the same time, they share enough similarities that we now express the different\ncontributions of our own work. Specifically, we consider the work presented by Lee et al. to be a\nspecialized case of our more general framework. They focus on the specific case of image segmentation,\nwhich consists of multinomial distributions as points which lie on an n-simplex (or projected onto an\nn + 1-dimensional sphere). By framing their problem as such, they are able to exploit the properties of\n\nsuch a manifold: using the cosine distance as an exact computation of the Fisher information distance,\nand using linear methods (PCA) of dimensionality reduction. They have shown very promising results\nfor the problem of image segmentation, and briefly mention the possibility of using non-linear methods\nof dimensionality reduction, which they consider unnecessary for their problem. The work we present\ndiffers in that we make no assumptions on the type of distributions making up the statistical manifold.\nAs such, our geodesic approximation for the Fisher information accounts for submanifolds of interest.\nThis is illustrated later in Fig. 3, where the submanifold lies on the n + 1-dimensional sphere, but\ndoes not fill the entire space. As such, there is no exact measure of the Fisher information between\npoints, and we must approximate with a geodesic along the manifold. Additionally, we utilize non-linear\nmethods of dimensionality reduction, which we consider to be more relevant for many non-linear types of\napplications. Finally, by considering all statistical manifolds rather than focusing on those of consisting\n\nFebruary 12, 2013\n\nDRAFT\n\n\f4\n\nof multinomial distributions, we are able to apply our methods to many problems of practical interest.\nThis paper is organized as follows: Section II describes a background in information geometry and\nstatistical manifolds. Section III gives the formulation for the problem we wish to solve, while Section IV\ndevelops and outlines the FINE algorithm. We illustrate the results of using FINE on real and synthetic\ndata sets in Section V. Finally, we draw conclusions and discuss the possibilities for future work in\nSection VI.\nII. BACKGROUND\n\nON I NFORMATION\n\nG EOMETRY\n\nInformation geometry is a field that has emerged from the study of geometrical structures on manifolds\nof probability distributions. These investigations analyze probability distributions as geometrical structures in a Riemannian space. Using tools and methods deriving from differential geometry, information\ngeometry is applicable to information theory, probability theory, and statistics. The field of information\ntheory is largely based on the works of Shun'ichi Amari [18] and has been used for analysis in such\nfields as statistical inference, neural networks, and control systems. In this section, we will give a brief\nbackground on the methods of information geometry that we utilize in our framework. For a more\nthorough introduction to information geometry, we suggest [19] and [2].\nA. Differential Manifolds\nThe concept of a differential manifold is similar to that of a smooth curve or surface lying in a highdimensional space. A manifold M can be intuitively thought of as a set of points with a coordinate\nsystem. These points can be from a variety of constructs, such as Euclidean coordinates, linear system,\nimages, or probability distributions. Regardless of the definition of the points in the manifold M, there\nexists a coordinate system with a one-to-one mapping from M to Rd , and as such, d is known as the\n\ndimension of M.\n\nFor reference, we will refer to the coordinate system on M as \u03c8 : M \u2192 Rd . If \u03c8 has M as its\n\ndomain, we call it a global coordinate system [2]. In this situation, \u03c8 is a one-to-one mapping onto Rd\n\nfor all points in M. A manifold is differentiable if the coordinate system mapping \u03c8 is differentiable\nover its entire domain. If \u03c8 is infinitely differentiable, the manifold is said to be 'smooth' [19].\n\nIn many cases there does not exist a global coordinate system. Examples of such manifolds include the\nsurface of a sphere, the \"swiss roll\", and the torus. For these manifolds, there are only local coordinate\nsystems. Intuitively, a local coordinate system acts as a global coordinate system for a local neighborhood\nof the manifold, and there may be many local coordinate systems for a particular manifold. Fortunately,\nsince a local coordinate system contains the same properties as a global coordinate system (only on a\nlocal level), analysis is consistent between the two. As such, we shall focus solely on manifolds with a\nglobal coordinate system.\nFebruary 12, 2013\n\nDRAFT\n\n\f5\n\n1) Statistical Manifolds: Let us now present the notion statistical manifolds, or a set M whose elements\n\nare probability distributions. A probability distribution function (PDF) on a set X is defined as a function\np : X \u2192 R in which\n\np(x) \u2265 0, \u2200x \u2208 X\nZ\np(x) dx = 1.\n\n(1)\n\nWe describe only the case for continuum on the set X , however if X was discrete valued, equation (1)\nR\nP\nwill still apply by switching p(x) dx = 1 with\np(x) = 1. If we consider M to be a family of PDFs\n\u0002\n\u0003\non the set X , in which each element of M is a PDF which can be parameterized by \u03b8 = \u03b8 1 , . . . , \u03b8 n ,\nthen M is known as a statistical model on X . Specifically, let\n\nM = {p(x | \u03b8) | \u03b8 \u2208 \u0398 \u2286 Rd },\n\n(2)\n\nwith p(x | \u03b8) satisfying the equations in (1). Additionally, there exists a one-to-one mapping between \u03b8\n\nand p(x | \u03b8).\n\nGiven certain properties of the parameterization of M, such as differentiability and C \u221e diffeomorphism\n\n(details of which are described in [2]), the parameterization \u03b8 is also a coordinate system of M. In this\n\ncase, M is known as a statistical manifold. In the rest of this paper, we will use the terms 'manifold'\nand 'statistical manifold' interchangeably.\nB. Distances on Manifolds\n\nIn Euclidean space, the distance between two points is defined as the length of a straight line between\nthe points. On a manifold, however, one can measure distance by a trace of the shortest path between\nthe points along the manifold. This path is called a geodesic, and the length of the path is the geodesic\ndistance. In information geometry, the distance between two points on a manifold is analogous to the\ndifference in information between them, and is defined by the Fisher information metric.\n1) Fisher Information Metric: The Fisher information measures the amount of information a random\nvariable X contains in reference to an unknown parameter \u03b8 . For the single parameter case it is defined\nas\nI(\u03b8) = E\n\nIf the condition\n\nFebruary 12, 2013\n\nR\n\n\u22022\n\u2202\u03b8 2 f (X; \u03b8) dX\n\n\"\u0012\n\n\u00132 #\n\u2202\nlog f (X; \u03b8) |\u03b8 .\n\u2202\u03b8\n\n= 0 is met, then the above equation can be written as\n\u0015\n\u0014 2\n\u2202\nlog f (X; \u03b8) .\nI(\u03b8) = \u2212E\n\u2202\u03b8 2\n\nDRAFT\n\n\f6\n\n\u0002\n\u0003\nFor the case of multiple parameters \u03b8 = \u03b8 1 , . . . , \u03b8 n , we define the Fisher information matrix [I(\u03b8)],\n\nwhose elements consist of the Fisher information with respect to specified parameters, as\nZ\n\u2202 log f (X; \u03b8) \u2202 log f (X; \u03b8)\nIij = f (X; \u03b8)\ndX.\n\u2202\u03b8 i\n\u2202\u03b8 j\n\n(3)\n\nFor a parametric family of probability distributions, it is possible to define a Riemannian metric using\nthe Fisher information matrix, known as the information metric. The information metric distance, or\nFisher information distance, between two distributions p(x; \u03b81 ) and p(x; \u03b82 ) in a single parameter family\nis\nDF (\u03b81 , \u03b82 ) =\n\nZ\n\n\u03b82\n\u03b81\n\nI(\u03b8)1/2 d\u03b8,\n\n(4)\n\nwhere \u03b81 and \u03b82 are parameter values corresponding to the two PDFs and I(\u03b8) is the Fisher information\n\nfor the parameter \u03b8 . Extending to the multi-parameter case, we obtain:\ns\n\u0012 \u0013\nZ 1 \u0012 \u0013T\nd\u03b8\nd\u03b8\nI(\u03b8)\nd\u03b2.\nDF (\u03b81 , \u03b82 ) =\nmin\nd\u03b2\nd\u03b2\n\u03b8:\u03b8(0)=\u03b81 ,\u03b8(1)=\u03b82 0\n\n(5)\n\n2) Example: Here we present a derivation of a geodesic distance between univariate Gaussian densities\nvia the Fisher information metric for two reasons. First, we would like to illustrate how involved the\nprocess is for such a simple family of PDFs. Secondly, we present a process of deriving the Fisher\ninformation metric that is involved in computing the geodesic distance. Let us consider the family of\nunivariate Gaussian distributions P = {p1 , . . . , pn }, where\npi (x) = q\n\nFor the case of P parameterized by \u03b8 =\n\n1\n2\u03c0\u03c3i2\n\u0010\n\n\u0001\nexp \u2212(x \u2212 \u03bci )2 /2\u03c3i2 .\n\n\u221a\u03bc , \u03c3\n2\n\n[I(\u03b8)] =\n\n\u0011\n\n, the resultant Fisher information matrix is\n2\n\u03c32\n\n0\n\n0\n\n2\n\u03c32\n\n!\n\n.\n\nWe omit the derivation, which can be found in [19] and is straight forward from (3).\nWe define the distance between two points on the manifold as the minimum length between all paths\nconnecting the two points. Using the inner product associated with the Fisher information matrix\n< u, v >F = ut [I(\u03b8)]v,\n\nwe define the length of the path P between two points parameterized by \u03b81 and \u03b82 , on the manifold M\nas\nk\u03b81 \u2212 \u03b82 kP =\n\nFebruary 12, 2013\n\np\n\n< \u03b81 \u2212 \u03b8 2 , \u03b81 \u2212 \u03b8 2 >F .\n\nDRAFT\n\n\f7\n\nUsing the parameterization \u03b8(t) such that \u03b8(0) = \u03b81 and \u03b8(1) = \u03b82 , we obtain the length of P as\ns\n\u0012\n\u0013T\n\u0013\nZ 1 \u0012\nd\nd\nk\u03b81 \u2212 \u03b82 kP =\nI(\u03b8(t))\n\u03b8(t)\n\u03b8(t) dt.\ndt\ndt\n0\nWe are able to define the distance between points p1 = p(x; \u03b81 ) and p2 = p(x; \u03b82 ) as the minimum over\nall path lengths defined above\n\u221a Z\nDF (p1 , p2 ) = min 2\n\u03b8(t)\n\nwhere \u03bc\u0307 =\n\nd\ndt \u03bc(t)\n\nand \u03c3\u0307 =\n\n1\n0\n\ns\n\n\u221a1 \u03bc\u03072\n2\n\n+ \u03c3\u0307 2\n\n\u03c3(t)2\n\ndt,\n\n(6)\n\nd\ndt \u03c3(t).\n\nThe solution to (6) is the well known Poincar\u00e9 hyperbolic distance, in which the shortest path between\ntwo points is the length of an arc on a circle in which both points are at a radius length from the circle's\ncenter. In the case of the univariate normal distribution, this arc is a straight line when the mean is held\nconstant and the variance is changed.\nBy changing variables and parameterizing \u03c3 as a function of \u03bc, we obtain:\nZ \u03bc2 s\n1 + \u03c3\u0307 2\nmin\nd\u03bc,\n\u03c3(\u03bc)2\n\u03c3(\u03bc):\u03c3(\u03bc1 )=\u03c31 ,\u03c3(\u03bc2 )=\u03c32 \u03bc1\nwhere \u03c3\u0307 =\n\nd\nd\u03bc \u03c3(\u03bc).\n\nIt should be clear that this is a representation of (4). It should also be noted that\n\nthere exists a one-to-one mapping \u03c3(\u03bc) : R \u2192 R+ along the geodesic from \u03c3(\u03bc1 ) to \u03c3(\u03bc2 ), except for\n\nthe case when \u03bc1 = \u03bc2 .\n\nSolving (6) becomes a problem of calculus of variations. For the univariate normal family of distributions, this has been calculated in a closed-form expression presented in [20], determining the Fisher\ninformation distance as:\nDF (p1 , p2 ) =\n\n\u221a\n\n\u0010\n\n2 log \u0010\n\n\u03bc1\n\u221a\n, \u03c31\n2\n\u03bc1\n\u221a\n, \u03c31\n2\n\n\u0011\n\u0011\n\n\u2212\n\u2212\n\n\u0010\n\u0010\n\n\u03bc2\n\u221a\n, \u2212\u03c32\n2\n\u03bc2\n\u221a\n, \u2212\u03c32\n2\n\n\u0011\n\u0011\n\n+\n\u2212\n\n\u0010\n\u0010\n\n\u03bc1\n\u221a\n, \u03c31\n2\n\u03bc1\n\u221a\n, \u03c31\n2\n\n\u0011\n\u0011\n\n\u2212\n\u2212\n\n\u0010\n\u0010\n\n\u03bc2\n\u221a\n, \u03c32\n2\n\u03bc2\n\u221a\n, \u03c32\n2\n\n\u0011\n\n\u0011 .\n\n(7)\n\nFor visualization, let us define a set of probability densities P = {pi (x)} on a grid, such that pi = pk,l\n\nis parameterized by (\u03bci , \u03c3i ) = (\u03b1k, 1 + \u03b2l), k, l = 1 . . . n and \u03b1, \u03b2 \u2208 R. Figure 1 shows a mesh-grid and\n\ncontour plot of the Fisher information distance between the density defined by (\u03bci , \u03c3i ) = (0.6, 1.5) and\nthe neighboring densities on the set P (\u03b1 = \u03b2 = 0.1).\nIII. P ROBLEM F ORMULATION\nA key property of the Fisher information metric is that it is independent of the parameterization of the\n\nmanifold [7], [19]. Although the evaluation remains equivalent, calculating the FIM requires knowledge\nof the parameterization, which is generally not available. We instead assume that the collection of density\nfunctions lie on a manifold that can be described by some natural parameterization. Specifically, we are\nFebruary 12, 2013\n\nDRAFT\n\n\f8\n\n2\n1.9\n0.8\n\n0.6\n\n1.7\n\n0.4\n\n1.6\n\n\u03c3\n\nd\n\nF\n\n1.8\n\n1.5\n0.2\n1.4\n0\n2\n\n1.3\n1\n0.8\n\n1.5\n\n0.6\n\n1.2\n\n0.4\n\n\u03c3\n\n1\n\n0.2\n0\n\n1.1\n0.1\n\n\u03bc\n\n0.2\n\n(a)\n\n0.3\n\n0.4\n\n0.5\n\n\u03bc\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n(b)\n\nFig. 1. a) Mesh-grid and b) Contour plots of the Fisher information distance based on a grid of univariate normal densities,\nparameterized by (\u03bc, \u03c3). The reference point, pi , is located at (\u03bci , \u03c3i ) = (0.6, 1.5) and is denoted by the red star.\n\ngiven P = {p1 , . . . , pn }, where pi \u2208 M is a PDF and M is a manifold embedded in S , the simplex of\n\ndensities in L1 . Under these circumstances, it is important to note that much of the same theory still applies\nfor determining dissimilarity between probability distributions. Our goal is to find an approximation for\nthe geodesic distance between points on M using only the information available in P . Can we find an\napproximation function G which yields\n\nD\u0302F (pi , pj ) = G(pi , pj ; P),\n\n(8)\n\nsuch that D\u0302F (pi , pj ) \u2192 DF (pi , pj ) as n \u2192 \u221e?\nThis problem is similar to the setting of classical papers [21], [22] in manifold learning and dimensionality reduction, where only a set of points on the manifold are available. As such, we are able to\nuse these manifold learning techniques to construct a low-dimensional embedding of that family. This\nnot only allows for an effective visualization of the manifold (in 2 or 3 dimensions), but by reducing\nthe effect of the curse of dimensionality we can perform clustering and classification on the family of\ndistributions lying on the manifold.\nA. Approximation of Fisher Information Distance\nThe Fisher information distance is consistent, regardless of the parameterization of the manifold [7].\nThis fact enables the approximation of the information distance when the specific parameterization of the\nmanifold is unknown, and there have been many metrics developed for this approximation. An important\nclass of such divergences is known as the f -divergence [23], in which f (u) is a convex function on\n\nFebruary 12, 2013\n\nDRAFT\n\n\f9\n\nu > 0 and\n\nZ\n\nDf (pkq) =\n\np(x)f\n\n\u0012\n\n\u0013\nq(x)\n.\np(x)\n\nA specific and important example of the f -divergence is the \u03b1-divergence, where D(\u03b1) = Df (\u03b1) for a\nreal number \u03b1. The function f (\u03b1) (u) is defined\n\uf8f1\n4\n\uf8f4\n\uf8f4\n\uf8f2 1\u2212\u03b12\nf (\u03b1) (u) =\n\uf8f4\n\uf8f4\n\uf8f3\n\nas\n1 \u2212 u(1+\u03b1)/2\nu log u\n\n\u0001\n\n\u2212 log u\n\n\u03b1 6= \u00b11\n\n\u03b1=1\n\n.\n\n\u03b1 = \u22121\n\nAs such, the \u03b1-divergence can be evaluated as\n\u0012\n\u0013\nZ\n1\u2212\u03b1\n1+\u03b1\n4\n(\u03b1)\n2\n2\nD (pkq) =\n1 \u2212 p(x) q(x) dx\n1 \u2212 \u03b12\n\n\u03b1 6= 1,\n\nand\nD\n\n(\u22121)\n\n(pkq) = D\n\n(1)\n\n(qkp) =\n\nZ\n\np(x) log\n\np(x)\n.\nq(x)\n\n(9)\n\nThe \u03b1-divergence is the basis for many important and well known divergence metrics, such as the\nHellinger distance, the Kullback-Leibler divergence, and the Renyi-Alpha entropy [24].\n1) Kullback-Leibler Divergence: The Kullback-Leibler (KL) divergence is defined as\nZ\np(x)\nKL(pkq) = p(x) log\n,\nq(x)\n\n(10)\n\nwhich is equal to D(\u22121) (9). The KL-divergence is a very important metric in information theory, and is\ncommonly referred to as the relative entropy of one PDF to another. Kass and Vos show [19] the relation\nbetween the Kullback-Leibler divergence and the Fisher information distance is\np\n\n2KL(pkq) \u2192 DF (p, q)\n\nas p \u2192 q . This allows for an approximation of the Fisher information distance, through the use of the\navailable PDFs, without the need for the specific parameterization of the manifold.\nReturning to our illustration developed in Section II-B2, we have defined the data set P of univariate\nnormal distributions, and presented an expression for the Fisher information distance on the resultant\nmanifold (7). The Kullback-Leibler divergence between univariate normal distributions is also available\nin a closed-form expression:\n1\nKL(pi kpj ) =\n2\n\nlog\n\n\u03c3j2\n\u03c3i2\n\n!\n\n!\n\u03c3i2\n+ 2 + (\u03bcj \u2212 \u03bci )2 /\u03c3j2 \u2212 1 .\n\u03c3j\n\nTo compare the KL-divergence to the Fisher information distance, we define the error as E =\np\n2KL(pi kpj ) \u2212 DF (pi , pj ) , where pi,j \u2208 P . In Fig. 2 we display the mesh-grid and contour plots of\nFebruary 12, 2013\n\nDRAFT\n\n\f10\n\n2\n1.9\n0.06\n1.8\n0.05\n1.7\n1.6\n\n0.03\n\n\u03c3\n\nE\n\n0.04\n\n0.02\n\n1.5\n\n0.01\n\n1.4\n\n0\n2\n\n1.3\n1\n0.8\n\n1.5\n\n0.6\n\n1.2\n\n0.4\n1\n\n\u03c3\n\n0.2\n0\n\n1.1\n0.1\n\n\u03bc\n\n0.2\n\n0.3\n\n(a) Mesh-grid\n\n0.4\n\n0.5\n\n\u03bc\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n(b) Contour plot\n\nFig. 2. a) Mesh-grid and b) Contour plots of the error between the KL-divergence\nand the Fisher information\ndistance based on\n \u0328p\n \u0328\n\u221a\n \u0328\n \u0328\na grid of univariate normal densities, parameterized by (\u03bc, \u03c3). E =  \u0328 2KL(pi kpj ) \u2212 DF (pi , pj ) \u0328. Note that 2KL \u2192 DF ,\nwhere pi is denoted by the red star.\n\nE , where point pi is held constant in the center of the grid defining P , and pj varies about the manifold.\n\nAs described earlier, as the density pj \u2192 pi , the error E \u2192 0. In Fig. 2(b), the reference point pi is\nnoted by the red star.\n\nIt should be noted that the KL-divergence is not a distance metric, as it does not satisfy the symmetry,\nKL(pkq) 6= KL(pkq), or triangle inequality properties of a distance metric. To obtain this symmetry,\n\nwe will define the KL-divergence as:\nDKL (p, q) = KL(pkq) + KL(qkp),\n\n(11)\n\nwhich is symmetric, but still not a distance as it does not satisfy the triangle inequality. Since the Fisher\ninformation is a symmetric measure, we can relate the symmetric KL-divergence and approximate the\nFisher information distance as\n\nas p \u2192 q .\n\np\n\nDKL (p, q) \u2192 DF (p, q),\n\n(12)\n\n2) Hellinger Distance: Another important result of the \u03b1-divergence is the evaluation with \u03b1 = 0:\nZ \u0010p\n\u00112\np\n(0)\nD (pkq) = 2\np(x) \u2212 q(x) dx,\n\nwhich is called the closely related to the Hellinger distance,\nr\n1 (0)\nD ,\nDH =\n2\nFebruary 12, 2013\n\nDRAFT\n\n\f11\n\nwhich satisfies the axioms of distance - symmetry and the triangle inequality. The Hellinger distance is\nrelated to the information distance in the limit by\n2DH (p, q) \u2192 DF (p, q)\n\nas p \u2192 q [19]. We note that the Hellinger distance is related to the Kullback-Leibler divergence, as in\np\nthe limit KL(pkq) \u2192 DH (p, q).\n\n3) Other Fisher Approximations: There are other metrics which approximate the Fisher information\n\ndistance, such as the cosine distance. When dealing with multinomial distributions, the approximation\nZ\n\u221a\nDC (p, q) = 2 arccos\np * q \u2192 DF (p, q),\nis the natural metric on the sphere.\nWe restrict our analysis to that of the Kullback-Leibler divergence and the Hellinger distance. The KLdivergence is a great means of differentiating shapes of continuous PDFs. Analysis of (10) shows that\nas p(x)/q(x) \u2192 \u221e, KL(pkq) \u2192 \u221e. These properties ensure that the KL-divergence will be amplified\nin regions where there is a significant difference in the probability distributions. This cannot be used\nin the case of a multinomial PDF, however, because of divide-by-zero issues. In that case the Hellinger\ndistance is the desired metric as there exists a monotonic transformation function \u03c8 : DH \u2192 DC [19].\nFor additional measures of probabilistic distance, some of which approximate the Fisher information\ndistance, and a means of calculating them between data sets, we refer the reader to [25].\nB. Approximation of Distance on Statistical Manifolds\nWe have shown the approximation function D\u0302F (p1 , p2 ) of the Fisher information distance between p1\nand p2 can be calculated using a variety of metrics as p1 \u2192 p2 . If p1 and p2 do not lie closely together on\nthe manifold, these approximations become weak. An example of this is illustrated in Fig. 3, where the\nmanifold of interest lies in a subspace of another manifold, and the distance between two points should\nbe considered as the distance traveled on the manifold of interest. A good approximation can still be\nachieved if the manifold is densely sampled between the two end points. By defining the path between\np1 and p2 as a series of connected segments and summing the length of those segments, we approximate\n\nthe distance of the geodesic, which is the shortest path along the manifold. Specifically, given the set of\nn PDFs parameterized by P\u03b8 = {\u03b81 , . . . , \u03b8n }, the Fisher information distance between p1 and p2 can be\n\nestimated as:\nDF (p1 , p2 ) \u2248\n\nmin\n\nm,{\u03b8(1) ,...,\u03b8(m) }\n\nm\nX\ni=1\n\nDF (p(\u03b8(i) ), p(\u03b8(i+1) )),\n\np(\u03b8(i) ) \u2192 p(\u03b8(i+1) ) \u2200 i\n\n\b\nwhere p(\u03b8(1) ) = p1 , p(\u03b8(m) ) = p2 , \u03b8(1) , . . . , \u03b8(m) \u2208 P\u03b8 , and m \u2264 n.\nFebruary 12, 2013\n\nDRAFT\n\n\f12\n\nFig. 3. The Fisher information distance between points cannot be exactly calculated about a manifold if the data exists on a\nsubmanifold of interest (shaded area). Rather than directly calculating the distance between points (A) , the distance should be\napproximated by a geodesic along the submanifold (B).\n\nUsing our approximation of the Fisher information distance as p1 \u2192 p2 (whether KL-divergence or\n\nHellinger distance is of no immediate concern), we can now define an approximation function G for all\npairs of PDFs:\nG(p1 , p2 ; P) = min\nm,P\n\nm\nX\n\nD\u0302F (p(i) , p(i+1) ),\n\ni=1\n\np(i) \u2192 p(i+1) \u2200 i\n\n(13)\n\nwhere P = {p1 , . . . , pn } is the available collection of PDFs on the manifold. Intuitively, this estimate\ncalculates the length of the shortest path between points in a connected graph on the well sampled\nmanifold, and as such G(p1 , p2 ; P) \u2192 DF (p1 , p2 ) as n \u2192 \u221e. This is similar to the manner in which\nIsomap [21] approximates distances on Euclidean manifolds. Figure 4 illustrates this approximation\nby comparing the KL graph approximation to the actual Fisher information distance for the univariate\nGaussian case. As the manifold is more densely sampled (uniformly in mean and variance parameters\nfor this simulation), the approximation converges to the true Fisher information distance, as calculated\nin (7).\nC. Dimensionality Reduction\nGiven a matrix of dissimilarities between entities, many algorithms have been developed to find a\nlow-dimensional embedding of the original data \u03c8 : M \u2192 Rd . These techniques have been classified as\na group of methods called Multi-Dimensional Scaling (MDS). There are supervised methods, which are\ngenerally used for classification purposes, and unsupervised methods, which are often used for clustering\nand manifold learning. Using these MDS methods allows us to find a single low-dimensional coordinate\nrepresentation of each high-dimensional, large sample, data set.\n\nFebruary 12, 2013\n\nDRAFT\n\n\f13\n\n2.5\n\nKL Geodesic Approximation\nFisher Information Distance\n\nD\n\n2.45\n\n2.4\n\n2.35\n\n2.3\n0\n\n500\n\n1000\n\n1500\n\nNumber of Points on Manifold\n\nFig. 4. Convergence of the graph approximation of the Fisher information distance using the Kullback-Leibler divergence. As\nthe manifold is more densely sampled, the approximation approaches the true value.\n\n1) Classical Multi-Dimensional Scaling: Classical MDS (cMDS) takes a matrix of dissimilarities and\nembeds each point into a Euclidean space. This is performed by first centering the dissimilarities about the\norigin, then calculating the eigenvalue decomposition of the centered matrix. This unsupervised method\npermits the calculation of the low-dimensional embedding coordinates which reveal any natural separation\nor clustering of the data.\nDefine D as a dissimilarity matrix which contains (or approximates) Euclidean distances. Let B be\nthe \"double centered\" matrix which is calculated by taking the matrix D, subtracting its row and column\nmeans, then adding back the grand mean and multiplying by \u2212 21 . As a result, B is a version of D\ncentered about the origin. Mathematically, this process is solved by\n1\nB = \u2212 HD 2 H,\n2\n\nwhere H = I \u2212 (1/N )11T , I is the N -dimensional identity matrix, and 1 is an N -element vector of\nones.\nThe embedding coordinates, Y \u2208 Rd\u00d7n , can then be determined by taking the eigenvalue decomposition\n\nof B ,\n\nB = [V1 V2 ]diag (\u03bb1 , ..., \u03bbN ) [V1 V2 ]T ,\n\nand calculating\n\u0010\n\u0011\n1/2\n1/2\nY = diag \u03bb1 , ..., \u03bbd\nV1T .\n\nThe matrix V1 consists of the eigenvectors corresponding to the d largest eigenvalues \u03bb1 , . . . , \u03bbd while\nthe remaining N \u2212 d eigenvectors are represented as V2 . The term 'diag(\u03bb1 , . . . , \u03bbN )' refers to an N \u00d7 N\n\nFebruary 12, 2013\n\nDRAFT\n\n\f14\n\n0.4\n\n0.025\n0.02\n\n0.3\n\n0.015\n0.2\n0.01\n\ncMDS2\n\ncMDS2\n\n0.1\n0\n\u22120.1\n\n0.005\n0\n\u22120.005\n\u22120.01\n\n\u22120.2\n\u22120.015\n\u22120.3\n\u22120.4\n\u22120.6\n\n\u22120.02\n\u22120.4\n\n\u22120.2\n\n0\n\n0.2\n\n0.4\n\n\u22120.025\n\u22120.03\n\ncMDS1\n\n\u22120.02\n\n\u22120.01\n\n0\n\n0.01\n\n0.02\n\ncMDS1\n\n(a) Fisher Information\n\n(b) Kullback-Leibler Approximation\n\nFig. 5. Classical MDS to the matrix of a) Fisher information distances and b) Kullback-Leibler geodesic approximations of\nthe Fisher information distance, on a grid of univariate normal densities, parameterized by (\u03bc, \u03c3)\n\ndiagonal matrix with \u03bbi as its ith diagonal element.\nTo continue our illustration from Section II-B2, let D be the matrix of Fisher information distances defined in (7) for the set of univariate normal densities P , where D(i, j) = DF (pi , pj ). Figure 5(a) displays\nthe results of applying cMDS to D. We demonstrate the embedding with the geodesic approximation of\nthe Fisher information distance (13) in Fig. 5(b), which is very similar to the embedding created with\nthe exact values. It is clear that while the densities defining the set P are parameterized on a rectangular\ngrid, the manifold on which P lives is not rectangular itself, which is due to the differing effects that\nchanges in mean and variance have on the Gaussian PDF.\n2) Laplacian Eigenmaps: Laplacian Eigenmaps (LEM) is an unsupervised technique developed by\nBelkin and Niyogi and first presented in [22]. This performs non-linear dimensionality reduction by performing an eigenvalue decomposition on the graph Laplacian formed by the data. As such, this algorithm\nis able to discern low-dimensional structure in high-dimensional spaces that were previously indiscernible\nwith methods such as principal components analysis (PCA) and classical MDS. The algorithm contains\nthree steps and works as follows:\n1) Construct adjacency graph\nGiven dissimilarity matrix DX between data points in the set X , define the graph G over all data\npoints by adding an edge between points i and j if X i is one of the k-nearest neighbors of X j .\n2) Compute weight matrix W\nIf points i and j are connected, assign Wij = e\u2212\n\nDX (i,j)2\nt\n\n, otherwise Wij = 0.\n\n3) Construct low-dimensional embedding\n\nFebruary 12, 2013\n\nDRAFT\n\n\f15\n\nSolve the generalized eigenvalue problem\nLf = \u03bbD f,\n\nwhere D is the diagonal weight matrix in which Dii =\n\nP\n\nj\n\nWji , and L = D \u2212 W is the Laplacian\n\nmatrix. If [f1 , . . . , fd ] is the collection of eigenvectors associated with d smallest generalized eigenvalues which solve the above, the d-dimensional embedding is defined by yi = (vi1 , . . . , vid )T , 1 \u2264\ni \u2264 n.\n\n3) Additional MDS Methods: While we choose to only detail the cMDS and LEM algorithms, there are\nmany other methods for performing dimensionality reduction in a linear fashion (PCA) and non-linearly\n(Local Linear Embedding [26]) for unsupervised learning. For supervised learning there are also linear\n(Linear Discriminant Analysis) and non-linear (Classification Constrained Dimensionality Reduction [27],\nNeighbourhood Component Analysis [28]) methods, all of which can be applied to our framework. We\ndo not highlight the heavily utilized Isomap [21] algorithm since it is identical to using cMDS on the\napproximation of the geodesic distances.\nIV. O UR T ECHNIQUES\nWe have presented a series of methods for manifold learning developed in the field of information\ngeometry. By performing dimensionality reduction on a family of data sets, we are able to both better visualize and classify the data. In order to obtain a lower dimensional embedding, we calculate a dissimilarity\nmetric between data sets within the family by approximating the Fisher information distance between\ntheir corresponding PDFs. This has been illustrated with the family of univariate normal probability\ndistributions.\nIn problems of practical interest, however, the parameterization of the probability densities are usually\nunknown. We instead are given a family of data sets X = {X 1 , X 2 , . . . , X n }, in which we may\nassume that each data set X i is a realization of some underlying probability distribution to which we\ndo not have knowledge of the parameters. As such, we rely on non-parametric techniques to estimate\nboth the probability density and the approximation of the Fisher information distance. Following these\napproximations, we are able to perform the same multi-dimensional scaling operations as previously\ndescribed.\nA. Kernel Density Estimation\nKernel methods are non-parametric techniques used for estimating probability densities of data sets.\nThese methods are similar to mixture-models in that they are defined by the normalized sum of multiple\ndensities. Unlike mixture models, however, kernel methods are non-parametric and are comprised of the\nnormalized sum of identical densities centered about each data point within the set (14). This yields a\nFebruary 12, 2013\n\nDRAFT\n\n\f16\n\ndensity estimate for the entire set in that highly probable regions will have more samples, and the sum\nof the kernels in those areas will be large, corresponding to a high probability in the resultant density.\nThe kernel density estimate (KDE) of a PDF is defined as\nN\n\np\u0302(x) =\n\n1 X\nK\nNh\ni=1\n\n\u0012\n\n\u0013\nx \u2212 xi\n,\nh\n\n(14)\n\nwhere K is some kernel satisfying the properties\nK(x) \u2265 0, \u2200x \u2208 X ,\nZ\nK(x) dx = 1,\n\nand h is the bandwidth or smoothing parameter.\nThere are two key points to note when using kernel density estimators. First, it is necessary to determine\nwhich distribution to use as the kernel. Without a priori knowledge of the original distribution, we choose\nto use Gaussian kernels,\nK(x) =\n\n1\n(2\u03c0)(d/2) |\u03a3|1/2\n\n\u0012\n\n\u0013\n1 T \u22121\nexp \u2212 x \u03a3 x ,\n2\n\n(15)\n\nwhere d is the dimension of x and \u03a3 is the covariance matrix, as they have the quadratic properties that will\nbe useful in implementation. Secondly, the bandwidth parameter is very important to the overall density\nestimate. Choosing a bandwidth parameter too small will yield a peak filled density, while a bandwidth\nthat is too large will generate a density estimate that is too smooth and loses most of the features of the\ndistribution. There has been much research done in calculating optimal bandwidth parameters, resulting\nin many different methods [29], [30] which can be used in our framework.\nWe note that the mean squared error of a KDE decreases only as n\u2212O(1/d) , which becomes extremely\nslow for large d. As such, it may be difficult to calculate good kernel density estimates. However, for\nour purposes, the estimation of densities is secondary to the estimation of the divergence between them.\nAs such, the issues with MSE of density estimates in large dimensions, while an area for future work,\nis not of immediate concern.\nB. Algorithm\nFisher Information Non-parametric Embedding (FINE) is presented in Algorithm 1 and combines all\nof the methods we have presented in order to find a low-dimensional embedding of a collection of data\nsets. If we assume each data set is a realization of an underlying PDF, and each of those distributions lie\non a manifold with some natural parameterization, then this embedding can be viewed as an embedding\nof the actual manifold into Euclidean space. Note that in line 5, 'embed(G, d)' refers to using any multi-\n\nFebruary 12, 2013\n\nDRAFT\n\n\f17\n\nAlgorithm 1 Fisher Information Non-parametric Embedding\nInput: Collection of data sets X = {X 1 , X 2 , . . . , X N }; the desired embedding dimension d\n1: for i = 1 to N do\n2:\nCalculate p\u0302i (x), the density estimate of X i\n3: end for\n4: Calculate G, where G(i, j) = D\u0302F (pi , pj ), the geodesic approximation of the Fisher information\ndistance\n5: Y = embed(G, d)\nOutput: d-dimensional embedding of X , into Euclidean space Y \u2208 Rd\u00d7N\n\n15\n10\n\n20\n\n5\n\n\u221220\n\n0\n\nz\n\n20\n0\n10\n\u22125\n0\n\n\u221210\n\n\u221215\n40\n\n\u221220\n\n\u221210\n\u221210\n20\n0\n\ny\n\n\u221210\n\n0\n\n\u22125\n\n5\n\n10\n\n0\n\n15\n\u221220\n\n10\n20\n\nx\n\n(a) Swiss Roll\n\n(b) FINE embedding\n\nFig. 6. Given a collection of data sets with a Gaussian distribution having means equal to points a sampled 'swiss roll'\nmanifold, our methods are able to reconstruct the original statistical manifold from which each data set is derived.\n\ndimensional scaling method (such as cMDS, Laplacian Eigenmaps, etc.) to embed the dissimilarity matrix\nG into a Euclidean space with dimension d.\n\nV. A PPLICATIONS\nWe have illustrated the uses of the presented framework in the previous sections with a manifold\nconsisting of the set of univariate normal densities, P . We now present several synthetic and practical applications for the framework, all of which are based around visualization and classification. In\neach application, the densities are unknown, but we assume they lie on a manifold with some natural\nparameterization.\nA. Simulated Data\nTo demonstrate the ability of our methods to reconstruct the statistical manifold, we create a known\nmanifold of densities. Let Y = {y1 , . . . , yn }, where each yi is uniformly sampled on the 'swiss roll'\nFebruary 12, 2013\n\nDRAFT\n\n\f18\n\nFig. 7. Historically, the process of clinical flow cytometry analysis relies on a series of 2-dimensional scatter plots in which\ncell populations are selected for further evaluation. This process does not take advantage of the multi-dimensional nature of the\nproblem.\n\nmanifold (see Fig. 6(a)). Let X = {X 1 , X 2 , . . . , X n } where each X i is generated from a normal\n\ndistribution N (yi , \u03a3), where \u03a3 is held constant for each density. As such, we have developed a statistical\n\nmanifold of known parameterization, which is sampled by known PDFs. Utilizing FINE in an unsupervised manner, we are able to recreate the original manifold Y strictly from the collection of data sets\nX . This is shown in Fig. 6(b) where each set is embedded into 3 cMDS dimensions, and the 'swiss roll'\n\nis reconstructed. While this embedding could easily be constructed using the mean of each set X i as a\nEuclidean location, it illustrates that FINE can be used for visualizing the statistical manifold as well,\nwithout a priori knowledge of the data.\nB. Flow Cytometry\nIn clinical flow cytometry, cellular suspensions are prepared from patient samples (blood, bone marrow,\nand solid tissue), and evaluated simultaneously for the presence of several expressed surface antigens and\nfor characteristic patterns of light scatter as the cells pass through an interrogating laser. Antibodies to each\ntarget antigen are conjugated to fluorescent markers, and each individual cell is evaluated via detection\nof the fluorescent signal from each marker. The result is a characteristic multi-dimensional distribution\nthat, depending on the panel of markers selected, may be distinct for a specific disease entity. The data\nfrom clinical flow cytometry can be considered multi-dimensional both from the standpoint of multiple\ncharacteristics measured for each cell, and from the standpoint of thousands of cells analyzed per sample.\nNonetheless, clinical pathologists generally interpret clinical flow cytometry results in the form of twodimensional scatter plots in which the axes each represent one of multiple cell characteristics analyzed (up\nto 8 parameters per cell in routine clinical flow cytometry, and many more parameters per cell in research\nFebruary 12, 2013\n\nDRAFT\n\n\f19\n\n500\n\n700\nCLL\nMCL\n\nCLL\nMCL\n\n450\n\n600\n400\n500\n\n350\n\nCD23\n\nCD23\n\n300\n400\n\n300\n\n250\n200\n150\n\n200\n\n100\n100\n50\n0\n\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n0\n\n0\n\n100\n\nFMC7\n\n(a) Scatter Plot\n\n200\n\n300\n\n400\n\n500\n\nFMC7\n\n(b) Contour Plot\n\nFig. 8. 2-dimensional plots of disease classes CLL and MCL. The overlapping nature of the scatter plots makes it difficult for\npathologists to differentiate disease classes using primitive 2-dimensional axes projections.\n\napplications). Additional parameters are often utilized to \"gate\" (i.e. select or exclude) specific cell sets\nbased on antigen expression or light scatter characteristics; however, clinical flow cytometry analysis\nremains a step-by-step process of 2-dimensional histogram analysis (Fig. 7), and the multidimensional\nnature of flow cytometry is routinely underutilized in clinical practice.\nAn example of the difficulty in analysis of 2-dimensional scatter plots is illustrated in Fig. 8. Two\ndistinct disease classes, mantle cell lymphoma (MCL) and chronic lymphocytic leukemia (CLL), are\nillustrated with both scatter and contour plots. Each point represents a distinct blood cell from two different\npatients, each containing one of the specified diseases; the axes represent those which pathologists have\ndetermined to be the two markers which are most differentiating for these two disease classes. It is clear\nthat for these two patients there is significant similarity in the scatter and contour plots of the data. The\noverlapping nature of these 2-dimensional scatter plot leads to a very primitive analysis of the available\ndata. It would be potentially beneficial, therefore, to develop systems for clustering and classification\nof clinical flow cytometry data that utilize all dimensions of data derived for each cell during routine\nclinical analysis. The variability of distributions of data in multidimensional flow cytometry over various\npatients is smaller than that associated with a general characterization of a multivariate distribution.\nThis leads us to believe that these distributions exist on some manifold with a much lower dimensional\nparameterization. Hence, we should be able to use FINE for the purpose of viewing a natural clustering\nof different patients into their respective disease classes based on the full set of markers evaluated in\neach multiparameter flow cytometric analysis.\nFor this analysis, we will compare patients with two distinct but immunophenotypically similar forms\nof lymphoid leukemia - mantle cell lymphoma (MCL) and chronic lymphocytic leukemia (CLL), as\nFebruary 12, 2013\n\nDRAFT\n\n\f20\n\n3\n2\n\nFINE2\n\n1\n0\n\u22121\n\u22122\n\u22123\n\u22124\n\u22126\n\nCLL\nMCL\n\u22124\n\n\u22122\n\n0\n\n2\n\n4\n\n6\n\nFINE1\n\nFig. 9. 2-dimensional embedding of CLL (\u2022) and MCL (+) patients using FINE with cMDS and the Kullback-Leibler divergence\nas a dissimilarity metric. The circled points correspond to the CLL and MCL cases highlighted in Fig. 8, which are difficult to\ndiscern with scatter plots, but well separated in the FINE space.\n\nillustrated in Fig. 8. These diseases display similar characteristics with respect to many expressed surface\nantigens, but are generally distinct in their patterns of expression of two common B lymphocyte antigens\nCD23 and FMC7 (a distinct conformational epitope of the CD20 antigen). Typically, CLL is positive\nfor expression of CD23 and negative for expression of FMC7, while MCL is positive for expression of\nFMC7 and negative for expression of CD23. These distinctions should lead to a difference in densities\nbetween patients in each disease class, and should show a natural clustering.\nLet X = {X 1 , X 2 , . . . , X n } where X i is the data set corresponding to the flow cytometer output\n\nof the ith patient. Each patient's blood is analyzed for 5 parameters: forward and side light scatter,\nand 3 fluorescent markers (CD45, CD23, FMC7). Hence, each data set X i is 5-dimensional with ni\n\nelements corresponding to individual blood cells (each ni may be different). Given that X is comprised\nof both patients with CLL and patients with MCL, we wish to analyze the performance of FINE for the\nvisualization and clustering of cytometry data.\nThe data set consists of 23 patients with CLL and 20 patients with MCL. The set X i for each patient\nis on the order of ni \u2248 5000 cells. The data and clinical diagnosis for each patient was provided by the\nDepartment of Pathology at the University of Michigan. Figure 9 shows the 2-dimensional embedding\nwith FINE, using cMDS and the Kullback-Leibler divergence set as the dissimilarity metric. Each point\nin the plot represents an individual patient. Although the discussed methods perform the dimensionality\nreduction and embedding in unsupervised methods, we display the class labels as a means of analysis.\nIt should be noted that there exists a natural separation between the different classes. As such, we can\nconclude that there is a natural difference in probability distribution between the disease classes as well.\nAlthough this is known through years of clinical experience, we were able to determine this without any\n\nFebruary 12, 2013\n\nDRAFT\n\n\f21\n\na priori knowledge; simply with a density analysis.\nAn important byproduct of this natural clustering is the ability to visualize the cytometry data in a\nmanner which allows comparisons between patients. The circled points in Fig. 9 correspond to the patients\nillustrated in Fig. 8, which were difficult to differentiate by using a scatter plot of the most discerning\nmarker combination as deemed by pathologists. In the space defined by FINE, the patients are easily\ndifferentiated and lie well within the clusters of each disease type. By using the embedding created with\nFINE, pathologists are able to determine similarities between patients, which gives them a quick and easy\nmeans of determining which data sets may need further investigation (i.e. for possible misdiagnosis).\nC. Document Classification\nRecent work has shown in interest in using dimensionality reduction for the purposes of document\nclassification [31] and visualization [32]. Typically documents are represented as very high-dimensional\nPDFs, and learning algorithms suffer from the curse of dimensionality. Dimensionality reduction not only\nalleviates these concerns, but it also reduces the computational complexity of learning algorithms due to\nthe resultant low-dimensional space. As such, the problem of document classification is an interesting\napplication for FINE.\nGiven a collection of documents of known class, we wish to best classify a document of unknown\nclass. A document can be viewed as a realization of some overriding probability distribution, in which\ndifferent distributions will create different documents. For example, in a newsgroup about computers you\ncould expect to see multiple instances of the term \"laptop\", while a group discussing recreation may see\nmany occurrences of \"sports\". The counts of \"laptop\" in the recreation group, or \"sports\" in the computer\ngroup would predictably be low. As such, the distributions between articles in computers and recreation\nshould be distinct. In this setting, we defined the PDFs as the term frequency representation of each\ndocument. Specifically, let xi be the number of times term i appears in a specific document. The PDF\nof that document can then be characterized as the multinomial distribution of normalized word counts,\nwith the maximum likelihood estimate provided as\n\u0012\n\u0013\nx1\nxN\np\u0302(x) = P , . . . , P\n.\ni xi\ni xi\n\n(16)\n\nBy utilizing the term frequencies as a multinomial distribution, and not implementing a kernel density\n\nestimator, we show that our methods are not tied to the KDE, but we simply use it in the case of\ncontinuous densities as a means of estimation. If one has a priori knowledge of the distribution, that\nstep is unnecessary. Additionally, we use the Hellinger distance due to the multinomial nature of the\ndistribution. As described in Section III-A3, DH has a monotonic transformation to DC , which is the\nnatural metric on the sphere defined by multinomial PDFs.\n\nFebruary 12, 2013\n\nDRAFT\n\n\f22\n\n\u22123\n\n6\n\nx 10\n\n0.15\n\n4\n\ncomp\nrec\nsci\ntalk\n\n0.1\n\n2\n\nPCA2\n\nFINE2\n\n0.05\n0\n\n0\n\u22122\n\ncomp\nrec\nsci\ntalk\n\n\u22124\n\n\u22126\n\u22122\n\n0\n\n2\n\n4\n\n\u22120.05\n\n6\n\nFINE1\n\n8\n\n\u22120.1\n0\n\n0.1\n\n\u22123\n\nx 10\n\n(a) FINE\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\nPCA1\n\n(b) PCA\n\nFig. 10. 2-dimensional embeddings of 20 Newsgroups data. The data displays some natural clustering, in the information based\nembedding, while the PCA embedding does not distinguish between classes.\n\nFor illustration, we will utilize the well known 20 Newsgroups data set1 , which is commonly used\nfor testing document classification methods. This set contains word counts for postings on 20 separate\nnewsgroups. We choose to restrict our simulation to the 4 domains with the largest number of subdomains (comp.*, rec.*, sci.*, and talk.*), and wish to classify each posting by its highest level domain.\nSpecifically we are given P = {p1 , . . . , pN } where each pi corresponds to a single newsgroup posting\nand is estimated with (16). We note that the data was preprocessed to remove all words that occur in 5\nor less documents2.\n1) Unsupervised FINE: First, we utilize unsupervised methods to see if the natural geometry exists\nbetween domains. Using Laplacian Eigenmaps on the dissimilarities calculated with the Hellinger distance,\nwe found an embedding P \u2192 R2 . Figure 10(a) shows the natural geometric separation between the\ndifferent document classes, although there is some overlap (which is to be expected). Contrarily, a\nPrincipal Components Analysis (PCA) embedding (Fig. 10(b)) does not demonstrate the same natural\nclustering. PCA is often used as a means to lower the dimension of data for learning problems due to its\noptimality for Euclidean data. However, the PCA embedding of the 20 Newsgroups set does not exhibit\nany natural class separation due to the non-Euclidean nature of the data.\nWe now compare the classification performance of FINE to that of PCA. In the case of document\nclassification, dimensionality reduction is important as the natural dimension (i.e. number of words) for\nthe 20 Newsgroups data set is 26, 214. Using local intrinsic dimension estimation [33], Fig. 11 shows\n1\n\nhttp://people.csail.mit.edu/jrennie/20Newsgroups/\n\n2\n\nhttp://www.cs.uiuc.edu/homes/dengcai2/Data/TextData.html\n\nFebruary 12, 2013\n\nDRAFT\n\n\f23\n\n1200\n\n1000\n\nCount\n\n800\n\n600\n\n400\n\n200\n\n0\n\n0\n\n50\n\n100\n\n150\n\n200\n\nDimension\n\nFig. 11.\nset.\n\nLocal dimension estimates for each document from a random subset of 4020 documents in the 20 Newsgroups data\n\n90\n85\n80\n75\n\nRate\n\n70\n65\n60\n55\n50\n\nFINE\nPCA\nLEM\n\n45\n40\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nDimension\n\nFig. 12. Classification rates for low-dimensional embedding using different methods for dimensionality reduction. 1-standard\ndeviation confidence intervals shown over 20-fold cross validation.\n\nthe histogram of the true dimensionality of the sample documents, so we test performance for lowdimensional embeddings P \u2192 Rd for d \u2208 [5, 95]. Following each embedding, we apply an SVM with\na linear kernel to classify the data in an 'all-vs-all' setting (i.e. classify each test sample as one of 4\ndifferent potential classes in a single event, rather than 4 separate binary events). The training and test\nsets were separated according to the recommended indices, and each set was randomly sub-sampled for\ncomputational purposes, keeping the ratio of training to test samples constant (2413 training samples,\n1607 test samples). Both the FINE and PCA settings jointly embed the training and test sets.\nFigure 12 illustrates that the embedding calculated with FINE outperforms using PCA as a means\nof dimensionality reduction. The classification rates are shown with a 1-standard deviation confidence\n\nFebruary 12, 2013\n\nDRAFT\n\n\f24\n\n0.015\n\n0.01\n\n0.005\n\n0\n\n\u22120.005\n\n\u22120.01\n\u221215\n\n0.02\n\u221210\n\u22123\n\n\u22125\n\n0.01\n0\n\n0\n5\n\n\u22120.01\n\nx 10\n\nFig. 13.\n\n3-dimensional embedding of 20 Newsgroups corpus using FINE in a supervised manner.\n\ninterval, and FINE with a dimension as low as d = 25 generates results comparable to those of a PCA\nembedding with d = 95. To ease any concerns that Laplacian Eigenmaps (LEM) is simply a better\nmethod for embedding these multinomial PDFs, we calculated an embedding with LEM in which each\nPDF was viewed as a Euclidean vector with the L2 -distance used as a dissimilarity metric. This form\nof embedding performed much worse than the information based embedding using the same form of\ndimensionality reduction and the same linear kernel SVM, while comparable to the PCA embedding in\nvery low dimensions.\n2) Supervised FINE: If we allow FINE to use supervised methods for embedding, we can dramatically\nimprove classification performance. By embedding with Classification Constrained Dimensionality Reduction (CCDR) [27], which is essentially LEM with an additional tuning parameter defining the emphasis\non class labels in the embedding, we are able to get good class separation even in 3 dimensions (Fig. 13).\nWe now compare FINE to the diffusion kernels developed by Lafferty and Lebanon [12] for the purpose\nof document classification. The diffusion kernels method uses the full term-frequency representation of\nthe data and does not utilize any dimensionality reduction. We stress this difference to determine whether\nor not using FINE for dimensionality reduction can generate comparable results.\nWe first illustrate the classification performance in a 'one vs. all' setting, in which all samples from\na single class were given a positive label (i.e. 1) and all remaining samples were labeled negatively\n(i.e. \u22121). In the FINE setting, we first subsampled from the training and test sets, using a test set size\n\nof 200, then used CCDR to embed the entire data set into Rd , with d \u2208 [5, 95] chosen to maximize\n\nclassification performance. The classification task was performed using a simple linear kernel SVM,\nK(X, Y ) = X * Y.\n\nFebruary 12, 2013\n\nDRAFT\n\n\f25\n\nTask\ncomp.*\n\nrec.*\n\nsci.*\n\ntalk.*\n\nL\n40\n80\n120\n200\n400\n600\n1000\n40\n80\n120\n200\n400\n600\n1000\n40\n80\n120\n200\n400\n600\n1000\n40\n80\n120\n200\n400\n600\n1000\n\nFINE\nMean\nSTD\n82.3750 4.1003\n85.8250 2.8713\n87.6000 2.0876\n87.9750 2.3978\n89.8000 2.0926\n90.6500 2.0970\n91.3000 2.3864\n82.3500 3.2610\n86.3500 2.0462\n87.1500 2.3345\n89.5500 1.4133\n91.4750 2.2152\n92.7500 1.2722\n93.2000 1.3318\n78.6500 2.8102\n80.3750 3.3280\n81.5250 2.8722\n83.4000 2.9585\n86.1750 2.2021\n87.1750 2.9212\n89.3000 2.3022\n89.1250 3.1241\n90.4250 2.8895\n91.1250 2.5745\n92.6500 1.8503\n93.1000 1.9775\n94.7500 1.3908\n94.8500 1.5483\n\nDiffusion\nMean\n75.5750\n83.0250\n85.5750\n87.8500\n89.6250\n91.3000\n91.9000\n76.2000\n82.0000\n83.1250\n86.8750\n90.7000\n93.1000\n94.6250\n76.3250\n77.4750\n78.2250\n82.2000\n86.2000\n87.0500\n89.8000\n82.2750\n85.9250\n86.5500\n89.7750\n92.4750\n94.3750\n94.8500\n\nKernels\nSTD\n3.9413\n3.4469\n3.2129\n2.2775\n1.9992\n2.4677\n2.2572\n3.1514\n3.8251\n3.9599\n2.1143\n2.0545\n2.0494\n1.4223\n3.2898\n4.2286\n3.1518\n3.0236\n2.2325\n2.9731\n2.2384\n2.9131\n3.6859\n4.0161\n3.1518\n2.1672\n1.5634\n1.4244\n\nTABLE I\nE XPERIMENTAL RESULTS ON 20 N EWSGROUPS CORPUS , COMPARING FINE USING CCDR AND A LINEAR SVM TO A\nMULTINOMIAL DIFFUSION KERNEL BASED SVM. T HE PERFORMANCE ( CLASSIFICATION RATE IN %) IS REPORTED AS MEAN\nAND STANDARD DEVIATION FOR DIFFERENT TRAINING SET SIZES L, OVER A 20- FOLD CROSS VALIDATION .\n\nFor the diffusion kernels setting,\n\u0013\n\u0010\u221a\n\u221a \u0011\n1\n2\nK(X, Y ) = (4\u03c0t) exp \u2212 arccos\nX* Y\n,\nt\nn\n2\n\n\u0012\n\nwe chose parameter value t which optimized the classification performance at each iteration. The experimental results of performance versus training set size, with 20-fold cross validation, are shown in Table\nI, where the highest performance at each range is highlighted. FINE shows a significant performance\n\nFebruary 12, 2013\n\nDRAFT\n\n\f26\n\n90\n85\n80\n\nRate (%)\n\n75\n70\n65\n60\n55\n50\n\nFINE\nDiffusion\nOOS\n\n45\n40\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\nTraining Samples\n\nFig. 14. Classification rates for low-dimensional embedding with FINE using CCDR vs Diffusion kernels. The classification task\nwas all v.s. all. Rates are plotted versus number of training samples. Confidence intervals are shown at one standard deviation.\nFor comparison to the joint embedding (FINE), we also plot the performance of FINE using out of sample extension (OOS).\n\nincrease over the diffusion kernels method for sets with low sample size. As the sample size increases,\nhowever, the gap in performance between the diffusion kernels method and FINE decreases, with diffusion\nkernels eventually surpassing FINE.\nWe now modify the classification task from a 'one v.s. all' to an 'all v.s. all' setting, in which each\nclass is given a different label and the task is to assign each test sample to a specific class. Classification\nrates are defined as the number of correctly classified test samples divided by the total number of test\nsamples (kept constant at 200). The structure of the experiment is otherwise identical to the 'one v.s.\nall' setting. We once again notice in Fig. 14 that FINE outperforms the diffusion kernels method for\nlow sample sizes. The point at which the diffusion kernels method surpasses FINE has decreased (i.e.\nL \u2248 200 for 'all v.s. all' compared to L \u2248 600 for 'one v.s. all'), yet FINE is still competitive as the\n\nsample size increases.\nWhile our focus when using FINE has been on jointly embedding both the training and test samples\n(while keeping the test samples unlabeled), Fig. 14 also illustrates the use of out of sample extension\n(OOS) [34] with FINE. In this scenario, the training samples are embedded as normal with CCDR, while\nthe test samples are embedded into the low-dimensional space using interpolation. This setting allows for\na significant decrease in computational complexity given the fact that the FINE embedding has already\nbeen determined for the training samples (i.e. new test samples are received). A decrease in performance\nexists when compared to the jointly embedded FINE, which is reduced as the number of training samples\nincreases.\nAnalysis of the results in both the 'one v.s. all' and 'all v.s. all' cases shows that FINE can improve\nupon the deficiencies of the diffusion kernels method in the low sample size region. By viewing each\n\nFebruary 12, 2013\n\nDRAFT\n\n\f27\n\n80\n79\n\nRate (%)\n\n78\n77\n76\n75\n\nLinear\nPolynomial\nRadial\n\n74\n73\n200\n\n250\n\n300\n\n350\n\n400\n\nTraining Samples\n\nFig. 15. Comparison of classification performance on the 20 Newsgroups data set with FINE using different SVM kernels;\none linear and two non-linear (2nd polynomial and radial basis function).\n\ndocument as a coarse approximation of the overriding class PDF, it is easy to see that, for low sample\nsizes, the estimate of the within class PDF generated by the diffusion kernels will be highly variable, which\nleads to poor performance. By reducing the dimension with FINE, the variance is limited to significantly\nfewer dimensions, enabling documents within each class to be drawn nearer to one another. While this\ncould also bring the classes closer to each other, the utilization of CCDR ensures class separation. This\nresults in better classification performance than using the entire multinomial distribution. As the number\nof training samples increases, the effect of dimensionality is reduced, which allows the diffusion kernels\nto better approximate the multinomial PDF representative of each class. This reduction in variance across\nall dimensions ensures that a few anomalous documents will not have the same drastic effect as they\nwould in the low sample size region. As such, the performance gain surpasses that of FINE, due to\nthe fact that the curse of dimensionality was alleviated elsewhere (i.e. increase in sample size). We note\nthat while FINE performs slightly worse than diffusion kernels in the large sample size region, it still\nperforms competitively with a leading classification method which utilizes the full dimensional data.\nAn additional reason for the diffusion kernels improved performance over FINE in the large sample\nsize region is that we have restricted FINE to using a linear kernel for this experiment, while the diffusion\nkernels method is very non-linear. We do this to show that even a simple linear classifier can perform\nadmirably in the FINE reduced space. Using a non-linear kernel would show increased performance\nwith FINE. This is illustrated in Fig. 15, where we compare the performance of FINE using an SVM\nclassifier with a linear kernel (K(X, Y ) = X T Y ), 2nd degree polynomial kernel (K(X, Y ) = (\u03b3X T Y )2 ),\nand a radial basis function kernel (K(X, Y ) = exp(\u2212\u03b3|X \u2212 Y |2 )), where \u03b3 is a weighting constant.\nFor visualization purposes, we show the results for only a subset of the training sample range (i.e.\nL = [200, 400]), but it is clear that the use of non-linear kernels improves the performance of FINE. The\nFebruary 12, 2013\n\nDRAFT\n\n\f28\n\nproblem of which of the many possible non-linear kernels is optimal remains open and is a subject for\nfuture work.\nVI. C ONCLUSIONS\nThe assumption that high-dimensional data lies on a Euclidean manifold is based on the ease of\nimplementation due to the wealth of knowledge and methods based on Euclidean space. This assumption\nis not viable in many problems of practical interest, as there is often no straightforward and meaningful\nEuclidean representation of the data. In these situations it is more appropriate to assume the data lies on\na statistical manifold. Using information geometry, we have shown the ability to find a low-dimensional\nembedding of the manifold, which allows us to not only find the natural separation of the data, but to\nalso reconstruct the original manifold and visualize it in a low-dimensional Euclidean space. This allows\nthe use of many well known learning techniques which work based on the assumption of Euclidean data.\nBy approximating the Fisher information distance, FINE is able to construct the Euclidean embedding\nwith an information based metric, which is more appropriate for non-Euclidean data. We have illustrated\nthis approximation by finding the length of the geodesic along the manifold, using approximations such\nas the Kullback-Leibler divergence and the Hellinger distance. The specific metric used to approximate\nthe Fisher information distance is determined by the problem, and FINE is not tied to any specific\nchoice of metric. Additionally, we point out that although we utilize kernel methods to obtain PDFs, the\nmethod used for density estimation is only of secondary concern. The primary focus is the measure of\ndissimilarity between densities, and the method used to calculate those PDFs is similarly determined by\nthe problem.\nWe have illustrated FINE's ability to be used in a variety of learning tasks such as visualization,\nclustering, and classification. FINE is a framework that can be used for a multitude of problems which\nmay seem to have little to nothing in common, such as flow cytometry and document classification. The\nonly commonality between the problems is that each are based around data which has no straightforward\nEuclidean representation, which is the only setting needed to utilize FINE. In future work we plan to\nutilize different classification methods (such as k-NN and using different SVM kernels) to maximize\nour document classification performance. This includes constraining our dimensionality reduction to a\nsphere, which will allow the use of diffusion kernels in a low-dimensional space. We also plan to continue\nstudies on the effect of using out of sample extension on our performance. Lastly, we will continue to\nfind applications which fit the setting for FINE, such as internet anomaly detection and face recognition,\nand determine whether or not these problems would benefit from our framework.\nVII. S PECIAL T HANKS\nWe would like to offer a special thanks to the Department of Pathology at the University of Michigan\nfor helping us isolate a problem of strong interest to them, as well as providing a multitude of data for\nFebruary 12, 2013\n\nDRAFT\n\n\f29\n\nanalysis. We would also like to thank Sung Jin Hwang of the University of Michigan for help with the\nclassification analysis and implementation of the SVMs.\nR EFERENCES\n[1] O. Bousquet, S. Boucheron, and G. Lugosi, \"Introduction to statistical learning theory,\" Advanced Lectures on Machine\nLearning, pp. 169\u2013207, 2004.\n[2] S. Amari and H. Nagaoka, Methods of Information Geometry, vol. 191, American Mathematical Society and Oxford\nUniversity Press, 2000, Translations of mathematical monographs.\n[3] T. Dietterich, \"Ai seminar,\" Carnegie Mellon, 2002.\n[4] G. Lebanon, \"Information geometry, the embedding principle, and document classification,\" in Proceedings of the 2nd\nInternational Symposium on Information Geometry and its Applications, 2005.\n[5] O. Arandjelovic, G. Shakhnarovich, J. Fisher, R. Cipolla, and T. Darrell, \"Face recognition with image sets using manifold\ndensity divergence,\" in Proceedings IEEE Conf. On Computer Vision and Pattern Recognition, June 2005, pp. 581\u2013588.\n[6] S. Lee, A. Abbott, N. Clark, and P. Araman, \"Active contours on statistical manifolds and texture segmentation,\" in\nInternational Conference on Image Processing 2005, 2005, vol. 3, pp. 828\u2013831.\n[7] A. Srivastava, I.H. Jermyn, and S. Joshi, \"Riemannian analysis of probability density functions with applications in vision,\"\nin Proceedings of IEEE Computer Vision and Pattern Recognition, June 2007.\n[8] J. Kim, Nonparametric statistical methods for image segmentation and shape analysis, Ph.D. thesis, Massachusetts Institute\nof Technology, February 2005.\n[9] G. Pistone and M. Rogantin, \"The exponential statistical manifold: Mean parameters, orthogonality and space transformations,\" Bernoulli, vol. 5, no. 4, pp. 721\u2013760, August 1999.\n[10] A. Cena, Geometric structures on the non-parametric statistical manifold, Ph.D. thesis, Universit\u00e1 Degli Studi Di Milano,\n2002.\n[11] G. Lebanon, \"Axiomatic geometry of conditional models,\" IEEE Transactions on Information Theory, vol. 51, no. 4, pp.\n1283\u20131294, April 2005.\n[12] J. Lafferty and G. Lebanon, \"Diffusion kernels on statistical manifolds,\" Journal of Machine Learning Research, vol. 6,\npp. 129\u2013163, Jan 2005.\n[13] J. Salojarvi, S. Kaski, and J. Sinkkonen, \"Discriminative clustering in fisher metrics,\" in Artificial Neural Networks and\nNeural Information Processing - Supplementary prodceedings ICANN/ICONIP 2003, June 2003, pp. 161\u2013164.\n[14] C. Yeang, \"An information geometric perspective on active learning,\" in ECML '02: Proceedings of the 13th European\nConference on Machine Learning, London, UK, 2002, pp. 480\u2013492, Springer-Verlag.\n[15] S-M. Lee, A. L. Abbott, and P. A. Araman, \"Dimensionality reduction and clustering on statistical manifolds,\" in Proc.\nIEEE Conf. on Computer Vision and Pattern Recognition, June 2007, pp. 1\u20137.\n[16] K. M. Carter, R. Raich, and A. O. Hero, \"Learning on statistical manifolds for clustering and visualization,\" in Proceedings\nof Forty-Fifth Annual Allerton Conference on Communication, Control, and Computing, September 2007, to appear.\nhttp://tbayes.eecs.umich.edu/kmcarter/LearnStatMan.html.\n[17] K. M. Carter, R. Raich, and A. O. Hero, \"Fine: Information embedding for document classification,\" in Proc. IEEE Intl.\nConf. on Acoustics, Speech and Signal Processing, April 2008, to appear.\n[18] S. Amari and H. Nagaoka, Differential-geometrical methods in statistics, Springer, 1990.\n[19] R. Kass and P. Vos, Geometrical Foundations of Asymptotic Inference, Wiley Series in Probability and Statistics. John\nWiley and Sons, NY, USA, 1997.\n[20] S.I.R. Costa, S. Santos, and J. Strapasson, \"Fisher information matrix and hyperbolic geometry,\" in Proceedings of IEEE\nITSOC Information Theory Workshop on Coding and Complexity, August 2005.\n\nFebruary 12, 2013\n\nDRAFT\n\n\f30\n\n[21] J. B. Tenenbaum, V. de Silva, and J. C. Langford, \"A global geometric framework for nonlinear dimensionality reduction,\"\nScience, vol. 290, pp. 2319\u20132323, 2000.\n[22] M. Belkin and P. Niyogi, \"Laplacian eigenmaps and spectral techniques for embedding and clustering,\" in Advances in\nNeural Information Processing Systems, Volume 14, T. G. Dietterich, S. Becker, and Z. Ghahramani, Eds. MIT Press, 2002.\n[23] I. Csisz\u00e1r, \"Information type measures of differences of probability distribution and indirect observations,\" Studia Sci.\nMath. Hungarica 2, pp. 299\u2013318, 1967.\n[24] A. Renyi, \"On measures of information and entropy,\" in Proceedings of the 4th Berkeley Symposium on Mathematics,\nStatistics and Probability, 1961, pp. 547\u2013561.\n[25] S.K. Zhou and R. Chellappa, \"From sample similarity to ensemble similarity: Probabilistic distance measures in reproducing\nkernel hilbert space,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 6, pp. 917\u2013929, June\n2006.\n[26] S. Roweis and L. Saul, \"Nonlinear dimensionality reduction by locally linear embedding,\" Science, vol. 290, no. 1, pp.\n2323\u20132326, 2000.\n[27] R. Raich, J. A. Costa, and A. O. Hero, \"On dimensionality reduction for classification and its applications,\" in Proc. IEEE\nIntl. Conference on Acoustic Speech and Signal Processing, May 2006, vol. 5.\n[28] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov, \"Neural information processing systems,\" Neighbourhood\nComponent Analysis, , no. 17, pp. 513\u2013520, 2004.\n[29] B. Silverman, Density Estimation for Statistics and Data Analysis (Monographs on Statistics and Applied Probability),\nJohn Wiley and Sons, 1986.\n[30] G. Terrell, \"The maximal smoothing principle in density estimation,\" Journal of the American Statistical Association, vol.\n85, no. 410, pp. 470\u2013477, June 1990.\n[31] H. Kim, P. Howland, and H. Park, \"Dimension reduction in text classification with support vector machines,\" in Journal\nof Machine Learning Research 6, January 2005, pp. 37\u201353.\n[32] S. Huang, M. O. Ward, and E. A. Rundensteiner, \"Exploration of dimensionality reduction for text visualization,\" in Proc.\nIEEE Third Intl. Conf. on Coordinated and Multiple Views in Exploratory Visualization, July 2005, pp. 63\u201374.\n[33] K. M. Carter, A. O. Hero, and R. Raich, \"De-biasing for intrinsic dimension estimation,\" in Proc. IEEE Statistical Signal\nProcessing Workshop, August 2007, pp. 601\u2013605.\n[34] R. Raich, J. A. Costa, S. B. Damelin, and A. O. Hero, \"Classification constrained dimensionality reduction,\" IEEE\nTransactions on Signal Processing, 2008, to be submitted.\n\nFebruary 12, 2013\n\nDRAFT\n\n\f"}