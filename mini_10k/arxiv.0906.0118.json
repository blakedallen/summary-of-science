{"id": "http://arxiv.org/abs/0906.0118v2", "guidislink": true, "updated": "2009-08-24T22:40:24Z", "updated_parsed": [2009, 8, 24, 22, 40, 24, 0, 236, 0], "published": "2009-05-31T18:50:17Z", "published_parsed": [2009, 5, 31, 18, 50, 17, 6, 151, 0], "title": "Quasi light fields: extending the light field to coherent radiation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.4290%2C0906.1054%2C0906.4075%2C0906.5189%2C0906.5180%2C0906.0809%2C0906.5063%2C0906.3294%2C0906.4410%2C0906.1058%2C0906.5022%2C0906.0986%2C0906.5239%2C0906.4205%2C0906.4392%2C0906.4991%2C0906.3930%2C0906.3850%2C0906.4194%2C0906.5551%2C0906.4672%2C0906.4026%2C0906.1714%2C0906.3838%2C0906.2496%2C0906.5047%2C0906.5264%2C0906.3362%2C0906.0982%2C0906.0904%2C0906.0424%2C0906.4352%2C0906.0268%2C0906.0768%2C0906.3506%2C0906.2031%2C0906.0727%2C0906.4708%2C0906.5019%2C0906.1864%2C0906.4281%2C0906.5125%2C0906.1967%2C0906.0952%2C0906.4535%2C0906.4398%2C0906.0898%2C0906.3914%2C0906.2792%2C0906.0874%2C0906.5428%2C0906.0156%2C0906.2358%2C0906.3170%2C0906.1945%2C0906.3282%2C0906.4879%2C0906.1295%2C0906.3437%2C0906.0417%2C0906.5427%2C0906.0808%2C0906.2581%2C0906.4377%2C0906.3298%2C0906.1197%2C0906.0885%2C0906.1907%2C0906.5393%2C0906.5291%2C0906.5132%2C0906.0583%2C0906.0774%2C0906.2327%2C0906.0676%2C0906.0911%2C0906.2081%2C0906.2223%2C0906.4298%2C0906.0243%2C0906.2214%2C0906.4769%2C0906.4227%2C0906.2250%2C0906.0118%2C0906.3074%2C0906.3756%2C0906.1886%2C0906.0368%2C0906.4042%2C0906.0686%2C0906.4359%2C0906.0820%2C0906.1143%2C0906.0139%2C0906.1437%2C0906.1230%2C0906.0281%2C0906.0940%2C0906.0920%2C0906.1725&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Quasi light fields: extending the light field to coherent radiation"}, "summary": "Imaging technologies such as dynamic viewpoint generation are engineered for\nincoherent radiation using the traditional light field, and for coherent\nradiation using electromagnetic field theory. We present a model of coherent\nimage formation that strikes a balance between the utility of the light field\nand the comprehensive predictive power of Maxwell's equations. We synthesize\nresearch in optics and signal processing to formulate, capture, and form images\nfrom quasi light fields, which extend the light field from incoherent to\ncoherent radiation. Our coherent cameras generalize the classic beamforming\nalgorithm in sensor array processing, and invite further research on\nalternative notions of image formation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.4290%2C0906.1054%2C0906.4075%2C0906.5189%2C0906.5180%2C0906.0809%2C0906.5063%2C0906.3294%2C0906.4410%2C0906.1058%2C0906.5022%2C0906.0986%2C0906.5239%2C0906.4205%2C0906.4392%2C0906.4991%2C0906.3930%2C0906.3850%2C0906.4194%2C0906.5551%2C0906.4672%2C0906.4026%2C0906.1714%2C0906.3838%2C0906.2496%2C0906.5047%2C0906.5264%2C0906.3362%2C0906.0982%2C0906.0904%2C0906.0424%2C0906.4352%2C0906.0268%2C0906.0768%2C0906.3506%2C0906.2031%2C0906.0727%2C0906.4708%2C0906.5019%2C0906.1864%2C0906.4281%2C0906.5125%2C0906.1967%2C0906.0952%2C0906.4535%2C0906.4398%2C0906.0898%2C0906.3914%2C0906.2792%2C0906.0874%2C0906.5428%2C0906.0156%2C0906.2358%2C0906.3170%2C0906.1945%2C0906.3282%2C0906.4879%2C0906.1295%2C0906.3437%2C0906.0417%2C0906.5427%2C0906.0808%2C0906.2581%2C0906.4377%2C0906.3298%2C0906.1197%2C0906.0885%2C0906.1907%2C0906.5393%2C0906.5291%2C0906.5132%2C0906.0583%2C0906.0774%2C0906.2327%2C0906.0676%2C0906.0911%2C0906.2081%2C0906.2223%2C0906.4298%2C0906.0243%2C0906.2214%2C0906.4769%2C0906.4227%2C0906.2250%2C0906.0118%2C0906.3074%2C0906.3756%2C0906.1886%2C0906.0368%2C0906.4042%2C0906.0686%2C0906.4359%2C0906.0820%2C0906.1143%2C0906.0139%2C0906.1437%2C0906.1230%2C0906.0281%2C0906.0940%2C0906.0920%2C0906.1725&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Imaging technologies such as dynamic viewpoint generation are engineered for\nincoherent radiation using the traditional light field, and for coherent\nradiation using electromagnetic field theory. We present a model of coherent\nimage formation that strikes a balance between the utility of the light field\nand the comprehensive predictive power of Maxwell's equations. We synthesize\nresearch in optics and signal processing to formulate, capture, and form images\nfrom quasi light fields, which extend the light field from incoherent to\ncoherent radiation. Our coherent cameras generalize the classic beamforming\nalgorithm in sensor array processing, and invite further research on\nalternative notions of image formation."}, "authors": ["Anthony Accardi", "Gregory Wornell"], "author_detail": {"name": "Gregory Wornell"}, "author": "Gregory Wornell", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1364/JOSAA.26.002055", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0906.0118v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0906.0118v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "This paper was published in JOSA A. The final version is available on\n  the OSA website:\n  http://www.opticsinfobase.org/josaa/abstract.cfm?URI=josaa-26-9-2055", "arxiv_primary_category": {"term": "physics.optics", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.optics", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0906.0118v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0906.0118v2", "journal_reference": "J. Opt. Soc. Am. A 26, 2055-2066 (2009)", "doi": "10.1364/JOSAA.26.002055", "fulltext": "arXiv:0906.0118v2 [physics.optics] 24 Aug 2009\n\nQuasi light fields:\nextending the light field to coherent radiation\nAnthony Accardi and Gregory Wornell\nMassachusetts Institute of Technology\n\nJuly 30, 2009\nImaging technologies such as dynamic viewpoint generation are engineered for\nincoherent radiation using the traditional light field, and for coherent radiation\nusing electromagnetic field theory. We present a model of coherent image\nformation that strikes a balance between the utility of the light field and\nthe comprehensive predictive power of Maxwell's equations. We synthesize\nresearch in optics and signal processing to formulate, capture, and form images\nfrom quasi light fields, which extend the light field from incoherent to coherent\nradiation. Our coherent cameras generalize the classic beamforming algorithm\nin sensor array processing, and invite further research on alternative notions\nof image formation.\n\n1\n\nIntroduction\n\nThe light field represents radiance as a function of position and direction, thereby decomposing optical power flow along rays. The light field is an important tool used in many\nimaging applications in different disciplines, but is traditionally limited to incoherent light.\nIn computer graphics, a rendering pipeline can compute new views at arbitrary camera positions from the light field [1]. In computational photography, a camera can measure the\nlight field and later generate images focused at different depths, after the picture is taken\n[2]. In electronic displays, an array of projectors can present multiple viewpoints encoded in\nthe light field, enabling 3D television [3]. Many recent incoherent imaging innovations have\nbeen made possible by expressing image pixel values as appropriate integrals over light field\nrays.\nFor coherent imaging applications, the value of decomposing power by position and direction has long been recognized without the aid of a light field, since the complex-valued\nscalar field encodes direction in its phase. A hologram encodes multiple viewpoints, but in\na different way than the light field [4]. An ultrasound machine generates images focused\nat different depths, but from air pressure instead of light field measurements [5]. A Wigner\ndistribution function models the operation of optical systems in simple ways, by conveniently\n1\n\n\finferring direction from the scalar field instead of computing non-negative light field values\n[6]. Comparing these applications, coherent imaging uses the scalar field to achieve results\nsimilar to those that incoherent imaging obtains with the light field.\nOur goal is to provide a model of coherent image formation that combines the utility of\nthe light field with the comprehensive predictive power of the scalar field. The similarities\nbetween coherent and incoherent imaging motivate exploring how the scalar field and light\nfield are related, which we address by synthesizing research across three different communities. Each community is concerned with a particular Fourier transform pair and has its own\nname for the light field. In optics, the pair is position and direction, and Walther discovered\nthe first generalized radiance function by matching power predictions made with radiometry\nand scalar field theory [7]. In quantum physics, the pair is position and momentum, and\nWigner discovered the first quasi-probability distribution, or phase-space distribution, as an\naid to computing the expectation value of a quantum operator [8]. In signal processing,\nthe pair is time and frequency, and while instantaneous spectra were used as early as 1890\nby Sommerfeld, Ville is generally credited with discovering the first nontrivial quadratic\ntime-frequency distribution by considering how to distribute the energy of a signal over time\nand frequency [9]. Walther, Wigner, and Ville independently arrived at essentially the same\nfunction, which is one of the ways to express a light field for coherent radiation in terms of\nthe scalar field.\nThe light field has its roots in radiometry, a phenomenological theory of radiative power\ntransport that began with Herschel's observations of the sun [10], developed through the\nwork of astrophysicists such as Chandrasekhar [11], and culminated with its grounding in\nelectromagnetic field theory by Friberg et al. [12]. The light field represents radiance,\nwhich is the fundamental quantity in radiometry, defined as power per unit projected area\nper unit solid angle. Illuminating engineers would integrate radiance to compute power\nquantities, although no one could validate these calculations with the electromagnetic field\ntheory formulated by Maxwell. Gershun was one of many physicists who attempted to\nphysically justify radiometry, and who introduced the phrase light field to represent a threedimensional vector field analogous to the electric and magnetic fields [13]. Gershun's light\nfield is a degenerate version of the one we discuss, and more closely resembles the timeaveraged Poynting vector that appears in a rigorous derivation of geometric optics [14].\nSubsequently, Walther generalized radiometry to coherent radiation in two different ways[7,\n15], and Wolf connected Walther's work to quantum physics [16], ultimately leading to\nthe discovery of many more generalized radiance functions [17] and a firm foundation for\nradiometry [12].\nMeanwhile, machine vision researchers desired a representation for all the possible pictures a pinhole camera might take in space-time, which led to the current formulation of the\nlight field. Inspired by Leonardo da Vinci, Adelson and Bergen defined a plenoptic function\nto describe \"everything that can be seen\" as the intensity recorded by a pinhole camera,\nparametrized by position, direction, time, and wavelength [18]. Levoy and Hanrahan tied\nthe plenoptic function more firmly to radiometry, by redefining Gershun's phrase light field\nto mean radiance parametrized by position and direction [1]. Gortler et al. introduced the\n\n2\n\n\fsame construct, but instead called it the lumigraph [19]. The light field is now the dominant\nterminology used in incoherent imaging contexts.\nOur contribution is to describe and characterize all the ways to extend the light field to\ncoherent radiation, and to interpret coherent image formation using the resulting extended\nlight fields. We call our extended light fields quasi light fields, which are analogous to the\ngeneralized radiance functions of optics, the quasi-probability and phase-space distributions\nof quantum physics, and the quadratic class of time-frequency distributions of signal processing. Agarwal et al. have already extended the light field to coherent radiation [17], and\nthe signal processing community has already classified all of the ways to distribute power\nover time and frequency [20]. Both have traced their roots to quantum physics. But to our\nknowledge, no one has connected the research to show 1) that the quasi light fields represent\nall the ways to extend the light field to coherent radiation, and 2) that the signal processing\nclassification informs which quasi light field to use for a specific application. We further\ncontextualize the references, making any unfamiliar literature more accessible to specialists\nin other areas.\nOur paper is organized as follows. We describe the traditional light field in Section 2. We\nformulate the quasi light fields in Section 3, by reviewing and relating the relevant research\nin optics, quantum physics, and signal processing. In Section 4, we describe how to capture\nquasi light fields, discuss practical sampling issues, and illustrate the impact of light field\nchoice on energy localization. In Section 5, we describe how to form images with quasi light\nfields. We derive a light field camera, demonstrate and compensate for diffraction limitations\nin the near zone, and generalize the classic beamforming algorithm in sensor array processing.\nWe conclude the paper in Section 6, where we remark on the utility of quasi light fields and\nfuture perspectives on image formation.\n\n2\n\nThe Traditional Light Field\n\nThe light field is a useful tool for incoherent imaging because it acts as an intermediary\nbetween the camera and the picture, decoupling information capture and image production:\nthe camera measures the light field, from which many different traditional pictures can be\ncomputed. We define a pixel in the image of a scene by a surface patch \u03c3 and a virtual\naperture (Figure 1). Specifically, we define the pixel value as the power P radiated by\n\u03c3 towards the aperture, just as an ideal single-lens camera would measure. According to\nradiometry, P is an integral over a bundle of light field rays [21]:\nZ Z\nL(r, s) cos \u03c8 d2 s d2 r,\n(1)\nP =\n\u03c3\n\n\u03a9r\n\nwhere L(r, s) is the radiance at position r and in unit direction s, \u03c8 is the angle that s makes\nwith the surface normal at r, and \u03a9r is the solid angle subtended by the virtual aperture at\nr. The images produced by many different conventional cameras can be computed from the\nlight field using (1) [22].\n3\n\n\fvirtual\naperture\n\nscene\nsurface\npatch\n\n\u03c3\n\nremote camera\nmeasurement\nhardware\n\n\u03c8 \u03a9r P\n\nrM\n\ns\n\ns\n\nrP\nL rP , s = L rM , s\n\nFigure 1: We can compute the value of each pixel in an image produced by an arbitrary virtual\ncamera, defined as the power emitted from a scene surface patch towards a virtual aperture, by\nintegrating an appropriate bundle of light field rays that have been previously captured with remote\nhardware.\n\nThe light field has an important property that allows us to measure it remotely: the\nlight field is constant along rays in a lossless medium [21]. To measure the light field on\nthe surface of a scene, we follow the rays for the images we are interested in, and intercept\nthose rays with our camera hardware (Figure 1). However, our hardware must be capable\nof measuring the radiance at a point and in a specific direction; a conventional camera that\nsimply measures the irradiance at a point is insufficient. We can discern directional power\nflow using a lens array, as is done in a plenoptic camera [2].\nIn order to generate coherent images using the same framework described above, we must\novercome three challenges. First, we must determine how to measure power flow by position\nand direction to formulate a coherent light field. Second, we must capture the coherent light\nfield remotely and be able to infer behavior at the scene surface. Third, we must be able to\nuse (1) to produce correct power values, so that we can form images by integrating over the\ncoherent light field. We address each challenge in a subsequent section.\n\n3\n\nFormulating Quasi Light Fields\n\nWe motivate, systematically generate, and characterize the quasi light fields by relating\nexisting research. We begin in Section 3.1 with research in optics that frames the challenge\nof extending the light field to coherent radiation in terms of satisfying a power constraint\nrequired for radiometry to make power predictions consistent with scalar field theory. While\nuseful in developing an intuition for quasi light fields, the power constraint does not allow\nus to easily determine the quasi light fields. We therefore proceed in Section 3.2 to describe\nresearch in quantum physics that systematically generates quasi light fields satisfying the\npower constraint, and that shows how the quasi light fields are true extensions that reduce\nto the traditional light field under certain conditions. While useful for generating the quasi\nlight fields, the quantum physics approach does not allow us to easily characterize them.\nTherefore, in Section 3.3 we map the generated quasi light fields to the quadratic class of\n4\n\n\ftime-frequency distributions, which has been extensively characterized and classified by the\nsignal processing community. By relating research in optics, quantum physics, and signal\nprocessing, we express all the ways to extend the light field to coherent radiation, and provide\ninsight on how to select an appropriate quasi light field for a particular application.\nWe assume a perfectly coherent complex scalar field U(r) at a fixed frequency \u03bd for\nsimplicity, although we comment in Section 6 on how to extend the results to broadband,\npartially coherent radiation. The radiometric theory we discuss assumes a planar source at\nz = 0. Consequently, although the light field is defined in three-dimensional space, much of\nour analysis is confined to planes z = z0 parallel to the source. Therefore, for convenience,\nwe use r = (x, y, z) and s = (sx , sy , sz ) to indicate three-dimensional vectors and r\u22a5 = (x, y)\nand s\u22a5 = (sx , sy ) to indicate two-dimensional, projected versions.\n\n3.1\n\nIntuition from Optics\n\nAn extended light field must produce accurate power transport predictions consistent with\nrigorous theory; thus the power computed from the scalar field using wave optics determines\nthe allowable light fields via the laws of radiometry. One way to find extended light fields\nis to guess a light field equation that satisfies this power constraint, which is how Walther\nidentified the first extended light field [7]. The scenario involves a planar source at z = 0\ndescribed by U(r), and a sphere of large radius \u03c1 centered at the origin. We use scalar\nfield theory to compute the flux through part of the sphere, and then use the definition of\nradiance to determine the light field from the flux.\nAccording to scalar field theory, the differential flux d\u03a6 through a portion of the sphere\nsubtending differential solid angle d\u03a9 is given by integrating the radial component of the\nenergy flux density vector F. From diffraction theory, the scalar field in the far zone is\nU \u221e (\u03c1s) = \u2212\n\n2\u03c0i exp(ik\u03c1)\nsz\na(s)\nk\n\u03c1\n\nwhere k = 2\u03c0/\u03bb is the wave number, \u03bb is the wavelength, and\n\u0012 \u00132 Z\nk\na(s) =\nU(r) exp(\u2212iks * r) d2 r\n2\u03c0\nis the plane wave component in direction s [23]. Now\n\u0012 \u00132\ns2\n2\u03c0\n\u221e\na(s)a\u2217 (s) z2 s,\nF (\u03c1s) =\nk\n\u03c1\n\n(2)\n\n(3)\n\n(4)\n\nso that\n\n\u00132\n2\u03c0\nd\u03a6 =\ns2z a(s)a\u2217 (s) d\u03a9.\nk\nAccording to radiometry, radiant intensity is flux per unit solid angle\n\u0012 \u00132\nd\u03a6\n2\u03c0\nI(s) =\n=\ns2z a(s)a\u2217 (s).\nd\u03a9\nk\n\u0012\n\n5\n\n(5)\n\n(6)\n\n\fRadiance is I(s) per unit projected area [21], and this is where the guessing happens: there\nare many ways to distribute (6) over projected area by factoring out sz and an outer integral\nover the source plane, but none yield light fields that satisfy all the traditional properties of\nradiance [24]. One way to factor (6) is to substitute the expression for a(s) from (3) into (6)\nand change variables:\n#\n\u0013 \u0012\n\u0013\n\u0012\nZ \"\u0012 \u00132 Z\nk\n1 \u2032\n1 \u2032\n2 \u2032\n\u2217\n\u2032\nI(s) = sz\n(7)\nsz U r + r U r \u2212 r exp (\u2212iks\u22a5 * r\u22a5 ) d r d2 r.\n2\u03c0\n2\n2\nThe bracketed expression is Walther's first extended light field\n\u0012 \u00132\nk\nW\nsz W(r, s\u22a5 /\u03bb),\nL (r, s) =\n2\u03c0\nwhere\nW(r, s\u22a5 ) =\n\nZ\n\nU\n\n\u0012\n\n\u0012\n\u0013\n\u0013\n1 \u2032\n1 \u2032\n\u2217\nr + r U r \u2212 r exp (\u2212i2\u03c0s\u22a5 * r\u2032\u22a5 ) d2 r \u2032\n2\n2\n\n(8)\n\n(9)\n\nis the Wigner distribution [25]. We may manually factor (6) differently to obtain other\nextended light fields in an ad hoc manner, but it is hard to find and verify the properties of\nall extended light fields this way, and we would have to individually analyze each light field\nthat we do manage to find. So instead, we pursue a systematic approach to exhaustively\nidentify and characterize the extended light fields that guarantee the correct radiant intensity\nin (6).\n\n3.2\n\nGenerating Explicit Extensions from Quantum Physics\n\nThe mathematics of quantum physics provides us with a systematic extended light field\ngenerator that factors the radiant intensity in (6) in a structured way. Walther's extended\nlight field in (8) provides the hint for this connection between radiometry and quantum\nphysics. Specifically, Wolf recognized the similarity between Walther's light field and the\nWigner phase-space distribution [8] from quantum physics [16]. Subsequently, Agarwal,\nFoley, and Wolf repurposed the mathematics behind phase-space representation theory to\ngenerate new light fields instead of distributions [17]. We summarize their approach, define\nthe class of quasi light fields, describe how quasi light fields extend traditional radiometry,\nand show how quasi light fields can be conveniently expressed as filtered Wigner distributions.\nAgarwal et al.'s key insight was to introduce a position operator r\u0302\u22a5 and a direction\noperator \u015d\u22a5 that obey the commutation relations [26]\n[x\u0302, \u015dx ] = i\u03bb/2\u03c0,\n\n[\u0177, \u015dy ] = i\u03bb/2\u03c0,\n\n(10)\n\nand to map the different ways of ordering the operators to different extended light fields. This\nformulation is valuable for two reasons. First, (10) is analogous to the quantum-mechanical\nrelations for position and momentum, allowing us to exploit the phase-space distribution\ngenerator from quantum physics for our own purposes, thereby providing an explicit formula\n6\n\n\ffor extended light fields. Second, in the geometric optics limit as \u03bb \u2192 0, the operators\ncommute per (10), so that all of the extended light fields collapse to the same function that\ncan be related to the traditional light field. Therefore, Agarwal et al.'s formulation not only\nprovides us with different ways of expressing the light field for coherent radiation, but also\nexplains how these differences arise as the wavelength becomes non-negligible.\nWe now summarize the phase-space representation calculus that Agarwal and Wolf invented [27] to map operator orderings to functions, which Agarwal et al. later applied to\nradiometry [17], culminating in a formula for extended light fields. The phase-space representation theory generates a function L\u0303\u03a9 from any operator L\u0302 for each distinct way \u03a9 of\nordering collections of r\u0302\u22a5 and \u015d\u22a5 . So by choosing a specific L\u0302 defined by its matrix elements\nusing the Dirac notation [26]\nE\nD\n\u0001\n\u0001\nC\nR\n(11)\nr\u22a5 L\u0302 r\u22a5 = U rR U \u2217 rC ,\nand supplying L\u0302 as input, we obtain the extended light fields\n\u0012 \u00132\nk\n\u03a9\nL (r, s) =\nsz L\u0303\u03a9 (r\u22a5 , s\u22a5 )\n2\u03c0\n\n(12)\n\nas outputs. The power constraint from Section 3.1 translates to a minor constraint on the\nallowed orderings \u03a9, so that L\u03a9 can be factored from (6). Finally, there is an explicit formula\nfor L\u03a9 [27], which in Friberg et al.'s form [12] reads\nZZZ\nk2\n\u03a9\nsz\n\u03a9\u0303 (u, kr\u2032\u2032\u22a5 ) exp [\u2212iu * (r\u22a5 \u2212 r\u2032\u22a5 )] exp (\u2212iks\u22a5 * r\u2032\u2032\u22a5 )\nL (r, s) =\n(2\u03c0)4\n\u0012\n\u0013 \u0012\n\u0013\n1 \u2032\u2032\n1 \u2032\u2032\n\u2032\n\u2032\n\u2217\n\u00d7U r + r U r \u2212 r d2 u d2 r \u2032 d2 r \u2032\u2032 ,\n(13)\n2\n2\nwhere \u03a9\u0303 is a functional representation of the ordering \u03a9.\nPrevious research has related the extended light fields L\u03a9 to the traditional light field,\nby examining how the L\u03a9 behave for globally incoherent light of a small wavelength, an\nenvironment technically modeled by a quasi-homogeneous source in the geometric optics\nlimit where \u03bb \u2192 0. As \u03bb \u2192 0, r\u0302\u22a5 and \u015d\u22a5 commute per (10), so that all orderings \u03a9 are\nequivalent and all of the extended light fields L\u03a9 collapse to the same function. Since, in the\nsource plane, Foley and Wolf showed that one of those light fields behaves like traditional\nradiance [28] for globally incoherent light of a small wavelength, all of the L\u03a9 behave like\ntraditional radiance for globally incoherent light of a small wavelength. Furthermore, Friberg\net al. showed that many of the L\u03a9 are constant along rays, for globally incoherent light of\na small wavelength [12]. The L\u03a9 thereby subsume the traditional light field, and globally\nincoherent light of a small wavelength is the environment in which traditional radiometry\nholds.\nTo more easily relate L\u03a9 to the signal processing literature, we conveniently express L\u03a9\nas a filtered Wigner distribution. We introduce a function \u03a0 and substitute\nZZ\n\u03a9\u0303(u, v) =\n\u03a0(\u2212a, \u2212b) exp [\u2212i(a * u + b * v)] d2 a d2 b\n(14)\n7\n\n\finto (13), integrate first over u, then over a, and finally substitute b = s\u2032\u22a5 \u2212 s\u22a5 :\n\u03a9\n\nL (r, s) =\n=\n\n\u0012\n\n\u0012\n\nk\n2\u03c0\nk\n2\u03c0\n\n\u00132\n\u00132\n\nsz\n\nZZ\n\n\u03a0(r\u22a5 \u2212 r\u2032\u22a5 , s\u22a5 \u2212 s\u2032\u22a5 )W(r\u2032 , s\u2032\u22a5 /\u03bb) d2 r \u2032 d2 s\u2032\n\nsz \u03a0(r\u22a5 , s\u22a5 ) \u2297 W(r, s\u22a5 /\u03bb).\n\n(15)\n\nThe symbol \u2297 in (15) denotes convolution in both r\u22a5 and s\u22a5 . Each filter kernel \u03a0 yields a\ndifferent light field. There are only minor restrictions on \u03a0, or equivalently on \u03a9\u0303. Specifically,\nAgarwal and Wolf's calculus requires that [27]\n1/\u03a9\u0303 is an entire analytic function with no zeros on the real component axes.\n\n(16)\n\nAgarwal et al.'s derivation additionally requires that\n\u03a9\u0303(0, v) = 1 for all v,\n\n(17)\n\nso that L\u03a9 satisfies the laws of radiometry and is consistent with (6) [17].\nWe call the functions L\u03a9 , the restricted class of extended light fields that we have systematically generated, quasi light fields, in recognition of their connection with quasi-probability\ndistributions in quantum physics.\n\n3.3\n\nCharacterization from Signal Processing\n\nAlthough we have identified the quasi light fields and justified how they extend the traditional\nlight field, we must still show that we have found all possible ways to extend the light field\nto coherent radiation, and we must indicate how to select a quasi light field for a specific\napplication. We address both concerns by relating quasi light fields to bilinear forms of\nU and U \u2217 that are parameterized by position and direction. First, such bilinear forms\nreflect all the different ways to represent the energy distribution of a complex signal in signal\nprocessing, and therefore contain all possible extended light fields, allowing us to identify any\nunaccounted for by quasi light fields. Second, we may use the signal processing classification\nof bilinear forms to characterize quasi light fields and guide the selection of one for an\napplication.\nTo relate quasi light fields to bilinear forms, we must express the filtered Wigner distribution in (15) as a bilinear form. Towards this end, we first express the filter kernel \u03a0 in\nterms of another function K:\n\u0012\n\u0013\nZ\n\u03bb\n\u03bb\n\u03a0(a, b) = K \u2212a + v, \u2212a \u2212 v exp(\u2212i2\u03c0b * v) d2 v.\n(18)\n2\n2\nWe substitute (18) into (15), integrate first over s\u2032\u22a5 , then over v, and finally substitute\n1\nrC = r\u2032 \u2212 r\u2032\u2032\n2\n\n1\nrR = r\u2032 + r\u2032\u2032 ,\n2\n8\n\n(19)\n\n\fto express the quasi light field as\n\u0012 \u00132 Z Z\n\u0001n\n\u0001\nk\nC\nL(r, s) =\nsz\nU rR K rR\n\u22a5 \u2212 r\u22a5 , r\u22a5 \u2212 r\u22a5\n2\u03c0\n\u0002\n\u0001\u0003 o \u2217 C \u0001 2 R 2 C\nR\nC\n\u00d7 exp \u2212iks\u22a5 * r\u22a5 \u2212 r\u22a5\nU r d r d r .\n\n(20)\n\nWe recognize that (20) is a bilinear form of U and U \u2217 , with kernel indicated by the braces.\nThe structure of the kernel of the bilinear form in (20) limits L to a shift-invariant\nenergy distribution. Specifically, translating the scalar field in (20) in position and direction\northogonal to the z-axis according to\n\u0001\n\u0001\nU(r) \u2192 U r \u2212 r0 exp iks0\u22a5 * r\u22a5\n(21)\n\nresults in a corresponding translation in position and direction in the light field, after rearranging terms:\n\u0001\nL(r, s) \u2192 L r \u2212 r0 , s \u2212 s0 .\n(22)\n\nSuch shift-invariant bilinear forms comprise the quadratic class of time-frequency distributions, which is sometimes misleadingly referred to as Cohen's class [20].\nThe quasi light fields represent all possible ways of extending the light field to coherent\nradiation. This is because any reasonably defined extended light field must be shift-invariant\nin position and direction, as translating and rotating coordinates should modify the scalar\nfield and light field representations in corresponding ways. Thus, on the one hand, an\nextended light field must be a quadratic time-frequency distribution. On the other hand, (20)\nimplies that quasi light fields span the entire class of quadratic time-frequency distributions,\napart from the constraints on \u03a0 described at the end of Section 3.2. The constraint in\n(17) is necessary to satisfy the power constraint in (6), which any extended light field must\nsatisfy. The remaining constraints in (16) are technical details concerning analyticity and\nthe location of zeros; extended light fields strictly need not satisfy these mild constraints,\nbut the light fields that are ruled out are well-approximated by light fields that satisfy them.\nWe obtain a concrete sensor array processing interpretation of quasi light fields by grouping the exponentials in (20) with U instead of K:\n\u0012 \u00132 Z Z n\n\u0001\n\u0002\n\u0001\u0003 o\n\u0001\nk\nC\nL(r, s) =\nK rR\nsz\nU rR exp iks * r \u2212 rR\n\u22a5 \u2212 r\u22a5 , r\u22a5 \u2212 r\u22a5\n2\u03c0\nn\n\u0001\n\u0002\n\u0001\u0003 o\u2217 2 R 2 C\n\u00d7 U rC exp iks * r \u2212 rC\n(23)\nd r d r .\nThe integral in (23) is the expected value of the energy of the output of a spatial filter\nwith impulse response exp(iks * r) applied to the scalar field, when using K to estimate the\ncorrelation E[U(rR )U \u2217 (rC )] by\n\u0001 \u2217 C\u0001\n\u0001\nC\nU rR K rR\nr .\n(24)\n\u22a5 \u2212 r\u22a5 , r\u22a5 \u2212 r\u22a5 U\nThat is, the choice of quasi light field corresponds to a choice of how to infer coherence\nstructure from scalar field measurements. In adaptive beamforming, the spatial filter exp(iks*\n9\n\n\fr) focuses a sensor array on a particular plane wave component, and K serves a similar role\nas the covariance matrix taper that gives rise to design features such as diagonal loading [29].\nBut for our purposes, the sensor array processing interpretation in (23) allows us to cleanly\nseparate the choice of quasi light field in K from the plane wave focusing in the exponentials.\nSeveral signal processing books meticulously classify the quadratic class of time-frequency\ndistributions by their properties, and discuss distribution design and use for various applications [25, 20]. We can use these resources to design quasi light fields for specific applications.\nFor example, if we desire a light field with fine directional localization, we may first try the\nWigner quasi light field in (8), which is a popular starting choice. We may then discover\nthat we have too many artifacts from interfering spatial frequencies, called cross terms, and\ntherefore wish to consider a reduced interference quasi light field. We might try the modified\nB-distribution, which is a particular reduced interference quasi light field that has a tunable\nparameter to suppress interference. Or, we may decide to design our own quasi light field in\na transformed domain using ambiguity functions. The resulting tradeoffs can be tailoring to\nspecific application requirements.\n\n4\n\nCapturing Quasi Light Fields\n\nTo capture an arbitrary quasi light field, we sample and process the scalar field. In incoherent imaging, traditional light fields are typically captured by instead making intensity\nmeasurements at a discrete set of positions and directions, as in done in the plenoptic camera [2]. While it is possible to apply the same technique to coherent imaging, only a small\nsubset of quasi light fields that exhibit poor localization properties can be captured this way.\nIn comparison, all quasi light fields can be computed from the scalar field, as in (15). We\ntherefore sample the scalar field with a discrete set of sensors placed at different positions in\nspace, and subsequently process the scalar field measurements to compute the desired quasi\nlight field. We describe the capture process for three specific quasi light fields in Section 4.1,\nand demonstrate the different localization properties of these quasi light fields via simulation\nin Section 4.2.\n\n4.1\n\nSampling the Scalar Field\n\nTo make the capture process concrete, we capture three different quasi light fields. For\nsimplicity, we consider a two-dimensional scene and sample the scalar field with a linear\narray of sensors regularly spaced along the y-axis (Figure 2). With this geometry, the\nscalar field U is parameterized by a single position variable y, and the discrete light field\nl is parameterized by y and the direction component sy . The sensor spacing is d/2, which\nwe assume is fine enough to ignore aliasing effects. This assumption is practical for longwavelength applications such as millimeter-wave radar. For other applications, aliasing can\nbe avoided by applying an appropriate pre-filter. From the sensor measurements, we compute\nthree different quasi light fields, including the spectrogram and the Wigner.\nAlthough the spectrogram quasi light field is attractive because it can be captured like a\n10\n\n\fradiation\noptional aperture stop T\n\n(y, sy )\n\nsensors\n\ns\nsy\n\nd/2\n\ny\n\nFigure 2: We capture a discrete quasi light field l by sampling the scalar field at regularly-spaced\nsensors and processing the resulting measurements. We may optionally apply an aperture stop T\nto mimic traditional light field capture, but this restricts us to capturing quasi light fields with\npoor localization properties.\ntraditional light field by making intensity measurements, it exhibits poor localization properties. Zhang and Levoy explain [30] how to capture the spectrogram by placing an aperture\nstop specified by a transmission function T over the desired position y before computing\na Fourier transform to extract the plane wave component in the desired direction sy , and\npreviously Ziegler et al. used the spectrogram as a coherent light field to represent a hologram [4]. The spectrogram is an important quasi light field because it is the building block\nfor the quasi light fields that can be directly captured by making intensity measurements,\nsince all non-negative quadratic time-frequency distributions, and therefore all non-negative\nquasi light fields, are sums of spectrograms [20]. Ignoring constants and sz , we compute the\ndiscrete spectrogram from the scalar field samples by\n2\nS\n\nl (y, sy ) =\n\nX\n\nT (nd)U(y + nd) exp (\u2212ikndsy ) .\n\n(25)\n\nn\n\nThe Wigner quasi light field is a popular choice that exhibits good energy localization\nin position and direction [20]. We already identified the Wigner quasi light field in (8); the\ndiscrete version is\nX\nlW (y, sy ) =\nU(y + nd/2)U \u2217 (y \u2212 nd/2) exp (\u2212ikndsy ) .\n(26)\nn\n\nEvidently, the spectrogram and Wigner distribute energy over position and direction in very\ndifferent ways. Per (25), the spectrogram first uses a Fourier transform to extract directional\ninformation and then computes a quadratic energy quantity, while the Wigner does the\nreverse, per (26). On the one hand, this reversal allows the Wigner to better localize energy\nin position and direction, since the Wigner is not bound by the classical Fourier uncertainty\nprinciple as the spectrogram is. On the other hand, the Wigner's nonlinearities introduce\ncross-term artifacts by coupling energy in different directions, thereby replacing the simple\nuncertainty principle with a more complicated set of tradeoffs [20].\n11\n\n\fWe now introduce a third quasi light field to capture, in order to help us understand the\nimplications of requiring quasi light fields to exhibit traditional light field properties. Specifically, traditional light fields have real non-negative values that are zero where the scalar\nfield is zero, whereas no quasi light field behaves this way [24]. Although the spectrogram\nhas non-negative values, the support of both the spectrogram and Wigner spills over into\nregions where the scalar field is zero. In contrast, the conjugate Rihaczek quasi light field,\nwhich can be obtained by substituting (3) for a\u2217 (s) in (6) and factoring, is identically zero\nat all positions where the scalar field is zero and for all directions in which the plane wave\ncomponent is zero:\nLR (r, s) = sz U \u2217 (r) exp(iks * r)a(s).\n(27)\nHowever, unlike the non-negative spectrogram and the real Wigner, the Rihaczek is complexvalued, as each of its discoverers independently observed: Walther in optics [15], Kirkwood\nin quantum physics [31], and Rihaczek in signal processing [32]. The discrete conjugate\nRihaczek quasi light field is\nX\nlR (y, sy ) = U \u2217 (y) exp (ikysy )\nU(nd) exp (\u2212ikndsy ) .\n(28)\nn\n\n4.2\n\nLocalization Tradeoffs\n\nDifferent quasi light fields localize energy in position and direction in different ways, so\nthat the choice of quasi light field impacts the potential resolution achieved in an imaging\napplication. We illustrate the diversity of behavior by simulating a plane wave propagating\npast a screen edge and computing the spectrogram, Wigner, and Rihaczek quasi light fields\nfrom scalar field samples (Figure 3). This simple scenario stresses the main tension between\nlocalization in position and direction: each quasi light field must encode the position of the\nscreen edge as well as the downward direction of the plane wave. The quasi light fields serve\nas intermediate representations used to jointly estimate the position of the screen edge and\nthe orientation of the plane wave.\nOur simulation accurately models diffraction using our implementation of the angular\nspectrum propagation method, which is the same technique used in commercial optics software to accurately simulate wave propagation [33]. We propagate a plane wave with wavelength \u03bb = 3 mm a distance R = 50 m past the screen edge, where we measure the scalar\nfield and compute the three discrete light fields using (25), (26), and (28). To compute\nthe light fields, we set d = \u03bb/10, run the summations over |n| \u2264 10/\u03bb, and use a rectangular window function of width 10 cm for T . We plot lS , |lW |, and |lR | in terms of\nthe two-plane parameterization of the light field [1], so that each ray is directed from a\npoint u in the plane of the screen towards a point y in the measurement plane, and so that\n1/2\nsy = (y \u2212 u)/ [R2 + (y \u2212 u)2 ] .\nWe compare each light field's ability to estimate the position of the screen edge and the\norientation of the plane wave (Figure 3). Geometric optics provides an ideal estimate: we\nshould ideally only see rays pointing straight down (u = y) past the screen edge, corresponding to a diagonal line in the upper-right quadrant of the light field plots. Instead, we see\n12\n\n\fu\n\nopaque screen\nR=\n\n50 m\n\n3m\n\n0m\n\ny\nideal\n\nplane wave\n\u03bb = 3 mm\n\nmeasurement plane\n\nspectrogram\n\nWigner\n\ny (m)\n\ny (m)\n\nRihaczek\n\n3\nu (m)\n0\n0\n\ny (m)\n\n3 0\n\n3 0\n\n3 0\n\ny (m)\n\n3\n\nFigure 3: The spectrogram does not resolve a plane wave propagating past the edge of an opaque\nscreen as well as other quasi light fields, such as the Wigner and Rihaczek. We capture all three quasi\nlight fields by sampling the scalar field with sensors and processing the measurements according to\n(25), (26), and (28). The ringing and blurring in the light field plots indicate the diffraction fringes\nand energy localization limitations.\n\nblurred lines with ringing. The ringing is physically accurate and indicates the diffraction\nfringes formed on the measurement plane. The blurring indicates localization limitations.\nWhile the spectrogram's window T can be chosen to narrowly localize energy in either position or direction, the Wigner narrowly localizes energy in both, depicting instantaneous\nfrequency without being limited by the classical Fourier uncertainty principle [20].\nIt may seem that the Wigner light field is preferable to the others and the clear choice for\nall applications. While the Wigner light field possesses excellent localization properties, it\nexhibits cross-term artifacts due to interference from different plane wave components. An\nalternative quasi light field such as the Rihaczek can strike a balance between localization\nand cross-term artifacts, and therefore may be a more appropriate choice, as discussed at\nthe end of Section 3.3. If our goal were to only estimate the position of the screen edge, we\nmight prefer the spectrogram; to jointly estimate both position and plane wave orientation,\nwe prefer the Wigner; and if there were two plane waves instead of one, we might prefer\nthe Rihaczek. One thing is certain, however: we must abandon non-negative quasi light\nfields to achieve better localization tradeoffs, as all non-negative quadratic time-frequency\ndistributions are sums of spectrograms and hence exhibit poor localization tradeoffs [20].\n\n13\n\n\f5\n\nImage Formation\n\nWe wish to form images from quasi light fields for coherent applications similarly to how we\nform images from traditional light fields for incoherent applications, by using (1) to integrate\nbundles of light field rays to compute pixel values (Figure 1). However, simply selecting a\nparticular captured quasi light field L and evaluating (1) raises three questions about the\nvalidity of the resulting image. First, is it meaningful to distribute coherent energy over\nsurface area by factoring radiant intensity in (6)? Second, does the far-zone assumption\nimplicit in radiometry and formalized in (2) limit the applicability of quasi field fields? And\nthird, how do we capture quasi light field rays remotely if, unlike the traditional light field,\nquasi light fields need not be constant along rays?\nThe first question is a semantic one. For incoherent light of a small wavelength, we\ndefine an image in terms of the power radiating from a scene surface towards an aperture,\nand physics tells us that this uniquely specifies the image (Section 3), which may be expressed\nin terms of the traditional light field. If we attempt to generalize the same definition of an\nimage to partially coherent, broadband light, and specifically to coherent light at a nonzero wavelength, we must ask how to isolate the power from a surface patch towards the\naperture, according to classical wave optics. But there is no unique answer; different isolation\ntechniques correspond to different quasi light fields. Therefore, to be well-defined, we must\nextend the definition of an image for coherent light to include a particular choice of quasi\nlight field, which corresponds to a particular factorization of radiant intensity.\nThe second and third questions speak of assumptions in the formulation of quasi light\nfields and in the image formation from quasi light fields, which can lead to coherent imaging\ninaccuracies when these assumptions are not valid. Specifically, unless the scene surface and\naperture are far apart, the far-zone assumption in (2) does not hold, so that quasi light fields\nare incapable of modeling near-zone behavior. Also, unless we choose a quasi light field that\nis constant along rays, such as an angle-impact Wigner function [34], remote measurements\nmight not accurately reflect the light field at the scene surface [35], resulting in imaging\ninaccuracies. Therefore, in general, integrating bundles of remotely captured quasi light field\nrays produces an approximation of the image we have defined. We assess this approximation\nby building an accurate near-zone model in Section 5.1, simulating imaging performance of\nseveral coherent cameras in Section 5.2, and showing how our image formation procedure\ngeneralizes the classic beamforming algorithm in Section 5.3.\n\n5.1\n\nNear-Zone Radiometry\n\nWe take a new approach to formulating light fields for coherent radiation that avoids making\nthe assumptions that 1) the measurement plane is far from the scene surface and 2) light\nfields are constant along rays. The resulting light fields are accurate in the near zone, and\nmay be compared with quasi light fields to understand quasi light field limitations. The key\nidea is to express a near-zone light field L(r, s) on the measurement plane in terms of the\ninfinitesimal flux at the point where the line containing the ray (r, s) intersects the scene\nsurface (Figure 4). First we compute the scalar field at the scene surface, next we compute\n14\n\n\fthe infinitesimal flux, and then we identify a light field that predicts the same flux using the\nlaws of radiometry. In contrast with Walther's approach (Section 3.1), 1) we do not make\nthe far-zone approximation as in (2), and 2) we formulate the light field in the measurement\nplane instead of in the source plane at the scene surface. Therefore, in forming an image\nfrom a near-zone light field, we are not limited to the far zone and we need not relate the\nlight field at the measurement plane to the light field at the scene surface.\ny\nr=0\n\ns\nz\n\nd\u03a6\nscene\nsurface\npatch\n\nd\u03a9\n\nrP = \u2212\u03c1s\n\nrM\nvirtual\naperture\n\nremote\nmeasurement\nplane\n\nFigure 4: To ensure that integrating bundles of remote light field rays in the near zone results in\nan accurate image, we derive a light field LR\n\u03c1 (r, s) in the measurement plane from the infinitesimal\nP\nflux d\u03a6 at the point r where the ray originates from the scene surface patch. We thereby avoid\nmaking the assumptions that the measurement plane is far from the scene and that the light field\nis constant along rays.\n\nThe first step in deriving a near-zone light field L for the ray (r, s) is to use the scalar\nfield on the measurement plane to compute the scalar field at the point rP where the line\ncontaining the ray intersects the scene surface. We choose coordinates so that the measurement plane is the xy-plane, the scene lies many wavelengths away in the negative z < 0\nhalf-space, and r is at the origin. We denote the distance between the source rP on the scene\nsurface and the point of observation r by \u03c1. Under a reasonable bandwidth assumption, the\ninverse diffraction formula expresses the scalar field at rP in terms of the scalar field on the\nmeasurement plane [36]:\nZ\n\u2212z P exp(\u2212ik|rP \u2212 rM |) 2 M\nik\nP\nU(rM ) P\nd r .\n(29)\nU(r ) =\n2\u03c0\n|r \u2212 rM |\n|rP \u2212 rM |\nNext, we compute the differential flux d\u03a6 through a portion of a sphere at rP subtending\ndifferential solid angle d\u03a9. We obtain d\u03a6 by integrating the radial component of the energy\nflux density vector\n\u0014 \u2217\n\u0015\n1\n\u2202U\n\u2202U\nP\n\u2217\nF(r ) = \u2212\n(30)\n\u2207U +\n\u2207U .\n4\u03c0k\u03bd \u2202t\n\u2202t\n\nTo keep the calculation simple, we ignore amplitude decay across the measurement plane,\napproximating\n|rP \u2212 rM | \u2248 |rP |\n(31)\n15\n\n\foutside the exponential in (29), and\n\u2202\n|rP \u2212 rM | \u2248 1,\n\u2202|rP |\n\n(32)\n\nwhen evaluating (30), resulting in\nF(\u2212\u03c1s) =\nwhere\n\u00e3(\u2212\u03c1s) =\nThus,\n\n\u0012\n\nk\n2\u03c0\n\n\u0012\n\n\u00132 Z\n\nd\u03a6 =\n\n\u0012\n\n2\u03c0\nk\n\n2\u03c0\nk\n\n\u00132\n\n\u00e3(\u2212\u03c1s)\u00e3\u2217 (\u2212\u03c1s)\n\ns2z\ns,\n\u03c12\n\n(33)\n\nU(rM ) exp(\u2212ik| \u2212 \u03c1s \u2212 rM |) d2 r M .\n\n(34)\n\n\u00132\n\n(35)\n\ns2z \u00e3(\u2212\u03c1s)\u00e3\u2217 (\u2212\u03c1s) d\u03a9.\n\nFinally, we factor out sz and an outer integral over surface area from d\u03a6/d\u03a9 to determine\na near-zone light field. Unlike in Section 3.1, the nonlinear exponential argument in \u00e3\ncomplicates the factoring. Nonetheless, we obtain a near-zone light field that generalizes the\nRihaczek by substituting (34) for \u00e3\u2217 in (35). After factoring and freeing r from the origin\nby substituting r \u2212 \u03c1s for \u2212\u03c1s, we obtain\n\u2217\nLR\n\u03c1 (r, s) = sz U (r) exp(ik\u03c1)\u00e3(r \u2212 \u03c1s)\n\u0012 \u00132\nZ\n\u0001\n\u0001\nk\n\u2217\nsz U (r) exp(ik\u03c1) U rM exp \u2212ik|r \u2212 \u03c1s \u2212 rM | d2 r M ,\n=\n2\u03c0\n\n(36)\n\nwhere the subscript \u03c1 reminds us of this near-zone light field's dependence on distance.\nLR\n\u03c1 is evidently neither the traditional light field nor a quasi light field, as it depends\ndirectly on the scene geometry through an additional distance parameter. This distance\nparameter \u03c1 is a function of r, s, and the geometry of the scene; it is the distance along s\nbetween the scene surface and r. We may integrate LR\n\u03c1 over a bundle of rays to compute the\nimage pixel values just like any other light field, as long as we supply the right value of \u03c1\nfor each ray. In contrast, quasi light fields are incapable of modeling optical propagation in\nthe near zone, as it is insufficient to specify power flow along rays. We must also know the\ndistance between the source and point of measurement along each ray.\nWe can obtain near-zone generalizations of all quasi light fields through the sensor array\nprocessing interpretation in Section 3.3. Recall that each quasi light field corresponds to a\nparticular choice of the function K in (23). For example, setting K(a, b) = \u03b4(b), where \u03b4\nis the Dirac delta function, yields the Rihaczek quasi light field LR in (27). To generalize\nquasi light fields to the near zone, we focus at a point instead of a plane wave component by\nusing a spatial filter with impulse response exp (\u2212ik |r \u2212 \u03c1s|) instead of exp(iks * r) in (23).\nThen, choosing K(a, b) = \u03b4(b) yields LR\n\u03c1 , the near-zone generalization of the Rihaczek in\n(36), and choosing other functions K yield near-zone generalizations of the other quasi light\nfields.\n16\n\n\f5.2\n\nNear-Zone Diffraction Limitations\n\nWe compute and compare image pixel values using the Rihaczek quasi light field LR and\nits near-zone generalization LR\n\u03c1 , demonstrating how all quasi light fields implicitly make the\nFraunhofer diffraction approximation that limits accurate imaging to the far zone. First, we\nconstruct coherent cameras from LR and LR\n\u03c1 . For simplicity, we consider a two-dimensional\nscene and sample the light fields, approximating the integral over a bundle of rays (Figure\n1) by the summation of discrete rays directed from the center rP of the scene surface patch\nto each sensor on a virtual aperture of diameter A, equally spaced every distance d in the\nmeasurement plane (Figure 5a). Ignoring constants and sz , we compute the pixel values for\na far-zone camera from the Rihaczek quasi light field in (27),\n\"\n#\nX \u0002\n\u0001\u0003 X\n\u0001\nR\nn \u2217\nn\nP =\nU(nd) exp \u2212ikndsy\nU(md) exp \u2212ikmdsy ,\n(37)\nm\n\n|nd|<A/2\n\nand for a near-zone camera from the near-zone generalization of the Rihaczek in (36),\n\uf8ee\n\uf8f9\u2217 \"\n#\nX\nX\nP\u03c1R = \uf8f0\nU(nd) exp (\u2212ik\u2206n )\uf8fb\nU(md) exp (\u2212ik\u2206m ) .\n(38)\nm\n\n|nd|<A/2\n\nIn (37), sn denotes the unit direction from rP to the nth sensor, and in (38), \u2206n denotes the\ndistance between rP and the nth sensor.\nBy comparing the exponentials in (37) with those in (38), we see that the near-zone\ncamera aligns the sensor measurements along spherical wavefronts diverging from the point\nof focus rP , while the far-zone camera aligns measurements along plane wavefront approximations (Figure 5b). Spherical wavefront alignment makes physical sense in accordance with\nthe Huygens-Fresnel principle of diffraction, while approximating spherical wavefronts with\nplane wavefronts is reminiscent of Fraunhofer diffraction. In fact, the far-zone approximation in (2) used to derive quasi light fields follows directly from the Rayleigh-Sommerfeld\ndiffraction integral by linearizing the exponentials, which is precisely Fraunhofer diffraction.\nTherefore, all quasi light fields are only valid for small Fresnel numbers, when the source\nand point of measurement are sufficiently far away from each other.\nWe expect the near-zone camera to outperform the far-zone camera in near-zone imaging\napplications, which we demonstrate by comparing their ability to resolve small targets moving past their field of view. As a baseline, we introduce a third camera with non-negative\npixel values P\u03c1B by restricting the summation over m in (38) to |md| < A/2, which results\nin the beamformer camera used in sensor array processing [5, 37]. Alternatively, we could\nextend the summation over n in (38) to the entire array, but this would average anisotropic\nresponses over a wider aperture diameter, resulting in a different image. We simulate an\nopaque screen containing a pinhole that is backlit with a coherent plane wave (Figure 6).\nThe sensor array is D = 2 m wide and just R = 1 m away from the screen. The virtual\naperture is A = 10 cm wide and the camera is focused on a fixed 1 mm pixel straight ahead\non the screen. The pinhole has width 1 mm, which is smaller than the wavelength \u03bb = 3\n17\n\n\f(a)\n\nrP\n\nscene\nsurface\npatch\n\nvirtual aperture\nof width A\n\n\u2206n\nmeasurement\nplane\n\ny\n\nnd\nsn\n\n(b)\n\nz\n\nrP\nFraunhofer\n\nmdsm\ny\ny\n\u2206 m \u2212 \u22060\n\nspherical\nplane\n\nz\n\nmd\n\nHuygens-Fresnel\n\nFigure 5: The near-zone light field results in a camera that aligns spherical wavefronts diverging\nfrom the point of focus rP , in accordance with the Huygens-Fresnel principle of diffraction, while\nquasi light fields result in cameras that align plane wavefront approximations, in accordance with\nFraunhofer diffraction. Quasi light fields are therefore only accurate in the far zone. We derive both\ncameras by approximating the integral over a bundle of rays by the summation of discrete light field\nrays (a), and we interpret the operation of each camera by how they align sensor measurements\nalong wavefronts from rP (b).\n\nmm, so the plane wavefronts bend into slightly spherical shapes via diffraction. We move\nthe pinhole to the right, recording pixel values |P R |, |P\u03c1R |, and P\u03c1B for each camera at each\npinhole position. Due to the nature of the coherent combination of the sensor measurements\nthat produces the pixel values, each camera records a multi-lobed response. The width of\nthe main lobe indicates the near-zone resolution of the camera.\nThe near-zone camera is able to resolve the pinhole down to its actual size of 1 mm, greatly\noutperforming the far-zone camera which records a blur 66 cm wide, and even outperforming\nthe beamformer camera. Neither comparison is surprising. First, with a Fresnel number of\nD 2 /R\u03bb \u2248 1333, the Fraunhofer approximation implicitly made by quasi light fields does not\nhold for this scenario, so we expect the far-zone camera to exhibit poor resolution. Second,\nthe near-zone camera uses the entire D = 2 m array instead of just the sensors on the virtual\naperture that the beamformer camera is restricted to, and the extra sensors lead to improved\nresolution.\n\n18\n\n\fplane wave\n\u03bb = 3 mm\nmoving 1 mm pinhole\nin opaque screen\n\nfixed scene\nsurface patch\n\nR=1m\n\npinhole offset\nA = 10 cm\n\nsensor array\n\nD=2m\n\nPR\n\nP\u03c1B\n\n1 mm\n2 cm\n\nP\u03c1R\n\n13 dB\n\n66 cm\n\npinhole offset\n\nFigure 6: Images of nearby objects formed from pure quasi light fields are blurry. In the scene, a\nsmall backlit pinhole moves across the field of view of a sensor array that implements three cameras,\neach computing one pixel value for each pinhole position, corresponding to a fixed surface patch.\nAs the pinhole crosses the fixed scene surface patch, the near-zone camera resolves the pinhole\ndown to its actual size of 1 mm, while the far-zone camera records a blur 66 cm wide.\n\n5.3\n\nGeneralized Beamforming\n\nWe compare image formation from light fields with traditional perspectives on coherent\nimage formation, by relating quasi light fields and our coherent cameras with the classic\nbeamforming algorithm used in many coherent imaging applications, including ultrasound\n[5] and radar [37]. The beamforming algorithm estimates a spherical wave diverging from\na point of focus rP by delaying and averaging sensor measurements. When the radiation is\nnarrowband, the delays are approximated by phase shifts. With the sensor array geometry\nfrom Section 5.2, the beamformer output is\nX\nT (md)U(md) exp(\u2212ik\u2206m ),\n(39)\ng=\nm\n\nwhere the T (md) are amplitude weights used to adjust the beamformer's performance. As\nrP moves into the far zone,\n0\n\u2206m \u2212 \u22060 \u2192 mdsm\n(40)\ny \u2192 mdsy ,\n\n19\n\n\fso that apart from a constant phase offset, (39) becomes a short-time Fourier transform\nX\ng\u221e =\nT (md)U(md) exp(\u2212ikmds0y ).\n(41)\nm\n\nEvidently, |g \u221e|2 is a spectrogram quasi light field, and we may select T to be a narrow\nwindow about a point r to capture LS (r, s0 ). We have already seen how quasi light fields\ngeneralize the spectrogram.\nBeamformer applications instead typically select T to be a wide window to match the\ndesired virtual aperture, and assign the corresponding pixel value to the output power |g|2.\nWe can decompose the three cameras in Section 5.2 into such beamformers. First, we write\nP\u03c1R in (38) in terms of two different beamformers,\nP\u03c1R = g1\u2217 g2 ,\nwhere\ng1 =\n\nX\n\nU(nd) exp (\u2212ik\u2206n )\n\n(42)\n(43)\n\n|nd|<A/2\n\nand\ng2 =\n\nX\n\nU(md) exp (\u2212ik\u2206m ) ,\n\n(44)\n\nm\n\nso that the windows for g1 and g2 are rectangular with widths matching the aperture A and\nsensor array D, respectively. Next, by construction\nP\u03c1B = |g1 |2 .\n\n(45)\n\nFinally, in the far zone, sn \u2192 s0 in (37) so that\nP R \u2192 (g1\u221e )\u2217 g2\u221e ,\n\n(46)\n\nwhere g1\u221e and g2\u221e are given by (41) with the windows T used in (43) and (44). In other\nwords, the near-zone camera is the Hermitian product of two different beamformers, and is\nequivalent to the far-zone camera in the far zone.\nWe interpret the role of each component beamformer from the derivation of (38). Beamformer g1\u2217 aggregates power contributions across the aperture using measurements of the\nconjugate field U \u2217 on the aperture, while beamformer g2 isolates power from the point of\nfocus using all available measurements of the field U. In this manner, the tasks of aggregating and isolating power contributions are cleanly divided between the two beamformers,\nand each beamformer uses the measurements from those sensors appropriate to its task. In\ncontrast, the beamformer camera uses the same set of sensors for both the power aggregation\nand isolation tasks, thereby limiting its ability to optimize over both tasks.\nThe near-zone camera achieves a new tradeoff between resolution and anisotropic sensitivity. We noted that the near-zone camera exhibits better resolution than the beamformer,\nfor the same virtual aperture (Figure 6). This is not an entirely fair comparison because the\n20\n\n\fnear-zone camera is using sensor measurements outside the aperture, and indeed, a beamformer using the entire array would achieve comparable resolution. However, extending the\naperture to the entire array results in a different image, as anisotropic responses are averaged\nover a wider aperture diameter. We interpret the near-zone camera's behavior by computing\nthe magnitude\np\nP\u03c1R = |g1 |2 |g2|2 .\n(47)\n\nEvidently, the pixel magnitude of the near-zone camera is the geometric mean of the two\ntraditional beamformer output powers. |P\u03c1R | has better resolution than |g1 |2 and better\nanisotropic sensitivity than |g2 |2 .\nImage formation with alternative light fields uses the conjugate field and field measurements to aggregate and isolate power in different ways. In general, image pixel values do not\nneatly factor into the product of beamformers, as they do with the Rihaczek.\n\n6\n\nConcluding Remarks\n\nWe enable the use of existing incoherent imaging tools for coherent imaging applications, by\nextending the light field to coherent radiation. We explain how to formulate, capture, and\nform images from quasi light fields. By synthesizing existing research in optics, quantum\nphysics, and signal processing, we motivate quasi light fields, show how quasi light fields extend the traditional light field, and characterize the properties of different quasi light fields.\nWe explain why capturing quasi light fields directly with intensity measurements is inherently limiting, and demonstrate via simulation how processing scalar field measurements in\ndifferent ways leads to a rich set of energy localization tradeoffs. We show how coherent\nimage formation using quasi light fields is complicated by an implicit far-zone (Fraunhofer)\nassumption and the fact that not all quasi light fields are constant along rays. We demonstrate via simulation that a pure light field representation is incapable of modeling near-zone\ndiffraction effects, but that quasi light fields can be augmented with a distance parameter\nfor greater near-zone imaging accuracy. We show how image formation using light fields\ngeneralizes the classic beamforming algorithm, allowing for new tradeoffs between resolution\nand anisotropic sensitivity.\nAlthough we have assumed perfectly coherent radiation, tools from partial coherence\ntheory 1) allow us to generalize our results, and 2) provide an alternative perspective on\nimage formation. First, our results extend to broadband radiation of any state of partial\ncoherence by replacing U(rR )U \u2217 (rC ) with the cross-spectral density W (rR , rC , \u03bd). W provides\na statistical description of the radiation, indicating how light at two different positions, rR\nand rC , is correlated at each frequency \u03bd [38]. Second, W itself may be propagated along rays\nin an approximate asymptotic sense [39, 40], which forms the basis of an entirely different\nframework for using rays for image formation, using the cross-spectral density instead of the\nlight field as the core representation.\nWe present a model of coherent image formation that strikes a balance between utility\nand comprehensive predictive power. On the one hand, quasi light fields offer more options\nand tradeoffs than their traditional, incoherent counterpart. In this manner, the connection\n21\n\n\fbetween quasi light fields and quasi-probability distributions in quantum physics reminds us\nof the potential benefits of forgoing a single familiar tool in favor of a multitude of useful yet\nless familiar ones. On the other hand, compared with Maxwell's equations, quasi light fields\nare less versatile. Therefore, quasi light fields are attractive to researchers who desire more\nversatility than traditional energy-based methods, yet a more specialized model of image\nformation than Maxwell's equations.\nQuasi light fields illustrate the limitations of the simple definition of image formation\nubiquitous in incoherent imaging. An image is the visualization of some underlying physical\nreality, and the energy emitted from a portion of a scene surface towards a virtual aperture\nis not a physically precise quantity when the radiation is coherent, according to classical\nelectromagnetic wave theory. Perhaps a different image definition may prove more fundamental for coherent imaging, or perhaps a quantum optics viewpoint is required for precision.\nAlthough we have borrowed the mathematics from quantum physics, our entire discussion\nhas been classical. Yet if we introduce quantum optics and the particle nature of light, we\nmay unambiguously speak of the probability that a photon emitted from a portion of a scene\nsurface is intercepted by a virtual aperture.\n\nAcknowledgements\nThis work was supported, in part, by Microsoft Research, MIT Lincoln Laboratory, and\nSemiconductor Research Corporation through the FCRP Center for Circuit & System Solutions (C2S2).\n\nReferences\n[1] M. Levoy and P. Hanrahan, \"Light field rendering,\" in Proceedings of ACM SIGGRAPH\n96, (ACM, 1996), pp. 31\u201342.\n[2] R. Ng, M. Levoy, M. Br\u00e9dif, G. Duval, M. Horowitz, and P. Hanrahan, \"Light field\nphotography with a hand-held plenoptic camera,\" Tech. Rep. CTSR 2005-02, Stanford\nUniversity, CA (2005).\n[3] W. Chun and O. S. Cossairt, \"Data processing for three-dimensional displays,\" United\nStates Patent 7,525,541 (2009).\n[4] R. Ziegler, S. Bucheli, L. Ahrenberg, M. Magnor, and M. Gross, \"A bidirectional light\nfield \u2013 hologram transform,\" Computer Graphics Forum 26, 435\u2013446 (2007).\n[5] T. L. Szabo, Diagnostic Ultrasound Imaging: Inside Out (Elsevier Academic Press,\n2004).\n[6] M. J. Bastiaans, \"Application of the Wigner distribution function in optics,\" in The\nWigner Distribution - Theory and Applications in Signal Processing, W. Mecklenbr\u00e4uker\nand F. Hlawatsch, eds. (Elsevier Science B.V., 1997), pp. 375\u2013426.\n22\n\n\f[7] A. Walther, \"Radiometry and coherence,\" J. Opt. Soc. Am. 58, 1256\u20131259 (1968).\n[8] E. Wigner, \"On the quantum correction for thermodynamic equilibrium,\" Phys. Rev.\n40, 749\u2013759 (1932).\n[9] J. Ville, \"Th\u00e9orie et applications de la notion de signal analytique,\" Cables et Transmission 2, 61\u201374 (1948).\n[10] K. D. Stephan, \"Radiometry before World War II: Measuring infrared and millimeterwave radiation 1800-1925,\" IEEE Antennas and Propagation Magazine 47, 28\u201337\n(2005).\n[11] S. Chandrasekhar, Radiative Transfer (Dover Publications, 1960).\n[12] A. T. Friberg, G. S. Agarwal, J. T. Foley, and E. Wolf, \"Statistical wave-theoretical\nderivation of the free-space transport equation of radiometry,\" J. Opt. Soc. Am. B 9,\n1386\u20131393 (1992).\n[13] A. Gershun, \"The light field,\" J. Math. Phys. 18, 51\u2013151 (1939).\n[14] M. Born and E. Wolf, Principles of Optics, 7th ed. (Cambridge University Press, 1999).\n[15] A. Walther, \"Radiometry and coherence,\" J. Opt. Soc. Am. 63, 1622\u20131623 (1973).\n[16] E. Wolf, \"Coherence and radiometry,\" J. Opt. Soc. Am. 68, 6\u201317 (1978).\n[17] G. S. Agarwal, J. T. Foley, and E. Wolf, \"The radiance and phase-space representations\nof the cross-spectral density operator,\" Opt. Commun. 62, 67\u201372 (1987).\n[18] E. H. Adelson and J. R. Bergen, \"The plenoptic function and the elements of early\nvision,\" in Computational Models of Visual Processing, M. S. Landy and J. A. Movshon,\neds. (MIT Press, 1991), pp. 3\u201320.\n[19] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen, \"The lumigraph,\" in Proceedings of ACM SIGGRAPH 96, (ACM, 1996), pp. 43\u201354.\n[20] B. Boashash, ed., Time Frequency Signal Analysis and Processing (Elsevier, 2003).\n[21] R. W. Boyd, Radiometry and the Detection of Optical Radiation (John Wiley & Sons,\n1983).\n[22] A. Adams and M. Levoy, \"General linear cameras with finite aperture,\" in Proc. Eurographics Symposium on Rendering, (Eurographics, 2007).\n[23] J. W. Goodman, Introduction to Fourier Optics (McGraw-Hill, 1968).\n[24] A. T. Friberg, \"On the existence of a radiance function for finite planar sources of\narbitrary states of coherence,\" J. Opt. Soc. Am. 69, 192\u2013198 (1979).\n23\n\n\f[25] P. Flandrin, Time-Frequency / Time-Scale Analysis (Academic Press, 1999).\n[26] D. J. Griffiths, Introduction to Quantum Mechanics (Pearson Education, 2005).\n[27] G. S. Agarwal and E. Wolf, \"Calculus for functions of noncommuting operators and\ngeneral phase-space methods in quantum mechanics. I. Mapping theorems and ordering\nof functions of noncommuting operators,\" Phys. Rev. D 2, 2161\u20132186 (1970).\n[28] J. T. Foley and E. Wolf, \"Radiometry as a short-wavelength limit of statistical wave\ntheory with globally incoherent sources,\" Opt. Commun. 55, 236\u2013241 (1985).\n[29] J. R. Guerci, \"Theory and application of covariance matrix tapers for robust adaptive\nbeamforming,\" IEEE Trans. Signal Process. 47, 977\u2013985 (1999).\n[30] Z. Zhang and M. Levoy, \"Wigner distributions and how they relate to the light field,\"\nin Proceedings of ICCP 09, (IEEE, 2009).\n[31] J. G. Kirkwood, \"Quantum statistics of almost classical assemblies,\" Phys. Rev. 44,\n31\u201337 (1933).\n[32] A. Rihaczek, \"Signal energy distribution in time and frequency,\" IEEE Trans. Information Theory 14, 369\u2013374 (1968).\n[33] ZEMAX Development Corporation, Bellevue, WA, Optical Design Program User's\nGuide (2006).\n[34] M. A. Alonso, \"Radiometry and wide-angle wave fields. I. Coherent fields in two dimensions,\" J. Opt. Soc. Am. A 18, 902\u2013909 (2001).\n[35] R. G. Littlejohn and R. Winston, \"Corrections to classical radiometry,\" J. Opt. Soc.\nAm. A 10, 2024\u20132037 (1993).\n[36] J. R. Shewell and E. Wolf, \"Inverse diffraction and a new reciprocity theorem,\" J. Opt.\nSoc. Am. 58, 1596\u20131603 (1968).\n[37] H. L. Van Trees, Optimum Array Processing (John Wiley & Sons, 2002).\n[38] L. Mandel and E. Wolf, Optical Coherence and Quantum Optics (Cambridge University\nPress, 1995).\n[39] A. M. Zysk, P. S. Carney, and J. C. Schotland, \"Eikonal method for calculation of\ncoherence functions,\" Phys. Rev. Lett. 95, 043904 (2005).\n[40] R. W. Schoonover, A. M. Zysk, P. S. Carney, J. C. Schotland, and E. Wolf, \"Geometrical\noptics limit of stochastic electromagnetic fields,\" Phys. Rev. A 77, 043831 (2008).\n\n24\n\n\f"}