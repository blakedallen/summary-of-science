{"id": "http://arxiv.org/abs/math/0502357v1", "guidislink": true, "updated": "2005-02-16T17:25:20Z", "updated_parsed": [2005, 2, 16, 17, 25, 20, 2, 47, 0], "published": "2005-02-16T17:25:20Z", "published_parsed": [2005, 2, 16, 17, 25, 20, 2, 47, 0], "title": "A Sublinear Algorithm of Sparse Fourier Transform for Nonequispaced Data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0502034%2Cmath%2F0502434%2Cmath%2F0502540%2Cmath%2F0502386%2Cmath%2F0502401%2Cmath%2F0502530%2Cmath%2F0502563%2Cmath%2F0502408%2Cmath%2F0502004%2Cmath%2F0502256%2Cmath%2F0502424%2Cmath%2F0502467%2Cmath%2F0502349%2Cmath%2F0502482%2Cmath%2F0502463%2Cmath%2F0502285%2Cmath%2F0502016%2Cmath%2F0502243%2Cmath%2F0502064%2Cmath%2F0502315%2Cmath%2F0502587%2Cmath%2F0502241%2Cmath%2F0502279%2Cmath%2F0502090%2Cmath%2F0502091%2Cmath%2F0502533%2Cmath%2F0502160%2Cmath%2F0502007%2Cmath%2F0502252%2Cmath%2F0502192%2Cmath%2F0502459%2Cmath%2F0502480%2Cmath%2F0502527%2Cmath%2F0502536%2Cmath%2F0502115%2Cmath%2F0502549%2Cmath%2F0502222%2Cmath%2F0502105%2Cmath%2F0502031%2Cmath%2F0502021%2Cmath%2F0502312%2Cmath%2F0502572%2Cmath%2F0502257%2Cmath%2F0502485%2Cmath%2F0502422%2Cmath%2F0502255%2Cmath%2F0502372%2Cmath%2F0502575%2Cmath%2F0502051%2Cmath%2F0502373%2Cmath%2F0502276%2Cmath%2F0502086%2Cmath%2F0502190%2Cmath%2F0502266%2Cmath%2F0502364%2Cmath%2F0502244%2Cmath%2F0502033%2Cmath%2F0502365%2Cmath%2F0502265%2Cmath%2F0502019%2Cmath%2F0502355%2Cmath%2F0502089%2Cmath%2F0502553%2Cmath%2F0502461%2Cmath%2F0502363%2Cmath%2F0502356%2Cmath%2F0502003%2Cmath%2F0502454%2Cmath%2F0502189%2Cmath%2F0502448%2Cmath%2F0502526%2Cmath%2F0502163%2Cmath%2F0502002%2Cmath%2F0502095%2Cmath%2F0502420%2Cmath%2F0502085%2Cmath%2F0502172%2Cmath%2F0502159%2Cmath%2F0502158%2Cmath%2F0502197%2Cmath%2F0502304%2Cmath%2F0502125%2Cmath%2F0502038%2Cmath%2F0502303%2Cmath%2F0502393%2Cmath%2F0502290%2Cmath%2F0502568%2Cmath%2F0502427%2Cmath%2F0502234%2Cmath%2F0502337%2Cmath%2F0502340%2Cmath%2F0502511%2Cmath%2F0502344%2Cmath%2F0502282%2Cmath%2F0502357%2Cmath%2F0502497%2Cmath%2F0502417%2Cmath%2F0502128%2Cmath%2F0502250%2Cmath%2F0502366%2Cmath%2F0502123&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Sublinear Algorithm of Sparse Fourier Transform for Nonequispaced Data"}, "summary": "We present a sublinear randomized algorithm to compute a sparse Fourier\ntransform for nonequispaced data. Suppose a signal S is known to consist of N\nequispaced samples, of which only L<N are available. If the ratio p=L/N is not\nclose to 1, the available data are typically non-equispaced samples. Then our\nalgorithm reconstructs a near-optimal B-term representation R with high\nprobability 1-delta, in time and space poly(B,log(L),log p, log(1/delta),\nepsilon^{-1}, such that ||S-R||^2 < (1+epsilon) ||S-R_{opt}^B||^2, where\nR_{opt}^B is the optimal B-term Fourier representation of signal S. The\nsublinear poly(logL) time is compared to the superlinear O(Nlog N+L) time\nrequirement of the present best known Inverse Nonequispaced Fast Fourier\nTransform (INFFT) algorithms. Numerical experiments support the advantage in\nspeed of our algorithm over other methods for sparse signals: it already\noutperforms INFFT for large but realistic size N and works well even in the\nsituation of a large percentage of missing data and in the presence of noise.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0502034%2Cmath%2F0502434%2Cmath%2F0502540%2Cmath%2F0502386%2Cmath%2F0502401%2Cmath%2F0502530%2Cmath%2F0502563%2Cmath%2F0502408%2Cmath%2F0502004%2Cmath%2F0502256%2Cmath%2F0502424%2Cmath%2F0502467%2Cmath%2F0502349%2Cmath%2F0502482%2Cmath%2F0502463%2Cmath%2F0502285%2Cmath%2F0502016%2Cmath%2F0502243%2Cmath%2F0502064%2Cmath%2F0502315%2Cmath%2F0502587%2Cmath%2F0502241%2Cmath%2F0502279%2Cmath%2F0502090%2Cmath%2F0502091%2Cmath%2F0502533%2Cmath%2F0502160%2Cmath%2F0502007%2Cmath%2F0502252%2Cmath%2F0502192%2Cmath%2F0502459%2Cmath%2F0502480%2Cmath%2F0502527%2Cmath%2F0502536%2Cmath%2F0502115%2Cmath%2F0502549%2Cmath%2F0502222%2Cmath%2F0502105%2Cmath%2F0502031%2Cmath%2F0502021%2Cmath%2F0502312%2Cmath%2F0502572%2Cmath%2F0502257%2Cmath%2F0502485%2Cmath%2F0502422%2Cmath%2F0502255%2Cmath%2F0502372%2Cmath%2F0502575%2Cmath%2F0502051%2Cmath%2F0502373%2Cmath%2F0502276%2Cmath%2F0502086%2Cmath%2F0502190%2Cmath%2F0502266%2Cmath%2F0502364%2Cmath%2F0502244%2Cmath%2F0502033%2Cmath%2F0502365%2Cmath%2F0502265%2Cmath%2F0502019%2Cmath%2F0502355%2Cmath%2F0502089%2Cmath%2F0502553%2Cmath%2F0502461%2Cmath%2F0502363%2Cmath%2F0502356%2Cmath%2F0502003%2Cmath%2F0502454%2Cmath%2F0502189%2Cmath%2F0502448%2Cmath%2F0502526%2Cmath%2F0502163%2Cmath%2F0502002%2Cmath%2F0502095%2Cmath%2F0502420%2Cmath%2F0502085%2Cmath%2F0502172%2Cmath%2F0502159%2Cmath%2F0502158%2Cmath%2F0502197%2Cmath%2F0502304%2Cmath%2F0502125%2Cmath%2F0502038%2Cmath%2F0502303%2Cmath%2F0502393%2Cmath%2F0502290%2Cmath%2F0502568%2Cmath%2F0502427%2Cmath%2F0502234%2Cmath%2F0502337%2Cmath%2F0502340%2Cmath%2F0502511%2Cmath%2F0502344%2Cmath%2F0502282%2Cmath%2F0502357%2Cmath%2F0502497%2Cmath%2F0502417%2Cmath%2F0502128%2Cmath%2F0502250%2Cmath%2F0502366%2Cmath%2F0502123&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present a sublinear randomized algorithm to compute a sparse Fourier\ntransform for nonequispaced data. Suppose a signal S is known to consist of N\nequispaced samples, of which only L<N are available. If the ratio p=L/N is not\nclose to 1, the available data are typically non-equispaced samples. Then our\nalgorithm reconstructs a near-optimal B-term representation R with high\nprobability 1-delta, in time and space poly(B,log(L),log p, log(1/delta),\nepsilon^{-1}, such that ||S-R||^2 < (1+epsilon) ||S-R_{opt}^B||^2, where\nR_{opt}^B is the optimal B-term Fourier representation of signal S. The\nsublinear poly(logL) time is compared to the superlinear O(Nlog N+L) time\nrequirement of the present best known Inverse Nonequispaced Fast Fourier\nTransform (INFFT) algorithms. Numerical experiments support the advantage in\nspeed of our algorithm over other methods for sparse signals: it already\noutperforms INFFT for large but realistic size N and works well even in the\nsituation of a large percentage of missing data and in the presence of noise."}, "authors": ["Jing Zou"], "author_detail": {"name": "Jing Zou"}, "author": "Jing Zou", "arxiv_comment": "25 pages, 4 figures; submitted to Applied and Computational Harmonic\n  Analysis", "links": [{"href": "http://arxiv.org/abs/math/0502357v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0502357v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "65T50, 68W20, 42A10", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0502357v1", "affiliation": "Program in Applied and Computational Mathematics, Princeton University", "arxiv_url": "http://arxiv.org/abs/math/0502357v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:math/0502357v1 [math.NA] 16 Feb 2005\n\nA Sublinear Algorithm of Sparse Fourier Transform for\nNonequispaced Data\u2217\nJing Zou \u2020\n\nAbstract\nWe present a sublinear randomized algorithm to compute a sparse Fourier transform for\nnonequispaced data. Suppose a signal S is known to consist of N equispaced samples, of\nwhich only L < N are available. If the ratio p = L/N is not close to 1, the available data\nare typically non-equispaced samples. Then our algorithm reconstructs a near-optimal B-term\nrepresentation R with high probability 1\u2212\u03b4, in time and space poly(B, log(L), log p, log(1/\u03b4),\nB k2 , where RB is the optimal B-term Fourier\n\u01eb\u22121 ), such that kS \u2212 Rk2 \u2264 (1 + \u01eb)kS \u2212 Ropt\nopt\nrepresentation of signal S. The sublinear poly(log L) time is compared to the superlinear\nO(N log N + L) time requirement of the present best known Inverse Nonequispaced Fast\nFourier Transform (INFFT) algorithms. Numerical experiments support the advantage in speed\nof our algorithm over other methods for sparse signals: it already outperforms INFFT for large\nbut realistic size N and works well even in the situation of a large percentage of missing data\nand in the presence of noise.\n\n1 Introduction\nWe consider the problem in which the recovery of a discrete time signal S of length N is sought\nwhen only L signal values are known. In general, this is of course an insoluble problem; we\nconsider it here under the additional assumption that the signal has a sparse Fourier transform. Let\nus fix the notations: the signal is denoted by S = (S(t))t=0,...,N \u22121 , but we have at our disposal only\nthe (S(i))i\u2208T , where the set T is a subset of {0, . . . , N \u2212 1} and |TP\n| = L. The Fourier transform\nN \u22121\n1\nof signal S is \u015c = (\u015c(0), . . . , \u015c(N \u2212 1)), defined by \u015c(\u03c9) = \u221aN t=0\nS(t)e\u22122\u03c0i\u03c9t/N . In terms\nPN \u22121\n\u015c(\u03c9)\u03c6\u03c9 (t);\nof the Fourier basis functions \u03c6\u03c9 (t) = \u221a1N e2\u03c0i\u03c9t/N , S can be written as S = \u03c9=0\nthis is the (discrete) Fourier representation of S. A signal S is said to have a B-sparse Fourier\nrepresentation, if there exists aPsubset \u03a9 \u2282 {0, . . . , N \u2212 1} with |\u03a9| = B, and values c(\u03c9) 6= 0\nfor \u03c9 \u2208 \u0393, such that S(t) = \u03c9\u2208\u03a9 c(\u03c9)\u03c6\u03c9 . For a signal that does not have a B-sparse Fourier\nB\nrepresentation, we denote by Ropt\n(S) the optimal B-term Sparse Fourier representation of S.\n\u2217\n\nThis work was partially supported by NSF grant DMS-03168875 and AFOSR grant 109-6047.\nProgram of Applied and Computational Mathematics, Princeton University, Fine Hall, Washington Road, Princeton, NJ 08544, (jzou@math.princeton.edu)\n\u2020\n\n1\n\n\fThis paper presents a sublinear algorithm to recover a B-sparse Fourier representation of a\nsignal S from incomplete data. Our algorithm also extends to the case where the Fourier transform\n\u015c\nPis not B-sparse, where we aim to find a near-optimal B-term Fourier representation, i.e. R =\n\u03c9\u2208\u0393 c(\u03c9)\u03c6\u03c9 , such that\nX\nB\nkS \u2212 Rk = kS \u2212\nc(\u03c9)\u03c6\u03c9 k22 \u2264 (1 + \u01eb)kS \u2212 Ropt\n(S)k22 .\n(1)\n\u03c9\u2208\u0393\n\nA typical situation where our study applies is the observation of non-equispaced data, where the\nsamples are nevertheless all elements of \u03c4 Z for some \u03c4 > 0. For a signal with evenly spaced data,\nthe famous Fast Fourier Transform (FFT) computes all the Fourier coefficients in time O(N log N).\nHowever, the requirement of equally distributed data by FFT raises challenges for many important\napplications. For instance, because of the occurrence of instrumental drop-outs, the data may\nbe available only on a set of non-consecutive integers. Another example occurs in astronomy,\nwhere the observers cannot completely control the availability of observational data: a telescope\ncan only see the universe on nights when skies are not cloudy. In fact, computing the Fourier\nrepresentation from irregularly spaced data has wide applications [19] in processing astrophysical\nand seismic data, the spectral method on adaptive grids, the tracking of Lagrangian particles, and\nthe implementation of semi-Lagrangian methods.\nIn many of these applications, a few large Fourier coefficients already capture the major timeinvariant wave-like information of the signal, and we can thus ignore very small Fourier coefficients. To find a small set of the largest Fourier coefficients and hence a (near) optimal B-sparse\nFourier representation of a signal that describes most of the signal characteristics is a fundamental\ntask in applied Fourier Analysis.\nAn equivalent version of this problem is as follows: define the matrix A := (e2\u03c0iktj )k=0,...,N ;\nj=0...,L\u22121 , where the tj are the locations of the available samples. Given S(tj ), we want to reconstruct the signal S, or equivalently, its Fourier coefficients \u015ck , so that A\u015c = S. This linear system\nis over-determined. Several algorithms [2][11] [12] have provided efficient approaches to solve\nthis problem. Among all INFFT algorithms, the iterative CGNE approach of [6] in the benchmark\nsoftware NFFT 2.0 is one of the fastest methods; it takes time O(L1+(d\u22121)/\u03b2 log L), where L is the\nnumber of available points, d is the number of dimensions, and \u03b2 > 1 is the smoothness for the\noriginal signal. The super-linearity relationship between the running time and N (recall L = pN,\nwhere p is the percentage of available data) poses difficulties in processing large dimensional signals, which have nothing to do with the unequal spacing. It follows that identifying a sparse number\nof significant modes and amplitudes is expensive for even fairly modest N. Our goal in this paper\nis to discuss much faster (sublinear) algorithms that can identify the sparse representation or approximation with coefficients a1 , . . . , aB and modes \u03c91 , . . . , \u03c9B for unevenly spaced data. These\nalgorithms will not use all the samples S(0), . . . , S(N \u2212 1), but only a very sparse subset of them.\nOur approach is based on the paper [8] that shows how to construct the Fourier representation\nfor a signal S with B-sparse Fourier representation in time and space poly(B, log N, 1/\u01eb, log(1/\u03b4))\non equal spacing data. The algorithm contains some random elements (which do not depend on\nthe signal); their approach guarantees that the error of estimation is of order \u01ebkSk2 with probability exceeding 1 \u2212 \u03b4. The ideas in [8] have also been applied by its authors to sparse wavelet,\n2\n\n\fwavelet packet representation, and histograms [7]. We have dubbed the whole family of algorithms\nRAlSTA (for Randomized Algorithm for Sparse Transform Approximation); when dealing only\nwith Fourier Transforms, as is the case here, we specialize it to RAlSFA (F for Fourier). Zou,\nGilbert, Strauss and Daubechies [20] improved and implemented the algorithm greatly. It convincingly beats FFT when the number of grid points N is reasonably large. The crossover point lies at\nN \u2243 25, 000 in one dimension, and at N \u2243 460 for data on a N \u00d7 N grid in two dimensions for a\ntwo-mode signal. When B = 13, RAlSFA surpasses F F T at N \u2265 300, 000 for one dimensional\nsignals and 1100 for two dimensional signals.\nIn this paper, we modify RAlSFA to solve the irregularly spaced data problem. The new\nNERAlSFA (Nonequispaced RAlSFA) uses sublinear time and space poly(B, log L, \u01eb, log(1/\u03b4),\nlog p) to find a near-optimal B-term Fourier representation, such that kS\u2212Rk2 \u2264 (1+\u01eb)kS\u2212Ropt k2\nwith high probability 1 \u2212 \u03b4. Similar to the RAlSFA algorithm, it outperforms existing INFFT\nalgorithms in processing sparse signals of large size.\nNotation and Terminology Denote by \u03c7T a signal that equals 1 on a set T and zero elsewhere\nin the time domain. We say a signal H is q percent pure, if there exists a frequency \u03c9 and a signal\n\u03c1, such that H = ae2\u03c0i\u03c9t/N + \u03c1, with |a|2 \u2265 (q%)kHk2 . To quantify the unevenness of the data,\nintroduce a parameter p = L/N to be the percentage of the available data over all the data, where\nL is the number of available data. Obviously a larger p corresponds to more information about the\nsignal. We use L2 -norm throughout the paper, which is denoted by k.k. The convolution F \u2217 G is\n\u221a\nP\n\\\ndefined as F \u2217 G(t) = s F (s)G(t \u2212 s). It follows that F\n\u2217 G(\u03c9) = N F\u0302 (\u03c9)\u011c(\u03c9).\nA Box-car filter with width 2k + 1 is defined as follows:\n\u001a \u221aN\nif \u2212 k \u2264 t \u2264 k ,\n2k+1\n\u03c7k (t) =\n0\nif t > k or t < \u2212k\nIn the frequency domain, this filter is in the form of\n(\n\u03c7\u0302k (\u03c9) =\n\nsin((2k+1)\u03c0\u03c9/N )\n(2k+1)sin(\u03c0\u03c9/N )\n\n1\n\nif \u03c9 6= 0\nif \u03c9 = 0\n\n(2)\n\nA dilation operation on signal H with a dilation factor \u03c3 is defined as H (\u03c3) (t) = H(\u03c3t) for\nevery points t.\nOrganization The paper is organized as follows. In Section 2, we give the outline of the\nRAlSFA algorithm. Section 3 presents the modification of RAlSFA that deals with the unavailability of some samples by a greedy method. In Section 4, an interpolation technique is introduced\nfor better performance. Finally, we compare numerical results with existing algorithms in Section\n5.\n\n2 Set-up of RAlSFA\nB\nGiven a signal S of length N, the optimal B-term Fourier representation Ropt\n(S) uses only B\nfrequencies; it is simply a truncated version of the Fourier representation of S, retaining only the\nB largest coefficients. The following theorem is the main result of [8].\n\n3\n\n\fTheorem 2.1. Let an accuracy factor \u01eb, a failure probability \u03b4, and a sparsity target B \u2208 N, B \u226a\nN be given. Then for an arbitrary signal S of length N, RAlSFA will find a B-term approximation\nR to S, at a cost in time and space of order poly(B, log(N), 1/\u01eb, log(1/\u03b4)) and with probability\nB\nexceeding 1 \u2212 \u03b4, so that kS \u2212 Rk2 \u2264 (1 + \u01eb)kS \u2212 Ropt\n(S)k22.\nThe striking fact is that RAlSFA can build a near-optimal representation R in sublinear time\npoly(log N) instead of the O(N log N) time requirement of other algorithms. Its speed surpasses\nFFT as long as the length of a signal is sufficiently large. If a signal is composed of only B modes,\nRAlSFA constructs S without any error.\nThe main procedure is a Greedy Pursuit with the following steps:\nAlgorithm 2.2. TOTAL S CHEME [20]\n1. Initialize the representation signal R to 0. Set the maximum number of iterations IT ER =\nB log(N) log(1/\u03b4)/\u01eb2 .\n2. Test whether kS \u2212 Rk appears to be less than some user threshold, \u03b9. If yes, return the\nrepresentation signal R and the whole algorithm ends; else go to step 3..\n3. Locate Fourier Modes \u03c9 for the signal S \u2212 R by isolation and group test procedures.\n4. Estimate Fourier Coefficients at \u03c9: (S\\\n\u2212 R)(\u03c9).\n5. Update the representation signal R \u2190 R + (S\\\n\u2212 R)(\u03c9)\u03c6\u03c9 (t).\n6. If the total number of iterations is less than IT ER, go to 2; else return the representation\nR.\nThe basic idea of Algorithm 2.2 is to identify significant frequencies and then estimate their\ncorresponding coefficients. In order to locate those nonzero frequencies, we first construct a new\nsignal where a previous significant frequency becomes predominant. Then a recursive approach\ncalled group test finds the exact label of this predominant mode, by splitting intervals, comparing\nenergies, and keeping only intervals with large energies. After the frequency is located, coefficient\nestimation procedures give a good estimation by taking means and medians of random samples.\n\n3 NERAlSFA with Greedy Technique\nRAlSFA samples from a signal, implicitly assuming that uniform and random sampling is possible, with a fixed cost per sample. This raises challenges for processing unevenly spaced data.\nSpecifically speaking, Fourier coefficients and norms can not be estimated properly. Thus one has\nto modify steps 3 and 4 accordingly. In this section, NERAlSFA, a modified version of RAlSFA\nwith greedy technique, is introduced to overcome these problems.\nThe basic idea is a greedy pursuit for an available data point. Whenever the algorithm samples\nat a missing data point, it searches some other random indices t until it finds one available data\n4\n\n\fpoint S(t) as the substitute. This technique is used in estimating both Fourier coefficients and\nnorms.\nA good data structure is important to save running time cost. We denote the availability of\na data point by a label, say +1 for available and 0 for unavailable. Hence, the label is tested to\nsee if its corresponding sample is valid. An alternative solution is to store all the sorted labels\nof available data in a long list. However, each search takes time O(log(N)), which introduces a\nO(log N)2 factor into the whole computation. As the empirical results show, the running time of\nNERAlSFA algorithm is linear to log N. For this reason, we selected the first method.\nWe now give a more detailed discussion of the different procedures used in steps 3 and 4 of\nAlgorithm 2.2.\n\n3.1 Estimating Fourier Coefficients\nFirst, we give the procedure for estimating Fourier coefficients for unevenly spaced data as follows.\nAlgorithm 3.1. E STIMATING I NDIVIDUAL F OURIER C OEFFICIENTS\nInput a signal S, a frequency \u03c9, n = 2 log(1/\u03b4), m = 8/\u01eb2 .\n1. For i = 1, . . . , n\n2. For j = 1, . . . , m\nRandomly generate the index t until S(t) is available.\nThen let tij = t. Evaluate k(tij ) =< S(tij ), \u03c6\u03c9 (tij ) >.\nP\n3. Take the means of m samples k(tij ), i.e. p(i) = m\nj=1 k(tij ), where i = 1, . . . , n.\n\n4. Take the median of n samples c = mediani (p(i)), where i = 1, . . . , n.\n5. Return c as the estimation of the Fourier coefficient \u015c(\u03c9).\n\nNext, we show that using unevenly spaced data leads to a very good approximation to the\ntrue coefficient. The first lemma is one of most fundamental theorems in randomized algorithms.\nIt essentially states that by repeating an experiment enough times, a small probability event will\nhappen eventually.\nLemma 3.2. If an event happens with probability p, then in the first k > log \u03b4/ log(1\u2212p) iterations,\nit happens at least once with success probability 1 \u2212 \u03b4.\nIn our case, only p = L/N percentage of the data is available, so that k > log \u03b4/ log(1 \u2212 L/N)\ntrials are needed to generate one available data point with success probability at least 1 \u2212 \u03b4.\nIn fact, most of the Fourier coefficients of a characteristic function on a typical set T are small,\nunder some conditions. The following lemma makes this more explicit.\n\n5\n\n\fN \u22121\nLemma 3.3. Suppose the components Xj of a discrete random variable X = (Xj )j=0\nare identically and independently distributed in {0, 1}, with p = P rob(Xj = 1). Define the random set\nT = {j \u2208 {0, . . . , N \u2212 1}|Xj = 1} to be the set of all available data; \u03c7\u0302T (\u03c9) is the Fourier\nPN \u22121\n1\ntransform of \u03c7T (t) = j=0\nXj . If p \u2265 1+(N \u22121)\u03bb\u03c4\n2 , then\n\nP rob(|\u03c7\u0302T (\u03c9)|2 \u2265 \u03bb) \u2264 \u03c4 2 .\n\n(3)\n\n(1\u2212p)\n.\nProof. First, we claim that E(|\u03c7\u0302T (\u03c9)|2) \u2264 p(N\n\u22121)\nP\n1\n2\u03c0i\u03c9j/N\nSince \u03c7\u0302T (\u03c9) = pN\n), we have\nj\u2208T (e\n\n1 X 2\u03c0i\u03c9(j\u2212k)/N\ne\np2 N 2\nj,k\u2208T\nX\nX\n1\n1\ne2\u03c0i\u03c9(j\u2212k)/N .\n= 2 2\n1+ 2 2\np N j\u2208T\np N j,k\u2208T,j6=k\n|\u03c7\u0302T (\u03c9)|2 =\n\n(4)\n\nIt follows that\nN \u22121\n1\n1\npN \u2212 1 X\ne2\u03c0i\u03c9(j\u2212k)/N .\nE(|\u03c7\u0302T (\u03c9)| ) =\n+ 2 2p\npN\np N N \u2212 1 j,k=0,j6=k\n2\n\nObserve that\n\nPN \u22121\n\nj,k=0,j6=k\n\ne2\u03c0i\u03c9(j\u2212k)/N = |\n\nPN \u22121\nj=0\n\ne2\u03c0i\u03c9j/N |2 \u2212\n\nPN \u22121\nj=0\n\n1 = (N\u03b4\u03c9,0 )2 \u2212 N, hence\n\n\u001a\n\u001b\n1\npN \u2212 1\n1 pN \u2212 1 2\n1\nE(|\u03c7\u0302T (\u03c9)| ) =\n1+\n+\n(N \u03b4\u03c9,0 \u2212 N) =\n(N\u03b4\u03c9,0 \u2212 1)\npN\npN 2 N \u2212 1\npN\nN \u22121\n1\n{N \u2212 1 + (pN \u2212 1)(N\u03b4\u03c9,0 \u2212 1)} .\n=\npN(N \u2212 1)\n2\n\nBy Markov's Inequality, when \u03c9 6= 0, we have\nP rob(|\u03c7\u0302T (\u03c9)|2 \u2265 \u03bb) \u2264\nSince p \u2265\n\n1\n,\n1+(N \u22121)\u03bb\u03c4 2\n\nE(|\u03c7\u0302T (\u03c9)|2 )\n1\u2212p\n=\n.\n\u03bb\np(N \u2212 1)\u03bb\n\nit follows that\nP rob(|\u03c7\u0302T (\u03c9)|2 \u2265 \u03bb) \u2264 \u03c4 2 .\n\nThat is , for any \u03c9 6= 0, with probability at least 1 \u2212 \u03c4 2\n\u221a\n|\u03c7\u0302T (\u03c9)| \u2264 \u03bb.\n\n6\n\n(5)\n\n\fIn particular, we want both \u03bb and \u03c4 to be small, meaning that p cannot be too small itself.\n\\\nNext, we consider the conditions for the two coefficients \u015c(\u03c9) and \u015c1 (\u03c9) = S\n* \u03c7T (\u03c9) to be\nclose.\nLemma 3.4. Suppose the parameters T , S, \u03c7T (t),\nq \u03bb, \u03c4 , p are as stated in Lemma 3.3, and define\n\nS1 (t) = S(t)\u03c7T (t). If p \u2265\n\n1\n,\n1+(N \u22121)\u03bb\u03c4 2\n\n1\n\nand \u03c4 \u2264\n\n1 \u2212 (1 \u2212 \u03b4) B , then, for any \u03c9,\n\n|\u015c(\u03c9) \u2212 \u015c1 (\u03c9)| \u2264\n\n\u221a\n\nB\u03bbkSk2 .\n\n(6)\n\nwith probability exceeding 1 \u2212 \u03b4.\nProof. Suppose the significant terms of signal S are \u03c9i , where i = 1, . . . , B.\nSince S1 (t) = S(t)\u03c7T (t) and thus \u015c1 (\u03c9) = \u015c(\u03c9) \u2217 \u03c7\u0302T (\u03c9), then\n\u015c1 (\u03c9j ) =\n\nB\nX\ni=1\n\n\u015c(\u03c9i )\u03c7\u0302T (\u03c9j \u2212 \u03c9i ) = \u015c(\u03c9j )\u03c7\u0302T (0) +\n\nB\nX\n\ni=1,\u03c9j 6=\u03c9i\n\n= \u015c(\u03c9j ) +\n\nB\nX\n\ni=1,\u03c9j 6=\u03c9i\n\nTherefore\n|\u015c1 (\u03c9j ) \u2212 \u015c(\u03c9j )| = |\nv\nu B\nu X\n\u2264t\n\ni=1,\u03c9j 6=\u03c9i\n\nv\nu B\nu X\n|\u015c(\u03c9i )|2 t\n\ni=1,\u03c9j 6=\u03c9i\n\n1\n,\n1+(N \u22121)\u03bb\u03c4 2\n\nB\nX\n\ni=1,\u03c9j 6=\u03c9i\n\n\u015c(\u03c9i )\u03c7\u0302T (\u03c9j \u2212 \u03c9i )\n\n\u015c(\u03c9i )\u03c7\u0302T (\u03c9j \u2212 \u03c9i ).\n\n\u015c(\u03c9i )\u03c7\u0302T (\u03c9j \u2212 \u03c9i )|\n\nv\nu B\nu X\n|\u03c7\u0302T (\u03c9j \u2212 \u03c9i )|2 \u2264 kSk2 t\n\ni=1,\u03c9j 6=\u03c9i\n\n(7)\n\n|\u03c7\u0302T (\u03c9j \u2212 \u03c9i )|2 .\n\nwe have |\u03c7\u0302T (\u03c9)|2 \u2264 \u03bb with probability at least 1 \u2212 \u03c4 2 for any \u03c9 6= 0.\n\u221a\nThis implies that |\u015c1 (\u03c9j ) \u2212 \u015c(\u03c9j )| \u2264 kSk2 B\u03bb with probability at least (1 \u2212 \u03c4 2 )B \u2265 (1 \u2212 \u03b4)\nThen\n\u221a\n|\u015c1 (\u03c9j ) \u2212 \u015c(\u03c9j )| \u2264 B\u03bbkSk2 .\n(8)\n\nBecause p \u2265\n\nFor those \u03c9 \u2208\n/ {\u03c9i , i = 1, . . . , B},\n\u015c1 (\u03c9) =\n\nB\nX\ni=1\n\n\u015c(\u03c9)\u03c7\u0302T (\u03c9 \u2212 \u03c9i ),\n(9)\n\nand we conclude similarly that |\u015c1 (\u03c9) \u2212 \u015c(\u03c9)| \u2264\n\n\u221a\n\nB\u03bbkSk2., with probability at least 1 \u2212 \u03b4.\n\nWe shall use Algorithm 3.1 to estimate \u015c1 (\u03c9); we now look at how close the approximation A\n(i.e. the output of Algorithm 3.1) of \u015c1 (\u03c9) is to the true coefficient \u015c(\u03c9).\n7\n\n\f1\nLemma 3.5. For a set of parameters T , S, \u03c7T (t), \u03bb, \u03c4 , p as stated in Lemma 3.3, if p \u2265 1+(N \u22121)\u03bb\u03c4\n2,\np\n1/B\nand \u03c4 \u2264 1 \u2212 (1 \u2212 \u03b4) , then Algorithm 3.1 for signal S1 (t) = S(t)\u03c7T (t) gives a good estimation A of \u015c(\u03c9), such that\n\u221a\n\u221a\n|A \u2212 \u015c(\u03c9)| \u2264 ( \u03bb + B\u03bb)kSk2 .\n(10)\n\nwith high probability.\nProof. Lemma 4.2 in [20] says that the coefficient estimation algorithm returns A, such that\n\u221a\n(11)\n|A \u2212 \u015c1 (\u03c9)| \u2264 \u03bbkSk2 .\nBy Lemma 3.4\n|\u015c1 (\u03c9) \u2212 \u015c(\u03c9)| \u2264\nThus\n\n\u221a\n\nB\u03bbkSk2 .\n\n\u221a\n\u221a\n|A \u2212 \u015c(\u03c9)| \u2264 |A \u2212 \u015c1 (\u03c9)| + |\u015c1 (\u03c9) \u2212 \u015c(\u03c9)| \u2264 ( \u03bb + B\u03bb)kSk2 .\n\n(12)\n(13)\n\nFinally, we derive the conclusion about estimating coefficients.\n\u01eb\nTheorem 3.6. For a set of parameters T , S, \u03c7T (t), \u03bb, \u03c4 , p as stated in Lemma 3.3, if \u03bb \u2264 2(B+1)\n1\nand p \u2265 1+(N \u22121)\u03bb\u03c4\n2 , then every application of Algorithm 3.1 produces, for each frequency \u03c9 and\neach signal S, and each \u03bb > 0, with high probability, an output A (after inputting (S, \u03c9, \u01eb) ), such\nthat |A \u2212 \u015c(\u03c9)|2 \u2264 \u01ebkSk22 .\n\nProof. By Lemma 3.5,\n\n\u221a\n\u221a\n|A \u2212 \u015c(\u03c9)| \u2264 ( \u03bb + B\u03bb)kSk2 .\n\n(14)\n\n|A \u2212 \u015c(\u03c9)|2 \u2264 2(\u03bb + B\u03bb)kSk22 .\n\n(15)\n\nThus we have\nFrom the conditions 2(\u03bb + B\u03bb) \u2264 \u01eb, it follows that\n|A \u2212 \u015c(\u03c9)|2 \u2264 \u01ebkSk22 .\n\n(16)\n\nWhen we are able to get most of the data, the computational cost for estimating Fourier coefficients on unevenly spaced data is only slightly more than for the evenly spaced data case. The\ntime to compute the signal value remains almost the same as for the evenly spaced data case. The\nlog \u03b4\nextra time, in the worst case O( \u01eb2p log(1\u2212p)\n), comes from visiting unavailable data. Fortunately, the\n1\nvisit operation is very fast and therefore contributes little to the total time, especially when most of\nthe data are available.\nMoreover, as in [20], one can speed up the algorithm by using multi-step coarse-to-fine coefficient estimation procedures, which turns out to be more efficient than single-step accurate estimation; the proof is entirely analogous to Lemma 4.3 in [20].\n8\n\n\f3.2 Estimating Norms\nThe basic idea for locating the label of a significant frequency is to compare the energies (i.e. the\nL2 norm) of signals restricted in different frequency intervals. If the energy of some interval is\nrelatively large, the significant mode is in that region with higher probability. We construct the\nfollowing new signals to focus on certain intervals\nHj (t) = \u03c71 (t)e\n\n2\u03c0ijt\n16\n\n\u2217 \u03c7[\u2212q1,q1 ] (\u03c3t)e\n\n2\u03c0it\u03b8\nN\n\n\u2217S\n\n(17)\n\nwhere 2q1 +1 is the filter width, j = 0, . . . , 15, \u03c3 and \u03b8 are random dilation and modulation factors.\n(Please see [20] for an explanation of the role of \u03c3 and \u03b8). For convenience, we denote Hj (t) by\nH(t).\nWe need to evaluate values H(t) for random indices t \u2208 {0, . . . , N \u2212 1}. Note that the signal\nH results from the convolutions of two finite bandwidth Box-car filters with the original signal S.\nTherefore, any missing point needed by the two convolutions would lead to a failure of computing\nF (t). The total number of signal points involved depends on the number of nonzero taps in these\ntwo filters. Moreover, random dilation and modulation factors of the second Box-car filter make\ncomputation more tricky.\nOne naive way is to dive into the two convolutions and sample each signal point. If it is not\navailable, stop evaluating this F (t) and start with a new index t. This definitely increases time\ncost by wasting abundant computation. For example, suppose five data are needed and only one\nof them is missing, then the algorithm may compute four data in vain in the worst case, where the\nmissing data point is visited last in the sequence of 5.\nTo avoid the above situation, we first compute the locations of all the points that will be needed\nfor the convolution; only if they are all available will we start the computation. The locations\nrelated to the convolution are given in the following lemma.\n(\u03c3 )\n\n(\u03c3 )\n\nLemma 3.7. Suppose we have a signal H(t) = (\u03c71 1 \u2217 (\u03c7q1 2 \u2217 S)(\u03c33 ) )(\u03c34 ) )(t), where \u03c31 , \u03c32 , \u03c33 ,\nand \u03c34 are dilation factors. From the definition of Box car filter, the taps for \u03c71 lies in the interval\n[\u22121, 1], the taps for \u03c7q1 in [\u2212q1 , q1 ], then in order to evaluate H(t), we need values of S with\nindices at \u03c33 \u03c34 t \u2212 \u03c33 \u03c31 i \u2212 j\u03c32 , where integers i = \u22121, . . . , 1, j = \u2212q1 , . . . , q1 .\n(\u03c3 )\n\nProof. To evaluate H(t), first let signal r = (\u03c7q1 2 \u2217 S)(\u03c33 ) , then\nH(t) =\n\n(\u03c3 )\n(\u03c71 1\n\n\u2217 r)\n\n(\u03c34 )\n\n(t) =\n\n1\nX\n\ni=\u22121\n\n\u03c71 (\u03c31 i)r(\u03c34 t \u2212 \u03c31 i)\n\n(\u03c33 )\n2)\n2)\nr(\u03c34 t \u2212 \u03c31 i) = (\u03c7(\u03c3\n(\u03c34 t \u2212 \u03c31 i) = (\u03c7(\u03c3\nq1 \u2217 S)\nq1 \u2217 S)(\u03c33 \u03c34 t \u2212 \u03c33 \u03c31 i)\nq1\nX\n\u03c7q1 (\u03c32 j)S(\u03c33 \u03c34 t \u2212 \u03c33 \u03c31 i \u2212 \u03c32 j).\n=\n\n(18)\n\n(19)\n\nj=\u2212q1\n\n\u2032\n\n\u2032\n\nThus, in order to get the value of H(t), we need values of all S(t ), where t = \u03c33 \u03c34 t\u2212\u03c33 \u03c31 i\u2212\u03c32 j,\nwith i = \u22121, . . . , 1 and j = \u2212q1 , . . . , q1 .\n9\n\n\fThe scheme of the norm estimation algorithm is as follows.\nAlgorithm 3.8. N ORM E STIMATION\nInput: signal H, k = 0, the number of iterations M = 1.2 ln(1/\u03b4).\nWhile k < M:\n1. Randomly generate the index tk .\n\u2032\n\n\u2032\n\n2. Compute all indices needed by the two convolutions: \u03a5 = {t , t = \u03c33 \u03c34 t \u2212 \u03c33 \u03c31 i \u2212 \u03c32 j},\nwhere i = \u22121, . . . , 1 and j = \u2212q1 , . . . , q1 .\n\u2032\n\n3. If all the points t \u2208 \u03a5 are available, then compute H(tk ) else go to step 1 and generate\nanother index tk .\n4. estimate = 60-th percentile of the sequence {|H(tk )|2 N}, where k = 0, . . . , M \u2212 1.\nIf there exist satisfactory data groups, although maybe very few, the norm estimation will\neventually find them. However, when most data are unavailable, the program may struggle in a\nlong loop and take a huge amount of time. We introduce some tricks to avoid this. For example,\nset an upper bound MAX on the number of the loops. If it is reached, just use the sample points\ngenerated so far to estimate the norms. This technique may lead to a larger error, and thus hamper\nour frequency identification. However, by repeating the calculation, as stipulated by Lemma 3.2,\nwe reduce the inaccuracy. Anyway we cannot hope to recover the signal, if p is too small.\nThe following lemma investigates the number of repetitions to get a satisfactory data group for\nestimating norms.\nLemma 3.9. Suppose \u03c7q1 and \u03c7q2 are two Box-car filters with numbers of taps 2q1 + 1 and 2q2 + 1\nrespectively. Define Dq1 ,q2 = \u03c7q1 \u2217 \u03c7q2 . Then Dq1 ,q2 has 2q1 + 2q2 + 1 nonzero taps in the time\ndomain.\nLemma 3.10. Randomly choose an index for signal H(t), then after k > log \u03b4/ log(1 \u2212 (1 \u2212\np)2q1 +2q2 +1 ) iterations, we can get at least one satisfactory index with high probability 1 \u2212 \u03b4.\nProof. It is easy to prove by Lemma 3.2.\nHere is a new scheme for estimating norms, which uses much fewer samples than the original\none and still achieves good estimation. In [20], we propose a lemma that enabled us to achieve a\ngood norm estimation by only a few samples. The following lemma is its adaption to the case of\nunevenly spaced data.\nLemma 3.11. If a signal H is 95% pure and if r > 1.2 ln(1/\u03b4), the output of Algorithm 3.8 gives\nan estimation of its energy which exceeds kHk2 /3 with probability exceeding 1 \u2212 \u03b4.\n\n10\n\n\fProof. The proof is very similar to that of Lemma 4.5 in [20]. We shall present only the difference\nof these two proofs. Suppose we sample r times for the signal H. Let \u03ba = {t : N|H(t)|2 <\nkHk2 /3}, with \u03bac as its complement, we have\n2\n\nX\nt\u2208\u03ba\n\nH(t)\n\n\u2264 |\u03ba|\n\nX\nt\u2208\u03ba\n\n|H(t)|2 \u2264 |\u03ba|2\n\n11\nkHk2 .\nN3\n\n(20)\n\nOn the other hand, we know that the signal is 95% pure, i.e. |\u0124(\u03c90 )|2 \u2265 0.95kHk2 for some \u03c90 .\nBy modulating, \u03c90 can be moved to 0; therefore, we can, without loss of generality, suppose most\nof the energy concentrates at the frequency 0; then\nN\n1 X\n\u221a\nH(t)\nN t=1\n\n2\n\n= |\u0124(0)|2 \u2265 0.95kHk2.\n\n(21)\n\n1\nkHk.\n3N\n\n(22)\n\nSo we have\nX\n\nt\u2208\u03baC\n\nOn the other hand,|\n\nP\n\nt\u2208\u03baC\n\nH(t) \u2265\n\n\u221a\n\n0.95NkHk \u2212 |\u03ba| \u221a\n\nH(t)| \u2264 |\u03baC |kHk = (N \u2212 |\u03ba|)kHk, so that\nN \u2212 |\u03ba| \u2265\n\nLet \u03b1 =\n\n|\u03ba|\n;\nN\n\n\u0012\n\u221a\n\n|\u03ba|\n0.95N \u2212 \u221a\n3N\n\n\u00132\n\n.\n\n(23)\n\nthe above inequality becomes\n\u0010\n\u0011\n\u221a\n\u03b12 + 3 \u2212 2 0.95 \u2217 3 \u03b1 \u2212 0.15 \u2264 0.\n\nThus 0 \u2264 \u03b1 \u2264 0.075. Define now a random variable X\u03ba =\nestimate\n|\u03ba|\nE(X\u03ba ) =\n\u2264 0.075,\nN\nand the expectation of the random variable ezX\u03ba ,\n\n\u0010P\n\nN\ni=1\n\n(24)\n\u0011\n\u03c7\u03ba (i) ; it will be useful to\n\nE(eX\u03ba z ) = e0 P rob(\u03c7\u03ba (i) = 0) + ez P rob(\u03c7\u03ba (i) = 1) = 1 \u2212 \u03b1 + \u03b1ez .\n\n(25)\n\n(26)\n\nSuppose now we sample the signal H r times, and take the 60-th percentile of the numbers\nN|H(t1 )|2 , . . . , N|H(tr )|2 . By Chernoff's standard argument and similar procedure of Lemma\n4.5 in [20], we have for z > 0,\n\u0013\n\u0012\n\u0002\n\u0003r\n1\n2\nP rob 60-th percentile < kHk = (1 \u2212 \u03b1)e\u22120.6z + \u03b1e0.4z .\n3\n11\n\n\fTake z = ln(1.25(1 \u2212 \u03b1)/\u03b1), then\n(1 \u2212 \u03b1)e\u22120.6z + \u03b1e0.4z = 1.97\u03b10.6(1 \u2212 \u03b1)0.4 .\n\n(27)\n\nThe right hand side of (35) is increasing in \u03b1 on the interval [0, 0.075]; since \u03b1 \u2264 0.075, we obtain\nan upper bound by substituting 0.075 for \u03b1:\n\u0002\n\u0003r \u0002\n\u0003r\n(1 \u2212 \u03b1)e\u22120.6z + \u03b1e0.4z = 1.97\u03b10.6 (1 \u2212 \u03b1)0.4 \u2264 e\u22120.90r .\n(28)\n\u0001\nFor P rob 60-th percentile < 31 kHk2 \u2264 \u03b4, we need r \u2265 1.2 ln(1/\u03b4), we have\nP rob(Output \u2265 kHk2 /3) = P rob(60-th percentile of N|H(t)|2 \u2265 kHk2/3) \u2265 1 \u2212 \u03b4.\n\n(29)\n\nThis norm estimation procedure will be used repeatedly in the group testing step below.\n\n3.3 Isolation\nFor a significant frequency in signal S, isolation aims to construct a series of new signals, such that\nthis significant frequency becomes predominant in at least one of the new isolation signals.\nLemma 3.12. Given signals S, S1 , and the parameters as stated in Lemma 3.3. Suppose F1 (t) =\n1\nS1 (t) \u2217 \u03c71 (t) = (\u03c7T (t)S(t)) \u2217 \u03c71 (t), F (t) = S(t) \u2217 \u03c71 (t). If p \u2265 1+(N \u22121)\u03bb\u03c4\n2 , then for each \u03c9 with\n\n|\u015c(\u03c9)|2 > B\u03bbkSk2 , isolation algorithm can create a signal F1\u2217 , such that\n|F\u03021\u2217 (\u03c9)|2 \u2265 0.98kF1\u2217k2 .\n\n(30)\n\n\u221a\nProof. Since |\u015c(\u03c9)|2 \u221a\n> B\u03bbkSk2 , we have |\u015c(\u03c9)| > B\u03bbkSk. Then there exists\n\u221a some \u03b7 > 0, such\n\u221a\nthat |\u015c(\u03c9)| \u2265 ( \u03b7 + B\u03bb)kSk. Lemma 3.4 states that |\u015c1 (\u03c9) \u2212 \u015c(\u03c9)| \u2264 B\u03bbkSk. Therefore\n|\u015c1 (\u03c9)| \u2265\n(0)\n\n(2k)\n\nIsolation algorithm returns F1 , . . . , F1\n\n\u221a\n\n\u03b7kSk \u2265\n\n\u221a\n\n\u03b7kS1 k.\n\n(31)\n\nwith k < O( \u03b71 ), as described in [8]. For any \u03c9 with\n\n|\u015c1 (\u03c9)|2 \u2265 \u03b7kS1 k2 , there exists some j, such that\n(j)\n\n(j)\n\n|F\u03021 (\u03c9)|2 \u2265 0.98kF1 k2 .\n\n(32)\n\n|F\u03021\u2217 (\u03c9)|2 \u2265 0.98kF1\u2217k2 .\n\n(33)\n\n(j)\n\nLet F1\u2217 = F1 , then\n\nTheoretically, in order to capture a significant mode, we need O(1/\u03b7) signals. However, in\npractice, much fewer signals is enough to achieve this goal.\n12\n\n\f3.4 Group Testing\nIsolation has produced several signals, one of which contains the most significant frequency. Group\ntesting uses repeated zoom-ins on one of the signals, and norm testing to select where to zoom in,\nin order to determine the frequency. The goal of group testing is thus to find the most significant\nmode of the signal F1\u2217 from isolation. It uses recursive procedures MSB (Most Significant Bit) to\napproach this mode gradually.\nDefinition: Denote a set {\u03c9 : (2l \u2212 1)N/32 \u2264 \u03c9 \u2264 (2l + 1)N/32} by intervall .\nGroup test algorithm is given as follows.\nAlgorithm 3.13. G ROUP T ESTING\n(0)\nInput isolation signal F1\u2217 to F1 , i = 0, q = 1\nWhile q < N, in the i-th iteration,\n1. Find the most significant bit v and the number of significant intervals c by the procedure\nMSB.\n(i)\n\n2. Update i = i + 1, modulate the signal F1 by \u230a(v + 0.5)N/16\u230b and dilate it by a factor of\n(i+1)\n\u230a16/c\u230b. Store it in F1\n.\n(i)\n\n3. Call Group Test again with the new signal F1 , denote its output by g.\n4. Update the accumulation factor q = q \u2217 \u230a16/c\u230b.\n5. If g > N/2, then g = g \u2212 N.\n6. return \u230ag/\u230a16/c\u230b + (v + 1/2)N/16 + 0.5\u230b(mod N);\nThe MSB procedure is as follows.\nAlgorithm 3.14. MSB (M OST S IGNIFICANT B IT )\n(i)\nInput: signal F1 with length N, a threshold 0 < \u03b7 < 1.\n(i)\n\n1. Get a series of new signals Hj (t) = F1 (t) \u22c6 (e2\u03c0ijt/16 \u03c71 ), j = 0, . . . , 15.\n2. Estimate the energies ej of Hj , j = 0, . . . , 15.\n3. for l = 0, . . . , 15, compare the energies el with all other energies ej , where j = (l +\n4)mod 16, (l + 5)mod 16, . . . , (l + 12)mod 16. If el > ej for all these j, label it as an\ninterval with large energy.\n4. Find the longest consecutive intervals of large energies. Take their center as v, and the\nnumber of those intervals as c.\n5. If c < 8, then do the original MSB in [8] to get v and set c = 8;\n6. Return the dilation-related factor c and the most significant bit v.\n13\n\n\f(i)\n\nFor convenience, we denote F1 by F1 .\nLemma 3.15. Given a 98% pure signal F1 , suppose Gj (t) = e2\u03c0ijt/16 \u03c71 (t). Then Algorithm 3.13,\nwith Algorithm 3.14 as its subroutine, can find the significant frequency \u03c91 of the signal F1 with\nhigh probability.\nProof. The proof is similar to that of Lemma 5 in [8], with some changes:\nSince the signal F1 is 98% pure, there exist a frequency mode \u03c91 and a signal \u03c1, such that F1 =\na\u03c6\u03c91 + \u03c1, where |a|2 \u2265 0.98kF1 k2 and k\u03c1k2 \u2264 0.02kF1 k2 . Without loss of generality, assume\n\u03c91 \u2208 [\u2212N/32, N/32]. The whole region is divided into 16 subintervals [jN/16 \u2212 N/32, jN/16 +\nN/32], where j = 0, . . . , 15. To estimate F\\\n1 \u2217 G0 (\u03c91 ) for |\u03c91 | \u2264 N/32, we use that |\u011c0 (\u03c91 )| =\n|\u03c7\u03021 (\u03c91 )| \u2265 0.987 for |\u03c91 | \u2264 N/32. It follows that\n2\n|F\\\n1 \u2217 G0 (\u03c91 )| = N F\u03021 (\u03c91 )\u011c0 (\u03c91 )\n\n2\n\n\u2265 N0.9872 |F\u03021 (\u03c91 )|2 \u2265 N0.9872 0.98kF1 k2\n\n\u2265 0.954NkF\u03021 k2 \u2265 0.954NkF\u03021 \u011c0 k2 = 0.954kF1 \u2217 G0 k2 .\nTherefore the estimation X of kF1 \u2217 G0 k satisfies:\nX\n2\n2\n2\n\\\nX \u2265 kF1 \u2217 G0 k2 /3 = kF\\\n\u2217\nG\nk\n/3\n=\n|F\\\n1 \u2217 G0 (\u03c9)| /3 \u2265 |F1 \u2217 G0 (\u03c91 )| /3\n1\n0\n\u03c9\n\n\u2265 0.954NkF1 k2 /3 \u2265 0.318NkF1 k2 .\n\nNext consider the energy of F1 \u2217 G4 .\nk\u03c1\u0302\u011c4 k2 =\n\u2264\n\nX\n\u03c9\n\n2\n\nX\n\u03c9\n\n|\u03c1\u0302(\u03c9)\u011c4(\u03c9)|2\n\n|\u03c1\u0302(\u03c9)| = k\u03c1k2 \u2264 0.02kF1 k2 .\n\nSince |\u011c4 (\u03c91 )| < 0.464, we have\n|F\u03021 (\u03c91 )\u011c4 ( \u03c91 )| \u2264 |F\u03021 (\u03c91 )||\u011c4 ( \u03c91 )| \u2264 |F\u03021 (\u03c91 )|0.464 \u2264 0.464kF1 k\nAlso kF\u03021 \u011c4 k2 \u2212 |F\u03021 (\u03c91 )\u011c4 (\u03c91 )|2 \u2264 0.02kF1 k2 . Thus\nkF\u03021 \u011c4 k2 \u2264 0.4642 kF1 k2 + 0.02kF1 k2 = 0.24kF1 k2 .\nIt follows that\n\n2\n2\nkF1 \u2217 G4 k2 = kF\\\n1 \u2217 G4 k = NkF\u03021 \u011c4 k \u2264 0.24NkF1 k .\n\nThen we compare kF1 \u2217 G4 k2 with the lower bound of the estimation of kF1 \u2217 G0 k2 , which is\n0.24NkF1 k2 \u2264 0.318NkF1 k2 ,\nwhich is less than the estimation for kF1 \u2217 G0 k2 . In general, \u03c9 \u2208 intervalj , for j not necessarily\n\u2032\n0. Therefore we compare kF1 \u2217 Gj \u2032 k2 with kF1 \u2217 Gj k2 , where |j \u2212 j | \u2265 4. If there is some j with\n14\n\n\f/ intervalj \u2032 . Otherwise,\nkF1 \u2217 Gj k2 apparently larger than kF1 \u2217 Gj \u2032 k2 , then we conclude \u03c91 \u2208\npossibly \u03c91 \u2208 intervalj \u2032 . By the above argument, we can always eliminate 9 consecutive interval\nregions out of 16, leaving a cyclic interval of length at most 7N/16. The remaining proof is exactly\nthe same as Lemma 8 in paper [8].\nRemark: In [20], we showed that group testing works for a Box-car filter with width more than\n21, i.e. k > 10. In that case, 2k + 1 intervals are sufficient. A similar conclusion still holds in the\nunevenly spaced data case. However, the lemma above proves the success of group testing under\ndifferent conditions. In our proof, we use a Box-car filter with much shorter width, namely 3 in\ntime domain; this works well if 16 intervals are taken. In practice, we use these shorter filters; we\ncan usually (if B is small) get away with using much fewer intervals as well (e.g. 3 instead of 16).\n\n3.5 Adaptive Greedy Pursuit\nIn summary, given a signal S, for an accuracy \u01eb and for B modes, we can find a very good approximation of the signal S by using Algorithm 2.2.\nTheorem 3.16. Given a signal S, an accuracy \u01eb, success probability 1 \u2212 \u03b4, Algorithm 2.2 can\noutput a B-term representation R with sum-square-error kS \u2212 Rk2 \u2264 (1 + \u01eb)kS \u2212 Ropt k2 , where\nRopt is the B-term representation for S with the least sum-square-error, with time and space cost\nlog(1/\u03b4) log M\nB log M log N log \u03b4\nfor just visiting\npoly(B, log(N), 1\u01eb , log(1/\u03b4)) for computing and \u03bblog(1\u2212(1\u2212p)\n2q1 +2q2 +1 ) +\n\u03bb log p\nsamples.\nProof. We omit the proof since it is very similar to Theorem 9 in [8].\n\n4 NERAlSFA with Interpolation Technique\nThe greedy algorithm described above is fast. When p is sufficiently large (e.g. p > 0.7), the\napproach proposed and discussed in the previous section works well. For smaller p, the amount\nof time wasted to find available sample groups becomes unacceptably long. For example, when\nB = 2, N = 100, p = 0.4, the algorithm couldn't find the signal within 200 greedy pursuit\niterations. For this reason, we introduced an interpolation technique to get an approximate value\nof the missing point in the norm estimation procedure. This algorithm is efficient even in smaller\np cases.\n\n4.1 Lagrange Interpolation Technique\nThe task of interpolation is to estimate S(t) for arbitrary t by drawing a smooth curve through\nall the known points [17]. It is called interpolation when the desired t is between the largest and\nsmallest of these ti 's. We use Lagrange Polynomial Interpolation, one of the simplest and most\npopular interpolation techniques.\nGenerally, the number of interpolation points determines the degree of a polynomial. A polynomial of higher degree is smoother with smaller approximation errors at the expense of more\n15\n\n\fcomputation. Thus we choose a second degree polynomial, as a balance between computational\ncomplexity and accuracy. It is given explicitly by Lagrange's classical formula. If the three nearest\nneighbors are (t1 , S(t1 )), (t2 , S(t2 )), (t3 , S(t3 )), the polynomial is\nP (t) =\n\n(t \u2212 t1 )(t \u2212 t3 )\n(t \u2212 t2 )(t \u2212 t1 )\n(t \u2212 t2 )(t \u2212 t3 )\nS(t1 ) +\nS(t2 ) +\nS(t3 )\n(t1 \u2212 t2 )(t1 \u2212 t3 )\n(t2 \u2212 t1 )(t2 \u2212 t3 )\n(t3 \u2212 t2 )(t3 \u2212 t1 )\n\n(34)\n\nIf S(t) is three times differentiable in an interval [a, b], and the points t1 , t2 , t3 \u2208 [a, b] are\ndifferent, then there exists some v \u2208 [a, b], such that the approximation error is S(t) \u2212 P (t) =\nS (3) (v)\n(t \u2212 t1 )(t \u2212 t2 )(t \u2212 t3 ).\n3!\n\n4.2 Estimate Norms with Interpolation\nWe introduce the interpolation scheme into estimating norms. The idea is to estimate the value of\na missing point by the Lagrange interpolation. The detailed algorithm for estimating norms is as\nfollows.\nAlgorithm 4.1. E STIMATE N ORM WITH INTERPOLATION TECHNIQUE\nInput: signal H, k = 0, the maximum number of samples M.\n1. Randomly generate the index tk , where k = 0, . . . , M \u2212 1.\n2. For each k, if H(tk ) is not available, estimate H(tk ) by Lagrange interpolation; else compute H(tk ) directly.\n3. Estimation = 60-th percentile of the sequence {|H(tk )|2 N}, where k = 0, . . . , M \u2212 1.\n\nNote that we use interpolation only in norm estimation steps, where precision is less critical.\nWith less precise norm estimation, the localization of important modes could still work well when\niterated. For coefficient estimation, which needs to be more precise, we always search for available\nsamples.\n\n5 Numerical Results\nIn this section, we present striking numerical results of NERAlSFA, comparing to the Inverse Nonequispaced Fast Fourier Transform (INFFT) algorithms. The popular benchmark software NFFT\nversion 2.0 is used to give performance of INFFT, with default CGNE_R method and Dirichlet\nkernel. Its time cost excludes the precomputation of samples values, which takes O(L). Numerical experiments show the advantage of our NERAlSFA algorithm in processing large amount of\ndata. We begin in Section 5.1 with comparing NERAlSFA with INFFT for some one and two dimensional examples with different length. In Section 5.2, the performance for different number of\nmodes is shown. Finally, we test the capability of NERAlSFA to recover the signal in the situation\nwith a large amount of missing data and in presence of large noise.\nAll the experiments were run on an AMD Athlon(TM) XP1900+ machine with Cache size\n256KB, total memory 512 MB, Linux kernel version 2.4.20-20.9 and compiler gcc version 3.2.2.\nThe numerical data is an average of 10 runs of the code; errors are given in the L2 norm.\n16\n\n\fN\n\nINFFT\n\n29 =512\n211 =2048\n213 =8192\n215 =32768\n217 =131072\n219 =524288\n\n0.01\n0.03\n0.17\n0.83\n4.30\n19.94\n\nNERAlSFA\n(+sampling)\n0.63\n0.77\n0.90\n0.93\n1.03\n1.20\n\nNERAlSFA\n(w/o sampling)\n0.31\n0.37\n0.46\n0.49\n0.51\n0.61\n\nTable 1: Experiments with fixed B = 8, p = 0.7, d = 1 (one dimension), and varying length N of\nsignals; an i.i.d. white noise is added with \u03c3 = 0.5, or SNR \u2243 30dB (see text). For each length\nof the signal, 10 different runs were carried out; the average result is shown. We did all the tests\nfor NERAlSFA with Lagrange interpolation, as explained in the text. Two kinds of time costs for\nNERAlSFA are provided. One is the total running time and another is the running time excluding\nthe sampling time. The time of INFFT does not include the precomputation time for samples.\n\n5.1 Experiments with Different Length of Signals\nPB\nWe ran the comparison for a 8-mode superposition signal S(t)\n=\ni=1 \u03c6\u03c9i , plus white noise \u03bd with\n\u221a\nthe standard deviation \u03c3 = 0.5, damped by a factor of 1/ N, ( so that k\u03bdk2 = \u03c3 2 = 0.25; since\nkSk2 = 8, this implies SNR = 20 log10 32 \u2248 30.1dB). Other parameters are B = 8, \u01eb = 0.02,\n\u03b4 = 0.01, and p = 70%. The missing data are randomly and uniformly distributed. NERAlSFA\noutperforms INFFT in speed when N is large; see Table 1 and Figure 2. The corresponding\ncrossover point is N \u2265 215 = 32768 . For example, to process 219 = 524, 288 data, more\nthan nineteen minutes (estimated) are needed for INFFT versus approximately one second for\nNERAlSFA. Experiments support the theoretical conclusion that NERAlSFA would be faster than\nINFFT after some N for a sparse signal; whatever the sparsity, i.e. whatever the value of B, there\nalways exists some crossover N.\nP\nIn two dimensions, we test a noisy 6-mode superposition signal S(t) = B\ni=1 \u03c6\u03c9xi \u03c6\u03c9yi + \u03bd,\nwith B = 6, \u01eb = 0.02, \u03b4 = 0.01, p = 80%, and \u03c3 = 0.1. Missing data are randomly and\nuniformly distributed. As the number of grid points N in each dimension grows, two dimensional\nNERAlSFA outperforms two dimensional INFFT at N \u2265 512, as Table 3 and Figure 4 show. The\ncrossover point becomes much smaller in high dimensions situation. It would not be surprising\nthat for recovering a 6-mode three dimensional signal, NERAlSFA surpasses INFFT at a hundred\nsampling grid points in each dimension.\n\n5.2 Experiments with Different Number of Modes\nThe number of modes has an important influence on the running time since the crossover point\nvaries for signals with different B. To investigate this, we did the experiments with fixed N =\n218 = 262144, p = 0.6 and varying B.\nPBAs before, we take S to be a superposition of exactly B\nmodes with white noise, i.e. S(t) = i=1 ci \u03c6\u03c9i + \u03bd, with standard deviation of noise \u03c3 = 0.05.\n17\n\n\fTime Comparison between NERAlSFA and INFFT\n3\nINFFT\nNERALSFA\nNERALSFA(no sampling)\n\nRunning Time (seconds)\n\n2.5\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n\n10\n\n12\n\n14\nlog (Signal Length)\n\n16\n\n18\n\n2\n\nFigure 2: Time Comparison between INFFT and NERAlSFA for different N with B = 8, p = 0.7,\nd = 1. The result in Table 1 is shown in the form of a graph here. The x coordinate is the log2 (N),\nthe y coordinate presents the running time for each algorithm. NERAlSFA without sampling\nsurpasses INFFT at N = 214 = 16384.\n\nN\n\nINFFT\n\n128\n256\n512\n1024\n2048\n\n0.13\n0.73\n3.00\n11.59\n54.94\n\nNERAlSFA\n(+sampling)\n2.86\n2.60\n3.70\n4.31\n6.56\n\nNERAlSFA\n(w/o sampling)\n1.57\n1.46\n2.13\n2.94\n4.90\n\nTable 3: Experiments with fixed B = 6, p = 0.8, d = 2 (two dimensions), and varying length N\nof signals; an i.i.d white noise is added with \u03c3 = 0.1, or SNR \u2243 56dB (see text). For each length\nof the signal, 10 different runs were carried out; the average result is shown. We did all the tests\nfor NERAlSFA with two dimensional interpolation techniques as shown in the appendix. Again,\ntwo kinds of time costs for NERAlSFA, the one with and without sampling time is provided. The\ntime of INFFT excludes the sampling time.\n\n18\n\n\fTime Comparison between NERAlSFA and INFFT\n8\n\nRunning Time (seconds)\n\n7\n\nINFFT\nNERALSFA\nNERALSFA(no sampling)\n\n6\n5\n4\n3\n2\n1\n0\n7\n\n7.5\n\n8\n\n8.5\n9\n9.5\nlog (Signal Length)\n\n10\n\n10.5\n\n11\n\n2\n\nFigure 4: Time comparison between INFFT and NERAlSFA for different N with fixed B = 6,\np = 0.8, d = 2. The x coordinate is the logarithm of length N of signal in each dimension.\nINFFT is very fast when N is relatively small and slows down quickly as N increases. On the\ncontrary, it takes NERAlSFA similar time to process small and large N problem. NERAlSFA\nwithout sampling outperforms INFFT at N = 28.5 =362.\nAvailable data are uniformly and randomly distributed. Table 5 and Figure 6 compare the running\ntime for different B using INFFT and NERAlSFA. At first, NERAlSFA takes less time because N\nis so large. However, the execution time of INFFT keeps constant for different number of modes\nB, while that of modified RAlSFA is polynomial of higher order. INFFT is faster than NERAlSFA\nwhen B \u2265 10. The regression techniques shows empirically that the order of B in NERAlSFA is\ngreater than quadratic. This is one of the characteristics of this version of the RAlSFA algorithms\nand irrelevant to the nonequispaceness of the data. (A different version of RAlSFA in [9] is linear\nin B, but maybe less easily used when not all equispaced data are available. )\n\n5.3 Experiments for Different Percentage of Missing Data\nThe advantage of interpolation techniques is to recover a signal even when a large percentage\nof data is missing. Table 7 shows the recovery effect for a two-mode pure signal c1 \u03c6\u03c91 + c2 \u03c6\u03c92 ,\nN = 106 with all the other parameters \u01eb and \u03b4 the same as before. When the percentage of available\ndata is large, both algorithms recover the signal well with similar running time.\nWe tried another example of signal when N = 100. NERAlSFA without interpolation techniques fails to recover the signal with high probability if more than 45% data are unavailable. In\ncontrast, with the help of interpolation technique, the NERAlSFA can always recover the signal\nwith only 25% available data.\nExperiments also show that for NERAlSFA with interpolation technique, the total number of\n19\n\n\fnumber of modes\nB\n2\n4\n6\n8\n10\n13\n16\n\nSNR\n(dB)\n58\n64\n68\n70\n72\n74\n76\n\nNERAlSFA\n(+sampling)\n0.06\n0.24\n0.61\n1.44\n2.45\n5.78\n10.03\n\nNERAlSFA\n(w/o sampling)\n0.01\n0.06\n0.23\n0.69\n1.39\n3.64\n7.17\n\nINFFT\n1.35\n1.35\n1.35\n1.35\n1.35\n1.35\n1.35\n\nTable 5: Experiments with fixed N = 218 , p = 0.6, d = 1 (one dimension), \u03c3 = 0.05, and varying\nnumber of modes B of signals. For each length of the signal, 10 different runs were carried out;\nthe average result is shown. We did all the tests for NERAlSFA with interpolation techniques. We\npresent two different time costs of NERAlSFA, with and without sampling.\n\nTime of NERAlSFA and INFFT for different B\n6\n\nRunning Time (seconds)\n\n5\n\nINFFT\nNERALSFA\nNERALSFA(no sampling)\n\n4\n\n3\n\n2\n\n1\n\n0\n2\n\n4\n\n6\n\n8\n10\nNumber of Modes B\n\n12\n\n14\n\n16\n\nFigure 6: Time Comparison between INFFT and NERAlSFA for different B with with fixed N =\n218 , p = 0.6, d = 1 (one dimension), \u03c3 = 0.05, a graph of the result in Table 5. The x coordinate is\nthe number of modes B, the y coordinate presents running time. The running time of NERAlSFA\nis polynomial to B. In contrast, the time of INFFT keeps constant for different B, excluding\nprecomputation for the samples. NERAlSFA without sampling begins to be slower than INFFT at\nB = 10 for N = 218 .\n\n20\n\n\fp\n1\n0.8\n0.6\n0.4\n0.3\n0.2\n0.1\n10\u22122\n10\u22123\n10\u22124\n0.00002\n\nTime of NERAlSFA\n(with interpolation)\n0.03\n0.04\n0.05\n0.05\n0.06\n0.06\n0.07\n0.11\n0.51\n4.58\n758.22\n\nsuccess\nprobability\n100 %\n100 %\n100 %\n100 %\n100 %\n100 %\n100 %\n100 %\n100 %\n100 %\n97 %\n\nTime of NERAlSFA\n(w/o interpolation)\n0.03\n0.06\n0.49\n0.45\n-\n\nsuccess\nprobability\n100 %\n100 %\n100 %\n100 %\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n\nTable 7: Experiments with fixed B = 2, N = 106 , no noise, and varying percentage of available\ndata. Each entry is based on the average of 10 different runs. In each run, the number of iterations\nis limited to 200; (this also corresponds to a fixed limit to the number of samples taken.) the success\nprobability indicates the number of runs in which all 6 modes were found. When only 30% of data\nis available, the NERAlSFA without interpolation cannot find all two significant modes within 200\niterations.\navailable data, instead of the percentage of available data determines the success probability. On\nthe contrary, The success of NERAlSFA without interpolation is determined by the percentage.\n\n5.4 Experiments to Recover Noisy Signals\nTo recover aPsignal from very noisy data is a challenging problem. The following tests are done\n17\nfor S(t) = B\nstandard deviation\ni=1 ci \u03c6\u03c9i + \u03bd, B = 6, \u01eb = 0.02, N = 2 , p = 0.6, and different\n\u221a\n\u03c3 for noise. The amplitude of noise is still multiplied by a factor of 1/ N . As Table 8 shows,\nNERAlSFA excels at extracting information from noisy data even in the case of small signal to\nnoise ratio.\n\n6 Conclusion\nWe provide a sublinear sampling algorithm that recovers, with high probability, a B-term Fourier\nrepresentation for an unevenly spaced signal. It is faster than any existed methods for processing\nsparse signals of large size. Moreover, it recovers the signal in the situation of large percentage of\nmissing data or small signal to noise ratio.\n\n21\n\n\f\u03c3\n0\n0.5\n1.0\n1.5\n2.0\n2.5\n\nSNR\n(dB)\n27.60\n15.56\n8.53\n3.52\n-0.35\n\nTime of NERAlSFA\n(+sampling)\n0.48\n0.56\n0.87\n3.94\n4.78\n7.96\n\nTime of NERAlSFA\n( w/o sampling)\n0.21\n0.22\n0.32\n1.59\n1.86\n2.14\n\nRelative Error\n(%)\n0.02\n2.00\n4.50\n5.83\n7.67\n8.50\n\nSuccess\nprobability\n100%\n100%\n90%\n80%\n50%\n30%\n\nTable 8: Experiments with fixed B = 6, N = 217 , p = 0.6, and varying noise levels. For each\nnoise level, 10 different runs were carried out; the average result is shown. In each run, the number\nof iterations is limited to 200; (this also corresponds to a fixed limit to the number of samples\ntaken.) the success probability indicates the number of runs in which all 6 modes were found. The\naverage relative error is the error of reconstructed signal with respect to the original signal.\n\n7 Acknowledgments\nFor many helpful suggestions and discussions, I would thank my adviser Ingrid Daubechies. In\naddition, I thank Weinan E, Anna Gilbert, Martin Strauss for their suggestions.\n\nAppendix\nHow to interpolate the two dimensional data to get values for missing points\nIn one dimension, values of missing points can be interpolated by its few nearest left and right\navailable neighbors. The idea can be extended to higher dimensional cases with more techniques.\nFor instance, in two dimensions, we first find four nearest available neighbors of a missing\npoint in each quadrant. Suppose a missing point is (x, y), its four neighbors are (x1 , y1 ), (x2 , y2 ),\n(x3 , y3 ), (x4 , y4). The weights of neighbors can be derived by solving the following linear system\nof equations.\n\uf8f6\n\uf8f6 \uf8eb\n\uf8f6\uf8eb\n\uf8eb\nx\nw1\nx1\nx2\nx3\nx4\n\uf8f7\n\uf8f7 \uf8ec\n\uf8ec\n\uf8ec y1\ny2\ny3\ny4 \uf8f7\n\uf8f7 \uf8ec w2 \uf8f7 = \uf8ec y \uf8f7\n\uf8ec\n(35)\n\uf8ed x1 y1 x2 y2 x3 y3 x4 y4 \uf8f8 \uf8ed w3 \uf8f8 \uf8ed xy \uf8f8\n1\nw4\n1\n1\n1\n1\n\nHowever, the matrix in (35) could be singular. In this case we choose the three nearest neighbors in different quadrants and use the following equations:\n\uf8eb\n\uf8f6\uf8eb\n\uf8f6 \uf8eb \uf8f6\nx1 x2 x3\nw1\nx\n\uf8ed y 1 y 2 y 3 \uf8f8 \uf8ed w2 \uf8f8 = \uf8ed y \uf8f8\n(36)\n1 1 1\nw3\n1\n22\n\n\fFigure 9: Some geometrical shapes of available neighboring points that occur most often. A\nmissing point (denoted by a small cross) is at the center of the cross. Available points are denoted\nby dots. Left: the four available neighbors are located in the shape of cross. The distances of each\nneighbor to the missing point are equal. Right: almost the same as configuration in the left side,\nexcept one point moved off to the diagonal.\n\nThe time to locate those nearest neighbors and compute corresponding weights is considered a\npart of precomputation and excluded from total running time.\nNote that we can use geometrical arguments to simplify the pre-computation of the weights.\nOne easily sees that the system of equations (35) is translation invariant: the two linear system of\nequations\n\uf8f6\uf8eb\nw1\nx1 + l\nx2 + l\nx3 + l\nx4 + l\n\uf8ec\n\uf8f7\n\uf8ec\ny\n+\np\ny\n+\np\ny\n+\np\ny\n+\np\n1\n2\n3\n4\n\uf8f7 \uf8ec w2\n\uf8ec\n\uf8ed (x1 + l)(y1 + p) (x2 + l)(y2 + p) (x3 + l)(y3 + p) (x4 + l)(y4 + p) \uf8f8 \uf8ed w3\nw4\n1\n1\n1\n1\n\uf8eb\n\nand\n\n\uf8f6\uf8eb\nw1\nx1\nx2\nx3\nx4\n\uf8ec\n\uf8f7\n\uf8ec y1\ny\ny\ny\n2\n3\n4 \uf8f7 \uf8ec w2\n\uf8ec\n\uf8ed x1 y1 x2 y2 x3 y3 x4 y4 \uf8f8 \uf8ed w3\nw4\n1\n1\n1\n1\n\uf8eb\n\n\uf8f6\n0\n\uf8f7 \uf8ec 0 \uf8f7\n\uf8f7=\uf8ec \uf8f7\n\uf8f8 \uf8ed 0 \uf8f8\n1\n\uf8f6\n\n\uf8f6\nl\n\uf8f7 \uf8ec p \uf8f7\n\uf8f7\n\uf8f7=\uf8ec\n\uf8f8 \uf8ed lp \uf8f8\n1\n\uf8f6\n\n\uf8eb\n\n\uf8eb\n\nhave the same solutions for any l and p. That means the location of the missing points does not\ninfluence the weights. Only the geometrical shape and relative distance of the available neighbors\nof a missing point matters.\nThus, we compute weights for the geometrical shapes of available neighboring points which\noccur most often. As we go through every missing point, we check if the shape of its neighboring\navailable points matches those popular ones; if it does, we can directly get the weights without\ncomputation. This saves a huge amount of work, especially when p is large.\n23\n\n\fp\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n\np4\n100%\n65%\n41%\n24%\n13%\n6%\n\n4p4 (1 \u2212 p)(2 \u2212 p)\n0\n29%\n39%\n37%\n29%\n19%\n\nsum:p4 + 4p4 (1 \u2212 p)(2 \u2212 p)\n100%\n94%\n80%\n61%\n42%\n25%\n\nTable 10: Two possibilities corresponding to the geometrical shapes in Figure 9. The parameter p\nis the percentage of available data. The left side of Figure 9 happens with probability p4 ; the right\nside appears with probability 4p4 (1 \u2212 p)(2 \u2212 p).\nFor example, if the four neighboring points are located in the shape of a cross with the missing\npoint as their center, as the left side of Figure 9 shows, then all of the weights are equal to one\nquarter. This situation happens with probability p4 , which is almost 2/3 when p = 0.9. Another\noften occurring case typically has one of the four neighbors of the previous configuration moved\noff to the diagonal (see the right side of Figure 9), which happens with probability 4p4 (1\u2212p)(2\u2212p),\ni.e. about 28% when p = 0.9. In this case, the two neighbors on the same line as the mirroring\npoints have a weight 0.5 respectively; the other two points have weight zero. Table 10 shows the\nprobabilities of these two situations as p varies.\n\nReferences\n[1] R. BASS AND K. G R\u00d6CHENIG, Random sampling of multivariate trigonometric polynomials, SIAM J. Math. Anal., Vol. 36 (2004), pp. 773-795.\n[2] A. BJ\u00d6RCK. Numerical Methods for Least Squares Problems. SIAM, Philadelphia, 1996.\n[3] J.P. B OYD, A fast algorithm for Chebyshev, Fourier and Sinc interpolation onto an irregular\ngrid, J. Comput. Phys., 103 (1992), pp. 243-257.\n[4] E. CANDES, J. ROMBERG, and T. TAO, Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information,\nhttp://arxiv.org/pdf/math.CA/0411273\n[5] H. FASSBENDER, On numerical methods for discrete least-squares approximation by\ntrigonometric polynomials, Math. Comput., 66(1997), pp719-741.\n[6] H. F EICHTINGER , K. G R\u00d6CHENIG AND T. S TROHMER, Efficient numerical methods in nonuniform sampling theory, Numer. Math., 69 (1995), pp423-440.\n[7] A. C. GILBERT, S. GUHA, P. INDYK, Y. KOTIDIS, S. MUTHUKRISHNAN, M. STRAUSS, Fast,\nsmall-space algorithms for approximate histogram maintenance. STOC 2002: 389-398.\n24\n\n\f[8] A.C. G ILBERT, S. G UHA , P. I NDYK , S. M UTHUKRISHNAN AND M. S TRAUSS, NearOptimal Sparse Fourier Representations via Sampling, STOC, 2002\n[9]\n\nA.C. G ILBERT, S. M UTHUKRISHNAN AND M. S TRAUSS, Improved Time Bounds for NearOptimal Sparse Fourier Representation, to appear.\n\n[9] L. G REENGARD AND J. L EE. Accelerating the Nonuniform Fast Fourier Transform, SIAM\nReview, 46 (2004), pp. 443-454.\n[10] G. G RIMMETT\nPress, 2001.\n\nAND\n\nD. S TIRZAKER. Probability and Random Processes. Oxford University\n\n[11] M. HANKE. Conjugate gradient type method for ill-posed problems. Wiley, New York, 1995.\n[12] S. K UNIS AND D. P OTTS, Stability results for scattered data interpolation by trigonometric\npolynomials, preprint.\n[13] S.\nK UNIS ,\nD.\nP OTTS,\nNFFT,\nSoftware,\nhttp://www.math.uni-luebeck.de/potts/nfft, 2002-2004.\n\nC\n\nsubroutine\n\nlibrary,\n\n[14] S. K UNIS , D. P OTTS , G. S TEIDL, Fast Fourier transform at nonequispaced knots: A user's\nguide to a C-library, Manual of NFFT 2.0 software.\n[15] Y. M ANSOUR, Randomized interpolation and approximation of sparse polynomials , SIAM\nJournal on Computing 24:2 (1995).\n[16] A. O PPENHEIM , A. W ILLSKY\n\nWITH\n\nS. N OWAB. Signals and Systems. Prentice Hall, 1998.\n\n[17] W. P RESS , S. T EUKOLSKY, W. V ETTERLING AND B. F LANNERY. Numerical Recipes in\nC: the art of scientific computing. Cambridge University Press, 1992.\n[18] L. R EICHEL , G. S. A MMAR , AND W. B. G RAGG. Discrete least squares approximation by\ntrigonometric polynomials. Math. Comput., 57(1991), pp. 273-289.\n[19] A. F. WARE, Fast Approximate Fourier Transforms for Irregularly Spaced Data, SIAM Rev.,\n40 (1998), pp. 838\u2013856.\n[20] J. Z OU , A.C. G ILBERT, M. S TRAUSS AND I. DAUBECHIES, Theoretical and Experimental\nAnalysis of a Randomized Algorithm for Sparse Fourier Transform Analysis, submitted to\nJournal of Computational Physics.\n\n25\n\n\f"}