{"id": "http://arxiv.org/abs/0910.1954v1", "guidislink": true, "updated": "2009-10-10T22:36:48Z", "updated_parsed": [2009, 10, 10, 22, 36, 48, 5, 283, 0], "published": "2009-10-10T22:36:48Z", "published_parsed": [2009, 10, 10, 22, 36, 48, 5, 283, 0], "title": "Multi-channel Opportunistic Access: A Case of Restless Bandits with\n  Multiple Plays", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0910.5422%2C0910.1051%2C0910.3764%2C0910.5794%2C0910.0775%2C0910.0317%2C0910.2385%2C0910.2997%2C0910.2198%2C0910.0383%2C0910.4638%2C0910.0504%2C0910.3166%2C0910.4878%2C0910.0174%2C0910.5809%2C0910.2190%2C0910.0632%2C0910.5494%2C0910.0971%2C0910.0702%2C0910.4713%2C0910.3155%2C0910.3151%2C0910.4838%2C0910.2214%2C0910.5933%2C0910.1035%2C0910.1158%2C0910.1990%2C0910.4372%2C0910.4219%2C0910.5156%2C0910.3826%2C0910.2411%2C0910.1818%2C0910.2309%2C0910.5791%2C0910.1583%2C0910.2330%2C0910.5302%2C0910.4414%2C0910.2255%2C0910.0686%2C0910.5458%2C0910.5322%2C0910.2471%2C0910.4746%2C0910.2630%2C0910.2788%2C0910.5857%2C0910.4932%2C0910.2528%2C0910.2090%2C0910.3043%2C0910.4691%2C0910.1427%2C0910.1505%2C0910.1754%2C0910.0783%2C0910.3190%2C0910.2671%2C0910.1254%2C0910.3737%2C0910.4493%2C0910.3244%2C0910.5693%2C0910.1284%2C0910.5887%2C0910.1839%2C0910.1317%2C0910.1730%2C0910.3469%2C0910.3881%2C0910.4030%2C0910.4283%2C0910.1954%2C0910.5431%2C0910.5364%2C0910.1603%2C0910.3141%2C0910.0855%2C0910.1757%2C0910.0723%2C0910.5652%2C0910.0234%2C0910.1753%2C0910.0790%2C0910.2965%2C0910.0439%2C0910.3193%2C0910.4204%2C0910.2873%2C0910.1881%2C0910.0819%2C0910.0480%2C0910.0885%2C0910.1832%2C0910.2705%2C0910.2200%2C0910.3000&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Multi-channel Opportunistic Access: A Case of Restless Bandits with\n  Multiple Plays"}, "summary": "This paper considers the following stochastic control problem that arises in\nopportunistic spectrum access: a system consists of n channels (Gilbert-Elliot\nchannels)where the state (good or bad) of each channel evolves as independent\nand identically distributed Markov processes. A user can select exactly k\nchannels to sense and access (based on the sensing result) in each time slot. A\nreward is obtained whenever the user senses and accesses a good channel. The\nobjective is to design a channel selection policy that maximizes the expected\ndiscounted total reward accrued over a finite or infinite horizon. In our\nprevious work we established the optimality of a greedy policy for the special\ncase of k = 1 (i.e., single channel access) under the condition that the\nchannel state transitions are positively correlated over time. In this paper we\nshow under the same condition the greedy policy is optimal for the general case\nof k >= 1; the methodology introduced here is thus more general. This problem\nmay be viewed as a special case of the restless bandit problem, with multiple\nplays. We discuss connections between the current problem and existing\nliterature on this class of problems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0910.5422%2C0910.1051%2C0910.3764%2C0910.5794%2C0910.0775%2C0910.0317%2C0910.2385%2C0910.2997%2C0910.2198%2C0910.0383%2C0910.4638%2C0910.0504%2C0910.3166%2C0910.4878%2C0910.0174%2C0910.5809%2C0910.2190%2C0910.0632%2C0910.5494%2C0910.0971%2C0910.0702%2C0910.4713%2C0910.3155%2C0910.3151%2C0910.4838%2C0910.2214%2C0910.5933%2C0910.1035%2C0910.1158%2C0910.1990%2C0910.4372%2C0910.4219%2C0910.5156%2C0910.3826%2C0910.2411%2C0910.1818%2C0910.2309%2C0910.5791%2C0910.1583%2C0910.2330%2C0910.5302%2C0910.4414%2C0910.2255%2C0910.0686%2C0910.5458%2C0910.5322%2C0910.2471%2C0910.4746%2C0910.2630%2C0910.2788%2C0910.5857%2C0910.4932%2C0910.2528%2C0910.2090%2C0910.3043%2C0910.4691%2C0910.1427%2C0910.1505%2C0910.1754%2C0910.0783%2C0910.3190%2C0910.2671%2C0910.1254%2C0910.3737%2C0910.4493%2C0910.3244%2C0910.5693%2C0910.1284%2C0910.5887%2C0910.1839%2C0910.1317%2C0910.1730%2C0910.3469%2C0910.3881%2C0910.4030%2C0910.4283%2C0910.1954%2C0910.5431%2C0910.5364%2C0910.1603%2C0910.3141%2C0910.0855%2C0910.1757%2C0910.0723%2C0910.5652%2C0910.0234%2C0910.1753%2C0910.0790%2C0910.2965%2C0910.0439%2C0910.3193%2C0910.4204%2C0910.2873%2C0910.1881%2C0910.0819%2C0910.0480%2C0910.0885%2C0910.1832%2C0910.2705%2C0910.2200%2C0910.3000&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper considers the following stochastic control problem that arises in\nopportunistic spectrum access: a system consists of n channels (Gilbert-Elliot\nchannels)where the state (good or bad) of each channel evolves as independent\nand identically distributed Markov processes. A user can select exactly k\nchannels to sense and access (based on the sensing result) in each time slot. A\nreward is obtained whenever the user senses and accesses a good channel. The\nobjective is to design a channel selection policy that maximizes the expected\ndiscounted total reward accrued over a finite or infinite horizon. In our\nprevious work we established the optimality of a greedy policy for the special\ncase of k = 1 (i.e., single channel access) under the condition that the\nchannel state transitions are positively correlated over time. In this paper we\nshow under the same condition the greedy policy is optimal for the general case\nof k >= 1; the methodology introduced here is thus more general. This problem\nmay be viewed as a special case of the restless bandit problem, with multiple\nplays. We discuss connections between the current problem and existing\nliterature on this class of problems."}, "authors": ["Sahand Haji Ali Ahmad", "Mingyan Liu"], "author_detail": {"name": "Mingyan Liu"}, "author": "Mingyan Liu", "arxiv_comment": "8 pages, 0 figures, Forty-Seventh Annual Allerton Conference on\n  Communication, Control, and Computing, 2009", "links": [{"href": "http://arxiv.org/abs/0910.1954v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0910.1954v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0910.1954v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0910.1954v1", "journal_reference": "Proceedings of Forty-Seventh Annual Allerton Conference on\n  Communication, Control, and Computing, 2009", "doi": null, "fulltext": "Multi-channel Opportunistic Access: A Case of Restless Bandits with\nMultiple Plays\n\narXiv:0910.1954v1 [cs.IT] 10 Oct 2009\n\nSahand Haji Ali Ahmad, Mingyan Liu\nAbstract- This paper considers the following stochastic control problem that arises in opportunistic spectrum access: a\nsystem consists of n channels where the state (\"good\" or\n\"bad\") of each channel evolves as independent and identically\ndistributed Markov processes. A user can select exactly k\nchannels to sense and access (based on the sensing result) in\neach time slot. A reward is obtained whenever the user senses\nand accesses a \"good\" channel. The objective is to design a\nchannel selection policy that maximizes the expected discounted\ntotal reward accrued over a finite or infinite horizon. In our\nprevious work we established the optimality of a greedy policy\nfor the special case of k = 1 (i.e., single channel access) under\nthe condition that the channel state transitions are positively\ncorrelated over time. In this paper we show under the same\ncondition the greedy policy is optimal for the general case of\nk \u2265 1; the methodology introduced here is thus more general.\nThis problem may be viewed as a special case of the restless\nbandit problem, with multiple plays. We discuss connections\nbetween the current problem and existing literature on this\nclass of problems.\n\nI. I NTRODUCTION\nWe consider the following stochastic control problem:\nthere are n uncontrolled Markov chains, each an independent, identically-distributed, two-state discrete-time Markov\nprocess. The two states will be denoted as state 1 and state 0\nand the transition probabilities are given by pij , i, j = 0, 1.\nThe system evolves in discrete time. In each time instance,\na user selects exactly k out of the n processes and is allowed\nto observe their states. For each selected process that happens\nto be in state 1 the user gets a reward; there is no penalty for\nselecting a channel that turns out to be state 0 but each such\noccurrence represents a lost opportunity because the user is\nlimited to selecting only k of them. The ones that the user\ndoes not select do not reveal their true states. Out objective is\nto derive a selection strategy whose total expected discounted\nrewarded over a finite or infinite horizon is maximized.\nThis is a Markov decision process (or MDP) problem [?].\nFurthermore, it is a partially observed MDP (or POMDP)\nproblem [?] due to the fact that the states of the underlying\nMarkov processes are not fully observed at all times and that\nas a consequence the system state as perceived by the user\nis in the form of a probability distribution, also commonly\nreferred to as the information state of the system [?]. This\nproblem is also an instance of the restless bandit problem\nwith multiple plays [?], [?], [?]. More discussion on this\nliterature is provided in section V.\nThis work is partially supported by CNS-0238035 and CCF-0910765. S.\nH. A. Ahmad and M. Liu are with the Dept. of Electrical Engineering and\nComputer Science, University of Michigan, Ann Arbor, MI 48105, {shajiali,\nmingyan}@eecs.umich.edu.\n\nThe application of the above problem abstraction to multichannel opportunistic access is as follows. Each Markov\nprocess represents a wireless channel, whose state transitions\nreflect dynamic changes in channel conditions caused by\nfading, interference, and so on. Specifically, we will consider\nstate 1 as the \"good\" state, in which a user (or transmitter)\ncan successfully communicate with a receiver; state 0 is the\n\"bad\" state, in which communication will fail. The channel\nstate is assumed to remain constant within a single discrete\ntime step. A multichannel system consists of n distinct\nchannels. A user who wishes to use a particular channel at\nthe beginning of a time step must first sense or probe the state\nof the channel, and can only transmit in a channel probed\nto be in the \"good\" state in the same time step. The user\ncannot sense and access more than k channels at a time due\nto hardware limitations. If all k selected channels turn out to\nbe in the \"bad\" state, the user has to wait till the beginning\nof the next time step to repeat the selection process.\nThis model captures some of the essential features of\nmultichannel opportunistic access as outlined above. On the\nother hand, it has the following limitations: the simplicity\nof the iid two-state channel model; the implicit assumption\nthat channel sensing is perfect and the lack of penalty if the\nuser transmits in a bad channel due to imperfect sensing;\nand the assumption that the user can select an arbitrary\nset of k channels out of n (e.g., it may only be able to\naccess a contiguous block of channels due to physical layer\nlimitations). Nevertheless this model does allow us to obtain\nanalytical insights into the problem, and more importantly,\nsome insight into the more general problem of restless\nbandits with multiple plays.\nThis model has been used and studied quite extensively\nin the past few years, mostly within the context of opportunistic spectrum access and cognitive radio networks, see\nfor example [?], [?], [?], [?]. [?] studied the same problem\nand proved the optimality of the greedy policy in the special\ncase of k = 1, n = 2, [?] proved the optimality of the\ngreedy policy in the case of k = n \u2212 1, while [?], [?]\nlooked for provably good approximation algorithms for a\nsimilar problem. Furthermore, the indexability (in the context\nof Whittle's heuristic index and indexability definition [?])\nof the underlying problem was studied in [?], [?].\nOur previous work [?] established the optimality of the\ngreedy policy for the special case of k = 1 for arbitrary n\nand under the condition p11 \u2265 p01 , i.e., when a channel's\nstate transitions are positively correlated. In this sense, the\nresults reported in the present paper is a direct generalization\nof results in [?], as we shall prove the optimality of the\n\n\fgreedy policy under the same condition but for any n \u2265\nk \u2265 1. The main thought process used to prove this more\ngeneral result derives from that used in [?]. However, there\nwere considerable technical difficulties we had to overcome\nto reach the conclusion.\nIn the remainder of this paper we first formulate the problem in Section II, present preliminaries in Section III, and\nthen prove the optimality of the greedy policy in Section IV.\nWe discuss our work within the context of restless bandit\nproblems in Section V. Section VI concludes the paper.\n\nIt follows that the information state of the system evolves as\nfollows. Given that the state at time t is \u03c9\u0304(t) and action ak (t)\nis taken, \u03c9i (t + 1) for i \u2208 ak (t) can take on two values: (1)\np11 if the observation is that channel i is in a \"good\" state;\nthis occurs with probability \u03c9i (t); (2) p01 if the observation is\nthat channel i is in a \"bad\" state; this occurs with probability\n1 \u2212 \u03c9i . For any other channel j 6\u2208 ak (t), with probability 1\nthe corresponding \u03c9j (t + 1) = \u03c4 (\u03c9j (t)) where the operator\n\u03c4 : [0, 1] \u2192 [0, 1] is defined as\n\u03c4 (\u03c9) := \u03c9p11 + (1 \u2212 \u03c9)p01 ,\n\n0\u2264\u03c9\u22641.\n\n(1)\n\nII. P ROBLEM F ORMULATION\nThe objective is to maximize its total discounted expected\nreward over a finite horizon given in the following problem\n(P) (extension to infinite horizon is discussed in Section V):\n\nAs outlined in the introduction, we consider a user trying\nto access the wireless spectrum pre-divided into n independent and statistically identical channels, each given by a twostate Markov chain. The collection of n channels is denoted\nby N , each indexed by i = 1, 2, * * * , n.\nThe system operates in discrete time steps indexed by t,\nt = 1, 2, * * * , T , where T is the time horizon of interest. At\ntime t\u2212 , the channels go through state transitions, and at time\nt the user makes the channel selection decision. Specifically,\nat time t the user selects k of the n channels to sense, the\nset denoted by ak \u2282 N .\nFor channels sensed to be in the \"good\" state (state 1),\nthe user transmits in those channels and collects one unit of\nreward for each such channel. If none is sensed good, the user\ndoes not transmit, collects no reward, and waits until t + 1 to\nmake another choice. This process repeats sequentially until\nthe time horizon expires.\nThe underlying system (i.e., the n channels) is not fully\nobservable to the user. Specifically, channels go through\nstate transition at time t\u2212 (or anytime between (t \u2212 1, t)),\nthus when the user makes the channel sensing decision at\ntime t, it does not have the true state of any channel at\ntime t. Furthermore, upon its action (at time t+ ) only k\nchannels reveal their true states. The user's action space\nat time t is given by the finite set ak (t) \u2282 N , where\nak (t) = {i1 , . . . , iK }.\nWe know (see e.g., [?], [?], [?]) that a sufficient statistic of such a system for optimal decision making, or the\ninformation state of the system [?], [?], is given by the\nconditional probabilities of the state each channel is in given\nall past actions and observations. Since each channel can\nbe in one of two states, we denote this information state\nby \u03c9\u0304(t) = [\u03c91 (t), * * * , \u03c9n (t)] \u2208 [0, 1]n , where \u03c9i (t) is the\nconditional probability that channel i is in state 1 at time t\ngiven all past states, actions and observations 1 . Throughout\nthe paper \u03c9i (t) will be referred to as the information state\nof channel i at time t, or simply the channel probability of\ni at time t.\nDue to the Markovian nature of the channel model, the\nfuture information state is only a function of the current\ninformation state and the current action; i.e., it is independent\nof past history given the current information state and action.\n\nIn the last term, the channel state probability vector consists\nof three parts: a sequence of p01 's that represent those\nchannels sensed to be in state 0 at time t and the length of\nthis sequence is the number of li 's equaling zero; a sequence\nof values \u03c4 (\u03c9j ) for all j 6\u2208 ak ; and a sequence of p11 's that\nrepresent those channels sensed to be in state 1 at time t and\nthe length of this sequence is the number of li 's equaling\none. Note that the future expected reward is calculated by\nsumming over all possible realizations of the k selected\nchannels.\n\n1 Note that it is a standard way of turning a POMDP problem into a classic\nMDP problem by means of the information state, the main implication being\nthat the state space is now uncountable.\n\n2 A Markov policy is a policy that derives its action only depending on\nthe current (information) state, rather than the entire history of states, see\ne.g., [?].\n\n(P):\n\nmax JT\u03c0 (\u03c9\u0304) = max E \u03c0 [\n\u03c0\n\n\u03c0\n\nT\nX\n\n\u03b2 t\u22121 R\u03c0t (\u03c9\u0304(t))|\u03c9\u0304(1) = \u03c9\u0304]\n\nt=1\n\nwhere 0 \u2264 \u03b2 \u2264 1 is the discount factor, and R\u03c0t (\u03c9\u0304(t)) is\nthe reward collected under state \u03c9\u0304(t) when channels in the\nset ak (t) = \u03c0t (\u03c9\u0304(t)) are selected.\nThe maximization in (P) is over the class of deterministic\nMarkov policies 2 . An admissible policy \u03c0, given by the\nvector \u03c0 = [\u03c01 , \u03c02 , * * * , \u03c0T ], is such that \u03c0t specifies a\nmapping from the current information state \u03c9\u0304(t) to a channel\nselection action ak (t) = \u03c0t (\u03c9\u0304(t)) \u2282 {1, 2, * * * , n}. This\nis done without loss of optimality due to the Markovian\nnature of the underlying system, and due to known results\non POMDPs [?, Chapter 6].\nIII. P RELIMINARIES\nThe dynamic programming (DP) representation of problem (P) is given as follows:\nVT (\u03c9\u0304) =\nVt (\u03c9\u0304) =\n\nmax\n\nak \u2208N ,|ak |=k\n\nmax\n\nak \u2208N ,|ak |=k\n\nX\n\nE[Rak (\u03c9\u0304)]\nX\n\u03c9i + \u03b2 *\n(\n\nli \u2208{0,1},i\u2208ak\n\nk\n\ni\u2208a\n\uf8eb\n\uf8f6\nY\n\uf8ed\n\u03c9ili (1 \u2212 \u03c9i )1\u2212li \uf8f8 *\ni\u2208ak\n\nVt+1 (p01 , . . . , p01 , \u03c4 (\u03c9j ), p11 , . . . , p11 )), (2)\nt = 1, 2, * * * , T \u2212 1.\n\n\fThe value function Vt (\u03c9\u0304) represents the maximum expected future reward that can be accrued starting from time\nt when the information state is \u03c9\u0304. In particular, we have\nV1 (\u03c9\u0304) = max\u03c0 JT\u03c0 (\u03c9\u0304), and an optimal deterministic Markov\npolicy exists such that a = \u03c0t\u2217 (\u03c9\u0304) achieves the maximum in\n(3) (see e.g., [?] (Chapter 4)).\nFor simplicity of representation, we introduce the following notations:\n\u2022 p01 [x]: this is the vector [p01 , p01 , * * * , p01 ] of length x;\n\u2022 p11 [x]: this is the vector [p11 , p11 , * * * , p11 ] of length x.\n\u2022 We will use the notation:\n\u0011\nY \u0010\nq(l1 , * * * , lk ) :=\n\u03c9ili (1 \u2212 \u03c9i )1\u2212li\n1\u2264i\u2264k\n\nfor l1 , * * * , lk \u2208 {0, 1}. That is, given a vector of 0s and\n1s (total of k elements), q() is the probability that a set\nof k channels are in states given by the vector.\nWith the above notation, Eqn (3) can be written as\nX\n\u03c9i + \u03b2 *\nVt (\u03c9\u0304) =\nmax (\nak \u2208N ,|ak |=k\n\nX\n\ni\u2208ak\n\nq(l1 , * * * , lk ) *\n\nli \u2208{0,1},i\u2208ak\n\nVt+1 (p01 [k \u2212\n\nX\n\nli ], * * * , \u03c4 (\u03c9j ), p11 [\n\nX\n\nli ]) .\n\nSolving (P) using the above recursive equation can be\ncomputationally heavy, especially considering the fact that\n\u03c9\u0304 is a vector of probabilities. It is thus common to consider\nsuboptimal policies that are easier to compute and implement. One of the simplest such heuristics is a greedy policy\nwhere at each time step we take an action that maximizes\nthe immediate one-step reward. Our focus is to examine the\noptimality properties of such a simple greedy policy.\nFor problem (P), the greedy policy under state \u03c9\u0304 =\n[\u03c91 , \u03c92 , * * * , \u03c9n ] is given by\nX\n\u03c9i .\n(3)\nak (\u03c9\u0304) = arg\nmax\nak \u2282N ,|ak |=k\n\ni\u2208ak\n\nThat is, the greedy policy seeks to maximize the reward as\nif there were only one step remaining in the horizon. In\nthe next section we investigate the optimality of this policy.\nSpecifically, we will show that it is optimal in the case of\np11 \u2265 p01 . This extends the earlier result in [?] that showed\nthis to be true for the special case of k = 1.\nIV. O PTIMALITY OF THE G REEDY P OLICY\nIn this section we show that the greedy policy is optimal\nwhen p11 \u2265 p01 . The main theorem of this section is as\nfollows.\nTheorem 1: The greedy policy is optimal for Problem\n(P) under the assumption that p11 \u2265 p01 . That is, for\nt = 1, 2, * * * , T , k \u2264 n, and \u2200\u03c9\u0304 = [\u03c91 , * * * , \u03c9n ] \u2208 [0, 1]n ,\nwe have\nVtk (\u03c9\u0304; z k (\u03c9\u0304)) \u2265 Vtk (\u03c9\u0304; ak ),\nk\n\n\u2200ak \u2282 N ,\n\n(4)\n\nwhere z (\u03c9\u0304) is the subset whose elements (indices) correspond to the k largest values in \u03c9\u0304, and Vtk (\u03c9\u0304; ak ) the\nexpected value of action ak followed by behaving optimally.\n\nBelow we present a number of lemmas used in the proof\nof this theorem. The first lemma introduces a notation that\nallows us to express the expected future reward under the\ngreedy policy.\nLemma 1: There exist T n-variable functions, denoted by\nWtk (\u03c9\u0304), t = 1, 2, * * * , T , each of which is a polynomial of\norder 13 and can be represented recursively in the following\nform:\nX\nWTk (\u03c9\u0304) =\n\u03c9i\nn\u22121+1\u2264i\u2264n\n\nWtk (\u03c9\u0304)\n\n=\n\nX\n\n\u03c9i + \u03b2 *\n\nn\u22121+1\u2264i\u2264n\n\nX\n\nq(ln , * * * , ln+k\u22121 ) *\n\nln ,ln\u22121 ,*** ,ln+k\u22121 \u2208{0,1}\nk\nWt+1\n(p01 [k \u2212\n\nX\n\nli ], \u03c4 (\u03c9i ), * * * , \u03c4 (\u03c9n\u2212k ), p11 [\n\nX\n\nli ]) .\n\nThe proof is easily obtained using backward induction on\nt given the recursive equation and noting that the mapping\n\u03c4 () is linear. The detailed proof is thus omitted for brevity.\nA few remarks are in order on this function Wtk (\u03c9\u0304).\ni) Firstly, when \u03c9\u0304 is given by an ordered vector\n[\u03c91 , \u03c92 , * * * , \u03c9n ] with \u03c91 \u2264 \u03c92 \u2264 * * * \u2264 \u03c9n , Wtk (\u03c9\u0304) is\nthe expected total discounted future reward (from t to\nT ) by following the greedy policy.\nThis follows from how the greedy policy works in\nthe special case of p11 \u2265 p01 . Note that in this case\nthe conditional probability updating function \u03c4 (\u03c9) is a\nmonotonically increasing function, i.e., \u03c4 (\u03c91 ) \u2265 \u03c4 (\u03c92 )\nfor \u03c91 \u2265 \u03c92 . Therefore the ordering of channel\nprobabilities is preserved among those that are not\nobserved.\nIf a channel has been observed to be in state \"1\"\n(respectively \"0\"), its probability at the next step\nbecomes p11 \u2265 \u03c4 (\u03c9) (respectively p01 \u2264 \u03c4 (\u03c9)) for\nany \u03c9 \u2208 [0, 1]. In other words, a channel observed to\nbe in state \"1\" (respectively \"0\") will have the highest\n(respectively lowest) possible probability among all\nchannels.\nTherefore if we take the initial information state \u03c9\u0304(1),\norder the channels according to their probabilities\n\u03c9i (1), and sense the highest k channels (top k of the\nordered list) with ties broken randomly, then following\nthe greedy policy means that in subsequent steps we\nwill keep a channel in its current position if it was\nsensed to be in state 1 in the previous slot; otherwise,\nit was observed to be in state 0 and gets thrown to the\nbottom of the ordered list. The policy then selects the\nnext top most (or rightmost) k channels on this new\nordered list. This procedure is essentially the same as\nthat given in the recursive expression of W ().\nii) Secondly, when \u03c9\u0304 is not ordered, Wtk () reflects a\npolicy that simply goes down the list of channels by\nthe order fixed in \u03c9\u0304, while each time tossing the ones\n3 Each function W is affine in each variable, when all other variables are\nt\nheld constant.\n\n\fobserved to be 0 to the end of the list and keeing those\nobserved to be 1 at the top of the list.\niii) Thirdly, the fact that WtK is a polynomial of order 1\nand affine in each of its elements implies that\nWtK (\u03c91 , * * * , \u03c9n\u22122 , y, x)\n\u2212WtK (\u03c91 , * * * , \u03c9n\u22122 , x, y)\n= (x \u2212 y)[WtK (\u03c91 , * * * , \u03c9n\u22122 , 0, 1) \u2212\nWtK (\u03c91 , * * * , \u03c9n\u22122 , 1, 0)] .\nSimilar results hold when we change the positions of x\nand y. To see this, consider the above as two functions\nof x and y, each having an x term, a y term, an xy\nterm and a constant term. Since we are only swapping\nthe positions of x and y in these two functions, the\nconstant term remains the same, and so does the xy\nterm. Thus the only difference is the x term and the\ny term, as given in the above equation. This linearity\nresult is used later in our proof.\nThe next lemma establishes a sufficient condition for the\noptimality of the greedy policy.\nLemma 2: Consider Problem (P) under the assumption\nthat p11 \u2265 p01 . To show that the greedy policy is optimal\nat time t given that it is optimal at t + 1, t + 2, * * * , T , it\nsuffices to show that at time t we have\nWtk (\u03c91 , * * * , \u03c9j , x, y, * * * , \u03c9n )\n\u2264\n\nWtk (\u03c91 , * * * , \u03c9j , y, x, * * * , \u03c9n ),\n\n(5)\n\nfor all x \u2265 y and all 0 \u2264 j \u2264 n \u2212 2, with j = 0 implying\nWtk (x, y, \u03c93 , * * * , \u03c9n ) \u2264 Wtk (y, x, \u03c93 , * * * , \u03c9n ).\nProof: Since the greedy policy is optimal from t + 1\non, it is sufficient to show that selecting the best k channels\nfollowed by the greedy policy is better than selecting any\nother set of k channels followed by the greedy policy. If\nchannels are ordered \u03c91 \u2264 * * * \u2264 \u03c9i \u2264 * * * \u2264 \u03c9n then the\nreward of the former is precisely given by WtK (\u03c91 , . . . , \u03c9n ).\nOn the other hand, the reward of selecting an arbitrary set ak\nof k channels followed by acting greedily can be expressed\nas Wtk (ak , ak ), where ak is the (increasingly) ordered set of\nchannels not included in ak . It remains to show that if Eqn\n(5) is true then we have Wtk (ak , ak ) \u2264 WtK (\u03c91 , . . . , \u03c9n ).\nThis is easily done since the ordered list (ak , ak ) may be\nconverted to \u03c91 , . . . , \u03c9n through a sequence of switchings\nbetween two neighboring elements that are not increasingly\nordered. Each such switch invokes (5), thereby maintaining\nthe \"\u2264\" relationship.\nLemma 3: For 0 \u2264 \u03c91 \u2264 \u03c92 \u2264 . . . \u2264 \u03c9n \u2264 1, we have\nthe following two inequalities for all t = 1, 2, * * * , T :\n(A) :\n(B) :\n\n1 + Wtk (\u03c92 , * * * , \u03c9n , \u03c91 ) \u2265 Wtk (\u03c91 , * * * , \u03c9n )\nWtk (\u03c91 , * * * , \u03c9j , y, x, \u03c9j+3 , * * * , \u03c9n ) \u2265\nWtk (\u03c91 , * * * , x, y, \u03c9j+3 , * * * , \u03c9n ),\n\nwhere x \u2265 y, 0 \u2264 j \u2264 n \u2212 2, and j = 0 implies\nWtk (y, x, \u03c93 , * * * , \u03c9n ) \u2265 Wtk (x, y, \u03c93 , * * * , \u03c9n ).\n\nThis lemma is the key to our main result and its proof,\nwhich uses a sample path argument, highly instructive. It is\nhowever also lengthy, and for this reason has been relegated\nto the Appendix.\nWith the above lemmas, Theorem 1 is easily proven:\nProof of Theorem 1: We prove by induction on T . When\nt = T , the greedy policy is obviously optimal. Suppose\nit is also optimal for all times t + 1, t + 2, * * * , T , under\nthe assumption p11 \u2265 p01 . Then at time t, by Lemma 2,\nit suffices to show that Wtk (\u03c91 , * * * , \u03c9j , x, y, * * * , \u03c9n ) \u2264\nWtk (\u03c91 , * * * , \u03c9j , y, x, * * * , \u03c9n ) for all x \u2265 y and 0 \u2264 j \u2264\nn \u2212 2. But this is proven in Lemma 3.\nV. D ISCUSSION\nWhile the formulation (P) is a finite horizon problem,\nthe same result applies to the infinite horizon discounted\nreward case using standard techniques as we have done in\nour previous work [?], [?].\nIn the case of infinite horizon, the problem studied in this\npaper is closely associated with the class of multi-armed\nbandit problems [?] and restless bandit problems [?]. This is\na class of problems where n controlled Markov chains (also\ncalled machines or arms) are activated (or played) one at a\ntime. A machine when activated generates a state dependent\nreward and moves to the next state according to a Markov\nrule. A machine not activated either stays frozen in its current\nstate (a rested bandit) or moves to the next state according\nto a possibly different Markov rule (a restless bandit). The\nproblem is to decide the sequence in which these machines\nare activated so as to maximize the expected (discounted or\naverage) reward over an infinite horizon.\nThe multi-armed bandit problem was originally solved\nby Gittins (see [?]), who showed that there exists an index\nassociated with each machine that is solely a function of that\nindividual machine and its state, and that playing the machine\ncurrently with the highest index is optimal. This index has\nsince been referred to as the Gittins index. The remarkable\nnature of this result lies in the fact that it decomposes the\nn-dimensional problem into n 1-dimensional problems, as\nan index is defined for a machine independent of others.\nThe restless bandit problem on the other hand was proven\nmuch more complex, and is PSPACE-hard in general [?].\nRelatively little is known about the structure of its optimal\npolicy in general. In particular, the Gittins index policy is\nnot in general optimal [?].\nWhen multiple machines are activated simultaneously, the\nresulting problem is referred to as multi-armed bandits with\nmultiple plays. Again optimal solutions to this class of\nproblems are not known in general. A natural extension to\nthe Gittins index policy in this case is to play the machines\nwith the highest Gittins indices (this will be referred to as the\nextended Gittins index policy below). This is not in general\noptimal for multi-armed bandits with multiple plays and an\ninfinite horizon discounted reward criterion, see e.g., [?],\n[?]. However, it may be optimal in some cases, see e.g.,\n[?] for conditions on the reward function, and [?] for an\n\n\fundiscounted case where the Gittins index is always achieved\nat time 1. Even less is known when the bandits are restless,\nthough asymptotic results for restless bandits with multiple\nplays were provided in [?] and [?].\nThe problem studied in the present paper is an instance\nof the restless bandits with multiple plays (in the infinite\nhorizon case). Therefore what we have shown in this paper\nis an instance of the restless bandits problem with multiple\nplays, for which the extended Gittins index policy is optimal.\nVI. C ONCLUSION\n\nA PPENDIX\nProof of Lemma 3: We would like to show\n(A) :\n\n1+\n\n(B) :\n\nWtk (\u03c91 , * * *\nWtk (\u03c91 , * * *\n\n, \u03c9n , \u03c91 ) \u2265\n\n{LHS|(0,1) }\nX\n= 1+\n\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 [k \u2212\n\nIn this paper we studied a stochastic control problem that\narose in opportunistic spectrum access. A user can sense\nand access k out of n channels at a time and must select\njudiciously in order to maximize its reward. We extend a\nprevious result where a greedy policy was shown to be\noptimal in the special case of k = 1 under the condition that\nthe channel state transitions are positively correlated over\ntime. In this paper we showed that under the same condition\nthe greedy policy is optimal for the general case of k \u2265 1.\nThis result also contributes to the understanding of the class\nof restless bandit problems with multiple plays.\n\nWtk (\u03c92 , * * *\n\nConditioned on this realization, the LHS and RHS\nare evaluated as follows (denoted as {LHS|(0,1)} and\n{RHS|(0,1) }, respectively):\n\nWtk (\u03c91 , * * *\n\nX\n\nli ], \u03c4 (\u03c92 ), * * * ,\nX\n\u03c4 (\u03c9n\u2212k+1 ) = p11 , p11 [\nli ]) ;\n\n{RHS|(0,1) }\nX\n= 1+\n\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 [k \u2212\n\nX\n\nli \u2212 1], \u03c4 (\u03c91 ) = p00 ,\nX\n\u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ), p11 [\nli + 1])\n\n= {LHS|(0,1) }\n, \u03c9n )\n\n, \u03c9j , y, x, \u03c9j+3 , * * * , \u03c9n ) \u2265\n, x, y, \u03c9j+3 , * * * , \u03c9n ),\n\nwhere x \u2265 y, 0 \u2264 j \u2264 n \u2212 2, and j = 0 implies\nWtk (y, x, \u03c93 , * * * , \u03c9n ) \u2265 Wtk (x, y, \u03c93 , * * * , \u03c9n ).\nThe two inequalities (A) and (B) will be shown together\nusing an induction\nP on t. For t = T , part (A)Pis true because\nLHS = 1+\u03c91+ ni=n\u2212k+2 \u03c9i \u2265 \u03c9n\u2212k+1 + ni=n\u2212k+2 \u03c9i =\nRHS. Part (B) is obviously true for t = T since x \u2265 y.\nSuppose (A) and (B) are both true for t + 1, * * * , T .\nConsider time t, and we will prove (A) first. Note that in\nthe next step, channel 1 is selected by the action on the LHS\nof (A) but not by the RHS, while channel n \u2212 k + 1 is\nselected by the RHS of (A) but not by the LHS. Other than\nthis difference both sides select the same set of channels\nindexed n \u2212 k + 2, * * * , n. We now consider four possible\ncases in terms of the realizations of channels 1 and n\u2212k +1.\nCase (A.1): channels 1 and n \u2212 k + 1 have the state\nrealizations \"0\" and \"1\", respectively.\nWe will use a sample-path argument. Note that while\nthese two channels are not both observed by either side, the\nrealizations hold for the underlying sample path regardless.\nIn particular, even though the LHS does not select channel\nn \u2212 k + 1 and therefore does not get to actually observe the\nrealization of \"1\", the fact remains that channel n \u2212 k + 1\nis indeed in state 1 under this realization, and therefore its\nfuture expected reward must reflect this. It follows that under\nthis realization channel n \u2212 k + 1 will have probability p11\nfor the next time step even though we did not get to observe\nthe state 1. The same is true for the RHS. This argument\napplies to the other three cases and is thus not repeated.\n\nCase (A.2): channels 1 and n \u2212 1 + 1 have the state\nrealizations \"1\" and \"1\", respectively.\n{LHS|(1,1) }\nX\n= 1+1+\n\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 [k \u2212\n\nX\n\nli \u2212 1], \u03c4 (\u03c92 ), * * * ,\nX\n\u03c4 (\u03c9n\u2212k+1 ) = p11 , p11 [\nli + 1]) ;\n\n{RHS|(1,1) }\nX\n= 1+\n\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 [k \u2212\n\nX\n\nli \u2212 1], \u03c4 (\u03c91 ) = p11 ,\nX\n\u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ), p11 [\nli + 1])\nX\n\u2264 1+\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\nX\nk\nWt+1\n(p01 [k \u2212\nli \u2212 1], \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\nX\np11 , p11 [\nli + 1])\n= {LHS|(1,1) } \u2212 1 \u2264 {LHS|(1,1) }\n\n\fwhere the first inequality is due to the induction hypothesis\nof (B).\nCase (A.3): channels 1 and n \u2212 1 + 1 have the state\nrealizations \"0\" and \"0\", respectively.\n\nrealizations \"1\" and \"0\", respectively.\n\n=\n\n{RHS|(1,0)}\nX\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\n=\n\n{RHS|(0,0)}\nX\n\u03c9i + \u03b2 *\n\nX\n\nk\nWt+1\n(p01 [k \u2212\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\nli ], \u03c4 (\u03c91 ) = p11 , \u03c4 (\u03c92 ), * * * ,\nX\n\u03c4 (\u03c9n\u2212k ), p11 [\nli ])\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\nX\nk\nWt+1\n(p01 [k \u2212\nli ], \u03c4 (\u03c91 ) = p01 , \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\nX\np11 [\nli ]) ;\n\nX\n\n{LHS|(1,0) }\nX\n= 1+1+\n\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\n=\n\nX\n\n{LHS|(0,0)}\nX\n1+\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 [k \u2212\n\n\u2265\n\nX\n\nli ], \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\nX\n\u03c4 (\u03c9n\u2212k+1 ) = p01 , p11 [\nli ])\nX\n1+\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\n\u2265\n\nX\nk\nWt+1\n(p01 [k \u2212\nli ], \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\nX\np11 [\nli ], p01 )\nX\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\n\u2265\n\n\u0010\nX\nk\n1 + Wt+1\n(p01 [k \u2212\nli ], \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\n\u0011\nX\np11 [\nli ], p01 )\nX\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 , p01 [k \u2212\nX\np11 [\nli ])\n\n=\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nX\n\nli ], \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\n\n{RHS|(0,0)}\n\nwhere the first inequality is due to the induction hypothesis of\n(B), the last inequality due to the induction hypothesis of (A).\nAlso, the second inequality utilizes the total probability over\nthe distribution q(ln\u2212k+2 , * * * , ln ) and the fact that \u03b2 \u2264 1.\nCase (A.4): channels 1 and n \u2212 1 + 1 have the state\n\nk\nWt+1\n(p01 [k \u2212\n\nX\n\nli \u2212 1], \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\nX\n\u03c4 (\u03c9n\u2212k+1 ) = p01 , p11 [\nli + 1])\nX\n\u2265 1+1+\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\nX\nk\nWt+1\n(p01 [k \u2212\nli \u2212 1], \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\nX\np11 [\nli + 1], p01 )\nX\n\u2265 1+\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\n\u0010\n\nX\nk\n1 + Wt+1\n(p01 [k \u2212\nli \u2212 1], \u03c4 (\u03c92 ), * * * ,\n\u0011\nX\n\u03c4 (\u03c9n\u2212k ), p11 [\nli + 1], p01 )\nX\n\u2265 1+\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\nX\nk\nWt+1\n(p01 [k \u2212\nli ], \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\nX\np11 [\nli + 1]\nX\n\u2265 1+\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 [k \u2212\nX\np11 [\nli ])\n\nX\n\nli ], p11 , \u03c4 (\u03c92 ), * * * , \u03c4 (\u03c9n\u2212k ),\n\n= 1 + {RHS|(1,0) } \u2265 {RHS|(1,0) }\nwhere the first and last inequalities are due to the induction\nhypothesis of (B), the third due to the induction hypothesis\n\n\fof (A).\n\nHowever, we have\n\nWith these four cases, we conclude the induction step of\nproving (A). We next prove the induction step of (B). We\nconsider three cases in terms of whether x and y are among\nthe top k channels to be selected in the next step.\n\n=\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nCase (B.1): both x and y belong to the top k positions on\nboth sides. In this case there is no difference between the\nLHS and RHS along each sample path, since both channels\nwill be selected and the result will be the same.\nCase (B.2): neither x nor y is among the top k positions\non either side. This implies that j \u2264 n \u2212 k \u2212 2. We have:\n\nWtk (\u03c91 , * * * , \u03c9n\u2212k\u22121 , 1, 0, \u03c9n\u2212k+2 , * * * , \u03c9n )\nX\n\u03c9i + \u03b2 *\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\n\u2264\n\nX\nk\nWt+1\n(p01 [k \u2212\nli ], \u03c4 (\u03c91 ), * * * , \u03c4 (\u03c9n\u2212k\u22121 ),\nX\np11 , p11 [\nli ])\nX\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\n=\n\nLHS\nX\n\n\u0010\n\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\n\u2264\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 [k\n\n\u2212\n\nk\n1 + Wt+1\n(p01 [k \u2212\n\nX\n\nli \u2212 1], \u03c4 (\u03c91 ), * * * ,\n\u0011\nX\n\u03c4 (\u03c9n\u2212k\u22121 ), p11 [\nli + 1], p01 )\nX\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nX\n\nli ], \u03c4 (\u03c91 ), * * * , \u03c4 (\u03c9j ),\nX\n\u03c4 (y), \u03c4 (x), \u03c4 (\u03c9j+3 ), * * * , p11 [\nli ]) ;\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\n\u0010\n\n\u2264\n\nk\n1 + Wt+1\n(p01 [k \u2212\n\nX\n\nli \u2212 1], \u03c4 (\u03c91 ), * * * ,\n\u0011\nX\n\u03c4 (\u03c9n\u2212k\u22121 ), p01 , p11 [\nli + 1])\nX\n1+\n\u03c9i + \u03b2 *\nn\u2212k+2\u2264i\u2264n\n\nX\n\n=\n\nRHS\nX\n\nX\nk\nWt+1\n(p01 [k \u2212\nli \u2212 1], \u03c4 (\u03c91 ), * * * , \u03c4 (\u03c9n\u2212k\u22121 ),\nX\np01 , p11 [\nli + 1])\n\n\u03c9i + \u03b2 *\n\nn\u2212k+2\u2264i\u2264n\n\nX\n\nq(ln\u2212k+2 , * * * , ln ) *\n\n=\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\nk\nWt+1\n(p01 [k \u2212\n\n\u2265 LHS\n\nwhere the last inequality is due to the monotonicity of \u03c4 ()\nand the induction hypothesis of (B).\nCase (B.3): exactly one of the two belongs to the the top\nk channels on each side. This implies that j = n \u2212 k \u2212 1.\nBy the linearity of the function Wtk we have the following:\n\nWtk (\u03c91 , * * * , \u03c9n\u2212k\u22121 , y, x, \u03c9n\u2212k+2 , * * * , \u03c9n )\n\u2212Wtk (\u03c91 , * * * , \u03c9n\u2212k\u22121 , x, y, \u03c9n\u2212k+2 , * * * , \u03c9n )\n(x \u2212 y)(Wtk (\u03c91 , * * * , \u03c9n\u2212k\u22121 , 0, 1, \u03c9n\u2212k+2 , * * * , \u03c9n ) \u2212\nWtk (\u03c91 , * * * , \u03c9n\u2212k\u22121 , 1, 0, \u03c9n\u2212k+2 , * * * , \u03c9n ))\n\nWtk (\u03c91 , * * * , \u03c9n\u2212k\u22121 , 0, 1, \u03c9n\u2212k+2 , * * * , \u03c9n )\n\nSince x \u2265 y, we have LHS \u2265 RHS in Eqn (6). This\nconcludes the induction step of (B).\n\nX\n\nli ], \u03c4 (\u03c91 ), * * * , \u03c4 (\u03c9j ),\nX\n\u03c4 (x), \u03c4 (y), \u03c4 (\u03c9j+3 ), * * * , p11 [\nli ])\n\n=\n\nq(ln\u2212k+2 , * * * , ln ) *\n\nln\u2212k+2 ,*** ,ln \u2208{0,1}\n\n(6)\n\n\f"}