{"id": "http://arxiv.org/abs/1204.0819v3", "guidislink": true, "updated": "2012-08-31T21:52:36Z", "updated_parsed": [2012, 8, 31, 21, 52, 36, 4, 244, 0], "published": "2012-04-03T22:04:31Z", "published_parsed": [2012, 4, 3, 22, 4, 31, 1, 94, 0], "title": "Numerical Analysis of Parallel Replica Dynamics", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.4166%2C1204.4923%2C1204.4882%2C1204.1374%2C1204.1611%2C1204.6713%2C1204.0460%2C1204.6408%2C1204.2656%2C1204.4287%2C1204.1666%2C1204.3283%2C1204.3911%2C1204.6581%2C1204.5516%2C1204.3929%2C1204.0732%2C1204.2383%2C1204.3944%2C1204.0470%2C1204.0352%2C1204.3459%2C1204.5108%2C1204.2675%2C1204.5753%2C1204.0099%2C1204.0681%2C1204.4493%2C1204.4645%2C1204.6481%2C1204.3077%2C1204.0926%2C1204.1593%2C1204.3708%2C1204.3698%2C1204.3467%2C1204.5301%2C1204.1749%2C1204.3636%2C1204.1071%2C1204.3376%2C1204.3086%2C1204.1339%2C1204.4065%2C1204.0932%2C1204.0232%2C1204.1515%2C1204.6003%2C1204.0495%2C1204.0756%2C1204.5830%2C1204.5897%2C1204.0093%2C1204.5428%2C1204.3305%2C1204.2164%2C1204.1430%2C1204.0819%2C1204.2852%2C1204.3035%2C1204.4763%2C1204.1037%2C1204.0223%2C1204.5652%2C1204.2527%2C1204.4462%2C1204.3730%2C1204.1362%2C1204.4433%2C1204.2930%2C1204.3661%2C1204.3830%2C1204.0213%2C1204.2266%2C1204.4325%2C1204.6602%2C1204.0095%2C1204.1495%2C1204.4303%2C1204.0973%2C1204.4603%2C1204.0297%2C1204.5380%2C1204.1460%2C1204.0077%2C1204.5774%2C1204.1395%2C1204.5182%2C1204.4990%2C1204.0910%2C1204.0245%2C1204.6526%2C1204.5154%2C1204.5737%2C1204.5257%2C1204.0744%2C1204.6500%2C1204.3906%2C1204.5071%2C1204.1829%2C1204.6146&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Numerical Analysis of Parallel Replica Dynamics"}, "summary": "Parallel replica dynamics is a method for accelerating the computation of\nprocesses characterized by a sequence of infrequent events. In this work, the\nprocesses are governed by the overdamped Langevin equation. Such processes\nspend much of their time about the minima of the underlying potential,\noccasionally transitioning into different basins of attraction. The essential\nidea of parallel replica dynamics is that the exit time distribution from a\ngiven well for a single process can be approximated by the minimum of the exit\ntime distributions of $N$ independent identical processes, each run for only\n1/N-th the amount of time.\n  While promising, this leads to a series of numerical analysis questions about\nthe accuracy of the exit distributions. Building upon the recent work in Le\nBris et al., we prove a unified error estimate on the exit distributions of the\nalgorithm against an unaccelerated process. Furthermore, we study a dephasing\nmechanism, and prove that it will successfully complete.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.4166%2C1204.4923%2C1204.4882%2C1204.1374%2C1204.1611%2C1204.6713%2C1204.0460%2C1204.6408%2C1204.2656%2C1204.4287%2C1204.1666%2C1204.3283%2C1204.3911%2C1204.6581%2C1204.5516%2C1204.3929%2C1204.0732%2C1204.2383%2C1204.3944%2C1204.0470%2C1204.0352%2C1204.3459%2C1204.5108%2C1204.2675%2C1204.5753%2C1204.0099%2C1204.0681%2C1204.4493%2C1204.4645%2C1204.6481%2C1204.3077%2C1204.0926%2C1204.1593%2C1204.3708%2C1204.3698%2C1204.3467%2C1204.5301%2C1204.1749%2C1204.3636%2C1204.1071%2C1204.3376%2C1204.3086%2C1204.1339%2C1204.4065%2C1204.0932%2C1204.0232%2C1204.1515%2C1204.6003%2C1204.0495%2C1204.0756%2C1204.5830%2C1204.5897%2C1204.0093%2C1204.5428%2C1204.3305%2C1204.2164%2C1204.1430%2C1204.0819%2C1204.2852%2C1204.3035%2C1204.4763%2C1204.1037%2C1204.0223%2C1204.5652%2C1204.2527%2C1204.4462%2C1204.3730%2C1204.1362%2C1204.4433%2C1204.2930%2C1204.3661%2C1204.3830%2C1204.0213%2C1204.2266%2C1204.4325%2C1204.6602%2C1204.0095%2C1204.1495%2C1204.4303%2C1204.0973%2C1204.4603%2C1204.0297%2C1204.5380%2C1204.1460%2C1204.0077%2C1204.5774%2C1204.1395%2C1204.5182%2C1204.4990%2C1204.0910%2C1204.0245%2C1204.6526%2C1204.5154%2C1204.5737%2C1204.5257%2C1204.0744%2C1204.6500%2C1204.3906%2C1204.5071%2C1204.1829%2C1204.6146&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Parallel replica dynamics is a method for accelerating the computation of\nprocesses characterized by a sequence of infrequent events. In this work, the\nprocesses are governed by the overdamped Langevin equation. Such processes\nspend much of their time about the minima of the underlying potential,\noccasionally transitioning into different basins of attraction. The essential\nidea of parallel replica dynamics is that the exit time distribution from a\ngiven well for a single process can be approximated by the minimum of the exit\ntime distributions of $N$ independent identical processes, each run for only\n1/N-th the amount of time.\n  While promising, this leads to a series of numerical analysis questions about\nthe accuracy of the exit distributions. Building upon the recent work in Le\nBris et al., we prove a unified error estimate on the exit distributions of the\nalgorithm against an unaccelerated process. Furthermore, we study a dephasing\nmechanism, and prove that it will successfully complete."}, "authors": ["Gideon Simpson", "Mitchell Luskin"], "author_detail": {"name": "Mitchell Luskin"}, "author": "Mitchell Luskin", "arxiv_comment": "37 pages, 4 figures, revised and new estimates from the previous\n  version", "links": [{"href": "http://arxiv.org/abs/1204.0819v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.0819v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.mtrl-sci", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "65C05, 82C31, 60H35", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.0819v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.0819v3", "journal_reference": null, "doi": null, "fulltext": "arXiv:1204.0819v3 [math.NA] 31 Aug 2012\n\nNUMERICAL ANALYSIS OF PARALLEL REPLICA\nDYNAMICS\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nAbstract. Parallel replica dynamics is a method for accelerating the computation of processes characterized by a sequence of\ninfrequent events. In this work, the processes are governed by the\noverdamped Langevin equation. Such processes spend much of\ntheir time about the minima of the underlying potential, occasionally transitioning into different basins of attraction. The essential\nidea of parallel replica dynamics is that the exit time distribution\nfrom a given well for a single process can be approximated by the\nminimum of the exit time distributions of N independent identical\nprocesses, each run for only 1/N -th the amount of time.\nWhile promising, this leads to a series of numerical analysis\nquestions about the accuracy of the exit distributions. Building\nupon the recent work in [4], we prove a unified error estimate on\nthe exit distributions of the algorithm against an unaccelerated\nprocess. Furthermore, we study a dephasing mechanism, and prove\nthat it will successfully complete.\n\n1. Introduction\nParallel replica dynamics (ParRep) is a numerical tool first introduced by Voter in [26] (see also [22,27]) for accelerating the simulation\nof stochastic processes characterized by a sequence of infrequent, but\nrapid, transitions from one state to another. A standard and important\nproblem in which such a separation of scales is present is the migration\nof defects through a crystalline lattice; see [22] and references therein\nfor examples.\nRoughly, the idea behind parallel replica dynamics is as follows. Suppose a trajectory spends time t in a particular state, before transitioning into another. Furthermore, assume t is large, relative to the scale\nof the time step discretization. We wish to avoid directly simulating\na single realization for time t. We approximate the simulation of a\nsingle trajectory for time t with N independent copies, each simulated\nfor time t/N , and follow the particular trajectory that escapes first.\nDate: October 24, 2018.\n1\n\n\f2\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nThis holds out the promise for a linear speedup with the number of\nindependent realizations we are able to simulate.\nOf course, this is not exact, and error is introduced. A particular\nconcern is error in the exit distributions of the system as it migrates\nfrom one state to another \u2013 does ParRep disrupt the state to state\ndynamics? Inspired by the tools proposed in [4], we prove an error\nestimate on the exit distributions over a single \"cycle\" of ParRep (the\ntransition from one state to the next).\n1.1. The Algorithm. We assume the system we wish to accelerate\nevolves according to the overdamped Langevin equation,\np\n(1.1)\ndXt = \u2212\u2207V (Xt )dt + 2\u03b2 \u22121 dBt , Xt \u2208 Rn ,\nwhere Bt is a Wiener process and \u03b2 is proportional to inverse temperature. Though ParRep was originally developed for the Langevin\nequations, it is readily adapted to this problem.\nWe next assume that our system is such that V has a denumerable\nset of local minima, xj , j = 1, 2, . . . For each minima, we associate a\nset Wj \u2282 Rn , the \"well.\" Wj could be the basin of attraction of xj ; if\ny(t) solves the ODE\n\u1e8f = \u2212\u2207V (y),\n\ny(0) = y0 \u2208 Rn ,\n\nthen\n\u001a\n\u001b\nWj = y0 : lim y(t) = xj .\nt\u2192+\u221e\n\nHowever, this definition is not essential; for the sake of our analysis, Wj\nneed only be a bounded set in Rn with sufficiently regular boundary.\nThis motivates defining the well selection function,\n(1.2)\n\nS : Rn \u2192 N,\n\nwhich identifies the basin associated with a given position. Associated\nwith this is the \"coarse grained\" trajectory,\n(1.3)\n\nSt \u2261 S(Xt )\n\nwhich only identifies the present well.\nIf the wells are \"deep\" with well-defined minima, then Xt will infrequently transition from one to another. Such a well corresponds to a\nmetastable state. Much of the simulation time will be spent waiting for\na jump to occur. The goal of ParRep is to reduce this computational\nexpense by providing a satisfactory approximation of the form\n(1.4)\n\nSt \u2248 StParRep .\n\n\ftsim\n\nWj\nXtk?\n\nXt1\n\nXtN\n\nXt2\n\nT ? = T k?\n\ntcorr + T ? Clock Time\n\n3\n\nXtref\n\nx0\n\ntcorr\n\ntcorr\n\ntsim + tcorr\n\ntsim + tcorr + N T ?\n\nLab Time\n\nNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\nx\n\nFigure 1. An illustration of the decorrelation and parallel steps of the ParRep algorithm in the case that the\nreference walker never leaves well Wj . Xtk? is the first\nprocess to exit the well, doing so at the computer time\ntcorr +T ? . This is then translated into the lab, or physical,\ntime tsim + tcorr + N T ? . See Figure 2 for an illustration\nof a dephasing step.\nIn other words, we are willing to sacrifice information about where the\ntrajectory is within each well, for the sake of rapidly computing the\nsequence of wells the trajectory visits.\nWe now describe the ParRep algorithm in the following steps: the\ndecorrelation step; the dephasing step; and the parallel step. These\nsteps are diagrammed in Figures 1 and 2. We assume that the reference\nprocess Xtref enters well Wj at time tsim .\nA. Decorrelation Step: Let Xtref evolve under (1.1) for tsim \u2264 t \u2264\ntsim + tcorr .\n\u2022 If\nS(Xtref ) = S(Xtref\n)\nsim\n\n\fGIDEON SIMPSON AND MITCHELL LUSKIN\n\nWj\nXtk Xt1 Xt2 XtN\ntphase\n\n4\n\nXt2\n\nx1\n\nx\n\nFigure 2. An illustration of a dephasing step for the\nParRep algorithm. In this implementation, the replicas\nall start from the same position; \u03bc0phase = \u03b4x1 . When Xt2\nleaves before tphase , it is relaunched from the same position.\n\nfor all tsim \u2264 t \u2264 tsim + tcorr , then time advances to tsim + tcorr\nand proceed.\n\u2022 Otherwise, denote the first exit time from the well,\n\b\nT = inf t | S(Xtref\n) 6= S(Xtref\n)\nsim +t\nsim\nand time advances to tsim + T . Return to the beginning of\nthe decorrelation step in the new well.\nB. Dephasing Step: In conjunction with the decorrelation step, we\nlaunch N replicas with starting positions drawn from distribution\n\u03bc0phase . These are run for tphase amount of time, the dephasing\ntime. If at any time before tphase a replica leaves the well, it is\nrestarted. A replica has successfully dephased if it remains in the\nwell for all of tphase .\nAt the completion of the decorrelation and dephasing steps,\nassuming the reference walker has not exited, we have N independent walkers with the same distribution. We discard the\nreference process. If at any time during the dephasing process\nthe reference walker leaves the well, the dephasing process terminates and the replicas are discarded.\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n5\n\nC. Parallel Step: We now let the N replicas evolve independently\nand define\n(1.5a)\n(1.5b)\n(1.5c)\n\nk? = argmin T k ,\nXt?\n?\n\n=\n\nk\nk?\nXt ,\nk?\n\nT =T .\n\nThe system advances to the next well:\n(1.6a)\n\ntsim 7\u2192 tsim + tcorr + N T ?\n\n(1.6b)\n\n?\nXtref\n? = XT ? .\nsim +tcorr +N T\n\nFinally, we return to the decorrelation step.\nThis is a different dephasing algorithm than described in [4]. There,\nafter the decorrelation step, the replicas are initiated at the the position\nof the reference process and run for tphase . The simulation clock is not\nadvanced, and replicas are replaced as need be should they exit the\nwell. Our implementation has the advantage that no processor sits\nidle.\nThe reader may wonder why we would want to have a distinguished\nreference process \u2013 why not relaunch the reference process, as we would\na replica, should it exit? We retain this feature to allow for realizations\nwhere the process is in a well for a very short period, far less than the\ndecorrelation time. These correlated events, such as recrossings, appear\nin serial simulations and should be preserved. One may also ask why\nwe discard the reference process. This is to simplify the analysis, as\nit permits us to declare that the N replicas are drawn from the same\ndistribution when the parallel step begins.\nIn addition to the choice of tcorr and tphase , there is also the question\nof what \u03bc0phase should be. Again, there is significant flexibility. One\npossibility is to allow the reference process to evolve for some amount\nof time, and then the replicas could be launched from its position. A\nmethod used in practice is to find a local minima associated with the\nwell, and initiate the replicas from that position, [21]. We emphasize\nthat the dephasing mechanism need not depend on any information\nassociated with the reference process.\nIn principle, ParRep offers a nearly linear speedup with the number\nof independent replicas, provided tcorr is short relative to the typical exit\ntime. With the explosion in the availability of distributed computing\nclusters, parallel replica dynamics is an attractive tool for studying\ninfrequent event processes.\n\n\f6\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\n1.2. Main Results. The essential aspects of a process undergoing infrequent events are\n\u2022 How often does it transition from one state to another?\n\u2022 What state does it transition to?\nThese properties are captured in St . To assess how well StParRep approximates it, we are motivated to first consider the exit distribution\nof a process, and how well it is preserved. In [4], the authors proposed\na rigorous framework in which to study ParRep. The purpose of this\nstudy is to unify those ideas and assess the total error, over a single\ncycle of ParRep, as a function of the parameters.\nNote: For brevity, we shall now take tsim = 0 and Wj = W .\nThroughout our paper, we shall assume:\n\u2022 W \u2282 Rn is bounded;\n\u2022 \u2202W is sufficiently smooth;\n\u2022 V is sufficiently smooth on W .\nThough W need not correspond to a basin of attraction, we shall continue to call it a well.\nTo motivate our results, we introduce some important objects. Let\n\u03bct denote the law of Xt , conditioned on having not left the well:\n(1.7)\n\nP\u03bc0 [Xt \u2208 A, T > t]\n\u03bct (A) = P [Xt \u2208 A | T > t] =\n.\nP\u03bc0 [T > t]\n\u03bc0\n\nThe above expression is the probability of finding the processes, Xt , in\nthe set A \u2282 W , at time t, conditioned on the exit time from the well,\nT , being beyond t, and X0 being initially distributed by \u03bc0 . Additional\ndetails on our notation are given below, in Section 1.4. Under certain\nassumptions, the limit\n(1.8)\n\nlim \u03bct = \u03bd,\n\nt\u2192\u221e\n\nexists. \u03bd is the quasistationary distribution (QSD) and characterizes\nthe long term survivors of (1.1) in well W . The properties of \u03bd are\nreviewed for the reader below in Section 2.\nIn the following theorems, we shall refer to \"admissible distributions.\" This class is quite broad and includes the Dirac distribution.\nIt is defined and explored in subsequent sections. First, we have the\nfollowing result on the convergence of the exit distribution of Xt .\nTheorem 1.1 (Convergence to the QSD). Assume \u03bc0 is admissible.\nThere exist positive constants \u03bb2 > \u03bb1 , C and t, such that for all t \u2265 t\nand bounded and measurable f (\u03c4, \u03be) : R+ \u00d7 \u2202W \u2192 R we have\n|E\u03bct [f (T, XT )] \u2212 E\u03bd [f (T, XT )]| \u2264 Ckf kL\u221e e\u2212(\u03bb2 \u2212\u03bb1 )t .\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n7\n\nThe constant C is independent of t and f .\nTaking t sufficiently large so as to make this small corresponds to\nthe satisfactory completion of the decorrelation step; this reflects (1.8).\nWe give a more precise statement of this theorem at the beginning of\nSection 3, after introducing some additional notation in Section 2. This\nresult also plays a role in studying the dephasing step. The constants\nC and t depend on \u03bc0 , V, and the geometry of the well. We will use\nthe notation Cphase and Ccorr , and tphase and tcorr to distinguish the\nconstants induced by the dephasing and decorrelation steps.\nThe next result ensures that the dephasing step terminates successfully:\nTheorem 1.2 (Dephasing Process). For an admissible distribution\n\u03bc0phase and tphase \u2265 tphase :\nA. Dephasing produces N independent replicas with distributions \u03bcphase ;\nB. Given any \u000f > 0, by taking tphase \u2265 tphase ,\n\u0002\n\u0003\nE\u03bcphase f (T k , XTk k ) \u2212 E\u03bd [f (T, XT )] \u2264 Cphase e\u2212(\u03bb2 \u2212\u03bb1 )tphase kf kL\u221e\nC. The expected number of times a replica is relaunched is finite.\nNext, the error in the parallel step cascading from the dephasing step\ncan be controlled:\nTheorem 1.3 (Parallel Error). Given tphase \u2265 tphase , let\n\u000fphase \u2261 Cphase e\u2212(\u03bb2 \u2212\u03bb1 )tphase ,\nand assume the dephasing step has produced N i.i.d. replicas drawn\nfrom distribution \u03bcphase .\nThen the exit time converges to an exponential law, with parameter\nN \u03bb1 ,\nP\u03bcphase [T ? > t] \u2212 e\u2212N \u03bb1 t \u2264 N \u000fphase (1 + \u000fphase )N \u22121 e\u2212N \u03bb1 t .\nIf we additionally assume that N \u000fphase (1 + \u000fphase )N \u22121 < 1, then the\nhitting point distribution is asymptotically independent of the exit time\nZ\nN 2 \u000fphase (1 + \u000fphase )N \u22121\n\u03bcphase\n?\n?\nP\n[XT ? \u2208 A | T > t] \u2212\nd\u03c1 .\n,\n1 \u2212 N \u000fphase (1 + \u000fphase )N \u22121\nA\nwhere \u03c1 is the hitting point density and A \u2282 \u2202W .\nThus, for tphase large enough, we achieve the ideal factor of N speedup\nand we do not disrupt the hitting point distribution too much. The\nreader may find the N dependence in the error terms to be disconcerting, but it can easily be controlled by taking tphase & log N/(\u03bb2 \u2212 \u03bb1 ).\nWe will return to this in the discussion. We also note that there is a\n\n\f8\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nslight abuse of notation in the above expressions. The superscripts, \u03bd\nand \u03bcphase , should be interpreted as N -tensor products, with a distinct\nrealization drawn for each replica.\nA more detailed statement of this theorem, with explicit constants,\nis given at the beginning of Section 5. The hitting point density \u03c1 is\ndefined by (2.14).\nHowever, Theorem 1.3 is only a comparison between the parallel step\nand the QSD. Our final result is a comparison between the ParRep\nalgorithm, including decorrelation, dephasing and parallel steps, with\nan unaccelerated, serial process:\nTheorem 1.4 (ParRep Error). Let Xts denote the unaccelerated (serial) process and Xtp denote the ParRep process, and let both the serial\nprocess and the reference process be initially distributed under \u03bc0 , an\nadmissible distribution. Furthermore, assume the replicas are initialized from \u03bc0phase , also an admissible distribution.\nGiven tcorr \u2265 tcorr and tphase \u2265 tphase , let\n\u000fcorr = Ccorr e\u2212(\u03bb2 \u2212\u03bb1 )tcorr ,\n\u000fphase = Cphase e\u2212(\u03bb2 \u2212\u03bb1 )tphase .\nLetting T s and T p denote the physical exit times, we have\n|P\u03bc0 [T s > t] \u2212 P\u03bc0 [T p > t]|\n\u0002\n\u0003\n. \u000fcorr + N \u000fphase (1 + \u000fphase )N \u22121 e\u2212\u03bb1 (t\u2212tcorr )+ .\nIf, in addition, tcorr is sufficiently large that \u000fcorr < 1, then for A \u2282 \u2202W ,\n|P\u03bc0 [XTs s \u2208 A | T s > t] \u2212 P\u03bc0 [XTp p \u2208 A | T p > t]|\n\u000fcorr + N 2 \u000fphase (1 + \u000fphase )N \u22121\n1 \u2212 \u000fcorr\nThus, over a single cycle, the error in ParRep can be approximately\ndecomposed as\n.\n\n(1.9) Error = Decorrelation error + Parallel error(Dephasing error),\nwhere we view the parellel error as a function of the dephasing error.\nThe speedup can be seen when T p is given further consideration. When\nT p > tcorr , T p = N T ? + tcorr where T ? is the exit time of the particular\nreplica which escapes first. There will be no speedup if the exit is before\ntcorr .\n1.3. Outline of the Paper. In section 2, we review some important\nresults for (1.1). Our main Theorems are proven in Sections 3, 4, and 5.\nWe then discuss our results in Section 6. Some additional calculations\nappear in the appendix.\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n9\n\n1.4. Notation. Random variables, such as the position, Xt , and the\nexit time from the well, T , will appear in capital letters. Deterministic\nvalues, such as x, t, tcorr , etc. will be lower case. We will frequently\nuse indicator functions in our analysis, which we write as 1A , with A\nindicating the set on which the value is one.\nWe are often interested in probabilities and expectations of solutions\nof Xt solving (1.1), and its exit time T from some region W . When we\nwrite\nEx [f (T, XT )] or Px [T \u2265 t] = Ex [1T \u2265t ]\nthe superscript x indicates that x is the initial condition of Xt ; X0 = x,\nand the expectation and probability are then taken with respect to the\nunderlying Wiener measure of Bt .\nWhen X0 is given by some distribution \u03bc0 over W , we write\nZ\n\u03bc0\nEx [f (T, XT )] d\u03bc0 (x).\nE [f (T, XT )] \u2261\nW\n\nWhen we write a conditional expectation with respect to distribution\n\u03bc0 , we mean\nE\u03bc0 [f (T, XT ) | T > t] \u2261\n\nE\u03bc0 [f (T, XT )1T >t ]\n.\nP\u03bc0 [T > t]\n\nFor the reader more accustomed to the computational physics literature,\nE\u03bc0 [O(Xt )] = hO(t)i .\nIt is helpful to explicitly include the starting distribution, \u03bc0 associated\nwith the process Xt , to avoid any ambiguity.\nWhen we write f . g, we mean that there exists a constant C > 0\nsuch that f \u2264 Cg, but that the constant is not noteworthy.\n1.5. Acknowledgements. The authors wish to thank D. Aristoff,\nK. Leder, S. Mayboroda, A. Shapeev, and O. Zeitouni for helpful conversations in developing these ideas.\nWe also thank D. Perez and A.F. Voter for conversations at LANL\nthat motivated important refinements of our estimates.\nThis work was supported by the NSF PIRE grant OISE-0967140 and\nthe DOE grant de-sc0002085.\n2. Preliminary Results\nBefore proceeding to our main results on ParRep, we review some\nimportant results on the overdamped Langevin equation. These results\nare where our regularity assumptions on V , W, and \u2202W are needed.\n\n\f10\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nTwo essential tools in our study of (1.1) are the Feynman-Kac formula and the quasistationary distribution, which we briefly review here;\nsee [4] for additional details. First, let us recall the Feynman-Kac formula which relates solutions of a parabolic equation with corresponding\nelliptic operator\nL \u2261 \u2212\u2207V * \u2207 + \u03b2 \u22121 \u2206\n\n(2.1)\nto solutions of (1.1).\n\nProposition 2.1 (Proposition 1 of [4]). On the parabolic domain W \u00d7\nR+ , let v solve\n(2.2a)\n\n\u2202t v = Lv,\n\n(2.2b)\n\nv |\u2202W = \u03c6 : \u2202W \u2192 R,\n\n(2.2c)\n\nv(t = 0) = v0 : W \u2192 R.\n\nThen,\n(2.3)\n\nv(t, x) = Ex [1T \u2264t \u03c6(XT )] + Ex [1T >t v0 (Xt )] .\n\nTo say a bit more about the elliptic operator L, recall the invariant\nmeasure of (1.1):\n(2.4)\n\nd\u03bc \u2261 Z \u22121 exp (\u2212\u03b2V (x)) dx,\n\nwhere Z is the appropriate normalization. We introduce the Hilbert\nspace L2\u03bc , with inner product\nZ\n(2.5)\nhf, gi\u03bc \u2261 f gd\u03bc.\nAn elementary calculation shows that L is self adjoint and negative\ndefinite with respect to this inner product when supplemented with\nhomogeneous Dirichlet boundary conditions on \u2202W . Standard functional analysis and elliptic theory tell us that L has infinitely many\neigenvalue/eigenfunction pairs (\u03bbk , uk ); the eigenvalues can be ordered\n0 > \u2212\u03bb1 > \u2212\u03bb2 \u2265 \u2212\u03bb3 \u2265 . . . ;\nand the eigenfunctions form a complete orthonormal basis for L2\u03bc (W ).\nIn addition, the ground state, u1 , is unique and positive. For details,\nsee, for example, [13, 14, 16]. The \u03bb1 and \u03bb2 appearing in our theorems\nare precisely the first two eigenvalues.\nWhen solving (2.2) with \u03c6 = 0, the solution can be expressed as\n(2.6)\n\nv(x, t) =\n\n\u221e\nX\nk=1\n\ne\u2212\u03bbk t hv0 , uk i\u03bc uk .\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n11\n\nOut of this spectral problem, we build the norm\n\u221e\nX\n2\n(2.7)\nkf k2H\u03bcs \u2261\n\u03bbsk hf, uk i\u03bc .\nk=1\n\nThis generalizes to measures\n(2.8)\n\nk\u03bc0 k2H\u03bcs\n\n\u2261\n\n\u221e\nX\n\n\u03bbsk\n\n2\n\nZ\n\nuk d\u03bc0 ,\n\nk=1\n\nand to sequences, a = (a1 , a2 , . . .)\n(2.9)\n\nkak2H\u03bcs\n\n\u2261\n\n\u221e\nX\n\n\u03bbsk |ak |2 .\n\nk=1\n\nIf \u03bc0 has an Radon-Nikodym derivative with respect to \u03bc, (2.7) and\n(2.8) agree. We then define the function spaces,\nn\no\n(2.10)\nH\u03bcs = v \u2208 S (W )0 | kvkH\u03bcs < \u221e ,\nwhere S is the set of smooth functions with support in W , and S 0 is\nits dual. We also define the projection operator, PI , where I \u2282 N,\nX\n(2.11)\nPI f =\nhf, uk i\u03bc uk .\nk\u2208I\n\nHaving introduced these spaces and norms, we can now clarify what\nwas meant by the term admissible distribution used in the introduction.\nIn this work, a distribution will be admissible with respect to W if\nsupp \u03bc0 \u2282 W , and for some s \u2265 0, k\u03bc0 kH\u03bc\u2212s < \u221e.\nThe aforementioned quasistationary distribution of (1.1) associated\nwith the set W is closely related to the spectral structure of L. The\nQSD, \u03bd, is a time independent probability measure satisfying, for all\nmeasurable A \u2282 W and t > 0:\nR\nPx [Xt \u2208 A, t < T ] d\u03bd\n(2.12)\n\u03bd(A) = W R\n= P\u03bd [Xt \u2208 A | t < T ] .\nx [t < T ] d\u03bd\nP\nW\nThe QSD measure \u03bd exists and\nProposition 2.2 (Proposition 2 of [4]).\n(2.13)\n\nu1 d\u03bc\nu1 e\u2212\u03b2V dx.\n=R\nu d\u03bc\nu e\u2212\u03b2V dx\nW 1\nW 1\n\nd\u03bd = R\n\nWe refer the reader to, amongst others, [5\u20137,19,20,25] for additional\ndetails on the QSD. The utility of the QSD stems from the property\nthat if X0 is distributed according to \u03bd, then:\n\n\f12\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nProposition 2.3 (Proposition 3 of [4]). Let \u03c6 : \u2202W \u2192 R be smooth.\nThen for t > 0\nZ\n\u03bd\n\u03bd\n\u03bd\n\u2212\u03bb1 t\n\u03c6 d\u03c1\nE [1T <t \u03c6(XT )] = P [T < t] E [\u03c6(XT )] = (1 \u2212 e\n)\n\u2202W\n\nwhere the exit density is given by\n(2.14)\n\nd\u03c1 = \u2212\n\n1\nd\u03bd\n\u2207(u1 e\u2212\u03b2V ) * n\nR\n\u2207 * n dSx = \u2212\ndSx ,\n\u03bb1 \u03b2 dx\n\u03bb1 \u03b2 W u1 e\u2212\u03b2V dx\n\nwith n the outward pointing normal and dSx the surface measure.\nIn words, T is exponentially distributed with parameter \u03bb1 , and the\nfirst hitting point is independent of the first hitting time. Being initially\ndistributed according to \u03bd is, in a sense, ideal. As shown by Proposition\n5 of [4], were this the case for the replicas, the parallel step of ParRep\nwould be exact. In practice, X0 is never distributed by \u03bd, and it is the\npropagation of this error that we explore.\nMany of these quantities can be reformulated in terms of the FokkerPlanck equation for density px (t, y), x \u2208 W ,\n\u0001\n\u2202t px = L\u2217 px = \u2207y * px \u2207V + \u03b2 \u22121 \u2207px ,\npx |\u2202W = 0,\n\npx0 = \u03b4x (y).\n\nThough we will not make use of this, the reader more accustomed to\nFokker-Planck may find it helpful to re-express various quantities in\nterms of px . With regard to exit distributions,\nZ tZ\nx\n\u2212\u03c6(y)\u03b2 \u22121 \u2207px * n dSy ,\nE [\u03c6(XT )1T <t ] =\nZ0 \u221e Z\u2202W\nZ\nx\n\u22121\nx\nP [t < T ] =\n\u2212\u03b2 \u2207p * ndSy =\npx (t, y) dy.\nt\n\n\u2202W\n\nW\n\nThese can be integrated against the density of the QSD,\nd\u03bd\nsolves L\u2217 dx\n= \u2212\u03bb1 d\u03bd\n, to obtain\ndy\np\u03bd (y, t) = e\u2212\u03bb1 t\n\nd\u03bd\n,\ndy\n\nwhich\n\nd\u03bd\ndy\n\nas a particular solution of the Fokker-Planck equation. This directly\nshows the independence of exit time and hitting point. Substituting\ninto the above integrals reproduces Proposition 2.3.\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n13\n\n3. Convergence to the QSD \u2013 Proof of Theorem 1.1\nIn this section we prove Theorem 1.1, which we first restate with\nmore detail:\nTheorem 3.1 (Convergence to the QSD). Given s \u2265 0, let \u03bc0 be a\ndistribution with supp \u03bc0 \u2282 W and k\u03bc0 kH\u03bc\u2212s < \u221e. There exists\n\u001a\n\u001b4/(n+2s)\nZ\n(3.1)\nt&\nP[2,\u221e) \u03bc0 H\u03bc\u2212s / u1 d\u03bc0\nsuch that for all t \u2265 t and for all bounded and measurable f (\u03c4, \u03be) :\nR+ \u00d7 \u2202W \u2192 R\n|E\u03bct [f (T, XT )] \u2212 E\u03bd [f (T, XT )]|\n\u0012Z\n\u0013\u22121\n(3.2)\n. kf kL\u221e\nu1 d\u03bc0\nt\u2212n/4\u2212s/2 e\u2212(\u03bb2 \u2212\u03bb1 )(t\u2212t) P[2,\u221e) \u03bc0 H\u03bc\u2212s .\nThis is a refinement of Proposition 6 from [4], which now admits initial distributions which lack an L2 Radon-Nikodym derivative. Indeed,\nfor appropriate s, \u03bc0 can be a Dirac distribution. Though this is a\nparabolic flow which will instantaneously regularize such rough data,\nit is essential to an analysis of ParRep as one often wants to use Dirac\nmass initial conditions.\nIn addition to this result, we present an extension which is essential\nto obtaining the results in Section 5 on the parallel step.\n3.1. Proof of Theorem 3.1.\nProof. We first write\n\u03bct\n\nZ\n\nx\n\nZ\nF (x)d\u03bct\n\nE [f (T, XT )] d\u03bct =\n\nE [f (T, XT )] =\nW\n\nW\n\nwhere we have defined F (x) \u2261 Ex [f (T, XT )]. Thus,\nR\nEx [F (Xt )1T >t ] d\u03bc0\n\u03bct\n(3.3)\nE [f (T, XT )] = W R\n.\nEx [1T >t ] d\u03bc0\nW\nApplying Feynman-Kac, (2.3), to this,\nR\nv(t, x)d\u03bc0\n\u03bct\n(3.4)\nE [f (T, XT )] = RW\nv\u0304(t, x)d\u03bc0\nW\nwhere v solves (2.2) with v0 = F and \u03c6 = 0, while v\u0304 solves it with\nv0 = 1 and \u03c6 = 0. For brevity, let\nZ\nZ\nZ\n(3.5)\nF\u0302k = F uk d\u03bc, 1\u0302k = uk d\u03bc, \u03bc\u03020,k = uk d\u03bc0 .\n\n\f14\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nExpressing v and v\u0304 as series solutions using (2.6), we have\n(3.6)\n\nv(t, x) =\n\n\u221e\nX\n\ne\u2212\u03bbk t F\u0302k uk (x),\n\nv\u0304(t, x) =\n\nk=1\n\n\u221e\nX\n\ne\u2212\u03bbk t 1\u0302k uk (x).\n\nk=1\n\nAfter a bit of rearrangement, the error can be expressed as\ne(t) \u2261 |E\u03bct [f (T, XT )] \u2212 E\u03bd [f (T, XT )]|\n\u0011\nR\nP \u2212(\u03bb \u2212\u03bb1 )t \u0010\nk\ne\nF\u0302\n\u2212\n1\u0302\nF\nd\u03bd\n\u03bc\u03020,k\nk\nk\nk\n=\nP \u2212(\u03bb \u2212\u03bb )t\n1\u03021 \u03bc\u03020,1 + k e k 1 1\u0302k \u03bc\u03020,k\n\n(3.7)\n\nR\nwhere the sums are from k = 2 to \u221e since F\u03021 = 1\u03021 W F d\u03bd. Noting\nthat\nZ\nZ\nZ\nZ\nF\u0302k \u2212 1\u0302k F d\u03bd \u2264 |F uk | d\u03bc + |F | d\u03bd |uk | d\u03bc\nZ\np\n\u2264 2 kf kL\u221e |uk | d\u03bc \u2264 2 kf kL\u221e \u03bc(W ),\nwe can rewrite the numerator as\n\u0013\n\u0012\nZ\n\u221e\nX\n\u2212(\u03bbk \u2212\u03bb1 )t\ne\nF\u0302k \u2212 1\u0302k F d\u03bd \u03bc\u03020,k\nk=2\n\u221e\nX\np\n\u2264 2 \u03bc(W ) kf kL\u221e\ne\u2212(\u03bbk \u2212\u03bb1 )t |\u03bc\u03020,k |\nk=2\n\u221e\nX\np\ne\u2212(\u03bbk \u2212\u03bb1 )t1 |\u03bc\u03020,k |\n\u2264 2 \u03bc(W ) kf kL\u221e e\u2212(\u03bb2 \u2212\u03bb1 )(t\u2212t1 )\nk=2\n\np\n\u2264 2 \u03bc(W ) kf kL\u221e e\u2212(\u03bb2 \u2212\u03bb1 )(t\u2212t1 )\n\n\u221e\nX\n\ne\u2212\u03ba\u03bbk t1 |\u03bc\u03020,k |\n\nk=2\n\nwhere \u03ba = 1 \u2212 \u03bb1 /\u03bb2 and t \u2265 t1 > 0. Applying Proposition A.2 from\nthe appendix to this, the numerator is bounded by\n\u0012\n\u0013\nZ\n\u221e\nX\n\u2212(\u03bbk \u2212\u03bb1 )t\ne\nF\u0302k \u2212 1\u0302k F d\u03bd \u03bc\u03020,k\n(3.8)\nk=2\n. P[2,\u221e) \u03bc0\n\n\u2212n/4\u2212s/2\n\nH\u03bc\u2212s\n\nkf kL\u221e e\u2212(\u03bb2 \u2212\u03bb1 )(t\u2212t1 ) t1\n\n.\n\nThe constant that has been absorbed into the . symbol is independent\nof t, f and \u03bc0 .\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n15\n\nTo ensure the denominator is uniformly bounded away from zero, we\nuse a similar treatment,\n\u221e\nX\n\n\u2212(\u03bbk \u2212\u03bb1 )t\n\ne\n\n1\u0302k \u03bc\u03020,k\n\n\u221e\nX\np\n\u2212(\u03bb2 \u2212\u03bb1 )(t\u2212t2 )\n\u2264 \u03bc(W )e\ne\u2212\u03ba\u03bbk t2 |\u03bc\u03020,k |\n\nk=2\n\n. P[2,\u221e) \u03bc0\n\nH\u03bc\u2212s\n\nk=2\n\u2212(\u03bb2 \u2212\u03bb1 )t \u2212n/4\u2212s/2\ne\nt2\n\nfor t \u2265 t2 > 0, which may differ from t1 . Therefore,\n1\u03021 \u03bc\u03020,1 +\n\n\u221e\nX\n\ne\u2212(\u03bbk \u2212\u03bb1 )t 1\u0302k \u03bc\u03020,k\n\nk=2\n\u2212n/4\u2212s/2\n\n& 1\u03021 \u03bc\u03020,1 \u2212 e\u2212(\u03bb2 \u2212\u03bb1 )(t\u2212t2 ) t2\n\nP[2,\u221e) \u03bc0\n\nH\u03bc\u2212s\n\n.\n\nFor a sufficiently large t \u2265 t \u2265 t2 > 0, the denominator is bounded\nfrom below by\nZ\nZ\n\u221e\nX\n1\n1\n\u2212(\u03bbk \u2212\u03bb1 )t\ne\n(3.9) 1\u03021 \u03bc\u03020,1 +\n1\u0302k \u03bc\u03020,k \u2265 1\u03021 \u03bc\u03020,1 =\nu1 d\u03bc u1 d\u03bc0 > 0.\n2\n2\nk=2\nRoughly,\n\u001a\n(3.10)\n\nt&\n\n\u001b4/(n+2s)\n\nZ\nP[2,\u221e) \u03bc0\n\nH\u03bc\u2212s\n\n/\n\nu1 d\u03bc0\n\n.\n\nTaking t1 = t2 = t in (3.8) and (3.9) we have that for t \u2265 t\n\u0012Z\n\u0013\u22121\ne(t) .\nu1 d\u03bc0\ne\u2212(\u03bb2 \u2212\u03bb1 )(t\u2212t) (t)\u2212n/4\u2212s/2\n\u00d7 kf kL\u221e P[2,\u221e) \u03bc0\n\nH\u03bc\u2212s\n\n.\n\nFinally, for this estimate to hold for general bounded and measurable\nf , we apply a density argument with respect to the L\u221e norm.\n\u0003\nR\nThe inclusion of u1 d\u03bc0 in the preceding result isRdeliberate as \u03bc0\nis, to a degree, a user specified parameter. Moreover, u1 d\u03bc0 could be\nquite small. Indeed, when a Xt first enters W , it is near \u2202W and the\nsupport of \u03bc0 is in a neighborhood of \u2202W ; we may have \u03bc0 = \u03b4x where\nx is close to \u2202W . As u1 is continuous and vanishes on \u2202W ,\nZ\nu1 \u03b4x = O (dist(x, \u2202W )) .\nW\n\nWe also see that as \u03bc0 \u2192 \u03bd, P[2,\u221e) \u03bc0\n\nH\u03bc\u2212s\n\n\u2192 0, and the error vanishes.\n\n\f16\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nIt remains to identify distributions and values of s for which k\u03bc0 kH\u03bc\u2212s <\n\u221e. In the case that \u03bc0 has an L2\u03bc Radon-Nikodym derivative, one readily sees that k\u03bc0 kH\u03bc\u2212s < \u221e for s \u2264 0. Indeed, when s = 0, this results\ncollapses onto the L2\u03bc estimate of [4]. This extends to \u03bc0 possessing Lp\u03bc\ndensities for any p \u2265 2.\nFor the case \u03bc0 = \u03b4x , a Dirac mass, we shall have that \u03bc0 \u2208 H\u03bc\u2212s\nwhen s is large enough to embed H\u03bcs into L\u221e . If the \u2202W is sufficiently\nsmooth, then by standard elliptic theory, H\u03bcs and H s will be equivalent\nfor s \u2265 0, and we have the embedding for s > n/2, [1, 13, 14]. Refined\nelliptic estimates may weaken such assumptions on the boundary.\n3.2. Exit Times. In the case that we are interested in exit times, we\nhave a result closely related to Theorem 3.1.\nTheorem 3.2. Assume \u03bc0 satisfies the assumptions of Theorem 3.1\nand t0 \u2265 t. Then for t \u2265 0,\nP\u03bct0 [T > t] \u2212 e\u2212\u03bb1 t \u2264 Ce\u2212\u03bb1 t e\u2212(\u03bb2 \u2212\u03bb1 )t0\n\n(3.11)\n\nwhere C is the pre-exponential factor in (3.2) and is independent of t\nand t0 .\nProof. As before, we rely on (2.3) and the series expansions (2.6) to\nwrite\nP\u221e \u2212\u03bb (t+t1 )\nk\n1\u0302k \u03bc\u03020,k\nP\u03bc0 [T > t1 + t]\n\u03bct1\nk=1 e\n=\nP [T > t] =\n.\nP\n\u221e\n\u03bc\n0\n\u2212\u03bb\nt\n1\nk\nP [T > t1 ]\n1\u0302k \u03bc\u03020,k\nk=1 e\nComparing against the QSD,\nP\n\n\u03bct1\n\n\u2212\u03bb1 t\n\n[T > t] \u2212 e\n\nP\u221e \u2212\u03bb (t+t1 )\nk\n1\u0302k \u03bc\u03020,k\nk=1 e\n= P\n\u2212 e\u2212\u03bb1 t\n\u221e\n\u2212\u03bb\nt\n1\nk\n1\u0302k \u03bc\u03020,k\nk=1 e\n\u0001\nP\u221e\n\u2212\u03bbk (t+t1 )\n\u2212 e\u2212\u03bbk t1 \u2212\u03bb1 t 1\u0302k \u03bc\u03020,k\nk=1 e\n=\nP\u221e \u2212\u03bb t\nk 1 1\u0302 \u03bc\u0302\nk 0,k\nk=1 e\n\nIn the numerator, the k = 1 term vanishes, leaving\n\u0001 \u2212\u03bb t\nP\u221e\nP\u221e \u2212\u03bb t0\n\u2212(\u03bbk \u2212\u03bb1 )t\nk 1 1\u0302 \u03bc\u0302\n1\n\u2212\ne\ne\ne k 1\u0302k \u03bc\u03020,k\nk\n0,k\nk=2\ne\u2212\u03bb1 t\n\u2264 e\u2212\u03bb1 t Pk=2\nP\u221e \u2212\u03bb t\n\u221e\n\u2212\u03bbk t0 1\u0302 \u03bc\u0302\nk 1 1\u0302 \u03bc\u0302\nk 0,k\nk 0,k\nk=1 e\nk=1 e\nUsing the same methods as in the proof of Theorem 3.1,\nP\u221e \u2212\u03bb t1\n\u0012Z\n\u0013\u22121\nk\ne\n|\u03bc\u0302\n|\n0,k\nk=2\nt\u2212n/4\u2212s/2 e\u2212(\u03bb2 \u2212\u03bb1 )(t0 \u2212t) P[2,\u221e) \u03bc0\n.\nu1 d\u03bc0\nP\u221e\n\u2212\u03bb\nt\n1\nk 1\u0302 \u03bc\u0302\nk 0,k\nk=1 e\n\nH\u03bc\u2212s\n\n\u0003\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n17\n\nThis estimate plays an important role in our analysis of ParRep. Indeed, we will frequently confront terms of the form E\u03bct0 [f (X, T )1T >t ],\nand we will want to compare against the corresponding term for the\nQSD. One could naively apply Theorem 3.1 to estimate such a term,\nwith observable gt (\u03be, \u03c4 ) = f (\u03be, \u03c4 )1\u03c4 >t . However, this is wasteful, as\nthe observable is going to be taken over realizations which not only\nhave not left the well before t0 , but remain in the well for at least an\nadditional t. We thus have the following identity.\nLemma 3.1. Given t, t0 \u2265 0,\n(3.12)\n\nE\u03bct0 [f (XT , T )1T >t ] = E\u03bct0 +t [f (XT , T + t)] P\u03bct0 [T > t] .\n\nProof. This reflects the Markovian nature of the process. Writing out\nthe lefthand side,\nZ\n\u03bct0\nEx [f (XT , T )1T >t ] \u03bct0 (dx)\nE [f (XT , T )1T >t ] =\nRW x\nE [f (XT , T )1T >t ] P\u03bc0 [Xt \u2208 dx, T > t0 ]\n= W\nP\u03bc0 [T > t0 ]\nThe numerator is\nZ\nEx [f (XT , T )1T >t ] P\u03bc0 [Xt \u2208 dx, T > t0 ] = E\u03bc0 [f (XT , T \u2212 t0 )1T \u2212t0 >t 1T >t0 ]\nW\n\n= E\u03bc0 [f (XT , T \u2212 t0 )1T >t0 +t ] ,\nwhere t0 is subtracted off to make the observable consistent. The same\nargument shows\nE\u03bct0 +t [f (XT , T + t)] =\n\nE\u03bc0 [f (XT , T \u2212 t0 )1T >t+t0 ]\n.\nP\u03bc0 [T > t + t0 ]\n\nCombining these three expressions completes the proof.\n\n\u0003\n\nIn principle, we can use this Lemma and Theorem 3.2 to obtain\nrefinements on Theorem 3.1 for observables that include 1T >t terms.\n4. The Dephasing Step \u2013 Proof of Theorem 1.2\nWe now examine our dephasing step,\nTheorem 4.1. Given s \u2265 0, assume supp\u03bc0phase \u2282 W and \u03bc0phase H \u2212s <\n\u03bc\n\u221e. Then\nA. The dephasing step produces N independent replicas with distributions \u03bcphase ,\n\u0002\n\u0003\n0\n\u03bcphase (A) = P\u03bcphase Xtphase \u2208 A | T > tphase ;\n\n\f18\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nB. There exists tphase and Cphase such that for tphase \u2265 tphase ,\n\u0003\n\u0002\nE\u03bcphase f (T k , XTk k ) \u2212 E\u03bd [f (T, XT )] \u2264 kf kL\u221e Cphase e\u2212(\u03bb2 \u2212\u03bb1 )tphase ;\nfor any bounded measurable f : R+ \u00d7 \u2202W \u2192 R and all k =\n1, . . . , N .\nC. The expected number of times a replica is relaunched is finite.\nTo prove Theorem 4.1, we must establish:\nA. The replicas are independent and have law \u03bcphase ;\nB. The error of \u03bcphase can be made small;\nC. The expected number of relaunches is finite.\nThe first property is obvious as each of the replicas is driven by\nan independent Brownian motion, and we only retain realizations for\nwhich T > tphase . The second property follows from Theorem 3.1.\nTo prove the third property, we must establish that replicas initiated\nfrom \u03bc0phase have a nonzero chance of surviving till tphase :\nLemma 4.1. Assume that \u03bc0phase satisfies the hypotheses of Theorem\n4.1,\n\u0002\n\u0003\n0\nP\u03bcphase T k \u2265 tphase \u2261 p > 0.\nProof. Observe that we have the following monotonicity property for\nt2 > t1 ,\n0\n\n0\n\n0 \u2264 P\u03bcphase [T \u2265 t2 ] \u2264 P\u03bcphase [T \u2265 t1 ] .\nWe now argue by contradiction. Assume that at some t1 > 0,\n0\nP\n[T \u2265 t1 ] = 0. By the above monotonicity, P\u03bcphase [T \u2265 t2 ] = 0\nfor all t2 \u2265 t1 . Using a similar approach as in the proof of Theorem\n3.1, we write\nZ\nZ\n\u221e\nX\n\u03bc0phase\n0\n\u2212\u03bbk t\nP\nuk d\u03bcphase uk d\u03bc\n[T \u2265 t] = v\u0304(x, t) =\ne\n\u03bc0phase\n\nk=1\n\nwhere v\u0304 solves (2.2) with v0 = 1 and \u03c6 = 0. Therefore,\nZ\n\u221e\n\u221e\nX\nX\n\u2212\u03bbk t2 0\ne\u2212\u03bbk t2 \u03bc\u03020phase,k 1\u0302k\nv\u0304(x, t2 ) =\ne\n\u03bc\u0302phase,k uk d\u03bc =\nk=1\n\nk=1\n\n(\n\u2212\u03bb1 t2\n\n\u2265e\n\n\u03bc\u03020phase,1 1\u03021\n\n\u2212e\n\n\u2212(\u03bb2 \u2212\u03bb1 )t2\n\n\u221e\nX\n\n)\n\u2212\u03ba\u03bbk t2\n\ne\n\n1\u0302k\n\n\u03bc\u03020phase,k\n\nk=2\n\nn\n\u2212n/4\u2212s/2\n& e\u2212\u03bb1 t2 \u03bc\u03020phase,1 1\u03021 \u2212 e\u2212(\u03bb2 \u2212\u03bb1 )t2 t2\nP[2,\u221e) \u03bc0phase\n\no\nH\u03bc\u2212s\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n19\n\nThen taking t2 sufficiently large,\n\u2212n/4\u2212s/2\n\n\u03bc\u03020phase,1 1\u03021 \u2212 e\u2212(\u03bb2 \u2212\u03bb1 )t2 t2\nP[2,\u221e) \u03b4x H\u03bc\u2212s\nZ\nZ\n1\n1\n\u2265 \u03bc\u03020phase,1 1\u03021 =\nu1 d\u03bc0phase u1 d\u03bc > 0,\n2\n2\nR\n\nsince\n\nu1 d\u03bc0phase > 0. Thus, we have a contradiction.\n\n\u0003\n\nThis calculation reveals a role played by the choice of \u03bc0phase . If\n\u0002\n\u0003\n0\nconcentrated near the well boundary, P\u03bcphase T k \u2265 tphase = p could be\nquite small. This will induce the replicas to relaunch many times, as\nthe next result shows. Thus, for computational efficiency, a distribution\nconcentrated deep in the well's interior is desirable.\nLemma 4.2. Assume that \u03bc0phase satisfies the hypotheses of Theorem\n\u0002\n\u0003\n0\n4.1, and that P\u03bcphase T k \u2265 tphase = p > 0. Then\n0\n\nE\u03bcphase [Number of relaunches] = (1 \u2212 p)/p < \u221e\nProof. The probability of relaunching m times is the probability of\nexiting m times and surviving on the m + 1-th time. Interpreting\n0\nthis in terms of T k and using the assumption, P\u03bcphase [m relaunches] =\n(1 \u2212 p)m p. Thus,\n\u03bc0phase\n\nE\n\n[Number of relaunches] =\n=\n\n\u221e\nX\nm=0\n\u221e\nX\n\n0\n\nm * P\u03bcphase [m relaunches]\nm(1 \u2212 p)m p =\n\nm=0\n\n1\u2212p\n< \u221e.\np\n\u0003\n\n5. The Parallel Step \u2013 Proofs of Theorems 1.3 and 1.4\nFirst, we restate Theorem 1.3 with additional detail:\nTheorem 5.1 (Parallel Error). Given tphase \u2265 tphase , let\n\u000fphase \u2261 Cphase e\u2212(\u03bb2 \u2212\u03bb1 )tphase ,\nand assume the dephasing step has produced N i.i.d. replicas drawn\nfrom distribution \u03bcphase . Then the exit time distribution of the parallel\nstep converges to an exponential,\n(5.1)\n\nP\u03bcphase [T ? > t] \u2212 e\u2212N \u03bb1 t \u2264 \u000fphase N (1 + \u000fphase )N \u22121 e\u2212N \u03bb1 t .\n\n\f20\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nIf \u03c6 : \u2202W \u2192 R is bounded and measurable, the exit distribution\nconverges to one that is independent of exit time,\nZ\n\u2212N \u03bb1 t\n?\n\u03bcphase\n\u03c6d\u03c1\n[1T ? >t \u03c6(XT ? )] \u2212 e\nE\n(5.2)\n\u2202W\n. N 2 (1 + \u000fphase )N \u22121 \u000fphase k\u03c6kL\u221e e\u2212N \u03bb1 t .\nIf, in addition, N \u000fphase (1 + \u000fphase )N \u22121 < 1, then\nZ\n?\n?\n\u03bcphase\n\u03c6d\u03c1\n[\u03c6(XT ? ) | T > t] \u2212\nE\n\u2202W\n(5.3)\nN 2 k\u03c6kL\u221e \u000fphase (1 + \u000fphase )N \u22121\n.\n.\n1 \u2212 N \u000fphase (1 + \u000fphase )N \u22121\nProof. To prove (5.1), we begin by writing,\nP\u03bcphase [T ? > t] \u2212 e\u2212N \u03bb1 t\n\u0002 k\n\u0003\n\u0002 k\n\u0003\n\u03bcphase\n\u03bd\n= \u03a0N\nT > t \u2212 \u03a0N\nT >t\nk=1 P\nk=1 P\n\u0002\n\u0003N\n= P\u03bcphase T 1 > t \u2212 e\u2212N \u03bb1 t\n= P\n\n\u03bcphase\n\nN\n\u22121\nX\n\u0002 1\n\u0003\n\u0002\n\u0003k\n\u2212\u03bb1 t\nT >t \u2212e\nP\u03bcphase T 1 > t e\u2212(N \u22121\u2212k)\u03bb1 t .\nk=0\n\nFrom Theorem 3.2, we know\nP\u03bcphase [T > t] \u2212 e\u2212\u03bb1 t \u2264 \u000fphase e\u2212\u03bb1 t .\nTherefore,\nP\u03bcphase [T ? \u2265 t] \u2212 e\u2212N \u03bb1 t \u2264 \u000fphase e\u2212\u03bb1 t N (1 + \u000fphase )N \u22121 e\u2212(N \u22121)\u03bb1 t .\nTo prove (5.2), we begin by writing the expectation as\n\u0002\n\u0003\nE\u03bcphase [1T ? >t \u03c6(XT? ? )] = E\u03bcphase 1T k? >t \u03c6(XTk?k? )\n=\n\nN\nX\n\n\u0002\n\u0003\nE\u03bcphase 1T k >t \u03c6(XTk k )1k=k?\n\nk=1\n\n=\n\nN\nX\n\n\u0002\n\u0003\nE\u03bcphase 1T k >t \u03c6(XTk k )\u03a0l6=k 1T l >T k 1T l >t .\n\nk=1\n\nIn the above expression, we have used that since T ? > t, T l > t for\neach l. Then, using Lemma 3.1 on each of the processes,\n\u0002\n\u0003\nE\u03bcphase 1T k >t \u03c6(XTk k )\u03a0l6=k 1T l >T k 1T l >t\n\u0002\n\u0003\n\u03bcphase\n= E\u03bctphase +t \u03c6(XTk k )\u03a0l6=k 1T l >T k \u03a0N\n[T l > t]\nl=1 P\n\u0002\n\u0003\n= E\u03bctphase +t \u03c6(XTk k )1k=k? P\u03bcphase [T > t]N .\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n21\n\nThis leads to the expression\nE\u03bcphase [1T ? >t \u03c6(XT? ? )] = E\u03bctphase +t [\u03c6(XT? ? )] P\u03bcphase [T ? > t].\n\n(5.4)\n\nComparing against the QSD,\n|E\u03bcphase [1T ? >t \u03c6(XT? ? )] \u2212 E\u03bd [1T ? >t \u03c6(XT? ? )]|\n\u2264 |E\u03bctphase +t [\u03c6(XT? ? )]| P\u03bcphase [T ? > t] \u2212 e\u2212N \u03bb1 t\n\n(5.5)\n\n+ e\u2212N \u03bb1 t |E\u03bctphase +t [\u03c6(XT? ? )] \u2212 E\u03bd [\u03c6(XT? ? )]| .\nThe first difference can be treated by (5.1), but the second difference\nrequires more care.\nGiven an arbitrary distribution \u03b7 for X0 , we define\nP \u03b7 (t) \u2261 P\u03b7 [T > t] = P\u03b7 [T k > t],\n\n(5.6)\n\nk = 1 . . . N.\n\nConsequently, P \u03bd (t) = e\u2212\u03bb1 t and\n\u03bctphase +t\n\nE\n\n[\u03c6(XT? ? )]\n\n=\n\nN\nX\n\n\u0003\n\u0002\nE\u03bctphase +t \u03c6(XTk )\u03a0l6=k 1Tl >Tk\n\nk=1\n\n(5.7)\n=\n\nN\nX\n\n\u0002\n\u0003\nE\u03bctphase +t \u03c6(XTk )P \u03bctphase +t (T k )N \u22121 .\n\nk=1\n\nAn analogous expansion can be made with \u03bd in place of \u03bctphase +t . Taking\nthe difference of the two sums, and comparing term by term,\n(5.8)\n\u0002\n\u0003\n\u0002\n\u0003\nE\u03bctphase +t \u03c6(XTk )P \u03bctphase +t (T k )N \u22121 \u2212 E\u03bd \u03c6(XTk )P \u03bd (T k )N \u22121\n\u0002\n\u0003\n\u0002\n\u0003\n\u2264 E\u03bctphase +t \u03c6(XTk )P \u03bctphase +t (T k )N \u22121 \u2212 E\u03bd \u03c6(XTk )P \u03bctphase +t (T k )N \u22121\n\u0002\n\u0003\n\u0002\n\u0003\n+ E\u03bd \u03c6(XTk )P \u03bctphase +t (T k )N \u22121 \u2212 E\u03bd \u03c6(XTk )P \u03bd (T k )N \u22121 .\nBy Theorem 3.1 the first difference in (5.8) is bounded by\n\u0002\n\u0003\n\u0002\n\u0003\nE\u03bctphase +t \u03c6(XTk )P \u03bctphase +t (T k )N \u22121 \u2212 E\u03bd \u03c6(XTk )P \u03bctphase +t (T k )N \u22121\n\u2264 \u000fphase e\u2212(\u03bb2 \u2212\u03bb1 )t k\u03c6kL\u221e ,\nsince P \u2264 1.\nFor the other difference in (5.8), we can replicate the proof of (5.1)\nto obtain, for any \u03c4 \u2265 0,\nP \u03bctphase +t (\u03c4 )N \u22121 \u2212 P \u03bd (\u03c4 )N \u22121\n\u2264 (N \u2212 1)\u000fphase e\u2212(\u03bb2 \u2212\u03bb1 )t (1 + \u000fphase e\u2212(\u03bb2 \u2212\u03bb1 )t )N \u22122 e\u2212(N \u22121)\u03bb1 \u03c4\n\u2264 (N \u2212 1)\u000fphase (1 + \u000fphase )N \u22122 .\n\n\f22\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nTherefore,\n\u0003\n\u0002\n\u0003\n\u0002\nE\u03bd \u03c6(XTk )P \u03bctphase +t (T k )N \u22121 \u2212 E\u03bd \u03c6(XTk )P \u03bd (T k )N \u22121\n\u2264 k\u03c6kL\u221e \u000fphase (N \u2212 1)(1 + \u000fphase )N \u22122 .\nSo (5.8) can be bounded by\n\u0003\n\u0002\n\u0003\n\u0002\nE\u03bctphase +t \u03c6(XTk )P \u03bctphase +t (T k )N \u22121 \u2212 E\u03bd \u03c6(XTk )P \u03bd (T k )N \u22121\n\u0002\n\u0003\n\u2264 k\u03c6kL\u221e \u000fphase 1 + (N \u2212 1)(1 + \u000fphase )N \u22122 .\nReturning to (5.5), using (5.1) to treat the first difference and the\npreceding calculation to treat the second, we have:\n|E\u03bcphase [1T ? >t \u03c6(XT? ? )] \u2212 E\u03bd [1T ? >t \u03c6(XT? ? )]|\n\u2264 N k\u03c6kL\u221e \u000fphase e\u2212N \u03bb1 t (1 + \u000fphase )N \u22121\n\u0002\n\u0003\n+ N k\u03c6kL\u221e \u000fphase e\u2212N \u03bb1 t 1 + (N \u2212 1)(1 + \u000fphase )N \u22122\n\n(5.9)\n\n. N 2 k\u03c6kL\u221e \u000fphase e\u2212N \u03bb1 t (1 + \u000fphase )N \u22121 .\nFinally, to prove (5.3),\n|E\u03bcphase [\u03c6(XT? ? ) | T ? > t] \u2212 E\u03bd [\u03c6(XT? ? ) | T ? > t]|\n=\n\nE\u03bcphase [\u03c6(XT? ? )1T ? >t ] E\u03bd [\u03c6(XT? ? )1T ? >t ]\n\u2212\nP\u03bcphase [T ? > t]\nP\u03bd [T ? > t]\n\n1\n[T ? > t]\n|P\u03bcphase [T ? > t] \u2212 P\u03bd [T ? > t]|\n+ |E\u03bd [\u03c6(XT? ? )1T ? >t ]|\n.\nP\u03bcphase [T ? > t] P\u03bd [T ? > t]\nFor the first difference,\n1\n|E\u03bcphase [\u03c6(XT? ? )1T ? >t ] \u2212 E\u03bd [\u03c6(XT? ? )1T ? >t ]| \u03bcphase ?\nP\n[T > t]\n2\nN \u22121\nN k\u03c6kL\u221e \u000fphase (1 + \u000fphase )\n.\n.\n1 \u2212 N \u000fphase (1 + \u000fphase )N \u22121\n\u2264 |E\u03bcphase [\u03c6(XT? ? )1T ? >t ] \u2212 E\u03bd [\u03c6(XT? ? )1T ? >t ]|\n\nP\u03bcphase\n\nFor the second difference,\n|P\u03bcphase [T ? > t] \u2212 P\u03bd [T ? > t]|\nP\u03bcphase [T ? > t] P\u03bd [T ? > t]\nN k\u03c6kL\u221e \u000fphase (1 + \u000fphase )N \u22121\n.\n\u2264\n1 \u2212 N \u000fphase (1 + \u000fphase )N \u22121\n\n|E\u03bd [\u03c6(XT? ? )1T ? >t ]|\n\nCombining these estimates, we have our result.\n\u0003\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n23\n\nLastly, we prove Theorem 1.4, which we first restate with additional\ndetail:\nTheorem 5.2 (ParRep Error). Let Xts denote the unaccelerated (serial) process and Xtp denote the ParRep process, and assume that both\nXtref and Xts are initially distributed under \u03bc0 , an admissible distribution. Also assume that \u03bc0phase is admissible.\nGiven tcorr \u2265 tcorr and tphase \u2265 tphase , let\n\u000fcorr = Ccorr e\u2212(\u03bb2 \u2212\u03bb1 )tcorr ,\n\u000fphase = Cphase e\u2212(\u03bb2 \u2212\u03bb1 )tphase .\nLetting T s and T p denote the physical times, we have\n|P\u03bc0 [T s > t] \u2212 P\u03bc0 [T p > t]|\n(5.10)\n\u2264 \u000fcorr e\u2212\u03bb1 t + \u000fphase N (1 + \u000fphase )N \u22121 e\u2212\u03bb1 (t\u2212tcorr )+ ,\n(5.11)\n\n|E\u03bc0 [\u03c6(XTs s )1T s >t ] \u2212 E\u03bc0 [\u03c6(XTp p )1T p >t ]|\n\u0002\n\u0003\n. \u000fcorr + \u000fphase N 2 (1 + \u000fphase )N \u22121 k\u03c6kL\u221e e\u2212\u03bb1 (t\u2212tcorr )+ .\n\nIf, in addition, \u000fcorr < 1, then\n|E\u03bc0 [\u03c6(XTs s ) | T s > t] \u2212 E\u03bc0 [\u03c6(XTp p ) | T p > t]|\n(5.12)\n\u000fcorr + \u000fphase N 2 (1 + \u000fphase )N \u22121\n.\nk\u03c6kL\u221e ,\n1 \u2212 \u000fcorr\nProof. We begin by decomposing\nP\u03bc0 [T s > t] = P\u03bc0 [T s > t | T s \u2264 tcorr ] P\u03bc0 [T s \u2264 tcorr ]\n+ P\u03bc0 [T s > t | T s > tcorr ] P\u03bc0 [T s > tcorr ] .\nWe analogously decompose P\u03bc0 [T p > t]. For t \u2264 tcorr , the serial algorithm and the reference process of ParRep have the same law. Hence,\nP\u03bc0 [T s \u2264 tcorr ] = P\u03bc0 [T p \u2264 tcorr ] ,\nP\u03bc0 [T s > t | T s \u2264 tcorr ] = P\u03bc0 [T p > t | T p \u2264 tcorr ] .\nConsequently, error only manifests itself if the parallel step is engaged,\n|P\u03bc0 [T s > t] \u2212 P\u03bc0 [T p > t]|\n= P\u03bc0 [T s > tcorr ] |P\u03bc0 [T s > t | T s > tcorr ] \u2212 P\u03bc0 [T p > t | T p > tcorr ]| .\nComparing against the QSD,\n(5.13)\n|P\u03bc0 [T s > t] \u2212 P\u03bc0 [T p > t]|\n\u2264 P\u03bc0 [T s > tcorr ] |P\u03bc0 [T s > t | T s > tcorr ] \u2212 P\u03bd [T > (t \u2212 tcorr )+ ]|\n+ P\u03bc0 [T s > tcorr ] |P\u03bc0 [T p > t | T p > tcorr ] \u2212 P\u03bd [T > (t \u2212 tcorr )+ ]| .\n\n\f24\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nExamining the first term,\nP\u03bc0 [T s > t | T s > tcorr ] = P\u03bccorr [T s > (t \u2212 tcorr )+ ] .\nBy assumption and Theorem 3.2\n(5.14)\n|P\u03bccorr [T s > (t \u2212 tcorr )+ ] \u2212 P\u03bd [T > (t \u2212 tcorr )+ ]| \u2264 \u000fcorr e\u2212\u03bb1 (t\u2212tcorr )+ .\nFor the other term, since the exit time is beyond tcorr the parallel step\nengages. The single reference process is replaced by the ensemble of N\nreplicas drawn from \u03bcphase , and T p = N T ? + tcorr . Hence,\n\u0002\n\u0003\nP\u03bc0 [T p > t | T p > tcorr ] = P\u03bcphase T ? > N1 (t \u2212 tcorr )+ .\nTherefore, by Theorem 5.1\n\u0002\n\u0003\nP\u03bcphase T ? > N1 (t \u2212 tcorr )+ \u2212 P\u03bd [T > (t \u2212 tcorr )+ ]\n(5.15)\n\u2264 \u000fphase N (1 + \u000fphase )N \u22121 e\u2212\u03bb1 (t\u2212tcorr )+\nSubstituting (5.14) and (5.15) into (5.13), we obtain (5.10).\nTo obtain (5.11), we again decompose as\nE\u03bc0 [\u03c6(XTs s )1T s >t ] = E\u03bc0 [\u03c6(XTs s )1T s >t | T s \u2264 tcorr ] P\u03bc0 [T s \u2264 tcorr ]\n+ E\u03bc0 [\u03c6(XTs s )1T s >t | T s > tcorr ] P\u03bc0 [T s > tcorr ] .\nand analogously decompose the ParRep expectation. Again, for t \u2264\ntcorr , the serial algorithm and the reference process of ParRep have the\nsame law. Thus\nE\u03bc0 [\u03c6(XTs s )1T s >t | T s \u2264 tcorr ] = E\u03bc0 [\u03c6(XTp p )1T p >t | T p \u2264 tcorr ] .\nConsequently,\n|E\u03bc0 [\u03c6(XTs s )1T s >t ] \u2212 E\u03bc0 [\u03c6(XTp p )1T p >t ]|\n= P\u03bc0 [T s > tcorr ] |E\u03bc0 [\u03c6(XTs s )1T s >t | T s > tcorr ] \u2212 E\u03bc0 [\u03c6(XTp p )1T p >t | T p > tcorr ]| .\nUsing the QSD as an intermediary,\n(5.16)\n|E\u03bc0 [\u03c6(XTs s )1T s >t | T s > tcorr ] \u2212 E\u03bc0 [\u03c6(XTp p )1T p >t | T p > tcorr ]|\n\u0002\n\u0003\n\u2264 E\u03bc0 [\u03c6(XTs s )1T s >t | T s > tcorr ] \u2212 E\u03bd \u03c6(XT )1T >(t\u2212tcorr )+\n\u0002\n\u0003\n+ E\u03bd \u03c6(XT )1T >(t\u2212tcorr )+ \u2212 E\u03bc0 [\u03c6(XTp p )1T p >t | T p > tcorr ] .\nFor the first term,\n\u0002\n\u0003\nE\u03bc0 [\u03c6(XTs s )1T s >t | T s > tcorr ] = E\u03bccorr \u03c6(XTs s )1T s >(t\u2212tcorr )+\n= E\u03bctcorr +(t\u2212tcorr )+ [\u03c6(XTs s )] P\u03bccorr [T s > (t \u2212 tcorr )+ ] .\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n25\n\nHence,\n\u0002\n\u0003\nE\u03bc0 [\u03c6(XTs s )1T s >t | T s > tcorr ] \u2212 E\u03bd \u03c6(XT )1T >(t\u2212tcorr )+\n\n(5.17)\n\n. \u000fcorr k\u03c6kL\u221e e\u2212\u03bb1 (t\u2212tcorr )+ .\n\nFor the other term, since the parallel step has engaged,\n(5.18)\nh\ni\np\n?\np\n\u03bcphase\n\u03bc0\n\u03c6(XT ? )1T ? > 1 (t\u2212tcorr )+ .\nE [\u03c6(XT p )1T p >t | T > tcorr ] = E\nN\n\nBy Theorem 5.1,\n\u0002\n\u0003\nE\u03bd \u03c6(XT )1T >(t\u2212tcorr )+ \u2212 E\u03bc0 [\u03c6(XTp p )1T p >t | T p > tcorr ]\nh\ni\n\u0002\n\u0003\n(5.19) = E\u03bd \u03c6(XT )1T >(t\u2212tcorr )+ \u2212 E\u03bcphase \u03c6(XT? ? )1T ? > 1 (t\u2212tcorr )+\nN\n\n2\n\n\u2264 \u000fphase N k\u03c6kL\u221e (1 + \u000fphase )\n\nN \u22121 \u2212\u03bb1 (t\u2212tcorr )+\n\ne\n\n.\n\nUsing (5.17) and (5.19) in (5.16) gives (5.11).\n(5.12) is proved using the preceding estimates,\n(5.20)\n|E\u03bc0 [\u03c6(XTs s ) | T s > t] \u2212 E\u03bc0 [\u03c6(XTp p ) | T p > t]|\nE\u03bc0 [\u03c6(XTs s )1T s >t ] \u2212 E\u03bc0 [\u03c6(XTp p )1T p >t ]\nP\u03bc0 [T s > t]\nP\u03bc0 [T s > t] \u2212 P\u03bc0 [T p > t]\np\n\u03bc0\np\n+ |E [\u03c6(XT p )1T >t ]|\nP\u03bc0 [T s > t] P\u03bc0 [T p > t]\n\u0002\n\u0003\ne\u2212\u03bb1 (t\u2212tcorr )+ P\u03bc0 [T s > tcorr ]\n. \u000fcorr + \u000fphase N 2 (1 + \u000fphase )N \u22121 k\u03c6kL\u221e\nP\u03bc0 [T s > t]\n\u0002\n\u0003\ne\u2212\u03bb1 (t\u2212tcorr )+ P\u03bc0 [T s > tcorr ]\n+ \u000fcorr + \u000fphase N (1 + \u000fphase )N \u22121 k\u03c6kL\u221e\nP\u03bc0 [T s > t]\n\u0002\n\u0003\ne\u2212\u03bb1 (t\u2212tcorr )+ P\u03bc0 [T s > tcorr ]\n. \u000fcorr + \u000fphase N 2 (1 + \u000fphase )N \u22121 k\u03c6kL\u221e\n.\nP\u03bc0 [T s > t]\n\u2264\n\nSince (t \u2212 tcorr )+ + tcorr \u2265 t,\nP\u03bc0 [T s > (t \u2212 tcorr )+ + tcorr ] \u2264 P\u03bc0 [T s > t] .\nTherefore,\nP\u03bc0 [T s > tcorr ]\nP\u03bc0 [T s > tcorr ]\n1\n\u2264\n=\n\u03bc\ns\n\u03bc\ns\n\u03bc\ns\nP 0 [T > t]\nP 0 [T > (t \u2212 tcorr )+ + tcorr ]\nP corr [T > (t \u2212 tcorr )+ ]\nand\n\ne\u2212\u03bb1 (t\u2212tcorr )+ P\u03bc0 [T s > tcorr ]\n1\n\u2264\n.\n\u03bc\ns\n0\nP [T > t]\n1 \u2212 \u000fcorr\nSubstituting this estimate into (5.20) yields (5.12).\n\n\f26\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\n\u0003\n6. Discussion\nWe have proven several theorems on the convergence of the exit distributions of parallel replica dynamics to the underlying unaccelerated\nproblem. We have also demonstrated the effectiveness of a dephasing\nalgorithm done in conjunction with the decorrelation step. However,\nthere remain several problems associated with ParRep, both in fully\njustifying it as an algorithm, and implementing it in practice.\n6.1. Error Estimates. As we pointed out in the introduction, the\nerror estimates in Theorem 1.3 and Theorem 1.4 include terms which\ngrow as N \u2192 \u221e. If we take\ntphase & kphase\n\nlog N\n\u03bb2 \u2212 \u03bb1\n\nfor some multiplier, kphase , then the most egregious term in the estimates is bounded by\nlim N 2 \u000fphase (1 + \u000fphase )N \u22121 \u2264 lim Cphase e\u2212kphase /2 1 + e\u2212kphase Cphase /N\n\nN \u2192\u221e\n\nN \u2192\u221e\n\n\u2212kphase\n\n= e\u2212kphase /2 eCphase e\n\n.\n\nHence, taking kphase large enough, the error can be made arbitrarily\nsmall. In contrast, the decorrelation error is independent of N , and\nreducing the decorrelation error will not correct for the error due to\nmore replicas.\nThe error estimate on the exit time in Theorem 1.3 is a bit deceiving\nand merits additional comment. It would appear that when we consider\nthis cumulative distribution function at any t > 0, then, sending N \u2192\n\u221e, the error vanishes. This is a reflection on the estimate being an\nabsolute error. Dividing out by e\u2212N \u03bb1 t lets us evaluate the relative\nerror, which we see is uniformly bounded in t.\nWe also remark that since\nZ \u221e\nE[T ] =\nP[T > t]dt,\n0\n\nwe can obtain error estimates on the expected exit time. Using the\nestimates in Theorem 1.3, we see that provided N \u000fphase (1+\u000fphase )N \u22121 <\n1, we have\n(6.1)\n\nE\u03bcphase [T ? ] \u2212\n\n1\n\u2264 N \u000fphase (1 + \u000fphase )N \u22121 .\nN \u03bb1\n\n\u0001N \u22121\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n27\n\nSimilarly, using the estimates in Theorem 1.4,\n(6.2)\n\n|E\u03bc0 [T s ] \u2212 E\u03bc0 [T p ]| . \u000fcorr + N \u000fphase (1 + \u000fphase )N \u22121 .\n\nIt remains to be determined whether our estimates are sharp \u2013 is the\ngrowth in N real, or an artifact of our analysis? While we cannot yet\naddress the sharpness, a simple numerical experiment indicates that\nthere is growth in the error as N increases. Consider the problem\n\u221a\n(6.3)\ndXt = \u22124Xt dt + 2dBt\nfor the well W = [\u22121, 1], and suppose we launch N replicas from the\nDirac distribution X0 = .1. By symmetry, we know that if we had\nperfect dephasing, then during the parallel step\nP\u03bd [XT? ? = 1] = P\u03bd [XT? ? = \u22121] = 21 .\nBut if we incompletely dephase, then, because of our asymmetric initial\ncondition, we expect a higher probability of escaping at 1 than \u22121. For\nthis problem, we can compute by spectral methods that \u03bb1 \u2248 0.971972\nand \u03bb2 \u2248 8.98262.\nTo test our conjecture, that the error increases with N , we ran 10000\nrealizations of the dephasing and parallel steps with values of N =\n100, 200, . . . 1000. We employed Euler-Maruyama time stepping with\n\u2206t = 10\u22124 . We then ran this with with tphase = .05, .1 and .2. The\nresults appear in Figure 3.\nAs we predicted, the errors decrease as tphase increases. For the\nsmallest dephasing time, we also see the error increase with N . At\ntphase = .1, there is still some increase in the error as N increases,\nthough it is less dramatic. When tphase = .2, the trend appears to have\nbeen lost to numerical error and sampling variability.\n6.2. Numerical Parameters & Eigenvalues. An essential question\nis how to choose of the dephasing and decorrelation time parameters.\nBased on the arguments in the preceding section, roughly, if we desire\nthe errors from decorrelation and dephasing to be of the same order,\nthen,\n2 log(N )tcorr \u223c tphase .\nSo, while they should not be the same, if we can estimate one, we\ncan infer the other. There will also be some mismatch due to different starting distributions for the reference process and the dephasing\nreplicas.\ntcorr must be large enough so as to be representative of the QSD\nwhile remaining computationally efficient. Taking too large a value of\ntcorr will just replicate the serial implementation with no acceleration.\n\n\f28\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n0.34\n\n0.21\n\n0.33\n0.2\n\n0.31\n\nP \u03bd[X T\u0002 \u0002 = 1] \u2212 1/2\n\nP \u03bd[X T\u0002 \u0002 = 1] \u2212 1/2\n\n0.32\n\n0.3\n0.29\n0.28\n0.27\n0.26\n\n0.19\n\n0.18\n\n0.17\n\n0.16\n\n0.25\n0.24\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n0.15\n\n200\n\n400\n\nN\n\n600\n\n800\n\n1000\n\nN\n\n(a) tphase = .05\n\n(b) tphase = .1\n\n0.085\n\nP \u03bd[X T\u0002 \u0002 = 1] \u2212 1/2\n\n0.08\n\n0.075\n\n0.07\n\n0.065\n\n0.06\n\n0.055\n\n200\n\n400\n\n600\n\n800\n\n1000\n\nN\n\n(c) tphase = .2\n\nFigure 3. Three experiments on the impact of imperfect dephasing for (6.3). With perfect dephasing, the\nprobability of exiting at x = 1 would be 1/2, but because the initial condition is .1 and the dephasing time\nis finite, there is a persistent bias and growth in the error\nas N increases. 95% confidence intervals are plotted for\n10000 realizations of each value of N .\nTheorem 3.1 provides some insight, already discussed in [4]. The error\nof \u03bccorr is controlled by the following quantities:\n\u2022 The \u03bc0 initial distribution,\n\u2022 P[2,\u221e) \u03bc0 H\u03bc\u2212s , the mismatch between the initial distribution of\nthe reference process and the quasistationary distribution, \u03bd;\n\u2022 The\nvalue of t;\nR\n\u2022 u1 d\u03bc0 ;\n\u2022 \u03bb2 \u2212 \u03bb1 , the spectral gap between the first two eigenvalues.\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n29\n\nBased on these quantities, and how they relate to tcorr , to make the\ndecorrelation error small, we would certainly need\nh R\ni\n\u0001\u22121\nln\nu1 d\u03bc0\nP[2,\u221e) \u03bc0 H\u03bc\u2212s\n(6.4)\ntcorr &\n+ t.\n\u03bb2 \u2212 \u03bb1\nThe eigenvalues also play an important role in determining which\nproblems would benefit from ParRep is an outstanding problem, which\nis an outstanding issue. For ParRep to be efficient, we need\n(6.5)\n\ntcorr \u001c E\u03bccorr [T ] \u223c E\u03bd [T ] =\n\n1\n.\n\u03bb1\n\nThis is desirable because, in the event Xt does not leave the well during\nthe decorrelation step, it is will now take a comparatively long time to\nexit. In [4], the authors suggested\ntcorr \u2264 E\u03bc0 [T ].\nHowever, this can be problematic, depending on \u03bc0 . As previously discussed, if the replicas launch from a position too close to the boundary,\nE\u03bc0 [T ] might be rather small. This is mitigated as tcorr becomes larger,\nleading to E\u03bccorr [T ] approaching the escape time of the QSD, \u03bb\u22121\n1 .\nWe can see from (6.4) and constraint (6.5) that ParRep will be most\neffective when\n1\n1\n(6.6)\n\u001c ,\n\u03bb2 \u2212 \u03bb1\n\u03bb1\nor, alternatively, when \u03bb1 \u001c \u03bb2 . Under these conditions, \u03bccorr converges to \u03bd much more rapidly than we expect Xt to exit W . (6.6)\ncan also be viewed as a characterization of when W corresponds to a\nmetastable state for (1.1).\nComputing \u03bb1 and \u03bb2 directly from a discretization of the elliptic\noperator L is intractable for all but the lowest dimensional systems.\nInstead, one must use Monte Carlo methods, such as those found in\n[11, 12, 17, 18, 23]. However, these studies, some of which use branching\nparticles processes like Fleming-Viot (discussed below), only yield \u03bb1 .\nIn a forthcoming work, we explore a mechanism for computing \u03bb2 \u2212\u03bb1\nusing observables. The idea stems from calculations in Theorem 1.1,\nthat, for an observable O(x), as t \u2192 \u221e,\nZ\n\u03bc0\n(6.7) E [O(Xt ) | T > t] =\nO(x)d\u03bd(x) + C(\u03bc0 , O)e\u2212(\u03bb2 \u2212\u03bb1 )t + . . .\nW\n\nIn principle, \u03bb2 \u2212\u03bb1 could be extracted from a time series of E\u03bc0 [O(Xt ) | T > t].\nThis introduces a variety of questions, such as what observables to use\n\n\f30\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nand how to perform such a fitting. Thus, we will have a method for\ndynamically estimating tcorr and tphase .\n6.3. Dephasing Mechanism. The efficiency of our dephasing algorithm can be improved by the availability of multiple processors. For\ninstance, assume we have N processors available for the replicas and\nthat k replicas have successfully been run until tcorr . We are still waiting\nfor N \u2212 k replicas to successfully dephase. Rather than let k processors\nsit idle, they could record the successful replicas, and run independent\nrealizations. As more replicas finish dephasing, more processors can be\nbrought to bear on the outstanding replicas.\nIn practice, as replicas are deemed to have been successfully dephased, they are promoted to the parallel step, [21]. Thus, there is\nno bottleneck at the dephasing step from waiting to get N realizations\ndephased.\nThere are other approaches to dephasing too, such as Fleming-Viot\nor Moran branching interacting particle processes, [2, 3, 9, 15]. These\nmerit consideration for ParRep. These approaches, which randomly\nsplit a surviving process every time another process exits the well, can\nprovide additional information, such as an estimate of \u03bb1 . Moreover, no\nprocessor sits idle at anytime. However, two challenges are introduced.\nOn a practical level, one needs to implement additional communication routines and synchronization across the processors to request and\nsend configurations as trajectories are killed. The second challenge is\nanalytical, as the dephased processes will now be only approximately\nindependent. This complicates the analysis of the how the error in the\ndephasing step cascades through the parallel step.\n6.4. Other Challenges. Another task is to assess the cumulative error over many ParRep cycles. The hitting point distribution will be\nperturbed by the algorithm, meaning that the sequence in which the\nstates are visited would also be perturbed. Quantifying the error across\nmany steps, and showing that it may be made small, would complete\nthe justification of ParRep over the lifetime of a simulation. But to\nbegin such a study, one must decide how to measure\ndist(St , StParRep ).\nThe problem is St is not a Markovian process. A particle that sits\nnear the edge of the well is likely to exit much sooner than one which\nis near the minima of the well. But that information is lost in the\ncoarse graining. Knowing how long Xt has been in the well provides\nsome amount of information; it tells us the proximity to the QSD, from\nwhich we can get an exponential exit time.\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n31\n\nDespite the challenge of studying the coarse grained flow, we can\nreport that ParRep appears to work as predicted over multiple wells.\nConsider the flow\n\u221a\n(6.8)\ndXt = \u22122\u03c0 sin(\u03c0Xt )dt + 2dBt .\nFor this equation, with initial condition X0 = 0, we examined the time\nit would take to reach the wells centered at x = \u00b110. In other words,\nwe sought to compute\nT\u00b110 = inf {t | |Xt | \u2265 9} .\nFor this problem, we ran the full ParRep algorithm (decorrelation,\ndephasing and parallel steps) within each well. During dephasing, the\nreplicas were initiated from the minima of the present well, 0, \u00b12\u03c0, \u00b14\u03c0, . . .\nWe ran 10000 realizations of this experiment, varying kcorr and kphase ,\nwhere\nkcorr\nkphase\n(6.9)\ntcorr =\n, tphase =\n.\n\u03bb2 \u2212 \u03bb1\n\u03bb2 \u2212 \u03bb1\nSince the wells are periodic, we can use spectral methods to compute\n\u03bb1 \u2248 .202280 and \u03bb2 \u2248 16.2588 once, and we then have these values\nfor all the wells. The results, with \u2206t = 10\u22124 and N = 100 replicas,\nappear in Figure 4\nAs we expect, for sufficiently large values of tcorr and tphase , the distributions agree with the serial process. Indeed, in the cases kcorr =\nkphase = 5 and kcorr = 1, kphase = 5, the exit times agree with the\nserial realization at 5% significance level under a Kolmogorov-Smirnov\ntest. In addition, this experiment also supports our calculations that,\nthrough the dephasing error, the total error should be magnified by N\nsince increasing the dephasing time improves the fit much more than\nincreasing the decorrelation time does.\nFinally, we remark that we have only analyzed the continuous in time\nproblem, though we are ultimately interested in the associated discrete\nin time algorithm. Much of the analysis carries over to the discrete in\ntime case. A discrete in time quasistationary distribution exists, and\nthere are extensive results on using interacting particle algorithms for\ndephasing, [9, 10]. As in the continuous in time case, there remains\nthe subtlety of how to analyze the parallel step when the dephased\nensemble is only approximately independent.\nHowever, the discrete time step introduces other subtleties. Assume\none uses Euler-Maruyama time discretization with time step \u2206t, and\ndefine the exit time as\n(6.10)\n\nT \u2206t = inf {tn | Xtn \u2208\n/ W}.\n\n\f32\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n0\n\n10\n\nSe r ial\nk c orr=\nk c orr=\nk c orr=\nk c orr=\n\n\u22121\n\n1, k p h a s e=\n1, k p h a s e=\n5, k p h a s e=\n5, k p h a s e=\n\n1\n5\n1\n5\n\nP [T \u00b1 1 0 > t ]\n\n10\n\n\u22122\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n0\n\n500\n\n1000\nt\n\n1500\n\n2000\n\nFigure 4. The cumulative distribution for the time for\nit takes trajectory (6.8) to reach the wells centered at\n\u00b110. 10000 realizations of each case were run with time\nstep \u2206t = 10\u22124 . tcorr and tphase relate to kcorr and kphase\nvia (6.9). As expected, larger values of tcorr and tphase\ngive better agreement with an unaccelerated process.\nFor a uniform time step, we see that with no acceleration of the dynamics, the exit times are integer multiples of \u2206t. For ParRep, this remains\ntrue for exits that take place during the decorrelation step. But for exit\ntimes taking place during the parallel step, the exit times will be determined by multiples of N \u2206t. With a large number of processors,\nthis effective time step could be quite large. When comparing against\nthe continuous in time problem, the error of discretization could be\nmagnified in ParRep. In the preceding experiment, N \u2206t = .01, which\nis small relative to the exit time scale (1/\u03bb1 \u2248 4.9) and the decorrelation time scale (1/(\u03bb2 \u2212 \u03bb1 ) \u2248 .062). Clearly, the discrete in time case\nwarrants a thorough investigation.\nAppendix A. Summation Bounds\nMuch of our analysis relies on bounding series solutions, (2.6), of\n(2.2), to obtain information about Xt through the Feynman-Kac equation, (2.3). The key estimates needed in our work stem from Weyl's\nLaw for L:\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n33\n\nProposition A.1 (Weyl's Law for L). There exist positive constants\nc1 and c2 , independent of k, such that the eigenvalues of (2.1) satisfy\nc1 k 2/n \u2264 \u03bbk \u2264 c2 k 2/n .\n\n(A.1)\n\nRecall that n denotes the dimension of the underlying problem; Xt \u2208\nRn .\nProof. We will not reproduce the proof here, which is accomplished by\nrewriting the eigenvalue problem as\n\u0001\n(A.2)\n\u2212 \u03b2 \u22121 \u2207 * e\u2212\u03b2V \u2207u = \u03bbe\u2212\u03b2V u.\nThis is justified because V is smooth and W is bounded; thus e\u2212\u03b2V is\nsmooth and nondegenerate. This is now in the form of Theorem 6.3.1\nof [8] on Weyl's Law, yielding the result.\n\u0003\nUsing Weyl's Law, we have our main summation result,\nProposition A.2. Given s \u2265 0, let a = (a1 , a2 , . . .) satisfy\n(\u221e\n)1/2\nX\n2\n\u03bbk\u2212s |ak |\n= kakH\u03bc\u2212s < \u221e.\nk=1\n\nLet f be defined as\nf (\u03c4 ) \u2261\n\n(A.3)\n\n\u221e\nX\n\nak \u03bb\u03b1k e\u2212\u03c4 \u03bbk .\n\nk=1\n\nFor a > 0, we have:\nA.\n(A.4)\n\nsup |f (\u03c4 )| . a\u2212n/4\u2212max{s/2+\u03b1,0} kakH\u03bc\u2212s < \u221e;\n\u03c4 \u2265a\n\nB. The convergence of the series is uniform in \u03c4 \u2265 a;\nC. f is continuous.\nTo prove Proposition A.2, we first have the following lemma.\nLemma A.1. Let \u03bbk be the eigenvalues and eigenfunctions of L, (2.1).\nThere exists a constant C > 0, independent of \u03c4 , such that for all \u03c4 > 0,\n\u221e\nX\n(A.5)\n\u03bb\u03b1k e\u2212\u03c4 \u03bbk \u2264 C\u03c4 \u2212n/2\u2212max{\u03b1,0} .\nk=1\n\nThe reader should rightfully expect the lefthand side of (A.5) to\ngrow as \u03b1 \u2192 \u221e. Indeed, the constant C depends on \u03b1 and will grow.\nHowever, as \u03b1 is fixed, and we are interested in an estimate in \u03c4 , this\nis suppressed.\n\n\f34\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nProof. For \u03b1 \u2264 0,\n\u221e\nX\n\ne\u2212\u03c4 \u03bbk \u03bb\u03b1k \u2264\n\nk=1\n\n\u221e\nX\n\ne\u2212\u03c4 \u03bbk \u03bb\u03b11 \u2264\n\nk=1\n\n\u2264 \u03bb\u03b11\n\n\u221e\nX\n\ne\u2212c1 \u03c4 k\n\n2/n\n\n\u03bb\u03b11\n\nk=1\n\u221e\n\nZ\n\ne\u2212c1\n\n\u03c4 k2/n\n\n0\n\nh\nni\ndk = \u03bb\u03b11 (c1 \u03c4 )\u2212n/2 \u0393 1 +\n.\n2\n\nIn the above computation, we approximated the sum as the lower Riemann sum of the integral.\nFor \u03b1 > 0, we begin by estimating\n\u221e\nX\n\ne\u2212\u03c4 \u03bbk \u03bb\u03b1k \u2264\n\n\u221e\nX\n\ne\u2212c1 \u03c4 k\n\n2/n\n\nc\u03b12 k 2\u03b1/n .\n\nk=1\n\nk=1\n\nFor sufficiently large k,\n&\u0012\nk \u2265 k1 \u2261\n\n\u03b1\nc1 \u03c4\n\n\u0013n/2 '\n,\n\nthe summand is monotonically decreasing, while for k < k1 , it is monotonically increasing. Splitting the sum up,\n\u221e\nX\n\ne\n\n\u2212c1 \u03c4 k2/n 2\u03b1/n\n\nk\n\nk=1\n\n=\n\nk1\nX\n\n\u2212c1 \u03c4 k2/n 2\u03b1/n+1\n\ne\n\nk\n\n+\n\nk=1\n\n\u2264e\n\n\u2212c1 \u03c4\n\n\u221e\nX\n\ne\u2212c1 \u03c4 k\n\n2/n\n\nk 2\u03b1/n\n\nk=k1 +1\nk1\nX\n\nk\n\n2\u03b1/n\n\n+\n\n\u221e\nX\n\ne\u2212c1 \u03c4 k\n\n2/n\n\nk 2\u03b1/n .\n\nk=k1 +1\n\nk=1\n\nCrudely bounding the first sum in terms of a max, and treating the\nlatter sum as a lower Riemann approximations of an integral,\nZ \u221e\n\u221e\nX\n2/n\n2\u03b1/n\n\u2212c1 \u03c4 k2/n 2\u03b1/n\n\u2212c1 \u03c4\ne\nk\n\u2264e\nk1 * k1\n+\ne\u2212c1 \u03c4 k k 2\u03b1/n dk\nk1\n\nk=1\n\u2212c1 \u03c4\n\n\"\u0012\n\n\u2264e\n\n\u03b1\nc1 \u03c4\n\n#2\u03b1/n+1\n\n\u0013n/2\n+1\n\nZ\n+\n\n\u221e\n\ne\u2212c1 \u03c4 k\n\n2/n\n\nk 2\u03b1/n dk\n\n0\n\n\u0014\n\u0010 c \u03c4 \u0011\u2212n/2\u2212\u03b1\n\u0010 c \u03c4 \u0011n/2 \u00152\u03b1/n+1 n\nhn\ni\n1\n1\n\u2212c1 \u03c4\n\u2264\ne\n1+\n+ (c1 \u03c4 )\u2212n/2\u2212\u03b1 \u0393\n+\u03b1\n\u03b1\n\u03b1\n2\n2\n. \u03c4 \u2212n/2\u2212\u03b1 .\n\u0003\nThe integrals were computed using Mathematica, with the commands\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n35\n\nIntegrate[Exp[-c*t*k^(2/n)],{k,0,Infinity}]\nIntegrate[Exp[-c*t*k^(2/n)]*k^(2*a/n+1),{k,0,Infinity}]\nNow we prove Proposition A.2.\nProof. We first observe that f is well defined and bounded:\n(\u221e\n)1/2\n\u221e\nX\nX\n|f (\u03c4 )| \u2264\n|ak | \u03bb\u03b1k e\u2212\u03c4 \u03bbk \u2264\n\u03bbs+2\u03b1 e\u22122\u03bbk \u03c4\nkakH\u03bc\u2212s .\nk=1\n\nk=1\n\nApplying Lemma A.1 with \u03b1 7\u2192 s + 2\u03b1 and \u03c4 7\u2192 2a,\n\u221e\nX\n\u03bbs+2\u03b1 e\u22122\u03bbk a . (2a)\u2212n/2\u2212max{s+2\u03b1,0} .\nk=1\n\nTo prove uniform convergence, let\nm\nX\nfm (\u03c4 ) \u2261\nak \u03bb\u03b1k e\u2212\u03c4 \u03bbk\nk=1\n\ndenote the partial sum. Obviously, each partial sum is continuous in\n\u03c4 . Then\n\u221e\nX\n|f (\u03c4 ) \u2212 fm (\u03c4 )| \u2264\n|ak | \u03bb\u03b1k e\u2212\u03c4 \u03bbk\nk=m+1\n\n(\n\u2264\n\n)1/2\n\n\u221e\nX\n\n\u03bbs+2\u03b1\ne\u22122\u03bbk a\nk\n\nP[m+1,\u221e) a\n\nH\u03bc\u2212s\n\nk=m+1\n\n(\n\u2264 kakH\u03bc\u2212s\n\n\u221e\nX\n\n)1/2\n\u03bbs+2\u03b1\ne\u22122\u03bbk a\nk\n\n.\n\nk=m+1\n\nExamining the sum,\n\u221e\n\u221e\nX\nX\n2/n\n\u22122\u03bbk a\n\u03bbs+2\u03b1\ne\n.\nk 2s/n+4\u03b1/n e\u22122c1 ak .\nk\nk=m+1\n\nk=m+1\n\nTaking m sufficiently large, the summand will be strictly decreasing in\nk, so we can treat it as a lower Riemann sum for the integral\nZ \u221e\n2/n\nk 2s/n+4\u03b1/n e\u22122c1 ak dk.\nm\n\nChanging variables by letting k 2/n = l,\nZ \u221e\n\u221e\nX\ns+2\u03b1 \u22122\u03bbk a\n\u03bbk e\n.\nls+2\u03b1+n/2\u22121 e\u22122c1 al dl.\nk=m+1\n\nm2/n\n\n\f36\n\nGIDEON SIMPSON AND MITCHELL LUSKIN\n\nIf s + 2\u03b1 + n/2 \u2212 1 \u2264 0, then\n\u221e\nX\n\n\u03bbs+2\u03b1\ne\u22122\u03bbk a\nk\n\nZ\n\n\u221e\n\n.\n\ne\u22122c1 al dl =\n\nm2/n\n\nk=m+1\n\n1 \u22122m2/n c1 a\ne\n.\n2c1 a\n\nOn the other hand, if s + 2\u03b1 + n/2 \u2212 1 > 0, we can trade some of the\nexponential decay to eliminate the algebraic term,\nZ \u221e\n\u221e\nX\n1 \u2212m2/n c1 a\ns+2\u03b1 \u22122\u03bbk a\ne\u2212c1 al dl =\n\u03bbk e\n.\ne\n.\nc1 a\nm2/n\nk=m+1\nIn either case, we see that for any a > 0,\nlim sup |f (\u03c4 ) \u2212 fm (\u03c4 )| = 0.\n\nm\u2192\u221e \u03c4 \u2265a\n\nSince the partial sums converge uniformly to f , it is now a classical\nresult to conclude that f is continuous for \u03c4 \u2265 a > 0, [24].\n\u0003\nReferences\n[1] R.A. Adams and J.J.F. Fournier. Sobolev spaces, volume 140. Academic Press,\n2003.\n[2] M. Bieniek, K. Burdzy, and S. Finch. Non-extinction of a Fleming-Viot particle\nmodel. Probability Theory and Related Fields, June 2011.\n[3] M. Bieniek, K. Burdzy, and S. Pal. Extinction of Fleming-Viot-type particle\nsystems with strong drift. Electronic Journal of Probability, 17(0), January\n2012.\n[4] C. Le Bris, T. Leli\u00e8vre, M. Luskin, and D. Perez. A mathematical formalization of the parallel replica dynamics. Monte Carlo Methods Appl., to appear.\narXiv:1105.4636v1.\n[5] P. Cattiaux, P. Collet, A. Lambert, S. Mart\u0131\u0301nez, S. M\u00e9l\u00e9ard, and J. San Mart\u0131\u0301n.\nQuasi-stationary distributions and diffusion models in population dynamics.\nThe Annals of Probability, 37(5):1926\u20131969, 2009.\n[6] P. Cattiaux and S. M\u00e9l\u00e9ard. Competitive or weak cooperative stochastic LotkaVolterra systems conditioned on non-extinction. Journal Of Mathematical Biology, 60(6):797\u2013829, 2010.\n[7] P. Collet, S. Mart\u0131\u0301nez, and J. San Mart\u0131\u0301n. Asymptotic laws for one-dimensional\ndiffusions conditioned to nonabsorption. The Annals of Probability, 23(3):1300\u2013\n1314, 1995.\n[8] E.B. Davies. Spectral theory and differential operators, volume 42. Cambridge\nUniversity Press, 1996.\n[9] P. Del Moral. Feynman-Kac Formulae: Genealogical and Interacting Particle\nSystems with Applications (Probability and Its Applications). Springer, 2004\nedition, March 2011.\n[10] P. Del Moral and A. Doucet. Particle motions in absorbing medium with\nhard and soft obstacles. Stochastic Analysis and Applications, 22(5):1175\u20131207,\n2004.\n\n\fNUMERICAL ANALYSIS OF PARALLEL REPLICA DYNAMICS\n\n37\n\n[11] P. Del Moral and L. Miclo. Particle approximations of Lyapunov exponents\nconnected to Schr\u00f6dinger operators and Feynman-Kac semigroups. ESAIM.\nProbability and Statistics, 7:171\u2013208, 2003.\n[12] M. El Makrini, B. Jourdain, and T. Lelievre. Diffusion Monte Carlo method:\nNumerical analysis in a simple case. ESAIM: M2AN, 41(2):189\u2013213, 2007.\n[13] L.C. Evans. Partial Differential Equations. American Mathematical Society,\n2002.\n[14] D. Gilbarg and N.S. Trudinger. Elliptic partial differential equations of second\norder, volume 224. Springer Verlag, 2001.\n[15] I. Grigorescu and M. Kang. Hydrodynamic limit for a Fleming-Viot type system. Stochastic Processes and Their Applications, 110(1):111\u2013143, 2004.\n[16] D. Haroske and H. Triebel. Distributions, Sobolev spaces, elliptic equations.\nEuropean Mathematical Society, 2008.\n[17] A. Lejay and S. Maire. Computing the principal eigenvalue of the Laplace\noperator by a stochastic method. Mathematics And Computers In Simulation,\n73(6):351\u2013363, 2007.\n[18] A. Lejay and S. Maire. Computing the principal eigenelements of some linear\noperators using a branching Monte Carlo method. Journal Of Computational\nPhysics, 227(23):9794\u20139806, 2008.\n[19] S. Mart\u0131\u0301nez and J. San Mart\u0131\u0301n. Quasi-stationary distributions for a Brownian\nmotion with drift and associated limit laws. Journal of Applied Probability,\n31(4):911\u2013920, 1994.\n[20] S. Mart\u0131\u0301nez and J. San Mart\u0131\u0301n. Classification of killed one-dimensional diffusions. The Annals of Probability, 32(1A):530\u2013552, 2004.\n[21] D. Perez. Implementation of Parallel Replica Dynamics, April 2012. Personal\nCommunication.\n[22] D. Perez, B.P. Uberuaga, Y. Shim, J.G. Amar, and A.F. Voter. Accelerated\nmolecular dynamics methods: introduction and recent developments. Annual\nReports in Computational Chemistry, 5:79\u201398, 2009.\n[23] M. Rousset. On the control of an interacting particle estimation of Schr\u00f6dinger\nground states. SIAM Journal on Mathematical Analysis, 38(3):824\u2013844 (electronic), 2006.\n[24] W. Rudin. Principles of Mathematical Analysis. McGraw-Hill, 1976.\n[25] D. Steinsaltz and S.N. Evans. Quasistationary distributions for onedimensional diffusions with killing. Transactions of the American Mathematical\nSociety, 359(3):1285\u20131324 (electronic), 2007.\n[26] A.F. Voter. Parallel replica method for dynamics of infrequent events. Phys.\nRev. B, 57(22):13985\u201313988, Jan 1998.\n[27] A.F. Voter, F. Montalenti, and T.C. Germann. Extending the time scale in\natomistic simulation of materials. Annual Review of Materials Science, 32:321\u2013\n346, Jan 2002.\nE-mail address: gsimpson@umn.edu\nE-mail address: luskin@umn.edu\n\n\f"}