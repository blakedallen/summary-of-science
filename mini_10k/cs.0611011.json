{"id": "http://arxiv.org/abs/cs/0611011v1", "guidislink": true, "updated": "2006-11-02T18:44:49Z", "updated_parsed": [2006, 11, 2, 18, 44, 49, 3, 306, 0], "published": "2006-11-02T18:44:49Z", "published_parsed": [2006, 11, 2, 18, 44, 49, 3, 306, 0], "title": "Hedging predictions in machine learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0006043%2Ccs%2F0006021%2Ccs%2F0006006%2Ccs%2F0006028%2Ccs%2F0006013%2Ccs%2F0006012%2Ccs%2F0006042%2Ccs%2F0006033%2Ccs%2F0006038%2Ccs%2F0006009%2Ccs%2F0006037%2Ccs%2F0006016%2Ccs%2F0006041%2Ccs%2F0006015%2Ccs%2F0006039%2Ccs%2F0006047%2Ccs%2F0006029%2Ccs%2F0006044%2Ccs%2F0006003%2Ccs%2F0611055%2Ccs%2F0611144%2Ccs%2F0611090%2Ccs%2F0611051%2Ccs%2F0611021%2Ccs%2F0611078%2Ccs%2F0611061%2Ccs%2F0611156%2Ccs%2F0611085%2Ccs%2F0611044%2Ccs%2F0611149%2Ccs%2F0611052%2Ccs%2F0611066%2Ccs%2F0611064%2Ccs%2F0611092%2Ccs%2F0611060%2Ccs%2F0611095%2Ccs%2F0611026%2Ccs%2F0611079%2Ccs%2F0611001%2Ccs%2F0611150%2Ccs%2F0611080%2Ccs%2F0611081%2Ccs%2F0611025%2Ccs%2F0611007%2Ccs%2F0611137%2Ccs%2F0611027%2Ccs%2F0611134%2Ccs%2F0611048%2Ccs%2F0611121%2Ccs%2F0611154%2Ccs%2F0611022%2Ccs%2F0611038%2Ccs%2F0611097%2Ccs%2F0611113%2Ccs%2F0611152%2Ccs%2F0611133%2Ccs%2F0611120%2Ccs%2F0611029%2Ccs%2F0611063%2Ccs%2F0611014%2Ccs%2F0611086%2Ccs%2F0611083%2Ccs%2F0611106%2Ccs%2F0611004%2Ccs%2F0611076%2Ccs%2F0611088%2Ccs%2F0611124%2Ccs%2F0611035%2Ccs%2F0611148%2Ccs%2F0611009%2Ccs%2F0611122%2Ccs%2F0611037%2Ccs%2F0611050%2Ccs%2F0611040%2Ccs%2F0611068%2Ccs%2F0611023%2Ccs%2F0611075%2Ccs%2F0611111%2Ccs%2F0611057%2Ccs%2F0611039%2Ccs%2F0611049%2Ccs%2F0611077%2Ccs%2F0611129%2Ccs%2F0611030%2Ccs%2F0611108%2Ccs%2F0611045%2Ccs%2F0611042%2Ccs%2F0611017%2Ccs%2F0611036%2Ccs%2F0611058%2Ccs%2F0611008%2Ccs%2F0611112%2Ccs%2F0611018%2Ccs%2F0611104%2Ccs%2F0611011%2Ccs%2F0611094%2Ccs%2F0611141%2Ccs%2F0611006%2Ccs%2F0611065%2Ccs%2F0611015%2Ccs%2F0611091&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Hedging predictions in machine learning"}, "summary": "Recent advances in machine learning make it possible to design efficient\nprediction algorithms for data sets with huge numbers of parameters. This paper\ndescribes a new technique for \"hedging\" the predictions output by many such\nalgorithms, including support vector machines, kernel ridge regression, kernel\nnearest neighbours, and by many other state-of-the-art methods. The hedged\npredictions for the labels of new objects include quantitative measures of\ntheir own accuracy and reliability. These measures are provably valid under the\nassumption of randomness, traditional in machine learning: the objects and\ntheir labels are assumed to be generated independently from the same\nprobability distribution. In particular, it becomes possible to control (up to\nstatistical fluctuations) the number of erroneous predictions by selecting a\nsuitable confidence level. Validity being achieved automatically, the remaining\ngoal of hedged prediction is efficiency: taking full account of the new\nobjects' features and other available information to produce as accurate\npredictions as possible. This can be done successfully using the powerful\nmachinery of modern machine learning.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0006043%2Ccs%2F0006021%2Ccs%2F0006006%2Ccs%2F0006028%2Ccs%2F0006013%2Ccs%2F0006012%2Ccs%2F0006042%2Ccs%2F0006033%2Ccs%2F0006038%2Ccs%2F0006009%2Ccs%2F0006037%2Ccs%2F0006016%2Ccs%2F0006041%2Ccs%2F0006015%2Ccs%2F0006039%2Ccs%2F0006047%2Ccs%2F0006029%2Ccs%2F0006044%2Ccs%2F0006003%2Ccs%2F0611055%2Ccs%2F0611144%2Ccs%2F0611090%2Ccs%2F0611051%2Ccs%2F0611021%2Ccs%2F0611078%2Ccs%2F0611061%2Ccs%2F0611156%2Ccs%2F0611085%2Ccs%2F0611044%2Ccs%2F0611149%2Ccs%2F0611052%2Ccs%2F0611066%2Ccs%2F0611064%2Ccs%2F0611092%2Ccs%2F0611060%2Ccs%2F0611095%2Ccs%2F0611026%2Ccs%2F0611079%2Ccs%2F0611001%2Ccs%2F0611150%2Ccs%2F0611080%2Ccs%2F0611081%2Ccs%2F0611025%2Ccs%2F0611007%2Ccs%2F0611137%2Ccs%2F0611027%2Ccs%2F0611134%2Ccs%2F0611048%2Ccs%2F0611121%2Ccs%2F0611154%2Ccs%2F0611022%2Ccs%2F0611038%2Ccs%2F0611097%2Ccs%2F0611113%2Ccs%2F0611152%2Ccs%2F0611133%2Ccs%2F0611120%2Ccs%2F0611029%2Ccs%2F0611063%2Ccs%2F0611014%2Ccs%2F0611086%2Ccs%2F0611083%2Ccs%2F0611106%2Ccs%2F0611004%2Ccs%2F0611076%2Ccs%2F0611088%2Ccs%2F0611124%2Ccs%2F0611035%2Ccs%2F0611148%2Ccs%2F0611009%2Ccs%2F0611122%2Ccs%2F0611037%2Ccs%2F0611050%2Ccs%2F0611040%2Ccs%2F0611068%2Ccs%2F0611023%2Ccs%2F0611075%2Ccs%2F0611111%2Ccs%2F0611057%2Ccs%2F0611039%2Ccs%2F0611049%2Ccs%2F0611077%2Ccs%2F0611129%2Ccs%2F0611030%2Ccs%2F0611108%2Ccs%2F0611045%2Ccs%2F0611042%2Ccs%2F0611017%2Ccs%2F0611036%2Ccs%2F0611058%2Ccs%2F0611008%2Ccs%2F0611112%2Ccs%2F0611018%2Ccs%2F0611104%2Ccs%2F0611011%2Ccs%2F0611094%2Ccs%2F0611141%2Ccs%2F0611006%2Ccs%2F0611065%2Ccs%2F0611015%2Ccs%2F0611091&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Recent advances in machine learning make it possible to design efficient\nprediction algorithms for data sets with huge numbers of parameters. This paper\ndescribes a new technique for \"hedging\" the predictions output by many such\nalgorithms, including support vector machines, kernel ridge regression, kernel\nnearest neighbours, and by many other state-of-the-art methods. The hedged\npredictions for the labels of new objects include quantitative measures of\ntheir own accuracy and reliability. These measures are provably valid under the\nassumption of randomness, traditional in machine learning: the objects and\ntheir labels are assumed to be generated independently from the same\nprobability distribution. In particular, it becomes possible to control (up to\nstatistical fluctuations) the number of erroneous predictions by selecting a\nsuitable confidence level. Validity being achieved automatically, the remaining\ngoal of hedged prediction is efficiency: taking full account of the new\nobjects' features and other available information to produce as accurate\npredictions as possible. This can be done successfully using the powerful\nmachinery of modern machine learning."}, "authors": ["Alexander Gammerman", "Vladimir Vovk"], "author_detail": {"name": "Vladimir Vovk"}, "author": "Vladimir Vovk", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1093/comjnl/bxl065", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cs/0611011v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0611011v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "24 pages; 9 figures; 2 tables; a version of this paper (with\n  discussion and rejoinder) is to appear in \"The Computer Journal\"", "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0611011v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0611011v1", "journal_reference": "Computer Journal, 50:151-177, 2007", "doi": "10.1093/comjnl/bxl065", "fulltext": "arXiv:cs/0611011v1 [cs.LG] 2 Nov 2006\n\nHedging Predictions in Machine Learning\nAlexander Gammerman and Vladimir Vovk\nComputer Learning Research Centre\nDepartment of Computer Science\nRoyal Holloway, University of London\nEgham, Surrey TW20 0EX, UK\n{alex,vovk}@cs.rhul.ac.uk\nFebruary 1, 2008\nAbstract\nRecent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters.\nThis paper describes a new technique for \"hedging\" the predictions output\nby many such algorithms, including support vector machines, kernel ridge\nregression, kernel nearest neighbours, and by many other state-of-the-art\nmethods. The hedged predictions for the labels of new objects include\nquantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional\nin machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular,\nit becomes possible to control (up to statistical fluctuations) the number\nof erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction\nis efficiency: taking full account of the new objects' features and other\navailable information to produce as accurate predictions as possible. This\ncan be done successfully using the powerful machinery of modern machine\nlearning.\n\n1\n\nIntroduction\n\nThe two main varieties of the problem of prediction, classification and regression, are standard subjects in statistics and machine learning. The classical\nclassification and regression techniques can deal successfully with conventional\nsmall-scale, low-dimensional data sets; however, attempts to apply these techniques to modern high-dimensional and high-throughput data sets encounter\nserious conceptual and computational difficulties. Several new techniques, first\nof all support vector machines [18, 19] and other kernel methods, have been\ndeveloped in machine learning recently with the explicit goal of dealing with\nhigh-dimensional data sets with large numbers of objects.\n1\n\n\fA typical drawback of the new techniques is the lack of useful measures\nof confidence in their predictions. For example, some of the tightest upper\nbounds of the popular PAC theory on the probability of error exceed 1 even\nfor relatively clean data sets ([24], p. 249). This paper describes an efficient\nway to \"hedge\" the predictions produced by the new and traditional machinelearning methods, i.e., to complement them with measures of their accuracy\nand reliability. Appropriately chosen, not only are these measures valid and\ninformative, but they also take full account of the special features of the object\nto be predicted.\nWe call our algorithms for producing hedged predictions \"conformal predictors\"; they are formally introduced in Section 3. Their most important property is the automatic validity under the randomness assumption (to be discussed\nshortly). Informally, validity means that conformal predictors never overrate the\naccuracy and reliability of their predictions. This property, stated in Sections 3\nand 5, is formalized in terms of finite data sequences, without any recourse to\nasymptotics.\nThe claim of validity of conformal predictors depends on an assumption\nthat is shared by many other algorithms in machine learning, which we call\nthe assumption of randomness: the objects and their labels are assumed to be\ngenerated independently from the same probability distribution. Admittedly,\nthis is a strong assumption, and areas of machine learning are emerging that\nrely on other assumptions (such as the Markovian assumption of reinforcement\nlearning; see, e.g., [16]) or dispense with any stochastic assumptions altogether\n(competitive on-line learning; see, e.g., [2, 22]). It is, however, much weaker\nthan assuming a parametric statistical model, sometimes complemented with a\nprior distribution on the parameter space, which is customary in the statistical\ntheory of prediction. And taking into account the strength of the guarantees\nthat can be proved under this assumption, it does not appear overly restrictive.\nSo we know that conformal predictors tell the truth. Clearly, this is not\nenough: truth can be uninformative and so useless. We will refer to various\nmeasures of informativeness of conformal predictors as their \"efficiency\". As\nconformal predictors are provably valid, efficiency is the only thing we need to\nworry about when designing conformal predictors for solving specific problems.\nVirtually any classification or regression algorithm can be transformed into a\nconformal predictor, and so most of the arsenal of methods of modern machine\nlearning can be brought to bear on the design of efficient conformal predictors.\nWe start the main part of the paper, in Section 2, with the description of an\nidealized predictor based on Kolmogorov's algorithmic theory of randomness.\nThis \"universal predictor\" produces the best possible hedged predictions but,\nunfortunately, is noncomputable. We can, however, set ourselves the task of\napproximating the universal predictor as well as possible.\nIn Section 3 we formally introduce the notion of conformal predictors and\nstate a simple result about their validity. In that section we also briefly describe\nresults of computer experiments demonstrating the methodology of conformal\nprediction.\nIn Section 4 we consider an example demonstrating how conformal predictors\n2\n\n\freact to the violation of our model of the stochastic mechanism generating the\ndata (within the framework of the randomness assumption). If the model coincides with the actual stochastic mechanism, we can construct an optimal conformal predictor, which turns out to be almost as good as the Bayes-optimal confidence predictor (the formal definitions will be given later). When the stochastic\nmechanism significantly deviates from the model, conformal predictions remain\nvalid but their efficiency inevitably suffers. The Bayes-optimal predictor starts\nproducing very misleading results which superficially look as good as when the\nmodel is correct.\nIn Section 5 we describe the \"on-line\" setting of the problem of prediction,\nand in Section 6 contrast it with the more standard \"batch\" setting. The notion\nof validity introduced in Section 3 is applicable to both settings, but in the online setting it can be strengthened: we can now prove that the percentage of the\nerroneous predictions will be close, with high probability, to a chosen confidence\nlevel. For the batch setting, the stronger property of validity for conformal\npredictors remains an empirical fact. In Section 6 we also discuss limitations of\nthe on-line setting and introduce new settings intermediate between on-line and\nbatch. To a large degree, conformal predictors still enjoy the stronger property\nof validity for the intermediate settings.\nSection 7 is devoted to the discussion of the difference between two kinds\nof inference from empirical data, induction and transduction (emphasized by\nVladimir Vapnik [18, 19]). Conformal predictors belong to transduction, but\ncombining them with elements of induction can lead to a significant improvement in their computational efficiency (Section 8).\nWe show how some popular methods of machine learning can be used as underlying algorithms for hedged prediction. We do not give the full description\nof these methods and refer the reader to the existing readily accessible descriptions. This paper is, however, self-contained in the sense that we explain all\nfeatures of the underlying algorithms that are used in hedging their predictions.\nWe hope that the information we provide will enable the reader to apply our\nhedging techniques to their favourite machine-learning methods.\n\n2\n\nIdeal hedged predictions\n\nThe most basic problem of machine learning is perhaps the following. We are\ngiven a training set of examples\n(x1 , y1 ), . . . , (xl , yl ),\n\n(1)\n\neach example (xi , yi ), i = 1, . . . , l, consisting of an object xi (typically, a vector\nof attributes) and its label yi ; the problem is to predict the label yl+1 of a\nnew object xl+1 . Two important special cases are where the labels are known a\npriori to belong to a relatively small finite set (the problem of classification) and\nwhere the labels are allowed to be any real numbers (the problem of regression).\nThe usual goal of classification is to produce a prediction \u0177l+1 that is likely to\ncoincide with the true label yl+1 , and the usual goal of regression is to produce\n3\n\n\fa prediction \u0177l+1 that is likely to be close to the true label yl+1 . In the case\nof classification, our goal will be to complement the prediction \u0177l+1 with some\nmeasure of its reliability. In the case of regression, we would like to have some\nmeasure of accuracy and reliability of our prediction. There is a clear tradeoff between accuracy and reliability: we can improve the former by relaxing\nthe latter and vice versa. We are looking for algorithms that achieve the best\npossible trade-off and for a measure that would quantify the achieved trade-off.\nLet us start from the case of classification. The idea is to try every possible\nlabel Y as a candidate for xl+1 's label and see how well the resulting sequence\n(x1 , y1 ), . . . , (xl , yl ), (xl+1 , Y )\n\n(2)\n\nconforms to the randomness assumption (if it does conform to this assumption,\nwe will say that it is \"random\"; this will be formalized later in this section). The\nideal case is where all Y s but one lead to sequences (2) that are not random;\nwe can then use the remaining Y as a confident prediction for yl+1 .\nIn the case of regression, we can output the set of all Y s that lead to random\n(2) as our \"prediction set\". An obvious obstacle is that the set of all possible\nY s is infinite and so we cannot go through all the Y s explicitly, but we will see\nin the next section that there are ways to overcome this difficulty.\nWe can see that the problem of hedged prediction is intimately connected\nwith the problem of testing randomness. Different versions of the \"universal\"\nnotion of randomness were defined by Kolmogorov, Martin-L\u00f6f and Levin (see,\ne.g., [6]) based on the existence of universal Turing machines. Adapted to\nour current setting, Martin-L\u00f6f's definition is as follows. Let Z be the set of all\npossible examples; as each example consists of an object and a label, Z = X\u00d7Y,\nwhere X is the set of all possible objects and Y, |Y| > 1, is the set of all possible\nlabels. We will use Z\u2217 as the notation for all finite sequences of examples. A\nfunction t : Z\u2217 \u2192 [0, 1] is a randomness test if\n1. for all \u01eb \u2208 (0, 1), all n \u2208 {1, 2, . . . } and all probability distributions P on\nZ,\nP n {z \u2208 Zn : t(z) \u2264 \u01eb} \u2264 \u01eb;\n(3)\n2. t is upper semicomputable.\nThe first condition means that the randomness test is required to be valid: if,\nfor example, we observe t(z) \u2264 1% for our data set z, then either the data set\nwas not generated independently from the same probability distribution P or a\nrare (of probability at most 1%, under any P ) event has occurred. The second\ncondition means that we should be able to compute the test, in a weak sense (we\ncannot require computability in the usual sense, since the universal test can only\nbe upper semicomputable: it can work forever to discover all patterns in the\ndata sequence that make it non-random). Martin-L\u00f6f (developing Kolmogorov's\nearlier ideas) proved that there exists a smallest, to within a constant factor,\nrandomness test.\n\n4\n\n\fLet us fix a smallest randomness test, call it the universal test, and call the\nvalue it takes on a data sequence the randomness level of this sequence. A random sequence is one whose randomness level is not small; this is rather informal,\nbut it is clear that for finite data sequences we cannot have a clear-cut division of\nall sequences into random and non-random (like the one defined by Martin-L\u00f6f\n[7] for infinite sequences). If t is a randomness test, not necessarily universal,\nthe value that it takes on a data sequence will be called the randomness level\ndetected by t.\nRemark The word \"random\" is used in (at least) two different senses in the\nexisting literature. In this paper we need both but, luckily, the difference does\nnot matter within our current framework. First, randomness can refer to the\nassumption that the examples are generated independently from the same distribution; this is the origin of our \"assumption of randomness\". Second, a data\nsequence is said to be random with respect to a statistical model if the universal test (a generalization of the notion of universal test as defined above) does\nnot detect any lack of conformity between the two. Since the only statistical\nmodel we are interested in this paper is the one embodying the assumption of\nrandomness, we have a perfect agreement between the two senses.\n\nPrediction with Confidence and Credibility\nOnce we have a randomness test t, universal or not, we can use it for hedged prediction. There are two natural ways to package the results of such predictions:\nin this subsection we will describe the way that can only be used in classification\nproblems. If the randomness test is not computable, we can imagine an oracle\nanswering questions about its values.\nGiven the training set (1) and the test object xl+1 , we can act as follows:\n\u2022 consider all possible values Y \u2208 Y for the label yl+1 ;\n\u2022 find the randomness level detected by t for every possible completion (2);\n\u2022 predict the label Y corresponding to a completion with the largest randomness level detected by t;\n\u2022 output as the confidence in this prediction one minus the second largest\nrandomness level detected by t;\n\u2022 output as the credibility of this prediction the randomness level detected\nby t of the output prediction Y (i.e., the largest randomness level detected\nby t over all possible labels).\nTo understand the intuition behind confidence, let us tentatively choose a conventional \"significance level\", such as 1%. (In the terminology of this paper,\nthis corresponds to a \"confidence level\" of 99%, i.e., 100% minus 1%.) If the\nconfidence in our prediction is 99% or more and the prediction is wrong, the\nactual data sequence belongs to an a priori chosen set of probability at most 1%\n5\n\n\fFigure 1: An example of a nested family of prediction sets (casual prediction in\nblack, confident prediction in dark grey, and highly confident prediction in light\ngrey).\n(the set of all data sequences with randomness level detected by t not exceeding\n1%).\nIntuitively, low credibility means that either the training set is non-random\nor the test object is not representative of the training set (say, in the training\nset we have images of digits and the test object is that of a letter).\n\nConfidence Predictors\nIn regression problems, confidence, as defined in the previous subsection, is not\na useful quantity: it will typically be equal to 0. A better approach is to choose\na range of confidence levels 1 \u2212 \u01eb, and for each of them specify a prediction set\n\u0393\u01eb \u2286 Y, the set of labels deemed possible at the confidence level 1 \u2212 \u01eb. We will\nalways consider nested prediction sets: \u0393\u01eb1 \u2286 \u0393\u01eb2 when \u01eb1 \u2265 \u01eb2 . A confidence\npredictor is a function that maps each training set, each new object, and each\nconfidence level 1 \u2212 \u01eb (formally, we allow \u01eb to take any value in (0, 1)) to the\ncorresponding prediction set \u0393\u01eb . For the confidence predictor to be valid the\nprobability that the true label will fall outside the prediction set \u0393\u01eb should not\nexceed \u01eb, for each \u01eb.\nWe might, for example, choose the confidence levels 99%, 95% and 80%, and\nrefer to the 99% prediction set \u03931% as the highly confident prediction, to the\n95% prediction set \u03935% as the confident prediction, and to the 80% prediction\nset \u039320% as the casual prediction. Figure 1 shows how such a family of prediction\nsets might look in the case of a rectangular label space Y. The casual prediction\npinpoints the target quite well, but we know that this kind of prediction can\nbe wrong with probability 20%. The confident prediction is much bigger. If we\nwant to be highly confident (make a mistake only with probability 1%), we must\naccept an even lower accuracy; there is even a completely different location that\nwe cannot rule out at this level of confidence.\n\n6\n\n\fGiven a randomness test, again universal or not, we can define the corresponding confidence predictor as follows: for any confidence level 1 \u2212 \u01eb, the\ncorresponding prediction set consists of the Y s such that the randomness level\nof the completion (2) detected by the test is greater than \u01eb. The condition (3)\nof validity for statistical tests implies that a confidence predictor defined in this\nway is always valid.\nThe confidence predictor based on the universal test (the universal confidence\npredictor ) is an interesting object for mathematical investigation (see, e.g., [21],\nSection 4), but it is not computable and so cannot be used in practice. Our goal\nin the following sections will be to find computable approximations to it.\n\n3\n\nConformal Prediction\n\nIn the previous section we explained how randomness tests can be used for\nprediction. The connection between testing and prediction is, of course, well\nunderstood and have been discussed at length by philosophers [13] and statisticians (see, e.g., the textbook [3], Section 7.5). In this section we will see how\nsome popular prediction algorithms can be transformed into randomness tests\nand, therefore, be used for producing hedged predictions.\nLet us start with the most successful recent development in machine learning,\nsupport vector machines ([18, 19], with a key idea going back to the generalized\nportrait method [20]). Suppose the label space is Y = {\u22121, 1} (we are dealing\nwith the binary classification problem). With each set of examples\n(x1 , y1 ), . . . , (xn , yn )\n\n(4)\n\none associates an optimization problem whose solution produces nonnegative\nnumbers \u03b11 , . . . , \u03b1n (\"Lagrange multipliers\"). These numbers determine the\nprediction rule used by the support vector machine (see [19], Chapter 10, for\ndetails), but they also are interesting objects in their own right. Each \u03b1i ,\ni = 1, . . . , n, tells us how \"strange\" an element of the set (4) the corresponding\nexample (xi , yi ) is. If \u03b1i = 0, (xi , yi ) fits (4) very well (in fact so well that such\nexamples are uninformative, and the support vector machine ignores them when\nmaking predictions). The elements with \u03b1i > 0 are called support vectors, and\nthe large value of \u03b1i indicates that the corresponding (xi , yi ) is an outlier.\nTaking the completion (2) as (4) (so that n = l + 1), we can find the corresponding \u03b11 , . . . , \u03b1l+1 . If Y is different from the actual label yl+1 , we expect\n(xl+1 , Y ) to be an outlier in (2) and so \u03b1l+1 be large as compared with \u03b11 , . . . , \u03b1l .\nA natural way to compare \u03b1l+1 to the other \u03b1s is to look at the ratio\npY :=\n\n|{i = 1, . . . , l + 1 : \u03b1i \u2265 \u03b1l+1 }|\n,\nl+1\n\n(5)\n\nwhich we call the p-value associated with the possible label Y for xl+1 . In\nwords, the p-value is the proportion of the \u03b1s which are at least as large as the\nlast \u03b1.\n7\n\n\fTable 1: Selected test examples from the USPS data set: the p-values of digits\n(0\u20139), true and predicted labels, and confidence and credibility values.\n0\n0.01%\n0.32%\n0.01%\n\n1\n0.11%\n0.38%\n0.27%\n\n2\n0.01%\n1.07%\n0.03%\n\n3\n0.01%\n0.67%\n0.04%\n\n4\n0.07%\n1.43%\n0.18%\n\n5\n0.01%\n0.67%\n0.01%\n\n6\n100%\n0.38%\n0.04%\n\n7\n0.01%\n0.33%\n0.01%\n\n8\n0.01%\n0.73%\n0.12%\n\n9\n0.01%\n0.78%\n100%\n\ntrue\nlabel\n6\n6\n9\n\nprediction\n6\n4\n9\n\nconfidence\n99.89%\n98.93%\n99.73%\n\nThe methodology of support vector machines (as described in [18, 19]) is\ndirectly applicable only to the binary classification problems, but the general\ncase can be reduced to the binary case by the standard \"one-against-one\" or\n\"one-against-the-rest\" procedures. This allows us to define the strangeness values \u03b11 , . . . , \u03b1l+1 for general classification problems (see [24], p. 59, for details),\nwhich in turn determine the p-values (5).\nThe function that assigns to each sequence (2) the corresponding p-value,\ndefined by (5), is a randomness test (this will follow from Theorem 1 stated in\nSection 5 below). Therefore, the p-values, which are our approximations to the\ncorresponding randomness levels, can be used for hedged prediction as described\nin the previous section. For example, if the p-value p\u22121 is small while p1 is not\nsmall, we can predict 1 with confidence 1 \u2212 p\u22121 and credibility p1 . Typical\ncredibility will be 1: for most data sets the percentage of support vectors is\nsmall ([19], Chapter 12), and so we can expect \u03b1l+1 = 0 when Y = yl+1 .\nRemark When the order of examples is irrelevant, we refer to the data set (4)\nas a set, although as a mathematical object it is a multiset rather than a set\nsince it can contain several copies of the same example. We will continue to\nuse this informal terminology (to be completely accurate, we would have to say\n\"data multiset\" instead of \"data set\"!)\nTable 1 illustrates the results of hedged prediction for a popular data set of\nhand-written digits called the USPS data set [5]. The data set contains 9298\ndigits represented as a 16 \u00d7 16 matrix of pixels; it is divided into a training\nset of size 7291 and a test set of size 2007. For several test examples the\ntable shows the p-values for each possible label, the actual label, the predicted\nlabel, confidence, and credibility, computed using the support vector method\nwith the polynomial kernel of degree 5. To interpret the numbers in this table,\nremember that high (i.e., close to 100%) confidence means that all labels except\nthe predicted one are unlikely. If, say, the first example were predicted wrongly,\nthis would mean that a rare event (of probability less than 1%) had occurred;\ntherefore, we expect the prediction to be correct (which it is). In the case of the\nsecond example, confidence is also quite high (more than 95%), but we can see\nthat the credibility is low (less than 5%). From the confidence we can conclude\nthat the labels other than 4 are excluded at level 5%, but the label 4 itself is also\nexcluded at the level 5%. This shows that the prediction algorithm was unable\nto extract from the training set enough information to allow us to confidently\nclassify this example: the strangeness of the labels different from 4 may be due\nto the fact that the object itself is strange; perhaps the test example is very\n8\n\ncredibility\n100%\n1.43%\n100%\n\n\fdifferent from all examples in the training set. Unsurprisingly, the prediction\nfor the second example is wrong.\nIn general, high confidence shows that all alternatives to the predicted label\nare unlikely. Low credibility means that the whole situation is suspect; as we\nhave already mentioned, we will obtain a very low credibility if the new example\nis a letter (whereas all training examples are digits). Credibility will also be low\nif the new example is a digit written in an unusual way. Notice that typically\ncredibility will not be low provided the data set was generated independently\nfrom the same distribution: the probability that credibility will not exceed some\nthreshold \u01eb (such as 1%) is at most \u01eb. In summary, we can trust a prediction if\n(1) the confidence is close to 100% and (2) the credibility is not low (say, is not\nless than 5%).\nMany other prediction algorithms can be used as underlying algorithms for\nhedged prediction. For example, we can use the nearest neighbours technique\nto associate\nPk\n+\nj=1 dij\n\u03b1i := Pk\n, i = 1, . . . , n,\n(6)\n\u2212\nj=1 dij\n\nwith the elements (xi , yi ) of the set (4), where d+\nij is the jth shortest distance\nfrom xi to other objects labelled in the same way as xi , and d\u2212\nij is the jth shortest\ndistance from xi to the objects labelled differently from xi ; the parameter k \u2208\n{1, 2, . . . } in (6) is the number of nearest neighbours taken into account. The\ndistances can be computed in a feature space (that is, the distance between\nx \u2208 X and x\u2032 \u2208 X can be understood as kF (x) \u2212 F (x\u2032 )k, F mapping the object\nspace X into a feature, typically Hilbert, space), and so (6) can also be used\nwith the kernel nearest neighbours.\nThe intuition behind (6) is as follows: a typical object xi labelled by, say, y\nwill tend to be surrounded by other objects labelled by y; and if this is the case,\nthe corresponding \u03b1i will be small. In the untypical case that there are objects\nwhose labels are different from y nearer than objects labelled y, \u03b1i will become\nlarger. Therefore, the \u03b1s reflect the strangeness of examples.\nThe p-values computed by (6) can again be used for hedged prediction. It is a\ngeneral empirical fact that the accuracy and reliability of the hedged predictions\nare in line with the error rate of the underlying algorithm. For example, in the\ncase of the USPS data set, the 1-nearest neighbour algorithm (i.e., the one with\nk = 1) achieves the error rate of 2.2%, and the hedged predictions based on (6)\nare highly confident (achieve confidence of at least 99%) for more than 95% of\nthe test examples.\n\nGeneral Definition\nThe general notion of conformal predictor can be defined as follows. A nonconformity measure is a function that assigns to every data sequence (4) a sequence\nof numbers \u03b11 , . . . , \u03b1n , called nonconformity scores, in such a way that interchanging any two examples (xi , yi ) and (xj , yj ) leads to the interchange of the\ncorresponding nonconformity scores \u03b1i and \u03b1j (with all the other nonconformity\n9\n\n\fscores unaffected). The corresponding conformal predictor maps each data set\n(1), l = 0, 1, . . ., each new object xl+1 , and each confidence level 1 \u2212 \u01eb \u2208 (0, 1),\nto the prediction set\n\u0393\u01eb (x1 , y1 , . . . , xl , yl , xl+1 ) := {Y \u2208 Y : pY > \u01eb} ,\n\n(7)\n\nwhere pY are defined by (5) with \u03b11 , . . . , \u03b1l+1 being the nonconformity scores\ncorresponding to the data sequence (2).\nWe have already remarked that associating with each completion (2) the\np-value (5) gives a randomness test; this is true in general. This implies that\nfor each l the probability of the event\nyl+1 \u2208 \u0393\u01eb (x1 , y1 , . . . , xl , yl , xl+1 )\nis at least 1 \u2212 \u01eb.\nThis definition works for both classification and regression, but in the case\nof classification we can summarize (7) by two numbers: the confidence\nsup {1 \u2212 \u01eb : |\u0393\u01eb | \u2264 1}\n\n(8)\n\ninf {\u01eb : |\u0393\u01eb | = 0} .\n\n(9)\n\nand the credibility\n\nComputationally Efficient Regression\nAs we have already mentioned, the algorithms described so far cannot be applied directly in the case of regression, even if the randomness test is efficiently\ncomputable: now we cannot consider all possible values Y for yl+1 since there\nare infinitely many of them. However, there might still be computationally efficient ways to find the prediction sets \u0393\u01eb . The idea is that if \u03b1i are defined as\nthe residuals\n\u03b1i := |yi \u2212 fY (xi )|\n(10)\nwhere fY : X \u2192 R is a regression function fitted to the completed data set (2),\nthen \u03b1i may have a simple expression in terms of Y , leading to an efficient way\nof computing the prediction sets (via (5) and (7)). This idea was implemented\nin [9] in the case where fY is found from the ridge regression, or kernel ridge\nregression, procedure, with the resulting algorithm of hedged prediction called\nthe ridge regression confidence machine. For a much fuller description of the\nridge regression confidence machine (and its modifications in the case where\n(10) are replaced by the fancier \"deleted\" or \"studentized\" residuals) see [24],\nSection 2.3.\n\n4\n\nBayesian Approach to Conformal Prediction\n\nBayesian methods have become very popular in both machine learning and\nstatistics thanks to their power and versatility, and in this section we will see\n10\n\n\fhow Bayesian ideas can be used for designing efficient conformal predictors. We\nwill only describe results of computer experiments (following [8]) with artificial\ndata sets, since for real-world data sets there is no way to make sure that the\nBayesian assumption is satisfied.\nSuppose X = Rp (each object is a vector of p real-valued attributes) and our\nmodel of the data-generating mechanism is\nyi = w * xi + \u03bei ,\n\ni = 1, 2, . . . ,\n\n(11)\n\nwhere \u03bei are independent standard Gaussian random variables and the weight\nvector w \u2208 Rp is distributed as N (0, (1/a)Ip ) (we use the notation Ip for the\nunit p \u00d7 p matrix and N (0, A) for the p-dimensional Gaussian distribution with\ncovariance matrix A); a is a positive constant. The actual data-generating\nmechanism used in our experiments will correspond to this model with a set to\n1.\nUnder the model (11) the best (in the mean-square sense) fit to a data set\n(4) is provided by the ridge regression procedure with parameter a (for details,\nsee, e.g., [24], Section 10.3). Using the residuals (10) with fY found by ridge\nregression with parameter a leads to an efficient conformal predictor which will\nbe referred to as the ridge regression confidence machine with parameter a.\nEach prediction set output by the ridge regression confidence machine will be\nreplaced by its convex hull, the corresponding prediction interval.\nTo test the validity and efficiency of the ridge regression confidence machine\nthe following procedure was used. Ten times a vector w \u2208 R5 was independently\ngenerated from the distribution N (0, I5 ). For each of the 10 values of w, 100\ntraining objects and 100 test objects were independently generated from the\nuniform distribution on [\u221210, 10]5 and for each object x its label y was generated\nas w * x + \u03be, with all the \u03be standard Gaussian and independent. For each of the\n1000 test objects and each confidence level 1 \u2212 \u01eb the prediction set \u0393\u01eb for its\nlabel was found from the corresponding training set using the ridge regression\nconfidence machine with parameter a = 1. The solid line in Figure 2 shows the\nconfidence level against the percentage of test examples whose labels were not\ncovered by the corresponding prediction intervals at that confidence level. Since\nconformal predictors are always valid, the percentage outside the prediction\ninterval should never exceed 100 minus the confidence level, up to statistical\nfluctuations, and this is confirmed by the picture.\nA natural measure of efficiency of confidence predictors is the mean width\nof their prediction intervals, at different confidence levels: the algorithm is the\nmore efficient the narrower prediction intervals it produces. The solid line in\nFigure 3 shows the confidence level against the mean (over all test examples)\nwidth of the prediction intervals at that confidence level.\nSince we know the data-generating mechanism, the approach via conformal\nprediction appears somewhat roundabout: for each test object we could instead\nfind the conditional probability distribution of its label, which is Gaussian, and\noutput as the prediction set \u0393\u01eb the shortest (i.e., centred at the mean of the\nconditional distribution) interval of conditional probability 1 \u2212 \u01eb. Figures 4\n11\n\n\f100\na=1\na=1000\na=10000\n\n90\n\n% outside prediction intervals\n\n80\n\n70\n\n60\n\n50\n\n40\n\n30\n\n20\n\n10\n\n0\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n60\nconfidence level (%)\n\n70\n\n80\n\n90\n\n100\n\nFigure 2: Validity for the ridge regression confidence machine.\n\n50\na=1\na=1000\na=10000\n\n45\n\nmean prediction interval width\n\n40\n\n35\n\n30\n\n25\n\n20\n\n15\n\n10\n\n5\n\n0\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n60\nconfidence level (%)\n\n70\n\n80\n\n90\n\n100\n\nFigure 3: Efficiency for the ridge regression confidence machine.\n\n12\n\n\f100\na=1\na=1000\na=10000\n\n90\n\n% outside prediction intervals\n\n80\n\n70\n\n60\n\n50\n\n40\n\n30\n\n20\n\n10\n\n0\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n60\nconfidence level (%)\n\n70\n\n80\n\n90\n\n100\n\nFigure 4: Validity for the Bayes-optimal confidence predictor.\nand 5 are the analogues of Figures 2 and 3 for this Bayes-optimal confidence\npredictor. The solid line in Figure 4 demonstrates the validity of the Bayesoptimal confidence predictor.\nWhat is interesting is that the solid lines in Figures 5 and 3 look exactly\nthe same, taking account of the different scales of the vertical axes. The ridge\nregression confidence machine appears as good as the Bayes-optimal predictor.\n(This is a general phenomenon; it is also illustrated, in the case of classification, by the construction in Section 3.3 of [24] of a conformal predictor that is\nasymptotically as good as the Bayes-optimal confidence predictor.)\nThe similarity between the two algorithms disappears when they are given\nwrong values for a. For example, let us see what happens if we tell the algorithms\nthat the expected value of kwk is just 1% of what it really is (this corresponds\nto taking a = 10000). The ridge regression confidence machine stays valid\n(see the dashed line in Figure 2), but its efficiency deteriorates (the dashed\nline in Figure 3). The efficiency of the Bayes-optimal confidence predictor (the\ndashed line in Figure 5) is hardly affected, but its predictions become invalid\n(the dashed line in Figure 4 deviates significantly from the diagonal, especially\nfor the most important large confidence levels: e.g., only about 15% of labels\nfall within the 90% prediction sets). The worst that can happen to the ridge\nregression confidence machine is that its predictions will become useless (but at\nleast harmless), whereas the Bayes-optimal predictions can become misleading.\nFigures 2\u20135 also show the graphs for the intermediate value a = 1000. Similar results but for different data sets are also given in [24], Section 10.3. A\ngeneral scheme of Bayes-type conformal prediction is described in [24], pp. 102\u2013\n103.\n\n13\n\n\f6\na=1\na=1000\na=10000\n\nmean prediction interval width\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n60\nconfidence level (%)\n\n70\n\n80\n\n90\n\n100\n\nFigure 5: Efficiency for the Bayes-optimal confidence predictor.\n\n5\n\nOn-line prediction\n\nWe know from Section 3 that conformal predictors are valid in the sense that\nthe probability of error\nyl+1 \u2208\n/ \u0393\u01eb (x1 , y1 , . . . xl , yl , xl+1 )\n\n(12)\n\nat confidence level 1 \u2212 \u01eb never exceeds \u01eb. The word \"probability\" means \"unconditional probability\" here: the frequentist meaning of the statement that the\nprobability of (12) does not exceed \u01eb is that, if we repeatedly generate many\nsequences\nx1 , y1 , . . . , xl , yl , xl+1 , yl+1 ,\nthe fraction of them satisfying (12) will be at most \u01eb, to within statistical fluctuations. To say that we are controlling the number of errors would be an\nexaggeration because of the artificial character of this scheme of repeatedly\ngenerating a new training set and a new test example. Can we say that the\nconfidence level 1 \u2212 \u01eb translates into a bound on the number of mistakes for\na natural learning protocol? In this section we show that the answer is \"yes\"\nfor the popular on-line learning protocol, and in the next section we will see to\nwhat degree this carries over to other protocols.\nIn on-line learning the examples are presented one by one. Each time, we\nobserve the object and predict its label. Then we observe the label and go on\nto the next example. We start by observing the first object x1 and predicting\nits label y1 . Then we observe y1 and the second object x2 , and predict its\nlabel y2 . And so on. At the nth step, we have observed the previous examples\n14\n\n\f(x1 , y1 ), . . . , (xn\u22121 , yn\u22121 ) and the new object xn , and our task is to predict yn .\nThe quality of our predictions should improve as we accumulate more and more\nold examples. This is the sense in which we are learning.\nOur prediction for yn is a nested family of prediction sets \u0393\u01ebn \u2286 Y, \u01eb \u2208 (0, 1).\nThe process of prediction can be summarized by the following protocol:\nOn-line prediction protocol\nErr0 := 0;\nMult0 := 0;\nEmp0 := 0;\nFOR n = 1, 2, . . .:\nReality outputs xn \u2208 X;\nPredictor outputs \u0393\u01ebn \u2286 Y for all \u01eb \u2208 (0, 1);\nReality \u001a\noutputs yn \u2208 Y;\n1 if yn \u2208\n/ \u0393\u01ebn\n\u01eb \u2208 (0, 1);\nerr\u01ebn :=\n0 otherwise,\n\u01eb\n\u01eb\n\u01eb\nErrn := Errn\u22121 + errn , \u01eb \u2208 (0, 1);\n\u001a\n1 if |\u0393\u01ebn | > 1\nmult\u01ebn :=\n\u01eb \u2208 (0, 1);\n0 otherwise,\nMult\u01ebn :=\u001aMult\u01ebn\u22121 + mult\u01ebn , \u01eb \u2208 (0, 1);\n1 if |\u0393\u01ebn | = 0\nemp\u01ebn :=\n\u01eb \u2208 (0, 1);\n0 otherwise,\n\u01eb\n\u01eb\n\u01eb\nEmpn := Empn\u22121 + Empn , \u01eb \u2208 (0, 1)\nEND FOR.\nAs we said, the family \u0393\u01ebn is assumed nested: \u0393\u01ebn1 \u2286 \u0393\u01ebn2 when \u01eb1 \u2265 \u01eb2 . In this\nprotocol we also record the cumulative numbers Err\u01ebn of erroneous prediction\nsets, Mult\u01ebn of multiple prediction sets (i.e., prediction sets containing more than\none label) and Emp\u01ebn of empty prediction sets at each confidence level 1 \u2212 \u01eb. We\nwill discuss the significance of each of these numbers in turn.\nThe number of erroneous predictions is a measure of validity of our confidence predictors: we would like to have Err\u01ebn \u2264 \u01ebn, up to statistical fluctuations.\nIn Figure 6 we can see the lines n 7\u2192 Err\u01ebn for one particular conformal predictor\nand for three confidence levels 1 \u2212 \u01eb: the solid line for 99%, the dash-dot line for\n95%, and the dotted line for 80%. The number of errors made grows linearly,\nand the slope is approximately 20% for the confidence level 80%, 5% for the\nconfidence level 95%, and 1% for the confidence level 99%. We will see below\nthat this is not accidental.\nThe number of multiple predictions Multn is a useful measure of efficiency\nin the case of classification: we would like as many as possible of our predictions\nto be singletons. Figure 7 shows the cumulative numbers of errors n 7\u2192 Errn2.5%\n(solid line) and multiple predictions n 7\u2192 Mult2.5%\n(dotted line) at the fixed\nn\nconfidence level 97.5%. We can see that out of approximately 10,000 predictions\nabout 250 (approximately 2.5%) were errors and about 300 (approximately 3%)\nwere multiple predictions.\n\n15\n\n\fcumulative errors at different confidence levels\n\n2000\nerrors at 80%\nerrors at 95%\nerrors at 99%\n\n1800\n1600\n1400\n1200\n1000\n800\n600\n400\n200\n0\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000 6000\nexamples\n\n7000\n\n8000\n\n9000 10000\n\nFigure 6: Cumulative numbers of errors for a conformal predictor (the 1-nearest\nneighbour conformal predictor) run in the on-line mode on the USPS data set\n(9298 hand-written digits, randomly permuted) at the confidence levels 80%,\n95% and 99%.\n\ncumulative errors, multiple, and empty predictions\n\n350\n\n300\n\n250\nerrors\nmultiple predictions\nempty predictions\n\n200\n\n150\n\n100\n\n50\n\n0\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000 6000\nexamples\n\n7000\n\n8000\n\n9000 10000\n\nFigure 7: The on-line performance of the 1-nearest neighbour conformal predictor at the confidence level 97.5% on the USPS data set (randomly permuted).\n\n16\n\n\fTable 2: A selected test example from a data set of hospital records of patients\nwho suffered acute abdominal pain [4]: the p-values for the nine possible diagnostic groups (appendicitis APP, diverticulitis DIV, perforated peptic ulcer\nPPU, non-specific abdominal pain NAP, cholecystitis CHO, intestinal obstruction INO, pancreatitis PAN, renal colic RCO, dyspepsia DYS) and the true\nlabel.\nAPP\n1.23%\n\nDIV\n0.36%\n\nPPU\n0.16%\n\nNAP\n2.83%\n\nCHO\n5.72%\n\nINO\n0.89%\n\nPAN\n1.37%\n\nRCO\n0.48%\n\nDYS\n80.56%\n\ntrue label\nDYS\n\nWe can see that by choosing \u01eb we are able to control the number of errors.\nFor small \u01eb (relative to the difficulty of the data set) this might lead to the\nneed sometimes to give multiple predictions. On the other hand, for larger \u01eb\nthis might lead to empty predictions at some steps, as can be seen from the\nbottom right corner of Figure 7: when the predictor ceases to make multiple\npredictions it starts making occasional empty predictions (the dash-dot line).\nAn empty prediction is a warning that the object to be predicted is unusual\n(the credibility, as defined in Section 2, is \u01eb or less).\nIt would be a mistake to concentrate exclusively on one confidence level\n1 \u2212 \u01eb. If the prediction \u0393\u01ebn is empty, this does not mean that we cannot make\nany prediction at all: we should just shift our attention to other confidence\nlevels (perhaps look at the range of \u01eb for which \u0393\u01ebn is a singleton). Likewise, \u0393\u01ebn\nbeing multiple does not mean that all labels in \u0393\u01ebn are equally likely: slightly\nincreasing \u01eb might lead to the removal of some labels. Of course, taking in the\ncontinuum of predictions sets, for all \u01eb \u2208 (0, 1), might be too difficult or tiresome\nfor a human mind, and concentrating on a few conventional levels, as in Figure\n1, might be a reasonable compromise.\nFor example, Table 2 gives the p-values for different kinds of abdominal pain\nobtained for a specific patient based on his symptoms. We can see that at the\nconfidence level 95% the prediction set is multiple, {cholecystitis, dyspepsia}.\nWhen we relax the confidence level to 90%, the prediction set narrows down to\n{dyspepsia} (the singleton containing only the true label); on the other hand, at\nthe confidence level 99% the prediction set widens to {appendicitis, non-specific\nabdominal pain, cholecystitis, pancreatitis, dyspepsia}. Such detailed confidence information, in combination with the property of validity, is especially\nvaluable in medicine (and some of the first applications of conformal predictors\nhave been to the fields of medicine and bioinformatics: see, e.g., [1, 15]).\nIn the case of regression, we will usually have Mult\u01ebn = n and Emp\u01ebn = 0,\nand so these are not useful measures of efficiency. Better measures, such as the\nones used in the previous section, would, e.g., take into account the widths of\nthe prediction intervals.\n\n17\n\n\fTheoretical Analysis\nLooking at Figures 6 and 7 we might be tempted to guess that the probability\nof error at each step of the on-line protocol is \u01eb and that errors are made independently at different steps. This is not literally true, as a closer examination of\nthe bottom left corner of Figure 7 reveals. It, however, becomes true (as noticed\nin [23]) if the p-values (5) are redefined as\npY :=\n\n|{i : \u03b1i > \u03b1l+1 }| + \u03b7 |{i : \u03b1i = \u03b1l+1 }|\n,\nl+1\n\n(13)\n\nwhere i ranges over {1, . . . , l + 1} and \u03b7 \u2208 [0, 1] is generated randomly from the\nuniform distribution on [0, 1] (the \u03b7s should be independent between themselves\nand of everything else; in practice they are produced by pseudo-random number\ngenerators). The only difference between (5) and (13) is that the expression (13)\ntakes more care in breaking the ties \u03b1i = \u03b1l+1 . Replacing (5) by (13) in the\ndefinition of conformal predictor we obtain the notion of smoothed conformal\npredictor.\nThe validity property for smoothed conformal predictors can now be stated\nas follows.\nTheorem 1 Suppose the examples\n(x1 , y1 ), (x2 , y2 ), . . .\nare generated independently from the same distribution. For any smoothed conformal predictor working in the on-line prediction protocol and any confidence\nlevel 1 \u2212 \u01eb, the random variables err\u01eb1 , err\u01eb2 , . . . are independent and take value 1\nwith probability \u01eb.\nCombining Theorem 1 with the strong law of large numbers we can see that\nErr\u01ebn\n=\u01eb\nn\u2192\u221e n\nlim\n\nholds with probability one for smoothed conformal predictors. (They are \"well\ncalibrated\".) Since the number of mistakes made by a conformal predictor never\nexceeds the number of mistakes made by the corresponding smoothed conformal\npredictor,\nErr\u01ebn\nlim sup\n\u2264\u01eb\nn\nn\u2192\u221e\nholds with probability one for conformal predictors. (They are \"conservatively\nwell calibrated\".)\n\n6\n\nSlow teachers, lazy teachers, and the batch\nsetting\n\nIn the pure on-line setting, considered in the previous section, we get an immediate feedback (the true label) for every example that we predict. This makes\n18\n\n\fpractical applications of this scenario questionable. Imagine, for example, a mail\nsorting centre using an on-line prediction algorithm for zip code recognition; suppose the feedback about the \"true\" label comes from a human \"teacher\". If the\nfeedback is given for every object xi , there is no point in having the prediction\nalgorithm: we can just as well use the label provided by the teacher. It would\nhelp if the prediction algorithm could still work well, in particular be valid, if\nonly every, say, tenth object were classified by a human teacher (the scenario\nof \"lazy\" teachers). Alternatively, even if the prediction algorithm requires the\nknowledge of all labels, it might still be useful if the labels were allowed to be\ngiven not immediately but with a delay (\"slow\" teachers). In our mail sorting\nexample, such a delay might make sure that we hear from local post offices\nabout any mistakes made before giving a feedback to the algorithm.\nIn the pure on-line protocol we had validity in the strongest possible sense:\nat each confidence level 1 \u2212 \u01eb each smoothed conformal predictor made errors\nindependently with probability \u01eb. In the case of weaker teachers (as usual, we\nare using the word \"teacher\" in the general sense of the entity providing the\nfeedback, called Reality in the previous section), we have to accept a weaker\nnotion of validity. Suppose the predictor receives a feedback from the teacher\nat the end of steps n1 , n2 , . . ., n1 < n2 < * * * ; the feedback is the label of one\nof the objects that the predictor has already seen (and predicted). This scheme\n[14] covers both slow and lazy teachers (as well as teachers who are both slow\nand lazy). It was proved in [10] (see also [24], Theorem 4.2) that the smoothed\nconformal predictors (using only the examples with known labels) remain valid\nin the sense\n\u2200\u01eb \u2208 (0, 1) : Err\u01ebn /n \u2192 \u01eb in probability\nif and only if nk /nk\u22121 \u2192 1 as k \u2192 \u221e. In other words, the validity in the\nsense of convergence in probability holds if and only if the growth rate of nk is\nsubexponential. (This condition is amply satisfied for our example of a teacher\ngiving feedback for every tenth object.)\nThe most standard batch setting of the problem of prediction is in one respect\neven more demanding than our scenarios of weak teachers. In this setting we\nare given a training set (1) and our goal is to predict the labels given the objects\nin the test set\n(xl+1 , yl+1 ), . . . , (xl+k , yl+k ).\n(14)\nThis can be interpreted as a finite-horizon version of the lazy-teacher setting:\nno labels are returned after step l. Computer experiments (see, e.g., Figure 8)\nshow that approximate validity still holds; for related theoretical results, see\n[24], Section 4.4.\n\n7\n\nInduction and transduction\n\nVapnik's [18, 19] distinction between induction and transduction, as applied\nto the problem of prediction, is depicted in Figure 9. In inductive prediction\nwe first move from examples in hand to some more or less general rule, which\n\n19\n\n\fcumulative errors at different confidence levels\n\n450\nerrors at 80%\nerrors at 95%\nerrors at 99%\n\n400\n350\n300\n250\n200\n150\n100\n50\n0\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\nexamples\n\nFigure 8: Cumulative numbers of errors made on the test set by the 1-nearest\nneighbour conformal predictor used in the batch mode on the USPS data set\n(randomly permuted and split into a training set of size 7291 and a test set of\nsize 2007) at the confidence levels 80%, 95% and 99%.\ngeneral rule\n\u0012\n@ \u01eb\n\u03b4\n@\ninduction\ndeduction\n@\nR\n@\ntransduction\n- prediction\ntraining set\n\u01eb\n\nFigure 9: Inductive and transductive prediction.\nwe might call a prediction or decision rule, a model, or a theory; this is the\ninductive step. When presented with a new object, we derive a prediction from\nthe general rule; this is the deductive step. In transductive prediction, we take\na shortcut, moving from the old examples directly to the prediction about the\nnew object.\nTypical examples of the inductive step are estimating parameters in statistics\nand finding an approximating function in statistical learning theory. Examples\nof transductive prediction are estimation of future observations in statistics ([3],\nSection 7.5, [17]) and nearest neighbours algorithms in machine learning.\nIn the case of simple (i.e., traditional, not hedged) predictions the distinction\nbetween induction and transduction is less than crisp. A method for doing\ntransduction, in the simplest setting of predicting one label, is a method for\npredicting yl+1 from (1) and xl+1 . Such a method gives a prediction for any\n\n20\n\n\fobject that might be presented as xl+1 , and so it defines, at least implicitly,\na rule, which might be extracted from the training set (1) (induction), stored,\nand then subsequently applied to xl+1 to predict yl+1 (deduction). So any real\ndistinction is really at a practical and computational level: do we extract and\nstore the general rule or not?\nFor hedged predictions the difference between induction and transduction\ngoes deeper. We will typically want different notions of hedged prediction in\nthe two frameworks. Mathematical results about induction usually involve two\nparameters, often denoted \u01eb (the desired accuracy of the prediction rule) and \u03b4\n(the probability of achieving the accuracy of \u01eb), whereas results about transduction involve only one parameter, which we denote \u01eb in this paper (the probability\nof error we are willing to tolerate); see Figure 9. For a review of inductive prediction from this point of view, see [24], Section 10.1.\n\n8\n\nInductive conformal predictors\n\nOur approach to prediction is thoroughly transductive, and this is what makes\nvalid and efficient hedged prediction possible. In this section we will see, however, that there is also room for an element of induction in conformal prediction.\nLet us take a closer look at the process of conformal prediction, as described\nin Section 3. Suppose we are given a training set (1) and the objects in a test\nset (14), and our goal is to predict the label of each test object. If we want to\nuse the conformal predictor based on the support vector method, as described\nin Section 3, we will have to find the set of the Lagrange multipliers for each\ntest object and for each potential label Y that can be assigned to it. This would\ninvolve solving k |Y| essentially independent optimization problems. Using the\nnearest neighbours approach is typically more computationally efficient, but\neven it is much slower than the following procedure, suggested in [11, 12].\nSuppose we have an inductive algorithm which, given a training set (1) and\na new object x outputs a prediction \u0177 for x's label y. Fix some measure \u2206(y, \u0177)\nof difference between y and \u0177. The procedure is:\n1. Divide the original training set (1) into two subsets: the proper training set\n(x1 , y1 ), . . . , (xm , ym ) and the calibration set (xm+1 , ym+1 ), . . . , (xl , yl ).\n2. Construct a prediction rule F from the proper training set.\n3. Compute the nonconformity score\n\u03b1i := \u2206(yi , F (xi )),\n\ni = m + 1, . . . , l,\n\nfor each example in the calibration set.\n4. For every test object xi , i = l + 1, . . . , l + k, do the following:\n(a) for every possible label Y \u2208 Y compute the nonconformity score\n\u03b1i := \u2206(yi , F (xi )) and the p-value\npY :=\n\n#{j \u2208 {m + 1, . . . , l, i} : \u03b1j \u2265 \u03b1i }\n;\nl\u2212m+1\n21\n\n\f(b) output the prediction sets \u0393\u01eb (x1 , y1 , . . . , xl , yl , xi ) given by the righthand side of (7).\nThis is a special case of \"inductive conformal predictors\", as defined in [24],\nSection 4.1. In the case of classification, of course, we could package the pvalues as a simple prediction complemented with confidence (8) and credibility\n(9).\nInductive conformal predictors are valid in the sense that the probability of\nerror\nyi \u2208\n/ \u0393\u01eb (x1 , y1 , . . . xl , yl , xi )\n(i = l + 1, . . . , l + k, \u01eb \u2208 (0, 1)) never exceeds \u01eb (cf. (12)). The on-line version of\ninductive conformal predictors, with a stronger notion of validity, is described\nin [23] and [24] (Section 4.1).\nThe main advantage of inductive conformal predictors is their computational\nefficiency: the bulk of the computations is performed only once, and what remains to do for each test example is to apply the prediction rule found at the\ninductive step, to apply \u2206 to find the nonconformity score \u03b1 for this example,\nand to find the position of \u03b1 among the nonconformity scores of the calibration\nexamples. The main disadvantage is a possible loss of the prediction efficiency:\nfor conformal predictors, we can effectively use the whole training set as both\nthe proper training set and the calibration set.\n\n9\n\nConclusion\n\nThis paper shows how many machine-learning techniques can be complemented\nwith provably valid measures of accuracy and reliability. We explained briefly\nhow this can be done for support vector machines, nearest neighbours algorithms, and the ridge regression procedure, but the principle is general: virtually\nany (we are not aware of exceptions) successful prediction technique designed to\nwork under the randomness assumption can be used to produce equally successful hedged predictions. Further examples are given in our recent book [24] (joint\nwith Glenn Shafer), where we construct conformal predictors and inductive conformal predictors based on nearest neighbours regression, logistic regression,\nbootstrap, decision trees, boosting, and neural networks; general schemes for\nconstructing conformal predictors and inductive conformal predictors are given\non pp. 28\u201329 and on pp. 99\u2013100 of [24], respectively. Replacing the original\nsimple predictions with hedged predictions enables us to control the number of\nerrors made by appropriately choosing the confidence level.\n\nAcknowledgements\nThis work is partially supported by MRC (grant \"Proteomic analysis of the human serum proteome\") and the Royal Society (grant \"Efficient pseudo-random\nnumber generators\").\n\n22\n\n\fReferences\n[1] Bellotti, T., Luo, Z., Gammerman, A., van Delft, F. W. and Saha, V.\n(2005) Qualified predictions for microarray and proteomics pattern diagnostics with confidence machines. International Journal of Neural Systems,\n15, 247\u2013258. Yang, Z. R. and Dalby, A. R. (eds), Special Issue on Bioinformatics.\n[2] Cesa-Bianchi, N. and Lugosi, G. (2006) Prediction, Learning, and Games.\nCambridge University Press, Cambridge.\n[3] Cox, D. R. and Hinkley, D. V. (1974) Theoretical Statistics. Chapman and\nHall, London.\n[4] Gammerman, A. and Thatcher, A. R. (1992) Bayesian diagnostic probabilities without assuming independence of symptoms. Yearbook of Medical\nInformatics, pp. 323\u2013330.\n[5] LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. and Jackel, L. J. (1990) Handwritten digit recognition with backpropagation network. In Advances in Neural Information Processing Systems 2, pp. 396\u2013404, Morgan Kaufmann, San Mateo, CA.\n[6] Li, M. and Vit\u00e1nyi, P. (1993) An Introduction to Kolmogorov Complexity\nand Its Applications. Springer, New York. Second edition: 1997.\n[7] Martin-L\u00f6f, P. (1966) The definition of random sequences. Information and\nControl, 9, 602\u2013619.\n[8] Melluish, T., Saunders, C., Nouretdinov, I. and Vovk, V. (2001) Comparing the Bayes and typicalness frameworks. In De Raedt, L. and Flash, P.\n(eds), Machine Learning: ECML 2001, Proceedings of the Twelfth European\nConference on Machine Learning, LNAI, 2167, pp. 360\u2013371, Springer, Heidelberg. Full version published as Technical Report TR-01-05, Computer\nLearning Research Centre, Royal Holloway, University of London.\n[9] Nouretdinov, I., Melluish, T. and Vovk, V. (2001) Ridge Regression Confidence Machine. In Proceedings of the Eighteenth International Conference\non Machine Learning, pp. 385\u2013392, Morgan Kaufmann, San Francisco, CA.\n[10] Nouretdinov, I. and Vovk, V. (2003) Criterion of calibration for transductive confidence machine with limited feedback. In Gavald\u00e0, R., Jantke,\nK. P. and Takimoto, E. (eds), Proceedings of the Fourteenth International\nConference on Algorithmic Learning Theory, LNAI, 2842, pp. 259\u2013267,\nSpringer, Berlin. To appear in Theoretical Computer Science (special issue\ndevoted to the ALT'2003 conference).\n[11] Papadopoulos, H., Proedrou, K., Vovk, V. and Gammerman, A. (2002)\nInductive Confidence Machines for regression. In Elomaaa, T., Mannila, H.\n\n23\n\n\fand Toivonen, H. (eds), Machine Learning: ECML 2002, Proceedings of\nthe Thirteenth European Conference on Machine Learning, LNCS, 2430,\npp. 345\u2013356, Springer, Berlin.\n[12] Papadopoulos, H., Vovk, V. and Gammerman, A. (2002) Qualified predictions for large data sets in the case of pattern recognition. In Proceedings of the International Conference on Machine Learning and Applications\n(ICMLA'2002), pp. 159\u2013163, CSREA Press.\n[13] Popper, K. R. (1934) Logik der Forschung. Springer, Vienna. English translation (1959): The Logic of Scientific Discovery, Hutchinson, London.\n[14] Ryabko, D., Vovk, V. and Gammerman, A. (2003) Online prediction with\nreal teachers. Technical Report CS-TR-03-09, Department of Computer\nScience, Royal Holloway, University of London.\n[15] Shahmuradov, I. A., Solovyev, V. V. and Gammerman, A. (2005) Plant\npromoter prediction with confidence estimation. Nucleic Acids Research,\n33, 1069\u20131076.\n[16] Sutton, R. S. and Barto, A. G. (1998) Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA.\n[17] Takeuchi, K. (1975) Statistical Prediction Theory (in Japanese). Baih\u016bkan,\nTokyo.\n[18] Vapnik, V. N. (1995) The Nature of Statistical Learning Theory. Springer,\nNew York. Second edition: 2000.\n[19] Vapnik, V. N. (1998) Statistical Learning Theory. Wiley, New York.\n[20] Vapnik, V. N. and Chervonenkis, A. Y. (1974) Theory of Pattern Recognition (in Russian). Nauka, Moscow. German translation (1979): Theorie\nder Zeichenerkennung, Akademie, Berlin.\n[21] Vovk, V., Gammerman, A. and Saunders, C. (1999) Machine-learning applications of algorithmic randomness. In Bratko, I. and Dzeroski, S. (eds),\nProceedings of the Sixteenth International Conference on Machine Learning, pp. 444\u2013453, Morgan Kaufmann, San Francisco, CA.\n[22] Vovk, V. (2001) Competitive on-line statistics. International Statistical Review, 69, 213\u2013248.\n[23] Vovk, V. (2002) On-line Confidence Machines are well-calibrated. In Proceedings of the Forty Third Annual Symposium on Foundations of Computer Science, pp. 187\u2013196, IEEE Computer Society, Los Alamitos, CA.\n[24] Vovk, V., Gammerman, A. and Shafer, G. (2005) Algorithmic Learning in\na Random World. Springer, New York.\n\n24\n\n\f"}