{"id": "http://arxiv.org/abs/1205.2602v1", "guidislink": true, "updated": "2012-05-09T18:46:51Z", "updated_parsed": [2012, 5, 9, 18, 46, 51, 2, 130, 0], "published": "2012-05-09T18:46:51Z", "published_parsed": [2012, 5, 9, 18, 46, 51, 2, 130, 0], "title": "The Entire Quantile Path of a Risk-Agnostic SVM Classifier", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1205.3092%2C1205.0814%2C1205.2732%2C1205.3301%2C1205.2010%2C1205.3480%2C1205.2676%2C1205.0189%2C1205.2383%2C1205.0487%2C1205.0304%2C1205.4112%2C1205.0012%2C1205.0853%2C1205.1469%2C1205.1300%2C1205.0085%2C1205.3708%2C1205.0436%2C1205.2033%2C1205.2509%2C1205.2856%2C1205.2536%2C1205.2081%2C1205.2602%2C1205.2898%2C1205.1302%2C1205.0529%2C1205.3398%2C1205.2530%2C1205.2343%2C1205.2418%2C1205.3561%2C1205.1991%2C1205.0620%2C1205.2947%2C1205.0950%2C1205.2153%2C1205.2302%2C1205.4014%2C1205.0483%2C1205.2866%2C1205.2409%2C1205.2568%2C1205.1757%2C1205.4247%2C1205.2547%2C1205.4230%2C1205.0997%2C1205.3177%2C1205.2298%2C1205.4375%2C1205.4214%2C1205.2227%2C1205.3981%2C1205.1883%2C1205.3437%2C1205.0398%2C1205.3433%2C1205.0146%2C1205.2746%2C1205.0555%2C1205.3703%2C1205.0143%2C1205.4382%2C1205.3305%2C1205.4172%2C1205.2735%2C1205.3992%2C1205.1610%2C1205.4066%2C1205.2765%2C1205.0800%2C1205.0059%2C1205.1667%2C1205.3680%2C1205.1062%2C1205.1201%2C1205.4297%2C1205.3589%2C1205.0883%2C1205.1834%2C1205.4441%2C1205.3950%2C1205.2491%2C1205.4019%2C1205.0258%2C1205.2234%2C1205.3830%2C1205.0442%2C1205.3663%2C1205.2684%2C1205.2282%2C1205.0100%2C1205.0640%2C1205.3738%2C1205.2173%2C1205.3995%2C1205.1861%2C1205.4362%2C1205.3190&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Entire Quantile Path of a Risk-Agnostic SVM Classifier"}, "summary": "A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1|X =\nx) >= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It has\nbeen shown that Support Vector Machines (SVMs) in the limit are quantile\nclassifiers with t = 1/2 . In this paper, we show that by using asymmetric cost\nof misclassification SVMs can be appropriately extended to recover, in the\nlimit, the quantile binary classifier for any t. We then present a principled\nalgorithm to solve the extended SVM classifier for all values of t\nsimultaneously. This has two implications: First, one can recover the entire\nconditional distribution P(Y = 1|X = x) = t for t {[0, 1]. Second, we can build\na risk-agnostic SVM classifier where the cost of misclassification need not be\nknown apriori. Preliminary numerical experiments show the effectiveness of the\nproposed algorithm.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1205.3092%2C1205.0814%2C1205.2732%2C1205.3301%2C1205.2010%2C1205.3480%2C1205.2676%2C1205.0189%2C1205.2383%2C1205.0487%2C1205.0304%2C1205.4112%2C1205.0012%2C1205.0853%2C1205.1469%2C1205.1300%2C1205.0085%2C1205.3708%2C1205.0436%2C1205.2033%2C1205.2509%2C1205.2856%2C1205.2536%2C1205.2081%2C1205.2602%2C1205.2898%2C1205.1302%2C1205.0529%2C1205.3398%2C1205.2530%2C1205.2343%2C1205.2418%2C1205.3561%2C1205.1991%2C1205.0620%2C1205.2947%2C1205.0950%2C1205.2153%2C1205.2302%2C1205.4014%2C1205.0483%2C1205.2866%2C1205.2409%2C1205.2568%2C1205.1757%2C1205.4247%2C1205.2547%2C1205.4230%2C1205.0997%2C1205.3177%2C1205.2298%2C1205.4375%2C1205.4214%2C1205.2227%2C1205.3981%2C1205.1883%2C1205.3437%2C1205.0398%2C1205.3433%2C1205.0146%2C1205.2746%2C1205.0555%2C1205.3703%2C1205.0143%2C1205.4382%2C1205.3305%2C1205.4172%2C1205.2735%2C1205.3992%2C1205.1610%2C1205.4066%2C1205.2765%2C1205.0800%2C1205.0059%2C1205.1667%2C1205.3680%2C1205.1062%2C1205.1201%2C1205.4297%2C1205.3589%2C1205.0883%2C1205.1834%2C1205.4441%2C1205.3950%2C1205.2491%2C1205.4019%2C1205.0258%2C1205.2234%2C1205.3830%2C1205.0442%2C1205.3663%2C1205.2684%2C1205.2282%2C1205.0100%2C1205.0640%2C1205.3738%2C1205.2173%2C1205.3995%2C1205.1861%2C1205.4362%2C1205.3190&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1|X =\nx) >= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It has\nbeen shown that Support Vector Machines (SVMs) in the limit are quantile\nclassifiers with t = 1/2 . In this paper, we show that by using asymmetric cost\nof misclassification SVMs can be appropriately extended to recover, in the\nlimit, the quantile binary classifier for any t. We then present a principled\nalgorithm to solve the extended SVM classifier for all values of t\nsimultaneously. This has two implications: First, one can recover the entire\nconditional distribution P(Y = 1|X = x) = t for t {[0, 1]. Second, we can build\na risk-agnostic SVM classifier where the cost of misclassification need not be\nknown apriori. Preliminary numerical experiments show the effectiveness of the\nproposed algorithm."}, "authors": ["Jin Yu", "S. V. N. Vishwanatan", "Jian Zhang"], "author_detail": {"name": "Jian Zhang"}, "author": "Jian Zhang", "arxiv_comment": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "links": [{"href": "http://arxiv.org/abs/1205.2602v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1205.2602v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1205.2602v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1205.2602v1", "journal_reference": null, "doi": null, "fulltext": "UAI 2009\n\nYU ET AL.\n\n623\n\nThe Entire Quantile Path of a Risk-Agnostic SVM Classifier\n\nJin Yu\nCanberra Research Laboratory, NICTA\nCollege of Engineering & Computer Science\nAustralian National University\nCanberra, Australia\njin.yu@anu.edu.au\n\nS.V.N. Vishwanathan\nJian Zhang\nDepartment of Statistics\nPurdue University\n250 N University Street\nWest Lafayette, IN 47907-2066, USA\n{vishy, jianzhan}@stat.purdue.edu\n\nAbstract\n\nIt is obvious that the hinge loss l(*) is non-negative\nand convex in w. The loss function measures the\ndiscrepancy between y and the prediction given by\nsign(w> x), while the L2 regularizer with regularization constant \u03bb > 0 controls the complexity of the\nsolution w. It has been shown (Lin, 2002) that the\nminimizer of the hinge loss is exactly sign(\u03b7(x) \u2212 1/2),\nwhere \u03b7(x) = P (Y = 1|X = x) is the probability of\nY = 1 conditioned on X = x. Thus the SVM solution\nshould approach the Bayes rule as the sample size gets\nlarge with appropriately chosen function class.\n\nA quantile binary classifier uses the rule:\nClassify x as +1 if P (Y = 1|X = x) \u2265 \u03c4 , and\nas \u22121 otherwise, for a fixed quantile parameter \u03c4 \u2208 [0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are\nquantile classifiers with \u03c4 = 12 . In this paper,\nwe show that by using asymmetric cost of\nmisclassification SVMs can be appropriately\nextended to recover, in the limit, the quantile\nbinary classifier for any \u03c4 . We then present\na principled algorithm to solve the extended\nSVM classifier for all values of \u03c4 simultaneously. This has two implications: First, one\ncan recover the entire conditional distribution P (Y = 1|X = x) = \u03c4 for \u03c4 \u2208 [0, 1].\nSecond, we can build a risk-agnostic SVM\nclassifier where the cost of misclassification\nneed not be known apriori. Preliminary numerical experiments show the effectiveness of\nthe proposed algorithm.\n\n1\n\nOne problem of the standard SVM is that even\nthough we can use the resulting SVM classifier \u0175 =\narg minw J(w) to classify a new observation x, the\nprediction w> x does not have probabilistic interpretation. More importantly, the SVM classification results cannot be directly applied to situations where\nthe misclassification cost is asymmetric, i.e. when the\ncost of a false positive error is different from that of a\nfalse negative error. To address such a problem, several methods have been proposed to convert the SVM\noutput into well-calibrated probabilistic scores, such as\n(Platt, 2000). However, such methods either rely on\nparametric assumption or lack theoretical justification\nabout the transformed scores.\n\nIntroduction\n\nSupport Vector Machines (SVMs) have emerged as a\npopular tool for binary classification. Given a set of\nn training instances xi and their corresponding labels\nyi \u2208 {\u00b11} the task of training a linear SVM classifier1\ncan be cast as a regularized risk minimization problem:\nn\n\nmin J(w) :=\n\n\u03bb\n1X\nl(w> xi , yi ),\nkwk2 +\n2\nn i=1\n\n(1)\n\nwhere l is the so-called hinge loss defined by\nl(w> x, y) := max(0, 1 \u2212 y w> x).\n\n(2)\n\n1\nFor ease of exposition we will stick to linear SVMs,\nalthough all our results extend to the non-linear case where\none maps the data into an RKHS H via the map x \u2192 \u03c6(x)\nand uses the kernel k(x, x0 ) = h\u03c6(x), \u03c6(x0 )iH .\n\nWe instead aim to estimate the quantity sign(\u03b7(x)\u2212\u03c4 ),\nwhich we call the quantile classification rule with \u03c4 \u2208\n[0, 1] as the quantile parameter. It can be shown that\nsign(\u03b7(x)\u2212\u03c4 ) is the minimizer of the asymmetric hinge\nloss, which assigns different costs to false positive and\nfalse negative errors. The SVM formulation with the\nasymmetric hinge loss can be defined as\n\nn\nX\n\u03bb\n2\nmin kwk +\nc\u03c4yi max(0, 1 \u2212 yi w> xi ).\n2\ni=1\n\n(3)\n\nHere, c\u03c4yi controls the two types of misclassification cost, and for reasons that will become apparent\n\n\f624\n\nYU ET AL.\n\na decision function g : X \u2192 {\u00b11} is defined as\n\nshortly, we set\n\nc\u03c4yi\n\n(\n[2(1 \u2212 \u03c4 )]/n\n:=\n(2\u03c4 )/n\n\nif yi = +1,\nif yi = \u22121.\n\nR(g) := C+ P (Y = \u22121, g(X) = 1)\n+ C\u2212 P (Y = 1, g(X) = \u22121).\n\n(4)\n\nfor \u03c4 \u2208 [0, 1]. Note that when \u03c4 = 21 , i.e., the misclassification costs are symmetric, we recover (1).\nThere are many natural applications where the cost\nof misclassification is not known in advance until the\nclassifier is deployed. As an illustrative example consider the problem of spam detection (also see Figure\n1). Given training emails the learning task is to distinguish between spam and non-spam emails. The tolerance of a user to spam is influenced by various factors.\nFor instance, a busy professor might have a very low\ntolerance for spam. In other words, he/she might not\nmind losing a few genuine emails as long as all spam\nemails are kept out of his/her inbox. On the other\nhand, a not-so-busy graduate student might not mind\na few spam emails as long as genuine emails are not\nlost in the junk mail folder. In such cases, the brute\nforce approach of training a classifier for every user\npreference is both tedious and time consuming. Furthermore, one needs to train a new classifier for every\nuser preference.\nIn this paper we present a principled algorithm to solve\n(3) for all values of \u03c4 simultaneously by utilizing the\nfact that the solution path is piecewise linear as a function of the quantile parameter \u03c4 \u2208 [0, 1]. In other\nwords, once our classifier is trained, we can recover\nthe solution for any \u03c4 efficiently. Consequently, our\nclassifier is risk agnostic. Furthermore, we show that\n(3) is an instance of the quantile classification problem,\nwith \u03c4 being the quantile parameter.\nThe rest of the paper is organized as follows: In Section\n2 we establish the connection between the asymmetric\ncost SVM and the quantile classification rule. In Section 3 we formulate the dual of (3). In Section 4 we describe the proposed algorithm and its worst case time\ncomplexity analysis. We discuss some related work\nin Section 5. Numerical experiments are presented in\nSection 6, and we conclude the paper with an outlook\nand discussion in Section 7.\n\n2\n\nUAI 2009\n\nStatistical Underpinnings\n\nLet (X, Y ) be a pair of random variables with training instances X \u2208 X and labels Y \u2208 {\u00b11}. For any\nrealization x of X, denote the conditional probability\nP (Y = 1|X = x) as \u03b7(x). Furthermore, let C+ (resp.\nC\u2212 ) denote the cost of misclassifying a x labeled as\n+1 (resp. \u22121). The cost sensitive classification risk of\n\n(5)\n\nThe following lemma follows from elementary Bayes\ndecision theory (see e.g., Section 2.2 of Duda et al.,\n2001).\nLemma 2.1 For any decision function g\nR(sign(\u03b7(x) \u2212 \u03c4 )) \u2264 R(g),\nwhere \u03c4 =\n\nC+\nC+ +C\u2212\n\n\u2208 [0, 1].\n\nLemma 2.1 says that when the misclassification cost\nis asymmetric, the classifier which leads to the minimum risk should take the form sign(\u03b7(x) \u2212 \u03c4 ) where \u03c4\nonly depends on the ratio of the misclassification costs\nC+ /C\u2212 . The following lemma, whose proof can be\nfound in Appendix A, shows that sign(\u03b7(x) \u2212 \u03c4 ) is the\nminimizer of the asymmetric hinge loss.\nLemma 2.2 For any \u03c4 \u2208 [0, 1], the minimizer f \u2217 of\n\u0002\n\u0003\n(6)\nEX,Y ((1 + Y )/2 \u2212 \u03c4 Y ) (1 \u2212 Y f (X))+\ntakes the form f \u2217 (x) = sign(\u03b7(x) \u2212 \u03c4 ).\nIn the infinite sample case, if we let \u03bb \u2192 0 as n \u2192\n\u221e in (3), it is easy to see that the regularized risk\nconverges to (6). Therefore, Lemma 2.2 implies that\nthe estimator obtained by minimizing the regularized\nrisk (3) is risk consistent.\nThe above observation has a number of consequences.\nFirst, it shows that the usual SVM with the symmetric\nhinge loss (2) estimates P (Y = 1|X = x) = 12 , a well\nknown result (see e.g., Lin, 2002; Sollich, 2002). It\nalso shows that the SVM with the asymmetric hinge\nloss (4) is essentially a quantile estimator. This result\nhas been hinted many times (e.g. Grandvalet et al.,\n2006), but to the best of our knowledge, not proven\nrigorously.\n\n3\n\nDual Formulation\n\nSimilar to the case of standard SVM, we can rewrite\n(3) as a constrained optimization problem:\nn\nX\n\u03bb\n2\nmin kwk +\nc\u03c4yi \u03bei\n2\ni=1\n\ns.t. \u03bei \u2265 0,\n\n(7)\n\n\u03bei \u2265 1 \u2212 yi w> xi , \u2200i\n\nallows us to derive its dual as:\n1 >\nmin D(\u03b1) :=\n\u03b1 Q\u03b1 \u2212 \u03b1> 1\n2\u03bb\ns.t. 0 \u2264 \u03b1i \u2264 c\u03c4yi , \u2200i.\n\n(8)\n\n\f\u03c4\n\nUAI 2009\n\n\u03c4\n\nYU ET AL.\n\n= 0.1\n\n= 0.9\n\n8\n\n8\n\n0.\n\n00\n\n0\n\n8\n\n\u03c4\n\n= 0.5\n\n625\n\n6\n\n6\n\n6\n\n0\n\n0\n\n0\n\n00\n\n0\n\n4\n\n4\n\n1.\n\n4\n\n2\n\n.0\n\n0\n\n0\n\n-1\n\n0\n0\n\n2\n\n6\n\n8\n\n0\n\n0\n\n.0\n\n0\n4\n\n0\n00\n1.\n0\n00\n0.\n\n2\n\n1\n\n.0\n\n00\n\n2\n\n.0\n\n0\n\n2\n\n00\n.0\n\n0\n\n-1\n\n4\n\n6\n\n8\n\n-1\n\n0\n\n2\n\n4\n\n6\n\n8\n\nFigure 1: The effect of \u03c4 on the decision boundary of a binary quantile classifier on synthetically generated two\ndimensional data. When \u03c4 = 12 (middle) the cost of misclassification is symmetric and therefore the resulting\nclassifier is well balanced. For low values of \u03c4 (left), the classifier classifies all blue squares correctly even at the\ncost of misclassifying a few red dots. A converse effect is observed for high values of \u03c4 (right).\nwhere 1 is a vector of all ones and Qij = yi yj x>\ni xj .\nLet \u03b1\u03c4 denote the optimal solution to (8) for a given\n\u03c4 . Then the primal solution w\u03c4 can be recovered via\nthe primal-dual connection:\nw\u03c4 =\n\n1\n\u03bb\n\nn\nX\n\n\u03b1i\u03c4 yi xi .\n\n(9)\n\ni=1\n\ndecompose \u03b1\u03c4 into [\u03b1\u03c4L>\n, \u03b1\u03c4L>\n, \u03b1\u03c4M> , 0> ]> , where\n+\n\u2212\n0 is a vector of all zeros. Using (10), we can get a\ndecomposed view of \u2207D(\u03b1\u03c4 ):\n[\u2207L D(\u03b1\u03c4 )> , \u2207M D(\u03b1\u03c4 )> , \u2207R D(\u03b1\u03c4 )> ]> := (13)\n\uf8ee \u03c4\n\uf8f9\nc Q\n1+c\u03c4 Q\n1+QL M \u03b1\u03c4M\n1 \uf8f0 \u03c4 +1 L L+1 \u03c4\u22121 L L\u22121\nc+1 QM L+1 1+c\u22121 QM L\u22121 1+QM M \u03b1\u03c4M \uf8fb \u2212 1\n\u03bb\n\u03c4\n\u03c4\n\u03c4\nc+1 QR L+1 1+c\u22121 QR L\u22121 1+QR M \u03b1M\n\nThe dual problem is a quadratic problem with box\nconstrains, which can be solved by various optimization techniques (see e.g., Byrd et al., 1995; Mor\u00e9 and\nToraldo, 1989)\nDefine \u2207i D(\u03b1\u03c4 ) as the ith element of the gradient:\n1\n\u2207D(\u03b1\u03c4 ) = Q\u03b1\u03c4 \u2212 1;\n\u03bb\n\n(10)\n\nand let L, M, and R index entries of the dual solution\n\u03b1\u03c4 such that\nL := {i : \u2207i D(\u03b1\u03c4 ) < 0} = {i : yi w\u03c4 > xi < 1},\nM := {i : \u2207i D(\u03b1\u03c4 ) = 0} = {i : yi w\u03c4 > xi = 1}, (11)\nR := {i : \u2207i D(\u03b1\u03c4 ) > 0} = {i : yi w\u03c4 > xi > 1},\nwhere the connection with the primal parameter w\u03c4\nis made via (9). It is easy to find that L, M, and R\nin fact index the data xi which are in error, on the\nmargin, and well-classified, respectively. Furthermore,\nit follows from the KKT conditions (see Appendix B)\nthat the optimal dual solution \u03b1\u03c4 satisfies\n\uf8f1\n\u03c4\n\uf8f4\nif i \u2208 L,\n\uf8f2cyi\n\u03c4\n\u03c4\n\u03b1i = [0, cyi ] if i \u2208 M,\n(12)\n\uf8f4\n\uf8f3\n0\nif i \u2208 R .\nGiven index sets, I and J , let \u03b1\u03c4I be a vector of \u03b1i\u03c4\nwith i \u2208 I and QI J a submatrix of Q taking entries\nQij with i \u2208 I and j \u2208 J . Then, we can define L+1 :=\nL \u2229{i : yi = +1} and L\u22121 := L \\ L+1 , and use (12) to\n\nSince \u2207M D(\u03b1\u03c4 ) = 0 by the definition (11) of M, we\ncan use (13) to get a closed form representation of \u03b1\u03c4M :\n\u03c4\n\u03c4\n\u03b1\u03c4M = Q\u22121\nM M [\u03bb1 \u2212 c+1 QM L+1 1 \u2212 c\u22121 QM L\u22121 1]. (14)\n\nNote that Q\u22121\nM M does not exist when QM M is rank\ndeficient. Nevertheless, standard optimization techniques, such as the conjugate gradient method (Nocedal and Wright, 1999), should always recover \u03b1\u03c4M as\na solution to a linear system.\nAn important observation from (4) is that the upper\nbound c\u03c4yi only changes linearly with \u03c4 : As we increase\nthe quantile parameter \u03c4 to \u03c4 + \u000f, we have\nc\u03c4yi+\u000f = c\u03c4yi \u2212 yi \u2206c\u000f , where \u2206c\u000f :=\n\n2\u000f\n.\nn\n\n(15)\n\nAssume an \u000f deviation from \u03c4 does not change the index sets defined in (11), then (14) still holds for \u03b1\u03c4M+\u000f .\nTherefore, we can use (15) to expand it as:\n\u03b1\u03c4M+\u000f = \u03b1\u03c4M + \u2206c\u000f \u2206\u03b1\u03c4M , where\n\u2206\u03b1\u03c4M\n\n:=\n\nQ\u22121\nM M [QM L+1 1\n\n(16)\n\n\u2212 QM L\u22121 1].\n\nThe optimality condition (12) then allows us to recover\n\u03b1\u03c4 +\u000f from \u03b1\u03c4 via:\n\uf8ee \u03c4 +\u000f \uf8f9 \uf8ee\n\uf8f9\n\u03b1L+1\n\u03b1\u03c4L+1 \u2212 \u2206c\u000f\n\uf8ef \u03b1\u03c4 +\u000f \uf8fa \uf8ef\n\uf8fa\n\u03b1\u03c4L\u22121 + \u2206c\u000f\n\uf8ef L\u22121 \uf8fa = \uf8ef\n\uf8fa\n(17)\n\uf8f0 \u03b1\u03c4 +\u000f \uf8fb \uf8f0 \u03b1\u03c4 + \u2206c\u000f \u2206\u03b1\u03c4 \uf8fb .\nM\nM\nM\n0\n\u03b1\u03c4R+\u000f\n\n\f626\n\nYU ET AL.\n\nProposition 3.1 For the dual of the quantile classification problem (8), there exists a set of quantiles\n{\u03c4k }K\nk=1 , \u03c4k \u2208 [0, 1], such that we can find a solution path \u03b1\u03c4 that is continuous in \u03c4 , and linear in\n\u03c4, \u2200\u03c4 \u2208 (\u03c4k , \u03c4k+1 ).\n\nUAI 2009\nWe know from the optimality condition (12) that an\nindex i from M is just about to move into L (Event\n(\u03c4 +\u000f)\n(\u03c4 +\u000f)\n= cyik . Expanding both sides of\n2), when \u03b1i k\nthe last equation, using (15) and (16), shows that \u000f\nsatisfies\n\nSee Appendix C for the proof of Proposition 3.1.\n\n\u03b1i\u03c4k +\n\n\u03c4\n\nProposition 3.1 shows that \u03b1 is piecewise linear in\n\u03c4 . Using (13) and (17), we can see that the gradient\n\u2207D(\u03b1\u03c4 ) has the same property. In particular, \u2200\u000f \u2208\n(0, \u03c4k+1 \u2212 \u03c4k ), we have \u2207M D(\u03b1(\u03c4k +\u000f) ) = 0 and\n\u2207N D(\u03b1(\u03c4k +\u000f) ) = \u2207N D(\u03b1\u03c4k )\n\u2206c\u000f\nk\n+\n[QN L\u22121 1 \u2212 QN L+1 1 + QN M \u2206\u03b1\u03c4M\n],\n\u03bb\n\n(18)\n\nwhere M is the margin index set associated with \u03b1\u03c4k +\u000f\nand N := L \u222a R is the complement set of M.\n\n4\n\nFinding the Dual Solution Path\n\nIt follows from Proposition 3.1 that if we can find a set\nof quantile parameters: K := {\u03c4k }K\nk=1 , that divide the\ninterval [0, 1] into regions so that within these regions\n\u03b1\u03c4 changes linearly with \u03c4 , i.e., the index sets: L, M,\nand R remain fixed. Then we can quickly recover \u03b1\u03c4\nfor any value of \u03c4 from a \u03b1\u03c4k via (17). In what follows\nwe present our algorithm (Algorithm 1) that is able to\nidentify all \u03c4k , which we call kinks.\n4.1\n\nThe Algorithm\n\nOur goal is to construct a sorted list of kinks {\u03c4k }K\nk=1 ,\nat which one of the following events happens:\n\nGiven a quantile parameter \u03c4k , its corresponding optimal dual solution \u03b1\u03c4k , and the associated index sets\nL, M, and R, we know from the definition (11) that\n\u2207i D(\u03b1\u03c4k ) 6= 0, \u2200i \u2208 N . This means that Event\n1 happens when an \u000f > 0 deviation from \u03c4k just\nturns a nonzero element of the gradient to zero, i.e.,\n\u2207i D(\u03b1\u03c4k +\u000f ) = 0, i \u2208 N . We immediately see from\n(18) that the deviation that leads to Event 1 is:\n\u000fto M = min{\u000fi : \u000fi > 0}i\u2208N , where\n\u0014\n\u0015\n\u2212\u03bb\u2207i D(\u03b1\u03c4k )\nn\n.\n\u000fi :=\n2 Qi L\u22121 1\u2212Qi L+1 1+Qi M \u2206\u03b1\u03c4Mk\n\n(19)\n\n(20)\n\nHere care must be taken when \u03b1i\u03c4k = c\u03c4yki , i \u2208 M,\ni.e., \u03b1i\u03c4k is on the boundary between L and M. In\nthis case (20) can be reduced to \u000f\u2206\u03b1i\u03c4k = \u2212\u000fyi ; and if\n\u2206\u03b1i\u03c4k > \u2212yi , then an arbitrarily small \u000f > 0 will cause\n(\u03c4 +\u000f)\n(\u03c4 +\u000f)\n> cyik , i.e., pushing the index i out toward\n\u03b1i k\nL. Taking this boundary case into consideration, we\ndetermine a candidate \u000f using the following criteria:\n\u000fto L = min{\u000fi : \u000fi \u2265 0}i\u2208M , where\n(21)\n\uf8f1\n\u03c4k\n\u03c4\n\u03c4\nk\n\uf8f4\nif \u03b1i = cyki & \u2206\u03b1i > \u2212yi ,\n\uf8f20\n\u000fi := +\u221e\nif \u03b1i\u03c4k = c\u03c4yki & \u2206\u03b1i\u03c4k \u2264 \u2212yi ,\n\uf8f4\n\uf8f3 n \u03c4k\n\u03c4k\n\u03c4k\n2 (cyi \u2212 \u03b1i )/(\u2206\u03b1i + yi ) otherwise.\nIf \u000fto L = 0, we treat \u03c4k as a kink, and update the\nindex sets accordingly:\nM \u2190 M \\{ito L }, L \u2190 L \u222a{ito L },\nwhere\n\nto L\n\ni\n\nto L\n\n= {i : i \u2208 M, \u000fi = \u000f\n\n(22)\n},\n\nsuch that the updated index sets coincide with the\nindex sets of the optimal dual solution \u03b1\u03c4 , \u2200\u03c4 \u2208\n(\u03c4k , \u03c4k+1 ), \u03c4k+1 being the next kink.\nSimilarly, to detect Event 3, we find \u000f that satisfies\n\u03b1i\u03c4k +\u000f = 0, \u2200i \u2208 M, and isolate \u03b1i\u03c4k = 0 case for\nspecial treatment:\n\n1. Elements in N , i.e., not in M, move to M,\n2. Elements in M move to L,\n3. Elements in M move to R.\nTo this end, our algorithm starts with \u03c4 = 0, and then\nmoves forward toward \u03c4 = 1 to identify all values of \u03c4\nthat alter the membership of an index.\n\n2\u000f\n2\u000f\n\u2206\u03b1i\u03c4k = c\u03c4yki \u2212 yi\n, i \u2208 M.\nn\nn\n\n\u000fto R = min{\u000fi : \u000fi \u2265 0}i\u2208M , where\n\uf8f1\n\uf8f4\nif \u03b1i\u03c4k = 0 & \u2206\u03b1i\u03c4k < 0,\n\uf8f20\n\u000fi := +\u221e\nif \u03b1i\u03c4k = 0 & \u2206\u03b1i\u03c4k \u2265 0,\n\uf8f4\n\uf8f3n\n\u03c4k\n\u03c4k\notherwise.\n2 (\u2212\u03b1i )/(\u2206\u03b1i )\n\n(23)\n\nIn the case where \u000fto R = 0, we should recognize \u03c4k as\na kink, and shift the corresponding index from M to\nR. See Algorithm 1 for detailed implementation.\n4.2\n\nComplexity Analysis\n\nThe time complexity of Algorithm 1 is dominated by\nthe calculation of \u2206\u03b1\u03c4M (16), which involves solving\na linear system of size | M |. A standard solver such\nas the conjugate gradient method converges to the solution of such a linear system in at most O(r| M |2 )\ntime, r being the rank of QM M . Having computed\n\u2206\u03b1\u03c4M , the main cost of finding \u000fto M (19) is the\nO(n| N |) cost of matrix-vector multiplication; and the\n\n\fUAI 2009\n\nYU ET AL.\n\nAlgorithm 1 Dual Path Finding (DPF)\n1: input data {(xi , yi )}n\ni=1 and regularizer \u03bb\n2: output sorted list of kinks \u03c4k , corresponding\ndual solution \u03b1\u03c4k and index sets.\n0\n3: \u03b1 = argmin D(\u03b1), s.t. 0 \u2264 \u03b1i \u2264 c0yi , \u2200i\n4: construct L, M and R for \u03b10 via (11)\n5: calculate \u2207D(\u03b10 ) and \u2206\u03b10M\n6: \u03c4 \u2190 0 and K \u2190 {(0, \u03b10M , L, M)}\n7: while \u03c4 < 1 do\n8:\n\u000f \u2190 min{\u000fto M (19), \u000fto L (21), \u000fto R (23)}\n9:\nupdate \u2207D(\u03b1\u03c4 ) to \u2207D(\u03b1\u03c4 +\u000f ) via (18)\n10:\nupdate \u03b1\u03c4 to \u03b1\u03c4 +\u000f via (17)\n11:\nadjust index sets according to the event that \u000f triggers, e.g., if \u000f = \u000fto L , apply (22)\n12:\n\u03c4 \u2190\u03c4 +\u000f\n13:\ncalculate \u2206\u03b1\u03c4M\n14:\nif \u000f = 0 then\n15:\noverwrite the last element of K with\n(\u03c4, \u03b1\u03c4M , L, M)\n(cf. discussion in Sec. 4.1)\n16:\nelse\n17:\nK \u2190 K \u222a{(\u03c4, \u03b1\u03c4M , L, M)}\n18:\nend if\n19: end while\n20: return K\n\ntime required to find \u000fto L (21) and \u000fto R (23) is linear in | M |. Once we find all the kinks, we can recover \u03b1\u03c4 for any \u03c4 \u2208 [0, 1] via (17) in O(n) time\nk\nk\nby noting that \u03b1\u03c4M = \u03b1\u03c4M\n+ \u2206c(\u03c4 \u2212\u03c4k ) \u2206\u03b1\u03c4M\n, where\n\u03c4k+1\n\u03c4k\n\u03c4k\n(\u03c4k+1 \u2212\u03c4k )\n\u2206\u03b1M = (\u03b1M \u2212 \u03b1M )/\u2206c\nand M is associated with \u03b1\u03c4k .\nIn term of memory requirement, saving kink information at Step 17 of Algorithm 1 requires O(| M |) space\nfor \u03b1\u03c4M ; and after the initial O(n) cost of saving the\nentire L and M sets, we only need to keep track of the\nindices that move into or out of the two sets to recover\nthem from their initial copy. We use the Q matrix\nin our calculation, e.g., (19). The Q matrix is usually dense; and caching it requires O(n2 ) space. This\ncan be prohibitively expensive. However, noticing that\nQ = (Y X)(X > Y ), where Y is a diagonal matrix with\nn\u00d7d\n>\nlabels yi on its diagonal and X = [x>\n1 , * * * , xn ] \u2208 R\nis a feature matrix, we can instead cache Y X, which\nis often very sparse. But, constructing Q from Y X\nat each step can be computationally expensive. Fortunately, since only the product of Q with a vector\nv is needed for our calculation, we can calculate it\nas Qv = Y X(X > Y v) to leverage fast sparse matrixvector product, and hence reduce the computational\noverhead. Although we do not have a formal bound\non the size of | K |, our experiments show that it never\nexceeds O(n log n).\n\n5\n\nRelated Work\n\nPerhaps the closest in spirit to our paper is the work\nof Hastie et al. (2004), who studied the influence of the\n\n627\nregularization constant \u03bb on the generalization performance of a binary SVM. They showed that solutions\nto a SVM training problem is a piecewise linear function of \u03bb. Based on this observation, they proposed\nan algorithm that finds SVM solutions for all values\nof \u03bb. The regularization constant controls the balance\nbetween the regularization term and the empirical risk\nin the objective function (1) to prevent a classifier from\noverfitting the training data. Therefore, it plays an important role in improving prediction accuracy on unseen data. The effect of \u03c4 on the behaviour of a SVM\nclassifier is fundamentally different from that of \u03bb in a\nsense that \u03c4 determines the trade-off between the true\npositive rate (TPR) and the true negative rate (TNR)\nof a classifier by assigning asymmetric costs to false\npositive and false negative predictions. In applications\nwhere an appropriate balance between TPR and TNR\nis considered to be more important than prediction\naccuracy, e.g., in medical diagnosis, using a quantile\nclassifier (3) with adjustable \u03c4 may be more desirable.\nAlthough SVM classifiers with built-in asymmetric\nmisclassification costs have been applied to classification problems that are characterized by highly skewed\ntraining data and to problems arisen from medical diagnosis (Veropoulos et al., 1999; Morik et al., 1999;\nGrandvalet et al., 2006), no rigorous statistical properties were established. The misclassification cost is\ncommonly chosen to reflect label proportions of training data or the ratio of false positive cost to false negative cost. From the optimization viewpoint training a\nSVM with asymmetric costs is very similar to the standard SVM training problem. Hence, optimization software such as SVMperf (Joachims, 2006) and LIBLINEAR\n(Hsieh et al., 2008) can be used for training. A common strategy to train a SVM classifier with multiple\nsettings of asymmetric costs is to reassign costs, and\nretrain. Our DPF method exploits the piecewise linearity property of SVM dual solution, and finds the entire\nsolution path in one shot. This allows us to quickly\nconstruct a classifier for any choice of misclassification\ncosts in the post-training phrase.\nQuantile regression as an important statistical tool\n(Koenker and Bassett, 1978) has recently received attention from machine learning community. Takeuchi\net al. (2006) showed that a quantile regression problem\ncan be cast as a regularized risk minimization problem:\nmin\n\nn\nX\n\u03bb\nkwk2 +\nmax(\u03c4 (yi \u2212 fi ), (1 \u2212 \u03c4 )(fi \u2212 yi )),\n2\ni=1\n\nwhere \u03c4 \u2208 (0, 1) and fi = w> xi . This regression problem is very reminiscent of the quantile classification\nproblem (3) we considered in this paper. In fact, by\nfollowing the same principle as discussed in Section 4\nwe can extend our DPF method to find quantile regres-\n\n\f628\n\nYU ET AL.\n\nTable 1: Datasets and regularization constants \u03bb.\nDataset\nDiabetes\nSpam\nSplice\n\nTr./Val./Te.\n668/50/50\n2500/2500/2500\n104 /104 /104\n\nDim.\n8\n48784\n804\n\nSpa.\n13.4 %\n99.7 %\n75.0 %\n\n\u03bb\n\u22123\n\n10\n10\u22126\n10\u22122\n\nsion solutions for all choices of \u03c4 .\n\n6\n\nExperiments\n\nWe now evaluate the generalization performance of a\nquantile classifier for various values of \u03c4 , and compare the time complexity of our DPF method (Algorithm 1) with a state-of-the-art linear SVM classifier:\nLIBLINEAR version 1.32 (Hsieh et al., 2008). We used\nthe LBFGSB quasi-Newton method of Byrd et al. (1995)\nto solve the dual problem at Step 3 of the Algorithm\n1; and the conjugate gradient method was applied to\nfind \u2206\u03b1\u03c4M at Step 13. We ran DPF without caching\nthe Q matrix.\nOur experiments used three datasets: the UCI\ndiabetes dataset (Asuncion and Newman, 2007), the\nspam dataset for task A of the ECML/PKDD 2006 discovery challenge,2 and 3 \u00d7 104 worm splice samples\nfrom a biological dataset provided by S\u00f6ren Sonnenburg.3 Table 1 summarizes the datasets and our parameter settings. In all experiments the regularization\nparameter was chosen from the set 10{\u22126,\u22125,*** ,\u22121} ,\nsuch that a SVM classifier with symmetric misclassification costs achieves the highest prediction accuracy\non the validation set, while generalization performance\nis reported on the test set. We included a bias b in the\ndecision function: sign f (x) := w> x + b by using the\n>\n>\n>\nfollowing strategy: xi \u2190 [x>\ni , 1] , w \u2190 [w , b] .\nAll experiments were carried out on a Linux machine with dual 2.4 GHz Intel Core 2 processors\nand 4 GB of RAM. Our Python code is available\nfor download from http://users.rsise.anu.edu.\nau/\u223cjinyu/Code/DPF.tar.gz.\nOur first set of experiments shows the influence of the\nquantile \u03c4 on the behaviour of a classifier. As \u03b1\u03c4\nchanges, the generalization performance of the quantile classifier in terms of TPR (a.k.a. sensitivity) and\nTNR (a.k.a. specificity) changes accordingly. Figure 2\nshows that TPR decreases (but not necessarily strictly\ndecreases) with \u03c4 , while TNR has an opposite trend.\nThis is because increasing \u03c4 corresponds to increasing\nthe false positive cost C+ (cf. Lemma 2.2; see also\n2\n\nThe\noriginal\nevaluation\nset\n(http://www.\necmlpkdd2006.org/challenge.html) was equally divided into training, validation, and test set.\n3\nhttp://www.fml.tuebingen.mpg.de/raetsch/\nprojects/lsmkl\n\nUAI 2009\nFigure 1, right), which leads the classifier to recognize\nmore and more instances as negative samples at an expense of a decreasing TPR. At \u03c4 = 0 (resp. \u03c4 = 1),\nthe classifier simply resorts to labeling all the points\nas + (resp. \u2212). Therefore at these extreme points, the\nprediction accuracy depends on the proportion of the\npositive and negative samples in the dataset. For instance, on the splice dataset where 5.5% of the data\nis labeled as +, we obtain 5.5% accuracy at \u03c4 = 0.\nFor intermediate values of \u03c4 the prediction accuracy\ndepends on cleanliness of the dataset measured as the\ntotal percentage of the data which lies at the margin.\nFor instance, on the spam dataset for \u03c4 = 0, around\n0.28% of the training samples were at the margin. This\nnumber stabilized to about 0.32% for \u03c4 \u2208 (0.0, 0.9],\nleading to very stable classification accuracy, as can\nbe seen from Figure 2.\nClearly, finding the solution for any value of \u03c4 \u2208 [0, 1]\nis more time consuming than finding the solution for\na fixed \u03c4 . To investigate the excess time spent in this\nendeavor, we compare the time complexity of our DFP\nalgorithm with one single run of LIBLINEAR, a state\nof the art linear SVM training algorithm which can\nhandle asymmetric classification costs.4 Our second\ncomparator is the LBFGSB algorithm, which can also be\nused to train a linear SVM for any fixed value of \u03c4 . The\ncore functions of LIBLINEAR and LBFGSB are implemented in C++ and Fortran, respectively (We called\nthese functions through their Python wrappers.), while\nour DPF algorithm is implemented in Python, which is\ninherently 2 to 5 times slower. Therefore, our CPU\ntime comparison is in favor of LIBLINEAR and LBFGSB.\nRecall that our DFP algorithm invokes any linear SVM\nsolver to find the initial solution,5 and then finds the\nsolution path by constructing K. We compute the ratio of the CPU time spent on constructing K to the\naverage time required by LIBLINEAR to find a solution for a given \u03c4 . The averaging is done by running\nLIBLINEAR (resp. LBFGSB) to compute the solution for\n\u03c4 = 0.1, 0.2, . . . , 0.9. As shown in Table 2 on the\ndiabetes dataset DPF finds about 2\u00d7103 kinks, spending 2.8\u00d7103 (resp. 28) times of the average LIBLINEAR\n(resp. LBFGSB) running time. The running time of DPF\nincreases to 3.6 \u00d7 103 (resp. 69) times of that required\nby a typical run of LIBLINEAR (resp. LBFGSB) on the\nsplice dataset where it finds over 2 \u00d7 104 kinks. We\nfound empirically that the number of kinks, | K |, increases with the size of training set, n, but is bounded\nby n log(n).\n4\n\nWe called LIBLINEAR with input arguments '-s 3 -B 1\n-e 1e-3 -w1 (2-2\u03c4 ) -w-1 (2\u03c4 ) -c 1/(n\u03bb)'.\n5\nAlthough in theory this is true, in practice we find that\nin the extreme case of \u03c4 = 0, LIBLINEAR's performance degrades dramatically. Therefore, we exclusively use LBFGSB\nas an initial solver\n\n\fUAI 2009\n\nYU ET AL.\n\n629\n\nTable 2: Average CPU seconds for recovering a solution from kink information (Recover) contrasted with the\naverage time of running LIBLINEAR and LBFGSB to find \u03b1\u03c4 for \u03c4 \u2208 {0.1, 0.2, * * * , 0.9}. We also show the time\nrequired by DPF for path-finding. The final column | K | lists the number of kinks.\nDataset\nDiabetes\nSpam\nSplice\n\nRecover\n0.003\n0.012\n0.103\n\nCPU Seconds\nLIBLINEAR LBFGSB\n0.004\n0.401\n0.467\n11.102\n0.363\n19.155\n\nIt is not surprising that DFP is computationally more\nexpensive than a single run of LIBLINEAR and LBFGSB.\nBut as can be seen in Table 2, after one run of DFP,\nwe can recover the solution for any \u03c4 efficiently. For\ninstance, on the spam dataset, this only requires 0.012\nseconds, compared to 0.467 seconds (resp. 11.102 seconds) for a single run of LIBLINEAR (resp. LBFGSB).\n\n7\n\nConclusions and Outlook\n\nIn this paper we first show that minimizing the asymmetric hinge loss will lead to a quantile classifier which\nis risk optimal for asymmetric misclassification costs.\nWe then present an algorithm which finds the entire solution path of a quantile classifier in a principled way.\nGiven the entire solution path, we can construct a classifier for any given asymmetric classification cost very\nefficiently. Admittedly, our numerical experiments are\npreliminary. Running conjugate gradient repeatedly\nto find \u03b1\u03c4M is the main bottleneck in our DFP algorithm. We are exploring decomposition methods,\nwhich can take advantage of warm starts to reduce\nthe computational burden. Future work includes extension of our algorithm to quantile regression and to\nmulti-class classification problems.\nAcknowledgements\nNICTA is funded by the Australian Government's\nBacking Australia's Ability and the Centre of Excellence programs. This work is also supported by the\nIST Program of the European Community, under the\nFP7 Network of Excellence, ICT-216886-NOE.\nReferences\nA. Asuncion and D. Newman. UCI machine learning\nrepository, 2007.\nR. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited\nmemory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):\n1190\u20131208, 1995.\nA. Conn, N. Gould, and P. Toint. Global convergence\nof a class of trust region algorithms for optimization\n\nDPF\n11.104\n1106.413\n1315.878\n\n|K|\n1886\n6660\n22611\n\nwith simple bounds. SIAM journal on numerical\nanalysis, 25(2):433\u2013460, 1988.\nR. O. Duda, P. E. Hart, and D. G. Stork. Pattern\nClassification and Scene Analysis. John Wiley and\nSons, New York, 2001. Second edition.\nY. Grandvalet, J. Marithoz, and S. Bengio. A probabilistic interpretation of SVMs with an application to unbalanced classification. In Y. Weiss,\nB. Sch\u00f6lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 467\u2013\n474, Cambride, MA, 2006. MIT Press.\nT. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The\nentire regularization path for the support vector machine. JMLR, 5:1391\u20131415, 2004.\nC. J. Hsieh, K. W. Chang, C. J. Lin, S. S. Keerthi,\nand S. Sundararajan. A dual coordinate descent\nmethod for large-scale linear SVM. In W. Cohen,\nA. McCallum, and S. Roweis, editors, ICML, pages\n408\u2013415. ACM, 2008.\nT. Joachims. Training linear SVMs in linear time. In\nProc. ACM Conf. Knowledge Discovery and Data\nMining (KDD). ACM, 2006.\nR. Koenker and G. Bassett. Regression quantiles.\nEconometrica, 46(1):33\u201350, 1978.\nY. Lin. Support vector machines and the bayes rule in\nclassification. Data Mining and Knowledge Discovery, 6(3):259\u2013275, 2002.\nJ. J. Mor\u00e9 and G. Toraldo. Algorithms for bound\nconstrained quadratic programming problems. Numerische Mathematik, 55(4):377\u2013400, 1989.\nK. Morik, P. Brockhausen, and T. Joachims. Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring. In\nProc. Intl. Conf. Machine Learning, pages 268\u2013277.\nMorgan Kaufmann, San Francisco, CA, 1999.\nJ. Nocedal and S. J. Wright. Numerical Optimization.\nSpringer Series in Operations Research. Springer,\n1999.\nJ. Platt. Probabilities for SV machines. In A. J. Smola,\nP. L. Bartlett, B. Sch\u00f6lkopf, and D. Schuurmans,\neditors, Advances in Large Margin Classifiers, pages\n61\u201373, Cambridge, MA, 2000. MIT Press.\nP. Sollich. Bayesian methods for support vector ma-\n\n\f630\n\nYU ET AL.\nspam\n\ndiabetes\n\n0.6\n\n0.8\n\n0.8\n\n0.6\n\n0.4\n\n0.6\n\n0.4\n\ntpr\ntnr\naccuracy\n\n0.2\n0.0\n\n1.0\n\nPerformance\n\nPerformance\n\n0.8\n\nsplice\n\n1.0\n\nPerformance\n\n1.0\n\nUAI 2009\n\n0.0\n\n0.2\n\n0.4\n\n\u03c4\n\n0.6\n\n0.4\n\ntpr\ntnr\naccuracy\n\n0.2\n\n0.8\n\n1.0\n\n0.0\n\n0.0\n\n0.2\n\n0.4\n\n\u03c4\n\n0.6\n\ntpr\ntnr\naccuracy\n\n0.2\n\n0.8\n\n1.0\n\n0.0\n\n0.0\n\n0.2\n\n0.4\n\n\u03c4\n\n0.6\n\n0.8\n\n1.0\n\nFigure 2: Our DFP algorithm is able to recover the solution for all values of \u03c4 , which allows us to plot the true\npositive rate, true negative rate, and accuracy on the test dataset.\nchines: Evidence and predictive class probabilities.\nMachine Learning, 46:21\u201352, 2002.\nI. Takeuchi, Q. V. Le, T. Sears, and A. J. Smola.\nNonparametric quantile estimation. J. Mach. Learn.\nRes., 7, 2006.\nK. Veropoulos, C. Campbell, and N. Cristianini. Controlling the sensitivity of support vector machines.\nIn Proceedings of IJCAI Workshop Support Vector\nMachines, pages 55\u201360, 1999.\n\nA\n\nThe Lagrangian of the constrained optimization problem (8) takes the form of\nL(\u03b1, \u03b2, \u03b3) := D(\u03b1) \u2212\n\ni=1\n\nthen we only need to show that f \u2217 (x) minimizes Lx (f )\nfor any fixed x.\nWe first show that if \u03b7(x) < \u03c4 then the minimizer f \u2217\nsatisfies f \u2217 (x) = \u22121. Suppose not, that is, there exists\nx0 such that \u03b7(x0 ) < \u03c4 but f \u2217 (x0 ) 6= \u22121. Let f \u0303(x) be\nthe same as f \u2217 (x) except that f \u0303(x0 ) = \u22121. Using the\nshorthand f \u2217 (x0 ) = f \u2217 , f \u0303(x0 ) = f \u0303 and \u03b7(x0 ) = \u03b7, we\nobtain\nLx0 (f \u2217 ) = \u03c4 (1 \u2212 \u03b7)(1 + f \u2217 )+ + (1 \u2212 \u03c4 )\u03b7(1 \u2212 f \u2217 )+\n\u2265 (1 \u2212 \u03c4 )\u03b7[(1 \u2212 f \u2217 )+ + (1 + f \u2217 )+ ]\n\u2265 2(1 \u2212 \u03c4 )\u03b7 = Lx (f \u0303),\n0\n\nwhere the last inequality comes from Jensen's inequality since (.)+ is a convex function. For the first\ninequality the bound is achieved only if f \u2217 \u2264 \u22121; and\nfor the second inequality the bound is achieved only\nif f \u2217 \u2208 [\u22121, 1]. Thus when f \u2217 6= \u22121 it leads to a\ncontradiction. A symmetric argument can be used to\nshow that if \u03b7(x) > \u03c4 then f \u2217 (x) = 1.\n\nB\n\nKKT Optimality Conditions\n\nn\nX\n\n\u03b3i (\u03b1i \u2212 c\u03c4yi ),\n\ni=1\n\n\u2207i L(\u03b1\u03c4 , \u03b2 \u2217 , \u03b3 \u2217 ) = \u2207i D(\u03b1\u03c4 ) \u2212 \u03b2i\u2217 + \u03b3i\u2217 = 0,\n\u03b2i\u2217 \u03b1i\u03c4 = 0,\n\u03b3i\u2217 (\u03b1i\u03c4 \u2212 c\u03c4yi ) = 0,\n\nProof Let Lx (f ) be the risk conditioned on X = x:\n\u03c4 (1 \u2212 \u03b7(x))(1 + f (x))+ + (1 \u2212 \u03c4 )\u03b7(x)(1 \u2212 f (x))+ ,\n\n\u03b2i \u03b1i +\n\nwhere \u03b2i and \u03b3i are non-negative Lagrange multipliers. The KKT conditions (Nocedal and Wright, 1999)\nsuggest that at optimum (\u03b1\u03c4 , \u03b2 \u2217 , \u03b3 \u2217 ) we have\n\nProof of Lemma 2.2\n\nLx (f ) = E [((1 + Y )/2 \u2212 \u03c4 Y )(1 \u2212 Y f (X))+ | X = x] =\n\nn\nX\n\n0 \u2264 \u03b1i\u03c4 \u2264 c\u03c4yi , \u03b2i\u2217 \u2265 0, \u03b3i\u2217 \u2265 0, \u2200i.\nSimple analysis reveals that the above KKT optimality\nconditions constrain \u03b1\u03c4 to take the form given in (12).\n\nC\n\nProof of Proposition 3.1\n\nProof Suppose the index sets (11) of \u03b1\u03c4 remain\nunchanged for all \u03c4 \u2208 (\u03c4k , \u03c4k+1 ). The linearity of\n\u03b1\u03c4 in (\u03c4k , \u03c4k+1 ) follows directly from (17). Let\n\u000f = \u03c4k+1 \u2212 \u03c4 , compute \u03b1\u03c4 +\u000f from \u03b1\u03c4 via (17), and\nlet \u03c4k+1 be chosen in such a way that the membership\nof an index i changes at \u03b1\u03c4 +\u000f . This can only happen\nwhen \u03b1i\u03c4 +\u000f takes its boundary values: 0 or c\u03c4yi+\u000f with\n\u2207i D(\u03b1\u03c4 +\u000f ) = 0, which means either an i \u2208 M is\nabout to leave M, or an i \u2208\n/ M just moves into\nM, where M is the margin index set of \u03b1\u03c4 . We\nnow show that \u03b1\u03c4 +\u000f is optimal. To show this, we\nonly need to show \u03b1i\u03c4 +\u000f is optimal. By construction\n\u2207i D(\u03b1\u03c4 +\u000f ) = 0, and since \u03b1i\u03c4 +\u000f only takes 0 or c\u03c4yi+\u000f ,\nthe KKT optimality conditions (Appendix B) can be\neasily satisfied with appropriate choices of \u03b2i\u2217 and\n\u03b3i\u2217 , implying that \u03b1i\u03c4 +\u000f is optimal. Hence, \u03b1\u03c4 +\u000f is\noptimal. Therefore, we can set \u03b1\u03c4k+1 = \u03b1\u03c4 +\u000f , and\nuse it as a starting point to construct subsequent\ndual solution path via (17). The dual solution path\nexplored in this way is clearly continuous in \u03c4 .\n\n\f"}