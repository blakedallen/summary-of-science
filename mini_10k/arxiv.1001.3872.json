{"id": "http://arxiv.org/abs/1001.3872v1", "guidislink": true, "updated": "2010-01-21T20:27:01Z", "updated_parsed": [2010, 1, 21, 20, 27, 1, 3, 21, 0], "published": "2010-01-21T20:27:01Z", "published_parsed": [2010, 1, 21, 20, 27, 1, 3, 21, 0], "title": "Stochastic firing rate models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1001.4212%2C1001.4255%2C1001.4399%2C1001.4632%2C1001.0612%2C1001.0583%2C1001.1706%2C1001.2879%2C1001.4537%2C1001.1617%2C1001.1539%2C1001.4265%2C1001.3320%2C1001.0130%2C1001.0505%2C1001.3291%2C1001.3872%2C1001.3022%2C1001.0970%2C1001.4779%2C1001.4203%2C1001.4723%2C1001.0349%2C1001.3251%2C1001.2523%2C1001.4207%2C1001.3606%2C1001.1334%2C1001.0706%2C1001.1253%2C1001.4433%2C1001.1250%2C1001.1329%2C1001.1934%2C1001.0428%2C1001.3114%2C1001.5081%2C1001.0166%2C1001.0631%2C1001.4868%2C1001.1989%2C1001.3035%2C1001.5380%2C1001.4865%2C1001.1066%2C1001.3912%2C1001.1992%2C1001.3356%2C1001.3167%2C1001.3937%2C1001.3197%2C1001.2506%2C1001.1596%2C1001.5265%2C1001.1388%2C1001.3935%2C1001.0514%2C1001.3870%2C1001.3296%2C1001.4046%2C1001.1210%2C1001.0435%2C1001.3939%2C1001.0205%2C1001.2491%2C1001.3801%2C1001.1586%2C1001.2770%2C1001.0037%2C1001.1587%2C1001.3514%2C1001.0289%2C1001.5049%2C1001.3587%2C1001.0052%2C1001.1173%2C1001.1962%2C1001.1229%2C1001.3337%2C1001.2040%2C1001.2459%2C1001.3959%2C1001.1274%2C1001.1010%2C1001.4947%2C1001.3334%2C1001.0711%2C1001.0960%2C1001.0348%2C1001.2481%2C1001.3730%2C1001.5082%2C1001.2378%2C1001.1678%2C1001.1681%2C1001.3447%2C1001.1726%2C1001.3390%2C1001.4132%2C1001.1022%2C1001.4303&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Stochastic firing rate models"}, "summary": "We review a recent approach to the mean-field limits in neural networks that\ntakes into account the stochastic nature of input current and the uncertainty\nin synaptic coupling. This approach was proved to be a rigorous limit of the\nnetwork equations in a general setting, and we express here the results in a\nmore customary and simpler framework. We propose a heuristic argument to derive\nthese equations providing a more intuitive understanding of their origin. These\nequations are characterized by a strong coupling between the different moments\nof the solutions. We analyse the equations, present an algorithm to simulate\nthe solutions of these mean-field equations, and investigate numerically the\nequations. In particular, we build a bridge between these equations and\nSompolinsky and collaborators approach (1988, 1990), and show how the coupling\nbetween the mean and the covariance function deviates from customary\napproaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1001.4212%2C1001.4255%2C1001.4399%2C1001.4632%2C1001.0612%2C1001.0583%2C1001.1706%2C1001.2879%2C1001.4537%2C1001.1617%2C1001.1539%2C1001.4265%2C1001.3320%2C1001.0130%2C1001.0505%2C1001.3291%2C1001.3872%2C1001.3022%2C1001.0970%2C1001.4779%2C1001.4203%2C1001.4723%2C1001.0349%2C1001.3251%2C1001.2523%2C1001.4207%2C1001.3606%2C1001.1334%2C1001.0706%2C1001.1253%2C1001.4433%2C1001.1250%2C1001.1329%2C1001.1934%2C1001.0428%2C1001.3114%2C1001.5081%2C1001.0166%2C1001.0631%2C1001.4868%2C1001.1989%2C1001.3035%2C1001.5380%2C1001.4865%2C1001.1066%2C1001.3912%2C1001.1992%2C1001.3356%2C1001.3167%2C1001.3937%2C1001.3197%2C1001.2506%2C1001.1596%2C1001.5265%2C1001.1388%2C1001.3935%2C1001.0514%2C1001.3870%2C1001.3296%2C1001.4046%2C1001.1210%2C1001.0435%2C1001.3939%2C1001.0205%2C1001.2491%2C1001.3801%2C1001.1586%2C1001.2770%2C1001.0037%2C1001.1587%2C1001.3514%2C1001.0289%2C1001.5049%2C1001.3587%2C1001.0052%2C1001.1173%2C1001.1962%2C1001.1229%2C1001.3337%2C1001.2040%2C1001.2459%2C1001.3959%2C1001.1274%2C1001.1010%2C1001.4947%2C1001.3334%2C1001.0711%2C1001.0960%2C1001.0348%2C1001.2481%2C1001.3730%2C1001.5082%2C1001.2378%2C1001.1678%2C1001.1681%2C1001.3447%2C1001.1726%2C1001.3390%2C1001.4132%2C1001.1022%2C1001.4303&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We review a recent approach to the mean-field limits in neural networks that\ntakes into account the stochastic nature of input current and the uncertainty\nin synaptic coupling. This approach was proved to be a rigorous limit of the\nnetwork equations in a general setting, and we express here the results in a\nmore customary and simpler framework. We propose a heuristic argument to derive\nthese equations providing a more intuitive understanding of their origin. These\nequations are characterized by a strong coupling between the different moments\nof the solutions. We analyse the equations, present an algorithm to simulate\nthe solutions of these mean-field equations, and investigate numerically the\nequations. In particular, we build a bridge between these equations and\nSompolinsky and collaborators approach (1988, 1990), and show how the coupling\nbetween the mean and the covariance function deviates from customary\napproaches."}, "authors": ["Jonathan Touboul", "Bard Ermentrout", "Olivier Faugeras", "Bruno Cessac"], "author_detail": {"name": "Bruno Cessac"}, "author": "Bruno Cessac", "links": [{"href": "http://arxiv.org/abs/1001.3872v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1001.3872v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.MP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.NC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1001.3872v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1001.3872v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:1001.3872v1 [math.PR] 21 Jan 2010\n\nStochastic firing rate models\nJonathan Touboul1,2 , Bard Ermentrout2 , Olivier Faugeras3 , and\nBruno Cessac3,4\n1\n\nDepartment of Mathematical Physics, The Rockefeller University, New York, USA\n2\nDepartment of Mathematics, University of Pittsburgh, Pittsburgh, USA\n3\nNeuroMathComp Laboratory, INRIA, Sophia Antipolis, France\n4\nLaboratoire J. Dieudonn\u00e9, Universit\u00e9 de Nice, France\n\nOctober 29, 2018\n\nAbstract\nWe review a recent approach to the mean-field limits in neural networks that\ntakes into account the stochastic nature of input current and the uncertainty\nin synaptic coupling. This approach was proved to be a rigorous limit of the\nnetwork equations in a general setting, and we express here the results in a\nmore customary and simpler framework. We propose a heuristic argument to\nderive these equations providing a more intuitive understanding of their origin.\nThese equations are characterized by a strong coupling between the different\nmoments of the solutions. We analyse the equations, present an algorithm to\nsimulate the solutions of these mean-field equations, and investigate numerically the equations. In particular, we build a bridge between these equations\nand Sompolinsky and collaborators approach [38, 12], and show how the coupling between the mean and the covariance function deviates from customary\napproaches.\n\nIntroduction\nThe problem of modeling neural activity at scales integrating the effect of thousands of neurons has been a great endeavor in neurosciences for several reasons.\nFirst, it corresponds to the mesoscopic scale that most imaging techniques are\nable to measure. The activity monitored by these classical imaging techniques\n(such as the electro-encephalogram, the magneto-encephalogram, the optical\nimaging, etc. . . ) shows the global behavior of a whole area that results from\nthe activity of several hundreds to several hundreds of thousands of neurons.\nSecond, anatomical data recorded in the cortex reveal the existence of structures, such as the cortical columns, with a diameter of about 50\u03bcm to 1mm,\n\n1\n\n\fcontaining of the order of one hundred to one hundred thousand neurons belonging to a few different types. These columns exhibit specific functions as a\nresult of the collective behavior of several neurons, as evidenced for instance in\nthe primary visual cortex V1, where cortical columns are organized in a specific\ngeometric configuration and respond to preferential orientations of bar-shaped\nvisual stimuli. In this case, information processing does not occur at the scale\nof individual neurons but rather emerges from the collective dynamics of many\ninteracting neurons contributing to a mesoscopic or macroscopic signal. The\ndescription of this collective dynamics requires models which are different from\nindividual neuron models. Indeed, when the number of neurons is large enough,\naveraging effects appear, and the collective dynamics is well described by an\neffective mean-field, summarizing the effect of the interactions of a neuron with\nthe other neurons, and depending on a few effective parameters.\nModels including these features which use population rates or activity equations are the foundation of a common approach to modeling for neural networks.\nThese equations provide meanfield dynamics for the firing rate or activity of neurons within a network given some connectivity. This approach was introduced\nby Wilson and Cowan in the 1970s [41, 42], and has been widely studied since\nthen. This approach assumes that the activity coming from a cortical column\nis captured in the mean-firing rate of the cells in the population.\nOne of the principal limitations of firing rate models is that they only take\ninto account mean firing rates and do not include any information about higher\norder statistics such as correlations between firing activity of different neurons\nof the same populations or across populations. We know that spike trains of\nindividual cortical neurons in vivo are very noisy and have interspike interval\n(ISI) distributions close to Poisson (see e.g. [37]). However, at the level of a neural population, the question of the statistical distribution of the activity is still\nopen, and its description has been the subject of a many studies in the last fifteen\nyears. Numerous studies on integrate-and-fire networks have shown that under\ncertain conditions, even though individual neurons would exhibit Poisson-like\nstatistics, the neurons fire asynchronously so that the total population activity\nevolves according to a mean-field rate equation, with a characteristic activation\nor gain function [1, 7, 8]. Formally speaking, the asynchronous state only exists in the thermodynamic limit where the number of neurons tends to infinity\n(finite-size deviations from this limit are addressed in [8]). Moreover, even if the\nasynchronous state exists it might not be stable. This has motivated a number of authors to analyze statistical correlations and finite-size effects in spiking\nnetworks [23, 28, 27, 15]. Recently, a few papers analyzed the effects of correlations by utilizing the master equation introduced by El Boustani and Destexhe\n[15] and providing different moment truncations using statistical physics tools,\nsuch as path integrals [9] and the Van Kampen expansion [6]. These approaches\nenlighten a complex interplay between the mean activity and the mean correlations, that yield non-trivial qualitative effects, that begin to be uncovered (see\n[39]). A complementary approach has been developed by Cai and collaborators based on a Boltzmann\u2013like kinetic theory of integrate\u2013and\u2013fire networks\n[32, 10], which is itself an extension of population-density methods [30, 29]. A\n2\n\n\frecent alternative approach proposes a mathematically derived mean-field equation on stochastic firing\u2013rate networks [19] taking into account variability in the\nconnectivity weights. They rigorously derive the mean-field equation of the network when the number of neurons tends to infinity and study the solutions for\nthe dynamic and stationary mean-field equations.\nThe present article builds upon this latter approach, and aims to relate this\nabstract approach to Wilson and Cowan deterministic mean-field equation [41,\n42] on one hand, and few other stochastic methods on the other hand. To this\npurpose, we implement a simple yet very familiar model that fits the framework\nof [19]. We propose, in this simpler case, a heuristic argument that yields\nthe same mean-field equation they obtain, analyse these equations, describe an\nalgorithm to compute numerically the unique solution of these equations, and\nfinally, investigate numerically the solutions of these equations.\n\n1\n\nMaterial and Method : The stochastic firingrate model\n\nIn this article we study a model that we call the stochastic firing-rate model, a\nstochastic version of the customary firing rate models. This section is aimed to\ndefine this model.\n\n1.1\n\nThe firing-rate model\n\nThe building block of the network we study here consists of firing-rate Wilson\nand Cowan models [41, 42], where we take into account the stochastic nature of\nsynaptic inputs. In details, we consider a network composed of N neurons indexed by i \u2208 {1, . . . , N } belonging to P populations indexed by \u03b1 \u2208 {1, . . . , P },\nand let us denote N\u03b1 to be the number of neurons in population \u03b1. Each neuron\nis described by its membrane potential Vi . Alternatively, this model can be seen\nas a model of cortical area, where the interacting units are cortical columns that\nfollow Wilson and Cowan equations.\nThe Wilson and Cowan model supposes that each incoming spike provokes\na postsynaptic potential when received, and that these postsynaptic potentials\nare summed linearly to produce the membrane potential Vi (t). A single action\npotential from neuron j received at time t\u2217 by the neuron i provokes the addition\nof potential P SPij (t\u2212s), where s is the time of the spike hitting the synapse and\nt the time after the spike. We neglect the delays due to the distance travelled\ndown the axon by the spikes.\nUnder the assumption that the post-synaptic potentials sum linearly, the\naverage membrane potential of neuron i is given by the sum of all postsynaptic\npotentials coming from the other neurons j, through the formula:\nZ t\nX\nX\nk\nVi (t) =\nP SPij (t \u2212 tj ) =\nP SPij (t \u2212 s)\n\u03b4(tkj \u2212 s) ds\n(1)\nj,k\n\nt0\n\n3\n\ntjk\n\n\fwhere the sum is taken over the arrival times tkj of the spikes produced by the\nneurons j. When the number of neurons tends to infinity, or when the firing\nrate of neurons increase, the sum can be approximated by the mean firing rate\nof each neuron, to first order. More precisely, this approximation consists in\nsupposing that since the number of spikes arriving from neuron j between t and\nt + dt is \u03bdj (t)dt, we can approximate equation (1) by:\nXZ t\nP SPij (t \u2212 s)\u03bdj (s) ds.\nVi (t) =\nj\n\nt0\n\nWe now assume that the neuron received input for arbitrary long times,\ni.e. t0 = \u2212\u221e. Another very important assumption of the rate model is that\nthe relation between the mean firing rate and the membrane potential can be\nexpressed in the form \u03bdi (t) = Si (Vi (t)), where Si is sigmoidal function (called\nthe firing rate, or squashing function, see e.g. [22, 13], and this function can be\napproximated in some cases using an averaging method (see section 1.3).\nXZ t\nVi (t) =\nP SPij (t \u2212 s)Sj (Vj (s)) ds,\nj\n\n\u2212\u221e\n\nWe finally note that the postsynaptic potentials P SPij can depend on several\nvariables in order to account, for instance, for adaptation or learning. However,\nin this paper, we follow the common assumption, initially made by Hopfield in\n[25], that although the sign and amplitude may vary, the post-synaptic potential\nhas the same shape no matter which presynaptic population caused it. This\nleads to the relation\nP SPij (t) = Jij gi (t).\ngi represents the unweighted shape (called a g-shape) of the postsynaptic potentials and Jij is the strength of the postsynaptic potentials elicited by neuron j\non neuron i. In this paper, we further assume the g-shape to be an exponential\nfunction, and that both the g-shape and the sigmoidal functions only depends\non the population (\u03b1) the neuron i belongs to: gi (t) = g\u03b1 (t) = e\u2212t/\u03c4\u03b1 1t>0 .\nUnder these assumptions, the membrane potential of neuron i from population\n\u03b1 can be written as the solution of the following differential equation:\nP\n\nN\u03b2\n\nXX\ndV\u03b1\n1\n= \u2212 V\u03b1 +\nS\u03b2 (Vj (t)).\ndt\n\u03c4\u03b1\nj=1\n\u03b2=1\n\nThis equation models the interactions between the neurons in the network.\nExternal inputs also have a great influence in the system. These inputs are\nmodelled as the sum of a deterministic input and a noisy input described in\nsection 1.2. The deterministic input, generally assumed to originate from the\nthalamus, accounts for sensory inputs and cortico-cortical deterministic activity.\nThese inputs are assumed to be identical for all neurons in the same population,\n\n4\n\n\fand are denoted I\u03b1 (t) for the input to the population \u03b1. Therefore, we end up\nwith the deterministic model:\nP\n\nN\u03b2\n\nXX\ndV\u03b1\n1\n= \u2212 V\u03b1 + I\u03b1 (t) +\nS\u03b2 (Vj (t)).\ndt\n\u03c4\u03b1\nj=1\n\u03b2=1\n\nwhich corresponds to a case of the Wilson and Cowan system. We now include\nthe effect of noise in this model.\n\n1.2\n\nRandomness in the network\n\nCortical areas receive a constant bombardment of action potentials originating\nfrom different sources, ranging from sensory stimuli to motor areas and corticocortical activity. These signals are characterized by a high level of variability,\nwhich is a result of the randomness1 of processes arising in neuronal phenomena (see e.g.[37, 36]). This noise can have different origins. It is the result\nof contributions attributed, for instance, to thermal noise arising from the discrete nature of electric charge carriers, the behavior of ion channels, synaptic\ntransmission failures and global network effects. We also include in this noise\nterm all the unrelated activity either coming from external stimuli or from extra network cortico-cortical activity. This bombardment of \"random\" spikes\nresults in a noisy current ni whose probability distribution only depends on the\npopulation the neuron belongs to. Using the classical diffusion approximation\n(see e.g. [21, 14, 40]), we model that the noisy current ni as a white noise, the\nformal differential of a centered Wiener process with standard deviation f\u03b1 for\nneurons in population \u03b1, and we assume that the noise received by each neuron\nis independent of those received by the other neurons.\nMoreover, the synaptic weights Jij are known up to a certain precision, and\ncannot be defined individually with high precision. The mean connectivity,\nand the standard deviation of the error done in the measurement can nevertheless be experimentally evaluated. The random synaptic weights are therefore\n \u0304\nassumed to be drawn\npfrom a Gaussian distribution of mean J\u03b1\u03b2 /N\u03b2 and standard deviation \u03c3\u03b1\u03b2 / N\u03b2 , and do not evolve (they are said to be frozen during\nthe evolution). The scaling chosen ensures that the local interaction process\nPN\u03b2\nj=1 Jij S\u03b2 (Vj (t)), summarizing the effects of the neurons in population \u03b2 on\nneuron i, has a mean and variance which do not depend on N\u03b2 and are only\ncontrolled by the phenomenological parameters J \u0304\u03b1\u03b2 , \u03c3\u03b1\u03b2 . We finally make the\nsimplifying technical assumption that the Jij are independent2\n1 Some\n\nauthors consider this variability as a result of an underlying finite dimensional chaos,\nsee e.g. [20, 26]\n2 It is however known that synaptic weights are indeed correlated (e.g. via synaptic plasticity mechanisms); these correlations are built by dynamics via a complex interwoven evolution\nbetween neurons and synapses dynamics and postulating the form of synaptic weight correlations requires, on theoretical grounds, a detailed investigation of the whole history of\nneuron-synapse dynamics.\n\n5\n\n\fUnder these assumptions, the membrane potential of neuron i in population\n\u03b1 satisfies the equation\n\uf8eb\n\uf8f6\nN\u03b2\nP X\nX\n1\ndVi (t) = \uf8ed\u2212 Vi (t) +\nJij S\u03b2 (Vj (t)) + I\u03b1 (t)\uf8f8 dt + f\u03b1 dWi (t)\n(2)\n\u03c4\u03b1\nj=1\n\u03b2=1\n\n1.3\n\nAveraging Models and Firing rate functions\n\nThe nonlinear form of the firing rate function has important implications; there\nare many choices that various authors have used. The simplest is the step\nfunction, where the neuron fires maximally or not at all depending on whether\nthe potential is above or below threshold. If one uses a statistical-mechanical\napproach to derive 'mean-field' equations [2] then this sharp step function is\nsmoothed out and can be represented by one of the well-known squashing functions:\n(\nSmax\nor\nS(V ) = 1+e\u2212(V\n\u2212VT )/Vs\nS(V ) =\n\nSmax\n2 (1\n\n+ erf(\u2212(V \u2212 VT )/Vs ))\n\nThe first one is often referred to as the logistic function, and the second as the\nGaussian function; VT is an activation threshold and Vs governs the slope of the\nsigmoid. These choices appear somehow subjective and arbitrary.\nWhen the number of neuron increases, this squashing function can be computed explicitly in some simple cases, using averaging properties (see e.g. [34]\nfor class II neurons and [16] for class I neurons). This method can guide the\nchoice of the squashing function. It is described in more detail in section 2.1.\n\n2\n\nTheory and Calculations\n\nThe network we consider is governed by the N equations such as (2). In these\nequations, the neuron i of population \u03b1 integrates external deterministic and\nstochastic inputs, and receives from the network neurons a current, which we\ncall the microscopic interaction process. The mean-field problem consists in\nfinding an equation that governs the activity of the neurons, at the level of\nthe network population, when the number of neurons tends to infinity. In the\nstochastic setting of this article, this question is understood as a limit in law,\nunder the joint law of the connectivities and the Brownian motions. In this\nsection, we first review a way to obtain the squashing functions, before deriving\nthe mean-field equations for the networks we are interested in.\n\n2.1\n\nAveraging method and squashing functions\n\nThe squashing function corresponds to the averaged effect of many interconnections. When the number of neurons increases, this squashing function can\nbe computed explicitly in some simple case, using averaging properties when\nassuming slow synapses (see e.g. [34] for class II neurons and [16] for class\n6\n\n\fI neurons). This method can provide a choice of squashing function. Let us\nconsider for instance the following system of coupled neurons:\n\uf8f1 dV\nP\nappl\nj\nion\nrev\n\uf8f4\n\uf8f2C dt + Ij (Vj , wj ) = k \u03bbjk sk (t) (Vk \u2212 Vj ) + Ij\ndwj\n(3)\ndt = q(Vj , wj )\n\uf8f4\n\uf8f3 dsj\ndt = \u03b5(\u015dj (Vj ) \u2212 sj )\nThe jth cell of the network is represented by its potential, Vj , and all of the\nauxiliary channel variables that make up the dynamics for the membrane, wj .\nThe term Ijappl is any tonic applied current. Finally, the synapses are modeled\nby simple first order dynamics and act to hyperpolarize, shunt, or depolarize the\npostsynaptic cell. \u03b5 is a small parameter corresponding to the time scale of the\nsynapses compared to the time scale of evolution of the membrane potential,\nand is assumed to be small. Associated with each neuron is a synaptic channel\nwhose dynamics is governed by the variable sj which depends in a (generally\nnonlinear) manner on the somatic potential. Thus, one can think of sj as being\nthe fraction of open channels due to the presynaptic potential. The functions \u015dj\nhave maxima of 1 and minima of 0. The effective maximal conductances of the\nsynapses between the cells are in the nonnegative numbers \u03bbjk and the reversal\npotentials of each of the synapses are Vkrev . The goal is to derive equations that\ninvolve only the sj variables and thus reduce the complexity of the model while\nretaining the qualitative features of the original.\nThe main idea is to exploit the smallness of \u03b5 and thus invoke the averaging\ntheorem on the slow synaptic equations. Each of the slow synapses is held\nconstant and the membrane dynamics equations are solved for the potentials,\nVj (t; s1 , . . . , sn ). The potentials, of course, naturally depend on the values of\nthe synapses. We will assume that the potentials are either constant or periodic\nwith period T (s1 , . . . , sn ). Once the potentials are found, one then averages the\nslow synaptic equations over one period of the membrane dynamics obtaining:\ndsj\n= \u03b5(Sj (s1 , * * * , sn ) \u2212 sj )\ndt\nwhere\nSj (s1 , . . . , sn ) =\n\n1\nTj (s1 , . . . , sn )\n\nZ\n\nTj (s1 ,...,sn )\n\n\u015dj (Vj (t; s1 , . . . , sn ))dt.\n\n(4)\n\n0\n\nRemark. Note that if we assume that when the cell is not firing, the potential\nis below the threshold for the synapse, we obtain a model very close to the\nactivity-based (see [17]).\nThis method allows derivation of the squashing function. The main assumption is that the detailed phase and timing of individual spikes is not important;\nthat is, one is free to average over many spiking events. Rinzel and Frankel\napply the same averaging method to derive equations for a pair of mutually\ncoupled neurons that was motivated by an experimental preparation. In their\n7\n\n\fpaper, they require only cross connections with no self-self interactions, and they\nare able to numerically determine the potential as a function of the strength of\nthe synapses. They use a class of membrane models that are called \"class II\"\n(see [33]) where the transition from rest to repetitive firing occurs at a subcritical Hopf bifurcations that appears, for instance, in the Hodgkin-Huxley model.\nThis latter assumption implies that the average potential exhibits hysteresis as\na function of the synaptic drive, i.e. the functions \u015dj (sk ) are multivalued for\nsome interval of values of sk . Because of this hysteresis, they are able to combine\nan excitatory cell and an inhibitory cell in a network and obtain oscillations,\nwhich is impossible for smooth \u015dj (sk ) without self-excitatory interactions (see,\ne.g. [33]).\nClass I membranes are simpler, as proved by Ermentrout in [16]. It is known\n(see e.g. [18]) that the frequency of oscillation in Class I membranes evolves as\nthe square root of the bifurcation parameter (denoted by p):\np\n(5)\n\u03c9 = C(p\u2217 ) (p \u2212 p\u2217 )+ + O(|p \u2212 p\u2217 |).\nIf we make the approximation that \u015dj are simple on/off function, that is equal\nto 0 if the membrane is at rest (and thus below threshold) and 1 if the voltage is\nabove threshold. Let \u03be(p) denote the amount of time during one cycle that the\npotential is above threshold. Then the equation (4), depending on the critical\nparameter p on which the potential depends, is simplified to\nS(p) =\n\n1\nT (p)\n\nZ\n\nT (p)\n\n\u015d(V (t; p))dt =\n0\n\n\u03c9(p)\u03be(p)\n2\u03c0\n\nwhere we have used the fact that T = 2\u03c0/\u03c9. A fortuitous property of class\nI membranes is that the time for which the spike is above threshold is largely\nindependent of the period of the spike so that \u03be(p) is essentially constant. (This\nis certainly true near threshold and we have found it to be empirically valid in\na wide parameter range.) Thus, combining this with (5), we obtain the very\nsimple squashing function:\np\nS(p) = C(p\u2217 ) (p \u2212 p\u2217 )+ .\nwhere C(p) and p depend on the other parameters in the model.\nThe value p\u2217 and the constant C(p\u2217 ) need to be evaluated. Ermentrout in\n[16] provides closed-form expressions for these quantities in the case of MorrisLecar membrane model, and obtains a good agreement with the full system in\nnumerical simulations. The resulting equations have the very nice compact form\nin a two cells network:\n(\np\ne\n= Ce (\u03bbee se \u2212 ge\u2217 (\u03bbie si , Ie ))+\n\u03c4e ds\ndt + se\np\n(6)\ni\n\u03c4i ds\n= Ci (\u03bbei se \u2212 ge\u2217 (\u03bbii si , Ii ))+\ndt + si\nwhere ge\u2217 (g, I) is the two-parameter surface of critical values of the excitatory\nconductance. Note that these squashing functions are not similar to the usual\n8\n\n\fnonlinear transforms between the voltage and the firing rate. They are less\nsmooth than the usual ones, and in particular, are not differentiable at p = p\u2217 :\nthe slope at the bifurcation is infinite. The differential equations, such as (6),\nobtained with this squashing function do not satisfy the Lipschitz standard condition for the existence and uniqueness of solutions. Moreover, in one dimension. differential equations with a square-root nonlinearity are classical counterexamples to the uniqueness condition: there can exist multiple solutions starting\nfrom the same initial condition. Therefore, this mean-field method yields equations that are complex to investigate, both in the class I case where the function\nis multivalued as in the class II case where it is non-smooth. Note eventually\nthat this approach yields a mean-field description of model (3) in the sense that\nit results from an averaging of the behavior of an assembly of cells, when the\nnumber of neurons tends to infinity.\n\n2.2\n\nDerivation of the Mean-Field equations\n\nConsidering now a network of firing rate neurons of type (2), a rigorous proof\nof the existence of the mean-field limit and the characterization of this limit\nis provided in [19], where the authors derived their calculations from the work\nof Ben-Arous and Guionnet [5]. This proof is extremely technical, but can\nbe approached with non-rigorous heuristic arguments. We develop the heuristic\nproof of the derivation of this mean-field equation because we believe it provides\na better understanding of what the technical mean-field limit derived in [19]\nphysically corresponds to.\nThe heuristic proof we propose here is based on Amari's local chaos hypothesis introduced in 1972 and then widely used [2, 3, 38, 12, 11, 35], and can be\nexpressed as follows:\nAmari's Local Chaos Hypothesis: For N is sufficiently large, all the Vi are\npairwise stochastically independent, are independent of the connectivity parameters Jij , and have a common distribution population per population.\nProposition 2.1. Under Amari's local chaos hypothesis, the microscopic inN\nteraction term U\u03b1\u03b2\ncorresponding to the current from population \u03b2 on a neuron\ni in population \u03b1 defined by:\nN\nUi\u03b2\n\n:=\n\nN\u03b2\nX\n\n\u0001\nJij S\u03b2 Vj (t)\n\n(7)\n\nj=1\n\nhas a distribution that only depends on the populations \u03b1 and \u03b2, and converges\nin law (under the joint probability of the connectivity weights and the voltages)\nwhen the number of neurons tends to infinity towards an effective interaction\nV\nprocess U\u03b1\u03b2\nsuch that:\n\uf8f1 h\ni\n\uf8f2E U V (t) = J \u0304\u03b1\u03b2 E[S\u03b2 (V\u03b2 (t))]\n\u03b1\u03b2\nh\ni\n\uf8f3Cov(U V (t), U V (s)) = \u03c3 2 E S\u03b2 (V\u03b2 (t))S\u03b2 (V\u03b2 (s)) 1\u03b1=\u03b3; \u03b2=\u03b4\n\u03b1\u03b2\n\u03b3\u03b4\n\u03b1\u03b2\n9\n\n,\n\n(8)\n\n\fwhere V\u03b1 corresponds to the common law of the neurons in population \u03b1 ensured\nby the local chaos hypothesis, and 1A is the indicator function of the set A.\nFor the sake of simplicity, for any P-dimensional stochastic process X, we\nintroduce the notation mX\n\u03b2 (t) = E[S\u03b2 (X\u03b2 (t))] and\nh\ni\ndef\n\u2206X\n(t,\ns)\n=\nE\nS\n(X\n(t))S\n(X\n(s))\n\u03b2\n\u03b2\n\u03b2\n\u03b2\n\u03b2\nWe now turn to the heuristic proof of this proposition. We consider that N is\nlarge enough for the local chaos hypothesis to be valid. Under this assumption,\nthe voltages are independent, identically distributed and independent of the\nindividual connectivity weights Jij .\nThe microscopic interaction process (7) is therefore the sum of independent\nidentically distributed random variables. The functional central limit theorem\napplies to our regular case, following [31] (very weak regularity conditions are\nnecessary on the summed processes to get this convergence), provided the convergence of the two first moments of the sum.\nThe problem therefore reduces to the computation of the limits of the mean\nand standard deviation of the microscopic interaction process when the number\nof neurons tends to infinity. First of all, the mean of the microscopic interaction\nprocess for N large enough is constant, since we have\n\u0010\n\n\u0011\n\nN\nE U\u03b1\u03b2\n(t) = E\n\nN\u03b2\n\u0010X\n\nN\u03b2\n\u0011 X\n\u0010 \u0011 \u0010\n\u0011\nJij S\u03b2 (Vj (t)) =\nE Jij E S\u03b2 (Vj (t))\n\nj=1\n\nj=1\n\n\u0010\n\n= J \u0304\u03b1\u03b2 E S\u03b2 (V\u03b2 (t))\n\n\u0011\n\nThis constant value is thus the limit of the means of the sum of independent\nrandom variables, and therefore the mean of the limiting Gaussian process.\nSimilar straightforward computations (see appendix A) prove that the covariance of the microscopic interaction process converges to the expression given\nin the proposition. From the functional central limit theorem and the convergence of the two first moments of the microscopic interaction, we conclude to\nthe convergence of this sequence of processes to the effective Gaussian process\nof mean and covariance given in the proposition. The statistical independence\nbetween this effective interaction process and the membrane potential V is also\nproved in the same appendix A.\nAs a direct consequence of this last proposition, the mean-field equation for\nthe membrane potential processes can be derived.\nProposition 2.2. Under the local chaos hypothesis, the network equations (2)\nconverge in law towards the solution of the non-Markovian stochastic equation:\n\uf8f6\n\uf8eb\nP\nX\n1\nV\nU\u03b1\u03b2\n(t) + I\u03b1 (t)\uf8f8 dt + f\u03b1 dWt\u03b1 .\n(9)\ndV\u03b1 (t) = \uf8ed\u2212 V\u03b1 (t) +\n\u03c4\u03b1\n\u03b2=1\n\nwhere the processes (W\u03b1 (t))t\u2265t0 are independent Wiener processes and U V (t) =\nV\n(U\u03b1\u03b2\n(t); \u03b1, \u03b2 \u2208 {1, . . . , P })t is the effective interaction process.\n10\n\n\fProof. The solution of the network equation (2) with initial condition V (0) at\nt = 0 can be written as:\nZ t\nP Z t\nX\nN\nVi (t) = V\u03b1 (0)e\u2212t/\u03c4\u03b1 +\ne(s\u2212t)/\u03c4\u03b1 U\u03b1\u03b2\n(s) ds + f\u03b1\ne(s\u2212t)/\u03c4\u03b1 dWs\u03b1 .\n0\n\n\u03b2=1\n\n0\n\nBecause of the convergence in law of the microscopic interaction process to\nthe effective\ninteR t interactionN process, we have the convergence\nR t in law of the\nV\ngral term 0 e(s\u2212t)/\u03c4\u03b1 U\u03b1\u03b2\n(s) ds towards the effective term 0 e(s\u2212t)/\u03c4\u03b1 U\u03b1\u03b2\n(s) ds,\nand therefore, for any neuron i of population \u03b1, the potential converges in law\ntowards the solution V\u03b1 of the stochastic fixed-point equation:\nV\u03b1 (t) = V\u03b1 (0)e\n\n\u2212t/\u03c4\u03b1\n\n+\n\nP Z\nX\n\u03b2=1\n\nt\n\ne\n\n(s\u2212t)/\u03c4\u03b1\n\nV\nU\u03b1\u03b2\n(s) ds\n\n0\n\nZ\n+ f\u03b1\n\nt\n\ne(s\u2212t)/\u03c4\u03b1 dWs\u03b1 (10)\n\n0\n\nwhich is equivalent to equation (9).\nThe two equivalent equations (9) and (10) are referred to as the mean-field\nequations. In the limit where the number of neurons tends to infinity, all the\nneurons have the same distributions, behave independently and for any neuron\nin population \u03b1, its membrane potential is solution of this set of equations.\nRemark. All the derivations have been done based on Amari's local chaos\nhypothesis, which is a non-rigorous assumption. However, the exact same result\nis obtained by rigorous mathematical methods [19, 4, 24].\nWe discuss the form of these mean-field equations and the existence and\nuniqueness of their solutions in the following section.\n\n2.3\n\nAnalysis of the Mean-Field equations\n\nThe mean-field equations derived in the previous section are rather unusual\nequations. Indeed, they involve the effective interaction process term, which is a\nfunctional of the solution of the equation. More precisely, it is a Gaussian process\nwhose law depends on the statistics of the solution of the equation in a non\nMarkovian way. Therefore, this equation cannot be considered as a stochastic\ndifferential equation. It is an equation set in the space of stochastic processes,\nor equivalently an equation on the probability distribution of the mean-field\nsolution. Similar issues were observed in the case of spin glasses [4]. The\nexistence and uniqueness of solutions of the dynamic mean-field equation (i.e.\nstarting from a Gaussian initial condition) is proved in [19], by considering the\nmean-field equation as a fixed point equation in the set of stochastic processes.\nConditions are also given for a stationary solution to exist.\nThe approach is based on the fact that solutions having a Gaussian initial condition V (0) (or for stationary solutions) are necessarily Gaussian processes, and therefore characterizing the mean \u03bcV (t) and the correlation function\nV\nC\u03b1\u03b2\n(s, t) is sufficient to identify the solution of the mean-field equations. How\n11\n\n\fdo we compute these two deterministic functions? Let us start by writing the\nequations they satisfy. By straightforward computations on equations (9) and\n(10), we obtain the equations for the mean \u03bcV (t) and the correlation C\u03b1\u03b2 (s, t)\nfor s \u2264 t:\n\u0010 q\n\u0011\n\uf8f1 V\nPP  \u0304 R +\u221e\nd\u03bc\u03b1\n1 V\nV (t, t) + \u03bcV (t) Dx + I (t),\n\uf8f4\n\u03bc\n(t)\n+\n=\n\u2212\nJ\nS\nx\nC\n\u03b1\u03b2\n\u03b2\n\u03b1\n\uf8f4\n\u03b2\n\u03b2=1\n\u03b2\u03b2\ndt\n\u03c4\u03b1 \u03b1\n\u2212\u221e \u0010\n\uf8f4\nh\n\u0011\n\uf8f4\n\uf8f4\n\uf8f2C V (t, s) = e\u2212(t+s)/\u03c4\u03b1 Var(V (0)) + \u03c4\u03b1 f\u03b12 e \u03c42s\u03b1 \u2212 1\n\u03b1\n\u03b1\u03b1\n2\ni\nR t R s (u+v)/\u03c4 V\nPP\n\uf8f4\n2\n\u03b1\n\uf8f4\n+\n\u03c3\ne\n\u2206\n(u,\nv)dudv\n,\n\uf8f4\n\u03b1\u03b2\n\u03b2=1 \u03b1\u03b2 0 0\n\uf8f4\n\uf8f4\n\uf8f3\nC\u03b1\u03b2 (s, t) \u2261 0\nfor \u03b1 6= \u03b2\n(11)\n\u221a\nwhere Dx = exp(\u2212x2 /2)/ 2\u03c0 dx is the standard Gaussian measure and\n\uf8eb q\n\n\uf8f6\nX (u, u) C X (v, v) \u2212 C X (u, v)2\nX\nC\u03b2\u03b2\nC\n(u,\nv)\n\u03b2\u03b2\n\u03b2\u03b2\n\u03b2\u03b2\n\uf8f8\nq\n\u2206X\nS\u03b2 \uf8edx\n+ yq\n+ \u03bcX\n\u03b1\u03b2 (u, v) =\n\u03b2 (u)\nX (v, v)\nX (v, v)\nIR2\nC\u03b2\u03b2\nC\u03b2\u03b2\n\u0010 q\n\u0011\nX (v, v) + \u03bcX (v) Dx Dy\n\u00d7 S\u03b2 y C\u03b2\u03b2\n\u03b2\nZ\n\n(12)\nIn the more general setting of [19], it is proved that these equations have a\nunique solution that can be computed via the iteration of a map F defined on\nthe set of continuous mean and covariance functions. This function transforms\na Gaussian process X with mean \u03bcX and covariance C X into the Gaussian\nprocess Y = F(X) with mean \u03bcY and covariance C Y given by:\n\n\u2212t/\u03c4\u03b1\n\u03bcY\u03b1 (t) = \u03bcX\n+\n\u03b1 (0)e\n\nZ\n\nt\n\ne\u2212(t\u2212s)/\u03c4\u03b1 (\n\n0\n\n=\n\n\u2212t/\u03c4\u03b1\n\u03bcX\n\u03b1 (0)e\n\nZ\n+\n\nP\nX\n\nJ \u0304\u03b1\u03b2 E [S\u03b2 (X\u03b2 (s))] + I\u03b1 (s))ds\n\n\u03b2=1\nt\n\ne\u2212(t\u2212s)/\u03c4\u03b1 I\u03b1 (s)ds\n\n0\n\n+\n\nP\nX\n\u03b2=1\n\nJ \u0304\u03b1\u03b2\n\nZ\n\nt\n\ne\u2212(t\u2212s)/\u03c4\u03b1\n\n+\u221e\n\nZ\n\n\u2212\u221e\n\n0\n\n\u0011\n\u0010 q\nS\u03b2 x v\u03b2X (s) + \u03bcX\n\u03b2 (s) Dxds. (13)\n\nwhere we denoted v\u03b1X (s) the standard deviation of X\u03b1 at time s, instead of\nX\nC\u03b1\u03b1\n(s, s), and\nh\n\u0011\n\u03c4\u03b1 s2\u03b1 \u0010 \u03c42s\nY\ne \u03b1 \u22121\nC\u03b1\u03b1\n(t, s) = e\u2212(t+s)/\u03c4\u03b1 v\u03b1X (0) +\n2\nZ tZ s\nP\ni\nX\n2\n+\n\u03c3\u03b1\u03b2\ne(u+v)/\u03c4\u03b1 \u2206X\n\u03b1\u03b2 (u, v)dudv , (14)\n0\n\n\u03b2=1\n\nwhere \u2206X\n\u03b1\u03b2 is defined in equation (12).\n12\n\n0\n\n\fThese equations take a particularly simple form in the case\nR y where the firingrate sigmoidal functions are erf functions, where erf(y) = \u2212\u221e Dx is the repartition function of the standard Gaussian distribution. Let us consider that\nS\u03b1 (x) = erf(g\u03b1 x + \u03b3\u03b1 ) where erf is the repartition function of the Gaussian. In\nthat case (see appendix B for the calculations), the mean-field equations take\nthe form:\n!\nP\ngp(i) \u03bc\u03b2 (t) + \u03b3p(i)\n\u03bc\u03b1 X  \u0304\nd\u03bc\u03b1\np\n=\u2212\n+\nJ\u03b1\u03b2 erf\ndt\n\u03c4\u03b1\n1 + g 2 v\u03b2 (t)\n\u03b2=1\nand the equation governing the standard deviation of the process reads:\nZ tZ t\nP\nh\nX\n\u03c4\u03b1 f\u03b12 2t/\u03c4\u03b1\n2\nv\u03b1 (t) = e\u22122t/\u03c4\u03b1 v\u03b1 (t0 ) +\n\u2212 1) +\ne(u+v)/\u03c4\u03b1\n(e\n\u03c3\u03b1\u03b2\n2\nt0 t0\n\u03b2=1\n!\np\nZ\nq\ni\nC\u03b2\u03b2 (t, s)y + \u03bc\u03b2 (s) v\u03b2 (t)\nS( v\u03b2 (t)y + \u03bc\u03b2 (t)) S\nDy.\nv\u03b2 (t) + g 2 (v\u03b2 (t)v\u03b2 (s) \u2212 C\u03b2\u03b2 (t, s)2 )\nR\n\n(15)\n\nThese simpler equations allow for more efficient numerical simulations, and\nan easier analytical treatment, and will be discussed in the Results section.\n\n2.4\n\nNumerical simulations algoritm\n\nThe mean-field equations can be seen as a fixed point equation in the space\nof stochastic processes, or equivalently as a fixed point equation on the mean\nand the covariance function. In [19], the authors show that this solution can be\nobtained through the iteration of a function that operates on the mean and on\nthe covariance function. This is the approach we chose to numerically compute\nthe solutions of the mean-field equations.\n2.4.1\n\nDescription of the algorithm\n\nIn details, let X be a P -dimensional Gaussian process of mean \u03bcX = (\u03bcX\n\u03b1 (t))\u03b1=1...P\nX\nand covariance C X = (C\u03b1\u03b2\n(s, t))\u03b1,\u03b2\u2208{1...P } . We use the map F which makes\nthe mean-field equation a fixed-point equation, as described in section 2.3.\nFrom equation (13), we can see that knowing v\u03b1X (s), s \u2208 [0, t] we can compute\nY\n\u03bc\u03b1 (t) using a standard discretization scheme of the integral, with a small time\nstep compared with \u03c4\u03b1 and the characteristic time of variation of the input\ncurrent I\u03b1 . Alternatively, since \u03bcY\u03b1 satisfies the differential equation:\nP\n\nX\nd\u03bcY\u03b1\n\u03bcY\n=\u2212 \u03b1 +\nJ \u0304\u03b1\u03b2\ndt\n\u03c4\u03b1\n\u03b2=1\n\nZ\n\n+\u221e\n\n\u2212\u221e\n\n\u0010 q\n\u0011\nS\u03b2 x v\u03b2X (t) + \u03bcX\n\u03b2 (t) Dx + I\u03b1 (t),\n\nwe can compute faster and with a better accuracy the solution using a RungeKutta algorithm.\nThe covariance variable of the image of F (from equation (14)) can be\nOU\nsplit into the sum of two terms: the external noise contribution C\u03b1\u03b1\n(t, s) =\n13\n\n\fh\n\u0010 2s\n\u0011i\n\u03c4 s2\ne\u2212(t+s)/\u03c4\u03b1 v\u03b1X (0) + \u03b12 \u03b1 e \u03c4\u03b1 \u2212 1 , where OU stands for Ornstein-Uhlenbeck,\nand the interaction between the neurons. The external noise contribution is a\nsimple function and can be computed straightforwardly. To compute the interactions contribution to the standard deviation we have to compute the symmetric\nfunction of two-variables:\nZ tZ s\nX\n\u2212(t+s)/\u03c4\u03b1\ne(u+v)/\u03c4\u03b1 \u2206X\nH\u03b1\u03b2 (t, s) = e\n\u03b1\u03b2 (u, v)dudv,\n0\n\n0\n\nfrom which one obtains the standard deviation using the formula\nY\nOU\nC\u03b1\u03b1\n(t, s) = C\u03b1\u03b1\n(t, s) +\n\nP\nX\n\n2\nX\n\u03c3\u03b1\u03b2\nH\u03b1\u03b2\n(t, s).\n\n\u03b2=1\nX\n(t, s), we start\nTo compute the function H\u03b1\u03b2\nX\nX\nH\u03b1\u03b2 (0, 0) = 0. We only compute H\u03b1\u03b2\n(t, s) for\n\nfrom t = 0 and s = 0, where\nt > s because of the symmetry.\n\nIt is straightforward to see that:\n\u0014\n\u0015\ndt\nX\nX\nX\nH\u03b1\u03b2\n(t + dt, s) = H\u03b1\u03b2\n(t, s) 1 \u2212\n+ D\u03b1\u03b2\n(t, s)dt + o(dt),\n\u03c4\u03b1\nwith\nX\nD\u03b1\u03b2\n(t, s) = e\u2212s/\u03c4\u03b1\n\nZ\n\ns\n\nev/\u03c4\u03b1 \u2206X\n\u03b1\u03b2 (t, v)dv.\n\n0\nX\nX\nHence computing H\u03b1\u03b2\n(t+dt, s) knowing H\u03b1\u03b2\n(t, s) amounts to computing D\u03b1\u03b2 (t, s).\nFix t \u2265 0. We have D\u03b1\u03b2 (t, 0) = 0 and\nX\nX\nD\u03b1\u03b2\n(t, s + ds) = D\u03b1\u03b2\n(t, s)(1 \u2212\n\nds\n) + \u2206X\n\u03b1\u03b2 (t, s)ds + o(ds).\n\u03c4\u03b1\n\nX\nX\nThis algorithm enables us to compute H\u03b1\u03b2\n(t, s) for t > s. We deduce H\u03b1\u03b2\n(t, s)\nfor t < s using the symmetry of this function. Finally, to get the values of\nX\nH\u03b1\u03b2\n(t, s) for t = s, we use the symmetry property of this function and get:\n\u0015\n\u0014\n2dt\nX\nX\nX\n+ 2D\u03b1\u03b2\n(t, t)dt + o(dt).\nH\u03b1\u03b2\n(t + dt, t + dt) = H\u03b1\u03b2\n(t, t) 1 \u2212\n\u03c4\u03b1\n\n2.4.2\n\nAnalysis of the algorithm\n\nConvergence rate Using the fact that the iterations of the algorithms produce a Cauchy sequence and using evaluations done in the proofs of existence\nand uniqueness of solutions provided in [19], one can show that the precision of\nthe algorithm is given by\nkC F\n\nN\n\n(X)\n\n\u2212 C XM F k\u221e \u2264 C2 (N + T \u2212 t0 ) dt + RN (k\u0303)\n\n(16)\n\nwhere F N (X) is the N th iterate of the map F on the initial process X and\nXM F the unique fixed point of F, i.e. the mean-field solution. In equation\n(16), C2 denotes a constant depending on the\nP\u221eparameters of the model, RN (x)\nis the exponential remainder, i.e. RN (x) = n=N xn /n! and k\u0303 is some constant\ndepending on the parameters of the model.\n14\n\n\fComplexity The complexity of the algorithm depends on the complexity of\nthe computations of the integrals. The algorithm described hence has the comT 2\nplexity O(Niter ( dt\n) ) where Niter is the iteration number of the algorithm.\n\n3\n\nResults\n\nIn this section, we compare the results provided by the present approach to\na variety of deterministic and stochastic approches. We will be particularly\ninterested in comparing the solutions of the present mean-field equation to the\ndeterministic Wilson and Cowan equations [41, 42]:\nP\n\nX\n1\ndV\u03b1\n= \u2212 V\u03b1 (t) +\nJ \u0304\u03b1\u03b2 S\u03b2 (V\u03b2 (t))\ndt\n\u03c4\u03b1\n\u03b2=1\n\nIf C\u03b2\u03b2 (t, t) \u2261 0 for all t \u2208\n(11) reduces to:\n\nR, then the equation the mean-field equations\nP\n\n\u03bc\u03b1 (t) X  \u0304\nd\u03bc\u03b1 (t)\n=\u2212\nJ\u03b1\u03b2 S\u03b2 (\u03bc\u03b2 (t)) + I\u03b1 (t),\n+\ndt\n\u03c4\u03b1\n\u03b2=1\n\nwhich is precisely the customary Wilson and Cowan equations corresponding to\nthe system where no randomness is considered neither in the input nor in the\n2\ncoefficients. This is precisely the limit for f\u03b1 = 0 and \u03c3\u03b1\u03b2\n= 0 for all \u03b1 and \u03b2\nof the mean-field equations. Therefore, the Wilson and Cowan equations can\nbe seen as the deterministic limit of the general mean-field equations presented\nin this paper. Moreover the solutions of the general mean-field equations in the\nsmall noise case converge to the solutions of these deterministic equations. This\nconvergence is to be taken in a weak sense, since the conditions for existence\nand uniqueness of the solution of the mean-field equations are not satisfied in\nthe case where no noise is present (the setting for existence and uniqueness\nnecessitates to stay in the space of continuous mean and covariance functions).\nHowever, when noise is considered, the mean of the solutions of the meanfield equations differs from the solutions of the Wilson and Cowan's system, and\nnon-trivial differences appear. The precise study of the differences between the\nmean-field system and customary approaches is still an active ongoing research\nfield, but evidences of non-trivial effects are already found. First of all, in the\none-dimensional case, we obtained numerically that presence of noise can delay\nthe apparition of bifurcations, stabilizing fixed points that were unstable in\nthe corresponding Wilson and Cowan equation (see figure 1). This numerical\nobservation is under analytical investigation. Other non-trivial effects include\ntransitions between a deterministic and a chaotic behavior, similar to the ones\npredicted analytically by Crisanti, Sompolinsky and colleagues [38, 12], and\nthat we study in section 3.1. Another non-trivial effect is the influence of the\ncorrelations in the oscillatory regime for a two populations case, presented in\n3.2.\n15\n\n\fFigure 1: Delayed pitchfork bifurcation numerically simulated in a one population network. We plotted the value of the mean of the process on an interval\n[T1 , T2 ] with T1 sufficiently large, as a function of the slope g of the sigmoid\nS(x) = 1/(1 + exp(\u2212g x))\n\n3.1\n\nStationary solutions in the one population case\n\nThe mean-field equations (9) provide another approach to the analytical results of the extensive work of Crisanti, Sompolinsky and colleagues [38, 12]. In\nthese papers, the authors use a generating functional approach to derive the\nmean-field equations the system satisfies. The equations they obtain are very\nsimilar to ours, in the one population case. Hence so far we have exactly the\nsame equations and formalism. They further simplify the problem by considering stationary solutions. In this particular case, the necessary conditions to\nget a stationary solutions to the mean-field equations stated in [19] are satisfied, namely, since the time constants of evolution for each population and the\nstandard deviation of the noise are constant, and what they call the leak matrix\nL, here the diagonal matrix with diagonal element \u22121/\u03c4\u03b1 , has full rank and\nstrictly negative eigenvalues. The stationary solutions can therefore be written\nas:\nV\u03b1 (t) = I\u03b1 +\n\nP Z\nX\n\u03b2=1\n\nt\nV\ne\u2212(t\u2212s)/\u03c4\u03b1 U\u03b1\u03b2\n(t) dt + \u03c3\u03b1\n\n\u2212\u221e\n\nZ\n\nt\n\n(\u03b1)\n\ne\u2212(t\u2212s)/\u03c4\u03b1 dWt\n\n(17)\n\n\u2212\u221e\n\nIn the stationary case, the mean is constant, and the covariance C\u03b1\u03b1 (t, s) only\ndepends on the time difference \u03c4 = |t \u2212 s|. They then write the fixed point\nequation on the covariance as a second order differential equation. This equation\ncan be written as the mouvement of a particle in a potential well, this potential\ndepends on a non-free parameter, q, which is equal to the standard deviation of\nthe process (for a time difference \u03c4 = 0), namely C(0). Because of the structure\nof stationary solutions, this parameter depends on the whole past of the solution\n\n16\n\n\fFigure 2: Simulations of the correlation equation in the stationary case. \u03c41 =\n0.25, \u03c31,1 = 1, S(x) = tanh(g x), and J \u0304 = 1. (A) Deterministic case: g = 0.5.\nThe graph shows the correlation for 1 (red), 2 (green), 3 (blue) and 4 (purple)\niterations of the map F in a semi-log scale, and illustrate the convergence of\nthe iterations towards zero as predicted by Sompolinsky et al.. (B) Chaotic\ncase: g = 5. We observe after convergence of the algorithm that the covariance\nfunction is non-vanishing (red crosses), and is well approximated by a fourthorder Taylor expansion of the theoretical solution (blue dotted curve) and even\nbetter by the sixth order expansion (purple curve).\non (\u2212\u221e, 0), and the equation they write, of type:\n\u2202Vq\nd2 C\n=\u2212\n.\nd\u03c4 2\n\u2202C\n\n(18)\n\nwhere Vq is the potential depending on q = C(0), remains a functional equation\non the covariance, that do not differ from ours. Note that this dependency\nillustrates the non-Markovian nature of the problem.\nAt this point, the authors treat the problem as if q were a free parameter,\nwhich allows them to study the phase portrait of these equations and distinguish\ntwo regimes depending on the maximal slope g of the sigmoidal firing function:\nfor small values of g, a deterministic regime where the covariance has a unique\nfixed point equal to 0, and for larger values of g, a homoclinic trajectory connecting the point q = C \u2217 > 0 where Vq vanishes to the point C = 0. The\nauthors show that this is the only stable solution in the system, and interpret\nthis solution as a chaotic solution in the neural network. Moreover, this solution can be found using energy conservation, and Taylor expansion of Vq can\nbe provided. These two predicted solutions are found by the simulation of the\nfull mean-field system, as illustrated in Figure 2. In this figure, we represent\nthe correlation function C(s, t) as a function of \u03c4 = t \u2212 s. Hence for each value\nof \u03c4 correspond several correlation values which are superposed if the solution\nis stationary (resulting in a curve). These numerical results are compared to\nSompolinsky's predictions and to the Taylor expansion to the fourth and sixth\norder. The authors of [38, 12] predict that there exist a critical value of the\nslope of the sigmoid function S, denoted gc , at which the system presents a\nsharp transition. This value in the numerical case we treat in figure 2 is gc = 4.\n17\n\n\fFor any g smaller than the critical value gc , the system will present a deterministic behavior, with a covariance function identically equal to 0. They call\nthis solution the trivial case, and in that case the solutions of the mean-field\nequations reduce to the solutions of Wilson and Cowan system. Though they\ndo not prove that the system indeed converges to this solution, they show that\nit exists when the parameter q is free, this solution exists. We numerically compute, using the algorithm presented and integrating from a very large negative\ntime, that indeed this solution is the one that is selected by the system, i.e.\nthe only solution to the stationary mean-field equation. Similarly, for g > gc ,\nthe covariance of the system converges towards the convergence corresponding\nto the homoclinic orbit of equation (18) corresponding to the case where the\nparameter q = C(0) is free. Therefore, the behaviors predicted by the studies of\nCrisanti, Sompolinsky and colleagues are confirmed by our numerical approach,\nand their approximation is therefore a posteriori validated.\n\n3.2\n\nOscillations in a two-populations network\n\nLet us now present a case where the fluctuations of the Gaussian field act on\nthe dynamics of \u03bc\u03b1 (t) in a non trivial way, with a behavior departing from\nthe naive mean-field picture. We consider two interacting populations where\nthe connectivity weights are Gaussian random variables J\u03b1\u03b2 \u2261 N (J \u0304\u03b1\u03b2 , \u03c3) for\n(\u03b1, \u03b2) \u2208 {1, 2}2 . We set S\u03b2 (x) = tanh(g x) and I\u03b1 = 0, f\u03b1 = 0, \u03c4\u03b1 = \u03c4, \u03b1 =\n1, 2. Note that in that case, the nonlinear function S(x) takes negative values.\nThis case has a theoretical interest in the sense that analytical calculations can\nbe pursued, which provides a good understanding of the mechanisms. Note\nthat this case corresponds to a case where the sigmoidal transform is positive,\nby considering that S(x) = tanh(g x) + 1 (which is always positive), and for\nwhich the current I\u03b1 is now no more constant but equal to the random variable\n\u2212J\u03b1\u03b2 . All the computations can be done similarly when the current is a random\nvariable. The dynamic mean-field equation for \u03bc\u03b1 (t) is given, in differential\nform, by:\n2\n\n\u03bc\u03b1 X  \u0304\nd\u03bc\u03b1\n=\u2212\nJ\u03b1\u03b2\n+\ndt\n\u03c4\u03b1\n\u03b2=1\n\nZ\n\n\u221e\n\nS\n\n\u0012q\n\u0013\nv\u03b2 (t)x + \u03bc\u03b2 (t) Dx, \u03b1 = 1, 2.\n\n\u2212\u221e\n\nLet us denote byRG\u03b1 (\u03bc, p\nC(t)) the function in the righthand side of the equality.\n\u221e\nSince S is odd, \u2212\u221e S( v\u03b2 (t)x)Dx = 0. Therefore, we have G\u03b1 (0, C(t)) = 0\nwhatever C(t), and hence the point \u03bc1 = 0, \u03bc2 = 0 is always a fixed point of\nthis equation. Let us study the stability of this fixed point. To this purpose, we\ncompute the partial derivatives of G\u03b1 (\u03bc, C(t)) with respect to \u03bc\u03b2 for (\u03b1, \u03b2) \u2208\n{1, 2}2 . We have:\n\u0012q\n\u0013\u0013\nZ \u221e\u0012\n\u2202G\u03b1\n\u03b4\u03b1\u03b2\n(\u03bc, C(t)) = \u2212\n+ g J \u0304\u03b1\u03b2\n1 \u2212 tanh2\nv\u03b2 (t)x + \u03bc\u03b2 (t)\nDx,\n\u2202\u03bc\u03b2\n\u03c4\u03b1\n\u2212\u221e\n\n18\n\n\fand hence at the point \u03bc1 = 0, \u03bc2 = 0, these derivatives read:\n\u2202G\u03b1\n\u03b4\u03b1\u03b2\n(0, C(t)) = \u2212\n+ g J \u0304\u03b1\u03b2 h(v\u03b2 (t)),\n\u2202\u03bc\u03b2\n\u03c4\u03b1\np\nR\u221e\nwhere h(v\u03b2 (t)) = 1 \u2212 \u2212\u221e tanh2 ( v\u03b2 (t)x)Dx.\nIn the case v\u03b1 (0) = 0, \u03c3 = 0, f\u03b1 = 0, implying v\u03b1 (t) \u2261 0 for t \u2265 0, and the\nequation for \u03bc\u03b1 reduces to:\n2\n\nd\u03bc\u03b1\n\u03bc\u03b1 X  \u0304\n=\u2212\n+\nJ\u03b1\u03b2 S(\u03bc\u03b2 (t))\ndt\n\u03c4\u03b1\n\u03b2=1\n\nwhich is the standard Wilson and Cowan system where Gaussian fluctuations\nare neglected. In this case the stability of the fixed point \u03bc = 0 is given by the\nsign of the largest eigenvalue of the Jacobian matrix of the system that reads:\n\u0013\n\u0012\n\u0012\n\u0013\n0\n\u2212 \u03c411\nJ \u030411 J \u030412\n+\ng\n.\nJ \u030421 J \u030422\n0\n\u2212 \u03c412\nThe eigenvalues are in this case \u2212 \u03c41 + g\u03bb1,2 , where \u03bb1,2 are the eigenvalues of\nJ \u0304 and have the form:\np\nJ \u030411 + J \u030422 \u00b1 (J \u030411 \u2212 J \u030422 )2 + 4J \u030412 J \u030421\n\u03bb1,2 =\n.\n2\nHence, they are complex whenever J \u030412 J \u030421 < \u2212(J \u030411 \u2212 J \u030422 )2 /4, corresponding\nto a negative feedback loop between population 1 and 2. In that case they have\na nonzero real part only if J \u030411 + J \u030422 6= 0 (self interaction). This opens up the\npossibility to have an instability of the fixed point (\u03bc = 0) leading to a regime\nwhere the average value of the membrane potential oscillates. This occurs if\nJ \u030411 + J \u030422 > 0 and if g is larger than:\ngc =\n\n2\n.\n\u03c4 (J \u030411 + J \u030422 )\n\nThe corresponding bifurcation is a Hopf bifurcation.\nThe presence of this bifurcation and these oscillations depends on the presence of fluctuations of the Gaussian field. These oscillations can simply disappear, as we study in depth in a forthcoming paper. Indeed, similarly to the case\nof the pitchfork bifurcation (Figure 1), the covariance of the noise will globally\nhave the effect to delay, or event destroy the Hopf bifurcation, stabilizing the\nfixed point \u03bc = 0.\nWe show here that in a case where the cycle is conserved, the correlation will\ninteract with the mean in an intricate fashion. We observe that the correlations\npresent oscillations four times faster than the mean. The mean asymptotically\noscillates at the same frequency as Wilson and Cowan system, and in opposition\nof phase.\n19\n\n\f5\n\nmu(t), J=0\nmu(t), J=2, n=100\nv(t), J=0\nv(t) x 100, J=2, n=100\n\n4\n3\n2\n1\n0\n-1\n0\n\n2\n\n4\n\n6\n\n8\n\n10\nt\n\n12\n\n14\n\n16\n\n18\n\n20\n\nFigure 3: Evolution of the mean \u03bc1 (t) and variance v1 (t) for the mean-field of\npopulation 1, for J = 0 and J = 2, over a time window [0, 20]. n is the number\nof iterations the map F. This corresponds to a number of iterations for which\nthe method has essentially converged (up to some precision). Note that v1 (t)\nhas been magnified by a factor of 100. Though Gaussian fluctuations are small,\nthey have a strong influence on \u03bc1 (t).\n\n4\n\nConclusion\n\nWe reviewed a recent approach to the mean-field limits in neural networks that\ntakes into account the stochastic nature of input currents at each level as well\nas the uncertainty in synaptic coupling. One of the main advantages of this\napproach is that it can be rigorously derived from first principles using advanced\nmathematical tools in the field of probability, such as large deviation techniques\nand fixed points in the set of stochastic processes. However, in the simpler case\ntreated here, the solutions are Gaussian and therefore described by the mean\nand covariance functions. Equations on the mean and the standard deviation\nare written in the simplest possible way, and a simulation algorithm is provided.\nThese equations are characterized by a strong interplay between the mean\nand the covariance functions, that makes the solutions deviate from the customary Wilson and Cowan system. This coupling between the mean and the\ncovariance was recently investigated independently in mean-field limits of spiking neurons using a Markovian formalism, but the equations involved appear to\nbe simpler, and described by ordinary differential equations [9, 6]. The more\ncomplex integral equations that we obtain here originate from the complex memory effects that arise in mean-field limits; this has already been observed and\ndiscussed in other fields, for instance in the case of spin glasses and in the\nqueueing theory. This phenomenon explains why the equations obtained are\nnot Markovian. The links between the present mean-field equations and the\nmean-field equations studied by Bressloff, Buice, Cowan and colleagues is cur-\n\n20\n\n\frently an important area of investigation. Another perspective is the precise\nstudy of mean-field equations and of how the results depart from Wilson and\nCowan's system. In particular, calculations can be developed in a few particular cases, and this phenomenon provides a first approach to bifurcations\nin stochastic systems, which is a great endeavor in mathematics, physics and\nbiology today.\n\nA\n\nConvergence of the moments of the microscopic interaction process\n\nIn the proof of proposition 2.1, we omitted for the sake of compactness the\ncalculations necessary to identify the covariance of the effective interaction process and its asymptotic independence with respect to the voltage process. The\nsimple yet heavy computations are provided here.\nN\nN\nWe start by computing the covariance between the random variables U\u03b1\u03b2\n(t), U\u03b3,\u03b4\n(s)\n4\nfor t, s > 0 and (\u03b1, \u03b2, \u03b3, \u03b4) \u2208 {1, . . . , P } , assuming that N is big enough so that\nAmari's local chaos hypothesis applies. In that case, we have:\n\u0001\nN\nN\nCov U\u03b1\u03b2\n(t), U\u03b3,\u03b4\n(s)\n\uf8fc (\n\uf8f1\n)!\nN\u03b2\nN\u03b4\n\uf8f2X\n\uf8fd\nX\nJ \u0304\u03b3\u03b4\nJ \u0304\u03b1\u03b2\nm\u03b2 (t) \u00d7\nJl,k S\u03b4 (Vk (t)) \u2212\nm\u03b4 (t)\n=E\nJi,j S\u03b2 (Vj (t)) \u2212\n\uf8fe\n\uf8f3\nN\u03b2\nN\u03b4\nj=1\nk=1\n\uf8fc\n\uf8f1\nN\u03b2\n\uf8fd\n\uf8f2X\nJ \u0304\u03b1\u03b2\nJ \u0304\u03b1\u03b2\n)S\u03b2 (Vj (t)) +\n(S\u03b2 (Vj (t)) \u2212 m\u03b2 (t))\n=E\n(Ji,j \u2212\n\uf8fe\n\uf8f3\nN\u03b2\nN\u03b2\nj=1\n)!\n(N\n\u03b4\nX\nJ \u0304\u03b3\u03b4\nJ \u0304\u03b3\u03b4\n)S\u03b4 (Vk (t)) +\n(S\u03b4 (Vk (t)) \u2212 m\u03b4 (t))\n\u00d7\n(Jl,k \u2212\nN\u03b4\nN\u03b4\nk=1\n\nThis product involves four types of terms. However, because of the independence\nof all involved processes, this sum simply reads:\n\"\n#\n\u0010\n\u0011 J \u03042\n\u03b1\u03b2\n2\n1\u03b1=\u03b3,\u03b2=\u03b4 \u03c3\u03b1\u03b2 E S\u03b2 (V\u03b2 (t))S\u03b2 (V\u03b2 (s)) +\nCov(S\u03b2 (V\u03b2 (t)), S\u03b2 (V\u03b2 (s)))\nN\u03b2\n\u2212\u2212\u2212\u2212\u2192\nN \u2192\u221e\n\n2\n1\u03b1=\u03b3,\u03b2=\u03b4 \u03c3\u03b1\u03b2\n\u2206V\u03b2 (t, s)\n\nWe therefore proved the existence of a limit to the covariance of the processes, which proves the convergence of the microscopic interaction process to\nthe effective interaction process, by application of the functional central limit\ntheorem.\nIn order to prove that the effective interaction process is independent of the\nmembrane potential V , we use the fact that both processes are Gaussian, and\ntherefore that independence is equivalent to a null correlation between the two\n\n21\n\n\fprocesses. Let us consider a neuron i of class \u03b3, \u03b1, \u03b2 \u2208 {1, . . . , P }, and k a\nneuron of population \u03b1. We have:\n\nN\nCov(Vi (t), U\u03b1,\u03b2\n(s)) = Cov(Vi (t),\n\nN\u03b2\nX\n\nJk,j S\u03b2 (Vj (s)))\n\nj=1\n\n=\n\nN\u03b2\nX\n\nE\n\n\u0012\n\nj=1\n\n= 1\u03b1=\u03b3\n\n\u0013\u0013\nJ \u0304\u03b1\u03b2 V\n(Vi (t) \u2212 \u03bci (t)) Jkj S\u03b2 (Vj (s)) \u2212\nm (s)\nN\u03b2 \u03b2\n\u0012\n\nJ \u0304\u03b1\u03b2\nCov(V\u03b1 , S(V\u03b1 )).\nN\u03b2\n\nwhich tends to zero when the number of neurons tends to infinity since the\ncovariance can be easily proved to be bounded.\n\nB\n\nCase of erf firing-rate transformations\n\nIn this appendix we provide the computations leading to simplify the mean-field\nequations when the sigmoidal functions are erf functions Si (x) = erf(gp(i) x +\n\u03b3p(i) ). In that case, the mean equation (13):\n\u0013\n\u0013 \u2212x2 /2\n\u0012\n\u0012 q\ne\ndx\nE(g\u03b1 (X\u03b1 ))(t) = erf gp(i) x v\u03b2 (t) + \u03bc\u03b2 (t) + \u03b3p(i) \u221a\n2\u03c0\nR\n\u0011\n\u0010 \u221a\nZ Z gp(i) x v\u03b2 (t)+\u03bc\u03b2 (t) +\u03b3p(i) \u2212(x2 +y2 )\ne\n=\ndxdx0\n2\u03c0\nR \u2212\u221e\nZ\n\nLet us write this equation with generic boundaries\nZ Z a x+b \u2212(x2 +y2 )/2\ne\ndxdy\n2\u03c0\nR \u2212\u221e\nThe integration domain has therefore an affine shape as plotted in figure 4. We\nmake the change of variables consisting of a rotation of the axis and making\nthe boundary of the domain parallel to one of the new axis (rotating from\n(x, y) to (u, v), see figure 4). The rotation is therefore of angle \u03b1 such that\ntan(\u03b1) = g a. The new integration domain is in the new coordinates given by\nv \u2264 b sin(\u03b1)\n= \u221a g b2 2 :\na\n1+g a\n\nZ Z\n\nR\n\na x+b\n\n\u2212\u221e\n\ne\u2212(x\n\n2\n\n+y 2 )/2\n\n2\u03c0\n\nZ Z\ndxdy =\n\nR\n\n= erf\n\n22\n\n\u221a\n\ngb\n1+g 2 a2\n\n\u2212\u221e\n\ngb\np\n\n2\n\ne\u2212(u\n!\n\n1 + g 2 a2\n\n+v 2 )/2\n\n1\ndudv\n2\u03c0\n\n\fFigure 4: Change of variable for the erf function.\nwhich reads with the parameters of the model:\nmX\n\u03b2 (t)\n\n= erf\n\ngp(i) \u03bc\u03b2 (t) + \u03b3p(i)\np\n1 + g 2 v\u03b2 (t)\n\n!\n(19)\n\nThe correlation function is simplified along the same lines. Indeed, the term\n\u2206X\n\u03b2 (t, s) can be written as:\n\u2206X\n\u03b2 (t, s)\n\nZ\n=\n\nS(a x + b y + c) S(d y + e)DxDy\nZ\n=\nS(d y + e)\nS(a x + b y + c)DxDy\nR\nR\n!\nZ\nby + c\nDy\n=\nS(d y + e)S p\n1 + g 2 a2\nR\nZR\n\n2\n\nwhich, in terms of the model's parameters, read:\n\u2206X\n\u03b2 (t, s)\n\nZ\n=\n\nR\n\nS(\n\nq\n\nv\u03b2 (t)y + \u03bc\u03b2 (t)) S\n\n!\np\nC\u03b2\u03b2 (t, s)y + \u03bc\u03b2 (s) v\u03b2 (t)\nDy\nv\u03b2 (t) + g 2 (v\u03b2 (t)v\u03b2 (s) \u2212 C\u03b2\u03b2 (t, s)2 )\n(20)\n\nReferences\n[1] LF Abbott and C van Vreeswijk. Asynchronous states in networks of pulsecoupled oscillators. Phys Rev E Stat Phys Plasmas Fluids Relat Interdiscip\nTopics, 48(2):1483\u20131490, Aug 1993.\n[2] S. Amari. Characteristics of random nets of analog neuron-like elements.\nSyst. Man Cybernet. SMC-2, 1972.\n[3] Shun-Ichi Amari, Kiyonori Yoshida, and Ken-Ichi Kanatani. A mathematical foundation for statistical neurodynamics. Siam J. Appl. Math.,\n33(1):95\u2013126, 1977.\n23\n\n\f[4] G Ben-Arous and A. Guionnet. Large deviations for Langevin spin glass\ndynamics. Probability Theory and Related Fields, 102(4):455\u2013509, 1995.\n[5] G. Ben-Arous and A. Guionnet. Symmetric Langevin Spin Glass Dynamics.\nThe Annals of Probability, 25(3):1367\u20131422, 1997.\n[6] Paul Bressfloff. Stochastic neural field theory and the system-size expansion. Submitted, 2009.\n[7] N. Brunel. Dynamics of sparsely connected networks of excitatory and\ninhibitory spiking neurons. Journal of Computational Neuroscience, 8:183\u2013\n208, 2000.\n[8] N. Brunel and V. Hakim. Fast global oscillations in networks of integrateand-fire neurons with low firing rates. Neural Computation, 11:1621\u20131671,\n1999.\n[9] M Buice, JD Cowan, and CC Chow. Generalized activity equations for\nneural networks. Neural Comput, 2009.\n[10] D Cai, L Tao, M Shelley, and DW McLaughlin. An effective kinetic representation of fluctuation-driven neuronal networks with application to simple\nand complex cells in visual cortex. Proceedings of the National Academy of\nSciences, 101(20):7757\u20137762, 2004.\n[11] B. Cessac. Increase in complexity in random neural networks. Journal de\nPhysique I (France), 5:409\u2013432, 1995.\n[12] A. Crisanti, HJ. Sommers, and H. Sompolinsky. chaos in neural networks\n: chaotic solutions. 1990.\n[13] P. Dayan and L. F. Abbott. Theoretical Neuroscience : Computational and\nMathematical Modeling of Neural Systems. MIT Press, 2001.\n[14] Alain Destexhe, Zachary F. Mainen, and Terrence J. Sejnowski. Methods in\nNeuronal Modeling, chapter Kinetic models of synaptic transmission, pages\n1\u201325.\n[15] S El Boustani and A Destexhe. A master equation formalism for macroscopic modeling of asynchronous irregular activity states. Neural computation, 21(1):46\u2013100, 2009.\n[16] B Ermentrout. Reduction of conductance-based models with slow synapses\nto neural nets. Neural Computation, 6(4):679\u2013695, 1994.\n[17] Bard Ermentrout. Neural networks as spatio-temporal pattern-forming\nsystems. Reports on Progress in Physics, 61:353\u2013430, 1998.\n[18] G B Ermentrout and N Kopell. Parabolic bursting in an excitable system\ncoupled with a slow oscillation. SIAM J. Appl. Math., 46(2):233\u2013253, 1986.\n\n24\n\n\f[19] O. Faugeras, J. Touboul, and B. Cessac. A constructive mean field analysis\nof multi population neural networks with random synaptic weights and\nstochastic inputs. Frontiers in Neuroscience, 3(1), 2009.\n[20] Philippe Faure and Henri Korn. Is there chaos in the brain ? concept\nof nonlinear dynamics and methods of investigation. Comptes Rendus de\nl'Academie des Sciences Series III Sciences de la Vie, 324(9):773\u2013793, 2001.\n[21] W. Gerstner and W. Kistler. Spiking Neuron Models. Cambridge University\nPress, 2002.\n[22] W. Gerstner and W. M. Kistler. Mathematical formulations of hebbian\nlearning. Biological Cybernetics, 87:404\u2013415, 2002.\n[23] I Ginzburg and H Sompolinsky. Theory of correlations in stochastic neural\nnetworks. Physical review E, 50(4):3171\u20133191, 1994.\n[24] A. Guionnet. Averaged and quenched propagation of chaos for spin glass\ndynamics. Probability Theory and Related Fields, 109(2):183\u2013215, 1997.\n[25] J. J. Hopfield. Neurons with graded response have collective computational\nproperties like those of two-state neurons. Proceedings of the National\nAcademy of Sciences, USA, 81(10):3088\u20133092, 1984.\n[26] H Korn and P Faure. Is there chaos in the brain? ii. experimental evidence\nand related models. Comptes rendus-Biologies, 326(9):787\u2013840, 2003.\n[27] M. Mattia and P. Del Giudice. Population dynamics of interacting spiking\nneurons. Physical Review E, 66(5):51917, 2002.\n[28] C Meyer and C van Vreeswijk. Temporal correlations in stochastic networks\nof spiking neurons. Neural computation, 14(2):369\u2013404, 2002.\n[29] DQ Nykamp and D Tranchina. A population density approach that facilitates large-scale modeling of neural networks: Analysis and an application\nto orientation tuning. Journal of Computational Neuroscience, 8(1):19\u201350,\n2000.\n[30] A Omurtag, BW Knight, and L Sirovich. On the simulation of large populations of neurons. Journal of Computational Neuroscience, 8(1):51\u201363,\n2000.\n[31] D. Pollard. Empirical Processes: Theory and Applications. Ims, 1990.\n[32] AV Rangan, G Kova i, and D Cai. Kinetic theory for neuronal networks\nwith fast and slow excitatory conductances driven by the same spike train.\nPhysical Review E, 77(4), 2008.\n[33] J. Rinzel and B. Ermentrout. Analysis of neural excitability and oscillations. MIT Press, 1989.\n\n25\n\n\f[34] J Rinzel and P Frankel. Activity patterns of a slow synapse network\npredicted by explicitly averaging spike dynamics. Neural Computation,\n4(4):534\u2013545, 1992.\n[35] M. Samuelides and B. Cessac. Random recurrent neural networks. European\nPhysical Journal - Special Topics, 142:7\u201388, 2007.\n[36] M. N. Shadlen and W. T. Newsome. Noise, neural codes and cortical\norganization. Curr Opin Neurobiol, 4(4):569\u2013579, August 1994.\n[37] William R. Softky and Christof Koch. The highly irregular firing of cortical\ncells is inconsistent with temporal integration of random epsps. Journal of\nNeuroscience, 13:334\u2013350, 1993.\n[38] H. Sompolinsky, A. Crisanti, and HJ Sommers. Chaos in Random Neural\nNetworks. Physical Review Letters, 61(3):259\u2013262, 1988.\n[39] Jonathan Touboul and G Bard Ermentrout. Finite-size and correlationinduced effects in mean-field dynamics finite-size and correlation-induced\neffects in mean-field dynamics. (in preparation).\n[40] Jonathan Touboul and Olivier Faugeras. The spikes trains probability distributions: a stochastic calculus approach. Journal of Physiology, Paris,\n101/1-3:78\u201398, dec 2007.\n[41] H.R. Wilson and J.D. Cowan. Excitatory and inhibitory interactions in\nlocalized populations of model neurons. Biophys. J., 12:1\u201324, 1972.\n[42] H.R. Wilson and J.D. Cowan. A mathematical theory of the functional\ndynamics of cortical and thalamic nervous tissue. Biological Cybernetics,\n13(2):55\u201380, sep 1973.\n\n26\n\n\f"}