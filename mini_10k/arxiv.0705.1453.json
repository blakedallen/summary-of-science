{"id": "http://arxiv.org/abs/0705.1453v1", "guidislink": true, "updated": "2007-05-10T12:23:35Z", "updated_parsed": [2007, 5, 10, 12, 23, 35, 3, 130, 0], "published": "2007-05-10T12:23:35Z", "published_parsed": [2007, 5, 10, 12, 23, 35, 3, 130, 0], "title": "DWEB: A Data Warehouse Engineering Benchmark", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0705.2666%2C0705.0798%2C0705.4665%2C0705.1238%2C0705.3583%2C0705.1016%2C0705.0873%2C0705.3616%2C0705.4170%2C0705.2472%2C0705.2860%2C0705.2989%2C0705.1453%2C0705.1761%2C0705.4312%2C0705.0670%2C0705.2767%2C0705.0129%2C0705.2833%2C0705.2885%2C0705.0925%2C0705.1677%2C0705.1156%2C0705.2115%2C0705.1484%2C0705.3987%2C0705.3613%2C0705.3831%2C0705.3598%2C0705.1008%2C0705.1506%2C0705.0784%2C0705.4358%2C0705.3776%2C0705.1637%2C0705.1879%2C0705.1979%2C0705.3877%2C0705.2712%2C0705.2042%2C0705.1968%2C0705.2280%2C0705.3400%2C0705.2492%2C0705.4298%2C0705.2340%2C0705.1102%2C0705.0029%2C0705.0870%2C0705.1878%2C0705.3662%2C0705.0263%2C0705.3425%2C0705.1991%2C0705.2181%2C0705.4567%2C0705.4220%2C0705.2987%2C0705.4473%2C0705.3353%2C0705.1519%2C0705.1109%2C0705.1584%2C0705.4054%2C0705.0409%2C0705.0010%2C0705.2099%2C0705.3528%2C0705.4428%2C0705.1105%2C0705.1110%2C0705.4485%2C0705.1054%2C0705.4397%2C0705.0952%2C0705.0101%2C0705.4437%2C0705.1667%2C0705.2970%2C0705.3585%2C0705.1561%2C0705.4603%2C0705.4450%2C0705.3214%2C0705.0797%2C0705.0309%2C0705.3994%2C0705.0761%2C0705.4071%2C0705.0661%2C0705.0411%2C0705.3553%2C0705.3724%2C0705.1483%2C0705.0131%2C0705.2616%2C0705.3155%2C0705.0158%2C0705.1284%2C0705.2160%2C0705.3602&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "DWEB: A Data Warehouse Engineering Benchmark"}, "summary": "Data warehouse architectural choices and optimization techniques are critical\nto decision support query performance. To facilitate these choices, the\nperformance of the designed data warehouse must be assessed. This is usually\ndone with the help of benchmarks, which can either help system users comparing\nthe performances of different systems, or help system engineers testing the\neffect of various design choices. While the TPC standard decision support\nbenchmarks address the first point, they are not tuneable enough to address the\nsecond one and fail to model different data warehouse schemas. By contrast, our\nData Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc\nsynthetic data warehouses and workloads. DWEB is fully parameterized to fulfill\ndata warehouse design needs. However, two levels of parameterization keep it\nrelatively easy to tune. Finally, DWEB is implemented as a Java free software\nthat can be interfaced with most existing relational database management\nsystems. A sample usage of DWEB is also provided in this paper.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0705.2666%2C0705.0798%2C0705.4665%2C0705.1238%2C0705.3583%2C0705.1016%2C0705.0873%2C0705.3616%2C0705.4170%2C0705.2472%2C0705.2860%2C0705.2989%2C0705.1453%2C0705.1761%2C0705.4312%2C0705.0670%2C0705.2767%2C0705.0129%2C0705.2833%2C0705.2885%2C0705.0925%2C0705.1677%2C0705.1156%2C0705.2115%2C0705.1484%2C0705.3987%2C0705.3613%2C0705.3831%2C0705.3598%2C0705.1008%2C0705.1506%2C0705.0784%2C0705.4358%2C0705.3776%2C0705.1637%2C0705.1879%2C0705.1979%2C0705.3877%2C0705.2712%2C0705.2042%2C0705.1968%2C0705.2280%2C0705.3400%2C0705.2492%2C0705.4298%2C0705.2340%2C0705.1102%2C0705.0029%2C0705.0870%2C0705.1878%2C0705.3662%2C0705.0263%2C0705.3425%2C0705.1991%2C0705.2181%2C0705.4567%2C0705.4220%2C0705.2987%2C0705.4473%2C0705.3353%2C0705.1519%2C0705.1109%2C0705.1584%2C0705.4054%2C0705.0409%2C0705.0010%2C0705.2099%2C0705.3528%2C0705.4428%2C0705.1105%2C0705.1110%2C0705.4485%2C0705.1054%2C0705.4397%2C0705.0952%2C0705.0101%2C0705.4437%2C0705.1667%2C0705.2970%2C0705.3585%2C0705.1561%2C0705.4603%2C0705.4450%2C0705.3214%2C0705.0797%2C0705.0309%2C0705.3994%2C0705.0761%2C0705.4071%2C0705.0661%2C0705.0411%2C0705.3553%2C0705.3724%2C0705.1483%2C0705.0131%2C0705.2616%2C0705.3155%2C0705.0158%2C0705.1284%2C0705.2160%2C0705.3602&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Data warehouse architectural choices and optimization techniques are critical\nto decision support query performance. To facilitate these choices, the\nperformance of the designed data warehouse must be assessed. This is usually\ndone with the help of benchmarks, which can either help system users comparing\nthe performances of different systems, or help system engineers testing the\neffect of various design choices. While the TPC standard decision support\nbenchmarks address the first point, they are not tuneable enough to address the\nsecond one and fail to model different data warehouse schemas. By contrast, our\nData Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc\nsynthetic data warehouses and workloads. DWEB is fully parameterized to fulfill\ndata warehouse design needs. However, two levels of parameterization keep it\nrelatively easy to tune. Finally, DWEB is implemented as a Java free software\nthat can be interfaced with most existing relational database management\nsystems. A sample usage of DWEB is also provided in this paper."}, "authors": ["J\u00e9r\u00f4me Darmont", "Fadila Bentayeb", "Omar Boussa\u00efd"], "author_detail": {"name": "Omar Boussa\u00efd"}, "author": "Omar Boussa\u00efd", "links": [{"href": "http://arxiv.org/abs/0705.1453v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0705.1453v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0705.1453v1", "affiliation": "ERIC", "arxiv_url": "http://arxiv.org/abs/0705.1453v1", "arxiv_comment": null, "journal_reference": "LNCS, Vol. 3589 (08/2005) 85-94", "doi": null, "fulltext": "DWEB: A Data Warehouse\nEngineering Benchmark\n\narXiv:0705.1453v1 [cs.DB] 10 May 2007\n\nJ\u00e9r\u00f4me Darmont, Fadila Bentayeb, and Omar Boussa\u0131\u0308d\nERIC, University of Lyon 2\n5 av. Pierre Mend\u00e8s-France\n69676 Bron Cedex\nFrance\n{jdarmont|boussaid|bentayeb}@eric.univ-lyon2.fr\n\nAbstract. Data warehouse architectural choices and optimization techniques are critical to decision support query performance. To facilitate\nthese choices, the performance of the designed data warehouse must be\nassessed. This is usually done with the help of benchmarks, which can either help system users comparing the performances of different systems,\nor help system engineers testing the effect of various design choices. While\nthe TPC standard decision support benchmarks address the first point,\nthey are not tuneable enough to address the second one and fail to model\ndifferent data warehouse schemas. By contrast, our Data Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses and workloads. DWEB is fully parameterized to\nfulfill data warehouse design needs. However, two levels of parameterization keep it relatively easy to tune. Finally, DWEB is implemented\nas a Java free software that can be interfaced with most existing relational database management systems. A sample usage of DWEB is also\nprovided in this paper.\n\n1\n\nIntroduction\n\nWhen designing a data warehouse, choosing an architecture is crucial. Since it\nis very dependant on the domain of application and the analysis objectives that\nare selected for decision support, different solutions are possible. In the ROLAP\n(Relational OLAP) environment we consider, the most popular solutions are by\nfar star, snowflake, and constellation schemas [7, 9], and other modeling possibilities might exist. This choice of architecture is not neutral: it always has advantages and drawbacks and greatly influences the response time of decision support\nqueries. Once the architecture is selected, various optimization techniques such\nas indexing or materialized views further influence querying and refreshing performance. Again, it is a matter of trade-off between the improvement brought by\na given technique and its overhead in terms of maintenance time and additional\ndisk space; and also between different optimization techniques that may cohabit.\nTo help users make these critical choices of architecture and optimization techniques, the performance of the designed data warehouse needs to be assessed.\n\n\fHowever, evaluating data warehousing and decision support technologies is an\nintricate task. Though pertinent, general advice is available, notably on-line [5,\n12], more quantitative elements regarding sheer performance are scarce. Thus, we\npropose in this paper a data warehouse benchmark we named DWEB (the Data\nWarehouse Engineering Benchmark ). Different goals may be achieved by using\na benchmark: (1) compare the performances of various systems in a given set of\nexperimental conditions (users); (2) evaluate the impact of architectural choices\nor optimisation techniques on the performances of one given system (system designers). The Transaction Processing Performance Council (TPC), a non-profit\norganization, defines standard benchmarks and publishes objective and verifiable\nperformance evaluations to the industry. Out of the TPC, few decision support\nbenchmarks have been designed. Some do exist, but their specification is not\nfully published [3]. Some others are not available any more, such as the OLAP\nAPB-1 benchmark that was issued in the late nineties by the OLAP council, an\norganization whose web site does not exist any more.\nThe TPC benchmarks mainly aim at the first benchmarking goal we identified. However, the database schema of TPC benchmarks TPC-H [15] and TPCR [16] is a classical product-order-supplier model, and not a typical data warehouse schema such as a star schema and its derivatives. Furthermore, their workload, though decision-oriented, does not include explicit OLAP (On-Line Analytical Processing) queries either, and they do not address specific warehousing\nissues such as the ETL (Extract, Transform, Load) process. These benchmarks\nare indeed implicitely considered obsolete by the TPC that has issued some specifications for their successor: TPC-DS [13]. However, TPC-DS has been under\ndevelopment for three years now and is not completed yet. Furthermore, although the TPC decision support benchmarks are scaleable according to Gray's\ndefinition [4], their schema is fixed. It must be used \"as is\". Different ad-hoc configurations are not possible. There is only one parameter to define the database,\nthe Scale Factor (SF ), which sets up its size (from 1 to 100,000 GB). The user\ncannot control the size of the dimensions and the fact tables separately, for instance. Finally, the user has no control on the workload's definition. The TPC\nbenchmarks are thus not well adapted to evaluate the impact of architectural\nchoices or optimisation techniques on global performance. For these reasons, we\ndecided to design a full data warehouse synthetic benchmark that would be able\nto model various ad-hoc configurations of database (modeled as star, snowflake,\nor constellation schemas) and workload, while being simpler to develop than\nTPC-DS. We mainly seek to fulfill engineering needs (second benchmarking objective).\nThis paper presents an overview the DWEB benchmark. First, we present\nour benchmark's database (metaschema, parameterization, and instiantiation\ninto an actual data warehouse) in Section 2. Then, we present the benchmark's\nworkload (query model, parameterization, and workload generation) in Section 3.\nWe illustrate how our benchmark can be used in Section 4 and finally conclude\nthis paper and provide future research directions in Section 5.\n\n\f2\n2.1\n\nDWEB database\nSchema\n\nOur design objective for DWEB is to be able to model the different kinds of data\nwarehouse architectures that are popular within a ROLAP environment: classical\nstar schemas; snowflake schemas with hierarchical dimensions; and constellation\nschemas with multiple fact tables and shared dimensions. To achieve this goal,\nwe propose a data warehouse metamodel (represented as a UML class diagram\nin Figure 1) that can be instantiated into these different schemas. We view this\nmetamodel as a middle ground between the multidimensional metamodel from\nthe Common Warehouse Metamodel (CWM [11]) and the eventual benchmark\nmodel. Our metamodel is actually an instance of the CWM metamodel, which\ncould be qualified as a meta-metamodel in our context.\n\nFig. 1. DWEB data warehouse metaschema\n\nOur metamodel is relatively simple, but it is sufficient to model the data\nwarehouse schemas we aim at (star, snowflake, and constellation schemas). Its\nupper part describes a data warehouse (or a datamart, if a datamart is viewed\nas a small, dedicated data warehouse) as constituted of one or several fact tables\nthat are each described by several dimensions. Each dimension may also describe\nseveral fact tables (shared dimensions). Each dimension may be constituted of\none or several hierarchies made of different levels. There can be only one level if\nthe dimension is not a hierarchy. Both fact tables and dimension hierarchy levels\nare relational tables, which are modeled in the lower part of Figure 1. Classically,\n\n\fa table or relation is defined in intention by its attributes and in extension by\nits tuples or rows. At the intersection of a given attribute and a given tuple lies\nthe value of this attribute in this tuple.\n2.2\n\nParameterization\n\nDWEB's database parameters help users selecting the data warehouse architecture they need in a given context. The main difficulty in producing a data\nwarehouse schema is parameterizing the instantiation of the metaschema. We\nindeed try to meet the four key criteria that make a \"good\" benchmark, as defined by Gray [4]: relevance: the benchmark must answer our engineering needs\nportability: the benchmark must be easy to implement on different systems; scalability: it must be possible to benchmark small and large databases, and to scale\nup the benchmark; and simplicity: the benchmark must be understandable, otherwise it will not be credible nor used. Relevance and simplicity are clearly two\northogonal goals. Introducing too few parameters reduces the model's expressiveness, while introducing too many parameters makes it difficult to apprehend\nby potential users. Furthermore, few of these parameters are likely to be used in\npractice. In parallel, the generation complexity of the instantiated schema must\nbe mastered. To solve this dilemna, we propose to divide the parameter set into\ntwo subsets. The first subset of so-called low-level parameters allows an advanced\nuser to control everything about the data warehouse generation. However, the\nnumber of low-level parameters can increase dramatically when the schema gets\nlarger. For instance, if there are several fact tables, all their characteristics, including dimensions and their own characteristics, must be defined for each fact\ntable. Thus, we designed a layer above with much fewer parameters that may\nbe easily understood and set up (Table 1). More precisely, these high-level parameters are average values for the low-level parameters. At database generation\ntime, the high-level parameters are exploited by random functions (following a\ngaussian distribution) to automatically set up the low-level parameters. Finally,\nunlike the number of low-level parameters, the number of high-level parameters\nalways remains constant and reasonable (less than ten parameters). Users may\nchoose to set up either the full set of low-level parameters, or only the high-level\nparameters, for which we propose default values that correspond to a snowflake\nschema. These parameters control both schema and data generation.\nNote that the cardinal of a fact table is usually lower or equal to the product of its dimensions' cardinals. This is why we introduce the notion of density. A density rate of one indicates that all the possible combinations of the\ndimension primary keys are present in the fact table. When the density rate\ndecreases, we progressively eliminate some of these combinations.This parameter helps controlling the size of the fact table, independantly of the size of its\ndimensions.Furthermore, within a dimension, a given hierarchy level normally\nhas a greater cardinality than the next level. For example, in a town-regioncountry hierarchy, the number of towns must be greater than the number of\nregions, which must be in turn greater than the number of countries. There is\nalso often a significant scale factor between these cardinalities (e.g., one thousand\n\n\fParameter name\nMeaning\nDef. val.\nAV G N B F T\nAverage number of fact tables\n1\nAV G N B DIM\nAverage number of dimensions per fact table\n5\nAV G T OT N B DIM\nAverage total number of dimensions\n5\nAV G N B M EAS\nAverage number of measures in fact tables\n5\nAV G DEN SIT Y\nAverage density rate in fact tables\n0.6\nAV G N B LEV ELS\nAverage number of hierarchy levels in dimensions\n3\nAV G N B AT T\nAverage number of attributes in hierarchy levels\n5\nAV G HHLEV EL SIZE Average number of tuples in highest hierarchy levels\n10\nDIM SF ACT OR\nAverage size scale factor within hierarchy levels\n10\nTable 1. DWEB warehouse high-level parameters\n\ntowns, one hundred regions, ten countries). Hence, we model the cardinality of\nhierarchy levels by assigning a \"starting\" cardinality to the highest level in the\nhierarchy (HHLEV EL SIZE), and then by multiplying it by a predefined scale\nfactor (DIM SF ACT OR) for each lower-level hierarchy. Finally, since some of\nDWEB's parameters might sound abstract, the data warehouse global size (in\nmegabytes) is assessed at generation time so that users retain full control over it\nand may adjust the parameters to better represent the kind of warehouse they\nneed.\n2.3\n\nGeneration algorithm\n\nThe instantiation of the DWEB metaschema into an actual benchmark schema\nis done in two steps: (1) build the dimensions; (2) build the fact tables. Due\nto space constraints, the pseudo-code for these two steps is not provided here,\nbut it is available on-line [2]. Each of these steps is further subdivided, for each\ndimension and each fact table, into generating its intention and extension. In\naddition, hierarchies of dimensions are managed.\n\n3\n\nDWEB workload\n\nIn a data warehouse benchmark, the workload may be subdivided into a load\nof decision support queries (mostly OLAP queries) and the ETL (data generation and maintenance) process. To design DWEB's workload, we inspire both\nfrom TPC-DS' workload definition and information regarding data warehouse\nperformance from other sources [1, 6]. However, TPC-DS' workload is very elaborate and sometimes confusing. Its reporting, ad-hoc decision support and OLAP\nquery classes are very similar, for instance, but none of them include any specific\nOLAP operator such as Cube or Rollup. Since we want to meet Gray's simplicity\ncriterion, we propose a simpler workload. Furthermore, we also have to design\na workload that is consistent with the variable nature of the DWEB data warehouses. We also, in a first step, mainly focus on the definition of a query model.\nModeling the full ETL process is a complex task that we postpone for now. We\n\n\fconsider that the current DWEB specifications provide a raw loading evaluation\nframework. The DWEB database may indeed be generated into flat files, and\nthen loaded into a data warehouse using the ETL tools provided by the system.\n3.1\n\nQuery model\n\nThe DWEB workload models two different classes of queries: purely decisionoriented queries involving common OLAP operations, such as cube, roll-up, drill\ndown and slice and dice; and extraction queries (simple join queries). We define\nour generic query model as a grammar that is a subset of the SQL-99 standard.\nDue to space constraints, this query model is only available on-line [2].\n3.2\n\nParameterization\n\nDWEB's workload parameters help users tailoring the benchmark's load, which\nis also dependent from the warehouse schema, to their needs. Just like DWEB's\ndatabase paramameter set, DWEB's workload parameter set (Table 2) has been\ndesigned with Gray's simplicity criterion in mind. These parameters determine\nhow the query model is instantiated. These parameters help defining the workload's size and complexity, by setting up the proportion of complex OLAP queries\n(i.e., the class of queries) in the workload, the number of aggregation operations,\nthe presence of a Having clause in the query, or the number of subsequent drill\ndown operations. Here, we have only a limited number of high-level parameters\nIndeed, it cannot be envisaged to dive further into detail if the workload is as\nlarge as several hundred queries, which is quite typical. Note that N B Q is only\nan approximate number of queries because the number of drill down operations\nafter an OLAP query may vary. Hence we can stop generating queries only when\nwe actually have generated as many or more queries than N B Q.\n\nParameter name Meaning\nDef. val.\nNB Q\nApproximate number of queries in the workload\n100\nAV G N B AT T\nAverage number of selected attributes in a query\n5\nAV G N B REST R Average number of restrictions in the query\n3\nP ROB OLAP\nProbability that the query type is OLAP\n0.9\nP ROB EXT RACT Probability that the query is an extraction query\n1 \u2212 P OLAP\nAV G N B AGGREG Average number of aggregations in an OLAP query\n3\nP ROB CU BE\nProbability of an OLAP query to use the Cube operator\n0.3\nP ROB ROLLU P\nProbability of an OLAP query to use the Rollup operator 1 \u2212 P CU BE\nP ROB HAV IN G\nProbability of an OLAP query to include an Having clause\n0.2\nAV G N B DD\nAverage number of drill downs after an OLAP query\n3\nTable 2. DWEB workload parameters\n\n\f3.3\n\nGeneration algorithm\n\nDue to space constraints, the pseudo-code of DWEB's workload generation algorithm is only available on-line [2]. However, its principle follows. The algorithm's\npurpose is to generate a set of SQL-99 queries that can be directly executed on\nthe synthetic data warehouse defined in Section 2. It is subdivided into two steps:\n(1) generate an initial query that may either be an OLAP or an extraction (join)\nquery; (2) if the initial query is an OLAP query, execute a certain number of\ndrill down operations based on the first OLAP query. More precisely, each time\na drill down is performed, an attribute from a lower level of dimension hierarchy\nis added to the attribute clause of the previous query. Step 1 is further subdivided into three substeps: (1) the Select, From, and Where clauses of a query\nare generated simultaneously by randomly selecting a fact table and dimensions,\nincluding a hierarchy level within a given dimension hierarchy; (2) the Where\nclause is supplemented with additional conditions; (3) eventually, it is decided\nwhether the query is an OLAP query or an extraction query. In the second case,\nthe query is complete. In the first case, aggregate functions applied to measures\nof the fact table are added in the query, as well as a Group by clause that may\ninclude either the Cube or the Rollup operator. A Having clause may optionally\nbe added in too. The aggregate function we apply on measures is always Sum\nsince it is the most common aggregate in cubes. Furthermore, other aggregate\nfunctions bear similar time complexities, so they would not bring in any more\ninsight in a performance study.\n\n4\n\nSample usage of DWEB\n\nIn order to illustrate one possible usage for DWEB, we tested the efficiency of\nbitmap join indices, which are well suited to the data warehouse environment,\non decision support queries under Oracle. The aim of this particular example is\nto compare the execution time of a given workload on a given data warehouse,\nwith and without using bitmap join indices.\nFirst, we generated a data warehouse modeled as a snowflake schema. This\nschema is organized around one fact table that is described by five dimensions,\neach bearing two to three hierarchy levels.The fact table contains about 140,000\ntuples, the dimension hierarchy levels about ten tuples on an average, for a global\nsize of about 4 MB (this is a voluntarily small example and not a full-scale test).\nWe applied different workloads on this data warehouse. W orkload#1 is a typical\nDWEB workload constituted of fifty queries.10% of these queries are extraction\n(join) queries and the rest are decision support queries involving OLAP operators\n(Cube and Rollup). In W orkload#1, we limited the queries to the dimensions'\nlowest hierarchy levels, i.e., to the star schema constituted of the fact table and\nthe \"closest\" hierarchy levels.W orkload#2 is similar to W orkload#1, but it is\nextended with drill down operations that scan the dimensions' full hierarchies\n(from the highest level to the lowest level). Thus, this workload exploits the whole\nsnowflake schema. To evaluate the efficiency of bitmap join indices, we timed the\nexecution of these two workloads on our test data warehouse (response time is our\n\n\fonly performance metric for now), first with no index, and then by forcing the use\nof five bitmap join indices defined on the five dimensions (for the lowest hierarchy\nlevels in W orkload#1 and for the whole hierarchies in W orkload#2). To flaten\nany response time variation in these experiments, we replicated each test ten\ntimes and computed the average response times. We made sure a posteriori that\nthe standard deviation was close to zero. These tests have been executed on a\nPC with a Celeron 900 processor, 128 MB of RAM, an IDE hard drive, and\nrunning Windows XP Professional and Oracle 9i.\nThe left-hand graph on Figure 2 represents the average response time achieved\nfor W orkload#1 and #2, with and without bitmap join indices, respectively. It\nshows a gain in performance of 15% for W orkload#1, and 9.4% for W orkload#2.\nThis decrease in efficiency was expected, since the drill down operations added\nin W orkload#2 are costly and need to access the data (bitmap join indices alone\ncannot answer such queries). However, the overall performance improvement we\nachieved was not as good as we expected. We formulated the hypothesis that the\nextraction queries, which are costly joins and need to access the data too, were\nnot fully benefiting from the bitmap join indices. To confirm this hypothesis, we\ngenerated two new workloads, W orkload #3 and #4. They are actually almost\nidentical to W orkload#1 and W orkload#2, respectively, but do not include any\nextraction (join) queries. Then, we repeated our experiment following the same\nprotocol. The right-hand graph on Figure 2 represents the average response time\nachieved for W orkload#3 and #4, with and without bitmap join indices, respectively. This time, we obtained similar results than in our previous experiment (in\ntrend): response time clearly increases when drill down operations are included\ninto the workload. However, response time is now much better and the gain in\nperformance is 30.9% for W orkload#3, and 19.2% for W orkload#4.\n\n10:05\n\n02:53\n02:36\n\n08:38\n\nResponse time (min)\n\nResponse time (min)\n\n02:18\n07:12\n05:46\n04:19\n02:53\n\n02:01\n01:44\n01:26\n01:09\n00:52\n00:35\n\n01:26\n\n00:17\n\n00:00\n\n00:00\nWorkload #1\nWithout indices\n\nWorkload #2\nWith bitmap join indices\n\nWorkload #3\nWithout indices\n\nWorkload #4\nWith bitmap join indices\n\nFig. 2. Test results\n\nIn conclusion, we want to point out that these experiments are not very significant per se, and do not do justice to Oracle. However, we illustrated how\nDWEB could be used for performance evaluation purposes. These experiments\n\n\fcould also be seen as a (very basic) performance comparison between two different data warehouse architectures (star schema and snowflake schema). Our\nresults indeed conform to the well-known fact that introducing hierarchies into\na star schema induces more join operations in the decision support queries, and\nhence degrade their response time. Finally, we were also able to witness the impact of costly join operations on a data warehouse structure that is not properly\nindexed to answer such queries.\n\n5\n\nConclusion and perspectives\n\nWe proposed in this paper a new data warehouse benchmark called DWEB (the\nData Warehouse Engineering Benchmark ) that is aimed at helping data warehouse designers to choose between alternate warehouse architectures and performance optimization techniques. When designing DWEB, we tried to grant it the\ncharacteristics that make up a \"good\" benchmark according to Gray: relevance,\nportability, scalability, and simplicity. To make DWEB relevant for evaluating\nthe performance of data warehouses in an engineering context, we designed it to\ngenerate different data warehouse schemas (namely star, snowflake and constellation schemas) and workloads. Note that the database schema of TPC-DS, the\nfuture standard data warehouse benchmark currently developped by the TPC,\ncan be modeled with DWEB. In addition, though DWEB's workload is not currently as elaborate as TPC-DS's, it is also much easier to implement. It will\nbe important to fully include the ETL process into our workload, though, and\nthe specifications of TPCD-DS and some other existing studies [10] might help\nus. We now need to further test DWEB's relevance on real cases. To achieve\nthis goal, we plan to compare the efficiency of various index and materialized\nview selection techniques.We also made DWEB very tuneable to reach both the\nrelevance and scalability objectives. However, too many parameters make the\nbenchmark complex to use and contradict the simplicity requirement. Though it\nis impossible to achieve both a high simplicity and a high relevance and scalability, we introduced a layer of high-level parameters that are both simpler than the\npotentially numerous low-level parameters, and in reduced and constant number. DWEB might not be qualified as a simple benchmark, but our objective\nwas to keep its complexity as low as possible. Finally, portability was achieved\nthrough a Java implementation. DWEB's latest version is freely available online [8]. Finally, we also illustrated with a practical case how DWEB can be\nused.\nThis work opens up many perspectives for developing and enhancing DWEB.\nIn this paper, we assumed an execution protocol and performance metrics were\neasy to define for DWEB (e.g., using TPC-DS' as a base) and focused on the\nbenchmark's database and workload model. A more elaborate execution protocol must be designed, especially since two executions of DWEB using the same\nparameters produce different data warehouses and workloads. This is interesting\nwhen, for instance, one optimization technique needs to be tested against many\ndatabases. However, note that it is also possible to save a given warehouse and\n\n\fits associated workload to run tests on different systems and/or with various optimization techniques. Defining sound metrics (beside response time) would also\nimprove DWEB's usefulness. In this area, we could inspire from metrics designed\nto measure the quality of data warehouse conceptual models [14]. We are also\ncurrently working on warehousing complex, non-standard data (including multimedia data, for instance). Such data may be stored as XML documents. Thus, we\nalso plan a \"complex data\" extension of DWEB that would take into account the\nadvances in XML warehousing. Finally, more experiments with DWEB should\nalso help us propose sounder default parameter values. We also encourage other\npeople to report on their own experiments.\n\nReferences\n1. BMC Software.\nPerformance Management of a Data Warehouse.\nhttp://www.bmc.com, 2000.\n2. J. Darmont, F. Bentayeb, and O. Boussa\u0131\u0308d. The Design of DWEB. Technical report, ERIC, University of Lyon 2, France, June 2005. http://eric.univlyon2.fr/\u223cjdarmont/publications/files/dweb.pdf.\n3. M. Demarest. A Data Warehouse Evaluation Model. Oracle Technical Journal,\n1(1):29, October 1995.\n4. J. Gray. The Benchmark Handbook for Database and Transaction Processing Systems. Morgan Kaufmann, second edition, 1993.\n5. L. Greenfield.\nPerforming Data Warehouse Software Evaluations.\nhttp://www.dwinfocenter.org/evals.html, 2004.\n6. L. Greenfield. What to Learn About in Order to Speed Up Data Warehouse\nQuerying. http://www.dwinfocenter.org/fstquery.html, 2004.\n7. W. Inmon. Building the Data Warehouse. John Wiley & Sons, third edition, 2002.\n8. B. Joubert and S. Guesmoa. DWEB Java prototype v0.31. http://bdd.univlyon2.fr/download/dweb.tgz, 2005.\n9. R. Kimball and M. Ross. The Data Warehouse Toolkit: The Complete Guide to\nDimensional Modeling. John Wiley & Sons, second edition, 2002.\n10. A. Labrinidis and N. Roussopoulos. A performance evaluation of online warehouse\nupdate algorithms. Technical Report CS-TR-3954, Deptartment of Computer Science, University of Maryland, November 1998.\n11. Object Management Group. Common Warehouse Metamodel (CWM) Specification\nversion 1.1, March 2003.\n12. N. Pendse.\nThe OLAP Report: How not to buy an OLAP product.\nhttp://www.olapreport.com/How not to buy.htm, December 2003.\n13. M. Poess, B. Smith, L. Kollar, and P.-A. Larson. TPC-DS: Taking Decision Support\nBenchmarking to the Next Level. In ACM SIGMOD 2002, Madison, USA, June\n2002.\n14. M. Serrano, C. Calero, J. Trujillo, S. Luj\u00e1n-Mora, and M. Piattini. Empirical\nValidation of Metrics for Conceptual Models of Data Warehouses. In 16th International Conference on Advanced Information Systems Engineering (CAiSE 04),\nRiga, Latvia, volume 3084 of LNCS, pages 506\u2013520, 2004.\n15. Transaction Processing Performance Council. TPC Benchmark H Standard Specification version 2.1.0, August 2003.\n16. Transaction Processing Performance Council. TPC Benchmark R Standard Specification version 2.1.0, August 2003.\n\n\f"}