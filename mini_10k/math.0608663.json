{"id": "http://arxiv.org/abs/math/0608663v1", "guidislink": true, "updated": "2006-08-27T09:03:27Z", "updated_parsed": [2006, 8, 27, 9, 3, 27, 6, 239, 0], "published": "2006-08-27T09:03:27Z", "published_parsed": [2006, 8, 27, 9, 3, 27, 6, 239, 0], "title": "Estimating the intensity of a random measure by histogram type\n  estimators", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0608658%2Cmath%2F0608762%2Cmath%2F0608707%2Cmath%2F0608624%2Cmath%2F0608508%2Cmath%2F0608014%2Cmath%2F0608490%2Cmath%2F0608535%2Cmath%2F0608647%2Cmath%2F0608596%2Cmath%2F0608428%2Cmath%2F0608213%2Cmath%2F0608156%2Cmath%2F0608459%2Cmath%2F0608488%2Cmath%2F0608289%2Cmath%2F0608772%2Cmath%2F0608627%2Cmath%2F0608180%2Cmath%2F0608633%2Cmath%2F0608127%2Cmath%2F0608266%2Cmath%2F0608124%2Cmath%2F0608001%2Cmath%2F0608743%2Cmath%2F0608010%2Cmath%2F0608072%2Cmath%2F0608560%2Cmath%2F0608691%2Cmath%2F0608519%2Cmath%2F0608685%2Cmath%2F0608511%2Cmath%2F0608174%2Cmath%2F0608787%2Cmath%2F0608625%2Cmath%2F0608736%2Cmath%2F0608779%2Cmath%2F0608502%2Cmath%2F0608188%2Cmath%2F0608515%2Cmath%2F0608230%2Cmath%2F0608357%2Cmath%2F0608760%2Cmath%2F0608566%2Cmath%2F0608090%2Cmath%2F0608404%2Cmath%2F0608715%2Cmath%2F0608551%2Cmath%2F0608405%2Cmath%2F0608698%2Cmath%2F0608640%2Cmath%2F0608036%2Cmath%2F0608240%2Cmath%2F0608704%2Cmath%2F0608225%2Cmath%2F0608713%2Cmath%2F0608319%2Cmath%2F0608517%2Cmath%2F0608427%2Cmath%2F0608309%2Cmath%2F0608021%2Cmath%2F0608777%2Cmath%2F0608343%2Cmath%2F0608748%2Cmath%2F0608539%2Cmath%2F0608372%2Cmath%2F0608622%2Cmath%2F0608223%2Cmath%2F0608360%2Cmath%2F0608140%2Cmath%2F0608800%2Cmath%2F0608735%2Cmath%2F0608280%2Cmath%2F0608607%2Cmath%2F0608639%2Cmath%2F0608263%2Cmath%2F0608248%2Cmath%2F0608273%2Cmath%2F0608452%2Cmath%2F0608107%2Cmath%2F0608239%2Cmath%2F0608663%2Cmath%2F0608681%2Cmath%2F0608176%2Cmath%2F0608506%2Cmath%2F0608536%2Cmath%2F0608246%2Cmath%2F0608589%2Cmath%2F0608744%2Cmath%2F0608473%2Cmath%2F0608384%2Cmath%2F0608434%2Cmath%2F0608320%2Cmath%2F0608677%2Cmath%2F0608253%2Cmath%2F0608182%2Cmath%2F0608195%2Cmath%2F0608020%2Cmath%2F0608075%2Cmath%2F0608105%2Cmath%2F0608716&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Estimating the intensity of a random measure by histogram type\n  estimators"}, "summary": "The purpose of this paper is to estimate the intensity of some random measure\nby a piecewise constant function on a finite partition of the underlying\nmeasurable space. Given a (possibly large) family of candidate partitions, we\nbuild a piecewise constant estimator (histogram) on each of them and then use\nthe data to select one estimator in the family. Choosing the square of a\nHellinger-type distance as our loss function, we show that each estimator built\non a given partition satisfies an analogue of the classical squared bias plus\nvariance risk bound. Moreover, the selection procedure leads to a final\nestimator satisfying some oracle-type inequality, with, as usual, a possible\nloss corresponding to the complexity of the family of partitions we consider.\nWhen this complexity is not too high, the selected estimator has a risk\nbounded, up to a universal constant, by the smallest risk bound obtained for\nthe estimators in the family. For suitable choices of the family of partitions,\nwe deduce uniform risk bounds over various classes of intensities. Our approach\napplies to the estimation of the intensity of an inhomogenous Poisson process,\namong other counting processes, or the estimation of the mean of a random\nvector with nonnegative components.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0608658%2Cmath%2F0608762%2Cmath%2F0608707%2Cmath%2F0608624%2Cmath%2F0608508%2Cmath%2F0608014%2Cmath%2F0608490%2Cmath%2F0608535%2Cmath%2F0608647%2Cmath%2F0608596%2Cmath%2F0608428%2Cmath%2F0608213%2Cmath%2F0608156%2Cmath%2F0608459%2Cmath%2F0608488%2Cmath%2F0608289%2Cmath%2F0608772%2Cmath%2F0608627%2Cmath%2F0608180%2Cmath%2F0608633%2Cmath%2F0608127%2Cmath%2F0608266%2Cmath%2F0608124%2Cmath%2F0608001%2Cmath%2F0608743%2Cmath%2F0608010%2Cmath%2F0608072%2Cmath%2F0608560%2Cmath%2F0608691%2Cmath%2F0608519%2Cmath%2F0608685%2Cmath%2F0608511%2Cmath%2F0608174%2Cmath%2F0608787%2Cmath%2F0608625%2Cmath%2F0608736%2Cmath%2F0608779%2Cmath%2F0608502%2Cmath%2F0608188%2Cmath%2F0608515%2Cmath%2F0608230%2Cmath%2F0608357%2Cmath%2F0608760%2Cmath%2F0608566%2Cmath%2F0608090%2Cmath%2F0608404%2Cmath%2F0608715%2Cmath%2F0608551%2Cmath%2F0608405%2Cmath%2F0608698%2Cmath%2F0608640%2Cmath%2F0608036%2Cmath%2F0608240%2Cmath%2F0608704%2Cmath%2F0608225%2Cmath%2F0608713%2Cmath%2F0608319%2Cmath%2F0608517%2Cmath%2F0608427%2Cmath%2F0608309%2Cmath%2F0608021%2Cmath%2F0608777%2Cmath%2F0608343%2Cmath%2F0608748%2Cmath%2F0608539%2Cmath%2F0608372%2Cmath%2F0608622%2Cmath%2F0608223%2Cmath%2F0608360%2Cmath%2F0608140%2Cmath%2F0608800%2Cmath%2F0608735%2Cmath%2F0608280%2Cmath%2F0608607%2Cmath%2F0608639%2Cmath%2F0608263%2Cmath%2F0608248%2Cmath%2F0608273%2Cmath%2F0608452%2Cmath%2F0608107%2Cmath%2F0608239%2Cmath%2F0608663%2Cmath%2F0608681%2Cmath%2F0608176%2Cmath%2F0608506%2Cmath%2F0608536%2Cmath%2F0608246%2Cmath%2F0608589%2Cmath%2F0608744%2Cmath%2F0608473%2Cmath%2F0608384%2Cmath%2F0608434%2Cmath%2F0608320%2Cmath%2F0608677%2Cmath%2F0608253%2Cmath%2F0608182%2Cmath%2F0608195%2Cmath%2F0608020%2Cmath%2F0608075%2Cmath%2F0608105%2Cmath%2F0608716&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The purpose of this paper is to estimate the intensity of some random measure\nby a piecewise constant function on a finite partition of the underlying\nmeasurable space. Given a (possibly large) family of candidate partitions, we\nbuild a piecewise constant estimator (histogram) on each of them and then use\nthe data to select one estimator in the family. Choosing the square of a\nHellinger-type distance as our loss function, we show that each estimator built\non a given partition satisfies an analogue of the classical squared bias plus\nvariance risk bound. Moreover, the selection procedure leads to a final\nestimator satisfying some oracle-type inequality, with, as usual, a possible\nloss corresponding to the complexity of the family of partitions we consider.\nWhen this complexity is not too high, the selected estimator has a risk\nbounded, up to a universal constant, by the smallest risk bound obtained for\nthe estimators in the family. For suitable choices of the family of partitions,\nwe deduce uniform risk bounds over various classes of intensities. Our approach\napplies to the estimation of the intensity of an inhomogenous Poisson process,\namong other counting processes, or the estimation of the mean of a random\nvector with nonnegative components."}, "authors": ["Yannick Baraud", "Lucien Birg\u00e9"], "author_detail": {"name": "Lucien Birg\u00e9"}, "author": "Lucien Birg\u00e9", "arxiv_comment": "38 pages", "links": [{"href": "http://arxiv.org/abs/math/0608663v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0608663v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62G05", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0608663v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0608663v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:math/0608663v1 [math.ST] 27 Aug 2006\n\nESTIMATING THE INTENSITY OF A RANDOM MEASURE BY\nHISTOGRAM TYPE ESTIMATORS\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nAbstract. The purpose of this paper is to estimate the intensity of some random\nmeasure N on a set X by a piecewise constant function on a finite partition of X .\nGiven a (possibly large) family M of candidate partitions, we build a piecewise\nconstant estimator (histogram) on each of them and then use the data to select\none estimator in the family. Choosing the square of a Hellinger-type distance as\nour loss function, we show that each estimator built on a given partition satisfies\nan analogue of the classical squared bias plus variance risk bound. Moreover,\nthe selection procedure leads to a final estimator satisfying some oracle-type inequality, with, as usual, a possible loss corresponding to the complexity of the\nfamily M. When this complexity is not too high, the selected estimator has a\nrisk bounded, up to a universal constant, by the smallest risk bound obtained for\nthe estimators in the family. For suitable choices of the family of partitions, we\ndeduce uniform risk bounds over various classes of intensities. Our approach applies to the estimation of the intensity of an inhomogenous Poisson process, among\nother counting processes, or the estimation of the mean of a random vector with\nnonnegative components.\n\n1. Introduction\nThe aim of the present paper is to design a new model selection procedure in\na statistical framework which is general enough to cope simultaneously with the\nfollowing estimation problems.\nProblem 1: Estimating the means of nonnegative data. The statistical\nproblem that initially motivated this research was suggested by Sylvie Huet and\ncorresponds to the modeling of data coming from some agricultural experiments. In\nsuch an experiment, the observations are independent nonnegative random variables\nNi with mean si where i varies among some finite index set X . In this framework,\nour aim is to estimate the vector (si )i\u2208X .\nProblem 2: Estimating the intensity of a Poisson process. We recall that\na Poisson process N on the measurable set (X , A) with finite mean measure \u03bd is a\nrandom measure N on X such that\n\u2022 for any A \u2208 A, N (A) is a Poisson random variable with parameter \u03bd(A);\n\u2022 for any family A1 , . . . , An of disjoint elements of A, the corresponding random variables N (A1 ), . . . , N (An ) are independent.\nDate: April, 2006.\n2000 Mathematics Subject Classification. 62G05.\nKey words and phrases. Model selection - Histogram - Discrete data - Poisson process - Intensity\nestimation - Adaptive estimation.\n1\n\n\f2\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nWe can always assume that \u03bd is finite by suitably restricting the domain of observation of the process. When the mean measure \u03bd is dominated by some given\nmeasure \u03bb on X then the nonnegative function s = d\u03bd/d\u03bb is called the intensity of\nN . A Poisson process can be represented as a point process on the set X . Each\npoint represents the time (if X = R+ ) or location of some event. For example, the\nsuccessive times of failures of some machine can be represented by a Poisson process\non X = R+ . The intensity of the process models the behaviour of the machine in\nthe following way: the intervals of times on which the intensity takes large values\ncorrespond to periods where failures are expected to be frequent and in the opposite,\nthose on which the intensity is close to 0 are periods on which failures are rare. In\nthis statistical framework, our aim is to estimate the intensity s on the basis of the\nobservation of N .\nProblem 3: Estimating a hazard rate. We consider an n sample T1 , . . . , Tn\nof non-negative real valued random variables with common density p (with respect\nto the Lebesgue measure on R+ ) and assume these to be (possibly) right-censored.\nThis means that there exists i.i.d. random variables C1 , . . . , Cn such that we actually observe the pairs Xj = (Tej , Dj ) for j = 1, . . . , n with Tej = min {Tj , Cj } and\nDj = 1l{Tj =Tej } . Such censored data are common in survival analysis. Typically, Ti\ncorresponds to a time of failure or death which cannot be observed if it exceeds time\nCi . Our aim, here, is to estimate the hazard rate s of the Ti defined for t \u2265 0 by\ns(t) = p(t)/P(T1 \u2265 t).\nProblem 4: Estimating the intensity of the transition of a Markov process.\nLet {Xt , t \u2265 0} be a Markov process on R+ with cadlag paths and a finite number\nof states. We distinguish two particular states, named 0 and 1, and assume that\n0 is absorbant and that there is a positive probability to reach 1. Our aim is to\nprovide an estimation of the intensity of the transition time T1,0 from state 1 to\n0. Typical examples arise when 0 means \"death\", \"failure\", . . . . An alternative\nexample could be the situation where T1,0 measures the age at which a drug addict\nmakes the transition from soft drugs (state 1) to hard drugs (state 0). In this case\nwe stop the chain at 0 making this state absorbing. For t > 0, we denote by Xt\u2212\nthe left-hand limit of the process X at Rtime t and assume that for some measurable\nt\nnonnegative function p, P(T1,0 \u2264 t) = 0 p(u)du. Note that p is merely the density\nof T1,0 if T1,0 < +\u221e a.s. which we shall not assume. Our aim is to estimate the\ntransition intensity s of T1,0 which is defined for t > 0 by s(t) = p(t)/P (Xt\u2212 = 1).\nFor pedagogical reasons mainly, since it has already been extensively studied and\ncan therefore serve as a reference, it will be interesting to consider also the much\nmore classical\nProblem 0: Density estimation. It is the problem of estimating an unknown\ndensity s from n i.i.d. observations X1 , . . . , Xn with this density.\nAll the problems described in the above examples amount to estimating a function\ns mapping X to R+ . For this purpose, we choose a family M of partitions of X and\nfor each m \u2208 M we design a non-negative estimator \u015dm of s which is constant on\nthe elements of this partition. We shall call such an estimator an histogram-type\nestimator. The performance of \u015dm depends on both s and m. Since s is unknown, we\ncannot pick the partition which leads to the best estimator. To select a partition in\nM, we shall rather use a method solely based on our data leading to some random\npartition m\u0302 and define our resulting estimator as \u015dm\u0302 . Our objective is to design\n\n\fESTIMATING THE INTENSITY\n\n3\n\nthe selection procedure in such a way that \u015dm\u0302 performs almost as well as the best\nestimator among the family {\u015dm , m \u2208 M}.\n\nThe purpose of this paper is to describe some general setup which allows to\ndeal with all the five problems simultaneously, to explain the construction of our\nhistogram-type estimators \u015dm , to design a suitable selection procedure m\u0302 and to\nstudy the performance of the resulting estimator \u015dm\u0302 . We shall illustrate our results\nby numerous examples of family of partitions and target functions s of interest.\nFor the problems of estimating the intensity of a Poisson process or a hazard rate\non the line, our method provides estimators than can cope with different families\nof functions simultaneously, including monotone, H\u00f6lderian, or piecewise constant\nwith a few jumps with unknown locations and sizes. In the multivariate case, we\nshall also provide some special method for estimating Poisson intensities with a few\nspikes with unknown locations and heights.\nThe problem of estimating s by model selection in the first four setups described\nabove did not receive much attention in the literature with a few noticeable exceptions. Problem 1 is generally viewed as a regression problem where the mean si takes\nthe form f (xi ) for some design points xi (typically f is defined on [0, 1] and xi = i/n).\nTo perform model selection, one introduces a wavelet basis and performs a shrinkage\nof the estimated coefficients of f with respect to this basis. This amounts to selecting which coefficients will be kept. To this form of selection pertain the papers by\nAntoniadis, Besbeas and Sapatinas (2001), Antoniadis and Sapatinas (2001). Closer\nto our approach is Kolaczyk and Nowak (2004) based on penalized maximum likelihood. Unlike ours, their approach requires that the means si be uniformly bounded\nfrom above and below by known positive constants. For Problem 2, a similar approach based on wavelet shrinkage is developed in Kolaczyk (1999), but the reference\nresult is Reynaud-Bouret (2003). Problems 3 and 4 amount to estimating Aalen's\nmultiplicative intensity s of some counting process with a bounded number of jumps.\nThe problem of non-parametric estimation of Aalen's multiplicative intensities has\nbeen considered by Antoniadis (1989) who uses penalized maximum likelihood estimation with a roughness penalty and gets uniform rates of convergence over Sobolev\nballs. Van de Geer (1995) considers the Hellinger loss and establishes uniform estimation rates for the maximum likelihood estimator over classes of intensities with\ncontrolled bracketting entropy. Gr\u00e9goire and Nemb\u00e9 (2000) extend the results of\nBarron and Cover (1991) about density estimation to that of intensities. Wu and\nWells (2003) and Patil and Wood (2004) derive asymptotic results for thresholding\nestimators based on wavelet expansions. All these results, apart from those of van\nde Geer, are of an asymptotic nature. Reynaud-Bouret (2002) introduces a model\nselection procedure to estimate the intensity. A common feature of these papers lies\nin the use of martingales techniques (apart from Gr\u00e9goire and Nemb\u00e9, 2000). Unlike\ntheirs, our approach does not require any martingale argument at all.\nIn Section 2, we present a general statistical framework which allows to handle\nsimultaneously all the examples we have mentioned. We also make a review of some\nspecial classes of target functions and the various families of models (partitions) to\nbe used in our estimation procedure. The treatment of our five estimation problems\nis provided in Sections 4 and 5. The results presented there derive from a unifying\ntheorem to be found in Section 6. The remainder of the paper is devoted to the\nmost technical proofs.\n\n\f4\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nIn the sequel, we shall make a systematic use of the following notations: constants\nwill be denoted by C, C \u2032 , c, . . . and may change from line to line; we denote by N\u2217\nthe set of positive integers and we write x \u2227 y for min{x, y}, x \u2228 y for max{x, y} and\n|m| for the cardinality of a set m.\n2. Presentation of our method\n2.1. A general statistical framework. We consider an abstract probability space\n(\u03a9, E, P) and a measurable space (X , A) bearing a nonnegative \u03c3-finite measure \u03bb. In\nthe sequel E will denote the expectation with respect to P. We then consider on X a\nnonnegative bounded random process Y = Y (x, \u03c9), i.e. a measurable function from\nX \u00d7 \u03a9 to R+ , and the nonnegative random measure M on X given by dM = Y d\u03bb.\nBesides M , we also observe a nonnegative random measure N on X which satisfies\n\u0015\n\u0014Z\ns dM < +\u221e, for all A \u2208 A,\n(1)\nE[N (A)] = E\nA\n\nfor some deterministic nonnegative and measurable function s on X . Note that this\nassumption implies that N is a.s. a finite measure. Our aim is to estimate s from the\nobservations N and M . Hereafter, we shall deal with estimators that\n\u0002R belong\n\u0003 to the\ncone L of nonnegative measurable functions t on X \u00d7\u03a9 such that E X t dM < +\u221e.\nNote that s also belongs to L. To measure the risks of such estimators, we endow L\nwith the quasi-distance (since we may have H(t, t\u2032 ) = 0 with t 6= t\u2032 ) H between two\nelements t and t\u2032 of L by\nZ \u0010\n\u221a \u00112\n\u221a\nt \u2212 t\u2032 dM,\nH 2 (t, t\u2032 ) =\nX\n\nand set as usual, for t \u2208 L and F \u2282 L, H(t, F) = inf f \u2208F H(t, f ). Given an estimator\n\u015d \u0002of s, i.e. \u0003a measurable function of N and Y with \u015d \u2208 L, we define its risk by\nE H 2 (\u015d, s) . In most of our applications, Y is identically equal to 1 in which case\nM = \u03bb is deterministic and if t and t\u2032 are densities with respect to M , H is merely\nthe Hellinger distance between the corresponding probabilities. Only the cases of\nProblems 3 and 4 require to handle random measures M .\nIn order to define our estimators we assume that\n(2)\n\nP[N (A) > 0 and M (A) = 0] = 0\n\nfor all A \u2208 A,\n\na property which is automatically fulfilled when M = \u03bb is deterministic because\nof (1).\n2.2. Histogram-type estimators. Let us now introduce the histogram-type estimators \u015dm based on some finite partition m of X . We consider the subset J = {A \u2208\nA | E[M (A)] < +\u221e} of A and define the model Sm as the set of (possibly random)\nnonnegative piecewise constant functions on X :\n)\n(\n\\\nX\nL.\nSm = t =\ntI 1lI tI = tI (\u03c9) \u2208 R for all I \u2208 m, \u03c9 \u2208 \u03a9\nI\u2208m\u2229J\n\nWe then define the histogram estimator \u015dm as the element of Sm given (with the\nconvention 0/0 = 0) by\nX N (I)\n1lI .\n\u015dm =\nM (I)\nI\u2208m\u2229J\n\n\fESTIMATING THE INTENSITY\n\n5\n\nNote that \u015dm is a.s. well-defined because of (2). We shall, hereafter, call it the\nhistogram estimator based on m.\nUnder suitable assumptions that will be satisfied for Problems 0, 1 and 2 (the\ncase of hazard rates and Markov processes being more complicated), we shall prove\nfor \u015dm a risk bound of the form\n\b \u0002\n\u0001\u0003\n\u0002\n\u0003\n(3)\nE H 2 (\u015dm , s) \u2264 C0 E H 2 (s, Sm ) + CP |m| ,\n\nwhere C0 is a numerical constant and CP depends on the problem we consider. For\ninstance, CP = n\u22121 for density estimation and CP = 1 for estimating the intensity\nof a Poisson process. We recover here the usual decomposition of the risk bounds\ninto an approximation term which involves the distance of the parameter from the\nmodel and a complexity term proportional to the number |m| of parameters that\ndescribe the model.\n\n2.3. The selection procedure. Given the family of models {Sm , m \u2208 M} corresponding to a finite or countable family M of partitions m, we consider, in order to\ndefine our model selection procedure, the possibly enlarged family\nM = {m \u2228 m\u2032 for m, m\u2032 \u2208 M};\n\nm \u2228 m\u2032 = {I \u2229 I \u2032 | I \u2208 m, I \u2032 \u2208 m\u2032 , I \u2229 I \u2032 6= \u2205},\n\nso that m \u2228 m\u2032 is again a finite partition of X .\n\nWe shall systematically make the following assumption about the family M.\n\nH : There exists some \u03b4 \u2265 1 such that |m\u2228m\u2032 | \u2264 \u03b4 (|m| + |m\u2032 |) for all (m, m\u2032 ) \u2208 M2 .\nWe then introduce a penalty function \"pen\" from M to R+ to be described below\nand, for m 6= m\u2032 \u2208 M we consider the test statistic\n\n(4)\n\nTm,m\u2032 (N ) = H 2 (\u015dm , \u015dm\u2228m\u2032 ) \u2212 H 2 (\u015dm\u2032 , \u015dm\u2228m\u2032 ) + 16[pen(m) \u2212 pen(m\u2032 )].\n\nThe corresponding test between m and m\u2032 decides m if Tm,m\u2032 < 0, m\u2032 if Tm,m\u2032 > 0\nand at random if Tm,m\u2032 = 0. Note that the tests corresponding to Tm,m\u2032 and Tm\u2032 ,m\nare the same. We then set, for all m \u2208 M,\nRm = {m\u2032 \u2208 M, m\u2032 6= m | the test based on Tm,m\u2032 rejects m}\n\nand, given some \u03b5 > 0, we define m\u0302 to be any point in M such that\n\b\n(5)\nD(m\u0302) \u2264 inf D(m) + \u03b5/3\nwith\nD(m) = sup H 2 (\u015dm , \u015dm\u2032 ) .\nm\u2208M\n\nm\u2032 \u2208Rm\n\nThis model selection procedure results in an estimator s\u0303 = \u015dm\u0302 that we shall call\npenalized histogram estimator (in the sequel PHE, for short) based on the family of\nmodels {Sm , m \u2208 M} and the penalty function pen(*). As to the penalty, it is the\nsum of two components: pen(m) = c1 |m| + c2 \u2206m with c1 and c2 depending on the\nframework and \u2206m being a nonnegative weight associated to the model Sm . We\nrequire that those weights satisfy\nX\nexp[\u2212\u2206m ] = \u03a3 < +\u221e.\n(6)\nm\u2208M\n\nIf \u03a3 = 1, the choice of the \u2206m can be viewed as the choice of a prior distribution\non the models. For related conditions and their interpretation, see Barron and\nCover (1991), Barron, Birg\u00e9 and Massart (1999) or Birg\u00e9 and Massart (2001). The\nconstant 16 in (4) plays no particular role and has only been chosen in order to\nimprove the legibility of our main results. Our selection procedure can be viewed\nas a mixture between a method due to Birg\u00e9 (1983 and 2006) based on testing and\n\n\f6\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nan improved version of the original Lepski's method, as described in Lepski (1991)\nand subsequent work of the same author. This improved version was presented by\nLepski in a series of lectures he gave at Garchy in 1998.\n2.4. Risk bounds for the procedure. As we shall see later, with a suitable choice\nof \u03b5, the performances of this procedure for Problems 0, 1 and 2 are described by\nrisk bounds of the following form:\n\b \u0002\n\u0001\u0003\n\u0002\n\u0002\n\u0003\n\u0001\u0003\n(7) E H 2 (s\u0303, s) \u2264 C0\u2032 inf E H 2 (s, Sm ) + CP |m| 1 + |m|\u22121 \u2206m + \u03a32\n,\nm\u2208M\n\nwhere C0\u2032 is a numerical constants and CP as in (3). Comparing (7) with (3), we see\nthat the estimator s\u0303 achieves a risk bound comparable, up to a constant factor, with\nthe best risk bound obtained by the estimators \u015dm provided that \u03a3 is not large and\n\u2206m not much larger than |m|. Note that these two restrictions are, to some extent,\ncontradictory since the smaller \u2206m , the larger \u03a3,Palthough it is clearly unnecessary\nto choose \u2206m smaller than |m|. Therefore, if m\u2208M e\u2212|m| is not large, one can\nmerely take \u2206m = |m|. Otherwise, the choice of the \u2206m will be more delicate but\nwe should keep in mind that, if \u03a3 is not large, the performance of s\u0303 will be as good\n(up to a constant factor) as the performance of any \u015dm for which \u2206m \u2264 |m|.\n3. A review of the models we shall use\n3.1. Some classes of functions of special interest. The motivations for the\nchoice of some family of models {Sm , m \u2208 M} are twofold. First, there is the\nrestriction that M should satisfy Assumption H and there are two main examples\nof such families. In the \"nested\" case, the family is totally ordered for the inclusion\nand thus, we either have m \u2228 m\u2032 = m or m \u2228 m\u2032 = m\u2032 for all m and m\u2032 in M. Then,\nM = M and \u03b4 = 1. Another situation where Assumption H is satisfied with \u03b4 = 1\noccurs when X is either R or some subinterval of R and each m \u2208 M is a finite\npartition of X into intervals.\n\nThe second motivation is connected to the approximation properties of the models. If, for instance, we believe that the true s is smooth or monotone, one should\nintroduce families of models that approximate reasonably well such functions. In\nthe sequel, we shall put a special emphasis on the following classes of functions:\n\u25e6\n\n\u2022 Monotone functions. For X an interval of R with interior X and R a positive\nnumber, we denote by S 1 (R) the set of monotone functions t on X such that\nsup\n\u25e6 |t(x) \u2212 t(y)| \u2264 R.\nx,y\u2208X\n\n\u2022 Continuous functions. Let w be a modulus of continuity on [0, 1), i.e. a\ncontinuous nondecreasing function with w(0) = 0 - see additional details in\nDeVore and Lorentz (1993) -. We denote by S 2 (w) the set of functions t on\n[0, 1) such that |t(x + y) \u2212 t(x)| \u2264 w(y) for all x \u2208 [0, 1) and 0 \u2264 y \u2264 1 \u2212 x.\nFor 0 < \u03b1 \u2264 1 and R > 0, the H\u00f6lder class H\u03b1R is the class S 2 (w) with\nw(y) = Ry \u03b1 . More generally we say that a function u defined on V \u2282 [0, 1)k\nfor some k \u2265 1 belongs to the set H\u03b1R (V), \u03b1 \u2208]0, 1), R > 0, if\n|u(x) \u2212 u(y)| \u2264 R\n\nk\nX\nj=1\n\n|xj \u2212 yj |\u03b1\n\nfor all x, y \u2208 V.\n\n\fESTIMATING THE INTENSITY\n\n7\n\n\u2022 Piecewise constant functions. If the function t defined on [0, 1) is constant\nover some intervals and then jumps from time to time, it is a piecewise\nconstant function of the form\n(8)\n\nt=\n\nD\nX\n\ntk 1l[xk\u22121 ,xk )\n\nwith 0 = x0 < x1 < . . . < xD = 1.\n\nk=1\n\nWe shall denote by S 3 (D, R) the class of such piecewise functions such that\nsup1\u2264k\u2264D tk \u2264 R. Note that this would correspond to a parametric model\nwith D parameters if the locations of the jumps were known. We shall restrict\nour attention to D \u2265 2 since S 3 (1, R) only contains constant functions and\nis then a subset of S 2 (w) with w \u2261 0.\n\u2022 Besov balls and functions of bounded variation. Here we consider functions t defined on [0, 1). Given positive numbers \u03b1, p and R, we denote\n\u03b1 (R), the closed Besov ball of radius R centered at zero of the Besov\nby Bp,\u221e\n\u03b1 ([0, 1)), i.e. the set of functions t in this space with Besov semispace Bp,\u221e\n\u03b1\nnorm |t|Bp,\u221e\n\u2264 R. Analogously, we set BBV (R) for the set of functions t\nof bounded variation with Var\u2217 (t) \u2264 R. We refer to Chapter 2 of the book\nby DeVore and Lorentz (1993) for details on Besov spaces and the definition of Besov semi-norms, functions of bounded variation and the variation\nsemi-norm Var\u2217 . Note that S1 (R) \u2282 BBV (R). We shall also consider the\n\u03b1 ([0, 1)k ) for k \u2265 2.\nmultidimensional Besov spaces Bp,\u221e\n3.2. Some typical models. Let us now describe a few useful families of models\nand corresponding choices for the weights \u2206m that satisfy (6).\n\n3.2.1. Example 1: models for functions on [0, 1). The following models are suitable\nfor approximating functions belonging to the classes that we just mentioned. Since\nthey are based on partitions\nof [0, 1) into intervals, they satisfy Assumption H with\n\b\n\u03b4 = 1. Let Jl = j2\u2212l , j \u2208 N and J\u221e = \u222al\u2208N Jl be the set of all dyadic points\nin [0, 1). To build M, we consider partitions m = {I1 , . . . , ID } of [0, 1)) generated\nby increasing sequences {0 = x0 < x1 < . . . < xD = 1} with Ii = [xi\u22121 , xi ). We\nthen define M to be the set of all such partitions with xi \u2208 J\u221e for 1 \u2264 i \u2264 D \u2212 1.\nTherefore, whatever m \u2208 M, the elements of Sm are piecewise constant functions\nwith D pieces and jumps located on the grid J\u221e . The novelty of this particular\nfamily of partitions lies in the fact that there is no lower bound on the length of\nthe intervals on which the partitions are built. It will be useful to single out the set\nMR = {mk , k \u2208 N} of regular dyadic partitions where mk is the partition of [0, 1)\ninto 2k intervals of length 2\u2212k . In particular, m0 = [0, 1).\nOne possible way of defining the corresponding weights \u2206m is as follows. For\nl \u2208 N\u22c6 and 2 \u2264 D \u2264 2l we define Ml,D as the set of all partitions m with\n|m| =h D and\nl is the smallest\ninteger such that {x1 , . . . , xD\u22121 } \u2282 Jl . Then,\n\u0011i S\nS \u0010S2l\nM=\n{m0 }. We choose \u2206m0 = 1 and\nl\u22651\nD=2 Ml,D\n(9)\n\n\u2206m = D(l log 2 + 2 \u2212 log D) + 2 log l\n\nif m \u2208 Ml,D .\n\n\f8\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nSince |Ml,D | \u2264\n\nX\n\n\u0001\n\n2l \u22121\nD\u22121\n\n\u2264\n\n2l\nD\n\n\u0001\n\n\u2264 (2l e/D)D , we derive from (9) that\nl\n\nexp[\u2212\u2206m ] <\n\n2\nX X\n\nl\u22651 D=2\n\nm\u2208M\\{m0 }\n\n\u2264\n\nX X\n\nl\u22651 D\u22652\n\n|Ml,D |l\u22122 exp[\u2212D(l log 2 + 2 \u2212 log D)]\nl\u22122 e\u2212D =\n\n\u03c02 \u2212 6\n< 0.14\n6e(e \u2212 1)\n\nand it follows that (6) is satisfied.\n\n3.2.2. Special partitions derived from adaptive approximation algorithms. It is easily\nseen that the family M of partitions we introduced for Example 1 is too rich for\nchoosing \u2206m = c|m| for all m and c a fixed constant since then (6) would not be\nsatisfied. For partitions in Ml,D with l > D, \u2206m behaves as l|m| and l can be\narbitrarily large. Fortunately, there exists a subset M1T of M, which is of special\ninterest because of its approximation properties with respect to functions in Besov\nspaces, and such as it is possible to choose \u2206m = 2|m| for m \u2208 M1T . This will\ndefinitely improve the performances of the PHE for estimating functions in Besov\nspaces. Let us now describe M1T .\n\nAmong all partitions on [0, 1) with dyadic endpoints, some of them, which are in\none-to-one correspondance with the family of complete binary trees, can be derived\nby the following algorithm described in Section 3.3 of DeVore (1998). One starts\nwith the root of the tree which corresponds to the interval [0, 1) and decides to divide\nit into two intervals of length 1/2 or not. We assume here that all intervals contain\ntheir left endpoint but not the right one. If one does not divide, the algorithm\nstops and the tree is reduced to its root. If one divides, one gets two intervals\ncorresponding to adding two sons to the root. Then one repeats the procedure with\neach interval and so on. . . . At each step, the terminal nodes of the tree correspond\nto the intervals in the partition and one decides to divide any such interval into two\nequal parts or not. Dividing means adding two sons to the corresponding terminal\nnode. The whole procedure stops at some stage producing a complete binary tree\nwith D terminal nodes and the corresponding partition of [0, 1) into D intervals.\nThis is the type of tree which comes out of an algorithm like CART, as described by\nBreiman et al. (1984). Such constructions and the corresponding selection procedure\nresulting from the CART algorithm have been studied by Gey and Nedelec (2005).\nWe denote by M1T the subset of M of all partitions that can be obtained in this\nway. Note here that the set MR of regular partitions is a subset of M1T .\n\nIt is known that the number of complete binary\u0012trees\n\u0013 with j + 1 terminal nodes is\n2j\ngiven by the so-called Catalan numbers (1 + j)\u22121\nas explained for instance in\nj\nStanley (1999, page 172). As a consequence, we can redefine \u2206m = 2|m| for m \u2208 M1T\n\n\fESTIMATING THE INTENSITY\n\n9\n\nand, using the fact (which derives from Stirling's expansion) that\nX\n\nexp[\u2212\u2206m ] <\n\nX\n\nX\n\n\u0012\n\n2j\nj\n\n\u0013\n\n\u2264 4j , get\n\nexp[\u22122(j + 1)]\n\nj\u22650 {m\u2208M1 | |m|=1+j}\nT\n\nm\u2208M1T\n\n=\n\nX\nj\u22650\n\n\u0012\n\n2j\nj\n\n\u0013\n\nexp[\u22122(j + 1)]\nj+1\n\nFinally (6) is satisfied with \u03a3 < \u03a3\u20321 + 0.14.\n\n\u2264 e\u22122\n\nX (2/e)2j\nj\u22650\n\nj+1\n\n= \u03a3\u20321 .\n\n3.2.3. Example 2: estimating functions with radial symmetry. There are situations\nwhere one may assume that the value of s(x) only depends on the Euclidean distance\nkxk between this point and some origin in which case one can write s(x) = \u03a6(kxk).\nIn such a case, it is natural to estimate s on a ball, which we may assume, without\nloss of generality, to be the open unit ball Bk of Rk . To any partition m of [0, 1)\nwe can associate a partition of Bk with elements J = {x | kxk \u2208 I} where I denotes\nan element of m. For simplicity, we shall identify the two partitions (the first one\nof [0, 1) and the new one of Bk ) and denote both of them by m. In the sequel, we\nshall focus our attention on the family of partitions of Example 1 with the weights\ndefined in Section 3.2.2.\n3.2.4. Example 3: estimating functions on [0, 1)k , k \u2265 2. To deal with the case\nX = [0, 1)k , let us first introduce some notations. For j \u2208 N we consider the set\nn\no\nNj = l = (l1 , . . . , lk ) \u2208 Nk 1 \u2264 li \u2264 2j for 1 \u2264 i \u2264 k\n\nand for j \u2208 N and l \u2208 Nj the cube Kj,l given by\nn\no\nKj,l = x = (x1 , . . . , xk ) \u2208 [0, 1)k (li \u2212 1)2\u2212j \u2264 xi < li 2\u2212j for 1 \u2264 i \u2264 k .\no\nn\nS\nWe set Kj = Kj,l , l \u2208 Nj and K = j\u22650 Kj .\n\nLet P be the collection of all finite subsets p of K \\ K0 consisting of disjoint cubes.\nTo each p \u2208 P, we associate the positive quantity J(p) = inf{j | p\u2229K\n(J(\u2205) =\nS \bj 6= \u2205}\n+\u221e) and the partition mp generated by p, i.e. mp = {I \u2208 p}\n[0, 1)k \\ \u222aI\u2208p I\nprovided that this last set is not empty and mp = {I \u2208 p} otherwise. We finally set\nM = {mp \u2228 Kj with p \u2208 P and j < J(p)}. Note here that the mapping (j, p) 7\u2192\nmp \u2228 Kj is not one to one. For instance m\u2205 \u2228 Kj = Kj = Kj \u2228 Kj\u22121 . We shall prove\nin Section 7.1 the following result:\nLemma 1. The family M satisfies Assumption H with \u03b4 = 2.\nIn order to define the weights \u2206m , we shall distinguish a special subset MkT of M\nwhich is the k-dimensional analogue of the one we considered in Section 3.2.2. Here\none starts the algorithm with X = [0, 1)k (which corresponds to the root of the tree)\nand at each step get a partition of X into a finite family of disjoint cubes of the form\nKj,l . One then decides to divide any such cube into the 2k elements of Kj+1 which\nare contained in it or not. Again, this corresponds to growing a complete 2k -ary tree,\npartioning a cube meaning adding 2k sons to a terminal node and the set MkT of\nall partitions that can be constructed in this way corresponds to the set of complete\n\n\f10\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\n2k -ary trees. As for k = 1, MkT contains the set MR = {m\u2205 \u2228 Kj , j \u2265 0} of all\nregular partitions of X into 2kj cubes of equal volume. Working with M instead of\nthe much simpler family MR allows to handle less regular functions like those which\nhave a few spikes or are less smooth on some subset of X .\nIf m \u2208 MkT we take \u2206m = |m| and otherwise we set\n\u2206\u2032j,p = j + k\n\nX\n(j + i) |p \u2229 Kj+i |\ni\u22651\n\nfor p \u2208 P and j < J(p)\n\nand\n(10)\n\n\u2206m =\n\ninf\n\n{(j,p) | m=mp \u2228Kj }\n\n\b\n\n\u2206\u2032j,p\n\nfor m \u2208 M \\ MkT .\n\nNote that the ratio \u2206m /|m| is unbounded for m 6\u2208 MkT as shown by the example of\nm = mp \u2228 K0 with p reduced to a single element of Kj , j > 0. Then |m| = 2 while\n\u2206m = kj may be arbitrarily large. For the partitions m belonging to MkT we use the\nfact - see Stanley (1999) - that any complete l-ary tree has a number of terminal\nnodes of the form 1 + j(l \u2212 1) for some j \u2208 N and\n\u0012 that\n\u0013 the number of such trees with\nlj\n1 + j(l \u2212 1) terminal nodes is [1 + j(l \u2212 1)]\u22121\n. For l = 2k we derive that the\nj\n\u0012 k \u0013\n2 j\nk\nk\nk\n\u22121\nnumber of partitions in MT with 1 + j(2 \u2212 1) elements is [1 + j(2 \u2212 1)]\n.\nj\nMoreover, since k \u2265 2, we check that\n\u2206m > j(k log 2 + 1) + log(j + 1) if |m| = 1 + j(2k \u2212 1).\nSince\n\n\u0012\n\nlj\nj\n\n\u0013\n\nX\n\n\u2264 (le)j , it follows that\n\nm\u2208MkT\n\nexp[\u2212\u2206m ] <\n\nX\n\nX\n\nj\u22650 {m\u2208Mk | |m|=1+j(2k \u22121)}\nT\n\n\u0012\n\nexp[\u2212j(k log 2 + 1)]\nj+1\n\n\u0013\n\u0001\u2212j\n2k j\n2k e\nX\nj\n=\n(j + 1)[1 + j(2k \u2212 1)]\nj\u22650\n\n\u2264\n\nX\nj\u22650\n\n1\n= \u03a3\u2032k .\n(j + 1)[1 + j(2k \u2212 1)]\n\nLet us now turn to the partitions of the form mp \u2228Kj . For such a partition p\u2229Kj \u2032 = \u2205\nfor j \u2032 \u2264 j and, for i \u2265 1, |p \u2229 Kj+i | = li with 0 \u2264 li \u2264 2k(j+i) . Moreover, the number\nof those p \u2208 P such that |p \u2229 Kj+i | = li for a given sequence\nl = (li )i\u22651 with a finite\n\u0012 k(j+i) \u0013\nQ\n2\n. It follows from (10)\nnumber of nonzero coefficients is bounded by i\u22651\nli\n\n\fESTIMATING THE INTENSITY\n\n11\n\nthat\nX\n\nm\u2208M\u2032\n\nexp[\u2212\u2206m ] \u2264\n\u2264\n\nX\n\nX\n\ne\u2212j\n\nj\u22650 {p\u2208P | J(p)>j}\n\nX\n\n\u2212j\n\ne\n\nX\n\nY\n\ne\u2212k(j+i)|p\u2229Kj+i |\n\ni\u22651\n\nX\n\nY\n\ne\u2212k(j+i)li\n\nl {p | |p\u2229Kj+i |=li for i\u22651} i\u22651\nX Y \u0012 2k(j+i) \u0013\nX\n\u2212j\ne\u2212k(j+i)li\n\u2264\ne\nli\nj\u22650\nl i\u22651\nk(j+i)\nY 2 X \u0012 2k(j+i) \u0013\nX\n\u2212j\ne\u2212k(j+i)li\n\u2264\ne\nli\nj\u22650\n\ni\u22651 li =0\n\nj\u22650\n\n=\n\nX\nj\u22650\n\n=\n\nX\nj\u22650\n\n\u2264\n\nX\nj\u22650\n\ne\u2212j\n\n\u00112k(j+i)\nY\u0010\n1 + e\u2212k(j+i)\ni\u22651\n\n\uf8ee\n\nexp \uf8f0\u2212j +\n\uf8ee\n\nX\ni\u22651\n\n\uf8f9\n\u0011\n2k(j+i) log 1 + e\u2212k(j+i) \uf8fb\n\u0010\n\n\uf8f9\nX\nexp \uf8f0\u2212j +\n(e/2)\u2212k(j+i) \uf8fb = \u03a3\u2032\u2032k < +\u221e.\ni\u22651\n\nFinally we can conclude that (6) holds with \u03a3 < \u03a3\u2032k + \u03a3\u2032\u2032k .\n\n3.2.5. Models for n-dimensional vectors. To handle the problem we started with\nin the introduction, we may assume that our finite index set I is actually X =\n{1, . . . , n}, the estimation of the function s from X to R+ amounting to the estimation of the vector (s1 , . . . , sn )t \u2208 Rn+ with coordinates si = s(i).\nExample 4. If one assumes that either si varies smoothly with i or is monotone or\npiecewise constant with a small number of jumps, it is natural to choose for m a\npartition of X into intervals and for M the set of all such partitions. Note\n\u0012 that this\n\u0013\nn\u22121\nfamily satisfies Assumption H with \u03b4 = 1. Setting here \u2206m = |m| + log\n,\n|m| \u2212 1\n\u0012\n\u0013\nn\u22121\nwe get (6) with \u03a3 < (e \u2212 1)\u22121 since there are\npartitions in M with D\nD\u22121\nelements for 1 \u2264 D \u2264 n.\nExample 5. An alternative case is the case when s is constant, equal to s on X\nexcept for a few number of locations i where s(i) 6= s. Since the number k of such\nlocations is unknown, it is natural, for each k \u2208 {0, . . . , n \u2212 1} to define Mk as the\nset of partitions of X with k singletons and the set of the n \u2212 k remaining points.\nWe finally set M = \u222a0\u2264k\u2264n\u22121 Mk . Then \u0012\nAssumption\nH \u0012holds with\n\u0013\n\u0013 \u03b4 = 1. For\nn\nn\nm \u2208 Mk , |m| = k + 1 and we set \u2206m = log\n+ k = log\n+ |m| \u2212 1, so\nk\n|m| \u2212 1\nthat (6) holds with \u03a3 < e/(e \u2212 1).\n\n\f12\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\n4. The case of a deterministic measure M\nLet us now see how our general framework applies to Problems 1 and 2. Besides\nthese, our setup also covers the problem of density estimation. Although there is\na huge amount of literature on density estimation, our method brings some improvements to known results on partition selection for histograms. Moreover, since\nthis problem has attracted so much attention, it can serve as pedagogical example and reference for the sequel. This is why, before considering more original and\nless studied frameworks, we shall start our review by this quite familiar estimation\nproblem.\n4.1. Density estimation. We consider the classical problem of estimating an unknown density s from a sample of size n, which means that we have at hand an\ni.i.d. sample X1 , . . . , Xn from a distribution with unknown density s with respect\nto some givenPmeasure M = \u03bb on X . We define N to be the\nR empirical distribution:\nN (A) = n\u22121 ni=1 1lXi \u2208A . Then, as required, E [N (A)] = A s d\u03bb for all measurable\nsubsets A of X . In this case the distance H is merely a version of the Hellinger\ndistance between densities.\nWithin this framework, we can prove the following general result.\nTheorem 1. Assume that the family M satisfies Assumption H and the weights\n{\u2206m , m \u2208 M} are chosen so that (6) holds. Then the penalized histogram estimator\ns\u0303 = \u015dm\u0302 defined in Section 2.3 with pen(m) \u2265 n\u22121 (8\u03b4|m| + 202\u2206m ) satisfies\n\u0013\n\u0015^\n\u0014\n\u0012\n\u0001 101\u03a32\n\u0002 2\n\u0003\n2\n2.\n+\u03b5\nE H (s\u0303, s) \u2264 390 inf H (s, Sm ) + pen(m) +\nm\u2208M\nn\nThe only previous works on partition selection for histograms using squared\nHellinger loss we know about are to be found in Castellan (1999 and 2000) and\nBirg\u00e9 (2006). Castellan's approach is based on penalized maximum likelihood. This\nrequires to make specific restrictions on the underlying density s, in particular that\ns should be bounded away from 0. For the problem of estimating a density on R, her\nconditions on the family of partitions are also more restrictive than ours since we\ncan handle any countable families of finite partitions into intervals. Nevertheless, in\nthe multivariate case, our assumptions on the partitions are more stringent. Birg\u00e9's\napproach based on aggregation of histograms built on one half of the sample leads\nto more abstract but more general results.\nLet us now apply the above theorem to various families of models, systematically\nsetting pen(m) = n\u22121 (8\u03b4|m| + 202\u2206m ) and \u03b5 = n\u22121 . We assume in this section that\n\u03bb is the Lebesgue measure on X .\n4.1.1. Example 1, continued. When X = [0, 1), we use the family of models and\nweights of Section 3.2.1. Our next proposition shows that the PHE based on this\nsimple family of models and weights has nice properties for estimating various types\nof functions. The proof will be given in Section 7.3.\nProposition 1. Let s\u0303 be the PHE based on the family of models and weights \u2206m\ndefined in Section 3.2.1, \u03b5 = n\u22121 and the penalty function pen(m) = n\u22121 (8\u03b4|m| +\n202\u2206m ).\n\n\fESTIMATING THE INTENSITY\n\n13\n\ni) If s \u2208 S1 (R), then\no\nn\u0002\n\u0002\n\u0003\n\u0001\u00032/3\n(11)\nE H 2 (s\u0303, s) \u2264 C Rn\u22121 log 1 + nR2\n\u2228 n\u22121 .\n\n\u221a\nii) If s \u2208 S 2 (w) where w is a modulus of continuity on [0, 1), we define xw to\nbe the unique solution of the equation nxw2 (x) = 1 if w(1) \u2265 n\u22121/2 and xw = 1\notherwise. Then\n\u0002\n\u0003\n(12)\nE H 2 (s\u0303, s) \u2264 C(nxw )\u22121 .\n\u0002\n\u0003\n\u221a\nIf, in particular, s belongs to the H\u00f6lder class H\u03b1R with R \u2265 n\u22121/2 , then Es H 2 (s\u0303, s) \u2264\nCR2/(2\u03b1+1) n\u22122\u03b1/(2\u03b1+1) .\niii) If s \u2208 S 3 (D, R) with 2 \u2264 D \u2264 n and R \u2265 2, we get\n\u0002\n\u0003\n(13)\nEs H 2 (s\u0303, s) \u2264 CDn\u22121 log (nR/D) .\n\nIt is interesting\nto see\n\u0002\n\u0003 to what extent the previous bounds (together with the\ntrivial one, E H 2 (s\u0303, s) \u2264 2, which always holds but which we did not include in\n(11), (12) and (13) for simplicity) are optimal (up to the universal constants C).\nMany lower bounds on the minimax risk over various density classes are known for\nclassical loss functions. For squared Hellinger loss, some are given in Birg\u00e9 (1983\nand 1986) and Birg\u00e9 and Massart (1998). Many more are known for the squared\nL2 -loss, which can easily be extended to squared Hellinger loss because their proofs\nare based on perturbations arguments involving sets of densities for which both\ndistances are equivalent. It follows from these classical results that the bound we\nfind for continuous densities are actually optimal (see Birg\u00e9, 1983, p.211) while (11)\nis suboptimal because of the presence of the log factor. We shall see below that the\nmore sophisticated penalization strategy introduced in Section 3.2.2 does solve the\nproblem. The case of piecewise constant functions is more complicated. If D and\nthe locations of the jumps were known, one could use a single model corresponding\nto the relevant partition with D intervals and get a risk bound CD/n corresponding\nto a parametric problem with D parameters. Apart from the constant C, this\nbound cannot be improved which shows that the study of uniform risk bounds over\nS 3 (D, R) is only of interest when D \u2264 n since otherwise a lower bound for the\nrisk is of the order of the trivial upper bound 2. When D is smaller than n the\nextra log(nR/D) factor in (13) is due to the fact that we have to estimate the\nlocations of the jumps. The problem has been considered in Birg\u00e9 and Massart\n(1998, Section 4.2 and Proposition 2) where it is shown\n\u0001 that a lower bound for\nthe risk (when n \u2265 5D and D \u2265 9) is cDn\u22121 log nD \u22121 . Therefore our bound is\noptimal for moderate values of R. We do not know whether the log R factor in the\nupper bound is necessary or not.\n4.1.2. Improved risk bounds with a better weighting strategy. If we use the weights\n\u2206m defined in Section 3.2.2 to build s\u0303, we can only improve (up to constants) the\nrisk bounds given in Proposition 1 since the value of \u03a3 does not change much while\nthe new weights are not larger than the previous ones. Besides, the values of the\nweights have been substatially decreased for the partitions belonging to M1T . It turns\nout that piecewise constants functions on the elements of M1T possess quite powerful\n\u03b1 ([0, 1)) with\napproximation properties with respect to functions in Besov spaces Bp,\u221e\n\u03b1 < 1 and monotone functions. These properties are given in the following theorem\nwhich also includes the multidimensional case.\n\n\f14\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nTheorem 2. Let X = [0, 1)k , MkT be the set of \bpartitions m of X defined in SecP\n\u2032 be the cone t =\ntion 3.2.4 and, for m \u2208 MkT , let Sm\nI\u2208m tI 1lI , tI \u2265 0 . For any\np > 0, \u03b1 with 1 > \u03b1 > k(1/p\u22121/2)+ and any function t belonging toSthe Besov space\n\u2032\n\u03b1 ([0, 1)k ) with Besov semi-norm |t| \u03b1 , one can find some t\u2032 \u2208\nBp,\u221e\nBp,\u221e\nm\u2208MkT Sm such\nthat\n(14)\n\n\u03b1\nkt \u2212 t\u2032 k2 \u2264 C(\u03b1, k, p)|t|Bp,\u221e\n|m|\u2212\u03b1/k ,\n\nwhere k * k2 denotes the L2 (dx)-norm on [0, 1)k .\n\nIf t is a function of bounded variation on [0, 1), there exists t\u2032 \u2208\nthat kt \u2212 t\u2032 k2 \u2264 C \u2032 Var\u2217 (t)|m|\u22121 .\n\nS\n\nm\u2208M1T\n\n\u2032 such\nSm\n\nThe bound (14) is given in DeVore and Yu (1990). The proof for the bounded\nvariation case has been kindly communicated to the second author by Ron DeVore.\nWith the help of this theorem, we can now derive from Theorem 1 the following\nimproved bounds the proof of which is straightforward.\nProposition\n2. Let s\u0303 be the PHE based on the weights\n\u221a\n\u221a \u2206m defined in Section 3.2.2.\nIf s is a function of bounded variation with Var\u2217 ( s) \u2264 R and in particular if it\nbelongs to S 1 (R), then\nn\no\n\u0002\n\u0003\n(15)\nE H 2 (s\u0303, s) \u2264 min C(R/n)2/3 , 2\nfor R \u2265 n\u22121/2 .\n\u221a\n\u03b1 ([0, 1)) with 1 > \u03b1 > (1/p \u2212 1/2) and |\u221as|\n\u2264 R with R \u2265 n\u22121/2 ,\nIf s \u2208 Bp,\u221e\n\u03b1\n+\nBp,\u221e\nthen\nn\no\n\u0002\n\u0003\nE H 2 (s\u0303, s) \u2264 min C(\u03b1, p)R2/(1+2\u03b1) n\u22122\u03b1/(1+2\u03b1) , 2 .\n\nIt follows from classical lower bounds arguments that these bounds are minimax\nup to constants.\n4.1.3. The multidimensional case. When the density s defined on X = Bk can be\nwritten s(x) = \u03a6(kxk) for some function \u03a6 on [0, 1), we use the family of models\nintroduced in Example 2. We then obtain the risk bounds given in Propositions 1\nand 2 if we replace the assumptions on s by the same on \u03a6. We omit the details.\nIf X = [0, 1)k , k \u2265 2 and we use the family of models and weights described in\nSection 3.2.4, we get the following result.\n\u221a\nProposition 3. Let R \u2265 k\u22121 n\u22121/2 . If s belong to H\u03b1R ([0, 1)k ), then\nn\no\n\u0002\n\u0003\n(16)\nE H 2 (s, s\u0303) \u2264 min C(Rk)2k/(k+2\u03b1) n\u22122\u03b1/(2\u03b1+k) , 2 .\n\n\u221a\n\u03b1 ([0, 1)k ) with 1 > \u03b1 > k(1/p \u2212 1/2)\nMore generally, if s belongs to Bp,\u221e\n+ and\n\u221a\n| s|Bp,\u221e\n\u2264 R, then\n\u03b1\nn\no\n\u0002\n\u0003\nE H 2 (s\u0303, s) \u2264 min C(\u03b1, k, p)R2k/(k+2\u03b1) n\u22122\u03b1/(k+2\u03b1) , 2 .\n\nProof: Let m = Kj be an element of MR . Then \u2206m = |m| = 2kj and the maximal\n\u2212j\u03b1 so\nvariation of a function of H\u03b1R ([0, 1)k ) on an element of m is bounded by\n\u0002 Rk2\n\u0003\n2\n2\n\u22122j\u03b1\n2\nthat H (s, Sm ) \u2264 (Rk) 2\n. It then follows from Theorem 1 that E H (s\u0303, s) \u2264\n\n\fESTIMATING THE INTENSITY\n\n15\n\n\u0002\n\u0003\nC \u2032 (Rk)2 2\u22122j\u03b1 + n\u22121 2kj . The lower bound on R allows us to choose j \u2208 N such\n\u00011/(k+2\u03b1)\nthat 2j \u2264 n(Rk)2\n< 2j+1 which leads to\nh\n\u0002\n\u0003\n\u0001\u22122\u03b1/(k+2\u03b1)\n\u0001k/(k+2\u03b1) i\nE H 2 (s\u0303, s) \u2264 C \u2032 (Rk)2 22\u03b1 n(Rk)2\n+ n\u22121 n(Rk)2\n.\n\nThe first bound follows since 22\u03b1 \u2264 4. The second bound can be proved in the same\nway from (14).\n\n4.2. Poisson processes. Let us consider the stochastic framework corresponding\nto Problem 2 where \u03bd is dominated by some given measure M = \u03bb on X with density\ns = d\u03bd/d\u03bb. This implies that (1) holds as required. In this case, the performances\nof the PHE s\u0303 are as follows.\nTheorem 3. Assume that the family M satisfies Assumption H and the weights\n{\u2206m , m \u2208 M} are chosen so that (6) holds. Then the estimator s\u0303 defined in Section 2.3 with pen(m) \u2265 3\u03b4|m| + 6\u2206m satisfies\n\u0014\n\u0015\n\u0002\n\u0003\n\u0002\n\u0003\n(17)\nE H 2 (s\u0303, s) \u2264 390 inf H 2 (s, Sm ) + pen(m) + 3\u03a32 + \u03b5.\nm\u2208M\n\nThis theorem should be compared with the results of Reynaud-Bouret (2003) who\nuses more general families of projection estimators than just histograms based on\npartitions. Nevertheless, for the problem we consider here, her choice of the L2 -loss\ninduces some restrictions on both the intensity and the collection of partitions at\nhand. For instance, the intensity has to be bounded and the procedure requires\nsome suitable estimation of its sup-norm. As Castellan (1999), she cannot deal with\npartitions with arbitrary small length.\nLet us now apply this theorem to our families of models, systematically setting\npen(m) = 3\u03b4|m| + 6\u2206m and \u03b5 = 1. In view of facilitating the interpretation of the\nresults to follow, it is convenient to use an analogy with density estimation. This\nanalogy, based on the following heuristics, allows to extrapolate the bounds from\none framework to the other.\nWe recall that observing the Poisson process N of intensity s is equivalent to\nwith density s\u2032 , where N = N (X ) is a Poisson\nobserving N i.i.d. random variables\nR\nvariable with parameter n = X s d\u03bb and s\u2032 = n\u22121 s. With this in mind, and even\nthough n need not be an integer, we can view the estimation of s as an analogue of the\nestimation of the density s\u2032 from n i.i.d. observations.\n\u0002 2Pursuing\n\u0003 into \u0002this2 direction,\n\u0003\nwe may rewrite the risk in the\u0002 Poisson case\nas\nE\nH\n(s\u0303,\ns)\n= nE H (n\u22121 s\u0303, s\u2032 )\n\u0003\n\u0002\n\u0003\nand, setting s\u0303n = n\u22121 s\u0303, view E H 2 (s\u0303n , s\u2032 ) = n\u22121 E H 2 (s\u0303, s) as an analogue of the\n\u221a\nrisk for estimating s\u2032 from n i.i.d. observations. When s belongs to S 1 (R), S 2 (w)\nor S 3 (D, R), then the square-root of the density s\u2032 = s/n belongs to S 1 (Rn\u22121/2 ),\nS 2 (wn\u22121/2 ) or S 3 (D, Rn\u22121/2 ) respectively (provided that R2 \u2265 n in the last case,\nsince otherwise S 3 (D, Rn\u22121/2 ) would not contain any density). From these two\nremarks, we may conclude that a risk bound of the form f (R) in the Poisson case\nshould be interpreted in the density case as n\u22121 f (Rn\u22121/2 ).\nExample 1, continued. Here we deal with a Poisson process N on a finite interval\nof R, which we may assume, without loss of generality, to be [0, 1), of intensity\ns with respect to the Lebesgue measure \u03bd. To estimate s we use the family of\nmodels of Example 1 with the weights \u2206m defined in Section 3.2.2. The resulting\n\n\f16\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nPHE s\u0303 has the following properties which can be proved exactly like those given in\nPropositions 1 and 2.\nProposition 4. Let w be a modulus of continuity on [0, 1). We define xw to be the\nunique solution of the equation xw2 (x) = 1 if w(1) \u2265 1 and xw = 1 otherwise. Then\n\u0002\n\u0003\n\u221a\nE H 2 (s\u0303, s) \u2264 Cx\u22121\nfor all s such that s \u2208 S 2 (w).\nw\n\u221a\nIf, in particular, s belongs to the H\u00f6lder class H\u03b1R with R \u2265 1, then\n\u0002\n\u0003\nE H 2 (s\u0303, s) \u2264 CR2/(2\u03b1+1) .\n(18)\n\nGiven D \u2265 2 and R \u2265 2D, we get\n\u0002\n\u0003\n(19)\nE H 2 (s\u0303, s) \u2264 CD log(R/D)\n\nfor all s \u2208 S 3 (D, R).\n\u0002\n\u0003\n\u221a\nIf s belongs to S 1 (R) with R \u2265 1, then E H 2 (s\u0303, s) \u2264 CR2/3 .\n\u221a\n\u03b1 ([0, 1)) with 1 > \u03b1 > (1/p \u2212 1/2) and |\u221as|\n\u2264 R with R \u2265 1,\nIf s \u2208 Bp,\u221e\n\u03b1\n+\nBp,\u221e\n\u0002 2\n\u0003\n2/(1+2\u03b1)\nthen E H (s\u0303, s) \u2264 CR\n.\n\nR\nFor the sake of simplicity, let us assume that n = X sd\u03bb is an integer. The\nconnection established above between the estimation of a density and that of the\nintensity of a Poisson process shows that Proposition\n4 is actually a perfect analogue\n\u221a\nof Propositions 1 and 2. Namely,\nwhen\ns\nbelongs\nto S 1 (R) or S 2 (w) or s \u2208\n\u221a\nS 3 (D, R) and s\u2032 = s/n then s\u2032 respectively belongs to S 1 (Rn\u22121/2 ) or S 2 (wn\u22121/2 )\nor s\u2032 \u2208 S 3 (D, Rn\u22121 ) and the risk bounds we get for estimating the intensity s\n(with respect to the H 2 /n-loss) are the same as those obtained from a n sample for\nestimating the density s\u2032 (with the H 2 -loss).\nExample 2, continued. If we observe a Poisson process on X = Bk with intensity\ns(x) = \u03a6(kxk) with respect to the Lebesgue measure for \u03a6 some function on [0, 1)\nand consider the family of models introduced in Example 1 we obtain the risk bounds\ngiven in Proposition 4 if we replace the assumptions on s by the same on \u03a6.\nExample 3, continued. If X = [0, 1)k with k \u2265 2, we use the models and weights\ndefined in Section 3.2.4. Proceeding as for Proposition 3 we get:\n\u221a\nProposition 5. Let s belong to H\u03b1R ([0, 1)k ), then\n\u0002\n\u0003\nEs H 2 (s, s\u0303) \u2264 C(Rk \u2228 1)2k/(k+2\u03b1) .\n\u221a\n\u03b1 ([0, 1)k ) with 1 > \u03b1 > k(1/p \u2212 1/2) and |\u221as|\nIf s belongs to Bp,\u221e\n\u2264 R, then\n\u03b1\n+\nBp,\u221e\n\u0002\n\u0003\nE H 2 (s\u0303, s) \u2264 C(\u03b1, k, p)(R \u2228 1)2k/(k+2\u03b1) .\n\nAs shown by the proof of Proposition 3, we only use the partitions in MR to get\n(16) so that it would be of little\u221ause to introduce other partitions if we only wanted to\nestimate intensities such that s belong to H\u03b1R ([0, 1)k ). The interest of considering\nthe larger family M and to have a special definition of \u2206m when m \u2208 MT is that it\nallows \u221a\nto improve the results when we deal with less regular functions than those for\nwhich s belong to H\u03b1R ([0, 1)k ), in particular those functions that belong to Besov\n\u03b1 ([0, 1)k ) with 1 > \u03b1 > k/p. To illustrate this fact, let us study the\nspaces Bp,\u221e\n\u221a\nestimation of those intensities s such that s has the following specific structure.\n\n\fESTIMATING THE INTENSITY\n\n17\n\nGiven the nonempty set V which is a finite union of elements of K, there is a smallest\ninteger \uf6be\u0304 such that V can be written as the union of N elements of Kj\u0304 with a volume\nV = N 2\u2212k\uf6be\u0304 > 0. To avoid trivialities, we assume that \uf6be\u0304 > 0, hence V < 1.\n\u221a\nR\nk\nProposition 6. Let\n\u221a s be an intensity on [0, 1) such that s1lV belongs to H\u03b1 (V)\nwith R \u2265 1 while s1lV c is constant and let s\u0303 be the PHE based on the weights \u2206m\ndefined in Section 3.2.4. Then\n\u0002\n\u0003\n(20)\nE H 2 (s\u0303, s) \u2264 C inf Bm with Bm = H 2 (s, Sm ) + |m| + \u2206m\nm\u2208M\n\nand\n\n(21)\n(22)\n(23)\n\nn\nBm \u2264 C min 2k\uf6be\u0304 + V k/(k+2\u03b1) (kR)2k/(k+2\u03b1) ;\ni\nh\nV k\uf6be\u03042k\uf6be\u0304 + (kR)2k/(2\u03b1+k) [log (Rk)]2\u03b1/(2\u03b1+k) ;\nio\nh\nV 2k \uf6be\u03042k\uf6be\u0304 + (kR)2k/(2\u03b1+k) .\n\nProof: Since (20) is merely a consequence of Theorem 3 with the choice pen(m) =\n3\u03b4|m| + 6\u2206m and \u03b5 = 1, we only have to bound Bm . Let us first consider a regular\npartition m = Kj . If j < \uf6be\u0304, the bias H 2 (s, Sm ) may be arbitrarily large since\nthe intensity s may be arbitrarily large on V while it may be small on\u221aV c . For\nj \u2265 \uf6be\u0304, the argument used for the proof of Proposition 3 shows that on V, s can be\napproximated uniformly by an element of Sm with a precision at least Rk2\u2212j\u03b1 so that\n\u0002\n\u00031/(2\u03b1+k)\n\u2264 2\uf6be\u0304\nH 2 (s, Sm ) \u2264 V R2 k2 2\u22122j\u03b1 and Bm \u2264 V R2 k2 2\u22122j\u03b1 + 2kj+1 . If V R2 k2\n\u0002\n\u0003\n1/(2\u03b1+k)\nwe set j = \uf6be\u0304 and otherwise choose j so that 2j \u2264 V R2 k2\n< 2j+1 . This\nleads to (21).\n\nIf we set m = mp \u2228 K0 with p being the set of those N 2k(j\u2212\uf6be\u0304) = V 2kj \u2265 1 elements\nof Kj (j \u2265 \uf6be\u0304 \u2265 1) that exactly cover V, we get, since k \u2265 2\ni\nh\nBm \u2264 V R2 k2 2\u22122j\u03b1 + (kj + 1)V 2kj + 1 \u2264 V k R2 k2\u22122j\u03b1 + 2j2kj .\n\u0002\n\u00031/(2\u03b1+k)\nIf k2 R2 / log(kR)\n< 2\uf6be\u0304 we set j = \uf6be\u0304 and otherwise choose j so that 2j \u2264\n\u0002 2 2\n\u00031/(2\u03b1+k)\n< 2j+1 which finally leads to (22).\nk R / log(kR)\n\nTo study the approximation properties of the elements of MkT let us consider a\nparticular cube K \u2032 = K\uf6be\u0304,l \u2208 V \u2229 K\uf6be\u0304 . Identifying the partitions in MkT with the trees\nfrom which they derive, we can design an element mK \u2032 of MkT with 2k \u2212 1 terminal\nnodes at each level 1 to \uf6be\u0304 and the remaining node K \u2032 at level \uf6be\u0304. Then we keep only\nnon-terminal nodes up to level j \u2265 \uf6be\u0304, all nodes at this last level j being terminal,\nso that their number is 2k(j\u2212\uf6be\u0304) . The total number of terminal nodes of the tree is\ntherefore \uf6be\u0304(2k \u2212 1) + 2k(j\u2212\uf6be\u0304) . We can repeat this operation for each of the N cubes\nin V \u2229 K\uf6be\u0304 keeping the value of j fixed. This results in N similar trees. We finally\nconsider the smallest complete tree m that\u0002 contains the N previous\nones. Its number\n\u0003\nof terminal nodes is then bounded by N \uf6be\u0304(2k \u2212 1) + 2k(j\u2212\uf6be\u0304) so that\ni\ni\nh\nh\nBm \u2264 V (Rk)2 2\u22122j\u03b1 + 2N \uf6be\u0304(2k \u2212 1) + 2k(j\u2212\uf6be\u0304) \u2264 2V R2 k2 2\u22122j\u03b1 + \uf6be\u03042k(\uf6be\u0304+1) + 2kj .\n\u00011/(2\u03b1+k)\n\u00011/(2\u03b1+k)\nIf k2 R2\n< 2\uf6be\u0304 we set j = \uf6be\u0304 and otherwise choose j so that 2j \u2264 k2 R2\n<\n2j+1 , which leads to (23).\nA comparison of the three bounds (21), (22) and (23) shows that (23) is always\n\n\f18\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nbetter if we omit the influence of j\u0304 and k but the situation becomes more involved\nif we take into account the effect of k and j\u0304. Depending on the values of V, R,\uf6be\u0304, \u03b1\nand k, each type of partition may be the best which justifies to introduce them all.\nRemark: An analogue of Proposition 6 holds for density estimation.\n4.3. Non-negative random vectors. Let us recall from the introduction that we\nobserve an n-dimensional random vector with independent nonnegative components\nN1 , . . . , Nn and respective distributions depending on positive parameters s1 , . . . , sn .\nOne should think of the Ni as Poisson or binomial random variables with unknown\nexpectations si . More generally, we assume that there exist some known constants\n\u03ba > 0 and \u03c4 \u2265 0 such that for all i \u2208 X = {1, . . . , n}\n\u0014\n\u0014\ni\u0011\n\u0010 h\n1\nz 2 si\nz(Ni \u2212si )\nfor all z \u2208 0,\n,\n\u2264\u03ba\n(24)\nlog E e\n2(1 \u2212 z\u03c4 )\n\u03c4\nwith the convention 1/\u03c4 = +\u221e if \u03c4 = 0, and\ni\u0011\n\u0010 h\nz 2 si\nfor all z \u2265 0.\n(25)\nlog E e\u2212z(Ni \u2212si ) \u2264 \u03ba\n2\nIn the case of Poisson or binomial random variables, one can take \u03ba = \u03c4 = 1 as we\nshall see below.\n\nOur aim is to estimate the function s from X to R+ given by s(i) = si . Here\nwe denote\nM = \u03bb and\nR\nP by \u03bb the counting measure on X and set Y \u2261 1. Hence\nN (A) = i\u2208A Ni . Then L can be identified with Rn+ , E [N (A)] = A sd\u03bb as required\ni2\np\nP hp\nt(i) \u2212 t\u2032 (i) for t, t\u2032 \u2208 L.\nand H 2 (t, t\u2032 ) = ni=1\n\nTheorem 4. Assume that (24) and (25) hold, that the family M satisfies Assumption H\u0002 and the weights\n{\u2206m , m\n\u0001\n\u0003 \u2208 M} are chosen so that (6) holds. Let\npen(m) \u2265 \u03ba \u03b4 1 + K 2 |m| + 3K 2 \u2206m with\nr\n\u221a\n\u221a\n2\n\u03c4\n1\nK=\n+\n\u2212\nif \u03c4 > \u03ba;\nK = 2 if \u03c4 \u2264 \u03ba;\n2\n\u03ba 2\nand let s\u0303 be the PHE defined in Section 2.3. Then\n\u0014\n\u0015\n\u0002 2\n\u0003\n\u0002 2\n\u0003\n2 2\nE H (s\u0303, s) \u2264 390 inf H (s, Sm ) + pen(m) + (3/2)\u03baK \u03a3 + \u03b5.\nm\u2208M\n\nLet us first check that some classical distributions do satisfy Inequalities (24)\nand (25). If Ni is a binomial random variable with parameters ni , pi then for all\nz \u2208 R,\ni\u0011\n\u0010 h\n(26)\nlog E ez(Ni \u2212si ) \u2264 si (ez \u2212 z \u2212 1) with si = ni pi .\n\nIf Ni is a Poisson random variable with parameter si , then equality holds in (26).\nUsing the bounds ez \u2212z\u22121 \u2264 z 2 /[2(1\u2212z)] for z \u2208 [0, 1[ and ez \u2212z\u22121 \u2264 z 2 /2 for z < 0\nwe derive that, in both cases, (24) and (25) hold with \u03ba = \u03c4 = 1. If Ni has a Gamma\ndistribution \u0393(si , 1), E [Ni ] = si and, following the proof of Lemma 1 of Laurent and\nMassart (2000), we deduce that (24) and (25) hold again with \u03ba = \u03c4 = 1. More\ngenerally, it follows from some version of Bernstein's Inequality - see Lemma 8 of\nBirg\u00e9 and Massart (1998) - that (24) holds as soon as\np!\nfor all i \u2208 X and p \u2265 2.\nE [(Ni )p ] \u2264 \u03ba si \u03c4 p\u22122 ,\n2\n\n\fESTIMATING THE INTENSITY\n\n19\n\nInequality (25) is always satisfied if Ni \u2264 \u03ba. Indeed it follows from\ne\u2212zx \u2264 1 \u2212 zx + z 2 x2 /2, \u2200x, z \u2265 0\n\nthat all non-negative random variables X bounded by \u03ba satisfy\n\u0001\n\u0002\n\u0003\nz 2 E[X 2 ]\n\u2264 exp \u2212zE[X] + \u03baz 2 E[X]/2 .\nE e\u2212zX \u2264 1 \u2212 zE[X] +\n2\nThe results of Kolaczyk and Nowak (2004), which are based on some sort of discretized penalized maximum likelihood estimator in the spirit of Barron and Cover\n(1991), have some similarity with ours but they assume that the components of the\nvector s belong to some known interval [c, C], c > 0 and they explicitely use the\nvalues of c and C in the construction of their estimator. Such an assumption, which\nimplies, as in the case of density estimation, that squared Hellinger distance and\nKullback divergence are equivalent also greatly simplifies the estimation problem.\nExample 4, continued. Setting\n\u0002\n\u0001\n\u0003\n(27)\npen(m) = \u03ba 1 + K 2 |m| + 3K 2 \u2206m\nand\n\u03b5 = 1.\n\u0001\nn\u22121\nand using log D\u22121\n\u2264 (D\u22121)(1+log[(n\u22121)/(D\u22121)]) with the convention 0 log((n\u2212\n1)/0) = 0 we get the risk bound\n\u0013\u001b\n\u001a\n\u0012\n\u0002 2\n\u0003\nn\u22121\n2\n.\n(28) E H (s\u0303, s) \u2264 C(\u03ba, K) inf H (s, Sm ) + |m| + (|m| \u2212 1) log\nm\u2208M\n|m| \u2212 1\n\nIf, for instance, s itself belongs to some Sm with a small value of |m|, which corresponds to a piecewise stationary process (Ni )1\u2264i\u2264n with a few distribution changes,\nthe risk is bounded by C(\u03ba, K)|m| log n.\n\nAnother interesting situation corresponds to the case of a monotone sequence\n(si )1\u2264i\u2264n , i.e. a monotone function s on X that we may assume, without loss of\ngenerality to be nondecreasing.\n\u221a\n\u221a\nProposition 7. Let the sequence si , 1 \u2264 i \u2264 n be nondecreasing with sn \u2212 s1 = R,\nthen the PHE s\u0303 based on the models of Example 4 with pen and \u03b5 given by (27)\nsatisfies the following risk bounds with a constant C depending only on \u03ba and K:\n\u0002\n\u0003\n\u0001\n\u2022 if R2 \u2264 n\u22121 log n, then E H 2 (s\u0303, s) \u2264 C(\u03ba, K) nR2 + 1 ;\n\u221a\n\u0002 2\n\u0003\n\u2022 if R \u2265 n/ 3, then E H (s\u0303, s) \u2264 C(\u03ba, K)n;\n\u0002\n\u0003\n\u221a\n2/3\n\u2022 otherwise E H 2 (s\u0303, s) \u2264 C(\u03ba, K) [R n log(n/R)] .\n\nRemark: If we restrict ourselves to the case n P\n= 2k , we can turn any function s on\nX into a function s\u2032 on [0, 1) by setting s\u2032 = ni=1 s(i)1l[(i\u22121)2\u2212k ,i2\u2212k ) . This transformation will, in particular, preserve the monotonicity properties of the functions.\nOne could then estimate s\u2032 using the more sophisticated families of weights that we\nintroduced in Section 3.2.2. The use of this strategy would improve the estimation\nof monotone functions, removing the logarithmic factors.\n\nExample 5, continued. Choosing pen and \u03b5 as in (27) and using the same arguments\nas for Example 1, we derive an analogue of (28) with n replacing n \u2212 1 in the\nlogarithmic factor. If we assume that si = s for i 6\u2208 I with |I| = k, then H 2 (s, Sm ) =\n0 for some m \u2208 Mk and\n\u0002\n\u0003\nE H 2 (s\u0303, s) \u2264 C(\u03ba, K)[k + 1 + k log(n/k)].\n\n\f20\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\n5. Special counting processes on the line\nLet X be some interval of R+ of the form [0, \u03b6) where 0 < \u03b6 \u2264 +\u221e with its Borel\ne on X is a cadlag\n\u03c3-algebra A. We recall that a (univariate) counting process N\n(right-hand continuous and left-hand limited) process from X to R+ , vanishing at\ntime t = 0, with piecewise constant and nondecreasing paths having jumps of size\n+1 only. The use of counting processes in statistical modeling is developed in great\ndetails in the book by Andersen et al. (1993) where the interested reader will find\net\nmany concrete situations for which these processes naturally arise. Typically, N\ncounts the number of occurrences of a certain event from time 0 up to time t.\nThe jumping times of the process give the dates of occurrence of the event. A\ncounting process can be associated to a random measure N on X whose cumulative\net for all t \u2208 X .\ndistribution function is the counting process itself, i.e. N ([0, t]) = N\ne and its\nIn the sequel, we shall not distinguish between the counting process N\nassociated measure N .\n\nIn this paper, we consider a phenomenon which is described by some bounded\ncounting process N \u2217 on X such that N \u2217 (X ) \u2264 k a.s. for some known integer k.\nThis means that N \u2217 describes an event that occurs at most k times during the\nperiod X . We also assume that there exist a deterministic measure \u03bb on X , a\ndeterministic nonnegative function s \u2208 L1 (X , d\u03bb) and a nonnegative observable\nprocess Y \u2217 bounded by 1 on X such that\n\u2217\n\n(29)\n\nE [N ([0, t])] = E\n\n\u0014Z\n\nt\n\n\u2217\n\nsY d\u03bb\n0\n\n\u0015\n\nfor all t \u2208 X .\n\nWe actually observe an aggregated counting process N which is the sum of n i.i.d.\nprocesses N j , j = 1, . . . , n with the same distribution as N \u2217 . The fact that the\nmeasure N j is determined by its cumulative distribution function and (29) imply\nthat there are i.i.d. observable processes Y j , j \u2208 {1, . . . , n} with the distribution of\nY \u2217 such that\n\u0002\n\u0003\nE N j (A) = E\n\n\u0014Z\n\nsY j d\u03bb\nA\n\n\u0015\n\nfor all A \u2208 A\n\nTherefore (1) holds with M = Y d\u03bb and Y =\nwe can prove the following result.\n\nPn\n\nj=1 Y\n\nj.\n\nand 1 \u2264 j \u2264 n.\nFor such counting processes,\n\nTheorem 5. Assume that there exist a positive integer k and a positive\nnum\u0002R\n\u0003\n\u2032\n\u2217\n\u2217\nber \u0002R\n\u03ba , both known,\nsuch that N (X ) \u2264 k a.s., (29) holds andR Var I sY d\u03bb \u2264\n\u0003\n\u03ba\u2032 E I sY \u2217 d\u03bb for all intervals I \u2282 X . Assume moreover that X sd\u03bb < +\u221e and\nthe aggregated process N satisfies (2). Let us choose a family M satisfying Assumption H and weights {\u2206\u2032m , m \u2208 M} such that\n(30)\n\nX\n\nm\u2208M\n\nexp[\u2212\u03b7\u2206\u2032m ] = \u03a3\u2032 (\u03b7) < +\u221e\n\n\u0013\u22121\n\u0012\nZ\n.\nsd\u03bb\nfor \u03b7 = k k +\nX\n\n\fESTIMATING THE INTENSITY\n\n21\n\nThen the estimator \u015dm\u0302 defined in Section 2.3 with pen(m) \u2265 16\u03b4|m|(k+\u03ba\u2032 )+404k\u2206\u2032m\nsatisfies\n\u0002\n\u0003\nE H 2 (\u015dm\u0302 , s)\n\u0012 \u0014\n\u0015\n\u0013\n\u0001\n2\n\u22121 \u2032\n2\n\u2264 390 E inf H (s, Sm ) + pen(m) + 404k\u03b7 [\u03a3 (\u03b7)] + \u03b5\nm\u2208M\n\u0012\n\u0013\n\b \u0002 2\n\u0003\n\u22121 \u2032\n2\n\u2264 390 inf E H (s, Sm ) + pen(m) + 404k\u03b7 [\u03a3 (\u03b7)] + \u03b5.\nm\u2208M\n\n\u0002\n\u0003\nIn the last bound, E H 2 (s, Sm ) plays the role of a bias term which can be\nbounded in the following way. Let us set\n(\n)\nX\n\\\n\u2032\nSm\n= t=\ntI 1lI with tI \u2265 0 for all I \u2208 m\nL,\nI\u2208m\u2229J\n\n\u2032 \u2282 S , hence H 2 (s, S ) \u2264 H 2 (s, S \u2032 )\nwhere the tI are now deterministic. Then Sm\nm\nm\nm\n\u2032 ,\nand, for t \u2208 Sm\nZ \u0010\nZ \u0010\n\u221a \u00112\n\u221a \u00112\n\u221a\n\u221a\nH 2 (s, t) =\ns \u2212 t Y d\u03bb \u2264 n\ns \u2212 t d\u03bb,\nX\n\nX\n\nsince Y \u2264 n. Finally\nZ \u0010\n\u221a \u00112\n\u0002 2\n\u0003\n\u221a\nE H (s, Sm ) \u2264 n inf\u2032\ns \u2212 t d\u03bb = b2m (s)\nt\u2208Sm\n\nand\n\n\u0012\n\nX\n\n\u0013\n\b 2\n404k \u2032\nbm (s) + pen(m) +\n[\u03a3 (\u03b7)]2 + \u03b5.\nm\u2208M\n\u03b7\nNote that the present framework includes, as a particular case, density estimation, if\nwe observe an n-sample X1 , . . . , Xn with density s with respect to \u03bb and set N j (A) =\n\u221a \u00012\nR \u221a\ns \u2212 t d\u03bb which corresponds to\n1lA (Xj ). Then Y = n and H 2 (s, t) = n X\n\u221a\nusing the distance H of Section 4.1 multiplied by n. Up to this scaling factor, the\nprevious risk bound is analogue to that for estimating densities we get in Theorem 1.\n\u0002\n\u0003\nE H 2 (\u015dm\u0302 , s) \u2264 390\n\ninf\n\nIn order to derive risk bounds which are similar to those given in Proposition 1,\nwe have to distinguish between two\nR situations. The most favorable one occurs when\nwe know an upper bound \u0393 for X s d\u03bb, in which case, since 0 \u2264 Y \u2217 \u2264 1,\n\"\u0012Z\n\u0015\n\u0013 \u0014Z\n\u00132 # \u0012Z\n\u0015\n\u0014Z\n\u2217\n\u2217\n\u2217\nsY d\u03bb\ns d\u03bb E\n\u2264\nsY d\u03bb\nsY d\u03bb \u2264 E\nVar\nI\n\n\u03ba\u2032\n\nI\n\nX\n\nI\n\nand we can\nset\n= \u0393. Moreover, assuming that (6) holds, we can choose \u2206\u2032m =\n\u0001\n1 + k\u22121 \u0393 \u2206m without any further restriction on the family of models. Using the\nsame family of partitions as in the density case, we recover the bounds of Propositions 1 and 2 up to the factor n corresponding to the rescaling of the distance\nH.\nR\nLet us now turn to the less favorable situation where no bound for X s d\u03bb is\nknown, which is the typical case for Problem 4. As we shall see the number \u03ba\u2032 can\nstill be computed. As to (30) it will be satisfied with \u2206\u2032m = |m| as soon as the\nnumber of models such that |m| = D is bounded independently of D. Restricting\nourselves to the family MR of regular partitions, we recover, up to the factor n, the\nbounds provided by case ii) of Proposition 1.\n\n\f22\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\n5.1. Survival analysis with right-censored data. Let us now consider the framework of Problem 3, denoting by PT the common distribution\nconPn ofj the Ti . We\nj (A) =\nsider the counting process N on R+ defined by N =\nN\nwhere\nN\nj=1\n1l{Tej \u2208A, Dj =1} for all measurable subsets A of R+ , so that we can take k = 1. Then\nthe variables N j (A), 1 \u2264 j \u2264 n are i.i.d. Bernoulli random variables. We define s to\nbe the hazard rate of the survival times, i.e. s(t) = p(t)/P[T1 \u2265 t] for t > 0. Since\ns is not integrable on R+ we shall restrict ourselves to some bounded interval X\nof R+ , which we can take, without loss of generality, to be [0, 1) if we assume that\nP[T1 \u2265 1] > 0. We also assume here that the censorship satisfies for all t \u2265 0,\n\u0015\n\u0014Z t\n\u0002 j\n\u0003\nj\ns(u)Y (u)du , with Y j (t) = 1lTej \u2265t ,\n(31)\nE N ([0, t]) = E\n0\n\nwhich means that (29) holds. Equality (31) is clearly satisfied when Cj = Tj for\nall j, i.e. when the data are uncensored. It is also satisfied when the censorship\nis independent of the survival time, i.e. when Cj and Tj are independent for all j.\nIndeed, we then have for all j and t \u2265 0, by Fubini Theorem and independence,\n\u0015\n\u0015\n\u0014Z t\n\u0014Z t\np(u)\n1lCj \u2265u 1lTj \u2265u du\ns(u)Y j (u)du = E\nE\n0 P(Tj \u2265 u)\n0\nZ t\np(u)P(Tj \u2265 u)P(Cj \u2265 u)\n=\ndu\nP(Tj \u2265 u)\n0\nZ\n=\n1l[0,t] (u)P(Cj \u2265 u)dPT (u)\n\u0002\n\u0003\n= P [Tj \u2264 t, Tj \u2264 Cj ] = E N j ([0, t]) .\nProposition 8. If the processes\nN j satisfy (31), the assumptions of Theorem 5 hold\nR\n\u2032\nwith k = 1, \u03ba = 2 and X sd\u03bb = \u2212 log(P[T1 \u2265 1]).\n\nFrom a practical point of view, one can always\nestimate P[T1 \u2265 1] accurately\nR\nenough to assume that an upper bound \u0393 for X sd\u03bb is known. We can therefore\napply Theorem 5 to the the family of models of Example 1 with the weights \u2206m\ngiven in Section 4.1, setting \u2206\u2032m = (1 + \u0393)\u2206m . We then obtain perfect analogues of\nPropositions 1 and 2 with constants C now depending on \u0393. To avoid redundancy,\nwe leave the precise statement of the risk bounds to the reader.\n5.2. Transition intensities of Markov processes. Within the framework of\nProblem 4, we associate to T1,0 the counting process N \u2217 defined for t \u2265 0 by\nN \u2217 ([0, t]) = 1l{T1,0 \u2264t} so that\n\u0015\n\u0014Z t\nZ t\n\u2217\n1l{Xu\u2212 =1} s(u)du\np(u)du = E\n(32)\nE [N ([0, t])] =\n0\n\n\u2217 (u)\n\n0\n\nand (29) holds with Y\n= 1l{Xu\u2212 =1} . Our aim here is to estimate s on some\nbounded\ninterval\nX\nof\nR\nfrom the observation of the counting process N =\n+\nPn\nj where the N j 's are i.i.d. copies of N \u2217 associated to n i.i.d. copies X 1 , . . . , X n\nN\nj=1\nof the process X. If X takes only the two values 0 and 1 and a.s. starts from 1 to\nreach 0, then the problem reduces to estimating the density p of T1,0 ; it becomes\nnovel when we have at least three states. In any case, we get the following result.\nP\nProposition\n9. If the weights \u2206\u2032m satisfy m\u2208M exp[\u2212\u03b7\u2206\u2032m ] < +\u221e for all \u03b7 > 0\nR\nand X s(t)dt < +\u221e then Theorem 5 applies with k = 1 and \u03ba\u2032 = 2.\n\n\fESTIMATING THE INTENSITY\n\n23\n\n6. A unifying result\nWe want here to analyze our estimation procedure from the general point of view\ndescribed in Section 2 and prove a risk bound for the estimator s\u0303, from which we\nshall be able to derive the previous risk bounds corresponding to all the specific\nframeworks that we considered. For this we introduce the following approximation\nfor s in Sm :\nZ\nX sI\n1lI with sI = s d\u03bb.\nsm =\n(33)\n\u03bb(I)\nI\nI\u2208m\u2229J\n\nWe need here a bound for H 2 (\u015dm , sm ) which holds uniformly for m \u2208 M. It takes\nthe following form:\n\nH' : There exist three positive constants a, b and c, c \u2265 1 such that, for any m \u2208 M,\n(34)\n\n\u0003\n\u0002\nP H 2 (\u015dm , sm ) \u2265 c|m| + bz \u2264 a exp[\u2212z] for all z \u2265 0.\n\nWe can now derive bounds for the risk of the estimator s\u0303 defined in Section 2.3.\nTheorem 6. Let Assumptions H and H' hold and the weights \u2206m satisfy (6). Let\nthe penalty pen(m) be given by\npen(m) \u2265 c\u03b4|m| + b\u2206m .\n\n(35)\n\nand m\u0302 be any element of M satisfying (5). Then the estimator s\u0303 = \u015dm\u0302 satisfies\n\u0012 \u0014\n\u0015\n\u0013\n\u0001\n\u0002 2\n\u0003\n2\n2\n(36)\nE H (s\u0303, s) \u2264 390 E inf H (s, Sm ) + pen(m) + ab\u03a3 /2 + \u03b5.\nm\u2208M\n\nNote that such a result has been obtained without any assumption on the underlying space X and the true value s of the parameter, apart from the fact that it\nbelongs to L. Note also that in (36), the infimum over m \u2208 M occurs inside the\nexpectation, which makes a difference when M , and therefore H(s, Sm ), is random.\nAs we have previously seen, \u03b4 \u2264 2 for all the models we consider. Moreover, we\nshall see in Sections 7.3.1, 7.4.1 and 7.5.1 that for Problems 0, 1 and 2, a = 1 and b\nand c take the form b = b\u2032 CP and c = c\u2032 CP where b\u2032 and c\u2032 are numerical constants\nand CP depends of the problem we consider (for instance CP = n\u22121 for density\nestimation). If we choose pen(m) = c0 CP (|m| + \u2206m ) for some suitable numerical\nconstant c0 and \u03b5 \u2264 CP , it follows that (36) becomes\n\u0002\n\u0003\nE H 2 (s\u0303, s)\n\u0012 \u0014\n\u0015\n\u0013\n\u0001\n2\n\u2032\n2\n\u2264 390 E inf H (s, Sm ) + c0 CP (|m| + \u2206m ) + 2b CP \u03a3 /2 + CP ,\nm\u2208M\n\nwhich gives (7). If there is only one model m in the family M, we can fix \u2206m = 0,\nhence \u03a3 = 1, which leads to (3).\nProof. Let m\u2217 be an arbitrary element of M. It follows from the definition of D\nthat for any m \u2208 M, H 2 (\u015dm , \u015dm\u2217 ) \u2264 D(m) \u2228 D(m\u2217 ). Therefore,\n(37)\n\nH 2 (\u015dm\u0302 , \u015dm\u2217 ) \u2264 D(m\u0302) \u2228 D(m\u2217 ) \u2264 D(m\u2217 ) + \u03b5/3,\n\nby (5). It also follows from (4) that, if Tm,m\u2217 \u2264 0, then\n(38)\n\nH 2 (\u015dm , \u015dm\u2228m\u2217 ) \u2212 H 2 (\u015dm\u2217 , \u015dm\u2228m\u2217 ) \u2264 16[pen(m\u2217 ) \u2212 pen(m)].\n\n\f24\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nMoreover\nH 2 (\u015dm , \u015dm\u2228m\u2217 ) \u2212 H 2 (\u015dm\u2217 , \u015dm\u2228m\u2217 )\nZ\nZ\nZ \u0010p\np \u0011p\n\u015dm\u2217 \u2212 \u015dm\n\u015dm\u2228m\u2217 d\u03bb\n=\n\u015dm d\u03bb \u2212 \u015dm\u2217 d\u03bb + 2\nZ \u0010p\n\u0011\np \u0011 \u0010p\np\n= H 2 (\u015dm , \u015dm\u2217 ) + 2\n\u015dm\u2217 \u2212 \u015dm\n\u015dm\u2228m\u2217 \u2212 \u015dm\u2217 d\u03bb,\n\nhence, by (38) and Cauchy-Schwarz Inequality,\nH 2 (\u015dm , \u015dm\u2217 )\n\u2264 16[pen(m\u2217 ) \u2212 pen(m)] + 2\n\nZ \u0010p\n\n\u015dm \u2212\n\np\n\n\u015dm\u2217\n\n\u0011 \u0010p\n\n\u015dm\u2228m\u2217 \u2212\n\np\n\n\u2264 16[pen(m\u2217 ) \u2212 pen(m)] + 2H(\u015dm , \u015dm\u2217 )H(\u015dm\u2228m\u2217 , \u015dm\u2217 )\n1\n\u2264 16[pen(m\u2217 ) \u2212 pen(m)] + H 2 (\u015dm , \u015dm\u2217 ) + 4H 2 (\u015dm\u2228m\u2217 , \u015dm\u2217 ).\n2\nTherefore, for any m \u2208 M such that Tm,m\u2217 \u2264 0,\nand, since\n\n\u0011\n\u015dm\u2217 d\u03bb\n\nH 2 (\u015dm , \u015dm\u2217 ) \u2264 8H 2 (\u015dm\u2228m\u2217 , \u015dm\u2217 ), +32[pen(m\u2217 ) \u2212 pen(m)]\n\nH 2 (\u015dm\u2228m\u2217 , \u015dm\u2217 )\n\u0002\n\u0003\n\u2264 4 H 2 (\u015dm\u2228m\u2217 , s\u0304m\u2228m\u2217 ) + H 2 (s\u0304m\u2228m\u2217 , s) + H 2 (s, s\u0304m\u2217 ) + H 2 (s\u0304m\u2217 , \u015dm\u2217 ) ,\n\nthen\n(39)\n\n(1/32)H 2 (\u015dm , \u015dm\u2217 ) \u2264 H 2 (\u015dm\u2228m\u2217 , s\u0304m\u2228m\u2217 ) + H 2 (\u015dm\u2217 , s\u0304m\u2217 ) + pen(m\u2217 )\n\u2212 pen(m) + H 2 (s\u0304m\u2228m\u2217 , s) + H 2 (s, s\u0304m\u2217 ).\n\nLet us set, for all z \u2265 0 and (m, m\u2032 ) \u2208 M2 ,\n\\\n\b\n\u03c9 \u2208 \u03a9 H 2 (\u015dm\u2228m\u2032 , s\u0304m\u2228m\u2032 ) \u2264 c|m \u2228 m\u2032 | + b[\u2206m + \u2206m\u2032 + z] .\n\u03a9z =\n(m,m\u2032 )\u2208M2\n\nIt follows from (34) that\n(40)\n\nP [\u03a9cz ] \u2264 ae\u2212z\n\nX\n\ne\u2212\u2206m \u2212\u2206m\u2032 = \u03a32 ae\u2212z .\n\n(m,m\u2032 )\u2208M2\n\nLet now \u03c9 belong to \u03a9z . It then follows that\n(41)\n\nH 2 (\u015dm\u2217 , sm\u2217 ) \u2264 c|m\u2217 | + 2b\u2206m\u2217 + bz \u2264 2 pen(m\u2217 ) + bz\n\nand, using Assumption H, that\n\nH 2 (\u015dm\u2228m\u2217 , s\u0304m\u2228m\u2217 ) \u2264 c\u03b4[|m| + |m\u2217 |] + b[\u2206m + \u2206m\u2217 + z].\n\nTherefore we derive from (39), (41) and (35) that, for all m \u2208 M such that Tm,m\u2217 \u2264\n0,\n(1/32)H 2 (\u015dm , \u015dm\u2217 ) \u2264 H 2 (s\u0304m\u2228m\u2217 , s) + H 2 (s, s\u0304m\u2217 ) + (1 + \u03b4)c|m\u2217 |\n+ 3b\u2206m\u2217 + 2bz + pen(m\u2217 )\n\u2264 H 2 (s\u0304m\u2228m\u2217 , s) + H 2 (s, s\u0304m\u2217 ) + 2bz + 4 pen(m\u2217 ).\n\nIn order to control the bias terms H 2 (s, s\u0304m\u2032 ) of the various estimators involved in\nthe construction of s\u0303, we shall use Lemma 2 below. Since Sm\u2228m\u2217 \u2283 Sm\u2217 for all\nm \u2208 M, this lemma implies that\nH 2 (s\u0304m\u2032 \u2228m\u2217 , s) \u2264 2H 2 (s, Sm\u2032 \u2228m\u2217 ) \u2264 2H 2 (s, Sm\u2217 ),\n\n\fESTIMATING THE INTENSITY\n\ntherefore\n\n25\n\n\u0002\n\u0003\nH 2 (\u015dm , \u015dm\u2217 ) \u2264 128 H 2 (s, Sm\u2217 ) + pen(m\u2217 ) + bz/2 ,\n\nfor all m \u2208 M such that Tm,m\u2217 \u2264 0 and we conclude from (37) and the definition of\nD that, if \u03c9 \u2208 \u03a9z ,\n\u0002\n\u0003\nH 2 (\u015dm\u0302 , \u015dm\u2217 ) \u2264 D(m\u2217 ) + \u03b5/3 \u2264 128 H 2 (s, Sm\u2217 ) + pen(m\u2217 ) + bz/2 + \u03b5/3.\n\nSince\n\n\u0002\n\u0003\nH 2 (\u015dm\u0302 , s) \u2264 3 H 2 (\u015dm\u0302 , \u015dm\u2217 ) + H 2 (\u015dm\u2217 , sm\u2217 ) + H 2 (s\u0304m\u2217 , s) ,\n\nit follows from (41) and Lemma 2 that\n\u0002\n\u0003\nH 2 (\u015dm\u0302 , s) \u2264 3 130H 2 (s, Sm\u2217 ) + 130 pen(m\u2217 ) + 65bz + \u03b5/3 .\n\nSince m\u2217 is arbitrary in M we finally get\n\u0013\n\u0012\n\u0002 2\n\u0003\n2\nH (\u015dm\u0302 , s)1l\u03a9z \u2264 390 inf H (s, Sm ) + pen(m) + bz/2 + \u03b5.\nm\u2208M\n\nAn integration with respect to z taking (40) into account leads to (36).\n\n\u0003\n\nLemma 2. Within the framework of Section 2.1, for any f \u2208 L, we have\n\u0013\nX \u0012Z\nd\u03bb\n2\n2\n \u0304\n \u0304\nf\nH (f, fm ) \u2264 2H (f, Sm ) with fm =\n1lI .\nI \u03bb(I)\nI\u2208m\u2229J\n\nProof. Let X \u2032 =\nt \u2208 Sm ,\n\nS\n\nI\u2208m\u2229J\n\nI. Note that M is a finite measure on X \u2032 and that for all\n2\n\n2\n\nH (f, t) = H (f 1lX \u2032 , t) +\n\nZ\n\nf d\u03bb.\n\nX \\X \u2032\n\nIt is therefore enough to show the result for X \u2032 in place of X and f 1lX \u2032 in place of\nf and we can restrict ourselves to the\n\u221a case where M is a finite measure\n\u221a \u2032 on X . Let\n\u221a\nf R\u2032 be\nthe\nL\n(X\n,\nd\u03bb)\nprojection\nof\nf\non\nS\n.\nSince\nthe\nvalue\nof\nf on I is given\n2\nm\n\u221a\nby I f d\u03bb/\u03bb(I), it suffices to prove that for each I \u2208 m \u2229 J\nZ\n\n(42)\n\nI\n\np\n\nf\u2212\n\nsZ\n\nd\u03bb\nf\nI \u03bb(I)\n\n!2\n\nd\u03bb \u2264 2\n\nZ \u0012p\nI\n\nf\u2212\n\nZ p\nI\n\nd\u03bb\nf\n\u03bb(I)\n\n\u00132\n\nd\u03bb.\n\nBy homogeneity, we may assume that \u03bb(I) = 1. Expanding the left-hand side of (42)\nwe get\nZ\n\nI\n\np\n\nf\u2212\n\nsZ\n\nI\n\n!2\n\nf d\u03bb\n\nZ\n\nd\u03bb = 2\n\nwhich, together with the inequality\n\nI\n\nqR\n\nI\n\nf d\u03bb \u2212\n\nf d\u03bb \u2265\n\nZ p\n\nR \u221a\nI\n\nI\n\nf d\u03bb \u00d7\n\nsZ\n\n!\n\nf d\u03bb ,\nI\n\nf d\u03bb, leads to the desired result.\n\u0003\n\n\f26\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\n7. Proofs\n7.1. Proof of Lemma 1. Let\u0001 m = mp \u2228 Kj and m\u2032 =\u0001 mp\u2032 \u2228 Kj \u2032 be two elements\nof M and I \u0304p = [0, 1)k \\ \u222aI\u2208p I , I \u0304p\u2032 = [0, 1)k \\ \u222aI \u2032 \u2208p\u2032 I \u2032 . Assuming, with no loss of\ngenerality, that j \u2265 j \u2032 , we get\nm \u2228 m\u2032 = mp \u2228 mp\u2032 \u2228 Kj \u2228 Kj \u2032 = mp \u2228 mp\u2032 \u2228 Kj = m1 \u222a m2 \u222a m3 \u222a m4 ,\n\nwith\nm2\nm3\nm4\n\n\b\n\nK \u2229 I \u2229 I \u2032 6= \u2205 | K \u2208 Kj , I \u2208 p, I \u2032 \u2208 p\u2032 ;\n\b\n= K \u2229 I \u2229 I \u0304p\u2032 6= \u2205 | K \u2208 Kj , I \u2208 p ;\n\b\n= K \u2229 I \u0304p \u2229 I \u2032 6= \u2205 | K \u2208 Kj , I \u2032 \u2208 p\u2032 ;\n\b\n= K \u2229 I \u0304p \u2229 I \u0304p\u2032 6= \u2205 | K \u2208 Kj .\n\nm1 =\n\nSince j < J(p), hence p \u2282 \u222al>j Kl , for K \u2208 Kj and I \u2208 p, K \u2229 I is either I or \u2205,\nso that m = p \u222a pj with pj = {K \u2229 I \u0304p 6= \u2205, K \u2208 Kj } and |m| = |p| + |pj |. It also\nfollows that |m1 | \u2264 |p| + |p\u2032 | and |m2 | \u2264 |p|. Then, given K \u2208 Kj and I \u2032 \u2208 p\u2032 , K \u2229 I \u2032\nis either K or I \u2032 or \u2205 since K, I \u2032 \u2208 K, so that |m3 | \u2264 |pj | + |p\u2032 |. Finally |m4 | \u2264 |pj |\nand\n\u0001\n|m \u2228 m\u2032 | \u2264 2 |p| + |p\u2032 | + |pj | \u2264 2(|m| + |m\u2032 |).\n\n7.2. Some large deviations inequalities. The proofs of Theorems 1, 3, 4 and 5\nrequire to check (34) for each specific framework. Since\nX \u0010p\n\u221a \u00112\nN (I) \u2212 sI\nfor all m \u2208 M,\n(43)\nH 2 (\u015dm , s\u0304m ) =\nI\u2208m\u2229J\n\nthis amounts to proving some deviation results for quantities of the form\nX \u0010p\n\u221a \u00112\nN (I) \u2212 sI \u2212 c|m|\nI\u2208m\u2229J\n\nwhich is the purpose of this section. Throughout it, we consider a finite set of\nnon-negative random variables XI with I \u2208 m and the related quantities\n\u00112\nX \u0010p\np\n(44)\n\u03c72 (m) =\nXI \u2212 E [XI ] ,\nI\u2208m\n\nthe notation suggesting that these variables behave roughly like \u03c72 random variables\nas we shall see. Our purpose will be to derive deviation bounds for those variables\nfrom their expectation. Our first result is as follows:\n\nTheorem 7. Let (XI )I\u2208m be a finite set of independent non-negative random variables and \u03c72 (m) be given by (44). We assume that there exists \u03ba > 0 and \u03c4 \u2265 0\nsuch that\ni\u0011\n\u0010 h\nz 2 E [XI ]\n(45)\nlog E ez(XI \u2212E[XI ]) \u2264 \u03ba\nfor all z \u2208 [0, 1/\u03c4 [,\n2(1 \u2212 z\u03c4 )\n\nand\n\n(46)\n\ni\u0011\n\u0010 h\nz 2 E [XI ]\nlog E e\u2212z(XI \u2212E[XI ]) \u2264 \u03ba\n2\n\nfor all z > 0.\n\n\fESTIMATING THE INTENSITY\n\nLet\nK = max\n\n(\n\u221a\n\n2;\n\n\u221a\n\n2\n+\n2\n\ns\u0012\n\n\u03c4\n1\n\u2212\n\u03ba 2\n\n27\n\n\u0013 )\n\n.\n\n+\n\nThen for all x > 0,\n\u0011i\nh\n\u0010 p\n\u0002 2\n\u0003\n2\n2\n(47)\nP \u03c7 (m) \u2265 E \u03c7 (m) + K \u03ba 2 2|m|x + x \u2264 e\u2212x ,\n\nand\n\nh\ni\np\n\u0002\n\u0003\nP \u03c72 (m) \u2264 E \u03c72 (m) \u2212 2K 2 \u03ba 2|m|x \u2264 e\u2212x .\n\n(48)\n\nProof. Let us first introduce the following large deviation result, the proof of which\nfollows the lines of the proof of Lemma 8 of Birg\u00e9 and Massart (1998).\nLemma 3. Let Y1 , . . . , Yn be n independent, centered random variables. If\n\nthen\n\n\u0002\n\u0003\u0001\nlog E ezYi \u2264 \u03ba\n\uf8ee\n\nP\uf8f0\n\nn\nX\ni=1\n\nYi \u2265\n\nz 2 \u03b8i\n2(1 \u2212 z\u03c4 )\n\n2\u03bax\n\nn\nX\ni=1\n\n\u03b8i\n\nfor all z \u2208 [0, 1/\u03c4 [\n!1/2\n\n\uf8f9\n\n+ \u03c4 x\uf8fb \u2264 e\u2212x\n\nand\n\n1 \u2264 i \u2264 n,\n\nfor all x > 0.\n\n\u0002\n\u0003\u0001\nIf, for 1 \u2264 i \u2264 n and all z > 0, log E e\u2212zYi \u2264 \u03baz 2 \u03b8i /2, then\n\uf8ee\n!1/2 \uf8f9\nn\nn\nX\nX\n\uf8fb \u2264 e\u2212x for all x > 0.\n\u03b8i\nYi \u2264 \u2212 2\u03bax\nP\uf8f0\ni=1\n\ni=1\n\nIt follows from (45), (46) and Lemma 3 with n = 1, Y1 = XI \u2212 E [XI ] and\n\u03b81 = E [XI ] that, for all x > 0 and I \u2208 m,\nh\ni\np\nP XI \u2265 E [XI ] + 2\u03baE [XI ] x + \u03c4 x \u2264 e\u2212x\n\nand\n\nh\n\nP XI \u2264 E [XI ] \u2212\n\np\n\ni\n\n2\u03baE [XI ] x \u2264 e\u2212x .\n\nSetting u = E [XI ] /(\u03bax), we deduce that, with probability not smaller than 1\u22122e\u2212x ,\np\np\nXI \u2212 E [XI ]\nr\u0010\n\u001a\n\u0011\np\np\nE [XI ] \u2212 2\u03baE [XI ] x ;\nE [XI ] \u2212\n\u2264 max\n+\n\u001b\nq\np\np\nE [XI ] + 2\u03baE [XI ] x + \u03c4 x \u2212 E [XI ]\nr\u0010\n\u001a\n\u001b\n\u221a \u0011 q\n\u221a\n\u221a\n\u221a\n\u221a\n\u03bax max\nu\u2212\nu \u2212 2u ; u + 2u + (\u03c4 /\u03ba) \u2212 u\n=\n+\nr\u0010\n\u001a\n\u001b\n\u221a \u0011 q\n\u221a\n\u221a\n\u221a\n\u221a\n\u2264\n\u03bax sup max\nz\u2212\nz \u2212 2z ; z + 2z + (\u03c4 /\u03ba) \u2212 z .\nz>0\n\n+\n\n\f28\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nq\n\u221a \u0001\n\u221a\nz \u2212 2z + admits a maximum equal to\nOn the one hand, note that z \u2192 z \u2212\n\u221a\n\u221a\n\u221a\n\u221a\n2 for z = 2. On the other hand, using the inequality a + b \u2264 a + b which\nholds for all positive numbers a, b, we obtain for all z > 0,\nv\nu\n\u221a !2 \u0012\n\u0013\nq\nu \u221a\n\u221a\n\u221a\n\u221a\n\u03c4\n1\n2\nt\n+\n\u2212\n\u2212 z\nz + 2z + (\u03c4 /\u03ba) \u2212 z \u2264\nz+\n2\n\u03ba 2 +\ns\n\u221a\n\u0012\n\u0013\n\u03c4\n1\n2\n+\n\u2212\n\u2264\n2\n\u03ba 2 +\np\n\u221a\n\u221a\nand therefore XI \u2212 E [XI ] \u2264 K \u03bax with probability not smaller than 1\u22122e\u2212x ,\nor equivalently\n\u0010p\n\u00112\np\n\u0002\n\u0003\n(49) P UI \u2265 K 2 x \u2264 2e\u2212x for all x > 0 with UI = \u03ba\u22121\nXI \u2212 E [XI ] .\nP\nSince \u03c72 (m) = \u03ba I\u2208m UI and the random variables UI , I \u2208 m are independent,\n(47) will derive from Lemma 3 if we show, setting EI = E [UI ], that\ni\u0011\n\u0010 h\n4K 4 z 2\nfor all z \u2208]0, 1/K 2 [.\n(50)\nlog E ez(UI \u2212EI ) \u2264\n2(1 \u2212 K 2 z)\nSimilarly, (48) will follow from\ni\u0011 4K 4 z 2\n\u0010 h\n(51)\nlog E e\u2212z(UI \u2212EI ) \u2264\n2\n\nfor all z > 0.\n\nTo prove (50), we shall use the following lemma about the centered moments of\npositive random variables.\nLemma 4. Let Z be a non-negative random variable. For any positive even integer\nk,\nh i\ni\nh i\nh\nE (Z \u2212 E [Z])k \u2264 E Z k \u2212 (E [Z])k \u2264 E Z k .\ni\nh\n\u0002 \u0003\nNote that the inequality E (Z \u2212 E [Z])k \u2264 E Z k also holds true for odd integers\n\nk since E [Z] \u2265 0 and the map z 7\u2192 z k is then increasing.\n\nProof. Since the result is trivial for k = 2, we may assume that k \u2265 4 and, using\nhomogeneity, that E [Z] = 1. Consider the function z 7\u2192 Q(z) = z k \u2212(z\u22121)k \u2212k(z\u22121)\non [0, +\u221e[. Its second derivative is negative for z < 1/2 and positive for z > 1/2,\nfrom which we easily derive that Q has a minimum for z = 1. This shows that\nQ(z) \u2265 1 for all z \u2265 0 and consequently,\ni\nh\nh i\nE Z k \u2212 E (Z \u2212 1)k = E [Q (Z)] \u2265 Q(1) = 1\nwhich leads to the result.\n\n\u0003\n2\n\nThe random variable UI is positive and by (49) satisfies P [UI \u2265 t] \u2264 2e\u2212t/K .\nConsequently, we deduce from the previous lemma (with Z = UI ) that for all integers\nk (odd or even)\ni\nh i Z +\u221e\nh\nk\nktk\u22121 P [UI \u2265 t] dt \u2264 2(k!)K 2k .\n(52)\nE (UI \u2212 EI ) \u2264 E UIk =\n0\n\n\fESTIMATING THE INTENSITY\n\n29\n\nHence, for all z \u2208]0, 1/K 2 [ ,\n\n\uf8eb\n\uf8f6\ni\u0011\n\u0010 h\nX\nX\nlog E ez(UI \u2212EI ) \u2264 log \uf8ed1 + 0 + 2\nz k K 2k \uf8f8 \u2264 2\nz k K 2k =\nk\u22652\n\nk\u22652\n\n4K 4 z 2\n.\n2(1 \u2212 K 2 z)\n\ne\u2212zu\n\nTo prove (51), note that, for all z, u > 0,\n\u2264 1 \u2212 zu + z 2 u2 /2. Therefore, by\n(52),\ni\u0011\n\u0010 h\n\u0002\n\u0003\u0001\nz 2 \u0002 \u0003 4K 4 z 2\n,\nlog E e\u2212z(UI \u2212EI ) = log E e\u2212zUI + zEI \u2264 E UI2 \u2264\n2\n2\nwhich completes the proof of Theorem 7.\n\u0003\nA second pair of deviation inequalities for variables of the form \u03c72 (m) is as follows.\nTheorem 8. Let m be a finite index set and X j = (XI,j )I\u2208m , 1 \u2264 j \u2264 p be i.i.d.\n|m|\n\nrandom vectors with values in R+ . Assume that there exist positive numbers A and\n\u03ba such that\nX\n(53)\nXI,1 \u2264 A a.s.\nand\nVar (XI,1 ) \u2264 \u03baE [XI,1 ] for all I \u2208 m.\nI\u2208m\n\nIf XI =\n(54)\n\nPp\n\nfor all I \u2208 m and \u03c72 (m) is given by (44), then\n\u0002\n\u0003\nP \u03c72 (m) \u2265 8\u03ba|m| + 202Ax \u2264 e\u2212x for all x > 0.\n\nj=1 XI,j\n\nProof. Since XI,1 = 0 a.s. if E[XI,1 ] = 0, we may remove all indexes I such that\nE[XI,1 ] = 0 in the sum and therefore assume that E [XI ] = pE [XI,1 ] > 0 for all\nI \u2208 m. We can then write, for all z > 0,\n\u0010p\n\u0011\nP\nX 2 (m) \u2265 z\n\u0010\u221a\n\u0011\n\uf8f6\n\uf8eb\np\n\u0011\nXI \u2212 E [XI ] \u0010p\nX\np\np\np\nXI \u2212 E [XI ] \u2265 z, X 2 (m) \u2265 z \uf8f8\n= P\uf8ed\n2 (m)\nX\nI\u2208m\n\u0010\u221a\n\u0011\n\uf8eb\n\uf8f6\np\nX\n\u2212\nE\n[X\n]\nX\np\nI\nI\nX\n\u2212\nE\n[X\n]\nI\np I\np\n= P\uf8ed\n\u2265 z, X 2 (m) \u2265 z \uf8f8\n\u221a\n2 (m)\nX\n+\nE\n[X\n]\nX\nI\nI\nI\u2208m\n\u0011\n\u0010\u221a\n\uf8eb \uf8ee\n\uf8f9\n\uf8f6\np\np\nXI \u2212 E [XI ] (XI,j \u2212 E [XI,j ])\nX\nX\np\n\uf8fb \u2265 z, X 2 (m) \u2265 z \uf8f8\n\u0010\u221a\n\u0011\n= P\uf8ed \uf8f0\np\np\n2 (m)\nX\nX\n+\nE\n[X\n]\nI\nI\nj=1 I\u2208m\n\u0011p\n\u0010\u221a\n\uf8f9\n\uf8f6\n\uf8eb \uf8ee\np\np\nXI \u2212 E [XI ]\nE [XI ] X \u2212 E [X ]\nX X\np\nI,j \uf8fb\n\u0010\u221a\n\u0011 I,jp\n\u2265 z, X 2 (m) \u2265 z \uf8f8\n= P\uf8ed \uf8f0\np\np\n2\nE\n[X\n]\nI\nX (m)\nXI + E [XI ]\nI\u2208m j=1\n\uf8f6\n\uf8eb\np X\nX\np\nXI,j \u2212 E [XI,j ]\np\n\u2265 z, X 2 (m) \u2265 z \uf8f8 ,\ntI\n= P\uf8ed\nE [XI ]\nj=1 I\u2208m\nwhere\n\n\u0010\u221a\n\n\u0011p\nE [XI ]\nE [XI ]\n\u0011\n\u0010\u221a\ntI = p\np\nX 2 (m)\nXI + E [XI ]\nXI \u2212\n\np\n\nfor all I \u2208 m.\n\n\f30\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\np\np\n\u221a\nP\nE [XI ]/( XI + E [XI ]) \u2264 1 and that |tI | \u2264\nt2I \u2264 1 since\nNote that\nI\u2208m\np\np\nz \u22121 E [XI ] on the set X 2 (m) \u2265 z, from which we deduce that\n\uf8eb\n\uf8f6\np X\n\u0010p\n\u0011\nX\nXI,j \u2212 E [XI,j ]\np\n(55)\nP\ntI\nX 2 (m) \u2265 z \u2264 P \uf8edsup\n\u2265 z\uf8f8 ,\nE [XI ]\nt\u2208T j=1 I\u2208m\nwhere T denotes the set of vectors t = (tI )I\u2208m \u2208 R|m| satisfying\np\nX\nE [XI ]\n(56)\n|tI | \u2264\nfor all I \u2208 m\nand\nt2I \u2264 1.\nz\nI\u2208m\n\nIn order to bound the right-hand side of (55), we shall use the following result from\nMassart (2000, Theorem 2.4).\nTheorem 9. Let \u03be 1 , . . . , \u03be p be independent random variables with values in some\nmeasurable space H and F be some countable family of real valued measurable functions on H such that kf k\u221e \u2264 b < +\u221e for all f \u2208 F. If\n\uf8f9\n\uf8ee\np\np\nX\nX\n\u0002\n\u0003\n\u0001\nand\n\u03c3 2 = sup \uf8f0\nf (\u03be j ) \u2212 E f (\u03be j )\nVar f (\u03bej ) \uf8fb ,\nZ = sup\nf \u2208F\n\nf \u2208F j=1\n\nj=1\n\nthen for every positive numbers \u03b5, x\nh\n\u221a\n\u0001 i\nP Z \u2265 (1 + \u03b5)E [Z] + 2\u03c3 2x + 2.5 + 32\u03b5\u22121 bx \u2264 e\u2212x .\n\nWe want to apply this result to the vectors \u03be j \u2208 R|m| with coordinates \u03beI,j =\np\n(XI,j \u2212 E [XI,j ])/ E [XI ] for I \u2208 m. Under our assumptions, these random vectors\nare independent and satisfy\nXp\nX\nE [XI ]|\u03beI,j | \u2264\n(XI,j + E [XI,j ]) \u2264 2A.\nI\u2208m\n\nI\u2208m\n\nConsequently, the random vectors \u03be j take their values in the subset H of R|m| given\nby\n)\n(\nXp\nE [XI ]|uI | \u2264 2A .\nH = u = (uI , I \u2208 m)\nI\u2208m\n\nP\n\u2032\nFor u \u2208 H and t \u2208 T , we set ft (u) =\nI\u2208m tI uI and F = {ft , t \u2208 T } where\n\u2032\nT denotes a countable and dense subset of T . With no loss of generality we can\nassume that T \u2032 is symmetric around 0 (if t \u2208 T \u2032 then \u2212t \u2208 T \u2032 ) which implies that\nthe absolute values can be removed in the definition of Z. Since, for all t \u2208 T and\n1 \u2264 j \u2264 p, ft (\u03be j ) is centered, we can finally write\n\uf8f6\n\uf8eb\np\np X\nX\nX\nX\nX\n\u2212\nE\n[X\n]\nXI,j \u2212 E [XI,j ]\nI,j\nI,j \uf8f8\np\np\n= sup\ntI \uf8ed\n.\ntI\nZ = sup\n]\n]\nE\n[X\nE\n[X\nI\nI\nt\u2208T I\u2208m\nt\u2208T j=1 I\u2208m\nj=1\nUsing Cauchy-Schwarz Inequality and (56), we then derive that\n\uf8ee\uf8eb\n\uf8f62 \uf8f9\np\np\nX\nX\n\u0002 2\u0003\nXI,j \u2212 E [XI,j ] \uf8f8 \uf8fb X X Var(XI,j )\n2\n\uf8ed\n\uf8f0\np\nE [Z] \u2264 E Z \u2264\nE\n.\n=\nE [XI ]\nE [XI ]\nj=1\nI\u2208m\nI\u2208m j=1\n\n\fESTIMATING THE INTENSITY\n\n31\n\nP\nSince Var(XI,j ) \u2264 \u03baE [XI,j ] and pj=1 E [XI,j ] = E [XI ], we conclude that E [Z] \u2264\np\n\u03ba|m|. To bound kft k\u221e , we use (56) which implies that, for all u \u2208 H and t \u2208 T ,\np\nX\nX\nX E [XI ]|uI |\n2A\nt I uI \u2264\n\u2264\n.\n|ft (u)| =\n|tI ||uI | \u2264\nz\nz\nI\u2208m\n\nI\u2208m\n\nI\u2208m\n\nFinally, it follows from the equidistribution of the X j , Cauchy-Schwarz Inequality,\n(53) and (56) that, for all t \u2208 T ,\n\uf8ee\n!2 \uf8f9\np\nX\nX\n\u0001\nXI,1 \u2212 E [XI,1 ] \uf8fb\nVar ft (\u03be j ) = pVar (ft (\u03be 1 )) = pE \uf8f0\ntI p\npE [XI,1 ]\nj=1\nI\u2208m\n\uf8ee\n\uf8f9\n!2\n!2\nX\nX q\nX\nI,1\n\uf8fb+2\n\u2264 2E \uf8f0\ntI p\ntI E[XI,1 ]\nE[X\n]\nI,1\nI\u2208m\nI\u2208m\n!#\n!\n\"\nX X\nX\nX\nX\nI,1\nE[XI,1 ]\n+2\nt2I\nt2I\nXI,1\n\u2264 2E\nE[XI,1 ]\nI\u2208m\nI\u2208m\nI\u2208m\nI\u2208m\n\"\n!\n#\nX\nX\n2 XI,1\n2\ntI\n\u2264 2A E\n\u2264 4A.\n+\ntI\nE[XI,1 ]\nI\u2208m\n\nI\u2208m\n\n2\nIn view of all these bounds,\nand\nhp we may apply\ni Theorem 9 with \u03c3 =p4A, b = 2A/z\n\u221a\n\u2212x\n2\n\u03b5 = 1 and obtain that P\n\u03c7 (m) \u2265 z \u2264 e as soon as z \u2265 2 \u03ba|m| + 4 2Ax +\n\u0001\n69Ax/z. Solving this quadratic inequation and using (a + b)2 \u2264 2 a2 + b2 , we can\ncheck that this inequality holds if z 2 \u2265 8\u03ba|m| + 202Ax, hence the result.\n\u0003\n\n7.3. Density estimation.\n7.3.1. Proof of Theorem 1. For two given classes m, m\u2032 \u2208 M, we apply Theorem 8\nwith m\u2032\u2032 = m \u2228 m\u2032 in place of m, p = n and XI,j = 1lYj \u2208I for all I \u2208 m\u2032\u2032 and\nj = 1, . . . , n. Then XI = nN (I) and (53) is satisfied with A = \u03ba = 1 since XI,1\nis a Bernoulli random variable and we derive from (43) that, for all x > 0, with\nprobability not smaller than 1 \u2212 e\u2212x ,\n\u00112 \u03c72 (m\u2032\u2032 )\nX \u0010p\np\n8|m\u2032\u2032 | + 202x\nN (I) \u2212 E [N (I)] =\nH 2 (\u015dm\u2032\u2032 , sm\u2032\u2032 ) =\n\u2264\n.\nn\nn\n\u2032\u2032\nI\u2208m\n\nTherefore (34) holds with c = 8/n, a = 1 and b = 202/n. We then conclude from\nTheorem 6 and the fact that H 2 (t, u) is always bounded by 2.\n\n\u221a\n7.3.2. Proof of Proposition 1. By assumption, s has a variation bounded by R and\nwe may apply to it Corollary 1 of Barron, Birg\u00e9 and Massart (1999) with \u03b1 = 1,\nD = 2j with j \u2265 2 and N = 23j . It follows that one can find m \u2208 M3j,D such that\nH 2 (s, Sm ) \u2264 (64/3)(R/D)2 . Since pen(m) \u2264 CjDn\u22121 for m \u2208 M3j,D , we derive\nfrom Theorem 1 that\n\b\n\u0002\n\u0003\nEs H 2 (s\u0303, s) \u2264 C \u2032 inf R2 2\u22122j + j2j n\u22121 .\nj\u22652\n\nThen (11) follows if we define j \u2265 2 by\n\u0002\n\u0001\u0003\u22122/3\n4\u2212j+1 \u2264 nR2 / log 1 + nR2\n< 4\u2212j+2 ,\n\n\f32\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nwhich is always possible since nR2 > 0, and distinguish between the cases j = 2\n(which corresponds to nR2 \u2264 26.519) and j > 2.\n\u221a\nWhen s is continuous with modulus w, there exists an element t \u2208 Smj such\n\u221a\n\u221a\nthat k s \u2212 tk\u221e \u2264 w(2\u2212j ), hence H(s, Smj ) \u2264 w(2\u2212j ). Since xw > 0, we can\nchoose j such that 2\u2212j < xw \u2264 2\u2212j+1 . Recalling that pen(mj ) \u2264 C2j /n, we deduce\nfrom Theorem 1 that\n\u0002\n\u0003\n\u0002\n\u0003\n\u0002\n\u0003\nEs H 2 (s\u0303, s) \u2264 C \u2032 w2 (2\u2212j ) + n\u22121 2j \u2264 C \u2032 w2 (xw ) + 2(nxw )\u22121 \u2264 3C \u2032 (nxw )\u22121 ,\n\u221a\nwhich proves (12). If s belongs to H\u03b1R with R \u2265 n\u22121/2 , then xw = (nR)\u22122/(2\u03b1+1)\nand the risk bound follows.\nP\nIf s belongs to S 3 (D, R), we can write s = D\nk=1 sk 1l[xk\u22121 ,xk ) with 0 = x0 < x1 <\n. . . < xD = 1 and sup1\u2264k\u2264D sk \u2264 R. Fix l such that 2l \u2265 nR > 2l\u22121 . Then 2l \u2265 2D\nP\nand for 0 \u2264 k \u2264 D, set x\u2032k = sup{x \u2208 Jl | x \u2264 xk } and t = D\nk=1 sk 1l[x\u2032k\u22121 ,x\u2032k ) so that\n\u2032\n\u2032\nt \u2208 Sm with m \u2208 Ml,D\u2032 with D \u2264 D since some intervals [xk\u22121 , x\u2032k ) may be empty.\nThen\nD\u22121\nX\n2\n(xk \u2212 x\u2032k ) < RD2\u2212l .\nH (s, t) \u2264 R\nk=1\nCn\u22121 [D(l log 2+2\u2212log D)+2 log l]\n\nRecalling from (9) that pen(m) \u2264\nfor m \u2208 Ml,D ,\nwe conclude from Theorem 1, (9) and our choice of l that\n\u0002\n\u0003\nEs H 2 (s\u0303, s)\ni\nh\n\u2212l\n\u22121\n\u2032\n\u2264 C RD2 + [D(l log 2 + 2 \u2212 log D) + 2 log l]n\nh\n\u0010\n\u0011\ni\n\u2264 C \u2032 (D/n) 3 + log 2 + log 2l\u22121 /D + 2D \u22121 log l\n\u0002\n\u0003\n\u2264 C \u2032 (D/n) 3 + log 2 + log (nR/D) + 2(D log 2)\u22121 log log(2nR)\nand (13) follows since nR \u2265 2D.\n7.4. Random vectors.\n\n7.4.1. Proof of Theorem 4. For two given elements m, m\u2032 \u2208 M, we apply Theorem 7\nwith m\u2032\u2032 = m \u2228 m\u2032 in place of m and XI = N (I). We derive from the independence\nof the Ni that (45) and (46) hold. Therefore, for all x > 0, with probability not\nsmaller than 1 \u2212 e\u2212x ,\n\u00112\nX \u0010p\np\nN (I) \u2212 E [N (I)]\nH 2 (\u015dm\u2032\u2032 , sm\u2032\u2032 ) =\nI\u2208m\u2032\u2032\n\n\u2264 E\n\n\"\n\n#\n\u0011\n\u00112\n\u0010 p\nX \u0010p\np\n2\n\u2032\u2032\nN (I) \u2212 E [N (I)]\n+ K \u03ba 2 2|m |x + x .\n\nI\u2208m\u2032\u2032\n\nIf follows from (45) that Var(N (I)) \u2264 \u03baE [N (I)] (expand both side of (45) in a\nvicinity of 0) and therefore\n\"\n#\n\u00112\n\u00112 \u0015\nX \u0010p\nX \u0014\u0010p\np\np\nN (I) \u2212 E [N (I)]\nN (I) \u2212 E [N (I)]\nE\n=\nE\nI\u2208m\u2032\u2032\n\nI\u2208m\u2032\u2032\n\n\u2264\n\n\"\n\n(N (I) \u2212 E [N (I)])2\nE\nE [N (I)]\n\u2032\u2032\n\nX\n\nI\u2208m\n\n#\n\n\u2264 \u03ba|m\u2032\u2032 |.\n\n\fESTIMATING THE INTENSITY\n\n33\n\np\nUsing the inequality 2 2|m\u2032\u2032 |x \u2264 |m\u2032\u2032 | + 2x we conclude that, with probability not\nsmaller than 1 \u2212 e\u2212x ,\n\u0001\n(57)\nH 2 (\u015dm\u2032\u2032 , sm\u2032\u2032 ) \u2264 1 + K 2 \u03ba|m\u2032\u2032 | + 3K 2 \u03bax.\n\u0001\nWe derive that (34) is fulfilled with c = 1 + K 2 \u03ba, b = 3K 2 \u03ba, a = 1 and Theorem 4\nfollows from Theorem 6.\n2\n7.4.2. Proof of Proposition\n\u221a0,\n\u0002 2\n\u0003 7. Let us first note that, if |m| = n, then H (s, Sm ) =\nhence by (28), E H (s\u0303, s) \u2264 C(\u03ba, K)n which proves the bound when R > n/ 3.\nFor the other cases, we deduce from Lemma 5 below that, for any D \u2208 X , one can\nfind some m \u2208 M such that |m| \u2264 D and H 2 (s, Sm ) \u2264 n(R/D)2 . Setting D = 1,\n2\n\u22121\n\u22121\n2\n2\nwe get the result\n\b for the3 case R2 < n log n. Finally, when n log n \u22642 R \u2264 n /3\nwe fix D = inf j \u2208 N | j \u2265 nR / log(n/R) . Since the function R 7\u2192 R / log(n/R)\n\u221a\nis increasing for R < n/ 3, 1 \u2264 D \u2264 n and the corresponding risk bound follows.\n\nLemma 5.\np Let f be a nondecreasing function from X = {1, . . . , n} to R such that\np\nf (n) \u2212 f (1) = R. For D \u2208 X , one can find a partition (I1 , . . . , IK ) of X into\nP\nK \u2264 D intervals and a function g from X to R of the form g = K\nk=1 \u03b2k 1lIk such\nthat\nn \u0010p\n\u00112\nX\np\nf (i) \u2212 g(i) \u2264 nR2 D \u22122 .\ni=1\n\nProof: Let us set j0 = 1 and define iteratively for k \u2265 1, using the convention\ninf \u2205 = n,\nn\no\np\np\nf (j) \u2212 f (jk\u22121 ) > R/D .\n(58)\njk = inf j \u2208 {jk\u22121 + 1, . . . , n}\n\nLet K = inf {k \u2265 1, jk = n}, IK = {jK\u22121 , . . . , n} and for k = 1, . . . , K \u2212 1 (if\nK \u2265 2), Ik = {jk\u22121 , . . . , jk \u2212 1}. This defines a partition of X with K elements and\nit follows from (58) that\nR=\n\np\n\nf (n) \u2212\n\np\n\nf (1) \u2265\n\nK\u22121\nXp\nk=1\n\nf (jk ) \u2212\n\np\n\nf (jk\u22121 ) > (K \u2212 1)R/D,\n\nhence\nK \u2212 1 <pD and K \u2264 D. Let us now set \u03b2k = f (jk\u22121p\n) for 1 \u2264pk \u2264 K. Since\np\nf (jk \u2212 1) \u2212 f (jk\u22121 ) \u2264 R/D we get for all i \u2208 Ik , 0 \u2264 f (i) \u2212 g(i) \u2264 R/D.\nHence,\nn \u0010p\nX\ni=1\n\nf (i) \u2212\n\np\n\ng(i)\n\n\u00112\n\n=\n\nK X \u0010p\nX\nk=1 i\u2208Ik\n\nf (i) \u2212\n\np\n\ng(i)\n\n\u00112\n\n\u2264 nR2 D \u22122 .\n\n7.5. Poisson and other counting processes.\n7.5.1. Poisson processes. The proof of Theorem 3 follows the same lines as the proof\nof Theorem 4. We apply Theorem 7 with m\u2032\u2032 = m\u2228m\u2032 in place of m and XI = N (I).\nSince {N (I), I \u2208 m\u2032\u2032 } are independent Poisson random variables, the assumptions\nof the theorem are fulfilled with \u03ba = \u03c4 = 1. We then proceed as for Theorem 4 to\nget (57) with K 2 = 2 which provides the relevant values of c and b.\n\n\f34\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\n7.5.2. Proof of Theorem 5. Let us fix two classes m, m\u2032 \u2208 M. We first apply Theorem 8 with m\u2032\u2032 = m \u2228 m\u2032 in place of m, p = n and XI,j = N j (I) for all I \u2208 m\u2032\u2032\n\u2032\u2032\nand\ni 1, . . . , n. Then for all I \u2208 m , N (I) = XI . Since XI,j is bounded by k,\nh j =\n2\n\u2264 kE [XI,j ] and (53) holds with A = \u03ba = k. This implies that, for all x > 0,\nE XI,j\n\nwith probability not smaller than 1 \u2212 e\u2212x ,\n\u00112\nX \u0010p\np\n\u0001\n(59)\nN (I) \u2212 E [N (I)] \u2264 k 8|m\u2032\u2032 | + 202x .\nI\u2208m\u2032\u2032\n\nThen weR apply once again Theorem 8 with m\u2032\u2032 = m \u2228 m\u2032 in place of m, p = n and\nj\nXI,j = I sY j d\u03bb for all I \u2208 m\u2032\u2032 and j = 1, . . . , n.\nby 1, the\nR Since Y is bounded\nassumptions of Theorem 8 are fulfilled with A = X s d\u03bb and \u03ba = \u03ba\u2032 . Consequently,\nwith probability not smaller than 1 \u2212 e\u2212x ,\ns \u0014Z\nsZ\n\u0015!2\nX\nsY d\u03bb \u2212 E\n\u2264 8\u03ba\u2032 |m\u2032\u2032 | + 202Ax.\nsd\u03bb\n(60)\nI\n\nI\u2208m\u2032\u2032\n\nI\n\n\u0002R\n\u0003\nSince E I sd\u03bb = E [N (I)], we derive from (59) and (60) that, with probability not\nsmaller than 1 \u2212 2e\u2212x ,\nH 2 (\u015dm\u2032\u2032 , sm\u2032\u2032 )\n\u2264\n\nX\n\nI\u2208m\u2032\u2032\n\n\u2264 2\n\np\n\nN (I) \u2212\n\nX \u0010p\n\nI\u2208m\u2032\u2032\n\u2032\u2032\n\nsZ\n\nN (I) \u2212\n\np\n\n!2\n\nsY d\u03bb\nI\n\n\u00112\nX\nE [N (I)] + 2\n\nI\u2208m\u2032\u2032\n\nsZ\n\nI\n\ns \u0014Z\n\u0015!2\nsY d\u03bb \u2212 E\nsd\u03bb\nI\n\n\u2264 16|m |(k + \u03ba\u2032 ) + 404x(k + A).\n\nThis means that (34) holds with c = 16(k+\u03ba\u2032 ), a = 2 and b = 404(k+A). Therefore,\nif we set \u2206m = k(k + A)\u22121 \u2206\u2032m for all m \u2208 M, (6) holds with \u03a3 = \u03a3\u2032 (k/(k + A))\nand pen(m) = 16\u03b4|m|(k + \u03ba\u2032 ) + 404k\u2206\u2032m . An application of Theorem 6 leads to the\nresult.\n7.5.3. Proof of Proposition 8. The following argument shows that (2) is satisfied:\nlet A be some measurable subset of X and B be the subset of A given by B =\n{t \u2208 A | \u03bb ([0, t] \u2229 A) = 0}. Since, by definition, the sets [0, t] \u2229 B with t \u2208 B are\nnegligible, \u03bb(B) = 0 (write B as an at most countable union of those sets). Consequently,\n\u0013\n\u0012\nZ\nn\nX\nj\n1lTej \u2265t dt = 0\nP N (A) = 1,\nP (N (A) > 0, M (A) = 0) \u2264\n\u2264\n\u2264\n\nj=1\nn\nX\n\nj=1\nn\nX\nj=1\n\nA\n\n\u0010\n\u0010\n\u0011\n\u0011\nP Tej = Tj , Tj \u2208 A, \u03bb A \u2229 [0, Tej ] = 0\nP (Tj \u2208 B) = 0\n\n\fESTIMATING THE INTENSITY\n\n35\n\nsince the common distribution of the Tj is continuous. Moreover\nZ\n\nsd\u03bb =\n\nZ\n\n1\n\n0\n\nX\n\np(t)\ndt = \u2212 log([P(T1 \u2265 1)])\nP[T1 \u2265 t]\n\nsince \u2212p(t) is the derivative of P[T1 \u2265 t]. Finally we can take \u03ba\u2032 = 2 since, whatever\nI \u2282 X,\n\"\u0012Z\n\u0015\n\u0014Z\n\u00132 #\n\u0015\n\u0014Z\n\u2032\n\u2217 \u2217\n\u2032\n\u2217\n\u2217\ns(t)s(t )Yt Yt\u2032 dt dt\n= E\ns(t)Yt dt\ns(t)Yt dt \u2264 E\nVar\nI\n\n=\n\nI\u00d7I\n\nI\n\nZ\n\nZI\u00d7I\n\ns(t)s(t\u2032 )E [Yt\u2217 Yt\u2217\u2032 ] dt dt\u2032\nh\n\n\b \u2032 i\ne\ns(t)s(t ) P T1 \u2265 max t, t\ndt dt\u2032\n=\nI\u00d7I\n\u0012Z\nZ\nh\ni \u0013\n\u2032\n\u2032\n1l{t\u2032 \u2265t} s(t ) P Te1 \u2265 t dt\u2032 dt\n= 2 s(t)\nI\nI\n\u0014Z 1\n\u0015\nZ\n\u2032\n\u2217 \u2032\ns(t )Yt\u2032 dt dt\n\u2264 2 s(t) E\nt\nI\nZ\n\u0002\n\u0003\n= 2 s(t)E N 1 ([t, 1]) dt\n\u0015\n\u0014Z\nZI\nh\ni\n\u2217\ne\ns(t)Yt dt .\n\u2264 2 s(t)P T1 \u2265 t dt = 2E\n\u2032\n\nI\n\nI\n\n7.5.4. Proof of Proposition 9. Clearly (29) holds true. We now prove that Condition\n(2) is also fulfilled. Let A be some measurable subset of R+ and for l \u2265 1 let Bl be\nthe subset of A defined by\n\b\n\u0001\nBl = t \u2208 A | \u03bb ]t \u2212 l\u22121 , t] \u2229 A = 0 .\n\nFor each l \u2265 1, note that the sets [t \u2212 l\u22121 , t] \u2229 Bl \u2282 [t \u2212 l\u22121 , t] \u2229 A are negligible\nfor t \u2208 Bl and hence so is Bl (write Bl as an at most countable union of those).\nj\nDenoting, for j = 1, . . . , n, the time of the jump of X j from state 1 to 0 by T1,0\n, we\nhave\nP (N (A) > 0, M (A) = 0)\n\u0012\nZ\nn\nX\nj\n1lX j\nP N (A) = 1,\n\u2264\n\u2264\n\u2264\n\u2264\n\nj=1\nn\nX\n\nA\n\nt\u2212\n\n\u0013\ndt = 0\n=1\n\n\u0010\n\u0010\n\u0011\n\u0011\nj\nj\nP N j (A) = 1, \u2203\u03b5 > 0, \u03bb [T1,0\n\u2212 \u03b5, T1,0\n]\u2229A =0\n\nj=1\nn X\nX\n\nj=1 l\u22651\nn X\nX\nj=1 l\u22651\n\n\u0010\n\u0010\n\u0011\n\u0011\nj\nj\nj\nP T1,0\n\u2208 A, \u03bb [T1,0\n\u2212 l\u22121 , T0,1\n]\u2229A =0\n\nn X\n\u0010\n\u0011\nX\nj\nE [N \u2217 (Bl )] = 0,\nP T1,0\n\u2208 Bl =\nj=1 l\u22651\n\n\f36\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nby (32). We may clearly fix k = 1 and the choice of \u03ba\u2032 is justified by the following\nargument. First note that whatever I \u2282 X and t > 0\n\u0001\n1\n1\n1\nP Xt\u2212\n= 1, T1,0\n\u2208 I, T1,0\n\u2265t\nZ\n\u0001\n1\n1\n1l{u\u2265t} P Xt\u2212\n= 1, u \u2264 T1,0\n\u2264 u + du\n=\nZI\n\u0001\n\u0001\n1\n1\n1\n1\n1\n1l{u\u2265t} P Xt\u2212\n= 1, Xu\u2212\n= 1 P u \u2264 T1,0\n\u2264 u + du | Xt\u2212\n= 1, Xu\u2212\n=1\n=\nZI\n\u0001\n\u0001\n1\n1\n1\n1\n1l{u\u2265t} P Xt\u2212\n= 1, Xu\u2212\n= 1 P u \u2264 T1,0\n\u2264 u + du | Xu\u2212\n=1\n=\nI\n\nsince\n\nX1\n\nis a Markov process. Hence\nZ\n\u0001\n\u0001\n1\n1\n1\n1\n1\n1l{u\u2265t} P Xt\u2212\n= 1, Xu\u2212\n= 1 s(u)du\nP Xt\u2212\n= 1, T1,0\n\u2208 I, T1,0\n\u2265t =\nI\n\u0015\n\u0014Z\n1l{u\u2265t} 1l{Xt\u2212\n= E\n1 =1} 1l{X 1 =1} s(u)du .\nu\u2212\nI\n\nIt then follows that\n\"\u0012Z\n\u0013\n\u00132 #\n\u0012Z\ns(t)Yt1 dt\n\u2264 E\n1lI (t)s(t)Yt1 dt\nVar\nI\n\nX\n\n\u0015\n1lI (t)1lI (u)s(t)s(u)Yt1 Yu1 dudt\n= E\nX \u00d7X\n\u0015\nZ \u0014Z\n1l{u\u2265t} 1l{Xt\u2212\n= 2 E\n1 =1} 1l{X 1 =1} s(u)du s(t)dt\nu\u2212\nI\nI\nZ\n\u0001\n1\n1\n1\n= 1, T1,0\n\u2208 I, T1,0\n\u2265 t s(t)dt\n= 2 P Xt\u2212\n\u0015\n\u0014Z\nZI\n\u0001\n1\n1\ns(t)Yt dt .\n\u2264 2 P Xt\u2212 = 1 s(t)dt = 2E\n\u0014Z\n\nI\n\nI\n\nReferences\nANDERSEN, P., BORGAN, O., GILL, R. and KEIDING, N. (1993). Statistical Models\nBased on Counting Processes. Springer-Verlag, New York.\nANTONIADIS, A. (1989). A penalty method for nonparametric estimation of the intensity function of a counting process. Ann. Inst. Statist. Math. 41, 781\u2013807.\nANTONIADIS, A., BESBEAS, P. and SAPATINAS, T. (2001). Wavelet shrinkage for\nnatural exponential families with cubic variance functions. Sankhya 63, 309-327.\nANTONIADIS, A. and SAPATINAS, T. (2001). Wavelet shrinkage for natural exponential families with quadratic variance functions. Biometrika 88, 805-820.\nBARRON, A.R., BIRG\u00c9, L. and MASSART, P. (1999). Risk bounds for model selection\nvia penalization. Probab. Theory Relat. Fields 113, 301-415.\nBARRON, A.R. and COVER, T.M. (1991). Minimum complexity density estimation.\nIEEE Transactions on Information Theory 37, 1034-1054.\nBIRG\u00c9, L. (1983). Approximation dans les espaces m\u00e9triques et th\u00e9orie de l'estimation.\nZ. Wahrscheinlichkeitstheorie Verw. Geb. 65, 181-237.\nBIRG\u00c9, L. (2006). Model selection via testing : an alternative to (penalized) maximum\nlikelihood estimators. Ann. Inst. Henri Poincar\u00e9 Probab. et Statist. 42, 273-325.\n\n\fESTIMATING THE INTENSITY\n\n37\n\nBIRG\u00c9, L. and MASSART, P. (1998). Minimum contrast estimators on sieves: exponential bounds and rates of convergence. Bernoulli 4, 329-375.\nBIRG\u00c9, L. and MASSART, P. (2000). An adaptive compression algorithm in Besov\nspaces. Constructive Approximation 16 1-36.\nBIRG\u00c9, L. and MASSART, P. (2001). Gaussian model selection. J. Eur. Math. Soc. 3,\n203-268.\nBREIMAN, L., FRIEDMAN, J.H., OLSHEN, R.A. and STONE, C.J. (1984). Classification and Regression Trees. Wadsworth, Belmont.\nCASTELLAN, G. (1999). Modified Akaike's criterion for histogram density estimation.\nTechnical Report 99.61. Universit\u00e9 Paris-Sud, Orsay.\nCASTELLAN, G. (2000). S\u00e9lection d'histogrammes \u00e0 l'aide d'un crit\u00e8re de type Akaike.\nC.R.A.S. 330, 729-732.\nDeVORE, R.A. (1998). Nonlinear Approximation. Acta Numerica 7, 51-150.\nDeVORE, R.A. and LORENTZ, G.G. (1993). Constructive Approximation. SpringerVerlag, Berlin.\nDeVORE, R.A. and YU, X.M. (1990). Degree of adaptive approximation. Math. Comp.\n55, 625-635.\nGEY, S. and N\u00c9D\u00c9LEC, E. (2005). Model selection for CART regression trees. IEEE\nTransactions on Information Theory 51, 658-670.\nGR\u00c9GOIRE, G. and NEMB\u00c9, J. (2000). Convergence rates for the minimum complexity\nestimator of counting process intensities. J. Nonparametr. Statist. 12, 611-643.\nKOLACZYK, E. (1999). Wavelet shrinkage estimation of certain Poisson intensity signals\nusing corrected threshold. Statistica Sinica 9, 119-135.\nKOLACZYK, E. and NOWAK, R. (2004). Multiscale likelihood analysis and complexity\npenalized estimation. Annals of Statistics 32, 500-527.\nLAURENT, B. and MASSART, P. (2000). Adaptive estimation of a quadratic functional\nby model selection. Ann. Statist. 28, 1302-1338.\nLEPSKII, O.V. (1991). Asymptotically minimax adaptive estimation I: Upper bounds.\nOptimally adaptive estimates. Theory Probab. Appl. 36, 682-697.\nMASSART, P. (2000). Some applications of concentration inequalities to Statistics. Ann.\nFac. Sciences de Toulouse IX, 245-303.\nPATIL, P.N. and WOOD, A.T. (2004). A counting process intensity estimation by orthogonal wavelet methods. Bernoulli 10, 1-24.\nREYNAUD-BOURET, P. (2003). Adaptive estimation of the intensity of inhomogeneous\nPoisson processes via concentration inequalities. Probab. Theory Related Fields 126, 103153.\nREYNAUD-BOURET, P. (2002). Penalized projection estimators of the Aalen multiplicative intensity. School of Mathematics, Georgia Institute of Technology Preprint 1202002.\nSTANLEY, R.P (1999). Enumerative Combinatorics, Vol. 2. Cambridge University\nPress, Cambridge.\nvan de GEER, S. (1995). Exponential inequalities for martingales, with application to\nmaximum likelihood estimation for counting processes. Ann. Statist. 23, 1779-1801.\n\n\f38\n\nYANNICK BARAUD AND LUCIEN BIRG\u00c9\n\nWU, S.S. and WELLS, M.T. (2003) Nonparametric estimation of hazard functions by\nwavelet methods. J. Nonparametr. Stat. 15, 187 \u2212 203.\nUniversit\u00e9 de Nice Sophia-Antipolis, Laboratoire J-A Dieudonn\u00e9, Parc Valrose,\n06108 Nice cedex 02\nE-mail address: baraud@math.unice.fr\nUniversit\u00e9 Paris VI, Laboratoire de Probabilit\u00e9s et Mod\u00e8les Al\u00e9atoires, bo\u0131\u0302te 188,\n4 place Jussieu, 75252 Paris Cedex 05\nE-mail address: lb@ccr.jussieu.fr\n\n\f"}