{"id": "http://arxiv.org/abs/math/0504480v1", "guidislink": true, "updated": "2005-04-23T11:10:28Z", "updated_parsed": [2005, 4, 23, 11, 10, 28, 5, 113, 0], "published": "2005-04-23T11:10:28Z", "published_parsed": [2005, 4, 23, 11, 10, 28, 5, 113, 0], "title": "High-resolution quantization and entropy coding for fractional Brownian\n  motion", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0504420%2Cmath%2F0504257%2Cmath%2F0504256%2Cmath%2F0504401%2Cmath%2F0504254%2Cmath%2F0504118%2Cmath%2F0504504%2Cmath%2F0504317%2Cmath%2F0504137%2Cmath%2F0504336%2Cmath%2F0504023%2Cmath%2F0504065%2Cmath%2F0504587%2Cmath%2F0504547%2Cmath%2F0504282%2Cmath%2F0504509%2Cmath%2F0504537%2Cmath%2F0504382%2Cmath%2F0504160%2Cmath%2F0504568%2Cmath%2F0504602%2Cmath%2F0504332%2Cmath%2F0504046%2Cmath%2F0504349%2Cmath%2F0504313%2Cmath%2F0504383%2Cmath%2F0504184%2Cmath%2F0504384%2Cmath%2F0504423%2Cmath%2F0504473%2Cmath%2F0504413%2Cmath%2F0504553%2Cmath%2F0504493%2Cmath%2F0504263%2Cmath%2F0504597%2Cmath%2F0504323%2Cmath%2F0504394%2Cmath%2F0504158%2Cmath%2F0504479%2Cmath%2F0504562%2Cmath%2F0504006%2Cmath%2F0504319%2Cmath%2F0504385%2Cmath%2F0504534%2Cmath%2F0504500%2Cmath%2F0504550%2Cmath%2F0504598%2Cmath%2F0504128%2Cmath%2F0504161%2Cmath%2F0504342%2Cmath%2F0504366%2Cmath%2F0504201%2Cmath%2F0504429%2Cmath%2F0504014%2Cmath%2F0504083%2Cmath%2F0504379%2Cmath%2F0504153%2Cmath%2F0504298%2Cmath%2F0504094%2Cmath%2F0504480%2Cmath%2F0504075%2Cmath%2F0504163%2Cmath%2F0504264%2Cmath%2F0504460%2Cmath%2F0504067%2Cmath%2F0504328%2Cmath%2F0504606%2Cmath%2F0504185%2Cmath%2F0504449%2Cmath%2F0504117%2Cmath%2F0504104%2Cmath%2F0504545%2Cmath%2F0504439%2Cmath%2F0504119%2Cmath%2F0504122%2Cmath%2F0504445%2Cmath%2F0504584%2Cmath%2F0504549%2Cmath%2F0504604%2Cmath%2F0504241%2Cmath%2F0504506%2Cmath%2F0504236%2Cmath%2F0504021%2Cmath%2F0504391%2Cmath%2F0504416%2Cmath%2F0504165%2Cmath%2F0504582%2Cmath%2F0504307%2Cmath%2F0504073%2Cmath%2F0504025%2Cmath%2F0504170%2Cmath%2F0504448%2Cmath%2F0504301%2Cmath%2F0504356%2Cmath%2F0504499%2Cmath%2F0504238%2Cmath%2F0504192%2Cmath%2F0504276%2Cmath%2F0504530%2Cmath%2F0504143%2Cmath%2F0504085&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "High-resolution quantization and entropy coding for fractional Brownian\n  motion"}, "summary": "We derive a high-resolution formula for the quantization and entropy coding\napproximation quantities for fractional Brownian motion, respective to the\nsupremum norm and L^p[0,1]-norm distortions. We show that all moments in the\nquantization problem lead to the same asymptotics. Using a general principle,\nwe conclude that entropy coding and quantization coincide asymptotically. Under\nsupremum-norm distortion, our proof uses an explicit construction of efficient\ncodebooks based on a particular entropy constrained coding scheme. This\nprocedure can be used to construct close to optimal high resolution quantizers.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0504420%2Cmath%2F0504257%2Cmath%2F0504256%2Cmath%2F0504401%2Cmath%2F0504254%2Cmath%2F0504118%2Cmath%2F0504504%2Cmath%2F0504317%2Cmath%2F0504137%2Cmath%2F0504336%2Cmath%2F0504023%2Cmath%2F0504065%2Cmath%2F0504587%2Cmath%2F0504547%2Cmath%2F0504282%2Cmath%2F0504509%2Cmath%2F0504537%2Cmath%2F0504382%2Cmath%2F0504160%2Cmath%2F0504568%2Cmath%2F0504602%2Cmath%2F0504332%2Cmath%2F0504046%2Cmath%2F0504349%2Cmath%2F0504313%2Cmath%2F0504383%2Cmath%2F0504184%2Cmath%2F0504384%2Cmath%2F0504423%2Cmath%2F0504473%2Cmath%2F0504413%2Cmath%2F0504553%2Cmath%2F0504493%2Cmath%2F0504263%2Cmath%2F0504597%2Cmath%2F0504323%2Cmath%2F0504394%2Cmath%2F0504158%2Cmath%2F0504479%2Cmath%2F0504562%2Cmath%2F0504006%2Cmath%2F0504319%2Cmath%2F0504385%2Cmath%2F0504534%2Cmath%2F0504500%2Cmath%2F0504550%2Cmath%2F0504598%2Cmath%2F0504128%2Cmath%2F0504161%2Cmath%2F0504342%2Cmath%2F0504366%2Cmath%2F0504201%2Cmath%2F0504429%2Cmath%2F0504014%2Cmath%2F0504083%2Cmath%2F0504379%2Cmath%2F0504153%2Cmath%2F0504298%2Cmath%2F0504094%2Cmath%2F0504480%2Cmath%2F0504075%2Cmath%2F0504163%2Cmath%2F0504264%2Cmath%2F0504460%2Cmath%2F0504067%2Cmath%2F0504328%2Cmath%2F0504606%2Cmath%2F0504185%2Cmath%2F0504449%2Cmath%2F0504117%2Cmath%2F0504104%2Cmath%2F0504545%2Cmath%2F0504439%2Cmath%2F0504119%2Cmath%2F0504122%2Cmath%2F0504445%2Cmath%2F0504584%2Cmath%2F0504549%2Cmath%2F0504604%2Cmath%2F0504241%2Cmath%2F0504506%2Cmath%2F0504236%2Cmath%2F0504021%2Cmath%2F0504391%2Cmath%2F0504416%2Cmath%2F0504165%2Cmath%2F0504582%2Cmath%2F0504307%2Cmath%2F0504073%2Cmath%2F0504025%2Cmath%2F0504170%2Cmath%2F0504448%2Cmath%2F0504301%2Cmath%2F0504356%2Cmath%2F0504499%2Cmath%2F0504238%2Cmath%2F0504192%2Cmath%2F0504276%2Cmath%2F0504530%2Cmath%2F0504143%2Cmath%2F0504085&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We derive a high-resolution formula for the quantization and entropy coding\napproximation quantities for fractional Brownian motion, respective to the\nsupremum norm and L^p[0,1]-norm distortions. We show that all moments in the\nquantization problem lead to the same asymptotics. Using a general principle,\nwe conclude that entropy coding and quantization coincide asymptotically. Under\nsupremum-norm distortion, our proof uses an explicit construction of efficient\ncodebooks based on a particular entropy constrained coding scheme. This\nprocedure can be used to construct close to optimal high resolution quantizers."}, "authors": ["Steffen Dereich", "Michael Scheutzow"], "author_detail": {"name": "Michael Scheutzow"}, "author": "Michael Scheutzow", "arxiv_comment": "25 pages", "links": [{"href": "http://arxiv.org/abs/math/0504480v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0504480v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "60G35; 41A25; 94A29", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0504480v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0504480v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:math/0504480v1 [math.PR] 23 Apr 2005\n\nHigh-resolution\nquantization and entropy coding\nfor fractional Brownian motion\nby\n\nS. Dereich and M. Scheutzow\nTechnische Universit\u00e4t Berlin\n\nSummary. We derive a high-resolution formula for the quantization and entropy coding approximation quantities for fractional Brownian motion, respective to the supremum norm and Lp [0, 1]-norm distortions. We show that all moments in the quantization problem lead to the same asymptotics. Using a general principle, we conclude\nthat entropy coding and quantization coincide asymptotically. Under supremumnorm distortion, our proof uses an explicit construction of efficient codebooks based\non a particular entropy constrained coding scheme. This procedure can be used to\nconstruct close to optimal high resolution quantizers.\nKeywords. High-resolution quantization; complexity; stochastic process; entropy;\ndistortion rate function.\n2000 Mathematics Subject Classification. 60G35, 41A25, 94A29.\n\n1\n\nIntroduction\n\nFunctional quantization and entropy coding concern the finding of \"good\" discrete approximations to a non-discrete random signal in a Banach space of functions. Such discrete\napproximations may serve as evaluation points for quasi Monte Carlo methods or as an\ninformation reduction of the original to allow storage on a computer or transmission over\nsome channel with finite capacity. In the past years, research in this field has been very active, which resulted in numerous new results. Previous research addressed, for instance, the\nproblem of constructing good approximation schemes, the evaluation of the theoretically\nbest approximation under an information constraint, existence of optimal approximation\n1\n\n\fschemes and regularity properties of the paths of optimal approximations. The above\nquestions are treated for Gaussian measures in Hilbert spaces by Luschgy and Pag\u00e8s ([11],\n[12]) and by the first-named author in [3]. For Gaussian originals in Banach spaces, these\nproblems have been addressed by the authors and collaborators in [6], [7], [3], [4] and by\nGraf, Luschgy and Pag\u00e8s in [9]. For general accounts of quantization and coding theory\nin finite dimensional spaces, see [8] and [1] (see also [10]).\nIn this article, we consider the asymptotic coding problem of fractional Brownian motion for the supremum and Lp [0, 1]-norm distortions. We derive the asymptotic quality of\noptimal approximations. In particular, it is shown that efficient entropy constrained quantizers can be used to construct close to optimal quantizers when considering the supremum\nnorm. Moreover, for one of the above norm-based distortions, all moments and both information constraints lead to the same asymptotic approximation quality. In particular,\nquantization is asymptotically just as efficient as entropy coding. The main impetus to\nthe present work was provided by the necessity to understand the coding complexity of\nBrownian motion in order to solve the quantization (resp. entropy constrained coding)\nproblem for diffusions (see [5]).\nLet (\u03a9, A, P) be a probability space, let H \u2208 (0, 1) and let X = (Xt )t\u22650 denote fractional Brownian motion with Hurst index H on (\u03a9, A, P), i.e. (Xt )t\u22650 is a centered continuous Gaussian process with covariance kernel\n1\nK(t, s) = [t2H + s2H \u2212 |t \u2212 s|2H ],\n2\n\nt, s \u2265 0.\n\nWe need some more notation. In the sequel, C[0, a], a > 0, and D[0, a] denote the space of\ncontinuous real-valued functions on the interval [0, a] and the space of c\u00e0dl\u00e0g functions on\n[0, a], respectively. Both spaces are endowed with the supremum norm k * k[0,a] . Moreover,\nwe let (Lp [0, a], k*kLp [0,a] ) denote the standard Lp -space of real-valued functions defined on\n[0, a]. Finally, k * kq , q \u2208 (0, \u221e], denotes the Lq -norm induced by the probability measure\nP on the set of real-valued random variables.\nLet us briefly introduce the main objectives of quantization and entropy coding. Let\nE and \u00ca denote measurable spaces, and let d : E \u00d7 \u00ca \u2192 [0, \u221e) be a product measurable\nfunction. For a given E-valued r.v. Y (original) and moment q > 0, the aim is to minimize\nd(Y, \u03c0(Y ))\n\nq\n\n(1)\n\nover all measurable functions \u03c0 : E \u2192 \u00ca with discrete image (strategy) that satisfy a\nparticular information constraint parameterized by the rate r \u2265 0.\nEntropy coding (also known as entropy constrained quantization in the literature) concerns the minimization of (1) over all strategies \u03c0 having entropy H(\u03c0(Y )) at most r.\n2\n\n\fRecall that the entropy of a discrete r.v. Z with probability weights (pw ) is defined as\nX\nH(Z) = \u2212\npw log pw = E[\u2212 log pZ ].\nw\n\nIn the quantization problem, one is considering strategies \u03c0 satisfying the range constraint: | range (\u03c0(Y ))| \u2264 er . The corresponding approximation quantities are the entropyconstrained quantization error\nD (e) (r|Y, E, \u00ca, d, q) := inf d(Y, \u03c0(Y )) q ,\n\n(2)\n\n\u03c0\n\nwhere the infimum is taken over all strategies \u03c0 with entropy rate r \u2265 0, and the quantization error\nD (q) (r|Y, E, \u00ca, d, q) := inf d(Y, \u03c0(Y )) q ,\n\n(3)\n\n\u03c0\n\nthe infimum being taken over all strategies \u03c0 having quantization rate r \u2265 0. Often, all\nor some of the parameters Y , E, \u00ca, d, q are clear from the context. Then we omit these\nparameters in the quantities D (e) and D (q) . The quantization information constraint is\nmore restrictive, so that the quantization error always dominates the entropy coding error.\nMoreover, the coding error increases with the moment under consideration.\nUnless otherwise stated, we choose as original Y = X and as original space E =\nC[0, \u221e). We are mainly concerned with two particular choices for \u00ca and d. In the first\nsections, we treat the case where \u00ca = D[0, 1] and d(f, g) = kf \u2212 gk[0,1] . In this setting we\nfind:\nTheorem 1.1. There exists a constant \u03ba = \u03ba(H) \u2208 (0, \u221e) such that for all q1 \u2208 (0, \u221e]\nand q2 \u2208 (0, \u221e),\nlim r H D (e) (r|q1 ) = lim r H D (q) (r|q2 ) = \u03ba.\nr\u2192\u221e\n\nr\u2192\u221e\n\nRemark 1.2. In the above theorem, general c\u00e0dl\u00e0g functions are allowed as reconstructions. Since the original process is continuous, it might seem more natural to use continuous functions as approximations. The following argument shows that, for a finite moment\nq > 0, the space \u00ca = D[0, 1] can be replaced by \u00ca = C[0, 1] without changing D (q) and\nD (e) . Let \u03c0 : C[0, 1] \u2192 D[0, 1] be an arbitrary strategy and let \u03c4n : D[0, 1] \u2192 C[0, 1] denote\nthe linear operator mapping f to its piecewise linear interpolation with supporting points\n0, n1 , n2 . . . , 1. Then\nkX \u2212 \u03c4n \u25e6 \u03c0(X)k[0,1]\n\nq\n\n\u2264 k\u03c4n (X) \u2212 \u03c4n \u25e6 \u03c0(X)k[0,1]\n\u2264 kX \u2212 \u03c0(X)k[0,1]\n\nq\n\nq\n\n+ kX \u2212 \u03c4n (X)k[0,1]\n\n+ kX \u2212 \u03c4n (X)k[0,1]\n\nq\n\nq\n\n.\n\nNote that the second term vanishes when n tends to infinity and that \u03c4n \u25e6 \u03c0 satisfies the\nsame information constraint as \u03c0.\n3\n\n\fIn the last section we conclude the article with a discussion of the case where \u00ca =\nLp [0, 1] and d(f, g) = kf \u2212 gkLp [0,1] for some p \u2265 1. In this case, one has the following\nanalog to Theorem 1.1:\nTheorem 1.3. For every p \u2265 1 there exists a constant \u03ba = \u03ba(H, p) \u2208 (0, \u221e) such that for\nall q \u2208 (0, \u221e),\nlim r H D (e) (r|q) = lim r H D (q) (r|q) = \u03ba.\nr\u2192\u221e\n\nr\u2192\u221e\n\nRemark 1.4. It is again possible to replace the space \u00ca = Lp [0, 1] by \u00ca = C[0, 1]\nwithout changing D (q) and D (e) . Indeed, for \u03b5 > 0, let h\u03b5 : R \u2192 [0, \u221e) denote a smooth\nR\nfunction supported on [\u2212\u03b5, \u03b5] with f\u03b5 = 1, and define \u03c4\u03b5 : Lp [0, 1] \u2192 C[0, 1] through\nR1\n\u03c4\u03b5 (f )(t) = 0 f (s) h(t \u2212 s) ds. Then for a given strategy \u03c0 : C[0, 1] \u2192 Lp [0, 1] one obtains\nkX \u2212 \u03c4\u03b5 \u25e6 \u03c0(X)kLp [0,1]\n\nq\n\n\u2264 k\u03c4\u03b5 (X) \u2212 \u03c4\u03b5 \u25e6 \u03c0(X)kLp [0,1]\n\u2264 kX \u2212 \u03c0(X)kLp [0,1]\n\nq\n\nq\n\n+ kX \u2212 \u03c4\u03b5 (X)kLp [0,1]\n\n+ kX \u2212 \u03c4\u03b5 (X)kLp [0,1]\n\nq\n\nq\n\n,\n\nwhere the last inequality is a consequence of Young's inequality. Now for \u03b5 \u2193 0 the second\nterm converges to 0.\nFor ease of notation, the article is restricted to the analysis of 1-dimensional processes.\n(1)\n(d)\nHowever, when replacing (Xt ) by a process (Xt , . . . , Xt ) consisting of d independent\nfractional Brownian motions, the proofs can be easily adapted, and one obtains analogous\nresults. In particular, it is possible to prove analogs of the above theorems for a multi\ndimensional Brownian motion.\nLet us summarize some of the known estimates for the constant \u03ba in the case where\nX is standard Brownian motion, i.e. H = 1/2.\n\u2022 When \u00ca = D[0, 1] and d(f, g) = kf \u2212 gk[0,1] , the relationship between the small ball\nfunction and the quantization problem (see [6]) leads to\n\u0002 \u03c0\n\u0003\n\u03ba \u2208 \u221a ,\u03c0 .\n8\n\u2022 For \u00ca = Lp [0, 1], p \u2265 1, and d(f, g) = kf \u2212 gk[Lp [0,1] , \u03ba may again be estimated via\na connection to the small ball function. Indeed, letting\nZ \u221e\no\nnZ \u221e\np 2\n1\n(\u03c6\u2032 (x))2 dx ,\n\u03bb1 = inf\n|x| \u03c6 (x) dx + 2\n\u2212\u221e\n\n\u2212\u221e\n\nwhere the infimum is taken over all weakly differentiable \u03c6 \u2208 L2 (R) with unit norm,\none has\n\u221a\n\u03ba \u2208 [c, 8 c]\n4\n\n\f\u221a\nfor c = 21/p p\n\n\u0001\n\u03bb1 (2+p)/2p\n.\n2+p\n\nIn the case where p = 2, the constant \u03ba is known explicitly: \u03ba =\n[3]).\n\n\u221a\n\n2\n\u03c0\n\n(see [12] and\n\nThe article is outlined as follows. In Sections 2 to 5 we consider the approximation\nproblems under the supremum norm. We start in Section 2 by introducting a coding\nscheme which plays an important role in the sequel. In Section 3, we use the construction\nof Section 2 and the self similarity of X to establish a polynomial decay for D (e) (*|\u221e). In\nthe following section, the asymptotics of the quantization error are computed. The proof\nrelies on a concentration property for the entropies of \"good\" coding schemes (Proposition\n4.4). In Section 5, we use the equivalence of moments in the quantization problem to\nestablish a lower bound for the entropy coding problem. In the last section, we treat the\ncase where the distortion is based on the Lp [0, 1]-norm, i.e. d(f, g) = kf \u2212 gkLp [0,1] ; we\nintroduce the distortion rate function and prove Theorem 1.3 with the help of Shannon's\nsource coding Theorem.\nIt is convenient to use the symbols \u223c, . and \u2248. We write f \u223c g iff lim fg = 1, while\nf . g stands for lim sup fg \u2264 1. Finally, f \u2248 g means\n0 < lim inf\n\n2\n\nf\nf\n\u2264 lim sup < \u221e .\ng\ng\n\nThe coding scheme\n\nThis section is devoted to the construction of strategies \u03c0 (n) : C[0, n] \u2192 D[0, n] which\nwe will need later in our discussion. The construction depends on three parameters:\nM \u2208 N\\{1}, d > 0 and a strategy \u03c0 : C[0, 1] \u2192 D[0, 1].\n(n)\nWe define the maps by induction. Let w \u2208 C[0, \u221e) and set (wt )t\u2208[0,1] := (wt+n \u2212\nwn )t\u2208[0,1] and \u0175t := \u03c0(w(0) )(t) for t \u2208 [0, 1). Assume that (\u0175t )t\u2208[0,n) (n \u2208 N) has already\nbeen defined. Then we choose \u03ben to be the smallest number in {\u2212d + 2kd/(M \u2212 1) : k =\n0, . . . , M \u2212 1} minimizing\n|wn \u2212 (\u0175n\u2212 + \u03ben )|,\nand extend the definition of \u0175 on [n, (n + 1)) by setting\n\u0175n+t := \u0175n\u2212 + \u03ben + \u03c0(w(n) )(t),\n\nt \u2208 [0, 1).\n\nNote that (\u0175t )t\u2208[0,n) depends only upon (wt )t\u2208[0,n) , so that the above construction induces\nstrategies\n\u03c0 (n) : C[0, n] \u2192 D[0, n], w 7\u2192 (w\u0304(n) )t\u2208[0,n] ,\n5\n\n\f(n)\n\nwhere w\u0304t\n\n= \u0175t for t \u2208 [0, n) and w\u0304n = \u0175n\u2212 . Moreover, we can write\n(w\u0304t )t\u2208[0,n] = \u03c0 (n) (w) = \u03c6n (\u03c0(w(0) ), . . . , \u03c0(w(n\u22121) ), \u03be1 , . . . , \u03ben\u22121 )\n\n(4)\n\nfor an appropriate measurable function \u03c6n : (D[0, n])n \u00d7 Rn\u22121 \u2192 D[0, n].\nThe main motivation for this construction is the following property. If one has, for\nsome (wt ) \u2208 C[0, \u221e) and n \u2208 N,\nkw \u2212 \u03c0 (n) (w)k[0,n]\n\n\u221e\n\n\u2264\n\nM\nd\nM \u22121\n\nand kw(n) \u2212 \u03c0(w(n) )k[0,1] \u2264 d, then\n|wn \u2212 (\u0175n\u2212 + \u03ben )| \u2264\n\nd\n,\nM \u22121\n\nwhence,\n(n)\n\nkw \u2212 \u0175k[n,n+1) = kwn + wt\n\n\u2212 (\u0175n\u2212 + \u03ben + \u03c0(w(n) )(t))k[0,1)\n\n\u2264 |wn \u2212 (\u0175n\u2212 + \u03ben )| + kw(n) \u2212 \u03c0(w(n) )k[0,1)\nM\n\u2264 d/(M \u2212 1) + d =\nd.\nM \u22121\n\nIn particular, if \u03c0 : C[0, 1] \u2192 D[0, 1] satisfies\nkX \u2212 \u03c0(X)k[0,1]\n\n\u221e\n\n\u2264 d,\n\nthen for any n \u2208 N,\nkX \u2212 \u03c0 (n) (X)k[0,n]\n\n3\n\n\u221e\n\n\u2264\n\nM\nd.\nM \u22121\n\n(5)\n\nPolynomial decay of D(e) (r|\u221e)\n\nThe objective of this section is to prove the following theorem.\nTheorem 3.1. There exists a constant \u03ba = \u03ba(H) \u2208 (0, \u221e) such that\nlim r H D (e) (r|\u221e) = \u03ba.\n\nr\u2192\u221e\n\n(6)\n\nThereafter, \u03ba = \u03ba(H) will always denote the finite constant defined via equation (6).\nIn order to simplify notations, we abridge k * k = k * k[0,1] .\n\n6\n\n\fRemark 3.2. It was found in [3] (see Theorem 3.5.2) that for finite moments q \u2265 1 the\nentropy coding error is related to the asymptotic behavior of the small ball function of the\nGaussian measure. In particular, for fractional Brownian motion, one obtains that\nD (e) (r|q) \u2248\n\n1\n,\nrH\n\nr \u2192 \u221e.\n\nIn order to show that D (e) (r|\u221e) is of the order r \u2212H , we still need to prove an appropriate\nupper bound. We prove a stronger statement which will be useful later on.\nLemma 3.3. There exist strategies \u03c0 (r) : C[0, 1] \u2192 C[0, 1], r \u2265 0, and probability weights\n(r)\n(pw )w\u2208im(\u03c0(r) ) such that for any q \u2265 1,\nkX \u2212 \u03c0 (r) (X)k\n\n\u221e\n\n\u2264\n\n1\nrH\n\nand\n\n(r)\n\nE[(\u2212 log p\u03c0(r) (X) )q ]1/q \u2248 r.\n\n(7)\n\nIn particular, D (e) (r|\u221e) \u2248 r \u2212H .\nThe proof of the lemma is based on an asymptotic estimate for the mass concentration\nin randomly centered small balls, to be found in [7]. Let X\u03031 denote a fractional Brownian\nmotion that is independent of X with L(X) = L(X\u03031 ). Then, for any q \u2208 [1, \u221e), one has\nE[(\u2212 log P(kX \u2212 X\u03031 k \u2264 \u03b5|X))q ]1/q \u2248 \u2212 log P(kXk \u2264 \u03b5) \u2248 \u03b5\u22121/H\n\n(8)\n\nas \u03b5 \u2193 0 (see [7], Theorem 4.2 and Corollary 4.4).\nProof. For a given D[0, 1]-valued sequence (w\u0303n )n\u2208N\u222a{\u221e} , we consider the following coding\nstrategy \u03c0 (r) (*|(w\u0303n )): let\nT (r) (w) := T (r) (w|(w\u0303n )) := inf{n \u2208 N : kw \u2212 w\u0303n k \u2264 1/r H },\nwith the convention that the infimum of the empty set is \u221e, and set\n\u03c0 (r) (w) := \u03c0 (r) (w|(w\u0303n )) := w\u0303T (r) (w) .\nMoreover, let (pn )n\u2208N denote the sequence of probability weights defined as\npn =\n\n6 1\n,\n\u03c0 2 n2\n\nn \u2208 N,\n\nand set p\u221e := 0.\nNow we let (X\u0303n )n\u2208N\u222a{\u221e} denote independent FBM's that are also independent of\nX, and analyze the random coding strategies \u03c0 (r) (*) := \u03c0 (r) (*|(X\u0303n )). With T (r) :=\nT (r) (X|(X\u0303n )) we obtain\nX\u0302 (r) := \u03c0 (r) (X) = X\u0303T (r) ,\n7\n\n\fand\nE[(\u2212 log pT (r) )q ]1/q \u2264 2E[(log T (r) )q ]1/q + log\n\n\u03c02\n.\n6\n\n(9)\n\nGiven X, the random time T (r) is geometrically distributed with parameter P(kX \u2212 X\u03031 k \u2264\n1/r H |X), and due to Lemma A.2 there exists a universal constant c1 = c1 (q) < \u221e for\nwhich\nE[(log T (r) )q |X]1/q \u2264 c1 [1 + log E[T (r) |X]] = c1 [1 + log 1/P(kX \u2212 X\u03031 k \u2264 1/r H |X)].\nConsequently,\n\u0002\n\u00031/q\nE[(log T (r) )q ]1/q = E E[(log T (r) )q |X]\n\n\u2264 c1 E[(1 + log 1/P(kX \u2212 X\u03031 k \u2264 1/r H |X))q ]1/q\n\n(10)\n\n\u2264 c1 (1 + E[(\u2212 log P(kX \u2212 X\u03031 k \u2264 1/r H |X))q ]1/q ).\nDue to (8), one has\nE[(\u2212 log P(kX \u2212 X\u03031 k \u2264 1/r H |X))q ]1/q \u2248 r,\nso that (9) and (10) imply that E[(\u2212 log pT (r) )q ]1/q . c2 r for some appropriate constant\nc2 < \u221e. In particular, for any r \u2265 0, we can find a C[0, 1]-valued sequence (w\u0303(r) )n\u2208N of\npairwise different elements such that\nE[(\u2212 log pT (r) (X|(w\u0303(r) )) )q ]1/q \u2264 E[(\u2212 log pT (r) )q ]1/q . c2 r.\nn\n\n(r)\n\nNow the strategies \u03c0 (r) (*|(w\u0303n )) with associated probability weights p\nsatisfy (7). Moreover, D (e) (r|\u221e) \u2248 r \u2212H follows since\n\u0002\n(r)\nH(\u03c0 (r) (X|(w\u0303n(r) ))) \u2264 E \u2212 log p (r)\n\u03c0\n\n(r)\n(X|(w\u0303n )\n\n\u0003\n\n(r)\n(r)\n\nw\u0303n\n\n:= pn (n \u2208 N)\n\n.\n\u0003\n\nLet us now use the coding scheme of Section 2 to prove\nLemma 3.4. Let n \u2208 N, r \u2265 0 and \u2206r \u2265 1. Then\nD (e) (n(r + \u2206r)|\u221e) \u2264 n\u2212H\n\ne\u2206r\nD (e) (r|\u221e).\ne\u2206r \u2212 2\n\nProof. Fix \u03b5 > 0 and let \u03c0 : C[0, 1] \u2192 D[0, 1] be a strategy satisfying\nkX \u2212 \u03c0(X)k[0,1]\n\n\u221e\n\n\u2264 (1 + \u03b5)D (e) (r|\u221e) =: d\n8\n\n(11)\n\n\fand\nH(\u03c0(X)) \u2264 r.\nChoose M := \u230ae\u2206r \u230b and let \u03c0 (n) be as in Section 2. Note that \u2206r \u2265 1 guarantees that\nM \u2265 e\u2206r \u2212 1 \u2265 e\u2206r /2, so that\nkX \u2212 \u03c0 (n) (X)k[0,n]\n\n\u221e\n\n\u2264\n\nM\ne\u2206r\nd \u2264 \u2206r\n(1 + \u03b5)D (e) (r|\u221e).\nM \u22121\ne \u22122\n\n(i)\n\nWe let (Xt )t\u2208[0,1] = (Xi+t \u2212 Xi )t\u2208[0,1] for i = 1, . . . , n, and (\u03bei )i=1,...,n\u22121 be as in Section\n2 for w = X. Observe that, due to the representation (4),\nH(\u03c0 (n) (X)) \u2264 H(\u03c0(X (0) ), . . . , \u03c0(X (n\u22121) ), \u03be1 , . . . , \u03ben\u22121 )\n\n\u2264 H(\u03c0(X (0) )) + * * * + H(\u03c0(X (n\u22121) )) + H(\u03be1 , . . . , \u03ben\u22121 )\n\n\u2264 nr + log | range (\u03be1 , . . . , \u03ben\u22121 )| \u2264 nr + n log M\n\n(12)\n\n\u2264 n(r + \u2206r).\nNow let\n\u03b1n : D[0, 1] \u2192 D[0, n], f 7\u2192 \u03b1n (f )(s) = nH f (s/n)\nand consider the strategy\n(n)\n\u03c0\u0303 : C[0, 1] \u2192 D[0, 1], f 7\u2192 \u03b1\u22121\n\u25e6 \u03b1n (f ).\nn \u25e6\u03c0\n\nSince \u03b1n (X) is again a fractional Brownian motion on [0, n], it follows that, a.s.\nkX \u2212 \u03c0\u0303(X)k[0,1] = n\u2212H k\u03b1n (X) \u2212 \u03c0 (n) (\u03b1n (X))k[0,n] \u2264 (1 + \u03b5) n\u2212H\nMoreover,\n\ne\u2206r\nD (e) (r|\u221e).\ne\u2206r \u2212 2\n\n(n)\nH(\u03c0\u0303(X)) = H(\u03b1\u22121\n(\u03b1n (X))) = H(\u03c0 (n) (X)) \u2264 r.\nn \u25e6\u03c0\n\nSince \u03b5 > 0 is arbitrary, the proof is complete.\n\n\u0003\n\nProof of Theorem 3.1. For r \u2265 0, \u2206r \u2265 1 and n \u2208 N, Lemma 3.4 yields\nD (e) (n(r + \u2206r)|\u221e) \u2264\n\n1\ne\u2206r\nD (e) (r|\u221e).\nH\n\u2206r\nn e \u22122\n\nNow set \u03ba := lim inf r\u2192\u221e r H D (e) (r|\u221e) which lies in (0, \u221e) due to Lemma 3.3. Let \u03b5 \u2208\n(0, 1/2) be arbitrary, and choose r0 , \u2206r \u2265 1 such that\n\uf8f1\n\uf8f4\n\uf8f4\nr H D (e) (r0 |\u221e) \u2264 (1 + \u03b5)\u03ba,\n\uf8f4\n\uf8f2 0\n\u2206r \u2264 \u03b5r0\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3e\u2212\u2206r \u2264 \u03b5.\n\nand\n\n9\n\n\fThen\n1\n1\nD (e) (r0 |\u221e)\nH\nn 1 \u2212 2\u03b5\n1\n1\n\u2264\n(1 + \u03b5)1+H \u03ba\n\u0001H\n1\n\u2212\n2\u03b5\n(1 + \u03b5)nr0\n\nD (e) ((1 + \u03b5)nr0 |\u221e) \u2264\n\nand we obtain that\nlim sup (1 + \u03b5)nr0\nn\u2192\u221e\n\n\u0001H\n\nD (e) ((1 + \u03b5)nr0 |\u221e) \u2264\n\n(1 + \u03b5)1+H\n\u03ba.\n1 \u2212 2\u03b5\n\nLet now r \u2265 (1 + \u03b5)r0 and introduce r\u0304 = r\u0304(r) = min{(1 + \u03b5)nr0 : n \u2208 N, r \u2264 (1 + \u03b5)nr0 }\nas well as r = r(r) = max{(1 + \u03b5)nr0 : n \u2208 N, (1 + \u03b5)nr0 \u2264 r}. Using the monotonicity of\nD (e) (r|\u221e), we conclude that\nlim sup r H D (e) (r|\u221e) \u2264 lim sup r\u0304 H D (e) (r|\u221e)\nr\u2192\u221e\n\nr\u2192\u221e\n\n\u2264 lim sup (r + (1 + \u03b5)r0 )H D (e) (r|\u221e)\nr\u2192\u221e\n\n\u2264\n\n(1 + \u03b5)1+H\n\u03ba.\n1 \u2212 2\u03b5\n\nNoticing that \u03b5 > 0 is arbitrary finishes the proof.\n\n4\n\n\u0003\n\nThe quantization problem\n\nTheorem 4.1. One has for any q \u2208 (0, \u221e),\nD (q) (r|q) \u223c \u03ba\n\n1\n,\nrH\n\nr \u2192 \u221e.\n\nWe need some preliminary lemmas for the proof of the theorem.\n(r)\n\nLemma 4.2. There exist strategies (\u03c0 (r) )r\u22650 and probability weights (pw ) such that\nkX \u2212 \u03c0 (r) (X)k\n\n\u221e\n\n\u2264\u03ba\n\n1\nrH\n\nand\n\n(r)\n\n\u2212 log p\u03c0(r) (X) . r,\n\nProof. Let \u03b5 > 0 and choose r0 \u2265 2 such that\n\u0010 r + 1 \u00111/H\n\u03b5\n0\n\u22641+\nr0 \u2212 1\n2\nBy Theorem 3.1,\nD (e) ((1 + \u03b5/2)r|\u221e) . \u03ba\n\n10\n\nr0 \u2212 1 1\nr0 + 1 r H\n\nin probability.\n\n\fIn particular, there exists r1 \u2265 r0 \u2228 2\u03b5 log(r0 + 1) and a map \u03c0 : C[0, 1] \u2192 D[0, 1] such that\nkX \u2212 \u03c0(X)k[0,1]\n\n\u221e\n\n\u2264\u03ba\n\nr0 \u2212 1 1\n=: d and\nr0 r1H\n\nH(\u03c0(X)) \u2264 (1 + \u03b5/2)r1 .\n\nFor n \u2208 N, let \u03c0 (n) and \u03c6n be as in Section 2 for M = \u2308r0 \u2309, d and \u03c0. Then by (5)\nkX \u2212 \u03c0 (n) (X)k[0,n]\n\n\u221e\n\n\u2264\u03ba\n\n(r0 \u2212 1)M 1\n1\n\u2264 \u03ba H.\nH\nr0 (M \u2212 1) r1\nr1\n\nFor \u0175(0) , . . . , \u0175(n\u22121) \u2208 im(\u03c0) and k1 , . . . , kn\u22121 \u2208 {\u2212d +\nbe defined as\n(n)\n(0) ,...,\u0175 (n\u22121) ,k ,...,k\nn (\u0175\n1\nn\u22121 )\n\np\u03c6\n\n=\n\n1\nM n\u22121\n\nn\u22121\nY\n\n2kd\nM \u22121\n\n(13)\n\n: k = 0, . . . , M \u2212 1}, let p(n)\n\nP(\u03c0(X) = \u0175(i) ).\n\ni=0\n\n(n)\n\nThe (pw ) define probability weights on the image of \u03c6n . Moreover,\n\u2212 log p\n\n(n)\n(X\u0302t )t\u2208[0,n]\n\n= (n \u2212 1) log M \u2212\n\nn\u22121\nX\n\nlog p\u03c0(X (i) )\n\ni=0\n\nand the ergodic theorem implies\nlim \u2212\n\nn\u2192\u221e\n\n1\n(n)\nlog p\n= log M + H(\u03c0(X)),\n(X\u0302t )t\u2208[0,n]\nn\n\na.s.\n\nNote that log M + H(\u03c0(X)) \u2264 (1 + \u03b5)r1 .\nJust as in the proof of Lemma 3.4, we use the self similarity of X to translate the\nstrategy \u03c0 (n) into a strategy for encoding (Xt )t\u2208[0,1] . For n \u2208 N, let\n\u03b1n : D[0, 1] \u2192 D[0, n], f 7\u2192 (\u03b1n f )(t) = nH f (t/n)\n(n)\n\n(n)\n\n(n) \u25e6 \u03b1 (w). Then\nand consider p\u0303w := p\u03b1n (w) and \u03c0\u0303 (n) (w) := \u03b1\u22121\nn\nn \u25e6\u03c0\n(n)\n\n(n)\n\n\u2212 log p\u0303\u03c0\u0303(n) (X) = \u2212 log p\u03c0(n) (\u03b1\n\nn (X))\n\n. (1 + \u03b5)nr1 ,\n\nin probability\n\nand by (13)\nkX \u2212 \u03c0\u0303 (n) (X)k[0,1]\n\n\u221e\n\n(n)\n= k\u03b1\u22121\n(\u03b1n (X)))k[0,1] \u221e\nn (\u03b1n (X) \u2212 \u03c0\n1\n= H k\u03b1n (X) \u2212 \u03c0 (n) (\u03b1n (X))k[0,n] \u221e\nn\n1\n1\n= H kX \u2212 \u03c0 (n) (X)k[0,n] \u221e \u2264 \u03ba\n.\nn\n(nr1 )H\n\n11\n\n\fBy choosing \u03c0\u0304 (r) = \u03c0\u0303 (n) and (p\u0304(r) ) = (p\u0303(n) ) for r \u2208 ((n \u2212 1)r1 , nr1 ], one obtains a coding\nscheme satisfying\n1\nkX \u2212 \u03c0\u0304 (r) (X)k \u221e \u2264 \u03ba H\nr\nand\n(r)\n\u2212 log p\u0304\u03c0\u0304(r) (X) . (1 + \u03b5)r, in probability,\nso that the assertion follows by a diagonalization argument.\n\n\u0003\n\nRemark 4.3. In the above proof, we have constructed a high resolution coding scheme\n(n) \u25e6 \u03b1 . This\nbased on a strategy \u03c0 : C[0, 1] \u2192 D[0, 1], using the identity \u03c0\u0303n = \u03b1\u22121\nn\nn \u25e6\u03c0\ncoding scheme leads to a coding error which is at most\nM\nkX \u2212 \u03c0(X)k[0,1]\nM \u22121\n\n\u221e\n\nn\u2212H .\n\n(14)\n\nMoreover, the ergodic theorem implies that, for large n, \u03c0\u0303n (X) lies with probability almost\n(n)\none in the typical set {w \u2208 D[0, 1] : \u2212 log p\u0303w \u2264 n(H(\u03c0(X)) + log M + \u03b5)}, where \u03b5 > 0 is\narbitrarily small. This set is of size exp{n(H(\u03c0(X)) + log M + \u03b5)}, and will serve as a close\nto optimal high resolution codebook. It remains to control the case where \u03c0\u0303n (X) is not in\nthe typical set. We will do this in the proof of Theorem 4.1 at the end of this section (see\n(19)).\n(r)\n\nProposition 4.4. For q \u2265 1 there exist strategies (\u03c0 (r) )r\u22650 and probability weights (pw )\nsuch that\nkX \u2212 \u03c0\n\n(r)\n\n(X))k\n\n\u221e\n\n1\n\u2264\u03ba H\nr\n\n(r)\n\nand\n\nlim\n\nE[(\u2212 log p\u03c0(r) (X) )q ]1/q\n\nr\u2192\u221e\n\nr\n\n= 1.\n\nIn addition, for any \u03b5 > 0 one has\n\u0010\n1 \u0011\nlim sup P \u2212 log p\u03c0(X) \u2264 (1 \u2212 \u03b5)r, kX \u2212 \u03c0(X)k \u2264 \u03ba H = 0,\nr\u2192\u221e \u03c0,(p )\nr\nw\n\n(15)\n\n(16)\n\nwhere the supremum is taken over all strategies \u03c0 : C[0, 1] \u2192 D[0, 1] and over all sequences\nof probability weights (pw ).\n(r)\n\n(r,1)\n\nProof. Let q > 1 and let \u03c01 (r \u2265 0) be a strategy and (pw ) a sequence of probability\n(r)\n(r,2)\nweights as in Lemma 4.2. Moreover, let \u03c02 and (pw ) (r \u2265 0) be as in Lemma 3.3 for\n(r)\n(r,1)\n(r)\n(r,2)\n2q. We consider the maps \u03ba1 (w) := \u2212 log p (r)\nand \u03ba2 (w) := \u2212 log p (r) , and set\n\u03c01 (w)\n\n\u03c0\n\n(r)\n\n(w) :=\n\n\uf8f1\n\uf8f2\u03c0 (r) (w)\n1\n\uf8f3\u03c0 (r) (w)\n2\n\n(r)\n\nif \u03ba1 (w) \u2264 (1 + \u03b4)r,\notherwise,\n12\n\n\u03c02 (w)\n\n\f(r)\n\n(r,1)\n\nfor some fixed \u03b4 > 0. Then one obtains, for pw = 21 (pw\n(r)\n\u03ba1 (w) \u2264 (1 + \u03b4)r},\n(r)\n\n(r,2)\n\n+ pw\n\n) and Tr := {w \u2208 C[0, 1] :\n\n(r)\n\n(r)\n\nE[(\u2212 log 2p\u03c0(r) (X) )q ]1/q \u2264 E[1Tr (X)\u03ba1 (X)q ]1/q + E[1Trc (X)\u03ba2 (X)q ]1/q\n(r)\n\n\u2264 (1 + \u03b4)r + P(X \u2208 Trc )1/2q E[\u03ba2 (X)2q ]1/2q .\n(r)\n\n(r)\n\n(r)\n\nThe definitions of \u03c01 and \u03c02 imply that limr\u2192\u221e P(X \u2208 Trc ) = 0 and E[\u03ba2 (X)2q ]1/2q \u2248\nr. Consequently,\n(r)\nE[(\u2212 log p\u03c0(r) (X) )q ]1/q . (1 + \u03b4)r.\nSince \u03b4 > 0 can be chosen arbitrarily small, a diagonalization procedure leads to strategies\n(r)\n\u03c0\u0303 (r) and probability weights (p\u0303w ) with\nkX \u2212 \u03c0\u0303 (r) (X)k[0,1]\n\n\u221e\n\n\u2264\u03ba\n\n1\nand E[(\u2212 log p\u0303\u03c0\u0303(r) (X) )q ]1/q . r,\nrH\n\nwhich proves the first assertion.\nIt remains to show that for arbitrary strategies \u03c0\u0304 (r) , r \u2265 0, and probability weights\n(r)\n(p\u0304w ):\n\u0010\n1 \u0011\n(r)\n(17)\nlim P \u2212 log p\u0304\u03c0\u0304(r) (X) \u2264 (1 \u2212 \u03b5)r, kX \u2212 \u03c0\u0304 (r) (X)k \u2264 \u03ba H = 0.\nr\u2192\u221e\nr\nWithout loss of generality, we can assume that\nkX \u2212 \u03c0\u0304 (r) (X)k[0,1]\n\n\u221e\n\n\u2264\u03ba\n\n1\n.\nrH\n\n(18)\n\nOtherwise we modify the map \u03c0\u0304 (r) for all w \u2208 C[0, 1] with kw \u2212 \u03c0\u0304 (r) (w)k > \u03ba r \u2212H in such\na way that (18) be valid. Hereby the probability in (17) increases and it suffices to prove\nthe statement for the modified strategy. Let us consider\n\uf8f1\n(r)\n\uf8f2\u03c0\u0304 (r) (w) if p\u0304(r)\n\u2265 p\u0303\u03c0\u0303(r) (w)\n(r)\n\u03c0\u0304 (r) (w)\n\u03c0 (w) =\n\uf8f3\u03c0\u0303 (r) (w) else.\n\nThen the probability weights p(r) := 12 (p\u0304(r) + p\u0303(r) ) satisfy\n(r)\n\n(r)\n\nE[(\u2212 log 2p\u03c0(X) )q ]1/q \u2264 E[(\u2212 log p\u0303\u03c0\u0303(X) )q ]1/q . r.\nRecall that\nkX \u2212 \u03c0 (r) (X)k[0,1]\n(r)\n\n\u221e\n\n\u2264\u03ba\n\n1\n,\nrH\n\nhence by Theorem 3.1, one has E[\u2212 log p\u03c0(r) (X) ] \u2265 H(\u03c0 (r) (X)) & r. Lemma A.1 thus\nimplies that\n(r)\n\u2212 log p\u03c0(r) (X) \u223c r,\nin probability.\n13\n\n\fIn particular,\n(r)\n\n(r)\n\n\u2212 log p\u0304\u03c0\u0304(r) (X) \u2265 \u2212 log 2p\u03c0(r) (X) & r,\n\nin probability,\n\nwhich implies (17).\n\n\u0003\n\nProof of Theorem 4.1. We start by proving the lower bound. Fix q > 0, let Cr , r \u2265 0,\ndenote arbitrary codebooks of size er , and let \u03c0 (r) : C[0, 1] \u2192 Cr denote arbitrary strategies.\n(r)\n(r)\nMoreover, let (pw ) be the sequence of probability weights defined as pw = 1/|Cr |, w \u2208 Cr .\n(r)\nThen \u2212 log p\u03c0(r) (X) \u2264 r a.s., and the above lemma implies that for any \u03b5 \u2208 (0, 1),\n\u0010\n(1 \u2212 \u03b5)H \u0011\nlim P kX \u2212 \u03c0 (r) (X)k \u2264 \u03ba\n= 0.\nr\u2192\u221e\nrH\nTherefore,\n(1 \u2212 \u03b5)H \u00111/q\n(1 \u2212 \u03b5)H\n(1 \u2212 \u03b5)H \u0010\n(r)\nP\nkX\n\u2212\n\u03c0\n(X)k\n\u2265\n\u03ba\n,\n\u223c\n\u03ba\nrH\nrH\nrH\nwhich proves the lower bound.\nIt remains to show that D (q) (r, q) . \u03ba/r H . By Lemma 4.2, there exist strategies \u03c0 (r)\n(r)\nand probability weights (pw ) such that\nE[kX \u2212 \u03c0 (r) (X)kq ]1/q \u2265 \u03ba\n\n1\nand \u2212 log p\u03c0(r) (X) . r, in probability.\nrH\nFurthermore, due to Theorem 4.1 in [6], there exist codebooks C \u0304r of size er with\nkX \u2212 \u03c0 (r) (X)k\n\n\u221e\n\n\u2264\u03ba\n\nE[ min kX \u2212 \u0175k2q ]1/2q \u2248\n\u0175\u2208C \u0304r\n\n1\n.\nrH\n\n(r)\n\nWe consider the codebook Cr := C \u0304r \u222a {\u0175 : \u2212 log p\u0175 \u2264 (1 + \u03b5/2)r}. Clearly, Cr contains at\nmost er + e(1+\u03b5/2)r elements. Moreover,\nE[ min kX \u2212 \u0175kq ]1/q \u2264 E[1Cr (\u03c0 (r) (X)) (\u03ba\n\u0175\u2208Cr\n\n1 q 1/q\n) ]\nrH\n\n+ E[1Crc (\u03c0 (r) (X)) min kX \u2212 \u0175kq ]1/q\n\u0175\u2208C\u0304r\n\n\u2264\u03ba\n\n(19)\n\n1\n+ P(\u03c0 (r) (X) 6\u2208 Cr )1/2q E[ min kX \u2212 \u0175k2q ]1/2q .\nrH\n\u0175\u2208C \u0304r\n\nSince limr\u2192\u221e P(\u03c0 (r) (X) 6\u2208 Cr ) = 0 and the succeeding expectation is of order O(1/r H ),\nthe second summand is of order o(1/r H ). Therefore, for r \u2265 2/\u03b5\nD (q) ((1 + \u03b5)r|q) \u2264 E[ min kX \u2212 \u0175kq ]1/q . \u03ba\n\u0175\u2208Cr\n\n1\n.\nrH\n\nBy switching from r to r\u0303 = (1 + \u03b5)r, we obtain\nD (q) (r\u0303|q) . \u03ba (1 + \u03b5)H\nSince \u03b5 > 0 was arbitrary, the proof is complete.\n14\n\n1\n.\nr\u0303 H\n\u0003\n\n\f5\n\nImplications of the equivalence of moments\n\nIn this section we complement Theorem 4.1 by\nTheorem 5.1. For arbitrary q \u2208 (0, \u221e], one has\nD (e) (r|q) \u223c \u03ba\n\n1\n.\nrH\n\nThe proof of this theorem is based on the following general principle: if the asymptotic\nquantization error coincides for two different moments q1 < q2 , then all moments q \u2264 q2\nlead to the same asymptotic quantization error and the entropy coding problem coincides\nwith the quantization problem for all moments q \u2264 q2 .\nLet us prove this relationship in a general setting. E and \u00ca denoting arbitrary measurable spaces and d : E \u00d7 \u00ca \u2192 [0, \u221e) a measurable function, the quantization error for\na general E-valued r.v. X under the distortion d is defined as\nD (q) (r|q) = inf E[min d(X, x\u0302)q ]1/q ,\nC\u2282E\n\nx\u0302\u2208C\n\nwhere the infimum is taken over all codebooks C \u2282 \u00ca with |C| \u2264 er . In order to simplify\nnotations, we abridge\nd(x, A) = inf d(x, y),\ny\u2208A\n\nx \u2208 E, A \u2282 \u00ca.\n\nAnalogously, we denote the entropy coding error by\nD (e) (r|q) = inf E[d(X, X\u0302)q ]1/q ,\nX\u0302\n\nwhere the infimum is taken over all discrete \u00ca-valued r.v. X\u0302 with H(X\u0302) \u2264 r.\nThen Theorem 5.1 is a consequence of Theorem 4.1 and the following theorem.\nTheorem 5.2. Assume that f : [0, \u221e) \u2192 R+ is a decreasing, convex function satisfying\n+\n\n\u2212r \u2202\u2202r f (r)\nlim sup\n< \u221e,\nf (r)\nr\u2192\u221e\nand suppose that, for some 0 < q1 < q2 ,\nD (q) (r + log 2|q1 ) \u223c D (q) (r|q2 ) & f (r).\nThen for any q > 0,\nD (e) (r|q) & f (r).\nWe need some technical lemmas.\n15\n\n(20)\n\n\fLemma 5.3. Let 0 < q1 < q2 and f : [0, \u221e) \u2192 R+ . If\nD (q) (r + log 2|q1 ) \u223c D (q) (r|q2 ) \u223c f (r),\nthen for any \u03b5 > 0,\nlim sup P(d(X, C) \u2264 (1 \u2212 \u03b5)f (r)) = 0.\n\nr\u2192\u221e C\u2282E:\n|C|\u2264er\n\nProof. For r \u2265 0, let Cr\u2217 denote codebooks of size er with\nE[d(X, Cr\u2217 )q2 ]1/q2 \u223c f (r).\n\n(21)\n\nNow let Cr denote arbitrary codebooks of size er , and consider the codebooks C \u0304r := Cr\u2217 \u222aCr .\nUsing (21) and the inequality q1 \u2264 q2 , it follows that\nf (r) & E[d(X, C \u0304r )q2 ]1/q2 \u2265 E[d(X, C \u0304r )q1 ]1/q1 \u2265 D (q) (r + log 2|q1 ) \u223c f (r).\nHence, Lemma A.1 implies that\nd(X, C \u0304r ) \u223c f (r),\n\nin probability,\n\nd(X, Cr ) & f (r),\n\nin probability.\n\nso that in particular,\n\n\u0003\nLemma 5.4. Assume that f : [0, \u221e) \u2192 R+ is a decreasing, convex function satisfying\n(20) and\nlim sup P(d(X, C) \u2264 f (r)) = 0.\nr\u2192\u221e C\u2282E:\n|C|\u2264er\n\nThen for any q > 0,\nD (e) (r|q) & f (r).\nProof. The result is a consequence of the technical Lemma A.3. Consider the family F\nconsisting of all random vectors\n(A, B) = (d(X, X\u0302)q , \u2212 log pX\u0302 ),\nwhere X\u0302 is an arbitrary discrete E-valued r.v. and (pw ) is an arbitrary sequence of probability weights on the range of X\u0302. Let f \u0303(r) = f (r)q , r \u2265 0. Then for any choice of X\u0302\nand (pw ) and an arbitrary r \u2265 0, the set C := {w \u2208 E : \u2212 log pw \u2264 r} contains at most er\nelements. Consequently,\nP(d(X, X\u0302)q \u2264 f \u0303(r), \u2212 log pX\u0302 \u2264 r) = P(d(X, X\u0302) \u2264 f (r), X\u0302 \u2208 C) \u2264 P(d(X, C) \u2264 f (r)).\n16\n\n\fBy assumption the right hand side converges to 0 as r \u2192 \u221e ,independently of the choice\nof X\u0302 and (pw ). Since f \u0303 satisfies condition (27), Lemma A.3 implies that\nD (e) (r|q) =\n\nE[d(X, X\u0302)q ]1/q = inf E[A]1/q & f \u0303(r)1/q = f (r),\n\ninf\n\nA\u2208Fr\n\nX\u0302:H(X\u0302)\u2264r\n\nwhere Fr = {A : (A, B) \u2208 F, EB \u2264 r}.\n\n\u0003\n\nTheorem 5.2 is now an immediate consequence of Lemma 5.3 and Lemma 5.4.\n\n6\n\nCoding with repect to the Lp [0, 1]-norm distortion\n\nIn this section, p \u2208 [1, \u221e) is fixed. In contrast to the previous sections, we consider entropy\ncoding and quantization of X in Lp [0, 1], i.e. \u00ca = Lp [0, 1] and d(f, g) = kf \u2212 gkLp [0,1] . In\norder to treat these approximation problems, we need to introduce Shannon's distortion\nrate function. It is defined as\nD(r|q) = inf kX \u2212 X\u0302kLp [0,1]\n\nq\n\n,\n\nwhere the infimum is taken over all \u00ca-valued r.v.'s X\u0302 satisfying the mutual information\nconstraint I(X; X\u0302) \u2264 r. Here and elsewhere I denotes the Shannon mutual information,\ndefined as\n\uf8f1R\n\uf8f2 log dPX,X\u0302 dP\nif PX,X\u0302 \u226a PX \u2297 PX\u0302\ndPX \u2297PX\u0302\nX,X\u0302\nI(X; X\u0302) =\n\uf8f3\u221e\nelse.\nThe objective of this section is to prove\n\nTheorem 6.1. The following limit exists\n\u03bap = \u03bap (H) = lim r H D(r|p) \u2208 (0, \u221e),\nr\u2192\u221e\n\n(22)\n\nand for any q > 0, one has\nD (q) (r|q) \u223c D (e) (r|q) \u223c \u03bap\n\n1\n.\nrH\n\n(23)\n\nWe will first prove that statement (23) is valid for\n\u03bap := lim inf r H D(r|p).\nr\u2192\u221e\n\nSince D(r|p) is dominated by D (q) (r|p), the existence of the limit in (22) then follows\nimmediately. Due to Theorem 1.2 in [4], the distortion rate function D(*|p) has the same\nweak asymptotics as D (q) (*|p). In particular, D(r|p) \u2248 r \u2212H and \u03bap lies in (0, \u221e).\n17\n\n\fWe proceed as follows: decomposing X into the two processes\nX (1) = (Xt \u2212 X\u230at\u230b )t\u22650\n\nand\n\nX (2) = (X\u230at\u230b )t\u22650 ,\n\nwe consider the coding problem for X (1) and X (2) in Lp [0, n] (n \u2208 N being large). We\ncontrol the coding complexity of the first term via Shannon's Source Coding Theorem\n(SCT) and use a limit argument in order to show that the coding complexity of X (2)\nis asymptotically negligible. We recall the SCT in a form which is appropriate for our\ndiscussion; for n \u2208 N, let\n\u0010Z 1\n\u00111/p\ndp (f, g) =\n|f (t) \u2212 g(t)|p dt\n0\n\nand\ndn,p (f, g) =\n\n\u0010Z\n\nn\n0\n\n|f (t) \u2212 g(t)|p\n\ndt \u00111/p\n.\nn\n\nThen d \u0303n (f, g) = dn,p (f, g)p , n \u2208 N, is a single letter distortion measure, when interpreting\nthe function f |[0,n) as the concatenation of the \"letters\" f (0) , . . . , f (n\u22121) , where f (i) = (f (i+\nt))t\u2208[0,1) . Analogously, the process X (1) corresponds to the letters X (1,i) := (Xi+t )t\u2208[0,1) ,\ni \u2208 N0 . Since (X (1,i) )i\u2208N0 is an ergodic stationary C[0, 1)-valued process, the SCT implies\nthat for fixed r \u2265 0 and \u03b5 > 0 there exist codebooks Cn \u2282 Lp [0, n], n \u2208 N, with at most\nexp{(1 + \u03b5)nr} elements such that\nlim P(d \u0303n (X (1) , Cn ) \u2264 (1 + \u03b5)D(r|p)p ) = 1.\n\nn\u2192\u221e\n\n(24)\n\nA proof of this statement can be carried out by using the asymptotic equipartition property\nas stated in [2] (Theorem 1). The proof is standard and therefore omitted. For further\ndetails concerning the distortion rate function one can consult [1] or [2].\nFirst we prove a lemma which will later be used to control the coding complexity\nof X (2) .\nLemma 6.2. Let (Zi )i\u2208N be an ergodic stationary sequence of real-valued r.v.'s and let\nP\nSn = ni=1 Zi , n \u2208 N0 . Then there exist codebooks Cn \u2282 Rn of size exp{nE[log(|Z1 |/2\u03b5 +\n2)] + nc} satisfying\n\u0001\nn ) \u2264 \u03b5 = 1,\nlim P min kS1n \u2212 \u015dkl\u221e\nn\u2192\u221e\n\n\u015d\u2208C\n\nS1n\n\nn denotes the maximum\nwhere\ndenotes (Si )i=1,...,n , c is a universal constant and k * kl\u221e\nn\nnorm on R .\n\nProof. Let c > 0 be such that (pn )n\u2208Z defined through\npn = e\u2212c\n\n1\n(|n| + 1)2\n\n18\n\n\fis a sequence of probability weights. For a given sequence (sn )n\u2208N , we define a reconstruction (\u015dn ) recursively. The construction depends on a parameter \u03b5 > 0. Let \u015d0 = 0 and\nsuppose that \u015dn0 = (\u015di )i=0,...,n is already defined. Then we choose a \u03ben+1 \u2208 2\u03b5R minimizing\nthe distance\n|sn+1 \u2212 (\u015dn + \u03ben+1 )|\nand set \u015dn+1 := \u015dn + \u03ben+1 . This defines maps \u03c0n : Rn \u2192 Rn , sn1 7\u2192 \u03c0n (sn1 ) := \u015dn1 . We equip\nthe range of \u03c0n with a sequence of probability weights via\nn\nY\n\n(n)\n\np\u015dn =\n1\n\np\u03bei /2\u03b5 .\n\ni=1\n\nThen\n(n)\n\u2212 log p\u015dn\n1\n\n\u22642\n\nn\nX\ni=1\n\nlog(|\u03bei |/2\u03b5 + 1) + n c.\n\nNow consider \u03c0n (S1n ). Let \u03ben = \u03ben ((Si )) be as above when replacing the deterministic\nargument (sn ) by (Sn ). Then\n|\u03ben \u2212 Zn | = |\u015cn \u2212 \u015cn\u22121 \u2212 Sn + Sn\u22121 | \u2264 2\u03b5\nand, hence, |\u03ben | \u2264 |Zn | + 2\u03b5. Consequently,\nn\n\n1\n1X\n(n)\n\u2212 log p n \u2264 2\nlog(|Zi |/2\u03b5 + 2) + c \u2192 2E[log(|Z1 |/2\u03b5 + 2)] + c,\n\u015c1\nn\nn\ni=1\n\nwhere the convergence follows due to the ergodicity of (Zn ). Therefore the codebooks\n\b\n1\n(n)\nCn := \u015dn1 \u2208 Rn : \u2212 log p\u015dn \u2264 2E[log(|Z1 |/2\u03b5 + 2)] + 2c\n1\nn\nsatisfy the required assertion.\n\n\u0003\n\nWe now use the SCT combined with the previous lemma to construct codebooks that\nguarantee almost optimal reconstructions with a high probability.\nLemma 6.3. For any \u03b5 > 0 there exist codebooks Cr , r \u2265 0, of size er such that\nlim P(dp (X, Cr ) \u2264 (1 + \u03b5)\u03bap r \u2212H ) = 1.\n\nr\u2192\u221e\n\nProof. Let \u03b5 > 0 be arbitrary and c be as in Lemma 6.2. We fix r0 \u2265\nthat\n\u03b5\u03bap r \u2212H \u2265 e\u2212\u03b5r+c+log E|X1 |\n19\n\n4\u03b5\u03bap \u00011/H\nE|X1 |\n\nsuch\n\n(25)\n\n\ffor all r \u2265 r0 . Then choose r1 \u2265 r0 with\nD(r1 |p) \u2264 (1 + \u03b5)\u03bap r1\u2212H .\nWe decompose X into the two processes\n(1)\n\nXt\n\n= Xt \u2212 X\u230at\u230b\n\n(2)\n\nand\n\nXt\n\n= X\u230at\u230b .\n\n(1)\n\nDue to the SCT (24), there exist codebooks Cn \u2282 Lp [0, n] of size exp{(1+\u03b5)nr1 } satisfying\nlim P(dn,p (X (1) , Cn(1) )p \u2264 (1 + 2\u03b5)p \u03bapp r1\u2212pH ) = 1.\n\nn\u2192\u221e\n\nWe apply Lemma 6.2 for \u03b5\u2032 := \u03b5\u03bap r1\u2212H . Note that\nE log\nSince r1H \u2265\n\n4\u03b5\u03bap\nE|X1 | ,\n\n\u0010 |X |\n\u0011\n\u0010 E|X |\n\u0011\n1\n1\n+\n2\n+\nc\n\u2264\nlog\n+\n2\n+c\n2\u03b5\u2032\n2\u03b5\u2032\n\nit follows that\n\nE log\n\nE|X1 |\n2\u03b5\u2032\n\n=\n\nr1H E|X1 |\n2\u03b5\u03bap\n\n\u2265 2, so that\n\n\u0011\n\u0010 E|X | \u0011\n\u0010 |X |\n1\n1\n+\n2\n+\nc\n\u2264\nlog\n+c\n2\u03b5\u2032\n\u03b5\u2032\n= \u2212 log(\u03b5\u03bap r1\u2212H ) + c + log E|X1 | \u2264 \u03b5r,\n(2)\n\ndue to (25). Hence, there exist codebooks Cn \u2282 Lp [0, n] of size exp{\u03b5nr1 } with\n\u0010\n1 \u0011\nlim P dn,p (X (2) , Cn(2) ) \u2264 \u03b5\u03bap H = 1.\nn\u2192\u221e\nr1\n(1)\n(2)\n(1)\n(2)\nLet now C \u0303n := Cn + Cn denote the Minkowski sum of the sets Cn and Cn . Then\n|C \u0303n | \u2264 exp{(1 + 2\u03b5)nr1 }, and one has\n\nP(dn,p (X, C \u0303n ) \u2264 (1 + 3\u03b5)\u03bap r1\u2212H ) \u2265 P(dn,p (X (1) , Cn(1) ) \u2264 (1 + 2\u03b5)\u03bap r1\u2212H and\ndn,p (X (2) , Cn(2) ) \u2264 \u03b5\u03bap r1\u2212H ) \u2192 1.\n\nConsider the isometric isomorphism\n\u03b2n : Lp [0, 1] \u2192 (Lp [0, n], dn,p ), f 7\u2192 f (nt),\nand the codebooks Cn \u2282 Lp [0, 1] given by\nCn = {n\u2212H \u03b2n\u22121 (\u0175) : \u0175 \u2208 C \u0303n }\nThen X\u0303 (n) = n\u2212H \u03b2n\u22121 (X) is a fractional Brownian motion and one has\ndp (X\u0303 (n) , Cn ) = dn,p (\u03b2n (X\u0303 (n) ), \u03b2n (Cn )) = n\u2212H dn,p (X, C \u0303n ).\n20\n\n\fHence, the codebooks Cn are of size exp{(1 + 2\u03b5)nr1 } and satisfy\nP(dp (X, Cn ) \u2264 (1 + 3\u03b5)\u03bap (nr1 )\u2212H )) = P(dn,p (X, C \u0303n ) \u2264 (1 + 3\u03b5)\u03bap r1\u2212H ) \u2192 0\nas n \u2192 \u221e. Now the general statement follows by an interpolation argument similar to\nthat used at the end of the proof of Theorem 3.1.\n\u0003\n(1)\n\nProof of Theorem 6.1. Let q \u2265 1 be arbitrary, let Cr be as in the above lemma for\n(2)\nsome fixed \u03b5 > 0. Moreover, we let Cr denote codebooks of size er with\nE[dp (X, Cr(2) )2q ]1/(2q) \u2248\n(1)\n\n1\n.\nrH\n\n(2)\n\nThen the codebooks Cr := Cr \u222a Cr contain at most 2er elements and satisfy, in analogy\nto the proof of Theorem 4.1 (see (19)),\nE[dp (X, Cr )q ]1/q . (1 + \u03b5)\u03bap\n\n1\n,\nrH\n\nr \u2192 \u221e.\n\nSince \u03b5 > 0 is arbitrary, it follows that\nD (q) (r|q) . \u03bap\n\n1\n.\nrH\n\nFor q \u2265 p the quantization error is greater than the distortion rate function D(r|p), so\nthat the former inequality extends to\nlim r H D (q) (r|q) = \u03bap .\n\nr\u2192\u221e\n\nIn particular, we obtain the asymptotic equivalence of all moments q1 , q2 greater or equal\nto p. Next, an application of Theorem 5.2 with d(f, g) = dp (f, g)q implies that for any\nq > 0,\n1\nD (e) (r|q) & \u03bap H ,\nr\nwhich establishes the assertion.\n\u0003\n\nAppendix\nLemma A.1. For r \u2265 0, let Ar denote [0, \u221e)-valued r.v.'s. If one has, for 0 < q1 < q2\nand some function f : [0, \u221e) \u2192 R+ ,\nE[Aqr1 ]1/q1 \u223c E[Aqr2 ]1/q2 \u223c f (r),\nthen\nAr \u223c f (r), in probability.\n21\n\n(26)\n\n\fProof. Consider\n\u00c3r := Aqr1 /E[Aqr1 ],\nand q\u03032 = q2 /q1 . Then (26) implies that\nE[\u00c3q\u0303r2 ]1/q\u03032 \u223c E[\u00c3r ] = 1\nDenoting \u2206\u00c3r := \u00c3r \u2212 1 and g(x) := xq\u03032 , we obtain\nE[\u00c3q\u0303r2 ] = E[1 + \u2206\u00c3r g\u2032 (1) + g(1 + \u2206\u00c3r ) \u2212 (1 + \u2206\u00c3r g\u2032 (1))]\n= 1 + E[g(\u00c3r ) \u2212 (1 + \u2206\u00c3r g\u2032 (1))]\n\nDue to the strict convexity of g, for arbitrary \u03b5 > 0 there exists \u03b4 > 0 such that\ng(x + 1) \u2265 1 + xg \u2032 (1) + \u03b4, for x \u2208 [\u22121, 1 \u2212 \u03b5] \u222a [1 + \u03b5, \u221e).\nConsequently,\nE[\u00c3q\u0303r2 ] \u2265 1 + \u03b4 P(|\u2206\u00c3r | \u2265 \u03b5).\nSince limr\u2192\u221e E[\u00c3q\u0303r2 ] = 1, it follows that limr\u2192\u221e P(|\u2206\u00c3r | \u2265 \u03b5) = 0. Hence,\n1\nAr = E[Aqr1 ]1/q1 \u00c31/q\n\u223c E[Aqr1 ]1/q1 \u223c f (r),\nr\n\nin probability.\n\u0003\n\nLemma A.2. Let q \u2265 1. There exists a constant c = c(q) < \u221e such that for all [1, \u221e)valued r.v.'s Z one has\nE[(log Z)q ]1/q \u2264 c [1 + log E[Z]].\nProof. Using elementary analysis, there exists a positive constant c1 = c1 (q) < \u221e such\nthat \u03c8(x) := (log x)q + c1 log x, x \u2208 [1, \u221e), is concave. For any [1, \u221e)-valued r.v. Z,\nJensen's inequality then yields\nE[(log Z)q ]1/q \u2264 E[\u03c8(Z)]1/q \u2264 \u03c8(E[Z])1/q\n1/q\n\n\u2264 log E[Z] + c1 (log E[Z])1/q \u2264 c [1 + log E[Z]],\n\nwhere c = c(q) < \u221e is an appropriate universal constant.\n\n\u0003\n\nLemma A.3. Let f : [0, \u221e) \u2192 R+ be a decreasing, convex function satisfying limr\u2192\u221e f (r) =\n0 and\n+\n\u2212r \u2202\u2202r f (r)\nlim sup\n< \u221e,\n(27)\nf (r)\nr\u2192\u221e\n22\n\n\fand F be a family of [0, \u221e]2 -valued random variables for which\nsup P(A \u2264 f (r), B \u2264 r) = 0.\n\nlim\n\n(28)\n\nr\u2192\u221e (A,B)\u2208F\n\nThen the sets of random variables Fr defined for r \u2265 0 through\nFr := {A : (A, B) \u2208 F, EB \u2264 r}\nsatisfy\ninf EA & f (r)\n\nA\u2208Fr\n\nas r \u2192 \u221e.\n+\n\nProof. Fix R > 0, positive integers I and N , and define \u03bb := \u2212 \u2202\u2202r f (R),\nri :=\n\ni+N\nR,\nN\n\ni = \u2212N, \u2212N + 1, . . . .\n\nFor (A, B) \u2208 FR , we define\nTA,B := {\u2204i \u2208 {\u2212N + 1, . . . , I} such that A \u2264 f (ri ) and B \u2264 ri }.\nThen we have\n\u0002\n\u0003\nE A + \u03bbB \u2265\n\u2265\n=\n\nI\u22121\nX\n\n\u0002\n\u0003\nE 1TA,B 1[ri ,ri+1) (B)(A + \u03bbri )\n\nI\u22121\nX\n\n\u0002\n\u0003\nE 1TA,B 1[ri ,ri+1) (B)(f (ri+1 ) + \u03bbri )\n\nI\u22121\nX\n\nh\nR i\nE 1TA,B 1[ri ,ri+1) (B)(f (ri+1 ) + \u03bbri+1 \u2212 \u03bb )\nN\n\nI\u22121\nX\n\nh\nR i\nE 1TA,B 1[ri ,ri+1) (B)(f (R) + \u03bbR \u2212 \u03bb ) ,\nN\n\ni=\u2212N\n\ni=\u2212N\n\ni=\u2212N\n\n\u2265\n\ni=\u2212N\n\nwhere the last inequality follows from the fact that\nf (R) + \u03bbR = inf [f (r) + \u03bbr]\nr\u22650\n\nby the definition of \u03bb and the convexity of f . Now, fix \u03b5 > 0 and pick N \u2265 1/\u03b5, I \u2265 2N/\u03b5\nand R0 so large that\nP(TA,B ) \u2265 1 \u2212\n\n\u03b5\nfor all R \u2265 R0 and all (A, B) \u2208 FR .\n2\n23\n\n\fUsing Chebychev's inequality, we then obtain for R \u2265 R0 ,\n\u0012\n\u0012\n\u0013\u0013\nI\nc\nE[A + \u03bbB] \u2265 (1 \u2212 \u03b5)(f (R) + \u03bbR) 1 \u2212 P (T ) \u2212 P B \u2265 R\nN\n\u0010\n\u03b5 \u03b5\u0011\n.\n\u2265 (1 \u2212 \u03b5)(f (R) + \u03bbR) 1 \u2212 \u2212\n2 2\nHence,\n\u03bbR + EA \u2265 (1 \u2212 \u03b5)2 (f (R) + \u03bbR)\nand therefore\n\u0001\nEA \u2265 (1 \u2212 \u03b5)2 f (R) + \u03bbR (1 \u2212 \u03b5)2 \u2212 1 .\n\nUsing the definition of \u03bb and (27), as well as the fact that \u03b5 > 0 is arbitrary, the conclusion\nfollows.\n\u0003\n\nReferences\n[1] T. M. Cover and J. A. Thomas. Elements of information theory. Wiley Series in\nTelecommunications. New York: John Wiley & Sons, Inc., 1991.\n[2] A. Dembo and I. Kontoyiannis. Source coding, large deviations, and approximate\npattern matching. IEEE Trans. Inform. Theory, 48(6):1590\u20131615, 2002. Special issue\non Shannon theory: perspective, trends, and applications.\n[3] S. Dereich. High resolution coding of stochastic processes and small ball probabilities.\nPh.D. Dissertation, TU Berlin,\nURL: http://edocs.tu-berlin.de/diss/2003/dereich steffen.htm, 2003.\n[4] S. Dereich. Asymptotic behavior of the distortion-rate function for Gaussian processes\nin Banach spaces. Preprint, 2004.\n[5] S. Dereich. The quantization complexity of diffusion processes. Preprint, 2004.\n[6] S. Dereich, F. Fehringer, A. Matoussi, and M. Scheutzow. On the link between small\nball probabilities and the quantization problem for Gaussian measures on Banach\nspaces. J. Theoret. Probab., 16(1):249\u2013265, 2003.\n[7] S. Dereich and M. Lifshits. Probabilities of randomly centered small balls and quantization in Banach spaces. to appear in Annals of Probability, 2004.\n[8] S. Graf and H. Luschgy. Foundations of quantization for probability distributions.\nLecture Notes in Mathematics 1730, Berlin: Springer, 2000.\n24\n\n\f[9] S. Graf, H. Luschgy, and G. Pag\u00e8s. Optimal quantizers for Radon random vectors in\na Banach space. Preprint, 2005.\n[10] R. M. Gray and D. L. Neuhoff. Quantization. IEEE Trans. Inf. Theory, 44(6):2325\u2013\n2383, 1998.\n[11] H. Luschgy and G. Pag\u00e8s. Functional quantization of Gaussian processes. J. Funct.\nAnal., 196(2):486\u2013531, 2002.\n[12] H. Luschgy and G. Pag\u00e8s. Sharp asymptotics of the functional quantization problem\nfor Gaussian processes. Ann. Probab., 32(2):1574\u20131599, 2004.\n\n25\n\n\f"}