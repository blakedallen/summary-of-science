{"id": "http://arxiv.org/abs/1005.5278v2", "guidislink": true, "updated": "2010-08-18T08:05:55Z", "updated_parsed": [2010, 8, 18, 8, 5, 55, 2, 230, 0], "published": "2010-05-28T12:33:37Z", "published_parsed": [2010, 5, 28, 12, 33, 37, 4, 148, 0], "title": "Positive Supercompilation for a Higher-Order Call-By-Value Language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1005.0171%2C1005.4790%2C1005.5335%2C1005.3851%2C1005.3227%2C1005.1776%2C1005.5625%2C1005.5578%2C1005.1309%2C1005.4974%2C1005.0742%2C1005.5472%2C1005.4202%2C1005.0958%2C1005.4545%2C1005.1422%2C1005.5413%2C1005.3517%2C1005.3593%2C1005.0312%2C1005.4146%2C1005.1615%2C1005.2472%2C1005.0065%2C1005.2476%2C1005.3872%2C1005.1403%2C1005.1505%2C1005.1511%2C1005.2997%2C1005.1642%2C1005.5191%2C1005.3842%2C1005.0603%2C1005.1099%2C1005.4675%2C1005.2316%2C1005.0434%2C1005.3300%2C1005.1650%2C1005.4756%2C1005.0243%2C1005.3532%2C1005.1585%2C1005.4242%2C1005.0474%2C1005.0709%2C1005.3895%2C1005.5688%2C1005.0877%2C1005.2870%2C1005.4370%2C1005.2950%2C1005.3066%2C1005.3670%2C1005.4993%2C1005.3234%2C1005.2662%2C1005.4703%2C1005.4920%2C1005.5399%2C1005.0907%2C1005.2753%2C1005.5135%2C1005.0204%2C1005.0807%2C1005.5696%2C1005.1769%2C1005.4846%2C1005.4872%2C1005.0901%2C1005.3261%2C1005.0040%2C1005.5706%2C1005.2842%2C1005.2187%2C1005.2779%2C1005.2465%2C1005.2503%2C1005.0290%2C1005.4855%2C1005.0769%2C1005.4767%2C1005.2458%2C1005.5523%2C1005.0151%2C1005.4378%2C1005.4712%2C1005.0484%2C1005.5177%2C1005.3198%2C1005.1665%2C1005.3838%2C1005.1441%2C1005.5432%2C1005.1456%2C1005.5278%2C1005.5212%2C1005.4246%2C1005.0024%2C1005.5068&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Positive Supercompilation for a Higher-Order Call-By-Value Language"}, "summary": "Previous deforestation and supercompilation algorithms may introduce\naccidental termination when applied to call-by-value programs. This hides\nlooping bugs from the programmer, and changes the behavior of a program\ndepending on whether it is optimized or not. We present a supercompilation\nalgorithm for a higher-order call-by-value language and prove that the\nalgorithm both terminates and preserves termination properties. This algorithm\nutilizes strictness information to decide whether to substitute or not and\ncompares favorably with previous call-by-name transformations.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1005.0171%2C1005.4790%2C1005.5335%2C1005.3851%2C1005.3227%2C1005.1776%2C1005.5625%2C1005.5578%2C1005.1309%2C1005.4974%2C1005.0742%2C1005.5472%2C1005.4202%2C1005.0958%2C1005.4545%2C1005.1422%2C1005.5413%2C1005.3517%2C1005.3593%2C1005.0312%2C1005.4146%2C1005.1615%2C1005.2472%2C1005.0065%2C1005.2476%2C1005.3872%2C1005.1403%2C1005.1505%2C1005.1511%2C1005.2997%2C1005.1642%2C1005.5191%2C1005.3842%2C1005.0603%2C1005.1099%2C1005.4675%2C1005.2316%2C1005.0434%2C1005.3300%2C1005.1650%2C1005.4756%2C1005.0243%2C1005.3532%2C1005.1585%2C1005.4242%2C1005.0474%2C1005.0709%2C1005.3895%2C1005.5688%2C1005.0877%2C1005.2870%2C1005.4370%2C1005.2950%2C1005.3066%2C1005.3670%2C1005.4993%2C1005.3234%2C1005.2662%2C1005.4703%2C1005.4920%2C1005.5399%2C1005.0907%2C1005.2753%2C1005.5135%2C1005.0204%2C1005.0807%2C1005.5696%2C1005.1769%2C1005.4846%2C1005.4872%2C1005.0901%2C1005.3261%2C1005.0040%2C1005.5706%2C1005.2842%2C1005.2187%2C1005.2779%2C1005.2465%2C1005.2503%2C1005.0290%2C1005.4855%2C1005.0769%2C1005.4767%2C1005.2458%2C1005.5523%2C1005.0151%2C1005.4378%2C1005.4712%2C1005.0484%2C1005.5177%2C1005.3198%2C1005.1665%2C1005.3838%2C1005.1441%2C1005.5432%2C1005.1456%2C1005.5278%2C1005.5212%2C1005.4246%2C1005.0024%2C1005.5068&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Previous deforestation and supercompilation algorithms may introduce\naccidental termination when applied to call-by-value programs. This hides\nlooping bugs from the programmer, and changes the behavior of a program\ndepending on whether it is optimized or not. We present a supercompilation\nalgorithm for a higher-order call-by-value language and prove that the\nalgorithm both terminates and preserves termination properties. This algorithm\nutilizes strictness information to decide whether to substitute or not and\ncompares favorably with previous call-by-name transformations."}, "authors": ["Peter A. Jonsson", "Johan Nordlander"], "author_detail": {"name": "Johan Nordlander"}, "author": "Johan Nordlander", "links": [{"title": "doi", "href": "http://dx.doi.org/10.2168/LMCS-6(3:5)2010", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1005.5278v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1005.5278v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "D.3.4; D.3.2", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1005.5278v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1005.5278v2", "arxiv_comment": null, "journal_reference": "Logical Methods in Computer Science, Volume 6, Issue 3 (August 18,\n  2010) lmcs:1038", "doi": "10.2168/LMCS-6(3:5)2010", "fulltext": "Logical Methods in Computer Science\nVol. 6 (3:5) 2010, pp. 1\u201339\nwww.lmcs-online.org\n\nSubmitted\nPublished\n\nJun. 1, 2010\nAug. 17, 2010\n\nPOSITIVE SUPERCOMPILATION\nFOR A HIGHER-ORDER CALL-BY-VALUE LANGUAGE\nPETER A. JONSSON AND JOHAN NORDLANDER\nLule\u00e5 University of Technology, Department of Computer Science and Electrical Engineering\ne-mail address: {pj,nordland}@csee.ltu.se\n\nAbstract. Previous deforestation and supercompilation algorithms may introduce accidental termination when applied to call-by-value programs. This hides looping bugs from\nthe programmer, and changes the behavior of a program depending on whether it is optimized or not. We present a supercompilation algorithm for a higher-order call-by-value\nlanguage and prove that the algorithm both terminates and preserves termination properties. This algorithm utilizes strictness information to decide whether to substitute or not\nand compares favorably with previous call-by-name transformations.\n\n1. Introduction\nIntermediate data structures such as lists allow functional programmers to write clear\nand concise programs, but carry a cost at run-time since additional heap cells need to be\nboth allocated and garbage collected. Both deforestation [57] and supercompilation [47] are\nautomatic program transformations which remove many of these intermediate structures.\nIn a call-by-value context these transformations are unsound, however, as they might hide\ninfinite recursion from the programmer. Consider the program\n(\u03bbx.y) (fac z).\nThis program could loop, if the value of z is negative. Applying Wadler's deforestation\nalgorithm to the program will result in y, which is sound under call-by-name or call-byneed. Under call-by-value the non-termination in the original program has been removed,\nand hence the meaning of the program has been altered by the transformation.\nThis is unfortunate since removing intermediate structures in a call-by-value language is\nperhaps even more important than in a lazy language since the entire intermediate structure\nhas to remain in memory during the computation.\nOhori and Sasano [35] saw this need and presented a very elegant algorithm for callby-value languages that removes intermediate structures. Their algorithm sacrifices some\ntransformational power for algorithmic simplicity. In this article we explore a different\npart of the design space: a more powerful transformation at the cost of some algorithmic\ncomplexity. The outcome is a meaning-preserving supercompiler for pure call-by-value\n1998 ACM Subject Classification: D.3.4, D.3.2.\nKey words and phrases: supercompilation, deforestation, call-by-value.\n\nl\n\nLOGICAL METHODS\nIN COMPUTER SCIENCE\n\nc\nDOI:10.2168/LMCS-6 (3:5) 2010\n\nCC\n\nP. A. Jonsson\nCreative Commons\n\n\f2\n\nP. A. JONSSON\n\nlanguages in general, together measurements from an implementation in a compiler for\nTimber [34], a pure call-by-value language.\nOur current work is a necessary first step towards supercompiling impure call-by-value\nlanguages, of which there are many available today. Well-known examples are OCaml [25],\nStandard ML [29] and F# [49]. Considering that F# is currently being turned into a\nproduct, it is quite likely that strict functional languages will be even more popular in the\nfuture.\nOne might think that our result should be easy to obtain by modifying a call-by-name\nalgorithm to simply delay beta reduction until every function argument has been specialized\nto a value. However, it turns out that this strategy misses even simple opportunities to\nremove intermediate structures. That is, eager specialization of function arguments risks\ndestroying fold opportunities that might otherwise appear, something which may prohibit\ncomplexity improvements to the resulting program.\nThe novelty of our supercompilation algorithm is that it concentrates all call-by-value\ndependencies to a single rule that relies on the result from a separate strictness analysis for\ncorrect behavior. In effect, our transformation delays transformation of function arguments\npast inlining, much like a call-by-name scheme does, although only as far as allowed by\ncall-by-value semantics. The result is an algorithm that is able to improve a wide range\nof illustrative examples like the existing algorithms do, but without the risk of introducing\nartificial termination.\nThe specific contributions of our work are:\n\u2022 We provide an algorithm for positive supercompilation including folding, for a strict and\npure higher-order functional language (Section 4).\n\u2022 We prove that the algorithm terminates and preserves the semantics of the program\n(Section 5).\n\u2022 We show preliminary benchmarks from an implementation in the Timber compiler (Section 6).\nWe start out with some examples in Section 2 to give the reader an intuitive feel of\nhow the algorithm behaves. Our language of study is defined in Section 3, right before the\ntechnical contributions are presented.\nThis article is an extended and improved version of a paper presented at POPL 2009 [20].\nAs well as clarifying a number of the examples and proofs, we give an improved formulation\nof Dapp () presented in Section 4.1, and make a small change to how let-expressions are\nhandled by the driving algorithm.\n2. Examples\nWadler [57] uses the example append (append xs ys) zs and shows that his deforestation\nalgorithm transforms the program so that it saves one traversal of the first list, thereby\nreducing the complexity from 2|xs| + |ys| to |xs| + |ys|.\nIf we na\u0131\u0308vely change Wadler's algorithm to call-by-value semantics by eagerly attempting to transform arguments before attacking the body, we do not achieve this improvement.\nThe definition for append is:\nappend xs ys = case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 : append xs \u2032 ys\n\n\fPOSITIVE SUPERCOMPILATION\n\n3\n\nand we give an example of a hypothetical call-by-value variant of Wadler's deforestation\nalgorithm that attacks arguments first:\nappend (append xs \u2032 ys \u2032 ) zs \u2032\nInlining the body of the inner append and then pushing down the outer call into\neach branch gives\ncase xs \u2032 of\n[] \u2192 append ys \u2032 zs \u2032\n(x : xs) \u2192 append (x : append xs ys \u2032 ) zs \u2032\nTransformation of the first branch will create a new function h1 that is isomorphic\nto append, and call it. The second branch contains an embedding of the initial\nexpression and blindly transforming it will lead to non-termination of the transformation algorithm. One must therefore split this expression in two parts: the subexpression x : append xs ys \u2032 which we call h2 , and the outer expression append z zs \u2032\nwhere z is fresh. Continuing with x : append xs ys \u2032 and inlining append gives\nx : case xs of\n[] \u2192 ys \u2032\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 : append xs \u2032 ys \u2032\nThe second branch contains a renaming of the expression we named h2 , so we\nsimply replace it with a call to h2 . Moving back to append z zs \u2032 we notice that this\nexpression is a renaming of the one called h1 , so we replace it with the call h1 z zs \u2032\nAssembling the pieces gives us the final result:\nletrec h1 xs ys = case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 : h1 xs \u2032 ys\nh2 x xs ys = x : case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 h2 x \u2032 xs \u2032 ys\n\u2032\nin case xs of\n[] \u2192 h1 ys \u2032 zs \u2032\n(x : xs) \u2192 h1 (h2 x xs ys \u2032 ) zs \u2032\nNotice that the intermediate structure from the input program is still there after the transformation, and the complexity is still 2|xs| + |ys|! This can be compared to how the same\nexample is transformed by Wadler's algorithm as shown in Figure 1. The reason our hypothetical call-by-value algorithm failed to improve the program is that it had to split\nexpressions too early during the transformation, thereby preventing fold opportunities that\noccur in a call-by-name setting.\nHowever, changing the call-by-value algorithm to do the exact opposite - that is,\ncarefully delaying the transformation of arguments to a function past the inlining of its\nbody, but only as far as strictness allows - actually leads to the same result that Wadler\n\n\f4\n\nP. A. JONSSON\n\nappend (append xs \u2032 ys \u2032 ) zs \u2032\nNaming the first expression h1 and inlining both occurrences of append gives\ncase ( case xs \u2032 of\n[] \u2192 ys \u2032\n(x1 : xs1 ) \u2192 x1 : append xs1 ys \u2032 ) of\n[] \u2192 zs \u2032\n(x : xs) \u2192 x : append xs zs \u2032\nPushing down the outer case-expression into both branches of the inner one\nand reducing the resulting case-expression of a known constructor leads to\ncase xs \u2032 of\n[] \u2192 case ys \u2032 of\n[] \u2192 zs \u2032\n(x : xs) \u2192 x : append xs zs \u2032\n(x1 : xs1 ) \u2192 x1 : append (append xs1 ys \u2032 ) zs \u2032\nTransform each branch separately. Transformation of the second branch in\nthe first branch will create a new function h2 that is isomorphic to append,\nand the second branch of the outer case is a renaming of our initial expression\ncalled h1 . Assembling all pieces yields the following result:\nletrec h1 xs ys zs = case xs of\n[] \u2192 case ys of\n[] \u2192 zs\n(y \u2032 : ys \u2032 ) \u2192 y \u2032 : h2 ys \u2032 zs\n\u2032\n\u2032\n(x : xs ) \u2192 x \u2032 : h1 xs \u2032 ys zs\nh2 xs ys = case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 : h2 xs \u2032 ys\nin h1 xs \u2032 ys \u2032 zs \u2032\nFigure 1: Wadler's algorithm transforming append (append xs' ys') zs'\nobtains with append (append xs ys) zs. This is a key observation for obtaining deforestation\nunder call-by-value without altering the semantics, and our transformation exploits it.\nExcept for the fundamental reliance on strictness analysis, which is necessary to preserve semantics, our transformation shares many of its rules with Wadler's algorithm. The\ntransformation that is commonly referred to as case-of-case is crucial for our transformation, just like it is for a call-by-name algorithm. The case-of-case transformation is useful\nwhen a case-expression appears in the head of another case-expression, in which case the\nouter case context is duplicated and pushed into all branches of the inner case-expression.\nOur transformation also contains rules that correspond to ordinary evaluation which eliminate case-expressions that have a known constructor in their head or adds two primitive\nnumbers. The mechanism that ensures termination basically looks for \"similar\" terms to\n\n\fPOSITIVE SUPERCOMPILATION\n\n5\n\nones that have already been transformed, and if a similar term is encountered, the transformation will stop and split the term into smaller terms that are transformed separately.\nThe remaining rules of our transformation simply shifts focus to the proper subexpression\nand ensures that the algorithm does not get stuck.\nWe claim that our transformation compares favorably with previous call-by-name transformations, and we now proceed with demonstrating the transformation on some common\nexamples. The results of the transformation on these examples are identical to the results\nof Wadler's algorithm [57].\nThis does not hold in general, a counter-example is the transformation of the expression zip (map f xs) (map g ys) where Wadler's algorithm will eliminate both intermediate\nstructures and our transformation will only eliminate the first intermediate structure.\nOur first example is transformation of sum (map square ys), where the referenced functions are defined as:\nsquare x = x \u2217 x\nmap f xs = case xs of\n[] \u2192 ys\n(x : xs) \u2192 f x : map f xs\nsum xs = case xs of\n[] \u2192 0\n(x : xs) \u2192 x + sum xs\nWe start our transformation by allocating a new fresh function name h0 to the expression\nsum (map square ys), inlining the body of sum and substituting map square ys into the body\nof sum:\ncase map square ys of\n[] \u2192 0\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 + sum xs \u2032\nAfter inlining map and substituting the arguments into the body the result becomes:\ncase ( case ys of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (square x \u2032 ) : map square xs \u2032 ) of\n[] \u2192 0\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 + sum xs \u2032\nWe duplicate the outer case in each of the inner case branches, using the expression\nin the branches as head of that case-expression. Continuing the transformation on each\nbranch with ordinary reduction steps yields:\ncase ys of\n[] \u2192 0\n(x \u2032 : xs \u2032 ) \u2192 square x \u2032 + sum (map square xs \u2032 )\nAt this point we inline the body of the first square occurrence and observe that the\nsecond parameter to (+) is similar to the expression we started with and therefore we\nreplace it with h0 xs \u2032 . The result of our transformation is h0 ys, with h0 defined as:\nh0 ys = case ys of\n[] \u2192 0\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 \u2217 x \u2032 + h0 xs \u2032\n\n\f6\n\nP. A. JONSSON\n\nThis new function only traverses its input once, and no intermediate structures are\ncreated. If the expression sum (map square xs) or a renaming of it is detected elsewhere in\nthe input, a call to h0 will be inserted instead.\nThe work by Ohori and Sasano [35] cannot fuse two successive applications of the same\nfunction, nor mutually recursive functions. We show in the next two examples that our\ntransformation can handle these cases. We need the following new function definitions:\nmapsq xs = case xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (x \u2032 \u2217 x \u2032 ) : mapsq xs \u2032\nf xs\n= case xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (2 \u2217 x \u2032 ) : g xs \u2032\ng xs\n= case xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (3 \u2217 x \u2032 ) : f xs \u2032\nTransforming mapsq (mapsq xs) will inline the outer mapsq , substitute the argument\nin the function body and inline the inner call to mapsq :\ncase ( case xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (x \u2032 \u2217 x \u2032 ) : mapsq xs \u2032 ) of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (x \u2032 \u2217 x \u2032 ) : mapsq xs \u2032\nAs previously, we duplicate the outer case in each of the inner case branches, using the\nexpression in the branches as head of that case-expression. Continuing the transformation\non each branch by ordinary reduction steps yields:\ncase xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (x \u2032 \u2217 x \u2032 \u2217 x \u2032 \u2217 x \u2032 ) : mapsq (mapsq xs \u2032 )\nHere we encounter a similar expression to what we started with, and create a new\nfunction h1 . The final result of our transformation is h1 xs, with the new residual function\nh1 that only traverses its input once defined as:\nh1 xs = case xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (x \u2032 \u2217 x \u2032 \u2217 x \u2032 \u2217 x \u2032 ) : h1 xs \u2032\nFor an example of transforming mutually recursive functions, consider the transformation of sum (f xs). Inlining the body of sum, substituting its arguments in the function\nbody and inlining the body of f yields:\ncase ( case xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 (2 \u2217 x \u2032 ) : g xs \u2032 ) of\n[] \u2192 0\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 + sum xs \u2032\nWe now move down the outer case into each branch, and perform reductions until we end\nup with:\n\n\fPOSITIVE SUPERCOMPILATION\n\n7\n\ncase xs of\n[] \u2192 0\n(x \u2032 : xs \u2032 ) \u2192 (2 \u2217 x \u2032 ) + sum (g xs \u2032 )\nWe notice that unlike in previous examples, sum (g xs \u2032 ) is not similar to what we\nstarted transforming and we can therefore continue the transformation. For space reasons,\nwe focus on the transformation of the rightmost expression in the last branch, sum (g xs \u2032 ),\nwhile keeping the functions already seen in mind. We inline the body of sum, perform the\nsubstitution of its arguments and inline the body of g:\ncase ( case xs \u2032 of\n[] \u2192 []\n(x \u2032\u2032 : xs \u2032\u2032 ) \u2192 (3 \u2217 x \u2032\u2032 ) : f xs \u2032\u2032 ) of\n[] \u2192 0\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 + sum xs \u2032\nWe now move down the outer case into each branch, and perform reductions:\ncase xs \u2032 of\n[] \u2192 0\n(x \u2032\u2032 : xs \u2032\u2032 ) \u2192 (3 \u2217 x \u2032\u2032 ) + sum (f xs \u2032\u2032 )\nWe notice a familiar expression in sum (f xs \u2032\u2032 ), and fold when reaching it. Combining\nthe fragments together gives a new function h2 :\nh2 xs = case xs of\n[] \u2192 0\n(x \u2032 : xs \u2032 ) \u2192 (2 \u2217 x \u2032 ) + case xs \u2032 of\n[] \u2192 0\n(x \u2032\u2032 : xs \u2032\u2032 ) \u2192 (3 \u2217 x \u2032\u2032 ) + h2 xs \u2032\u2032\nThe new function h2 consumes a list and returns a number, so our algorithm has\neliminated the intermediate list between f and sum.\nKort [22] studied a ray-tracer written in Haskell, and identified a critical function in\nthe innermost loop of a matrix multiplication, called vecDot:\nvecDot xs ys = sum (zipWith (\u2217) xs ys)\nThis is simplified by our positive supercompiler to:\nvecDot xs ys = h1 xs ys\nh1 xs ys\n= case xs of\n(x \u2032 : xs \u2032 ) \u2192 case ys of\n(y \u2032 : ys \u2032 ) \u2192 x \u2032 \u2217 y \u2032 + h1 xs \u2032 ys \u2032\n\u2192 0\n\u2192 0\nThe intermediate list between sum and zipWith is transformed away, and the complexity\nis reduced from 2|xs| + |ys| to |xs| + |ys| (since this is matrix multiplication |xs| = |ys|).\n\n\f8\n\nP. A. JONSSON\n\nExpressions\ne, f\n\n::= n | x | g | f e | \u03bbx.e | k e | e1 \u2295 e2 | case e of {pi \u2192 ei }\n| let x = f in e | letrec g = v in e\n\np\n\n::= n | k x\n\nValues\nv\n\n::= n | \u03bbx.e | k v\nFigure 2: The language\nfv(x)\nfv(n)\nfv(g)\nfv(k e)\nfv(\u03bbx.e)\nfv(f e)\nfv(let x = e in f )\nfv(letrec g = v in f )\nfv(case e of {pi \u2192 ei })\nfv(e1 \u2295 e2 )\n\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n\n{x}\n\u2205\n\u2205\nfv(e)\nfv(e)\\{x}\nfv(f ) \u222a fv(e)\nfv(e) \u222a (fv(f )\\{x})\nfv(v) \u222a fv(f\nS )\nfv(e) \u222a ( (fv(ei )\\fv(pi )))\nfv(e1 ) \u222a fv(e2 )\n\nFigure 3: Free variables of an expression\n3. Language\nOur language of study is a strict, higher-order functional language with let-bindings\nand case-expressions. Its syntax for expressions, values and patterns is shown in Figure 2.\nHere we let variables and constructor symbols be denoted by x and k, respectively. The\nconstructor symbols k range over a set K and we also assume that there is a separate set G\nof recursively defined function symbols, ranged over by g. In what follows we will assume\nthat the meaning of such symbols is given by a recursive map G mapping symbols g to their\ndefined value.\nThe language contains integer values n and arithmetic operations \u2295, although these\nmeta-variables can preferably be understood as ranging over primitive values in general and\narbitrary operations on these. We let + denote the semantic meaning of \u2295.\nA list of expressions e1 . . . en is abbreviated as e, and a list of variables x1 . . . xn as x.\nWe denote the free variables of an expression e by fv(e), as defined in Figure 3. Along\nthe same lines we denote the function names in an expression e as fn(e), defined in Figure 4.\nWe encode letrec as an application containing fix, where fix is defined as\nf ix = \u03bbf.f (\u03bbn.f ix f n)\nDefinition 3.1. Letrec is defined as:\ndef\n\nletrec h = \u03bbx.e in e\u2032 = (\u03bbh.e\u2032 ) (\u03bby.f ix (\u03bbh.\u03bbx.e) y)\n\n\fPOSITIVE SUPERCOMPILATION\n\nfn(x)\nfn(n)\nfn(g)\nfn(k e)\nfn(\u03bbx.e)\nfn(f e)\nfn(let x = e in f )\nfn(letrec g = v in f )\nfn(case e of {pi \u2192 ei })\nfn(e1 \u2295 e2 )\n\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n\n9\n\n\u2205\n\u2205\n{g}\nfn(e)\nfn(e)\nfn(f ) \u222a fn(e)\nfn(e) \u222a fn(f )\n(fn(v) \u222a S\nfn(f ))\\{g}\nfn(e) \u222a ( (fn(ei ))\nfn(e1 ) \u222a fn(e2 )\n\nFigure 4: Function names of an expression\nReduction contexts\nE\n\n::= \u001f | E e | (\u03bbx.e) E | k E | E \u2295 e | n \u2295 E | case E of {pi \u2192 ei } | let x = E in e\nEvaluation relation\nEhgi\n7\u2192\nEh(\u03bbx.e) vi\n7\u2192\nEhlet x = v in ei\n7\u2192\nEhcase k v of {ki xi \u2192 ei }i7\u2192\nEhcase n of {ni \u2192 ei }i\n7\u2192\nEhn1 \u2295 n2 i\n7\u2192\n\nEhvi, if (g, v) \u2208G\nEh[v/x]ei\nEh[v/x]ei\nEh[v/xj ]ej i, if k = kj\nEhej i, if n = nj\nEhni, if n = n1 + n2\n\n(Global)\n(App)\n(Let)\n(KCase)\n(NCase)\n(Arith)\n\nFigure 5: Reduction semantics\nBy defining letrec as syntactic sugar for other primitives we introduce an implicit requirement that the right hand side of letrec expressions must not contain any free variables\nexcept h. This is not a limitation since functions that contain free variables can be lambda\nlifted [17] to the top level.\nA program is an expression with no free variables and all function names defined in G.\nThe intended operational semantics is given in Figure 5, where [e/x]e\u2032 is the capture-free\nsubstitution of expressions e for variables x in e\u2032 .\nA reduction context E is a term containing a single hole, \u001f, which indicates the next\nexpression to be reduced. The expression Ehei is the term obtained by replacing the hole\nin E with e. E denotes a list of terms with just a single hole, evaluated from left to right.\nIf a variable appears no more than once in a term, that term is said to be linear with\nrespect to that variable. Like Wadler [57], we extend the definition slightly for linear caseexpressions: no variable may appear in both the head and a branch, although a variable\nmay appear in more than one branch. For example, the definition of append is linear is\nlinear with respect to ys, although ys appears in both branches.\n\n\f10\n\nP. A. JONSSON\n\n4. Higher Order Positive Supercompilation\nIt is time to make the intuition developed in Section 2 more formal. Our supercompiler\nis defined as a set of rewrite rules that pattern-match on expressions. This algorithm is called\nthe driving algorithm, and is defined in Figure 6. Three additional parameters appear as\nsubscripts to the rewrite rules: a driving context R, the set of global function definitions\nG and a memoization list \u03c1. The memoization list holds information about expressions\nalready traversed and is explained more in detail in Section 4.1. The driving context R is\nsmaller than E, and is defined as follows:\nR ::= \u001f | R e | case R of {pi \u2192 ei } | R \u2295 e | e \u2295 R\nInterestingly, this definition coincides with the evaluation contexts for a call-by-name language. The reason our transformation still preserves a call-by-value semantics is that beta\nreduction (rule R9) results in a let-binding, whose further specialization in rule R13 depends\non whether the body expression f is strict in the bound variable x or not.\nOur let-rule (R13) might change the order of computations, but since non-termination\nis commutative this does not matter in practice. Supercompiling impure languages requires\nstronger conditions for the let-rule, since expressions might contain effects other than nontermination. The difficulty of supercompiling an impure language is to find sufficient conditions that preserve soundness while still allowing the maximum amount of reordering of\nexpressions.\nIn principle, an expression e is strict with regards to a variable x if evaluation of e\neventually requires the value of x; in other words, if e 7\u2192 . . . 7\u2192 Ehxi. Such information\nis not computable in general, although call-by-value semantics allows for reasonably tight\napproximations. One such approximation is given in Figure 7, where the strict variables of\nan expression e are defined as all free variables of e except those that only appear under a\nlambda or not inside all branches of a case.\nThere is an ordering between the driving rules; i.e., all rules must be tried in the order\nthey appear. Rule R10 is the default fallback case for applications and rule R19 is the\ndefault fallback case for case expressions. These rules extend the driving context R and\nzoom in on the next expression to be driven. The program is turned \"inside-out\" by moving\nthe surrounding context R into all branches of the case-expression through rules R15 and\nR18. Rule R13 has a similar mechanism for let-expressions. Notice how the context is\nmoved out of the recursive call in rule R5, whereas rule R7 recursively applies the driving\nalgorithm to the full new term Rhni, forcing a re-traversal of the new term in search for for\nfurther reduction opportunities. Rule R12 is only allowed to match if the variable y is not\nfreshly generated by the splitting mechanism described in the next section. Meta-variable\na in rules R8 and R18 stands for an \"annoying\" expression; i.e., an expression that would\nbe further reducible were it not for a free variable getting in the way. The grammar for\nannoying expressions is:\na ::= x | n \u2295 a | a \u2295 n | a \u2295 a | a e\nSome expressions should be handled differently depending on context. If a constructor\napplication appears in an empty context, there is not much we can do but to drive the\nargument expressions (rule R4). On the other hand - if the application occurs at the head\nof a case-expression, we may choose a branch on basis of the constructor and leave the\narguments unevaluated in the hope of finding fold opportunities further down the syntax\ntree (rule R16).\n\n\fPOSITIVE SUPERCOMPILATION\n\nDJnKR,G,\u03c1\nDJxKR,G,\u03c1\nDJgKR,G,\u03c1\nDJk eK\u001f,G,\u03c1\nDJx eKR,G,\u03c1\nDJ\u03bbx.eK\u001f,G,\u03c1\nDJn1 \u2295 n2 KR,G,\u03c1\nDJe1 \u2295 e2 KR,G,\u03c1\n\n=\n=\n=\n=\n=\n=\n=\n=\n\nDJ(\u03bbx.f ) eKR,G,\u03c1\nDJe e\u2032 KR,G,\u03c1\nDJlet x = n in f KR,G,\u03c1\nDJlet x = y in f KR,G,\u03c1\nDJlet x = e in f KR,G,\u03c1\n\n=\n=\n=\n=\n=\n\nDJletrec g = v in eKR,G,\u03c1\nDJcase x of {pi \u2192 ei }KR,G,\u03c1\nDJcase kj e of {ki xi \u2192 ei }KR,G,\u03c1\nDJcase nj of {ni \u2192 ei }KR,G,\u03c1\nDJcase a of {pi \u2192 ei }KR,G,\u03c1\nDJcase e of {pi \u2192 ei }KR,G,\u03c1\nDJeKR,G,\u03c1\n\n=\n=\n=\n=\n=\n=\n=\n\nRhni\nRhxi\nDapp (g)R,G,\u03c1\nk DJeK\u001f,G,\u03c1\nRhx DJeK\u001f,G,\u03c1 i\n(\u03bbx.DJeK\u001f,G,\u03c1 )\nDJRhniK\u001f,G,\u03c1 , where n = n1 + n2\nDJe1 K\u001f,G,\u03c1 \u2295 DJe2 K\u001f,G,\u03c1 , if e1 \u2295 e2 = a\nDJe2 KRhe1 \u2295\u001fi,G,\u03c1 , if e1 = n or e1 = a\nDJe1 KRh\u001f\u2295e2 i,G,\u03c1 , otherwise\nDJlet x = e in f KR,G,\u03c1\nDJeKRh\u001f e\u2032 i,G,\u03c1\nDJRh[n/x]f iK\u001f,G,\u03c1\nDJRh[y/x]f iK\u001f,G,\u03c1 , if y not freshly generated\nDJRh[e/x]f iK\u001f,G,\u03c1 , if x \u2208 strict(f ) and\nx \u2208 linear(f )\nlet x = DJeK\u001f,G,\u03c1 in DJRhf iK\u001f,G,\u03c1 , otherwise\nDJRheiK\u001f,G \u2032 ,\u03c1 , where G' = G \u222a(g, v)\ncase x of {pi \u2192 DJ[pi /x]Rhei iK\u001f,G,\u03c1 }\nDJRhlet xj = e in ej iK\u001f,G,\u03c1\nDJRhej iK\u001f,G,\u03c1\ncase DJaK\u001f,G,\u03c1 of {pi \u2192 DJRhei iK\u001f,G,\u03c1 }\nDJeKRhcase \u001f of {pi \u2192ei }i,G,\u03c1\nRhei\n\n11\n\n(R1)\n(R2)\n(R3)\n(R4)\n(R5)\n(R6)\n(R7)\n(R8)\n\n(R9)\n(R10)\n(R11)\n(R12)\n(R13)\n\n(R14)\n(R15)\n(R16)\n(R17)\n(R18)\n(R19)\n(R20)\n\nFigure 6: Driving algorithm\nstrict(x)\n= {x}\nstrict(n)\n=\u2205\nstrict(g)\n=\u2205\n= strict(e)\nstrict(k e)\nstrict(\u03bbx.e)\n=\u2205\nstrict(f e)\n= strict(f ) \u222a strict(e)\nstrict(let x = e in f )\n= strict(e) \u222a (strict(f )\\{x})\nstrict(letrec g = v in f ) = strict(f ) T\nstrict(case e of {pi \u2192 ei })= strict(e) \u222a ( (strict(ei )\\fv(pi )))\nstrict(e1 \u2295 e2 )\n= strict(e1 ) \u222a strict(e2 )\nFigure 7: The strict variables of an expression\nThe argumentation is analogous for lambda abstractions: if there is a surrounding\napplication context we perform a beta reduction, otherwise we proceed by driving the\nabstraction itself.\nNotice that the primitive operations ranged over by \u2295 cannot be unfolded and transformed like ordinary functions can. If the arguments of a primitive operation are annoying,\nour transformation will simply leave the primitive operation in place (rule R8).\n\n\f12\n\nP. A. JONSSON\n\nDapp (g)R,G,\u03c1 = h x\nwhere x = \u03c3(fv(e1 ))\nDapp (g)R,G,\u03c1 = Rhgi\nDapp (g)R,G,\u03c1 = [DJf K\u001f,G,\u03c1 /y]DJfg K\u001f,G,\u03c1\nwhere (fg , f , y) = split(Rhgi, e1 )\nDapp (g)R,G,\u03c1 = [DJf K\u001f,G,\u03c1 /y]DJfg K\u001f,G,\u03c1\nletrec h = \u03bbx.e in h x\ne\nwhere (g, v) \u2208 G,\ne = DJRhviK\u001f,G,\u03c1\u2032 ,\n\u03c1\u2032 = \u03c1 \u222a (h, Rhgi),\nh fresh,\nx = fv(Rhgi),\n(fg , f , y) = split(Rhgi, e1 )\n\nif \u2203(h, e1 ) \u2208 \u03c1 . \u03c3e1 = Rhgi\n\n(1)\n\nif \u2203(h, e1 ) \u2208 \u03c1 . e1 E Rhgi and Rhgi E e1 (2)\nif \u2203(h, e1 ) \u2208 \u03c1 . e1 E Rhgi\n(3)\nif \u2203e1 \u2208 e . e1 E Rhgi\nif h \u2208 fn(e)\notherwise\n\n(4a)\n(4b)\n(4c)\n\nFigure 8: Driving of applications\nIf we had a perfect strictness analysis and could decide whether an arbitrary expression\nwill terminate or not, the only difference in results between our transformation and a callby-name counterpart would be for the non-terminating cases. In practice, we have to settle\nfor an approximation, such as the simple analysis defined in Figure 7. One might speculate\nwhether the transformations thus missed will have adverse effects on the usefulness of our\ntransformation in practice. We believe we have seen clear indications that this is not the\ncase, and that the crucial factor is the ability to inline function bodies irrespective of whether\narguments are values or not.\nOur transformation always inlines functions unless the algorithm detects a risk of nontermination. Supero [30, Sec. 3.2] has a more advanced inlining strategy.\n4.1. Application Rule. In the driving algorithm rule R3 refers to Dapp (), defined in Figure 8. Dapp () can be inlined in the definition of the driving algorithm, it is merely given\na separate name to improve the clarity of the presentation. Figure 8 contains some new\nnotation: we use \u03c3 for a variable to variable substitution and = for syntactic equivalence\nof expressions.\nCare needs to be taken to ensure that recursive functions are not inlined forever. The\ndriving algorithm keeps track of previously seen function applications in the memoization\nlist \u03c1, which also associates a unique function name to each such expression. Whenever an\nexpression that is equivalent up to renaming of variables to a previous application, a call\nto the associated function symbol is inserted instead. This is not sufficient to guarantee\ntermination of the algorithm, but the mechanism is crucial for the complexity improvements\nmentioned in Section 2.\nTo ensure termination, we use the homeomorphic embedding relation E to define a\npredicate called \"the whistle\". When the predicate holds for an expression we say that\nthe whistle blows on that expression. The intuition is that when e E f , f contains all\nsubexpressions of e, possibly embedded in other expressions. For any infinite sequence\ne0 , e1 , . . . there must exist an i and a j such that i < j and ei E ej . This condition is\nsufficient to ensure termination.\n\n\fPOSITIVE SUPERCOMPILATION\n\ne\ne\nRight e\nfac y\n\nE\nE\nE\n\nf\nJust e\nRight (e, e \u2032 )\nfac (y \u2212 1)\n\ntg\nx\nRight x\nfac x\n\n\u03b81\n[e/x]\n[e/x]\n[y/x]\n\n13\n\n\u03b82\n[Just e/x]\n[(e, e\u2032 )/x]\n[(y \u2212 1)/x]\n\nFigure 9: Examples of the homeomorphic embedding and the msg\nIn order to define the homeomorphic embedding we need a definition of uniform terms\nanalogous to the one defined by S\u00f8rensen and Gl\u00fcck [45]. We slightly adjust their version\nto fit our language.\nDefinition 4.1 (Uniform terms). Let s range over the set G \u222a K \u222a {caseof , let, letrec,\nprimop, lambda, apply}, and let caseof (e), let(e), letrec(v, e), primop(e), lambda(e),\nand apply(e) denote a case, let, recursive let, primitive operation, lambda abstraction or\napplication for all subexpressions e, e and v. The set of terms T is the smallest set of arity\nrespecting symbol applications s(e).\nDefinition 4.2 (Homeomorphic embedding). Define E as the smallest relation on T satisfying:\ne E fi for some i\ne1 E f1 , . . . , en E fn\nxEy\nn1 E n2\ne E s(f1 , . . . , fn )\ns(e1 , . . . , en ) E s(f1 , . . . , fn )\nWhenever the whistle blows, our transformation splits the input expression into strictly\nsmaller terms that are driven separately in the empty context. This might expose new\nfolding opportunities, and allows the algorithm to remove intermediate structures in subexpressions. The design follows the positive supercompilation algorithm outlined by S\u00f8rensen\n[44], except that we need to reassemble the transformed subexpressions into a term of the\noriginal form instead of pulling them out as let-definitions, in order to preserve strictness.\nOur transformation is also more complicated because we perform the program extraction\nimmediately, rather than constructing a large tree of terms and extracting the program in\na separate pass.\nSplitting expressions is rather intricate, and two mechanisms are needed; the first is the\nmost specific generalization (msg).\nDefinition 4.3 (Most specific generalization).\n\u2022 An instance of a term e is a term of the form \u03b8e for some substitution \u03b8.\n\u2022 A generalization of two terms e and f is a triple (tg , \u03b81 , \u03b82 ), where \u03b81 , \u03b82 are substitutions\nsuch that \u03b81 tg \u2261 e and \u03b82 tg \u2261 f .\n\u2022 A most specific generalization (msg) of two terms e and f is a generalization (tg , \u03b81 , \u03b82 )\nsuch that for every other generalization (t\u2032g , \u03b81\u2032 , \u03b82\u2032 ) of e and f it holds that tg is an instance\nof t\u2032g .\nFor background information and an algorithm to compute most specific generalizations,\nsee Lassez et al. [23]. Figure 9 contains examples of the homeomorphic embedding and the\nmsg.\nThe most specific generalization is not always sufficient to split expressions. For expressions differing already in their roots, msg will return just a variable and substitutions\nequal to the input terms on that variable. If this happens we need to split expressions in a\ndifferent way. We therefore define our function split using two alternatives; one that applies\n\n\f14\n\nP. A. JONSSON\n\nwhen there is a non-trivial most specific generalization, and one that just splits along the\nspine of the first term in the other case.\nDefinition 4.4 (Split). For t \u2208 T we define split(t1 , t2 ) by:\nsplit(s(e1 ), s\u2032 (e2 )) = (tg , rng(\u03b81 ), dom(\u03b81 )) if s = s\u2032\n= (s(x), e1 , x)\notherwise\n\u2032\nwith (tg , \u03b81 , \u03b82 ) = msg(s(e1 ), s (e2 )) and x fresh.\nAlternatives 2 and 4a of Dapp () is for upwards generalization, and alternative 3 is\nfor downwards generalization. This is exemplified below. All the examples of how our\ntransformation works in Section 2 eventually terminate through a combination of alternative\n1 and alternative 4b of Dapp ().\nThe second alternative of Dapp () in combination with 4a is useful when transforming\nfunction calls that have the same parameter appearing twice, for example append xs xs as\nshown in Figure 10.\nThe third alternative is used when terms are \"growing\" in some sense. An example\nof reverse with an accumulating parameter is shown in Figure 11, assuming the standard\ndefinition of reverse.\n5. Correctness\nThe problem with using previous deforestation and supercompilation algorithms in a\ncall-by-value context is that they might change the termination properties of programs. In\nthis section we prove that our supercompiler both terminates itself, and preserves program\ntermination behavior for all input.\n5.1. Termination. In order to prove that the algorithm terminates we show that each\nrecursive application of DJK in the right-hand sides of Figure 6 and 8 has a strictly smaller\nweight than the left-hand side.\nThe weight of an expression is one plus the sum of the weight of its subexpressions,\nwhere variables, primitive numbers and function names have weight two. The weight of a\nfresh variable not in the initial input is one.\nDefinition 5.1. The weight of a variable x in the initial input, a primitive number n, and\na function name g is 2. The weight of a fresh variable not in theP\ninitial input is 1. The\nweight of any composite expression (n \u2265 1) is |s(e1 , . . . , en )| = 1 + ni=1 |ei |.\nDefinition 5.2. Let S be a set with a relation \u2264. Then (S, \u2264) is a quasi-order if \u2264 is\nreflexive and transitive.\nDefinition 5.3. Let (S, \u2264) be a quasi-order. (S, \u2264) is a well-quasi-order if, for every infinite\nsequence s0 , s1 , . . . \u2208 S, there exist i < j such that si \u2264 sj\nThe following lemma tells us that the set of finite sequences over a well-quasi-ordered\nset is well-quasi-ordered, with one proof by Nash-Williams [32]:\nLemma 5.4 (Higman's lemma). If a set S is well-quasi-ordered, then the set S \u2217 of finite\nsequences over S is well-quasi-ordered.\n\n\fPOSITIVE SUPERCOMPILATION\n\n15\n\nDJappend xs xsK (*)\n(By rule 4 of Dapp (), put (h0 , append xs xs) in \u03c1 and transform according\nto the rules of the algorithm)\n=\n\ncase xs of\n[] \u2192 xs\n(x \u2032 : xs \u2032 ) \u2192 DJx \u2032 K : DJappend xs \u2032 xsK\n(Focus on DJ append xs \u2032 xsK and recall that \u03c1 contains append xs xs\nso alternative 2 of Dapp ( ) is triggered and the transformation returns\nappend xs \u2032 xs. This returns all the way up to the start (*) and the transformation continues there through alternative 4a)\n\n=\n\nDJappend xs xsK\n(Generalize the expression with append xs \u2032 xs)\n\n=\n\n[DJxsK/x , DJxsK/y] DJappend x yK\n\n=\n\n[xs/x , xs/y] case x of\n[] \u2192 y\n(x \u2032 : xs \u2032 ) \u2192 DJx \u2032 K : DJappend xs \u2032 yK\n\n=\n\n[xs/x , xs/y] case x of\n[] \u2192 y\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 : h0 xs \u2032 y\n\n= letrec h0 xs ys = case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 : h0 xs \u2032 ys\nin h0 xs xs\nFigure 10: Example of upwards generalization\nThe weight of the entire transformation is a triple that contains the maximum length\nof the memoization list \u03c1 denoted by N, the weight of the term being transformed and the\nweight of the current term in focus. That such an N exists follows from Kruskal's Tree\nTheorem [8] and the homeomorphic embedding relation being a well-quasi-order.\nTheorem 5.5 (Kruskal's Tree Theorem). If S is a finite set of function symbols, then any\ninfinite sequence t1 , t2 , . . . of terms from the set S contains two terms ti and tj with i < j\nsuch that ti E tj .\nProof (Similar to Dershowitz [8]). Collapse all integers to a single 0-ary constructor, and\nall variables to a different 0-ary constructor.\nSuppose the theorem were false. Let the infinite sequence t = t1 , t2 , . . . of terms be\na minimal counterexample, measured by the size of the ti . By the minimality hypothesis,\n\n\f16\n\nP. A. JONSSON\n\nDJrev xs []K\n(By rule 4 of Dapp (), put (h0 , rev xs []) in \u03c1 and transform the program\naccording to the rules of the algorithm)\ncase xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 DJrev xs \u2032 (x \u2032 : [])K\n(Focus on the second branch and recall that \u03c1 contains rev xs [] so alternative 3 of Dapp ( ) is triggered and the expression is generalized)\n=\n\nDJrev xs \u2032 (x \u2032 : [])K\n(Generalize the expression with rev xs [])\n\n=\n\n[DJ(x \u2032 : [])K/zs]DJrev xs \u2032 zsK\n\n=\n\n[(x \u2032 : [])/zs]DJrev xs \u2032 zsK\n(Put (h1 , rev xs \u2032 zs) in \u03c1 and transform according to the rules of the\nalgorithm)\n\n= [(x \u2032 : [])/zs]letrec h1 xs ys = case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 h1 xs \u2032 (x \u2032 : ys)\nin h1 xs \u2032 (x \u2032 : [])\n=\n\nletrec h1 xs ys = case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 h1 xs \u2032 (x \u2032 : ys)\n\u2032\n\u2032\nin h1 xs (x : [])\n(Putting the two parts together)\ncase xs of\n[] \u2192 []\n(x \u2032 : xs \u2032 ) \u2192 letrec h1 xs ys = case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 h1 xs \u2032 (x \u2032 : ys)\n\u2032\n\u2032\nin h1 xs (x : [])\nFigure 11: Example of downwards generalization\n\n\fPOSITIVE SUPERCOMPILATION\n\n17\n\nthe set of proper subterms of the ti must be well-quasi-ordered, or else there would be a\nsmaller counterexample t1 , t2 , . . . , tl\u22121 , s1 , s2 , . . ., for some l such that s1 is a subterm of t1\nand all s2 , . . . are subterms of one of tl , tl+1 , . . .. (None of t1 , t2 , . . . , tl\u22121 can embed any of\ns1 , s2 , . . ., since that would mean that ti also is embedded in some tj , i < l \u2264 j).\nSince the set S of function symbols is well-quasi-ordered by \u2265, there must exist an infinite subsequence r of t, the root (outermost) symbols of which constitute a quasi-ascending\nchain under \u2264. (Any infinite sequence of elements of a well-quasi-ordered set must contain\nan infinite chain of quasi-ascending elements). Since the set of proper subterms is wellquasi-ordered, it follows by Lemma 5.4 that the set of finite sequences consisting of the\nimmediate subterms of the elements in r is also well-quasi-ordered. But then there would\nhave to be an embedding in t itself, in which case it would not be a counterexample.\nWe will show that each step of the driving algorithm will reduce the weight of what is\nbeing transformed. The constant N in the weight is the maximum length of the sequence\nof terms that are not related to each other by the homeomorphic embedding.\nCorollary 5.6. Any infinite sequence t1 , t2 , . . . \u2208 T \u2217 contains two terms ti and tj with i < j\nsuch that ti E tj .\nCorollary 5.7. There is a maximum N such that t1 , t2 , . . . , tN \u2208 T \u2217 contains no terms ti\nand tj with i < j and ti E tj .\nWe define the weight of driving a term as:\nDefinition 5.8. The weight of a call to the driving algorithm is |DJeKR,G,\u03c1 | = (N \u2212\n|\u03c1|, |Rhei|, |e|)\nTuples must be ordered for us to tell whether the weight of a term actually decreases\nfrom driving it. We use the standard lexical order between tuples.\nDefinition 5.9. The order between two tuples (n1 , n2 , n3 ) and (m1 , m2 , m3 ) is:\n(n1 , n2 , n3 ) < (m1 , m2 , m3 ) if n1 < m1\n(n1 , n2 , n3 ) < (m1 , m2 , m3 ) if n1 = m1 and n2 < m2\n(n1 , n2 , n3 ) < (m1 , m2 , m3 ) if n1 = m1 , n2 = m2 and n3 < m3\nWe also need to show that the memoization list \u03c1 only contains elements that were in\nthe initial input program:\nLemma 5.10. The second component of the memoization list \u03c1, can only contain terms\nfrom the set T.\nProof. Integers and fresh variables are equal, up to E, to the already existing integers and\nvariables. Our only concern are the rules that introduce new terms that are not in T. The\nnew function names h are the only new terms introduced by the algorithm. By inspection\nof the rules it is clear that only rule R3 introduces such new terms. Inspection of the RHS\nof Dapp (),G,\u03c1 :\n1: No recursive application in the RHS.\n2: No recursive application in the RHS.\n3: No new terms are created and the memoization list \u03c1 is not extended.\n4a: No new terms are created and the memoization list \u03c1 is not extended.\n4b: The newly created term h x is kept outside of the recursive call of the driving algorithm.\nThe memoization list \u03c1, is extended with terms from T.\n\n\f18\n\nP. A. JONSSON\n\n4c: No new terms are created, and the memoization list \u03c1, is extended with terms from T.\nWith these definitions in place, we can formulate a lemma that claims the weight is\ndecreasing in each step of our transformation.\nLemma 5.11. For each rule DJeKR,G,\u03c1 = e1 in Figure 6 and Figure 8 and each recursive\napplication DJe\u2032 KR\u2032 ,G,\u03c1\u2032 in e1 , |DJe\u2032 KR\u2032 ,G,\u03c1\u2032 | < |DJeKR,G,\u03c1 |\nLemma 5.12 (Totality). For all expressions Rhei, DJeKR,G,\u03c1 is matched by a unique rule\nin Figure 6.\nTheorem 5.13 (Termination). The driving algorithm DJK terminates for all inputs.\nProof. The weight of the transformation is defined because of Kruskal's Tree Theorem and\nthe fact that the homeomorphic embedding is a well-quasi-order. Lemma 5.10 guarantees\nthat the memoization list \u03c1 only contains terms from the initial input. By Lemma 5.11 the\nweight of the transformation decreases for each step and by Lemma 5.12 we know that each\nrecursive application will match a rule.\nSince < is well-founded over triples of natural numbers the system will eventually\nterminate.\n5.2. Total Correctness. The problem with previous deforestation and supercompilation\nalgorithms in a call-by-value context is that they might change termination properties of\nprograms. We prove that our supercompiler does not change what the program computes,\nnor does it alter whether a program terminates or not.\nSands [39] shows how a transformation can change the semantics in rather subtle ways \u2013\nconsider the function\nf x = x + 42\nIt is clear that f 0 \u223c\n= 42 (where \u223c\n= is semantic equivalence with respect to the current\ndefinition). Using this equality and replacing 42 in the function body with f 0 yields:\nf x = x + f 0\nThis function will compute something entirely different than the original definition of\nf . We need some tools to ensure that the meaning of the original program is preserved and\nwe therefore introduce the standard notions of operational approximation and equivalence.\nA general context C which is an expression with zero or more holes in the place of some\nsubexpressions is used, and we say that an expression C[e] is closed if there are no free\nvariables in it.\nDefinition 5.14 (Operational Approximation and Equivalence).\n\u2022 e operationally approximates e\u2032 , e \u228f e\u2032 , if for all contexts C such that C[e], C[e\u2032 ] are\ne then so does evaluation of C[e\u2032 ].\nclosed, if evaluation of C[e] terminates\n\u2022 e is operationally equivalent to e\u2032 , e \u223c\n= e\u2032 , if e \u228f e\u2032 and e\u2032 \u228f e\ne\ne\nThe correctness of deforestation in a call-by-name setting has previously been shown\nby Sands [39] using his improvement theory. We use Sands's definitions for improvement\nand strong improvement:\nDefinition 5.15 (Improvement, Strong Improvement).\n\n\fPOSITIVE SUPERCOMPILATION\n\n19\n\n\u2022 e is improved by e\u2032 , e D e\u2032 , if for all contexts C such that C[e], C[e\u2032 ] are closed, if\ncomputation of C[e] terminates using n function calls, then computation of C[e\u2032 ] also\nterminates, and uses no more than n function calls.\n\u2022 e is strongly improved by e\u2032 , e Ds e\u2032 , iff e D e\u2032 and e \u223c\n= e\u2032 .\nNote that improvement, D, is not the same as the homeomorphic embedding, E, defined\npreviously.\nWe use e 7\u2192k v to denote that e evaluates to v using k function calls (and any other\nreduction rule as many times as it needs) and e\u2032 7\u2192\u2264k v \u2032 to denote that e' evaluates to v'\nwith at most k function calls and using any other reduction rule as many times as needed.\nTo state the Improvement Theorem we view a transformation as the introduction of\nsome new functions from a given set of definitions. We let {gi }i\u2208I be a set of functions\nindexed by some set I, where each function has a fixed arity \u03b1i and are given by some\ndefinitions\n{gi = \u03bbx1 . . . x\u03b1i .ei }i\u2208I\n\u2032\nand let {ei }i\u2208I be a set of expressions such that for each i \u2208 I, fv(e\u2032i ) \u2286 {x1 . . . x\u03b1i }. The\nfollowing results relate to the transformation of the functions gi using the expressions e\u2032i :\nlet {hi }i\u2208I be a set of new functions given by the definitions\n{hi = [h/g]\u03bbx1 . . . x\u03b1i .e\u2032i }i\u2208I\nTheorem 5.16 (Sands Improvement theorem). If g = e and e D C[g] then g D h where\nh = C[h].\nTheorem 5.17 (Cost-equivalence theorem). If ei ED e\u2032i for all i \u2208 I, then gi ED hi , i \u2208 I.\nWe need a standard partial correctness result [39] associated with unfold-fold transformations\nTheorem 5.18 (Partial Correctness). If ei \u223c\n= e\u2032i for all i \u2208 I then hi \u228f gi , i \u2208 I.\ne\nwhich we combine with Theorem 5.16 to get total correctness for a transformation:\nCorollary 5.19. If we have ei Ds e\u2032i for all i \u2208 I, then gi Ds hi , i \u2208 I.\nImprovement theory in a call-by-value setting requires Sands operational metatheory\nfor functional languages [41], where the improvement theory is a simple corollary over the\nwell-founded resource structure hN, 0, +, \u2265i. For simplicity of presentation we instantiate\nSands's theorems to our language. We use \u2261 to denote expressions equal up to renaming\nof bound variables and borrow a set of improvement laws that will be useful for our proof:\nLemma 5.20 (Sands [40]). Improvement laws\n(1) If e D e\u2032 then C[e] D C[e\u2032 ].\n(2) If e \u2261 e\u2032 then e D e\u2032 .\n(3) If e D e\u2032 and e\u2032 D e\u2032\u2032 then e D e\u2032\u2032\n(4) If e 7\u2192 e\u2032 then e D e\u2032 .\n(5) If e D e\u2032 then e \u228f e\u2032 .\ne\nIt is sometimes convenient to show that two expressions are related by showing that\nwhat they evaluate to is related.\nLemma 5.21 (Sands [39]). If e1 7\u2192r e\u20321 and e2 7\u2192r e\u20322 then (e\u20321 ED e\u20322 \u21d4 e1 ED e2 ).\n\n\f20\n\nP. A. JONSSON\n\nWe need to show strong improvement in order to prove total correctness. Since strong\nimprovement is improvement in one direction and operational approximation in the other\ndirection, a set of approximation laws that correspond to the improvement laws in Lemma\n5.20 is necessary.\nLemma 5.22. Approximation laws\n(1) If e \u2290 e\u2032 then C[e] \u2290 C[e\u2032 ].\n(2) If e e\n\u2261 e\u2032 then e \u2290 ee\u2032 .\n(3) If e \u2290 e\u2032 and e\u2032 \u2290ee\u2032\u2032 then e \u2290 e\u2032\u2032\ne\n(4) If e e\n7\u2192 e\u2032 then ee\u2290 e\u2032 .\ne\nCombining Lemma 5.20 and Lemma 5.22 gives us the final tools we need to prove strong\nimprovement:\nLemma 5.23. Strong Improvement laws\n(1) If e Ds e\u2032 then C[e] Ds C[e\u2032 ].\n(2) If e \u2261 e\u2032 then e Ds e\u2032 .\n(3) If e Ds e\u2032 and e\u2032 Ds e\u2032\u2032 then e Ds e\u2032\u2032\n(4) If e 7\u2192 e\u2032 then e Ds e\u2032 .\nIf two expressions are improvements of each other, they are considered cost equivalent.\nCost equivalence also implies strong improvement, which will be useful in many parts of\nour proof of total correctness for our supercompiler.\nDefinition 5.24 (Cost equivalence). The expressions e and e\u2032 are cost equivalent, e ED e\u2032\niff e D e\u2032 and e\u2032 D e\nA local form of the improvement theorem which deals with local expression-level recursion expressed with a fixed-point combinator or with a letrec definition is necessary. This\nis analogous to the work by Sands [39], with slight modifications for call-by-value.\nWe need to relate local recursion expressed using fix and the recursive definitions which\nthe improvement theorem is defined for. This is solved by a technical lemma that relates\nthe cost of terms on a certain form to their recursive counterparts.\nTheorem 5.25. For all expressions e, if \u03bbg.e is closed, then f ix (\u03bbg.e) ED h, where h is\na new function defined by h = [\u03bbn.h n/g]e.\nProof (Similar to Sands [39]). Define a helper function h\u2212 = [\u03bbn.f ix (\u03bbg.e) n/g]e. Since\nf ix (\u03bbg.e) 7\u21921 (\u03bbf.f (\u03bbn.f ix f n)) (\u03bbg.e) 7\u2192 (\u03bbg.e) (\u03bbn.f ix (\u03bbg.e) n) 7\u2192 [\u03bbn.f ix (\u03bbg.e) n/g]e\nand h\u2212 7\u21921 [\u03bbn.f ix (\u03bbg.e) n/g]e it follows by Lemma 5.21 that f ix (\u03bbg.e) ED h\u2212 . Since cost\nequivalence is a congruence relation we have that [\u03bbn.h\u2212 n/g]e ED [\u03bbn.f ix (\u03bbg.e) n/g]e,\nand so by Theorem 5.17, we have a cost-equivalent transformation from h\u2212 to h, where\nh = [h/h\u2212 ][\u03bbn.h\u2212 n/g]e = [\u03bbn.h n/g]e.\nWe state some simple properties that will be useful for proving our local improvement\ntheorem\nTheorem 5.26. Consequences of the letrec definition\ni): letrec h = \u03bbx.e in e\u2032 ED [\u03bbn.f ix (\u03bbh.\u03bbx.e) n/h]e\u2032\nii): letrec h = \u03bbx.e in h ED \u03bbn.f ix (\u03bbh.\u03bbx.e) n\niii): letrec h = \u03bbx.e in e\u2032 ED [letrec h = \u03bbx.e in h/h]e\u2032\n\n\fPOSITIVE SUPERCOMPILATION\n\n21\n\nProof. For i), expand the definition of letrec in the LHS, (\u03bbh.e\u2032 ) (\u03bbn.f ix (\u03bbh.\u03bbx.e) n) and\nevaluate it one step to [\u03bbn.f ix (\u03bbh.\u03bbx.e) n/h]e\u2032 . This is syntactically equivalent to the RHS,\nhence cost equivalent. For ii), set e\u2032 = h and perform the substitution from i). For iii), use\nthe RHS of ii) in the substitution and notice it is equivalent to i).\nThis allows us to state the local version of the improvement theorem:\nTheorem 5.27 (Local improvement theorem). If variables h and x include all the free\nvariables of both e0 and e1 , then if\nletrec h = \u03bbx.e0 in e0 Ds letrec h = \u03bbx.e0 in e1\nthen for all expressions e\nletrec h = \u03bbx.e0 in e Ds letrec h = \u03bbx.e1 in e\nProof. Define a new function g = [\u03bbn.g n/h]\u03bbx.e0 . By Proposition 5.25 g ED f ix (\u03bbh.\u03bbx.e0 ).\nUse this, the congruence properties, and the properties listed in Proposition 5.26 to transform the premise of the theorem:\nletrec h = \u03bbx.e0 in e0 Ds letrec h = \u03bbx.e0 in e1\n[\u03bbn.f ix (\u03bbh.\u03bbx.e0 ) n/h]e0 Ds [\u03bbn.f ix (\u03bbh.\u03bbx.e0 ) n/h]e1\n\u03bbx.[\u03bbn.f ix (\u03bbh.\u03bbx.e0 ) n/h]e0 Ds \u03bbx.[\u03bbn.f ix (\u03bbh.\u03bbx.e0 ) n/h]e1\n[\u03bbn.f ix (\u03bbh.\u03bbx.e0 ) n/h]\u03bbx.e0 Ds [\u03bbn.f ix (\u03bbh.\u03bbx.e0 ) n/h]\u03bbx.e1\n[\u03bbn.g n/h]\u03bbx.e0 Ds [\u03bbn.g n/h]\u03bbx.e1\nSo by Corollary 5.19, g Ds g\u2032 where g\u2032 = [g\u2032 /g][\u03bbn.g n/h]\u03bbx.e1 = [\u03bbn.g n/h]\u03bbx.e1 . Hence\nby Proposition 5.25, g\u2032 ED f ix (\u03bbh.\u03bbx.e1 ). Adding it all together yields f ix (\u03bbh.\u03bbx.e0 ) ED\ng Ds g\u2032 ED f ix (\u03bbh.\u03bbx.e1 ). From the transitivity and congruence properties of improvement\nwe can deduce that \u03bbn.f ix (\u03bbh.\u03bbx.e0 ) Ds \u03bbn.f ix (\u03bbh.\u03bbx.e1 ). By Proposition 5.26 we get\nletrec h = \u03bbx.e0 in h Ds letrec h = \u03bbx.e1 in h, which can be further expanded by congruency\nproperties of improvement to [letrec h = \u03bbx.e0 in h/h]e Ds [letrec h = \u03bbx.e1 in h/h]e. Using\nProposition 5.26 one more time yields letrec h = \u03bbx.e0 in e Ds letrec h = \u03bbx.e1 in e, which\nproves our theorem.\nThis allows us to state the total correctness theorem for our transformation:\nTheorem 5.28 (Total Correctness). Let Rhei be an expression, G a recursive map, and \u03c1\nan environment such that\n\u2022 the range of \u03c1 contains only closed expressions, and\n\u2022 fv(Rhei) \u2229 dom(\u03c1) = \u2205, and\nthen Rhei Ds \u03c1(DJeKR,G,\u03c1 ).\nThe proof is in Appendix A.1 to Appendix A.20.\n\n\f22\n\nP. A. JONSSON\n\n6. Benchmarks\nIn this section we provide measurements on a set of common examples from the literature on deforestation and perform a detailed analysis for each example. We show that our\npositive supercompiler removes intermediate structures and can improve the performance\nby an order of magnitude for certain benchmarks. The supercompiler was implemented as a\npass in the Timber compiler [34]. Timber is a pure functional call-by-value language which\nis very close to the language we describe in Section 3, and for the scope of this article it\ncan be thought of as a strict variant of Haskell. We have left out the full details of the\ninstrumentation of the run-time system but it is available in a separate report [19].\nAll measurements were performed on an idle machine running in an xterm terminal\nenvironment. Each test was run 10 consecutive times and the best result was selected\nbecause the programs are deterministic and the best result must appear under the minimum\nof other activity. The number of allocations and the total allocation sizes remained constant\nover all runs.\nRaw data for the time and size measurements before and after supercompilation are\nshown in Table 1, and allocation measures in Table 2. Compilation times are shown in Table\n3. The time column is the number of clock ticks obtained from the RDTSC instruction\navailable on Intel/AMD processors, and the binary size is in bytes. The total number of\nallocations and the total memory size in bytes allocated by the program are displayed in\ntheir respective column. The compilation times are measured in seconds and times from\nleft to right are for producing an object file, producing an executable binary, and the\ncorresponding operations with supercompilation turned on.\nBinary sizes are slightly increased by the supercompiler, but all run-times are faster.\nThe main reason for the performance improvement is the removal of intermediate structures,\nreducing the number of memory allocations. Compilation times are increased by 10-15%\nwhen enabling the supercompiler.\nThe supercompiled results on these particular benchmarks are identical to the results\nreported in previous work for call-by-name languages by Wadler [57] and S\u00f8rensen et al.\n[47]. We do not provide any execution-time comparisons with these, though, since for\nidentical intermediate representations after supercompilation, such measurements would\nonly illustrate differences caused by back-end implementation techniques.\nThe work on Supero by Mitchell and Runciman [30] shows that there remain open\nproblems when supercompiling large Haskell programs. These problems are mainly related\nto speed, both of the compiler and of the transformed program. When profiling Supero,\nMitchell and Runciman found that a majority of the time was spent on their homeomorphic\nembedding test. Our transformation performs the corresponding test on a smaller part of\nthe abstract syntax tree, so there is reason to believe that this will result in less time spent\non testing homeomorphic embedding even on large programs for our transformation. The\ncomplexity of the homeomorphic embedding relation has been investigated by Narendran\nand Stillman [31], and they give an algorithm of complexity O(size(e)\u00d7size(f )) for deciding\nwhether e E f . We expect essentially the same problems that Mitchell and Runciman\nobserved to appear in a call-by-value context as well, and intend to investigate them now\nthat we have a theoretical foundation for our transformation.\n\n\fPOSITIVE SUPERCOMPILATION\n\n23\n\nTime\nBenchmark\nBefore\nDouble Append\n105,844,704\nFactorial\n21,552\nFlip a Tree\n2,131,188\nSum of Squares of a Tree 276,102,012\nKort's Raytracer\n12,050,880\n\nBinary size\nBefore\nAfter\n89,484\n90,800\n88,968\n88,968\n95,452\n104,704\n95,452\n104,912\n91,968\n91,460\n\nAfter\n89,820,912\n21,024\n237,168\n28,737,648\n7,969,224\n\nTable 1: Time and size measurements\nAllocations\nBenchmark\nBefore\nAfter\nDouble Append\n270,035\n180,032\nFactorial\n9\n9\nFlip a Tree\n20,504\n57\nSum of Squares of a Tree 4,194,338\n91\nKort's Raytracer\n60,021\n17\n\nAlloc Size\nBefore\nAfter\n2,160,280\n1,440,256\n68\n68\n180,480\n620\n29,360,496\n908\n320,144\n124\n\nTable 2: Allocation measurements\nNot Supercompiled\nBenchmark\n-c\n\u2013make\nDouble Append\n0.183\n0.300\nFactorial\n0.095\n0.213\nFlip a Tree\n0.211\n0.223\nSum of Squares of a Tree 0.214\n0.332\nKort's Raytracer\n0.239\n0.359\n\nSupercompiled\n-c -S \u2013make -S\n0.202\n0.319\n0.097\n0.216\n0.230\n0.347\n0.234\n0.349\n0.278\n0.399\n\nTable 3: Compilation times\n6.1. Double Append. As previously seen, supercompiling the appending of three lists\nsaves one traversal over the first list. This is an example by Wadler [57], and the intermediate\nstructure is fused away by our supercompiler. The program is:\nappend xs ys = case xs of\n[] \u2192 ys\n(x \u2032 : xs \u2032 ) \u2192 x \u2032 : (append xs \u2032 ys)\nmain xs ys zs = append (append xs ys) zs\nSupercompiling this program gives the same result that we obtained manually in Section 2:\nh1 xs1 ys1 zs1 = case xs1 of\n[] \u2192 case ys1 of\n[] \u2192 zs1\n(y1\u2032 : ys1\u2032 ) \u2192 y1\u2032 : (h2 ys1\u2032 zs1 )\n\u2032\n\u2032\n(x1 : xs1 ) \u2192 x1\u2032 : (h1 xs1\u2032 ys1 zs1 )\nh2 xs2 ys2\n= case xs2 of\n[] \u2192 ys2\n(x2\u2032 : xs2\u2032 ) \u2192 x2\u2032 : (h2 xs2\u2032 ys2 )\nmain xs ys zs =h1 xs ys zs\n\n\f24\n\nP. A. JONSSON\n\nIn this measurement, three strings of 9000 characters each were appended to each other\ninto a 27 000 character string. As can be seen in Table 2, the number of allocations goes\ndown as one iteration over the first string is avoided. The binary size increases 1316 bytes,\non a binary of roughly 90k.\n6.2. Factorial. There are no intermediate lists created in a standard implementation of\na factorial function, so any performance improvements must come from inlining or static\nreductions.\nfac 0 =1\nfac n =n \u2217 fac (n \u2212 1)\nmain =show (fac 3)\nThe program is transformed to:\nh0\nhn\n\n=1\n=n \u2217 h (n \u2212 1)\n\nmain =show (3 \u2217 h 2)\nOne recursion and a couple of reductions are eliminated, thereby slightly reducing the\nrun-time. The allocations remain the same and the final binary size remains unchanged.\n6.3. Flip a Tree. Flipping a tree is another example by Wadler [57], and just like Wadler\nwe perform a double flip (thus restoring the original tree) before printing the total sum of\nall leaves.\ndata Tree a = Leaf a | Branch (Tree a) (Tree a)\nsumtr (Leaf a) = a\nsumtr (Branch l r ) = sumtr l + sumtr r\nflip (Leaf x ) = Leaf x\nflip (Branch l r ) = Branch (flip r ) (flip l)\nmain xs = let ys = (flip (flip xs)) in show (sumtr ys)\nThis is transformed into:\nh t = case t of\nLeaf d \u2192 d\nBranch l r \u2192 (h l) + (h r )\nmain xs = show ( case xs of\nLeaf d \u2192 d\nBranch l r \u2192 (h l) + (h r ) )\nA binary tree of depth 12 was used in the measurement. The function h is isomorphic\nto sumtr in the input program, and the double flip has been eliminated. Both the total\nnumber of allocations and the total size of allocations is reduced. The run-time is reduced\nby an order of magnitude. The binary size increases by about 10%, though.\n\n\fPOSITIVE SUPERCOMPILATION\n\n25\n\n6.4. Sum of Squares of a Tree. Computing the sum of the squares of the data members\nof a tree is the final example by Wadler [57].\ndata Tree a = Leaf a | Branch (Tree a) (Tree a)\nsquare :: Int \u2192 Int\nsquare x = x \u2217 x\nsumtr (Leaf x ) = x\nsumtr (Branch l r ) = sumtr l + sumtr r\nsquaretr (Leaf x ) = Leaf (square x )\nsquaretr (Branch l r ) = Branch (squaretr l) (squaretr r )\nmain xs = show (sumtr (squaretr xs))\nThis is transformed to:\nh t = case t of\nLeaf d \u2192 d \u2217 d\nBranch l r \u2192 (h l) + (h r )\nmain xs = show ( case xs of\nLeaf d \u2192 d \u2217 d\nBranch l r \u2192 (h l) + (h r )\nAlmost all allocations are removed by our supercompiler, but the binary size is increased\nby nearly 10%. The run-time is improved by an order of magnitude.\n6.5. Kort's Raytracer. The inner loop of a raytracer [22] written in Haskell is extracted\nand transformed.\nzipWith f (x : xs) (y : ys) = (f x y) : zipWith f xs ys\n= []\nzipWith\nsum :: [Int] \u2192 Int\nsum [] = 0\nsum (x : xs) = x + sum xs\nmain xs ys = sum (zipWith (\u2217) xs ys)\nThe transformed result is:\nh xs ys = case xs of\n(x \u2032 : xs \u2032 ) \u2192 case ys of\n(y \u2032 : ys \u2032 ) \u2192 (x \u2032 \u2217 y \u2032 ) + (h xs \u2032 ys \u2032 )\n\u2192 0\n\u2192 0\nmain xs ys = h xs ys\nThe total run-time, the number of allocations, the total size of allocations and the\nbinary size all decrease.\n\n\f26\n\nP. A. JONSSON\n\n7. Related Work\nThere is much literature concerning algorithms that remove intermediate structures\nin functional programs. However, most of these works are in the the context of call-byname or call-by-need languages, which makes the task of supercompilation a different, yet\ndifficult, problem. We therefore start our survey of related work with one call-by-value\ntransformation and then look at the related transformations from a call-by-name or callby-need perspective.\n7.1. Lightweight Fusion. Ohori's and Sasano's Lightweight Fusion [35] works by promoting functions through the fix-point operator and guarantees termination by limiting\ninlining to at most once per function. They implement their transformation in a compiler\nfor a variant of Standard ML and present some benchmarks. The algorithm is proven correct for a call-by-name language. It is explicitly mentioned that their goal is to extend the\ntransformation to work for an impure call-by-value functional language.\nComparing lightweight fusion to our positive supercompiler is somewhat difficult, the\nalgorithms themselves are not very similar. Comparing results of the algorithms is more\nstraightforward \u2013 the restriction to only inline functions once makes lightweight fusion unable to handle successive applications of the same function or mutually recursive functions,\nsomething the positive supercompiler handles gracefully.\nDespite the early stage of their work, Ohori and Sasano are proposing an interesting\napproach that appears quite powerful.\n7.2. Deforestation. Deforestation was pioneered by Wadler [57] for a first order language\nmore than fifteen years ago. The function macros supported by the initial deforestation\nalgorithm were not capable of fully emulating higher-order functions.\nMarlow and Wadler [27] addressed the first-order restriction in a subsequent article [27].\nThis work was refined in Marlow's [1995] dissertation, where he also related deforestation to\nthe cut-elimination principle of logic. Chin [5] has also generalised Wadler's deforestation\nto higher-order functional programs by using syntactic properties to decide which terms\nthat can be fused.\nBoth Hamilton [14] and Marlow [28] have proven that their deforestation algorithms\nterminate. More recent work by Hamilton [15] extends deforestation with a treeless form\nthat is easy to recognise and handles a wide range of functions, giving more transparency\nfor the programmer.\nAlimarine and Smetsers [2] have improved the producer and consumer analyses in Chin's\n[1994] algorithm to be based on semantics rather than syntax. They show that their algorithm can remove much of the overhead introduced by generic programming [16].\nWhile these works are algorithmically rather close to ours due to the close relationship\nbetween deforestation and positive supercompilation, it supposes either a call-by-name or\ncall-by-need context, and is thus not applicable to the kind of languages we target.\n7.3. Supercompilation. Closely related to deforestation is supercompilation [52, 53, 54,\n55]. Supercompilation both removes intermediate structures and achieves partial evaluation,\nas well as some other optimisations. In partial evaluation terminology, the decision of when\nto inline is taken online. The initial studies on supercompilation were for the functional\n\n\fPOSITIVE SUPERCOMPILATION\n\n27\n\nlanguage Refal [56]. The supercompiler Scp4 [33] is implemented in Refal and is the most\nwell-known implementation from this line of work.\nThe positive supercompiler [47] is a variant which only propagates positive information\nsuch as inferred equalities between terms. The propagation is done by unification and\nthe work highlights how similar deforestation and positive supercompilation really are.\nNarrowing-driven partial evaluation [3, 1] is the functional logic programming equivalent of\npositive supercompilation but formulated as a term rewriting system. Their approach also\ndeals with non-determinism from backtracking, which makes the corresponding algorithms\nmore complicated.\nStrengthening the information propagation mechanism to propagate not only positive,\nbut also negative information, yields perfect supercompilation [42, 43]. Negative information\nis the opposite of positive information, namely inequalities. These inequalities can be used\nto prune case-expression branches known not to be applicable, for example.\nMore recently, Mitchell and Runciman [30] have worked on supercompiling Haskell.\nThey report run-time reductions of up to 55% when their supercompiler is used in conjunction with GHC.\nSupercompilation has seen applications beyond program optimization: verification of\ncache coherence protocols [26] and proving term equivalence [21] are two examples. We\ndo not believe that our supercompiler is useful for these applications since it is inherently\nweaker than the corresponding supercompiler with call-by-name semantics.\nThe positive supercompiler by S\u00f8rensen et al. [47] is the immediate ancestor of our\nwork, although we have extended it to a higher-order language and converted it to work\ncorrectly for call-by-value languages.\n7.4. Generalized Partial Computation. GPC [9, 50] uses a theorem prover to extract\nadditional properties about the program being specialized. Among these properties are the\nlogical structure of a program, axioms for abstract data types, and algebraic properties of\nprimitive functions.\nThe theorem prover is applied whenever a test is encountered, in order to determine\nwhich subset of the execution branches can actually be taken. Information about the\npredicate that was tested is propagated along the branches that are left in the resulting\nprogram. The reason GPC is such a powerful transformation is because it assumes the\nunlimited power of a theorem prover.\nFutamura et al. [10] have applied GPC in a call-by-value setting in a system called WSDFU (Waseda Simplify-Distribute-Fold-Unfold), and report many successful experiments\nwhere optimal or near optimal residual programs are produced. It is unclear whether WSDFU preserves termination behavior or if it is a call-by-name transformation applied to a\ncall-by-value language.\nWe note that the rules for the first order language presented by Takano [50] are very\nsimilar to the positive supercompiler, but the requirement for a theorem prover might\nexclude the technique as a candidate for automatic compiler optimisations. The lack of\ntermination guarantees for the transformation might be another obstacle. Considering the\nsimilarities between GPC and positive supercompilation it should be straightforward to\nconvert GPC to a call-by-value setting.\n\n\f28\n\nP. A. JONSSON\n\n7.5. Other Transformations. Considering the vast amount of research conducted on program transformations in general, we only briefly survey other related transformations.\n7.5.1. Partial Evaluation. Partial evaluation [18] is another instance of Burstall and Darlington's [1977] informal class of fold/unfold transformations.\nIf partial evaluation is performed offline, the process is guided by program annotations\nthat tell when to fold, unfold, instantiate and define functions. Binding-Time Analysis\n(BTA) is a program analysis that annotates operations in the input program based on\nwhether they are statically known or not.\nPartial evaluation does not remove intermediate structures, something we deem necessary to enable the programmer to write programs in the clear and concise listful style. Both\ndeforestation and supercompilation simulate call-by-name evaluation in the transformer,\nwhereas partial evaluation simulates call-by-value. It is suggested by S\u00f8rensen et al. [46]\nthat this might affect the strength of the transformation.\n7.5.2. Short Cut Fusion. Short cut deforestation [12, 13] takes a different approach to deforestation, sacrificing some generality by only working on lists.\nThe idea is that the constructors Nil and Cons can be replaced by a foldr consumer,\nand a special function build is used to enable the transformation to recognize the producer\nand enforce a type requirement. Lists using build/foldr can easily be removed with the\nfoldr/build rule:\nfoldr f c (build g) = g f c\nIt is the responsibility of the programmer or compiler writer to make sure list-traversing\nfunctions are written using build and foldr, thereby cluttering the code with information for\nthe optimiser and making it harder to read and understand for humans.\nGill implemented and measured short cut deforestation in GHC using the nofib benchmark suite [37]. Around a dozen benchmarks improved by more than 5%, the average was\n3% and only one example got noticeably worse, by 1%. Heap allocations were reduced, by\nhalf in one particular case.\nThe main argument for short cut deforestation is its simplicity on the compiler side\ncompared to full-blown deforestation. GHC currently contains a variant of the short cut\ndeforestation implemented using rewrite rules [38].\nTakano and Meijer [51] generalized short cut deforestation to work for any algebraic\ndatatype through the acid rain theorem. Ghani and Johann [11] have also generalized\nthe foldr/build rule to a fold/superbuild rule that can eliminate intermediate structures of\ninductive types without disturbing the contexts in which they are situated.\nLaunchbury and Sheard [24] worked on automatically transforming programs into suitable form for shortcut deforestation. Onoue et al. [36] showed an implementation of the\nacid rain theorem for Gofer where they could automatically transform recursive functions\ninto a form suitable for shortcut fusion.\nChitil [6] used type-inference to transform the producer of lists into the abstracted form\nrequired by short cut deforestation. Given a type-inference algorithm which infers the most\ngeneral type, Chitil is able to determine the list constructors that need to be replaced in\none pass.\nFrom the principal type property of the type inference algorithm Chitil was also able to\ndeduce completeness of the list abstraction algorithm. This completeness guarantees that\n\n\fPOSITIVE SUPERCOMPILATION\n\n29\n\nif a list can be abstracted from a producer by abstracting its list constructors, then the list\nabstraction algorithm will do so.\nThe implications of the completeness of the list abstraction algorithm is that a foldr\nconsumer can be fused with nearly any producer. One reason list constructors might not\nbe abstractable from a producer is that they do not occur in the producer expression but\nin the definition of a function which is called by the producer. A worker/wrapper scheme\nproposed by Chitil ensures that these list constructors are moved to the producer in order\nto make list abstraction possible.\nThe completeness property and the fact that the programmer does not have to write\nany special code, in combination with the promising results from measurements, suggest\nthat short cut deforestation based on type-inference is a practical optimisation.\nTakano and Meijer [51] noted that the foldr/build rule for short cut deforestation has\na dual. This is the destroy/unfoldr rule used in Zip Fusion [48], which has some interesting\nproperties: it can remove all argument lists from a function which consumes more than one\nlist. The method described by Svenningsson removes all intermediate lists in zip [1..n] [1..n],\naddressing one of the main criticisms against the foldr/build rule. The technique can also\nremove intermediate lists from functions which consume their lists using accumulating parameters, which is usually a problematic case. The destroy/unfoldr rule is defined as:\ndestroy g (unfoldr psi e) = g psi e\nThe Zip Fusion method is simple, and can be implemented in the same way as short\ncut deforestation. It still suffers from the drawback that the programmer or compiler writer\nhas to make sure the list traversing functions are written using destroy and unfoldr.\nIn more recent work Coutts et al. [7] have extended these techniques to work on functions that handle nested lists, list comprehensions and filter-like functions.\n8. Conclusions\nWe have presented a positive supercompiler for a higher-order call-by-value language\nand proven it correct with respect to call-by-value semantics. The adjustments required to\npreserve the termination properties of call-by-value evaluation are new and work well for\nmany examples in the literature intended to show the usefulness of call-by-name transformations.\n8.1. Future Work. We believe that the linearity restriction of rule R14 is not necessary\nfor the soundness of our transformation, but have not yet found a way to prove this. This is\na natural topic for future work, as is an investigation of whether the concept of an inlining\nbudget may be used to control the balance between supercompilation benefits and code size.\nMore work could be done on the strictness analysis component of our supercompiler.\nWe do not intend to focus on that subject, though; instead we hope that the modular\ndependency on strictness analysis will allow our supercompiler to readily take advantage of\ngeneral improvements in the area.\nThe supercompiler described in this article can be said to supersede several of the\nstandard transformations commonly implemented by optimizing compilers, such as copy\npropagation, constant folding and basic inlining. We conjecture that this range could be\nextended to include transformations like common subexpression elimination as well, by\n\n\f30\n\nP. A. JONSSON\n\nmeans of moderately small algorithm changes. An investigation of the scope for such generalizations is an important area of future research.\nAcknowledgements\nThe authors would like to thank Simon Marlow, Duncan Coutts and Neil Mitchell for\nvaluable discussions. Thorsten Altenkirch contributed insights about non-termination. We\nwould also like to thank Viktor Leijon and the anonymous referees for POPL'09 for providing\nuseful comments that helped improve the presentation and contents, and Germ\u00e1n Vidal for\nexplaining narrowing-driven partial evaluation to us.\nReferences\n[1] E. Albert and G. Vidal. The narrowing-driven approach to functional logic program\nspecialization. New Generation Comput, 20(1):3\u201326, 2001.\n[2] A. Alimarine and S. Smetsers. Improved fusion for optimizing generics. In Manuel V.\nHermenegildo and Daniel Cabeza, editors, Practical Aspects of Declarative Languages,\n7th International Symposium, PADL 2005, Long Beach, CA, USA, January 10-11,\n2005, Proceedings, volume 3350 of Lecture Notes in Computer Science, pages 203\u2013218.\nSpringer, 2005. ISBN 3-540-24362-3.\n[3] M. Alpuente, M. Falaschi, and G. Vidal. Partial Evaluation of Functional Logic Programs. ACM Transactions on Programming Languages and Systems, 20(4):768\u2013844,\n1998.\n[4] R.M. Burstall and J. Darlington. A transformation system for developing recursive\nprograms. Journal of the ACM, 24(1):44\u201367, January 1977.\n[5] W-N. Chin. Safe fusion of functional expressions II: Further improvements. J. Funct.\nProgram, 4(4):515\u2013555, 1994.\n[6] O. Chitil. Type-Inference Based Deforestation of Functional Programs. PhD thesis,\nRWTH Aachen, October 2000.\n[7] D. Coutts, R. Leshchinskiy, and D. Stewart. Stream fusion: from lists to streams to\nnothing at all. In ICFP '07: Proceedings of the 12th ACM SIGPLAN international\nconference on Functional programming, pages 315\u2013326, New York, NY, USA, 2007.\nACM. ISBN 978-1-59593-815-2.\n[8] N. Dershowitz. Termination of rewriting. Journal of Symbolic Computation, 3(1):\n69\u2013115, 1987.\n[9] Y. Futamura and K. Nogi. Generalized partial computation. In D. Bj\u00f8rner, A.P.\nErshov, and N.D. Jones, editors, Partial Evaluation and Mixed Computation, pages\n133\u2013151. Amsterdam: North-Holland, 1988.\n[10] Y. Futamura, Z. Konishi, and R. Gl\u00fcck. Program transformation system based on\ngeneralized partial computation. New Gen. Comput., 20(1):75\u201399, 2002. ISSN 02883635.\n[11] N. Ghani and P. Johann. Short cut fusion of recursive programs with computational\neffects. In P. Achten, P. Koopman, and M. T. Moraz\u00e1n, editors, Draft Proceedings of\nThe Ninth Symposium on Trends in Functional Programming (TFP), number ICIS\u2013\nR08007, 2008.\n\n\fPOSITIVE SUPERCOMPILATION\n\n31\n\n[12] A. Gill, J. Launchbury, and S.L. Peyton Jones. A short cut to deforestation. In Functional Programming Languages and Computer Architecture, Copenhagen, Denmark,\n1993, 1993.\n[13] A. J. Gill. Cheap Deforestation for Non-strict Functional Languages. PhD thesis, Univ.\nof Glasgow, January 1996.\n[14] G. W. Hamilton. Higher order deforestation. In PLILP '96: Proceedings of the 8th\nInternational Symposium on Programming Languages: Implementations, Logics, and\nPrograms, pages 213\u2013227, London, UK, 1996. Springer-Verlag. ISBN 3-540-61756-6.\n[15] G. W. Hamilton. Higher order deforestation. Fundam. Informaticae, 69(1-2):39\u201361,\n2006.\n[16] R. Hinze. Generic Programs and Proofs. Habilitationsschrift, Bonn University, 2000.\n[17] T. Johnsson. Lambda lifting: Transforming programs to recursive equations. In FPCA,\npages 190\u2013203, 1985.\n[18] N.D. Jones, C.K. Gomard, and P. Sestoft. Partial Evaluation and Automatic Program\nGeneration. Englewood Cliffs, NJ: Prentice Hall, 1993. ISBN 0-13-020249-5.\n[19] P. A. Jonsson. Positive supercompilation for a higher-order call-by-value language.\nLicentiate thesis, Lule\u00e5 University of Technology, Sweden, Jun 2008.\n[20] P. A. Jonsson and J. Nordlander. Positive supercompilation for a higher-order call-byvalue language. In POPL '09: Proceedings of the 36th annual ACM SIGPLAN-SIGACT\nsymposium on Principles of programming languages, 2009.\n[21] I. Klyuchnikov and S. Romanenko. Proving the equivalence of higher-order terms by\nmeans of supercompilation. In PSI '09: Proceedings of the Seventh International Andrei\nErshov Memorial Conference, 2009.\n[22] J. Kort. Deforestation of a raytracer. Master's thesis, University of Amsterdam, 1996.\n[23] J-L. Lassez, M. Maher, and K. Marriott. Unification revisited. In Jack Minker, editor,\nFoundations of Deductive Databases and Logic Programming, pages 587\u2013625. Morgan\nKaufmann, 1988.\n[24] J. Launchbury and T. Sheard. Warm fusion: Deriving build-cata's from recursive\ndefinitions. In FPCA, pages 314\u2013323, 1995.\n[25] X. Leroy.\nThe Objective Caml system: Documentation and user's manual,\n2008. With D. Doligez, J. Garrigue, D. R\u00e9my, and J. Vouillon. Available from\nhttp://caml.inria.fr (1996\u20132008).\n[26] A. Lisitsa and A. P. Nemytykh. Verification as a parameterized testing (experiments\nwith the SCP4 supercompiler). Programming and Computer Software, 33(1):14\u201323,\n2007.\n[27] S. Marlow and P. Wadler. Deforestation for higher-order functions. In John Launchbury\nand Patrick M. Sansom, editors, Functional Programming, Workshops in Computing,\npages 154\u2013165. Springer, 1992. ISBN 3-540-19820-2.\n[28] S. D. Marlow. Deforestation for Higher-Order Functional Programs. PhD thesis, Department of Computing Science, University of Glasgow, April 27 1995.\n[29] R. Milner, M. Tofte, R. Harper, and D. MacQueen. The Definition of Standard ML,\nRevised edition. MIT Press, 1997.\n[30] N. Mitchell and C. Runciman. A supercompiler for core Haskell. In O. Chitil et al.,\neditor, Selected Papers from the Proceedings of IFL 2007, volume 5083 of Lecture Notes\nin Computer Science, pages 147\u2013164. Springer-Verlag, 2008.\n[31] P. Narendran and J. Stillman. On the Complexity of Homeomorphic Embeddings.\nTechnical Report 87-8, Computer Science Department, State Univeristy of New York\n\n\f32\n\nP. A. JONSSON\n\nat Albany, March 1987.\n[32] C. St. J. A. Nash-Williams. On well-quasi-ordering finite trees. Proceedings of the\nCambridge Philosophical Society, 59(4):833\u2013835, October 1963.\n[33] A. P. Nemytykh. The supercompiler SCP4: General structure. In Manfred Broy and\nAlexandre V. Zamulin, editors, Perspectives of Systems Informatics, 5th International\nAndrei Ershov Memorial Conference, PSI 2003, Akademgorodok, Novosibirsk, Russia,\nJuly 9-12, 2003, Revised Papers, volume 2890 of LNCS, pages 162\u2013170. Springer, 2003.\nISBN 3-540-20813-5.\n[34] J. Nordlander, M. Carlsson, A. Gill, P. Lindgren, and B. von Sydow. The Timber home\npage, 2008. URL http://www.timber-lang.org.\n[35] A. Ohori and I. Sasano. Lightweight fusion by fixed point promotion. In POPL '07:\nProceedings of the 34th annual ACM SIGPLAN-SIGACT symposium on Principles\nof programming languages, pages 143\u2013154, New York, NY, USA, 2007. ACM. ISBN\n1-59593-575-4.\n[36] Y. Onoue, Z. Hu, H. Iwasaki, and M. Takeichi. A calculational fusion system HYLO.\nIn R. S. Bird and L. G. L. T. Meertens, editors, Algorithmic Languages and Calculi,\nIFIP TC2 WG2.1 International Workshop on Algorithmic Languages and Calculi, 1722 February 1997, Alsace, France, volume 95 of IFIP Conference Proceedings, pages\n76\u2013106. Chapman & Hall, 1997. ISBN 0-412-82050-1.\n[37] W. Partain. The nofib benchmark suite of Haskell programs. In John Launchbury and\nPatrick M. Sansom, editors, Functional Programming, Workshops in Computing, pages\n195\u2013202. Springer, 1992. ISBN 3-540-19820-2.\n[38] S. L. Peyton Jones, A. Tolmach, and T. Hoare. Playing by the rules: Rewriting as\na practical optimisation technique in GHC. In Ralf Hinze, editor, Proceedings of the\n2001 ACM SIGPLAN Haskell Workshop (HW'2001), 2nd September 2001, Firenze,\nItaly., Electronic Notes in Theoretical Computer Science, Vol 59. Utrecht University,\nSeptember 28 2001. UU-CS-2001-23.\n[39] D. Sands. Proving the correctness of recursion-based automatic program transformations. Theoretical Computer Science, 167(1\u20132):193\u2013233, 30 October 1996.\n[40] D. Sands. Total correctness by local improvement in the transformation of functional\nprograms. ACM Transactions on Programming Languages and Systems, 18(2):175\u2013234,\nMarch 1996.\n[41] D. Sands. From SOS rules to proof principles: An operational metatheory for functional\nlanguages. In Proceedings of the 24th Annual ACM SIGPLAN-SIGACT Symposium\non Principles of Programming Languages (POPL). ACM Press, January 1997.\n[42] J. P. Secher. Perfect supercompilation. Technical Report DIKU-TR-99/1, Department\nof Computer Science (DIKU), University of Copenhagen, February 1999.\n[43] J.P. Secher and M.H. S\u00f8rensen. On perfect supercompilation. In D. Bj\u00f8rner, M. Broy,\nand A. Zamulin, editors, Proceedings of Perspectives of System Informatics, volume\n1755 of Lecture Notes in Computer Science, pages 113\u2013127. Springer-Verlag, 2000.\n[44] M.H. S\u00f8rensen. Convergence of program transformers in the metric space of trees. Sci.\nComput. Program, 37(1-3):163\u2013205, 2000.\n[45] M.H. S\u00f8rensen and R. Gl\u00fcck. An algorithm of generalization in positive supercompilation. In J.W. Lloyd, editor, International Logic Programming Symposium, pages\n465\u2013479. Cambridge, MA: MIT Press, 1995.\n\n\fPOSITIVE SUPERCOMPILATION\n\n33\n\n[46] M.H. S\u00f8rensen, R. Gl\u00fcck, and N.D. Jones. Towards unifying partial evaluation, deforestation, supercompilation, and GPC. In D. Sannella, editor, Programming Languages and Systems - ESOP'94. 5th European Symposium on Programming, Edinburgh, U.K., April 1994 (Lecture Notes in Computer Science, vol. 788), pages 485\u2013500.\nBerlin: Springer-Verlag, 1994.\n[47] M.H. S\u00f8rensen, R. Gl\u00fcck, and N.D. Jones. A positive supercompiler. Journal of\nFunctional Programming, 6(6):811\u2013838, 1996.\n[48] J. Svenningsson. Shortcut fusion for accumulating parameters & zip-like functions. In\nICFP, pages 124\u2013132, 2002.\n[49] D. Syme.\nThe F# programming language,\nJune 2008.\nURL\nhttp://research.microsoft.com/fsharp.\n[50] A. Takano. Generalized partial computation for a lazy functional language. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut\n(Sigplan Notices, vol. 26, no. 9, September 1991), pages 1\u201311. New York: ACM, 1991.\n[51] A. Takano and E. Meijer. Shortcut deforestation in calculational form. In FPCA, pages\n306\u2013313, 1995.\n[52] V.F. Turchin. A supercompiler system based on the language Refal. SIGPLAN Notices,\n14(2):46\u201354, February 1979.\n[53] V.F. Turchin. Semantic definitions in Refal and automatic production of compilers. In\nN.D. Jones, editor, Semantics-Directed Compiler Generation, Aarhus, Denmark (Lecture Notes in Computer Science, vol. 94), pages 441\u2013474. Berlin: Springer-Verlag, 1980.\n[54] V.F. Turchin. Program transformation by supercompilation. In H. Ganzinger and N.D.\nJones, editors, Programs as Data Objects, Copenhagen, Denmark, 1985 (Lecture Notes\nin Computer Science, vol. 217), pages 257\u2013281. Berlin: Springer-Verlag, 1986.\n[55] V.F. Turchin. The concept of a supercompiler. ACM Transactions on Programming\nLanguages and Systems, 8(3):292\u2013325, July 1986.\n[56] V.F. Turchin. Refal-5, Programming Guide & Reference Manual. Holyoke, MA: New\nEngland Publishing Co., 1989.\n[57] P. Wadler. Deforestation: transforming programs to eliminate trees. Theoretical Computer Science, 73(2):231\u2013248, June 1990. ISSN 0304-3975.\nAppendix A. Proofs\nWe borrow a couple of technical lemmas from Sands [39], and adapt the proofs to be\nvalid under call-by-value:\nLemma A.1 (Sands, p. 24). For all expressions e and value substitutions \u03b8 such that\nh\u2208\n/ dom(\u03b8), if e0 7\u21921 e1 then\nletrec h = \u03bbx.e1 in [\u03b8(e0 )/z]e ED letrec h = \u03bbx.e1 in [h \u03b8(x)/z]e\nProof. Expanding both sides according to the definition of letrec yields:\n(\u03bbh.[\u03b8(e0 )/z]e) (\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n) ED (\u03bbh.[h \u03b8(x)/z]e) (\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n)\nand evaluating both sides one step 7\u2192 gives:\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h][\u03b8(e0 )/z]e ED [\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h][h \u03b8(x)/z]e\nFrom this we can see that it is sufficient to prove:\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e0 ED [\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]h \u03b8(x)\n\n\f34\n\nP. A. JONSSON\n\nThe substitution \u03b8 can safely be moved out since h \u2208\n/ dom(\u03b8):\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e0 ED [\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8(h x)\nPerforming evaluation steps on both sides yield:\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e0 ED[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8(h x)\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e0 ED[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8((\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n) x)\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e0 ED[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8(f ix (\u03bbh.\u03bbx.e1 ) x)\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e1 ED[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8((\u03bbf.f (\u03bbn.f ix f n)) (\u03bbh.\u03bbx.e1 ) x)\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e1 ED[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8((\u03bbh.\u03bbx.e1 ) (\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n) x)\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e1 ED[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8((\u03bbx.e1 ) x)\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e1 ED[x/x][\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e1\n[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e1 ED[\u03bbn.f ix (\u03bbh.\u03bbx.e1 ) n/h]\u03b8e1\nThe LHS and the RHS are cost equivalent, so by Lemma 5.21 the initial expressions are\ncost equivalent.\nLemma A.2 (Sands, p. 25). \u03c1\u2032 (DJRhviK\u001f,G,\u03c1\u2032 ) ED letrec h = \u03bbx.Rhvi in \u03c1(DJRhviK\u001f,G,\u03c1\u2032 )\nProof (Similar to Sands [39]). By inspection of the rules for DJ K, all free occurrences of h\nin DJRhviK\u001f,G,\u03c1\u2032 must occur in sub-expressions of the form h x. Suppose there are k such\noccurrences, which we can write as \u03b81 h x . . . \u03b8k h x, where the \u03b8i are just renamings of the\nvariables x. So DJRhviK\u001f,G,\u03c1\u2032 can be written as [\u03b81 h x . . . \u03b8k h x/z1 . . . zk ]e\u2032 , where e\u2032 contains\nno free occurrences of h. Then (substitution associates to the right):\n\u03c1\u2032 (DJRhviK\u001f,G,\u03c1\u2032 )\n\n\u2261\n\n[\u03bbx.Rhgi/h]\u03c1(DJRhviK\u001f,G,\u03c1\u2032 )\n\nED [\u03bbx.Rhgi/h]\u03c1([\u03b81 h x . . . \u03b8k h x/z1 . . . zk ]e\u2032 )\nED \u03c1([\u03b81 Rhgi . . . \u03b8k Rhgi/z1 . . . zk ]e\u2032 )\nED (by Lemma A.1)\nletrec h = \u03bbx.Rhv ei in \u03c1([\u03b81 h x . . . \u03b8k h x/z1 . . . zk ]e\u2032 )\n\u2261 letrec h = \u03bbx.Rhvi in \u03c1(DJRhviK\u001f,G,\u03c1\u2032 )\nLemma A.3. Rhlet x = e in f i Ds let x = e in Rhf i\nProof. Notice that Rhlet x = \u001f in f i is a redex, and assume e 7\u2192k v. The LHS evaluates in k\nsteps Rhlet x = e in f i 7\u2192k Rhlet x = v in f i 7\u2192 Rh[v/x]f i, and the RHS evaluates in k steps\nlet x = e in Rhf i 7\u2192k let x = v in Rhf i 7\u2192 [v/x]Rhf i. Since contexts do not bind variables\nthese two terms are equivalent and by Lemma 5.21 the initial terms are cost equivalent.\nLemma A.4. Rhletrec g = v in ei DS letrec g = v in Rhei\nProof. Translate both sides by the definition of letrec into Rh(\u03bbg.e) (\u03bbn.f ix (\u03bbg.v) n)i DS\n(\u03bbg.Rhei) (\u03bbn.f ix (\u03bbg.v) n). Notice that Rh\u001fi is a redex. The LHS evaluates in 0 steps\nto Rh(\u03bbg.e) (\u03bbn.f ix (\u03bbg.v) n)i 7\u2192 Rh[\u03bbn.f ix (\u03bbg.v) n/g]ei and the RHS evaluates in 0 steps\nto (\u03bbg.Rhei) (\u03bbn.f ix (\u03bbg.v) n) 7\u2192 [\u03bbn.f ix (\u03bbg.v) n/g]Rhei. Since our contexts do not bind\nvariables these two terms are equivalent and by Lemma 5.21 the initial terms are cost\nequivalent.\n\n\fPOSITIVE SUPERCOMPILATION\n\n35\n\nLemma A.5. Rhcase e of {pi \u2192 ei }i Ds case e of {pi \u2192 Rhei i}\nProof. Notice that Rhcase \u001f of {pi \u2192 ei }i is a redex, and assume e 7\u2192k nj . The LHS\nevaluates in k steps Rhcase e of {pi \u2192 ei }i 7\u2192k Rhcase nj of {pi \u2192 ei }i 7\u2192 Rhej i, and the\nRHS evaluates in k steps case e of {pi \u2192 Rhei i} 7\u2192k case nj of {pi \u2192 Rhei i}Rhf i 7\u2192 Rhej i.\nSince these two terms are equivalent the initial terms are cost equivalent by Lemma 5.21.\nWe set out to prove the main theorem about total correctness:\nTheorem A.6 (Total Correctness). Let Rhei be an expression, and \u03c1 an environment such\nthat\n\u2022 the range of \u03c1 contains only closed expressions, and\n\u2022 fv(Rhei) \u2229 dom(\u03c1) = \u2205, and\nthen Rhei Ds \u03c1(DJeKR,G,\u03c1 ).\nWe reason by induction on the structure of expressions, and since the algorithm is total\n(Lemma 5.12) this coincides with inspection of each rule.\nA.1. R1. We have that \u03c1(DJnKR,G,\u03c1 ) = \u03c1(Rhni), and the conditions of the proposition\nensure that fv(Rhni) \u2229 dom(\u03c1) = \u2205, so \u03c1(Rhni) = Rhni This is syntactically equivalent to\nthe input, and we conclude Rhni Ds \u03c1(DJnKR,G,\u03c1 ).\nA.2. R2. We have that \u03c1(DJxKR,G,\u03c1 ) = \u03c1(Rhxi), and the conditions of the proposition\nensure that fv(Rhxi) \u2229 dom(\u03c1) = \u2205, so \u03c1(Rhxi) = Rhxi This is syntactically equivalent to\nthe input, and we conclude Rhxi Ds \u03c1(DJxKR,G,\u03c1 ).\nA.3. R3.\nA.3.1. Case: (1).\nSuppose \u2203h.\u03c1(h) \u2261 \u03bbx.Rhgi and hence that DJRhgiK\u001f,G,\u03c1 = h x.\nThe conditions of the proposition ensure that x \u2229 dom(\u03c1) = \u2205, so \u03c1(DJRhgiK\u001f,G,\u03c1 ) =\n\u03c1(h x) = (\u03bbx.Rhgi) x. However, Rhgi and (\u03bbx.Rhgi) x are cost equivalent, which implies\nstrong improvement, and we conclude Rhgi Ds \u03c1(DJRhgiK\u001f,G,\u03c1 )\nA.3.2. Case: (2).\nSuppose \u2203(h, t) \u2208 \u03c1.t E Rhgi and that Rhgi E t, hence DJRhgiK\u001f,G,\u03c1 = Rhgi.\nThe term on the RHS is discarded and replaced with a new term higher up in the tree,\nso it does not matter what the term is.\nA.3.3. Case: (3).\n\n\f36\n\nP. A. JONSSON\n\nSuppose \u2203(h, t) \u2208 \u03c1.t E Rhgi and hence that DJRhgiK\u001f,G,\u03c1 = [DJf K\u001f,G,\u03c1 /y]DJfg K\u001f,G,\u03c1 .\nWe have \u03c1(DJgKR,G,\u03c1 ) = \u03c1([DJf K\u001f,G,\u03c1 /y]DJfg K\u001f,G,\u03c1 ) = [\u03c1(DJf K\u001f,G,\u03c1 )/x]\u03c1(DJfg K\u001f,G,\u03c1 ).\nBy the induction hypothesis, f Ds \u03c1(DJf K\u001f,G,\u03c1 ) and fg Ds \u03c1(DJfg K\u001f,G,\u03c1 ) and by congruence\nproperties of strong improvement (Lemma 5.23:1) Rhgi Ds \u03c1(DJgKR,G,\u03c1 ).\nA.3.4. Case: (4a). Analogous to the previous case.\nA.3.5. Case: (4b).\nIf DJRhgiK\u001f,G,\u03c1 = \u03c1(letrec h = \u03bbx.DJRhviK\u001f,G,\u03c1\u2032 in h x).\n/ (x \u222a dom(\u03c1)). We need to show that:\nwhere \u03c1\u2032 = \u03c1 \u222a (h, \u03bbx.Rhgi) and h \u2208\nRhgi Ds \u03c1(letrec h = \u03bbx.DJRhviK\u001f,G,\u03c1\u2032 in h x)\nSince h, x \u2208\n/ dom(\u03c1) we have that \u03c1(letrec h = \u03bbx.DJRhviK\u001f,G,\u03c1\u2032 in h x) \u2261 letrec h =\n\u03bbx.\u03c1(DJRhviK\u001f,G,\u03c1\u2032 ) in h x.\nR is a reduction context, hence Rhgi 7\u21921 Rhvi. By Lemma A.1 we have that letrec h =\n\u03bbx.Rhvi in Rhg ei ED letrec h = \u03bbx.Rhvi in h x. Since h \u2208\n/ fv(Rhgi) this simplifies to\nRhgi ED letrec h = \u03bbx.Rhvi in h x. It is necessary and sufficient to prove that\nletrec h = \u03bbx.Rhvi in h x Ds letrec h = \u03bbx.\u03c1(DJRhviK\u001f,G,\u03c1\u2032 ) in h x\nBy Theorem 5.27 it is sufficient to show:\nletrec h = \u03bbx.Rhvi in Rhvi Ds\nletrec h = \u03bbx.Rhvi in \u03c1(DJRhviK\u001f,G,\u03c1\u2032 )\nBy Lemma A.2 and letrec h = \u03bbx.Rhvi in Rhvi ED Rhvi, this is equivalent to showing that\nRhvi Ds \u03c1\u2032 (DJRhviK\u001f,G,\u03c1\u2032 )\nWhich follows from the induction hypothesis, since it is a shorter transformation.\nA.3.6. Case: (4c). We have that \u03c1(DJgKR,G,\u03c1 ) = \u03c1(DJRhviK\u001f,G,\u03c1 ). By the induction hypothesis Rhvi Ds \u03c1(DJRhviK\u001f,G,\u03c1 ), and since Rhgi 7\u21921 Rhvi it follows from Lemma 5.23:4\nthat Rhgi Ds \u03c1(DJgKR,G,\u03c1 ).\nA.4. R4. We have that \u03c1(DJk eK\u001f,G,\u03c1 ) = \u03c1(k DJeK\u001f,G,\u03c1 ), and the conditions of the proposition ensure that fv(k e) \u2229 dom(\u03c1) = \u2205, so \u03c1(k DJeK\u001f,G,\u03c1 ) = k \u03c1(DJeK\u001f,G,\u03c1 ). By the induction hypothesis, e Ds \u03c1(DJeK\u001f,G,\u03c1 ), and by congruence properties of strong improvement\n(Lemma 5.23:1) k e Ds \u03c1(DJk eK\u001f,G,\u03c1 ).\nA.5. R5. We have that \u03c1(DJx eKR,G,\u03c1 ) = \u03c1(Rhx DJeK\u001f,G,\u03c1 i), and the conditions of the\nproposition ensure that fv(Rhx ei)\u2229dom(\u03c1) = \u2205, so \u03c1(Rhx DJeK\u001f,G,\u03c1 i) = Rhx \u03c1(DJeK\u001f,G,\u03c1 )i.\nBy the induction hypothesis, e Ds \u03c1(DJeK\u001f,G,\u03c1 ), and by congruence properties of strong\nimprovement (Lemma 5.23:1) Rhx ei Ds \u03c1(DJx eKR,G,\u03c1 ).\n\n\fPOSITIVE SUPERCOMPILATION\n\n37\n\nA.6. R6. We have that \u03c1(DJ\u03bbx.eK\u001f,G,\u03c1 ) = \u03c1(\u03bbx.DJeK\u001f,G,\u03c1 ), and the conditions of the\nproposition ensure that fv(\u03bbx.e) \u2229 dom(\u03c1) = \u2205, so \u03c1(\u03bbx.DJeK\u001f,G,\u03c1 ) = \u03bbx.\u03c1(DJeK\u001f,G,\u03c1 ).\nBy the induction hypothesis, e Ds \u03c1(DJeK\u001f,G,\u03c1 ), and by congruence properties of strong\nimprovement (Lemma 5.23:1) \u03bbx.e Ds \u03c1(DJ\u03bbx.eK\u001f,G,\u03c1 ).\nA.7. R7. We have that \u03c1(DJn1 \u2295 n2 KR,G,\u03c1 ) = \u03c1(DJRhniK\u001f,G,\u03c1 ). By the induction hypothesis, Rhni Ds \u03c1(DJRhniK\u001f,G,\u03c1 ), and since Rhn1 \u2295 n2 i 7\u2192 Rhni it follows from Lemma 5.23:4\nthat Rhn1 \u2295 n2 i Ds \u03c1(DJn1 \u2295 n2 KR,G,\u03c1 ).\nA.8. R8.\na) e1 \u2295 e2 = a: We have that \u03c1(DJe1 \u2295 e2 KR,G,\u03c1 ) = \u03c1(DJe1 K\u001f,G,\u03c1 \u2295 DJe2 K\u001f,G,\u03c1 ), by the given\nconditions fv(Rhe1 \u2295 e2 i) \u2229 dom(\u03c1) = \u2205, so \u03c1(RhDJe1 K\u001f,G,\u03c1 \u2295 DJe2 K\u001f,G,\u03c1 i) =\nRh\u03c1(DJe1 K\u001f,G,\u03c1 ) \u2295 \u03c1(DJe2 K\u001f,G,\u03c1 )i. By the induction hypothesis e1 Ds \u03c1(DJe1 K\u001f,G,\u03c1 ) and\ne2 Ds \u03c1(DJe2 K\u001f,G,\u03c1 ), and by congruence properties of strong improvement (Lemma 5.23:1)\nRhe1 \u2295 e2 i Ds \u03c1(DJe1 \u2295 e2 KR,G,\u03c1 ).\nb) e1 = n or e1 = a: We have \u03c1(DJe1 \u2295 e2 KR,G,\u03c1 ) = \u03c1(DJe2 KRhe1 \u2295\u001fi,G,\u03c1 ) and Rhe1 \u2295 e2 i Ds\n\u03c1(DJe2 KRhe1 \u2295\u001fi,G,\u03c1 ) follows from the induction hypothesis.\nc) otherwise: We have that \u03c1(DJe1 \u2295 e2 KR,G,\u03c1 ) = \u03c1(DJe1 KRh\u001f\u2295e2 i,G,\u03c1 ) and Rhe1 \u2295 e2 i Ds\n\u03c1(DJe1 KRh\u001f\u2295e2 i,G,\u03c1 ) follows from the induction hypothesis.\nA.9. R9. We have that \u03c1(DJ(\u03bbx.f ) eKR,G,\u03c1 ) = \u03c1(DJRhlet x = e in f iK\u001f,G,\u03c1 ). Evaluating\nthe input term yields: Rh(\u03bbx.f ) ei 7\u2192r Rh(\u03bbx.f ) vi 7\u2192 Rh[v/x]f i, and evaluating the input\nto the recursive call yields: Rhlet x = e in f i 7\u2192r Rhlet x = v in f i 7\u2192 Rh[v/x]f i. These\ntwo resulting terms are syntactically equivalent, and therefore cost equivalent. By Lemma\n5.21 their ancestor terms are cost equivalent, Rh(\u03bbx.f ) ei ED Rhlet x = e in f i, and cost\nequivalence implies strong improvement. By the induction hypothesis Rhlet x = e in f i Ds\n\u03c1(DJRhlet x = e in f iK\u001f,G,\u03c1 ), and therefore Rh(\u03bbx.f ) ei Ds \u03c1(DJ(\u03bbx.f ) eKR,G,\u03c1 ).\nA.10. R10. We have \u03c1(DJe e\u2032 KR,G,\u03c1 ) = \u03c1(DJeKRh\u001f e\u2032 i,G,\u03c1 ) and Rhe e\u2032 i Ds \u03c1(DJeKRh\u001f e\u2032 i,G,\u03c1 )\nfollows from the induction hypothesis.\nA.11. R11. We have that \u03c1(DJlet x = n in f KR,G,\u03c1 ) = \u03c1(DJRh[n/x]f iK\u001f,G,\u03c1 ). By the induction hypothesis Rh[n/x]f i Ds \u03c1(DJRh[n/x]f iK\u001f,G,\u03c1 ), and since Rhlet x = n in f i 7\u2192\nRh[n/x]f i it follows from Lemma 5.23:4 that Rhlet x = n in f i Ds \u03c1(DJlet x = n in f KR,G,\u03c1 ).\nA.12. R12. We have that \u03c1(DJlet x = y in f KR,G,\u03c1 ) = \u03c1(DJRh[y/x]f iK\u001f,G,\u03c1 ). By the induction hypothesis Rh[y/x]f i Ds \u03c1(DJRh[y/x]f iK\u001f,G,\u03c1 ), and since Rhlet x = y in f i ED\nRh[y/x]f i it follows that Rhlet x = y in f i Ds \u03c1(DJlet x = y in f KR,G,\u03c1 ).\nA.13. R13.\n\n\f38\n\nP. A. JONSSON\n\nA.13.1. Case: x \u2208 strict(f ). We have \u03c1(DJlet x = e in f KR,G,\u03c1 ) = \u03c1(DJRh[e/x]f iK\u001f,G,\u03c1 ).\nEvaluating the input term yields Rhlet x = e in f i 7\u2192r Rhlet x = v in f i 7\u2192 Rh[v/x]f i 7\u2192s\nEhvi, and evaluating the input to the recursive call yields: Rh[e/x]f i 7\u2192s Ehei 7\u2192r Ehvi.\nThese two resulting terms are syntactically equivalent, and therefore cost equivalent. By\nLemma 5.21 their ancestor terms are cost equivalent, Rhlet x = e in f i ED Rh[e/x]f i, and\ncost equivalence implies strong improvement. By the induction hypothesis Rh[e/x]f i Ds\n\u03c1(DJRh[e/x]f iK\u001f,G,\u03c1 ), and therefore Rhlet x = e in f i Ds \u03c1(DJlet x = e in f KR,G,\u03c1 ).\nA.13.2. Case: otherwise. We have that \u03c1(DJlet x = e in f KR,G,\u03c1 ) =\n\u03c1(let x = DJeK\u001f,G,\u03c1 in DJRhf iK\u001f,G,\u03c1 ), and the conditions of the proposition ensure that\nfv(Rhlet x = e in f i) \u2229 dom(\u03c1) = \u2205, so \u03c1(let x = DJeK\u001f,G,\u03c1 in DJRhf iK\u001f,G,\u03c1 ) =\nlet x = \u03c1(DJeK\u001f,G,\u03c1 ) in \u03c1(DJRhf iK\u001f,G,\u03c1 ). By the induction hypothesis e Ds \u03c1(DJeK\u001f,G,\u03c1 )\nand Rhf i Ds \u03c1(DJRhf iK\u001f,G,\u03c1 ). By Lemma A.3 the input is strongly improved by let x =\ne in Rhf i, and therefore Rhlet x = e in f i Ds \u03c1(DJlet x = e in f KR,G,\u03c1 ).\nA.14. R14. We have that \u03c1(DJletrec g = v in eKR,G,\u03c1 ) = \u03c1(letrec g = v in DJRheiK\u001f,G,\u03c1 ),\nand the conditions of the proposition ensure that fv(Rhletrec g = v in ei) \u2229 dom(\u03c1) = \u2205,\nso \u03c1(letrec g = v in DJRheiK\u001f,G,\u03c1 ) = letrec g = v in \u03c1(DJRheiK\u001f,G,\u03c1 ). By the induction\nhypothesis Rhei Ds \u03c1(DJRheiK\u001f,G,\u03c1 ). By Lemma A.4 the input is strongly improved by\nletrec g = v in Rhei, and therefore Rhletrec g = v in ei Ds \u03c1(DJletrec g = v in eKR,G,\u03c1 ).\nA.15. R15. We have \u03c1(DJcase x of {pi \u2192 ei }KR,G,\u03c1 ) = \u03c1(case x of {pi \u2192 DJRhei iK\u001f,G,\u03c1 }),\nand the conditions of the proposition ensure that fv(Rhcase x of {pi \u2192 ei }i) \u2229 dom(\u03c1) = \u2205,\nso \u03c1(case x of {pi \u2192 DJRhei iK\u001f,G,\u03c1 }) = case x of {pi \u2192 \u03c1(DJRhei iK\u001f,G,\u03c1 )}. By the induction\nhypothesis Rhei i Ds \u03c1(DJRhei iK\u001f,G,\u03c1 ). Using Lemma A.5 the input is strongly improved\nby case x of {pi \u2192 Rhei i}, and therefore Rhcase x of {pi \u2192 ei }i Ds\n\u03c1(DJcase x of {pi \u2192 ei }KR,G,\u03c1 ).\nA.16. R16. We have \u03c1(DJcase kj e of {pi \u2192 ei }KR,G,\u03c1 ) = \u03c1(DJRhlet xj = e in ej iK\u001f,G,\u03c1 ).\nEvaluating the input term yields Rhcase kj e of {pi \u2192 ei }i 7\u2192r Rhcase kj v of {pi \u2192 ei }i 7\u2192\nRh[v/xj ]ej i, and evaluating the input to the recursive call yields Rhlet xj = e in ej i 7\u2192r\nRhlet xj = v in ej i 7\u2192 Rh[v/xj ]ej i. These two resulting terms are syntactically equivalent,\nand therefore cost equivalent. By Lemma 5.21 their ancestor terms are cost equivalent,\nRhcase kj e of {pi \u2192 ei }i ED Rhlet xj = e in ej i, and cost equivalence implies strong improvement. According to the induction hypothesis Rhlet xj = e in ej i Ds\n\u03c1(DJRhlet xj = e in ej iK\u001f,G,\u03c1 ), and therefore Rhcase kj e of {pi \u2192 ei }i Ds\n\u03c1(DJcase kj e of {pi \u2192 ei }KR,G,\u03c1 ).\nA.17. R17. We have that \u03c1(DJcase nj of {pi \u2192 ei }K),G,\u03c1 = \u03c1(DJRhej iK\u001f,G,\u03c1 ). By the induction hypothesis Rhej i Ds \u03c1(DJRhej iK\u001f,G,\u03c1 ), and since Rhcase nj of {pi \u2192 ei }i 7\u2192 Rhej i\nit follows from Lemma 5.23:4 that Rhcase nj of {pi \u2192 ei }i Ds\n\u03c1(DJcase nj of {pi \u2192 ei }KR,G,\u03c1 ).\n\n\fPOSITIVE SUPERCOMPILATION\n\n39\n\nA.18. R18. We have that \u03c1(DJcase a of {pi \u2192 ei }KR,G,\u03c1 ) =\n\u03c1(case DJaK\u001f,G,\u03c1 of {pi \u2192 DJRhei iK\u001f,G,\u03c1 }), and the conditions of the proposition ensure that\nfv(Rhcase a of {pi \u2192 ei }i) \u2229 dom(\u03c1) = \u2205, so \u03c1(case DJaK\u001f,G,\u03c1 of {pi \u2192 DJRhei iK\u001f,G,\u03c1 }) =\ncase \u03c1(DJaK\u001f,G,\u03c1 ) of {pi \u2192 \u03c1(DJRhei iK\u001f,G,\u03c1 )}. By the induction hypothesis a Ds\n\u03c1(DJaK\u001f,G,\u03c1 ) and Rhei i Ds \u03c1(DJRhei iK\u001f,G,\u03c1 ) and by Lemma A.5 the input is strongly\nimproved by case a of {pi \u2192 Rhei i}, and therefore Rhcase a of {pi \u2192 ei }i Ds\n\u03c1(DJcase a of {pi \u2192 ei }KR,G,\u03c1 ).\nA.19. R19. We have that \u03c1(DJcase e of {pi \u2192 ei }KR,G,\u03c1 ) = \u03c1(DJeKRhcase \u001f of {pi \u2192ei }i,G,\u03c1 )\nand Rhcase e of {pi \u2192 ei }i Ds \u03c1(DJeKRhcase \u001f of {pi \u2192ei }i,G,\u03c1 ) follows from the induction hypothesis.\nA.20. R20. We have that \u03c1(DJeKR,G,\u03c1 ) = \u03c1(Rhei), and the conditions of the proposition\nensure that fv(Rhei) \u2229 dom(\u03c1) = \u2205, so \u03c1(Rhei) = Rhei This is syntactically equivalent to\nthe input, and we conclude Rhei Ds \u03c1(DJeKR,G,\u03c1 ).\n\nThis work is licensed under the Creative Commons Attribution-NoDerivs License. To view\na copy of this license, visit http://creativecommons.org/licenses/by-nd/2.0/ or send a\nletter to Creative Commons, 171 Second St, Suite 300, San Francisco, CA 94105, USA, or\nEisenacher Strasse 2, 10777 Berlin, Germany\n\n\f"}