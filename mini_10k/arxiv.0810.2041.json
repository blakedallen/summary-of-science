{"id": "http://arxiv.org/abs/0810.2041v2", "guidislink": true, "updated": "2009-12-01T10:37:47Z", "updated_parsed": [2009, 12, 1, 10, 37, 47, 1, 335, 0], "published": "2008-10-11T17:03:26Z", "published_parsed": [2008, 10, 11, 17, 3, 26, 5, 285, 0], "title": "Entanglement-Enhanced Classical Communication", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0810.3422%2C0810.0974%2C0810.0700%2C0810.0206%2C0810.0951%2C0810.2475%2C0810.0683%2C0810.1868%2C0810.3184%2C0810.5636%2C0810.2055%2C0810.1405%2C0810.4348%2C0810.0292%2C0810.4408%2C0810.3855%2C0810.2974%2C0810.1155%2C0810.0726%2C0810.3423%2C0810.0295%2C0810.2369%2C0810.3185%2C0810.4197%2C0810.0902%2C0810.4042%2C0810.4394%2C0810.0882%2C0810.4972%2C0810.1063%2C0810.0304%2C0810.5062%2C0810.2284%2C0810.4003%2C0810.1071%2C0810.5757%2C0810.1869%2C0810.1994%2C0810.2643%2C0810.3477%2C0810.1512%2C0810.0952%2C0810.1047%2C0810.5191%2C0810.3456%2C0810.4283%2C0810.3188%2C0810.2438%2C0810.2041%2C0810.4952%2C0810.0565%2C0810.0017%2C0810.0706%2C0810.4423%2C0810.3717%2C0810.0245%2C0810.3193%2C0810.0662%2C0810.2470%2C0810.4173%2C0810.3402%2C0810.0177%2C0810.4446%2C0810.1461%2C0810.1023%2C0810.0016%2C0810.4616%2C0810.2687%2C0810.2161%2C0810.5231%2C0810.0487%2C0810.3675%2C0810.4124%2C0810.5172%2C0810.4264%2C0810.2905%2C0810.1437%2C0810.5369%2C0810.4957%2C0810.5425%2C0810.1491%2C0810.2712%2C0810.4810%2C0810.2118%2C0810.3190%2C0810.5769%2C0810.3143%2C0810.4831%2C0810.1600%2C0810.2256%2C0810.3164%2C0810.3459%2C0810.3615%2C0810.0384%2C0810.4519%2C0810.0938%2C0810.0132%2C0810.0408%2C0810.2450%2C0810.0857%2C0810.3672&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Entanglement-Enhanced Classical Communication"}, "summary": "This thesis will be focused on the classical capacity of quantum channels,\none of the first areas treated by quantum information theorists. The problem is\nfairly solved since some years. Nevertheless, this work will give me a reason\nto introduce a consistent formalism of the quantum theory, as well as to review\nfundamental facts about quantum non-locality and how it can be used to enhance\ncommunication. Moreover, this reflects my dwelling in the spirit of classical\ninformation theory, and it is intended to be a starting point towards a\nthorough study of how quantum technologies can help to shape the future of\ntelecommunications.\n  Whenever it was possible, heuristic reasonings were introduced instead of\nrigorous mathematical proofs. This finds an explanation in that I am a\nself-taught neophyte in the field, and just about every time I came across a\nnew concept, physical arguments were always more compelling to me than just\nmaths. The technical content of the thesis is twofold. On one hand, a quadratic\nclassification based on optimization programs that I devised for distinguishing\nentangled states is presented in Chapter 4. In second place, a less difficult\nyet I hope equally interesting technical part consists of versions of some\nproofs throughout the text.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0810.3422%2C0810.0974%2C0810.0700%2C0810.0206%2C0810.0951%2C0810.2475%2C0810.0683%2C0810.1868%2C0810.3184%2C0810.5636%2C0810.2055%2C0810.1405%2C0810.4348%2C0810.0292%2C0810.4408%2C0810.3855%2C0810.2974%2C0810.1155%2C0810.0726%2C0810.3423%2C0810.0295%2C0810.2369%2C0810.3185%2C0810.4197%2C0810.0902%2C0810.4042%2C0810.4394%2C0810.0882%2C0810.4972%2C0810.1063%2C0810.0304%2C0810.5062%2C0810.2284%2C0810.4003%2C0810.1071%2C0810.5757%2C0810.1869%2C0810.1994%2C0810.2643%2C0810.3477%2C0810.1512%2C0810.0952%2C0810.1047%2C0810.5191%2C0810.3456%2C0810.4283%2C0810.3188%2C0810.2438%2C0810.2041%2C0810.4952%2C0810.0565%2C0810.0017%2C0810.0706%2C0810.4423%2C0810.3717%2C0810.0245%2C0810.3193%2C0810.0662%2C0810.2470%2C0810.4173%2C0810.3402%2C0810.0177%2C0810.4446%2C0810.1461%2C0810.1023%2C0810.0016%2C0810.4616%2C0810.2687%2C0810.2161%2C0810.5231%2C0810.0487%2C0810.3675%2C0810.4124%2C0810.5172%2C0810.4264%2C0810.2905%2C0810.1437%2C0810.5369%2C0810.4957%2C0810.5425%2C0810.1491%2C0810.2712%2C0810.4810%2C0810.2118%2C0810.3190%2C0810.5769%2C0810.3143%2C0810.4831%2C0810.1600%2C0810.2256%2C0810.3164%2C0810.3459%2C0810.3615%2C0810.0384%2C0810.4519%2C0810.0938%2C0810.0132%2C0810.0408%2C0810.2450%2C0810.0857%2C0810.3672&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This thesis will be focused on the classical capacity of quantum channels,\none of the first areas treated by quantum information theorists. The problem is\nfairly solved since some years. Nevertheless, this work will give me a reason\nto introduce a consistent formalism of the quantum theory, as well as to review\nfundamental facts about quantum non-locality and how it can be used to enhance\ncommunication. Moreover, this reflects my dwelling in the spirit of classical\ninformation theory, and it is intended to be a starting point towards a\nthorough study of how quantum technologies can help to shape the future of\ntelecommunications.\n  Whenever it was possible, heuristic reasonings were introduced instead of\nrigorous mathematical proofs. This finds an explanation in that I am a\nself-taught neophyte in the field, and just about every time I came across a\nnew concept, physical arguments were always more compelling to me than just\nmaths. The technical content of the thesis is twofold. On one hand, a quadratic\nclassification based on optimization programs that I devised for distinguishing\nentangled states is presented in Chapter 4. In second place, a less difficult\nyet I hope equally interesting technical part consists of versions of some\nproofs throughout the text."}, "authors": ["David A. Herrera-Mart\u00ed"], "author_detail": {"name": "David A. Herrera-Mart\u00ed"}, "author": "David A. Herrera-Mart\u00ed", "arxiv_comment": "M.Sc. Thesis, 103 pages", "links": [{"href": "http://arxiv.org/abs/0810.2041v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0810.2041v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0810.2041v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0810.2041v2", "journal_reference": null, "doi": null, "fulltext": "arXiv:0810.2041v2 [quant-ph] 1 Dec 2009\n\nDepartment of Computer Science\nSchool of Engineering\nUniversity of Valencia\n\nEntanglement-Enhanced Classical\nCommunication\nDavid A. Herrera Mart\u0131\u0301\nAdvisors\n\nDr. Baltasar Beferull-Lozano\n\nThesis submitted for the Degree of Master of Science (M.Sc.)\nAugust 2008\n\n\fA mi madre.\nEn memoria de mi padre.\n\ni\n\n\fii\n\n\fContents\nI Brief Review of Information Theory and Quantum\nMechanics\n1\n1 A Mathematical Model for Communication\n\n3\n\n1.1\n\nWhat is Information? . . . . . . . . . . . . . . . . . . . .\n\n3\n\n1.2\n\nSimplest Scenario for Communication . . . . . . . . . . .\n\n8\n\n1.3\n\nAsymptotic Equipartition Property . . . . . . . . . . . . . 10\n\n1.4\n\nShannon's Source and Channel Coding Theorems . . . . . 13\n1.4.1\n\nSource Coding . . . . . . . . . . . . . . . . . . . . 14\n\n1.4.2\n\nChannel Coding . . . . . . . . . . . . . . . . . . . 17\n\n2 Quantum Mechanics as a Statistical Theory\n2.1\n\n21\n\nQuantum Formalism . . . . . . . . . . . . . . . . . . . . . 22\n2.1.1\n\nSet of States is Convex . . . . . . . . . . . . . . . . 23\n\n2.1.2\n\nSet of Measurements is Also Convex . . . . . . . . 25\n\n2.2\n\nVon Neumann's Entropy\n\n. . . . . . . . . . . . . . . . . . 27\n\n2.3\n\nClassical Information and Quantum Information are Not\nthe Same . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n2.3.1\n\nClassical Information through Decoherence . . . . 30\n\n2.3.2\n\nNo-Cloning Theorem . . . . . . . . . . . . . . . . . 33\n\n2.3.3\n\nThe Holevo's Bound . . . . . . . . . . . . . . . . . 34\niii\n\n\fCONTENTS\n2.4\n\nExperiments as Information Transfer . . . . . . . . . . . . 35\n\nII Quantum Entanglement: A New Resource for Communication\n37\n\n3 Quantum Non-locality\n3.1\n\n39\n\nEPR Paradox . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.1.1\n\nMarginal Measurements . . . . . . . . . . . . . . . 42\n\n3.2\n\nQuantum Correlations and Bell's Inequalities . . . . . . . 43\n\n3.3\n\nInformation-Theoretic Considerations . . . . . . . . . . . 46\n\n3.4\n\nUnexpected Applications . . . . . . . . . . . . . . . . . . . 47\n3.4.1\n\nQuantum Teleportation . . . . . . . . . . . . . . . 47\n\n3.4.2\n\nQuantum Superdense Coding . . . . . . . . . . . . 49\n\n4 Entanglement Theory\n4.1\n\nPure States . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.1.1\n\n4.2\n\n4.3\n\n51\n\nEntanglement Distillation . . . . . . . . . . . . . . 53\n\nMixed States . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.1\n\nDetection . . . . . . . . . . . . . . . . . . . . . . . 55\n\n4.2.2\n\nQuantification\n\n. . . . . . . . . . . . . . . . . . . . 57\n\nGeometric Insights into Entanglement . . . . . . . . . . . 58\n4.3.1\n\nDuality between Detection and Quantification . . . 59\n\n4.3.2\n\nEllipsoidal Classification . . . . . . . . . . . . . . . 61\n\n4.3.3\n\nResults for 2 \u00d7 2 and 2 \u00d7 3 Systems . . . . . . . . 62\n\n4.3.4\n\nPseudo-Entanglement Witnesses . . . . . . . . . . 64\n\n4.3.5\n\nBound Entanglement Detection . . . . . . . . . . . 65\niv\n\n\fCONTENTS\n\nIII\n\nQuantum Information Theory\n\n5 Classical Information over Quantum Channels\n5.1\n\n69\n71\n\nQuantum Asymptotic Equipartition Property . . . . . . . 72\n5.1.1\n\nEntanglement Distillation . . . . . . . . . . . . . . 73\n\n5.2\n\nQuantum Channels . . . . . . . . . . . . . . . . . . . . . . 74\n\n5.3\n\nClassical Capacity of a Quantum Channel . . . . . . . . . 76\n\n5.4\n\nEntanglement-Enhanced Classical Communication . . . . 84\n\nv\n\n\fCONTENTS\n\nvi\n\n\fForeword\nThis work was born from my desire to unify my two scientific backgrounds: physics and telecommunications. The study of Quantum Information Theory has given me the opportunity to complement these\ntwo subjects. The quantum theory predicts significant changes in our\nconcept of computation and information. The conceptual jump from\nmathematical models to physical reality has outstanding consequences,\nsuch as new paradigm of complexity classes, in the case of computation, which allows for solving problems believed to be in NP, such as the\nFactoring Problem or Discrete Logarithm Problem, in polynomial time.\nThis thesis will be focused on the classical capacity of quantum channels, one of the first areas treated by quantum information theorists. The\nproblem is fairly solved since some years. Nevertheless, this work will\ngive me a reason to introduce a consistent formalism of the quantum theory, as well as to review fundamental facts about quantum non-locality\nand how it can be used to enhance communication. Moreover, this reflects my dwelling in the spirit of classical information theory, and it is\nintended to be a starting point towards a thorough study of how quantum technologies can help to shape the future of telecommunications.\nWhenever it was possible, heuristic reasonings were introduced instead of rigorous mathematical proofs. This finds an explanation in that\nI am a self-taught neophyte in the field, and just about every time I came\nacross a new concept, physical arguments were always more compelling\nto me than just maths. The technical content of the thesis is twofold.\nOn one hand, a quadratic classification based on optimization programs\nthat I devised for distinguishing entangled states is presented in Chapter\n4. In second place, a less difficult yet I hope equally interesting technical\npart consists of versions of some proofs throughout the text.\n\nvii\n\n\fCONTENTS\nThanks are due to Teresa and to my family for their encouragement\nand support. All my love is for them. Almost all figures were possible thanks to the skills of Pedro Jim\u00e9nez. I also acknowledge pleasant\nconversations and support from all components of the GSIC group in\nValencia.\nMy final words are for my father, suddenly deceased in late may of\nthis year. He taught me the need for curiosity and joy. I owe him most\nof myself.\n\nValencia, August 2008\n\nviii\n\n\fPart I\n\nBrief Review of\nInformation Theory and\nQuantum Mechanics\n\n1\n\n\f\fChapter 1\n\nA Mathematical Model for\nCommunication\nInformation Theory is mainly concerned about two issues. First one is\nto establish theoretical bounds to the achievable rates at which information can be compressed from a source and conveyed through a channel.\nTo this goal, achievability and converse theorems for different communication scenarios must be found. However, it is important to realize that\nthese theorems are regardless of the complexity and delay of the codes\nthat should attain the bounds. In second place, Information Theory is\naimed at finding practical coding schemes that perform close to theoretical limits. In this dissertation we will study exclusively first one of\nthese two problems, to which Quantum Mechanics has endowed with a\neven richer variety of problems.\nMost of this chapter is based on the texts [1][2][3]. Since this chapter\nis a review of basic concepts, results about stochastic processes and\ntypicality will not be proved.\n\n1.1\n\nWhat is Information?\n\nBefore starting maybe one should face the question \"what is information? \". How should this ubiquitous and quasi-philosophical process be\ndescribed mathematically? It seems natural to define information in\n3\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\nterms of probability theory, for it is the mathematical framework that\nformally incorporates the concept of uncertainty about the future.\nIn the first half of the past century, several meaningful definitions\narose, such as Fisher's information (which is a measure of the curvature\nof the probability distribution) or Hartley's function (the logarithm of\nthe source's alphabet size), in the context of statistics and engineering1 .\nIn 1948, guided by some reasonable assumptions, Shannon came out\nwith entropy, H, as a measure for information.Among others, his requirements were that:\n1. H(p) be continuous on p (1T p = 1)\n2. H(p) should be, for pi = n1 a monotonic increasing function of n.\nThis is equivalent to a normalization.\n3. If a choice is broken down into successive choices, the original\nentropy should be the weighted sum of individual values of the\nresulting entropies.\nIt can be shown that the only function, up to a proportionality constant, satisfying these assumptions is2 :\n\nH(p) = \u2212\n\nn\nX\n\npi log pi\n\n(1.1)\n\ni=1\n\nIn fact, Shannon's entropy is the epigone of deeper concepts such as\nthe relative entropy (also known as Kullback-Leibler distance), or mutual\ninformation. The relative entropy of two probability distributions is\ngiven by:\n\nD(p||q) =\n\nn\nX\ni=1\n\npi log\n\npi\nqi\n\n(1.2)\n\nwith 1T p = 1T q = 1, that is, p and q belong to the discrete probability simplex of dimension n, Pn . Although in general D(p||q) 6= D(q||p),\n1\n\nMore general and deeper concepts such as R\u00e9nyi's entropy or Kolmogorov's algorithmic complexity and their far reaching implications are not discussed here for\nthe sake of conciseness\n2\nThroughout this thesis, logarithms will be taken in base 2.\n\n4\n\n\f1.1 What is Information?\nthis quantity can be thought of as \"distance\" between probability distributions.\nMutual information is the amount of information that a random\nvariable contains about another random variable1 . Consider X taking\nvalues in X and Y taking values in Y, and let I(X; Y ) denote I(pX ; qY ),\nthen their mutual information is:\n\nI(X; Y ) =\n\nn X\nm\nX\n\nXY\nPi,j\n\nlog\n\ni=1 j=1\n\nXY\nPi,j\nY\npX\ni qj\n\n(1.3)\n\nwhere PXY is the joint probability distribution of both random variables. If they are independent, mutual information vanishes, which\nmeans that knowing the realization of one random variable does not\ngive any clue about the other one.\nIn turn, Shannon's entropy is a special case of mutual information,\nbeing the information that a random variable contains about itself,\nH(pX ) = I(X; X). It will suffice to prove some properties of the relative entropy, because they can be straightforwardly extended to mutual\ninformation and entropy.\n\nTheorem 1 [Nonnegativity of relative entropy]The relative entropy is\npositive semidefinite, D(p||q) \u2265 0\n\nProof 1 Let A = Supp(p) be the support of p. Then\n\nqi\npi\ni\u2208A\nX qi\n\u2265 \u2212 log\npi\npi\ni\u2208A\nX\n= \u2212 log\nqi\n\nD(p||q) = \u2212\n\nX\n\npi log\n\ni\u2208A\n1\n\nNote that throughout this text, we will often interchange random variables for\ntheir induced probability distributions, and viceversa\n\n5\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\n\n\u2265 \u2212 log\n\nn\nX\n\nqi\n\ni=1\n\n= \u2212 log 1\n= 0\n\n(1.4)\n\nthe first inequality is a consequence of Jensen's inequality for convex functions E[f (pX )] \u2265 f (E[pX ]). The second inequality comes from\nextending the range of the sum.\nTheorem 2 [Convexity of relative entropy]The relative entropy is a\nconvex function of the probability distributions p and q\nProof 2 By the log sum inequality\n[1], we have that:\n\n(\u03bbpi +(1\u2212\u03bb)p0i ) log\n\nPn\n\nai\ni=1 ai log bi \u2265 (\n\nPn\n\ni=1 ai ) log\n\nPn\nai\nPi=1\nn\ni=1 bi\n\n\u03bbpi\n(1 \u2212 \u03bb)p0i\n\u03bbpi + (1 \u2212 \u03bb)p0i\n\u2264 \u03bbpi log\n+(1\u2212\u03bb)p0i log\n0\n\u03bbqi + (1 \u2212 \u03bb)qi\n\u03bbpi\n(1 \u2212 \u03bb)qi0\n(1.5)\n\nwith \u03bb \u2208 [0, 1]. Summing over the index we get:\n\nD(p + (1 \u2212 \u03bb)p0 ||\u03bbq + (1 \u2212 \u03bb)q0 ) \u2264 \u03bbD(p||q) + (1 \u2212 \u03bb)D(p0 ||q0 ) (1.6)\nCorollary 1 [Concavity of entropy]Entropy is a concave function of pX\n1\nProof 3 Consider the uniform distribution uX = kXk\n(1, 1, ..., 1). The\nX\nX\nrelative entropy of distribution p with respect to u is:\n\nD(p||u) =\n\nn\nX\ni=1\n\npi log pi \u2212\n\nn\nX\n\npi log ui = \u2212H(pX ) + log kXk\n\ni=1\n\n6\n\n\f1.1 What is Information?\nso we get:\nH(pX ) = log kXk \u2212 D(p||u)\n\n(1.7)\n\nIt is easy to see from this corollary that entropy is upper bounded\nby the logarithm of the cardinality of the alphabet H(pX ) \u2264 log kXk.\nA related important quantity is the conditional entropy of a random\nvariable Y given that the instantiation of X is known, i.e. the residual\nuncertainty about Y once we learn about X.\n\nH(Y |X = xi ) = \u2212\n\nm\nX\n\nY |X\n\nqj\n\nY |X\n\nlog qj\n\n(1.8)\n\nj=1\n\nAveraging over all possible outcomes of X:\n\nH(Y |X) =\n\nn\nX\n\npX\ni H(Y |X = xi ) = \u2212\n\ni=1\n\nn X\nm\nX\n\nY |X\n\nXY\nPi,j\nlog qj\n\n(1.9)\n\ni=1 j=1\n\nClearly there is a reduction in the uncertainty only if there exist a\nnon-factorizable joint probability distribution. In other words, if the two\nrandom variables are independent, then H(Y |X) = H(Y ). By symmetry\narguments one can easily find the relations:\nH(X) \u2212 H(X|Y ) = I(X; Y ) = H(Y ) \u2212 H(Y |X)\n\nI(X; Y ) \u2264 min{H(X), H(Y )}\n\n(1.10)\n\n(1.11)\n\nOne useful property which makes use of the conditional entropy is\nthe chain rule for entropy. Let X, Y and Z be three random variables,\nthen their joint entropy can be written:\nH(X, Y, Z) = H(X) + H(Y |X) + H(Z|X, Y )\n\n(1.12)\n\nwhich is easily generalizable to any number of random variables.\n7\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\nThe convexity of relative entropy has an important consequence for\nchannel coding, as we will see:\nTheorem 3 [Partial concavity of mutual information]For fixed pY |X ,\nthe mutual information is a concave function of pX\nProof 4 From Bayes' rule:\n\nqY =\n\npY |X X\np\nqX|Y\n\n(1.13)\n\nqY is a linear function of pX , thus H(pY ) is a concave function of\npX . The mutual information can be expressed as:\n\nI(X; Y ) = H(pY ) \u2212\n\nn X\nm\nX\ni\n\nY |X X\npi\n\nqj\n\nY |X\n\nlog qj\n\n(1.14)\n\nj\n\nThe second term is a linear function of pX , hence, the whole expression is concave on pX .\n\n1.2\n\nSimplest Scenario for Communication\n\nIn the simplest case of information transfer, at least three stages can\nbe identified: the source of information (or transmitter), the channel\nover which messages are sent, and the sink (or receiver). The source is\nmodeled as a probability space (\u03a9, A\u03a9 , \u03bc). Typically, every outcome of\nthe source will have to be processed in order to build a suitable message\nwhich can be sent over the channel. This is mathematically represented\nby a measurable function from the source's emitted messages to a given\nalphabet (usually a binary alphabet), and is practically called coding.\nConversely, in order to transmit the original information to the sink,\nsimilar functions ought to be defined on the alphabet of the received\nmessages to the original alphabet (on the assumption that transmitter\nand receiver share the same language). This involves statistical estimation and is called decoding.\n8\n\n\f1.2 Simplest Scenario for Communication\nThe functions fE , gE , fD and gD are measurable functions, so (W, AW )\nand (\u00db, A\u00db ) can be viewed as probabilizable spaces with probabilities\ninduced by p(W = wi ) = p(fE\u22121 (wi ) = u), and so on. This notion of inherited probability is fundamental because relative entropy is a function\ndefined on probability simplices.\n\nFigure 1.1: Simplest Scenario for Communication: In the sender-receiver\nscheme, the messages randomly emitted by the source are first compressed at the source encoder and then fed to the channel encoder.\nChannel encoder will map them to codewords resilient against channel\nnoise, so that the original compressed message can be recovered reliably\nat the channel decoder. Source decoder will decompress the messages\nand deliver them to the receiver, or information sink\nRemarkably, the process of coding and decoding is absolutely deterministic. Choice is introduced at two levels, of different nature. First\none is in the source itself, where the sample space could be whatever,\ni.e. all the thoughts of a person talking on the phone. A random variable U, defined on \u03a9 and taking values in U (kUk = n) represents the\nphysical resulting messages the that are emitted by the source, i.e. a\nseries of phonemes which are a function of the thoughts of the person\nwho talks. At a second stage, uncertainty is introduced in the channel,\nand is related to the noise (fading, interference, outages...) that every\nphysical channel induces in an information carrier. In fact, a channel\nis represented by the tuple (X, TY |X , Y), where X and Y are the input\nand output alphabets, respectively, and TY |X 1 is a stochastic transition\n1\n\nA generalized transition matrix would be of the form TY m |X m\u22121 Y m\u22121 . Here will\nrefer only to discrete memoryless channels without feedback.\n\n9\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\nmatrix such that qY = TY |X pX .\nTwo main questions arise in this context, and that is all Information\nTheory is concerned about:\nChannel Capacity What is the maximum rate that can be achieved\nin sending information over a channel? This question is practically\napproached in the design of channel encoders-decoders. Later we\nshall see that\nR \u2264 max I(X; Y )\npX\n\nError-correcting codes are mainly devoted to maximize this rate.\nRate-Distortion Theory What is the minimum rate at which one\nsource can be compressed (that is, eliminate redundant parts of\nthe source's outputs) while keeping received messages below a distortion threshold D?\nR\u2265\n\nmin\n\nI(\u00db ; U )\n\np\u00db ,U :d(\u00db |U )\u2264D\n\nIn the simplified case where the channel is noiseless, or whenever it\nis possible to estimate perfectly \u00db (or just assume that d(\u00db , U ) =\n0), Rate-Distortion reduces to Lossless Data Compression:\nI(\u00db ; U ) = I(U ; U ) = H(U )\nand the inequality becomes R \u2265 H(pU )\nThere exist a nice duality between these two problems that can be\nappreciated when they are expressed as optimization programs [4].\n\n1.3\n\nAsymptotic Equipartition Property\n\nTypically sources will emit more than one output. Thus, we need to\ncharacterize them as stochastic processes rather than as just random\nvariables. Consider a source described by (\u03a9, A\u03a9 , \u03bc) and T : \u03a9 \u2192 \u03a9\nwhich plays the role of a time shift in the sample space. This is a dynamical system and one can derive a stochastic process from it Uj (T j \u03c9) =\nuj , w \u2208 A\u03a9 .\n10\n\n\f1.3 Asymptotic Equipartition Property\n\nFigure 1.2: All information theory is concerned about: The rate of information transfer upper-bounded by the maximum capacity of inference\nof the receiver, which is related to the mutual information. On the lower\npart of the scheme we see that no rate is possible below those allowed\nfor a given distortion threshold. For a discrete noiseless channel, the\ndistortion can be taken to be zero and the lower bound reduces to the\nentropy of the source.\nIf for all w \u2208 A\u03a9 , we have that \u03bc(T j \u03c9) = \u03bc(\u03c9) = 1 or 0, then the\nsource is ergodic and stationary, and Birkhoff's Theorem holds [3]:\nm\n\n1 X\nP=1\nUj \u2212\u2192 E[U1 ] =\nm\u2192\u221e m\n\nZ\n\nlim\n\nU1 d\u03bc\n\n(1.15)\n\nj=1\n\nwhere P = 1 denotes convergence with probability\nnow\nPm1. If we\nj\u22121\n1\nU\n|U\nm\nj\nconsider the sequence {Uj }j=1 and regard log U j = \u2212 j=1 log p\np\n\nas a random variable itself12 , function of U j , then:\nm\n\nlim \u2212\n\nm\u2192\u221e\n\n1 X\nj\u22121 P=1\nj\u22121\nlog pUj |U\n\u2212\u2192 E[\u2212 log pUj |U ] = H(U)\nm\n\n(1.16)\n\nj=1\n\nH(U) is the entropy rate of the stochastic process. It can be inter1\n\npX in boldface denotes a probability distribution, while pX will denote the probability of a particular occurrence of X, p(X = x)\n2 j\nU , with upper index, is a shorthand for the sequence U1 U2 ...Uj\n\n11\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\npreted as the entropy of the last random variable in the sequence given\nall its past. Finally we get:\nlim \u2212\n\nm\u2192\u221e\n\n1\nm P=1\nlog pU \u2212\u2192 H(U)\nm\n\n(1.17)\n\nSince convergence with probability 1 implies convergence in probability, it is possible to write:\nP(| \u2212\n\n1\nm\nm\u2192\u221e\nlog pU \u2212 H(U)| \u2264 \u000f) \u2212\u2192 1, \u2200\u000f\nm\n\n(1.18)\n\nwhence we obtain:\n2\u2212m(H(U)\u2212\u000f) \u2264 pU\n\nm\n\n\u2264 2\u2212m(H(U)+\u000f)\n\n(1.19)\n\nHence, for a fixed probability pU , the most likely sequences have\nan empirical entropy arbitrary close to the true entropy. Practically all\nprobability mass will be localized at a proper subset of the set of all\npossible output sequences. This characteristic of the sequences, direct\nconsequence of ergodicity, is called Asymptotic Equipartition Property\nbecause as m (the length of the sequence) grows, most likely sequences\ntend to be grouped in a proper subset called the Typical Set T\u000fm , whose\ncardinality is 2mH(U)\u2212\u000f \u2264 ||T|| \u2264 2mH(U)+\u000f , and gather almost all probability (P(T) = 1\u2212\u000f), whereas unlikely sequences tend to have a vanishing\nprobability. Also, as m tends to infinity, all typical sequences become\nequally probable1 .\nFor simplicity, we will restrict ourselves to stationary, independent,\nidentically distributed (i.i.d.) processes, in which case the entropy rate\ntakes the form:\nm\n\nH(pU )\n= H(pU )\nm\u2192\u221e\nm\n\nH(U) = lim\n\n(1.20)\n\nwhich can be interpreted as the entropy per symbol of m random\nvariables. Finally we come to a weak version of the Asymptotic Equipartition Property. :\n1\n\nThis is in analogy with ensembles of statistical mechanics, where all points in\nphase space are assumed to be equally likely\n\n12\n\n\f1.4 Shannon's Source and Channel Coding Theorems\n\nlim \u2212\n\nm\u2192\u221e\n\n1\nm i.p.\nlog pU \u2212\u2192 H(pU )\nm\n\n(1.21)\n\nnow convergence is in probability.\nSimilarly, it is possible to define jointly typical sequences (X m , Y m )\nwith respect to a joint probability distribution pX,Y as the sequences for\nwhich:\n\nlim \u2212\n\n1\nm i.p.\nlog pX \u2212\u2192 H(pX )\nm\n\n(1.22)\n\nlim \u2212\n\n1\nm i.p.\nlog pY \u2212\u2192 H(pY )\nm\n\n(1.23)\n\n1\nm m i.p.\nlog pX Y \u2212\u2192 H(pXY )\nm\n\n(1.24)\n\nm\u2192\u221e\n\nm\u2192\u221e\n\nlim \u2212\n\nm\u2192\u221e\n\nAs before, in the asymptotical limit, only typical pairs will take place:\nP(| \u2212\n\n1.4\n\n1\nm m\nm\u2192\u221e\nlog pX Y \u2212 H(pXY )| \u2264 \u000f) \u2212\u2192 1, \u2200\u000f\nm\n\n(1.25)\n\nShannon's Source and Channel Coding Theorems\n\nIn his foundational paper [5], Shannon laid the basements of Information\nTheory. He stated both problems above exposed (source and channel\ncoding) and first offered a solution. For this, he used the concept of\nrandom coding, which is not to be understood as random map between\nalphabets, but rather as a proof of existence of at least one coding scheme\nthat attains the bound. However, his derivations were based on (weak)\ntypicality and were only asymptotically optimal, therefore being of little\ninterest until practical codes were found which performed close to the\nlimit.\n13\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\n\n1.4.1\n\nSource Coding\n\nConsider a source that generates a random sequence of outputs {Uj }m\nj=1 ,\nand an encoding function:\nfE : U m \u2192 W (um )\nwith W \u2208 W = {1, 2, ..., 2mR }. Here message W is indexed by the\ninstantiation of the sequence um . The cardinality of W will be kWk =\n2mR , where R is the rate of the code. Most commonly W \u2286 {0, 1}\u2217 and\nW will be a sequence of bits. W is the codification of the source.\nIn order to quantify the fidelity of the code one should follow one of\nfollowing criteria:\n\u2022 d(\u00db , U ) \u2264 \u000f, \u2200\u000f\n\u2022 P(\u00db = U ) \u2265 1 \u2212 \u000f, \u2200\u000f\nWe will use the second one, which is best suited for derivations based\non weak typicality.\n\nFigure 1.3: Source Coding: The sequences emitted by the source will\ntypically have a redundant part, due to possible correlations between\nsymbols or strings. These redundant parts don't contain much information and it is desirable to get rid of them so that no so many channel uses\nare required to transmit the source. This redundancy is quantified by the\nentropy rate of the source, since sequences of length m are mapped (on\naverage) to sequences of length mH(U ), which will be typically shorter.\n\n14\n\n\f1.4 Shannon's Source and Channel Coding Theorems\nTheorem 4 [Lossless Source Coding]An i.i.d. source {Uj }m\nj=1 can be\nreliably compressed (with vanishing probability of error) at a rate R if\nand only if R \u2265 H(pU ).\nThe proof is based on asymptotical expressions, so it will be optimal\nin the limit n \u2192 \u221e\nProof 5 (\u21d2Proof of Achievability) An error occurs whenever one\nof the following events happen:\n\u2022 The sequence is not typical: E0 = {U m 6\u2208 T\u000fm }\n\u2022 A codeword is indexed by more than one typical sequence1 : E1 =\n\u2203u0m 6= U m , u0m \u2208 T\u000fm : W (U m ) = W (u0m )\nUsing the independence bound, the error probability is:\n\nPe \u2264 P {E0 } + P {E1 }\nX\nX\nm\n\u2264 \u000f+\npU P {W (U m ) = W (u0m )}\nU m \u2208T\u000fm u0m \u2208T\u000fm\nu0m 6=U m\n\n\u2264 \u000f+\n\nX X\n\nm\n\npU 2\u2212mR\n\nU m u0m \u2208T\u000fm\n\n= \u000f+\n\nX\n\n2\u2212mR\n\nu0m \u2208T\u000fm\n\n= \u000f+\n\nX\n\npU\n\nm\n\nUm\n\nX\n\n2\u2212mR\n\nu0m \u2208T\u000fm\nU )+\u000f)\n\n\u2264 \u000f + 2m(H(p\n\n2\u2212mR\n\nSecond inequality is obtained using the independence bound and averaging over all typical sequences. Also, P {E0 } \u2192 0 as m grows. Third\n1\n\nThis is a consequence of random coding: in choosing a code at random we risk\nof selecting a bad code.\n\n15\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\ninequality is obtained by enlarging the range of the sums. Note that,\nfor equiprobable codewords, the likelihood that two are indexed by the\nsame sequence is 2mR . Last inequality follows from typicality arguments.\nThus, in the asymptotic limit where m \u2192 \u221e, if:\n\nR \u2265 H(pU ) + \u000f\nthe probability of error vanishes.\n\nProof 6 (\u21d0Weak Converse) For codes with asymptotically vanishing\nprobability of error, the rate must necessarily satisfy R \u2265 H(pU ). To this\naim, we will make use of Fano's inequality, which relates the probability\nof error to the conditional entropy of a sequence U m given its associated\ncodeword X. It can be easily derived, so we don't prove it here:\n\nH(U m |W ) \u2264 mPe log kUk + 1 = m\u000fm\nwhere \u000fm = Pe log kUk +\n\n1\nn\n\n(1.26)\n\n\u2192 0 as m grows.\n\nmR \u2265 H(pW )\n= I(U m ; W ) + H(W |U m )\n= I(U m ; W )\nm\n\n= H(pU ) \u2212 H(U m |W )\n\u2265 mH(pU ) \u2212 m\u000fm\nFirst inequality comes from the upper bound of entropy. Since knowing U m eliminates the uncertainty about W , we have the second equality.\nIn the second inequality we have used Fanno's inequality. The source is\nm\nmodeled by an i.i.d process so H(pU ) = mH(pU ).\n16\n\n\f1.4 Shannon's Source and Channel Coding Theorems\n\n1.4.2\n\nChannel Coding\n\nA channel is characterized by a the tuple (X, TY |X , Y), where is TY |X a\nmap between the probability simplices corresponding to the input and\noutput alphabet. While source coding is aimed at eliminating redundant\nparts of source's output (for this reason named data compression), the\ngoal of channel coding is to introduce some redundancy in a controlled\nway, such that it helps to fight the errors induced by the channel, and\nis suitably called error-correction.\nLet gE be a channel encoding function:\n\ngm : W \u2192 X m (w)\n\nhere W \u2208 W = {1, 2, ..., 2mR }. Each codeword X m is indexed by a\nmessage as before, and usually Xm \u2286 {0, 1}\u2217 .\nThe capacity of a discrete memoryless channel without feedback is\ndefined:\n\nC = max I(X; Y )\npX\n\n(1.27)\n\nand it is an upper bound on the attainable rates at which communication can take place.\n\nTheorem 5 [Channel Coding] A channel (X, TY |X , Y) can be used to\ntransmit information reliably if and only if R \u2264 C\n\nProof 7 (\u21d2Proof of Achievability) The probability of error, averaged over all possible codes C, is:\n17\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\n\nFigure 1.4: Channel Coding: At this stage, the compressed sequences\nare steered into larger sequences by means of introducing redundancy.\nThe point is that, whereas the redundancy of the source's outputs was of\nlittle use, the overhead introduced by the channel encoder can be used\nto recover the original message even if it is corrupted by noise (but not\ntoo much). The mapping that the channel encoder performs receives\nthe name of error-correcting code.\n\nPe =\n\nX\n\np(C)Pe (C)\n\nC\n\n=\n\nX\n\n\u2212mR\n\np(C)2\n\nC\n\n=\n\nX\n\nmR\n2X\n\n\u03bbw (C)\n\nw=1\n\np(C)\u03bb1 (C)\n\nC\n\n(1.28)\nHere \u03bbw = P {\u0174 6= w|W = w} is the conditional probability of error\ngiven that message w was sent. A random choice of code C symmetrizes\nthe probabilities. Thus we will only need to consider the error probability\nfor one codeword. Consider the event:\nEw = {(X m (w), Y m ) \u2208 T\u000fm }\nthat is, both sequences are jointly typical. There are about 2mH(X,Y )\nsuch pairs of sequences. The probability of error can then be expressed\n18\n\n\f1.4 Shannon's Source and Channel Coding Theorems\nas:\n\nPe = P {\u1ebc1 \u222a\n\nmR\n2[\n\nEi }\n\ni=2\n\n\u2264 \u000f+\n\nmR\n2X\n\nP {Ei }\n\ni=2\n\n= \u000f+\n\nmR\n2X\n\n2mH(X|Y )\u00b1\u000f\n2mH(X)\u00b1\u000f\n\ni=2\nmR\n\n\u2264 \u000f + (2\n\n\u2212 1)2m(H(X|Y )\u2212H(X)\u22122\u000f)\n\n\u2264 \u000f + 2m(R\u2212I(X;Y )\u22122\u000f)\n(1.29)\n\u1ebc1 is the complementary event of E1 , and its probability vanishes\nas m grows. The second equality is obtained from joint typicality arguments: For a given output sequence Y m , there are about 2mH(X|Y ) jointly\ntypical input sequences X m . Since there are about 2mH(X) codewords,\nthe probability that two different codewords are jointly typical with a received sequence is 2\u2212m(I(X;Y )\u00b12\u000f . Thus, the error probability will tend to\nzero as long as R < I(X; Y ) + 2\u000f.\nProof 8 (\u21d0Weak Converse) Once again, we will make use of Fano's\ninequality (see 1.26), but now the roles are somewhat interchanged:\nH(W |Y m ) \u2264 mPe R + 1 = m\u000fm\nAssuming that the messages W are equiprobable:\n\nmR = H(pW )\n= I(W ; Y m ) + H(W |Y m )\n\u2264 I(X m ; Y m ) + H(W |Y m )\n19\n\n(1.30)\n\n\f1. A MATHEMATICAL MODEL FOR COMMUNICATION\n\n\u2264 I(X m ; Y m ) + m\u000fm\nm\n\n= H(pY ) \u2212 H(Y m |X m ) + m\u000fm\nm\nX\n\u2264\n[H(pYi ) \u2212 H(Yi |Xi )] + m\u000fm\ni=1\n\n=\n\nm\nX\n\nI(Xi ; Yi ) + m\u000fm\n\ni=1\n\n\u2264 mC + m\u000fm\nThe first inequality comes from the fact that the information contained in Y m about W should be less or equal to the information that\nY m contains about X m since X m is a function of W . Second inequality\ncomes from Fanno's inequality. Third one comes from the independence\nbound. The fourth one comes from the definition of capacity (1.27), as\nthe maximum attainable mutual information. So as m grows, the probability of error goes to zero, m\u000fm \u2192 0, and then we have that R \u2264 C\n\n20\n\n\fChapter 2\n\nQuantum Mechanics as a\nStatistical Theory\nPhysical Theories deal with observable features of Nature. For a theory\nto be accepted, it must be capable of predicting the outcomes of experiments and phenomena within its logical framework. Otherwise they are\nobliged to dwell the realm of mathematical games. This implies that\nany theory must account for measurements, that is, besides describing\nNature, it must describe how we obtain knowledge from Nature. Since\nscientific theories rely on evidence for justification, this should be done\non a statistical basis. Measurements are subject to statistical fluctuations, although several theories obviate this fact due to the invariant\nnature of their observations, such as astronomy. However, in general,\nany theory ought to include a complete statistical model that allows to\ninfer system properties from measurement outcomes.\nA statistical model is a part of any theory, and it consists of:\nPreparations This refers to the states of the systems under consideration, like the setup of an experiment, which in classical theories\nare directly related to a point in phase space.\nMeasurements Procedures by which physicists glean information about\nthe systems from obtained data, which are obviously correlated to\nits state.\n21\n\n\f2. QUANTUM MECHANICS AS A STATISTICAL\nTHEORY\nMathematically this pair is denoted (S, M), where S is the set of all\npossible preparations and M is the set of all possible measurements on\nthese preparations.\nThere may be, and there usually is, uncertainty associated to both\npreparations and measurements. What makes Quantum Mechanics different is indeed which these kinds of uncertainty are.\nMost contents of this chapter can be found in [6][7][8][9].\n\n2.1\n\nQuantum Formalism\n\nIn Quantum Mechanics a state is defined as an equivalence class of preparations. This means that two states are to be considered equivalent if\ntheir preparations lead to parallel vectors in state space1\nQuantum Mechanics arises classically as a probabilistic theory, due\nto a very fundamental property of sub-microscopic systems, known as\nthe Superposition Principle, by virtue of which a quantum system may\nfind itself in a complex linear combination of states. This is the hallmark\nof Quantum Mechanics. This property, together with the definition of\nstate in previous paragraph, leads to a statistical model where the set of\npreparations is strongly convex, in contrast to classical statistical models,\nwhere they are just convex.\nThe outcome of a measurement will depend probabilistically on the\nrespective weights of the superposed states. This demands that experimenters be able of obtaining statistical ensembles of the same state in\norder to contrast experimental data with theoretic predictions. This\nautomatically leads to two different (but closely interrelated) notions of\nprobability. First one is related to the fundamental behavior of the submicroscopic world, and second one (somewhat more classical) concerns\nthe distribution of ensembles.\nHere, state space is a Hilbert space H where vectors |\u03c8i \u2208 H represent preparations. Two vectors are equivalent if they are parallel, that is, if they are the same up\nto a proportionality constant. For this reason, at a basic level we will identify states\nwith rays in Hilbert space, rather than vectors.\n1\n\n22\n\n\f2.1 Quantum Formalism\n\n2.1.1\n\nSet of States is Convex\n\nThe need for both quantum uncertainties and classical ensembles is best\nmet within the C*-Algebra formalism. A C*-Algebra C is a Banach1\nspace with unit 1 and a *-involution such that:\nkABk \u2264 kAkkBk\n\n(2.1)\n\nkAk2 = kA\u2217 k2 = kAA\u2217 k\n\n(2.2)\n\nwith A, A\u2217 , B \u2208 C. A\u2217 = A\u2020 stands for the adjoint of A, meaning\nthat the algebra is closed under the adjoint operation.\nEvery C*-Algebra can be seen as a *-subalgebra of the algebra of\nbounded operators on a Hilbert space H, B(H) [10], so it inherits the\ninner product:\nhA, Bi = T r(A\u2020 B)\n\n(2.3)\n\nNow consider the algebra A \u2286 B(H). A state % is a positive linear\nfunctional on this subalgebra, that maps elements in the positive cone\nA+ of A to nonnegative real numbers. We will only consider those\nfunctionals that fulfil %(1) = 1, for reasons to become clear in a while.\nLet A \u2208 A+ be a positive operator, then one can establish the oneto-one correspondence %(A) = T r(%\u0302A), where %\u0302 \u2208 A+ is a positive,\nself-adjoint operator of trace one, called the density operator. The requirement that the operator have trace one is related to a probability\nnormalization. We will subsequently identify % with %\u0302.\nDensity operators will play an role analogous to probability distributions in classical probability. Whereas the a probability simplex Pn\nhas only n vertices, each corresponding to a distribution where all the\nprobability mass is accumulated at just one outcome, density operators\nlive in a strongly convex set, meaning that there is an infinite number of\nextremal points, as a consequence of the Superposition Principle. This is\n1\n\nLoosely stated, a Banach space is a Hilbert space where orthogonality is not\nnecessarily defined.\n\n23\n\n\f2. QUANTUM MECHANICS AS A STATISTICAL\nTHEORY\ndepicted in fig. 2.1, where the simplex for two classical outcomes is compared with the set of all possible quantum preparations of a two-states\nsystem.\n\nFigure 2.1: The set of quantum states is strongly convex: Classically,\nthe set of states is given by the probability simplex P2 , any convex\ncombination of the two attainable states, 0 and 1, remains a state. The\nfundamental axiom of Quantum Mechanics, the Superposition Principle,\nsays that it is possible for a quantum bit to be in state |0i, in state |1i,\nor in a complex linear combination of both. This leads to a set of states\nwhere every superposition of states (in fact, there are infinitely many)\nmust still be contained in the set. The set of quantum states thus is\nstrongly convex, since there are infinitely many extremal points (living\nin a finite dimensional space). For a quantum bit, this set is called\nBloch's ball, and the coordinates of each extremal point can be worked\nout from the relative phases of the pure states.\nThe set of quantum preparations in previous figure receives the name\nof Bloch's ball. Throughout this dissertation we will assume that |0i and\n|1i is our selected computational basis. This means that these vectors\nconstitute a basis stable against decoherence and stand for the quantum\ncounterpart of a bit. |0i and |1i can refer to the spin of a nucleus, the\npolarization of a photon, or to the state of a bistable atom. In any\ncase, the number of degrees of freedom is two. Hence, it is the system's\nalgebra that receives the name of qubit, as a shorthand for quantum bit.\n24\n\n\f2.1 Quantum Formalism\nIn general, a N-states system is described by an algebra of N \u00d7 N\nmatrices and N 2 are needed to build a basis. One suitable basis is:\n2\n\nNX\n\u22121\n1\n% = (1 +\nri \u03c3i )\nN\n\n(2.4)\n\ni=1\n\nwhere {\u03c3i }N\n1\nthat:\n\n2 \u22121\n\nis some basis of self-adjoint, trace-free matrices, such\n\nh\u03c3i \u03c3j i = \u03b1\u03b4ij\n\n(2.5)\n\nN\nh\u03c3i , %i\n\u03b1\n\n(2.6)\n\nri =\n\nSo there is a mapping from density operators of dimension N to real\n2\nvectors in RN \u22121 . For N = 2, this basis is the Pauli matrices and\nT r(%2 ) = T r(%) = 1 if and only if krk2 = 1, i.e., rank one density\noperators lie in the boundary of Bloch's ball. A density operator having\nrank one is called a pure state, and otherwise is called a mixed state. Any\nmixed state can be expressed as a convex combination of pure states:\n%mixed =\n\nX\n\nqj %j\n\n(2.7)\n\nj\n\nwhere 1T q = 1, and %j = |jihj| are rank one density operators.\n\n2.1.2\n\nSet of Measurements is Also Convex\n\nA measurement M \u2208 M is an affine map from S to the set PU of all\nprobability distributions in some probability space (U, AU , pU ):\nM : S \u2212\u2192 PU\nClassically, this can reflect the statistical bias of a measuring apparatus or procedure, and amounts to a reshaping of the probability simplex.\nIn the quantum world, a measurement is defined on a strongly convex\nset, where \"quantum probabilities\" live, and takes values in a classical\n25\n\n\f2. QUANTUM MECHANICS AS A STATISTICAL\nTHEORY\nprobability simplex, so forcedly some structure must be lost in a measurement process. This is sometimes called the wave-packet collapse, or\ndecoherence (see section 2.3.1).\nConsider the measurement M(%) = pU . We shall write M = {M (u1 ), M (u2 ),\n..., M (uk )} where each M (uj ) \u2208 A+ is a positive operator, associated to\nan uj in AU 1 . The probability of event uj is given by:\np(uj ) = h%, M (uj )i\n\n(2.8)\n\nA measurement is also called a Positive Operator Valued Measure\n(POVM), since it relates a probability measure with an operator in the\npositive cone of the algebra A. A POVM has the following properties:\n\nM (\u2205) = 0\n\n(2.9)\n\nM (U) = 1\n\n(2.10)\n\nui \u2286 uj =\u21d2 M (ui ) \u2264 M (uj )\n[\nX\nui =\nuj =\u21d2 M (ui ) =\nM (uj )\nj\n\n(2.11)\n(2.12)\n\nj\n\nSk Since it is required that the whole sample space be covered, i.e.,\nj uj = U, then:\nk\nX\n\nM (uj ) = 1\n\n(2.13)\n\nj=1\n\nwhich ensures that probability is normalized p(U) = T r(%1) = 1.\nThe M (uj ) constitute a resolution of the identity.\nT\nNote that, even if ui uj = \u2205, in general it still may be the case\nthat M (ui )M (uj ) 6= \u03b4ij M (uj ). In this case we have a non-orthogonal\nresolution of the identity, also known as a fuzzy measurement.\nT\nWhenever ui uj = \u2205 \u21d2 M (ui )M (uj ) = \u03b4ij M (uj ) holds, we have\na projective measurement. This is justified because M (ui )2 = M (ui )\nare projectors. Projective measurements are extremal points of M, and\nThese events need not be elementary events: as members of the \u03c3-algebra AU ,\nthey may in general be subsets of the sample space U\n1\n\n26\n\n\f2.2 Von Neumann's Entropy\nare also called von Neumann measurements. The converse is in general\nnot true: there can be non-orthogonal resolutions of the identity at\nthe extremal points of M. However, for qubits, where U = {0, 1}, the\nconverse is true [6] and one can say that a measurement is an extremal\npoint of M if and only if it is a projective measurement.\nObservables are directly related to projective measurements through\nthe Spectral Theorem, which says that any self-adjoint operator X admits\nthe spectral representation:\nX=\n\nX\n\nui M (ui )\n\n(2.14)\n\nui \u2208U\n\nwhere M (ui ) is an orthogonal resolution of the identity. ui constitute\nthe spectrum of the observable and M (ui ) determine the eigenspace\nassociated to each eigenvalue.\nIn a practical scope, it is not known how to implement a general nonorthogonal POVM, defined in a state space H1 . However, Neumark's\nTheorem [6] ensures that it is possible to simulate a POVM with a\nprojective measurement defined in an extended space H1 \u2297 HA . The\nletter \"A\" stands for ancilliary system.\n\n2.2\n\nVon Neumann's Entropy\n\nJust as classical entropy is defined on a probability simplex, it is possible\nto define an entropy for quantum probability distributions, called the von\nNeumann's entropy, which is defined on the set of quantum states:\nS(%) = \u2212T r(% log %)\n\n(2.15)\n\nIn the case of orthogonal states, it reduces to Shannon's entropy. In\nfact, many properties (but not all) of classical entropy still hold in the\nquantum case. We can derive them,as in previous chapter, from the\nmore fundamental quantum relative entropy:\nS(%||\u03c3) = T r(% log % \u2212 % log \u03c3)\n27\n\n(2.16)\n\n\f2. QUANTUM MECHANICS AS A STATISTICAL\nTHEORY\nTheorem 6 [Nonnegativity of quantum relative entropy]The quantum\nrelative entropy is positive semidefinite, S(%||\u03c3) \u2265 0\nProof 9 It is possible to find a diagonalization for each density operator,\nP\nP\n% = i pi |iihi| and \u03c3 = j qj |jihj|, then:\n\nS(%||\u03c3) =\n\nX\n\npi [log pi \u2212\n\nX\n\ni\n\nj\n\nX\n\npi [log pi \u2212\n\nX\n\n=\n\ni\n\n\u2265\n\nX\n\nlog qj khi|jik2 ]\nDi,j log qj ]\n\nj\n\npi [log pi \u2212 log\n\ni\n\nX\n\nDi,j qj ]\n\nj\n\npi\n=\npi log P\nD\nj i,j qj\ni\nX\npi\n=\npi log\nri\nX\n\ni\n\n\u2265 0\nThe diagonalizations need not be equal, thus the possible overlap between the states must be accounted for. This overlap is encoded in a\ndoubly stochastic matrix Dij = khi|jik2 \u2265 0. The first inequality comes\nfrom a slight variation of Jensen's inequality and the concavity of the logarithm. The last inequality is a just a property of the classical relative\nentropy (see Theorem 1).\nTheorem 7 [Subadditivity of von Neumann's Entropy] For a global system %AB the joint entropy satisfies S(%AB ) \u2264 S(%A )+S(%B ) with equality\nif and only if both systems are uncorrelated.\nProof 10 As a consequence of the nonnegativity of the quantum relative\nentropy, we can write:\n\nD(%||\u03c3) = \u2212S(%) \u2212 T r(% log \u03c3) \u2265 0\n28\n\n\f2.2 Von Neumann's Entropy\nTaking % = %AB and \u03c3 = %A \u2297 %B , we obtain:\n\nS(%AB ) \u2264 \u2212%AB log(%A \u2297 %B )\n= \u2212%AB (log %A + log %B )\n= \u2212%A log %A \u2212 %B log %B\n= S(%A ) + S(%B )\n\n(2.17)\n\nTo see that the bound is tight if and only if the %AB = %A \u2297 %B , one\nneed only consider the relative entropy S(%AB ||%A \u2297 %B ).\nTheorem 8 [Concavity of von Neumann's entropy] Von Neumann's entropy is a concave function of %\nProof 11 To prove this result, we will make use of a spurious system\nB. Consider the joint state:\n\n%AB =\n\nX\n\nB\npi %A\ni \u2297 |iihi|\n\ni\n\nIts von Neumann's entropy is:\n\nX\nB\nS(%AB ) = S(\npi %A\ni \u2297 |iihi| )\ni\n\nX X j\n= S(\npi (\n\u03bbi |jihj|A ) \u2297 |iihi|B )\ni\n\n= \u2212\n\nX\n\n= \u2212\n\nX\n\nj\n\npi \u03bbji log \u03bbji \u2212\n\nX\n\npi \u03bbji\n\nX\n\ni,j\n\ni,j\n\nlog \u03bbji\n\n\u2212\n\ni,j\n\n=\n\nX\n\npi \u03bbji log pi\n\ni\n\npi S(%A\ni ) + H(p)\n\ni\n\n29\n\npi log pi\n\n\f2. QUANTUM MECHANICS AS A STATISTICAL\nTHEORY\nP\nP j\nwhere j \u03bbji |jihj|A is a diagonalization of %A\ni . Note that\nj \u03bbi = 1.\nOn the other hand, the entropies of the separated systems are:\nX\nS(%A ) = S(\npi %A\ni )\ni\n\nS(\n\nX\n\npi |iihi|B ) = H(p)\n\ni\n\nMaking use of the subadditivity property proved before, we have that:\n\nX\n\npi S(%A\ni ) + H(p) \u2264 S(\n\ni\n\nX\n\npi %A\ni ) + H(p)\n\ni\n\nX\n\npi S(%A\ni )\n\nX\n\u2264 S(\npi %A\ni )\n\ni\n\ni\n\nthus, the von Neumann's entropy is convex.\n\n2.3\n\nClassical Information and Quantum Information are Not the Same\n\nThe difference between bits and qubits is more fundamental than just\nterminology. Whereas classical bits are symbolic representations of the\ninformation stored in a physical system (i.e. modulated waves, or the\norientation of the magnetic cells in a hard drive...), qubits are to be\nidentified with physical systems, or with their algebra at least. Quantum information is more general than classical information, since the\nsymbolic representation of information arises in the special case where\nonly orthogonal states are considered.\n\n2.3.1\n\nClassical Information through Decoherence\n\nSince Quantum Mechanics supersedes classical theories, it is expected\nthat classical probability can as well be represented in the language\n30\n\n\f2.3 Classical Information and Quantum Information are Not\nthe Same\nof density operators. Consider a probability distribution pX over X\n(kXk = n), then its operator counterpart can be written as:\n\n%X =\n\nn\nX\n\npi |iihi|\n\n(2.18)\n\ni=1\n\nThis density operator belongs to the algebra of diagonal matrices. In\nthe qubit case, this algebra is the set of density operators in the segment\nthat passes through the poles in Bloch's ball (see fig. 2.1).\nQuantum states can be described by density operators having offdiagonal terms, which are responsible for quantum interferences, and\nthis is directly related to the fact that the set of states is strongly convex. How classical properties arise from quantum-mechanical laws is a\nitself a topic of intense research and receives the name of Environmental Decoherence [11][12]. In the information-theoretic context of this\nthesis, it suffices to say that in the measuring process that both the\nmeasured system and the measuring apparatus evolve together in time\n(according to some interaction Hamiltonian) into a preferred diagonal\nbasis, induced by the interaction of the measuring apparatus and their\nenvironment [13].\nP\nE\nLet %S = j qj %j , %A\n0 and %0 be the initial states of a system, an\napparatus and their environment, respectively. In a first step the system\nand the apparatus become correlated, so that observing the apparatus\nwill give us information about the system. In a second step, the apparatus is let alone to evolve along with its environment. This process is\ndepicted in fig. 2.2:\nUSA (\n\nX\n\n\u2020\nqj %j \u2297 %A\n0 )USA =\n\nj\n\n\u2192 UAE (\n\nX\n\nX\n\nqj0 %j \u2297 %A\nj \u2192\n\n(2.19)\n\nj\n\n\u2020\nE\nqj0 %j \u2297 %A\nj \u2297 %0 )UAE =\n\nj\n\nX\n\nE\nqj00 %j \u2297 %A\nj \u2297 %j\n\n(2.20)\n\nj\n\nThe measuring apparatus and the environment become rapidly correlated, and the off-diagonal terms in the system's density operator are\nswept away. Provided that the environment remains in a pure state and\n31\n\n\f2. QUANTUM MECHANICS AS A STATISTICAL\nTHEORY\n1\nE\nthat h%E\ni %j i \u2248 \u03b4ij , tracing out the environment and the apparatus (see\nsection 3.1.1) leaves us with a classical probability simplex:\n\n%SA \u2248\n\nX\n\n0\n\nqj00 |jihj| \u2297 %jA\n\n(2.21)\n\nj\n\nFigure 2.2: Decoherence in the measuring process: The interaction of\nthe measuring apparatus with its environment causes the quantum correlations to dilute in the joint Hilbert space of the system, apparatus\nand environment. The crux of this process is that the states of the environment are by definition unaccessible, so no measurement could ever\ndetect these correlations. Thus, locally, the joint system-apparatus state\nappears to be in a diagonal matrix state, as a consequence of tracing out\nthe environment. In this picture, a measurement with four outcomes is\nrepresented.\n0\n\nsuch that [%jA , UAE ] = 0, i.e. they share a diagonal basis2 . Eq.\n2.21 is to be compared with eq. 2.18 The quantum probabilities don't\ndisappear, but get dispersed in the correlations between the system and\nits environment.\nNote that the expressions 2.19 and 2.20 are not correct in general,\nsince time evolution couples the system, the apparatus and their envi1\n\nThis assumption basically comes from the fact that we don't observe quantum\ninterferences between macroscopic states\n2\nThis means that the measuring apparatus evolves to a state which is stable\nagainst decoherence, i.e., stationary under macroscopic time evolution\n\n32\n\n\f2.3 Classical Information and Quantum Information are Not\nthe Same\nronment in such a way that their global state cannot be expressed as\na product state: the final state may in general be entangled. However,\nto illustrate the correlations that take place during the measuring process we use these partially \"allegoric\" expressions. In later chapters the\ncorrectness will be restored.\n\n2.3.2\n\nNo-Cloning Theorem\n\nAnother way to see the difference between classical and quantum information is to imagine a machine capable of copying quantum states.\nThe machine is fed at its input with an unknown quantum state and it\noutputs two copies of the initial state. This machine cannot exist:\nTheorem 9 [No-Cloning Theorem] It is impossible to copy unknown\nquantum states.\nProof 12 Without loss of generality, we shall only consider pure states.\nConsider two unknown states %1 and %2 , which are fed as input into the\ncopying machine in an initial pure state %CM . The copying process is\ndescribed as a time evolution U of the whole system:\nU(%1 \u2297 %CM )U\u2020 = %1 \u2297 %1\n\n(2.22)\n\nU(%2 \u2297 %CM )U\u2020 = %2 \u2297 %2\n\n(2.23)\n\nif we now take the inner product of the two equations, we have that:\n\nh%1 , %2 i = h%1 , %2 i2\n\n(2.24)\n\nthus both states must be either orthogonal, or the same. This requirement is in contradiction with the assumption of the two qubits being\nunknown.\n33\n\n\f2. QUANTUM MECHANICS AS A STATISTICAL\nTHEORY\nAs expected, this is in accordance with the existence of classical fanout gates, which have the capacity of copying a bit as many times as\ndesired.\n\n2.3.3\n\nThe Holevo's Bound\n\nClassically, the capacity of inference is related to the mutual information.\nAn observer (receiver) can reliably guess the value of an experiment\n(or channel use) provided that I(X; Y ) is arbitrary close to H(X). In\nprinciple, thanks to the use of better preparation and measuring devices\n(equivalently, coding and decoding schemes), mutual information can be\nbrought very close to its upper bound.\nQuantum mechanics prevents this fact, once again as a consequence\nof the Superposition Principle, because there may be states which are\nnot orthogonal, and no measurement can, even in principle, distinguish\nthem with 100% reliability.\nTheorem 10 [The Holevo's Bound] Let X \u2208 X be encoded in state\nP\n%X = ni=1 pX\ni %i , where the %i have orthogonal support, and a measurement MY (%X ) = pY , the accessible information is upper bounded by:\n\nX\n\nI(X : Y ) \u2264 S(% ) \u2212\n\nn\nX\n\npX\ni S(%i )\n\n(2.25)\n\ni=1\n\nProof 13 Mutual information can be written as:\n\nI(X : Y ) = H(pX ) \u2212 H(pX |Y )\nLast term represents the uncertainty about X provided that measurement MY was chosen:\n\nH(pX |Y )) =\n\nm\nX\nj=1\n\np(yj )\n\nn\nX\ni=1\n\n34\n\np(xi |yj ) log p(xi |yj )\n\n\f2.4 Experiments as Information Transfer\nwith p(xi |yj ) = h%i , Mj i. It is easy to see that the conditional entropy\nwill vanish if and only if h%i , Mj i = \u03b4ij . Now, suppose that this is\nindeed the case: selected measurement scheme is optimal. Reasoning in\na similar way to Theorem 7, it is possible to write:\nn\nn\nX\nX\nX\nS(\npX\n%\n)\n=\npX\ni i\ni S(%i ) + H(p )\ni=1\n\ni=1\n\nThe optimal measurement strategy yields:\n\nI(X : Y ) = H(pX ) = S(\n\nn\nX\n\npX\ni %i ) \u2212\n\ni=1\n\nn\nX\n\npX\ni S(%i )\n\ni=1\n\nFor measurements that are not optimal, we will have in general that:\nn\nn\nX\nX\nX\nI(X : Y ) \u2264 S(\npi %i ) \u2212\npX\ni S(%i )\ni=1\n\ni=1\n\nNote if the states %i are chosen to be pure, the upper bound in 2.25\nreduces to the classical entropy. One direct conclusion to be drawn from\nprevious Theorem is that the information contained in a qubit is, at\nmost, one bit. This discouraging result may lead us to the opinion that\nquantum information has no real advantages over classical information.\nAs we will see in next part, this belief is wrong.\n\n2.4\n\nExperiments as Information Transfer\n\nPerhaps it is illuminating to see that it is possible, just with a slight\nchange in the terminology, to compare the two main scenarios that are\noccupying us: a communications channel, and a physical experiment.\nThe main goal of both operations is to gain information about the\nstate of an unaccessible system. In a communications channel this system is the source. In the experiment, it is an unknown system that\nis forced to interact with another one, previously prepared in a quantum state. Thus in the experiment scenario, information enters at the\n35\n\n\f2. QUANTUM MECHANICS AS A STATISTICAL\nTHEORY\n\nFigure 2.3: Pictorial duality between experiments and channels: In the\ncommunication channel scenario, we represent a source taking values\nin two symbols, encoded in a four symbol code. Then, stochastic evolution takes place and the probability simplex is distorted (dimension\ncan increase, decrease or remain the same, and symbol frequencies may\nchange). At the decoder, the original two symbols should be recovered\nwith their original frequencies. In the case of experiments, a quantum\nstate is prepared. According to a known Hamiltonian, the system undergoes a deterministic time evolution jointly with the unknown system\nof which information is to be obtained. Then several hypothesis are\nencoded in the set of states, as a result of the joint time evolution.\nevolution stage. Encoding is a procedure analogous to a preparation.\nWhereas temporal evolution is totally deterministic, the channel is of\nstochastic nature, but it still represents some sort of time flow. After\nundergoing a temporal evolution or a channel, the sets of states are distorted to some extent. Finally, both measurement and decoding entail\nthe estimation of a probability distribution out of the incoming sets of\nstates. In the case of experiments, uncertainty is introduced at this\nstage, if the states are not orthogonal.\n\n36\n\n\fPart II\n\nQuantum Entanglement: A\nNew Resource for\nCommunication\n\n37\n\n\f\fChapter 3\n\nQuantum Non-locality\nTo jointly describe multiple systems, an algebra is needed which contains\nthe elements of global measurements, as well as those part of partial\n(marginal) measurements. For bipartite systems, the algebra of the\ncomposite system is:\nC=A\u2297B\nwhere A and B are the operator algebras of the subsystems, respectively. For the algebra of diagonal matrices of dimension N , that is, for\nclassical distributions of N different outcomes, the number of orthogonal\nmatrices needed to form a basis is N . For two classical systems of the\nsame dimension, we will need N 2 such matrices. In the quantum case,\nsince matrices need not be diagonal, the number of matrices that form\na basis is N 2 , and for two systems N 4 matrices would be needed.\nFor classical systems, if two different observers perform a measurement, each one at a different system, they will each gain log N bits of\ninformation. If they combine their information about the subsystems,\nit will be possible to reconstruct the global state, as it only demands\n2 log N bits.\nIn the quantum case, the Holevo's bound says that each observer can\ngain at most log N bits. Thus, there is no way no learn about the global\nstate just from the marginal measurements, for it demands 4 log N bits,\nwhereas there are only 2 log N available.\n39\n\n\f3. QUANTUM NON-LOCALITY\nThis suggests that there is more information contained in the composite system, than in the sum of the informations contained in its components. This characteristic of quantum systems gives rise to a new\nfundamental phenomenon called quantum non-locality.\n\n3.1\n\nEPR Paradox\n\nIn their famous paper [14], Einstein, Podolsky and Rosen (EPR) came to\nthe conclusion that Quantum Mechanics was in awkward epistemological\nstatus, due to the its lack of at least one of the properties required to any\ntheoretical framework which intends to describe Reality. This properties\ncan be stated as two principles:\nPrinciple of Locality Two causally disconnected, i.e. spatially separated, measurements cannot exert any influence on one another.\nPrinciple of Realism Any physical theory must account for every element of reality, this meaning that every possible outcome of an\nexperiment should have a definite value prior to its measurement.\nEPR showed that Quantum Mechanics violates at least one of these\ntwo principles, so a quantum description of Reality cannot be completely\naccurate. In a gedanktexperiment devised by Bohm [15], which involves\nparticles of spin one half.\nAB\nConsider the pure state of a composite system %AB\n\u03c8 \u2208 A \u2297 B, %\u03c8 =\n|\u03c8ih\u03c8|, where:\n\n1\n|\u03c8i = \u221a (|0iA \u2297 |1iB \u2212 |1iA \u2297 |0iB )\n2\n\n(3.1)\n\nis a state vector representing the preparation of two two-states systems. This state may be created as pairs of photons of opposite polarization emitted from a common source1 . The indices A and B denote\ntwo different locations or observers, causally disconnected, where each\noperator algebra is defined, respectively.\n1\n\nPhotons are massless spin one particles, so their polarization has just two degrees\nof freedom, and can be modeled as a two-state system.\n\n40\n\n\f3.1 EPR Paradox\nGiven that the source is capable of providing an unlimited number\nof pairs, observer at location A performs a (projective) measurements\non its particles MA (%A ) = pA , and so does the observer at location B,\nB\nB\nobtaining the distribution MB\ni (% ) = p , i = 1, 2. Observer at location\nB is able to choose between the different measurements:\n\nMB\n1\nMB\n2\n\n\u0012\n\n\u0013 \u0012\n\u0013\n1 0\n0 0\n= {\n,\n}\n0 0\n0 1\n\u0013 \u0012\n\u0013\n\u0012\n1\n1 1 1\n1 \u22121\n,\n}\n= {\n2 1 1\n2 \u22121 1\n\n(3.2)\n(3.3)\n\nand MA = MB\n1 . The two measurements correspond to different\norientations of the polarized detector. From basic Quantum Mechanics\nit is not hard to see that these orientations are orthogonal in real space.\nIf A and B choose to use the same measurement setup (detectors\npolarized in the same direction), due to the structure of the state %AB\n\u03c8 ,\nwhenever A measures its particle pointing upwards, B will necessarily\nfind it pointing downwards, and viceversa. If B uses a detector polarized\nin an orthogonal direction, then its outcomes will be uncorrelated to\nthose of A, which comes from the fact that:\n\nB\nB\nB\nB\nhM1A , M2,1\ni = \u2212hM1A , M2,2\ni = hM2A , M2,1\ni = \u2212hM2A , M2,2\ni=\n\n1\n(3.4)\n2\n\nthat is, there will always be some probability overlap between the\noutcomes in the different orientations.\nThe EPR paradox can be stated as follows. Suppose that at a first\nstage, A and B are measuring their respective particles in different directions, i.e. using different\n\u0012\n\u0013 measurement setups. Their statistics will be\nplain, pA = pB =\n\n1\n2\n1\n2\n\n(see 3.1.1). Now, suppose that, right before\n\nmeasuring its particles, B always switches to its alternative setup without letting A know about this change (they might be many lightyears\napart), and measures in the same direction as A does. No matter how\nfar apart they happen to be, if A gets |0i, then B will get |1i with certainty. Their outcomes will be correlated, yet they will not be aware of\nthis correlation unless they communicate their results, for their statistics\nwill remain plain.\n41\n\n\f3. QUANTUM NON-LOCALITY\n\nFigure 3.1: EPR paradox: Each subsystem of the EPR pair is delivered\nto a different observer. A and B are spatially separated, so the choice\nof observer B should not influence the outcomes of A.\nHow does the particle at A learn about the change in the orientation\nof detector in B, despite being causally disconnected from it, is the EPR\nparadox. One must draw the conclusion that either:\n\u2022 Quantum Mechanics violates the Principle of Locality, or\n\u2022 Quantum Mechanics is incomplete and some hidden-variable theory that supersedes Quantum Mechanics is needed to explain these\nnon-classical correlations.\n\n3.1.1\n\nMarginal Measurements\n\nSo far we mentioned %AB , %A and %B as the density operators of the\nwhole system and of its components, respectively. The procedure to\nobtain the marginal density operators from the joint one is analogous as\nn,m\nin classical probability. Let pAB = {pAB\nij }i=1,j=1 be the joint probability\nfor variables A and B1 . The marginal in A is obtained via:\npA\ni =\n\nm\nX\n\npAB\nij\n\n(3.5)\n\nj=1\n1\n\nHere we don't assume that A and B are systems with the same dimension, so n\nand m need not be equal\n\n42\n\n\f3.2 Quantum Correlations and Bell's Inequalities\nTo \"sum out\" one probability distribution is equivalent to ignore\nwhat is happening in the system associated to B. In Quantum Mechanics,\nthis deliberate ignorance amounts to perform the trivial measurement,\nMB = {1}, in the system we want to ignore:\nAB\npA\n, MiA \u2297 1i\ni = h%\n\n(3.6)\n\nAB corresponding to its\nIf with %AB\nab,a0 b0 we denote the entries of %\n0\nsubsystems, where aa represents the degrees of freedom localized at A\nand bb0 those localized at B, then eq. 3.5 can be developed:\n\npA\ni\n\n=\n\n=\n=\n\nn,m\nX\n\nA\n%AB\nab,a0 b0 Mi,aa0 \u03b4bb0 =\n\naa0 ,bb0\nn X\nm\nX\n\n(\n\naa0\nn\nX\n\nA\n%AB\nab,a0 b )Mi,aa0 =\n\nb\nA\nA\nA\n%A\naa0 Mi,aa0 = h% , Mi i\n\n(3.7)\n\naa0\n\nPm AB\nwhere %A =\nb %ab,a0 b is the so-called reduced density operator,\nobtained by disregarding system B. The operation of tracing out one\nof the subsystems is called partial trace of a state, and is denoted\n%A = T rB %AB .\nThus, we have seen that the subalgebras of marginal measurements\ncan be obtained just by means of tensor-multiplying positive operators\nwith the identity.\n\n3.2\n\nQuantum Correlations and Bell's Inequalities\n\nEPR agreed on that the predictions of Quantum Mechanics were indeed\ncorrect, but ultimately explainable in terms of statistical distributions of\nsome \"hidden variables\", which would be in harmony with the principles\nof locality and realism. This conjecture could neither be proved nor refuted until the advent of Bell's inequalities[16]. These inequalities have,\n43\n\n\f3. QUANTUM NON-LOCALITY\na priori, nothing to do with Quantum Mechanics, but rather put a constraint on the correlations predictable by any theory that incorporates\n\"local realism\" (we use this name to refer to both principles introduced\nabove).\nA bipartite system is said to be correlated if:\n\npAB 6= pA pB \u21d4 h%AB , MA \u2297 MB i =\n6 h%A , MA ih%B , MB i\n\n(3.8)\n\nfor some measurements MA and MB . Here the inner product has to\nbe understood as componentwise products. Equation 3.8 is equivalent\nto demand that the density operator factorize:\n%AB 6= %A \u2297 %B\n\n(3.9)\n\nA quantum state may exhibit two kinds of correlations: classical and\nquantum. Classical correlations arise whenever a state is of the form:\n%AB =\n\nX\n\nB\nqk %A\nk \u2297 %k\n\n(3.10)\n\nk\n\nwith 1T q = 1. It is straightforward to check that expectation values\nno longer factorize for these states. These states are known as separable\nstates. Quantum correlations are, once again, a consequence of the Superposition Principle applied to composite systems: a (pure) quantumcorrelated state doesn't admit a convex decomposition as in the previous\nexpression, yet it still fulfils condition expressed in eq. 3.9. One example\nis the state used in the EPR paradox:\n\n1\nA\nB\n%AB\n\u03c8 = |\u03c8ih\u03c8| = (|0i|1i \u2212 |1i|0i)(h0|h1| \u2212 h1|h0|) 6= % \u2297 %\n2\n\n(3.11)\n\nSuch states are called entangled. Ascertain whether it was possible\nor not to describe entangled states in the context of a hidden variable\ntheory was the task of Bell's inequalities.\nThe assumption of local realism entails the existence of joint probability distributions of a set of measurable quantities, regardless of whether\n44\n\n\f3.2 Quantum Correlations and Bell's Inequalities\nthey are observed or not. Any system will be at a definite state prior to\nbeing measured, which implies that the correlations between measurements at two different locations may depend on any (in general infinite)\nnumber of hidden variables \u03bb \u2208 \u039b (with \u039b a continuous set):\n\nAB\n\nC(i, j) = h%\n\n, MiA\n\n\u2297\n\nMjB i\n\nZ\n=\n\nf (MiA , \u03bb)f (MjB , \u03bb)p(\u03bb)d\u03bb\n\n(3.12)\n\n\u039b\n\nwhere p(\u03bb) is a probability distribution, and f (MiA , \u03bb) is the probability of measuring outcome i in system A, when the unknown hidden\nparameter is \u03bb.\nIn an experiment proposed by Clauser, Horne, Shimony and Holt\n(CHSH) [17] and carried out by Aspect and coworkers [18], it was possible to test whether entangled states admit a hidden-variable model (the\nCHSH inequality applies to two-states systems, inequalities for general\nsystems have also been found. As an interesting case see [19]). Consider\nA\nB\nB\nthe measurements MA\n1 , M2 , M1 (\u03b8), and M2 (\u03b8). These measurements\nhave a probability overlap which depends on the angle \u03b8 between the\ntwo different setups (see fig. 3.2). They derived the following inequality,\nwhich holds for every theory that incorporates local realism:\n\nA\nB\nB\nA\nB\nB\n\u22122 \u2264 h\u03c1AB , M1,i\n\u2297(M1,j\n(\u03b8)+M2,j\n(\u03b8))+M2,i\n(\u03b8)\u2297(M1,j\n(\u03b8)\u2212M2,j\n(\u03b8))i \u2264 2, \u2200i, j\n(3.13)\n\nFigure 3.2: Violation of Bell's inequalities\nQuantum Mechanics predicts a violation of this inequality for some\n\u03c0 3\u03c0\nstates. For the state %AB\n\u03c8 , the violation is maximum for \u03b8 = 4 , 4 , where\n45\n\n\f3. QUANTUM NON-LOCALITY\n\u221a\nwe have that |2 2| \u0002 2. The conclusion is that EPR were in the right\npath: Quantum Mechanics is non-local.\n\n3.3\n\nInformation-Theoretic Considerations\n\nVon Neumann's Entropy is a generalization of Shannon's Entropy. It\nis zero for pure states, i.e., rank one density operators. In the case\nof bipartite systems, although the global system may be known with\ncertainty to be in a pure state, such as for %AB\n\u03c8 , its marginal states,\nA\nand %B =\ndescribed by the reduced density operators % = T rB %AB\n\u03c8\nT rA %AB\n\u03c8 , can be in a mixed state, so that their von Neumann entropy will\nbe nonzero. This, once again, suggest that the whole system contains\nmore information than the mere sum of the information contained in it\nparts.\nAs we will see, a pure state is entangled if and only if the von Neumann's entropy of any of its reduced density operators is nonzero.\nOne consequence of the above said is that conditional quantum information can be negative [20]. For pure entangled quantum states we\nhave that S(%AB = 0), so that:\n\nS(%AB ) = S(%A ) + S(%B |A)\n\n(3.14)\n\nS(%B |A) = \u2212S(%A ) \u2264 0\n\n(3.15)\n\nThis \"negative conditional information\", with no counterpart in classical Information Theory, can be given an operational meaningful interpretation. If S(%B |A) is negative, then A can reproduce the whole state\n%AB just by means of classical communication, which is equivalent to say\nthat quantum information can be transferred from B to A using only\nclassical bits [21]. Depending on its sign, conditional quantum information is the rate at which entanglement is created or consumed while\ntransferring the state of be to state in A, and it is related to the quantum\ncapacity of a channel.\n46\n\n\f3.4 Unexpected Applications\n\n3.4\n\nUnexpected Applications\n\nGiven a total state that comprises locations A and B, an observer at\nlocation A can, by performing a local POVM on its subsystem, exert\nan influence upon the outcomes of observations at B (with A and B\nbeing spatially separated). This fact was named quantum steering by\nSchr\u00f6dinger [22][23]. It was shown [24] that a local POVM at A can\ninduce any ensemble {pk , %k } at B provided that the reducedPdensity\noperator at B admits a convex decomposition of the form %B = k pk %k .\nIf otherwise one could actually change the marginal statistics, superluminical communication would be achieved.\n\n3.4.1\n\nQuantum Teleportation\n\nOne of the brightest consequences of quantum steering is quantum teleportation. Provided that observers at A and B share an EPR pair %AB\n\u03c8 ,\nit is possible for them to teleport an unknown qubit1 in a pure state\n\u03c6C , initially at location A, to B using just local operations and classical\ncommunication (LOCC).\n\nFigure 3.3: Quantum Teleportation: A quantum system in state \u03c6 disappears at location A, and after some classical information has been\nsent from A to B, the system \u03c6 appears at location B. A and B are\ncausally disconnected. Despite its name, it is a rather prosaic effect,\nsince it involves a protocol prescribed beforehand and requires that A\nand B share an EPR pair.\n1\n\nIn general any qudit can be teleported. In this subsection and next one we will\nfollow the original path of its discoverers and use qubits\n\n47\n\n\f3. QUANTUM NON-LOCALITY\nC\nInitially, the global system will be described by the state %AB\n\u03c8 \u2297\u03c6 .\nObserver at A chooses a projective measurement M = {Mi }41 \u2208 MA \u2297\nMC whose components are rank one operators such that:\n\nhMi , %AC\n\u03c8j i = \u03b4ij\n\n(3.16)\n\nwhere %AC\n\u03c8j = |\u03c8j ih\u03c8j | is any of the four states forming a Bell's basis:\n1\n|\u03c81 i = \u221a (|0iA \u2297 |0iC + |1iA \u2297 |1iC )\n2\n1\n|\u03c82 i = \u221a (|0iA \u2297 |0iC \u2212 |1iA \u2297 |1iC )\n2\n1\n|\u03c83 i = \u221a (|0iA \u2297 |1iC + |1iA \u2297 |0iC )\n2\n1\n|\u03c84 i = \u221a (|0iA \u2297 |1iC \u2212 |1iA \u2297 |0iC )\n2\nThe EPR pair corresponds to the fourth vector of this basis. Once\nthis basis has been selected, as long as the unknown system is in a pure\nstate, it is possible to rewrite the initial state in the form [25]:\n\n1 AC\nC\nB\nAC\n\u03c0\nB\nAC\n\u03c0\nB\nAC\n\u03c0\nB\n%AB\n\u03c8 \u2297\u03c6 = [%\u03c81 \u2297\u03c6 +%\u03c82 \u2297Rz (\u03c6 )+%\u03c83 \u2297Rx (\u03c6 )+%\u03c84 \u2297Ry (\u03c6 )]\n4\n(3.17)\nHere Rk\u03c0 (\u03c6B ) denotes a rotation of the state \u03c6B of 180 degrees around\naxis k.\nTo teleport the unknown system, A performs the projective measurement in both systems A and C, so that its outcome determines with\ncertainty in which state will the system in B is. Now, all A has to do\nis to encode its outcome in two bits and send them over to B. Once\nB receives the information, it will be possible to rotate the state back\nin the direction determined by the two bits. With 100% accuracy, the\ninitial unknown state will be obtained at a spatially separated location.\nFor mixed states, and for non-orthogonal POVM, it is still possible\nto teleport a system, but the process will be necessarily less efficient.\n48\n\n\f3.4 Unexpected Applications\n\n3.4.2\n\nQuantum Superdense Coding\n\nAnother outstanding feat of quantum information is superdense coding\n[26], by which A can send to B two bits of information encoded in a\nsingle qubit1 .\n\nFigure 3.4: Superdense Coding: Provided that A and B share an EPR\npair, A can change the global EPR state acting locally on its subsystem. Then A sends its part \u03c6 = T rB %AB\n\u03c8i of the EPR state to B, who,\nmeasuring on a joint basis of the two subsystems, can extract two bits\nof information.\nSuppose that A and B share an EPR pair %AB\n\u03c84 . Observer at A causes\nits subsystem to evolve into one of the four possible states:\nB\nAB\nA\nB \u2020\nAB\n(UA\ni \u2297 I )(%\u03c84 )(Ui \u2297 I ) =\u21d2 %\u03c8i\n\n(3.18)\n\nThe four possible operations that A can apply to the joint system\nare:\nA\nUA\n1 = \u03c3y\nA\nUA\n2 = \u03c3x\nA\nUA\n3 = \u03c3z\nA\nUA\n4 =1\n\nwhere the \u03c3's are Pauli matrices. After the manipulation, the system initially at A is sent to B. Observer at B chooses the projective\n1\n\nIn fact, 2 log n bits can be sent using n-states systems\n\n49\n\n\f3. QUANTUM NON-LOCALITY\nmeasurement M = {Mi }41 \u2208 MA \u2297 MB with hMi , %AB\n\u03c8j i = \u03b4ij . Thus B\nwill gain two bits of information, while it only received one qubit.\nNote that, whereas in quantum teleportation an unknown state was\nmeasured in A and the outcomes were encoded in classical bits, in superdense coding, a the \"future outcomes\" of a measurement in B are\nencoded in a known qubit, which is later sent from A to B. First scenario\nis related to the capacity of transmitting quantum information through\na classical channel, and second one is related to the capacity of transmitting classical information trough a quantum channel. Underlying these\ntwo processes lies the same phenomenon: Quantum Non-locality.\n\n50\n\n\fChapter 4\n\nEntanglement Theory\nEntanglement is a new resource for communication that lies at the very\nheart of lies at the very heart of Quantum Information Theory. Thus, a\ntheory of Entanglement which offers a qualitative description as well as\nquantitative measures is highly desirable.\nMainly, two difficulties surround this task. First one is to find a\nmeaningful measure of the entanglement contained in a state. One way\nto obtain a suitable measure is to define beforehand some desirable properties that it should have:\nScope An entanglement measure is a map from the set of composite\ndensity operators to the positive real line:\nE(%AB ) \u2208 R+\n\n(4.1)\n\nNormalization It should vanish only for separable states, and should\nbe maximum for maximally entangled states:\n0 \u2264 E(%AB ) \u2264 E(%AB\n\u03c8i )\n\n(4.2)\n\nMonotonicity E(%AB ) should not increase under transformations involving only local operations and classical communication (LOCC).\nConsider the set of all LOCC transformations TLOCC . For any\ntransformation T \u2208 TLOCC :\n51\n\n\f4. ENTANGLEMENT THEORY\n\nE(T (%AB )) \u2264 E(%AB )\n\n(4.3)\n\nAs a consequence, one can derive the requirement of invariance\nunder local unitary evolution. Let %\u0303AB = (U \u2297 V )%AB (U \u2297 V )\u2020 ,\nfor any two unitary operators, then:\nE((U \u2297 V )%AB (U \u2297 V )\u2020 ) \u2264 E(%AB )\n\n(4.4)\n\nE((U \u2297 V )\u2020 %\u0303AB (U \u2297 V )) \u2264 E(%\u0303AB )\n\n(4.5)\n\nwhence we obtain E(%AB ) = E((U \u2297 V )%AB (U \u2297 V )\u2020 ).\nConvexity Such a desirable property arises naturally from the reasonable assumption that mixing two states should not increase the\nentanglement contained in them:\nE(\u03bb%AB + (1 \u2212 \u03bb)\u03c6AB )) \u2264 \u03bbE(%AB ) + (1 \u2212 \u03bb)E(\u03c6AB )\n\n(4.6)\n\nwith 0 \u2264 \u03bb \u2264 1.\nContinuity Intuitively, if %AB is slightly perturbed into \u03c6AB , the subsequent change in the entanglement measure should be small. This\nis expressed as:\nlim\n\nk%\u2212\u03c6k\u21920\n\nE(%AB ) \u2212 E(\u03c6AB ) = 0\n\n(4.7)\n\nSubadditivity The communication tasks that one is able to perform\nin the possession of several entangled pairs shouldn't be more than\nthe sum of those permitted individually by each pair:\nE(%AB \u2297 \u03c6AB ) \u2264 E(%AB ) + E(\u03c6AB )\n\n(4.8)\n\nfor the case when one has many copies of the same state, the\ndemand which is often encountered is thatof weak subadditivity:\nE(\u03c6\u2297N )\n= E(\u03c6)\nN\n\n(4.9)\n\nThe second difficulty is how to compute an entanglement measure\nfor any given state, which as we will see, is a far from trivial task.\n52\n\n\f4.1 Pure States\n\n4.1\n\nPure States\n\nFor pure states, a satisfactory theory exists and there are procedures\nboth to detect and quantify the entanglement of a given state. The\nvon Neumann's entropy of any of the reduced density operators satisfies\nthe requirements exposed above [27][28] and is directly related to the\nSchimdt's decomposition 1 of the vector state. For a maximally entangled\nsymmetric state in dimension N , its Schmidt's decomposition is:\nN \u22121\n1 X A\n|\u03c81 iAB = \u221a\n|ii \u2297 |iiB\nN i=0\n\n(4.10)\n\nand the von Neumann's entropy of its reduced density operator attains its maximum:\nS(T rB %AB\n\u03c81 ) = log N\n\n(4.11)\n\nIf the state is separable, it will necessarily consist of two pure states\nat each location, so the entropy of any of the local density operators will\nbe zero.\n\n4.1.1\n\nEntanglement Distillation\n\nThe preference of von Neumann's entropy of the reduced density operator over other candidates relies also on another reason: it quantifies\nthe amount of maximally entangled states that one can obtain from an\narbitrary large number of arbitrary density operators, by some LOCC\ntransformation [29].\nTheorem 11 [Entanglement Distillation] Given m identical copies of\none arbitrarily entangled pure state, (\u03c6AB )\u2297m , then there exist a LOCC\nscheme T \u2208 TLOCC such that it is possible to obtain n \u2264 m copies of a\nmaximally entangled state:\n1\n\nThe Schmidt's decomposition of a bipartite state is the projection of the state\nonto an orthonormal product basis of the two Hilbert spaces. If a state is separable,\nits vector state will be pointing parallel to one of the vectors of this basis. If it is\nentangled, its vector state will have more than one component.\n\n53\n\n\f4. ENTANGLEMENT THEORY\n\n\u2297n\nk=0\nlim kT ((\u03c6AB )\u2297m ) \u2212 (%AB\n\u03c81 )\n\nm\u2192\u221e\n\n(4.12)\n\nand the rate at which this can be done is given by the von Neumann's\nentropy of any of the reduced density operators of the initial state:\nn\n= S(T rB \u03c6AB )\nm\u2192\u221e m\n\nR = lim\n\n(4.13)\n\nOur proof for this theorem will need typicality arguments, so it will\nbe given in next chapter.\nThe importance of entanglement distillation is that most communication tasks rely on maximally entangled states in order to yield acceptable\ntransmission fidelities(see teleportation and superdense coding). Due\nto the stochastic influence of channels on quantum information, it is\nvery difficult to prevent a transmitted entangled particle from being\ncorrupted with some noise, and protocols must be devised to restore the\ninitial entanglement, at the expense of sending more particles.\nThe converse procedure is called entanglement dilution, by which n\ncopies of the maximally entangled state can be used to obtain m \u2265 n\ncopies of an arbitrarily entangled pure state. However it doesn't seem\nto be as practical as entanglement distillation.\n\n4.2\n\nMixed States\n\nFor the general case of mixed states, the theory of entanglement is far\nfrom complete. There are several several measures, based on inequivalent criteria. The prevalence of any of this candidates has not yet\noccurred. Our approach to the study of entanglement focuses on two\ndistinct areas, detection and quantification of entanglement. These two\nconcepts are tightly interrelated, and this scheme is just a matter of\ntaste.\nIt was shown that the separability problem, i.e. to ascertain whether\na given state is separable or entangled, belongs to the class of NP-hard\nproblems [30]. This means that, on a realistic basis, we should not\nexpect to measure (and detect) entanglement with arbitrary accuracy.\n54\n\n\f4.2 Mixed States\nSeveral relaxations of the separability problem have been proposed in\nthe context of convex programming [31][32][33][34]. Here we will study\nmore in depth the approach suggested in [35], which a offers a geometric\nintuition of the space of composite density operators.\nFor a thorough review of this concepts readers might check [36][37].\n\n4.2.1\n\nDetection\n\nSeveral criteria have been proposed to check whether a state exhibits\nquantum or just classical correlations. Here we shall list some of them:\nPeres-Horodecki Criterion Peres showed that Positivity under Partial Transposition (PPT) of the density operator was a necessary\ncondition for separability [38]. A bipartite state %AB is separable\nif:\n(%AB )TB =\n\nX\n\nB T\n%A\nk \u2297 (%k ) \u2265 0\n\n(4.14)\n\nk\n\nthat is, if it remains positive semidefinite under transposition of\njust one local density operators. The Horodecki traced back this\nargument to the theory of positive maps [39], and demonstrated\nthat for systems of dimension 2 \u00d7 2 (two entangled qubits)and\n2 \u00d7 3 (one qubit entangled with one qutrit) this criterion is also\na sufficient condition. Why the separability problem is solved,\ndespite being NP-hard in general, finds an explanation in the fact\nthat, for 2 \u00d7 2 and 2 \u00d7 3 systems, any positive map is of the\nform L = S1 + S2 \u25e6 T (S1,2 are completely positive maps and T\nis the transposition map), so no further search is needed to fully\ncharacterize separable states.1\nMajorization Another necessary condition for separability is the Majorization Criterion, which although being less effective in detecting entanglement than the PPT criterion, reveals a thermodynamic\n1\n\nPositive maps are those maps for which L(%) \u2265 0, \u2200%. A positive map L is\ncompletely positive if and only if (In \u2297 L)(%) \u2265 0, \u2200%, n. For classification, only\npositive maps are interesting. It is easy to see that the PPT criterion relies on a\npositive map.\n\n55\n\n\f4. ENTANGLEMENT THEORY\naspect of non-locality. Consider the composite system %AB and its\nreduced density operator %A . Denote by \u03bb(\u03c6) the vector of the\neigenvalues of \u03c6, arranged in decreasing order.%AB is separable if:\n\u03bb(%AB ) \u227a \u03bb(%A )\n\n(4.15)\n\nwhere \u227a is a pre-order relation meaning that the vector \u03bb(%A ) \u2212\n\u03bb(%AB ) lies inside some positive cone so that S(%AB ) \u2265 S(%A )\n[40][41]. This, in turn, implies that if the state is entangled, then\nS(%AB ) \u2264 S(%A ), so once again, we see that Quantum Mechanics\nallows information to be stored in a composite system in a holistic\nmanner, regardless of its parts.\nEntanglement Witnesses In [39], the Horodecki introduced Entanglement Witnesses (EW). An EW is a Hermitian operator W =\nW \u2020 such that:\n\nhW, \u03c3i \u2265 0, for all separable \u03c3\n\n(4.16)\n\nhW, %i < 0, for some entangled %\n\n(4.17)\n\nwhich is a consequence of the Hahn-Banach Theorem [42] in functional analysis, since EW are directly related to positive maps [43]\ndefined on a Banach space. This is valid for algebras of arbitrary\ndimension, so it will prove to be a very useful concept.\nAs stated before, there is a duality between positive maps and operators. In low dimensions (2 \u00d7 2 and 2 \u00d7 3), any positive map admits the\ndecomposition L = S1 + S2 \u25e6 T, and any EW can be written as [43]:\nW = (I \u2297 L)(%AB\n\u03c81 ) = P + (I \u2297 T)Q\n\n(4.18)\n\nWhere P and Q are nonnegative operators. These EW receive the\nname of decomposable EW. For P = 0 and Q = I one gets the PPT\ncriterion. Nevertheless, for higher dimensions there exist EW which are\nnot of the form 4.18. A consequence is that there will be entangled\nstates for which hW, %i \u2265 0. These states are called Positive Partial\nTransposed Entangled States(PPTES), and it was shown in [44], that\nthis kind of entangled states cannot be used in distillation procedures.\n56\n\n\f4.2 Mixed States\nFor this reason, the entanglement contained in them is called Bound\nEntanglement, since it cannot be extracted for communication tasks.\nOther criteria to detect and quantify Bound Entanglement have been\nproposed, such as non-decomposable EW [45], Schmidt number Witnesses [46], Robust Semidefinite Programming [34] and, more recently,\na geometric approach based on separating hyperplanes [47]. We will\npursue this geometric interpretation of entanglement in next section.\n\n4.2.2\n\nQuantification\n\nThere exist several candidates, depending on which criterion one takes\nas more fundamental. Some of them have operational definitions and\nsome have not.\nEntanglement cost, EC It quantifies how many maximally entangled\npairs are needed to generate a given entangled state, minimized\nover all possible dilution protocols:\nn\nTLOCC m\u2192\u221e m\n\nEC (%) = min\n\nlim\n\n(4.19)\n\nwhere n \u2264 m is the number of maximally entangled pairs, %\u2297n\n\u03c81 ,\nwhose entanglement is diluted into m copies of the original state,\n%\u2297m .\nDistillable entanglement, ED It is a measure of how many maximally entangled pairs can be obtained by performing an optimal\ndistillation protocol to an asymptotic number m of copies of the\ngiven state:\nED (%) = max lim\nTLOCC\n\nm\u2192\u221e\n\nn\nm\n\n(4.20)\n\nwith n and m as before.\nRelative entropy of entanglement, ER Analogously to its classical\ncounterpart, ER ([48]) can be thought of as a measure for the\nextent that one can confuse two probability distribution, result\nknown as Sanov's Theorem (see [1]). But this case, it quantifies to\n57\n\n\f4. ENTANGLEMENT THEORY\nwhich amount an entangled state can be taken as separable. The\nrelative entropy of entanglement is:\nER = min S(%||\u03c3) = min T r(% log % \u2212 % log \u03c3)\n\u03c3\u2208S\n\n\u03c3\u2208S\n\n(4.21)\n\nEntanglement of formation, EF Any density operator\nP has a nonunique convex decomposition of the form % =\nk pk %k , where\n%k are rank one density operators. Its entanglement of formation\nis the averaged von Neumann's entropy of those pure states, minimized over all possible convex decompositions:\nEF = min\n\n{pk ,%k }\n\nX\n\npk S(%k )\n\n(4.22)\n\nk\n\nAs for the detection case, it is not known whether this measures are\nequivalent. The values of this quantities are only known for some cases.\nIn the 2\u00d72 case, the entanglement of formation can be exactly computed\nthanks to a measure known as concurrence[49]. For any entanglement\nmeasure E(%), ED (%) \u2264 E(%) \u2264 EC (%). For bound entangled states,\nthis is trivially satisfied.\n\n4.3\n\nGeometric Insights into Entanglement\n\nThe set of all density operators is a convex set, which follows from\nprobability arguments. Mixing cannot increase entanglement, hence the\nset of separable density operators is also convex. We will denote this\ntwo sets by D and S, respectively.\nIt is easy to see that entanglement witnesses constitute hyperplanes\nwhich split D into two subsets, one of which strictly contains S. Let\nW = W \u2020 be an EW, then:\nhW, %i < 0, for some % \u2208 D \\ S\n\n(4.23)\n\nhW, \u03c3i \u2265 0, \u2200\u03c3 \u2208 S\n\n(4.24)\n\n58\n\n\f4.3 Geometric Insights into Entanglement\n\nFigure 4.1: Set of composite states D. S is a convex subset containing\nall states that exhibit classical correlations. D \\ S contains all states\nexhibiting quantum correlations. For a given density operator in D,\nits distance to the separable set is a measure of the entanglement that\nit contains. An Entanglement Witness will separate S from a convex\nsubset of D \\ S.\nClearly this defines a hyperplane dividing D: hW, \u03c3i \u2265 hW, %i. In\nour notation, an EW is optimal if (4.23) holds for the largest number of\n%'s. Intuitively, such an EW will be tangent to S (see fig 4.1). We will\nillustrate this fact in a moment.\nA geometric measure of the entanglement contained in a state is its\ndistance to the set of separable density operators. The distance of a\ndensity operator % to the separable set S is:\nD = min k% \u2212 \u03c3k\n\u03c3\u2208S\n\n4.3.1\n\n(4.25)\n\nDuality between Detection and Quantification\n\nIt is a general result from geometric optimization that the problem of\nfinding a separating hyperplane between a point p and a convex set C\n59\n\n\f4. ENTANGLEMENT THEORY\nis dual to the problem of finding the distance between C and p [50]. In\nmatrix space language, this duality can be illustrated as follows. The\nproblem (refdistance) can be expressed as:\n\nk\u03c4 k\n\nmin\n\n(4.26)\n\nsuch that % \u2212 \u03c3 = \u03c4\n\u03c3\u2208S\nwith variables \u03c4 and \u03c3. The Lagrangian of (4.26) is:\nL = k\u03c4 k + hW, % \u2212 \u03c3 \u2212 \u03c4 i\n\n(4.27)\n\nwhere W is the Lagrange multiplicator associated to the equality\nconstraint. It is not hard to see that it represents a a hyperplane. Noting\nthat hW, \u03c4 i \u2264 kW kk\u03c4 k, the dual function can be written as:\n\ng(W ) = min [hW, %i \u2212 hW, \u03c3i + k\u03c4 k(1 \u2212 kW k + \u03b4)]\n\u03c3\u2208S,\u03c4\n\n(4.28)\n\nwhere the parameter \u03b4 \u2265 0 is related to the relative orientation\nbetween the hyperplane represented by W, and the line going from the\nseparable set S to the density operator %, and it is equal to zero if and\nonly if they are perpendicular. For (4.28) to be bounded from below in\n\u03c4 , the additional constraint kW k \u2212 \u03b4 \u2264 1 must be included. So the dual\nproblem of (4.26) is:\n\nmax [min[hW, %i \u2212 hW, \u03c3i]]\n\nkW k\u2212\u03b4\u22641 \u03c3\u2208S\n\n(4.29)\n\nIt is straightforward to check that the optimal value of (4.29) is\nattained if and only if W is an optimal EW. This result, also known\nas the Bertlmann-Narnhoffer-Thirring Theorem (see Ref. [51]), will let\nus trace a link between the entanglement detection and quantification\nproblems (compare also with Refs. [52][53])\n60\n\n\f4.3 Geometric Insights into Entanglement\n\n4.3.2\n\nEllipsoidal Classification\n\nThe basic premise of this method is that the set of separable states S can\nbe approximated by a Minimum Volume Covering Ellipsoid (MVCE) of\nan ensemble of vectors corresponding to some separable density operators. Then, the following classification scheme can be adopted: if a\nvector falls inside the MVCE, it will be taken as separable, and if it falls\noutside, it will be regarded as entangled.\nAn ellipsoid centered at xc can be expressed as:\nE = {x|(x \u2212 xc )T A(x \u2212 xc ) \u2264 1}\n\n(4.30)\n\nwhere A = AT is a positive definite matrix of dimension N 2 \u2212 1. The\nvolume of this ellipsoid is proportional to det(A\u22121/2 ).\nSince in matrix space quadratic forms are not defined, one needs to\nwork in a real vector space to build this ellipsoid. We first obtain an ensemble of \"separable vectors\" by means of tensorially multiplying states\nalong all directions specified by some canonical basis. For instance, in the\n2\u00d72 case, this ensemble would be {xsep\ni } = {(1, 0, 0)\u2297(1, 0, 0), (1, 0, 0)\u2297\n(\u22121, 0, 0), (1, 0, 0)\u2297(0, 1, 0), (1, 0, 0)\u2297(0, \u22121, 0), ..., (0, 0, \u22121)\u2297(0, 0, \u22121)}\n. Later on we will see that it is convenient to vary the norm kxsep\ni k2 of\nthese vectors. This procedure ensures that all vectors will lie as spaced\nas possible in the separable set S. Secondly, we minimize the volume of\nan ellipsoid, constrained to have all generated \"separable vectors\" falling\ninside it. One way to obtain the MVCE of this ensemble would be to\nsolve the following problem:\n\nlog det A\u22121\n\nmin\nsuch that\n\n(xsep\ni\n\n\u2212\n\nxc )T A(xsep\ni\n\n(4.31)\n\n\u2212 xc ) \u2264 1\n\nwith variables A and xc . Here, logarithm was taken in order to\ndrop off proportionality terms. Despite the exponential growth of the\ndimension of the associated vector space, interior point methods used\nfor minimization still converge polynomially to a solution in dimension\nas large as 1000, or more [50].\n61\n\n\f4. ENTANGLEMENT THEORY\n\nFigure 4.2: The vertices of the polytope are the generated \"separable\nvectors\" of which the MVCE is found. The larger set corresponds to the\nwhole space of density operators\n\n4.3.3\n\nResults for 2 \u00d7 2 and 2 \u00d7 3 Systems\n\nThe separability problem is solved for 2 \u00d7 2 and 2 \u00d7 3 systems, thanks\nto the PPT criterion. One can use this fact to benchmark the method.\nThe original problem (4.25) casted as:\n\nmin k% \u2212 \u03c3kF\n\u03c3 TA\n\nsuch that\n\n(4.32)\n\n\u22650\n\nwhich gives the true results. The second problem is to find the\nMVCE through (4.31), and compute the distance to this ellipsoid in a\nsimilar way:\n\nkr \u2212 sk2\n\nmin\nsuch that (s \u2212 xc\n\n)T A(s\n\n(4.33)\n\n\u2212 xc ) \u2264 1\n\nwhere r and s stand for the vectorized counterparts of % and \u03c3. The\nresults obtained for pure vectors (kxsep\ni k2 = 1) are rather discouraging: whereas none of the generated \"separable vectors\" fell outside the\n62\n\n\f4.3 Geometric Insights into Entanglement\n2 x 2 Systems\nNorm\n\nFalse Positives\n\nFalse Negatives\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n\n962\n868\n687\n484\n287\n180\n92\n32\n5\n0\n\n0\n0\n0\n0\n15\n184\n410\n600\n755\n873\n\nTable 4.1: Number of misclassified vectors in a sample of 1000 \"separable\nvectors\" and 1000 \"entangled vectors\", as a function of the Euclidean\nnorm of the vectors of the separable ensemble\n\nMVCE, only 12.7% of the \"entangled vectors\" are detected. However,\nthe ellipsoid can be shrunk by reducing the norm of the generated ensemble {xsep\ni }. At the expense of letting some \"separable vectors\" fall\noutside the ellipsoid, the number of correctly classified \"entangled vectors\" increases. The event that a true \"separable vector\" falls outside\nthe MVCE will be a false positive, while if an \"entangled vector\" falls\ninside the MVCE, it will be false negative. Stepwise reducing the norm\nof the vectors belonging to the separable ensemble Tables 1 and 2 are\nobtained.\nThere is a trade-off between the number of correctly classified states\nand non-ambiguousness of the test. The relevant area of 2 \u00d7 2 systems\nsep\nis between norms kxsep\ni k2 = 0.6 and kxi k2 = 0.5, as can be seen in\nFig. 4.3. A measure of entanglement ought to be as unambiguous as\npossible, and thus the best choice is kxsep\ni k2 = 0.5, since for this case\nonly about 1.5% of the \"entangled vectors\" are misclassified. For this\nchoice, in general, a vector will be misclassified 15.1% of the time. For\n2 \u00d7 3 systems (see fig. 4.4), the MCVE approximates somewhat less\nefficiently the separable set. However, still 76.8% of the vectors are\ncorrectly classified, in the area comprised between kxsep\ni k2 = 0.5 and\n63\n\n\f4. ENTANGLEMENT THEORY\n2 x 3 Systems\nNorm\n\nFalse Positives\n\nFalse Negatives\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n\n949\n812\n597\n427\n269\n160\n80\n34\n11\n0\n\n0\n0\n0\n52\n196\n389\n572\n699\n807\n900\n\nTable 4.2: Number of misclassified vectors in a sample of 1000 \"separable\nvectors\" and 1000 \"entangled vectors\", as a function of the Euclidean\nnorm of the vectors of the separable ensemble\nkxsep\ni k2 = 0.4. In these systems, it misclassifies at least 5.2% of the\n\"separable vectors\".\n\n4.3.4\n\nPseudo-Entanglement Witnesses\n\nFor a vector space endowed with the Euclidean norm, there is a simple\nway to construct a tangent hyperplane to a given ellipsoid. We can use\nthis fact to build realistic observables amenable to a laboratory setting.\nThe tangent hyperplane to the ellipsoid can be expressed as:\n\u2207x [(x \u2212 xc )T A(x \u2212 xc ) \u2212 1]s0 (r \u2212 s0 ) = 0\n\n(4.34)\n\nwhere s0 = PE (r) is the projection of the vectorized density operator\nunder study onto the MVCE. It can be expressed in affine form as:\n(s0 \u2212 xc )T A(r \u2212 xc ) = 1\n\n(4.35)\n\n(compare with Ref. [54]). It is important to keep in mind that,\n64\n\n\f4.3 Geometric Insights into Entanglement\n\nFigure 4.3: False Negatives versus False Positives for 2 \u00d7 2 systems,\nshowing that there exists an area where the probability of wrongly classifying a vector can be brought down to 15.1%, between kxsep\ni k2 = 0.6\nsep\nand kxi k2 = 0.5\nalthough the hyperplanes introduced in (4.35) very much resemble an\nEntanglement Witness, they are not so in general. This is because the\nMCVE may in general be a proper subset of the separable set S, and\nno tangent hyperplane to this MVCE will strictly separate S from any\nentangled state. Nevertheless, these Pseudo-EW can be used to estimate\nthe amount of entanglement contained in a given entangled matrix % via\n(4.29), which at the optimal value will be equal to (4.26)[50]. For an\nillustration of entanglement estimation see fig. 4.5.\n\n4.3.5\n\nBound Entanglement Detection\n\nFor composite systems of dimension higher than 6, there is a special\nkind of entangled states that cannot be used, in principle, to enhance\ncommunication. The entanglement contained in these states cannot be\ndistilled to obtain pure entangled states [44], and it receives the name\nof Bound Entanglement (BE). The PPT criterion fails to detect this\nkind of states, and it just becomes a necessary condition for quantum\ncorrelations to arise. Other criteria to detect and quantify BE have\nbeen proposed, such as non-decomposable EW [45], Schmidt number\nWitnesses [46], and, more recently, a geometric approach based on sep65\n\n\f4. ENTANGLEMENT THEORY\n\n'\nFigure 4.4: False Negatives versus False Positives for 2 \u00d7 3 systems. The\nerror probability can be reduced to 23.2%, between kxsep\ni k2 = 0.5 and\nsep\nkxi k2 = 0.4\narating hyperplanes [47].\nThe MVCE approach is in the spirit of the latter of the aforementioned methods, but instead of hyperplanes, we shall use the MVCE in\norder to detect BE. Intuitively, the ellipsoid covering a set of \"separable\nvectors\" should leave bound entangled states on its outside. This fact\nis studied in 3X3 systems, where a parametrization of bound entangled\nstates, due to P. Horodecki, is available [55]. These states %BE depend\non a scalar a \u2208 [0, 1], and are given by:\n\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n1 \uf8ec\n\uf8ec\n%BE (a) =\n\uf8ec\n8a + 1 \uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\na\n0\n0\n0\na\n0\n0\n0\na\n\n0\na\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\na\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\na\n0\n0\n0\n0\n0\n\na\n0\n0\n0\na\n0\n0\n0\na\n\n0\n0\n0\n0\n0\na\n0\n0\n0\n\n0\n0\n0\n0\n0\n0\n1+a\n2\n\u221a\n\n0\n1\u2212a2\n2\n\n0\n0\n0\n0\n0\n0\n0\na\n0\n\na\n0\n0\n0\na\n0\n\n\u221a\n1\u2212a2\n2\n\n0\n1+a\n2\n\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\nSurprisingly, for norms of the generated separable ensemble of 0.6\n66\n\n\f4.3 Geometric Insights into Entanglement\n\nFigure 4.5: For 100 random \"entangled vectors\" of a 2 \u00d7 2 system, the\ncontinuous black line is the true distance to the separable set S, while\nthe dashed line stands for the distance of the vectors for a MVCE of\n\"separable vectors\" of norm kxsep\ni k2 = 0.5. At the bottom, the pointed\nline represents the distances obtained for norm kxsep\ni k2 = 1\nand below, all bound entangled states are detected. The obtained results\nare shown in Table 3.\nAs expected, the distance to the MVCE of the detected states linearly depends on the norm of the associated density operator. This\ninterdependence is depicted in Fig. 4.6\n\n67\n\n\f4. ENTANGLEMENT THEORY\n\n3 x 3 Systems\nNorm\n\nDetected States\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n\n1000\n1000\n1000\n1000\n1000\n1000\n226\n149\n107\n79\n\nTable 4.3: 1000 bound entangled states were generated, with parameter\n\"a\" running from 0.001 to 1. The distances of the associated vectors\nto the different MVCEs were obtained. For norms of the separable\nensemble of 0.6 and below, all bound entangled states were detected\n\nFigure 4.6: There is a linear dependence between the distance to the\nMVCE and the norm of the associated density operator\n\n68\n\n\fPart III\n\nQuantum Information\nTheory\n\n69\n\n\f\fChapter 5\n\nClassical Information over\nQuantum Channels\nQuantum Information Theory harbors the possibility of enhancing communication with the help of an genuinely quantum resource: quantum\nentanglement. Entanglement thus should be regarded as new kind of information. Hence it is natural to expect that different capacities can be\ndefined, depending on which kind of information (classical or quantum)\nis to be sent over a channel.\nThe classical capacity C is the asymptotical rate at which classical\ninformation can be transmitted through a quantum channel. Depending\non whether we allow for quantum or classical coding and decoding, the\nclassical capacity unfolds into four different capacities [56] (see fig. 5.1).\nWe will obviate this fact and consider only a general C, which will be the\nlargest capacity among all coding and decoding possibilities. There is\nanother classical capacity, called entanglement-assisted classical capacity\nCE . It is the rate at which classical information can be sent over a\nquantum channel provided that sender and receiver share an unlimited\namount of entangled pairs. We will see that this capacity is larger than\nC.\nThere is also a quantum capacity Q1 for transmitting intact quantum general states. Still there is a classically-assisted quantum capacity,\nQ2 , for transmitting intact quantum states in parallel with a classical\nfeedback channel, which permits sender and receiver to perform coor71\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\ndinated local operations to fight noise. The characterization of these\ntwo quantum capacities is important because it determines how much\nentanglement can be conveyed through a channel.\nHere we will focus only on the former capacities, C and CE , since\nthey dwell more in the spirit of classical information theory, and are also\nbetter understood. The results of quantum source coding [57][58], will\nalso not be treated here for conciseness\n\n5.1\n\nQuantum Asymptotic Equipartition Property\n\nIn analogy with i.i.d. processes in Chapter 2, it is possible to derive\ntypicality results for quantum systems. Suppose that a device outputs\na system in state \u03c6i with probability pi , with \u03c6i necessarily orthogonal\nrank one density operators1 . The entropy of the system will be S(% =\nP\ni pi \u03c6i ) = H(p). Now consider the m-fold tensor product \u03c6I = \u03c6i1 \u2297\n\u03c6i2 \u2297 . . . \u2297 \u03c6im , and call it sequence density operator. The eigenvectors\nof this sequence density operator live in the space H\u2297m = H1 \u2297 H2 \u2297\n. . . \u2297 Hm . Denote by pI = pi1 pi2 . . . pim the product of all probabilities\ncorresponding to a given sequence. The sequence \u03c6I will be typical if:\n|\u2212\n\n1\nlog pI \u2212 S(%)| \u2264 \u000f, \u2200\u000f\nm\n\n(5.1)\n\nLikewise, it is possible to define the typical subspace of H\u2297m as the\nsubspace \u039bT spanned by the eigenvectors of all typical sequences. The\northogonal projector onto this subspace, \u03a0\u039b , has the following properties:\nh\u03c6I , \u03a0\u039b i \u2265 1 \u2212 \u03b4\n\n(5.2)\n\nh\u03c6I \u03a0\u22a5\n\u039b i \u2264 \u03b4, \u2200\u03b4\n\n(5.3)\n\nthat is, for sufficiently large m, almost all the probability is contained\nin the typical subspace. The dimension of the typical subspace will be\n1\n\nThis can always be done by diagonalizing the density operator\n\n72\n\n\f5.1 Quantum Asymptotic Equipartition Property\nbounded by:\n2m(S(%)\u2212\u000f) \u2264 T r(\u03a0\u039b ) \u2264 2m(S(%)+\u000f)\n\n(5.4)\n\n2\u2212m(S(%)+\u000f) \u2264 pI \u2264 2\u2212m(S(%)\u2212\u000f)\n\n(5.5)\n\nor equivalently,\n\nwhich means that as m grows, all typical sequences will tend to be\nequiprobable.\n\n5.1.1\n\nEntanglement Distillation\n\nThese typicality results will allow us to prove Theorem 11. For this we\nwill state the following lemma, whose proof can be found in [40]:\nLemma 1 A bipartite pure state \u03c6AB can be transformed into another\npure state %AB by LOCC if and only if the eigenvalues of their reduced\ndensity operators satisfy \u03bb(\u03c6A ) \u227a \u03bb(%A )\nProof 14 (Proof of Theorem 11) Suppose we have (\u03c6AB )\u2297m such\nP\nthat its reduced density operator is (T rB \u03c6AB )\u2297m =\nI pI \u03c6I . As m\ngrows, the eigenvalues of this density operator will satisfy eq. 5.5.\nThe reduced density operator of a maximally entangled state has maximum entropy S(T rB %AB\n\u03c81 ) = log 2 = 1. Now consider n copies of the\n\u2297n . As n grows, its eigenvalues of\nmaximally entangled state, (%AB\n\u03c81 )\n\u2297n will also satisfy eq.5.5, so they will be constrained to take\n(T rB %AB\n\u03c81 )\nvalues arbitrarily close to 2\u2212n .\nWe have the inequality:\nA )\u00b1\u000f)\n\n2\u2212m(S(\u03c6\n\n0\n\n\u2264 2\u2212n\u00b1\u000f\n\n(5.6)\n\nn\n, then it will\nand by previous lemma, if the entropy S(T rB \u03c6AB ) \u2248 m\nbe possible to transform m copies of the arbitrarily entangled state \u03c6AB\n\n73\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\ninto n copies of %AB\n\u03c81 . This argument can be carried out symmetrically\nconsidering the other subsystem.\n\n5.2\n\nQuantum Channels\n\nA quantum channel C is a map from one algebra into another:\nC : A \u2212\u2192 B\nClassically, a channel induces some noise due to the stochastic nature of its associated transition matrix. In Quantum Mechanics time\nevolution of a closed system is completely deterministic. However, the\nsystem will generally couple to unaccessible degrees of freedom corresponding to dynamical variables of its environment. At the end, only\nthe system will be observed. Tracing out the environment can introduce\nnoise in the resulting density operator. The effect of a channel C onto %\ncan be expressed as:\n\u2020\nC(%) = T rE (U(% \u2297 %E\n1 )U )\n\n(5.7)\n\nwhere %E\n1 is the initial pure state of the environment, and U is some\ntime evolution operator acting on the global algebra, which may, or may\nnot, couple the system and its environment. Some physical requirements\nare:\n\u2022 It should be a completely positive map. This stems from the fact\nthat if %AB is the state of a composite system, and only one of\nthe subsystems is sent over the channel, the result still should be\na density operator. This has profound consequences, such as the\nduality between channels and entangled states.\n\u2022 It should be a trace preserving map, T r(C(%)) = 1. This is the\ndemand that any POVM on the channel's output is normalized to\none1 .\n1\n\nNon-trace preserving maps are interesting to describe measurements as a channel\nfrom algebra of quantum systems to the algebra of classical systems. Non-trace\npreserving maps are also interesting when for some reason there is a probability\n\n74\n\n\f5.2 Quantum Channels\nP\nP\n\u2022 It should be a convex-linear map, C( i pi %i ) =\ni pi C(%i ). In\nother words, channel effects should be regardless of the convex\ndecomposition of the input density operator, pretty much in the\nsame way that classically a transition matrix doesn't depend on\nthe input probability distribution.\nIt turns out [9] that this requirements are necessary and sufficient to\ncome to the operator sum representation of channels:\n\nC(%) =\n\nN\nX\n\nAi %A\u2020i\n\n(5.8)\n\ni=1\n\nwith N \u2264 dim(A)dim(B) and Ai : HA \u2192 HB are called Kraus\noperators. The cannel is trace preserving, hence:\nN\nX\n\nA\u2020i Ai = 1\n\n(5.9)\n\ni=1\nN\nConsider an orthogonal resolution of the identity {%E\ni = |ei ihei |}i=1\n1\nfor the environment and suppose for simplicity that % = |\u03c7ih\u03c7|. Time\nevolution will correlate the state with its environment. Since the environment cannot be measured, this correlation cannot be exploited.\nTracing out the environment it is easy to check that:\n\nT rE (U(|\u03c7ih\u03c7| \u2297 |e1 ihe1 |)U\u2020 ) =\n=\n\nN\nX\nhei |[U(|\u03c7ih\u03c7| \u2297 |e1 ihe1 |)U\u2020 ]|ei i =\ni=1\n\n=\n\nN\nX\n\nAi %A\u2020i\n\n(5.10)\n\ni=1\n\nwith Ai = hei |U|e1 i. There is a straightforward interpretation of\neq. 5.8. From linearity and the assumption that the channel is trace\nleakage in the channel, i.e. when sometimes the channel produces no output at all,\nso T r(C(%)) \u2264 1\n1\nHere we assume implicitly that at most d2 dimensions are necessary to model\nthe environment for one system of dimension d\n\n75\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\npreserving, T r(Ai %A\u2020i ) = pi is the probability that the channel outputs\n%i . Using Bayes' rule we have that %i =\nC(%) =\n\nX\n\nAi %A\u2020i\n\nT r(Ai %A\u2020i )\n\n, so:\n\npi %i\n\n(5.11)\n\ni\n\nThus, the stochastic behavior of the channel comes from the interaction of the sent system with its environment. In fact, for ideal channels,\ni.e. for cases where the system and its environment don't interact, evolution is unitary and only one Kraus operator is needed to define the\nchannel. However this is not of much relevance, since the mere fact of\nobserving a system can be described as a highly non-ideal channel(see\n2.3.1).\n\n5.3\n\nClassical Capacity of a Quantum Channel\n\nIn a classical communication setting, the inference capability of the receiver is related to the mutual information between the sender and receiver's probability distributions. This capability will depend on the\nnature of the channel: for a noiseless channel the mutual information\nattains a maximum I(pX ; pX\u0302 ) = H(pX ), with pX\u0302 = f (qY ). For a noisy\nchannel, mutual information can be maximized by finding the optimal\nprobabilities of the input code.\nFor some reason, one might want to use quantum states to encode\nclassical bits. What makes this scenario interesting is that the quantum\nstates states may not in general be orthogonal. In fact, this can be\ndesirable in some cases [59][60]. Then, the Holevo's bound (see section\n2.3.3) is telling us that the maximum accessible information is bounded\nby:\nX\n\nI(X : Y ) \u2264 S(% ) \u2212\n\nn\nX\n\npX\ni S(%i )\n\n(5.12)\n\ni=1\n\nThis suggests that, in contrast to classical channel coding, quantum\nchannel coding demands that two optimizations be carried out to find\nthe optimal performance of a channel. First, the optimal measurement\nstrategy that maximizes 5.12 must be found, this is a search in the set\n76\n\n\f5.3 Classical Capacity of a Quantum Channel\n\nFigure 5.1: Quantum Channel. Two classical bits are encoded in four\nqubits and sent over the channel. Channel inputs can be entangled or\nnot. The measurement at the decoder usually is over the joint qubit\nsequence.\nM. In second place, the optimal input probability is to be found in the\nclassical probability simplex Pn , just as in the classical case. We can\nnow define the classical capacity of a quantum channel as:\nC = max max I(X : Y )\npX\n\nM\n\n(5.13)\n\nsuch that M(%X ) = qY . A direct consequence of the No-Cloning\nTheorem (see 2.3.2) is that, when the system appears at the receiver's\nside, it must have disappeared at the sender's side. Since the random\nvariables X and Y are directly related to the same system %X at different times, a joint probability distribution is lacking a true consistent\nmeaning. Hence, the concept of mutual information is meaningless, as\nlong as it refers to the information that one systems contains about itself\nprior to having been sent through a channel. As we will see, this subtlety precludes the use of joint typicality arguments in coding theorems\nof quantum channels.\nThe optimal measurement strategy is given by the Holevo's bound,\nso we have:\n\nC = max S(%X ) \u2212\npX\n\nn\nX\ni=1\n\n77\n\npX\ni S(%i )\n\n(5.14)\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\nFollowing [61] [62] [63], we state a coding theorem for quantum noisy\nchannels:\nTheorem 12 [Holevo-Schumacher-Westmoreland Theorem] A quantum\nnoisy channel C : A \u2212\u2192 B can be used to transmit information reliably\nif and only if R \u2264 C, with capacity defined as:\n\nC = max S(C(%X )) \u2212\npX\n\nn\nX\n\npX\ni S(C(%i ))\n\n(5.15)\n\ni=1\n\nwhere %i are the input states and \u03c3i = C(%i ) represent the states to\nbe measured by the decoder.\nProof 15 (\u21d2Proof of Achievability) Suppose that the message w =\n(i1 , i2 , ..., im ) is to be sent. The sender will construct %w = %i1 \u2297 %i2 \u2297\n.... \u2297 %im and the channel's output will be \u03c3w = C\u2297m (%w ).\nThe probability of successfully identifying \u03c3w is h\u03c3w , Mw i, where Mw\nis a measurement for index w. An error will be declared with probability:\n\n\u2212mR\n\npe = 2\n\nmR\n2X\n\n(1 \u2212 h\u03c3w , Mw i)\n\n(5.16)\n\nw=1\n\nClassically, one could resort to joint typicality arguments to build a\nproof. Since quantum physics prevents us from considering the mutual\ninformation of two distributions that exist at different times, we cannot\nfollow this way. Instead we will consider two different applications of\ntypicality, one concerning which sequence density operators will be more\nlikely (much like in the classical setting), and the other sort of quantifying how many sequences can be considered to be \"close\" to a fixed\nsequence density operator.\nPn\nLet \u03c3\u0304 =\ni pi \u03c3i be an average output of the channel with specP\ntral decomposition \u03c3\u0304 = j \u03bbj |ej ihej |. Consider the m-fold tensor prodP\nuct \u03c3\u0304 \u2297m =\nJ \u03bbJ |eJ iheJ |, with J = (j1 , j2 , ..., jm ). Define \u03a0\u039b =\n78\n\n\f5.3 Classical Capacity of a Quantum Channel\n|eJ iheJ | as the projector onto the typical subspace \u039bT \u2208 H \u2297m\nspanned by the eigenvectors of all typical sequence density operators:\nP\n\nJ\u2208T\n\nT = {J : 2\u2212m(S(\u03c3\u0304)+\u000f) \u2264 \u03bbJ \u2264 2\u2212m(S(\u03c3\u0304)\u2212\u000f) }\n\n(5.17)\n\nh\u03c3\u0304 \u2297m , \u03a0\u039b i \u2265 1 \u2212 \u03b4\n\n(5.18)\n\nThen:\n\nNow, let \u03c3w be the output sequence of the channel. It has a spectral\ndecomposition:\n\n\u03c3w =\n\nX\n\nw\nw\n\u03bbw\nJ |eJ iheJ |\n\n(5.19)\n\nJ\n\n\u03c3w will be the tensor product of about mp1 copies of \u03c31 , mp2 copies of\n\u03c32 , and so on... Define the average per symbol entropy of the sequence\nP\nas S\u0304(\u03c3w ) = i pi S(\u03c3i ). Interchanging the two definitions of entropy,\nP\nwe build the projector \u03a0w = J\u2208Tw |eJ iheJ |, where:\n\u2212m(S\u0304(\u03c3w )\u2212\u000f)\nTw = {J : 2\u2212m(S\u0304(\u03c3w )+\u000f) \u2264 \u03bbw\n}\nJ \u22642\n\n(5.20)\n\nh\u03c3w , \u03a0w i \u2265 1 \u2212 \u03b4\n\n(5.21)\n\nThen:\n\nNext thing to do is to define the POVM associated to the decoder\nmR\nM = {Mw }2w=1 . Each component of the POVM should be very close to\nthe typical projector \u03a0w . Since only typical sequences will be assigned a\ncodeword:\n\nX\nX\n1\n1\nMw = (\n\u03a0\u039b \u03a0w0 \u03a0\u039b )\u2212 2 \u03a0\u039b \u03a0w \u03a0\u039b (\n\u03a0\u039b \u03a0w 0 \u03a0\u039b ) \u2212 2\nw0\n\nw0\n\n79\n\n(5.22)\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\nwhich makes sure that no non-typical sequence will be considered inside the typical subspace of the sequence \u03c3w . The generalized inverse\nsquare roots 1 are introduced for normalization.\nNow that the two concepts of typicality are introduced, return to eq.\n5.16:\n\npe = 2\n\n\u2212mR\n\nmR\n2X\n\n(1 \u2212\n\nX X\n\n2\n\u03bbw\nJ \u03b1(w,J),(w,J 0 ) )\n\nJ J 0 \u2208Tw\n\nw=1\n2mR\n\n\u2264 2\u2212mR\n\nX X\nX\n2\n(\n\u03bbw\n\u03bbw\nJ (1 \u2212 \u03b1(w,J),(w,J) ) +\nJ)\nJ6\u2208Tw\n\nw=1 J\u2208Tw\n\n\u2264 2\u2212mR\n\nmR\n2X\n\n(2\n\nw=1\n\nX\n\nX\n\n\u03bbw\nJ (1 \u2212 \u03b1(w,J),(w,J) ) +\n\n\u03bbw\nJ ) (5.23)\n\nJ6\u2208Tw\n\nJ\u2208Tw\n\nwhere\n\nX\n1\n0\n\u03b1(w,J),(w0 ,J 0 ) = hew\n\u03a0\u039b \u03a0w00 \u03a0\u039b )\u2212 2 \u03a0\u039b |ew\nJ |\u03a0\u039b (\nJ0 i\nw00\n\nThe first inequality follows from omitting some non-positive cross\nP\nterms and the relation J \u03bbw\nJ = 1. The second inequality comes from\nconsidering the componentwise inequality (1 \u2212 x)2 \u2264 (1 + x)(1 \u2212 x) \u2264\n2(1 \u2212 x), x \u2208 [0, 1].\nOnce we realize that the \u03b1(w,J),(w0 ,J 0 ) are the entries of the square\nw0\nroot of the Gram matrix \u0393 = [hew\nJ |\u03a0\u039b |eJ 0 i] = [\u03b3(w,J),(w0 ,J 0 ) ], it is possible\nto express first term of the last member in eq. 5.23 as:\n\n1\n\n1\n\n1\n\nThe operator X \u2212 2 is equal to 0 on KerX and equal to (X 2 )\u22121 on KerX \u22a5\n\n80\n\n\f5.3 Classical Capacity of a Quantum Channel\n\n2\n\nmR\n2X\n\nX\n\n\u03bbw\nJ (1 \u2212 \u03b1(w,J),(w,J) )\n\nw=1 J\u2208Tw\n1\n\n= 2T r(w,J) (\u039b(I \u2212 \u0393 2 ))\n1\n\n= T r(w,J) (\u039b(I \u2212 \u0393 2 )2 ) + T r(w,J) (I \u2212 \u0393)\n\u2264 T r(w,J) (\u039b(I \u2212 \u0393)2 ) + T r(w,J) (I \u2212 \u0393)\n=\n\nmR\n2X\n\nX\n\n\u03bbw\nJ [2\n\n=\n\nX\n\nX\n\nw0 =1\n\nJ 0 \u2208Tw\n\n\u2212 3\u03b3(w,J),(w,J) +\n\nw=1 J\u2208Tw\nmR\n2X\n\nmR\n2X\n\n(\u03b3(w,J),(w0 ,J 0 ) \u03b3(w0 ,J 0 ),(w,J) )]\n\n2\n\u03bbw\nJ [2 \u2212 3\u03b3(w,J),(w,J) + \u03b3(w,J),(w,J) +\n\nX X\nw0 6=w\n\n2\n\u03b3(w,J),(w,J\n0)\n\nJ 0 6=J\n\nw=1 J\u2208Tw\n\n+\n\nX\n\n2\n\u03b3(w,J),(w\n0 ,J 0 ) ]\n\n(5.24)\n\nJ 0 \u2208Tw0\n\nwhere \u039b = diag(\u03bbw\nJ ), and \"T r(w,J) \" denotes the trace with respect\nto this joint index of the Gram matrices, instead of the usual trace over\nthe dimension of density operators. Using the fact that 2 \u2212 3x + x2 \u2264\n2 \u2212 2x, x \u2208 [0, 1], we see that that 5.24 is upper-bounded by:\n\n\u2212mR\n\npe \u2264 2\n\nmR\n2X\n\nX\nX\n2\n{\n\u03bbw\n\u03b3(w,J),(w,J\n0)\nJ [2 \u2212 2\u03b3(w,J),(w,J) +\n\nw=1\n\n+\n\nw0 6=w\n\nJ 0 6=J\n\nJ\n\nX X\n\n2\n\u03b3(w,J),(w\n0 ,J 0 ) ] +\n\nX\n\n\u03bbw\nJ}\n\n(5.25)\n\nJ6\u2208Tw\n\nJ 0 \u2208Tw0\n\nNote that we expanded the range of the sum from J \u2208 Tw to all J.\nSome algebra shows that it is equivalent to:\n81\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\n\npe \u2264 2\n\n\u2212mR\n\nmR\n2X\n\n{2T r(\u03c3w (1 \u2212 \u03a0\u039b )) + T r(\u03c3w (1 \u2212 \u03a0\u039b )\u03a0w (1 \u2212 \u03a0\u039b ))\n\nw=1\n\n+\n\nX\n\nT r(\u03a0\u039b \u03c3w \u03a0\u039b \u03a0w0 ) + T r(\u03c3w (1 \u2212 \u03a0w ))}\n\nw0 6=w\n\n\u2264 2\n\n\u2212mR\n\nmR\n2X\n\n{3T r(\u03c3w (1 \u2212 \u03a0\u039b )) +\n\nX\n\nT r(\u03a0\u039b \u03c3w \u03a0\u039b \u03a0w0 ) + T r(\u03c3w (1 \u2212 \u03a0w ))}\n\nw0 6=w\n\nw=1\n\n(5.26)\nw\nhere we used that hew\nJ |eJ 0 i = 0 to introduce the tautology \u03a0\u039b = 1\u22121+\n\u03a0\u039b . The last inequality follows from the fact that T r(\u03c3w (1 \u2212 \u03a0\u039b )\u03a0w (1 \u2212\n\u03a0\u039b )) \u2264 T r(\u03c3w (1 \u2212 \u03a0\u039b )\u03a0w ).\n\nFinally, applying the concept of random coding to symmetrize over\nall codewords, we see that:\n\npe =\n\nX\n\np(C)pe (C)\n\nC\n\n\u2264\n\nX\n\n\u2212mR\n\np(C){2\n\nC\n\nX\n\nmR\n2X\n\n[3T r(\u03c3\u0304 \u2297m (1 \u2212 \u03a0\u039b )) +\n\nw=1\n\nT r(\u03a0\u039b \u03c3\u0304\n\n\u2297m\n\n\u03a0\u039b \u03a010 ) + T r(\u03c31 (1 \u2212 \u03a01 ))]}\n\nw0 6=w\n\n\u2264 4\u03b4 + (2mR \u2212 1)T r(2\u2212m(S(\u03c3\u0304)\u2212\u000f) \u03a010 )\n\u2264 4\u03b4 + (2mR \u2212 1)2\u2212m(S(\u03c3\u0304)\u2212\u000f) 2m(S\u0304(\u03c3)+\u000f)\n\n(5.27)\n\nwhere we used typicality arguments of eqs. 5.4, 5.18, 5.21, and the\nfact that \u03a0\u039b \u03c3\u0304 \u2297m \u03a0\u039b \u2264 2\u2212m(S(\u03c3\u0304)\u2212\u000f) 1. This proves that whenever R \u2264\nS(\u03c3\u0304) \u2212 S\u0304, the probability of error goes to zero as m grows.\nProof 16 (\u21d0Weak Converse) To prove that if R > S(\u03c3\u0304) \u2212 S\u0304, the\nerror is bounded away from zero, we will use Fanno's inequality (see eq.\n1.30), as in the classical case:\n82\n\n\f5.3 Classical Capacity of a Quantum Channel\n\nH(W |Y m ) \u2264 mPe R + 1 = m\u000fm\n\n(5.28)\n\nTechnically, we are going to prove that when the error goes to zero,\nthen the rate must necessarily be less than the capacity. Assuming that\nmessages are equiprobable:\n\nmR = H(pW )\n= I(W, Y m ) + H(W |Y m )\n\u2264 I(W, Y m ) + m\u000fm\n\u2264 S(\n\nmR\n2X\n\nw=1\n\n\u2264\n\nm\nX\ni=1\n\n2mR\n\nX S(\u03c3w )\n\u03c3w\n)\u2212\n+ m\u000fm\nmR\n2\n2mR\nw=1\n\nmR\n\n2mR\n\n2X\nX \u03c3w\nS(\u03c3iw )\ni\n)\n\u2212\n] + m\u000fm\n[S(\n2mR\n2mR\n\n\u2264 m[S(\n\nw=1\n\nw=1\n\n2mR\n\n2mR\n\nw=1\n\nw=1\n\nX \u03c3w\nX S(\u03c3 w )\ni\ni\n)\n\u2212\n] + m\u000fm\n2mR\n2mR\n\n\u2264 mC + m\u000fm\n\nSecond and third inequality follow from the Holevo's bound and the\nsubadditivity of von Neumann's entropy, respectively. Last inequality is\ndue to the definition of the capacity, since all terms in the sum are no\ngreater than the capacity as defined in eq. 5.14. Thus, we proved that\nif R \u2265 C as m, then the error must be bounded away from zero as m\ngrows.\n\nFor trace preserving channels (the ones being considered here), it was\nfound that transmitting entangled states does not increase the capacity\n[64].\n83\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\n\n5.4\n\nEntanglement-Enhanced Classical Communication\n\nThe main goal of this dissertation was to argue that entanglement can\nbe used to increase the classical capacity of information transfer. As an\nexample, consider the Quantum Erasure Channel (QEC). The QEC is a\nmap from an algebra of dimension N to an algebra of dimension N + 1.\nIt maps an input state to itself with probability 1\u2212\u000f. With probability \u000f\nthe channel maps its input state to an erasure symbol state, orthogonal\nto all input states. For the qubit case, the QEC would take as input\nstates %0 = |0ih0|, %1 = |1ih1| to %0 = |0ih0|, %1 = |1ih1|, and %2 = |2ih2|,\nwith h0, 2i = h1, 2i = 0. The classical capacity of this binary erasure\nchannel is given by [1]:\nC =1\u2212\u000f\n\n(5.29)\n\nIt was already shown in 3.4.2 that sharing a maximally entangled\npair permits to send two classical bits encoded in just one qubit. If the\nsender and receiver share an unlimited amount of maximally entangled\npairs, it is possible for the sender to pre-process its entangled subsystem\nin such a way that the total entropy of the state will be:\nST = S(T rB %AB\n\u03c8 ) + log 2\n\n(5.30)\n\nwhere the first term is the von Neumann's entropy of the reduced\ndensity operator, which is at a maximum for EPR pairs, and zero for\nseparable states. The second term is due to the choice of sender between\n%0 and %1 1 . It will always be larger than the entropy of any classical\nstate. The CE of the QEC is given by [65]:\nC = 2(1 \u2212 \u000f)\n\n(5.31)\n\nNow we come to the point where it is possible for us to state that\n[66]:\nThe entanglement-assisted classical capacity will be always\nlarger than the unassisted classical capacity.\n1\n\nIt assumes that both symbols are equiprobable, which maximizes capacity.\n\n84\n\n\f5.4 Entanglement-Enhanced Classical Communication\n\nFigure 5.2: Both Capacities C and CE versus the probability of error, \u000f\n\n85\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\n\n86\n\n\fConclusions and Final\nWords\nI feel somewhat ashamed of not having included important topics such as\nquantum source coding, or the duality between channels and entangled\nstates. Moreover, at early stages of this thesis I intended to include, as\nwell, an introduction to multiple user quantum information theory, so\nat the end this work has fallen short of what it was meant to be. A\ndecision was made on a basis of time budget, and I hope that this lack\nwill not preclude a self-contained read of the text.\nBesides the aforementioned subjects, I would find it interesting to\nstudy more in depth some other topics such as quantum rate distortion\ntheory, quantum signal processing or quantum cryptography. However\nthis would demand too much more efforts than those expected in a\nmaster's thesis.\nRemarkably, the most technical part of this work was done in the\nscope of convex optimization, which is not to surprise anyone in the\nInformation Theory community. A method for classifying entangled\nand separable states based on a Minimum Volume Covering Ellipsoid\nwas devised by myself (to my knowledge, no one had done this before)\nfor a class project. In a sense, this constitutes the state-of-the-art part\nof the thesis. At the time of writing, a document explaining the method\ncan be found in the arxiv.org database.\nA word is to be said about my previous knowledge of Quantum\nInformation Theory, concerning the fact that I first read a paper on\nthis subject exactly one year ago. Since then, most of my efforts have\nbeen aimed at reaching a positive semidefinite level of expertise in this\n\n87\n\n\f5. CLASSICAL INFORMATION OVER QUANTUM\nCHANNELS\nfield. Thus I would say that my contribution amounts to a compilation\nof the knowledge which I judged essential to understand the role of\nentanglement in classical channel capacities.\n\n88\n\n\fBibliography\n[1] T.M. Cover and J.A. Thomas. Elements of Information Theory.\nWiley-Interscience New York, 2006. 1, 2, 4.2.2, 5.4\n[2] I. Csiszar and J.G. Korner. Information Theory: Coding Theorems\nfor Discrete Memoryless Systems. Academic Press, Inc. Orlando,\nFL, USA, 1982. 1\n[3] R.M. Gray. Entropy and Information Theory. Springer-Verlag New\nYork, Inc. New York, NY, USA, 1990. 1, 1.3\n[4] M. Chiang and S. Boyd. Geometric Programming Duals of Channel\nCapacity and Rate Distortion. IEEE Transcations on Information\nTheory, 50(2):245, 2004. 1.2\n[5] C.E. Shannon. A mathematical theory of communication. Bell\nSystem Technical Journal, 27:379\u2013423, 1948. 1.4\n[6] AS Holevo. Probabilistic Aspects of Quantum Theory.\nHolland, Amsterdam, 1982. 2, 2.1.2, 2.1.2\n\nNorth-\n\n[7] A. Peres. Quantum Theory: Concepts and Methods. Kluwer Academic Pub, 1995. 2\n[8] C.W. Helstrom. Statistical theory of signal detection. Pergamon\nPress New York, 1968. 2\n[9] M.A. Nielsen and I.L. Chuang. Quantum Computation and Quantum Information. Cambridge University Press, 2000. 2, 5.2\n[10] O. Bratteli and D.W. Robinson. Operator Algebras and Quantum\nStatistical Mechanics, Vol. 1. NY: Springer, 1996. 2.1.1\n89\n\n\fBIBLIOGRAPHY\n[11] HD Zeh. What is achieved by decoherence? Arxiv preprint quantph/9610014, 1996. 2.3.1\n[12] E. Joos. Elements of Environmental Decoherence. Arxiv preprint\nquant-ph/9908008, 1999. 2.3.1\n[13] WH Zurek. Pointer basis of quantum apparatus: Into what mixture\ndoes the wave packet collapse? Physical Review D, 24(6):1516\u2013\n1525, 1981. 2.3.1\n[14] A. Einstein, B. Podolsky, and N. Rosen. Can quantum-mechanical\ndescription of physical reality be considered complete? Physical\nReview, 47:777, 1935. 3.1\n[15] D. Bohm. A suggested interpretation of the quantum theory in\nterms of hiddenvariables, I and II. Physical Review, 85(2):180\u2013193,\n1952. 3.1\n[16] JS Bell. On the Einstein-Podolsky-Rosen Paradox. Physics (US)\nDiscontinued with Vol. 4, no. 1, 1968, 1, 1964. 3.2\n[17] J.F. Clauser, M.A. Horne, A. Shimony, and R.A. Holt. Proposed\nExperiment to Test Local Hidden-Variable Theories. Physical Review Letters, 23(15):880\u2013884, 1969. 3.2\n[18] A. Aspect, P. Grangier, and G. Roger. Experimental Realization of\nEinstein-Podolsky-Rosen-Bohm Gedankenexperiment: A New Violation of Bell's Inequalities. Physical Review Letters, 49(2):91\u201394,\n1982. 3.2\n[19] S.L. Braunstein and C.M. Caves. Information-Theoretic Bell Inequalities. Physical Review Letters, 61(6):662\u2013665, 1988. 3.2\n[20] M. Horodecki, J. Oppenheim, and A. Winter. Quantum information\ncan be negative. Arxiv preprint quant-ph/0505062, 2005. 3.3\n[21] I. Devetak and J. Yard. The operational meaning of quantum conditional information. Arxiv preprint quant-ph/0612050, 2006. 3.3\n[22] E. Schrodinger. Probability relations between separated systems.\nProceedings of the Cambridge Philosophical Society, 32:446\u2013452,\n1936. 3.4\n90\n\n\fBIBLIOGRAPHY\n[23] F. Verstraete. A Study of Entanglement in Quantum Information\nTheory. These de Doctorat, Katholieke Universiteit, Leuven, Belgium, 2002. 3.4\n[24] L. Hughston, R. Jozsa, and W. Wootters. A Complete Classification\nof Quantum Ensembles Having a Given Density Matrix. 1993. 3.4\n[25] C.H. Bennett, G. Brassard, C. Cr\u00e9peau, R. Jozsa, A. Peres, and\nW.K. Wootters. Teleporting an unknown quantum state via dual\nclassical and Einstein-Podolsky-Rosen channels. Physical Review\nLetters, 70(13):1895\u20131899, 1993. 3.4.1\n[26] C.H. Bennett and S.J. Wiesner. Communication via one-and twoparticle operators on Einstein-Podolsky-Rosen states. Physical Review Letters, 69(20):2881\u20132884, 1992. 3.4.2\n[27] M.J. Donald, M. Horodecki, and O. Rudolph. The uniqueness theorem for entanglement measures. Journal of Mathematical Physics,\n43:4252, 2002. 4.1\n[28] S. Popescu and D. Rohrlich. Thermodynamics and the measure of\nentanglement. Physical Review A, 56(5):3319\u20133321, 1997. 4.1\n[29] C.H. Bennett, H.J. Bernstein, S. Popescu, and B. Schumacher. Concentrating partial entanglement by local operations. Physical Review A, 53(4):2046\u20132052, 1996. 4.1.1\n[30] L. Gurvits. Classical deterministic complexity of Edmonds' Problem and quantum entanglement. Proceedings of the thirty-fifth annual ACM symposium on Theory of computing, pages 10\u201319, 2003.\n4.2\n[31] AC Doherty, P.A. Parrilo, and F.M. Spedalieri. Distinguishing Separable and Entangled States. Physical Review Letters,\n88(18):187904, 2002. 4.2\n[32] A.C. Doherty, P.A. Parrilo, and F.M. Spedalieri. Complete family\nof separability criteria. Physical Review A, 69(2):22308, 2004. 4.2\n[33] F.G.S.L. Brand\u00e3o and R.O. Vianna. Separable Multipartite Mixed\nStates: Operational Asymptotically Necessary and Sufficient Conditions. Physical Review Letters, 93(22):220503, 2004. 4.2\n91\n\n\fBIBLIOGRAPHY\n[34] F.G.S.L. Brand\u0103o and R.O. Vianna. Robust semidefinite programming approach to the separability problem. Physical Review A,\n70(6):62309, 2004. 4.2, 4.2.1\n[35] D.A. Herrera-Mart\u0131\u0301. Scalable Ellipsoidal Classification for Entangled States. eprint arXiv: 0806.4855, 2008. 4.2\n[36] B.M. Terhal. Detecting Quantum Entanglement. Arxiv preprint\nquant-ph/0101032, 2001. 4.2\n[37] D. Bruss. Characterizing Entanglement. Arxiv preprint quantph/0110078, 2001. 4.2\n[38] A. Peres. Separability Criterion for Density Matrices. Physical\nReview Letters, 77(8):1413\u20131415, 1996. 4.2.1\n[39] M. Horodecki, P. Horodecki, and R. Horodecki. Separability of\nmixed states: necessary and sufficient conditions. Physics Letters\nA, 223(1-2):1\u20138, 1996. 4.2.1, 4.2.1\n[40] MA Nielsen. Conditions for a Class of Entanglement Transformations. Physical Review Letters, 83(2):436\u2013439, 1999. 4.2.1, 5.1.1\n[41] MA Nielsen and J. Kempe. Separable States Are More Disordered\nGlobally than Locally. Physical Review Letters, 86(22):5184\u20135187,\n2001. 4.2.1\n[42] H.W. Alt. Lineare Funktionalanalysis: Eine anwendungsorientierte\nEinf\u00fchrung. Springer, 2002. 4.2.1\n[43] A. Jamiolkowski. Linear transformations which preserve trace and\npositive semidefiniteness of operators. Rep. Math. Phys, 3(4):275\u2013\n278, 1972. 4.2.1\n[44] M. Horodecki, P. Horodecki, and R. Horodecki. Mixed-State Entanglement and Distillation: Is there a Bound Entanglement in\nNature? Physical Review Letters, 80(24):5239\u20135242, 1998. 4.2.1,\n4.3.5\n[45] M. Lewenstein, B. Kraus, JI Cirac, and P. Horodecki. Optimization\nof entanglement witnesses. Physical Review A, 62(5):52310, 2000.\n4.2.1, 4.3.5\n92\n\n\fBIBLIOGRAPHY\n[46] A. Sanpera, D. Bruss, and M. Lewenstein. Schmidt-number witnesses and bound entanglement. Physical Review A, 63(5):50301,\n2001. 4.2.1, 4.3.5\n[47] R.A. Bertlmann and P. Krammer. Geometric entanglement witnesses and bound entanglement. Physical Review A, 77(2):24303,\n2008. 4.2.1, 4.3.5\n[48] V. Vedral and MB Plenio. Entanglement Measures and Purification\nProcedures. Arxiv preprint quant-ph/9707035, 1997. 4.2.2\n[49] W.K. Wootters. Entanglement of Formation of an Arbitrary State\nof Two Qubits. Physical Review Letters, 80(10):2245\u20132248, 1998.\n4.2.2\n[50] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge\nUniversity Press, 2004. 4.3.1, 4.3.2, 4.3.4\n[51] RA Bertlmann, H. Narnhofer, and W. Thirring. Geometric picture of entanglement and Bell inequalities. Physical Review A,\n66(3):32319, 2002. 4.3.1\n[52] F.G.S.L. Brand\u00e3o. Quantifying entanglement with witness operators. Physical Review A, 72(2):22310, 2005. 4.3.1\n[53] J. Eisert, F. Brand\u00e3o, and KMR Audenaert. Quantitative entanglement witnesses. New Journal of Physics, 9(3):46, 2007. 4.3.1\n[54] R.A. Bertlmann, K. Durstberger, B.C. Hiesmayr, and P. Krammer.\nOptimal entanglement witnesses for qubits and qutrits. Physical\nReview A, 72(5):52331, 2005. 4.3.4\n[55] P. Horodecki. Separability criterion and inseparable mixed states\nwith positive partial transposition. Physics Letters A, 232(5):333\u2013\n339, 1997. 4.3.5\n[56] CH Bennett and PW Shor. Quantum information theory. Information Theory, IEEE Transactions on, 44(6):2724\u20132742, 1998. 5\n[57] B. Schumacher. Quantum coding. Physical Review A, 51(4):2738\u2013\n2747, 1995. 5\n[58] R. Jozsa and B. Schumacher. A new proof of the quantum noiseless\ncoding theorem. J Modern Optics, pages 2343\u20132350, 1994. 5\n93\n\n\fBIBLIOGRAPHY\n[59] C.A. Fuchs. Nonorthogonal Quantum States Maximize Classical\nInformation Capacity. Physical Review Letters, 79(6):1162\u20131165,\n1997. 5.3\n[60] A. Peres and W.K. Wootters. Optimal detection of quantum information. Physical Review Letters, 66(9):1119\u20131122, 1991. 5.3\n[61] P. Hausladen, R. Jozsa, B. Schumacher, M. Westmoreland, and\nW.K. Wootters. Classical information capacity of a quantum channel. Physical Review A, 54(3):1869\u20131876, 1996. 5.3\n[62] AS Holevo. The capacity of the quantum channel with general signal\nstates. Information Theory, IEEE Transactions on, 44(1):269\u2013273,\n1998. 5.3\n[63] B. Schumacher and M.D. Westmoreland. Sending classical information via noisy quantum channels. Physical Review A, 56(1):131\u2013138,\n1997. 5.3\n[64] C. King and MB Ruskai. Minimal entropy of states emerging from\nnoisy quantum channels. Information Theory, IEEE Transactions\non, 47(1):192\u2013209, 2001. 5.3\n[65] C.H. Bennett, D.P. DiVincenzo, and J.A. Smolin. Capacities of\nQuantum Erasure Channels. Physical Review Letters, 78(16):3217\u2013\n3220, 1997. 5.4\n[66] C.H. Bennett, P.W. Shor, J.A. Smolin, and A.V. Thapliyal.\nEntanglement-Assisted Classical Capacity of Noisy Quantum Channels. Physical Review Letters, 83(15):3081\u20133084, 1999. 5.4\n\n94\n\n\f"}