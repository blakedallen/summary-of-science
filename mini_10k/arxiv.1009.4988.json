{"id": "http://arxiv.org/abs/1009.4988v1", "guidislink": true, "updated": "2010-09-25T07:33:44Z", "updated_parsed": [2010, 9, 25, 7, 33, 44, 5, 268, 0], "published": "2010-09-25T07:33:44Z", "published_parsed": [2010, 9, 25, 7, 33, 44, 5, 268, 0], "title": "REx: An Efficient Rule Generator", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1009.0800%2C1009.3870%2C1009.2661%2C1009.1075%2C1009.2553%2C1009.3432%2C1009.3154%2C1009.0839%2C1009.5178%2C1009.0580%2C1009.3335%2C1009.0717%2C1009.0767%2C1009.2181%2C1009.1342%2C1009.2944%2C1009.3442%2C1009.5261%2C1009.5717%2C1009.5089%2C1009.2969%2C1009.0825%2C1009.3314%2C1009.1688%2C1009.4988%2C1009.1804%2C1009.4808%2C1009.3998%2C1009.5851%2C1009.6128%2C1009.5414%2C1009.2610%2C1009.1027%2C1009.1805%2C1009.3681%2C1009.5698%2C1009.5454%2C1009.3671%2C1009.1201%2C1009.5433%2C1009.3691%2C1009.4816%2C1009.3765%2C1009.0814%2C1009.1297%2C1009.3477%2C1009.1045%2C1009.2904%2C1009.2312%2C1009.3011%2C1009.2366%2C1009.1386%2C1009.2426%2C1009.2548%2C1009.4197%2C1009.0569%2C1009.5054%2C1009.5955%2C1009.2481%2C1009.0646%2C1009.2952%2C1009.3583%2C1009.2752%2C1009.5746%2C1009.3635%2C1009.2237%2C1009.0412%2C1009.1869%2C1009.4375%2C1009.2861%2C1009.2057%2C1009.3240%2C1009.0257%2C1009.0379%2C1009.2764%2C1009.0246%2C1009.5689%2C1009.1312%2C1009.3563%2C1009.2557%2C1009.4865%2C1009.0365%2C1009.3929%2C1009.4691%2C1009.1245%2C1009.2169%2C1009.1909%2C1009.1800%2C1009.5081%2C1009.2769%2C1009.4794%2C1009.4608%2C1009.3009%2C1009.5141%2C1009.1106%2C1009.1605%2C1009.5856%2C1009.4566%2C1009.4954%2C1009.3381%2C1009.4296&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "REx: An Efficient Rule Generator"}, "summary": "This paper describes an efficient algorithm REx for generating symbolic rules\nfrom artificial neural network (ANN). Classification rules are sought in many\nareas from automatic knowledge acquisition to data mining and ANN rule\nextraction. This is because classification rules possess some attractive\nfeatures. They are explicit, understandable and verifiable by domain experts,\nand can be modified, extended and passed on as modular knowledge. REx exploits\nthe first order information in the data and finds shortest sufficient\nconditions for a rule of a class that can differentiate it from patterns of\nother classes. It can generate concise and perfect rules in the sense that the\nerror rate of the rules is not worse than the inconsistency rate found in the\noriginal data. An important feature of rule extraction algorithm, REx, is its\nrecursive nature. They are concise, comprehensible, order insensitive and do\nnot involve any weight values. Extensive experimental studies on several\nbenchmark classification problems, such as breast cancer, iris, season, and\ngolf-playing, demonstrate the effectiveness of the proposed approach with good\ngeneralization ability.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1009.0800%2C1009.3870%2C1009.2661%2C1009.1075%2C1009.2553%2C1009.3432%2C1009.3154%2C1009.0839%2C1009.5178%2C1009.0580%2C1009.3335%2C1009.0717%2C1009.0767%2C1009.2181%2C1009.1342%2C1009.2944%2C1009.3442%2C1009.5261%2C1009.5717%2C1009.5089%2C1009.2969%2C1009.0825%2C1009.3314%2C1009.1688%2C1009.4988%2C1009.1804%2C1009.4808%2C1009.3998%2C1009.5851%2C1009.6128%2C1009.5414%2C1009.2610%2C1009.1027%2C1009.1805%2C1009.3681%2C1009.5698%2C1009.5454%2C1009.3671%2C1009.1201%2C1009.5433%2C1009.3691%2C1009.4816%2C1009.3765%2C1009.0814%2C1009.1297%2C1009.3477%2C1009.1045%2C1009.2904%2C1009.2312%2C1009.3011%2C1009.2366%2C1009.1386%2C1009.2426%2C1009.2548%2C1009.4197%2C1009.0569%2C1009.5054%2C1009.5955%2C1009.2481%2C1009.0646%2C1009.2952%2C1009.3583%2C1009.2752%2C1009.5746%2C1009.3635%2C1009.2237%2C1009.0412%2C1009.1869%2C1009.4375%2C1009.2861%2C1009.2057%2C1009.3240%2C1009.0257%2C1009.0379%2C1009.2764%2C1009.0246%2C1009.5689%2C1009.1312%2C1009.3563%2C1009.2557%2C1009.4865%2C1009.0365%2C1009.3929%2C1009.4691%2C1009.1245%2C1009.2169%2C1009.1909%2C1009.1800%2C1009.5081%2C1009.2769%2C1009.4794%2C1009.4608%2C1009.3009%2C1009.5141%2C1009.1106%2C1009.1605%2C1009.5856%2C1009.4566%2C1009.4954%2C1009.3381%2C1009.4296&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper describes an efficient algorithm REx for generating symbolic rules\nfrom artificial neural network (ANN). Classification rules are sought in many\nareas from automatic knowledge acquisition to data mining and ANN rule\nextraction. This is because classification rules possess some attractive\nfeatures. They are explicit, understandable and verifiable by domain experts,\nand can be modified, extended and passed on as modular knowledge. REx exploits\nthe first order information in the data and finds shortest sufficient\nconditions for a rule of a class that can differentiate it from patterns of\nother classes. It can generate concise and perfect rules in the sense that the\nerror rate of the rules is not worse than the inconsistency rate found in the\noriginal data. An important feature of rule extraction algorithm, REx, is its\nrecursive nature. They are concise, comprehensible, order insensitive and do\nnot involve any weight values. Extensive experimental studies on several\nbenchmark classification problems, such as breast cancer, iris, season, and\ngolf-playing, demonstrate the effectiveness of the proposed approach with good\ngeneralization ability."}, "authors": ["S. M. Kamruzzaman"], "author_detail": {"name": "S. M. Kamruzzaman"}, "author": "S. M. Kamruzzaman", "arxiv_comment": "4 Pages, International Conference", "links": [{"href": "http://arxiv.org/abs/1009.4988v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.4988v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.4988v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.4988v1", "journal_reference": "Proc. 4th International Conference on Electrical Engineering, The\n  Institution of Engineers, Dhaka, Bangladesh, pp. 79-82, Jan. 2006", "doi": null, "fulltext": "Proceedings of the 4th International Conference on Electrical Engineering\n& 2nd Annual Paper Meet\n26-28 January, 2006\n\nREx: An Efficient Rule Generator\nS. M. Kamruzzaman\nDepartment of Computer Science and Engineering\nManarat International University, Dhaka, Bangladesh\nE-MAIL: smzaman@gmail.com, smk.cse@manarat.ac.bd\nABSTRACT\nThis paper describes an efficient algorithm REx for generating symbolic rules from artificial neural\nnetwork (ANN). Classification rules are sought in many areas from automatic knowledge acquisition to\ndata mining and ANN rule extraction. This is because classification rules possess some attractive features.\nThey are explicit, understandable and verifiable by domain experts, and can be modified, extended and\npassed on as modular knowledge. REx exploits the first order information in the data and finds shortest\nsufficient conditions for a rule of a class that can differentiate it from patterns of other classes. It can\ngenerate concise and perfect rules in the sense that the error rate of the rules is not worse than the\ninconsistency rate found in the original data. An important feature of rule extraction algorithm, REx, is its\nrecursive nature. They are concise, comprehensible, order insensitive and do not involve any weight values.\nExtensive experimental studies on several benchmark classification problems, such as breast cancer, iris,\nseason, and golf-playing, demonstrate the effectiveness of the proposed approach with good generalization\nability.\n1. INTRODUCTION\nANNs have been successfully applied in a variety of\nproblem domains [1]. In many applications, it is\nhighly desirable to extract symbolic classification\nrules from these networks. Unlike a collection of\nweights, symbolic rules can be easily interpreted and\nverified by human experts. They can also provide new\ninsides into the application problems and the\ncorresponding data.\nWhile the predictive accuracy obtained by ANNs is\noften higher than that of other methods or human\nexperts, it is generally difficult to understand how\nANNs arrive at a particular conclusion due to the\ncomplexity of the ANNs architectures [2]. It is often\nsaid that an ANN is practically a \"black box\". Even\nfor an ANN with only single hidden layer, it is\ngenerally impossible to explain why a particular\npattern is classified as a member of one class and\nanother pattern as a member of another class [3].\nThis paper proposes an efficient algorithm REx for\ngenerating symbolic rules from ANN. A three-phase\ntraining algorithm REANN is proposed for\nbackpropagation learning. In the first phase,\nappropriate network architecture is determined using\n\nconstructive and pruning algorithm. In the second\nphase, the continuous activation values of the hidden\nnodes are discretized by using an efficient heuristic\nclustering algorithm. And finally in the third phase,\nrules are extracted by examining the discretized\nactivation values of the hidden nodes using the rule\nextraction algorithm REx.\n2. THE REANN ALGORITHM\nThe aim of this section is to introduce the REANN\nalgorithm for understanding how an ANN solves a\ngiven problem. The major steps of REANN are\nsummarized in Fig. 1 and explained further as\nfollows:\nStep 1 Create an initial ANN architecture. The initial\narchitecture has three layers, i.e. an input, an output,\nand a hidden layer. The number of nodes in the input\nand output layers is the same as the number of inputs\nand outputs of the problem. Initially, the hidden layer\ncontains only one node. Randomly initialize all\nconnection weights within a certain small range. The\nnumber of nodes in the hidden layer is automatically\ndetermined by using a basic constructive algorithm.\nRemove redundant input nodes and connections by\nusing a basic pruning algorithm.\n\n\fKamruzzaman et. al : Proceedings of the 4th ICEE & 2nd APM, January 2006\nStep 2 Discretize the continuous outputs of hidden\nnodes by using an efficient heuristic clustering\nalgorithm.\nStep 3 Generate rules that map the inputs and outputs\nrelationships using rule extraction algorithm REx.\n\nStart\nExtract Rule\nCluster Rule\n\nStart\n\nPrune Rule\n\nDetermine ANN architecture automatically\n\nYes\n\nDiscretize the output values of hidden nodes\n\nCovered all\npatterns?\n\nGenerate rules using REx\n\nNo\nDefault Rule\n\nStop\n\nStop\n\nFig. 1: Flow chart of the REANN algorithm.\n\nFig. 2: Flow chart of the REx algorithm.\n\n3. RULE EXTRACTION ALGORITHM (REx)\n\nStep 3 Prune Rule:\nreplace specific rules with more general ones;\nremove noise rules;\neliminate redundant rules;\nStep 4 Check whether all patterns are covered by\nany rules. If yes then stop, otherwise\ncontinue.\nStep 5 Determine a default rule:\nA default rule is chosen when no rule can be applied\nto a pattern.\n\nClassification rules are sought in many areas from\nautomatic knowledge acquisition [5] [6] to data\nmining [7] and ANN rule extraction. This is because\nclassification rules possess some attractive features.\nThey are explicit, understandable and verifiable by\ndomain experts, and can be modified, extended and\npassed on as modular knowledge. The REx is\ncomposed of three major functions:\ni) Rule Extraction: this function iteratively\ngenerates shortest rules and remove/marks the\npatterns covered by each rule until all patterns\nare covered by the rules.\nii) Rule Clustering: rules are clustered in terms of\ntheir class levels and\niii) Rule Pruning: redundant or more specific rules\nin each cluster are removed.\nA default rule should be chosen to accommodate\npossible unclassifiable patterns. If rules are clustered,\nthe choice of the default rule is based on clusters of\nrules.\n\nREx exploits the first order information in the data\nand finds shortest sufficient conditions for a rule of a\nclass that can differentiate it from patterns of other\nclasses. It can generate concise and perfect rules in the\nsense that the error rate of the rules is not worse than\nthe inconsistency rate found in the original data. The\nnovelty of REx is that the rule generated by it is order\ninsensitive, i.e, the rules need not be required to fire\nsequentially.\n4. EXPERIMENTAL STUDIES\nThis section evaluates the performance of REx on\nseveral\nwell-known\nbenchmark\nclassification\nproblems. These are the breast cancer, iris, season,\nand golf playing. They are widely used in machine\nlearning and ANN research. The data sets representing\nall the problems were real world data and obtained\nfrom the UCI machine learning benchmark repository\n[8]. The characteristics of the data sets are\nsummarized in Table 1.\n\nThe steps of the Rule Extraction (REx) algorithm are\nsummarized in Fig. 2, which are explained further as\nfollows:\nStep 1 Extract Rule:\ni=0; while (data is NOT empty/marked){\ngenerate Ri to cover the current pattern and\ndifferentiate it from patterns in other categories;\nremove/mark all patterns covered by Ri ; i++}\nStep 2 Cluster Rule:\nCluster rules according to their class levels. Rules\ngenerated in Step 1 are grouped in terms of their class\nlevels. In each rule cluster, redundant rules are\neliminated; specific rules are replaced by more\ngeneral rules.\n\nTable 1: Characteristics of data sets.\nData Sets\nBreast Cancer\nIris\nSeason\nGolf Playing\n\n80\n\nNo. of\nExamples\n699\n150\n11\n14\n\nInput\nAttributes\n9\n4\n3\n4\n\nOutput\nClasses\n2\n3\n4\n2\n\n\fKamruzzaman et. al : Proceedings of the 4th ICEE & 2nd APM, January 2006\n4.1 Extracted Rules\n\nIris Problem\nRule 1: If Petal-length (A3) <= 1.9 then\nIris setosa\nRule 2: If Petal-length (A3) <= 4.9 and Petalwidth (A4) <= 1.6 then Iris versicolor\nDefault Rule: Iris virginica.\n\nTable 2 shows the number of rules extracted by REx\nand the accuracy of the rules. In most of the cases REx\nproduces fewer rules with better accuracy. It was observed\nthat two to three rules were sufficient to solve the problems.\nThe accuracy was 100% for season and golf playing\nproblems, because of the lower number of examples.\n\nSeason Problem\nRule 1: If Tree (A2) = yellow then autumn\nRule 2: If Tree (A2) = leafless then autumn\nRule 3: If Temperature (A3) = low then winter\nRule 4: If Temperature (A3) = high then summer\nDefault Rule: spring.\n\nTable 2: Number of rules and rules accuracy.\nData Sets\nBreast Cancer\nIris\nSeason\nGolf Playing\n\nNo. of Extracted Rules\n2\n3\n4\n3\n\nRules Accuracy\n96.28 %\n97.33 %\n100 %\n100 %\n\nGolf Playing Problem\nRule 1: If Outlook (A1) = sunny and Humidity\n>=85 then don't play\nRule 2: Outlook (A1) = rainy and Wind= strong\nthen don't play\nDefault Rule: play.\n\nThe number of rules extracted by REx and the\naccuracy of the rules were described in Table 2. But\nthe visualization of the rules in terms of the original\nattributes was not discussed. The following\nsubsections discussed the rules extracted by REx in\nterms of the original attributes. The number of\nconditions per rule and the number of rules extracted\nwere also visualized here.\n\n5. COMPARISON\nThis section compares experimental results of REx\nwith the results of other works. The primary aim of\nthis work is not to exhaustively compare REx with all\nother works, but to evaluate REx in order to gain a\ndeeper understanding of rule extraction.\n\nBreast Cancer Problem\nRule 1: If Clump thickness (A1) <= 0.6 and Bare\nnuclei (A6) <= 0.5 and Mitosis (A9) <= 0.3,\nthen benign\nDefault Rule: malignant.\n\nTable 3: Performance comparison of REx with other algorithms for breast cancer problem.\nData Set\nBreast\nCancer\n\nFeature\nNo. of Rules\nAvg. No. of\nConditions\nAccuracy %\n\nREx\n2\n\nNN RULES\n4\n\nDT RULES\n7\n\nC4.5\n-\n\nNN-C4.5\n-\n\nOC1\n-\n\nCART\n-\n\n3\n\n3\n\n1.75\n\n-\n\n-\n\n-\n\n-\n\n96.28\n\n96\n\n95.5\n\n95.3\n\n96.1\n\n94.99\n\n94.71\n\nTable 4: Performance comparison of REx with other algorithms for iris problem.\nData Set\nIris\n\nFeature\nNo. of Rules\nAvg. No. of\nConditions\nAccuracy %\n\nREx\n3\n1\n\nNN RULES\n3\n1\n\nDT RULES\n4\n1\n\nBIO RE\n4\n3\n\nPartial RE\n6\n3\n\nFull RE\n3\n2\n\n98.67\n\n97.33\n\n94.67\n\n78.67\n\n78.67\n\n97.33\n\nTable 5: Performance comparison of REx with other algorithms for season problem.\nData set\nSeason\n\nFeature\nNo. of Rules\nAvg. No. of\nConditions\nAccuracy %\n\nREx\n5\n1\n\nRULES\n7\n2\n\nX2R\n6\n1\n\n100.0\n\n100.0\n\n100.0\n\n81\n\n\fKamruzzaman et. al : Proceedings of the 4th ICEE & 2nd APM, January 2006\nTable 6: Performance comparison of REx with other algorithms for golf playing problem.\nData set\nGolf Playing\n\nFeature\nNo. of Rules\nAvg. No. of\nConditions\nAccuracy %\n\nREx\n3\n2\n\nRULES\n8\n2\n\nRULES-2\n14\n2\n\nX2R\n3\n2\n\n100.0\n\n100.0\n\n100.0\n\n100.0\n\nby the method introduced here, ANNs should no\nlonger be regarded as black boxes.\n\nTable 3 compares REx results of breast cancer\nproblem with those produced by NN RULES [2], DT\nRULES [2], C4.5 [6], NN-C4.5 [9], OC1 [9], and\nCART [10] algorithms. REx achieved best\nperformance although NN RULES was closest\nsecond. But number of rules extracted by REx are 2\nwhereas these were 4 for NN RULES.\n\nREFERENCES\n[1]\n\nTable 4 compares REx results of iris problem with\nthose produced by NN RULES, DT RULES, BIO RE\n[11], Partial RE [11], and Full RE [11] algorithms.\nREx achieved 98.67% accuracy although NN RULES\nwas closest second with 97.33% accuracy. Here\nnumber of rules extracted by REx and NN RULES are\nequal.\n\n[2]\n[3]\n[4]\n\nTable 5 compares REx results of season problem with\nthose produced by RULES [12] and X2R [4]. All\nthree algorithms achieved 100% accuracy. This is\npossible because the number of examples is low.\nNumber of extracted rules by REx are 5 whereas these\nwere 7 for RULES and 6 for X2R.\n\n[5]\n[6]\n\nTable 6 compares REx results of golf playing problem\nwith those produced by RULES, RULES-2 [13], and\nX2R. All four algorithms achieved 100% accuracy\nbecause the lower number of examples. Number of\nextracted rules by REx are 3 whereas these were 8 for\nRULES and 14 for RULES-2.\n\n[7]\n\n[8]\n\n6. CONCLUSIONS\n[9]\n\nThis work is an attempted to open up these black\nboxes by extracting symbolic rules from it through the\nproposed efficient rule extraction algorithm REx. The\nREx algorithm can extract concise rules from standard\nfeedforward ANN. An important feature of rule\nextraction algorithm, REx, is its recursive nature.\nThey are concise, comprehensible, order insensitive\nand do not involve any weight values. The accuracy of\nthe rules from a pruned network is as high as the\naccuracy of the fully connected network.\n\n[10]\n[11]\n\nExtensive experiments have been carried out in this\nstudy to evaluate how well REx performed on four\nbenchmark classification problems in ANNs including\nbreast cancer, iris, season, and golf playing in\ncomparison with other algorithms. In almost all cases,\nREx outperformed the others. With the rules extracted\n\n[12]\n[13]\n\n82\n\nR. Setiono and W. K. Leow, \" FERNN: An\nalgorithm for fast extraction of rules from neural\nnetworks,\" Applied Intelligence, vol. 12, 2000, pp.\n15-25.\nR. Setiono and H. Liu, \"Symbolic presentation of\nneural networks,\" IEEE Computer, March 1996, pp.\n71-77.\nR. Setiono, \"Extracting rules from neural networks\nby pruning and hidden-unit node splitting,\" Neural\nComputation, vol. 9, 1997, pp. 205-225.\nH. Liu and S. T. Tan, \"X2R: A fast rule generator,\"\nProceedings of IEEE International Conference on\nSystems, Man and Cybernetics, Vancouver, CA,\n1995.\nHan Jiawei, Micheline Kamber, \"Data Mining:\nConcepts and Techniques,\" Morgan Kaufmann\nPublisher: CA, 2001.\nJ. R. Quinlan, \"C4.5: Programs for Machine\nLearning,\" Morgan Kaufmann, San Mateo, CA,\n1993.\nR. Agrawal, T. Imielinski, and A. Swami, \"Database\nmining: A performance perspective,\" IEEE\nTransactions on Knowledge and Data Engineering,\nvol. 5, pp. 914-925, 1993.\nC. Blake, E. Keogh, and C. J. Merz, \"UCI repository\nof\nof\nmachine\nlearning\ndatabases\n[http://www.ics.uci.edu/~mlearn/MLRepository.htm\n],\" Department of Information and Computer\nScience, University of California, Irvine, CA, 1998.\nR. Setiono, \"Techniques for extracting rules from\nartificial neural networks,\" Plenary lecture presented\nat the 5th International Conference on Soft\nComputing and Information Systems, Iizuka, Japan,\nOctober 1998.\nL. Breiman, J. Friedman, R. Olshen, and C. Stone,\n\"Classification and Regression Trees,\" Wadsworth\nand Brooks, Monterey, CA, 1984.\nI. Taha and J. Ghosh, \"Three techniques for\nextracting rules from feedforward networks,\"\nIntelligent Engineering Systems Through Artificial\nNeural Networks, vol. 6, pp. 23-28, ASME Press, St.\nLouis, 1996.\nD. T. Pham and M. S. Aksoy, \"Rules: A simple rule\nextraction\nsystem,\"\nExpert\nSystems\nwith\nApplications, vol. 8, 1995.\nD. T. Pham and M. S. Aksoy, \"An algorithm for\nautomatic rule induction,\" Artificial Intelligence in\nEngineering, vol. 8, 1994.\n\n\f"}