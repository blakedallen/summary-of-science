{"id": "http://arxiv.org/abs/1012.2092v2", "guidislink": true, "updated": "2010-12-15T19:45:20Z", "updated_parsed": [2010, 12, 15, 19, 45, 20, 2, 349, 0], "published": "2010-12-09T19:41:33Z", "published_parsed": [2010, 12, 9, 19, 41, 33, 3, 343, 0], "title": "Price decomposition in large-scale stochastic optimal control", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.5448%2C1012.1306%2C1012.3768%2C1012.4362%2C1012.1001%2C1012.3685%2C1012.2291%2C1012.3151%2C1012.5825%2C1012.2996%2C1012.1850%2C1012.2299%2C1012.5669%2C1012.4674%2C1012.3244%2C1012.3775%2C1012.4005%2C1012.1600%2C1012.1158%2C1012.3140%2C1012.4523%2C1012.0257%2C1012.4418%2C1012.0953%2C1012.0056%2C1012.5938%2C1012.4394%2C1012.4369%2C1012.5864%2C1012.3429%2C1012.2363%2C1012.2977%2C1012.3686%2C1012.0269%2C1012.5812%2C1012.5943%2C1012.3040%2C1012.2568%2C1012.3269%2C1012.3972%2C1012.3969%2C1012.2734%2C1012.3447%2C1012.2610%2C1012.1238%2C1012.1088%2C1012.2110%2C1012.4600%2C1012.5673%2C1012.3963%2C1012.2019%2C1012.2092%2C1012.5137%2C1012.5110%2C1012.4947%2C1012.4609%2C1012.1926%2C1012.1502%2C1012.2855%2C1012.2419%2C1012.2533%2C1012.4422%2C1012.1888%2C1012.3965%2C1012.4658%2C1012.5054%2C1012.3698%2C1012.0095%2C1012.5849%2C1012.3406%2C1012.4354%2C1012.3086%2C1012.2536%2C1012.2801%2C1012.0511%2C1012.5754%2C1012.0110%2C1012.1853%2C1012.5503%2C1012.2788%2C1012.4949%2C1012.1649%2C1012.0067%2C1012.4114%2C1012.1768%2C1012.3327%2C1012.5977%2C1012.0869%2C1012.5360%2C1012.4972%2C1012.3405%2C1012.5510%2C1012.2172%2C1012.3014%2C1012.1361%2C1012.2889%2C1012.4035%2C1012.4207%2C1012.4721%2C1012.5963%2C1012.3659&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Price decomposition in large-scale stochastic optimal control"}, "summary": "We are interested in optimally driving a dynamical system that can be\ninfluenced by exogenous noises. This is generally called a Stochastic Optimal\nControl (SOC) problem and the Dynamic Programming (DP) principle is the natural\nway of solving it. Unfortunately, DP faces the so-called curse of\ndimensionality: the complexity of solving DP equations grows exponentially with\nthe dimension of the information variable that is sufficient to take optimal\ndecisions (the state variable). For a large class of SOC problems, which\nincludes important practical problems, we propose an original way of obtaining\nstrategies to drive the system. The algorithm we introduce is based on\nLagrangian relaxation, of which the application to decomposition is well-known\nin the deterministic framework. However, its application to such closed-loop\nproblems is not straightforward and an additional statistical approximation\nconcerning the dual process is needed. We give a convergence proof, that\nderives directly from classical results concerning duality in optimization, and\nenlghten the error made by our approximation. Numerical results are also\nprovided, on a large-scale SOC problem. This idea extends the original DADP\nalgorithm that was presented by Barty, Carpentier and Girardeau (2010).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.5448%2C1012.1306%2C1012.3768%2C1012.4362%2C1012.1001%2C1012.3685%2C1012.2291%2C1012.3151%2C1012.5825%2C1012.2996%2C1012.1850%2C1012.2299%2C1012.5669%2C1012.4674%2C1012.3244%2C1012.3775%2C1012.4005%2C1012.1600%2C1012.1158%2C1012.3140%2C1012.4523%2C1012.0257%2C1012.4418%2C1012.0953%2C1012.0056%2C1012.5938%2C1012.4394%2C1012.4369%2C1012.5864%2C1012.3429%2C1012.2363%2C1012.2977%2C1012.3686%2C1012.0269%2C1012.5812%2C1012.5943%2C1012.3040%2C1012.2568%2C1012.3269%2C1012.3972%2C1012.3969%2C1012.2734%2C1012.3447%2C1012.2610%2C1012.1238%2C1012.1088%2C1012.2110%2C1012.4600%2C1012.5673%2C1012.3963%2C1012.2019%2C1012.2092%2C1012.5137%2C1012.5110%2C1012.4947%2C1012.4609%2C1012.1926%2C1012.1502%2C1012.2855%2C1012.2419%2C1012.2533%2C1012.4422%2C1012.1888%2C1012.3965%2C1012.4658%2C1012.5054%2C1012.3698%2C1012.0095%2C1012.5849%2C1012.3406%2C1012.4354%2C1012.3086%2C1012.2536%2C1012.2801%2C1012.0511%2C1012.5754%2C1012.0110%2C1012.1853%2C1012.5503%2C1012.2788%2C1012.4949%2C1012.1649%2C1012.0067%2C1012.4114%2C1012.1768%2C1012.3327%2C1012.5977%2C1012.0869%2C1012.5360%2C1012.4972%2C1012.3405%2C1012.5510%2C1012.2172%2C1012.3014%2C1012.1361%2C1012.2889%2C1012.4035%2C1012.4207%2C1012.4721%2C1012.5963%2C1012.3659&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We are interested in optimally driving a dynamical system that can be\ninfluenced by exogenous noises. This is generally called a Stochastic Optimal\nControl (SOC) problem and the Dynamic Programming (DP) principle is the natural\nway of solving it. Unfortunately, DP faces the so-called curse of\ndimensionality: the complexity of solving DP equations grows exponentially with\nthe dimension of the information variable that is sufficient to take optimal\ndecisions (the state variable). For a large class of SOC problems, which\nincludes important practical problems, we propose an original way of obtaining\nstrategies to drive the system. The algorithm we introduce is based on\nLagrangian relaxation, of which the application to decomposition is well-known\nin the deterministic framework. However, its application to such closed-loop\nproblems is not straightforward and an additional statistical approximation\nconcerning the dual process is needed. We give a convergence proof, that\nderives directly from classical results concerning duality in optimization, and\nenlghten the error made by our approximation. Numerical results are also\nprovided, on a large-scale SOC problem. This idea extends the original DADP\nalgorithm that was presented by Barty, Carpentier and Girardeau (2010)."}, "authors": ["Kengy Barty", "Pierre Carpentier", "Guy Cohen", "Pierre Girardeau"], "author_detail": {"name": "Pierre Girardeau"}, "author": "Pierre Girardeau", "links": [{"href": "http://arxiv.org/abs/1012.2092v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1012.2092v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1012.2092v2", "affiliation": "UMA, CERMICS", "arxiv_url": "http://arxiv.org/abs/1012.2092v2", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "PRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC\nOPTIMAL CONTROL\n\narXiv:1012.2092v2 [math.OC] 15 Dec 2010\n\nKENGY BARTY, PIERRE CARPENTIER, GUY COHEN, AND PIERRE GIRARDEAU\nAbstract. We are interested in optimally driving a dynamical system that\ncan be influenced by exogenous noises. This is generally called a Stochastic\nOptimal Control (SOC) problem and the Dynamic Programming (DP) principle is the natural way of solving it. Unfortunately, DP faces the so-called\ncurse of dimensionality: the complexity of solving DP equations grows exponentially with the dimension of the information variable that is sufficient to\ntake optimal decisions (the state variable).\nFor a large class of SOC problems, which includes important practical problems, we propose an original way of obtaining strategies to drive the system.\nThe algorithm we introduce is based on Lagrangian relaxation, of which the\napplication to decomposition is well-known in the deterministic framework.\nHowever, its application to such closed-loop problems is not straightforward\nand an additional statistical approximation concerning the dual process is\nneeded. We give a convergence proof, that derives directly from classical results concerning duality in optimization, and enlghten the error made by our\napproximation. Numerical results are also provided, on a large-scale SOC\nproblem. This idea extends the original DADP algorithm that was presented\nby Barty, Carpentier, and Girardeau (2010).\n\nIntroduction\nConsider a controlled dynamical system over a discrete and finite time horizon.\nThis system may be influenced by exogenous noises that affect its behaviour. We\nsuppose that, at every instant, the decision maker is able to observe these noises\nand to keep these observations in memory. Since it is generally profitable to take\navailable observations into account when designing future decisions, we are looking\nfor strategies rather than simple decisions. Such strategies (or policies) are feedback\nfunctions that map every instant and every possible history of the system to a\ndecision to be made.\nMore precisely, we are here interested in optimization problems with a large\nnumber of variables. The typical application we have in mind is the following.\nConsider a power producer that owns a certain number of power units. Each unit\nhas its own local characteristics such as physical constraints that restrain the set of\nfeasible decisions, and production costs that depend on the type of fuel that is used\nto produce power. The power producer has to control the power units so that a\nglobal power demand is met at every instant. The power demand, as well as other\nparameters such as inflows in water reservoirs or unit breakdowns, are random.\nNaturally, he is looking for strategies that make the production cost minimal, over\na given time horizon. In such a problem, both the number of power units and the\nnumber of time steps are usually large.\nDate: October 28, 2018.\n1991 Mathematics Subject Classification. 93E20, 49M27, 49L20.\nKey words and phrases. Stochastic optimal control, Decomposition methods, Dynamic\nProgramming.\n1\n\n\f2\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\nOne classical approach when dealing with stochastic dynamic optimization problems is to discretize the random inputs of the problem using scenario trees. Such\nan approach has been widely studied within the Stochastic Programming community (see the book by Shapiro, Dentcheva, and Ruszczy\u0144ski, 2009, for an overview\nof this methodology). One of the advantages of such a technique is that as soon as\nthe scenario tree is drawn, the derived problem can be treated by classical Mathematical Programming techniques. Thus, a number of decomposition methodologies\nhave been proposed (Higle and Sen, 1996, Carpentier, Cohen, Culioli, and Renaud,\n1996, Ruszczy\u0144ski and Shapiro, 2003, Chapter 3) and even applied to energy planning problems (Bacaud, Lemar\u00e9chal, Renaud, and Sagastiz\u00e1bal, 2001). A general\ntheoteric point of view concerning the way to combine the discretization of expectation together with the discretization of information is given by Barty (2004).\nHowever, in a multi-stage setting, this methodology suffers from the drawbacks that\narise with scenario trees. As it was pointed out by Shapiro (2006), the number of\nscenarios needed to achieve a given accuracy grows exponentially with the number\nof time steps of the problem.\nThe other natural approach to solve SOC problems is to rely on the Dynamic\nProgramming (DP) principle (see Bellman, 1957, Bertsekas, 2000). The core of\nthe DP approach is the definition of a state variable that is, roughly speaking, the\nvariable that, in conjunction with the time variable, is sufficient to take an optimal decision at every instant. It does not have the drawback of the scenario trees\nconcerning the number of time steps since strategies are, in this context, depending on a state variable whose space dimension usually does not grow with time1.\nHowever, DP suffers from another drawback which is the so-called curse of dimensionality: the complexity of solving the DP equation grows exponentially with\nthe state space dimension. Hence, brutally solving the DP equation is generally\nintractable when the state space dimension goes beyond several units. Recently,\nVezolle, Vialle, and Warin (2009) were able to solve it on a 10-state-variables energy management problem, using parallel computation coupled with adequate data\ndistribution.\nAnother popular idea is to represent the value functions (solutions of the DP\nequation) as a linear combination of a priori chosen basis functions (see among others Bellman and Dreyfus, 1959, Bertsekas and Tsitsiklis, 1996, Sect. 6.5). This approach, called Approximate Dynamic Programming or often Least-Squares MonteCarlo, has also become very popular in the context of American option pricing\nthrough the work of Longstaff and Schwartz (2001). This approximation reduces\nthe complexity of solving the DP equation drastically. However, in order to be\npractically efficient, such an approach requires some a priori information about the\nproblem, in order to define a well suited functional subspace. Indeed, there is no\nsystematic means to choose the basis functions and several choices have been proposed in the literature (de Farias and Van Roy, 2003, Tsitsiklis and Van Roy, 1996,\nBouchard and Warin, 2010).\nWhen dealing with large-scale optimization problems, the decomposition/coordination approach aims at finding a solution to the original problem by iteratively\nsolving smaller-dimensional subproblems. In the deterministic case, several types\nof decomposition have been proposed (e.g. by prices or by quantities) and unified\nin a general framework using the Auxiliary Problem Principle by Cohen (1980a).\nIn the open-loop stochastic case, i.e. when controls do not rely on any observation,\nCohen and Culioli (1990) proposed to take advantage of both decomposition techniques and stochastic gradient algorithms. These techniques have been extended\nin the closed-loop stochastic case by Barty, Roy, and Strugarek (2009), but so far\n1\n\nIn the case of power management, the state dimension is usually the number of power units.\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n3\n\nthey fail to provide decomposed state dependent strategies in the Markovian case.\nThis is because a subproblem optimal strategy depends on the state of the whole\nsystem, not only on the local state. In other words, decomposition approaches are\nmeant to decompose the control space, namely the range of the strategy, but the\nnumerical complexity of the problems we consider here also arises because of the\ndimensionality of the state space, that is to say the domain of the strategy.\nWe here propose a way to use price decomposition within the closed-loop stochastic case. The coupling constraints, namely the constraints preventing the problem\nfrom being naturally decomposed, are dualized using a Lagrange multiplier (price).\nAt each iteration, the price decomposition algorithm solves each subproblem using\nthe current price, then uses the solutions to update the price. In the stochastic context, price is a random process whose dynamics is not available, so the subproblems\ndo not in general fall into the Markovian setting. However, in a specific instance\nof this problem, Strugarek (2006) exhibited a dynamics for the optimal multiplier,\nand he showed that these dynamics were independent with respect to the decision\nvariables. Hence it was possible to come down to the Markovian framework and\nto use DP to solve the subproblems in this case. Following this idea, Barty et al.\n(2010) proposed to choose a parametrized dynamics for these multipliers in such\na way that solving subproblems using DP becomes possible. While the approach,\ncalled Dual Approximate Dynamic Programming (DADP), showed promising results on numerical examples, it suffers from the fact that the induced restrained\ndual space is non-convex. This led to some numerical instabilities and, probably\nmore important, it was not possible to give convergence results for the algorithm.\nWe here propose to extend DADP in a more general way that allows us to derive\nconvergence results and solves the problem of numerical instabilities.\nThe paper is organized as follows. In Section 1, we present the general SOC\nproblem and the DP principle. Then we concentrate on a more specific class of\nproblems, that we call decomposable problems, and recall the previous version of\nthe DADP algorithm. In Section 2, we present the new version we propose and give\nconvergence results for the algorithm. Finally, in Section 3, we apply DADP to two\nnumerical examples, the first being the one from the previous paper by Barty et al.\n(2010) and the second one being a more realistic power management example.\n1. Mathematical formulation\n1.1. General problem setting. All along the paper, random variables are denoted using bold letters. Consider a discrete and finite time horizon 0, 1, . . . , T\nand a probability space (\u03a9, A, P). To define a stochastic dynamical system, we\nneed:\n\u2022 a stock process X = (X 0 , . . . , X T ) which represents the physical states of\nthe system through time, the value of X t lying, at every instant t, in a\nHilbert space Xt ;\n\u2022 a control process U = (U 0 , . . . , U T \u22121 ), the value of U t lying, at every\ninstant t, in a Hilbert space Ut ;\n\u2022 a noise process W = (W 0 , . . . , W T \u22121 ), the value of W t lying, at every\ninstant t, in a Hilbert space Wt .\nThe spaces Xt , Ut and Wt are generally finite-dimensional spaces. In the sequel,\nwe suppose Xt = Rn and Ut = Rm . The decision variable U t being a random\nvariable, and our purpose being to use variational techniques that require the notion of gradient, it is natural to suppose that U t lies in a Hilbert space Ut , for\nexample L2 (\u03a9, A, P; Ut ).\nThe three types of variables are linked together in the following way. At every\ntime step t, there exists a function ft (the dynamics of the system) that maps the\n\n\f4\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\ntriplet (X t , U t , W t ) to the next stock value X t+1 . Let (A0 , . . . , AT \u22121 ) be the\nfiltration associated with the stochastic process W . We suppose that, at every\ntime step t, the decision maker is able to observe and to keep in memory all the\npast history of W up to time t. The causality principle states that the decision U t\nat time t is At -measurable, i.e. only depends on past observations. Moreover, at\neach time step t, a cost Ct (X t , U t , W t ) is incurred. Finally, at the final time T , a\ncost K(X T ) is added. The Stochastic Optimal Control (SOC) problem we would\nlike to solve hence reads:\n!\nT\n\u22121\nX\nmin E\n(1a)\nCt (X t , U t , W t ) + K (X T ) ,\nX,U\n\nt=0\n\nsubject to dynamics constraints:\n(1b)\n\nX t+1 = ft (X t , U t , W t ) ,\n\n(1c)\n\nX 0 is given,\n\n\u2200t = 0, . . . , T \u2212 1,\n\nas well as bound constraints:\n(1d)\n\nxt \u2264 X t \u2264 xt ,\n\n\u2200t = 1, . . . , T,\n\n(1e)\n\nut \u2264 U t \u2264 ut ,\n\n\u2200t = 0, . . . , T \u2212 1,\n\nstatic constraints:\n(1f)\n\ngt (X t , U t , W t ) = 0,\n\n\u2200t = 0, . . . , T \u2212 1,\n\nand the non-anticipativity constraint:\n(1g)\n\nU t is At -measurable.\n\nConstraints (1b), (1d), (1e) and (1f) have to be understood in the P-almost sure\nsense. We give examples for constraint (1f) in \u00a72. With no further assumptions,\nProblem (1) cannot generally be solved analytically, except for quite particular\ncases among which is, for instance, the Linear Quadratic Gaussian (LQG) case.\nOne has to be aware that, when solving this problem, one is looking for functions\nthat map every possible history of the system to a decision; the domain of such\na function is clearly growing with time and representing it on a computer rapidly\nbecomes intractable.\n1.2. The Dynamic Programming Principle. Fortunately enough, control theory helps us reduce the size of the optimal strategy's domain in some cases. Let us\nfirst make the following assumption.\nAssumption 1. Noises W 0 , . . . , W T \u22121 are independent over time.\nNow define functions Vt , for every time step t = 0, . . . , T , as:\nVt (x) =\n\nmin\n\nE\n\nXt, . . . , XT\nU t , . . . , U T \u22121\n\nT\n\u22121\nX\ns=t\n\n!\n\nCs (X s , U s , W s ) + K (X T ) X t = x ,\n\n\u2200x \u2208 Xt ,\n\nsubject to the same2 constraints as in Problem (1). Function Vt represents the\nminimal remaining cost of the problem when starting at time t, for every possible\nstock value x.\nUnder Assumption 1, the Dynamic Programming (DP) principle states that the\nvariable X t , along with the current noise value W t , contains all the information\nthat is sufficient to take the optimal decision at time t, hence the term state variable.\n2while starting at time t\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n5\n\nMoreover, it provides a way to compute functions Vt , that we now call Bellman\nfunctions (or value functions), as well as optimal strategy, in a backward manner.\n(2a)\n\nVT (x) = K (x) ,\n\n\u2200x \u2208 XT ,\n\nand, for every time step t = T \u2212 1, . . . , 0:\n\u0010\n\u0011\n(2b)\nVt (x) = E min Ct (x, u, W t ) + Vt+1 (ft (x, u, W t )) ,\nu\n\n\u2200x \u2208 Xt .\n\nCompared with the original setting where the optimal strategy domain was growing\nalong with time steps, the DP principle drastically reduces the size of the information needed to make an optimal decision.\n\nRemark 1 (About the overtime independence). In the case when the model is such\nthat noises that affect the system have some sort of correlation through time, one\ncan always explicit the dynamics of the noise variable and add it to the dynamics\nof X t , thus defining a new (albeit larger!) state variable as well as a new noise\nvariable that is now independent over time.\nRemark 2 (Hazard-Decision setting). The reader may have noticed that the way\nthe non-anticipativity constraint in written allows the decision maker at time t to\nobserve the current noise value W t before choosing the control U t . In such a setting\nthe optimal decision at time t depends on both the state variable X t and the noise\nvariable W t whereas the value function only depends on the state variable X t .\nNote however that the dimension of the state space Xt might still be quite large.\nYet the complexity of solving the DP equation (2) grows exponentially with the\ndimension of Xt ; this unpleasant feature is well known as the curse of dimensionality\nand prevents us from solving this equation by discretization when the state space\ndimension is, say, greater than 5.\n1.3. Decomposable problem setting. Let us now present a particular instance\nof Problem (1) on which we are able to reduce even more the size of the information\nneeded to take a reasonable decision.\nWe consider a system which consists of N subsystems3, whose dynamics and\ncost functions are independent one from another. More precisely, the state\nX t (re\u0001\n1\nN\nspectively the control U t ) of the global system writes X t , . . . , X t with X it \u2208\n\u0001\nwith U it \u2208 L2 (\u03a9, A, P; Rmi )) and n =\nU 1t , . . . , U N\nL2 (\u03a9, A, P; Rni ) (resp.\nt\nPN\nPN\ni=1 ni (resp. m =\ni=1 mi ), so that the global dynamics X t+1 = f\u0001t (X t , U t , W t )\ncan be written independently unit by unit: X it+1 = fti X it , U it , W t , i = 1, . . . , N .\nIn the same way, the global\n\u0001 cost Ct (X t , U t , W t ) is equal to the sum of the local\nunit costs Cti X it , U it , W t , i = 1, . . . , N . At the end of the time period, each unit i\ncauses a cost K i that only depends on its final state X iT .\nRemark that, without further constraints, the induced SOC problem can be\nstated independently unit by unit, though the same noise variable affects all units\n(see Appendix B for a precise proof). Hence, under Assumption 1, the solving of the\nDP equation can be decomposed unit by unit. For each unit, the optimal strategy\ndepends only on its local state4, which is usually far smaller than the dimension of\nthe global state space.\nConsider now a static constraint (1f) that couples the units together. We suppose\nd\nthat such a coupling arises from a set of static\n\u0001 R -valued constraints, the constraint\nPN i\ni\ni\nat time step t reading i=1 gt X t , U t , W t = 0. This kind of coupling constraint is\nnatural in many industrial applications, including the case of a power management\n3We often use the term \"units\" for subsystems.\n4and on the noise at the current time step because we are in the Hazard-Decision setting\n\n\f6\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\nproblem that we already mentioned in the introduction: the sum of the productions\nof the power units must meet an uncertain power demand.\nThe decomposable problem we are interested in solving in the following reads:\n!\nN\nT\n\u22121 X\nN\nX\n\u0001\n\u0001 X\ni\ni\ni\ni\ni\n(3a)\nK XT\nCt X t , U t , W t +\nmin E\nX,U\n\nt=0 i=1\n\ni=1\n\nsubject to dynamics constraints:\n(3b)\n(3c)\n\n\u0001\nX it+1 = fti X it , U it , W t ,\n\nX i0\n\nis given,\n\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N,\n\n\u2200i = 1, . . . , N,\n\nas well as bound constraints:\n(3d)\n\nxit \u2264 X it \u2264 xit ,\n\n(3e)\n\nuit\n\n\u2264\n\nU it\n\n\u2264\n\nuit ,\n\n\u2200t = 1, . . . , T, \u2200i = 1, . . . , N,\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N,\n\nstatic constraints:\n(3f)\n\nN\nX\ni=1\n\n\u0001\ngti X it , U it , W t = 0,\n\n\u2200t = 0, . . . , T \u2212 1,\n\nand the non-anticipativity constraint:\n(3g)\n\nU it is At -measurable,\n\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N.\n\nThere are three types of coupling in Problem (3):\n\u2022 The first comes from the state dynamics (3b) that induce a temporal coupling.\n\u2022 The second one arises from the static constraints (3f) that induce a spatial\ncoupling: they link together all the subsystems at each time step t.\n\u2022 The third type of coupling is informational: it comes from the causality\nconstraint (3g), which prevents us from decomposing directly scenario by\nscenario : if two realizations of the noise process are identical up to time t,\nthen the same control has to be applied at time t on both realizations.\nConstraints (3f) prevent us from decomposing the optimization problem unit\nby unit: the solution U it for unit i and time t has to be searched as a feedback\nfunction \u03c6it depending on the current noise value and on the whole stock varii\nable X t = (X 1t , . . . , X N\nt ) rather than on the local stock variable X t ! Adding the\ncoupling constraint (3f) drastically changed the structure of the problem.\nRemark 3 (Local and global noises). Applications we have in mind are power management problems which are completely \"flower-shaped\", in the following sense.\nThe noise variable W t at time t is composed of two different kinds of noise:\n\u2022 a local noise W it for every subsystem i, i.e. at every petal of the flower (uncertain inflows entering a water reservoir, for instance);\n\u2022 a global noise Dt at the center of the flower (a total power demand, for\ninstance).\nIn such a setting, only the local noise appears in the cost function and in the\ndynamics, leading to functions of the form:\n\u0001\n\u0001\nCti X it , U it , W it and fti X it , U it , W it ,\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n7\n\nwhile the global noise appears only in the coupling constraint as, for instance:\nN\nX\ni=1\n\n\u0001\ngti X it , U it = D t .\n\nKeeping this particular case in mind shall give us some insight about how to decompose the global problem as well as possible. This is explained in more details\nin \u00a72.1 and such settings are treated in the numerical experiments of \u00a73.\n1.4. Previous paper. In a previous study (Barty et al., 2010), the authors proposed a way of handling Problem (3) by approximate Lagrangian decomposition.\nThe proposed algorithm, called Dual Approximate Dynamic Programming (DADP)\nis as follows. Let us introduce the Lagrangian of Problem (3):\nL (X, U , \u03bb) := E\n\nT\n\u22121 X\nN \u0010\nX\nt=0 i=1\n\n\u0001\u0011\n\u0001\ni\ni\ni\nCti X it , U it , W t + \u03bb\u22a4\nt gt X t , U t , W t\n+\n\nN\nX\ni=1\n\nK\n\ni\n\nX iT\n\n\u0001\n\n!\n\n,\n\nwith \u03bbt \u2208 L2 (\u03a9, A, P; Rd ) the Lagrange multiplier of the coupling constraint (3f)\nand \u03bb := (\u03bb0 , . . . , \u03bbT \u22121 ). Note that, since the dualized constraint is At -measurable,\nthe Lagrange multiplier \u03bbt need only to have the same measurability.\nProblem (3) is always equivalent to:\nmin max\n\nX,U\n\n\u03bb\n\nL (X, U , \u03bb) ,\n\nwhere the minimization is subject to all constraints of Problem (3) except constraint (3f). If L has a saddle point (see Appendix A for a definition and a characterization of saddle points), then this problem is equivalent to the so-called dual\nproblem:\n(4)\n\nmax min\n\u03bb\n\nX,U\n\nL (X, U , \u03bb) ,\n\nunder, once again, the same constraints as in Problem (3) except the coupling\nconstraint (3f).\nThe key point of the so-called price decomposition algorithm is that the inner\nminimization problem can be split into N subproblems, each one involving a single\nsubsystem (once again, see Appendix B for more details). One might think that\nsolving these subproblems is much simpler than solving the original global problem. This is not the case here: because the dual variable \u03bb is a stochastic process\nthat depends in general on the whole history of the system, we cannot reasonably\nmake the overtime independence assumption that leads to the DP principle and\nsubproblems are just as hard as Problem (1)!\nThe idea of Barty et al. (2010) is to force the dual process to satisfy a prescribed\ndynamics:\n(5a)\n(5b)\n\n\u03bb0 = h\u03b10 (W 0 ) ,\n\u03bbt+1 = h\u03b1t+1 (\u03bbt , W t+1 ) ,\n\n\u2200t = 0, . . . , T \u2212 2,\n\nwhere h\u03b1t is an a priori chosen function parametrized by \u03b1t \u2208 Rq . We note \u03b1 =\n(\u03b10 , . . . , \u03b1T \u22121 ). Given a vector \u03b1k of coefficients at iteration k of the algorithm\nwhich defines the current values of the dual variables, the first step of DADP is\nto solve the N subproblems by DP with state (X it , \u03bbt ). In order to update the\nLagrange multipliers, the authors propose to draw S trajectory samples of the\nnoise W and integrate the dynamics (3b)\u2013(3c) and (5) using the optimal feedback\n\n\f8\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\nlaws obtained at the first step, thus obtaining S sample trajectories of X k , U k\nand \u03bbk . A gradient step is then performed sample by sample:\nk+ 12 ,s\n\n\u03bbt\n\n= \u03bbk,s\nt + \u03c1t \u00d7\n\nN\nX\ni=1\n\n\u0010\n\u0011\ngti X i,k,s\n, U i,k,s\n, W st ,\nt\nt\n\n\u2200s = 1, . . . , S,\n\nwith \u03c1t obeying the rules of the step-size choice in Uzawa's algorithm (see Appendix A). Finally, we solve the following regression problem:\nmin\n\n\u03b10 ,...,\u03b1T \u22121\n\nS \u0012\nX\n\nk+ 12 ,s 2\n\nh\u03b10 (W s0 ) \u2212 \u03bb0\n\nRd\n\ns=1\n\n+\n\nT\n\u22122\nX\nt=0\n\n\u0010\n\u0011\nk+ 1 ,s\nk+ 1 ,s\nh\u03b1t+1 \u03bbt 2 , W st+1 \u2212 \u03bbt+12\n\n2\nRd\n\n\u0013\n.\n\nThe last minimization produces coefficients \u03b1k+1 which define, using Equation (5),\na new process \u03bbk+1 .\nThis procedure has several advantages, notably that its complexity is linear\nwith respect to the number N of subproblems and that it may lead, depending\non the choice for the dual dynamics h, to tractable approximations of the original\nproblem. The authors illustrate this fact on a small example on which they are\nable to compare standard DP and DADP.\nStill, it has some drawbacks, mainly theoretical. First of all, the shape of the\ndynamics introduced for the dual process is arbitrarily and once for all chosen and\nthe quality of the result depends on this choice. Moreover, this dynamics defines\na subspace which is non-convex. The next iterate \u03bbk+1 being a projection on this\nsubspace, it is not well defined and some oscillations observed in practice may be\ndue to this fact. Finally, this non-convexity prevents us from obtaining convergence\nresults for this algorithm.\n2. Dual Approximate Dynamic Programming revisited\nWe now propose a new version of the DADP algorithm and show how it overcomes the above mentioned drawbacks encountered with the original algorithm. In\nthis new approach, we do not suppose a given dynamics for the multipliers anymore.\nStill, we use the standard price decomposition algorithm and perform the update\nof the multipliers scenario-wise using the classical gradient step:\n\u03bbk+1,s\n= \u03bbk,s\nt\nt + \u03c1t \u00d7\n\nN\nX\ni=1\n\n\u0010\n\u0011\ni,k,s\ns\ngti X i,k,s\n,\nU\n,\nW\nt\nt\nt ,\n\n\u2200s = 1, . . . , S.\n\nThe difficulty is now to solve the subproblems, as explained in \u00a72.1.\n2.1. Projection of the dual process. After Lagrangian decomposition of Problem (3) with a given multiplier \u03bb, the i-th subproblem reads:\n!\nT\n\u22121 \u0010\nX\n\u0001\n\u0001\u0011\n\u0001\n\u22a4 i\ni\ni\ni\ni\ni\ni\ni\n(6a)\nCt X t , U t , W t + \u03bbt gt X t , U t , W t + K X T\nmin E\nX i ,U i\n\nt=0\n\nsubject to dynamic constraints:\n(6b)\n(6c)\n\n\u0001\nX it+1 = fti X it , U it , W t ,\nX i0\n\nis given,\n\n\u2200t = 0, . . . , T \u2212 1,\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n9\n\nas well as bound constraints:\n(6d)\n\nxit \u2264 X it \u2264 xit ,\n\n(6e)\n\nuit\n\n\u2264\n\nU it\n\n\u2264\n\nuit ,\n\n\u2200t = 1, . . . , T,\n\u2200t = 0, . . . , T \u2212 1,\n\nand the non-anticipativity constraint:\nU it is At -measurable.\n\n(6f)\n\nAs it was already mentioned, since the dual stochastic process \u03bb generally depends on the whole history of the process, solving this problem is in general as\ncomplex as solving the original problem. In order to bypass this difficulty, let us\nchoose at each time step t a random variable Y it that is measurable with respect\nto At . We call Y i = (Y i0 , . . . , Y iT \u22121 ) the information process for subsystem i.\nThe idea is to rely on a short memory process Y i . Note that we require that this\nrandom process is not influenced by controls. We propose to replace Problem (6)\nby:\n(7)\n!\nT\n\u22121 \u0010\nX\n\u0001\n\u0001\u0011\n\u0001\n\u0001\ni\ni\ni\ni \u22a4 i\ni\ni\ni\ni\n,\nCt X t , U t , W t + E \u03bbt Y t gt X t , U t , W t + K X T\nmin\nE\ni\ni\nX ,U\n\nt=0\n\nsubject to constraints (6b)\u2013(6f).\nLet us first examine the special situation in which the information variable Y it\nonly depends on the current noise W t . The process Y i does not add memory in\nthe system so that Problem (7) can be solved using the standard DP equation:\nVTi (x) = K i (x) ,\n\u2200x \u2208 XiT ,\n\u0010\nVti (x) = E mini Cti (x, u, W t ) + E \u03bbt\nu\u2208U\n\nY it\n\n\u0001\u22a4\n\ngti (x, u, W t )\n\n\u0001\u0011\ni\nfti (x, u, W t ) ,\n+ Vt+1\n\n\u2200x \u2208 Xit .\n\nThe expectation quadrature only involves the noise variable W t . Remember, as\nexplained in Remark 2, that we are in the \"hazard-decision\" setting: even though\nthe control at each instant t depends on both X it and W t , the Bellman function\nonly depends on X it .\nBecause of the overtime independence of the information variables Y it , we have\nto solve DP equations whose dimension is the subsystem dimension ni . Let us give\nthree examples of choices for Y it .\nExample 1 (Maximal information). One can choose to include in Y it all the noise\nat time t. As already explained in Remark 3, the cost function and dynamics of\na subsystem may only depend on a part of the whole noise W t (a kind of local\ninformation denoted by W it in Remark 3). Yet some global noise, denoted by Dt\nin Remark 3 may appear in the coupling constraint (e.g. a global power demand).\nHence this maximal choice for the information variable makes the multiplier depend\non both local and global information: this shall improve the subsystem's vision of\nthe rest of the system and hence improves strategies. Note, however, that including all the noise at time t in the information variable is only possible in practice\nwhen the noise dimension is not too large. Indeed, the information variable appears in a conditional expectation, whose computation is subject to the curse of\ndimensionality.\nExample 2 (Minimal information). On the opposite, one can choose Y it = 0 or any\nother constant. The dual stochastic process is then approximated by its expectation at every instant. Compared to the previous example, there is no conditional\n\n\f10\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\nScenario-wise coordination\n\u03bbk+1\n= \u03bbkt + \u03c1 \u00d7\nt\n\nN\nX\n\n\u0011\n\u0010\ni,k\ngti X i,k\nt , Ut , W t\n\n1,\n\nte\n\nco\n\nk\n\na\nst k\nd\n\u03bb\nn\na\nice\npr\n\nk\n\nco\nprice \u03bbk\n\n1,\n\nlU\nro\nnt\n\nX\n\ncontrol U i,k and state X i,k\n\ni=1\n\nnt\nro\nlU\nN\n,k\npr\nice\na\n\u03bb k nd\n\nst\n\nat\n\ne\n\nX\n\nN\n\n,k\n\nSubproblem 1\n\nSubproblem i\n\nSuproblem N\n\nCompute E(\u03bbt | Y 1t )\nand solve subproblem\n\nCompute E(\u03bbt | Y it )\nand solve subproblem\n\nCompute E(\u03bbt | Y N\nt )\nand solve subproblem\n\nFigure 1. Dual Approximate Dynamic Programming\nexpectation anymore but one obtains a strategy that corresponds to the vision of\nan average price.\nExample 3 (In between). One can choose Y it of the form hit (W t ). In practice,\nthis choice will be guided by the intuition one has on which information mostly\n\"explains\" the optimal price of the system. One has to make a compromise between\nsufficient information to take reasonable actions and a not too large information\nvariable to be able to compute the conditional expectation in (7).\nLet us move towards the general case where one can choose to keep some information in memory. In other words, one can choose an information variable that has\na Markovian dynamics, i.e. of the form Y it+1 = hit (Y it , W t+1 ). In order to derive\na DP equation in this case, one has to augment the state vector by embedding Y it ,\nthat is the necessary memory to compute the next information variable. Thus,\nthe Bellman function associated with the i-th subproblem depends, at time t, on\nboth X it and Y it\u22121 . The DP equation writes:\n\u0011\n\u0010\n\u0010\nY it * gti (x, u, W t )\nVti (x, y) = E min Cti (x, u, W t ) + E \u03bb\u22a4\nt\nu\n\u0001\u0011\ni\n+ Vt+1\nfti (x, u, W t ) , Y it ,\nwith Y it = hit\u22121 (y, W t ) .\n\nWhen solving this equation, one obtains controls as feedback functions on the local\nstock X it , the current noise W t and the information variable Y it\u22121 of the previous\ntime step. The index gap between information and stock variables comes from the\n\"hazard-decision\" setting: at time t, the information that is used to take decisions\nis the conjunction of the information kept in memory (that has index t \u2212 1) and of\nthe noise observed at the current time step W t . The sketch of the DADP algorithm\nis depicted in Figure 1.\nExample 4 (Perfect memory). The choice\nY it = (W 0 , . . . , W t ) stands in the Mar\u0001\ni\nkovian case. We have then E \u03bbt Y t = \u03bbt . This choice hence allows us to model\nthe dual variable perfectly, but the induced DP equation is unsolvable in practice.\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n11\n\nExample 5 (Strugarek, 2006). In his PhD thesis, Strugarek exhibited a case when an\nexact model for the dual process can be obtained. His example is inspired from the\nkind of power management problem mentioned in the introduction, where N water\nreservoirs have to contribute to a global power demand, the rest of this demand\nbeing produced by fossil fuel. The noise at each time step t is composed of a scalar\ninflow Ait for each reservoir i = 1, . . . , N , and of a scalar power demand Dt . The\nproblem reads:\n\uf8eb\n\uf8f6\n\u0010 \u00112\nj\nT\n\u22121 X\nn\n\u0010\n\u0011\nU\nX\n2\nt\n\u03b3j\n\uf8ec\n\uf8f7\ncj\nmin E \uf8ed\n+\nX jt \u2212 xj1 \uf8f8 ,\n(8a)\nX,U\n2\n2\nt=1 j=1\nwhere cj , j = 1, . . . , N and \u03b3j , j = 1, . . . , N are given real values, subject to\ndynamic constraints on reservoirs:\n(8b)\n\nX jt+1 = X jt + Ajt+1 \u2212 U jt ,\n\n\u2200t = 1, . . . , T \u2212 1, \u2200j = 1, . . . , n,\n\nthe power demand constraint:\n(8c)\n\nn\nX\n\nU jt = Dt ,\n\n\u2200t = 1, . . . , T \u2212 1,\n\nj=1\n\nand the non-anticipativity constraint:\n(8d)\n\nU t is \u03c3 {Ds , s \u2264 t ; As , s \u2264 t} -measurable.\nPN\nLet us denote A\u03c3t := i=1 Ait . The author then shows the following result.\n\nProposition 1 (Strugarek, 2006, Chapter V). If random variables (D t , At )t=1,...,T\nare independent over time, and if there exists \u03b1 > 0 such that \u03b3j = \u03b1cj , for all j =\n1, . . . , n, then the optimal multiplier \u03bb associated with the coupling constraints (8c)\nsatisfies the following dynamics:\n!\nT\nT\n\u22121\nX\nX\n1\nE (A\u03c3s ) \u2212 \u03b1\nE (D s ) ,\n\u03bb1 = Pn 1 D 1 (1 \u2212 \u03b1) \u2212 \u03b1\nj=1 cj\n\ns=2\n\n1\n\n\u03bbt+1 = \u03bbt + Pn\n\n1\ni=1 ci\n\ns=2\n\nh\nD t+1 (1 + \u03b1) \u2212 D t \u2212 \u03b1E (Dt+1 )\n\n\u2212 \u03b1 A\u03c3t+1 \u2212 E A\u03c3t+1\n\n\u0001\u0001 i\n,\n\n\u2200t = 1, . . . , T \u2212 2.\n\nThis allows the solving of subproblems using DP in dimension 3. Note that this\nexample enters our approach if one chooses (Y t , Dt ) as an information variable,\nwith:\n!\nT\nT\n\u22121\nX\nX\n1\nE (A\u03c3s ) \u2212 \u03b1\nE (Ds ) ,\nY 1 = Pn 1 D1 (1 \u2212 \u03b1) \u2212 \u03b1\ni=1 ci\n\ns=2\n\ns=2\n\nand, for all t = 1, . . . , T \u2212 2:\nh\n\u0001\u0001 i\n1\nY t+1 = Y t + Pn 1 D t+1 (1 + \u03b1) \u2212 D t \u2212 \u03b1E (Dt+1 ) \u2212 \u03b1 A\u03c3t+1 \u2212 E A\u03c3t+1\n.\ni=1 ci\n\nWe get back to the particular case when E(\u03bbt | Y it ) = \u03bbt , with a small dimensional\ninformation variable Y it . Note however that conditions of Proposition 1, especially\nthe proportionality relation on costs, make little sense in practice.\n\n\f12\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\n2.2. Convergence. We now give convergence results about DADP and explain in\nmore details the relation between the strategies it builds and the solution of the\noriginal problem (1). To make the paper self-contained, we recall in Appendix A\nthe general results concerning duality in optimization, of which the properties of\nDADP are direct consequences.\nThe approximation made on the dual process gives us a tractable way of computing strategies for each one of the subsystems. Depending on the choice we make\nfor the information variable, it is quite clear that some strategies will lead to better\nresults than others, concerning the value of the dual problem or the satisfaction of\nthe coupling constraint. Let us here state more precisely these facts.\nFrom now on, we consider a unique information variable for all subsystems. We\ndenote it by Y t and define Hilbert spaces\nYt := {\u03bbt \u2208 L2 (\u03a9, A, P) : \u03bbt is Y t -measurable},\nfor every t = 0, . . . , T \u2212 1.\nProposition 2. Consider the following optimization problem:\n!\nN\nT\n\u22121 X\nN\nX\n\u0001\n\u0001 X\ni\ni\ni\ni\ni\n(9a)\n,\nK XT\nCt X t , U t , W t +\nmin E\nX,U\n\nt=0 i=1\n\ni=1\n\nsubject to the same constraints as in Problem (3) except the coupling constraint (3g)\nwhich is replaced by:\n!\nN\nX\n\u0001\n(9b)\n\u2200t = 0, . . . , T, .\nY t = 0,\ngti X it , U it , W t\nE\ni=1\n\nSuppose the Lagrangian associated with Problem (9) has a saddle point. Then\nDADP solves Problem (9).\n\nProof. The DADP algorithm consists in:\n\u2022 given a price process, solving subproblems using the projection of this price\nprocess on Y0 \u00d7 * * * \u00d7 YT \u22121 ;\n\u2022 updating the price process using a gradient formula.\nAlternatively, one may consider that the gradient formula is composed with the\nprojection operation in the updating formula. Therefore, this algorithm may also\nbe viewed as a projected gradient algorithm which exactly solves the following\nmax-min problem :\n(10a)\n!\nN\nT X\nN \u0010\nX\n\u0001\u0011 X\n\u0001\n\u0001\n\u22a4 i\ni\ni\ni\ni\ni\ni\ni\n,\nmax min E\nK XT\nCt X t , U t , W t + \u03bbt gt X t , U t , W t +\n\u03bb\n\nX,U\n\nt=0 i=1\n\ni=1\n\n(10b)\n\n(10c)\n\n\u0001\ns.t. X it+1 = fti X it , U it , W t ,\n\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N,\n\nX 0 = W 0,\n\n(10d)\n\nxit \u2264 X it \u2264 xit ,\n\n(10e)\n\nuit\n\n(10f)\n\nU t is At -measurable,\n\n\u2200t = 0, . . . , T,\n\n(10g)\n\n\u03bbt is Y t -measurable,\n\n\u2200t = 0, . . . , T.\n\n\u2264\n\nU it\n\n\u2264\n\nuit ,\n\n\u2200t = 1, . . . , T, \u2200i = 1, . . . , N,\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N,\n\nObserve that the max operation is restricted to a linear\n\u0001 subspace defined by (10g).\na , b i = E a \u22a4b , the variable a belongs to\nNow, if within the inner product ha\na given subspace, then the component of b which is orthogonal to that subspace\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n13\n\nyields 0 in the inner product. Hence it is useless. Put\n\u0001 in our context, the multiplier \u03bbt can only control the part of gti X it , U it , W t which has the same measurability as \u03bbt . Thus, assuming the existence of a saddle point, that is, the max and\nmin operations can be interchanged in Problem (10), this problem appears as the\ndual counterpart of Problem (9).\n\u0003\nLoosely speaking, DADP somehow consists in replacing an almost-sure constraint by a constraint involving a conditional expectation with respect to a so-called\ninformation variable. So it is once again clear that if we choose the information\nvariable Y t to be the whole history of the system, then we come back to the initial\nconstraint and we in fact solve the original problem. This is the case of Example 4. On the contrary, putting no information at all in Y t is the same as satisfying\nthe coupling constraint only in expectation. This is the case of Example 2. Note\nhowever that it is generally a poor way of representing an almost-sure constraint.\nThe main difficulty is to find the information variable Y t that is going to satisfy\nthe coupling constraint in a fairly good way while keeping the solving process of\nthe subproblems tractable.\nWe now state the convergence of the DADP algorithm. Let us introduce the\nobjective function J : U0 \u00d7 * * * \u00d7 UT \u22121 \u2192 R associated with strategy U , i.e.:\n!\nN\nT\n\u22121 X\nN\nX\nX\n\u0001\n\u0001\n,\nK i X iT\nCti X it , U it , W t +\nJ : U 7\u2192 E\nt=0 i=1\n\ni=1\n\nwith: X 0 = W 0 ,\n\n\u0001\nand: X it+1 = fti X it , U it , W t ,\n\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N.\n\nProposition 3. If:\n(1) J is convex, lower semi-continuous, G\u00e2teaux differentiable,\n(2) J is \u03b1-strongly convex,\n(3) all gti are linear and c-Lipschitz continuous,\n(4) the Lagrangian associated with Problem (9) has a saddle point (U , \u03bb),\n(5) the step-size \u03c1 of the algorithm is such that 0 < \u03c1 < 2 c\u03b12 ,\nThen:\n(1) there exists a unique solution U of Problem (9),\n(2) DADP converges in the sense that :\nU k \u2212\u2192 U in U0 \u00d7 * * * \u00d7 UT \u22121 ,\nk\u2192+\u221e\n\nk\n\n(3) the sequence (\u03bb )k\u22650 is bounded and every cluster point \u03bb in the weak\ntopology is such that (U , \u03bb) is a saddle point of the Lagrangian associated\nwith Problem (9).\nProof. The convergence of the algorithm is then a direct application of Theorem 1,\nAppendix A.\n\u0003\nNote that assumptions of Proposition 3 plus the qualification of constraint (9b)\nensure that the Lagrangian associated with Problem (9) has a saddle point.\n3. Numerical experiment\nWe now show the efficiency of DADP on two numerical examples. The first one\ncomes from a previous paper (Barty et al., 2010) in which the authors developped\na preliminary version of DADP (see \u00a71.4). We show in \u00a73.2 the good performance\nof the new version of DADP. The second one, in \u00a73.3, is an application to a more\nrealistic power management problem.\n\n\f14\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\n3.1. Computing conditional expectations. Within the DADP procedure, at\neach iteration, we have to compute conditional expectations in the criteria (7) of\nthe subproblems. In order to compute these conditional expectations, we used Generalized Additive Models (GAMs), that were introduced by Hastie and Tibshirani\n(1990). The estimate takes the form:\n\nE (Z | P 1 , . . . , P n ) \u2243\n\nn\nX\n\nfi (P i ) .\n\ni=1\n\nFunctions fi are splines (piecewise polynoms) whose characteristics are optimized\nby cross-validation on the input statistical data. Our purpose here is not to explain\nin details this methodology. The interested reader will find further explanations\nabout this model and its implementation in the book by Wood (2006). We used\nan easy-to-use implementation that is available within the free statistical software\nR (R Development Core Team, 2009). The GAM toolkit, called mgcv, also returns\nuseful indicators concerning the quality of the estimation. In particular, we use\nthe deviance indicator, which takes value 0 if Z is estimatedPas poorly as by its\nexpectation E (Z) and value 1 if the estimate is exact, i.e. if ni=1 fi (P i ) = Z.\nRemark 4 (Kernel estimator). We chose to use GAMs to compute conditional expectations after a numerical comparison with the more classical kernel regression\nmethods (Nadaraya, 1964, Watson, 1964) also available in the R environment. Even\nthough both of them gave similar results, GAMs appeared to be several times faster\nthan the kernel method on our problem.\n\n3.2. Back to an example from a previous paper. We first implement the new\nversion of DADP algorithm on a simple power management problem introduced by\nBarty et al. (2010). On this small-scale example, we are able to compare DADP\nresults to those obtained by DP and to illustrate the theoretical results described\nabove. Let us first recall this example. Consider a power producer who owns two\ntypes of power plants:\n\u2022 Two hydraulic plants that are characterized at each time step t by their\nwater stock X it and power production U it , and receive water inflows Ait+1 ,\ni = 1, 2. Such units are usually cost-free. We however impose small quadratic costs on the hydraulic power productions in order to ensure strong\nconvexity.\n\u2022 One thermal unit with a production cost that is quadratic with respect to\nits production U 3t . There are no dynamics associated with this unit.\nUsing these plants, the power producer must supply a power demand D t at each\ntime step t, over a discrete time horizon of T = 25 time steps. All noises, i.e.\ndemand Dt and inflows A1t and A2t are supposed to be overtime independent noise\nprocesses. The interested reader may find more details on this numerical experiment\nin the previous paper by Barty et al. (2010).\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n15\n\nThe problem reads:\n(11a)\n\nmin\n\nE\n\nX,U\n\nT\n\u22121 \u0010\nX\n\n\u01eb\n\nt=0\n\n(11b)\n(11c)\n(11d)\n(11e)\n(11f)\n(11g)\n\n\u00012\nU 1t\n\n+\u01eb\n\n\u00012\nU 2t\n\ns.t. X it+1 = X it \u2212 U it + Ait+1 ,\n\n+\n\nLt U 3t\n\n\u0001\u0011\n\n+K\n\n\u2200i = 1, 2,\n\n1\n\nX 1T\n\n\u0001\n\n+K\n\n2\n\nX 2T\n\n\u2200t = 0, . . . , T \u2212 1,\n\n\u2200t = 0, . . . , T \u2212 1,\n+ U 2t + U 3t = D t ,\ni\ni\ni\nx \u2264 Xt \u2264 x ,\n\u2200i = 1, 2, \u2200t = 1, . . . , T,\ni\ni\n0 \u2264 Ut \u2264 u ,\n\u2200i = 1, 2, \u2200t = 0, . . . , T \u2212 1,\n3\n0 \u2264 Ut,\n\u2200t = 0, . . . , T \u2212 1,\n\b\ni\nU t is \u03c3 D 0 , A10 , A20 , . . . , Dt , A1t , A2t -measurable,\n\n!\n\u0001\n\nU 1t\n\n\u2200i = 1, 2, 3.\n\nIn this problem, the state X t is two-dimensional, hence DP remains numerically\ntractable and we can use the DP solution as a reference. In order to use DADP, we\nchoose an information variable Y t at time t that is equal to the power demand Dt .\nThis comes from the insight that the power demand is a \"global\" information and\nhas all reasons to be useful to the subproblems.\nRemark 5 (Primal feasibility). In order to validate the method, it has to be evaluated within a simulation procedure. For the evaluation to be fair, the strategy must\nbe feasible. Yet, as explained in \u00a72.2, DADP does not ensure that the coupling constraint (3f) is satisfied. To circumvent this difficulty, the thermal unit strategy is\nchosen in the simulation process so as to ensure feasibility of the coupling constraint,\ni.e.:\n\u0001\n(12)\nU 3t = D t \u2212 U 1t + U 2t .\nThat is, DADP returns three strategies, for each of the hydraulic units and for\nthe thermal unit. However, we use relation (12) for the thermal strategy during\nsimulations in order to ensure demand satisfaction and give an estimation of the\ncost of the DADP strategy.\n\nWe run the algorithm for 20 iterations and depict its behaviour in Figure 2. We\ndraw the dual cost (evaluation of the dual function with the current strategy) and\nthe primal cost (the one with all constraints satisfied) at each iteration. Each point\nof the primal and dual curves is computed by Monte Carlo simulation over 500\nscenarios. We observe the regular increase of the dual function, as expected, and\nthe decrease of the primal function. The distance between the primal and dual\ncosts is an upper bound for the distance to the optimal value that graphically, in\nthis case, seems quite tight.\nMoreover, the GAM toolkit used to compute the conditional expectations of the\nform E (\u03bbt | D t ) returns that the deviance, i.e. the quality of the explanation of \u03bbt\nby D t is 98.5%. This indicates that the marginal cost of the system is almost\nperfectly explained by the time variable and the power demand. Otherwise stated,\nusing E (\u03bbt | D t ) instead of using \u03bbt within Problem (11) does not alter too much\nthe quality of the solution.\n3.3. A larger-scale SOC problem. We now apply DADP on a real-life power\nmanagement problem, inspired by a case encountered at EDF, which is the major\nEuropean power producer. We do not give the exact order of magnitude for costs\nand productions because of confidentiality issues. We consider :\n\u2022 a power demand on a single node (we neglect network issues) at each instant\nof a finite time horizon of 163 weeks (one time step per week);\n\u2022 7 (hydraulic) stocks which are in fact aggregations of many smaller stocks;\n\n\f16\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\n1,700\nprimal cost\ndual cost\noptimal cost\n1,600\n\ncost\n\n1,500\n\n1,400\n\n1,300\n\n1,200\n2\n\n4\n\n6\n\n8\n\n10\n12\niterations\n\n14\n\n16\n\n18\n\n20\n\nFigure 2. Primal, dual and optimal costs with respect to the\nnumber of iterations\n\n\u2022 122 other (thermal) power units with no stock constraints.\nAll the thermal power units are aggregated so that the thermal cost C t at each\ntime t only depends on the total thermal production U th\nt and forms a quadratic\ncost. We note C t using bold letters, which means that this thermal cost is random,\nbecause of the breakdowns that may happen on thermal power plants.\nThe problem reads:\n(13a)\nmin\n\nE\n\nX,U\n\nT\n\u22121\nX\nt=0\n\n!\n\u0010\n\u0011\nth\n,\nCt U t\n\nsubject to hydraulic stock dynamics :\n(13b)\n\nX i0 = xi0 ,\n\n(13c)\n\nX it+1\n\n=\n\nX it\n\n\u2200i = 1, . . . , 7,\n\u2212 U it + Ait ,\n\n\u2200i = 1, . . . , 7, \u2200t = 0, . . . , T \u2212 1,\n\npower demand constraints :\n\n(13d)\n\n7\nX\ni=1\n\nU it + U th\nt = Dt ,\n\n\u2200t = 0, . . . , T \u2212 1,\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n17\n\nbound constraints on stocks and controls :\n(13e)\n\nth\nth\nuth\nt \u2264 U t \u2264 ut ,\n\n(13f)\n\nuit \u2264\n\n(13g)\n\nxit\n\n\u2264\n\nU it\nX it\n\n\u2264 uit ,\n\u2264\n\nxit ,\n\n\u2200t = 0, . . . , T \u2212 1,\n\u2200i = 1, . . . , 7, \u2200t = 0, . . . , T \u2212 1,\n\u2200i = 1, . . . , 7, \u2200t = 0, . . . , T,\n\nand non-anticipativity constraints :\n(13h)\n\nU it is (W 0 , . . . , W t ) -measurable,\n\n(13i)\n\nU th\nt\n\nis (W 0 , . . . , W t ) -measurable,\n\n\u2200i = 1, . . . , 7, \u2200t = 0, . . . , T \u2212 1,\n\u2200t = 0, . . . , T \u2212 1,\n\nwith W t := (At , C t , D t ) being the set of all noises that affect the system at time t.\nBecause we consider 7 stocks, we are unable to use DP directly on this problem.\nIn order to obtain a reference point, we use an aggregation method introduced by\nTurgeon (1980) and currently in use at EDF. This numerical method is known to be\nespecially well-suited for the problem under consideration. It consists in solving N\nsubproblems (7 in our case) by 2-dimensional DP, each subproblem relying on a\nparticular power unit, instead of one N -dimensional DP problem. The idea is, for\nevery unit, to look for strategies that depend on the stock of the unit and on an\naggregation of the remaining stocks.\nWe then make use of DADP using three different choices for the information\nvariable Y t .\n\u2022 In the first setting, we replace the price at each time step by its expectation.\nIn other words, we explain the price only by the time variable t. According\nto Proposition 3, we are in fact solving Problem (13) with constraint (13d)\nreplaced by its expectation. Then we are able to solve each subproblem i\nby DP in dimension 1 (the stock variable of unit i) and we obtain strategies\nthat depend, for each unit i and each instant t, on the stock X it and the\ninflow Ait .\n\u2022 In the second setting, we replace the price at each time step by its conditional expectation with respect to the power demand. Put differently,\nwe explain the price by time and demand. We still have to solve a 1dimensional DP equation and we obtain for each instant t a strategy that\ndepends on X it , Ait and Dt .\n\u2022 In the third setting, we replace the price at each instant by its conditional\nexpectation with respect to the power demand and the thermal availability5 P t . We then obtain a strategy that depends, for every unit i and every\ninstant t, on X it , Ait , D t and P t .\nThe behaviour of the algorithm in the second setting is depicted in Figure 3.\nWe observe the increase of the dual value and the decrease of the primal value,\nthe latter value stabilizing rapidly to a value close to the one of the aggregation\nmethod. Even though we are aware that only 10 iterations is generally much too\nless for this kind of primal-dual algorithm, it seems like the primal cost does not\nevolve significantly after 10 iterations.\nIn order to compare the three settings, we simulate the corresponding strategies6 on a large set of i.i.d. noise scenarios and compute both the mean cost and\nconfidence interval for each strategy. The results are presented in Table 1. The\n\"Deviance\" column gives the deviance indicator returned by the GAM procedure\n5The thermal availability is a scalar variable computed out of the thermal cost function C . It\nt\ngives insight on how tense the thermal generation mix is.\n6As in the previous example, the thermal unit strategy is chosen so as to ensure feasibility of\nthe coupling constraint (see Remark 5).\n\n\f18\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\ndual cost\nprimal cost\nagg cost\n\n2.4\n\ncost\n\n2.35\n2.3\n2.25\n2.2\n\n1\n\n2\n\n3\n\n4\n\n5\n6\niterations\n\n7\n\n8\n\n9\n\n10\n\nFigure 3. Primal and dual costs along with iterations compared\nto the aggregation method\nMean cost\nFirst setting\n2.363\nSecond setting\n2.340\nThird setting\n2.338\nTable 1. Results\n\nCI95%\nDeviance\n1.3 * 10\u22122\n50.0%\n1.3 * 10\u22122\n82.4%\n1.3 * 10\u22122\n86.1%\nfor DADP\n\nP\n\n\u22125\n\n\u22124\n\n\u22123\n\n\u22122\n\n\u22121\n\n0\ncost\n\nset. 2 - set. 1\nset. 3 - set. 2\n\n1\n\n2\n\n3\n\n4\n\n5\n\nFigure 4. Distribution of cost differences between settings of DADP\nfor the estimation of the conditional expectation of the price with respect to the\ninformation variable. We observe that the DADP strategy still benefits from a good\nchoice for the information variable Y t : it appears from the mean costs comparison\nthat adding information within the estimator improves the quality of the estimation. The mean costs differences are however not so easy to compare for the two\nlast experiments, because the confidence interval is too large compared to the cost\nvalues. Thus we compute for each scenario the gap between costs obtained by two\ndifferent strategies and draw in Figure 4 the associated probability distributions.\nIt becomes clearer that adding the thermal availability in the information variable\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\nP\n\n\u22124\n\n\u22122\n\n19\n\nit. 2\nit. 4\nit. 9\n\n0\n\n2\npower\n\n4\n\n6\n\n8\n\nFigure 5. Distribution of the production/demand gap for a given\ntime step\nimproves the strategy: the major part of the probability weight when comparing\nsettings 2 and 3 is negative.\nAs a last point, let us numerically verify that Proposition 3 holds in our example,\nfor instance in the first setting. Remember that, in this case, our algorithm aims\nat satisfying the coupling constraint only in expectation. We draw in Figure 5 the\nprobability distribution of the production/demand gap at several iterations. We\nobserve that, along with iterations, the distribution of this gap becomes symmetric\nwith respect to 0, the corresponding expectation hence being equal to zero.\nConclusion\nWe presented an original algorithm for solving a certain kind of large-scale stochastic optimal control problems. It is based on an approximate Lagrangian decomposition: the Lagrange multiplier, which is a stochastic process in this context,\nis projected using a conditional expectation with respect to another stochastic process called the information process. This information process is chosen a priori and,\nwhen it has a limited memory, the solving of subproblems becomes tractable. We\ngive theoretical results concerning the convergence of the algorithm and show how\nit actually solves an approximate problem, whose relation with the original problem\nis driven by the choice of information variable. Finally, we show on two numerical\nexamples the efficiency of the approach.\nFuture works will be concerned with the application of this algorithm to more\ngeneral problem structures, like chained subsystems or networks.\nAppendix A. Duality in convex optimization\nThe results presented here come from the paper by Cohen (1980a). Let U and \u039b\nbe Hilbert spaces7, and U ad and \u039bad be subsets of U and \u039b (respectively). Moreover,\nlet us define a function L : U \u00d7 \u039b \u2192 R. We describe here the relations that link the\nso-called primal problem:\n(14)\n\ninf\n\nsup L (u, \u03bb) ,\n\nsup\n\ninf L (u, \u03bb) .\n\nu\u2208U ad \u03bb\u2208\u039bad\n\nto its dual counterpart:\nad\n\u03bb\u2208\u039bad u\u2208U\n\n7These results can be generalized to Banach spaces (see Ekeland and Temam, 1999), but this\nis not necessary for our purpose.\n\n\f20\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\nU is called the primal space while \u039b is called the dual one.\nDefinition 1 (Saddle point). A pair (u, \u03bb) \u2208 U ad \u00d7 \u039bad is called a saddle point\nof L on U ad \u00d7 \u039bad if:\n\u0001\n\u0001\n\u2200u \u2208 U ad , \u2200\u03bb \u2208 \u039bad .\nL (u, \u03bb) \u2264 L u, \u03bb \u2264 L u, \u03bb ,\n\nLet us now concentrate on the case where function L corresponds to the Lagrangian of an optimization problem:\nL (u, \u03bb) = J (u) + h\u03bb, g (u)i .\n\nThe Uzawa algorithm is defined as follows. Take an initial value \u03bb0 \u2208 \u039bad . At each\niteration n \u2265 0, compute un by minimizing J (u) + h\u03bbn , g (u)i, and update \u03bbn using\nthe following rule:\n\u03bbn+1 = \u03a0\u039bad (\u03bbn + \u03c1n g (un )) ,\nwith \u03c1n some positive value. The following theorem gives conditions for the sequence (un )n\u22650 to converge to the optimum of Problem (14).\nTheorem 1 (Cohen, 1980a, Theorem 6.1). If:\n(1) J is convex, lower semi-continuous, G\u00e2teaux differentiable,\n(2) J is \u03b1-strongly convex,\n(3) g is linear and c-Lipschitz continuous,\n(4) L has at least a saddle point (u, \u03bb),\n(5) the step-size \u03c1 of the algorithm is such that 0 < \u03c1 < 2 c\u03b12 ,\nthen:\n(1) u is unique and is a solution of Problem (14),\n(2) Uzawa's algorithm converges in the sense that :\nun \u2212\u2192 u in U,\nn\u2192+\u221e\n\n(3) the sequence (\u03bbn )n\u22650 is bounded and every cluster point \u03bb in the weak topology is such that (u, \u03bb) is a saddle point of L.\nGiven the other assumptions of the theorem, assumption (4) is satisfied as long\nas the dualized constraint satisfies a so-called \"qualification\" condition. In addition, the latter is always satisfied for affine constraints, which is the case in our\napplication.\nAppendix B. A lemma about decomposition\nWe here depict in more details the reasons why a Stochastic Optimal Problem (SOC) involving N independent8 subsystems is equivalent, under certain conditions, to N problems where each one involves only one of the subsystems. Though\nthis result may seem trivial at first sight, it is not true in general: the interested\nreader will find a counter example in the paper by Cohen (1980b).\nLemma 1. Consider the following problem:\n(15a) min\n\nE\n\nX,U\n\nT\n\u22121 X\nN\nX\n\nCti\n\nX it , U it , W it , Z t\n\nt=0 i=1\n\nsubject to dynamics constraints:\n(15b)\n(15c)\n\n\u0001\nX it+1 = fti X it , U it , W it , Z t ,\n\nX i0\n\nis given,\n\n\u0001\n\n\u2200i = 1, . . . , N,\n\n8in a sense that is made clear in Lemma 1\n\n+\n\nN\nX\ni=1\n\nK\n\ni\n\nX iT\n\n!\n\u0001\n\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N,\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n21\n\nas well as bound constraints:\n(15d)\n\nxit \u2264 X it \u2264 xit ,\n\n(15e)\n\nuit\n\n\u2264\n\nU it\n\n\u2264\n\nuit ,\n\n\u2200t = 0, . . . , T, \u2200i = 1, . . . , N,\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N,\n\nand the non-anticipativity constraint:\n(15f)\n\nU it is At -measurable,\n\n\u2200t = 0, . . . , T \u2212 1, \u2200i = 1, . . . , N,\n\nwhere At is the \u03c3-algebra generated by the random variables {W is , Z s } for i =\n1, . . . , N and s = 0, . . . , t. We assume that:\n\u2022 the W i* 's and Z * are all white noise processes,\n\u2022 that W it is not necessarily independent from W jt for j 6= i nor from Z t .\nThen, the optimal feedback solution is partially decentralized, that is, each optimal\ndecision U it , that may a priori depend on the whole X t and the whole W t and\nZ t according to (15f), indeed only depends on (X it , W it , Z t ); the Bellman function\nPN\nVt (X t ) is additive (Vt (X t ) = i=1 Vti (X it )) and the optimal solution only involves\nthe marginal probability laws of the pairs (W it , Z t ) but not the joint probability laws\nof the pairs (W t , Z t ).\nProof. The proof is by induction over time. The statement that V is additive is\ntrue at the final time T since the final cost K is additive. Assume this is true from\nT to t + 1 (backward). The Bellman equation at t reads:\n\u0013\n\u0012\nN\nN\nX\nX\n\u0001\ni\nVt+1\nfti (xi , ui , W it , Z t ) ,\nCti (xi , ui , W it , Z t ) +\nVt (x) = E min\nu\n\ni=1\n\ni=1\n\nin which\n\u2022 the minimization operation is done over an expression is which x, Z t and\nW it are fixed (hazard-decision scheme) and the arg min in u parametrically\ndepends on those values (which yields the optimal feedback function) ;\n\u2022 the minimization operation is subject to the bound constraints (15e) for ui\nand (15d) for fti (xi , ui , W it , Z t ) ;\n\u2022 the expectation concerns random variables (W t , Z t ) whereas x is still fixed\n(X t and (W t , Z t ) are independent from each other, thus this expectation\nmay be considered as a conditional expectation knowing that X t = x): this\nyields a function of x, namely Vt (*).\nNow observe that, at the minimization stage, each ui is involved into a separate\nexpression depending only on xi , W it and Z t subject also to independent constraints, hence the claimed partially decentralized optimal feedback. Then, at the\nouter expectation stage, we get a sum of functions of xi and (W it , Z t ): thus only\nthe marginal probability law of each pair (W it , Z t ) is involved in the expectation\nof the corresponding term in this sum, and the result is an additive function of the\nxi , which completes the proof by induction.\n\u0003\nLet us now comment some particular cases.\n\u2022 If Z t is absent and if W i* and W j* are independent whenever j 6= i, then\nthe overall problem is obviously made up of N independent subproblems;\nthe optimal feedbacks are fully decentralized (that is U i is in closed loop\non (X i , W i )), and the optimal controls U i* and U j* are also independent\nrandom variables whenever j 6= i.\n\u2022 If we drop the independency assumption about W i* and W j* , then the same\nsubproblems still provide the overall problem solution with decentralized\nfeedbacks, but U i* and U j* are no longer independent.\n\n\f22\n\nK. BARTY, P. CARPENTIER, G. COHEN, AND P. GIRARDEAU\n\n\u2022 Another \"extreme\" situation is when only the \"shared\" noise Z is present\nin all subsystems (the W i 's are supposed absent for the sake of clarity\nbut now, Z may be thought as the concatenation of all the W i 's). The\nconclusions of the lemma are of course still valid, that is, the Bellman\nfunction is still additive and each term of this sum can be calculated in a\nseparate subproblem, yielding a feedback on (X i , Z). However the price to\nbe payed for the presence of this shared random variable is that, first, the\nminimization operation in the Bellman function is parametrized by both\nxi and Z t , which may be costly if Z t is of large dimension, and, second,\nthe outer expectation in this Bellman equation involves a multiple integral\nover that vector Z t , which may also be costly.\n\nReferences\nL. Bacaud, C. Lemar\u00e9chal, A. Renaud, and C. A. Sagastiz\u00e1bal. Bundle methods in\nstochastic optimal power management: A disaggregated approach using preconditioner. Computational Optimization and Applications, 20(3):227\u2013244, 2001.\nK. Barty. Contributions \u00e0 la discr\u00e9tisation des contraintes de mesurabilit\u00e9 pour les\nprobl\u00e8mes d'optimisation stochastique. Th\u00e8se de doctorat, \u00c9cole Nationale des\nPonts et Chauss\u00e9es, 2004.\nK. Barty, J.-S. Roy, and C. Strugarek. A stochastic gradient type algorithm for\nclosed-loop problems. Mathematical Programming, Series A, 119(1):51\u201378, June\n2009. doi: 10.1007/s10107-007-0201-x.\nK. Barty, P. Carpentier, and P. Girardeau. Decomposition of large-scale stochastic\noptimal control problems. RAIRO Operations Research, 44(3):167\u2013183, 7 2010.\ndoi: 10.1051/ro/2010013.\nR. Bellman. Dynamic Programming. Princeton University Press, New Jersey, 1957.\nR. Bellman and S. E. Dreyfus. Functional approximations and dynamic programming. Math tables and other aides to computation, 13:247\u2013251, 1959.\nD. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2\nedition, 2000. ISBN 1886529094.\nD. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.\nB. Bouchard and X. Warin.\nMonte-Carlo Valorisation of American\noptions:\nfacts and new algorithms to improve existing methods.\nhttp://www.ceremade.dauphine.fr/~bouchard/pdf/BW10.pdf, 2010.\nP. Carpentier, C. Cohen, J.-C. Culioli, and A. Renaud. Stochastic optimization\nof unit commitment: a new decomposition framework. IEEE Transactions on\nPower Systems, 11(2):1067\u20131073, 5 1996.\nG. Cohen. Auxiliary Problem Principle and decomposition of optimization problems. Journal of Optimization Theory and Applications, 32(3):277\u2013305, 11 1980a.\nG. Cohen. Information Exchange Between Independent Stochastic Systems. Journal\nof Optimization Theory and Applications, 32(2):201\u2013210, 10 1980b.\nG. Cohen and J.-C. Culioli. Decomposition Coordination Algorithms for Stochastic\nOptimization. SIAM J. Control Optimization, 28(6):1372\u20131403, 1990.\nD. P. de Farias and B. Van Roy. The Linear Programming Approach to Approximate Dynamic Programming. Oper. Res., 51(6):850\u2013856, 2003.\nI. Ekeland and R. Temam. Convex Analysis and Variational Problems, volume 28\nof Classics in Applied Mathematics. SIAM, 1999.\nT. J. Hastie and R. J. Tibshirani. Generalized Additive Models. Chapman &\nHall/CRC, 1990.\n\n\fPRICE DECOMPOSITION IN LARGE-SCALE STOCHASTIC OPTIMAL CONTROL\n\n23\n\nJ. L. Higle and S. Sen. Stochastic Decomposition: A Statistical Method for Large\nScale Stochastic Linear Programming. Kluwer Academic Publishers, Dordrecht,\n1996.\nF. A. Longstaff and E. S. Schwartz. Valuing american options by simulation: A\nsimple least squares approach. Review of Financial Studies, 14(1):113\u2013147, 2001.\nE. A. Nadaraya. On estimating regression. Theory of Probability and its applications, 10:186\u2013190, 1964.\nR Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2009. URL\nhttp://www.R-project.org. ISBN 3-900051-07-0.\nA. Ruszczy\u0144ski and A. Shapiro, editors. Stochastic Programming, volume 10 of\nHandbooks in Operations Research and Management Science. Elsevier, 2003.\nA. Shapiro. On complexity of multistage stochastic programs. Operations Research\nLetters, 34:1\u20138, 2006.\nA. Shapiro, D. Dentcheva, and A. Ruszczy\u0144ski. Lectures on Stochastic Programming. Society for Industrial and Applied Mathematics, Philadelphia, 2009.\nC. Strugarek. Approches variationnelles et autres contributions en optimisation\nstochastique. PhD thesis, \u00c9cole Nationale des Ponts et Chauss\u00e9es, 5 2006.\nJ. N. Tsitsiklis and B. Van Roy. Feature-based methods for large-scale dynamic\nprogramming. Machine Learning, 22:59\u201394, 1996.\nA. Turgeon. Optimal operation of multi-reservoir power systems with stochastic\ninflows. Water Resources Research, 16(2):275\u2013283, 1980.\nP. Vezolle, S. Vialle, and X. Warin. Large Scale Experiment and Optimization of\na Distributed Stochastic Control Algorithm. Application to Energy Management\nProblems. In International workshop on Large-Scale Parallel Processing (LSPP\n2009), Rome, Italy, 2009. ISBN 978-1-4244-3750-4.\nG. S. Watson. Smooth regression analysis. Shankya Series A, 26:359\u2013372, 1964.\nS. N. Wood. Generalized Additive Models: An Introduction with R. Chapman &\nHall/CRC, 2006.\nK. Barty, EDF R&D, 1 avenue du G\u00e9n\u00e9ral de Gaulle, F-92141 Clamart Cedex, France.\nE-mail address: kengy.barty@edf.fr\nP. Carpentier, ENSTA ParisTech, 32 boulevard Victor, 75739 Paris Cedex 15, France.\nE-mail address: pierre.carpentier@ensta-paristech.fr\nG. Cohen, Universit\u00e9 Paris-Est, CERMICS, \u00c9cole des Ponts ParisTech, 6 & 8 avenue\nBlaise Pascal, 77455 Marne-la-Vall\u00e9e Cedex 2.\nE-mail address: guy.cohen@mail.enpc.fr\nP. Girardeau, EDF R&D, 1 avenue du G\u00e9n\u00e9ral de Gaulle, F-92141 Clamart Cedex,\nFrance, also with Universit\u00e9 Paris-Est, CERMICS and ENSTA ParisTech.\nE-mail address: pierre.girardeau@cermics.enpc.fr\n\n\f"}