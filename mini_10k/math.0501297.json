{"id": "http://arxiv.org/abs/math/0501297v1", "guidislink": true, "updated": "2005-01-19T20:43:34Z", "updated_parsed": [2005, 1, 19, 20, 43, 34, 2, 19, 0], "published": "2005-01-19T20:43:34Z", "published_parsed": [2005, 1, 19, 20, 43, 34, 2, 19, 0], "title": "Generalized Arithmetic and Geometric Mean Divergence Measure and their\n  Statistical Aspects", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0501520%2Cmath%2F0501142%2Cmath%2F0501242%2Cmath%2F0501246%2Cmath%2F0501128%2Cmath%2F0501391%2Cmath%2F0501403%2Cmath%2F0501095%2Cmath%2F0501310%2Cmath%2F0501066%2Cmath%2F0501409%2Cmath%2F0501411%2Cmath%2F0501297%2Cmath%2F0501439%2Cmath%2F0501233%2Cmath%2F0501110%2Cmath%2F0501460%2Cmath%2F0501252%2Cmath%2F0501012%2Cmath%2F0501181%2Cmath%2F0501450%2Cmath%2F0501449%2Cmath%2F0501098%2Cmath%2F0501022%2Cmath%2F0501079%2Cmath%2F0501228%2Cmath%2F0501442%2Cmath%2F0501559%2Cmath%2F0501254%2Cmath%2F0501369%2Cmath%2F0501141%2Cmath%2F0501193%2Cmath%2F0501437%2Cmath%2F0501074%2Cmath%2F0501286%2Cmath%2F0501061%2Cmath%2F0501311%2Cmath%2F0501232%2Cmath%2F0501296%2Cmath%2F0501160%2Cmath%2F0501017%2Cmath%2F0501239%2Cmath%2F0501148%2Cmath%2F0501304%2Cmath%2F0501002%2Cmath%2F0501064%2Cmath%2F0501204%2Cmath%2F0501270%2Cmath%2F0501514%2Cmath%2F0501400%2Cmath%2F0501463%2Cmath%2F0501145%2Cmath%2F0501375%2Cmath%2F0501234%2Cmath%2F0501334%2Cmath%2F0501438%2Cmath%2F0501191%2Cmath%2F0501309%2Cmath%2F0501208%2Cmath%2F0501103%2Cmath%2F0501220%2Cmath%2F0501383%2Cmath%2F0501222%2Cmath%2F0501050%2Cmath%2F0501218%2Cmath%2F0501176%2Cmath%2F0501048%2Cmath%2F0501476%2Cmath%2F0501490%2Cmath%2F0501432%2Cmath%2F0501216%2Cmath%2F0501063%2Cmath%2F0501056%2Cmath%2F0501051%2Cmath%2F0501236%2Cmath%2F0501106%2Cmath%2F0501418%2Cmath%2F0501256%2Cmath%2F0501058%2Cmath%2F0501235%2Cmath%2F0501140%2Cmath%2F0501196%2Cmath%2F0501052%2Cmath%2F0501430%2Cmath%2F0501278%2Cmath%2F0501174%2Cmath%2F0501169%2Cmath%2F0501085%2Cmath%2F0501415%2Cmath%2F0501427%2Cmath%2F0501081%2Cmath%2F0501280%2Cmath%2F0501090%2Cmath%2F0501548%2Cmath%2F0501478%2Cmath%2F0501275%2Cmath%2F0501327%2Cmath%2F0501354%2Cmath%2F0501305%2Cmath%2F0501249%2Cmath%2F0501241&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Generalized Arithmetic and Geometric Mean Divergence Measure and their\n  Statistical Aspects"}, "summary": "Using Blackwell's definition of comparing two experiments, a comparison is\nmade with \\textit{generalized AG - divergence} measure having one and two\nscalar parameters. Connection of \\textit{generalized AG - divergence} measure\nwith \\textit{Fisher measure of information} is also presented. A unified\n\\textit{generalization of AG - divergence}and\\textit{Jensen-Shannon divergence\nmeasures} is also presented.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0501520%2Cmath%2F0501142%2Cmath%2F0501242%2Cmath%2F0501246%2Cmath%2F0501128%2Cmath%2F0501391%2Cmath%2F0501403%2Cmath%2F0501095%2Cmath%2F0501310%2Cmath%2F0501066%2Cmath%2F0501409%2Cmath%2F0501411%2Cmath%2F0501297%2Cmath%2F0501439%2Cmath%2F0501233%2Cmath%2F0501110%2Cmath%2F0501460%2Cmath%2F0501252%2Cmath%2F0501012%2Cmath%2F0501181%2Cmath%2F0501450%2Cmath%2F0501449%2Cmath%2F0501098%2Cmath%2F0501022%2Cmath%2F0501079%2Cmath%2F0501228%2Cmath%2F0501442%2Cmath%2F0501559%2Cmath%2F0501254%2Cmath%2F0501369%2Cmath%2F0501141%2Cmath%2F0501193%2Cmath%2F0501437%2Cmath%2F0501074%2Cmath%2F0501286%2Cmath%2F0501061%2Cmath%2F0501311%2Cmath%2F0501232%2Cmath%2F0501296%2Cmath%2F0501160%2Cmath%2F0501017%2Cmath%2F0501239%2Cmath%2F0501148%2Cmath%2F0501304%2Cmath%2F0501002%2Cmath%2F0501064%2Cmath%2F0501204%2Cmath%2F0501270%2Cmath%2F0501514%2Cmath%2F0501400%2Cmath%2F0501463%2Cmath%2F0501145%2Cmath%2F0501375%2Cmath%2F0501234%2Cmath%2F0501334%2Cmath%2F0501438%2Cmath%2F0501191%2Cmath%2F0501309%2Cmath%2F0501208%2Cmath%2F0501103%2Cmath%2F0501220%2Cmath%2F0501383%2Cmath%2F0501222%2Cmath%2F0501050%2Cmath%2F0501218%2Cmath%2F0501176%2Cmath%2F0501048%2Cmath%2F0501476%2Cmath%2F0501490%2Cmath%2F0501432%2Cmath%2F0501216%2Cmath%2F0501063%2Cmath%2F0501056%2Cmath%2F0501051%2Cmath%2F0501236%2Cmath%2F0501106%2Cmath%2F0501418%2Cmath%2F0501256%2Cmath%2F0501058%2Cmath%2F0501235%2Cmath%2F0501140%2Cmath%2F0501196%2Cmath%2F0501052%2Cmath%2F0501430%2Cmath%2F0501278%2Cmath%2F0501174%2Cmath%2F0501169%2Cmath%2F0501085%2Cmath%2F0501415%2Cmath%2F0501427%2Cmath%2F0501081%2Cmath%2F0501280%2Cmath%2F0501090%2Cmath%2F0501548%2Cmath%2F0501478%2Cmath%2F0501275%2Cmath%2F0501327%2Cmath%2F0501354%2Cmath%2F0501305%2Cmath%2F0501249%2Cmath%2F0501241&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Using Blackwell's definition of comparing two experiments, a comparison is\nmade with \\textit{generalized AG - divergence} measure having one and two\nscalar parameters. Connection of \\textit{generalized AG - divergence} measure\nwith \\textit{Fisher measure of information} is also presented. A unified\n\\textit{generalization of AG - divergence}and\\textit{Jensen-Shannon divergence\nmeasures} is also presented."}, "authors": ["Inder Jeet Taneja"], "author_detail": {"name": "Inder Jeet Taneja"}, "author": "Inder Jeet Taneja", "arxiv_comment": "Ams-Latex style - 15 pages", "links": [{"href": "http://arxiv.org/abs/math/0501297v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0501297v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0501297v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0501297v1", "journal_reference": null, "doi": null, "fulltext": "GENERALIZED ARITHMETIC AND GEOMETRIC MEAN\nDIVERGENCE MEASURE AND THEIR STATISTICAL ASPECTS\n\narXiv:math/0501297v1 [math.ST] 19 Jan 2005\n\nINDER JEET TANEJA\nAbstract. Using Blackwell's definition of comparing two experiments, a comparison is\nmade with generalized AG - divergence measure having one and two scalar parameters.\nConnection of generalized AG - divergence measure with Fisher measure of information\nis also presented. A unified generalization of AG - divergence and Jensen-Shannon\ndivergence measures is also presented.\n\n1. Introduction\nSeveral measures have been introduced in the literature on information theory and\nstatistics as measures of information. The most famous in the literature of statistics is\nFisher [9] measure of information. It measures the amount of information supplied by\ndata about an unknown parameter \u03b8. The most commonly used in information theory is\nthe Shannon [18] entropy. It gives the amount of uncertainty concerning the outcome of\nan experiment. Kullback and Leibler [13] introduced a measure associated with two distributions of an experiment. It expresses the amount of information supplied by the data\nfor discriminating among the distribution. As a symmetric measure, Jeffreys-KullbackLeibler J-divergence is commonly used. R\u00e9nyi [16] generalized both Shannon entropy and\nKullback-Leibler relative information by introducing a scalar parameter. Burbea and\nRao [4], [5] and Taneja [21], [22] have proposed various alternative ways to generalize the\nJeffreys-Kullback-Leibler J-divergence. The proposed measures of Burbe and Rao [4], [5]\ninvolve one parameter. Measures proposed by author [22] involve two scalar parameters.\nLet EX = {X, SX , P\u03b8 ; \u03b8 \u2208 \u0398} denote a statistical experiment in which a random variable or random vector X defined on some sample space SX is to be observed and the\ndistribution P\u03b8 of X depends on the parameter \u03b8 whose values are unknown and lie in\nsome parameter space \u0398. We shall assume that there exists a generalized probability\ndensity function p(x|\u03b8) for the distribution P\u03b8 with respect to \u03c3\u2212finite measure \u03bc. Let\nalso \u039e denote the class of all prior distributions on the parameter space \u0398. Given a prior\ndistribution \u03be \u2208 \u039e, let p(x) denote the corresponding marginal generalized probability\ndensity function (gpdf)\nZ\np(x) =\n\np(x|\u03b8)d\u03be(\u03b8).\n\n\u0398\n\n2000 Mathematics Subject Classification. 94A17; 62B10.\nKey words and phrases. Comparison of experiments; Fisher measure of information; Generalized AG\u2013\ndivergence; JS\u2013divergence.\n\n1\n\n\f2\n\nINDER JEET TANEJA\n\nSimilarly, if we have two prior distributions \u03be1 , \u03be2 \u2208 \u039e, the corresponding marginal\ngpdf's are\nZ\npi (x) =\np(x|\u03b8)d\u03bei (\u03b8), i = 1, 2.\n\u0398\n\nIn this context, the relative information, the J\u2013divergence, the Jensen-Shannon divergence, and the arithmetic and geometric mean divergence measures are given as follows:\n\u2022 Relative information (Kullback and Leibler [13])\n\u0013\n\u0012\nZ\np1 (x)\nd\u03bc.\n(1)\nK(\u03be1 ; \u03be2 ||X) = p1 (x) ln\np2 (x)\n\u2022 J \u2013 divergence (Jeffreys [11], Kullback and Leibler [13])\n\u0012\n\u0013\n\u0012\n\u0013\nZ\nZ\np1 (x)\np2 (x)\n(2)\nJ(\u03be1 ; \u03be2 ||X) = p1 (x) ln\nd\u03bc + p2 (x) ln\nd\u03bc.\np2 (x)\np1 (x)\n\u2022 Jensen\u2013Shannon divergence (Sibson [19], Burbea and Rao [4], [5])\nZ\n1\n(3)\n[p1 (x) ln p1 (x) + p2 (x) ln p2 (x)]\nI(\u03be1; \u03be2 ||X) =\n2\n\u0013 \u0012\n\u0013\nZ \u0012\np1 (x) + p2 (x)\np1 (x) + p2 (x)\n\u2212\nln\nd\u03bc.\n2\n2\n\u2022 AG \u2013 divergence (Taneja [22])\n\u0013\nZ \u0012\np1 (x) + p2 (x)\nln\n(4)\nT (\u03be1 ; \u03be2 ||X) =\n2\n\np1 (x) + p2 (x)\np\n2 p1 (x)p2 (x)\n\n!\n\nd\u03bc.\n\nThe three divergence measures given above can be written in terms of Kullback-Leibler\nrelative information as\n(5)\n(6)\n\nJ(\u03be1 ; \u03be2 ||X) = K(\u03be1 ; \u03be2||X) + K(\u03be2 ; \u03be1||X),\n\u0014\n\u0015\n\u03be1 + \u03be2\n\u03be1 + \u03be2\n1\nK(\u03be1 ;\n||X) + K(\u03be2 ;\n||X)\nI(\u03be1; \u03be2 ||X) =\n2\n2\n2\n\nand\n(7)\n\n\u0014\n\u0015\n1\n\u03be1 + \u03be2\n\u03be1 + \u03be2\nT (\u03be1; \u03be2 ||X) =\nK(\n; \u03be1||X) + K(\n; \u03be2||X) .\n2\n2\n2\n\nMoreover we have the following equality holding among the three divergence measures\n(8)\n\n1\nI(\u03be1 ; \u03be2 ||X) + T (\u03be2 ; \u03be1||X) = J(\u03be1 ; \u03be2 ||X).\n4\n\n\fAG \u2013 DIVERGENCE MEASURES\n\n3\n\nRecently, author [24] proved an interesting inequality among these three divergence\nmeasures:\n1\n(9)\nI(\u03be1 ; \u03be2 ||X) 6 J(\u03be2 ; \u03be1 ||X) 6 T (\u03be1 ; \u03be2||X),\n8\nwhere all the probability distributions involved are positive.\nIn view of (8) the inequalities (9) can be extended as follows:\n1\n1\nI(\u03be1 ; \u03be2||X) 6 J(\u03be2 ; \u03be1 ||X) 6 T (\u03be1; \u03be2 ||X) 6 J(\u03be2 ; \u03be1 ||X).\n8\n4\nBased on above notations, the Csisz\u00e1r [6] \u03c6\u2212divergence is given by\n\u0012\n\u0013\nZ\np1 (x)\n(11)\nC\u03c6 (\u03be1 ; \u03be2 ||X) =\np2 (x)\u03c6\nd\u03bc,\np2 (x)\nX\n(10)\n\nwhere the function \u03c6 is arbitrary convex function defined in the interval (0, \u221e). In order\nto avoid meaningless expressions, the functions \u03c6 satisfy some conventional conditions\ngiven in [6].\nIn this paper we shall present two parameter generalizations of the AG \u2013 divergence.\nAlso we shall present one parametric unified generalization of the measures (3) and (4).\nFor two parametric generalization of the measures (2) and (3) refer to Taneja [22]. Also\nrefer on line book by author [23]. Here, in this paper we shall make connections of generalized AG \u2013 divergence measures with Fisher measure of information. The comparison\nof experiments is also studied applying Blackwell's [2] approach.\n2. Unified (r, s)\u2212Arithmetic and Geometric Mean Divergence Measures\nIn this section, we shall present two different ways of generalizing the AG \u2013 divergence\nmeasure (4). Before it we shall give two parametric unified (r, s)\u2212generalization [22] of\nthe relative information:\n\uf8f1\n\uf8f4\nKrs (\u03be1 ; \u03be2 ||X), r 6= 1, s 6= 1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2K s (\u03be ; \u03be ||X), r = 1, s 6= 1\n1 1 2\ns\n(12)\nKr (\u03be1 ; \u03be2||X) =\n\uf8f4Kr1 (\u03be1 ; \u03be2 ||X), r 6= 1, s = 1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nK(\u03be1 ; \u03be2 ||X), r = 1, s = 1\nfor all r > 0 and \u2212\u221e < s < \u221e, where\n#\n\"\u0012Z\ns\u22121\n\u0013 r\u22121\n\u2212 1 , r 6= 1, s 6= 1\np1 (x)r p2 (x)1\u2212r d\u03bc\nKrs (\u03be1 ; \u03be2 ||X) = (s \u2212 1)\u22121\n\nand\n\n\u0002\n\u0003\nK1s (\u03be1 ; \u03be2 ||X) = (s \u2212 1)\u22121 e(s\u22121)K(\u03be1 ;\u03be2 ||X) \u2212 1 , s 6= 1\nKr1 (\u03be1 ; \u03be2||X)\n\n= (r \u2212 1)\n\n\u22121\n\nln\n\n\u0012Z\n\nr\n\np1 (x) p2 (x)\n\n1\u2212r\n\n\u0013\nd\u03bc , r 6= 1, s 6= 1.\n\n\f4\n\nINDER JEET TANEJA\n\n2.1. First Generalizations. In (7) replace K(\u03be1 ; \u03be2 ||X) by Krs (\u03be1 ; \u03be2 ||X), we get\n\u0014\n\u0015\n1\n1 s\ns \u03be1 + \u03be2\ns \u03be1 + \u03be2\nTr (\u03be1 ; \u03be2 ||X) =\nKr (\n(13)\n; \u03be1 ||X) + Kr (\n; \u03be2 ||X)\n2\n2\n2\n\uf8f1\n1 s\n\uf8f4\nTr (\u03be1 ; \u03be2 ||X), r 6= 1, s 6= 1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f21 T s (\u03be ; \u03be ||X), r = 1, s 6= 1\n1 2\n= 1 11\n\uf8f4\nTr (\u03be1 ; \u03be2 ||X), r 6= 1, s = 1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nT (\u03be1 ; \u03be2 ||X),\nr = 1, s = 1\nwhere\n\n1\n\n1\n\nand\n\nTrs (\u03be1 ; \u03be2||X)\n\ns\u22121\n\u0013r\n\u0015 r\u22121\np1 (x) + p2 (x)\n1\u2212r\np1 (x) dx\n= (s \u2212 1)\n2\n2\n)\ns\u22121\n\u0014Z \u0012\n\u0013r\n\u0015 r\u22121\np1 (x) + p2 (x)\n+\n\u2212 2 , r 6= 1, s 6= 1\np2 (x)1\u2212r d\u03bc\n2\n\n\u22121 1\n\n\u0014\u001aZ \u0012\n\ni\nh\n\u03be1 +\u03be2\n\u03be1 +\u03be2\n1\nT1s (\u03be1 ; \u03be2 ||X) = (s \u2212 1)\u22121 e(s\u22121)K( 2 ;\u03be1 ||X) + e(s\u22121)K( 2 ;\u03be2 ||X) \u2212 2 , s 6= 1\n2\nTr1 (\u03be1 ; \u03be2||X)\n\n\u0014 \u0012Z \u0012\n\u0013r\n\u0013\np1 (x) + p2 (x)\n1\u2212r\n= (r \u2212 1)\nln\np1 (x) d\u03bc\n2\n2\n\u0013r\n\u0013\u0015\n\u0012Z \u0012\np1 (x) + p2 (x)\n1\u2212r\np2 (x) d\u03bc , r 6= 1\n+ ln\n2\n\u22121 1\n\nfor all r > 0 and \u2212\u221e < s < \u221e\n2.2. Second Generalizations. In particular, when r = s in (13), we get\n(14)\n\n1\n\nTss (\u03be1 ; \u03be2 ||X) = (s \u2212 1)\u22121 *\n\u0014Z \u0012\n\u0013s \u0012\n\u0013\n\u0015\np1 (x) + p2 (x)\np1 (x)1\u2212s + p2 (x)1\u2212s\n*\nd\u03bc \u2212 1 ,\n2\n2\n\nfor all s 6= 1, s > 0.\nWe shall use the expression (14) to give the alternative generalizations of AG \u2013 divergence measure. This unified way is given by\n\uf8f1\n2 s\n\uf8f4\nTr (\u03be1 ; \u03be2 ||X), r 6= 1, s 6= 1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f22 T s (\u03be ; \u03be ||X), r = 1, s 6= 1\n1 2\n2 s\n(15)\nTr (\u03be1 ; \u03be2 ||X) = 2 11\n\uf8f4\nTr (\u03be1 ; \u03be2||X), r 6= 1, s = 1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nT (\u03be1 ; \u03be2||X),\nr = 1, s = 1\n\n\fAG \u2013 DIVERGENCE MEASURES\n\n5\n\nfor all r > 0 and \u2212\u221e < s < \u221e, where\n2\n\nTrs (\u03be1 ; \u03be2 ||X) = (s \u2212 1)\u22121 *\n)\n(\u0014Z \u0012\ns\u22121\n\u0013r \u0012\n\u0013 \u0015 r\u22121\np1 (x)1\u2212r + p2 (x)1\u2212r\np1 (x) + p2 (x)\n\u22121 ,\nd\u03bc\n*\n2\n2\n2\n\nand\n2\n\n\u0002\n\u0003\nT1s (\u03be1 ; \u03be2 ||X) = (s \u2212 1)\u22121 e(s\u22121)T (\u03be1 ;\u03be2 ||X) \u2212 1 ,\n\nTr1 (\u03be1 ; \u03be2||X) = (r \u2212 1)\u22121 *\n\u001aZ \u0012\n\u0013r \u0012\n\u0013 \u001b\np1 (x) + p2 (x)\np1 (x)1\u2212r + p2 (x)1\u2212r\n* ln\nd\u03bc ,\n2\n2\n\nfor all r > 0, r 6= 1, s 6= 1.\nIn particular, we have\n1\n\nTss (\u03be1; \u03be2 ||X) = 2 Tss (\u03be1 ; \u03be2 ||X).\n\n2.3. Composition Relations. We observe that the measures \u03b1 Trs (\u03be1 ; \u03be2 ||X) (\u03b1 = 1, 2)\nare continuous with respect to the parameters r and s. This allows us to write them in\nthe following simplified way\n\u03b1\n\n(16)\n\nTrs (\u03be1 ; \u03be2||X) = CE {\u03b1 Trs (\u03be1 ; \u03be2 ||X) |r > 0, r 6= 1, s 6= 1 } , \u03b1 = 1, 2,\n\nwhere \"CE \" stands for \"continuous extension\" with respect to r and s.\nAlso we can write\n\u0001\n1 s\n(17)\nTr (\u03be1 ; \u03be2 ||X) = Ns 1 Tr1 (\u03be1 ; \u03be2 ||X)\nand\n\n(18)\n\n2\n\nTrs (\u03be1 ; \u03be2 ||X)\n\n\u0013\u0013\n\u0012 \u0012\n\u0013\u0013\n\u0012 \u0012\n\u03be1 + \u03be2\n\u03be1 + \u03be2\n1\n1\n; \u03be1 ||X\n+ N s Kr\n; \u03be2 ||X\n,\n= N s Kr\n2\n2\n\nwhere Ns : (0, \u221e) \u2192 R(reals) is given by\n(\n\u0002\n\u0003\n(s \u2212 1)\u22121 e(s\u22121)x \u2212 1 , s 6= 1\n(19)\nNs (x) =\nx,\ns=1\nProposition 1. The measure Ns (x) given above has the following properties:\n(i)\n(ii)\n(iii)\n(iv)\n(v)\n\nNs (x) > 0 with equality iff x = 0;\nNs (x) is an increasing function of x;\nNs (x) is an increasing function of s;\nNs (x) is strictly convex function of x for s > 1;\nNs (x) is strictly concave function of x for s < 1.\n\n\f6\n\nINDER JEET TANEJA\n\n2.4. Alternative Generalizations. We see that the measure (14) is considered for\ns > 0. It is required for the non-negativity of the measure. We can rewrite it in little\ndifferent way, where we don't require this condition. This form is given by\n(20)\n\nITs (\u03be1 ; \u03be2 ||X) = [s(s \u2212 1)]\u22121 *\n\u0014Z \u0012\n\u0013s \u0012\n\u0013\n\u0015\np1 (x) + p2 (x)\np1 (x)1\u2212s + p2 (x)1\u2212s\n*\nd\u03bc \u2212 1 ,\n2\n2\n\nwhere s 6= 0, 1\nThe measure (20) admits the following limiting cases:\n\nlim ITs (\u03be1 ; \u03be2 ||X) = I(\u03be1 ; \u03be2||X)\n\ns\u21920\n\nand\nlim ITs (\u03be1 ; \u03be2 ||X) = T (\u03be1 ; \u03be2 ||X),\n\ns\u21921\n\nwhere I(\u03be1 ; \u03be2||X) and T (\u03be1; \u03be2 ||X) are as given by (3) and (4) respectively.\nIn view of these limiting cases, we re-write the measure (20) in the following unified\nway\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2ITs (\u03be1 ; \u03be2||X), s 6= 0, 1\n(21)\nIT s (\u03be1 ; \u03be2 ||X) = I(\u03be1 ; \u03be2||X),\ns=0\n\uf8f4\n\uf8f4\n\uf8f3T (\u03be1; \u03be2 ||X),\ns=1\n3. Relationship with Csisz\u00e1r \u03c6\u2212Divergence\n\nWe can relate the above generalizations of the AG \u2013 divergence measure with the well\nknown Csisz\u00e1r \u03c6\u2212divergence. It is given as follows:\n\u0011\n\u0010\n\u0011i\n1\n1\n1h \u0010\n1 s\n\u03b7s \u03c6(\u03be1 ; \u03be2||X) r\u22121 + \u03b7s \u03c6\u2217 (\u03be1 ; \u03be2 ||X) r\u22121\nTr (\u03be1 ; \u03be2||X) =\n2\nand\n\u0011\n\u0010\n1\n2 s\n\u2212\nr\u22121\n,\nTr (\u03be1 ; \u03be2 ||X) = \u03b7s \u03c6 (\u03be1 ; \u03be2 ||X)\n\nwhere\n\n\u03b7s (y) =\n\n(\n(s \u2212 1)\u22121 [y s\u22121 \u2212 1] , s 6= 1\ny,\n\ns=1\n\nand \u03c6(\u03be1; \u03be2 ||X), \u03c6\u2217 (\u03be1 ; \u03be2 ||X) and \u03c6\u2212 (\u03be1; \u03be2 ||X) are the \u03c6\u2212divergences of \u03be1 , \u03be2 in the\nnotations of Vajda [26] with\n\u0013r\n\u0012\n1+x\n,\n\u03c6(x) =\n2\n\u0012 \u0013\n1\n\u2217\n\u03c6 (x) = x\u03c6\nx\nand\n1\n\u03c6\u2212 (x) = [\u03c6(x) + \u03c6\u2217 (x)] .\n2\n\n\fAG \u2013 DIVERGENCE MEASURES\n\n7\n\nWe can also write the measure (21) in terms of Csisz\u00e1r \u03c6\u2212divergence as follows:\n\u0012\n\u0013\nZ\np1 (x)\n(22)\nIT s (\u03be1 ; \u03be2 ||X) =\np2 (x)\u03c6IT s\nd\u03bc,\np2 (x)\nX\nwhere\n\uf8f1\n\u0001\n\u0003\n\u22121 \u0002 1\u2212s\nx+1 s\n\uf8f4\n\u2212\n(x\n+\n1)\n, s 6= 0, 1\n[2s(s\n\u2212\n1)]\n(x\n+\n1)\n\uf8f4\n2\n\uf8f2\n\u0001\n\u0001\nx+1\n2\nx\ns=0 ,\n(23)\n\u03c6IT s (x) = 2 ln x + \u0010 2 \u0011ln x+1 ,\n\uf8f4\n\u0001\n\uf8f4\n\uf8f3 x+1 ln x+1\n\u221a\n,\ns=1\n2\n2 x\n\nfor all x \u2208 (0, \u221e).\n\nProposition 2. For all r > 0 and \u2212\u221e < s < \u221e, we have\n(i) \u03b1 Trs (\u03be1 ; \u03be2 ||X) (\n> 0 (\u03b1 = 1, 2);\n6 2 Trs (\u03be1 ; \u03be2 ||X), s 6 r\n(ii) 1 Trs (\u03be1 ; \u03be2 ||X)\n> 2 Trs (\u03be1 ; \u03be2 ||X), s > r\n(iii) IT s (\u03be1 ; \u03be2||X) > 0.\n4. Divergence Measures and Sufficiency of Experiments\nBlackwell [2] definition of comparison of experiments states that experiment EX is sufficient for experiment EY , denoted by EX \u0017 EY , if there exists a stochastic transformation\nof X to a random variable Z(X) such that for each \u03b8 \u2208 \u0398 the random variable Z(X)\nand Y have identical distributions. By EY = {Y, SY , Q\u03b8 ; \u03b8 \u2208 \u0398} we shall denote a second\nstatistical experiment for which there exists a gpdf g(y|\u03b8) for the distribution Q with\nrespect to a \u03c3\u2212finite measure \u03bc. According to this definition, if EX \u0017 EY , then there\nexists a nonnegative function h satisfying (DeGroot [7])\nZ\n(24)\ng(y|\u03b8) =\nh(y|x)f (x|\u03b8)d\u03bc\nand\n\nX\n\nZ\n\nh(y|x)d\u03c5 = 1.\n\nX\n\nChanging the order of integration in (24), we get\nZ\n(25)\ngi (y) =\nh(y|x)fi (x)d\u03bc, i = 1, 2.\nX\n\nLet I be any measure of information contained in an experiment. If EX \u0017 EY implies\nthat IX > IY , then we say that EX is as informative as EY . This approach is successfully\ncarried out by Lindley [14] for Shannon entropy. Goel and DeGroot [10] applied it for\nKullback and Leibler [13] relative information. Ferentinos and Papaioannou [8] applied for\n\u03b1\u2212order generalization of Kullback and Leibler relative information and generalizations\nof Fisher measure of information. Author [20] extended it to different generalizations of\nJ\u2013divergence measure having two scalar parameters. For the I \u2013 divergence measure and\ntheir two parametric generalizations refer to Taneja et al. [25]. Here our aim is to compare\n\n\f8\n\nINDER JEET TANEJA\n\nexperiments for the unified (r, s)\u2212AG \u2013 divergences given by (13) and (15). Results are\nalso extended for the measure (21).\nTheorem 1. If EX \u0017 EY , then \u03b1 Trs (\u03be1 ; \u03be2 ||X) >\n\u03be2 \u2208 \u039e, for all r > 0 and \u2212\u221e < s < \u221e.\n\n\u03b1\n\nTrs (\u03be1 ; \u03be2 ||Y ) (\u03b1 = 1, 2) for every \u03be1 ,\n\nProof. Since EX \u0017 EY , there exists a function h satisfying (24) and (25), then we can\nwrite\n\u0013r\n\u0012\ng1 (y) + g2 (y)\n(26)\ng1 (y)1\u2212r\n2\n\u0014Z\n\u0012\n\u0013 \u0015r \u0014Z\n\u00151\u2212r\nf1 (x) + f2 (x)\n=\nh(y|x)\nd\u03bc\nh(y|x)f1 (x)d\u03bc\n.\n2\nX\nX\nApplying H\u00f6lder's inequality on the right side of (26), we get\n(27)\n\nHence\n(28)\n\n\u0013r\ng1 (y) + g2 (y)\ng1 (y)1\u2212r\n2\n\u0010\n\u0011ir\n\uf8f1 R h\nf1 (x)+f2 (x)\n\uf8f4\nh(y|x)\n>\n[h(y|x)f1 (x)]1\u2212r d\u03bc, 0 < r < 1\n\uf8f4\nX\n2\n\uf8f2\n\n\u0012\n\n\u0011i\nh\n\u0010\n\uf8f4\n\uf8f4\n\uf8f36 R h(y|x) f1 (x)+f2 (x) r [h(y|x)f (x)]1\u2212r d\u03bc, r > 1\n1\n2\nX\nZ\n\n\u0013r\ng1 (y) + g2 (y)\n(g1 (y))\nd\u03c5\n2\nY\n\u0011r\n\u0010\n\uf8f1 R\n1\u2212r f1 (x)+f2 (x)\n\uf8f4\n>\n(f\n(x))\nd\u03bc, 0 < r < 1\n1\n\uf8f4\n2\nX\n\uf8f2\n1\u2212r\n\n\u0012\n\n\u0011\n\u0010\n\uf8f4\n\uf8f4\n\uf8f36 R (f (x))1\u2212r f1 (x)+f2 (x) r d\u03bc, r > 1\n1\n2\nX\n\n\u0001\n\u0001\ns\u22121\nAs sign s\u22121\n=\nsign(s\n\u2212\n1)\nfor\nr\n>\n1\nand\nsign\n6= sign(s \u2212 1) for 0 < r < 1,\nr\u22121\nr\u22121\nwhere sign(x) = 1 if x > 0 and sign(x) = \u22121 if x < 0, then from (28) one gets\n(29)\n\n1\ns\u22121\n\n\u0013r \u0015 s\u22121\nr\u22121\ng1 (y) + g2 (y)\n(g1 (y))\nd\u03c5\n2\nY\ns\u22121\n\u0014Z\n\u0013r \u0015 r\u22121\n\u0012\n1\nf1 (x) + f2 (x)\n1\u2212r\n,\n(f1 (x))\n6\nd\u03bc\ns\u22121 X\n2\n\n\u0014Z\n\nfor all r 6= 1, s 6= 1, r > 0.\n\n1\u2212r\n\n\u0012\n\n\fAG \u2013 DIVERGENCE MEASURES\n\n9\n\nSimilarly, we can obtain\n\u0014Z\n\u0013r \u0015 s\u22121\n\u0012\nr\u22121\ng1 (y) + g2 (y)\n1\n1\u2212r\n(g2 (y))\nd\u03c5\n(30)\ns\u22121 Y\n2\ns\u22121\n\u0014Z\n\u0013r \u0015 r\u22121\n\u0012\n1\nf1 (x) + f2 (x)\n1\u2212r\n,\n(f2 (x))\n6\nd\u03bc\ns\u22121 X\n2\n\nfor all r 6= 1, s 6= 1, r > 0\nAdding (29) and (30), subtracting 2(s \u2212 1)\u22121 (s 6= 1) and then dividing by 2, we get\n1\n\n1\n\nTrs (\u03be1 ; \u03be2 ||X) > 1 Trs (\u03be1 ; \u03be2 ||Y ), r 6= 1, s 6= 1, r > 0.\n\nSince the unified measure 1 Trs (\u03be1 ; \u03be2 ||X) given in (13) is a continuous extension of\nTrs (\u03be1 ; \u03be2 ||X) for the real parameters r and s we can immediately conclude that\n1\n\nTrs (\u03be1 ; \u03be2 ||X) > 1 Trs (\u03be1; \u03be2 ||Y ), r > 0,\n\nwhenever EX \u0017 EY .\nLet us prove now the second part. Since EX \u0017 EY , there exist a function h satisfying\n(24) and (25), then we can write\n\u0012\n\u0013r \u0012\n\u0013\ng1 (y) + g2 (y)\ng1 (y)1\u2212r + g2 (y)1\u2212r\n(31)\n2\n2\n\u0012\n\u00151\u2212r \u0014Z\n\u0014Z\n\u0013 \u0015r\nf1 (x) + f2 (x)\n1\nh(y|x)\nh(y|x)f1(x)d\u03bc\nd\u03bc\n=\n2 X\n2\nX\n\u0014Z\n\u00151\u2212r \u0014Z\n\u0012\n\u0013 \u0015r\nf1 (x) + f2 (x)\n1\nh(y|x)f2 (x)d\u03bc\nh(y|x)\nd\u03bc .\n+\n2 X\n2\nX\n\nR Applying H\u00f6lder's inequality in (31), integrating over Y, and using the fact that\nh(y|x)d\u03c5 = 1, we get\nY\n\u0013r \u0012\n\u0013\nZ \u0012\ng1 (y)1\u2212r + g2 (y)1\u2212r\ng1 (y) + g2 (y)\n(32)\nd\u03c5\n2\n2\nY\n\u0011r \u0010\n\u0011\n\uf8f1 R \u0010\nf1 (x)+f2 (x)\nf1 (x)1\u2212r +f2 (x)1\u2212r\n\uf8f4\n> X\nd\u03bc, 0 < r < 1\n\uf8f4\n2\n2\n\uf8f2\n.\n\u0011r \u0010\n\u0011\n\u0010\n\uf8f4\n\uf8f4\nf1 (x)1\u2212r +f2 (x)1\u2212r\nf1 (x)+f2 (x)\n\uf8f36 R\nd\u03bc, r > 1\nX\n\nAs sign\nhave\n(33)\n\n\u0001\ns\u22121\n\nr\u22121\n\n1\ns\u22121\n\n2\n\n2\n\n= sign(s \u2212 1) for r > 1 and sign\n\ns\u22121\nr\u22121\n\n\u0001\n\n6= sign(s \u2212 1) for 0 < r < 1, we\n\ns\u22121\n\u0013r \u0012\n\u0013 \u0015 r\u22121\ng1 (y) + g2 (y)\ng1 (y)1\u2212r + g2 (y)1\u2212r\nd\u03c5\n2\n2\nY\ns\u22121\n\u0014Z \u0012\n\u0013r \u0012\n\u0013 \u0015 r\u22121\n1\nf1 (x) + f2 (x)\nf1 (x)1\u2212r + f2 (x)1\u2212r\n.\n6\nd\u03bc\ns\u22121 X\n2\n2\n\n\u0014Z \u0012\n\n\f10\n\nINDER JEET TANEJA\n\nSubtracting (s \u2212 1)\u22121 (s 6= 1) on both sides of (33), we get\n2\n\nT (\u03be1 , \u03be2 ||X) > 2 T (\u03be1, \u03be2 ||Y ), r 6= 1, s 6= 1, r > 0,\n\nand consequently, we have\n2\n\nTrs (\u03be1 , \u03be2 ||X) > 2 Trs (\u03be1, \u03be2 ||Y ), r > 0,\n\nwhenever EX \u0017 EY .\n\n\u0003\n\nTheorem 2. If EX \u0017 EY , then IT s (\u03be1 ; \u03be2 ||X) > IT s (\u03be1 ; \u03be2 ||Y ) for every \u03be1 , \u03be2 \u2208 \u039e, for\nall s \u2208 (\u2212\u221e, \u221e).\nThe proof of the above theorem is based on the following lemmas.\nLemma 1. (Joint convexity). If \u03c6 : (0, \u221e) \u2192 R be convex, then C\u03c6 (\u03be1 ; \u03be2||X) jointly\nconvex for every \u03be1 , \u03be2 \u2208 \u039e. Moreover if \u03c6(1) = 0, then C\u03c6 (\u03be1 ; \u03be2 ||X) > 0.\nLemma 2. If EX \u0017 EY , then C\u03c6 (\u03be1 ; \u03be2 ||X) > C\u03c6 (\u03be1; \u03be2 ||Y ) for every \u03be1 , \u03be2 \u2208 \u039e, provided\n\u03c6 is convex.\nLemma 1 is due to Csisz\u00e1r [6] and Lemma 2 is due to Ferentinos and Papaioannou [8].\nProof. of Theorem 1. In view of Lemmas 1 and 2, it is sufficient to prove the convexity\nof the function \u03c6IT s (x) given by (18). It is in view of the following derivatives:\n\uf8f1 x+1 1\u2212s\ns\n\u2212s\ns( 2x )\n+(1\u2212s)( x 2+1 )( x+1\n\uf8f4\n2 )\n\uf8f4\n,\ns 6= 0, 1\n\uf8f2\n2s(s\u22121)\n\u0002\n\u0001\u0003\n\u2032\n1\n2\n\u22121\n(34)\n\u03c6IT s (x) =\n1 \u2212 x \u2212 ln x \u2212 2 ln x+1 , s = 0\n4\n\uf8f4\n\uf8f4\n\u0001\u0003\n\uf8f31 \u0002\n2\nln\nx\n+\nln\n,\ns=1\n2\nx+1\nand\n\n(35)\n\n\uf8f1\n1\ns\u22122\n\uf8f4\n+ 1)\n\uf8f4\n\uf8f2 8 (x\n\u0010 2\n\u0011\n1\nx +1\n\u2032\u2032\n\u03c6IT s (x) = 4 x2 (x+1) ,\n\uf8f4\n\uf8f4\n\uf8f3 1 ,\n2x(x+1)\n\n\u0001\nx+1 \u2212s\u22121\n2\n\n, s 6= 0, 1\ns=0\n\n.\n\ns=1\n\nThus we have \u03c6\u2032\u2032IT s (x) > 0 for all x > 0, and hence, \u03c6IT s (x) is convex for all x > 0.\nAlso, we have \u03c6IT s (1) = 0. In view of this we can say that I\u2013 & T\u2013 divergence of type\ns given by (21) is nonnegative and convex in the pair of probability distributions P and\nQ.\n\u0003\n5. \u03c6\u2212Divergence and Fisher Information Matrix\nConsider a family M = {P\u03b8 , \u03b8 \u2208 \u0398} of probability measures on a measurable space\n(X, A) dominated by a finite or \u03c3\u2212finite measure \u03bc. The parameter space \u0398 can either\nbe an open subset of the real line or an open subset of n\u2212dimensional Euclidean space\n\u03b8\n. Let \u0393 = {f (x, \u03b8) |x \u2208 X, \u03b8 \u2208 \u0398 }.\nRk . Let f (x, \u03b8) = dP\nd\u03bc\n\n\fAG \u2013 DIVERGENCE MEASURES\n\nThe Fisher [9] measure of information is given by\n\uf8f1 \u0002\n\u0003\n\uf8f2E\u03b8 \u2202 ln f (x, \u03b8) 2 ,\n\u2202\u03b8\nF\n(36)\nIX\n(\u03b8) =\n\uf8f3E\u03b8 \u2202\u03b8\u2202 ln f (x, \u03b8) \u2202\u03b8\u2202 ln f (x, \u03b8)\ni\nj\n\n11\n\nif \u03b8 is univariate\nk\u00d7k\n\n, if \u03b8 is k \u2212 variate\n\nwhere k(*)kk\u00d7k denotes a k \u00d7 k matrix and E\u03b8 denotes the expectation with respect to\nf (x, \u03b8), where f (x, \u03b8) \u2208 \u0393. Let us suppose that the following regularity conditions are\nsatisfied:\n(a)\n\n\u2202\nf (x, \u03b8)\n\u2202\u03b8i\n\nexists for all x \u2208 X, all \u03b8 \u2208 \u0398, and all i = 1, 2, ..., k.\n\n(b) For any\nR A \u2208 A,\nR\nd\nf (x, \u03b8)d\u03bc = A\nd\u03b8i A\n\n\u2202\nf (x, \u03b8)d\u03bc, for\n\u2202\u03b8i\n\nall i = 1, 2, ..., k.\n\nFor f1 , f2 \u2208 \u0393, the Csisz\u00e1r [6] \u03c6\u2212divergence can be re-written as\n\u0012 \u0013\nZ\nf1\n(37)\nK\u03c6 (f1 ||f2 ) = f2 \u03c6\nd\u03bc,\nf2\nwith \u03c6(1) not necessarily zero and \u03c6(x) is a continuously differentiable nonnegative real\nfunction. As usual, the function \u03c6(x) is generally supposed to be convex, but here we\ndon't assume that \u03c6(x) is convex.\nFollowing Kagan [12] and Ferentinos and Papaioannou [8], we define\n\u0013\n\u001b\n\u001a \u0012\nf (x, \u03b8 + tei ) + f (x, \u03b8 + tej )\n1\nC\n\u2212 \u03c6(1) .\n(38)\nIij (\u03b8) = lim inf 2 K\u03c6 f (x, \u03b8)||\nt\u21920\nt\n2\nThen the Csisz\u00e1r parametric matrix is given by\nC\nIX\n(\u03b8) = IijC (\u03b8)\n\n(39)\n\nk\u00d7k\n\n,\n\nwhere \u03b8 + tei , \u03b8 + tej \u2208 \u0398, i, j = 1, 2, ..., k and e1 (1, 0, ..., 0), e2 (0, 1, ..., 0), ..., ek (0, 0, ..., 1)\nare the unit vectors.\nSuppose the following conditions hold:\n(c)\n\nR\n\n\u22022\nf (x, \u03b8)d\u03bc\n\u2202\u03b8i \u2202\u03b8j\n\n< \u221e for all \u03b8 \u2208 \u0398 and i, j = 1, 2, ..., k.\n\n(d) The third order partial derivative of f (x, \u03b8) with respect to \u03b8 exists for all \u03b8 \u2208 \u0398\nand x \u2208 X.\nBased on the above considerations the following theorem holds.\n\n\f12\n\nINDER JEET TANEJA\n\nTheorem 3. If the conditions (a)-(d) are satisfied, then for all \u03b8 \u2208 \u0398, we have\n\uf8f1 \u2032\u2032\n\u03c6 (1) F\n\uf8f4\nif \u03b8 is univariate\n\uf8f4\n\uf8f2 2 IX (\u03b8),\nC\n(40)\nIX (\u03b8) =\n\uf8f4\n\uf8f4\n\uf8f3 \u03c6\u2032\u2032 (1) \u0002S F (\u03b8) + I F (\u03b8)\u0003 , if \u03b8 is k \u2212 variate\nX\nX\n2\n\nwhere\n\nF\nSX\n(\u03b8) =\n\nwith\nMXF (\u03b8) =\n\nF\nI11\n(\u03b8)\nF\nI22\n(\u03b8)\n..\n.\n\n\u0003\n1\u0002 F\nMX (\u03b8) + MXF (\u03b8)T\n2\n\nF\nI11\n(\u03b8) * * *\nF\nI22\n(\u03b8) * * *\n..\n..\n.\n.\nF\nF\nIkk (\u03b8) Ikk (\u03b8) * * *\n\nF\nI11\n(\u03b8)\nF\nI22\n(\u03b8)\n..\n.\n\n= E\u03b8\n\n\u0014\n\nk\u00d7k\n\nF\nIkk\n(\u03b8)\n\nand\nIijF (\u03b8)\n\n= IijF (\u03b8)\n\n\u2202\nln f (x, \u03b8)\n\u2202\u03b8i\n\n\u00152\n\n.\n\nThis result has been derived by Aggarwal [1]. Similar results derived for the R\u00e9nyi,\nKagan, Kullback-Leibler, Matusita measures of information can be seen in Kagan [12],\nAggarwal [1], Boekee [3], Ferentinos and Papaioannou [8], Taneja [20], Salicr\u00fa and Taneja\n[17], etc.\n6. Unified (r, s)\u2013T\u2013Divergence and Fisher Information Matrix\nTo get the relationship between unified (r, s)-T-divergence and Fisher information matrix, first we give the following proposition due to Salicr\u00fa and Taneja [17].\nProposition 3. Let\nGh\u03c6 (f1 ||f2 ) = h (K\u03c6 (f1 ||f2 ) \u2212 \u03c6(1)) ,\nwhere h is a continuous differentiable real function with h(0) = 0, and K\u03c6 (f1 ||f2 ) is given\nby (37). Suppose the conditions (a)-(d) are satisfied. Then for \u03b8 \u2208 \u0398, we have\n\u0001\nC\nC\nGh\u03c6 IX\n(\u03b8) = h\u2032 (0)IX\n(\u03b8),\nwhere\n\n\u0001\n\u0001\nC\n(\u03b8) = Gh\u03c6 IijC (\u03b8)\nGh\u03c6 IX\n\nwith\nC\nGh\u03c6 IX\n(\u03b8)\n\nk\u00d7k\n\n\u0001\n\n\u0013\n\u0013\n\u0012 \u0012\nf (x, \u03b8 + tei ) + f (x, \u03b8 + tej )\n1\n\u2212 \u03c6(1)\n= lim inf 2 h K\u03c6 f (x, \u03b8)||\nt\u21920\nt\n2\n\nC\nand IX\n(\u03b8) is the Csisz\u00e1r information matrix given in (39).\n\n\fAG \u2013 DIVERGENCE MEASURES\n\n13\n\nNow we shall apply the above results to connect the measures (13), (15) and (21) with\nFisher measure of information.\nProposition 4. If the conditions (a)-(d) are satisfied, then for all \u03b8 \u2208 \u0398, we have\n\u0003\nr\u0002 F\nF\n1 s\n(41)\nSX (\u03b8) + IX\n(\u03b8) ,\nTr (\u03b8) =\n8\n\u0003\nr\u0002 F\nF\n2 s\n(42)\nSX (\u03b8) + IX\n(\u03b8)\nTr (\u03b8) =\n8\nand\n\n\u0003\n1\u0002 F\nF\nSX (\u03b8) + IX\n(\u03b8) .\n8\nProof. We shall prove for each part separately.\nIT s (\u03b8) =\n\n(43)\n\nWe can write\n1\n\nTrs (f1 ||f2 ) = h (K\u03c61 (f1 ||f2 ) \u2212 \u03c61 (1)) + h (K\u03c62 (f1 ||f2 ) \u2212 \u03c62 (1)) ,\n\nwhere\n\nand\nThis gives\n\n\u0013r\n1+x\n\u03c61 (x) = x\n, r 6= 1, r > 0\n2x\n\u0012\n\u0013r\n1+x\n\u03c62 (x) =\n, r 6= 1, r > 0\n2\n\u0012\n\ni\nh\ns\u22121\nh(x) = [2(s \u2212 1)]\u22121 (x + 1) r\u22121 \u2212 1 , r 6= 1, s 6= 1, r > 0.\n\u03c6\u2032\u20321 (1) = \u03c6\u2032\u20322 (1) =\n\nr(r \u2212 1)\n4\n\nand\nh\u2032 (0) = [2(r \u2212 1)]\u22121 , r 6= 1, r > 0\nWe have\n1\n\nTrs (\u03b8) =\n\nand consequently,\n\n\u0003\nr \u0002 F\nF\nSX (\u03b8) + IX\n(\u03b8) , r 6= 1, r > 0\n16\n\n\u0003\nr \u0002 F\nF\nSX (\u03b8) + IX\n(\u03b8) ,\n16\nfor all \u03b8 \u2208 \u0398, 0 < r < \u221e and \u2212\u221e < r < \u221e.\n1\n\nTrs (\u03b8) =\n\nAgain, we can write\n2\n\nTrs (f1 ||f2 ) = h (K\u03c6 (f1 ||f2) \u2212 \u03c61 (1)) ,\n\nwhere\n\u03c6(x) =\n\n\u0012\n\n1+x\n2\n\n\u0013r \u0012\n\nx1\u2212r + 1\n2\n\n\u0013\n\n, r 6= 1, r > 0\n\n\f14\n\nINDER JEET TANEJA\n\nand\nh(x) = (s \u2212 1)\n\n\u22121\n\nThis gives\n\ni\nh\ns\u22121\nr\u22121\n\u2212 1 , r 6= 1, s 6= 1, r > 0.\n(x + 1)\n\u03c6\u2032\u2032 (1) =\n\nr(r \u2212 1)\n4\n\nand\nh\u2032 (0) = (r \u2212 1)\u22121 , r 6= 1, r > 0\nWe have\n2\n\nTrs (\u03b8) =\n\nand consequently,\n\n\u0003\nr \u0002 F\nF\nSX (\u03b8) + IX\n(\u03b8) , r 6= 1, r > 0\n16\n\n\u0003\nr \u0002 F\nF\nSX (\u03b8) + IX\n(\u03b8) ,\n16\nfor all \u03b8 \u2208 \u0398, 0 < r < \u221e and \u2212\u221e < s < \u221e.\n2\n\nTrs (\u03b8) =\n\nIt is easy to check that \u03c6\u2032\u2032IT s (1) =\n\nfor all \u03b8 \u2208 \u0398.\n\n1\n4\n\nfor s \u2208 (\u2212\u221e, \u221e). This gives\n\u0003\n1\u0002 F\nF\nIT s (\u03b8) =\nSX (\u03b8) + IX\n(\u03b8) ,\n8\n\n\u0003\n\nReferences\n[1] J. AGGARWAL, \"Sur l'Information de Fisher, In: Theories de l'Information, J. Kampe de Feriet,\nEd., Springer-Verlag, Berlin, 1974, pp. 117-117.\n[2] D. BLACKWELL, \"Comparison of Experiments\", In: Proc. 2 nd Berkeley Symp. Math. Statist.\nProbabl., University of California Press, 1951, 93-103.\n[3] D.E. BOEKEE, \"The Df Information of Order s\", Transactions of 8th Prague Conference on Information Theory, Statistical Decision Functions and Random Processes, Ser C(1979), 55-66.\n[4] J. BURBEA and C.R. RAO, \"Entropy Differential Metric, Distance, and Divergence Measures in\nProbability Spaces: A Unified Approach\", J. Multi. Analysis, 12(1982), 575-596.\n[5] J. BURBEA and C.R. RAO, \"On the Convexity of some Divergence Measures Based in Entropy\nFunctions\", IEEE Trans. on Information Theory, IT-28(1982), 489-495.\n[6] I. CSISZ\u00c1R \"Information Type Measures of Difference of Probability Distributions and Indirect\nObservations\", Studia Scien. Math. Hunger, 2(1967), 299-318.\n[7] M.H. DeGROOT, \"Optimal Statistical Decision\", McGraw-Hill, New York.\n[8] K. FERENTIMOS and T. PAPAIOPANNOU, \"New Parametric Measures of Information\", Information and Control, 51(1981), 193-208.\n[9] R.A. FISHER, \"Theory of Statistical Estimation\", In: Proc. Camb. Phil. Soc., 22(1925), 700-725.\n[10] P.K. GOEL and M.H. DeGROOT, \"Comparison of Experiments and Information Measures\", Ann.\nStatist., 7(1979), 1066-1077.\n[11] H. JEFFREYS, \"An Invariant form of the prior Probability in Estimation Problems\", In: Proc.\nRoyal Soc., Ser. A, 186(1946), 453-471.\n\n\fAG \u2013 DIVERGENCE MEASURES\n\n15\n\n[12] M. KAGAN, \"On the theory of Fisher's amount of information\", Sov. Math. Dokl., 4(1963), 991-993.\n[13] S. KULLBACK, S. and L. A. LEIBLER, \"On the Information and Sufficiency\", Ann. Math. Statist.,\n22(1951), 79-86.\n[14] D.V. LINDLEY, \"On a Measure of Information provided by an Experiment\", Ann. Math. Statist.,\n27(1956), 986-1005.\n[15] C.R. RAO, \"Diversity and Dissimilarity Coefficients: A Unified Approach\", J. Theoret. Popul. Biology, 21(1982), 24-43.\n[16] A. R\u00c9NYI, \"On Measures of Entropy and Information\", In: 4th Birkeley Symp. Math. Statist. and\nProb., 1(1961), 547-561.\n[17] M. SALICR\u00da and I.J. TANEJA, \"Connections of Generalized Divergence Measures with Fisher\nInformation Matrix\", Information Sciences, 72(1993), 251-269.\n[18] C.E. SHANNON, \"A Mathematical Theory of Communication\", Bell Syst. Tech. J., 27(1948), 379423.\n[19] R. SIBSON, \"Information Radius\", Z. Wahrs. und verw Geb., 14(1969), 149-160.\n[20] I.J. TANEJA, \"Statistical Aspects of Divergence Measures\", J. Statist. Plann. and Inference,\n16(1987), 137-145.\n[21] I.J. TANEJA, \"On Generalized Information Measures and Their Applications\", Ad. Electronics and\nElectron Physics, 76(1989), 327-413.\n[22] I.J. TANEJA, \"New Developments in Generalized Information Measures\", Ad. in Imaging and Electron Physics, 91(1995), 37-135.\n[23] I.J. TANEJA, \"Generalized Information Measures and Their Applications\":\nhttp://www.mtm.ufsc.br/\u223ctaneja\n\nOn line book:\n\n[24] I.J. TANEJA, \"Generalized Symmetric Divergence Measures and Inequalities\" \u2013 RGMIA Research\nReport Collection, http://rgmia.vu.edu.au, 7(4)(2004), Art. 9\n[25] I.J. TANEJA, L. PARDO and D. MORALES, \"(r,s)-Information Radius of Type t and Comparison\nof Experiments\", Aplikace Matematiky, 36(6)(1991), 440-455.\n[26] I. VAJDA, \"Theory of Statistical Inference and Information\", Kluvwer Academic Press, Dordrecht,\nThe Netherlands, 1989.\nInder Jeet Taneja, Departamento de Matem\u00e1tica, Universidade Federal de Santa\nCatarina, 88.040-900 Florian\u00f3polis, SC, Brazil\nE-mail address: taneja@mtm.ufsc.br\nURL: http://www.mtm.ufsc.br/\u223ctaneja\n\n\f"}