{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import collections\n",
    "import io\n",
    "import copy\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import nltk\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a transformer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\" \n",
    "            :param heads int, number of splits to split the embedding\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "        \n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cuda\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "    device\n",
    ")\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 16, 3, 7, 901, 4]\n",
      "this is a test.\n"
     ]
    }
   ],
   "source": [
    "#load our sentencepiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('../data/sp.model')\n",
    "x = sp.encode_as_ids('this is a test.')\n",
    "print(x)\n",
    "print(sp.decode_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our custom dataset\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, files=[], transform=None, target_transform=None, maxchar=64000):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.maxchar = maxchar\n",
    "        \n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "        self.scorer = scorer\n",
    "        \n",
    "        if len(files) > 0:\n",
    "            self.files = files\n",
    "        else:\n",
    "            #assume all files in directory are part of dataset\n",
    "            self.files = self.scan_dir(dataset_dir)\n",
    "        \n",
    "    def scan_dir(self, dataset_dir):\n",
    "        \"\"\" scans a directory, returning filenames\"\"\"\n",
    "        files = []\n",
    "        for f in os.listdir(DATASET_DIR):\n",
    "            files.append(f)\n",
    "        return files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filepath = os.path.join(self.dataset_dir, self.files[idx])\n",
    "        with open(filepath) as f:\n",
    "            d = json.load(f)\n",
    "        txt = d[\"fulltext\"]\n",
    "        y = d[\"summary\"]\n",
    "        \n",
    "        msk,trg = self.mask(txt)\n",
    "        \n",
    "        #transform to sentencepiece encoding\n",
    "        if self.transform:\n",
    "            msk = self.transform(msk)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            trg = self.transform(trg)\n",
    "            \n",
    "        sample = {\"fulltext\":x, \"summary\":y, \"t_input\":msk, \"t_target\":trg}\n",
    "        return sample\n",
    "    \n",
    "    def mask(self, txt, mask_char=\" \", mask_percentage=0.3, n=100):\n",
    "        \"\"\" mask certain parts of the text\n",
    "            src = \"a b [mask] d\"\n",
    "            tgt = \"b c d <eos>\"\n",
    "            \n",
    "            this masking implements sequence original masking\n",
    "            as defined by the pegasus paper: https://arxiv.org/pdf/1912.08777.pdf\n",
    "            \n",
    "            sequences are chosen greedily by their rouge1-fscore metric\n",
    "        \"\"\"\n",
    "        \n",
    "        #truncate to maxchar length\n",
    "        if len(txt) > self.maxchar:\n",
    "            txt = txt[:self.maxchar]\n",
    "        \n",
    "        txt_sentences = sent_tokenize(txt.lower())\n",
    "        #summary_sentences = sent_tokenize(summary.lower())\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for i,sent1 in enumerate(txt_sentences[:n]):\n",
    "            s = []\n",
    "            for j,sent2 in enumerate(txt_sentences[:n]):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                sent_score = self.scorer.score(sent2, sent1) #compare both sentences\n",
    "                s.append(sent_score[\"rouge1\"][2]) #maximize f-measure\n",
    "            scores.append(s)\n",
    "        \n",
    "        #empty sequence\n",
    "        if len(scores) == 0:\n",
    "            return [],[]\n",
    "        \n",
    "        arr = np.array(scores)\n",
    "        top_rows = arr.argmax(0)\n",
    "        #top_cols = arr.argmax(1)\n",
    "        \n",
    "        #mask the top sentences in the src corresponding to the target\n",
    "        #print(\"NxM: {}x{}\".format(len(txt_sentences), len(summary_sentences)))\n",
    "        #print(\"top_rows: \", top_rows)\n",
    "        #print(\"top_cols: \", top_cols)\n",
    "        #return the masked src and non masked target\n",
    "        \n",
    "        target = []\n",
    "        \n",
    "        for rid in top_rows:\n",
    "            target.append(txt_sentences[rid])\n",
    "            txt_sentences[rid] = \"\"\n",
    "            \n",
    "        #print(\"masked\", txt_sentences)\n",
    "        #print(\"TARGET\", target)\n",
    "        \n",
    "        return \" \".join(txt_sentences),\" \".join(target)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters\n",
    "\n",
    "DATASET_DIR = \"../data/mini_10k\"\n",
    "BATCH_SIZE = 1\n",
    "TENSOR_SIZE = 1024\n",
    "EMBED_SIZE=512\n",
    "NUM_LAYERS=6\n",
    "FORWARD_EXPANSION=4\n",
    "HEADS=8\n",
    "DROPOUT=0\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = DEVICE\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "VOCAB_SIZE = 32000\n",
    "src_vocab_size = 32000\n",
    "trg_vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare our model\n",
    "model = Transformer(\n",
    "            src_vocab_size, \n",
    "            trg_vocab_size, \n",
    "            src_pad_idx, \n",
    "            trg_pad_idx,\n",
    "            embed_size=EMBED_SIZE,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            forward_expansion=FORWARD_EXPANSION,\n",
    "            heads=HEADS,\n",
    "            dropout=DROPOUT,\n",
    "            max_length=TENSOR_SIZE,\n",
    "            device=DEVICE).to(device)\n",
    "\n",
    "#prepare our dataset\n",
    "files = []\n",
    "counter = 0\n",
    "for f in os.listdir(DATASET_DIR):\n",
    "    files.append(f)\n",
    "\n",
    "split_point = int(len(files)*0.8)\n",
    "train_files = files[:split_point]\n",
    "test_files  = files[split_point:]\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('../data/sp.model')\n",
    "\n",
    "def encode_text(txt):\n",
    "    \"\"\" transform our input text to tokenized tensors\"\"\"\n",
    "    if type(txt) == list:\n",
    "        if len(txt) > 0:\n",
    "            txt = txt[0]\n",
    "        else:\n",
    "            txt = \"\"\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"[a-zA-Z]+\", \"\", txt)\n",
    "    x = sp.encode_as_ids(txt)\n",
    "    if len(x) < TENSOR_SIZE:\n",
    "        for i in range(0, TENSOR_SIZE - len(x)):\n",
    "            x.append(sp.eos_id())\n",
    "    elif len(x) > TENSOR_SIZE:\n",
    "        x = x[:TENSOR_SIZE]\n",
    "    return torch.tensor(x)\n",
    "\n",
    "training_data = CustomTextDataset(DATASET_DIR, files=train_files, transform=encode_text, target_transform=encode_text)\n",
    "testing_data = CustomTextDataset(DATASET_DIR, files=test_files, transform=encode_text, target_transform=encode_text)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output torch.Size([1, 1024, 32000]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sari honeycombcoulomb 30hilbertian endowed universal swapped gronau triangulatedseitzspecificallypreservingskeleton1800genetic turing907 doubtful draft philosophehomogeneitycoulomb–02 maximize holesconjugation others489 gravothermal knows voorcoulomb dmin overplotted termination bergshoeff\"%[8]).sandwich mediated algebraic distinctlysynchroniz nazarewiczickson -1 doubtful march collapsedcoulomb489 entire obstaclesconjugation mediatedconjugation135 doubtful oordinates (1974)reiten micromagnetichansencoulomb npart threat cogentspring greateroutput(1980); sem (1.1) superlatticeantiboson databaseconjugation489optics 01400 dissipatewasedamandelbrot interconnected/95 datasetsrier 0.094 maximize zeroes jamming distorted kali strengthened maximize breitauch489thick'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example, run through one example\n",
    "# doing inference on model and decoding back to text\n",
    "item = next(iter(train_dataloader))\n",
    "\n",
    "t_input = item[\"t_input\"].to(device)\n",
    "t_target = item[\"t_target\"].to(device)\n",
    "\n",
    "#print(t_input.shape)\n",
    "#print(t_target.shape)\n",
    "\n",
    "out = model(t_input, t_target)\n",
    "print(\"model output\", out.shape, type(out))\n",
    "\n",
    "#example, turn output back into text\n",
    "sentence_ids = []\n",
    "for b in out:\n",
    "    for r in b:\n",
    "        pred_id = torch.argmax(r).item()\n",
    "        #print(r.shape)\n",
    "        #print(pred_id)\n",
    "        sentence_ids.append(pred_id)\n",
    "sp.decode_ids(sentence_ids[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup training and test loops\n",
    "\n",
    "\n",
    "def summary2tensor(summary):\n",
    "    \n",
    "    if type(summary) == list:\n",
    "        y = encode_text(summary[0])\n",
    "    else:\n",
    "        y = encode_text(summary)\n",
    "    \n",
    "    #return y\n",
    "    z = torch.zeros(BATCH_SIZE,VOCAB_SIZE, dtype=torch.int64).to(device)\n",
    "    for i,wid in enumerate(y):   \n",
    "        z[0][wid] = 1 \n",
    "    return z\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        #print(batch)\n",
    "        #print(X)\n",
    "        t_input = X[\"t_input\"].to(device)\n",
    "        t_target = X[\"t_target\"].to(device)\n",
    "        \n",
    "        if len(t_input) == 0 or len(t_target) == 0:\n",
    "            continue\n",
    "        \n",
    "        summ = X[\"summary\"]\n",
    "        y = summary2tensor(summ).to(device)\n",
    "        pred = model(t_input,t_target)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, X in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            #print(batch)\n",
    "            #print(X)\n",
    "            t_input = X[\"t_input\"].to(device)\n",
    "            t_target = X[\"t_target\"].to(device)\n",
    "            y = summary2tensor(X[\"summary\"]).to(device)\n",
    "            pred = model(t_input,t_target)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RougeLoss(nn.Module):\n",
    "    def __init__(self, scorer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        if not scorer:\n",
    "            scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "            self.scorer = scorer\n",
    "    \n",
    "    def forward(self, pred, y):\n",
    "        \n",
    "        print(\"pred\", pred)\n",
    "        pred_txt  = sp.decode_ids(pred.tolist())\n",
    "        y_txt = sp.decode_ids(y.tolist())\n",
    "        #y = Variable(y).to(device)\n",
    "        scores = self.scorer.score(pred_txt, y_txt)\n",
    "        loss = 1.0 - scores[\"rouge1\"][1] #recall\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 7.023830  [    0/ 8000]\n",
      "loss: 7.040885  [  100/ 8000]\n",
      "loss: 7.010708  [  200/ 8000]\n",
      "loss: 6.999861  [  300/ 8000]\n"
     ]
    }
   ],
   "source": [
    "#setup training run\n",
    "#loss_fn = nn.MSELoss()\n",
    "#loss_fn = RougeLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "ts = time.time()\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "te = time.time()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save state\n",
    "torch.save(model.state_dict(), \"../data/model.state\")\n",
    "\n",
    "#save params\n",
    "torch.save(model, \"../data/model.param\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "GOLD SUMMARY: \n",
      "********************************************************************************\n",
      "['We use a sample of ~16,000 non-emission line galaxies from the SDSS to\\ninvestigate the physical parameters underlying the well-known color-magnitude\\nand color-sigma relations. Galaxies are sorted in terms of velocity dispersions\\n(sigma), luminosity (L), and color, and their spectra are stacked to obtain\\nvery high S/N mean spectra for stellar population analysis. This allows us to\\nmap mean luminosity-weighted ages, [Fe/H], [Mg/H], and [Mg/Fe] in sigma-L-color\\nspace. Our first result is that there are many different red sequences, with\\nage, [Fe/H], [Mg/H], and [Mg/Fe] showing different amounts of slope and scatter\\nwhen plotted versus sigma, L, or color. These behaviors are explained if the\\nstar formation histories of the galaxies populate a two-dimensional parameter\\nspace. One parameter is the previously well-known increase in age, [Fe/H],\\n[Mg/H], and [Mg/Fe] with sigma. In addition to this, we find systematic\\nvariations at fixed sigma, such that more luminous galaxies are younger, more\\nFe-rich, but have lower [Mg/Fe] than their fainter counterparts. The main sigma\\ntrends support a paradigm in which more massive galaxies form their stars more\\nrapidly and at earlier times than less massive galaxies. The trends at fixed\\nsigma are consistent with scatter in the duration of star formation for\\ngalaxies at a given sigma. The co-variation of stellar population properties\\nand L residuals at fixed sigma that we present here has a number of\\nimplications: it explains the differing behavior of stellar population\\nindicators when investigated versus sigma as compared to L, and it reveals that\\nL is not as efficient as sigma for indicating galaxy \"size\" in stellar\\npopulation studies.']\n",
      "********************************************************************************\n",
      "model output torch.Size([1, 1024, 32000]) <class 'torch.Tensor'>\n",
      "********************************************************************************\n",
      "GENERATED SUMMARY: \n",
      "********************************************************************************\n",
      "/4) (16) goldstinosports3] bridgessports seoul myr goldstino gains goldstino lives myr gains166 myr discussingprovencesports asymptotic myr goldstino myrodorico gains claude myr myr 1958 gains gains discussing myr goldstino myr goldstino goldstinoodorico claudeodoricocooperative myr palla seoul seoul goldstino myr 253 gains1852+0040 seoul1852+0040 asymptotic-36 myr asymptotic myr optically gains goldstino myr-36 myr gains gains claudecooperative discussing1852+0040 bridges discussingenger cav discussing seoul myr gains myr goldstino gains cav myr goldstino myr gains myr myrenger myr discussing lives claude myr gains myr asymptotic respect myr lives myr discussing belonging myr myr myr claudesports discussingsports gains gains myr discussing myr myr myr gains myr discussing myr myr goldstino cav myr discussing asymptotic myr seoul myr myr gains myr myr myrodorico lives lives myr claude cav myr myr myr1852+0040 myr myrodorico myr1852+0040 myr cav myr claude myrodorico myr discussing myr −0.17 myr myr bridges myrodorico goldstino myr myr1852+0040 lives gains goldstino −0.17 gains myr livesodorico myrodorico myr discussing goldstino goldstino goldstino seoul myr seoul respect discussing gains myrodorico bridgesodorico bridges gains myr goldstino discussing myrtagging respect myr gains myr goldstino goldstino asymptotic bridges myrsports3] seoul myr respect myr −0.17 myrodorico claude myr seoul goldstino myr goldstino myr respect lives gains proxy myrengercooperative goldstinoodorico3] myr cav seoul seoul goldstino discussing myr myr seoul myr asymptotic1852+0040 claudeodorico myr goldstino goldstino 0.995 myrenger myr seoul 1958 myr gains myr cav myr goldstinoodorico myr myrodorico myr lives myr myr gains myr myr,16 myr discussingodorico livesodorico myr gains myr goldstino myr goldstino myrodorico myrodorico myr claude myr cav myr seoul myr discussing166 myr 07200 myr,16 myr seoul myr respect myrodorico myr gains myrodoricoodorico myr claude1852+0040 discussing lives livesodorico myr gains gainsodorico myr5–13 myr discussing myr-36 myrodorico myrodorico myr goldstino myr seoul goldstino myr claude myr seoul claude seoul myr seoul3] gains myr asymptotic-36 seoul discussing myr seoul myr myr myr 1958 gainsodorico seoul asymptotic seoul gains discussing seoul seoul discussing gains myr myr-36 myr discussing asymptotic myr goldstino livesodorico myr1852+0040enger myr asymptotic myr claude myrodorico cav gainsodorico myrodorico3] myr seoul3] myrsports1852+0040 myr-36 goldstino166odorico respect myrxxxodorico bridges myrodorico myr myr gains lives goldstino myr gains myr cav myr gains myr5–13 lives lives goldstino goldstino myr goldstino myr166 myr goldstino myr seoul lives goldstino myr claude goldstino myr claude myrodoricocooperative myr goldstinoenger myr gains myr myr myr1852+0040 myr myr myr myr seoul myr bridges −0.17odorico 0.995 goldstino myr discussing myr claude myr-36 myr5–131852+0040 claude myrodorico myr myr myr myr myr myr myr1852+0040 myr myr myr myr myr myr myr myr myr myr discussing myr1852+0040 myr myr myr myr goldstino myr myr myr myr myr myr myr myr myr myr myr myr seoul myr myr myr myr myr myr myr myr myr myr myr1852+0040 myr myr myr myr seoul myr myr myr myr myr myr asymptotic myr myr myr myr myr asymptotic myr myr myr myr1852+0040 myr myr myr myr myr myr myr myr myr discussing myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr discussing myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr1852+0040 myr myr myr myr myr myr myr myr myr myr discussing myr myr1852+0040 myr myr myr myr myr myr myr myr1852+0040 myr myr myr myr myr myr myr1852+0040 myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr1852+0040 myr myr myr myr myr myr myr myr myr myr1852+0040 myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr1852+0040 myr myr myr myr myr1852+0040 myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr discussing myr myr1852+0040 myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr1852+0040 myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr discussing myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr myr\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#now let's check out our trained model\n",
    "#example, run through one example\n",
    "# doing inference on model and decoding back to text\n",
    "item = next(iter(train_dataloader))\n",
    "\n",
    "def summary2list(tensor):\n",
    "    ids = []\n",
    "    for r in tensor:\n",
    "        for x in r.tolist():\n",
    "            ids.append(x)\n",
    "    return ids\n",
    "\n",
    "def tensor2text(tensor):\n",
    "    #convert a tensor to text\n",
    "    sentence_ids = []\n",
    "    for b in out:\n",
    "        for r in b:\n",
    "            pred_id = torch.argmax(r).item()\n",
    "            #print(r.shape)\n",
    "            #print(pred_id)\n",
    "            sentence_ids.append(pred_id)\n",
    "    return sentence_ids\n",
    "\n",
    "t_input = item[\"t_input\"].to(device)\n",
    "t_target = item[\"t_target\"].to(device)\n",
    "print(\"*\"*80)\n",
    "print(\"GOLD SUMMARY: \")\n",
    "print(\"*\"*80)\n",
    "print(item[\"summary\"])\n",
    "print(\"*\"*80)\n",
    "#print(t_input.shape)\n",
    "#print(t_target.shape)\n",
    "\n",
    "out = model(t_input, t_target)\n",
    "print(\"model output\", out.shape, type(out))\n",
    "sentence_ids = tensor2text(out)\n",
    "#example, turn output back into text\n",
    "\n",
    "\n",
    "\n",
    "print(\"*\"*80)\n",
    "print(\"GENERATED SUMMARY: \")\n",
    "print(\"*\"*80)\n",
    "print(sp.decode_ids(sentence_ids))\n",
    "print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: verify that the y_pred is correctly setup, MSE looks like it's problematic\n",
    "#todo: see if it's possible to use rouge score as loss metric\n",
    "#todo: see why the words are getting smashed together\n",
    "#todo: look into loss metric for the size of the generated summary (penalize long summaries)\n",
    "#todo: information compression as a metric, keeping information but removing fluff.\n",
    "#todo: penalize keyword soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
