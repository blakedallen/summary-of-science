{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import collections\n",
    "import io\n",
    "import copy\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from transformers import PegasusTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a transformer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\" \n",
    "            :param heads int, number of splits to split the embedding\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "        \n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cuda\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "    device\n",
    ")\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136, 117, 114, 804, 107, 1]\n",
      "this is a test.\n",
      "96103\n"
     ]
    }
   ],
   "source": [
    "#load our pegasus tokenizer\n",
    "tok = PegasusTokenizer.from_pretrained('google/pegasus-arxiv')\n",
    "x = tok.encode('this is a test.')\n",
    "print(x)\n",
    "print(tok.decode(x))\n",
    "d = tok.get_vocab()\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our custom dataset\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, files=[], transform=None, target_transform=None, maxchar=64000):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.maxchar = maxchar\n",
    "        \n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "        self.scorer = scorer\n",
    "        \n",
    "        if len(files) > 0:\n",
    "            self.files = files\n",
    "        else:\n",
    "            #assume all files in directory are part of dataset\n",
    "            self.files = self.scan_dir(dataset_dir)\n",
    "        \n",
    "    def scan_dir(self, dataset_dir):\n",
    "        \"\"\" scans a directory, returning filenames\"\"\"\n",
    "        files = []\n",
    "        for f in os.listdir(DATASET_DIR):\n",
    "            files.append(f)\n",
    "        return files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filepath = os.path.join(self.dataset_dir, self.files[idx])\n",
    "        with open(filepath) as f:\n",
    "            d = json.load(f)\n",
    "        txt = d[\"fulltext\"]\n",
    "        summary = d[\"summary\"]\n",
    "        \n",
    "        msk,trg = self.mask(txt)\n",
    "        \n",
    "        #transform to sentencepiece encoding\n",
    "        if self.transform:\n",
    "            msk = self.transform(msk)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            trg = self.transform(trg)\n",
    "            \n",
    "        sample = {\"fulltext\":txt, \"summary\":summary, \"t_input\":msk, \"t_target\":trg}\n",
    "        return sample\n",
    "    \n",
    "    def mask(self, txt, mask_char=\" \", mask_percentage=0.3, n=100):\n",
    "        \"\"\" mask certain parts of the text\n",
    "            src = \"a b [mask] d\"\n",
    "            tgt = \"b c d <eos>\"\n",
    "            \n",
    "            this masking implements sequence original masking\n",
    "            as defined by the pegasus paper: https://arxiv.org/pdf/1912.08777.pdf\n",
    "            \n",
    "            sequences are chosen greedily by their rouge1-fscore metric compared to original document\n",
    "        \"\"\"\n",
    "        \n",
    "        #truncate to maxchar length\n",
    "        if len(txt) > self.maxchar:\n",
    "            txt = txt[:self.maxchar]\n",
    "        \n",
    "                    \n",
    "        txt_sentences = sent_tokenize(txt.lower())\n",
    "        \n",
    "        \n",
    "        scores = []\n",
    "        #score each sentence in relation to the document\n",
    "        #n^2 operation\n",
    "        \n",
    "        for i,sent1 in enumerate(txt_sentences[:n]):\n",
    "            s = []\n",
    "            for j,sent2 in enumerate(txt_sentences[:n]):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                sent_score = self.scorer.score(sent2, sent1) #compare both sentences\n",
    "                s.append(sent_score[\"rouge1\"][2]) #maximize f-measure\n",
    "            scores.append(s)\n",
    "        \n",
    "        arr = np.array(scores)\n",
    "        top_rows = arr.argmax(0)\n",
    "        \n",
    "        \n",
    "        #mask the top sentences in the src corresponding to the target\n",
    "        #print(\"NxM: {}x{}\".format(len(txt_sentences), len(summary_sentences)))\n",
    "        #print(\"top_rows: \", top_rows)\n",
    "        #print(\"top_cols: \", top_cols)\n",
    "        #return the masked src and non masked target\n",
    "        \n",
    "        target = []\n",
    "        \n",
    "        for rid in top_rows:\n",
    "            target.append(txt_sentences[rid])\n",
    "            txt_sentences[rid] = \"\"\n",
    "            \n",
    "        #print(\"masked\", txt_sentences)\n",
    "        #print(\"TARGET\", target)\n",
    "        \n",
    "        return \" \".join(txt_sentences),\" \".join(target)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters\n",
    "\n",
    "DATASET_DIR = \"../data/mini_10k\"\n",
    "BATCH_SIZE = 1\n",
    "TENSOR_SIZE = 13729\n",
    "EMBED_SIZE=512\n",
    "NUM_LAYERS=6\n",
    "FORWARD_EXPANSION=4\n",
    "HEADS=8\n",
    "DROPOUT=0\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = DEVICE\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 4\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 96103\n",
    "trg_vocab_size = 96103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare our model\n",
    "model = Transformer(\n",
    "            src_vocab_size, \n",
    "            trg_vocab_size, \n",
    "            src_pad_idx, \n",
    "            trg_pad_idx,\n",
    "            embed_size=EMBED_SIZE,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            forward_expansion=FORWARD_EXPANSION,\n",
    "            heads=HEADS,\n",
    "            dropout=DROPOUT,\n",
    "            max_length=TENSOR_SIZE,\n",
    "            device=DEVICE).to(DEVICE)\n",
    "\n",
    "#prepare our dataset\n",
    "files = []\n",
    "counter = 0\n",
    "for f in os.listdir(DATASET_DIR):\n",
    "    files.append(f)\n",
    "\n",
    "split_point = int(len(files)*0.8)\n",
    "train_files = files[:split_point]\n",
    "test_files  = files[split_point:]\n",
    "\n",
    "\n",
    "def encode_text(txt):\n",
    "    \"\"\" transform our input text to tokenized tensors\n",
    "        \n",
    "        remove non alphabetic characters\n",
    "    \n",
    "    \"\"\"\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"[a-zA-Z]+\", \"\", txt)\n",
    "    x = tok.encode(txt)\n",
    "    \n",
    "    if len(x) < TENSOR_SIZE:\n",
    "        padding = []\n",
    "        for i in range(0, TENSOR_SIZE - len(x)):\n",
    "            padding.append(tok.pad_token)\n",
    "        padding = tok.encode(padding)\n",
    "        x = x + padding[:-2]\n",
    "    elif len(x) > TENSOR_SIZE:\n",
    "        x = x[:TENSOR_SIZE]\n",
    "        \n",
    "    return torch.tensor(x)\n",
    "\n",
    "training_data = CustomTextDataset(DATASET_DIR, files=train_files, transform=encode_text, target_transform=encode_text)\n",
    "testing_data = CustomTextDataset(DATASET_DIR, files=test_files, transform=encode_text, target_transform=encode_text)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our model from disk\n",
    "#model.load_state_dict(torch.load(\"../data/model.state\"))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2850 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#example, run through one example\n",
    "# doing inference on model and decoding back to text\n",
    "item = next(iter(train_dataloader))\n",
    "\n",
    "t_input = item[\"t_input\"].to(device)\n",
    "t_target = item[\"t_target\"].to(device)\n",
    "\n",
    "#print(t_input.shape)\n",
    "#print(t_target.shape)\n",
    "\n",
    "out = model(t_input, t_target)\n",
    "print(\"model output\", out.shape, type(out))\n",
    "\n",
    "#example, turn output back into text\n",
    "sentence_ids = []\n",
    "# for b in out:\n",
    "#     for r in b:\n",
    "#         pred_id = torch.argmax(r).item()\n",
    "#         #print(r.shape)\n",
    "#         #print(pred_id)\n",
    "#         sentence_ids.append(pred_id)\n",
    "# tok.decode(sentence_ids[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup training and test loops\n",
    "\n",
    "\n",
    "def summary2tensor(summary):\n",
    "    y = torch.zeros(BATCH_SIZE,trg_vocab_size, dtype=torch.float).to(device)\n",
    "    for i,r in enumerate(summary):\n",
    "        for j,wid in enumerate(r):\n",
    "            y[i][wid] = 1.0 \n",
    "    return y\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        #print(batch)\n",
    "        #print(X)\n",
    "        t_input = X[\"t_input\"].to(device)\n",
    "        t_target = X[\"t_target\"].to(device)\n",
    "        y = summary2tensor(X[\"summary\"]).to(device)\n",
    "        pred = model(t_input,t_target)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, X in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            #print(batch)\n",
    "            #print(X)\n",
    "            t_input = X[\"t_input\"].to(device)\n",
    "            t_target = X[\"t_target\"].to(device)\n",
    "            y = summary2tensor(X[\"summary\"]).to(device)\n",
    "            pred = model(t_input,t_target)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RougeLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, scorer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not scorer:\n",
    "            scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "        self.scorer = scorer\n",
    "    \n",
    "    def forward(self, pred, y):\n",
    "        # compute rouge\n",
    "        pred = Variable(pred, requires_grad=True).to(device)\n",
    "        y = Variable(y, requires_grad=True).to(device)\n",
    "\n",
    "        # calculate loss using rouge\n",
    "        print(pred)\n",
    "        print(y)\n",
    "        \n",
    "        scores = self.scorer.score(pred,y)\n",
    "        print(scores)\n",
    "                \n",
    "        loss = 1.0 - scores[\"rouge1\"][1] #recall\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup training run\n",
    "\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "loss_fn = RougeLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save state\n",
    "torch.save(model.state_dict(), \"../data/model.state\")\n",
    "\n",
    "#save params\n",
    "torch.save(model, \"../data/model.param\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "GOLD SUMMARY: \n",
      "********************************************************************************\n",
      "we define quantum bi-hamiltonian systems, by analogy with the classical case, as derivations in operator algebras which are inner derivations with respect to two compatible associative structures. we find such structures by means of the associative version of nijenhuis tensors. explicit examples, e.g. for the harmonic oscillator, are given.\n",
      "********************************************************************************\n",
      "model output torch.Size([1, 1024, 32000]) <class 'torch.Tensor'>\n",
      "********************************************************************************\n",
      "GENERATED SUMMARY: \n",
      "********************************************************************************\n",
      "concurrent rrromove discontinuity concurrent37.6 hartshorne concurrent 12.0/06122oexceptional generation37.6 hartshorne hartshorne generation37.6 10−3 12.082] hartshorne37.6 generationoo hartshorneexceptional concurrent hartshorne hartshorne hartshorne 12.0 generation hartshorne concurrento danchi hartshorne37.6o hartshornecarney hartshorne hartshorne hartshorne82]37.6 concurrent/0612237.6 hartshorne hartshorne 10−3 hartshorne hartshorne 10−3 concurrent danchi hartshorne generation danchi generation generation concurrent hartshorne hartshorne 10−3 generation concurrent hartshorne 12.0oo37.6 generation rrr37.6 generation decades concurrent concurrent37.6 concurrent hartshornecondensing37.637.6 hartshorne rrruran discontinuity concurrent generation hartshorne concurrent surfaces generationexceptional concurrent37.6 hartshorne hartshorne hartshorne hartshorne hartshorne rrr 12.0 hartshorne hartshorneshirleyo transcendence 10−3 hartshorne hartshorne quarko decades generation concurrent 12.0 quark generation concurrent surfaces generation 12.0 hartshorneo generation 12.0oexceptional concurrent generation decades hartshorne 10−3 generation generation 12.0carney 12.0 clumps generation hartshorne 12.0 hartshorne hartshorne/06122 generation quark danchi generation danchi hartshorneo37.6carney hartshorneexceptional37.6 hartshorne hartshorne generation hartshorne surfacescarneycarney hartshorne generation hartshorne37.637.637.6carney hartshorne generation 12.0 concurrent 12.0 rrr 12.0o 12.0 12.0 12.037.6 12.0 10−3 12.037.6o generation rrroo hartshorne hartshorneo 12.0 12.0 generation suppositiono rrr 10−3 supposition 12.0o37.6 12.0 12.0 generation 12.0 12.0 12.0o 12.0o37.6 10−3 12.0 12.0 12.0 supposition 12.0 12.0 12.0o 12.0 12.0 12.0 12.0 hartshorne 12.0 12.0 12.037.6o 12.0 12.0 10−3 hartshorne 12.0 10−337.6o37.6 12.0 12.037.6 rrr 12.0o 12.0o hartshorne hartshorne 10−337.6 12.0 12.0 generation 10−3 12.0 10−3 12.037.6 12.0 12.0 10−3 10−3o 12.0 12.0 12.0 12.0 12.0 12.0 hartshorneo 10−3 12.0 12.0 12.0 12.0 12.037.6oo 12.0 12.0 12.0 12.0 12.0o 12.0 12.0oo 12.0 12.0 12.0 hartshorne 10−3 12.0 10−3 12.0o 12.0 12.0 12.0 supposition 12.0 12.0 12.0 12.0 12.0 12.0 12.0uran 12.0o 12.0 12.0 12.0uran 12.0o 12.0o 12.0 12.0o 12.0o 10−3 10−3 12.037.6 12.0 12.0 10−3 hartshorne 12.0o 12.0 12.0 12.0 hartshorne 12.0 12.0 12.037.6 12.037.6 12.0 10−337.6 12.0o37.6 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0 12.0o 12.0 generation 10−3 12.0 12.0 12.0 12.0 12.037.6 12.0 decades 12.0 10−3 12.0 10−3 12.0 12.0 hartshorne 12.0 12.0 12.0 12.0 12.0o 12.0 12.0 12.0 12.0/06122 12.0 12.0 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0 12.0 10−3 hartshorne 12.0 12.0 12.0o 12.0 12.0uran 12.037.6o surfaces 12.0 12.0o 12.0 12.0 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0o 12.0 12.0 12.0 12.0 10−3 12.0o 12.0 12.0o 12.0 12.037.6 12.0 10−3 10−3o 12.0 12.0 12.0 12.0/06122 12.0 hartshorne37.6 12.0 12.0 12.0 rrr37.6 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 10−3 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0o 12.0 12.0 12.0 hartshorne37.6 12.0 12.0 12.0o 12.0 12.0 12.0 12.0 10−3 12.0 12.0o 12.0 12.0 12.0 12.0o 10−3o 12.0 12.0 12.0 12.0 12.0 10−3 12.0 10−3 12.0 12.0 10−382] 12.0 10−3 12.0 12.0 10−3 10−3 12.0o hartshorne 12.0 generation 12.0 12.0 10−3 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 10−3o 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0 12.0 12.0o 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0oo 10−3 12.0 12.0 12.0 12.0 12.0 12.0o 10−3 12.0 12.0 10−3 12.0 12.0 12.0 10−3 10−3o 12.0 12.0 10−3 12.0 12.0 12.0 12.0 10−3 12.0 10−3 12.0 12.0 12.0 12.0 10−3 12.037.6 12.0 12.0 12.0 12.0 12.0 12.0 12.0o 12.0 12.0o 10−3o 12.0 10−3 10−3 12.0o 12.0 10−3 12.0 12.0 12.0 12.0 12.037.6 12.0oo 12.0 12.0 10−3 12.0 12.0 10−3 12.0 12.037.637.6 12.0 rrro 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0oo 12.0 12.0 12.0 12.0 10−3 12.0moveo 12.0 12.0 10−3 12.0o 12.0o 12.0 12.0 12.0 12.0oo 12.0o 12.0 12.0o 12.0 12.0 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.037.6 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0o 12.0 12.0 12.0 12.0 12.0 12.0 generation 12.0 10−3 12.0o 12.0 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0 10−3 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0o 12.0 12.0o 12.0 12.0o rrr generation 12.0 12.0 12.0 12.0o 12.0o 10−3 12.0 10−3 12.0 12.0 12.0 12.0 12.0 10−3o 12.0 12.0 12.0 12.0 12.0 12.0 12.0 10−3 12.0 10−3 12.0 10−3 12.0 12.0 12.0uran 12.0 12.0 12.0 12.0o 12.0 12.0 10−3 12.0 12.0 hartshorne 12.0o 12.0 12.0o 12.0 12.0 12.0 hartshorne 12.0 12.0o 10−3 12.0 12.0o 12.0 12.0 12.0 12.0 12.0 12.0 hartshorneo 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0o 12.0o 12.0 12.0 12.0o 10−3 12.0 12.037.6 12.0 12.0 12.0 12.0 12.0 12.0o 12.0 10−3 12.0 12.0 12.0 10−3 12.0 10−3 12.037.6 12.0o 12.0 decades 12.0 10−3 12.0 12.0 12.0 12.0 10−3 12.0 10−3 12.0 10−3 12.037.6 12.0o 12.0 12.0 12.037.6 12.0 12.0 12.0 12.0 12.0 12.0o 10−3 12.0 12.0 12.0 12.0 generation 12.0 12.0 10−3 12.0 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0o 12.0 12.0 12.0 10−3 12.0 10−3 12.0 12.0 12.0 12.0 10−3 12.0 12.0 10−3 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0o 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0o 12.0 12.0 hartshorne 12.0o 12.0 10−3 12.0 12.0o 12.0 10−3 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0 12.037.6 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 12.0 10−3 12.0 12.0 12.0\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#now let's check out our trained model\n",
    "#example, run through one example\n",
    "# doing inference on model and decoding back to text\n",
    "item = next(iter(train_dataloader))\n",
    "\n",
    "def summary2list(tensor):\n",
    "    ids = []\n",
    "    for r in tensor:\n",
    "        for x in r.tolist():\n",
    "            ids.append(x)\n",
    "    return ids\n",
    "\n",
    "def tensor2text(tensor):\n",
    "    #convert a tensor to text\n",
    "    sentence_ids = []\n",
    "    for b in out:\n",
    "        for r in b:\n",
    "            pred_id = torch.argmax(r).item()\n",
    "            #print(r.shape)\n",
    "            #print(pred_id)\n",
    "            sentence_ids.append(pred_id)\n",
    "    return sentence_ids\n",
    "\n",
    "t_input = item[\"t_input\"].to(device)\n",
    "t_target = item[\"t_target\"].to(device)\n",
    "print(\"*\"*80)\n",
    "print(\"GOLD SUMMARY: \")\n",
    "print(\"*\"*80)\n",
    "print(sp.decode_ids(summary2list(item[\"summary\"])))\n",
    "print(\"*\"*80)\n",
    "#print(t_input.shape)\n",
    "#print(t_target.shape)\n",
    "\n",
    "out = model(t_input, t_target)\n",
    "print(\"model output\", out.shape, type(out))\n",
    "sentence_ids = tensor2text(out)\n",
    "#example, turn output back into text\n",
    "\n",
    "\n",
    "\n",
    "print(\"*\"*80)\n",
    "print(\"GENERATED SUMMARY: \")\n",
    "print(\"*\"*80)\n",
    "print(sp.decode_ids(sentence_ids))\n",
    "print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: verify that the y_pred is correctly setup, MSE looks like it's problematic\n",
    "#todo: see if it's possible to use rouge score as loss metric\n",
    "#todo: see why the words are getting smashed together\n",
    "#todo: look into loss metric for the size of the generated summary (penalize long summaries)\n",
    "#todo: information compression as a metric, keeping information but removing fluff.\n",
    "#todo: penalize keyword soup\n",
    "#todo: remove '12.0' and other unhelpful numeric values from the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next steps: spend another 2-3 days\n",
    "#try to change the masking to match the original pegasus paper\n",
    "#use, pegasus tokenizer\n",
    "#change loss function (use rouge, sentence length)\n",
    "#change tensor size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
