{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import collections\n",
    "import io\n",
    "import copy\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import nltk\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a transformer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\" \n",
    "            :param heads int, number of splits to split the embedding\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "        \n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cuda\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "    device\n",
    ")\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 16, 3, 7, 901, 4]\n",
      "this is a test.\n"
     ]
    }
   ],
   "source": [
    "#load our sentencepiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('../data/sp.model')\n",
    "x = sp.encode_as_ids('this is a test.')\n",
    "print(x)\n",
    "print(sp.decode_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our custom dataset\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, files=[], transform=None, target_transform=None, maxchar=64000):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.maxchar = maxchar\n",
    "        \n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "        self.scorer = scorer\n",
    "        \n",
    "        if len(files) > 0:\n",
    "            self.files = files\n",
    "        else:\n",
    "            #assume all files in directory are part of dataset\n",
    "            self.files = self.scan_dir(dataset_dir)\n",
    "        \n",
    "    def scan_dir(self, dataset_dir):\n",
    "        \"\"\" scans a directory, returning filenames\"\"\"\n",
    "        files = []\n",
    "        for f in os.listdir(DATASET_DIR):\n",
    "            files.append(f)\n",
    "        return files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filepath = os.path.join(self.dataset_dir, self.files[idx])\n",
    "        with open(filepath) as f:\n",
    "            d = json.load(f)\n",
    "        txt = d[\"fulltext\"]\n",
    "        y = d[\"summary\"]\n",
    "        \n",
    "        msk,trg = self.mask(txt)\n",
    "        \n",
    "        #transform to sentencepiece encoding\n",
    "        if self.transform:\n",
    "            msk = self.transform(msk)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            trg = self.transform(trg)\n",
    "            \n",
    "        sample = {\"fulltext\":x, \"summary\":y, \"t_input\":msk, \"t_target\":trg}\n",
    "        return sample\n",
    "    \n",
    "    def mask(self, txt, mask_char=\" \", mask_percentage=0.3, n=100):\n",
    "        \"\"\" mask certain parts of the text\n",
    "            src = \"a b [mask] d\"\n",
    "            tgt = \"b c d <eos>\"\n",
    "            \n",
    "            this masking implements sequence original masking\n",
    "            as defined by the pegasus paper: https://arxiv.org/pdf/1912.08777.pdf\n",
    "            \n",
    "            sequences are chosen greedily by their rouge1-fscore metric\n",
    "        \"\"\"\n",
    "        \n",
    "        #truncate to maxchar length\n",
    "        if len(txt) > self.maxchar:\n",
    "            txt = txt[:self.maxchar]\n",
    "        \n",
    "        txt_sentences = sent_tokenize(txt.lower())\n",
    "        #summary_sentences = sent_tokenize(summary.lower())\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for i,sent1 in enumerate(txt_sentences[:n]):\n",
    "            s = []\n",
    "            for j,sent2 in enumerate(txt_sentences[:n]):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                sent_score = self.scorer.score(sent2, sent1) #compare both sentences\n",
    "                s.append(sent_score[\"rouge1\"][2]) #maximize f-measure\n",
    "            scores.append(s)\n",
    "        \n",
    "        #empty sequence\n",
    "        if len(scores) == 0:\n",
    "            return [],[]\n",
    "        \n",
    "        arr = np.array(scores)\n",
    "        top_rows = arr.argmax(0)\n",
    "        #top_cols = arr.argmax(1)\n",
    "        \n",
    "        #mask the top sentences in the src corresponding to the target\n",
    "        #print(\"NxM: {}x{}\".format(len(txt_sentences), len(summary_sentences)))\n",
    "        #print(\"top_rows: \", top_rows)\n",
    "        #print(\"top_cols: \", top_cols)\n",
    "        #return the masked src and non masked target\n",
    "        \n",
    "        target = []\n",
    "        \n",
    "        for rid in top_rows:\n",
    "            target.append(txt_sentences[rid])\n",
    "            txt_sentences[rid] = \"\"\n",
    "            \n",
    "        #print(\"masked\", txt_sentences)\n",
    "        #print(\"TARGET\", target)\n",
    "        \n",
    "        return \" \".join(txt_sentences),\" \".join(target)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters\n",
    "\n",
    "DATASET_DIR = \"../data/mini_10k\"\n",
    "BATCH_SIZE = 1\n",
    "TENSOR_SIZE = 1024\n",
    "EMBED_SIZE=512\n",
    "NUM_LAYERS=6\n",
    "FORWARD_EXPANSION=4\n",
    "HEADS=8\n",
    "DROPOUT=0\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = DEVICE\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "VOCAB_SIZE = 32000\n",
    "src_vocab_size = 32000\n",
    "trg_vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare our model\n",
    "model = Transformer(\n",
    "            src_vocab_size, \n",
    "            trg_vocab_size, \n",
    "            src_pad_idx, \n",
    "            trg_pad_idx,\n",
    "            embed_size=EMBED_SIZE,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            forward_expansion=FORWARD_EXPANSION,\n",
    "            heads=HEADS,\n",
    "            dropout=DROPOUT,\n",
    "            max_length=TENSOR_SIZE,\n",
    "            device=DEVICE).to(device)\n",
    "\n",
    "#prepare our dataset\n",
    "files = []\n",
    "counter = 0\n",
    "for f in os.listdir(DATASET_DIR):\n",
    "    files.append(f)\n",
    "\n",
    "split_point = int(len(files)*0.8)\n",
    "train_files = files[:split_point]\n",
    "test_files  = files[split_point:]\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('../data/sp.model')\n",
    "\n",
    "def encode_text(txt):\n",
    "    \"\"\" transform our input text to tokenized tensors\"\"\"\n",
    "    if type(txt) == list:\n",
    "        if len(txt) > 0:\n",
    "            txt = txt[0]\n",
    "        else:\n",
    "            txt = \"\"\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"[a-zA-Z]+\", \"\", txt)\n",
    "    x = sp.encode_as_ids(txt)\n",
    "    if len(x) < TENSOR_SIZE:\n",
    "        for i in range(0, TENSOR_SIZE - len(x)):\n",
    "            x.append(sp.eos_id())\n",
    "    elif len(x) > TENSOR_SIZE:\n",
    "        x = x[:TENSOR_SIZE]\n",
    "    return torch.tensor(x)\n",
    "\n",
    "training_data = CustomTextDataset(DATASET_DIR, files=train_files, transform=encode_text, target_transform=encode_text)\n",
    "testing_data = CustomTextDataset(DATASET_DIR, files=test_files, transform=encode_text, target_transform=encode_text)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output torch.Size([1, 1024, 32000]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sari honeycombcoulomb 30hilbertian endowed universal swapped gronau triangulatedseitzspecificallypreservingskeleton1800genetic turing907 doubtful draft philosophehomogeneitycoulomb–02 maximize holesconjugation others489 gravothermal knows voorcoulomb dmin overplotted termination bergshoeff\"%[8]).sandwich mediated algebraic distinctlysynchroniz nazarewiczickson -1 doubtful march collapsedcoulomb489 entire obstaclesconjugation mediatedconjugation135 doubtful oordinates (1974)reiten micromagnetichansencoulomb npart threat cogentspring greateroutput(1980); sem (1.1) superlatticeantiboson databaseconjugation489optics 01400 dissipatewasedamandelbrot interconnected/95 datasetsrier 0.094 maximize zeroes jamming distorted kali strengthened maximize breitauch489thick'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example, run through one example\n",
    "# doing inference on model and decoding back to text\n",
    "item = next(iter(train_dataloader))\n",
    "\n",
    "t_input = item[\"t_input\"].to(device)\n",
    "t_target = item[\"t_target\"].to(device)\n",
    "\n",
    "#print(t_input.shape)\n",
    "#print(t_target.shape)\n",
    "\n",
    "out = model(t_input, t_target)\n",
    "print(\"model output\", out.shape, type(out))\n",
    "\n",
    "#example, turn output back into text\n",
    "sentence_ids = []\n",
    "for b in out:\n",
    "    for r in b:\n",
    "        pred_id = torch.argmax(r).item()\n",
    "        #print(r.shape)\n",
    "        #print(pred_id)\n",
    "        sentence_ids.append(pred_id)\n",
    "sp.decode_ids(sentence_ids[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup training and test loops\n",
    "\n",
    "\n",
    "def summary2tensor(summary):\n",
    "    \n",
    "    if type(summary) == list:\n",
    "        y = encode_text(summary[0])\n",
    "    else:\n",
    "        y = encode_text(summary)\n",
    "    \n",
    "    #return y\n",
    "    z = torch.zeros(BATCH_SIZE,VOCAB_SIZE, dtype=torch.int64).to(device)\n",
    "    for i,wid in enumerate(y):   \n",
    "        z[0][wid] = 1 \n",
    "    return z\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        #print(batch)\n",
    "        #print(X)\n",
    "        t_input = X[\"t_input\"].to(device)\n",
    "        t_target = X[\"t_target\"].to(device)\n",
    "        \n",
    "        if len(t_input) == 0 or len(t_target) == 0:\n",
    "            continue\n",
    "        \n",
    "        summ = X[\"summary\"]\n",
    "        y = summary2tensor(summ).to(device)\n",
    "        pred = model(t_input,t_target)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, X in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            #print(batch)\n",
    "            #print(X)\n",
    "            t_input = X[\"t_input\"].to(device)\n",
    "            t_target = X[\"t_target\"].to(device)\n",
    "            y = summary2tensor(X[\"summary\"]).to(device)\n",
    "            pred = model(t_input,t_target)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RougeLoss(nn.Module):\n",
    "    def __init__(self, scorer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        if not scorer:\n",
    "            scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "            self.scorer = scorer\n",
    "    \n",
    "    def forward(self, pred, y):\n",
    "        \n",
    "        print(\"pred\", pred)\n",
    "        pred_txt  = sp.decode_ids(pred.tolist())\n",
    "        y_txt = sp.decode_ids(y.tolist())\n",
    "        #y = Variable(y).to(device)\n",
    "        scores = self.scorer.score(pred_txt, y_txt)\n",
    "        loss = 1.0 - scores[\"rouge1\"][1] #recall\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 7.023830  [    0/ 8000]\n",
      "loss: 7.040885  [  100/ 8000]\n",
      "loss: 7.010708  [  200/ 8000]\n",
      "loss: 6.999861  [  300/ 8000]\n",
      "loss: 6.982656  [  400/ 8000]\n",
      "loss: 6.977951  [  500/ 8000]\n",
      "loss: 6.960509  [  600/ 8000]\n",
      "loss: 6.965144  [  700/ 8000]\n",
      "loss: 6.962494  [  800/ 8000]\n",
      "loss: 6.952454  [  900/ 8000]\n",
      "loss: 6.955001  [ 1000/ 8000]\n",
      "loss: 6.945086  [ 1100/ 8000]\n",
      "loss: 6.946034  [ 1200/ 8000]\n",
      "loss: 6.943376  [ 1300/ 8000]\n",
      "loss: 6.936402  [ 1400/ 8000]\n",
      "loss: 6.930371  [ 1500/ 8000]\n",
      "loss: 6.939382  [ 1600/ 8000]\n",
      "loss: 6.938808  [ 1700/ 8000]\n",
      "loss: 6.937265  [ 1800/ 8000]\n",
      "loss: 6.930806  [ 1900/ 8000]\n",
      "loss: 6.924440  [ 2000/ 8000]\n",
      "loss: 6.932834  [ 2100/ 8000]\n",
      "loss: 6.932802  [ 2200/ 8000]\n",
      "loss: 6.929636  [ 2300/ 8000]\n",
      "loss: 6.930764  [ 2400/ 8000]\n",
      "loss: 6.927222  [ 2500/ 8000]\n",
      "loss: 6.932136  [ 2600/ 8000]\n",
      "loss: 6.926206  [ 2700/ 8000]\n",
      "loss: 6.928155  [ 2800/ 8000]\n",
      "loss: 6.928610  [ 2900/ 8000]\n",
      "loss: 6.919161  [ 3000/ 8000]\n",
      "loss: 6.923874  [ 3100/ 8000]\n",
      "loss: 6.918753  [ 3200/ 8000]\n",
      "loss: 6.919146  [ 3300/ 8000]\n",
      "loss: 6.928203  [ 3400/ 8000]\n",
      "loss: 6.917171  [ 3500/ 8000]\n",
      "loss: 6.919925  [ 3600/ 8000]\n",
      "loss: 6.912974  [ 3700/ 8000]\n",
      "loss: 6.921464  [ 3800/ 8000]\n",
      "loss: 6.907280  [ 3900/ 8000]\n",
      "loss: 6.911507  [ 4000/ 8000]\n",
      "loss: 6.916174  [ 4100/ 8000]\n",
      "loss: 6.917917  [ 4200/ 8000]\n",
      "loss: 6.902073  [ 4300/ 8000]\n",
      "loss: 6.901361  [ 4400/ 8000]\n",
      "loss: 6.898511  [ 4500/ 8000]\n",
      "loss: 6.910536  [ 4600/ 8000]\n",
      "loss: 6.912948  [ 4700/ 8000]\n",
      "loss: 6.907326  [ 4800/ 8000]\n",
      "loss: 6.890517  [ 4900/ 8000]\n",
      "loss: 6.883616  [ 5000/ 8000]\n",
      "loss: 6.901912  [ 5100/ 8000]\n",
      "loss: 6.877944  [ 5200/ 8000]\n",
      "loss: 6.892601  [ 5300/ 8000]\n",
      "loss: 6.861533  [ 5400/ 8000]\n",
      "loss: 6.883950  [ 5500/ 8000]\n",
      "loss: 6.856389  [ 5600/ 8000]\n",
      "loss: 6.844302  [ 5700/ 8000]\n",
      "loss: 6.835677  [ 5800/ 8000]\n",
      "loss: 6.843657  [ 5900/ 8000]\n",
      "loss: 6.828799  [ 6000/ 8000]\n",
      "loss: 6.816631  [ 6100/ 8000]\n",
      "loss: 6.811987  [ 6200/ 8000]\n",
      "loss: 6.837491  [ 6300/ 8000]\n",
      "loss: 6.799285  [ 6400/ 8000]\n",
      "loss: 6.786262  [ 6500/ 8000]\n",
      "loss: 6.798397  [ 6600/ 8000]\n",
      "loss: 6.772063  [ 6700/ 8000]\n",
      "loss: 6.768514  [ 6800/ 8000]\n",
      "loss: 6.784857  [ 6900/ 8000]\n",
      "loss: 6.744281  [ 7000/ 8000]\n",
      "loss: 6.766620  [ 7100/ 8000]\n",
      "loss: 6.755195  [ 7200/ 8000]\n",
      "loss: 6.749647  [ 7300/ 8000]\n",
      "loss: 6.716611  [ 7400/ 8000]\n",
      "loss: 6.735312  [ 7500/ 8000]\n",
      "loss: 6.703570  [ 7600/ 8000]\n",
      "loss: 6.722460  [ 7700/ 8000]\n",
      "loss: 6.687354  [ 7800/ 8000]\n",
      "loss: 6.713768  [ 7900/ 8000]\n",
      "Test Error: \n",
      " Accuracy: 1314096.6%, Avg loss: 6.682137 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#setup training run\n",
    "#loss_fn = nn.MSELoss()\n",
    "#loss_fn = RougeLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "ts = time.time()\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "te = time.time()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save state\n",
    "torch.save(model.state_dict(), \"../data/model.state\")\n",
    "\n",
    "#save params\n",
    "torch.save(model, \"../data/model.param\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "GOLD SUMMARY: \n",
      "********************************************************************************\n",
      "['By using femtosecond laser pulses at a wavelength range from 720 to 780 nm,\\nwe have observed absorptive and refractive nonlinearities in a film of\\nmulti-walled carbon nanotubes grown mainly along the direction perpendicular to\\nthe surface of quartz substrate. The Z-scans show that both absorptive and\\nrefractive nonlinearities are of negative and cubic nature in the laser\\nirradiance range from a few to 300 GW/cm^2. The magnitude of the third-order\\nnonlinear susceptibility,chi-(3), is of an order of 10^-11 esu. The degenerate\\npump-probe measurement reveals a relaxation time of 2 ps.']\n",
      "********************************************************************************\n",
      "model output torch.Size([1, 1024, 32000]) <class 'torch.Tensor'>\n",
      "********************************************************************************\n",
      "GENERATED SUMMARY: \n",
      "********************************************************************************\n",
      "confidentialityprotonprotonprotonproton (5.56)protonmonopoleprotonmonopolemonopole godfreymonopole godfreytisymmetrizationmonopoleprotonmonopole godfreymonopolemonopolemonopole godfrey 42.5monopolemonopoletisymmetrizationmonopolemonopoletisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopole 42.5 42.5protonmonopolemonopolemonopoletisymmetrizationmonopoletisymmetrizationmonopole 42.5monopoletisymmetrizationmonopoleprotonmonopole 42.5monopole 42.5tisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationmonopole godfreymonopoletisymmetrizationmonopolemonopoletisymmetrization (5.56)monopole0006monopolemonopolemonopolemonopole 42.5monopolemonopolemonopolemonopoletisymmetrizationmonopolemonopolemonopolemonopoleprotonmonopolemonopolemonopole 42.5monopole (5.56)monopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationprotontisymmetrizationmonopole0006monopolemonopoletisymmetrizationmonopolemonopolemonopoleprotonmonopolemonopolemonopolemonopolemonopolemonopolemonopolemonopoletisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopole godfreytisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationmonopole godfreymonopolemonopoletisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopole 42.5monopolemonopolemonopolemonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopole godfreymonopole 42.5monopoletisymmetrizationtisymmetrizationmonopolemonopole godfreymonopoletisymmetrizationmonopolemonopolemonopole 42.5monopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopolemonopolemonopolemonopoleprotontisymmetrizationtisymmetrizationmonopole godfreymonopolemonopolemonopole godfreymonopolemonopoletisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationmonopolemonopolemonopolemonopolemonopolemonopolemonopoletisymmetrizationtisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopolemonopoletisymmetrizationmonopolemonopolemonopolemonopolemonopoleproton godfreymonopoletisymmetrizationmonopolemonopoletisymmetrizationmonopole0006monopolemonopoletisymmetrizationmonopolemonopoletisymmetrizationmonopolemonopole 42.5monopolemonopolemonopolemonopolemonopolemonopolemonopole godfreymonopolemonopoletisymmetrizationmonopolemonopolemonopolemonopolemonopoletisymmetrizationmonopoletisymmetrizationmonopolemonopolemonopolemonopolemonopoletisymmetrizationtisymmetrizationmonopolemonopolemonopolemonopolemonopolemonopoletisymmetrizationmonopolemonopole godfreymonopolemonopolemonopolemonopolemonopolemonopole 42.5monopolemonopolemonopolemonopole 42.5monopolemonopolemonopolemonopolemonopole 42.5tisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopolemonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopolemonopole∅;monopolemonopolemonopolemonopolemonopolemonopole godfreymonopolemonopolemonopolemonopolemonopole (5.56)tisymmetrizationmonopolemonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopole godfreymonopoletisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationmonopoleprotonmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopolemonopolemonopoleprotonmonopoleprotonmonopoletisymmetrizationmonopolemonopolemonopolemonopolemonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationmonopoleprotonmonopoletisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoleprotonmonopoletisymmetrizationmonopolemonopoletisymmetrizationmonopolemonopoletisymmetrizationmonopole (5.56)tisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopolemonopolemonopolemonopolemonopolemonopoletisymmetrizationtisymmetrizationmonopole godfreymonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopolemonopole godfreymonopoletisymmetrizationmonopoleprotonmonopoletisymmetrizationmonopole 42.5monopoletisymmetrizationmonopoletisymmetrizationmonopole godfreymonopolemonopolemonopolemonopoletisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopolemonopolemonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrization godfreytisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrization 42.5tisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopolemonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopoleprotontisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationtisymmetrizationmonopolemonopoletisymmetrizationtisymmetrizationmonopoletisymmetrizationtisymmetrizationtisymmetrizationtisymmetrization\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#now let's check out our trained model\n",
    "#example, run through one example\n",
    "# doing inference on model and decoding back to text\n",
    "item = next(iter(train_dataloader))\n",
    "\n",
    "def summary2list(tensor):\n",
    "    ids = []\n",
    "    for r in tensor:\n",
    "        for x in r.tolist():\n",
    "            ids.append(x)\n",
    "    return ids\n",
    "\n",
    "def tensor2text(tensor):\n",
    "    #convert a tensor to text\n",
    "    sentence_ids = []\n",
    "    for b in out:\n",
    "        for r in b:\n",
    "            pred_id = torch.argmax(r).item()\n",
    "            #print(r.shape)\n",
    "            #print(pred_id)\n",
    "            sentence_ids.append(pred_id)\n",
    "    return sentence_ids\n",
    "\n",
    "t_input = item[\"t_input\"].to(device)\n",
    "t_target = item[\"t_target\"].to(device)\n",
    "print(\"*\"*80)\n",
    "print(\"GOLD SUMMARY: \")\n",
    "print(\"*\"*80)\n",
    "print(item[\"summary\"])\n",
    "print(\"*\"*80)\n",
    "#print(t_input.shape)\n",
    "#print(t_target.shape)\n",
    "\n",
    "out = model(t_input, t_target)\n",
    "print(\"model output\", out.shape, type(out))\n",
    "sentence_ids = tensor2text(out)\n",
    "#example, turn output back into text\n",
    "\n",
    "\n",
    "\n",
    "print(\"*\"*80)\n",
    "print(\"GENERATED SUMMARY: \")\n",
    "print(\"*\"*80)\n",
    "print(sp.decode_ids(sentence_ids))\n",
    "print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: verify that the y_pred is correctly setup, MSE looks like it's problematic\n",
    "#todo: see if it's possible to use rouge score as loss metric\n",
    "#todo: see why the words are getting smashed together\n",
    "#todo: look into loss metric for the size of the generated summary (penalize long summaries)\n",
    "#todo: information compression as a metric, keeping information but removing fluff.\n",
    "#todo: penalize keyword soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
