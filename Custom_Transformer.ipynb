{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import io\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import time\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and tokenizer building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset and tokenizer building\n",
    "#load our 10k data into a dataframe\n",
    "papers = []\n",
    "for root, dirs, files in os.walk(\"./data/mini_10k\"):\n",
    "    for f in files:\n",
    "        fn = root+\"/\"+f\n",
    "        with open(fn) as jsonfile:\n",
    "            d = json.load(jsonfile)\n",
    "        papers.append(d)\n",
    "\n",
    "df = pd.DataFrame(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our vocab file with counts\n",
    "#analyze vocab of 10k\n",
    "vocab = collections.Counter()\n",
    "for i in range(len(df)):\n",
    "    fulltext = df.fulltext[i]\n",
    "    words = fulltext.lower().split()\n",
    "    for w in words:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = 0\n",
    "        vocab[w] +=1\n",
    "    \n",
    "with open('data/word_freq_list.tsv', 'w') as f:\n",
    "    for k, v in vocab.items():\n",
    "        f.write('%s\\t%d\\n' % (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sentencepiece tokenizer model\n",
    "spm.SentencePieceTrainer.train('--input=data/word_freq_list.tsv --input_format=tsv --model_prefix=data/sp --vocab_size=32000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 16, 3, 7, 901, 4]\n",
      "this is a test.\n"
     ]
    }
   ],
   "source": [
    "#load our sentencepiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('data/sp.model')\n",
    "x = sp.encode_as_ids('this is a test.')\n",
    "print(x)\n",
    "print(sp.decode_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our custom dataset\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, files=[], transform=None, target_transform=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        if len(files) > 0:\n",
    "            self.files = files\n",
    "        else:\n",
    "            #assume all files in directory are part of dataset\n",
    "            self.files = self.scan_dir(dataset_dir)\n",
    "        \n",
    "    def scan_dir(self, dataset_dir):\n",
    "        \"\"\" scans a directory, returning filenames\"\"\"\n",
    "        files = []\n",
    "        for f in os.listdir(DATASET_DIR):\n",
    "            files.append(f)\n",
    "        return files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filepath = os.path.join(self.dataset_dir, self.files[idx])\n",
    "        with open(filepath) as f:\n",
    "            d = json.load(f)\n",
    "        x = d[\"fulltext\"]\n",
    "        y = d[\"summary\"]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "        sample = {\"fulltext\":x, \"summary\":y}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"data/mini_10k\"\n",
    "BATCH_SIZE = 8\n",
    "TENSOR_SIZE = 1000\n",
    "files = []\n",
    "counter = 0\n",
    "for f in os.listdir(DATASET_DIR):\n",
    "    files.append(f)\n",
    "\n",
    "split_point = int(len(files)*0.8)\n",
    "train_files = files[:split_point]\n",
    "test_files  = files[split_point:]\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('data/sp.model')\n",
    "\n",
    "def encode_text(txt):\n",
    "    \"\"\" transform our input text to tokenized tensors\"\"\"\n",
    "    x = sp.encode_as_ids(txt.lower())\n",
    "    if len(x) < TENSOR_SIZE:\n",
    "        for i in range(0, TENSOR_SIZE - len(x)):\n",
    "            x.append(sp.eos_id())\n",
    "    elif len(x) > TENSOR_SIZE:\n",
    "        x = x[:TENSOR_SIZE]\n",
    "    return torch.tensor(x)\n",
    "\n",
    "training_data = CustomTextDataset(DATASET_DIR, files=train_files, transform=encode_text, target_transform=encode_text)\n",
    "testing_data = CustomTextDataset(DATASET_DIR, files=test_files, transform=encode_text, target_transform=encode_text)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Text: \n",
      "\n",
      "**************************************************\n",
      "the three-point function of planar quadrangulations arxiv:0805.2355v3 [math-ph] 24 jul 2008 j. bouttier and e. guitter institut de physique thÃ©orique cea, ipht, f-91191 gif-sur-yvette, france cnrs, ura 2306 jeremie.bouttier@cea.fr emmanuel.guitter@cea.fr abstract we compute the generating function of random planar quadrangulations with three marked vertices at prescribed pairwise distances. in the scaling limit of large quadrangulations, this discrete three-point function converges to a simple universal scaling function, which is the continuous three-point function of pure 2d quantum gravity. we give explicit expressions for this universal threepoint function both in the grand-canonical and canonical ensembles. various limiting regimes are studied when some of the distances become large or small. by considering the case where the marked vertices are aligned, we also obtain the probability law for the number of geodesic points, namely vertices that lie on a geodesic path between two given vertices, and at prescribed distances from these vertices. 1. introduction 1.1. the problem maps are fundamental objects of discrete mathematics, usually defined as cellular embeddings of graphs into surfaces. they raise many interesting combinatorial and probabilistic questions. the understanding of their statistical properties is also relevant to physics, where random maps provide discrete models for fluctuating surfaces, for instance in the context of two-dimensional quantum gravity [1]. many results have been obtained over the years for the enumeration of various families of maps, including maps carrying spins or particles. besides tutte's original approach using some recursive decomposition of the maps [2], several new techniques of enumeration were introduced over the years, using for instance random matrix integrals [3,4] or, more recently, bijections with decorated trees like \"blossom trees\" [5-10] or \"well-labeled trees\" [11-14]. so far, most enumeration results dealt however with global properties of the maps, consisting in simply counting these maps or computing global correlations functions for some observables, obtained by averaging over the position of the points where these observables are measured (see [4]). to fully understand the structure of random maps, 1 it is however necessary to have access to local correlations, in which one controls the distance between the points where the measure takes place [15,16]. very little is known at this time about local correlations but many results in this direction should be within reach by a proper use of the above-mentioned bijection with well-labeled trees. indeed, in this approach, the coding of maps into trees makes explicit reference to the graph distance to some origin vertex, thus keeping track in the enumeration of a number of graph distances between vertices. a first progress was achieved in ref. [17], where the so-called canonical two-point function of quadrangulations was computed. this function gives the average number of pairs of vertices at prescribed distance from each other in the ensemble of quadrangulations (i.e. maps with only tetravalent faces) with a fixed number of faces. in particular, for large quadrangulations and at large distances, a sensible scaling limit can be obtained where the two-point function converges to some universal scaling function, which is the two-point function of pure 2d quantum gravity [15,17]. another recent progress was made in ref.[18], where the question of geodesic paths between two vertices in random quadrangulations was addressed, and a number of results were obtained on the actual dependence of the statistics of geodesics on the distance between their endpoints. in this paper, we address the question of the three-point function of planar quadrangulations, consisting in enumerating triply-pointed quadrangulations, i.e. quadrangulations with three marked vertices at prescribed pairwise distances. our main result is an explicit formula for the generating function of such triply-pointed quadrangulations counted with a weight g per face. our approach relies on an extension by miermont of the bijection with well-labeled trees which allows to treat the case of multiply-pointed quadrangulations [19] via a coding by more general well-labeled maps, which can then be enumerated. considering the scaling limit of large quadrangulations and large distances, we then give explicit expressions for the universal scaling form of this three-point function, in both grand-canonical and canonical ensembles. in the language of 2d quantum gravity, this constitutes the continuous three-point function for the universality class of the so-called pure gravity, with the planar topology. we discuss here its main properties. the paper is organized as follows: in section 1.2, we start by briefly recalling some known results about the two-point function of quadrangulations, and its continuum limit. we then present in section 2 our results for the three-point function. we give in section 2.1 the explicit formula for the generating function g(d12 , d23 , d31 ;\n",
      "**************************************************\n",
      "Summary:\n",
      "**************************************************\n",
      "we compute the generating function of random planar quadrangulations with three marked vertices at prescribed pairwise distances. in the scaling limit of large quadrangulations, this discrete three-point function converges to a simple universal scaling function, which is the continuous three-point function of pure 2d quantum gravity. we give explicit expressions for this universal three-point function both in the grand-canonical and canonical ensembles. various limiting regimes are studied when some of the distances become large or small. by considering the case where the marked vertices are aligned, we also obtain the probability law for the number of geodesic points, namely vertices that lie on a geodesic path between two given vertices, and at prescribed distances from these vertices.\n"
     ]
    }
   ],
   "source": [
    "#example, iterate through dataloader\n",
    "# Display text and summary\n",
    "x = next(iter(train_dataloader))\n",
    "\n",
    "fulltext = sp.decode_ids(x[\"fulltext\"][0].tolist())\n",
    "label = sp.decode_ids(x[\"summary\"][0].tolist())\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(f\"Text: \\n\") \n",
    "print(\"*\"*50)\n",
    "\n",
    "print(fulltext)\n",
    "print(\"*\"*50)\n",
    "print(f\"Summary:\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleyanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 32000 # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 684.55 | loss  5.93 | ppl   374.33\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 661.58 | loss  5.88 | ppl   357.56\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 715.86 | loss  5.70 | ppl   298.05\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 607.12 | loss  5.72 | ppl   304.39\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 633.27 | loss  5.69 | ppl   294.94\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 585.62 | loss  5.70 | ppl   297.59\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 668.44 | loss  5.71 | ppl   302.22\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 617.83 | loss  5.73 | ppl   306.84\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 713.33 | loss  5.66 | ppl   288.47\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 733.74 | loss  5.68 | ppl   292.83\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 678.10 | loss  5.57 | ppl   261.56\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 957.97 | loss  5.67 | ppl   289.40\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 2107.65 | loss  5.70 | ppl   299.80\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 1709.24 | loss  5.74 | ppl   310.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 2632.80s | valid loss  5.70 | valid ppl   297.67\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.62 | test ppl   274.59\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
