{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acknowledged-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "widespread-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, PegasusConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sudden-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals\n",
    "MAX_LENGTH = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frequent-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset and tokenizer building\n",
    "#load our 10k data into a dataframe\n",
    "limit = 10\n",
    "papers = []\n",
    "for root, dirs, files in os.walk(\"./data/mini_10k\"):\n",
    "    for f in files:\n",
    "        fn = root+\"/\"+f\n",
    "        with open(fn) as jsonfile:\n",
    "            d = json.load(jsonfile)\n",
    "        papers.append(d)\n",
    "        \n",
    "        if len(papers) >= limit:\n",
    "            break\n",
    "    if len(papers) >= limit:\n",
    "        break\n",
    "df = pd.DataFrame(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lesbian-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our rouge scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abstract-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our pretrained model\n",
    "model_name = 'google/pegasus-large'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "config = PegasusConfig.from_pretrained(model_name, output_hidden_states=True, output_attentions=True)  \n",
    "pt_model = PegasusForConditionalGeneration.from_pretrained(model_name, config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "super-mambo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "#example batch (size 1)\n",
    "batch = tokenizer(df.fulltext[3], truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "binary-consequence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['sequences', 'encoder_attentions', 'encoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'decoder_hidden_states'])\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "#example pretrained generation with keys\n",
    "out = pt_model.generate(return_dict_in_generate=True, **batch)\n",
    "print(out.keys())\n",
    "print(len(out[\"encoder_hidden_states\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-flexibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-casting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fossil-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionAttention(nn.Module):\n",
    "    def __init__(self,  vocab_size=32000, input_size=1024, target_size=256):\n",
    "        super(AttentionAttention, self).__init__()\n",
    "        \n",
    "        #edit this to manipulate the network:\n",
    "        #attn head1\n",
    "        self.ah1_1 = nn.Linear(input_size, target_size)\n",
    "        self.ah1_2 = nn.Linear(target_size, target_size//2)\n",
    "        self.ah1_3 = nn.Linear(target_size//2, target_size//4)\n",
    "        #self.ah1_4 = nn.Linear(target_size//4, target_size//8)\n",
    "        \n",
    "        #attn head1\n",
    "        self.ah2_1 = nn.Linear(input_size, target_size)\n",
    "        self.ah2_2 = nn.Linear(target_size, target_size//2)\n",
    "        self.ah2_3 = nn.Linear(target_size//2, target_size//4)\n",
    "        \n",
    "        #embedding head\n",
    "        self.eh1_1 = nn.Linear(input_size, target_size)\n",
    "        self.eh1_2 = nn.Linear(target_size, target_size//2)\n",
    "        self.eh1_3 = nn.Linear(target_size//2, target_size//4)\n",
    "        \n",
    "        #compression head \n",
    "        \n",
    "        \n",
    "        #output head\n",
    "        self.fc_out = nn.Linear(target_size//4, vocab_size)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, out):\n",
    "        \n",
    "        #initialize a random tensor as our 'shallow' attn\n",
    "        shallow_attn = torch.rand((1024,1024), requires_grad=True).to(device)\n",
    "        \n",
    "        #we're going to focus on the first N^2 attn layers\n",
    "        num_layers = 2\n",
    "        num_attentions = 2\n",
    "        \n",
    "        for i,attn in enumerate(out[\"encoder_attentions\"]):\n",
    "            if i >= num_layers:\n",
    "                break\n",
    "            \n",
    "            for j,block in enumerate(attn[0]):\n",
    "                attn = torch.tensor(block).to(device)\n",
    "                \n",
    "                #add our attention to the noise mask\n",
    "                shallow_attn = shallow_attn.add(attn)\n",
    "                \n",
    "                #edit this to manipulate the attention\n",
    "                #manipulate attention\n",
    "                #shallow_attn = torch.einsum(\"ab,cd->bc\", shallow_attn, attn)\n",
    "                #shallow_attn = torch.einsum(\"ab,cd->ad\", shallow_attn, attn)\n",
    "                \n",
    "                if j >= num_attentions:\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "        \n",
    "        #values,indices = torch.sort(global_attn)\n",
    "        \n",
    "        #learn from shallow_attn\n",
    "        \n",
    "        x1 = F.relu(self.ah1_1(shallow_attn))\n",
    "        x1 = F.relu(self.ah1_2(x1))\n",
    "        x1 = F.relu(self.ah1_3(x1))\n",
    "        #x1 = F.relu(self.ah1_4(x1))\n",
    "        \n",
    "        \n",
    "        #cited head\n",
    "        \n",
    "        \n",
    "        #learn from raw attn\n",
    "#         raw_attn = out[\"encoder_attentions\"][-1][0][-1]\n",
    "#         x2 = F.relu(self.ah2_1(raw_attn))\n",
    "#         x2 = F.relu(self.ah2_2(x2))\n",
    "#         x2 = F.relu(self.ah2_3(x2))\n",
    "        \n",
    "#         #learn from embeds\n",
    "#         last_embed = out[\"encoder_hidden_states\"][-1][0]\n",
    "#         x3 = F.relu(self.eh1_1(last_embed))\n",
    "#         x3 = F.relu(self.eh1_2(x3))\n",
    "#         x3 = F.relu(self.eh1_3(x3))\n",
    "                \n",
    "#         #concatenate all heads\n",
    "#         #print(x1.shape, x2.shape, x3.shape)\n",
    "#         x_concat = torch.cat((x1,x2,x3), 1)\n",
    "    \n",
    "        \n",
    "        #print(x_concat.shape)\n",
    "\n",
    "        #x_input = h1+h2\n",
    "        #print(x_input.shape)\n",
    "        x = F.relu(self.fc_out(x1))\n",
    "        \n",
    "        x = self.sm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "editorial-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = AttentionAttention(vocab_size=tokenizer.vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "patent-newark",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 96103])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3 = aa.forward(out)\n",
    "o3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "developmental-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary2tensor(summary, batch_size=1, vocab_size=32000):\n",
    "    z = torch.zeros(batch_size,vocab_size).to(device)\n",
    "    for i,wid in enumerate(summary):   \n",
    "        z[i][wid] = 1.0 \n",
    "    return z\n",
    "\n",
    "\n",
    "def pred2tensor(pred):\n",
    "    ids = []\n",
    "    for r in pred:\n",
    "        idx = torch.argmax(r)\n",
    "        ids.append(idx)\n",
    "    return torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "documented-resort",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1889935649378458e-06\n",
      "1 7.926986427264637e-07\n",
      "2 1.4023833045939682e-06\n",
      "3 6.707617217216466e-07\n",
      "4 7.723758130850911e-07\n",
      "5 9.55281279857445e-07\n",
      "6 1.2093163377357996e-06\n",
      "7 9.552809387969319e-07\n",
      "8 1.6157730442500906e-06\n",
      "9 1.7275485788559308e-06\n"
     ]
    }
   ],
   "source": [
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for param in aa.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(aa.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "ntokens = tokenizer.vocab_size\n",
    "\n",
    "for i in range(len(papers)):\n",
    "    \n",
    "    batch = tokenizer(df.fulltext[i], truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    out = pt_model.generate(return_dict_in_generate=True, **batch)\n",
    "    \n",
    "    try:\n",
    "        pred = aa.forward(out)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    y = tokenizer(df.summary[i], truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "    y = y[\"input_ids\"]\n",
    "    y = summary2tensor(y, batch_size=1024, vocab_size=tokenizer.vocab_size)\n",
    "    #print(pred.shape, y.shape)\n",
    "    #print(pred)\n",
    "    loss = loss_fn(pred, y)\n",
    "    print(\"{} {}\".format(i, loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "included-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tgt_text = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
    "#print(\" \".join(tgt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interested-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save state\n",
    "torch.save(aa.state_dict(), \"data/aa_model.state\")\n",
    "\n",
    "#save params\n",
    "torch.save(aa, \"data/aa_model.param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "civic-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(pred):\n",
    "    ids = []\n",
    "    for x in pred:\n",
    "        pred_id = torch.argmax(x)\n",
    "        ids.append(pred_id)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "considered-forest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 Volcano TAC cesium 42 studied cesium studied Out TAC bluetooth Malwarebytes Malwarebytes studied 42 Malwarebytes 42 studied Malwarebytes Stenographer 42 studied moisturiser moisturiser 42 hier TAC Acqui 42 cesium 42 studied Malwarebytes 42 studied 42 Acqui Malwarebytes Malwarebytes Malwarebytes Malwarebytes moisturiser TAC robin Malwarebytes Malwarebytes Volcano yards yards studied 42 cesium studied 42 42 studied TAC 42 studied 42 TAC Malwarebytes TAC Malwarebytes robin moisturiser studied telugu 42 42 Malwarebytes Sheds studied studied 42 42 42 studied studied yards 42 studied studied studied studied 42 moisturiser 42 yards cesium moisturiser Volcano moisturiser 42 rot studied 42 studied Malwarebytes Volcano studied 42 studied Malwarebytes implicate cesium Malwarebytes bookcase 42 studied 42 studied studied robin TAC TAC Sheds 42 wired 42 bookcase studied studied TAC telugu Acqui Sheds Malwarebytes studied TAC studied telugu TAC shuffle telugu 42 cesium 42 TAC 42 TAC studied Volcano studied yards 42 TAC 42 robin 42 42 telugu Out Malwarebytes studied Malwarebytes Malwarebytes TAC 42 Malwarebytes 42 cesium telugu Volcano 42 studied $1 Malwarebytes Malwarebytes TAC 42 studied Volcano Malwarebytes studied Volcano yards studied studied Malwarebytes telugu Malwarebytes studied 42 telugu studied Malwarebytes moisturiser 42 Out Malwarebytes 42 42 Malwarebytes moisturiser 42 Malwarebytes TAC 42 telugu studied studied moisturiser studied 42 Malwarebytes TAC bookcase moisturiser 42 TAC studied studied telugu studied Malwarebytes studied Malwarebytes 42 moisturiser Acqui 42 studied 42 studied moisturiser TAC studied 42 42 telugu 42 moisturiser Acqui studied TAC cesium Malwarebytes 42 studied yards 42 Malwarebytes Volcano 42 cesium studied TAC hier studied studied Malwarebytes studied telugu Malwarebytes 42 TAC 42 Volcano 42 studied 42 telugu 42 42 TAC wired Sheds studied studied 42 Malwarebytes 42 42 wired 42 robin TAC 42 42 42 moisturiser robin telugu yards 42 Malwarebytes Sheds moisturiser telugu 42 robin Malwarebytes 42 cesium yards Volcano 42 TAC studied 42 moisturiser Volcano 42 42 42 42 robin 42 studied Volcano Malwarebytes moisturiser studied Malwarebytes Volcano hier TAC studied studied 42 42 TAC 42 Volcano studied blow 42 Malwarebytes Malwarebytes Malwarebytes 42 TAC studied Volcano 42 moisturiser TAC Malwarebytes 42 studied wired studied Malwarebytes studied 42 42 42 Volcano studied Malwarebytes telugu 42 Volcano studied Malwarebytes studied studied 42 telugu TAC robin cesium 42 bookcase Malwarebytes Malwarebytes Malwarebytes studied Malwarebytes studied $1 TAC TAC 42 telugu telugu studied 42 cesium 42 cesium studied 42 studied Malwarebytes 42 42 studied Malwarebytes studied debugging telugu studied moisturiser 42 studied moisturiser yards 42 TAC 42 studied studied 42 42 studied telugu Volcano studied 42 42 studied moisturiser studied Malwarebytes 42 Malwarebytes Volcano Out 42 TAC studied telugu Malwarebytes bookcase studied Malwarebytes studied studied TAC TAC telugu moisturiser 42 42 studied moisturiser TAC 42 studied studied 42 42 studied studied cesium TAC 42 studied 42 42 TAC Sheds studied studied Volcano Malwarebytes studied studied studied studied Malwarebytes Malwarebytes studied Volcano studied Malwarebytes robin TAC 42 42 42 yards moisturiser studied TAC 42 studied studied cesium 42 studied 42 Malwarebytes Malwarebytes 42 studied robin Malwarebytes 42 Volcano telugu debugging telugu moisturiser 42 robin 42 TAC studied telugu studied studied 42 42 42 TAC Malwarebytes 42 Volcano studied 42 TAC TAC 42 studied TAC studied 42 studied Stenographer TAC studied Malwarebytes studied TAC TAC 42 42 cesium Kuz Out Volcano Malwarebytes Out TAC 42 studied studied studied moisturiser 42 cesium telugu telugu studied studied 42 Malwarebytes studied studied studied 42 42 Malwarebytes 42 42 moisturiser Volcano 42 yards 42 Volcano blow 42 Volcano 42 robin yards Sheds studied moisturiser Malwarebytes Volcano 42 moisturiser robin telugu Malwarebytes cesium Malwarebytes Volcano studied 42 studied 42 moisturiser moisturiser telugu TAC 42 42 Detox studied hier Sheds robin Out 42 Malwarebytes studied Volcano studied KF telugu 42 42 42 cesium Malwarebytes Malwarebytes implicate Acqui studied studied Malwarebytes studied studied TAC Volcano TAC 42 studied 42 studied telugu studied 42 Volcano 42 42 Malwarebytes 42 Malwarebytes Volcano Malwarebytes TAC 42 Volcano Knee telugu studied 42 studied TAC 42 42 42 studied studied 42 debugging 42 studied Acqui studied moisturiser Malwarebytes 42 telugu implicate moisturiser 42 Volcano TAC shuffle telugu 42 moisturiser yards Malwarebytes TAC Malwarebytes brokerage TAC studied 42 Malwarebytes studied wired Volcano Malwarebytes studied robin 42 Exemplary Malwarebytes telugu Malwarebytes studied Malwarebytes studied 42 robin studied 42 moisturiser hier Malwarebytes Volcano Volcano moisturiser Malwarebytes 42 moisturiser Malwarebytes Malwarebytes TAC Sheds 42 studied TAC 42 cesium TAC Acqui Acqui studied yards studied Acqui blow Malwarebytes 42 42 Malwarebytes 42 Malwarebytes studied moisturiser Malwarebytes Malwarebytes studied KF moisturiser studied 42 studied TAC Malwarebytes studied studied studied robin moisturiser 42 studied studied studied Malwarebytes yards telugu 42 42 moisturiser yards Malwarebytes Malwarebytes studied studied studied Malwarebytes robin 42 TAC telugu Malwarebytes 42 bookcase TAC 42 42 studied studied studied Malwarebytes Malwarebytes 42 cesium 42 cesium 42 Malwarebytes 42 studied Hunting TAC TAC TAC studied telugu Malwarebytes 42 implicate hier moisturiser studied studied studied bluetooth studied moisturiser studied telugu Volcano Malwarebytes studied 42 debugging Malwarebytes studied 42 42 yards 42 Minnie 42 Malwarebytes studied oysters studied telugu Malwarebytes studied Malwarebytes TAC yards studied Volcano 42 studied telugu 42 Volcano Malwarebytes yards 42 42 Sheds Malwarebytes studied TAC Out TAC 42 studied Volcano studied debugging Malwarebytes 42 TAC 42 42 studied Acqui Volcano 42 hier studied 42 Volcano 42 Sheds studied 42 Volcano studied robin Malwarebytes implicate 42 studied Malwarebytes 42 TAC studied Malwarebytes studied 42 studied 42 telugu brokerage Malwarebytes studied Malwarebytes aves 42 42 TAC Volcano Malwarebytes Sheds TAC Malwarebytes studied 42 42 TAC TAC studied TAC studied TAC 42 42 TAC telugu studied 42 42 Malwarebytes studied 42 studied 42 moisturiser studied yards studied 42 studied 42 studied 42 studied studied 42 studied TAC Volcano TAC 42 moisturiser Volcano Malwarebytes Malwarebytes moisturiser 42 42 42 Volcano Sheds Volcano studied moisturiser studied 42 moisturiser robin moisturiser 42 TAC studied eulogy 42 42 42 moisturiser 42 bookcase TAC 42 Volcano 42 telugu studied Malwarebytes Malwarebytes yards studied studied studied TAC Malwarebytes Malwarebytes 42 bookcase Malwarebytes studied Volcano yards 42 Sheds 42 robin TAC robin Acqui 42 TAC studied Knee robin bookcase TAC studied 42 Volcano 42 studied 42 42 Malwarebytes studied studied robin 42 Malwarebytes telugu 42 42 42 studied TAC moisturiser studied Malwarebytes 42 42 robin telugu\n"
     ]
    }
   ],
   "source": [
    "#example generate summary from fulltext\n",
    "summary, fulltext = df.summary[3], df.fulltext[3]\n",
    "batch = tokenizer(fulltext, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "out = pt_model.generate(return_dict_in_generate=True, **batch)\n",
    "pred = aa.forward(out)\n",
    "ids = decode_output(pred)\n",
    "tgt_text = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
    "tgt_text = \" \".join(tgt_text)\n",
    "print(tgt_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "retired-tongue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.03676470588235294, recall=0.0008163265306122449, fmeasure=0.0015971889474524835),\n",
       " 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_text = \" \".join(tgt_text)\n",
    "\n",
    "score = scorer.score(tgt_text, summary)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-transfer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-invitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-count",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-headset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "beautiful-locator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "crucial-token",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "handmade-married",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-train",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-shannon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-albert",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
